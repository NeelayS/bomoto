Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=27, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1512-1567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950722
Iteration 2/25 | Loss: 0.00231684
Iteration 3/25 | Loss: 0.00157916
Iteration 4/25 | Loss: 0.00146808
Iteration 5/25 | Loss: 0.00141011
Iteration 6/25 | Loss: 0.00142268
Iteration 7/25 | Loss: 0.00125674
Iteration 8/25 | Loss: 0.00121837
Iteration 9/25 | Loss: 0.00122175
Iteration 10/25 | Loss: 0.00124294
Iteration 11/25 | Loss: 0.00124532
Iteration 12/25 | Loss: 0.00123290
Iteration 13/25 | Loss: 0.00122623
Iteration 14/25 | Loss: 0.00122743
Iteration 15/25 | Loss: 0.00122540
Iteration 16/25 | Loss: 0.00122670
Iteration 17/25 | Loss: 0.00122466
Iteration 18/25 | Loss: 0.00123957
Iteration 19/25 | Loss: 0.00123185
Iteration 20/25 | Loss: 0.00122133
Iteration 21/25 | Loss: 0.00121397
Iteration 22/25 | Loss: 0.00120507
Iteration 23/25 | Loss: 0.00119725
Iteration 24/25 | Loss: 0.00119595
Iteration 25/25 | Loss: 0.00119816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34610903
Iteration 2/25 | Loss: 0.00099635
Iteration 3/25 | Loss: 0.00099635
Iteration 4/25 | Loss: 0.00099035
Iteration 5/25 | Loss: 0.00099035
Iteration 6/25 | Loss: 0.00099035
Iteration 7/25 | Loss: 0.00099034
Iteration 8/25 | Loss: 0.00099034
Iteration 9/25 | Loss: 0.00099034
Iteration 10/25 | Loss: 0.00099034
Iteration 11/25 | Loss: 0.00099034
Iteration 12/25 | Loss: 0.00099034
Iteration 13/25 | Loss: 0.00099034
Iteration 14/25 | Loss: 0.00099034
Iteration 15/25 | Loss: 0.00099034
Iteration 16/25 | Loss: 0.00099034
Iteration 17/25 | Loss: 0.00099034
Iteration 18/25 | Loss: 0.00099034
Iteration 19/25 | Loss: 0.00099034
Iteration 20/25 | Loss: 0.00099034
Iteration 21/25 | Loss: 0.00099034
Iteration 22/25 | Loss: 0.00099034
Iteration 23/25 | Loss: 0.00099034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009903431637212634, 0.0009903431637212634, 0.0009903431637212634, 0.0009903431637212634, 0.0009903431637212634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009903431637212634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099034
Iteration 2/1000 | Loss: 0.00012178
Iteration 3/1000 | Loss: 0.00007867
Iteration 4/1000 | Loss: 0.00005209
Iteration 5/1000 | Loss: 0.00005432
Iteration 6/1000 | Loss: 0.00004681
Iteration 7/1000 | Loss: 0.00004777
Iteration 8/1000 | Loss: 0.00008399
Iteration 9/1000 | Loss: 0.00007137
Iteration 10/1000 | Loss: 0.00004501
Iteration 11/1000 | Loss: 0.00008153
Iteration 12/1000 | Loss: 0.00007187
Iteration 13/1000 | Loss: 0.00007029
Iteration 14/1000 | Loss: 0.00005705
Iteration 15/1000 | Loss: 0.00007251
Iteration 16/1000 | Loss: 0.00006298
Iteration 17/1000 | Loss: 0.00006657
Iteration 18/1000 | Loss: 0.00004117
Iteration 19/1000 | Loss: 0.00003625
Iteration 20/1000 | Loss: 0.00014861
Iteration 21/1000 | Loss: 0.00003710
Iteration 22/1000 | Loss: 0.00003285
Iteration 23/1000 | Loss: 0.00003101
Iteration 24/1000 | Loss: 0.00003010
Iteration 25/1000 | Loss: 0.00003098
Iteration 26/1000 | Loss: 0.00002872
Iteration 27/1000 | Loss: 0.00002826
Iteration 28/1000 | Loss: 0.00002775
Iteration 29/1000 | Loss: 0.00002723
Iteration 30/1000 | Loss: 0.00002693
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002632
Iteration 33/1000 | Loss: 0.00002603
Iteration 34/1000 | Loss: 0.00002582
Iteration 35/1000 | Loss: 0.00002578
Iteration 36/1000 | Loss: 0.00002576
Iteration 37/1000 | Loss: 0.00002575
Iteration 38/1000 | Loss: 0.00002574
Iteration 39/1000 | Loss: 0.00002574
Iteration 40/1000 | Loss: 0.00002573
Iteration 41/1000 | Loss: 0.00002573
Iteration 42/1000 | Loss: 0.00002569
Iteration 43/1000 | Loss: 0.00002564
Iteration 44/1000 | Loss: 0.00002563
Iteration 45/1000 | Loss: 0.00002562
Iteration 46/1000 | Loss: 0.00002560
Iteration 47/1000 | Loss: 0.00002559
Iteration 48/1000 | Loss: 0.00002554
Iteration 49/1000 | Loss: 0.00002550
Iteration 50/1000 | Loss: 0.00002548
Iteration 51/1000 | Loss: 0.00002546
Iteration 52/1000 | Loss: 0.00002545
Iteration 53/1000 | Loss: 0.00002542
Iteration 54/1000 | Loss: 0.00002542
Iteration 55/1000 | Loss: 0.00002541
Iteration 56/1000 | Loss: 0.00002541
Iteration 57/1000 | Loss: 0.00002541
Iteration 58/1000 | Loss: 0.00002540
Iteration 59/1000 | Loss: 0.00002540
Iteration 60/1000 | Loss: 0.00002539
Iteration 61/1000 | Loss: 0.00002538
Iteration 62/1000 | Loss: 0.00002538
Iteration 63/1000 | Loss: 0.00002537
Iteration 64/1000 | Loss: 0.00002537
Iteration 65/1000 | Loss: 0.00002536
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002535
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002535
Iteration 70/1000 | Loss: 0.00002535
Iteration 71/1000 | Loss: 0.00002534
Iteration 72/1000 | Loss: 0.00002534
Iteration 73/1000 | Loss: 0.00002534
Iteration 74/1000 | Loss: 0.00002533
Iteration 75/1000 | Loss: 0.00002533
Iteration 76/1000 | Loss: 0.00002533
Iteration 77/1000 | Loss: 0.00002533
Iteration 78/1000 | Loss: 0.00002533
Iteration 79/1000 | Loss: 0.00002532
Iteration 80/1000 | Loss: 0.00002532
Iteration 81/1000 | Loss: 0.00002532
Iteration 82/1000 | Loss: 0.00002532
Iteration 83/1000 | Loss: 0.00002531
Iteration 84/1000 | Loss: 0.00002531
Iteration 85/1000 | Loss: 0.00002530
Iteration 86/1000 | Loss: 0.00002529
Iteration 87/1000 | Loss: 0.00002529
Iteration 88/1000 | Loss: 0.00002528
Iteration 89/1000 | Loss: 0.00002528
Iteration 90/1000 | Loss: 0.00002528
Iteration 91/1000 | Loss: 0.00002527
Iteration 92/1000 | Loss: 0.00002527
Iteration 93/1000 | Loss: 0.00002527
Iteration 94/1000 | Loss: 0.00002527
Iteration 95/1000 | Loss: 0.00002527
Iteration 96/1000 | Loss: 0.00002526
Iteration 97/1000 | Loss: 0.00002526
Iteration 98/1000 | Loss: 0.00002526
Iteration 99/1000 | Loss: 0.00002526
Iteration 100/1000 | Loss: 0.00002525
Iteration 101/1000 | Loss: 0.00002525
Iteration 102/1000 | Loss: 0.00002525
Iteration 103/1000 | Loss: 0.00002524
Iteration 104/1000 | Loss: 0.00002524
Iteration 105/1000 | Loss: 0.00002524
Iteration 106/1000 | Loss: 0.00002524
Iteration 107/1000 | Loss: 0.00002524
Iteration 108/1000 | Loss: 0.00002524
Iteration 109/1000 | Loss: 0.00002524
Iteration 110/1000 | Loss: 0.00002524
Iteration 111/1000 | Loss: 0.00002524
Iteration 112/1000 | Loss: 0.00002523
Iteration 113/1000 | Loss: 0.00002523
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002522
Iteration 117/1000 | Loss: 0.00002522
Iteration 118/1000 | Loss: 0.00002522
Iteration 119/1000 | Loss: 0.00002521
Iteration 120/1000 | Loss: 0.00002521
Iteration 121/1000 | Loss: 0.00002521
Iteration 122/1000 | Loss: 0.00002520
Iteration 123/1000 | Loss: 0.00002520
Iteration 124/1000 | Loss: 0.00002520
Iteration 125/1000 | Loss: 0.00002520
Iteration 126/1000 | Loss: 0.00002520
Iteration 127/1000 | Loss: 0.00002520
Iteration 128/1000 | Loss: 0.00002519
Iteration 129/1000 | Loss: 0.00002519
Iteration 130/1000 | Loss: 0.00002519
Iteration 131/1000 | Loss: 0.00002519
Iteration 132/1000 | Loss: 0.00002519
Iteration 133/1000 | Loss: 0.00002519
Iteration 134/1000 | Loss: 0.00002518
Iteration 135/1000 | Loss: 0.00002518
Iteration 136/1000 | Loss: 0.00002518
Iteration 137/1000 | Loss: 0.00002518
Iteration 138/1000 | Loss: 0.00002518
Iteration 139/1000 | Loss: 0.00002517
Iteration 140/1000 | Loss: 0.00002517
Iteration 141/1000 | Loss: 0.00002517
Iteration 142/1000 | Loss: 0.00002517
Iteration 143/1000 | Loss: 0.00002517
Iteration 144/1000 | Loss: 0.00002517
Iteration 145/1000 | Loss: 0.00002517
Iteration 146/1000 | Loss: 0.00002517
Iteration 147/1000 | Loss: 0.00002517
Iteration 148/1000 | Loss: 0.00002517
Iteration 149/1000 | Loss: 0.00002517
Iteration 150/1000 | Loss: 0.00002517
Iteration 151/1000 | Loss: 0.00002517
Iteration 152/1000 | Loss: 0.00002517
Iteration 153/1000 | Loss: 0.00002517
Iteration 154/1000 | Loss: 0.00002516
Iteration 155/1000 | Loss: 0.00002516
Iteration 156/1000 | Loss: 0.00002516
Iteration 157/1000 | Loss: 0.00002516
Iteration 158/1000 | Loss: 0.00002516
Iteration 159/1000 | Loss: 0.00002516
Iteration 160/1000 | Loss: 0.00002516
Iteration 161/1000 | Loss: 0.00002516
Iteration 162/1000 | Loss: 0.00002516
Iteration 163/1000 | Loss: 0.00002516
Iteration 164/1000 | Loss: 0.00002516
Iteration 165/1000 | Loss: 0.00002515
Iteration 166/1000 | Loss: 0.00002515
Iteration 167/1000 | Loss: 0.00002515
Iteration 168/1000 | Loss: 0.00002515
Iteration 169/1000 | Loss: 0.00002515
Iteration 170/1000 | Loss: 0.00002514
Iteration 171/1000 | Loss: 0.00002514
Iteration 172/1000 | Loss: 0.00002514
Iteration 173/1000 | Loss: 0.00002514
Iteration 174/1000 | Loss: 0.00002513
Iteration 175/1000 | Loss: 0.00002513
Iteration 176/1000 | Loss: 0.00002513
Iteration 177/1000 | Loss: 0.00002513
Iteration 178/1000 | Loss: 0.00002513
Iteration 179/1000 | Loss: 0.00002513
Iteration 180/1000 | Loss: 0.00002513
Iteration 181/1000 | Loss: 0.00002513
Iteration 182/1000 | Loss: 0.00002513
Iteration 183/1000 | Loss: 0.00002513
Iteration 184/1000 | Loss: 0.00002513
Iteration 185/1000 | Loss: 0.00002513
Iteration 186/1000 | Loss: 0.00002512
Iteration 187/1000 | Loss: 0.00002512
Iteration 188/1000 | Loss: 0.00002512
Iteration 189/1000 | Loss: 0.00002512
Iteration 190/1000 | Loss: 0.00002512
Iteration 191/1000 | Loss: 0.00002512
Iteration 192/1000 | Loss: 0.00002512
Iteration 193/1000 | Loss: 0.00002512
Iteration 194/1000 | Loss: 0.00002511
Iteration 195/1000 | Loss: 0.00002511
Iteration 196/1000 | Loss: 0.00002511
Iteration 197/1000 | Loss: 0.00002511
Iteration 198/1000 | Loss: 0.00002511
Iteration 199/1000 | Loss: 0.00002511
Iteration 200/1000 | Loss: 0.00002511
Iteration 201/1000 | Loss: 0.00002511
Iteration 202/1000 | Loss: 0.00002511
Iteration 203/1000 | Loss: 0.00002510
Iteration 204/1000 | Loss: 0.00002510
Iteration 205/1000 | Loss: 0.00002510
Iteration 206/1000 | Loss: 0.00002510
Iteration 207/1000 | Loss: 0.00002510
Iteration 208/1000 | Loss: 0.00002510
Iteration 209/1000 | Loss: 0.00002510
Iteration 210/1000 | Loss: 0.00002510
Iteration 211/1000 | Loss: 0.00002510
Iteration 212/1000 | Loss: 0.00002510
Iteration 213/1000 | Loss: 0.00002510
Iteration 214/1000 | Loss: 0.00002510
Iteration 215/1000 | Loss: 0.00002510
Iteration 216/1000 | Loss: 0.00002510
Iteration 217/1000 | Loss: 0.00002510
Iteration 218/1000 | Loss: 0.00002510
Iteration 219/1000 | Loss: 0.00002510
Iteration 220/1000 | Loss: 0.00002510
Iteration 221/1000 | Loss: 0.00002510
Iteration 222/1000 | Loss: 0.00002510
Iteration 223/1000 | Loss: 0.00002510
Iteration 224/1000 | Loss: 0.00002510
Iteration 225/1000 | Loss: 0.00002510
Iteration 226/1000 | Loss: 0.00002510
Iteration 227/1000 | Loss: 0.00002510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.51016335823806e-05, 2.51016335823806e-05, 2.51016335823806e-05, 2.51016335823806e-05, 2.51016335823806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.51016335823806e-05

Optimization complete. Final v2v error: 3.8212788105010986 mm

Highest mean error: 10.365461349487305 mm for frame 20

Lowest mean error: 2.4906463623046875 mm for frame 2

Saving results

Total time: 108.81747269630432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773702
Iteration 2/25 | Loss: 0.00130102
Iteration 3/25 | Loss: 0.00112368
Iteration 4/25 | Loss: 0.00110226
Iteration 5/25 | Loss: 0.00109696
Iteration 6/25 | Loss: 0.00109576
Iteration 7/25 | Loss: 0.00109576
Iteration 8/25 | Loss: 0.00109576
Iteration 9/25 | Loss: 0.00109576
Iteration 10/25 | Loss: 0.00109576
Iteration 11/25 | Loss: 0.00109576
Iteration 12/25 | Loss: 0.00109576
Iteration 13/25 | Loss: 0.00109576
Iteration 14/25 | Loss: 0.00109576
Iteration 15/25 | Loss: 0.00109576
Iteration 16/25 | Loss: 0.00109576
Iteration 17/25 | Loss: 0.00109576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001095758518204093, 0.001095758518204093, 0.001095758518204093, 0.001095758518204093, 0.001095758518204093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095758518204093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26335907
Iteration 2/25 | Loss: 0.00068829
Iteration 3/25 | Loss: 0.00068829
Iteration 4/25 | Loss: 0.00068829
Iteration 5/25 | Loss: 0.00068829
Iteration 6/25 | Loss: 0.00068829
Iteration 7/25 | Loss: 0.00068828
Iteration 8/25 | Loss: 0.00068828
Iteration 9/25 | Loss: 0.00068828
Iteration 10/25 | Loss: 0.00068828
Iteration 11/25 | Loss: 0.00068828
Iteration 12/25 | Loss: 0.00068828
Iteration 13/25 | Loss: 0.00068828
Iteration 14/25 | Loss: 0.00068828
Iteration 15/25 | Loss: 0.00068828
Iteration 16/25 | Loss: 0.00068828
Iteration 17/25 | Loss: 0.00068828
Iteration 18/25 | Loss: 0.00068828
Iteration 19/25 | Loss: 0.00068828
Iteration 20/25 | Loss: 0.00068828
Iteration 21/25 | Loss: 0.00068828
Iteration 22/25 | Loss: 0.00068828
Iteration 23/25 | Loss: 0.00068828
Iteration 24/25 | Loss: 0.00068828
Iteration 25/25 | Loss: 0.00068828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068828
Iteration 2/1000 | Loss: 0.00004066
Iteration 3/1000 | Loss: 0.00002644
Iteration 4/1000 | Loss: 0.00002075
Iteration 5/1000 | Loss: 0.00001851
Iteration 6/1000 | Loss: 0.00001749
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001614
Iteration 9/1000 | Loss: 0.00001576
Iteration 10/1000 | Loss: 0.00001545
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001469
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001465
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001461
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001449
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001435
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001433
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001433
Iteration 128/1000 | Loss: 0.00001433
Iteration 129/1000 | Loss: 0.00001433
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.4327340977615677e-05, 1.4327340977615677e-05, 1.4327340977615677e-05, 1.4327340977615677e-05, 1.4327340977615677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4327340977615677e-05

Optimization complete. Final v2v error: 3.2185189723968506 mm

Highest mean error: 3.612269639968872 mm for frame 105

Lowest mean error: 2.6178040504455566 mm for frame 0

Saving results

Total time: 36.80617332458496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766999
Iteration 2/25 | Loss: 0.00143369
Iteration 3/25 | Loss: 0.00123420
Iteration 4/25 | Loss: 0.00119613
Iteration 5/25 | Loss: 0.00118049
Iteration 6/25 | Loss: 0.00119452
Iteration 7/25 | Loss: 0.00117459
Iteration 8/25 | Loss: 0.00116295
Iteration 9/25 | Loss: 0.00115452
Iteration 10/25 | Loss: 0.00115210
Iteration 11/25 | Loss: 0.00115099
Iteration 12/25 | Loss: 0.00115035
Iteration 13/25 | Loss: 0.00115017
Iteration 14/25 | Loss: 0.00115013
Iteration 15/25 | Loss: 0.00115013
Iteration 16/25 | Loss: 0.00115012
Iteration 17/25 | Loss: 0.00115012
Iteration 18/25 | Loss: 0.00115012
Iteration 19/25 | Loss: 0.00115012
Iteration 20/25 | Loss: 0.00115012
Iteration 21/25 | Loss: 0.00115012
Iteration 22/25 | Loss: 0.00115012
Iteration 23/25 | Loss: 0.00115012
Iteration 24/25 | Loss: 0.00115012
Iteration 25/25 | Loss: 0.00115012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79210007
Iteration 2/25 | Loss: 0.00075328
Iteration 3/25 | Loss: 0.00075327
Iteration 4/25 | Loss: 0.00075327
Iteration 5/25 | Loss: 0.00075327
Iteration 6/25 | Loss: 0.00075327
Iteration 7/25 | Loss: 0.00075327
Iteration 8/25 | Loss: 0.00075327
Iteration 9/25 | Loss: 0.00075327
Iteration 10/25 | Loss: 0.00075327
Iteration 11/25 | Loss: 0.00075327
Iteration 12/25 | Loss: 0.00075327
Iteration 13/25 | Loss: 0.00075327
Iteration 14/25 | Loss: 0.00075327
Iteration 15/25 | Loss: 0.00075327
Iteration 16/25 | Loss: 0.00075327
Iteration 17/25 | Loss: 0.00075327
Iteration 18/25 | Loss: 0.00075327
Iteration 19/25 | Loss: 0.00075327
Iteration 20/25 | Loss: 0.00075327
Iteration 21/25 | Loss: 0.00075327
Iteration 22/25 | Loss: 0.00075327
Iteration 23/25 | Loss: 0.00075327
Iteration 24/25 | Loss: 0.00075327
Iteration 25/25 | Loss: 0.00075327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075327
Iteration 2/1000 | Loss: 0.00005878
Iteration 3/1000 | Loss: 0.00002128
Iteration 4/1000 | Loss: 0.00004262
Iteration 5/1000 | Loss: 0.00001834
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00007227
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001650
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001649
Iteration 15/1000 | Loss: 0.00001649
Iteration 16/1000 | Loss: 0.00001647
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001627
Iteration 20/1000 | Loss: 0.00001626
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001621
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001613
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001609
Iteration 31/1000 | Loss: 0.00001608
Iteration 32/1000 | Loss: 0.00001604
Iteration 33/1000 | Loss: 0.00001604
Iteration 34/1000 | Loss: 0.00001604
Iteration 35/1000 | Loss: 0.00001604
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001603
Iteration 43/1000 | Loss: 0.00001603
Iteration 44/1000 | Loss: 0.00001603
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001598
Iteration 49/1000 | Loss: 0.00001597
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001594
Iteration 52/1000 | Loss: 0.00001594
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001593
Iteration 55/1000 | Loss: 0.00001593
Iteration 56/1000 | Loss: 0.00001593
Iteration 57/1000 | Loss: 0.00001593
Iteration 58/1000 | Loss: 0.00001592
Iteration 59/1000 | Loss: 0.00001592
Iteration 60/1000 | Loss: 0.00001592
Iteration 61/1000 | Loss: 0.00001592
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00001592
Iteration 66/1000 | Loss: 0.00001592
Iteration 67/1000 | Loss: 0.00001591
Iteration 68/1000 | Loss: 0.00001591
Iteration 69/1000 | Loss: 0.00001591
Iteration 70/1000 | Loss: 0.00001591
Iteration 71/1000 | Loss: 0.00001591
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001584
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001583
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001582
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001580
Iteration 123/1000 | Loss: 0.00001580
Iteration 124/1000 | Loss: 0.00001580
Iteration 125/1000 | Loss: 0.00001580
Iteration 126/1000 | Loss: 0.00001580
Iteration 127/1000 | Loss: 0.00001580
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001579
Iteration 131/1000 | Loss: 0.00001579
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001577
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001576
Iteration 144/1000 | Loss: 0.00001576
Iteration 145/1000 | Loss: 0.00001576
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001575
Iteration 151/1000 | Loss: 0.00001575
Iteration 152/1000 | Loss: 0.00001575
Iteration 153/1000 | Loss: 0.00001575
Iteration 154/1000 | Loss: 0.00001575
Iteration 155/1000 | Loss: 0.00001574
Iteration 156/1000 | Loss: 0.00001574
Iteration 157/1000 | Loss: 0.00001574
Iteration 158/1000 | Loss: 0.00001574
Iteration 159/1000 | Loss: 0.00001574
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001572
Iteration 163/1000 | Loss: 0.00001572
Iteration 164/1000 | Loss: 0.00001572
Iteration 165/1000 | Loss: 0.00001572
Iteration 166/1000 | Loss: 0.00001572
Iteration 167/1000 | Loss: 0.00001572
Iteration 168/1000 | Loss: 0.00001572
Iteration 169/1000 | Loss: 0.00001572
Iteration 170/1000 | Loss: 0.00001572
Iteration 171/1000 | Loss: 0.00001571
Iteration 172/1000 | Loss: 0.00001571
Iteration 173/1000 | Loss: 0.00001571
Iteration 174/1000 | Loss: 0.00001571
Iteration 175/1000 | Loss: 0.00001571
Iteration 176/1000 | Loss: 0.00001571
Iteration 177/1000 | Loss: 0.00001570
Iteration 178/1000 | Loss: 0.00001570
Iteration 179/1000 | Loss: 0.00001570
Iteration 180/1000 | Loss: 0.00001570
Iteration 181/1000 | Loss: 0.00001570
Iteration 182/1000 | Loss: 0.00001570
Iteration 183/1000 | Loss: 0.00001570
Iteration 184/1000 | Loss: 0.00001570
Iteration 185/1000 | Loss: 0.00001569
Iteration 186/1000 | Loss: 0.00001569
Iteration 187/1000 | Loss: 0.00001569
Iteration 188/1000 | Loss: 0.00001569
Iteration 189/1000 | Loss: 0.00001569
Iteration 190/1000 | Loss: 0.00001569
Iteration 191/1000 | Loss: 0.00001569
Iteration 192/1000 | Loss: 0.00001569
Iteration 193/1000 | Loss: 0.00001569
Iteration 194/1000 | Loss: 0.00001569
Iteration 195/1000 | Loss: 0.00001569
Iteration 196/1000 | Loss: 0.00001569
Iteration 197/1000 | Loss: 0.00001569
Iteration 198/1000 | Loss: 0.00001569
Iteration 199/1000 | Loss: 0.00001569
Iteration 200/1000 | Loss: 0.00001569
Iteration 201/1000 | Loss: 0.00001569
Iteration 202/1000 | Loss: 0.00001569
Iteration 203/1000 | Loss: 0.00001568
Iteration 204/1000 | Loss: 0.00001568
Iteration 205/1000 | Loss: 0.00001568
Iteration 206/1000 | Loss: 0.00001568
Iteration 207/1000 | Loss: 0.00001568
Iteration 208/1000 | Loss: 0.00001568
Iteration 209/1000 | Loss: 0.00001568
Iteration 210/1000 | Loss: 0.00001568
Iteration 211/1000 | Loss: 0.00001568
Iteration 212/1000 | Loss: 0.00001568
Iteration 213/1000 | Loss: 0.00001568
Iteration 214/1000 | Loss: 0.00001568
Iteration 215/1000 | Loss: 0.00001568
Iteration 216/1000 | Loss: 0.00001568
Iteration 217/1000 | Loss: 0.00001568
Iteration 218/1000 | Loss: 0.00001568
Iteration 219/1000 | Loss: 0.00001568
Iteration 220/1000 | Loss: 0.00001568
Iteration 221/1000 | Loss: 0.00001568
Iteration 222/1000 | Loss: 0.00001568
Iteration 223/1000 | Loss: 0.00001568
Iteration 224/1000 | Loss: 0.00001568
Iteration 225/1000 | Loss: 0.00001568
Iteration 226/1000 | Loss: 0.00001568
Iteration 227/1000 | Loss: 0.00001568
Iteration 228/1000 | Loss: 0.00001568
Iteration 229/1000 | Loss: 0.00001568
Iteration 230/1000 | Loss: 0.00001568
Iteration 231/1000 | Loss: 0.00001568
Iteration 232/1000 | Loss: 0.00001568
Iteration 233/1000 | Loss: 0.00001568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.5678118870710023e-05, 1.5678118870710023e-05, 1.5678118870710023e-05, 1.5678118870710023e-05, 1.5678118870710023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5678118870710023e-05

Optimization complete. Final v2v error: 3.2114789485931396 mm

Highest mean error: 4.1479644775390625 mm for frame 117

Lowest mean error: 2.767171621322632 mm for frame 22

Saving results

Total time: 55.89890217781067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868989
Iteration 2/25 | Loss: 0.00165170
Iteration 3/25 | Loss: 0.00130336
Iteration 4/25 | Loss: 0.00130138
Iteration 5/25 | Loss: 0.00125898
Iteration 6/25 | Loss: 0.00121325
Iteration 7/25 | Loss: 0.00120339
Iteration 8/25 | Loss: 0.00119693
Iteration 9/25 | Loss: 0.00119140
Iteration 10/25 | Loss: 0.00119050
Iteration 11/25 | Loss: 0.00119031
Iteration 12/25 | Loss: 0.00119012
Iteration 13/25 | Loss: 0.00118994
Iteration 14/25 | Loss: 0.00118986
Iteration 15/25 | Loss: 0.00118982
Iteration 16/25 | Loss: 0.00118981
Iteration 17/25 | Loss: 0.00118981
Iteration 18/25 | Loss: 0.00118981
Iteration 19/25 | Loss: 0.00118981
Iteration 20/25 | Loss: 0.00118981
Iteration 21/25 | Loss: 0.00118981
Iteration 22/25 | Loss: 0.00118981
Iteration 23/25 | Loss: 0.00118981
Iteration 24/25 | Loss: 0.00118981
Iteration 25/25 | Loss: 0.00118981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52270126
Iteration 2/25 | Loss: 0.00079921
Iteration 3/25 | Loss: 0.00079917
Iteration 4/25 | Loss: 0.00079916
Iteration 5/25 | Loss: 0.00079916
Iteration 6/25 | Loss: 0.00079916
Iteration 7/25 | Loss: 0.00079916
Iteration 8/25 | Loss: 0.00079916
Iteration 9/25 | Loss: 0.00079916
Iteration 10/25 | Loss: 0.00079916
Iteration 11/25 | Loss: 0.00079916
Iteration 12/25 | Loss: 0.00079916
Iteration 13/25 | Loss: 0.00079916
Iteration 14/25 | Loss: 0.00079916
Iteration 15/25 | Loss: 0.00079916
Iteration 16/25 | Loss: 0.00079916
Iteration 17/25 | Loss: 0.00079916
Iteration 18/25 | Loss: 0.00079916
Iteration 19/25 | Loss: 0.00079916
Iteration 20/25 | Loss: 0.00079916
Iteration 21/25 | Loss: 0.00079916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007991626625880599, 0.0007991626625880599, 0.0007991626625880599, 0.0007991626625880599, 0.0007991626625880599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007991626625880599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079916
Iteration 2/1000 | Loss: 0.00004593
Iteration 3/1000 | Loss: 0.00003193
Iteration 4/1000 | Loss: 0.00002916
Iteration 5/1000 | Loss: 0.00002770
Iteration 6/1000 | Loss: 0.00002667
Iteration 7/1000 | Loss: 0.00002581
Iteration 8/1000 | Loss: 0.00002534
Iteration 9/1000 | Loss: 0.00019849
Iteration 10/1000 | Loss: 0.00005254
Iteration 11/1000 | Loss: 0.00002866
Iteration 12/1000 | Loss: 0.00011357
Iteration 13/1000 | Loss: 0.00002471
Iteration 14/1000 | Loss: 0.00002395
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002307
Iteration 17/1000 | Loss: 0.00002279
Iteration 18/1000 | Loss: 0.00005459
Iteration 19/1000 | Loss: 0.00002271
Iteration 20/1000 | Loss: 0.00002271
Iteration 21/1000 | Loss: 0.00002270
Iteration 22/1000 | Loss: 0.00002267
Iteration 23/1000 | Loss: 0.00002255
Iteration 24/1000 | Loss: 0.00002253
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002250
Iteration 27/1000 | Loss: 0.00002250
Iteration 28/1000 | Loss: 0.00002250
Iteration 29/1000 | Loss: 0.00002249
Iteration 30/1000 | Loss: 0.00002249
Iteration 31/1000 | Loss: 0.00002249
Iteration 32/1000 | Loss: 0.00002248
Iteration 33/1000 | Loss: 0.00002247
Iteration 34/1000 | Loss: 0.00002246
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00002245
Iteration 37/1000 | Loss: 0.00002245
Iteration 38/1000 | Loss: 0.00002240
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002239
Iteration 42/1000 | Loss: 0.00006570
Iteration 43/1000 | Loss: 0.00017203
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00008321
Iteration 46/1000 | Loss: 0.00002410
Iteration 47/1000 | Loss: 0.00002267
Iteration 48/1000 | Loss: 0.00002240
Iteration 49/1000 | Loss: 0.00002233
Iteration 50/1000 | Loss: 0.00002231
Iteration 51/1000 | Loss: 0.00002231
Iteration 52/1000 | Loss: 0.00002231
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002229
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002228
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002228
Iteration 64/1000 | Loss: 0.00002228
Iteration 65/1000 | Loss: 0.00002228
Iteration 66/1000 | Loss: 0.00002227
Iteration 67/1000 | Loss: 0.00002227
Iteration 68/1000 | Loss: 0.00002227
Iteration 69/1000 | Loss: 0.00002227
Iteration 70/1000 | Loss: 0.00002227
Iteration 71/1000 | Loss: 0.00002227
Iteration 72/1000 | Loss: 0.00002227
Iteration 73/1000 | Loss: 0.00002227
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002226
Iteration 76/1000 | Loss: 0.00002226
Iteration 77/1000 | Loss: 0.00002226
Iteration 78/1000 | Loss: 0.00002226
Iteration 79/1000 | Loss: 0.00002226
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00002225
Iteration 82/1000 | Loss: 0.00002225
Iteration 83/1000 | Loss: 0.00002225
Iteration 84/1000 | Loss: 0.00002225
Iteration 85/1000 | Loss: 0.00002225
Iteration 86/1000 | Loss: 0.00002224
Iteration 87/1000 | Loss: 0.00002224
Iteration 88/1000 | Loss: 0.00002224
Iteration 89/1000 | Loss: 0.00002224
Iteration 90/1000 | Loss: 0.00002224
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002224
Iteration 93/1000 | Loss: 0.00002224
Iteration 94/1000 | Loss: 0.00002224
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002223
Iteration 97/1000 | Loss: 0.00002223
Iteration 98/1000 | Loss: 0.00002223
Iteration 99/1000 | Loss: 0.00002223
Iteration 100/1000 | Loss: 0.00002223
Iteration 101/1000 | Loss: 0.00002223
Iteration 102/1000 | Loss: 0.00002223
Iteration 103/1000 | Loss: 0.00002223
Iteration 104/1000 | Loss: 0.00002223
Iteration 105/1000 | Loss: 0.00002222
Iteration 106/1000 | Loss: 0.00002222
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002222
Iteration 110/1000 | Loss: 0.00002222
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002222
Iteration 114/1000 | Loss: 0.00002222
Iteration 115/1000 | Loss: 0.00002222
Iteration 116/1000 | Loss: 0.00002222
Iteration 117/1000 | Loss: 0.00002222
Iteration 118/1000 | Loss: 0.00002222
Iteration 119/1000 | Loss: 0.00002221
Iteration 120/1000 | Loss: 0.00002221
Iteration 121/1000 | Loss: 0.00002221
Iteration 122/1000 | Loss: 0.00002221
Iteration 123/1000 | Loss: 0.00002221
Iteration 124/1000 | Loss: 0.00002221
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Iteration 127/1000 | Loss: 0.00002221
Iteration 128/1000 | Loss: 0.00002221
Iteration 129/1000 | Loss: 0.00002221
Iteration 130/1000 | Loss: 0.00002221
Iteration 131/1000 | Loss: 0.00002221
Iteration 132/1000 | Loss: 0.00002221
Iteration 133/1000 | Loss: 0.00002221
Iteration 134/1000 | Loss: 0.00002221
Iteration 135/1000 | Loss: 0.00002221
Iteration 136/1000 | Loss: 0.00002221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.2208665541256778e-05, 2.2208665541256778e-05, 2.2208665541256778e-05, 2.2208665541256778e-05, 2.2208665541256778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2208665541256778e-05

Optimization complete. Final v2v error: 3.9365668296813965 mm

Highest mean error: 5.1013383865356445 mm for frame 53

Lowest mean error: 3.198850631713867 mm for frame 150

Saving results

Total time: 78.44142985343933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084527
Iteration 2/25 | Loss: 0.00205763
Iteration 3/25 | Loss: 0.00128614
Iteration 4/25 | Loss: 0.00124771
Iteration 5/25 | Loss: 0.00124980
Iteration 6/25 | Loss: 0.00123139
Iteration 7/25 | Loss: 0.00121294
Iteration 8/25 | Loss: 0.00120523
Iteration 9/25 | Loss: 0.00120474
Iteration 10/25 | Loss: 0.00120969
Iteration 11/25 | Loss: 0.00120885
Iteration 12/25 | Loss: 0.00120380
Iteration 13/25 | Loss: 0.00120043
Iteration 14/25 | Loss: 0.00120130
Iteration 15/25 | Loss: 0.00119885
Iteration 16/25 | Loss: 0.00119591
Iteration 17/25 | Loss: 0.00119971
Iteration 18/25 | Loss: 0.00120213
Iteration 19/25 | Loss: 0.00119493
Iteration 20/25 | Loss: 0.00119500
Iteration 21/25 | Loss: 0.00119275
Iteration 22/25 | Loss: 0.00119840
Iteration 23/25 | Loss: 0.00119383
Iteration 24/25 | Loss: 0.00119289
Iteration 25/25 | Loss: 0.00119300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05777252
Iteration 2/25 | Loss: 0.00078371
Iteration 3/25 | Loss: 0.00078371
Iteration 4/25 | Loss: 0.00078371
Iteration 5/25 | Loss: 0.00078371
Iteration 6/25 | Loss: 0.00078371
Iteration 7/25 | Loss: 0.00078371
Iteration 8/25 | Loss: 0.00078371
Iteration 9/25 | Loss: 0.00078371
Iteration 10/25 | Loss: 0.00078371
Iteration 11/25 | Loss: 0.00078371
Iteration 12/25 | Loss: 0.00078370
Iteration 13/25 | Loss: 0.00078370
Iteration 14/25 | Loss: 0.00078370
Iteration 15/25 | Loss: 0.00078370
Iteration 16/25 | Loss: 0.00078370
Iteration 17/25 | Loss: 0.00078370
Iteration 18/25 | Loss: 0.00078370
Iteration 19/25 | Loss: 0.00078370
Iteration 20/25 | Loss: 0.00078370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007837045704945922, 0.0007837045704945922, 0.0007837045704945922, 0.0007837045704945922, 0.0007837045704945922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007837045704945922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078370
Iteration 2/1000 | Loss: 0.00004966
Iteration 3/1000 | Loss: 0.00003410
Iteration 4/1000 | Loss: 0.00002947
Iteration 5/1000 | Loss: 0.00002673
Iteration 6/1000 | Loss: 0.00002450
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00003315
Iteration 9/1000 | Loss: 0.00002238
Iteration 10/1000 | Loss: 0.00002144
Iteration 11/1000 | Loss: 0.00019649
Iteration 12/1000 | Loss: 0.00002839
Iteration 13/1000 | Loss: 0.00002328
Iteration 14/1000 | Loss: 0.00001982
Iteration 15/1000 | Loss: 0.00002648
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00002641
Iteration 18/1000 | Loss: 0.00001837
Iteration 19/1000 | Loss: 0.00002860
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001814
Iteration 23/1000 | Loss: 0.00001814
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00004236
Iteration 27/1000 | Loss: 0.00001800
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001975
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001778
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00002455
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001764
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001760
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.7598902559257112e-05, 1.7598902559257112e-05, 1.7598902559257112e-05, 1.7598902559257112e-05, 1.7598902559257112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7598902559257112e-05

Optimization complete. Final v2v error: 3.4926838874816895 mm

Highest mean error: 4.255368709564209 mm for frame 19

Lowest mean error: 3.301257848739624 mm for frame 1

Saving results

Total time: 87.99925804138184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479588
Iteration 2/25 | Loss: 0.00139271
Iteration 3/25 | Loss: 0.00114995
Iteration 4/25 | Loss: 0.00112007
Iteration 5/25 | Loss: 0.00111674
Iteration 6/25 | Loss: 0.00111609
Iteration 7/25 | Loss: 0.00111609
Iteration 8/25 | Loss: 0.00111609
Iteration 9/25 | Loss: 0.00111609
Iteration 10/25 | Loss: 0.00111609
Iteration 11/25 | Loss: 0.00111609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011160927824676037, 0.0011160927824676037, 0.0011160927824676037, 0.0011160927824676037, 0.0011160927824676037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011160927824676037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36986160
Iteration 2/25 | Loss: 0.00081158
Iteration 3/25 | Loss: 0.00081157
Iteration 4/25 | Loss: 0.00081157
Iteration 5/25 | Loss: 0.00081157
Iteration 6/25 | Loss: 0.00081157
Iteration 7/25 | Loss: 0.00081157
Iteration 8/25 | Loss: 0.00081157
Iteration 9/25 | Loss: 0.00081157
Iteration 10/25 | Loss: 0.00081157
Iteration 11/25 | Loss: 0.00081157
Iteration 12/25 | Loss: 0.00081157
Iteration 13/25 | Loss: 0.00081157
Iteration 14/25 | Loss: 0.00081157
Iteration 15/25 | Loss: 0.00081157
Iteration 16/25 | Loss: 0.00081157
Iteration 17/25 | Loss: 0.00081157
Iteration 18/25 | Loss: 0.00081157
Iteration 19/25 | Loss: 0.00081157
Iteration 20/25 | Loss: 0.00081157
Iteration 21/25 | Loss: 0.00081157
Iteration 22/25 | Loss: 0.00081157
Iteration 23/25 | Loss: 0.00081157
Iteration 24/25 | Loss: 0.00081157
Iteration 25/25 | Loss: 0.00081157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081157
Iteration 2/1000 | Loss: 0.00003087
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001581
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001416
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001253
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001232
Iteration 24/1000 | Loss: 0.00001231
Iteration 25/1000 | Loss: 0.00001231
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001227
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001226
Iteration 30/1000 | Loss: 0.00001226
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001215
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001212
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001208
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001196
Iteration 163/1000 | Loss: 0.00001196
Iteration 164/1000 | Loss: 0.00001196
Iteration 165/1000 | Loss: 0.00001196
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001195
Iteration 168/1000 | Loss: 0.00001195
Iteration 169/1000 | Loss: 0.00001195
Iteration 170/1000 | Loss: 0.00001195
Iteration 171/1000 | Loss: 0.00001195
Iteration 172/1000 | Loss: 0.00001195
Iteration 173/1000 | Loss: 0.00001195
Iteration 174/1000 | Loss: 0.00001195
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Iteration 196/1000 | Loss: 0.00001195
Iteration 197/1000 | Loss: 0.00001195
Iteration 198/1000 | Loss: 0.00001195
Iteration 199/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1946041013288777e-05, 1.1946041013288777e-05, 1.1946041013288777e-05, 1.1946041013288777e-05, 1.1946041013288777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1946041013288777e-05

Optimization complete. Final v2v error: 2.7747750282287598 mm

Highest mean error: 4.300828456878662 mm for frame 81

Lowest mean error: 2.3458425998687744 mm for frame 156

Saving results

Total time: 40.46819877624512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570675
Iteration 2/25 | Loss: 0.00132677
Iteration 3/25 | Loss: 0.00120022
Iteration 4/25 | Loss: 0.00118091
Iteration 5/25 | Loss: 0.00117430
Iteration 6/25 | Loss: 0.00117323
Iteration 7/25 | Loss: 0.00117323
Iteration 8/25 | Loss: 0.00117323
Iteration 9/25 | Loss: 0.00117323
Iteration 10/25 | Loss: 0.00117323
Iteration 11/25 | Loss: 0.00117323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011732324492186308, 0.0011732324492186308, 0.0011732324492186308, 0.0011732324492186308, 0.0011732324492186308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011732324492186308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31915784
Iteration 2/25 | Loss: 0.00115934
Iteration 3/25 | Loss: 0.00115929
Iteration 4/25 | Loss: 0.00115929
Iteration 5/25 | Loss: 0.00115929
Iteration 6/25 | Loss: 0.00115929
Iteration 7/25 | Loss: 0.00115928
Iteration 8/25 | Loss: 0.00115928
Iteration 9/25 | Loss: 0.00115928
Iteration 10/25 | Loss: 0.00115928
Iteration 11/25 | Loss: 0.00115928
Iteration 12/25 | Loss: 0.00115928
Iteration 13/25 | Loss: 0.00115928
Iteration 14/25 | Loss: 0.00115928
Iteration 15/25 | Loss: 0.00115928
Iteration 16/25 | Loss: 0.00115928
Iteration 17/25 | Loss: 0.00115928
Iteration 18/25 | Loss: 0.00115928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011592840310186148, 0.0011592840310186148, 0.0011592840310186148, 0.0011592840310186148, 0.0011592840310186148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011592840310186148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115928
Iteration 2/1000 | Loss: 0.00003733
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002351
Iteration 5/1000 | Loss: 0.00002250
Iteration 6/1000 | Loss: 0.00002149
Iteration 7/1000 | Loss: 0.00002097
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002019
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001952
Iteration 13/1000 | Loss: 0.00001946
Iteration 14/1000 | Loss: 0.00001938
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001916
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001915
Iteration 23/1000 | Loss: 0.00001913
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001893
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001892
Iteration 31/1000 | Loss: 0.00001891
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001886
Iteration 37/1000 | Loss: 0.00001886
Iteration 38/1000 | Loss: 0.00001886
Iteration 39/1000 | Loss: 0.00001885
Iteration 40/1000 | Loss: 0.00001885
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001878
Iteration 65/1000 | Loss: 0.00001878
Iteration 66/1000 | Loss: 0.00001878
Iteration 67/1000 | Loss: 0.00001877
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001877
Iteration 70/1000 | Loss: 0.00001877
Iteration 71/1000 | Loss: 0.00001877
Iteration 72/1000 | Loss: 0.00001877
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001876
Iteration 76/1000 | Loss: 0.00001875
Iteration 77/1000 | Loss: 0.00001875
Iteration 78/1000 | Loss: 0.00001875
Iteration 79/1000 | Loss: 0.00001875
Iteration 80/1000 | Loss: 0.00001875
Iteration 81/1000 | Loss: 0.00001875
Iteration 82/1000 | Loss: 0.00001875
Iteration 83/1000 | Loss: 0.00001875
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001875
Iteration 88/1000 | Loss: 0.00001875
Iteration 89/1000 | Loss: 0.00001875
Iteration 90/1000 | Loss: 0.00001875
Iteration 91/1000 | Loss: 0.00001875
Iteration 92/1000 | Loss: 0.00001875
Iteration 93/1000 | Loss: 0.00001875
Iteration 94/1000 | Loss: 0.00001874
Iteration 95/1000 | Loss: 0.00001874
Iteration 96/1000 | Loss: 0.00001874
Iteration 97/1000 | Loss: 0.00001874
Iteration 98/1000 | Loss: 0.00001874
Iteration 99/1000 | Loss: 0.00001874
Iteration 100/1000 | Loss: 0.00001874
Iteration 101/1000 | Loss: 0.00001874
Iteration 102/1000 | Loss: 0.00001873
Iteration 103/1000 | Loss: 0.00001873
Iteration 104/1000 | Loss: 0.00001873
Iteration 105/1000 | Loss: 0.00001873
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001869
Iteration 124/1000 | Loss: 0.00001869
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001869
Iteration 127/1000 | Loss: 0.00001869
Iteration 128/1000 | Loss: 0.00001869
Iteration 129/1000 | Loss: 0.00001869
Iteration 130/1000 | Loss: 0.00001869
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001869
Iteration 134/1000 | Loss: 0.00001868
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001868
Iteration 137/1000 | Loss: 0.00001868
Iteration 138/1000 | Loss: 0.00001868
Iteration 139/1000 | Loss: 0.00001868
Iteration 140/1000 | Loss: 0.00001868
Iteration 141/1000 | Loss: 0.00001868
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001867
Iteration 144/1000 | Loss: 0.00001867
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001866
Iteration 148/1000 | Loss: 0.00001866
Iteration 149/1000 | Loss: 0.00001866
Iteration 150/1000 | Loss: 0.00001866
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001866
Iteration 157/1000 | Loss: 0.00001866
Iteration 158/1000 | Loss: 0.00001866
Iteration 159/1000 | Loss: 0.00001866
Iteration 160/1000 | Loss: 0.00001866
Iteration 161/1000 | Loss: 0.00001866
Iteration 162/1000 | Loss: 0.00001866
Iteration 163/1000 | Loss: 0.00001866
Iteration 164/1000 | Loss: 0.00001866
Iteration 165/1000 | Loss: 0.00001866
Iteration 166/1000 | Loss: 0.00001866
Iteration 167/1000 | Loss: 0.00001866
Iteration 168/1000 | Loss: 0.00001866
Iteration 169/1000 | Loss: 0.00001866
Iteration 170/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.865880039986223e-05, 1.865880039986223e-05, 1.865880039986223e-05, 1.865880039986223e-05, 1.865880039986223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.865880039986223e-05

Optimization complete. Final v2v error: 3.6238319873809814 mm

Highest mean error: 4.204862594604492 mm for frame 14

Lowest mean error: 3.272660970687866 mm for frame 114

Saving results

Total time: 44.939675092697144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383048
Iteration 2/25 | Loss: 0.00121560
Iteration 3/25 | Loss: 0.00112798
Iteration 4/25 | Loss: 0.00111360
Iteration 5/25 | Loss: 0.00110760
Iteration 6/25 | Loss: 0.00110598
Iteration 7/25 | Loss: 0.00110598
Iteration 8/25 | Loss: 0.00110598
Iteration 9/25 | Loss: 0.00110598
Iteration 10/25 | Loss: 0.00110598
Iteration 11/25 | Loss: 0.00110598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011059847893193364, 0.0011059847893193364, 0.0011059847893193364, 0.0011059847893193364, 0.0011059847893193364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011059847893193364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40365791
Iteration 2/25 | Loss: 0.00096135
Iteration 3/25 | Loss: 0.00096133
Iteration 4/25 | Loss: 0.00096133
Iteration 5/25 | Loss: 0.00096133
Iteration 6/25 | Loss: 0.00096133
Iteration 7/25 | Loss: 0.00096133
Iteration 8/25 | Loss: 0.00096133
Iteration 9/25 | Loss: 0.00096133
Iteration 10/25 | Loss: 0.00096133
Iteration 11/25 | Loss: 0.00096133
Iteration 12/25 | Loss: 0.00096133
Iteration 13/25 | Loss: 0.00096133
Iteration 14/25 | Loss: 0.00096133
Iteration 15/25 | Loss: 0.00096133
Iteration 16/25 | Loss: 0.00096133
Iteration 17/25 | Loss: 0.00096133
Iteration 18/25 | Loss: 0.00096133
Iteration 19/25 | Loss: 0.00096133
Iteration 20/25 | Loss: 0.00096133
Iteration 21/25 | Loss: 0.00096133
Iteration 22/25 | Loss: 0.00096133
Iteration 23/25 | Loss: 0.00096133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009613262373022735, 0.0009613262373022735, 0.0009613262373022735, 0.0009613262373022735, 0.0009613262373022735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009613262373022735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096133
Iteration 2/1000 | Loss: 0.00002748
Iteration 3/1000 | Loss: 0.00001729
Iteration 4/1000 | Loss: 0.00001514
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001215
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001210
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001199
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001189
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001184
Iteration 82/1000 | Loss: 0.00001184
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001184
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.1841613741125911e-05, 1.1841613741125911e-05, 1.1841613741125911e-05, 1.1841613741125911e-05, 1.1841613741125911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1841613741125911e-05

Optimization complete. Final v2v error: 2.9227309226989746 mm

Highest mean error: 3.452087640762329 mm for frame 78

Lowest mean error: 2.5586423873901367 mm for frame 54

Saving results

Total time: 31.418079137802124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467355
Iteration 2/25 | Loss: 0.00135855
Iteration 3/25 | Loss: 0.00117131
Iteration 4/25 | Loss: 0.00115373
Iteration 5/25 | Loss: 0.00115063
Iteration 6/25 | Loss: 0.00114955
Iteration 7/25 | Loss: 0.00114950
Iteration 8/25 | Loss: 0.00114950
Iteration 9/25 | Loss: 0.00114950
Iteration 10/25 | Loss: 0.00114950
Iteration 11/25 | Loss: 0.00114950
Iteration 12/25 | Loss: 0.00114950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011495010694488883, 0.0011495010694488883, 0.0011495010694488883, 0.0011495010694488883, 0.0011495010694488883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011495010694488883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40942240
Iteration 2/25 | Loss: 0.00077896
Iteration 3/25 | Loss: 0.00077894
Iteration 4/25 | Loss: 0.00077894
Iteration 5/25 | Loss: 0.00077894
Iteration 6/25 | Loss: 0.00077894
Iteration 7/25 | Loss: 0.00077894
Iteration 8/25 | Loss: 0.00077894
Iteration 9/25 | Loss: 0.00077894
Iteration 10/25 | Loss: 0.00077894
Iteration 11/25 | Loss: 0.00077894
Iteration 12/25 | Loss: 0.00077894
Iteration 13/25 | Loss: 0.00077894
Iteration 14/25 | Loss: 0.00077894
Iteration 15/25 | Loss: 0.00077894
Iteration 16/25 | Loss: 0.00077894
Iteration 17/25 | Loss: 0.00077894
Iteration 18/25 | Loss: 0.00077894
Iteration 19/25 | Loss: 0.00077894
Iteration 20/25 | Loss: 0.00077894
Iteration 21/25 | Loss: 0.00077894
Iteration 22/25 | Loss: 0.00077894
Iteration 23/25 | Loss: 0.00077894
Iteration 24/25 | Loss: 0.00077894
Iteration 25/25 | Loss: 0.00077894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007789413211867213, 0.0007789413211867213, 0.0007789413211867213, 0.0007789413211867213, 0.0007789413211867213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007789413211867213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077894
Iteration 2/1000 | Loss: 0.00003504
Iteration 3/1000 | Loss: 0.00002237
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001644
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001540
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001464
Iteration 11/1000 | Loss: 0.00001443
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001418
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001416
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001410
Iteration 24/1000 | Loss: 0.00001409
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001397
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001392
Iteration 31/1000 | Loss: 0.00001392
Iteration 32/1000 | Loss: 0.00001392
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001390
Iteration 37/1000 | Loss: 0.00001389
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001382
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001381
Iteration 62/1000 | Loss: 0.00001381
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001379
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001378
Iteration 67/1000 | Loss: 0.00001378
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001371
Iteration 86/1000 | Loss: 0.00001371
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001364
Iteration 106/1000 | Loss: 0.00001364
Iteration 107/1000 | Loss: 0.00001364
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001362
Iteration 118/1000 | Loss: 0.00001362
Iteration 119/1000 | Loss: 0.00001362
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001359
Iteration 128/1000 | Loss: 0.00001359
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001358
Iteration 131/1000 | Loss: 0.00001358
Iteration 132/1000 | Loss: 0.00001358
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Iteration 155/1000 | Loss: 0.00001356
Iteration 156/1000 | Loss: 0.00001356
Iteration 157/1000 | Loss: 0.00001356
Iteration 158/1000 | Loss: 0.00001356
Iteration 159/1000 | Loss: 0.00001356
Iteration 160/1000 | Loss: 0.00001356
Iteration 161/1000 | Loss: 0.00001356
Iteration 162/1000 | Loss: 0.00001356
Iteration 163/1000 | Loss: 0.00001356
Iteration 164/1000 | Loss: 0.00001356
Iteration 165/1000 | Loss: 0.00001356
Iteration 166/1000 | Loss: 0.00001356
Iteration 167/1000 | Loss: 0.00001356
Iteration 168/1000 | Loss: 0.00001356
Iteration 169/1000 | Loss: 0.00001356
Iteration 170/1000 | Loss: 0.00001356
Iteration 171/1000 | Loss: 0.00001356
Iteration 172/1000 | Loss: 0.00001356
Iteration 173/1000 | Loss: 0.00001356
Iteration 174/1000 | Loss: 0.00001356
Iteration 175/1000 | Loss: 0.00001356
Iteration 176/1000 | Loss: 0.00001356
Iteration 177/1000 | Loss: 0.00001356
Iteration 178/1000 | Loss: 0.00001356
Iteration 179/1000 | Loss: 0.00001356
Iteration 180/1000 | Loss: 0.00001356
Iteration 181/1000 | Loss: 0.00001356
Iteration 182/1000 | Loss: 0.00001356
Iteration 183/1000 | Loss: 0.00001356
Iteration 184/1000 | Loss: 0.00001356
Iteration 185/1000 | Loss: 0.00001356
Iteration 186/1000 | Loss: 0.00001356
Iteration 187/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.3559210856328718e-05, 1.3559210856328718e-05, 1.3559210856328718e-05, 1.3559210856328718e-05, 1.3559210856328718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3559210856328718e-05

Optimization complete. Final v2v error: 3.0356862545013428 mm

Highest mean error: 3.7589478492736816 mm for frame 70

Lowest mean error: 2.441232681274414 mm for frame 148

Saving results

Total time: 40.21094489097595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736464
Iteration 2/25 | Loss: 0.00125142
Iteration 3/25 | Loss: 0.00111755
Iteration 4/25 | Loss: 0.00108681
Iteration 5/25 | Loss: 0.00107089
Iteration 6/25 | Loss: 0.00106893
Iteration 7/25 | Loss: 0.00106850
Iteration 8/25 | Loss: 0.00106834
Iteration 9/25 | Loss: 0.00106824
Iteration 10/25 | Loss: 0.00106824
Iteration 11/25 | Loss: 0.00106824
Iteration 12/25 | Loss: 0.00106824
Iteration 13/25 | Loss: 0.00106824
Iteration 14/25 | Loss: 0.00106824
Iteration 15/25 | Loss: 0.00106823
Iteration 16/25 | Loss: 0.00106823
Iteration 17/25 | Loss: 0.00106823
Iteration 18/25 | Loss: 0.00106823
Iteration 19/25 | Loss: 0.00106823
Iteration 20/25 | Loss: 0.00106823
Iteration 21/25 | Loss: 0.00106823
Iteration 22/25 | Loss: 0.00106823
Iteration 23/25 | Loss: 0.00106823
Iteration 24/25 | Loss: 0.00106823
Iteration 25/25 | Loss: 0.00106823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77770591
Iteration 2/25 | Loss: 0.00087454
Iteration 3/25 | Loss: 0.00087454
Iteration 4/25 | Loss: 0.00087454
Iteration 5/25 | Loss: 0.00087454
Iteration 6/25 | Loss: 0.00087453
Iteration 7/25 | Loss: 0.00087453
Iteration 8/25 | Loss: 0.00087453
Iteration 9/25 | Loss: 0.00087453
Iteration 10/25 | Loss: 0.00087453
Iteration 11/25 | Loss: 0.00087453
Iteration 12/25 | Loss: 0.00087453
Iteration 13/25 | Loss: 0.00087453
Iteration 14/25 | Loss: 0.00087453
Iteration 15/25 | Loss: 0.00087453
Iteration 16/25 | Loss: 0.00087453
Iteration 17/25 | Loss: 0.00087453
Iteration 18/25 | Loss: 0.00087453
Iteration 19/25 | Loss: 0.00087453
Iteration 20/25 | Loss: 0.00087453
Iteration 21/25 | Loss: 0.00087453
Iteration 22/25 | Loss: 0.00087453
Iteration 23/25 | Loss: 0.00087453
Iteration 24/25 | Loss: 0.00087453
Iteration 25/25 | Loss: 0.00087453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087453
Iteration 2/1000 | Loss: 0.00001611
Iteration 3/1000 | Loss: 0.00001272
Iteration 4/1000 | Loss: 0.00001171
Iteration 5/1000 | Loss: 0.00001107
Iteration 6/1000 | Loss: 0.00001071
Iteration 7/1000 | Loss: 0.00001052
Iteration 8/1000 | Loss: 0.00001027
Iteration 9/1000 | Loss: 0.00000999
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000993
Iteration 12/1000 | Loss: 0.00000987
Iteration 13/1000 | Loss: 0.00000986
Iteration 14/1000 | Loss: 0.00000986
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000976
Iteration 19/1000 | Loss: 0.00000970
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000966
Iteration 22/1000 | Loss: 0.00000965
Iteration 23/1000 | Loss: 0.00000964
Iteration 24/1000 | Loss: 0.00000964
Iteration 25/1000 | Loss: 0.00000964
Iteration 26/1000 | Loss: 0.00000963
Iteration 27/1000 | Loss: 0.00000963
Iteration 28/1000 | Loss: 0.00000960
Iteration 29/1000 | Loss: 0.00000960
Iteration 30/1000 | Loss: 0.00000960
Iteration 31/1000 | Loss: 0.00000955
Iteration 32/1000 | Loss: 0.00000954
Iteration 33/1000 | Loss: 0.00000954
Iteration 34/1000 | Loss: 0.00000954
Iteration 35/1000 | Loss: 0.00000954
Iteration 36/1000 | Loss: 0.00000954
Iteration 37/1000 | Loss: 0.00000953
Iteration 38/1000 | Loss: 0.00000953
Iteration 39/1000 | Loss: 0.00000953
Iteration 40/1000 | Loss: 0.00000953
Iteration 41/1000 | Loss: 0.00000953
Iteration 42/1000 | Loss: 0.00000953
Iteration 43/1000 | Loss: 0.00000953
Iteration 44/1000 | Loss: 0.00000953
Iteration 45/1000 | Loss: 0.00000953
Iteration 46/1000 | Loss: 0.00000951
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000950
Iteration 49/1000 | Loss: 0.00000950
Iteration 50/1000 | Loss: 0.00000950
Iteration 51/1000 | Loss: 0.00000949
Iteration 52/1000 | Loss: 0.00000947
Iteration 53/1000 | Loss: 0.00000947
Iteration 54/1000 | Loss: 0.00000947
Iteration 55/1000 | Loss: 0.00000947
Iteration 56/1000 | Loss: 0.00000945
Iteration 57/1000 | Loss: 0.00000945
Iteration 58/1000 | Loss: 0.00000945
Iteration 59/1000 | Loss: 0.00000944
Iteration 60/1000 | Loss: 0.00000944
Iteration 61/1000 | Loss: 0.00000944
Iteration 62/1000 | Loss: 0.00000943
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000942
Iteration 65/1000 | Loss: 0.00000942
Iteration 66/1000 | Loss: 0.00000941
Iteration 67/1000 | Loss: 0.00000941
Iteration 68/1000 | Loss: 0.00000941
Iteration 69/1000 | Loss: 0.00000940
Iteration 70/1000 | Loss: 0.00000940
Iteration 71/1000 | Loss: 0.00000939
Iteration 72/1000 | Loss: 0.00000939
Iteration 73/1000 | Loss: 0.00000938
Iteration 74/1000 | Loss: 0.00000938
Iteration 75/1000 | Loss: 0.00000937
Iteration 76/1000 | Loss: 0.00000936
Iteration 77/1000 | Loss: 0.00000936
Iteration 78/1000 | Loss: 0.00000936
Iteration 79/1000 | Loss: 0.00000935
Iteration 80/1000 | Loss: 0.00000935
Iteration 81/1000 | Loss: 0.00000935
Iteration 82/1000 | Loss: 0.00000935
Iteration 83/1000 | Loss: 0.00000935
Iteration 84/1000 | Loss: 0.00000935
Iteration 85/1000 | Loss: 0.00000935
Iteration 86/1000 | Loss: 0.00000935
Iteration 87/1000 | Loss: 0.00000935
Iteration 88/1000 | Loss: 0.00000934
Iteration 89/1000 | Loss: 0.00000934
Iteration 90/1000 | Loss: 0.00000934
Iteration 91/1000 | Loss: 0.00000933
Iteration 92/1000 | Loss: 0.00000933
Iteration 93/1000 | Loss: 0.00000933
Iteration 94/1000 | Loss: 0.00000932
Iteration 95/1000 | Loss: 0.00000932
Iteration 96/1000 | Loss: 0.00000932
Iteration 97/1000 | Loss: 0.00000932
Iteration 98/1000 | Loss: 0.00000932
Iteration 99/1000 | Loss: 0.00000932
Iteration 100/1000 | Loss: 0.00000932
Iteration 101/1000 | Loss: 0.00000932
Iteration 102/1000 | Loss: 0.00000932
Iteration 103/1000 | Loss: 0.00000932
Iteration 104/1000 | Loss: 0.00000931
Iteration 105/1000 | Loss: 0.00000931
Iteration 106/1000 | Loss: 0.00000931
Iteration 107/1000 | Loss: 0.00000931
Iteration 108/1000 | Loss: 0.00000931
Iteration 109/1000 | Loss: 0.00000931
Iteration 110/1000 | Loss: 0.00000931
Iteration 111/1000 | Loss: 0.00000931
Iteration 112/1000 | Loss: 0.00000931
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00000930
Iteration 115/1000 | Loss: 0.00000930
Iteration 116/1000 | Loss: 0.00000930
Iteration 117/1000 | Loss: 0.00000930
Iteration 118/1000 | Loss: 0.00000930
Iteration 119/1000 | Loss: 0.00000930
Iteration 120/1000 | Loss: 0.00000930
Iteration 121/1000 | Loss: 0.00000930
Iteration 122/1000 | Loss: 0.00000930
Iteration 123/1000 | Loss: 0.00000930
Iteration 124/1000 | Loss: 0.00000930
Iteration 125/1000 | Loss: 0.00000929
Iteration 126/1000 | Loss: 0.00000929
Iteration 127/1000 | Loss: 0.00000929
Iteration 128/1000 | Loss: 0.00000929
Iteration 129/1000 | Loss: 0.00000929
Iteration 130/1000 | Loss: 0.00000929
Iteration 131/1000 | Loss: 0.00000929
Iteration 132/1000 | Loss: 0.00000929
Iteration 133/1000 | Loss: 0.00000929
Iteration 134/1000 | Loss: 0.00000928
Iteration 135/1000 | Loss: 0.00000928
Iteration 136/1000 | Loss: 0.00000928
Iteration 137/1000 | Loss: 0.00000928
Iteration 138/1000 | Loss: 0.00000928
Iteration 139/1000 | Loss: 0.00000928
Iteration 140/1000 | Loss: 0.00000927
Iteration 141/1000 | Loss: 0.00000927
Iteration 142/1000 | Loss: 0.00000927
Iteration 143/1000 | Loss: 0.00000927
Iteration 144/1000 | Loss: 0.00000927
Iteration 145/1000 | Loss: 0.00000927
Iteration 146/1000 | Loss: 0.00000927
Iteration 147/1000 | Loss: 0.00000927
Iteration 148/1000 | Loss: 0.00000927
Iteration 149/1000 | Loss: 0.00000927
Iteration 150/1000 | Loss: 0.00000927
Iteration 151/1000 | Loss: 0.00000926
Iteration 152/1000 | Loss: 0.00000926
Iteration 153/1000 | Loss: 0.00000926
Iteration 154/1000 | Loss: 0.00000926
Iteration 155/1000 | Loss: 0.00000926
Iteration 156/1000 | Loss: 0.00000926
Iteration 157/1000 | Loss: 0.00000926
Iteration 158/1000 | Loss: 0.00000926
Iteration 159/1000 | Loss: 0.00000926
Iteration 160/1000 | Loss: 0.00000926
Iteration 161/1000 | Loss: 0.00000925
Iteration 162/1000 | Loss: 0.00000925
Iteration 163/1000 | Loss: 0.00000925
Iteration 164/1000 | Loss: 0.00000925
Iteration 165/1000 | Loss: 0.00000924
Iteration 166/1000 | Loss: 0.00000924
Iteration 167/1000 | Loss: 0.00000924
Iteration 168/1000 | Loss: 0.00000924
Iteration 169/1000 | Loss: 0.00000924
Iteration 170/1000 | Loss: 0.00000923
Iteration 171/1000 | Loss: 0.00000923
Iteration 172/1000 | Loss: 0.00000923
Iteration 173/1000 | Loss: 0.00000922
Iteration 174/1000 | Loss: 0.00000922
Iteration 175/1000 | Loss: 0.00000922
Iteration 176/1000 | Loss: 0.00000922
Iteration 177/1000 | Loss: 0.00000922
Iteration 178/1000 | Loss: 0.00000922
Iteration 179/1000 | Loss: 0.00000922
Iteration 180/1000 | Loss: 0.00000921
Iteration 181/1000 | Loss: 0.00000921
Iteration 182/1000 | Loss: 0.00000921
Iteration 183/1000 | Loss: 0.00000921
Iteration 184/1000 | Loss: 0.00000921
Iteration 185/1000 | Loss: 0.00000921
Iteration 186/1000 | Loss: 0.00000920
Iteration 187/1000 | Loss: 0.00000920
Iteration 188/1000 | Loss: 0.00000920
Iteration 189/1000 | Loss: 0.00000920
Iteration 190/1000 | Loss: 0.00000920
Iteration 191/1000 | Loss: 0.00000920
Iteration 192/1000 | Loss: 0.00000920
Iteration 193/1000 | Loss: 0.00000920
Iteration 194/1000 | Loss: 0.00000920
Iteration 195/1000 | Loss: 0.00000920
Iteration 196/1000 | Loss: 0.00000920
Iteration 197/1000 | Loss: 0.00000920
Iteration 198/1000 | Loss: 0.00000920
Iteration 199/1000 | Loss: 0.00000920
Iteration 200/1000 | Loss: 0.00000920
Iteration 201/1000 | Loss: 0.00000920
Iteration 202/1000 | Loss: 0.00000920
Iteration 203/1000 | Loss: 0.00000920
Iteration 204/1000 | Loss: 0.00000920
Iteration 205/1000 | Loss: 0.00000920
Iteration 206/1000 | Loss: 0.00000920
Iteration 207/1000 | Loss: 0.00000920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [9.195929123961832e-06, 9.195929123961832e-06, 9.195929123961832e-06, 9.195929123961832e-06, 9.195929123961832e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.195929123961832e-06

Optimization complete. Final v2v error: 2.6305394172668457 mm

Highest mean error: 2.8702869415283203 mm for frame 104

Lowest mean error: 2.47792649269104 mm for frame 194

Saving results

Total time: 45.59343957901001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346247
Iteration 2/25 | Loss: 0.00113904
Iteration 3/25 | Loss: 0.00106059
Iteration 4/25 | Loss: 0.00104954
Iteration 5/25 | Loss: 0.00104511
Iteration 6/25 | Loss: 0.00104358
Iteration 7/25 | Loss: 0.00104344
Iteration 8/25 | Loss: 0.00104344
Iteration 9/25 | Loss: 0.00104344
Iteration 10/25 | Loss: 0.00104344
Iteration 11/25 | Loss: 0.00104344
Iteration 12/25 | Loss: 0.00104344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010434414725750685, 0.0010434414725750685, 0.0010434414725750685, 0.0010434414725750685, 0.0010434414725750685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010434414725750685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35801995
Iteration 2/25 | Loss: 0.00100357
Iteration 3/25 | Loss: 0.00100357
Iteration 4/25 | Loss: 0.00100357
Iteration 5/25 | Loss: 0.00100357
Iteration 6/25 | Loss: 0.00100357
Iteration 7/25 | Loss: 0.00100357
Iteration 8/25 | Loss: 0.00100357
Iteration 9/25 | Loss: 0.00100357
Iteration 10/25 | Loss: 0.00100357
Iteration 11/25 | Loss: 0.00100357
Iteration 12/25 | Loss: 0.00100357
Iteration 13/25 | Loss: 0.00100357
Iteration 14/25 | Loss: 0.00100357
Iteration 15/25 | Loss: 0.00100357
Iteration 16/25 | Loss: 0.00100357
Iteration 17/25 | Loss: 0.00100357
Iteration 18/25 | Loss: 0.00100357
Iteration 19/25 | Loss: 0.00100357
Iteration 20/25 | Loss: 0.00100357
Iteration 21/25 | Loss: 0.00100357
Iteration 22/25 | Loss: 0.00100357
Iteration 23/25 | Loss: 0.00100357
Iteration 24/25 | Loss: 0.00100357
Iteration 25/25 | Loss: 0.00100357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100357
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00001587
Iteration 4/1000 | Loss: 0.00001340
Iteration 5/1000 | Loss: 0.00001229
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001144
Iteration 8/1000 | Loss: 0.00001128
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001109
Iteration 11/1000 | Loss: 0.00001100
Iteration 12/1000 | Loss: 0.00001099
Iteration 13/1000 | Loss: 0.00001099
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001092
Iteration 18/1000 | Loss: 0.00001092
Iteration 19/1000 | Loss: 0.00001091
Iteration 20/1000 | Loss: 0.00001090
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001088
Iteration 23/1000 | Loss: 0.00001087
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001080
Iteration 26/1000 | Loss: 0.00001080
Iteration 27/1000 | Loss: 0.00001080
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001069
Iteration 34/1000 | Loss: 0.00001068
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001064
Iteration 40/1000 | Loss: 0.00001063
Iteration 41/1000 | Loss: 0.00001061
Iteration 42/1000 | Loss: 0.00001060
Iteration 43/1000 | Loss: 0.00001060
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001055
Iteration 54/1000 | Loss: 0.00001055
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001055
Iteration 57/1000 | Loss: 0.00001055
Iteration 58/1000 | Loss: 0.00001055
Iteration 59/1000 | Loss: 0.00001055
Iteration 60/1000 | Loss: 0.00001055
Iteration 61/1000 | Loss: 0.00001055
Iteration 62/1000 | Loss: 0.00001055
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001054
Iteration 66/1000 | Loss: 0.00001054
Iteration 67/1000 | Loss: 0.00001053
Iteration 68/1000 | Loss: 0.00001053
Iteration 69/1000 | Loss: 0.00001053
Iteration 70/1000 | Loss: 0.00001052
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001051
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001050
Iteration 76/1000 | Loss: 0.00001050
Iteration 77/1000 | Loss: 0.00001050
Iteration 78/1000 | Loss: 0.00001050
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001049
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001047
Iteration 90/1000 | Loss: 0.00001047
Iteration 91/1000 | Loss: 0.00001047
Iteration 92/1000 | Loss: 0.00001047
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001046
Iteration 95/1000 | Loss: 0.00001046
Iteration 96/1000 | Loss: 0.00001046
Iteration 97/1000 | Loss: 0.00001045
Iteration 98/1000 | Loss: 0.00001045
Iteration 99/1000 | Loss: 0.00001045
Iteration 100/1000 | Loss: 0.00001045
Iteration 101/1000 | Loss: 0.00001044
Iteration 102/1000 | Loss: 0.00001044
Iteration 103/1000 | Loss: 0.00001044
Iteration 104/1000 | Loss: 0.00001043
Iteration 105/1000 | Loss: 0.00001043
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001042
Iteration 110/1000 | Loss: 0.00001042
Iteration 111/1000 | Loss: 0.00001041
Iteration 112/1000 | Loss: 0.00001041
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001040
Iteration 116/1000 | Loss: 0.00001039
Iteration 117/1000 | Loss: 0.00001039
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001038
Iteration 121/1000 | Loss: 0.00001038
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Iteration 124/1000 | Loss: 0.00001036
Iteration 125/1000 | Loss: 0.00001036
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001035
Iteration 128/1000 | Loss: 0.00001034
Iteration 129/1000 | Loss: 0.00001034
Iteration 130/1000 | Loss: 0.00001034
Iteration 131/1000 | Loss: 0.00001033
Iteration 132/1000 | Loss: 0.00001033
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001031
Iteration 137/1000 | Loss: 0.00001031
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001031
Iteration 143/1000 | Loss: 0.00001031
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001029
Iteration 148/1000 | Loss: 0.00001029
Iteration 149/1000 | Loss: 0.00001029
Iteration 150/1000 | Loss: 0.00001029
Iteration 151/1000 | Loss: 0.00001029
Iteration 152/1000 | Loss: 0.00001029
Iteration 153/1000 | Loss: 0.00001029
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001028
Iteration 156/1000 | Loss: 0.00001028
Iteration 157/1000 | Loss: 0.00001028
Iteration 158/1000 | Loss: 0.00001028
Iteration 159/1000 | Loss: 0.00001028
Iteration 160/1000 | Loss: 0.00001028
Iteration 161/1000 | Loss: 0.00001027
Iteration 162/1000 | Loss: 0.00001027
Iteration 163/1000 | Loss: 0.00001027
Iteration 164/1000 | Loss: 0.00001027
Iteration 165/1000 | Loss: 0.00001027
Iteration 166/1000 | Loss: 0.00001027
Iteration 167/1000 | Loss: 0.00001027
Iteration 168/1000 | Loss: 0.00001027
Iteration 169/1000 | Loss: 0.00001026
Iteration 170/1000 | Loss: 0.00001026
Iteration 171/1000 | Loss: 0.00001026
Iteration 172/1000 | Loss: 0.00001026
Iteration 173/1000 | Loss: 0.00001026
Iteration 174/1000 | Loss: 0.00001026
Iteration 175/1000 | Loss: 0.00001026
Iteration 176/1000 | Loss: 0.00001026
Iteration 177/1000 | Loss: 0.00001026
Iteration 178/1000 | Loss: 0.00001026
Iteration 179/1000 | Loss: 0.00001026
Iteration 180/1000 | Loss: 0.00001026
Iteration 181/1000 | Loss: 0.00001026
Iteration 182/1000 | Loss: 0.00001026
Iteration 183/1000 | Loss: 0.00001026
Iteration 184/1000 | Loss: 0.00001026
Iteration 185/1000 | Loss: 0.00001026
Iteration 186/1000 | Loss: 0.00001026
Iteration 187/1000 | Loss: 0.00001026
Iteration 188/1000 | Loss: 0.00001026
Iteration 189/1000 | Loss: 0.00001026
Iteration 190/1000 | Loss: 0.00001026
Iteration 191/1000 | Loss: 0.00001026
Iteration 192/1000 | Loss: 0.00001026
Iteration 193/1000 | Loss: 0.00001026
Iteration 194/1000 | Loss: 0.00001026
Iteration 195/1000 | Loss: 0.00001026
Iteration 196/1000 | Loss: 0.00001026
Iteration 197/1000 | Loss: 0.00001026
Iteration 198/1000 | Loss: 0.00001026
Iteration 199/1000 | Loss: 0.00001026
Iteration 200/1000 | Loss: 0.00001026
Iteration 201/1000 | Loss: 0.00001026
Iteration 202/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.0257776011712849e-05, 1.0257776011712849e-05, 1.0257776011712849e-05, 1.0257776011712849e-05, 1.0257776011712849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0257776011712849e-05

Optimization complete. Final v2v error: 2.709226608276367 mm

Highest mean error: 3.562441349029541 mm for frame 10

Lowest mean error: 2.2255871295928955 mm for frame 132

Saving results

Total time: 37.90205264091492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814926
Iteration 2/25 | Loss: 0.00136971
Iteration 3/25 | Loss: 0.00116056
Iteration 4/25 | Loss: 0.00114453
Iteration 5/25 | Loss: 0.00114150
Iteration 6/25 | Loss: 0.00114150
Iteration 7/25 | Loss: 0.00114150
Iteration 8/25 | Loss: 0.00114150
Iteration 9/25 | Loss: 0.00114150
Iteration 10/25 | Loss: 0.00114150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011414991458877921, 0.0011414991458877921, 0.0011414991458877921, 0.0011414991458877921, 0.0011414991458877921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011414991458877921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90306181
Iteration 2/25 | Loss: 0.00047365
Iteration 3/25 | Loss: 0.00047364
Iteration 4/25 | Loss: 0.00047364
Iteration 5/25 | Loss: 0.00047364
Iteration 6/25 | Loss: 0.00047364
Iteration 7/25 | Loss: 0.00047364
Iteration 8/25 | Loss: 0.00047364
Iteration 9/25 | Loss: 0.00047364
Iteration 10/25 | Loss: 0.00047364
Iteration 11/25 | Loss: 0.00047364
Iteration 12/25 | Loss: 0.00047364
Iteration 13/25 | Loss: 0.00047364
Iteration 14/25 | Loss: 0.00047364
Iteration 15/25 | Loss: 0.00047364
Iteration 16/25 | Loss: 0.00047364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00047364202328026295, 0.00047364202328026295, 0.00047364202328026295, 0.00047364202328026295, 0.00047364202328026295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047364202328026295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047364
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00002272
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00002057
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001988
Iteration 8/1000 | Loss: 0.00001949
Iteration 9/1000 | Loss: 0.00001924
Iteration 10/1000 | Loss: 0.00001912
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001898
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001894
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001893
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001883
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001879
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001878
Iteration 67/1000 | Loss: 0.00001878
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001876
Iteration 77/1000 | Loss: 0.00001875
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001873
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001873
Iteration 88/1000 | Loss: 0.00001873
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001872
Iteration 93/1000 | Loss: 0.00001872
Iteration 94/1000 | Loss: 0.00001872
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001870
Iteration 106/1000 | Loss: 0.00001870
Iteration 107/1000 | Loss: 0.00001870
Iteration 108/1000 | Loss: 0.00001870
Iteration 109/1000 | Loss: 0.00001870
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001867
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001867
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001866
Iteration 120/1000 | Loss: 0.00001866
Iteration 121/1000 | Loss: 0.00001866
Iteration 122/1000 | Loss: 0.00001866
Iteration 123/1000 | Loss: 0.00001866
Iteration 124/1000 | Loss: 0.00001866
Iteration 125/1000 | Loss: 0.00001866
Iteration 126/1000 | Loss: 0.00001865
Iteration 127/1000 | Loss: 0.00001865
Iteration 128/1000 | Loss: 0.00001865
Iteration 129/1000 | Loss: 0.00001865
Iteration 130/1000 | Loss: 0.00001865
Iteration 131/1000 | Loss: 0.00001864
Iteration 132/1000 | Loss: 0.00001864
Iteration 133/1000 | Loss: 0.00001863
Iteration 134/1000 | Loss: 0.00001863
Iteration 135/1000 | Loss: 0.00001863
Iteration 136/1000 | Loss: 0.00001863
Iteration 137/1000 | Loss: 0.00001862
Iteration 138/1000 | Loss: 0.00001862
Iteration 139/1000 | Loss: 0.00001862
Iteration 140/1000 | Loss: 0.00001862
Iteration 141/1000 | Loss: 0.00001862
Iteration 142/1000 | Loss: 0.00001862
Iteration 143/1000 | Loss: 0.00001862
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001861
Iteration 147/1000 | Loss: 0.00001860
Iteration 148/1000 | Loss: 0.00001860
Iteration 149/1000 | Loss: 0.00001860
Iteration 150/1000 | Loss: 0.00001860
Iteration 151/1000 | Loss: 0.00001860
Iteration 152/1000 | Loss: 0.00001860
Iteration 153/1000 | Loss: 0.00001860
Iteration 154/1000 | Loss: 0.00001860
Iteration 155/1000 | Loss: 0.00001860
Iteration 156/1000 | Loss: 0.00001859
Iteration 157/1000 | Loss: 0.00001859
Iteration 158/1000 | Loss: 0.00001859
Iteration 159/1000 | Loss: 0.00001859
Iteration 160/1000 | Loss: 0.00001859
Iteration 161/1000 | Loss: 0.00001859
Iteration 162/1000 | Loss: 0.00001858
Iteration 163/1000 | Loss: 0.00001858
Iteration 164/1000 | Loss: 0.00001858
Iteration 165/1000 | Loss: 0.00001858
Iteration 166/1000 | Loss: 0.00001858
Iteration 167/1000 | Loss: 0.00001858
Iteration 168/1000 | Loss: 0.00001858
Iteration 169/1000 | Loss: 0.00001858
Iteration 170/1000 | Loss: 0.00001857
Iteration 171/1000 | Loss: 0.00001857
Iteration 172/1000 | Loss: 0.00001857
Iteration 173/1000 | Loss: 0.00001857
Iteration 174/1000 | Loss: 0.00001857
Iteration 175/1000 | Loss: 0.00001857
Iteration 176/1000 | Loss: 0.00001857
Iteration 177/1000 | Loss: 0.00001857
Iteration 178/1000 | Loss: 0.00001857
Iteration 179/1000 | Loss: 0.00001857
Iteration 180/1000 | Loss: 0.00001857
Iteration 181/1000 | Loss: 0.00001857
Iteration 182/1000 | Loss: 0.00001857
Iteration 183/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.8569302483228967e-05, 1.8569302483228967e-05, 1.8569302483228967e-05, 1.8569302483228967e-05, 1.8569302483228967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8569302483228967e-05

Optimization complete. Final v2v error: 3.5909464359283447 mm

Highest mean error: 3.7076709270477295 mm for frame 103

Lowest mean error: 3.4805562496185303 mm for frame 184

Saving results

Total time: 39.27438163757324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803604
Iteration 2/25 | Loss: 0.00145191
Iteration 3/25 | Loss: 0.00123102
Iteration 4/25 | Loss: 0.00120186
Iteration 5/25 | Loss: 0.00119494
Iteration 6/25 | Loss: 0.00119358
Iteration 7/25 | Loss: 0.00119358
Iteration 8/25 | Loss: 0.00119358
Iteration 9/25 | Loss: 0.00119358
Iteration 10/25 | Loss: 0.00119358
Iteration 11/25 | Loss: 0.00119358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011935782385990024, 0.0011935782385990024, 0.0011935782385990024, 0.0011935782385990024, 0.0011935782385990024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011935782385990024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05456936
Iteration 2/25 | Loss: 0.00085001
Iteration 3/25 | Loss: 0.00084996
Iteration 4/25 | Loss: 0.00084996
Iteration 5/25 | Loss: 0.00084996
Iteration 6/25 | Loss: 0.00084996
Iteration 7/25 | Loss: 0.00084996
Iteration 8/25 | Loss: 0.00084996
Iteration 9/25 | Loss: 0.00084996
Iteration 10/25 | Loss: 0.00084996
Iteration 11/25 | Loss: 0.00084996
Iteration 12/25 | Loss: 0.00084996
Iteration 13/25 | Loss: 0.00084996
Iteration 14/25 | Loss: 0.00084996
Iteration 15/25 | Loss: 0.00084996
Iteration 16/25 | Loss: 0.00084996
Iteration 17/25 | Loss: 0.00084996
Iteration 18/25 | Loss: 0.00084996
Iteration 19/25 | Loss: 0.00084996
Iteration 20/25 | Loss: 0.00084996
Iteration 21/25 | Loss: 0.00084996
Iteration 22/25 | Loss: 0.00084996
Iteration 23/25 | Loss: 0.00084996
Iteration 24/25 | Loss: 0.00084996
Iteration 25/25 | Loss: 0.00084996
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008499592659063637, 0.0008499592659063637, 0.0008499592659063637, 0.0008499592659063637, 0.0008499592659063637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008499592659063637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084996
Iteration 2/1000 | Loss: 0.00007601
Iteration 3/1000 | Loss: 0.00004915
Iteration 4/1000 | Loss: 0.00003796
Iteration 5/1000 | Loss: 0.00003433
Iteration 6/1000 | Loss: 0.00003269
Iteration 7/1000 | Loss: 0.00003122
Iteration 8/1000 | Loss: 0.00003046
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002912
Iteration 11/1000 | Loss: 0.00002867
Iteration 12/1000 | Loss: 0.00002836
Iteration 13/1000 | Loss: 0.00002810
Iteration 14/1000 | Loss: 0.00002801
Iteration 15/1000 | Loss: 0.00002779
Iteration 16/1000 | Loss: 0.00002754
Iteration 17/1000 | Loss: 0.00002732
Iteration 18/1000 | Loss: 0.00002711
Iteration 19/1000 | Loss: 0.00002707
Iteration 20/1000 | Loss: 0.00002702
Iteration 21/1000 | Loss: 0.00002702
Iteration 22/1000 | Loss: 0.00002697
Iteration 23/1000 | Loss: 0.00002693
Iteration 24/1000 | Loss: 0.00002692
Iteration 25/1000 | Loss: 0.00002692
Iteration 26/1000 | Loss: 0.00002692
Iteration 27/1000 | Loss: 0.00002692
Iteration 28/1000 | Loss: 0.00002692
Iteration 29/1000 | Loss: 0.00002692
Iteration 30/1000 | Loss: 0.00002692
Iteration 31/1000 | Loss: 0.00002692
Iteration 32/1000 | Loss: 0.00002691
Iteration 33/1000 | Loss: 0.00002691
Iteration 34/1000 | Loss: 0.00002689
Iteration 35/1000 | Loss: 0.00002689
Iteration 36/1000 | Loss: 0.00002688
Iteration 37/1000 | Loss: 0.00002688
Iteration 38/1000 | Loss: 0.00002685
Iteration 39/1000 | Loss: 0.00002682
Iteration 40/1000 | Loss: 0.00002682
Iteration 41/1000 | Loss: 0.00002681
Iteration 42/1000 | Loss: 0.00002680
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002679
Iteration 45/1000 | Loss: 0.00002678
Iteration 46/1000 | Loss: 0.00002678
Iteration 47/1000 | Loss: 0.00002677
Iteration 48/1000 | Loss: 0.00002673
Iteration 49/1000 | Loss: 0.00002673
Iteration 50/1000 | Loss: 0.00002672
Iteration 51/1000 | Loss: 0.00002672
Iteration 52/1000 | Loss: 0.00002671
Iteration 53/1000 | Loss: 0.00002671
Iteration 54/1000 | Loss: 0.00002670
Iteration 55/1000 | Loss: 0.00002670
Iteration 56/1000 | Loss: 0.00002670
Iteration 57/1000 | Loss: 0.00002669
Iteration 58/1000 | Loss: 0.00002669
Iteration 59/1000 | Loss: 0.00002669
Iteration 60/1000 | Loss: 0.00002668
Iteration 61/1000 | Loss: 0.00002668
Iteration 62/1000 | Loss: 0.00002667
Iteration 63/1000 | Loss: 0.00002666
Iteration 64/1000 | Loss: 0.00002666
Iteration 65/1000 | Loss: 0.00002666
Iteration 66/1000 | Loss: 0.00002666
Iteration 67/1000 | Loss: 0.00002666
Iteration 68/1000 | Loss: 0.00002666
Iteration 69/1000 | Loss: 0.00002666
Iteration 70/1000 | Loss: 0.00002666
Iteration 71/1000 | Loss: 0.00002666
Iteration 72/1000 | Loss: 0.00002665
Iteration 73/1000 | Loss: 0.00002665
Iteration 74/1000 | Loss: 0.00002664
Iteration 75/1000 | Loss: 0.00002664
Iteration 76/1000 | Loss: 0.00002664
Iteration 77/1000 | Loss: 0.00002662
Iteration 78/1000 | Loss: 0.00002662
Iteration 79/1000 | Loss: 0.00002662
Iteration 80/1000 | Loss: 0.00002662
Iteration 81/1000 | Loss: 0.00002662
Iteration 82/1000 | Loss: 0.00002662
Iteration 83/1000 | Loss: 0.00002660
Iteration 84/1000 | Loss: 0.00002660
Iteration 85/1000 | Loss: 0.00002660
Iteration 86/1000 | Loss: 0.00002659
Iteration 87/1000 | Loss: 0.00002659
Iteration 88/1000 | Loss: 0.00002659
Iteration 89/1000 | Loss: 0.00002659
Iteration 90/1000 | Loss: 0.00002659
Iteration 91/1000 | Loss: 0.00002659
Iteration 92/1000 | Loss: 0.00002659
Iteration 93/1000 | Loss: 0.00002658
Iteration 94/1000 | Loss: 0.00002658
Iteration 95/1000 | Loss: 0.00002658
Iteration 96/1000 | Loss: 0.00002658
Iteration 97/1000 | Loss: 0.00002657
Iteration 98/1000 | Loss: 0.00002657
Iteration 99/1000 | Loss: 0.00002657
Iteration 100/1000 | Loss: 0.00002657
Iteration 101/1000 | Loss: 0.00002656
Iteration 102/1000 | Loss: 0.00002656
Iteration 103/1000 | Loss: 0.00002656
Iteration 104/1000 | Loss: 0.00002656
Iteration 105/1000 | Loss: 0.00002656
Iteration 106/1000 | Loss: 0.00002656
Iteration 107/1000 | Loss: 0.00002655
Iteration 108/1000 | Loss: 0.00002655
Iteration 109/1000 | Loss: 0.00002655
Iteration 110/1000 | Loss: 0.00002655
Iteration 111/1000 | Loss: 0.00002655
Iteration 112/1000 | Loss: 0.00002655
Iteration 113/1000 | Loss: 0.00002655
Iteration 114/1000 | Loss: 0.00002655
Iteration 115/1000 | Loss: 0.00002655
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002654
Iteration 119/1000 | Loss: 0.00002654
Iteration 120/1000 | Loss: 0.00002654
Iteration 121/1000 | Loss: 0.00002654
Iteration 122/1000 | Loss: 0.00002654
Iteration 123/1000 | Loss: 0.00002654
Iteration 124/1000 | Loss: 0.00002654
Iteration 125/1000 | Loss: 0.00002654
Iteration 126/1000 | Loss: 0.00002654
Iteration 127/1000 | Loss: 0.00002654
Iteration 128/1000 | Loss: 0.00002654
Iteration 129/1000 | Loss: 0.00002654
Iteration 130/1000 | Loss: 0.00002654
Iteration 131/1000 | Loss: 0.00002654
Iteration 132/1000 | Loss: 0.00002654
Iteration 133/1000 | Loss: 0.00002654
Iteration 134/1000 | Loss: 0.00002654
Iteration 135/1000 | Loss: 0.00002654
Iteration 136/1000 | Loss: 0.00002654
Iteration 137/1000 | Loss: 0.00002654
Iteration 138/1000 | Loss: 0.00002654
Iteration 139/1000 | Loss: 0.00002654
Iteration 140/1000 | Loss: 0.00002654
Iteration 141/1000 | Loss: 0.00002654
Iteration 142/1000 | Loss: 0.00002654
Iteration 143/1000 | Loss: 0.00002654
Iteration 144/1000 | Loss: 0.00002654
Iteration 145/1000 | Loss: 0.00002654
Iteration 146/1000 | Loss: 0.00002654
Iteration 147/1000 | Loss: 0.00002654
Iteration 148/1000 | Loss: 0.00002654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.654282252478879e-05, 2.654282252478879e-05, 2.654282252478879e-05, 2.654282252478879e-05, 2.654282252478879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.654282252478879e-05

Optimization complete. Final v2v error: 4.1357550621032715 mm

Highest mean error: 5.448627948760986 mm for frame 36

Lowest mean error: 2.9807755947113037 mm for frame 0

Saving results

Total time: 51.43579864501953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00276303
Iteration 2/25 | Loss: 0.00127038
Iteration 3/25 | Loss: 0.00110178
Iteration 4/25 | Loss: 0.00107686
Iteration 5/25 | Loss: 0.00106914
Iteration 6/25 | Loss: 0.00106603
Iteration 7/25 | Loss: 0.00106496
Iteration 8/25 | Loss: 0.00106488
Iteration 9/25 | Loss: 0.00106488
Iteration 10/25 | Loss: 0.00106488
Iteration 11/25 | Loss: 0.00106488
Iteration 12/25 | Loss: 0.00106488
Iteration 13/25 | Loss: 0.00106488
Iteration 14/25 | Loss: 0.00106488
Iteration 15/25 | Loss: 0.00106488
Iteration 16/25 | Loss: 0.00106488
Iteration 17/25 | Loss: 0.00106488
Iteration 18/25 | Loss: 0.00106488
Iteration 19/25 | Loss: 0.00106488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010648817988112569, 0.0010648817988112569, 0.0010648817988112569, 0.0010648817988112569, 0.0010648817988112569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010648817988112569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30927098
Iteration 2/25 | Loss: 0.00111618
Iteration 3/25 | Loss: 0.00111618
Iteration 4/25 | Loss: 0.00111618
Iteration 5/25 | Loss: 0.00111618
Iteration 6/25 | Loss: 0.00111618
Iteration 7/25 | Loss: 0.00111618
Iteration 8/25 | Loss: 0.00111618
Iteration 9/25 | Loss: 0.00111618
Iteration 10/25 | Loss: 0.00111618
Iteration 11/25 | Loss: 0.00111618
Iteration 12/25 | Loss: 0.00111618
Iteration 13/25 | Loss: 0.00111618
Iteration 14/25 | Loss: 0.00111618
Iteration 15/25 | Loss: 0.00111618
Iteration 16/25 | Loss: 0.00111618
Iteration 17/25 | Loss: 0.00111618
Iteration 18/25 | Loss: 0.00111618
Iteration 19/25 | Loss: 0.00111618
Iteration 20/25 | Loss: 0.00111618
Iteration 21/25 | Loss: 0.00111618
Iteration 22/25 | Loss: 0.00111618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011161781148985028, 0.0011161781148985028, 0.0011161781148985028, 0.0011161781148985028, 0.0011161781148985028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011161781148985028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111618
Iteration 2/1000 | Loss: 0.00003759
Iteration 3/1000 | Loss: 0.00002274
Iteration 4/1000 | Loss: 0.00001712
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001377
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001245
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001203
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001196
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001191
Iteration 43/1000 | Loss: 0.00001191
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001190
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001173
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001172
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001167
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001167
Iteration 139/1000 | Loss: 0.00001167
Iteration 140/1000 | Loss: 0.00001167
Iteration 141/1000 | Loss: 0.00001167
Iteration 142/1000 | Loss: 0.00001167
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001166
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001165
Iteration 160/1000 | Loss: 0.00001165
Iteration 161/1000 | Loss: 0.00001165
Iteration 162/1000 | Loss: 0.00001165
Iteration 163/1000 | Loss: 0.00001165
Iteration 164/1000 | Loss: 0.00001165
Iteration 165/1000 | Loss: 0.00001165
Iteration 166/1000 | Loss: 0.00001165
Iteration 167/1000 | Loss: 0.00001165
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001165
Iteration 170/1000 | Loss: 0.00001165
Iteration 171/1000 | Loss: 0.00001165
Iteration 172/1000 | Loss: 0.00001165
Iteration 173/1000 | Loss: 0.00001165
Iteration 174/1000 | Loss: 0.00001165
Iteration 175/1000 | Loss: 0.00001165
Iteration 176/1000 | Loss: 0.00001165
Iteration 177/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.1646216989902314e-05, 1.1646216989902314e-05, 1.1646216989902314e-05, 1.1646216989902314e-05, 1.1646216989902314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1646216989902314e-05

Optimization complete. Final v2v error: 2.949093818664551 mm

Highest mean error: 3.129539966583252 mm for frame 195

Lowest mean error: 2.598968982696533 mm for frame 0

Saving results

Total time: 47.82930254936218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912809
Iteration 2/25 | Loss: 0.00299865
Iteration 3/25 | Loss: 0.00186112
Iteration 4/25 | Loss: 0.00182455
Iteration 5/25 | Loss: 0.00155361
Iteration 6/25 | Loss: 0.00153644
Iteration 7/25 | Loss: 0.00150426
Iteration 8/25 | Loss: 0.00149033
Iteration 9/25 | Loss: 0.00148337
Iteration 10/25 | Loss: 0.00148762
Iteration 11/25 | Loss: 0.00148945
Iteration 12/25 | Loss: 0.00147198
Iteration 13/25 | Loss: 0.00146674
Iteration 14/25 | Loss: 0.00146453
Iteration 15/25 | Loss: 0.00146352
Iteration 16/25 | Loss: 0.00146267
Iteration 17/25 | Loss: 0.00146224
Iteration 18/25 | Loss: 0.00146179
Iteration 19/25 | Loss: 0.00146130
Iteration 20/25 | Loss: 0.00146093
Iteration 21/25 | Loss: 0.00146068
Iteration 22/25 | Loss: 0.00146047
Iteration 23/25 | Loss: 0.00146023
Iteration 24/25 | Loss: 0.00145996
Iteration 25/25 | Loss: 0.00145969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14678240
Iteration 2/25 | Loss: 0.00607852
Iteration 3/25 | Loss: 0.00328406
Iteration 4/25 | Loss: 0.00315056
Iteration 5/25 | Loss: 0.00315056
Iteration 6/25 | Loss: 0.00315056
Iteration 7/25 | Loss: 0.00315056
Iteration 8/25 | Loss: 0.00315056
Iteration 9/25 | Loss: 0.00315056
Iteration 10/25 | Loss: 0.00315056
Iteration 11/25 | Loss: 0.00315056
Iteration 12/25 | Loss: 0.00315056
Iteration 13/25 | Loss: 0.00315056
Iteration 14/25 | Loss: 0.00315056
Iteration 15/25 | Loss: 0.00315056
Iteration 16/25 | Loss: 0.00315056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0031505608931183815, 0.0031505608931183815, 0.0031505608931183815, 0.0031505608931183815, 0.0031505608931183815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031505608931183815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315056
Iteration 2/1000 | Loss: 0.01381652
Iteration 3/1000 | Loss: 0.00902985
Iteration 4/1000 | Loss: 0.00409008
Iteration 5/1000 | Loss: 0.00237758
Iteration 6/1000 | Loss: 0.00296317
Iteration 7/1000 | Loss: 0.00040571
Iteration 8/1000 | Loss: 0.00138589
Iteration 9/1000 | Loss: 0.00187473
Iteration 10/1000 | Loss: 0.00038043
Iteration 11/1000 | Loss: 0.00017226
Iteration 12/1000 | Loss: 0.00065837
Iteration 13/1000 | Loss: 0.00186999
Iteration 14/1000 | Loss: 0.00117023
Iteration 15/1000 | Loss: 0.00042536
Iteration 16/1000 | Loss: 0.00088920
Iteration 17/1000 | Loss: 0.00264315
Iteration 18/1000 | Loss: 0.00565561
Iteration 19/1000 | Loss: 0.00371375
Iteration 20/1000 | Loss: 0.00247982
Iteration 21/1000 | Loss: 0.00222397
Iteration 22/1000 | Loss: 0.00215351
Iteration 23/1000 | Loss: 0.00229143
Iteration 24/1000 | Loss: 0.00154842
Iteration 25/1000 | Loss: 0.00255371
Iteration 26/1000 | Loss: 0.00123050
Iteration 27/1000 | Loss: 0.00073984
Iteration 28/1000 | Loss: 0.00067971
Iteration 29/1000 | Loss: 0.00044613
Iteration 30/1000 | Loss: 0.00025774
Iteration 31/1000 | Loss: 0.00039224
Iteration 32/1000 | Loss: 0.00068090
Iteration 33/1000 | Loss: 0.00016282
Iteration 34/1000 | Loss: 0.00017335
Iteration 35/1000 | Loss: 0.00044719
Iteration 36/1000 | Loss: 0.00027596
Iteration 37/1000 | Loss: 0.00007291
Iteration 38/1000 | Loss: 0.00097262
Iteration 39/1000 | Loss: 0.00028111
Iteration 40/1000 | Loss: 0.00053823
Iteration 41/1000 | Loss: 0.00021439
Iteration 42/1000 | Loss: 0.00005427
Iteration 43/1000 | Loss: 0.00033010
Iteration 44/1000 | Loss: 0.00067531
Iteration 45/1000 | Loss: 0.00135774
Iteration 46/1000 | Loss: 0.00100766
Iteration 47/1000 | Loss: 0.00084846
Iteration 48/1000 | Loss: 0.00063173
Iteration 49/1000 | Loss: 0.00071386
Iteration 50/1000 | Loss: 0.00104925
Iteration 51/1000 | Loss: 0.00063536
Iteration 52/1000 | Loss: 0.00046913
Iteration 53/1000 | Loss: 0.00039741
Iteration 54/1000 | Loss: 0.00026033
Iteration 55/1000 | Loss: 0.00008755
Iteration 56/1000 | Loss: 0.00017344
Iteration 57/1000 | Loss: 0.00003822
Iteration 58/1000 | Loss: 0.00003496
Iteration 59/1000 | Loss: 0.00003183
Iteration 60/1000 | Loss: 0.00002988
Iteration 61/1000 | Loss: 0.00022265
Iteration 62/1000 | Loss: 0.00002796
Iteration 63/1000 | Loss: 0.00002633
Iteration 64/1000 | Loss: 0.00002501
Iteration 65/1000 | Loss: 0.00028993
Iteration 66/1000 | Loss: 0.00012149
Iteration 67/1000 | Loss: 0.00006534
Iteration 68/1000 | Loss: 0.00009896
Iteration 69/1000 | Loss: 0.00023537
Iteration 70/1000 | Loss: 0.00008011
Iteration 71/1000 | Loss: 0.00013902
Iteration 72/1000 | Loss: 0.00002355
Iteration 73/1000 | Loss: 0.00002174
Iteration 74/1000 | Loss: 0.00002120
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00023581
Iteration 78/1000 | Loss: 0.00004684
Iteration 79/1000 | Loss: 0.00013930
Iteration 80/1000 | Loss: 0.00002010
Iteration 81/1000 | Loss: 0.00068389
Iteration 82/1000 | Loss: 0.00041469
Iteration 83/1000 | Loss: 0.00019717
Iteration 84/1000 | Loss: 0.00006081
Iteration 85/1000 | Loss: 0.00002114
Iteration 86/1000 | Loss: 0.00013336
Iteration 87/1000 | Loss: 0.00002758
Iteration 88/1000 | Loss: 0.00003757
Iteration 89/1000 | Loss: 0.00021504
Iteration 90/1000 | Loss: 0.00003542
Iteration 91/1000 | Loss: 0.00011966
Iteration 92/1000 | Loss: 0.00001903
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001750
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001746
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001729
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001723
Iteration 104/1000 | Loss: 0.00001723
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001715
Iteration 111/1000 | Loss: 0.00001715
Iteration 112/1000 | Loss: 0.00001715
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001713
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001713
Iteration 130/1000 | Loss: 0.00001712
Iteration 131/1000 | Loss: 0.00001712
Iteration 132/1000 | Loss: 0.00001712
Iteration 133/1000 | Loss: 0.00001712
Iteration 134/1000 | Loss: 0.00001712
Iteration 135/1000 | Loss: 0.00001712
Iteration 136/1000 | Loss: 0.00001711
Iteration 137/1000 | Loss: 0.00001711
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001711
Iteration 140/1000 | Loss: 0.00001711
Iteration 141/1000 | Loss: 0.00001711
Iteration 142/1000 | Loss: 0.00001711
Iteration 143/1000 | Loss: 0.00001711
Iteration 144/1000 | Loss: 0.00001710
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001709
Iteration 147/1000 | Loss: 0.00001709
Iteration 148/1000 | Loss: 0.00001709
Iteration 149/1000 | Loss: 0.00001709
Iteration 150/1000 | Loss: 0.00001708
Iteration 151/1000 | Loss: 0.00001708
Iteration 152/1000 | Loss: 0.00001708
Iteration 153/1000 | Loss: 0.00001708
Iteration 154/1000 | Loss: 0.00001708
Iteration 155/1000 | Loss: 0.00001708
Iteration 156/1000 | Loss: 0.00001708
Iteration 157/1000 | Loss: 0.00001708
Iteration 158/1000 | Loss: 0.00001708
Iteration 159/1000 | Loss: 0.00001708
Iteration 160/1000 | Loss: 0.00001708
Iteration 161/1000 | Loss: 0.00001708
Iteration 162/1000 | Loss: 0.00001708
Iteration 163/1000 | Loss: 0.00001708
Iteration 164/1000 | Loss: 0.00001708
Iteration 165/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.707997580524534e-05, 1.707997580524534e-05, 1.707997580524534e-05, 1.707997580524534e-05, 1.707997580524534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.707997580524534e-05

Optimization complete. Final v2v error: 3.2773985862731934 mm

Highest mean error: 5.206587791442871 mm for frame 116

Lowest mean error: 2.5991358757019043 mm for frame 23

Saving results

Total time: 195.4955768585205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830474
Iteration 2/25 | Loss: 0.00115236
Iteration 3/25 | Loss: 0.00107435
Iteration 4/25 | Loss: 0.00106666
Iteration 5/25 | Loss: 0.00106394
Iteration 6/25 | Loss: 0.00106338
Iteration 7/25 | Loss: 0.00106338
Iteration 8/25 | Loss: 0.00106338
Iteration 9/25 | Loss: 0.00106338
Iteration 10/25 | Loss: 0.00106338
Iteration 11/25 | Loss: 0.00106338
Iteration 12/25 | Loss: 0.00106338
Iteration 13/25 | Loss: 0.00106338
Iteration 14/25 | Loss: 0.00106338
Iteration 15/25 | Loss: 0.00106338
Iteration 16/25 | Loss: 0.00106338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010633771307766438, 0.0010633771307766438, 0.0010633771307766438, 0.0010633771307766438, 0.0010633771307766438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010633771307766438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55768383
Iteration 2/25 | Loss: 0.00087559
Iteration 3/25 | Loss: 0.00087559
Iteration 4/25 | Loss: 0.00087559
Iteration 5/25 | Loss: 0.00087559
Iteration 6/25 | Loss: 0.00087559
Iteration 7/25 | Loss: 0.00087559
Iteration 8/25 | Loss: 0.00087559
Iteration 9/25 | Loss: 0.00087559
Iteration 10/25 | Loss: 0.00087559
Iteration 11/25 | Loss: 0.00087559
Iteration 12/25 | Loss: 0.00087559
Iteration 13/25 | Loss: 0.00087559
Iteration 14/25 | Loss: 0.00087559
Iteration 15/25 | Loss: 0.00087559
Iteration 16/25 | Loss: 0.00087559
Iteration 17/25 | Loss: 0.00087559
Iteration 18/25 | Loss: 0.00087559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008755892631597817, 0.0008755892631597817, 0.0008755892631597817, 0.0008755892631597817, 0.0008755892631597817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008755892631597817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087559
Iteration 2/1000 | Loss: 0.00001735
Iteration 3/1000 | Loss: 0.00001165
Iteration 4/1000 | Loss: 0.00001054
Iteration 5/1000 | Loss: 0.00000997
Iteration 6/1000 | Loss: 0.00000956
Iteration 7/1000 | Loss: 0.00000921
Iteration 8/1000 | Loss: 0.00000903
Iteration 9/1000 | Loss: 0.00000893
Iteration 10/1000 | Loss: 0.00000875
Iteration 11/1000 | Loss: 0.00000867
Iteration 12/1000 | Loss: 0.00000864
Iteration 13/1000 | Loss: 0.00000860
Iteration 14/1000 | Loss: 0.00000858
Iteration 15/1000 | Loss: 0.00000858
Iteration 16/1000 | Loss: 0.00000855
Iteration 17/1000 | Loss: 0.00000855
Iteration 18/1000 | Loss: 0.00000855
Iteration 19/1000 | Loss: 0.00000851
Iteration 20/1000 | Loss: 0.00000845
Iteration 21/1000 | Loss: 0.00000845
Iteration 22/1000 | Loss: 0.00000844
Iteration 23/1000 | Loss: 0.00000844
Iteration 24/1000 | Loss: 0.00000843
Iteration 25/1000 | Loss: 0.00000843
Iteration 26/1000 | Loss: 0.00000842
Iteration 27/1000 | Loss: 0.00000841
Iteration 28/1000 | Loss: 0.00000841
Iteration 29/1000 | Loss: 0.00000839
Iteration 30/1000 | Loss: 0.00000839
Iteration 31/1000 | Loss: 0.00000839
Iteration 32/1000 | Loss: 0.00000838
Iteration 33/1000 | Loss: 0.00000838
Iteration 34/1000 | Loss: 0.00000837
Iteration 35/1000 | Loss: 0.00000837
Iteration 36/1000 | Loss: 0.00000837
Iteration 37/1000 | Loss: 0.00000837
Iteration 38/1000 | Loss: 0.00000836
Iteration 39/1000 | Loss: 0.00000836
Iteration 40/1000 | Loss: 0.00000835
Iteration 41/1000 | Loss: 0.00000835
Iteration 42/1000 | Loss: 0.00000834
Iteration 43/1000 | Loss: 0.00000834
Iteration 44/1000 | Loss: 0.00000834
Iteration 45/1000 | Loss: 0.00000833
Iteration 46/1000 | Loss: 0.00000833
Iteration 47/1000 | Loss: 0.00000833
Iteration 48/1000 | Loss: 0.00000832
Iteration 49/1000 | Loss: 0.00000832
Iteration 50/1000 | Loss: 0.00000832
Iteration 51/1000 | Loss: 0.00000832
Iteration 52/1000 | Loss: 0.00000832
Iteration 53/1000 | Loss: 0.00000832
Iteration 54/1000 | Loss: 0.00000832
Iteration 55/1000 | Loss: 0.00000832
Iteration 56/1000 | Loss: 0.00000832
Iteration 57/1000 | Loss: 0.00000832
Iteration 58/1000 | Loss: 0.00000831
Iteration 59/1000 | Loss: 0.00000831
Iteration 60/1000 | Loss: 0.00000831
Iteration 61/1000 | Loss: 0.00000831
Iteration 62/1000 | Loss: 0.00000831
Iteration 63/1000 | Loss: 0.00000831
Iteration 64/1000 | Loss: 0.00000831
Iteration 65/1000 | Loss: 0.00000831
Iteration 66/1000 | Loss: 0.00000831
Iteration 67/1000 | Loss: 0.00000830
Iteration 68/1000 | Loss: 0.00000830
Iteration 69/1000 | Loss: 0.00000830
Iteration 70/1000 | Loss: 0.00000830
Iteration 71/1000 | Loss: 0.00000830
Iteration 72/1000 | Loss: 0.00000830
Iteration 73/1000 | Loss: 0.00000830
Iteration 74/1000 | Loss: 0.00000829
Iteration 75/1000 | Loss: 0.00000829
Iteration 76/1000 | Loss: 0.00000829
Iteration 77/1000 | Loss: 0.00000828
Iteration 78/1000 | Loss: 0.00000828
Iteration 79/1000 | Loss: 0.00000828
Iteration 80/1000 | Loss: 0.00000828
Iteration 81/1000 | Loss: 0.00000828
Iteration 82/1000 | Loss: 0.00000828
Iteration 83/1000 | Loss: 0.00000828
Iteration 84/1000 | Loss: 0.00000828
Iteration 85/1000 | Loss: 0.00000828
Iteration 86/1000 | Loss: 0.00000828
Iteration 87/1000 | Loss: 0.00000828
Iteration 88/1000 | Loss: 0.00000828
Iteration 89/1000 | Loss: 0.00000828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [8.275641448562965e-06, 8.275641448562965e-06, 8.275641448562965e-06, 8.275641448562965e-06, 8.275641448562965e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.275641448562965e-06

Optimization complete. Final v2v error: 2.489863395690918 mm

Highest mean error: 2.884376049041748 mm for frame 53

Lowest mean error: 2.3431668281555176 mm for frame 133

Saving results

Total time: 29.235899209976196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476858
Iteration 2/25 | Loss: 0.00124957
Iteration 3/25 | Loss: 0.00113347
Iteration 4/25 | Loss: 0.00112556
Iteration 5/25 | Loss: 0.00112254
Iteration 6/25 | Loss: 0.00112254
Iteration 7/25 | Loss: 0.00112254
Iteration 8/25 | Loss: 0.00112254
Iteration 9/25 | Loss: 0.00112254
Iteration 10/25 | Loss: 0.00112254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011225425405427814, 0.0011225425405427814, 0.0011225425405427814, 0.0011225425405427814, 0.0011225425405427814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011225425405427814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84678125
Iteration 2/25 | Loss: 0.00072504
Iteration 3/25 | Loss: 0.00072503
Iteration 4/25 | Loss: 0.00072503
Iteration 5/25 | Loss: 0.00072503
Iteration 6/25 | Loss: 0.00072503
Iteration 7/25 | Loss: 0.00072503
Iteration 8/25 | Loss: 0.00072503
Iteration 9/25 | Loss: 0.00072503
Iteration 10/25 | Loss: 0.00072503
Iteration 11/25 | Loss: 0.00072503
Iteration 12/25 | Loss: 0.00072503
Iteration 13/25 | Loss: 0.00072503
Iteration 14/25 | Loss: 0.00072503
Iteration 15/25 | Loss: 0.00072503
Iteration 16/25 | Loss: 0.00072503
Iteration 17/25 | Loss: 0.00072503
Iteration 18/25 | Loss: 0.00072503
Iteration 19/25 | Loss: 0.00072503
Iteration 20/25 | Loss: 0.00072503
Iteration 21/25 | Loss: 0.00072503
Iteration 22/25 | Loss: 0.00072503
Iteration 23/25 | Loss: 0.00072503
Iteration 24/25 | Loss: 0.00072503
Iteration 25/25 | Loss: 0.00072503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072503
Iteration 2/1000 | Loss: 0.00003230
Iteration 3/1000 | Loss: 0.00002032
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001367
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001324
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001310
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001297
Iteration 56/1000 | Loss: 0.00001296
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001295
Iteration 61/1000 | Loss: 0.00001295
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001295
Iteration 64/1000 | Loss: 0.00001295
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001294
Iteration 67/1000 | Loss: 0.00001294
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001292
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001291
Iteration 77/1000 | Loss: 0.00001291
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001286
Iteration 95/1000 | Loss: 0.00001286
Iteration 96/1000 | Loss: 0.00001286
Iteration 97/1000 | Loss: 0.00001286
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001282
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001282
Iteration 124/1000 | Loss: 0.00001282
Iteration 125/1000 | Loss: 0.00001282
Iteration 126/1000 | Loss: 0.00001281
Iteration 127/1000 | Loss: 0.00001281
Iteration 128/1000 | Loss: 0.00001281
Iteration 129/1000 | Loss: 0.00001281
Iteration 130/1000 | Loss: 0.00001280
Iteration 131/1000 | Loss: 0.00001280
Iteration 132/1000 | Loss: 0.00001280
Iteration 133/1000 | Loss: 0.00001280
Iteration 134/1000 | Loss: 0.00001279
Iteration 135/1000 | Loss: 0.00001279
Iteration 136/1000 | Loss: 0.00001279
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001279
Iteration 140/1000 | Loss: 0.00001278
Iteration 141/1000 | Loss: 0.00001278
Iteration 142/1000 | Loss: 0.00001278
Iteration 143/1000 | Loss: 0.00001278
Iteration 144/1000 | Loss: 0.00001278
Iteration 145/1000 | Loss: 0.00001278
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001277
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001276
Iteration 155/1000 | Loss: 0.00001276
Iteration 156/1000 | Loss: 0.00001276
Iteration 157/1000 | Loss: 0.00001275
Iteration 158/1000 | Loss: 0.00001275
Iteration 159/1000 | Loss: 0.00001275
Iteration 160/1000 | Loss: 0.00001275
Iteration 161/1000 | Loss: 0.00001275
Iteration 162/1000 | Loss: 0.00001275
Iteration 163/1000 | Loss: 0.00001275
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001274
Iteration 168/1000 | Loss: 0.00001274
Iteration 169/1000 | Loss: 0.00001274
Iteration 170/1000 | Loss: 0.00001274
Iteration 171/1000 | Loss: 0.00001274
Iteration 172/1000 | Loss: 0.00001274
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001273
Iteration 177/1000 | Loss: 0.00001273
Iteration 178/1000 | Loss: 0.00001273
Iteration 179/1000 | Loss: 0.00001273
Iteration 180/1000 | Loss: 0.00001273
Iteration 181/1000 | Loss: 0.00001273
Iteration 182/1000 | Loss: 0.00001273
Iteration 183/1000 | Loss: 0.00001273
Iteration 184/1000 | Loss: 0.00001273
Iteration 185/1000 | Loss: 0.00001272
Iteration 186/1000 | Loss: 0.00001272
Iteration 187/1000 | Loss: 0.00001272
Iteration 188/1000 | Loss: 0.00001272
Iteration 189/1000 | Loss: 0.00001272
Iteration 190/1000 | Loss: 0.00001272
Iteration 191/1000 | Loss: 0.00001272
Iteration 192/1000 | Loss: 0.00001272
Iteration 193/1000 | Loss: 0.00001272
Iteration 194/1000 | Loss: 0.00001272
Iteration 195/1000 | Loss: 0.00001272
Iteration 196/1000 | Loss: 0.00001272
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001270
Iteration 203/1000 | Loss: 0.00001270
Iteration 204/1000 | Loss: 0.00001270
Iteration 205/1000 | Loss: 0.00001270
Iteration 206/1000 | Loss: 0.00001270
Iteration 207/1000 | Loss: 0.00001270
Iteration 208/1000 | Loss: 0.00001270
Iteration 209/1000 | Loss: 0.00001269
Iteration 210/1000 | Loss: 0.00001269
Iteration 211/1000 | Loss: 0.00001269
Iteration 212/1000 | Loss: 0.00001269
Iteration 213/1000 | Loss: 0.00001269
Iteration 214/1000 | Loss: 0.00001269
Iteration 215/1000 | Loss: 0.00001269
Iteration 216/1000 | Loss: 0.00001269
Iteration 217/1000 | Loss: 0.00001269
Iteration 218/1000 | Loss: 0.00001269
Iteration 219/1000 | Loss: 0.00001269
Iteration 220/1000 | Loss: 0.00001269
Iteration 221/1000 | Loss: 0.00001269
Iteration 222/1000 | Loss: 0.00001269
Iteration 223/1000 | Loss: 0.00001269
Iteration 224/1000 | Loss: 0.00001269
Iteration 225/1000 | Loss: 0.00001268
Iteration 226/1000 | Loss: 0.00001268
Iteration 227/1000 | Loss: 0.00001268
Iteration 228/1000 | Loss: 0.00001268
Iteration 229/1000 | Loss: 0.00001268
Iteration 230/1000 | Loss: 0.00001268
Iteration 231/1000 | Loss: 0.00001268
Iteration 232/1000 | Loss: 0.00001268
Iteration 233/1000 | Loss: 0.00001268
Iteration 234/1000 | Loss: 0.00001268
Iteration 235/1000 | Loss: 0.00001268
Iteration 236/1000 | Loss: 0.00001268
Iteration 237/1000 | Loss: 0.00001268
Iteration 238/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.2680491636274382e-05, 1.2680491636274382e-05, 1.2680491636274382e-05, 1.2680491636274382e-05, 1.2680491636274382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2680491636274382e-05

Optimization complete. Final v2v error: 3.0177979469299316 mm

Highest mean error: 3.3532097339630127 mm for frame 0

Lowest mean error: 2.8567419052124023 mm for frame 40

Saving results

Total time: 54.03885769844055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527843
Iteration 2/25 | Loss: 0.00119287
Iteration 3/25 | Loss: 0.00110319
Iteration 4/25 | Loss: 0.00109090
Iteration 5/25 | Loss: 0.00108742
Iteration 6/25 | Loss: 0.00108679
Iteration 7/25 | Loss: 0.00108679
Iteration 8/25 | Loss: 0.00108679
Iteration 9/25 | Loss: 0.00108679
Iteration 10/25 | Loss: 0.00108679
Iteration 11/25 | Loss: 0.00108679
Iteration 12/25 | Loss: 0.00108679
Iteration 13/25 | Loss: 0.00108679
Iteration 14/25 | Loss: 0.00108679
Iteration 15/25 | Loss: 0.00108679
Iteration 16/25 | Loss: 0.00108679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010867892997339368, 0.0010867892997339368, 0.0010867892997339368, 0.0010867892997339368, 0.0010867892997339368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010867892997339368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70993042
Iteration 2/25 | Loss: 0.00076881
Iteration 3/25 | Loss: 0.00076880
Iteration 4/25 | Loss: 0.00076879
Iteration 5/25 | Loss: 0.00076879
Iteration 6/25 | Loss: 0.00076879
Iteration 7/25 | Loss: 0.00076879
Iteration 8/25 | Loss: 0.00076879
Iteration 9/25 | Loss: 0.00076879
Iteration 10/25 | Loss: 0.00076879
Iteration 11/25 | Loss: 0.00076879
Iteration 12/25 | Loss: 0.00076879
Iteration 13/25 | Loss: 0.00076879
Iteration 14/25 | Loss: 0.00076879
Iteration 15/25 | Loss: 0.00076879
Iteration 16/25 | Loss: 0.00076879
Iteration 17/25 | Loss: 0.00076879
Iteration 18/25 | Loss: 0.00076879
Iteration 19/25 | Loss: 0.00076879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007687921752221882, 0.0007687921752221882, 0.0007687921752221882, 0.0007687921752221882, 0.0007687921752221882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007687921752221882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076879
Iteration 2/1000 | Loss: 0.00002404
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001399
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001123
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001094
Iteration 18/1000 | Loss: 0.00001093
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001093
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001091
Iteration 31/1000 | Loss: 0.00001090
Iteration 32/1000 | Loss: 0.00001090
Iteration 33/1000 | Loss: 0.00001088
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001087
Iteration 36/1000 | Loss: 0.00001087
Iteration 37/1000 | Loss: 0.00001086
Iteration 38/1000 | Loss: 0.00001086
Iteration 39/1000 | Loss: 0.00001085
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001083
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001082
Iteration 48/1000 | Loss: 0.00001082
Iteration 49/1000 | Loss: 0.00001082
Iteration 50/1000 | Loss: 0.00001082
Iteration 51/1000 | Loss: 0.00001082
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001075
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001071
Iteration 64/1000 | Loss: 0.00001071
Iteration 65/1000 | Loss: 0.00001070
Iteration 66/1000 | Loss: 0.00001070
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001066
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001062
Iteration 88/1000 | Loss: 0.00001062
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001058
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001057
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001055
Iteration 110/1000 | Loss: 0.00001055
Iteration 111/1000 | Loss: 0.00001055
Iteration 112/1000 | Loss: 0.00001055
Iteration 113/1000 | Loss: 0.00001054
Iteration 114/1000 | Loss: 0.00001054
Iteration 115/1000 | Loss: 0.00001054
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001053
Iteration 118/1000 | Loss: 0.00001053
Iteration 119/1000 | Loss: 0.00001053
Iteration 120/1000 | Loss: 0.00001053
Iteration 121/1000 | Loss: 0.00001052
Iteration 122/1000 | Loss: 0.00001052
Iteration 123/1000 | Loss: 0.00001052
Iteration 124/1000 | Loss: 0.00001052
Iteration 125/1000 | Loss: 0.00001052
Iteration 126/1000 | Loss: 0.00001052
Iteration 127/1000 | Loss: 0.00001051
Iteration 128/1000 | Loss: 0.00001051
Iteration 129/1000 | Loss: 0.00001051
Iteration 130/1000 | Loss: 0.00001051
Iteration 131/1000 | Loss: 0.00001051
Iteration 132/1000 | Loss: 0.00001051
Iteration 133/1000 | Loss: 0.00001051
Iteration 134/1000 | Loss: 0.00001050
Iteration 135/1000 | Loss: 0.00001050
Iteration 136/1000 | Loss: 0.00001050
Iteration 137/1000 | Loss: 0.00001050
Iteration 138/1000 | Loss: 0.00001050
Iteration 139/1000 | Loss: 0.00001050
Iteration 140/1000 | Loss: 0.00001050
Iteration 141/1000 | Loss: 0.00001050
Iteration 142/1000 | Loss: 0.00001050
Iteration 143/1000 | Loss: 0.00001050
Iteration 144/1000 | Loss: 0.00001050
Iteration 145/1000 | Loss: 0.00001050
Iteration 146/1000 | Loss: 0.00001050
Iteration 147/1000 | Loss: 0.00001050
Iteration 148/1000 | Loss: 0.00001049
Iteration 149/1000 | Loss: 0.00001049
Iteration 150/1000 | Loss: 0.00001049
Iteration 151/1000 | Loss: 0.00001049
Iteration 152/1000 | Loss: 0.00001049
Iteration 153/1000 | Loss: 0.00001049
Iteration 154/1000 | Loss: 0.00001049
Iteration 155/1000 | Loss: 0.00001049
Iteration 156/1000 | Loss: 0.00001049
Iteration 157/1000 | Loss: 0.00001049
Iteration 158/1000 | Loss: 0.00001049
Iteration 159/1000 | Loss: 0.00001049
Iteration 160/1000 | Loss: 0.00001049
Iteration 161/1000 | Loss: 0.00001048
Iteration 162/1000 | Loss: 0.00001048
Iteration 163/1000 | Loss: 0.00001048
Iteration 164/1000 | Loss: 0.00001048
Iteration 165/1000 | Loss: 0.00001048
Iteration 166/1000 | Loss: 0.00001048
Iteration 167/1000 | Loss: 0.00001048
Iteration 168/1000 | Loss: 0.00001048
Iteration 169/1000 | Loss: 0.00001048
Iteration 170/1000 | Loss: 0.00001048
Iteration 171/1000 | Loss: 0.00001048
Iteration 172/1000 | Loss: 0.00001048
Iteration 173/1000 | Loss: 0.00001048
Iteration 174/1000 | Loss: 0.00001048
Iteration 175/1000 | Loss: 0.00001048
Iteration 176/1000 | Loss: 0.00001048
Iteration 177/1000 | Loss: 0.00001048
Iteration 178/1000 | Loss: 0.00001048
Iteration 179/1000 | Loss: 0.00001047
Iteration 180/1000 | Loss: 0.00001047
Iteration 181/1000 | Loss: 0.00001047
Iteration 182/1000 | Loss: 0.00001047
Iteration 183/1000 | Loss: 0.00001047
Iteration 184/1000 | Loss: 0.00001047
Iteration 185/1000 | Loss: 0.00001047
Iteration 186/1000 | Loss: 0.00001047
Iteration 187/1000 | Loss: 0.00001046
Iteration 188/1000 | Loss: 0.00001046
Iteration 189/1000 | Loss: 0.00001046
Iteration 190/1000 | Loss: 0.00001046
Iteration 191/1000 | Loss: 0.00001046
Iteration 192/1000 | Loss: 0.00001046
Iteration 193/1000 | Loss: 0.00001046
Iteration 194/1000 | Loss: 0.00001046
Iteration 195/1000 | Loss: 0.00001046
Iteration 196/1000 | Loss: 0.00001046
Iteration 197/1000 | Loss: 0.00001046
Iteration 198/1000 | Loss: 0.00001046
Iteration 199/1000 | Loss: 0.00001046
Iteration 200/1000 | Loss: 0.00001046
Iteration 201/1000 | Loss: 0.00001045
Iteration 202/1000 | Loss: 0.00001045
Iteration 203/1000 | Loss: 0.00001045
Iteration 204/1000 | Loss: 0.00001045
Iteration 205/1000 | Loss: 0.00001045
Iteration 206/1000 | Loss: 0.00001045
Iteration 207/1000 | Loss: 0.00001045
Iteration 208/1000 | Loss: 0.00001044
Iteration 209/1000 | Loss: 0.00001044
Iteration 210/1000 | Loss: 0.00001044
Iteration 211/1000 | Loss: 0.00001044
Iteration 212/1000 | Loss: 0.00001044
Iteration 213/1000 | Loss: 0.00001044
Iteration 214/1000 | Loss: 0.00001044
Iteration 215/1000 | Loss: 0.00001044
Iteration 216/1000 | Loss: 0.00001044
Iteration 217/1000 | Loss: 0.00001044
Iteration 218/1000 | Loss: 0.00001044
Iteration 219/1000 | Loss: 0.00001044
Iteration 220/1000 | Loss: 0.00001044
Iteration 221/1000 | Loss: 0.00001044
Iteration 222/1000 | Loss: 0.00001044
Iteration 223/1000 | Loss: 0.00001044
Iteration 224/1000 | Loss: 0.00001044
Iteration 225/1000 | Loss: 0.00001044
Iteration 226/1000 | Loss: 0.00001044
Iteration 227/1000 | Loss: 0.00001044
Iteration 228/1000 | Loss: 0.00001044
Iteration 229/1000 | Loss: 0.00001044
Iteration 230/1000 | Loss: 0.00001044
Iteration 231/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.0441014637763146e-05, 1.0441014637763146e-05, 1.0441014637763146e-05, 1.0441014637763146e-05, 1.0441014637763146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0441014637763146e-05

Optimization complete. Final v2v error: 2.751242160797119 mm

Highest mean error: 3.538921356201172 mm for frame 70

Lowest mean error: 2.417900562286377 mm for frame 29

Saving results

Total time: 41.12810754776001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021414
Iteration 2/25 | Loss: 0.00225957
Iteration 3/25 | Loss: 0.00169486
Iteration 4/25 | Loss: 0.00145894
Iteration 5/25 | Loss: 0.00139300
Iteration 6/25 | Loss: 0.00131483
Iteration 7/25 | Loss: 0.00129392
Iteration 8/25 | Loss: 0.00127838
Iteration 9/25 | Loss: 0.00129238
Iteration 10/25 | Loss: 0.00125963
Iteration 11/25 | Loss: 0.00126607
Iteration 12/25 | Loss: 0.00126020
Iteration 13/25 | Loss: 0.00125152
Iteration 14/25 | Loss: 0.00123832
Iteration 15/25 | Loss: 0.00123706
Iteration 16/25 | Loss: 0.00123652
Iteration 17/25 | Loss: 0.00122850
Iteration 18/25 | Loss: 0.00122952
Iteration 19/25 | Loss: 0.00122985
Iteration 20/25 | Loss: 0.00123037
Iteration 21/25 | Loss: 0.00123063
Iteration 22/25 | Loss: 0.00122929
Iteration 23/25 | Loss: 0.00123457
Iteration 24/25 | Loss: 0.00122427
Iteration 25/25 | Loss: 0.00122568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34788728
Iteration 2/25 | Loss: 0.00210360
Iteration 3/25 | Loss: 0.00167740
Iteration 4/25 | Loss: 0.00167739
Iteration 5/25 | Loss: 0.00167739
Iteration 6/25 | Loss: 0.00167739
Iteration 7/25 | Loss: 0.00167739
Iteration 8/25 | Loss: 0.00167739
Iteration 9/25 | Loss: 0.00167739
Iteration 10/25 | Loss: 0.00167739
Iteration 11/25 | Loss: 0.00167739
Iteration 12/25 | Loss: 0.00167739
Iteration 13/25 | Loss: 0.00167739
Iteration 14/25 | Loss: 0.00167739
Iteration 15/25 | Loss: 0.00167739
Iteration 16/25 | Loss: 0.00167739
Iteration 17/25 | Loss: 0.00167739
Iteration 18/25 | Loss: 0.00167739
Iteration 19/25 | Loss: 0.00167739
Iteration 20/25 | Loss: 0.00167739
Iteration 21/25 | Loss: 0.00167739
Iteration 22/25 | Loss: 0.00167739
Iteration 23/25 | Loss: 0.00167739
Iteration 24/25 | Loss: 0.00167739
Iteration 25/25 | Loss: 0.00167739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167739
Iteration 2/1000 | Loss: 0.00059004
Iteration 3/1000 | Loss: 0.00110985
Iteration 4/1000 | Loss: 0.00014908
Iteration 5/1000 | Loss: 0.00049230
Iteration 6/1000 | Loss: 0.00024682
Iteration 7/1000 | Loss: 0.00025111
Iteration 8/1000 | Loss: 0.00029273
Iteration 9/1000 | Loss: 0.00029584
Iteration 10/1000 | Loss: 0.00031442
Iteration 11/1000 | Loss: 0.00044352
Iteration 12/1000 | Loss: 0.00011703
Iteration 13/1000 | Loss: 0.00041758
Iteration 14/1000 | Loss: 0.00023176
Iteration 15/1000 | Loss: 0.00010092
Iteration 16/1000 | Loss: 0.00017303
Iteration 17/1000 | Loss: 0.00030364
Iteration 18/1000 | Loss: 0.00010416
Iteration 19/1000 | Loss: 0.00020127
Iteration 20/1000 | Loss: 0.00011615
Iteration 21/1000 | Loss: 0.00021837
Iteration 22/1000 | Loss: 0.00010872
Iteration 23/1000 | Loss: 0.00011256
Iteration 24/1000 | Loss: 0.00012192
Iteration 25/1000 | Loss: 0.00013504
Iteration 26/1000 | Loss: 0.00010760
Iteration 27/1000 | Loss: 0.00011635
Iteration 28/1000 | Loss: 0.00022790
Iteration 29/1000 | Loss: 0.00034085
Iteration 30/1000 | Loss: 0.00013459
Iteration 31/1000 | Loss: 0.00012774
Iteration 32/1000 | Loss: 0.00010585
Iteration 33/1000 | Loss: 0.00017920
Iteration 34/1000 | Loss: 0.00010938
Iteration 35/1000 | Loss: 0.00015124
Iteration 36/1000 | Loss: 0.00010584
Iteration 37/1000 | Loss: 0.00011021
Iteration 38/1000 | Loss: 0.00026159
Iteration 39/1000 | Loss: 0.00014725
Iteration 40/1000 | Loss: 0.00014267
Iteration 41/1000 | Loss: 0.00013520
Iteration 42/1000 | Loss: 0.00012964
Iteration 43/1000 | Loss: 0.00011185
Iteration 44/1000 | Loss: 0.00012136
Iteration 45/1000 | Loss: 0.00011731
Iteration 46/1000 | Loss: 0.00023586
Iteration 47/1000 | Loss: 0.00011351
Iteration 48/1000 | Loss: 0.00018867
Iteration 49/1000 | Loss: 0.00012451
Iteration 50/1000 | Loss: 0.00012866
Iteration 51/1000 | Loss: 0.00014071
Iteration 52/1000 | Loss: 0.00011170
Iteration 53/1000 | Loss: 0.00011285
Iteration 54/1000 | Loss: 0.00011346
Iteration 55/1000 | Loss: 0.00012300
Iteration 56/1000 | Loss: 0.00013348
Iteration 57/1000 | Loss: 0.00012351
Iteration 58/1000 | Loss: 0.00012032
Iteration 59/1000 | Loss: 0.00011208
Iteration 60/1000 | Loss: 0.00011498
Iteration 61/1000 | Loss: 0.00011236
Iteration 62/1000 | Loss: 0.00028191
Iteration 63/1000 | Loss: 0.00012803
Iteration 64/1000 | Loss: 0.00011493
Iteration 65/1000 | Loss: 0.00011238
Iteration 66/1000 | Loss: 0.00011383
Iteration 67/1000 | Loss: 0.00011172
Iteration 68/1000 | Loss: 0.00011426
Iteration 69/1000 | Loss: 0.00024127
Iteration 70/1000 | Loss: 0.00011189
Iteration 71/1000 | Loss: 0.00011517
Iteration 72/1000 | Loss: 0.00094808
Iteration 73/1000 | Loss: 0.00094351
Iteration 74/1000 | Loss: 0.00042778
Iteration 75/1000 | Loss: 0.00096053
Iteration 76/1000 | Loss: 0.00021361
Iteration 77/1000 | Loss: 0.00014264
Iteration 78/1000 | Loss: 0.00007600
Iteration 79/1000 | Loss: 0.00010373
Iteration 80/1000 | Loss: 0.00009437
Iteration 81/1000 | Loss: 0.00012534
Iteration 82/1000 | Loss: 0.00006964
Iteration 83/1000 | Loss: 0.00007293
Iteration 84/1000 | Loss: 0.00013521
Iteration 85/1000 | Loss: 0.00008021
Iteration 86/1000 | Loss: 0.00010275
Iteration 87/1000 | Loss: 0.00009776
Iteration 88/1000 | Loss: 0.00006913
Iteration 89/1000 | Loss: 0.00012523
Iteration 90/1000 | Loss: 0.00020531
Iteration 91/1000 | Loss: 0.00016718
Iteration 92/1000 | Loss: 0.00018279
Iteration 93/1000 | Loss: 0.00008434
Iteration 94/1000 | Loss: 0.00010733
Iteration 95/1000 | Loss: 0.00007417
Iteration 96/1000 | Loss: 0.00010360
Iteration 97/1000 | Loss: 0.00007603
Iteration 98/1000 | Loss: 0.00007547
Iteration 99/1000 | Loss: 0.00007621
Iteration 100/1000 | Loss: 0.00024093
Iteration 101/1000 | Loss: 0.00007542
Iteration 102/1000 | Loss: 0.00008203
Iteration 103/1000 | Loss: 0.00007511
Iteration 104/1000 | Loss: 0.00015479
Iteration 105/1000 | Loss: 0.00006527
Iteration 106/1000 | Loss: 0.00009807
Iteration 107/1000 | Loss: 0.00006387
Iteration 108/1000 | Loss: 0.00017238
Iteration 109/1000 | Loss: 0.00041555
Iteration 110/1000 | Loss: 0.00032024
Iteration 111/1000 | Loss: 0.00012776
Iteration 112/1000 | Loss: 0.00007870
Iteration 113/1000 | Loss: 0.00011318
Iteration 114/1000 | Loss: 0.00005868
Iteration 115/1000 | Loss: 0.00006325
Iteration 116/1000 | Loss: 0.00006358
Iteration 117/1000 | Loss: 0.00006534
Iteration 118/1000 | Loss: 0.00006672
Iteration 119/1000 | Loss: 0.00006881
Iteration 120/1000 | Loss: 0.00006656
Iteration 121/1000 | Loss: 0.00006835
Iteration 122/1000 | Loss: 0.00020284
Iteration 123/1000 | Loss: 0.00052821
Iteration 124/1000 | Loss: 0.00033435
Iteration 125/1000 | Loss: 0.00014762
Iteration 126/1000 | Loss: 0.00008136
Iteration 127/1000 | Loss: 0.00007497
Iteration 128/1000 | Loss: 0.00014082
Iteration 129/1000 | Loss: 0.00007605
Iteration 130/1000 | Loss: 0.00020714
Iteration 131/1000 | Loss: 0.00015020
Iteration 132/1000 | Loss: 0.00011630
Iteration 133/1000 | Loss: 0.00007473
Iteration 134/1000 | Loss: 0.00019263
Iteration 135/1000 | Loss: 0.00057727
Iteration 136/1000 | Loss: 0.00032484
Iteration 137/1000 | Loss: 0.00033628
Iteration 138/1000 | Loss: 0.00028177
Iteration 139/1000 | Loss: 0.00023224
Iteration 140/1000 | Loss: 0.00016902
Iteration 141/1000 | Loss: 0.00013308
Iteration 142/1000 | Loss: 0.00006446
Iteration 143/1000 | Loss: 0.00010461
Iteration 144/1000 | Loss: 0.00010609
Iteration 145/1000 | Loss: 0.00005697
Iteration 146/1000 | Loss: 0.00019185
Iteration 147/1000 | Loss: 0.00013319
Iteration 148/1000 | Loss: 0.00019158
Iteration 149/1000 | Loss: 0.00006628
Iteration 150/1000 | Loss: 0.00005565
Iteration 151/1000 | Loss: 0.00006797
Iteration 152/1000 | Loss: 0.00005820
Iteration 153/1000 | Loss: 0.00006131
Iteration 154/1000 | Loss: 0.00006151
Iteration 155/1000 | Loss: 0.00008770
Iteration 156/1000 | Loss: 0.00006178
Iteration 157/1000 | Loss: 0.00008880
Iteration 158/1000 | Loss: 0.00009316
Iteration 159/1000 | Loss: 0.00006563
Iteration 160/1000 | Loss: 0.00007862
Iteration 161/1000 | Loss: 0.00008749
Iteration 162/1000 | Loss: 0.00018976
Iteration 163/1000 | Loss: 0.00013168
Iteration 164/1000 | Loss: 0.00008563
Iteration 165/1000 | Loss: 0.00005819
Iteration 166/1000 | Loss: 0.00007001
Iteration 167/1000 | Loss: 0.00027532
Iteration 168/1000 | Loss: 0.00039023
Iteration 169/1000 | Loss: 0.00032359
Iteration 170/1000 | Loss: 0.00006772
Iteration 171/1000 | Loss: 0.00016481
Iteration 172/1000 | Loss: 0.00014743
Iteration 173/1000 | Loss: 0.00008526
Iteration 174/1000 | Loss: 0.00005217
Iteration 175/1000 | Loss: 0.00005066
Iteration 176/1000 | Loss: 0.00013868
Iteration 177/1000 | Loss: 0.00006774
Iteration 178/1000 | Loss: 0.00024883
Iteration 179/1000 | Loss: 0.00011417
Iteration 180/1000 | Loss: 0.00021578
Iteration 181/1000 | Loss: 0.00006902
Iteration 182/1000 | Loss: 0.00012639
Iteration 183/1000 | Loss: 0.00006530
Iteration 184/1000 | Loss: 0.00013297
Iteration 185/1000 | Loss: 0.00006624
Iteration 186/1000 | Loss: 0.00009421
Iteration 187/1000 | Loss: 0.00007389
Iteration 188/1000 | Loss: 0.00004908
Iteration 189/1000 | Loss: 0.00020385
Iteration 190/1000 | Loss: 0.00005815
Iteration 191/1000 | Loss: 0.00005995
Iteration 192/1000 | Loss: 0.00006362
Iteration 193/1000 | Loss: 0.00005672
Iteration 194/1000 | Loss: 0.00004675
Iteration 195/1000 | Loss: 0.00004746
Iteration 196/1000 | Loss: 0.00015835
Iteration 197/1000 | Loss: 0.00079588
Iteration 198/1000 | Loss: 0.00018000
Iteration 199/1000 | Loss: 0.00018400
Iteration 200/1000 | Loss: 0.00005554
Iteration 201/1000 | Loss: 0.00005583
Iteration 202/1000 | Loss: 0.00018383
Iteration 203/1000 | Loss: 0.00004907
Iteration 204/1000 | Loss: 0.00008386
Iteration 205/1000 | Loss: 0.00004431
Iteration 206/1000 | Loss: 0.00004225
Iteration 207/1000 | Loss: 0.00007348
Iteration 208/1000 | Loss: 0.00004183
Iteration 209/1000 | Loss: 0.00004019
Iteration 210/1000 | Loss: 0.00004006
Iteration 211/1000 | Loss: 0.00003972
Iteration 212/1000 | Loss: 0.00003963
Iteration 213/1000 | Loss: 0.00003962
Iteration 214/1000 | Loss: 0.00003946
Iteration 215/1000 | Loss: 0.00005917
Iteration 216/1000 | Loss: 0.00003923
Iteration 217/1000 | Loss: 0.00003917
Iteration 218/1000 | Loss: 0.00003914
Iteration 219/1000 | Loss: 0.00003908
Iteration 220/1000 | Loss: 0.00003905
Iteration 221/1000 | Loss: 0.00003904
Iteration 222/1000 | Loss: 0.00003901
Iteration 223/1000 | Loss: 0.00007275
Iteration 224/1000 | Loss: 0.00014752
Iteration 225/1000 | Loss: 0.00005160
Iteration 226/1000 | Loss: 0.00003922
Iteration 227/1000 | Loss: 0.00006428
Iteration 228/1000 | Loss: 0.00003899
Iteration 229/1000 | Loss: 0.00003892
Iteration 230/1000 | Loss: 0.00003892
Iteration 231/1000 | Loss: 0.00003892
Iteration 232/1000 | Loss: 0.00003892
Iteration 233/1000 | Loss: 0.00003892
Iteration 234/1000 | Loss: 0.00003891
Iteration 235/1000 | Loss: 0.00003891
Iteration 236/1000 | Loss: 0.00003891
Iteration 237/1000 | Loss: 0.00003891
Iteration 238/1000 | Loss: 0.00003891
Iteration 239/1000 | Loss: 0.00003891
Iteration 240/1000 | Loss: 0.00003891
Iteration 241/1000 | Loss: 0.00003891
Iteration 242/1000 | Loss: 0.00003891
Iteration 243/1000 | Loss: 0.00003890
Iteration 244/1000 | Loss: 0.00003890
Iteration 245/1000 | Loss: 0.00003890
Iteration 246/1000 | Loss: 0.00003889
Iteration 247/1000 | Loss: 0.00003889
Iteration 248/1000 | Loss: 0.00003889
Iteration 249/1000 | Loss: 0.00003889
Iteration 250/1000 | Loss: 0.00003889
Iteration 251/1000 | Loss: 0.00003889
Iteration 252/1000 | Loss: 0.00003889
Iteration 253/1000 | Loss: 0.00003889
Iteration 254/1000 | Loss: 0.00003889
Iteration 255/1000 | Loss: 0.00003888
Iteration 256/1000 | Loss: 0.00003888
Iteration 257/1000 | Loss: 0.00003888
Iteration 258/1000 | Loss: 0.00003887
Iteration 259/1000 | Loss: 0.00003887
Iteration 260/1000 | Loss: 0.00003887
Iteration 261/1000 | Loss: 0.00003887
Iteration 262/1000 | Loss: 0.00003887
Iteration 263/1000 | Loss: 0.00003887
Iteration 264/1000 | Loss: 0.00003886
Iteration 265/1000 | Loss: 0.00003886
Iteration 266/1000 | Loss: 0.00003886
Iteration 267/1000 | Loss: 0.00003886
Iteration 268/1000 | Loss: 0.00003886
Iteration 269/1000 | Loss: 0.00003886
Iteration 270/1000 | Loss: 0.00003886
Iteration 271/1000 | Loss: 0.00003886
Iteration 272/1000 | Loss: 0.00003886
Iteration 273/1000 | Loss: 0.00003885
Iteration 274/1000 | Loss: 0.00003885
Iteration 275/1000 | Loss: 0.00003885
Iteration 276/1000 | Loss: 0.00003885
Iteration 277/1000 | Loss: 0.00003885
Iteration 278/1000 | Loss: 0.00003885
Iteration 279/1000 | Loss: 0.00003885
Iteration 280/1000 | Loss: 0.00003885
Iteration 281/1000 | Loss: 0.00003885
Iteration 282/1000 | Loss: 0.00003885
Iteration 283/1000 | Loss: 0.00003885
Iteration 284/1000 | Loss: 0.00003885
Iteration 285/1000 | Loss: 0.00003885
Iteration 286/1000 | Loss: 0.00003885
Iteration 287/1000 | Loss: 0.00003885
Iteration 288/1000 | Loss: 0.00003884
Iteration 289/1000 | Loss: 0.00003884
Iteration 290/1000 | Loss: 0.00003884
Iteration 291/1000 | Loss: 0.00003884
Iteration 292/1000 | Loss: 0.00003884
Iteration 293/1000 | Loss: 0.00003884
Iteration 294/1000 | Loss: 0.00003884
Iteration 295/1000 | Loss: 0.00003884
Iteration 296/1000 | Loss: 0.00003884
Iteration 297/1000 | Loss: 0.00003884
Iteration 298/1000 | Loss: 0.00003884
Iteration 299/1000 | Loss: 0.00003883
Iteration 300/1000 | Loss: 0.00003883
Iteration 301/1000 | Loss: 0.00003883
Iteration 302/1000 | Loss: 0.00003883
Iteration 303/1000 | Loss: 0.00003883
Iteration 304/1000 | Loss: 0.00003883
Iteration 305/1000 | Loss: 0.00003883
Iteration 306/1000 | Loss: 0.00003883
Iteration 307/1000 | Loss: 0.00003883
Iteration 308/1000 | Loss: 0.00003883
Iteration 309/1000 | Loss: 0.00003883
Iteration 310/1000 | Loss: 0.00003883
Iteration 311/1000 | Loss: 0.00003883
Iteration 312/1000 | Loss: 0.00003883
Iteration 313/1000 | Loss: 0.00003883
Iteration 314/1000 | Loss: 0.00003883
Iteration 315/1000 | Loss: 0.00003883
Iteration 316/1000 | Loss: 0.00003883
Iteration 317/1000 | Loss: 0.00003883
Iteration 318/1000 | Loss: 0.00003883
Iteration 319/1000 | Loss: 0.00003883
Iteration 320/1000 | Loss: 0.00003883
Iteration 321/1000 | Loss: 0.00003883
Iteration 322/1000 | Loss: 0.00003883
Iteration 323/1000 | Loss: 0.00003883
Iteration 324/1000 | Loss: 0.00003883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [3.882683449774049e-05, 3.882683449774049e-05, 3.882683449774049e-05, 3.882683449774049e-05, 3.882683449774049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.882683449774049e-05

Optimization complete. Final v2v error: 3.4941341876983643 mm

Highest mean error: 10.457054138183594 mm for frame 201

Lowest mean error: 2.5829286575317383 mm for frame 64

Saving results

Total time: 406.5801842212677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408392
Iteration 2/25 | Loss: 0.00122055
Iteration 3/25 | Loss: 0.00113541
Iteration 4/25 | Loss: 0.00112201
Iteration 5/25 | Loss: 0.00111879
Iteration 6/25 | Loss: 0.00111872
Iteration 7/25 | Loss: 0.00111872
Iteration 8/25 | Loss: 0.00111872
Iteration 9/25 | Loss: 0.00111872
Iteration 10/25 | Loss: 0.00111872
Iteration 11/25 | Loss: 0.00111872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001118723303079605, 0.001118723303079605, 0.001118723303079605, 0.001118723303079605, 0.001118723303079605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118723303079605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46362305
Iteration 2/25 | Loss: 0.00088551
Iteration 3/25 | Loss: 0.00088548
Iteration 4/25 | Loss: 0.00088548
Iteration 5/25 | Loss: 0.00088548
Iteration 6/25 | Loss: 0.00088548
Iteration 7/25 | Loss: 0.00088548
Iteration 8/25 | Loss: 0.00088548
Iteration 9/25 | Loss: 0.00088548
Iteration 10/25 | Loss: 0.00088548
Iteration 11/25 | Loss: 0.00088548
Iteration 12/25 | Loss: 0.00088548
Iteration 13/25 | Loss: 0.00088548
Iteration 14/25 | Loss: 0.00088548
Iteration 15/25 | Loss: 0.00088548
Iteration 16/25 | Loss: 0.00088548
Iteration 17/25 | Loss: 0.00088548
Iteration 18/25 | Loss: 0.00088548
Iteration 19/25 | Loss: 0.00088548
Iteration 20/25 | Loss: 0.00088548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008854786865413189, 0.0008854786865413189, 0.0008854786865413189, 0.0008854786865413189, 0.0008854786865413189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008854786865413189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088548
Iteration 2/1000 | Loss: 0.00002456
Iteration 3/1000 | Loss: 0.00001886
Iteration 4/1000 | Loss: 0.00001777
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001699
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001627
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001550
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001542
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001531
Iteration 45/1000 | Loss: 0.00001531
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001530
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001529
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001528
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001526
Iteration 56/1000 | Loss: 0.00001526
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001525
Iteration 59/1000 | Loss: 0.00001525
Iteration 60/1000 | Loss: 0.00001525
Iteration 61/1000 | Loss: 0.00001525
Iteration 62/1000 | Loss: 0.00001525
Iteration 63/1000 | Loss: 0.00001525
Iteration 64/1000 | Loss: 0.00001525
Iteration 65/1000 | Loss: 0.00001525
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001520
Iteration 89/1000 | Loss: 0.00001520
Iteration 90/1000 | Loss: 0.00001520
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001519
Iteration 93/1000 | Loss: 0.00001519
Iteration 94/1000 | Loss: 0.00001519
Iteration 95/1000 | Loss: 0.00001519
Iteration 96/1000 | Loss: 0.00001519
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001518
Iteration 99/1000 | Loss: 0.00001518
Iteration 100/1000 | Loss: 0.00001518
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001518
Iteration 111/1000 | Loss: 0.00001518
Iteration 112/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.5183655705186538e-05, 1.5183655705186538e-05, 1.5183655705186538e-05, 1.5183655705186538e-05, 1.5183655705186538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5183655705186538e-05

Optimization complete. Final v2v error: 3.343857765197754 mm

Highest mean error: 3.494635581970215 mm for frame 40

Lowest mean error: 3.1822447776794434 mm for frame 49

Saving results

Total time: 31.630412340164185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391716
Iteration 2/25 | Loss: 0.00113952
Iteration 3/25 | Loss: 0.00107372
Iteration 4/25 | Loss: 0.00106457
Iteration 5/25 | Loss: 0.00106139
Iteration 6/25 | Loss: 0.00106053
Iteration 7/25 | Loss: 0.00106053
Iteration 8/25 | Loss: 0.00106053
Iteration 9/25 | Loss: 0.00106053
Iteration 10/25 | Loss: 0.00106053
Iteration 11/25 | Loss: 0.00106053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001060525537468493, 0.001060525537468493, 0.001060525537468493, 0.001060525537468493, 0.001060525537468493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001060525537468493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58726192
Iteration 2/25 | Loss: 0.00088606
Iteration 3/25 | Loss: 0.00088606
Iteration 4/25 | Loss: 0.00088605
Iteration 5/25 | Loss: 0.00088605
Iteration 6/25 | Loss: 0.00088605
Iteration 7/25 | Loss: 0.00088605
Iteration 8/25 | Loss: 0.00088605
Iteration 9/25 | Loss: 0.00088605
Iteration 10/25 | Loss: 0.00088605
Iteration 11/25 | Loss: 0.00088605
Iteration 12/25 | Loss: 0.00088605
Iteration 13/25 | Loss: 0.00088605
Iteration 14/25 | Loss: 0.00088605
Iteration 15/25 | Loss: 0.00088605
Iteration 16/25 | Loss: 0.00088605
Iteration 17/25 | Loss: 0.00088605
Iteration 18/25 | Loss: 0.00088605
Iteration 19/25 | Loss: 0.00088605
Iteration 20/25 | Loss: 0.00088605
Iteration 21/25 | Loss: 0.00088605
Iteration 22/25 | Loss: 0.00088605
Iteration 23/25 | Loss: 0.00088605
Iteration 24/25 | Loss: 0.00088605
Iteration 25/25 | Loss: 0.00088605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008860511006787419, 0.0008860511006787419, 0.0008860511006787419, 0.0008860511006787419, 0.0008860511006787419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008860511006787419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088605
Iteration 2/1000 | Loss: 0.00002158
Iteration 3/1000 | Loss: 0.00001358
Iteration 4/1000 | Loss: 0.00001112
Iteration 5/1000 | Loss: 0.00001044
Iteration 6/1000 | Loss: 0.00001003
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000942
Iteration 9/1000 | Loss: 0.00000939
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000926
Iteration 12/1000 | Loss: 0.00000910
Iteration 13/1000 | Loss: 0.00000903
Iteration 14/1000 | Loss: 0.00000902
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000897
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000895
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000891
Iteration 21/1000 | Loss: 0.00000888
Iteration 22/1000 | Loss: 0.00000888
Iteration 23/1000 | Loss: 0.00000887
Iteration 24/1000 | Loss: 0.00000886
Iteration 25/1000 | Loss: 0.00000886
Iteration 26/1000 | Loss: 0.00000884
Iteration 27/1000 | Loss: 0.00000881
Iteration 28/1000 | Loss: 0.00000879
Iteration 29/1000 | Loss: 0.00000873
Iteration 30/1000 | Loss: 0.00000872
Iteration 31/1000 | Loss: 0.00000870
Iteration 32/1000 | Loss: 0.00000870
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000869
Iteration 35/1000 | Loss: 0.00000869
Iteration 36/1000 | Loss: 0.00000869
Iteration 37/1000 | Loss: 0.00000868
Iteration 38/1000 | Loss: 0.00000868
Iteration 39/1000 | Loss: 0.00000868
Iteration 40/1000 | Loss: 0.00000868
Iteration 41/1000 | Loss: 0.00000868
Iteration 42/1000 | Loss: 0.00000867
Iteration 43/1000 | Loss: 0.00000867
Iteration 44/1000 | Loss: 0.00000866
Iteration 45/1000 | Loss: 0.00000866
Iteration 46/1000 | Loss: 0.00000866
Iteration 47/1000 | Loss: 0.00000866
Iteration 48/1000 | Loss: 0.00000865
Iteration 49/1000 | Loss: 0.00000865
Iteration 50/1000 | Loss: 0.00000864
Iteration 51/1000 | Loss: 0.00000864
Iteration 52/1000 | Loss: 0.00000864
Iteration 53/1000 | Loss: 0.00000863
Iteration 54/1000 | Loss: 0.00000862
Iteration 55/1000 | Loss: 0.00000862
Iteration 56/1000 | Loss: 0.00000861
Iteration 57/1000 | Loss: 0.00000861
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000860
Iteration 62/1000 | Loss: 0.00000860
Iteration 63/1000 | Loss: 0.00000860
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000859
Iteration 66/1000 | Loss: 0.00000858
Iteration 67/1000 | Loss: 0.00000857
Iteration 68/1000 | Loss: 0.00000856
Iteration 69/1000 | Loss: 0.00000856
Iteration 70/1000 | Loss: 0.00000856
Iteration 71/1000 | Loss: 0.00000856
Iteration 72/1000 | Loss: 0.00000856
Iteration 73/1000 | Loss: 0.00000856
Iteration 74/1000 | Loss: 0.00000856
Iteration 75/1000 | Loss: 0.00000856
Iteration 76/1000 | Loss: 0.00000856
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000856
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000856
Iteration 82/1000 | Loss: 0.00000855
Iteration 83/1000 | Loss: 0.00000855
Iteration 84/1000 | Loss: 0.00000855
Iteration 85/1000 | Loss: 0.00000855
Iteration 86/1000 | Loss: 0.00000855
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000854
Iteration 90/1000 | Loss: 0.00000854
Iteration 91/1000 | Loss: 0.00000854
Iteration 92/1000 | Loss: 0.00000854
Iteration 93/1000 | Loss: 0.00000853
Iteration 94/1000 | Loss: 0.00000853
Iteration 95/1000 | Loss: 0.00000853
Iteration 96/1000 | Loss: 0.00000852
Iteration 97/1000 | Loss: 0.00000852
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000851
Iteration 102/1000 | Loss: 0.00000851
Iteration 103/1000 | Loss: 0.00000851
Iteration 104/1000 | Loss: 0.00000851
Iteration 105/1000 | Loss: 0.00000850
Iteration 106/1000 | Loss: 0.00000850
Iteration 107/1000 | Loss: 0.00000850
Iteration 108/1000 | Loss: 0.00000850
Iteration 109/1000 | Loss: 0.00000850
Iteration 110/1000 | Loss: 0.00000850
Iteration 111/1000 | Loss: 0.00000850
Iteration 112/1000 | Loss: 0.00000850
Iteration 113/1000 | Loss: 0.00000849
Iteration 114/1000 | Loss: 0.00000849
Iteration 115/1000 | Loss: 0.00000848
Iteration 116/1000 | Loss: 0.00000848
Iteration 117/1000 | Loss: 0.00000848
Iteration 118/1000 | Loss: 0.00000847
Iteration 119/1000 | Loss: 0.00000846
Iteration 120/1000 | Loss: 0.00000846
Iteration 121/1000 | Loss: 0.00000846
Iteration 122/1000 | Loss: 0.00000846
Iteration 123/1000 | Loss: 0.00000846
Iteration 124/1000 | Loss: 0.00000846
Iteration 125/1000 | Loss: 0.00000846
Iteration 126/1000 | Loss: 0.00000846
Iteration 127/1000 | Loss: 0.00000846
Iteration 128/1000 | Loss: 0.00000845
Iteration 129/1000 | Loss: 0.00000845
Iteration 130/1000 | Loss: 0.00000845
Iteration 131/1000 | Loss: 0.00000844
Iteration 132/1000 | Loss: 0.00000844
Iteration 133/1000 | Loss: 0.00000844
Iteration 134/1000 | Loss: 0.00000844
Iteration 135/1000 | Loss: 0.00000844
Iteration 136/1000 | Loss: 0.00000843
Iteration 137/1000 | Loss: 0.00000843
Iteration 138/1000 | Loss: 0.00000843
Iteration 139/1000 | Loss: 0.00000843
Iteration 140/1000 | Loss: 0.00000843
Iteration 141/1000 | Loss: 0.00000843
Iteration 142/1000 | Loss: 0.00000843
Iteration 143/1000 | Loss: 0.00000843
Iteration 144/1000 | Loss: 0.00000842
Iteration 145/1000 | Loss: 0.00000842
Iteration 146/1000 | Loss: 0.00000842
Iteration 147/1000 | Loss: 0.00000842
Iteration 148/1000 | Loss: 0.00000842
Iteration 149/1000 | Loss: 0.00000842
Iteration 150/1000 | Loss: 0.00000841
Iteration 151/1000 | Loss: 0.00000841
Iteration 152/1000 | Loss: 0.00000841
Iteration 153/1000 | Loss: 0.00000841
Iteration 154/1000 | Loss: 0.00000841
Iteration 155/1000 | Loss: 0.00000841
Iteration 156/1000 | Loss: 0.00000841
Iteration 157/1000 | Loss: 0.00000841
Iteration 158/1000 | Loss: 0.00000841
Iteration 159/1000 | Loss: 0.00000841
Iteration 160/1000 | Loss: 0.00000841
Iteration 161/1000 | Loss: 0.00000841
Iteration 162/1000 | Loss: 0.00000841
Iteration 163/1000 | Loss: 0.00000841
Iteration 164/1000 | Loss: 0.00000841
Iteration 165/1000 | Loss: 0.00000840
Iteration 166/1000 | Loss: 0.00000840
Iteration 167/1000 | Loss: 0.00000840
Iteration 168/1000 | Loss: 0.00000840
Iteration 169/1000 | Loss: 0.00000840
Iteration 170/1000 | Loss: 0.00000840
Iteration 171/1000 | Loss: 0.00000839
Iteration 172/1000 | Loss: 0.00000839
Iteration 173/1000 | Loss: 0.00000839
Iteration 174/1000 | Loss: 0.00000839
Iteration 175/1000 | Loss: 0.00000839
Iteration 176/1000 | Loss: 0.00000839
Iteration 177/1000 | Loss: 0.00000839
Iteration 178/1000 | Loss: 0.00000839
Iteration 179/1000 | Loss: 0.00000839
Iteration 180/1000 | Loss: 0.00000839
Iteration 181/1000 | Loss: 0.00000839
Iteration 182/1000 | Loss: 0.00000839
Iteration 183/1000 | Loss: 0.00000839
Iteration 184/1000 | Loss: 0.00000839
Iteration 185/1000 | Loss: 0.00000839
Iteration 186/1000 | Loss: 0.00000839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [8.38959931570571e-06, 8.38959931570571e-06, 8.38959931570571e-06, 8.38959931570571e-06, 8.38959931570571e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.38959931570571e-06

Optimization complete. Final v2v error: 2.5096426010131836 mm

Highest mean error: 2.7646937370300293 mm for frame 98

Lowest mean error: 2.306349515914917 mm for frame 12

Saving results

Total time: 37.51332116127014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761543
Iteration 2/25 | Loss: 0.00162542
Iteration 3/25 | Loss: 0.00120518
Iteration 4/25 | Loss: 0.00117178
Iteration 5/25 | Loss: 0.00117825
Iteration 6/25 | Loss: 0.00117808
Iteration 7/25 | Loss: 0.00116957
Iteration 8/25 | Loss: 0.00116045
Iteration 9/25 | Loss: 0.00116096
Iteration 10/25 | Loss: 0.00115799
Iteration 11/25 | Loss: 0.00115599
Iteration 12/25 | Loss: 0.00115450
Iteration 13/25 | Loss: 0.00115423
Iteration 14/25 | Loss: 0.00115414
Iteration 15/25 | Loss: 0.00115414
Iteration 16/25 | Loss: 0.00115414
Iteration 17/25 | Loss: 0.00115414
Iteration 18/25 | Loss: 0.00115414
Iteration 19/25 | Loss: 0.00115414
Iteration 20/25 | Loss: 0.00115414
Iteration 21/25 | Loss: 0.00115413
Iteration 22/25 | Loss: 0.00115413
Iteration 23/25 | Loss: 0.00115413
Iteration 24/25 | Loss: 0.00115413
Iteration 25/25 | Loss: 0.00115413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.47236347
Iteration 2/25 | Loss: 0.00056725
Iteration 3/25 | Loss: 0.00056698
Iteration 4/25 | Loss: 0.00056698
Iteration 5/25 | Loss: 0.00056698
Iteration 6/25 | Loss: 0.00056698
Iteration 7/25 | Loss: 0.00056698
Iteration 8/25 | Loss: 0.00056698
Iteration 9/25 | Loss: 0.00056698
Iteration 10/25 | Loss: 0.00056698
Iteration 11/25 | Loss: 0.00056698
Iteration 12/25 | Loss: 0.00056697
Iteration 13/25 | Loss: 0.00056697
Iteration 14/25 | Loss: 0.00056697
Iteration 15/25 | Loss: 0.00056697
Iteration 16/25 | Loss: 0.00056697
Iteration 17/25 | Loss: 0.00056697
Iteration 18/25 | Loss: 0.00056697
Iteration 19/25 | Loss: 0.00056697
Iteration 20/25 | Loss: 0.00056697
Iteration 21/25 | Loss: 0.00056697
Iteration 22/25 | Loss: 0.00056697
Iteration 23/25 | Loss: 0.00056697
Iteration 24/25 | Loss: 0.00056697
Iteration 25/25 | Loss: 0.00056697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056697
Iteration 2/1000 | Loss: 0.00002509
Iteration 3/1000 | Loss: 0.00019040
Iteration 4/1000 | Loss: 0.00016467
Iteration 5/1000 | Loss: 0.00020944
Iteration 6/1000 | Loss: 0.00002757
Iteration 7/1000 | Loss: 0.00002132
Iteration 8/1000 | Loss: 0.00001829
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001547
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001460
Iteration 14/1000 | Loss: 0.00001451
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001408
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001397
Iteration 28/1000 | Loss: 0.00001397
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001396
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001388
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001387
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001376
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001370
Iteration 96/1000 | Loss: 0.00001370
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001367
Iteration 110/1000 | Loss: 0.00001367
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001366
Iteration 113/1000 | Loss: 0.00001366
Iteration 114/1000 | Loss: 0.00001365
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001364
Iteration 119/1000 | Loss: 0.00001363
Iteration 120/1000 | Loss: 0.00001362
Iteration 121/1000 | Loss: 0.00001362
Iteration 122/1000 | Loss: 0.00001362
Iteration 123/1000 | Loss: 0.00001362
Iteration 124/1000 | Loss: 0.00001362
Iteration 125/1000 | Loss: 0.00001362
Iteration 126/1000 | Loss: 0.00001361
Iteration 127/1000 | Loss: 0.00001361
Iteration 128/1000 | Loss: 0.00001361
Iteration 129/1000 | Loss: 0.00001361
Iteration 130/1000 | Loss: 0.00001361
Iteration 131/1000 | Loss: 0.00001361
Iteration 132/1000 | Loss: 0.00001361
Iteration 133/1000 | Loss: 0.00001361
Iteration 134/1000 | Loss: 0.00001361
Iteration 135/1000 | Loss: 0.00001361
Iteration 136/1000 | Loss: 0.00001360
Iteration 137/1000 | Loss: 0.00001360
Iteration 138/1000 | Loss: 0.00001360
Iteration 139/1000 | Loss: 0.00001360
Iteration 140/1000 | Loss: 0.00001360
Iteration 141/1000 | Loss: 0.00001360
Iteration 142/1000 | Loss: 0.00001359
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Iteration 145/1000 | Loss: 0.00001359
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.3591429706139024e-05, 1.3591429706139024e-05, 1.3591429706139024e-05, 1.3591429706139024e-05, 1.3591429706139024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3591429706139024e-05

Optimization complete. Final v2v error: 3.1528751850128174 mm

Highest mean error: 4.155992031097412 mm for frame 5

Lowest mean error: 2.8871238231658936 mm for frame 67

Saving results

Total time: 65.4453125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770503
Iteration 2/25 | Loss: 0.00132481
Iteration 3/25 | Loss: 0.00118864
Iteration 4/25 | Loss: 0.00117337
Iteration 5/25 | Loss: 0.00117010
Iteration 6/25 | Loss: 0.00117010
Iteration 7/25 | Loss: 0.00117010
Iteration 8/25 | Loss: 0.00117010
Iteration 9/25 | Loss: 0.00117010
Iteration 10/25 | Loss: 0.00117010
Iteration 11/25 | Loss: 0.00117010
Iteration 12/25 | Loss: 0.00117010
Iteration 13/25 | Loss: 0.00117010
Iteration 14/25 | Loss: 0.00117010
Iteration 15/25 | Loss: 0.00117010
Iteration 16/25 | Loss: 0.00117010
Iteration 17/25 | Loss: 0.00117010
Iteration 18/25 | Loss: 0.00117010
Iteration 19/25 | Loss: 0.00117010
Iteration 20/25 | Loss: 0.00117010
Iteration 21/25 | Loss: 0.00117010
Iteration 22/25 | Loss: 0.00117010
Iteration 23/25 | Loss: 0.00117010
Iteration 24/25 | Loss: 0.00117010
Iteration 25/25 | Loss: 0.00117010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15291166
Iteration 2/25 | Loss: 0.00075970
Iteration 3/25 | Loss: 0.00075964
Iteration 4/25 | Loss: 0.00075964
Iteration 5/25 | Loss: 0.00075964
Iteration 6/25 | Loss: 0.00075964
Iteration 7/25 | Loss: 0.00075964
Iteration 8/25 | Loss: 0.00075964
Iteration 9/25 | Loss: 0.00075964
Iteration 10/25 | Loss: 0.00075964
Iteration 11/25 | Loss: 0.00075964
Iteration 12/25 | Loss: 0.00075964
Iteration 13/25 | Loss: 0.00075964
Iteration 14/25 | Loss: 0.00075964
Iteration 15/25 | Loss: 0.00075964
Iteration 16/25 | Loss: 0.00075964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007596405339427292, 0.0007596405339427292, 0.0007596405339427292, 0.0007596405339427292, 0.0007596405339427292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007596405339427292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075964
Iteration 2/1000 | Loss: 0.00004338
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001852
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001716
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001658
Iteration 11/1000 | Loss: 0.00001643
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001623
Iteration 16/1000 | Loss: 0.00001621
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001590
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001590
Iteration 36/1000 | Loss: 0.00001589
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001587
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001584
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001583
Iteration 51/1000 | Loss: 0.00001583
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001582
Iteration 54/1000 | Loss: 0.00001582
Iteration 55/1000 | Loss: 0.00001581
Iteration 56/1000 | Loss: 0.00001581
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001580
Iteration 59/1000 | Loss: 0.00001580
Iteration 60/1000 | Loss: 0.00001579
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001579
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001578
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001578
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001577
Iteration 69/1000 | Loss: 0.00001577
Iteration 70/1000 | Loss: 0.00001577
Iteration 71/1000 | Loss: 0.00001576
Iteration 72/1000 | Loss: 0.00001576
Iteration 73/1000 | Loss: 0.00001576
Iteration 74/1000 | Loss: 0.00001576
Iteration 75/1000 | Loss: 0.00001576
Iteration 76/1000 | Loss: 0.00001576
Iteration 77/1000 | Loss: 0.00001576
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001575
Iteration 81/1000 | Loss: 0.00001575
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001574
Iteration 88/1000 | Loss: 0.00001573
Iteration 89/1000 | Loss: 0.00001573
Iteration 90/1000 | Loss: 0.00001573
Iteration 91/1000 | Loss: 0.00001573
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001572
Iteration 98/1000 | Loss: 0.00001572
Iteration 99/1000 | Loss: 0.00001572
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001571
Iteration 102/1000 | Loss: 0.00001571
Iteration 103/1000 | Loss: 0.00001571
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001566
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001566
Iteration 121/1000 | Loss: 0.00001565
Iteration 122/1000 | Loss: 0.00001565
Iteration 123/1000 | Loss: 0.00001565
Iteration 124/1000 | Loss: 0.00001564
Iteration 125/1000 | Loss: 0.00001564
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001563
Iteration 129/1000 | Loss: 0.00001563
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001562
Iteration 134/1000 | Loss: 0.00001562
Iteration 135/1000 | Loss: 0.00001562
Iteration 136/1000 | Loss: 0.00001561
Iteration 137/1000 | Loss: 0.00001561
Iteration 138/1000 | Loss: 0.00001561
Iteration 139/1000 | Loss: 0.00001560
Iteration 140/1000 | Loss: 0.00001560
Iteration 141/1000 | Loss: 0.00001560
Iteration 142/1000 | Loss: 0.00001560
Iteration 143/1000 | Loss: 0.00001560
Iteration 144/1000 | Loss: 0.00001560
Iteration 145/1000 | Loss: 0.00001560
Iteration 146/1000 | Loss: 0.00001560
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001559
Iteration 150/1000 | Loss: 0.00001559
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001558
Iteration 153/1000 | Loss: 0.00001558
Iteration 154/1000 | Loss: 0.00001558
Iteration 155/1000 | Loss: 0.00001558
Iteration 156/1000 | Loss: 0.00001558
Iteration 157/1000 | Loss: 0.00001558
Iteration 158/1000 | Loss: 0.00001558
Iteration 159/1000 | Loss: 0.00001558
Iteration 160/1000 | Loss: 0.00001557
Iteration 161/1000 | Loss: 0.00001557
Iteration 162/1000 | Loss: 0.00001557
Iteration 163/1000 | Loss: 0.00001557
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001556
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001556
Iteration 169/1000 | Loss: 0.00001556
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001555
Iteration 172/1000 | Loss: 0.00001555
Iteration 173/1000 | Loss: 0.00001555
Iteration 174/1000 | Loss: 0.00001554
Iteration 175/1000 | Loss: 0.00001554
Iteration 176/1000 | Loss: 0.00001554
Iteration 177/1000 | Loss: 0.00001554
Iteration 178/1000 | Loss: 0.00001554
Iteration 179/1000 | Loss: 0.00001554
Iteration 180/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.554453410790302e-05, 1.554453410790302e-05, 1.554453410790302e-05, 1.554453410790302e-05, 1.554453410790302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.554453410790302e-05

Optimization complete. Final v2v error: 3.2828879356384277 mm

Highest mean error: 4.3410139083862305 mm for frame 211

Lowest mean error: 2.704721450805664 mm for frame 145

Saving results

Total time: 45.37960886955261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804380
Iteration 2/25 | Loss: 0.00160734
Iteration 3/25 | Loss: 0.00141496
Iteration 4/25 | Loss: 0.00138648
Iteration 5/25 | Loss: 0.00137452
Iteration 6/25 | Loss: 0.00138031
Iteration 7/25 | Loss: 0.00138193
Iteration 8/25 | Loss: 0.00135490
Iteration 9/25 | Loss: 0.00133046
Iteration 10/25 | Loss: 0.00131807
Iteration 11/25 | Loss: 0.00131537
Iteration 12/25 | Loss: 0.00130925
Iteration 13/25 | Loss: 0.00129855
Iteration 14/25 | Loss: 0.00129583
Iteration 15/25 | Loss: 0.00129538
Iteration 16/25 | Loss: 0.00129526
Iteration 17/25 | Loss: 0.00129515
Iteration 18/25 | Loss: 0.00129448
Iteration 19/25 | Loss: 0.00129630
Iteration 20/25 | Loss: 0.00130608
Iteration 21/25 | Loss: 0.00130294
Iteration 22/25 | Loss: 0.00129051
Iteration 23/25 | Loss: 0.00127894
Iteration 24/25 | Loss: 0.00128045
Iteration 25/25 | Loss: 0.00127698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28863966
Iteration 2/25 | Loss: 0.00331396
Iteration 3/25 | Loss: 0.00331394
Iteration 4/25 | Loss: 0.00331394
Iteration 5/25 | Loss: 0.00331394
Iteration 6/25 | Loss: 0.00331394
Iteration 7/25 | Loss: 0.00331394
Iteration 8/25 | Loss: 0.00331394
Iteration 9/25 | Loss: 0.00331394
Iteration 10/25 | Loss: 0.00331394
Iteration 11/25 | Loss: 0.00331394
Iteration 12/25 | Loss: 0.00331394
Iteration 13/25 | Loss: 0.00331394
Iteration 14/25 | Loss: 0.00331394
Iteration 15/25 | Loss: 0.00331394
Iteration 16/25 | Loss: 0.00331394
Iteration 17/25 | Loss: 0.00331394
Iteration 18/25 | Loss: 0.00331394
Iteration 19/25 | Loss: 0.00331394
Iteration 20/25 | Loss: 0.00331394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0033139390870928764, 0.0033139390870928764, 0.0033139390870928764, 0.0033139390870928764, 0.0033139390870928764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033139390870928764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00331394
Iteration 2/1000 | Loss: 0.00025716
Iteration 3/1000 | Loss: 0.00044350
Iteration 4/1000 | Loss: 0.00019908
Iteration 5/1000 | Loss: 0.00042468
Iteration 6/1000 | Loss: 0.00034548
Iteration 7/1000 | Loss: 0.00033374
Iteration 8/1000 | Loss: 0.00029401
Iteration 9/1000 | Loss: 0.00034877
Iteration 10/1000 | Loss: 0.00020061
Iteration 11/1000 | Loss: 0.00013776
Iteration 12/1000 | Loss: 0.00012473
Iteration 13/1000 | Loss: 0.00011444
Iteration 14/1000 | Loss: 0.00012047
Iteration 15/1000 | Loss: 0.00035583
Iteration 16/1000 | Loss: 0.00030789
Iteration 17/1000 | Loss: 0.00032922
Iteration 18/1000 | Loss: 0.00037687
Iteration 19/1000 | Loss: 0.00012616
Iteration 20/1000 | Loss: 0.00011331
Iteration 21/1000 | Loss: 0.00010663
Iteration 22/1000 | Loss: 0.00011074
Iteration 23/1000 | Loss: 0.00011055
Iteration 24/1000 | Loss: 0.00010182
Iteration 25/1000 | Loss: 0.00011734
Iteration 26/1000 | Loss: 0.00010333
Iteration 27/1000 | Loss: 0.00011157
Iteration 28/1000 | Loss: 0.00010633
Iteration 29/1000 | Loss: 0.00009331
Iteration 30/1000 | Loss: 0.00140560
Iteration 31/1000 | Loss: 0.00109272
Iteration 32/1000 | Loss: 0.00062723
Iteration 33/1000 | Loss: 0.00015784
Iteration 34/1000 | Loss: 0.00054647
Iteration 35/1000 | Loss: 0.00025370
Iteration 36/1000 | Loss: 0.00013313
Iteration 37/1000 | Loss: 0.00011962
Iteration 38/1000 | Loss: 0.00009390
Iteration 39/1000 | Loss: 0.00008557
Iteration 40/1000 | Loss: 0.00040484
Iteration 41/1000 | Loss: 0.00030278
Iteration 42/1000 | Loss: 0.00032979
Iteration 43/1000 | Loss: 0.00023705
Iteration 44/1000 | Loss: 0.00008488
Iteration 45/1000 | Loss: 0.00038508
Iteration 46/1000 | Loss: 0.00020468
Iteration 47/1000 | Loss: 0.00039808
Iteration 48/1000 | Loss: 0.00015502
Iteration 49/1000 | Loss: 0.00028363
Iteration 50/1000 | Loss: 0.00013801
Iteration 51/1000 | Loss: 0.00008882
Iteration 52/1000 | Loss: 0.00009708
Iteration 53/1000 | Loss: 0.00008924
Iteration 54/1000 | Loss: 0.00092022
Iteration 55/1000 | Loss: 0.00019966
Iteration 56/1000 | Loss: 0.00095232
Iteration 57/1000 | Loss: 0.00069466
Iteration 58/1000 | Loss: 0.00046014
Iteration 59/1000 | Loss: 0.00042321
Iteration 60/1000 | Loss: 0.00022030
Iteration 61/1000 | Loss: 0.00028888
Iteration 62/1000 | Loss: 0.00008842
Iteration 63/1000 | Loss: 0.00007819
Iteration 64/1000 | Loss: 0.00048439
Iteration 65/1000 | Loss: 0.00008899
Iteration 66/1000 | Loss: 0.00007374
Iteration 67/1000 | Loss: 0.00067121
Iteration 68/1000 | Loss: 0.00049470
Iteration 69/1000 | Loss: 0.00009471
Iteration 70/1000 | Loss: 0.00007649
Iteration 71/1000 | Loss: 0.00007124
Iteration 72/1000 | Loss: 0.00006780
Iteration 73/1000 | Loss: 0.00108841
Iteration 74/1000 | Loss: 0.00011018
Iteration 75/1000 | Loss: 0.00006714
Iteration 76/1000 | Loss: 0.00006165
Iteration 77/1000 | Loss: 0.00030379
Iteration 78/1000 | Loss: 0.00203630
Iteration 79/1000 | Loss: 0.00007990
Iteration 80/1000 | Loss: 0.00006031
Iteration 81/1000 | Loss: 0.00005585
Iteration 82/1000 | Loss: 0.00064688
Iteration 83/1000 | Loss: 0.00005652
Iteration 84/1000 | Loss: 0.00005092
Iteration 85/1000 | Loss: 0.00088944
Iteration 86/1000 | Loss: 0.00005833
Iteration 87/1000 | Loss: 0.00005009
Iteration 88/1000 | Loss: 0.00135082
Iteration 89/1000 | Loss: 0.00005247
Iteration 90/1000 | Loss: 0.00072411
Iteration 91/1000 | Loss: 0.00055056
Iteration 92/1000 | Loss: 0.00068299
Iteration 93/1000 | Loss: 0.00034980
Iteration 94/1000 | Loss: 0.00071005
Iteration 95/1000 | Loss: 0.00072195
Iteration 96/1000 | Loss: 0.00035441
Iteration 97/1000 | Loss: 0.00045102
Iteration 98/1000 | Loss: 0.00005032
Iteration 99/1000 | Loss: 0.00053955
Iteration 100/1000 | Loss: 0.00039800
Iteration 101/1000 | Loss: 0.00053355
Iteration 102/1000 | Loss: 0.00079126
Iteration 103/1000 | Loss: 0.00014445
Iteration 104/1000 | Loss: 0.00019698
Iteration 105/1000 | Loss: 0.00005235
Iteration 106/1000 | Loss: 0.00082217
Iteration 107/1000 | Loss: 0.00026397
Iteration 108/1000 | Loss: 0.00070342
Iteration 109/1000 | Loss: 0.00003887
Iteration 110/1000 | Loss: 0.00003143
Iteration 111/1000 | Loss: 0.00063485
Iteration 112/1000 | Loss: 0.00003159
Iteration 113/1000 | Loss: 0.00002662
Iteration 114/1000 | Loss: 0.00002351
Iteration 115/1000 | Loss: 0.00002051
Iteration 116/1000 | Loss: 0.00026173
Iteration 117/1000 | Loss: 0.00002388
Iteration 118/1000 | Loss: 0.00002115
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001794
Iteration 122/1000 | Loss: 0.00001734
Iteration 123/1000 | Loss: 0.00001681
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001601
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001575
Iteration 128/1000 | Loss: 0.00001573
Iteration 129/1000 | Loss: 0.00001572
Iteration 130/1000 | Loss: 0.00001571
Iteration 131/1000 | Loss: 0.00001570
Iteration 132/1000 | Loss: 0.00001570
Iteration 133/1000 | Loss: 0.00001569
Iteration 134/1000 | Loss: 0.00001566
Iteration 135/1000 | Loss: 0.00001565
Iteration 136/1000 | Loss: 0.00001560
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00002133
Iteration 142/1000 | Loss: 0.00001722
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001528
Iteration 146/1000 | Loss: 0.00001515
Iteration 147/1000 | Loss: 0.00001508
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001481
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001467
Iteration 155/1000 | Loss: 0.00001446
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001433
Iteration 162/1000 | Loss: 0.00001432
Iteration 163/1000 | Loss: 0.00001432
Iteration 164/1000 | Loss: 0.00001431
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001429
Iteration 169/1000 | Loss: 0.00001429
Iteration 170/1000 | Loss: 0.00001429
Iteration 171/1000 | Loss: 0.00001429
Iteration 172/1000 | Loss: 0.00001429
Iteration 173/1000 | Loss: 0.00001428
Iteration 174/1000 | Loss: 0.00001428
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001425
Iteration 179/1000 | Loss: 0.00001425
Iteration 180/1000 | Loss: 0.00001425
Iteration 181/1000 | Loss: 0.00001425
Iteration 182/1000 | Loss: 0.00001425
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001425
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001423
Iteration 192/1000 | Loss: 0.00001423
Iteration 193/1000 | Loss: 0.00001423
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001422
Iteration 196/1000 | Loss: 0.00001422
Iteration 197/1000 | Loss: 0.00001422
Iteration 198/1000 | Loss: 0.00001422
Iteration 199/1000 | Loss: 0.00001422
Iteration 200/1000 | Loss: 0.00001422
Iteration 201/1000 | Loss: 0.00001422
Iteration 202/1000 | Loss: 0.00001422
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001421
Iteration 207/1000 | Loss: 0.00001421
Iteration 208/1000 | Loss: 0.00001421
Iteration 209/1000 | Loss: 0.00001421
Iteration 210/1000 | Loss: 0.00001420
Iteration 211/1000 | Loss: 0.00001420
Iteration 212/1000 | Loss: 0.00001420
Iteration 213/1000 | Loss: 0.00001420
Iteration 214/1000 | Loss: 0.00001419
Iteration 215/1000 | Loss: 0.00001419
Iteration 216/1000 | Loss: 0.00001419
Iteration 217/1000 | Loss: 0.00001419
Iteration 218/1000 | Loss: 0.00001419
Iteration 219/1000 | Loss: 0.00001419
Iteration 220/1000 | Loss: 0.00001418
Iteration 221/1000 | Loss: 0.00001418
Iteration 222/1000 | Loss: 0.00001418
Iteration 223/1000 | Loss: 0.00001418
Iteration 224/1000 | Loss: 0.00001417
Iteration 225/1000 | Loss: 0.00001417
Iteration 226/1000 | Loss: 0.00001417
Iteration 227/1000 | Loss: 0.00001416
Iteration 228/1000 | Loss: 0.00001416
Iteration 229/1000 | Loss: 0.00001416
Iteration 230/1000 | Loss: 0.00001416
Iteration 231/1000 | Loss: 0.00001415
Iteration 232/1000 | Loss: 0.00001415
Iteration 233/1000 | Loss: 0.00001415
Iteration 234/1000 | Loss: 0.00001415
Iteration 235/1000 | Loss: 0.00001415
Iteration 236/1000 | Loss: 0.00001415
Iteration 237/1000 | Loss: 0.00001414
Iteration 238/1000 | Loss: 0.00001414
Iteration 239/1000 | Loss: 0.00001414
Iteration 240/1000 | Loss: 0.00001414
Iteration 241/1000 | Loss: 0.00001414
Iteration 242/1000 | Loss: 0.00001414
Iteration 243/1000 | Loss: 0.00001414
Iteration 244/1000 | Loss: 0.00001414
Iteration 245/1000 | Loss: 0.00001414
Iteration 246/1000 | Loss: 0.00001414
Iteration 247/1000 | Loss: 0.00001413
Iteration 248/1000 | Loss: 0.00001413
Iteration 249/1000 | Loss: 0.00001413
Iteration 250/1000 | Loss: 0.00001413
Iteration 251/1000 | Loss: 0.00001413
Iteration 252/1000 | Loss: 0.00001413
Iteration 253/1000 | Loss: 0.00001413
Iteration 254/1000 | Loss: 0.00001412
Iteration 255/1000 | Loss: 0.00001412
Iteration 256/1000 | Loss: 0.00001412
Iteration 257/1000 | Loss: 0.00001412
Iteration 258/1000 | Loss: 0.00001412
Iteration 259/1000 | Loss: 0.00001412
Iteration 260/1000 | Loss: 0.00001412
Iteration 261/1000 | Loss: 0.00001412
Iteration 262/1000 | Loss: 0.00001412
Iteration 263/1000 | Loss: 0.00001412
Iteration 264/1000 | Loss: 0.00001412
Iteration 265/1000 | Loss: 0.00001412
Iteration 266/1000 | Loss: 0.00001412
Iteration 267/1000 | Loss: 0.00001412
Iteration 268/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.4121987987891771e-05, 1.4121987987891771e-05, 1.4121987987891771e-05, 1.4121987987891771e-05, 1.4121987987891771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4121987987891771e-05

Optimization complete. Final v2v error: 3.105131149291992 mm

Highest mean error: 4.423696517944336 mm for frame 99

Lowest mean error: 2.5008351802825928 mm for frame 3

Saving results

Total time: 236.31022334098816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396168
Iteration 2/25 | Loss: 0.00121322
Iteration 3/25 | Loss: 0.00110290
Iteration 4/25 | Loss: 0.00109525
Iteration 5/25 | Loss: 0.00109200
Iteration 6/25 | Loss: 0.00109200
Iteration 7/25 | Loss: 0.00109200
Iteration 8/25 | Loss: 0.00109200
Iteration 9/25 | Loss: 0.00109200
Iteration 10/25 | Loss: 0.00109200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001092001679353416, 0.001092001679353416, 0.001092001679353416, 0.001092001679353416, 0.001092001679353416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001092001679353416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55878556
Iteration 2/25 | Loss: 0.00071629
Iteration 3/25 | Loss: 0.00071628
Iteration 4/25 | Loss: 0.00071628
Iteration 5/25 | Loss: 0.00071628
Iteration 6/25 | Loss: 0.00071628
Iteration 7/25 | Loss: 0.00071628
Iteration 8/25 | Loss: 0.00071628
Iteration 9/25 | Loss: 0.00071628
Iteration 10/25 | Loss: 0.00071628
Iteration 11/25 | Loss: 0.00071628
Iteration 12/25 | Loss: 0.00071628
Iteration 13/25 | Loss: 0.00071628
Iteration 14/25 | Loss: 0.00071628
Iteration 15/25 | Loss: 0.00071628
Iteration 16/25 | Loss: 0.00071628
Iteration 17/25 | Loss: 0.00071628
Iteration 18/25 | Loss: 0.00071628
Iteration 19/25 | Loss: 0.00071628
Iteration 20/25 | Loss: 0.00071628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007162819965742528, 0.0007162819965742528, 0.0007162819965742528, 0.0007162819965742528, 0.0007162819965742528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007162819965742528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071628
Iteration 2/1000 | Loss: 0.00002220
Iteration 3/1000 | Loss: 0.00001403
Iteration 4/1000 | Loss: 0.00001232
Iteration 5/1000 | Loss: 0.00001150
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001066
Iteration 8/1000 | Loss: 0.00001058
Iteration 9/1000 | Loss: 0.00001045
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00001001
Iteration 12/1000 | Loss: 0.00000990
Iteration 13/1000 | Loss: 0.00000989
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000983
Iteration 16/1000 | Loss: 0.00000979
Iteration 17/1000 | Loss: 0.00000978
Iteration 18/1000 | Loss: 0.00000965
Iteration 19/1000 | Loss: 0.00000962
Iteration 20/1000 | Loss: 0.00000961
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000952
Iteration 23/1000 | Loss: 0.00000951
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000951
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000950
Iteration 29/1000 | Loss: 0.00000949
Iteration 30/1000 | Loss: 0.00000949
Iteration 31/1000 | Loss: 0.00000948
Iteration 32/1000 | Loss: 0.00000948
Iteration 33/1000 | Loss: 0.00000947
Iteration 34/1000 | Loss: 0.00000947
Iteration 35/1000 | Loss: 0.00000946
Iteration 36/1000 | Loss: 0.00000946
Iteration 37/1000 | Loss: 0.00000946
Iteration 38/1000 | Loss: 0.00000946
Iteration 39/1000 | Loss: 0.00000946
Iteration 40/1000 | Loss: 0.00000946
Iteration 41/1000 | Loss: 0.00000945
Iteration 42/1000 | Loss: 0.00000945
Iteration 43/1000 | Loss: 0.00000945
Iteration 44/1000 | Loss: 0.00000945
Iteration 45/1000 | Loss: 0.00000944
Iteration 46/1000 | Loss: 0.00000944
Iteration 47/1000 | Loss: 0.00000944
Iteration 48/1000 | Loss: 0.00000943
Iteration 49/1000 | Loss: 0.00000943
Iteration 50/1000 | Loss: 0.00000943
Iteration 51/1000 | Loss: 0.00000943
Iteration 52/1000 | Loss: 0.00000943
Iteration 53/1000 | Loss: 0.00000943
Iteration 54/1000 | Loss: 0.00000942
Iteration 55/1000 | Loss: 0.00000942
Iteration 56/1000 | Loss: 0.00000941
Iteration 57/1000 | Loss: 0.00000941
Iteration 58/1000 | Loss: 0.00000940
Iteration 59/1000 | Loss: 0.00000940
Iteration 60/1000 | Loss: 0.00000940
Iteration 61/1000 | Loss: 0.00000940
Iteration 62/1000 | Loss: 0.00000940
Iteration 63/1000 | Loss: 0.00000939
Iteration 64/1000 | Loss: 0.00000939
Iteration 65/1000 | Loss: 0.00000939
Iteration 66/1000 | Loss: 0.00000938
Iteration 67/1000 | Loss: 0.00000937
Iteration 68/1000 | Loss: 0.00000937
Iteration 69/1000 | Loss: 0.00000936
Iteration 70/1000 | Loss: 0.00000936
Iteration 71/1000 | Loss: 0.00000936
Iteration 72/1000 | Loss: 0.00000936
Iteration 73/1000 | Loss: 0.00000936
Iteration 74/1000 | Loss: 0.00000936
Iteration 75/1000 | Loss: 0.00000936
Iteration 76/1000 | Loss: 0.00000936
Iteration 77/1000 | Loss: 0.00000936
Iteration 78/1000 | Loss: 0.00000936
Iteration 79/1000 | Loss: 0.00000936
Iteration 80/1000 | Loss: 0.00000936
Iteration 81/1000 | Loss: 0.00000936
Iteration 82/1000 | Loss: 0.00000936
Iteration 83/1000 | Loss: 0.00000936
Iteration 84/1000 | Loss: 0.00000936
Iteration 85/1000 | Loss: 0.00000936
Iteration 86/1000 | Loss: 0.00000935
Iteration 87/1000 | Loss: 0.00000935
Iteration 88/1000 | Loss: 0.00000935
Iteration 89/1000 | Loss: 0.00000935
Iteration 90/1000 | Loss: 0.00000935
Iteration 91/1000 | Loss: 0.00000934
Iteration 92/1000 | Loss: 0.00000934
Iteration 93/1000 | Loss: 0.00000934
Iteration 94/1000 | Loss: 0.00000934
Iteration 95/1000 | Loss: 0.00000934
Iteration 96/1000 | Loss: 0.00000934
Iteration 97/1000 | Loss: 0.00000934
Iteration 98/1000 | Loss: 0.00000934
Iteration 99/1000 | Loss: 0.00000934
Iteration 100/1000 | Loss: 0.00000934
Iteration 101/1000 | Loss: 0.00000934
Iteration 102/1000 | Loss: 0.00000934
Iteration 103/1000 | Loss: 0.00000933
Iteration 104/1000 | Loss: 0.00000933
Iteration 105/1000 | Loss: 0.00000933
Iteration 106/1000 | Loss: 0.00000933
Iteration 107/1000 | Loss: 0.00000933
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000933
Iteration 111/1000 | Loss: 0.00000932
Iteration 112/1000 | Loss: 0.00000932
Iteration 113/1000 | Loss: 0.00000932
Iteration 114/1000 | Loss: 0.00000932
Iteration 115/1000 | Loss: 0.00000932
Iteration 116/1000 | Loss: 0.00000932
Iteration 117/1000 | Loss: 0.00000932
Iteration 118/1000 | Loss: 0.00000932
Iteration 119/1000 | Loss: 0.00000932
Iteration 120/1000 | Loss: 0.00000932
Iteration 121/1000 | Loss: 0.00000932
Iteration 122/1000 | Loss: 0.00000932
Iteration 123/1000 | Loss: 0.00000932
Iteration 124/1000 | Loss: 0.00000932
Iteration 125/1000 | Loss: 0.00000932
Iteration 126/1000 | Loss: 0.00000932
Iteration 127/1000 | Loss: 0.00000932
Iteration 128/1000 | Loss: 0.00000932
Iteration 129/1000 | Loss: 0.00000932
Iteration 130/1000 | Loss: 0.00000932
Iteration 131/1000 | Loss: 0.00000932
Iteration 132/1000 | Loss: 0.00000932
Iteration 133/1000 | Loss: 0.00000932
Iteration 134/1000 | Loss: 0.00000932
Iteration 135/1000 | Loss: 0.00000932
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000932
Iteration 139/1000 | Loss: 0.00000932
Iteration 140/1000 | Loss: 0.00000932
Iteration 141/1000 | Loss: 0.00000932
Iteration 142/1000 | Loss: 0.00000932
Iteration 143/1000 | Loss: 0.00000932
Iteration 144/1000 | Loss: 0.00000932
Iteration 145/1000 | Loss: 0.00000932
Iteration 146/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [9.31885551835876e-06, 9.31885551835876e-06, 9.31885551835876e-06, 9.31885551835876e-06, 9.31885551835876e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.31885551835876e-06

Optimization complete. Final v2v error: 2.6399457454681396 mm

Highest mean error: 2.8685050010681152 mm for frame 170

Lowest mean error: 2.4944634437561035 mm for frame 82

Saving results

Total time: 37.438379526138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802118
Iteration 2/25 | Loss: 0.00117957
Iteration 3/25 | Loss: 0.00110122
Iteration 4/25 | Loss: 0.00108860
Iteration 5/25 | Loss: 0.00108494
Iteration 6/25 | Loss: 0.00108482
Iteration 7/25 | Loss: 0.00108482
Iteration 8/25 | Loss: 0.00108482
Iteration 9/25 | Loss: 0.00108482
Iteration 10/25 | Loss: 0.00108482
Iteration 11/25 | Loss: 0.00108482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010848173405975103, 0.0010848173405975103, 0.0010848173405975103, 0.0010848173405975103, 0.0010848173405975103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010848173405975103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.46915531
Iteration 2/25 | Loss: 0.00086138
Iteration 3/25 | Loss: 0.00086135
Iteration 4/25 | Loss: 0.00086135
Iteration 5/25 | Loss: 0.00086135
Iteration 6/25 | Loss: 0.00086135
Iteration 7/25 | Loss: 0.00086135
Iteration 8/25 | Loss: 0.00086135
Iteration 9/25 | Loss: 0.00086135
Iteration 10/25 | Loss: 0.00086135
Iteration 11/25 | Loss: 0.00086135
Iteration 12/25 | Loss: 0.00086135
Iteration 13/25 | Loss: 0.00086135
Iteration 14/25 | Loss: 0.00086135
Iteration 15/25 | Loss: 0.00086135
Iteration 16/25 | Loss: 0.00086135
Iteration 17/25 | Loss: 0.00086135
Iteration 18/25 | Loss: 0.00086135
Iteration 19/25 | Loss: 0.00086135
Iteration 20/25 | Loss: 0.00086135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008613493992015719, 0.0008613493992015719, 0.0008613493992015719, 0.0008613493992015719, 0.0008613493992015719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008613493992015719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086135
Iteration 2/1000 | Loss: 0.00002827
Iteration 3/1000 | Loss: 0.00002070
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001647
Iteration 6/1000 | Loss: 0.00001545
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001369
Iteration 11/1000 | Loss: 0.00001351
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001321
Iteration 21/1000 | Loss: 0.00001320
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001310
Iteration 37/1000 | Loss: 0.00001310
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001309
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001293
Iteration 79/1000 | Loss: 0.00001293
Iteration 80/1000 | Loss: 0.00001293
Iteration 81/1000 | Loss: 0.00001293
Iteration 82/1000 | Loss: 0.00001292
Iteration 83/1000 | Loss: 0.00001292
Iteration 84/1000 | Loss: 0.00001292
Iteration 85/1000 | Loss: 0.00001291
Iteration 86/1000 | Loss: 0.00001291
Iteration 87/1000 | Loss: 0.00001291
Iteration 88/1000 | Loss: 0.00001291
Iteration 89/1000 | Loss: 0.00001291
Iteration 90/1000 | Loss: 0.00001291
Iteration 91/1000 | Loss: 0.00001290
Iteration 92/1000 | Loss: 0.00001290
Iteration 93/1000 | Loss: 0.00001290
Iteration 94/1000 | Loss: 0.00001289
Iteration 95/1000 | Loss: 0.00001289
Iteration 96/1000 | Loss: 0.00001289
Iteration 97/1000 | Loss: 0.00001289
Iteration 98/1000 | Loss: 0.00001289
Iteration 99/1000 | Loss: 0.00001289
Iteration 100/1000 | Loss: 0.00001288
Iteration 101/1000 | Loss: 0.00001288
Iteration 102/1000 | Loss: 0.00001288
Iteration 103/1000 | Loss: 0.00001288
Iteration 104/1000 | Loss: 0.00001288
Iteration 105/1000 | Loss: 0.00001288
Iteration 106/1000 | Loss: 0.00001288
Iteration 107/1000 | Loss: 0.00001288
Iteration 108/1000 | Loss: 0.00001288
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001287
Iteration 116/1000 | Loss: 0.00001286
Iteration 117/1000 | Loss: 0.00001286
Iteration 118/1000 | Loss: 0.00001286
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001284
Iteration 122/1000 | Loss: 0.00001284
Iteration 123/1000 | Loss: 0.00001284
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001281
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001281
Iteration 141/1000 | Loss: 0.00001281
Iteration 142/1000 | Loss: 0.00001281
Iteration 143/1000 | Loss: 0.00001281
Iteration 144/1000 | Loss: 0.00001281
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001281
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001281
Iteration 150/1000 | Loss: 0.00001281
Iteration 151/1000 | Loss: 0.00001281
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001280
Iteration 154/1000 | Loss: 0.00001280
Iteration 155/1000 | Loss: 0.00001280
Iteration 156/1000 | Loss: 0.00001280
Iteration 157/1000 | Loss: 0.00001280
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001279
Iteration 163/1000 | Loss: 0.00001279
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001279
Iteration 167/1000 | Loss: 0.00001279
Iteration 168/1000 | Loss: 0.00001279
Iteration 169/1000 | Loss: 0.00001279
Iteration 170/1000 | Loss: 0.00001279
Iteration 171/1000 | Loss: 0.00001279
Iteration 172/1000 | Loss: 0.00001279
Iteration 173/1000 | Loss: 0.00001279
Iteration 174/1000 | Loss: 0.00001279
Iteration 175/1000 | Loss: 0.00001279
Iteration 176/1000 | Loss: 0.00001279
Iteration 177/1000 | Loss: 0.00001279
Iteration 178/1000 | Loss: 0.00001279
Iteration 179/1000 | Loss: 0.00001279
Iteration 180/1000 | Loss: 0.00001279
Iteration 181/1000 | Loss: 0.00001279
Iteration 182/1000 | Loss: 0.00001279
Iteration 183/1000 | Loss: 0.00001279
Iteration 184/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.2788573258148972e-05, 1.2788573258148972e-05, 1.2788573258148972e-05, 1.2788573258148972e-05, 1.2788573258148972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2788573258148972e-05

Optimization complete. Final v2v error: 3.0177955627441406 mm

Highest mean error: 3.7672016620635986 mm for frame 146

Lowest mean error: 2.6788127422332764 mm for frame 17

Saving results

Total time: 38.38141655921936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595921
Iteration 2/25 | Loss: 0.00115607
Iteration 3/25 | Loss: 0.00106836
Iteration 4/25 | Loss: 0.00105806
Iteration 5/25 | Loss: 0.00105441
Iteration 6/25 | Loss: 0.00105377
Iteration 7/25 | Loss: 0.00105377
Iteration 8/25 | Loss: 0.00105377
Iteration 9/25 | Loss: 0.00105377
Iteration 10/25 | Loss: 0.00105377
Iteration 11/25 | Loss: 0.00105377
Iteration 12/25 | Loss: 0.00105377
Iteration 13/25 | Loss: 0.00105377
Iteration 14/25 | Loss: 0.00105377
Iteration 15/25 | Loss: 0.00105377
Iteration 16/25 | Loss: 0.00105377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010537682101130486, 0.0010537682101130486, 0.0010537682101130486, 0.0010537682101130486, 0.0010537682101130486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010537682101130486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90855217
Iteration 2/25 | Loss: 0.00082811
Iteration 3/25 | Loss: 0.00082811
Iteration 4/25 | Loss: 0.00082811
Iteration 5/25 | Loss: 0.00082811
Iteration 6/25 | Loss: 0.00082811
Iteration 7/25 | Loss: 0.00082811
Iteration 8/25 | Loss: 0.00082811
Iteration 9/25 | Loss: 0.00082811
Iteration 10/25 | Loss: 0.00082811
Iteration 11/25 | Loss: 0.00082811
Iteration 12/25 | Loss: 0.00082811
Iteration 13/25 | Loss: 0.00082811
Iteration 14/25 | Loss: 0.00082811
Iteration 15/25 | Loss: 0.00082811
Iteration 16/25 | Loss: 0.00082811
Iteration 17/25 | Loss: 0.00082811
Iteration 18/25 | Loss: 0.00082811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008281081099994481, 0.0008281081099994481, 0.0008281081099994481, 0.0008281081099994481, 0.0008281081099994481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008281081099994481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082811
Iteration 2/1000 | Loss: 0.00002017
Iteration 3/1000 | Loss: 0.00001299
Iteration 4/1000 | Loss: 0.00001182
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001023
Iteration 9/1000 | Loss: 0.00000999
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000989
Iteration 13/1000 | Loss: 0.00000981
Iteration 14/1000 | Loss: 0.00000976
Iteration 15/1000 | Loss: 0.00000974
Iteration 16/1000 | Loss: 0.00000974
Iteration 17/1000 | Loss: 0.00000969
Iteration 18/1000 | Loss: 0.00000968
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000959
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000952
Iteration 24/1000 | Loss: 0.00000952
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000948
Iteration 29/1000 | Loss: 0.00000946
Iteration 30/1000 | Loss: 0.00000946
Iteration 31/1000 | Loss: 0.00000943
Iteration 32/1000 | Loss: 0.00000942
Iteration 33/1000 | Loss: 0.00000941
Iteration 34/1000 | Loss: 0.00000940
Iteration 35/1000 | Loss: 0.00000940
Iteration 36/1000 | Loss: 0.00000939
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000936
Iteration 45/1000 | Loss: 0.00000936
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000935
Iteration 48/1000 | Loss: 0.00000935
Iteration 49/1000 | Loss: 0.00000934
Iteration 50/1000 | Loss: 0.00000934
Iteration 51/1000 | Loss: 0.00000934
Iteration 52/1000 | Loss: 0.00000933
Iteration 53/1000 | Loss: 0.00000933
Iteration 54/1000 | Loss: 0.00000933
Iteration 55/1000 | Loss: 0.00000932
Iteration 56/1000 | Loss: 0.00000931
Iteration 57/1000 | Loss: 0.00000931
Iteration 58/1000 | Loss: 0.00000931
Iteration 59/1000 | Loss: 0.00000930
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000930
Iteration 63/1000 | Loss: 0.00000930
Iteration 64/1000 | Loss: 0.00000930
Iteration 65/1000 | Loss: 0.00000930
Iteration 66/1000 | Loss: 0.00000930
Iteration 67/1000 | Loss: 0.00000930
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000928
Iteration 70/1000 | Loss: 0.00000927
Iteration 71/1000 | Loss: 0.00000927
Iteration 72/1000 | Loss: 0.00000926
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000926
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000920
Iteration 88/1000 | Loss: 0.00000920
Iteration 89/1000 | Loss: 0.00000919
Iteration 90/1000 | Loss: 0.00000919
Iteration 91/1000 | Loss: 0.00000919
Iteration 92/1000 | Loss: 0.00000919
Iteration 93/1000 | Loss: 0.00000919
Iteration 94/1000 | Loss: 0.00000919
Iteration 95/1000 | Loss: 0.00000919
Iteration 96/1000 | Loss: 0.00000919
Iteration 97/1000 | Loss: 0.00000919
Iteration 98/1000 | Loss: 0.00000919
Iteration 99/1000 | Loss: 0.00000918
Iteration 100/1000 | Loss: 0.00000918
Iteration 101/1000 | Loss: 0.00000918
Iteration 102/1000 | Loss: 0.00000918
Iteration 103/1000 | Loss: 0.00000918
Iteration 104/1000 | Loss: 0.00000918
Iteration 105/1000 | Loss: 0.00000917
Iteration 106/1000 | Loss: 0.00000917
Iteration 107/1000 | Loss: 0.00000917
Iteration 108/1000 | Loss: 0.00000916
Iteration 109/1000 | Loss: 0.00000916
Iteration 110/1000 | Loss: 0.00000916
Iteration 111/1000 | Loss: 0.00000916
Iteration 112/1000 | Loss: 0.00000916
Iteration 113/1000 | Loss: 0.00000916
Iteration 114/1000 | Loss: 0.00000916
Iteration 115/1000 | Loss: 0.00000916
Iteration 116/1000 | Loss: 0.00000916
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000915
Iteration 119/1000 | Loss: 0.00000915
Iteration 120/1000 | Loss: 0.00000915
Iteration 121/1000 | Loss: 0.00000914
Iteration 122/1000 | Loss: 0.00000914
Iteration 123/1000 | Loss: 0.00000913
Iteration 124/1000 | Loss: 0.00000913
Iteration 125/1000 | Loss: 0.00000912
Iteration 126/1000 | Loss: 0.00000912
Iteration 127/1000 | Loss: 0.00000912
Iteration 128/1000 | Loss: 0.00000912
Iteration 129/1000 | Loss: 0.00000912
Iteration 130/1000 | Loss: 0.00000912
Iteration 131/1000 | Loss: 0.00000912
Iteration 132/1000 | Loss: 0.00000912
Iteration 133/1000 | Loss: 0.00000911
Iteration 134/1000 | Loss: 0.00000911
Iteration 135/1000 | Loss: 0.00000911
Iteration 136/1000 | Loss: 0.00000911
Iteration 137/1000 | Loss: 0.00000911
Iteration 138/1000 | Loss: 0.00000911
Iteration 139/1000 | Loss: 0.00000911
Iteration 140/1000 | Loss: 0.00000911
Iteration 141/1000 | Loss: 0.00000911
Iteration 142/1000 | Loss: 0.00000911
Iteration 143/1000 | Loss: 0.00000911
Iteration 144/1000 | Loss: 0.00000910
Iteration 145/1000 | Loss: 0.00000910
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000910
Iteration 154/1000 | Loss: 0.00000910
Iteration 155/1000 | Loss: 0.00000910
Iteration 156/1000 | Loss: 0.00000910
Iteration 157/1000 | Loss: 0.00000910
Iteration 158/1000 | Loss: 0.00000910
Iteration 159/1000 | Loss: 0.00000910
Iteration 160/1000 | Loss: 0.00000910
Iteration 161/1000 | Loss: 0.00000910
Iteration 162/1000 | Loss: 0.00000910
Iteration 163/1000 | Loss: 0.00000910
Iteration 164/1000 | Loss: 0.00000910
Iteration 165/1000 | Loss: 0.00000910
Iteration 166/1000 | Loss: 0.00000909
Iteration 167/1000 | Loss: 0.00000909
Iteration 168/1000 | Loss: 0.00000909
Iteration 169/1000 | Loss: 0.00000909
Iteration 170/1000 | Loss: 0.00000909
Iteration 171/1000 | Loss: 0.00000909
Iteration 172/1000 | Loss: 0.00000909
Iteration 173/1000 | Loss: 0.00000909
Iteration 174/1000 | Loss: 0.00000909
Iteration 175/1000 | Loss: 0.00000909
Iteration 176/1000 | Loss: 0.00000909
Iteration 177/1000 | Loss: 0.00000909
Iteration 178/1000 | Loss: 0.00000909
Iteration 179/1000 | Loss: 0.00000909
Iteration 180/1000 | Loss: 0.00000908
Iteration 181/1000 | Loss: 0.00000908
Iteration 182/1000 | Loss: 0.00000908
Iteration 183/1000 | Loss: 0.00000908
Iteration 184/1000 | Loss: 0.00000908
Iteration 185/1000 | Loss: 0.00000908
Iteration 186/1000 | Loss: 0.00000908
Iteration 187/1000 | Loss: 0.00000908
Iteration 188/1000 | Loss: 0.00000908
Iteration 189/1000 | Loss: 0.00000908
Iteration 190/1000 | Loss: 0.00000908
Iteration 191/1000 | Loss: 0.00000908
Iteration 192/1000 | Loss: 0.00000908
Iteration 193/1000 | Loss: 0.00000908
Iteration 194/1000 | Loss: 0.00000908
Iteration 195/1000 | Loss: 0.00000908
Iteration 196/1000 | Loss: 0.00000908
Iteration 197/1000 | Loss: 0.00000908
Iteration 198/1000 | Loss: 0.00000908
Iteration 199/1000 | Loss: 0.00000908
Iteration 200/1000 | Loss: 0.00000908
Iteration 201/1000 | Loss: 0.00000908
Iteration 202/1000 | Loss: 0.00000908
Iteration 203/1000 | Loss: 0.00000908
Iteration 204/1000 | Loss: 0.00000908
Iteration 205/1000 | Loss: 0.00000908
Iteration 206/1000 | Loss: 0.00000908
Iteration 207/1000 | Loss: 0.00000908
Iteration 208/1000 | Loss: 0.00000908
Iteration 209/1000 | Loss: 0.00000908
Iteration 210/1000 | Loss: 0.00000908
Iteration 211/1000 | Loss: 0.00000908
Iteration 212/1000 | Loss: 0.00000908
Iteration 213/1000 | Loss: 0.00000908
Iteration 214/1000 | Loss: 0.00000908
Iteration 215/1000 | Loss: 0.00000908
Iteration 216/1000 | Loss: 0.00000908
Iteration 217/1000 | Loss: 0.00000908
Iteration 218/1000 | Loss: 0.00000908
Iteration 219/1000 | Loss: 0.00000908
Iteration 220/1000 | Loss: 0.00000908
Iteration 221/1000 | Loss: 0.00000908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [9.078256880457047e-06, 9.078256880457047e-06, 9.078256880457047e-06, 9.078256880457047e-06, 9.078256880457047e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.078256880457047e-06

Optimization complete. Final v2v error: 2.61600661277771 mm

Highest mean error: 2.8659231662750244 mm for frame 113

Lowest mean error: 2.4181952476501465 mm for frame 0

Saving results

Total time: 38.73281955718994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436174
Iteration 2/25 | Loss: 0.00117343
Iteration 3/25 | Loss: 0.00110910
Iteration 4/25 | Loss: 0.00109457
Iteration 5/25 | Loss: 0.00109005
Iteration 6/25 | Loss: 0.00108928
Iteration 7/25 | Loss: 0.00108928
Iteration 8/25 | Loss: 0.00108928
Iteration 9/25 | Loss: 0.00108928
Iteration 10/25 | Loss: 0.00108928
Iteration 11/25 | Loss: 0.00108928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010892769787460566, 0.0010892769787460566, 0.0010892769787460566, 0.0010892769787460566, 0.0010892769787460566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010892769787460566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51142359
Iteration 2/25 | Loss: 0.00082231
Iteration 3/25 | Loss: 0.00082231
Iteration 4/25 | Loss: 0.00082231
Iteration 5/25 | Loss: 0.00082231
Iteration 6/25 | Loss: 0.00082230
Iteration 7/25 | Loss: 0.00082230
Iteration 8/25 | Loss: 0.00082230
Iteration 9/25 | Loss: 0.00082230
Iteration 10/25 | Loss: 0.00082230
Iteration 11/25 | Loss: 0.00082230
Iteration 12/25 | Loss: 0.00082230
Iteration 13/25 | Loss: 0.00082230
Iteration 14/25 | Loss: 0.00082230
Iteration 15/25 | Loss: 0.00082230
Iteration 16/25 | Loss: 0.00082230
Iteration 17/25 | Loss: 0.00082230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008223028271459043, 0.0008223028271459043, 0.0008223028271459043, 0.0008223028271459043, 0.0008223028271459043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008223028271459043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082230
Iteration 2/1000 | Loss: 0.00002476
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001314
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001212
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001205
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001187
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001184
Iteration 105/1000 | Loss: 0.00001184
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001183
Iteration 110/1000 | Loss: 0.00001183
Iteration 111/1000 | Loss: 0.00001183
Iteration 112/1000 | Loss: 0.00001183
Iteration 113/1000 | Loss: 0.00001183
Iteration 114/1000 | Loss: 0.00001183
Iteration 115/1000 | Loss: 0.00001183
Iteration 116/1000 | Loss: 0.00001183
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001182
Iteration 121/1000 | Loss: 0.00001182
Iteration 122/1000 | Loss: 0.00001182
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001181
Iteration 126/1000 | Loss: 0.00001181
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001180
Iteration 135/1000 | Loss: 0.00001180
Iteration 136/1000 | Loss: 0.00001180
Iteration 137/1000 | Loss: 0.00001179
Iteration 138/1000 | Loss: 0.00001179
Iteration 139/1000 | Loss: 0.00001179
Iteration 140/1000 | Loss: 0.00001179
Iteration 141/1000 | Loss: 0.00001179
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001178
Iteration 144/1000 | Loss: 0.00001178
Iteration 145/1000 | Loss: 0.00001178
Iteration 146/1000 | Loss: 0.00001178
Iteration 147/1000 | Loss: 0.00001178
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001178
Iteration 152/1000 | Loss: 0.00001178
Iteration 153/1000 | Loss: 0.00001178
Iteration 154/1000 | Loss: 0.00001178
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001178
Iteration 157/1000 | Loss: 0.00001178
Iteration 158/1000 | Loss: 0.00001178
Iteration 159/1000 | Loss: 0.00001178
Iteration 160/1000 | Loss: 0.00001178
Iteration 161/1000 | Loss: 0.00001178
Iteration 162/1000 | Loss: 0.00001178
Iteration 163/1000 | Loss: 0.00001178
Iteration 164/1000 | Loss: 0.00001178
Iteration 165/1000 | Loss: 0.00001178
Iteration 166/1000 | Loss: 0.00001178
Iteration 167/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1783523405028973e-05, 1.1783523405028973e-05, 1.1783523405028973e-05, 1.1783523405028973e-05, 1.1783523405028973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1783523405028973e-05

Optimization complete. Final v2v error: 2.9455318450927734 mm

Highest mean error: 3.3442041873931885 mm for frame 112

Lowest mean error: 2.845827579498291 mm for frame 42

Saving results

Total time: 37.53462028503418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490756
Iteration 2/25 | Loss: 0.00117146
Iteration 3/25 | Loss: 0.00109311
Iteration 4/25 | Loss: 0.00108281
Iteration 5/25 | Loss: 0.00107924
Iteration 6/25 | Loss: 0.00107920
Iteration 7/25 | Loss: 0.00107920
Iteration 8/25 | Loss: 0.00107920
Iteration 9/25 | Loss: 0.00107920
Iteration 10/25 | Loss: 0.00107920
Iteration 11/25 | Loss: 0.00107920
Iteration 12/25 | Loss: 0.00107920
Iteration 13/25 | Loss: 0.00107920
Iteration 14/25 | Loss: 0.00107920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010792042594403028, 0.0010792042594403028, 0.0010792042594403028, 0.0010792042594403028, 0.0010792042594403028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010792042594403028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.23478699
Iteration 2/25 | Loss: 0.00084167
Iteration 3/25 | Loss: 0.00084165
Iteration 4/25 | Loss: 0.00084165
Iteration 5/25 | Loss: 0.00084165
Iteration 6/25 | Loss: 0.00084165
Iteration 7/25 | Loss: 0.00084165
Iteration 8/25 | Loss: 0.00084165
Iteration 9/25 | Loss: 0.00084165
Iteration 10/25 | Loss: 0.00084164
Iteration 11/25 | Loss: 0.00084164
Iteration 12/25 | Loss: 0.00084164
Iteration 13/25 | Loss: 0.00084164
Iteration 14/25 | Loss: 0.00084164
Iteration 15/25 | Loss: 0.00084164
Iteration 16/25 | Loss: 0.00084164
Iteration 17/25 | Loss: 0.00084164
Iteration 18/25 | Loss: 0.00084164
Iteration 19/25 | Loss: 0.00084164
Iteration 20/25 | Loss: 0.00084164
Iteration 21/25 | Loss: 0.00084164
Iteration 22/25 | Loss: 0.00084164
Iteration 23/25 | Loss: 0.00084164
Iteration 24/25 | Loss: 0.00084164
Iteration 25/25 | Loss: 0.00084164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084164
Iteration 2/1000 | Loss: 0.00001709
Iteration 3/1000 | Loss: 0.00001378
Iteration 4/1000 | Loss: 0.00001293
Iteration 5/1000 | Loss: 0.00001207
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001166
Iteration 8/1000 | Loss: 0.00001138
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001082
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001081
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001080
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001057
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001046
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001045
Iteration 26/1000 | Loss: 0.00001045
Iteration 27/1000 | Loss: 0.00001044
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001036
Iteration 31/1000 | Loss: 0.00001036
Iteration 32/1000 | Loss: 0.00001036
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001027
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001019
Iteration 40/1000 | Loss: 0.00001019
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00001018
Iteration 43/1000 | Loss: 0.00001017
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001014
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001014
Iteration 49/1000 | Loss: 0.00001013
Iteration 50/1000 | Loss: 0.00001013
Iteration 51/1000 | Loss: 0.00001012
Iteration 52/1000 | Loss: 0.00001011
Iteration 53/1000 | Loss: 0.00001010
Iteration 54/1000 | Loss: 0.00001010
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00000998
Iteration 69/1000 | Loss: 0.00000996
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000993
Iteration 72/1000 | Loss: 0.00000993
Iteration 73/1000 | Loss: 0.00000992
Iteration 74/1000 | Loss: 0.00000991
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000986
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000985
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000976
Iteration 105/1000 | Loss: 0.00000976
Iteration 106/1000 | Loss: 0.00000976
Iteration 107/1000 | Loss: 0.00000976
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000976
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000975
Iteration 116/1000 | Loss: 0.00000975
Iteration 117/1000 | Loss: 0.00000975
Iteration 118/1000 | Loss: 0.00000975
Iteration 119/1000 | Loss: 0.00000975
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Iteration 122/1000 | Loss: 0.00000975
Iteration 123/1000 | Loss: 0.00000975
Iteration 124/1000 | Loss: 0.00000975
Iteration 125/1000 | Loss: 0.00000975
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000975
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000974
Iteration 133/1000 | Loss: 0.00000974
Iteration 134/1000 | Loss: 0.00000974
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000973
Iteration 141/1000 | Loss: 0.00000973
Iteration 142/1000 | Loss: 0.00000973
Iteration 143/1000 | Loss: 0.00000973
Iteration 144/1000 | Loss: 0.00000973
Iteration 145/1000 | Loss: 0.00000973
Iteration 146/1000 | Loss: 0.00000973
Iteration 147/1000 | Loss: 0.00000973
Iteration 148/1000 | Loss: 0.00000973
Iteration 149/1000 | Loss: 0.00000973
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [9.729281373438425e-06, 9.729281373438425e-06, 9.729281373438425e-06, 9.729281373438425e-06, 9.729281373438425e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.729281373438425e-06

Optimization complete. Final v2v error: 2.719595193862915 mm

Highest mean error: 3.0713953971862793 mm for frame 215

Lowest mean error: 2.516996383666992 mm for frame 245

Saving results

Total time: 44.01161003112793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592826
Iteration 2/25 | Loss: 0.00148777
Iteration 3/25 | Loss: 0.00121216
Iteration 4/25 | Loss: 0.00118179
Iteration 5/25 | Loss: 0.00117815
Iteration 6/25 | Loss: 0.00117707
Iteration 7/25 | Loss: 0.00117690
Iteration 8/25 | Loss: 0.00117678
Iteration 9/25 | Loss: 0.00117678
Iteration 10/25 | Loss: 0.00117678
Iteration 11/25 | Loss: 0.00117678
Iteration 12/25 | Loss: 0.00117678
Iteration 13/25 | Loss: 0.00117678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011767782270908356, 0.0011767782270908356, 0.0011767782270908356, 0.0011767782270908356, 0.0011767782270908356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011767782270908356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17623043
Iteration 2/25 | Loss: 0.00068336
Iteration 3/25 | Loss: 0.00068335
Iteration 4/25 | Loss: 0.00068334
Iteration 5/25 | Loss: 0.00068334
Iteration 6/25 | Loss: 0.00068334
Iteration 7/25 | Loss: 0.00068334
Iteration 8/25 | Loss: 0.00068334
Iteration 9/25 | Loss: 0.00068334
Iteration 10/25 | Loss: 0.00068334
Iteration 11/25 | Loss: 0.00068334
Iteration 12/25 | Loss: 0.00068334
Iteration 13/25 | Loss: 0.00068334
Iteration 14/25 | Loss: 0.00068334
Iteration 15/25 | Loss: 0.00068334
Iteration 16/25 | Loss: 0.00068334
Iteration 17/25 | Loss: 0.00068334
Iteration 18/25 | Loss: 0.00068334
Iteration 19/25 | Loss: 0.00068334
Iteration 20/25 | Loss: 0.00068334
Iteration 21/25 | Loss: 0.00068334
Iteration 22/25 | Loss: 0.00068334
Iteration 23/25 | Loss: 0.00068334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006833412917330861, 0.0006833412917330861, 0.0006833412917330861, 0.0006833412917330861, 0.0006833412917330861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006833412917330861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068334
Iteration 2/1000 | Loss: 0.00004874
Iteration 3/1000 | Loss: 0.00002662
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001687
Iteration 19/1000 | Loss: 0.00001686
Iteration 20/1000 | Loss: 0.00001686
Iteration 21/1000 | Loss: 0.00001686
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001685
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001682
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00001682
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001681
Iteration 33/1000 | Loss: 0.00001681
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001677
Iteration 36/1000 | Loss: 0.00001676
Iteration 37/1000 | Loss: 0.00001673
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001668
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001666
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001664
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001662
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001661
Iteration 61/1000 | Loss: 0.00001661
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001660
Iteration 64/1000 | Loss: 0.00001660
Iteration 65/1000 | Loss: 0.00001659
Iteration 66/1000 | Loss: 0.00001659
Iteration 67/1000 | Loss: 0.00001659
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001659
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001657
Iteration 75/1000 | Loss: 0.00001657
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001656
Iteration 78/1000 | Loss: 0.00001656
Iteration 79/1000 | Loss: 0.00001656
Iteration 80/1000 | Loss: 0.00001656
Iteration 81/1000 | Loss: 0.00001656
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001655
Iteration 84/1000 | Loss: 0.00001655
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001654
Iteration 90/1000 | Loss: 0.00001654
Iteration 91/1000 | Loss: 0.00001654
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00001654
Iteration 94/1000 | Loss: 0.00001653
Iteration 95/1000 | Loss: 0.00001653
Iteration 96/1000 | Loss: 0.00001653
Iteration 97/1000 | Loss: 0.00001653
Iteration 98/1000 | Loss: 0.00001652
Iteration 99/1000 | Loss: 0.00001652
Iteration 100/1000 | Loss: 0.00001652
Iteration 101/1000 | Loss: 0.00001652
Iteration 102/1000 | Loss: 0.00001652
Iteration 103/1000 | Loss: 0.00001652
Iteration 104/1000 | Loss: 0.00001652
Iteration 105/1000 | Loss: 0.00001652
Iteration 106/1000 | Loss: 0.00001652
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001651
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Iteration 162/1000 | Loss: 0.00001649
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001649
Iteration 165/1000 | Loss: 0.00001649
Iteration 166/1000 | Loss: 0.00001649
Iteration 167/1000 | Loss: 0.00001649
Iteration 168/1000 | Loss: 0.00001649
Iteration 169/1000 | Loss: 0.00001649
Iteration 170/1000 | Loss: 0.00001649
Iteration 171/1000 | Loss: 0.00001649
Iteration 172/1000 | Loss: 0.00001649
Iteration 173/1000 | Loss: 0.00001649
Iteration 174/1000 | Loss: 0.00001649
Iteration 175/1000 | Loss: 0.00001649
Iteration 176/1000 | Loss: 0.00001649
Iteration 177/1000 | Loss: 0.00001649
Iteration 178/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.648925172048621e-05, 1.648925172048621e-05, 1.648925172048621e-05, 1.648925172048621e-05, 1.648925172048621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.648925172048621e-05

Optimization complete. Final v2v error: 3.3216989040374756 mm

Highest mean error: 5.102518081665039 mm for frame 58

Lowest mean error: 2.8729727268218994 mm for frame 134

Saving results

Total time: 38.27420663833618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829412
Iteration 2/25 | Loss: 0.00112881
Iteration 3/25 | Loss: 0.00107736
Iteration 4/25 | Loss: 0.00106857
Iteration 5/25 | Loss: 0.00106624
Iteration 6/25 | Loss: 0.00106577
Iteration 7/25 | Loss: 0.00106577
Iteration 8/25 | Loss: 0.00106577
Iteration 9/25 | Loss: 0.00106577
Iteration 10/25 | Loss: 0.00106577
Iteration 11/25 | Loss: 0.00106577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010657700477167964, 0.0010657700477167964, 0.0010657700477167964, 0.0010657700477167964, 0.0010657700477167964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010657700477167964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.46386719
Iteration 2/25 | Loss: 0.00078446
Iteration 3/25 | Loss: 0.00078445
Iteration 4/25 | Loss: 0.00078445
Iteration 5/25 | Loss: 0.00078445
Iteration 6/25 | Loss: 0.00078445
Iteration 7/25 | Loss: 0.00078445
Iteration 8/25 | Loss: 0.00078445
Iteration 9/25 | Loss: 0.00078445
Iteration 10/25 | Loss: 0.00078445
Iteration 11/25 | Loss: 0.00078445
Iteration 12/25 | Loss: 0.00078445
Iteration 13/25 | Loss: 0.00078445
Iteration 14/25 | Loss: 0.00078445
Iteration 15/25 | Loss: 0.00078445
Iteration 16/25 | Loss: 0.00078445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007844488718546927, 0.0007844488718546927, 0.0007844488718546927, 0.0007844488718546927, 0.0007844488718546927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007844488718546927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078445
Iteration 2/1000 | Loss: 0.00002157
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001395
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001203
Iteration 10/1000 | Loss: 0.00001202
Iteration 11/1000 | Loss: 0.00001194
Iteration 12/1000 | Loss: 0.00001176
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001152
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001143
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001141
Iteration 42/1000 | Loss: 0.00001141
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001137
Iteration 49/1000 | Loss: 0.00001137
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001137
Iteration 53/1000 | Loss: 0.00001137
Iteration 54/1000 | Loss: 0.00001137
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001134
Iteration 71/1000 | Loss: 0.00001134
Iteration 72/1000 | Loss: 0.00001134
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001133
Iteration 75/1000 | Loss: 0.00001132
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001132
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001124
Iteration 104/1000 | Loss: 0.00001124
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001123
Iteration 108/1000 | Loss: 0.00001123
Iteration 109/1000 | Loss: 0.00001123
Iteration 110/1000 | Loss: 0.00001123
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001122
Iteration 113/1000 | Loss: 0.00001122
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001119
Iteration 118/1000 | Loss: 0.00001119
Iteration 119/1000 | Loss: 0.00001118
Iteration 120/1000 | Loss: 0.00001118
Iteration 121/1000 | Loss: 0.00001118
Iteration 122/1000 | Loss: 0.00001118
Iteration 123/1000 | Loss: 0.00001118
Iteration 124/1000 | Loss: 0.00001118
Iteration 125/1000 | Loss: 0.00001118
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001117
Iteration 128/1000 | Loss: 0.00001117
Iteration 129/1000 | Loss: 0.00001117
Iteration 130/1000 | Loss: 0.00001117
Iteration 131/1000 | Loss: 0.00001117
Iteration 132/1000 | Loss: 0.00001117
Iteration 133/1000 | Loss: 0.00001117
Iteration 134/1000 | Loss: 0.00001117
Iteration 135/1000 | Loss: 0.00001117
Iteration 136/1000 | Loss: 0.00001117
Iteration 137/1000 | Loss: 0.00001117
Iteration 138/1000 | Loss: 0.00001117
Iteration 139/1000 | Loss: 0.00001117
Iteration 140/1000 | Loss: 0.00001116
Iteration 141/1000 | Loss: 0.00001116
Iteration 142/1000 | Loss: 0.00001116
Iteration 143/1000 | Loss: 0.00001116
Iteration 144/1000 | Loss: 0.00001116
Iteration 145/1000 | Loss: 0.00001116
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001115
Iteration 148/1000 | Loss: 0.00001115
Iteration 149/1000 | Loss: 0.00001115
Iteration 150/1000 | Loss: 0.00001114
Iteration 151/1000 | Loss: 0.00001114
Iteration 152/1000 | Loss: 0.00001114
Iteration 153/1000 | Loss: 0.00001114
Iteration 154/1000 | Loss: 0.00001114
Iteration 155/1000 | Loss: 0.00001114
Iteration 156/1000 | Loss: 0.00001114
Iteration 157/1000 | Loss: 0.00001113
Iteration 158/1000 | Loss: 0.00001113
Iteration 159/1000 | Loss: 0.00001113
Iteration 160/1000 | Loss: 0.00001113
Iteration 161/1000 | Loss: 0.00001113
Iteration 162/1000 | Loss: 0.00001113
Iteration 163/1000 | Loss: 0.00001113
Iteration 164/1000 | Loss: 0.00001112
Iteration 165/1000 | Loss: 0.00001112
Iteration 166/1000 | Loss: 0.00001112
Iteration 167/1000 | Loss: 0.00001112
Iteration 168/1000 | Loss: 0.00001112
Iteration 169/1000 | Loss: 0.00001112
Iteration 170/1000 | Loss: 0.00001112
Iteration 171/1000 | Loss: 0.00001112
Iteration 172/1000 | Loss: 0.00001111
Iteration 173/1000 | Loss: 0.00001111
Iteration 174/1000 | Loss: 0.00001111
Iteration 175/1000 | Loss: 0.00001111
Iteration 176/1000 | Loss: 0.00001111
Iteration 177/1000 | Loss: 0.00001111
Iteration 178/1000 | Loss: 0.00001111
Iteration 179/1000 | Loss: 0.00001111
Iteration 180/1000 | Loss: 0.00001111
Iteration 181/1000 | Loss: 0.00001111
Iteration 182/1000 | Loss: 0.00001111
Iteration 183/1000 | Loss: 0.00001111
Iteration 184/1000 | Loss: 0.00001111
Iteration 185/1000 | Loss: 0.00001110
Iteration 186/1000 | Loss: 0.00001110
Iteration 187/1000 | Loss: 0.00001110
Iteration 188/1000 | Loss: 0.00001110
Iteration 189/1000 | Loss: 0.00001110
Iteration 190/1000 | Loss: 0.00001110
Iteration 191/1000 | Loss: 0.00001110
Iteration 192/1000 | Loss: 0.00001110
Iteration 193/1000 | Loss: 0.00001110
Iteration 194/1000 | Loss: 0.00001110
Iteration 195/1000 | Loss: 0.00001110
Iteration 196/1000 | Loss: 0.00001110
Iteration 197/1000 | Loss: 0.00001109
Iteration 198/1000 | Loss: 0.00001109
Iteration 199/1000 | Loss: 0.00001109
Iteration 200/1000 | Loss: 0.00001109
Iteration 201/1000 | Loss: 0.00001109
Iteration 202/1000 | Loss: 0.00001109
Iteration 203/1000 | Loss: 0.00001109
Iteration 204/1000 | Loss: 0.00001109
Iteration 205/1000 | Loss: 0.00001109
Iteration 206/1000 | Loss: 0.00001109
Iteration 207/1000 | Loss: 0.00001109
Iteration 208/1000 | Loss: 0.00001109
Iteration 209/1000 | Loss: 0.00001109
Iteration 210/1000 | Loss: 0.00001109
Iteration 211/1000 | Loss: 0.00001109
Iteration 212/1000 | Loss: 0.00001109
Iteration 213/1000 | Loss: 0.00001109
Iteration 214/1000 | Loss: 0.00001109
Iteration 215/1000 | Loss: 0.00001109
Iteration 216/1000 | Loss: 0.00001109
Iteration 217/1000 | Loss: 0.00001109
Iteration 218/1000 | Loss: 0.00001109
Iteration 219/1000 | Loss: 0.00001109
Iteration 220/1000 | Loss: 0.00001109
Iteration 221/1000 | Loss: 0.00001109
Iteration 222/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.109040022129193e-05, 1.109040022129193e-05, 1.109040022129193e-05, 1.109040022129193e-05, 1.109040022129193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.109040022129193e-05

Optimization complete. Final v2v error: 2.8329243659973145 mm

Highest mean error: 3.4808709621429443 mm for frame 101

Lowest mean error: 2.5331294536590576 mm for frame 48

Saving results

Total time: 38.4221830368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460338
Iteration 2/25 | Loss: 0.00134302
Iteration 3/25 | Loss: 0.00118402
Iteration 4/25 | Loss: 0.00117115
Iteration 5/25 | Loss: 0.00116968
Iteration 6/25 | Loss: 0.00116968
Iteration 7/25 | Loss: 0.00116968
Iteration 8/25 | Loss: 0.00116968
Iteration 9/25 | Loss: 0.00116968
Iteration 10/25 | Loss: 0.00116968
Iteration 11/25 | Loss: 0.00116968
Iteration 12/25 | Loss: 0.00116968
Iteration 13/25 | Loss: 0.00116968
Iteration 14/25 | Loss: 0.00116968
Iteration 15/25 | Loss: 0.00116968
Iteration 16/25 | Loss: 0.00116968
Iteration 17/25 | Loss: 0.00116968
Iteration 18/25 | Loss: 0.00116968
Iteration 19/25 | Loss: 0.00116968
Iteration 20/25 | Loss: 0.00116968
Iteration 21/25 | Loss: 0.00116968
Iteration 22/25 | Loss: 0.00116968
Iteration 23/25 | Loss: 0.00116968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011696816654875875, 0.0011696816654875875, 0.0011696816654875875, 0.0011696816654875875, 0.0011696816654875875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011696816654875875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35938060
Iteration 2/25 | Loss: 0.00084360
Iteration 3/25 | Loss: 0.00084358
Iteration 4/25 | Loss: 0.00084358
Iteration 5/25 | Loss: 0.00084358
Iteration 6/25 | Loss: 0.00084358
Iteration 7/25 | Loss: 0.00084358
Iteration 8/25 | Loss: 0.00084358
Iteration 9/25 | Loss: 0.00084358
Iteration 10/25 | Loss: 0.00084358
Iteration 11/25 | Loss: 0.00084358
Iteration 12/25 | Loss: 0.00084358
Iteration 13/25 | Loss: 0.00084358
Iteration 14/25 | Loss: 0.00084358
Iteration 15/25 | Loss: 0.00084358
Iteration 16/25 | Loss: 0.00084358
Iteration 17/25 | Loss: 0.00084358
Iteration 18/25 | Loss: 0.00084358
Iteration 19/25 | Loss: 0.00084358
Iteration 20/25 | Loss: 0.00084358
Iteration 21/25 | Loss: 0.00084358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008435804047621787, 0.0008435804047621787, 0.0008435804047621787, 0.0008435804047621787, 0.0008435804047621787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008435804047621787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084358
Iteration 2/1000 | Loss: 0.00002592
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001342
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001293
Iteration 20/1000 | Loss: 0.00001291
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001280
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001280
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001279
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001278
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001270
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001266
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001262
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001252
Iteration 103/1000 | Loss: 0.00001252
Iteration 104/1000 | Loss: 0.00001252
Iteration 105/1000 | Loss: 0.00001252
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001251
Iteration 108/1000 | Loss: 0.00001251
Iteration 109/1000 | Loss: 0.00001251
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001250
Iteration 114/1000 | Loss: 0.00001250
Iteration 115/1000 | Loss: 0.00001250
Iteration 116/1000 | Loss: 0.00001250
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001249
Iteration 121/1000 | Loss: 0.00001249
Iteration 122/1000 | Loss: 0.00001249
Iteration 123/1000 | Loss: 0.00001249
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001249
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001247
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Iteration 141/1000 | Loss: 0.00001245
Iteration 142/1000 | Loss: 0.00001245
Iteration 143/1000 | Loss: 0.00001245
Iteration 144/1000 | Loss: 0.00001245
Iteration 145/1000 | Loss: 0.00001245
Iteration 146/1000 | Loss: 0.00001245
Iteration 147/1000 | Loss: 0.00001245
Iteration 148/1000 | Loss: 0.00001245
Iteration 149/1000 | Loss: 0.00001245
Iteration 150/1000 | Loss: 0.00001245
Iteration 151/1000 | Loss: 0.00001245
Iteration 152/1000 | Loss: 0.00001244
Iteration 153/1000 | Loss: 0.00001244
Iteration 154/1000 | Loss: 0.00001244
Iteration 155/1000 | Loss: 0.00001244
Iteration 156/1000 | Loss: 0.00001244
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001244
Iteration 159/1000 | Loss: 0.00001244
Iteration 160/1000 | Loss: 0.00001244
Iteration 161/1000 | Loss: 0.00001244
Iteration 162/1000 | Loss: 0.00001244
Iteration 163/1000 | Loss: 0.00001244
Iteration 164/1000 | Loss: 0.00001244
Iteration 165/1000 | Loss: 0.00001244
Iteration 166/1000 | Loss: 0.00001244
Iteration 167/1000 | Loss: 0.00001243
Iteration 168/1000 | Loss: 0.00001243
Iteration 169/1000 | Loss: 0.00001243
Iteration 170/1000 | Loss: 0.00001243
Iteration 171/1000 | Loss: 0.00001243
Iteration 172/1000 | Loss: 0.00001243
Iteration 173/1000 | Loss: 0.00001243
Iteration 174/1000 | Loss: 0.00001243
Iteration 175/1000 | Loss: 0.00001243
Iteration 176/1000 | Loss: 0.00001243
Iteration 177/1000 | Loss: 0.00001243
Iteration 178/1000 | Loss: 0.00001243
Iteration 179/1000 | Loss: 0.00001243
Iteration 180/1000 | Loss: 0.00001242
Iteration 181/1000 | Loss: 0.00001242
Iteration 182/1000 | Loss: 0.00001242
Iteration 183/1000 | Loss: 0.00001242
Iteration 184/1000 | Loss: 0.00001242
Iteration 185/1000 | Loss: 0.00001242
Iteration 186/1000 | Loss: 0.00001242
Iteration 187/1000 | Loss: 0.00001242
Iteration 188/1000 | Loss: 0.00001242
Iteration 189/1000 | Loss: 0.00001242
Iteration 190/1000 | Loss: 0.00001241
Iteration 191/1000 | Loss: 0.00001241
Iteration 192/1000 | Loss: 0.00001241
Iteration 193/1000 | Loss: 0.00001241
Iteration 194/1000 | Loss: 0.00001241
Iteration 195/1000 | Loss: 0.00001241
Iteration 196/1000 | Loss: 0.00001241
Iteration 197/1000 | Loss: 0.00001241
Iteration 198/1000 | Loss: 0.00001241
Iteration 199/1000 | Loss: 0.00001240
Iteration 200/1000 | Loss: 0.00001240
Iteration 201/1000 | Loss: 0.00001240
Iteration 202/1000 | Loss: 0.00001240
Iteration 203/1000 | Loss: 0.00001240
Iteration 204/1000 | Loss: 0.00001240
Iteration 205/1000 | Loss: 0.00001240
Iteration 206/1000 | Loss: 0.00001240
Iteration 207/1000 | Loss: 0.00001240
Iteration 208/1000 | Loss: 0.00001240
Iteration 209/1000 | Loss: 0.00001240
Iteration 210/1000 | Loss: 0.00001240
Iteration 211/1000 | Loss: 0.00001240
Iteration 212/1000 | Loss: 0.00001239
Iteration 213/1000 | Loss: 0.00001239
Iteration 214/1000 | Loss: 0.00001239
Iteration 215/1000 | Loss: 0.00001239
Iteration 216/1000 | Loss: 0.00001239
Iteration 217/1000 | Loss: 0.00001239
Iteration 218/1000 | Loss: 0.00001239
Iteration 219/1000 | Loss: 0.00001239
Iteration 220/1000 | Loss: 0.00001239
Iteration 221/1000 | Loss: 0.00001239
Iteration 222/1000 | Loss: 0.00001239
Iteration 223/1000 | Loss: 0.00001239
Iteration 224/1000 | Loss: 0.00001239
Iteration 225/1000 | Loss: 0.00001239
Iteration 226/1000 | Loss: 0.00001239
Iteration 227/1000 | Loss: 0.00001239
Iteration 228/1000 | Loss: 0.00001239
Iteration 229/1000 | Loss: 0.00001239
Iteration 230/1000 | Loss: 0.00001239
Iteration 231/1000 | Loss: 0.00001239
Iteration 232/1000 | Loss: 0.00001239
Iteration 233/1000 | Loss: 0.00001239
Iteration 234/1000 | Loss: 0.00001239
Iteration 235/1000 | Loss: 0.00001239
Iteration 236/1000 | Loss: 0.00001239
Iteration 237/1000 | Loss: 0.00001239
Iteration 238/1000 | Loss: 0.00001239
Iteration 239/1000 | Loss: 0.00001239
Iteration 240/1000 | Loss: 0.00001239
Iteration 241/1000 | Loss: 0.00001239
Iteration 242/1000 | Loss: 0.00001239
Iteration 243/1000 | Loss: 0.00001239
Iteration 244/1000 | Loss: 0.00001239
Iteration 245/1000 | Loss: 0.00001239
Iteration 246/1000 | Loss: 0.00001239
Iteration 247/1000 | Loss: 0.00001239
Iteration 248/1000 | Loss: 0.00001239
Iteration 249/1000 | Loss: 0.00001239
Iteration 250/1000 | Loss: 0.00001239
Iteration 251/1000 | Loss: 0.00001239
Iteration 252/1000 | Loss: 0.00001239
Iteration 253/1000 | Loss: 0.00001239
Iteration 254/1000 | Loss: 0.00001239
Iteration 255/1000 | Loss: 0.00001239
Iteration 256/1000 | Loss: 0.00001239
Iteration 257/1000 | Loss: 0.00001239
Iteration 258/1000 | Loss: 0.00001239
Iteration 259/1000 | Loss: 0.00001239
Iteration 260/1000 | Loss: 0.00001239
Iteration 261/1000 | Loss: 0.00001239
Iteration 262/1000 | Loss: 0.00001239
Iteration 263/1000 | Loss: 0.00001239
Iteration 264/1000 | Loss: 0.00001239
Iteration 265/1000 | Loss: 0.00001239
Iteration 266/1000 | Loss: 0.00001239
Iteration 267/1000 | Loss: 0.00001239
Iteration 268/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.2391510608722456e-05, 1.2391510608722456e-05, 1.2391510608722456e-05, 1.2391510608722456e-05, 1.2391510608722456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2391510608722456e-05

Optimization complete. Final v2v error: 2.9538168907165527 mm

Highest mean error: 3.2092268466949463 mm for frame 94

Lowest mean error: 2.7028214931488037 mm for frame 43

Saving results

Total time: 39.81336236000061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052681
Iteration 2/25 | Loss: 0.00350971
Iteration 3/25 | Loss: 0.00216085
Iteration 4/25 | Loss: 0.00199901
Iteration 5/25 | Loss: 0.00189699
Iteration 6/25 | Loss: 0.00178827
Iteration 7/25 | Loss: 0.00177715
Iteration 8/25 | Loss: 0.00178804
Iteration 9/25 | Loss: 0.00173496
Iteration 10/25 | Loss: 0.00159378
Iteration 11/25 | Loss: 0.00156172
Iteration 12/25 | Loss: 0.00154096
Iteration 13/25 | Loss: 0.00151669
Iteration 14/25 | Loss: 0.00151550
Iteration 15/25 | Loss: 0.00150673
Iteration 16/25 | Loss: 0.00151393
Iteration 17/25 | Loss: 0.00151529
Iteration 18/25 | Loss: 0.00149972
Iteration 19/25 | Loss: 0.00153811
Iteration 20/25 | Loss: 0.00152481
Iteration 21/25 | Loss: 0.00148988
Iteration 22/25 | Loss: 0.00147243
Iteration 23/25 | Loss: 0.00146905
Iteration 24/25 | Loss: 0.00146477
Iteration 25/25 | Loss: 0.00146230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08757973
Iteration 2/25 | Loss: 0.00232049
Iteration 3/25 | Loss: 0.00232049
Iteration 4/25 | Loss: 0.00232049
Iteration 5/25 | Loss: 0.00232049
Iteration 6/25 | Loss: 0.00232048
Iteration 7/25 | Loss: 0.00232048
Iteration 8/25 | Loss: 0.00230250
Iteration 9/25 | Loss: 0.00230250
Iteration 10/25 | Loss: 0.00230250
Iteration 11/25 | Loss: 0.00230250
Iteration 12/25 | Loss: 0.00230250
Iteration 13/25 | Loss: 0.00230250
Iteration 14/25 | Loss: 0.00230250
Iteration 15/25 | Loss: 0.00230249
Iteration 16/25 | Loss: 0.00230249
Iteration 17/25 | Loss: 0.00230249
Iteration 18/25 | Loss: 0.00230249
Iteration 19/25 | Loss: 0.00230249
Iteration 20/25 | Loss: 0.00230249
Iteration 21/25 | Loss: 0.00230249
Iteration 22/25 | Loss: 0.00230249
Iteration 23/25 | Loss: 0.00230249
Iteration 24/25 | Loss: 0.00230249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0023024945985525846, 0.0023024945985525846, 0.0023024945985525846, 0.0023024945985525846, 0.0023024945985525846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023024945985525846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230249
Iteration 2/1000 | Loss: 0.00049582
Iteration 3/1000 | Loss: 0.00088167
Iteration 4/1000 | Loss: 0.00047446
Iteration 5/1000 | Loss: 0.00064751
Iteration 6/1000 | Loss: 0.00026127
Iteration 7/1000 | Loss: 0.00015210
Iteration 8/1000 | Loss: 0.00020021
Iteration 9/1000 | Loss: 0.00016943
Iteration 10/1000 | Loss: 0.00033930
Iteration 11/1000 | Loss: 0.00013608
Iteration 12/1000 | Loss: 0.00024349
Iteration 13/1000 | Loss: 0.00013199
Iteration 14/1000 | Loss: 0.00026183
Iteration 15/1000 | Loss: 0.00027429
Iteration 16/1000 | Loss: 0.00035378
Iteration 17/1000 | Loss: 0.00020979
Iteration 18/1000 | Loss: 0.00012801
Iteration 19/1000 | Loss: 0.00013711
Iteration 20/1000 | Loss: 0.00033952
Iteration 21/1000 | Loss: 0.00128132
Iteration 22/1000 | Loss: 0.00019349
Iteration 23/1000 | Loss: 0.00016317
Iteration 24/1000 | Loss: 0.00039070
Iteration 25/1000 | Loss: 0.00013107
Iteration 26/1000 | Loss: 0.00034915
Iteration 27/1000 | Loss: 0.00031993
Iteration 28/1000 | Loss: 0.00020463
Iteration 29/1000 | Loss: 0.00024039
Iteration 30/1000 | Loss: 0.00013615
Iteration 31/1000 | Loss: 0.00012978
Iteration 32/1000 | Loss: 0.00024969
Iteration 33/1000 | Loss: 0.00028161
Iteration 34/1000 | Loss: 0.00021875
Iteration 35/1000 | Loss: 0.00031624
Iteration 36/1000 | Loss: 0.00021046
Iteration 37/1000 | Loss: 0.00012089
Iteration 38/1000 | Loss: 0.00011890
Iteration 39/1000 | Loss: 0.00052035
Iteration 40/1000 | Loss: 0.00026266
Iteration 41/1000 | Loss: 0.00016065
Iteration 42/1000 | Loss: 0.00062052
Iteration 43/1000 | Loss: 0.00031009
Iteration 44/1000 | Loss: 0.00012779
Iteration 45/1000 | Loss: 0.00012000
Iteration 46/1000 | Loss: 0.00014219
Iteration 47/1000 | Loss: 0.00026818
Iteration 48/1000 | Loss: 0.00011682
Iteration 49/1000 | Loss: 0.00011358
Iteration 50/1000 | Loss: 0.00029320
Iteration 51/1000 | Loss: 0.00035696
Iteration 52/1000 | Loss: 0.00029949
Iteration 53/1000 | Loss: 0.00032493
Iteration 54/1000 | Loss: 0.00011268
Iteration 55/1000 | Loss: 0.00011082
Iteration 56/1000 | Loss: 0.00032925
Iteration 57/1000 | Loss: 0.00029939
Iteration 58/1000 | Loss: 0.00020088
Iteration 59/1000 | Loss: 0.00038976
Iteration 60/1000 | Loss: 0.00030318
Iteration 61/1000 | Loss: 0.00038235
Iteration 62/1000 | Loss: 0.00013791
Iteration 63/1000 | Loss: 0.00014974
Iteration 64/1000 | Loss: 0.00011219
Iteration 65/1000 | Loss: 0.00012615
Iteration 66/1000 | Loss: 0.00084711
Iteration 67/1000 | Loss: 0.00104308
Iteration 68/1000 | Loss: 0.00148598
Iteration 69/1000 | Loss: 0.00253233
Iteration 70/1000 | Loss: 0.00792839
Iteration 71/1000 | Loss: 0.00259288
Iteration 72/1000 | Loss: 0.00605985
Iteration 73/1000 | Loss: 0.00310968
Iteration 74/1000 | Loss: 0.00063629
Iteration 75/1000 | Loss: 0.00022511
Iteration 76/1000 | Loss: 0.00087314
Iteration 77/1000 | Loss: 0.00016677
Iteration 78/1000 | Loss: 0.00095483
Iteration 79/1000 | Loss: 0.00034508
Iteration 80/1000 | Loss: 0.00028202
Iteration 81/1000 | Loss: 0.00161233
Iteration 82/1000 | Loss: 0.00169706
Iteration 83/1000 | Loss: 0.00011152
Iteration 84/1000 | Loss: 0.00123009
Iteration 85/1000 | Loss: 0.00177945
Iteration 86/1000 | Loss: 0.00085568
Iteration 87/1000 | Loss: 0.00048834
Iteration 88/1000 | Loss: 0.00175321
Iteration 89/1000 | Loss: 0.00162098
Iteration 90/1000 | Loss: 0.00022586
Iteration 91/1000 | Loss: 0.00010305
Iteration 92/1000 | Loss: 0.00141401
Iteration 93/1000 | Loss: 0.00154032
Iteration 94/1000 | Loss: 0.00019595
Iteration 95/1000 | Loss: 0.00105852
Iteration 96/1000 | Loss: 0.00157397
Iteration 97/1000 | Loss: 0.00151005
Iteration 98/1000 | Loss: 0.00021318
Iteration 99/1000 | Loss: 0.00050317
Iteration 100/1000 | Loss: 0.00170211
Iteration 101/1000 | Loss: 0.00124008
Iteration 102/1000 | Loss: 0.00066531
Iteration 103/1000 | Loss: 0.00013710
Iteration 104/1000 | Loss: 0.00013015
Iteration 105/1000 | Loss: 0.00018171
Iteration 106/1000 | Loss: 0.00008524
Iteration 107/1000 | Loss: 0.00064703
Iteration 108/1000 | Loss: 0.00006930
Iteration 109/1000 | Loss: 0.00005825
Iteration 110/1000 | Loss: 0.00039053
Iteration 111/1000 | Loss: 0.00015094
Iteration 112/1000 | Loss: 0.00008292
Iteration 113/1000 | Loss: 0.00011378
Iteration 114/1000 | Loss: 0.00005634
Iteration 115/1000 | Loss: 0.00005413
Iteration 116/1000 | Loss: 0.00007900
Iteration 117/1000 | Loss: 0.00024955
Iteration 118/1000 | Loss: 0.00068541
Iteration 119/1000 | Loss: 0.00017960
Iteration 120/1000 | Loss: 0.00063162
Iteration 121/1000 | Loss: 0.00016407
Iteration 122/1000 | Loss: 0.00008350
Iteration 123/1000 | Loss: 0.00008002
Iteration 124/1000 | Loss: 0.00007795
Iteration 125/1000 | Loss: 0.00005907
Iteration 126/1000 | Loss: 0.00010013
Iteration 127/1000 | Loss: 0.00007947
Iteration 128/1000 | Loss: 0.00014739
Iteration 129/1000 | Loss: 0.00005307
Iteration 130/1000 | Loss: 0.00005406
Iteration 131/1000 | Loss: 0.00005163
Iteration 132/1000 | Loss: 0.00007144
Iteration 133/1000 | Loss: 0.00005092
Iteration 134/1000 | Loss: 0.00006672
Iteration 135/1000 | Loss: 0.00005155
Iteration 136/1000 | Loss: 0.00005137
Iteration 137/1000 | Loss: 0.00005043
Iteration 138/1000 | Loss: 0.00017463
Iteration 139/1000 | Loss: 0.00005152
Iteration 140/1000 | Loss: 0.00007121
Iteration 141/1000 | Loss: 0.00014089
Iteration 142/1000 | Loss: 0.00004966
Iteration 143/1000 | Loss: 0.00013188
Iteration 144/1000 | Loss: 0.00048670
Iteration 145/1000 | Loss: 0.00006606
Iteration 146/1000 | Loss: 0.00041335
Iteration 147/1000 | Loss: 0.00020145
Iteration 148/1000 | Loss: 0.00035180
Iteration 149/1000 | Loss: 0.00030806
Iteration 150/1000 | Loss: 0.00022524
Iteration 151/1000 | Loss: 0.00031978
Iteration 152/1000 | Loss: 0.00020438
Iteration 153/1000 | Loss: 0.00021033
Iteration 154/1000 | Loss: 0.00070314
Iteration 155/1000 | Loss: 0.00006862
Iteration 156/1000 | Loss: 0.00007638
Iteration 157/1000 | Loss: 0.00006184
Iteration 158/1000 | Loss: 0.00010278
Iteration 159/1000 | Loss: 0.00007102
Iteration 160/1000 | Loss: 0.00005719
Iteration 161/1000 | Loss: 0.00005600
Iteration 162/1000 | Loss: 0.00019235
Iteration 163/1000 | Loss: 0.00006260
Iteration 164/1000 | Loss: 0.00088252
Iteration 165/1000 | Loss: 0.00062124
Iteration 166/1000 | Loss: 0.00048575
Iteration 167/1000 | Loss: 0.00023247
Iteration 168/1000 | Loss: 0.00006961
Iteration 169/1000 | Loss: 0.00009227
Iteration 170/1000 | Loss: 0.00005120
Iteration 171/1000 | Loss: 0.00004817
Iteration 172/1000 | Loss: 0.00010086
Iteration 173/1000 | Loss: 0.00004785
Iteration 174/1000 | Loss: 0.00004721
Iteration 175/1000 | Loss: 0.00004696
Iteration 176/1000 | Loss: 0.00022659
Iteration 177/1000 | Loss: 0.00014681
Iteration 178/1000 | Loss: 0.00009662
Iteration 179/1000 | Loss: 0.00005333
Iteration 180/1000 | Loss: 0.00005413
Iteration 181/1000 | Loss: 0.00004719
Iteration 182/1000 | Loss: 0.00021004
Iteration 183/1000 | Loss: 0.00006095
Iteration 184/1000 | Loss: 0.00004721
Iteration 185/1000 | Loss: 0.00004705
Iteration 186/1000 | Loss: 0.00019768
Iteration 187/1000 | Loss: 0.00006702
Iteration 188/1000 | Loss: 0.00012507
Iteration 189/1000 | Loss: 0.00015398
Iteration 190/1000 | Loss: 0.00005928
Iteration 191/1000 | Loss: 0.00005998
Iteration 192/1000 | Loss: 0.00008354
Iteration 193/1000 | Loss: 0.00004555
Iteration 194/1000 | Loss: 0.00007618
Iteration 195/1000 | Loss: 0.00004465
Iteration 196/1000 | Loss: 0.00009306
Iteration 197/1000 | Loss: 0.00005037
Iteration 198/1000 | Loss: 0.00005292
Iteration 199/1000 | Loss: 0.00005831
Iteration 200/1000 | Loss: 0.00004434
Iteration 201/1000 | Loss: 0.00004420
Iteration 202/1000 | Loss: 0.00006121
Iteration 203/1000 | Loss: 0.00004411
Iteration 204/1000 | Loss: 0.00004409
Iteration 205/1000 | Loss: 0.00004408
Iteration 206/1000 | Loss: 0.00004408
Iteration 207/1000 | Loss: 0.00004407
Iteration 208/1000 | Loss: 0.00004407
Iteration 209/1000 | Loss: 0.00004407
Iteration 210/1000 | Loss: 0.00004406
Iteration 211/1000 | Loss: 0.00004404
Iteration 212/1000 | Loss: 0.00004404
Iteration 213/1000 | Loss: 0.00005689
Iteration 214/1000 | Loss: 0.00004409
Iteration 215/1000 | Loss: 0.00008197
Iteration 216/1000 | Loss: 0.00005405
Iteration 217/1000 | Loss: 0.00004443
Iteration 218/1000 | Loss: 0.00004527
Iteration 219/1000 | Loss: 0.00004394
Iteration 220/1000 | Loss: 0.00004394
Iteration 221/1000 | Loss: 0.00004394
Iteration 222/1000 | Loss: 0.00004394
Iteration 223/1000 | Loss: 0.00004391
Iteration 224/1000 | Loss: 0.00004386
Iteration 225/1000 | Loss: 0.00004384
Iteration 226/1000 | Loss: 0.00004384
Iteration 227/1000 | Loss: 0.00004384
Iteration 228/1000 | Loss: 0.00004384
Iteration 229/1000 | Loss: 0.00004383
Iteration 230/1000 | Loss: 0.00004383
Iteration 231/1000 | Loss: 0.00004383
Iteration 232/1000 | Loss: 0.00004383
Iteration 233/1000 | Loss: 0.00004383
Iteration 234/1000 | Loss: 0.00004383
Iteration 235/1000 | Loss: 0.00004383
Iteration 236/1000 | Loss: 0.00004383
Iteration 237/1000 | Loss: 0.00004383
Iteration 238/1000 | Loss: 0.00004383
Iteration 239/1000 | Loss: 0.00004383
Iteration 240/1000 | Loss: 0.00004383
Iteration 241/1000 | Loss: 0.00004383
Iteration 242/1000 | Loss: 0.00004383
Iteration 243/1000 | Loss: 0.00004383
Iteration 244/1000 | Loss: 0.00004383
Iteration 245/1000 | Loss: 0.00004383
Iteration 246/1000 | Loss: 0.00004383
Iteration 247/1000 | Loss: 0.00004382
Iteration 248/1000 | Loss: 0.00004382
Iteration 249/1000 | Loss: 0.00005647
Iteration 250/1000 | Loss: 0.00004778
Iteration 251/1000 | Loss: 0.00004382
Iteration 252/1000 | Loss: 0.00004382
Iteration 253/1000 | Loss: 0.00004381
Iteration 254/1000 | Loss: 0.00004381
Iteration 255/1000 | Loss: 0.00004381
Iteration 256/1000 | Loss: 0.00004381
Iteration 257/1000 | Loss: 0.00004381
Iteration 258/1000 | Loss: 0.00004381
Iteration 259/1000 | Loss: 0.00004381
Iteration 260/1000 | Loss: 0.00005192
Iteration 261/1000 | Loss: 0.00004381
Iteration 262/1000 | Loss: 0.00004380
Iteration 263/1000 | Loss: 0.00004380
Iteration 264/1000 | Loss: 0.00004380
Iteration 265/1000 | Loss: 0.00004380
Iteration 266/1000 | Loss: 0.00004380
Iteration 267/1000 | Loss: 0.00004380
Iteration 268/1000 | Loss: 0.00004380
Iteration 269/1000 | Loss: 0.00004380
Iteration 270/1000 | Loss: 0.00004380
Iteration 271/1000 | Loss: 0.00004380
Iteration 272/1000 | Loss: 0.00004380
Iteration 273/1000 | Loss: 0.00004380
Iteration 274/1000 | Loss: 0.00004380
Iteration 275/1000 | Loss: 0.00004380
Iteration 276/1000 | Loss: 0.00004380
Iteration 277/1000 | Loss: 0.00004380
Iteration 278/1000 | Loss: 0.00004379
Iteration 279/1000 | Loss: 0.00004379
Iteration 280/1000 | Loss: 0.00004379
Iteration 281/1000 | Loss: 0.00004379
Iteration 282/1000 | Loss: 0.00004379
Iteration 283/1000 | Loss: 0.00004379
Iteration 284/1000 | Loss: 0.00004379
Iteration 285/1000 | Loss: 0.00004379
Iteration 286/1000 | Loss: 0.00004379
Iteration 287/1000 | Loss: 0.00004379
Iteration 288/1000 | Loss: 0.00004379
Iteration 289/1000 | Loss: 0.00004379
Iteration 290/1000 | Loss: 0.00004379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [4.378987432573922e-05, 4.378987432573922e-05, 4.378987432573922e-05, 4.378987432573922e-05, 4.378987432573922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.378987432573922e-05

Optimization complete. Final v2v error: 4.523902893066406 mm

Highest mean error: 11.613357543945312 mm for frame 118

Lowest mean error: 3.1461615562438965 mm for frame 1

Saving results

Total time: 331.49824142456055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812841
Iteration 2/25 | Loss: 0.00126595
Iteration 3/25 | Loss: 0.00112443
Iteration 4/25 | Loss: 0.00111499
Iteration 5/25 | Loss: 0.00111191
Iteration 6/25 | Loss: 0.00111131
Iteration 7/25 | Loss: 0.00111131
Iteration 8/25 | Loss: 0.00111131
Iteration 9/25 | Loss: 0.00111131
Iteration 10/25 | Loss: 0.00111131
Iteration 11/25 | Loss: 0.00111131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001111311954446137, 0.001111311954446137, 0.001111311954446137, 0.001111311954446137, 0.001111311954446137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001111311954446137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.33818102
Iteration 2/25 | Loss: 0.00075236
Iteration 3/25 | Loss: 0.00075230
Iteration 4/25 | Loss: 0.00075230
Iteration 5/25 | Loss: 0.00075230
Iteration 6/25 | Loss: 0.00075230
Iteration 7/25 | Loss: 0.00075230
Iteration 8/25 | Loss: 0.00075230
Iteration 9/25 | Loss: 0.00075230
Iteration 10/25 | Loss: 0.00075230
Iteration 11/25 | Loss: 0.00075230
Iteration 12/25 | Loss: 0.00075230
Iteration 13/25 | Loss: 0.00075230
Iteration 14/25 | Loss: 0.00075230
Iteration 15/25 | Loss: 0.00075230
Iteration 16/25 | Loss: 0.00075230
Iteration 17/25 | Loss: 0.00075230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007523001986555755, 0.0007523001986555755, 0.0007523001986555755, 0.0007523001986555755, 0.0007523001986555755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007523001986555755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075230
Iteration 2/1000 | Loss: 0.00002869
Iteration 3/1000 | Loss: 0.00002042
Iteration 4/1000 | Loss: 0.00001893
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001728
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001582
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001540
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001518
Iteration 30/1000 | Loss: 0.00001518
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001502
Iteration 66/1000 | Loss: 0.00001502
Iteration 67/1000 | Loss: 0.00001502
Iteration 68/1000 | Loss: 0.00001502
Iteration 69/1000 | Loss: 0.00001502
Iteration 70/1000 | Loss: 0.00001502
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001499
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001498
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001497
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001497
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001496
Iteration 111/1000 | Loss: 0.00001496
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.4955825463403016e-05, 1.4955825463403016e-05, 1.4955825463403016e-05, 1.4955825463403016e-05, 1.4955825463403016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4955825463403016e-05

Optimization complete. Final v2v error: 3.2702558040618896 mm

Highest mean error: 4.312681198120117 mm for frame 66

Lowest mean error: 2.6900975704193115 mm for frame 174

Saving results

Total time: 42.752867221832275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642467
Iteration 2/25 | Loss: 0.00141171
Iteration 3/25 | Loss: 0.00121594
Iteration 4/25 | Loss: 0.00117102
Iteration 5/25 | Loss: 0.00116234
Iteration 6/25 | Loss: 0.00115683
Iteration 7/25 | Loss: 0.00114734
Iteration 8/25 | Loss: 0.00113981
Iteration 9/25 | Loss: 0.00113596
Iteration 10/25 | Loss: 0.00113459
Iteration 11/25 | Loss: 0.00113245
Iteration 12/25 | Loss: 0.00113197
Iteration 13/25 | Loss: 0.00113185
Iteration 14/25 | Loss: 0.00113184
Iteration 15/25 | Loss: 0.00113184
Iteration 16/25 | Loss: 0.00113183
Iteration 17/25 | Loss: 0.00113183
Iteration 18/25 | Loss: 0.00113183
Iteration 19/25 | Loss: 0.00113183
Iteration 20/25 | Loss: 0.00113183
Iteration 21/25 | Loss: 0.00113183
Iteration 22/25 | Loss: 0.00113183
Iteration 23/25 | Loss: 0.00113183
Iteration 24/25 | Loss: 0.00113183
Iteration 25/25 | Loss: 0.00113183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38804007
Iteration 2/25 | Loss: 0.00099191
Iteration 3/25 | Loss: 0.00099191
Iteration 4/25 | Loss: 0.00099191
Iteration 5/25 | Loss: 0.00099191
Iteration 6/25 | Loss: 0.00099191
Iteration 7/25 | Loss: 0.00099191
Iteration 8/25 | Loss: 0.00099191
Iteration 9/25 | Loss: 0.00099191
Iteration 10/25 | Loss: 0.00099191
Iteration 11/25 | Loss: 0.00099191
Iteration 12/25 | Loss: 0.00099191
Iteration 13/25 | Loss: 0.00099191
Iteration 14/25 | Loss: 0.00099191
Iteration 15/25 | Loss: 0.00099191
Iteration 16/25 | Loss: 0.00099191
Iteration 17/25 | Loss: 0.00099191
Iteration 18/25 | Loss: 0.00099191
Iteration 19/25 | Loss: 0.00099191
Iteration 20/25 | Loss: 0.00099191
Iteration 21/25 | Loss: 0.00099191
Iteration 22/25 | Loss: 0.00099191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009919111616909504, 0.0009919111616909504, 0.0009919111616909504, 0.0009919111616909504, 0.0009919111616909504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009919111616909504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099191
Iteration 2/1000 | Loss: 0.00005733
Iteration 3/1000 | Loss: 0.00003807
Iteration 4/1000 | Loss: 0.00003299
Iteration 5/1000 | Loss: 0.00003056
Iteration 6/1000 | Loss: 0.00002901
Iteration 7/1000 | Loss: 0.00002778
Iteration 8/1000 | Loss: 0.00002686
Iteration 9/1000 | Loss: 0.00002600
Iteration 10/1000 | Loss: 0.00002536
Iteration 11/1000 | Loss: 0.00002480
Iteration 12/1000 | Loss: 0.00002443
Iteration 13/1000 | Loss: 0.00002405
Iteration 14/1000 | Loss: 0.00002372
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002328
Iteration 17/1000 | Loss: 0.00002322
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002301
Iteration 20/1000 | Loss: 0.00027379
Iteration 21/1000 | Loss: 0.00004838
Iteration 22/1000 | Loss: 0.00002316
Iteration 23/1000 | Loss: 0.00027086
Iteration 24/1000 | Loss: 0.00005294
Iteration 25/1000 | Loss: 0.00002339
Iteration 26/1000 | Loss: 0.00019019
Iteration 27/1000 | Loss: 0.00005572
Iteration 28/1000 | Loss: 0.00019755
Iteration 29/1000 | Loss: 0.00002440
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002036
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00002011
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00002004
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001994
Iteration 43/1000 | Loss: 0.00001994
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001993
Iteration 46/1000 | Loss: 0.00001993
Iteration 47/1000 | Loss: 0.00001992
Iteration 48/1000 | Loss: 0.00049836
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00001979
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001844
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001817
Iteration 63/1000 | Loss: 0.00001817
Iteration 64/1000 | Loss: 0.00001816
Iteration 65/1000 | Loss: 0.00001816
Iteration 66/1000 | Loss: 0.00001815
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001813
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001806
Iteration 75/1000 | Loss: 0.00001806
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001805
Iteration 80/1000 | Loss: 0.00001805
Iteration 81/1000 | Loss: 0.00001804
Iteration 82/1000 | Loss: 0.00001804
Iteration 83/1000 | Loss: 0.00001804
Iteration 84/1000 | Loss: 0.00001804
Iteration 85/1000 | Loss: 0.00001803
Iteration 86/1000 | Loss: 0.00001803
Iteration 87/1000 | Loss: 0.00001803
Iteration 88/1000 | Loss: 0.00001803
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001801
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001801
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001800
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00001798
Iteration 97/1000 | Loss: 0.00001798
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001797
Iteration 101/1000 | Loss: 0.00001797
Iteration 102/1000 | Loss: 0.00001797
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001796
Iteration 105/1000 | Loss: 0.00001796
Iteration 106/1000 | Loss: 0.00001796
Iteration 107/1000 | Loss: 0.00001795
Iteration 108/1000 | Loss: 0.00001795
Iteration 109/1000 | Loss: 0.00001795
Iteration 110/1000 | Loss: 0.00001795
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001792
Iteration 121/1000 | Loss: 0.00001791
Iteration 122/1000 | Loss: 0.00001791
Iteration 123/1000 | Loss: 0.00001791
Iteration 124/1000 | Loss: 0.00001791
Iteration 125/1000 | Loss: 0.00001791
Iteration 126/1000 | Loss: 0.00001791
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001790
Iteration 129/1000 | Loss: 0.00001790
Iteration 130/1000 | Loss: 0.00001790
Iteration 131/1000 | Loss: 0.00001790
Iteration 132/1000 | Loss: 0.00001790
Iteration 133/1000 | Loss: 0.00001790
Iteration 134/1000 | Loss: 0.00001790
Iteration 135/1000 | Loss: 0.00001789
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001789
Iteration 138/1000 | Loss: 0.00001789
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001789
Iteration 142/1000 | Loss: 0.00001789
Iteration 143/1000 | Loss: 0.00001789
Iteration 144/1000 | Loss: 0.00001788
Iteration 145/1000 | Loss: 0.00001788
Iteration 146/1000 | Loss: 0.00001788
Iteration 147/1000 | Loss: 0.00001788
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001788
Iteration 151/1000 | Loss: 0.00001788
Iteration 152/1000 | Loss: 0.00001788
Iteration 153/1000 | Loss: 0.00001788
Iteration 154/1000 | Loss: 0.00001788
Iteration 155/1000 | Loss: 0.00001788
Iteration 156/1000 | Loss: 0.00001788
Iteration 157/1000 | Loss: 0.00001787
Iteration 158/1000 | Loss: 0.00001787
Iteration 159/1000 | Loss: 0.00001787
Iteration 160/1000 | Loss: 0.00001787
Iteration 161/1000 | Loss: 0.00001787
Iteration 162/1000 | Loss: 0.00001787
Iteration 163/1000 | Loss: 0.00001787
Iteration 164/1000 | Loss: 0.00001787
Iteration 165/1000 | Loss: 0.00001787
Iteration 166/1000 | Loss: 0.00001787
Iteration 167/1000 | Loss: 0.00001787
Iteration 168/1000 | Loss: 0.00001787
Iteration 169/1000 | Loss: 0.00001787
Iteration 170/1000 | Loss: 0.00001787
Iteration 171/1000 | Loss: 0.00001787
Iteration 172/1000 | Loss: 0.00001787
Iteration 173/1000 | Loss: 0.00001787
Iteration 174/1000 | Loss: 0.00001787
Iteration 175/1000 | Loss: 0.00001787
Iteration 176/1000 | Loss: 0.00001786
Iteration 177/1000 | Loss: 0.00001786
Iteration 178/1000 | Loss: 0.00001786
Iteration 179/1000 | Loss: 0.00001786
Iteration 180/1000 | Loss: 0.00001786
Iteration 181/1000 | Loss: 0.00001786
Iteration 182/1000 | Loss: 0.00001786
Iteration 183/1000 | Loss: 0.00001786
Iteration 184/1000 | Loss: 0.00001786
Iteration 185/1000 | Loss: 0.00001786
Iteration 186/1000 | Loss: 0.00001786
Iteration 187/1000 | Loss: 0.00001786
Iteration 188/1000 | Loss: 0.00001786
Iteration 189/1000 | Loss: 0.00001786
Iteration 190/1000 | Loss: 0.00001786
Iteration 191/1000 | Loss: 0.00001786
Iteration 192/1000 | Loss: 0.00001786
Iteration 193/1000 | Loss: 0.00001786
Iteration 194/1000 | Loss: 0.00001786
Iteration 195/1000 | Loss: 0.00001786
Iteration 196/1000 | Loss: 0.00001786
Iteration 197/1000 | Loss: 0.00001786
Iteration 198/1000 | Loss: 0.00001786
Iteration 199/1000 | Loss: 0.00001786
Iteration 200/1000 | Loss: 0.00001786
Iteration 201/1000 | Loss: 0.00001786
Iteration 202/1000 | Loss: 0.00001786
Iteration 203/1000 | Loss: 0.00001786
Iteration 204/1000 | Loss: 0.00001786
Iteration 205/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.785761196515523e-05, 1.785761196515523e-05, 1.785761196515523e-05, 1.785761196515523e-05, 1.785761196515523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.785761196515523e-05

Optimization complete. Final v2v error: 3.5647056102752686 mm

Highest mean error: 5.082322120666504 mm for frame 63

Lowest mean error: 2.940002202987671 mm for frame 134

Saving results

Total time: 101.29444885253906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499640
Iteration 2/25 | Loss: 0.00138033
Iteration 3/25 | Loss: 0.00115036
Iteration 4/25 | Loss: 0.00112185
Iteration 5/25 | Loss: 0.00111837
Iteration 6/25 | Loss: 0.00111821
Iteration 7/25 | Loss: 0.00111821
Iteration 8/25 | Loss: 0.00111821
Iteration 9/25 | Loss: 0.00111821
Iteration 10/25 | Loss: 0.00111821
Iteration 11/25 | Loss: 0.00111821
Iteration 12/25 | Loss: 0.00111821
Iteration 13/25 | Loss: 0.00111821
Iteration 14/25 | Loss: 0.00111821
Iteration 15/25 | Loss: 0.00111821
Iteration 16/25 | Loss: 0.00111821
Iteration 17/25 | Loss: 0.00111821
Iteration 18/25 | Loss: 0.00111821
Iteration 19/25 | Loss: 0.00111821
Iteration 20/25 | Loss: 0.00111821
Iteration 21/25 | Loss: 0.00111821
Iteration 22/25 | Loss: 0.00111821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011182118905708194, 0.0011182118905708194, 0.0011182118905708194, 0.0011182118905708194, 0.0011182118905708194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011182118905708194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36773694
Iteration 2/25 | Loss: 0.00082455
Iteration 3/25 | Loss: 0.00082454
Iteration 4/25 | Loss: 0.00082454
Iteration 5/25 | Loss: 0.00082454
Iteration 6/25 | Loss: 0.00082454
Iteration 7/25 | Loss: 0.00082454
Iteration 8/25 | Loss: 0.00082454
Iteration 9/25 | Loss: 0.00082454
Iteration 10/25 | Loss: 0.00082454
Iteration 11/25 | Loss: 0.00082454
Iteration 12/25 | Loss: 0.00082454
Iteration 13/25 | Loss: 0.00082454
Iteration 14/25 | Loss: 0.00082454
Iteration 15/25 | Loss: 0.00082454
Iteration 16/25 | Loss: 0.00082454
Iteration 17/25 | Loss: 0.00082454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008245414937846363, 0.0008245414937846363, 0.0008245414937846363, 0.0008245414937846363, 0.0008245414937846363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008245414937846363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082454
Iteration 2/1000 | Loss: 0.00002375
Iteration 3/1000 | Loss: 0.00001612
Iteration 4/1000 | Loss: 0.00001448
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001280
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001240
Iteration 11/1000 | Loss: 0.00001239
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001220
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001204
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.1767087016778532e-05, 1.1767087016778532e-05, 1.1767087016778532e-05, 1.1767087016778532e-05, 1.1767087016778532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1767087016778532e-05

Optimization complete. Final v2v error: 2.76257061958313 mm

Highest mean error: 4.497447490692139 mm for frame 83

Lowest mean error: 2.3559069633483887 mm for frame 218

Saving results

Total time: 38.82986664772034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998165
Iteration 2/25 | Loss: 0.00998165
Iteration 3/25 | Loss: 0.00998165
Iteration 4/25 | Loss: 0.00998165
Iteration 5/25 | Loss: 0.00998164
Iteration 6/25 | Loss: 0.00998164
Iteration 7/25 | Loss: 0.00998164
Iteration 8/25 | Loss: 0.00998164
Iteration 9/25 | Loss: 0.00998164
Iteration 10/25 | Loss: 0.00998164
Iteration 11/25 | Loss: 0.00998164
Iteration 12/25 | Loss: 0.00998163
Iteration 13/25 | Loss: 0.00998163
Iteration 14/25 | Loss: 0.00998163
Iteration 15/25 | Loss: 0.00998163
Iteration 16/25 | Loss: 0.00998163
Iteration 17/25 | Loss: 0.00998162
Iteration 18/25 | Loss: 0.00998162
Iteration 19/25 | Loss: 0.00998162
Iteration 20/25 | Loss: 0.00998162
Iteration 21/25 | Loss: 0.00998162
Iteration 22/25 | Loss: 0.00998162
Iteration 23/25 | Loss: 0.00998161
Iteration 24/25 | Loss: 0.00998161
Iteration 25/25 | Loss: 0.00998161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64760292
Iteration 2/25 | Loss: 0.13388485
Iteration 3/25 | Loss: 0.13045111
Iteration 4/25 | Loss: 0.13000663
Iteration 5/25 | Loss: 0.13000663
Iteration 6/25 | Loss: 0.13000663
Iteration 7/25 | Loss: 0.13000660
Iteration 8/25 | Loss: 0.13000660
Iteration 9/25 | Loss: 0.13000658
Iteration 10/25 | Loss: 0.13000657
Iteration 11/25 | Loss: 0.13000657
Iteration 12/25 | Loss: 0.13000655
Iteration 13/25 | Loss: 0.13000655
Iteration 14/25 | Loss: 0.13000655
Iteration 15/25 | Loss: 0.13000655
Iteration 16/25 | Loss: 0.13000655
Iteration 17/25 | Loss: 0.13000655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.1300065517425537, 0.1300065517425537, 0.1300065517425537, 0.1300065517425537, 0.1300065517425537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1300065517425537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13000655
Iteration 2/1000 | Loss: 0.00307030
Iteration 3/1000 | Loss: 0.00061061
Iteration 4/1000 | Loss: 0.00051735
Iteration 5/1000 | Loss: 0.00036667
Iteration 6/1000 | Loss: 0.00047985
Iteration 7/1000 | Loss: 0.00103276
Iteration 8/1000 | Loss: 0.00078726
Iteration 9/1000 | Loss: 0.00073646
Iteration 10/1000 | Loss: 0.00006265
Iteration 11/1000 | Loss: 0.00040132
Iteration 12/1000 | Loss: 0.00029785
Iteration 13/1000 | Loss: 0.00012412
Iteration 14/1000 | Loss: 0.00007766
Iteration 15/1000 | Loss: 0.00081049
Iteration 16/1000 | Loss: 0.00537658
Iteration 17/1000 | Loss: 0.00046363
Iteration 18/1000 | Loss: 0.00048474
Iteration 19/1000 | Loss: 0.00009333
Iteration 20/1000 | Loss: 0.00030166
Iteration 21/1000 | Loss: 0.00003264
Iteration 22/1000 | Loss: 0.00013999
Iteration 23/1000 | Loss: 0.00002506
Iteration 24/1000 | Loss: 0.00004740
Iteration 25/1000 | Loss: 0.00042340
Iteration 26/1000 | Loss: 0.00002310
Iteration 27/1000 | Loss: 0.00002187
Iteration 28/1000 | Loss: 0.00021864
Iteration 29/1000 | Loss: 0.00009299
Iteration 30/1000 | Loss: 0.00014442
Iteration 31/1000 | Loss: 0.00004710
Iteration 32/1000 | Loss: 0.00013847
Iteration 33/1000 | Loss: 0.00001907
Iteration 34/1000 | Loss: 0.00025911
Iteration 35/1000 | Loss: 0.00015866
Iteration 36/1000 | Loss: 0.00007061
Iteration 37/1000 | Loss: 0.00047844
Iteration 38/1000 | Loss: 0.00033439
Iteration 39/1000 | Loss: 0.00045120
Iteration 40/1000 | Loss: 0.00056997
Iteration 41/1000 | Loss: 0.00003606
Iteration 42/1000 | Loss: 0.00028210
Iteration 43/1000 | Loss: 0.00034268
Iteration 44/1000 | Loss: 0.00009337
Iteration 45/1000 | Loss: 0.00004380
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001768
Iteration 48/1000 | Loss: 0.00009166
Iteration 49/1000 | Loss: 0.00002298
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00029624
Iteration 52/1000 | Loss: 0.00041851
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00021043
Iteration 56/1000 | Loss: 0.00012183
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00011504
Iteration 60/1000 | Loss: 0.00050461
Iteration 61/1000 | Loss: 0.00026579
Iteration 62/1000 | Loss: 0.00065759
Iteration 63/1000 | Loss: 0.00040018
Iteration 64/1000 | Loss: 0.00206211
Iteration 65/1000 | Loss: 0.00017952
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00007791
Iteration 68/1000 | Loss: 0.00026766
Iteration 69/1000 | Loss: 0.00077968
Iteration 70/1000 | Loss: 0.00006634
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00003092
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001578
Iteration 75/1000 | Loss: 0.00005250
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00003187
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00003315
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001558
Iteration 97/1000 | Loss: 0.00001558
Iteration 98/1000 | Loss: 0.00012234
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001558
Iteration 101/1000 | Loss: 0.00001556
Iteration 102/1000 | Loss: 0.00001550
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00011672
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001540
Iteration 126/1000 | Loss: 0.00001540
Iteration 127/1000 | Loss: 0.00001540
Iteration 128/1000 | Loss: 0.00001540
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001540
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001539
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001538
Iteration 143/1000 | Loss: 0.00001538
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001535
Iteration 149/1000 | Loss: 0.00001535
Iteration 150/1000 | Loss: 0.00001535
Iteration 151/1000 | Loss: 0.00001535
Iteration 152/1000 | Loss: 0.00001535
Iteration 153/1000 | Loss: 0.00001535
Iteration 154/1000 | Loss: 0.00009304
Iteration 155/1000 | Loss: 0.00058746
Iteration 156/1000 | Loss: 0.00001643
Iteration 157/1000 | Loss: 0.00008206
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00007819
Iteration 160/1000 | Loss: 0.00020749
Iteration 161/1000 | Loss: 0.00001592
Iteration 162/1000 | Loss: 0.00017134
Iteration 163/1000 | Loss: 0.00036405
Iteration 164/1000 | Loss: 0.00001623
Iteration 165/1000 | Loss: 0.00002262
Iteration 166/1000 | Loss: 0.00010390
Iteration 167/1000 | Loss: 0.00001802
Iteration 168/1000 | Loss: 0.00008804
Iteration 169/1000 | Loss: 0.00004163
Iteration 170/1000 | Loss: 0.00005121
Iteration 171/1000 | Loss: 0.00005147
Iteration 172/1000 | Loss: 0.00010605
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001545
Iteration 175/1000 | Loss: 0.00001539
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001536
Iteration 178/1000 | Loss: 0.00001535
Iteration 179/1000 | Loss: 0.00001535
Iteration 180/1000 | Loss: 0.00001534
Iteration 181/1000 | Loss: 0.00001534
Iteration 182/1000 | Loss: 0.00001534
Iteration 183/1000 | Loss: 0.00001534
Iteration 184/1000 | Loss: 0.00001534
Iteration 185/1000 | Loss: 0.00001534
Iteration 186/1000 | Loss: 0.00001534
Iteration 187/1000 | Loss: 0.00001534
Iteration 188/1000 | Loss: 0.00001534
Iteration 189/1000 | Loss: 0.00001534
Iteration 190/1000 | Loss: 0.00001534
Iteration 191/1000 | Loss: 0.00001533
Iteration 192/1000 | Loss: 0.00001533
Iteration 193/1000 | Loss: 0.00001532
Iteration 194/1000 | Loss: 0.00001532
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001530
Iteration 197/1000 | Loss: 0.00001530
Iteration 198/1000 | Loss: 0.00001530
Iteration 199/1000 | Loss: 0.00001530
Iteration 200/1000 | Loss: 0.00001530
Iteration 201/1000 | Loss: 0.00001530
Iteration 202/1000 | Loss: 0.00001530
Iteration 203/1000 | Loss: 0.00001530
Iteration 204/1000 | Loss: 0.00001529
Iteration 205/1000 | Loss: 0.00001529
Iteration 206/1000 | Loss: 0.00001529
Iteration 207/1000 | Loss: 0.00001529
Iteration 208/1000 | Loss: 0.00001529
Iteration 209/1000 | Loss: 0.00001529
Iteration 210/1000 | Loss: 0.00001528
Iteration 211/1000 | Loss: 0.00001528
Iteration 212/1000 | Loss: 0.00001528
Iteration 213/1000 | Loss: 0.00001528
Iteration 214/1000 | Loss: 0.00001528
Iteration 215/1000 | Loss: 0.00001528
Iteration 216/1000 | Loss: 0.00001528
Iteration 217/1000 | Loss: 0.00001528
Iteration 218/1000 | Loss: 0.00001528
Iteration 219/1000 | Loss: 0.00001527
Iteration 220/1000 | Loss: 0.00001527
Iteration 221/1000 | Loss: 0.00001527
Iteration 222/1000 | Loss: 0.00001527
Iteration 223/1000 | Loss: 0.00001527
Iteration 224/1000 | Loss: 0.00001527
Iteration 225/1000 | Loss: 0.00001527
Iteration 226/1000 | Loss: 0.00001527
Iteration 227/1000 | Loss: 0.00001527
Iteration 228/1000 | Loss: 0.00001527
Iteration 229/1000 | Loss: 0.00001527
Iteration 230/1000 | Loss: 0.00001527
Iteration 231/1000 | Loss: 0.00001527
Iteration 232/1000 | Loss: 0.00001527
Iteration 233/1000 | Loss: 0.00001527
Iteration 234/1000 | Loss: 0.00001527
Iteration 235/1000 | Loss: 0.00001527
Iteration 236/1000 | Loss: 0.00001527
Iteration 237/1000 | Loss: 0.00001527
Iteration 238/1000 | Loss: 0.00001527
Iteration 239/1000 | Loss: 0.00001527
Iteration 240/1000 | Loss: 0.00001527
Iteration 241/1000 | Loss: 0.00001526
Iteration 242/1000 | Loss: 0.00001526
Iteration 243/1000 | Loss: 0.00001526
Iteration 244/1000 | Loss: 0.00001526
Iteration 245/1000 | Loss: 0.00001526
Iteration 246/1000 | Loss: 0.00001526
Iteration 247/1000 | Loss: 0.00001526
Iteration 248/1000 | Loss: 0.00001526
Iteration 249/1000 | Loss: 0.00001526
Iteration 250/1000 | Loss: 0.00001526
Iteration 251/1000 | Loss: 0.00001526
Iteration 252/1000 | Loss: 0.00001525
Iteration 253/1000 | Loss: 0.00001525
Iteration 254/1000 | Loss: 0.00001525
Iteration 255/1000 | Loss: 0.00001525
Iteration 256/1000 | Loss: 0.00001525
Iteration 257/1000 | Loss: 0.00001525
Iteration 258/1000 | Loss: 0.00001525
Iteration 259/1000 | Loss: 0.00001525
Iteration 260/1000 | Loss: 0.00001525
Iteration 261/1000 | Loss: 0.00001525
Iteration 262/1000 | Loss: 0.00001525
Iteration 263/1000 | Loss: 0.00001525
Iteration 264/1000 | Loss: 0.00001525
Iteration 265/1000 | Loss: 0.00001525
Iteration 266/1000 | Loss: 0.00001524
Iteration 267/1000 | Loss: 0.00001524
Iteration 268/1000 | Loss: 0.00001524
Iteration 269/1000 | Loss: 0.00001524
Iteration 270/1000 | Loss: 0.00001524
Iteration 271/1000 | Loss: 0.00001524
Iteration 272/1000 | Loss: 0.00001524
Iteration 273/1000 | Loss: 0.00001524
Iteration 274/1000 | Loss: 0.00001524
Iteration 275/1000 | Loss: 0.00001524
Iteration 276/1000 | Loss: 0.00001524
Iteration 277/1000 | Loss: 0.00001524
Iteration 278/1000 | Loss: 0.00001524
Iteration 279/1000 | Loss: 0.00001524
Iteration 280/1000 | Loss: 0.00001524
Iteration 281/1000 | Loss: 0.00001524
Iteration 282/1000 | Loss: 0.00001524
Iteration 283/1000 | Loss: 0.00001524
Iteration 284/1000 | Loss: 0.00001524
Iteration 285/1000 | Loss: 0.00001524
Iteration 286/1000 | Loss: 0.00001524
Iteration 287/1000 | Loss: 0.00001524
Iteration 288/1000 | Loss: 0.00001524
Iteration 289/1000 | Loss: 0.00001524
Iteration 290/1000 | Loss: 0.00001524
Iteration 291/1000 | Loss: 0.00001524
Iteration 292/1000 | Loss: 0.00001524
Iteration 293/1000 | Loss: 0.00001524
Iteration 294/1000 | Loss: 0.00001524
Iteration 295/1000 | Loss: 0.00001524
Iteration 296/1000 | Loss: 0.00001524
Iteration 297/1000 | Loss: 0.00001524
Iteration 298/1000 | Loss: 0.00001524
Iteration 299/1000 | Loss: 0.00001524
Iteration 300/1000 | Loss: 0.00001524
Iteration 301/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.5237152183544822e-05, 1.5237152183544822e-05, 1.5237152183544822e-05, 1.5237152183544822e-05, 1.5237152183544822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5237152183544822e-05

Optimization complete. Final v2v error: 3.327332019805908 mm

Highest mean error: 4.319653511047363 mm for frame 239

Lowest mean error: 2.7218480110168457 mm for frame 0

Saving results

Total time: 183.9368507862091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490824
Iteration 2/25 | Loss: 0.00137176
Iteration 3/25 | Loss: 0.00121010
Iteration 4/25 | Loss: 0.00120156
Iteration 5/25 | Loss: 0.00120105
Iteration 6/25 | Loss: 0.00120105
Iteration 7/25 | Loss: 0.00120105
Iteration 8/25 | Loss: 0.00120105
Iteration 9/25 | Loss: 0.00120105
Iteration 10/25 | Loss: 0.00120105
Iteration 11/25 | Loss: 0.00120105
Iteration 12/25 | Loss: 0.00120105
Iteration 13/25 | Loss: 0.00120105
Iteration 14/25 | Loss: 0.00120105
Iteration 15/25 | Loss: 0.00120105
Iteration 16/25 | Loss: 0.00120105
Iteration 17/25 | Loss: 0.00120105
Iteration 18/25 | Loss: 0.00120105
Iteration 19/25 | Loss: 0.00120105
Iteration 20/25 | Loss: 0.00120105
Iteration 21/25 | Loss: 0.00120105
Iteration 22/25 | Loss: 0.00120105
Iteration 23/25 | Loss: 0.00120105
Iteration 24/25 | Loss: 0.00120105
Iteration 25/25 | Loss: 0.00120105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35480702
Iteration 2/25 | Loss: 0.00088475
Iteration 3/25 | Loss: 0.00088475
Iteration 4/25 | Loss: 0.00088475
Iteration 5/25 | Loss: 0.00088475
Iteration 6/25 | Loss: 0.00088475
Iteration 7/25 | Loss: 0.00088474
Iteration 8/25 | Loss: 0.00088474
Iteration 9/25 | Loss: 0.00088474
Iteration 10/25 | Loss: 0.00088474
Iteration 11/25 | Loss: 0.00088474
Iteration 12/25 | Loss: 0.00088474
Iteration 13/25 | Loss: 0.00088474
Iteration 14/25 | Loss: 0.00088474
Iteration 15/25 | Loss: 0.00088474
Iteration 16/25 | Loss: 0.00088474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008847444551065564, 0.0008847444551065564, 0.0008847444551065564, 0.0008847444551065564, 0.0008847444551065564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008847444551065564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088474
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002205
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002115
Iteration 10/1000 | Loss: 0.00002114
Iteration 11/1000 | Loss: 0.00002101
Iteration 12/1000 | Loss: 0.00002083
Iteration 13/1000 | Loss: 0.00002077
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002074
Iteration 16/1000 | Loss: 0.00002069
Iteration 17/1000 | Loss: 0.00002064
Iteration 18/1000 | Loss: 0.00002051
Iteration 19/1000 | Loss: 0.00002049
Iteration 20/1000 | Loss: 0.00002046
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002044
Iteration 23/1000 | Loss: 0.00002043
Iteration 24/1000 | Loss: 0.00002042
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002033
Iteration 30/1000 | Loss: 0.00002027
Iteration 31/1000 | Loss: 0.00002019
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002015
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002012
Iteration 37/1000 | Loss: 0.00002012
Iteration 38/1000 | Loss: 0.00002012
Iteration 39/1000 | Loss: 0.00002012
Iteration 40/1000 | Loss: 0.00002011
Iteration 41/1000 | Loss: 0.00002011
Iteration 42/1000 | Loss: 0.00002010
Iteration 43/1000 | Loss: 0.00002007
Iteration 44/1000 | Loss: 0.00002007
Iteration 45/1000 | Loss: 0.00002006
Iteration 46/1000 | Loss: 0.00002006
Iteration 47/1000 | Loss: 0.00002004
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002003
Iteration 52/1000 | Loss: 0.00002003
Iteration 53/1000 | Loss: 0.00002003
Iteration 54/1000 | Loss: 0.00002002
Iteration 55/1000 | Loss: 0.00002002
Iteration 56/1000 | Loss: 0.00002002
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002002
Iteration 59/1000 | Loss: 0.00002002
Iteration 60/1000 | Loss: 0.00002002
Iteration 61/1000 | Loss: 0.00002002
Iteration 62/1000 | Loss: 0.00002002
Iteration 63/1000 | Loss: 0.00002002
Iteration 64/1000 | Loss: 0.00002002
Iteration 65/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.0022998796775937e-05, 2.0022998796775937e-05, 2.0022998796775937e-05, 2.0022998796775937e-05, 2.0022998796775937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0022998796775937e-05

Optimization complete. Final v2v error: 3.63779616355896 mm

Highest mean error: 4.132927417755127 mm for frame 157

Lowest mean error: 3.297903060913086 mm for frame 5

Saving results

Total time: 36.99162459373474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838724
Iteration 2/25 | Loss: 0.00171043
Iteration 3/25 | Loss: 0.00143859
Iteration 4/25 | Loss: 0.00141585
Iteration 5/25 | Loss: 0.00141976
Iteration 6/25 | Loss: 0.00141153
Iteration 7/25 | Loss: 0.00140335
Iteration 8/25 | Loss: 0.00139599
Iteration 9/25 | Loss: 0.00139397
Iteration 10/25 | Loss: 0.00139406
Iteration 11/25 | Loss: 0.00139375
Iteration 12/25 | Loss: 0.00139406
Iteration 13/25 | Loss: 0.00139376
Iteration 14/25 | Loss: 0.00139333
Iteration 15/25 | Loss: 0.00139315
Iteration 16/25 | Loss: 0.00139188
Iteration 17/25 | Loss: 0.00139116
Iteration 18/25 | Loss: 0.00139070
Iteration 19/25 | Loss: 0.00139066
Iteration 20/25 | Loss: 0.00139066
Iteration 21/25 | Loss: 0.00139066
Iteration 22/25 | Loss: 0.00139066
Iteration 23/25 | Loss: 0.00139066
Iteration 24/25 | Loss: 0.00139066
Iteration 25/25 | Loss: 0.00139065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35939550
Iteration 2/25 | Loss: 0.00058189
Iteration 3/25 | Loss: 0.00058188
Iteration 4/25 | Loss: 0.00058188
Iteration 5/25 | Loss: 0.00058188
Iteration 6/25 | Loss: 0.00058188
Iteration 7/25 | Loss: 0.00058188
Iteration 8/25 | Loss: 0.00058188
Iteration 9/25 | Loss: 0.00058188
Iteration 10/25 | Loss: 0.00058188
Iteration 11/25 | Loss: 0.00058188
Iteration 12/25 | Loss: 0.00058188
Iteration 13/25 | Loss: 0.00058188
Iteration 14/25 | Loss: 0.00058188
Iteration 15/25 | Loss: 0.00058188
Iteration 16/25 | Loss: 0.00058188
Iteration 17/25 | Loss: 0.00058188
Iteration 18/25 | Loss: 0.00058188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005818809731863439, 0.0005818809731863439, 0.0005818809731863439, 0.0005818809731863439, 0.0005818809731863439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005818809731863439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058188
Iteration 2/1000 | Loss: 0.00005012
Iteration 3/1000 | Loss: 0.00004130
Iteration 4/1000 | Loss: 0.00003892
Iteration 5/1000 | Loss: 0.00003775
Iteration 6/1000 | Loss: 0.00003667
Iteration 7/1000 | Loss: 0.00003595
Iteration 8/1000 | Loss: 0.00003558
Iteration 9/1000 | Loss: 0.00003527
Iteration 10/1000 | Loss: 0.00003506
Iteration 11/1000 | Loss: 0.00003490
Iteration 12/1000 | Loss: 0.00003484
Iteration 13/1000 | Loss: 0.00003483
Iteration 14/1000 | Loss: 0.00003482
Iteration 15/1000 | Loss: 0.00003481
Iteration 16/1000 | Loss: 0.00003481
Iteration 17/1000 | Loss: 0.00003480
Iteration 18/1000 | Loss: 0.00003480
Iteration 19/1000 | Loss: 0.00003480
Iteration 20/1000 | Loss: 0.00003480
Iteration 21/1000 | Loss: 0.00003480
Iteration 22/1000 | Loss: 0.00003480
Iteration 23/1000 | Loss: 0.00003480
Iteration 24/1000 | Loss: 0.00003480
Iteration 25/1000 | Loss: 0.00003480
Iteration 26/1000 | Loss: 0.00003479
Iteration 27/1000 | Loss: 0.00003479
Iteration 28/1000 | Loss: 0.00003479
Iteration 29/1000 | Loss: 0.00003479
Iteration 30/1000 | Loss: 0.00003479
Iteration 31/1000 | Loss: 0.00003479
Iteration 32/1000 | Loss: 0.00003479
Iteration 33/1000 | Loss: 0.00003479
Iteration 34/1000 | Loss: 0.00003479
Iteration 35/1000 | Loss: 0.00003479
Iteration 36/1000 | Loss: 0.00003479
Iteration 37/1000 | Loss: 0.00003479
Iteration 38/1000 | Loss: 0.00003479
Iteration 39/1000 | Loss: 0.00003479
Iteration 40/1000 | Loss: 0.00003479
Iteration 41/1000 | Loss: 0.00003479
Iteration 42/1000 | Loss: 0.00003479
Iteration 43/1000 | Loss: 0.00003479
Iteration 44/1000 | Loss: 0.00003479
Iteration 45/1000 | Loss: 0.00003479
Iteration 46/1000 | Loss: 0.00003479
Iteration 47/1000 | Loss: 0.00003479
Iteration 48/1000 | Loss: 0.00003479
Iteration 49/1000 | Loss: 0.00003479
Iteration 50/1000 | Loss: 0.00003479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [3.4790999052347615e-05, 3.4790999052347615e-05, 3.4790999052347615e-05, 3.4790999052347615e-05, 3.4790999052347615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4790999052347615e-05

Optimization complete. Final v2v error: 4.9213361740112305 mm

Highest mean error: 5.50484037399292 mm for frame 134

Lowest mean error: 4.378437519073486 mm for frame 3

Saving results

Total time: 54.41240096092224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769159
Iteration 2/25 | Loss: 0.00212896
Iteration 3/25 | Loss: 0.00141819
Iteration 4/25 | Loss: 0.00129417
Iteration 5/25 | Loss: 0.00125272
Iteration 6/25 | Loss: 0.00123313
Iteration 7/25 | Loss: 0.00122322
Iteration 8/25 | Loss: 0.00124601
Iteration 9/25 | Loss: 0.00120442
Iteration 10/25 | Loss: 0.00120336
Iteration 11/25 | Loss: 0.00119142
Iteration 12/25 | Loss: 0.00119009
Iteration 13/25 | Loss: 0.00118980
Iteration 14/25 | Loss: 0.00118976
Iteration 15/25 | Loss: 0.00118975
Iteration 16/25 | Loss: 0.00118975
Iteration 17/25 | Loss: 0.00118975
Iteration 18/25 | Loss: 0.00118975
Iteration 19/25 | Loss: 0.00118975
Iteration 20/25 | Loss: 0.00118974
Iteration 21/25 | Loss: 0.00118974
Iteration 22/25 | Loss: 0.00118974
Iteration 23/25 | Loss: 0.00118974
Iteration 24/25 | Loss: 0.00118974
Iteration 25/25 | Loss: 0.00118973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11702943
Iteration 2/25 | Loss: 0.00087270
Iteration 3/25 | Loss: 0.00081484
Iteration 4/25 | Loss: 0.00081483
Iteration 5/25 | Loss: 0.00081483
Iteration 6/25 | Loss: 0.00081483
Iteration 7/25 | Loss: 0.00081483
Iteration 8/25 | Loss: 0.00081483
Iteration 9/25 | Loss: 0.00081483
Iteration 10/25 | Loss: 0.00081483
Iteration 11/25 | Loss: 0.00081483
Iteration 12/25 | Loss: 0.00081483
Iteration 13/25 | Loss: 0.00081483
Iteration 14/25 | Loss: 0.00081483
Iteration 15/25 | Loss: 0.00081483
Iteration 16/25 | Loss: 0.00081483
Iteration 17/25 | Loss: 0.00081483
Iteration 18/25 | Loss: 0.00081483
Iteration 19/25 | Loss: 0.00081483
Iteration 20/25 | Loss: 0.00081483
Iteration 21/25 | Loss: 0.00081483
Iteration 22/25 | Loss: 0.00081483
Iteration 23/25 | Loss: 0.00081483
Iteration 24/25 | Loss: 0.00081483
Iteration 25/25 | Loss: 0.00081483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081483
Iteration 2/1000 | Loss: 0.00009883
Iteration 3/1000 | Loss: 0.00008476
Iteration 4/1000 | Loss: 0.00027149
Iteration 5/1000 | Loss: 0.00001657
Iteration 6/1000 | Loss: 0.00005869
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00013427
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00023872
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00004107
Iteration 14/1000 | Loss: 0.00008941
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002082
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001993
Iteration 25/1000 | Loss: 0.00028122
Iteration 26/1000 | Loss: 0.00011092
Iteration 27/1000 | Loss: 0.00018837
Iteration 28/1000 | Loss: 0.00005418
Iteration 29/1000 | Loss: 0.00016581
Iteration 30/1000 | Loss: 0.00002198
Iteration 31/1000 | Loss: 0.00008384
Iteration 32/1000 | Loss: 0.00003619
Iteration 33/1000 | Loss: 0.00006910
Iteration 34/1000 | Loss: 0.00003473
Iteration 35/1000 | Loss: 0.00009082
Iteration 36/1000 | Loss: 0.00003333
Iteration 37/1000 | Loss: 0.00005479
Iteration 38/1000 | Loss: 0.00001901
Iteration 39/1000 | Loss: 0.00011127
Iteration 40/1000 | Loss: 0.00002556
Iteration 41/1000 | Loss: 0.00004254
Iteration 42/1000 | Loss: 0.00004659
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00002279
Iteration 46/1000 | Loss: 0.00001309
Iteration 47/1000 | Loss: 0.00001735
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00004424
Iteration 50/1000 | Loss: 0.00003040
Iteration 51/1000 | Loss: 0.00001413
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00005073
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001348
Iteration 58/1000 | Loss: 0.00003982
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001161
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.1601843652897514e-05, 1.1601843652897514e-05, 1.1601843652897514e-05, 1.1601843652897514e-05, 1.1601843652897514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1601843652897514e-05

Optimization complete. Final v2v error: 2.9403324127197266 mm

Highest mean error: 3.2874562740325928 mm for frame 61

Lowest mean error: 2.768439292907715 mm for frame 148

Saving results

Total time: 99.06827020645142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666364
Iteration 2/25 | Loss: 0.00145756
Iteration 3/25 | Loss: 0.00136214
Iteration 4/25 | Loss: 0.00135471
Iteration 5/25 | Loss: 0.00135279
Iteration 6/25 | Loss: 0.00135279
Iteration 7/25 | Loss: 0.00135279
Iteration 8/25 | Loss: 0.00135279
Iteration 9/25 | Loss: 0.00135279
Iteration 10/25 | Loss: 0.00135279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013527865521609783, 0.0013527865521609783, 0.0013527865521609783, 0.0013527865521609783, 0.0013527865521609783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013527865521609783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14055181
Iteration 2/25 | Loss: 0.00079497
Iteration 3/25 | Loss: 0.00079497
Iteration 4/25 | Loss: 0.00079496
Iteration 5/25 | Loss: 0.00079496
Iteration 6/25 | Loss: 0.00079496
Iteration 7/25 | Loss: 0.00079496
Iteration 8/25 | Loss: 0.00079496
Iteration 9/25 | Loss: 0.00079496
Iteration 10/25 | Loss: 0.00079496
Iteration 11/25 | Loss: 0.00079496
Iteration 12/25 | Loss: 0.00079496
Iteration 13/25 | Loss: 0.00079496
Iteration 14/25 | Loss: 0.00079496
Iteration 15/25 | Loss: 0.00079496
Iteration 16/25 | Loss: 0.00079496
Iteration 17/25 | Loss: 0.00079496
Iteration 18/25 | Loss: 0.00079496
Iteration 19/25 | Loss: 0.00079496
Iteration 20/25 | Loss: 0.00079496
Iteration 21/25 | Loss: 0.00079496
Iteration 22/25 | Loss: 0.00079496
Iteration 23/25 | Loss: 0.00079496
Iteration 24/25 | Loss: 0.00079496
Iteration 25/25 | Loss: 0.00079496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079496
Iteration 2/1000 | Loss: 0.00003499
Iteration 3/1000 | Loss: 0.00002562
Iteration 4/1000 | Loss: 0.00002304
Iteration 5/1000 | Loss: 0.00002182
Iteration 6/1000 | Loss: 0.00002120
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00002002
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001976
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001960
Iteration 17/1000 | Loss: 0.00001960
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001959
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001959
Iteration 22/1000 | Loss: 0.00001958
Iteration 23/1000 | Loss: 0.00001958
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001956
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001952
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001950
Iteration 41/1000 | Loss: 0.00001950
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001949
Iteration 44/1000 | Loss: 0.00001949
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001948
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001945
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001944
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001943
Iteration 64/1000 | Loss: 0.00001943
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001943
Iteration 67/1000 | Loss: 0.00001942
Iteration 68/1000 | Loss: 0.00001942
Iteration 69/1000 | Loss: 0.00001942
Iteration 70/1000 | Loss: 0.00001942
Iteration 71/1000 | Loss: 0.00001942
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001941
Iteration 76/1000 | Loss: 0.00001941
Iteration 77/1000 | Loss: 0.00001941
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001940
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001939
Iteration 82/1000 | Loss: 0.00001939
Iteration 83/1000 | Loss: 0.00001939
Iteration 84/1000 | Loss: 0.00001939
Iteration 85/1000 | Loss: 0.00001939
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001938
Iteration 89/1000 | Loss: 0.00001938
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001934
Iteration 98/1000 | Loss: 0.00001934
Iteration 99/1000 | Loss: 0.00001934
Iteration 100/1000 | Loss: 0.00001934
Iteration 101/1000 | Loss: 0.00001934
Iteration 102/1000 | Loss: 0.00001934
Iteration 103/1000 | Loss: 0.00001934
Iteration 104/1000 | Loss: 0.00001934
Iteration 105/1000 | Loss: 0.00001934
Iteration 106/1000 | Loss: 0.00001934
Iteration 107/1000 | Loss: 0.00001934
Iteration 108/1000 | Loss: 0.00001934
Iteration 109/1000 | Loss: 0.00001933
Iteration 110/1000 | Loss: 0.00001933
Iteration 111/1000 | Loss: 0.00001933
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001931
Iteration 116/1000 | Loss: 0.00001931
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001931
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001931
Iteration 125/1000 | Loss: 0.00001930
Iteration 126/1000 | Loss: 0.00001930
Iteration 127/1000 | Loss: 0.00001930
Iteration 128/1000 | Loss: 0.00001930
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001928
Iteration 142/1000 | Loss: 0.00001928
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001927
Iteration 146/1000 | Loss: 0.00001927
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001927
Iteration 149/1000 | Loss: 0.00001927
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001926
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001925
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001924
Iteration 159/1000 | Loss: 0.00001924
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001923
Iteration 163/1000 | Loss: 0.00001923
Iteration 164/1000 | Loss: 0.00001923
Iteration 165/1000 | Loss: 0.00001923
Iteration 166/1000 | Loss: 0.00001923
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001922
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001922
Iteration 178/1000 | Loss: 0.00001922
Iteration 179/1000 | Loss: 0.00001922
Iteration 180/1000 | Loss: 0.00001922
Iteration 181/1000 | Loss: 0.00001922
Iteration 182/1000 | Loss: 0.00001922
Iteration 183/1000 | Loss: 0.00001922
Iteration 184/1000 | Loss: 0.00001922
Iteration 185/1000 | Loss: 0.00001922
Iteration 186/1000 | Loss: 0.00001922
Iteration 187/1000 | Loss: 0.00001922
Iteration 188/1000 | Loss: 0.00001922
Iteration 189/1000 | Loss: 0.00001922
Iteration 190/1000 | Loss: 0.00001922
Iteration 191/1000 | Loss: 0.00001922
Iteration 192/1000 | Loss: 0.00001922
Iteration 193/1000 | Loss: 0.00001922
Iteration 194/1000 | Loss: 0.00001922
Iteration 195/1000 | Loss: 0.00001922
Iteration 196/1000 | Loss: 0.00001922
Iteration 197/1000 | Loss: 0.00001922
Iteration 198/1000 | Loss: 0.00001922
Iteration 199/1000 | Loss: 0.00001922
Iteration 200/1000 | Loss: 0.00001922
Iteration 201/1000 | Loss: 0.00001922
Iteration 202/1000 | Loss: 0.00001922
Iteration 203/1000 | Loss: 0.00001922
Iteration 204/1000 | Loss: 0.00001922
Iteration 205/1000 | Loss: 0.00001922
Iteration 206/1000 | Loss: 0.00001922
Iteration 207/1000 | Loss: 0.00001922
Iteration 208/1000 | Loss: 0.00001922
Iteration 209/1000 | Loss: 0.00001922
Iteration 210/1000 | Loss: 0.00001922
Iteration 211/1000 | Loss: 0.00001922
Iteration 212/1000 | Loss: 0.00001922
Iteration 213/1000 | Loss: 0.00001922
Iteration 214/1000 | Loss: 0.00001922
Iteration 215/1000 | Loss: 0.00001922
Iteration 216/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.9220746253267862e-05, 1.9220746253267862e-05, 1.9220746253267862e-05, 1.9220746253267862e-05, 1.9220746253267862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9220746253267862e-05

Optimization complete. Final v2v error: 3.64338755607605 mm

Highest mean error: 4.122421741485596 mm for frame 60

Lowest mean error: 3.3691210746765137 mm for frame 193

Saving results

Total time: 38.99846816062927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598459
Iteration 2/25 | Loss: 0.00134105
Iteration 3/25 | Loss: 0.00124589
Iteration 4/25 | Loss: 0.00122975
Iteration 5/25 | Loss: 0.00122255
Iteration 6/25 | Loss: 0.00122109
Iteration 7/25 | Loss: 0.00122109
Iteration 8/25 | Loss: 0.00122109
Iteration 9/25 | Loss: 0.00122109
Iteration 10/25 | Loss: 0.00122109
Iteration 11/25 | Loss: 0.00122109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012210927670821548, 0.0012210927670821548, 0.0012210927670821548, 0.0012210927670821548, 0.0012210927670821548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012210927670821548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54927635
Iteration 2/25 | Loss: 0.00093263
Iteration 3/25 | Loss: 0.00093263
Iteration 4/25 | Loss: 0.00093263
Iteration 5/25 | Loss: 0.00093262
Iteration 6/25 | Loss: 0.00093262
Iteration 7/25 | Loss: 0.00093262
Iteration 8/25 | Loss: 0.00093262
Iteration 9/25 | Loss: 0.00093262
Iteration 10/25 | Loss: 0.00093262
Iteration 11/25 | Loss: 0.00093262
Iteration 12/25 | Loss: 0.00093262
Iteration 13/25 | Loss: 0.00093262
Iteration 14/25 | Loss: 0.00093262
Iteration 15/25 | Loss: 0.00093262
Iteration 16/25 | Loss: 0.00093262
Iteration 17/25 | Loss: 0.00093262
Iteration 18/25 | Loss: 0.00093262
Iteration 19/25 | Loss: 0.00093262
Iteration 20/25 | Loss: 0.00093262
Iteration 21/25 | Loss: 0.00093262
Iteration 22/25 | Loss: 0.00093262
Iteration 23/25 | Loss: 0.00093262
Iteration 24/25 | Loss: 0.00093262
Iteration 25/25 | Loss: 0.00093262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093262
Iteration 2/1000 | Loss: 0.00002435
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001314
Iteration 16/1000 | Loss: 0.00001312
Iteration 17/1000 | Loss: 0.00001307
Iteration 18/1000 | Loss: 0.00001306
Iteration 19/1000 | Loss: 0.00001305
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001288
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001288
Iteration 78/1000 | Loss: 0.00001288
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001287
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001287
Iteration 86/1000 | Loss: 0.00001287
Iteration 87/1000 | Loss: 0.00001287
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001282
Iteration 112/1000 | Loss: 0.00001282
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001281
Iteration 115/1000 | Loss: 0.00001281
Iteration 116/1000 | Loss: 0.00001281
Iteration 117/1000 | Loss: 0.00001281
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001275
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001275
Iteration 151/1000 | Loss: 0.00001275
Iteration 152/1000 | Loss: 0.00001275
Iteration 153/1000 | Loss: 0.00001275
Iteration 154/1000 | Loss: 0.00001274
Iteration 155/1000 | Loss: 0.00001274
Iteration 156/1000 | Loss: 0.00001274
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Iteration 164/1000 | Loss: 0.00001272
Iteration 165/1000 | Loss: 0.00001272
Iteration 166/1000 | Loss: 0.00001272
Iteration 167/1000 | Loss: 0.00001272
Iteration 168/1000 | Loss: 0.00001272
Iteration 169/1000 | Loss: 0.00001272
Iteration 170/1000 | Loss: 0.00001272
Iteration 171/1000 | Loss: 0.00001272
Iteration 172/1000 | Loss: 0.00001272
Iteration 173/1000 | Loss: 0.00001272
Iteration 174/1000 | Loss: 0.00001271
Iteration 175/1000 | Loss: 0.00001271
Iteration 176/1000 | Loss: 0.00001271
Iteration 177/1000 | Loss: 0.00001271
Iteration 178/1000 | Loss: 0.00001271
Iteration 179/1000 | Loss: 0.00001271
Iteration 180/1000 | Loss: 0.00001271
Iteration 181/1000 | Loss: 0.00001271
Iteration 182/1000 | Loss: 0.00001271
Iteration 183/1000 | Loss: 0.00001271
Iteration 184/1000 | Loss: 0.00001271
Iteration 185/1000 | Loss: 0.00001271
Iteration 186/1000 | Loss: 0.00001271
Iteration 187/1000 | Loss: 0.00001271
Iteration 188/1000 | Loss: 0.00001271
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.2705486369668506e-05, 1.2705486369668506e-05, 1.2705486369668506e-05, 1.2705486369668506e-05, 1.2705486369668506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2705486369668506e-05

Optimization complete. Final v2v error: 3.053581714630127 mm

Highest mean error: 3.572237968444824 mm for frame 87

Lowest mean error: 2.8172879219055176 mm for frame 8

Saving results

Total time: 38.89646649360657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947879
Iteration 2/25 | Loss: 0.00174853
Iteration 3/25 | Loss: 0.00144797
Iteration 4/25 | Loss: 0.00142626
Iteration 5/25 | Loss: 0.00141920
Iteration 6/25 | Loss: 0.00141861
Iteration 7/25 | Loss: 0.00141861
Iteration 8/25 | Loss: 0.00141861
Iteration 9/25 | Loss: 0.00141861
Iteration 10/25 | Loss: 0.00141861
Iteration 11/25 | Loss: 0.00141861
Iteration 12/25 | Loss: 0.00141861
Iteration 13/25 | Loss: 0.00141861
Iteration 14/25 | Loss: 0.00141861
Iteration 15/25 | Loss: 0.00141861
Iteration 16/25 | Loss: 0.00141861
Iteration 17/25 | Loss: 0.00141861
Iteration 18/25 | Loss: 0.00141861
Iteration 19/25 | Loss: 0.00141861
Iteration 20/25 | Loss: 0.00141861
Iteration 21/25 | Loss: 0.00141861
Iteration 22/25 | Loss: 0.00141861
Iteration 23/25 | Loss: 0.00141861
Iteration 24/25 | Loss: 0.00141861
Iteration 25/25 | Loss: 0.00141861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84789109
Iteration 2/25 | Loss: 0.00088556
Iteration 3/25 | Loss: 0.00088556
Iteration 4/25 | Loss: 0.00088556
Iteration 5/25 | Loss: 0.00088556
Iteration 6/25 | Loss: 0.00088556
Iteration 7/25 | Loss: 0.00088556
Iteration 8/25 | Loss: 0.00088556
Iteration 9/25 | Loss: 0.00088556
Iteration 10/25 | Loss: 0.00088556
Iteration 11/25 | Loss: 0.00088556
Iteration 12/25 | Loss: 0.00088556
Iteration 13/25 | Loss: 0.00088556
Iteration 14/25 | Loss: 0.00088556
Iteration 15/25 | Loss: 0.00088556
Iteration 16/25 | Loss: 0.00088556
Iteration 17/25 | Loss: 0.00088556
Iteration 18/25 | Loss: 0.00088556
Iteration 19/25 | Loss: 0.00088556
Iteration 20/25 | Loss: 0.00088556
Iteration 21/25 | Loss: 0.00088556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008855570340529084, 0.0008855570340529084, 0.0008855570340529084, 0.0008855570340529084, 0.0008855570340529084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008855570340529084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088556
Iteration 2/1000 | Loss: 0.00007285
Iteration 3/1000 | Loss: 0.00004829
Iteration 4/1000 | Loss: 0.00004171
Iteration 5/1000 | Loss: 0.00003933
Iteration 6/1000 | Loss: 0.00003831
Iteration 7/1000 | Loss: 0.00003721
Iteration 8/1000 | Loss: 0.00003654
Iteration 9/1000 | Loss: 0.00003590
Iteration 10/1000 | Loss: 0.00003553
Iteration 11/1000 | Loss: 0.00003511
Iteration 12/1000 | Loss: 0.00003475
Iteration 13/1000 | Loss: 0.00003444
Iteration 14/1000 | Loss: 0.00003414
Iteration 15/1000 | Loss: 0.00003386
Iteration 16/1000 | Loss: 0.00003359
Iteration 17/1000 | Loss: 0.00003341
Iteration 18/1000 | Loss: 0.00003337
Iteration 19/1000 | Loss: 0.00003335
Iteration 20/1000 | Loss: 0.00003334
Iteration 21/1000 | Loss: 0.00003332
Iteration 22/1000 | Loss: 0.00003332
Iteration 23/1000 | Loss: 0.00003321
Iteration 24/1000 | Loss: 0.00003316
Iteration 25/1000 | Loss: 0.00003309
Iteration 26/1000 | Loss: 0.00003309
Iteration 27/1000 | Loss: 0.00003309
Iteration 28/1000 | Loss: 0.00003297
Iteration 29/1000 | Loss: 0.00003294
Iteration 30/1000 | Loss: 0.00003290
Iteration 31/1000 | Loss: 0.00003287
Iteration 32/1000 | Loss: 0.00003287
Iteration 33/1000 | Loss: 0.00003286
Iteration 34/1000 | Loss: 0.00003285
Iteration 35/1000 | Loss: 0.00003285
Iteration 36/1000 | Loss: 0.00003285
Iteration 37/1000 | Loss: 0.00003285
Iteration 38/1000 | Loss: 0.00003285
Iteration 39/1000 | Loss: 0.00003285
Iteration 40/1000 | Loss: 0.00003285
Iteration 41/1000 | Loss: 0.00003284
Iteration 42/1000 | Loss: 0.00003284
Iteration 43/1000 | Loss: 0.00003282
Iteration 44/1000 | Loss: 0.00003282
Iteration 45/1000 | Loss: 0.00003282
Iteration 46/1000 | Loss: 0.00003282
Iteration 47/1000 | Loss: 0.00003282
Iteration 48/1000 | Loss: 0.00003282
Iteration 49/1000 | Loss: 0.00003282
Iteration 50/1000 | Loss: 0.00003282
Iteration 51/1000 | Loss: 0.00003282
Iteration 52/1000 | Loss: 0.00003282
Iteration 53/1000 | Loss: 0.00003282
Iteration 54/1000 | Loss: 0.00003282
Iteration 55/1000 | Loss: 0.00003281
Iteration 56/1000 | Loss: 0.00003281
Iteration 57/1000 | Loss: 0.00003281
Iteration 58/1000 | Loss: 0.00003281
Iteration 59/1000 | Loss: 0.00003281
Iteration 60/1000 | Loss: 0.00003281
Iteration 61/1000 | Loss: 0.00003281
Iteration 62/1000 | Loss: 0.00003281
Iteration 63/1000 | Loss: 0.00003281
Iteration 64/1000 | Loss: 0.00003281
Iteration 65/1000 | Loss: 0.00003281
Iteration 66/1000 | Loss: 0.00003280
Iteration 67/1000 | Loss: 0.00003280
Iteration 68/1000 | Loss: 0.00003280
Iteration 69/1000 | Loss: 0.00003280
Iteration 70/1000 | Loss: 0.00003280
Iteration 71/1000 | Loss: 0.00003280
Iteration 72/1000 | Loss: 0.00003280
Iteration 73/1000 | Loss: 0.00003279
Iteration 74/1000 | Loss: 0.00003279
Iteration 75/1000 | Loss: 0.00003278
Iteration 76/1000 | Loss: 0.00003278
Iteration 77/1000 | Loss: 0.00003278
Iteration 78/1000 | Loss: 0.00003277
Iteration 79/1000 | Loss: 0.00003277
Iteration 80/1000 | Loss: 0.00003277
Iteration 81/1000 | Loss: 0.00003277
Iteration 82/1000 | Loss: 0.00003277
Iteration 83/1000 | Loss: 0.00003276
Iteration 84/1000 | Loss: 0.00003276
Iteration 85/1000 | Loss: 0.00003276
Iteration 86/1000 | Loss: 0.00003276
Iteration 87/1000 | Loss: 0.00003276
Iteration 88/1000 | Loss: 0.00003276
Iteration 89/1000 | Loss: 0.00003276
Iteration 90/1000 | Loss: 0.00003276
Iteration 91/1000 | Loss: 0.00003276
Iteration 92/1000 | Loss: 0.00003276
Iteration 93/1000 | Loss: 0.00003276
Iteration 94/1000 | Loss: 0.00003276
Iteration 95/1000 | Loss: 0.00003276
Iteration 96/1000 | Loss: 0.00003276
Iteration 97/1000 | Loss: 0.00003275
Iteration 98/1000 | Loss: 0.00003275
Iteration 99/1000 | Loss: 0.00003275
Iteration 100/1000 | Loss: 0.00003274
Iteration 101/1000 | Loss: 0.00003274
Iteration 102/1000 | Loss: 0.00003274
Iteration 103/1000 | Loss: 0.00003274
Iteration 104/1000 | Loss: 0.00003274
Iteration 105/1000 | Loss: 0.00003274
Iteration 106/1000 | Loss: 0.00003274
Iteration 107/1000 | Loss: 0.00003273
Iteration 108/1000 | Loss: 0.00003273
Iteration 109/1000 | Loss: 0.00003273
Iteration 110/1000 | Loss: 0.00003273
Iteration 111/1000 | Loss: 0.00003273
Iteration 112/1000 | Loss: 0.00003273
Iteration 113/1000 | Loss: 0.00003273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.273466427344829e-05, 3.273466427344829e-05, 3.273466427344829e-05, 3.273466427344829e-05, 3.273466427344829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.273466427344829e-05

Optimization complete. Final v2v error: 4.7732157707214355 mm

Highest mean error: 5.755699157714844 mm for frame 89

Lowest mean error: 3.854686975479126 mm for frame 0

Saving results

Total time: 50.994091510772705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662849
Iteration 2/25 | Loss: 0.00133726
Iteration 3/25 | Loss: 0.00122808
Iteration 4/25 | Loss: 0.00121492
Iteration 5/25 | Loss: 0.00121051
Iteration 6/25 | Loss: 0.00120964
Iteration 7/25 | Loss: 0.00120964
Iteration 8/25 | Loss: 0.00120964
Iteration 9/25 | Loss: 0.00120964
Iteration 10/25 | Loss: 0.00120964
Iteration 11/25 | Loss: 0.00120964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001209639827720821, 0.001209639827720821, 0.001209639827720821, 0.001209639827720821, 0.001209639827720821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209639827720821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51506257
Iteration 2/25 | Loss: 0.00081716
Iteration 3/25 | Loss: 0.00081716
Iteration 4/25 | Loss: 0.00081716
Iteration 5/25 | Loss: 0.00081716
Iteration 6/25 | Loss: 0.00081716
Iteration 7/25 | Loss: 0.00081716
Iteration 8/25 | Loss: 0.00081716
Iteration 9/25 | Loss: 0.00081716
Iteration 10/25 | Loss: 0.00081716
Iteration 11/25 | Loss: 0.00081716
Iteration 12/25 | Loss: 0.00081716
Iteration 13/25 | Loss: 0.00081716
Iteration 14/25 | Loss: 0.00081716
Iteration 15/25 | Loss: 0.00081716
Iteration 16/25 | Loss: 0.00081716
Iteration 17/25 | Loss: 0.00081716
Iteration 18/25 | Loss: 0.00081716
Iteration 19/25 | Loss: 0.00081716
Iteration 20/25 | Loss: 0.00081716
Iteration 21/25 | Loss: 0.00081716
Iteration 22/25 | Loss: 0.00081716
Iteration 23/25 | Loss: 0.00081716
Iteration 24/25 | Loss: 0.00081716
Iteration 25/25 | Loss: 0.00081716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081716
Iteration 2/1000 | Loss: 0.00003289
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001558
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001399
Iteration 9/1000 | Loss: 0.00001386
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001328
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001312
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001283
Iteration 90/1000 | Loss: 0.00001283
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001282
Iteration 97/1000 | Loss: 0.00001282
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001280
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001280
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001274
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001274
Iteration 145/1000 | Loss: 0.00001274
Iteration 146/1000 | Loss: 0.00001274
Iteration 147/1000 | Loss: 0.00001274
Iteration 148/1000 | Loss: 0.00001274
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001272
Iteration 152/1000 | Loss: 0.00001272
Iteration 153/1000 | Loss: 0.00001272
Iteration 154/1000 | Loss: 0.00001272
Iteration 155/1000 | Loss: 0.00001271
Iteration 156/1000 | Loss: 0.00001271
Iteration 157/1000 | Loss: 0.00001271
Iteration 158/1000 | Loss: 0.00001271
Iteration 159/1000 | Loss: 0.00001271
Iteration 160/1000 | Loss: 0.00001271
Iteration 161/1000 | Loss: 0.00001271
Iteration 162/1000 | Loss: 0.00001270
Iteration 163/1000 | Loss: 0.00001270
Iteration 164/1000 | Loss: 0.00001270
Iteration 165/1000 | Loss: 0.00001270
Iteration 166/1000 | Loss: 0.00001270
Iteration 167/1000 | Loss: 0.00001269
Iteration 168/1000 | Loss: 0.00001269
Iteration 169/1000 | Loss: 0.00001269
Iteration 170/1000 | Loss: 0.00001269
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001268
Iteration 176/1000 | Loss: 0.00001268
Iteration 177/1000 | Loss: 0.00001268
Iteration 178/1000 | Loss: 0.00001268
Iteration 179/1000 | Loss: 0.00001268
Iteration 180/1000 | Loss: 0.00001268
Iteration 181/1000 | Loss: 0.00001268
Iteration 182/1000 | Loss: 0.00001268
Iteration 183/1000 | Loss: 0.00001268
Iteration 184/1000 | Loss: 0.00001268
Iteration 185/1000 | Loss: 0.00001268
Iteration 186/1000 | Loss: 0.00001268
Iteration 187/1000 | Loss: 0.00001268
Iteration 188/1000 | Loss: 0.00001268
Iteration 189/1000 | Loss: 0.00001268
Iteration 190/1000 | Loss: 0.00001268
Iteration 191/1000 | Loss: 0.00001268
Iteration 192/1000 | Loss: 0.00001268
Iteration 193/1000 | Loss: 0.00001268
Iteration 194/1000 | Loss: 0.00001268
Iteration 195/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.2682315173151437e-05, 1.2682315173151437e-05, 1.2682315173151437e-05, 1.2682315173151437e-05, 1.2682315173151437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2682315173151437e-05

Optimization complete. Final v2v error: 3.0016531944274902 mm

Highest mean error: 4.0288777351379395 mm for frame 72

Lowest mean error: 2.62880539894104 mm for frame 1

Saving results

Total time: 40.01495385169983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720104
Iteration 2/25 | Loss: 0.00160440
Iteration 3/25 | Loss: 0.00142814
Iteration 4/25 | Loss: 0.00139221
Iteration 5/25 | Loss: 0.00138442
Iteration 6/25 | Loss: 0.00138259
Iteration 7/25 | Loss: 0.00138246
Iteration 8/25 | Loss: 0.00138246
Iteration 9/25 | Loss: 0.00138246
Iteration 10/25 | Loss: 0.00138246
Iteration 11/25 | Loss: 0.00138246
Iteration 12/25 | Loss: 0.00138246
Iteration 13/25 | Loss: 0.00138246
Iteration 14/25 | Loss: 0.00138246
Iteration 15/25 | Loss: 0.00138246
Iteration 16/25 | Loss: 0.00138246
Iteration 17/25 | Loss: 0.00138246
Iteration 18/25 | Loss: 0.00138246
Iteration 19/25 | Loss: 0.00138246
Iteration 20/25 | Loss: 0.00138246
Iteration 21/25 | Loss: 0.00138246
Iteration 22/25 | Loss: 0.00138246
Iteration 23/25 | Loss: 0.00138246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013824553461745381, 0.0013824553461745381, 0.0013824553461745381, 0.0013824553461745381, 0.0013824553461745381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013824553461745381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28129375
Iteration 2/25 | Loss: 0.00101181
Iteration 3/25 | Loss: 0.00101174
Iteration 4/25 | Loss: 0.00101174
Iteration 5/25 | Loss: 0.00101173
Iteration 6/25 | Loss: 0.00101173
Iteration 7/25 | Loss: 0.00101173
Iteration 8/25 | Loss: 0.00101173
Iteration 9/25 | Loss: 0.00101173
Iteration 10/25 | Loss: 0.00101173
Iteration 11/25 | Loss: 0.00101173
Iteration 12/25 | Loss: 0.00101173
Iteration 13/25 | Loss: 0.00101173
Iteration 14/25 | Loss: 0.00101173
Iteration 15/25 | Loss: 0.00101173
Iteration 16/25 | Loss: 0.00101173
Iteration 17/25 | Loss: 0.00101173
Iteration 18/25 | Loss: 0.00101173
Iteration 19/25 | Loss: 0.00101173
Iteration 20/25 | Loss: 0.00101173
Iteration 21/25 | Loss: 0.00101173
Iteration 22/25 | Loss: 0.00101173
Iteration 23/25 | Loss: 0.00101173
Iteration 24/25 | Loss: 0.00101173
Iteration 25/25 | Loss: 0.00101173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101173
Iteration 2/1000 | Loss: 0.00009754
Iteration 3/1000 | Loss: 0.00005899
Iteration 4/1000 | Loss: 0.00004407
Iteration 5/1000 | Loss: 0.00004054
Iteration 6/1000 | Loss: 0.00003898
Iteration 7/1000 | Loss: 0.00003793
Iteration 8/1000 | Loss: 0.00003726
Iteration 9/1000 | Loss: 0.00003664
Iteration 10/1000 | Loss: 0.00003629
Iteration 11/1000 | Loss: 0.00003597
Iteration 12/1000 | Loss: 0.00003577
Iteration 13/1000 | Loss: 0.00003553
Iteration 14/1000 | Loss: 0.00003531
Iteration 15/1000 | Loss: 0.00003521
Iteration 16/1000 | Loss: 0.00003515
Iteration 17/1000 | Loss: 0.00003514
Iteration 18/1000 | Loss: 0.00003510
Iteration 19/1000 | Loss: 0.00003503
Iteration 20/1000 | Loss: 0.00003499
Iteration 21/1000 | Loss: 0.00003499
Iteration 22/1000 | Loss: 0.00003498
Iteration 23/1000 | Loss: 0.00003496
Iteration 24/1000 | Loss: 0.00003496
Iteration 25/1000 | Loss: 0.00003494
Iteration 26/1000 | Loss: 0.00003493
Iteration 27/1000 | Loss: 0.00003493
Iteration 28/1000 | Loss: 0.00003492
Iteration 29/1000 | Loss: 0.00003490
Iteration 30/1000 | Loss: 0.00003488
Iteration 31/1000 | Loss: 0.00003488
Iteration 32/1000 | Loss: 0.00003488
Iteration 33/1000 | Loss: 0.00003487
Iteration 34/1000 | Loss: 0.00003487
Iteration 35/1000 | Loss: 0.00003485
Iteration 36/1000 | Loss: 0.00003484
Iteration 37/1000 | Loss: 0.00003484
Iteration 38/1000 | Loss: 0.00003484
Iteration 39/1000 | Loss: 0.00003482
Iteration 40/1000 | Loss: 0.00003482
Iteration 41/1000 | Loss: 0.00003481
Iteration 42/1000 | Loss: 0.00003481
Iteration 43/1000 | Loss: 0.00003480
Iteration 44/1000 | Loss: 0.00003480
Iteration 45/1000 | Loss: 0.00003480
Iteration 46/1000 | Loss: 0.00003479
Iteration 47/1000 | Loss: 0.00003479
Iteration 48/1000 | Loss: 0.00003479
Iteration 49/1000 | Loss: 0.00003479
Iteration 50/1000 | Loss: 0.00003479
Iteration 51/1000 | Loss: 0.00003478
Iteration 52/1000 | Loss: 0.00003478
Iteration 53/1000 | Loss: 0.00003478
Iteration 54/1000 | Loss: 0.00003478
Iteration 55/1000 | Loss: 0.00003478
Iteration 56/1000 | Loss: 0.00003477
Iteration 57/1000 | Loss: 0.00003477
Iteration 58/1000 | Loss: 0.00003477
Iteration 59/1000 | Loss: 0.00003477
Iteration 60/1000 | Loss: 0.00003477
Iteration 61/1000 | Loss: 0.00003477
Iteration 62/1000 | Loss: 0.00003477
Iteration 63/1000 | Loss: 0.00003477
Iteration 64/1000 | Loss: 0.00003476
Iteration 65/1000 | Loss: 0.00003476
Iteration 66/1000 | Loss: 0.00003476
Iteration 67/1000 | Loss: 0.00003476
Iteration 68/1000 | Loss: 0.00003476
Iteration 69/1000 | Loss: 0.00003476
Iteration 70/1000 | Loss: 0.00003476
Iteration 71/1000 | Loss: 0.00003476
Iteration 72/1000 | Loss: 0.00003476
Iteration 73/1000 | Loss: 0.00003475
Iteration 74/1000 | Loss: 0.00003475
Iteration 75/1000 | Loss: 0.00003475
Iteration 76/1000 | Loss: 0.00003474
Iteration 77/1000 | Loss: 0.00003474
Iteration 78/1000 | Loss: 0.00003474
Iteration 79/1000 | Loss: 0.00003473
Iteration 80/1000 | Loss: 0.00003473
Iteration 81/1000 | Loss: 0.00003473
Iteration 82/1000 | Loss: 0.00003473
Iteration 83/1000 | Loss: 0.00003473
Iteration 84/1000 | Loss: 0.00003473
Iteration 85/1000 | Loss: 0.00003473
Iteration 86/1000 | Loss: 0.00003473
Iteration 87/1000 | Loss: 0.00003473
Iteration 88/1000 | Loss: 0.00003472
Iteration 89/1000 | Loss: 0.00003472
Iteration 90/1000 | Loss: 0.00003472
Iteration 91/1000 | Loss: 0.00003472
Iteration 92/1000 | Loss: 0.00003471
Iteration 93/1000 | Loss: 0.00003471
Iteration 94/1000 | Loss: 0.00003471
Iteration 95/1000 | Loss: 0.00003471
Iteration 96/1000 | Loss: 0.00003471
Iteration 97/1000 | Loss: 0.00003471
Iteration 98/1000 | Loss: 0.00003471
Iteration 99/1000 | Loss: 0.00003471
Iteration 100/1000 | Loss: 0.00003470
Iteration 101/1000 | Loss: 0.00003470
Iteration 102/1000 | Loss: 0.00003470
Iteration 103/1000 | Loss: 0.00003470
Iteration 104/1000 | Loss: 0.00003470
Iteration 105/1000 | Loss: 0.00003470
Iteration 106/1000 | Loss: 0.00003470
Iteration 107/1000 | Loss: 0.00003470
Iteration 108/1000 | Loss: 0.00003470
Iteration 109/1000 | Loss: 0.00003470
Iteration 110/1000 | Loss: 0.00003470
Iteration 111/1000 | Loss: 0.00003470
Iteration 112/1000 | Loss: 0.00003470
Iteration 113/1000 | Loss: 0.00003470
Iteration 114/1000 | Loss: 0.00003469
Iteration 115/1000 | Loss: 0.00003469
Iteration 116/1000 | Loss: 0.00003469
Iteration 117/1000 | Loss: 0.00003469
Iteration 118/1000 | Loss: 0.00003469
Iteration 119/1000 | Loss: 0.00003469
Iteration 120/1000 | Loss: 0.00003469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [3.469470902928151e-05, 3.469470902928151e-05, 3.469470902928151e-05, 3.469470902928151e-05, 3.469470902928151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.469470902928151e-05

Optimization complete. Final v2v error: 4.740151882171631 mm

Highest mean error: 5.664098739624023 mm for frame 109

Lowest mean error: 3.601685047149658 mm for frame 16

Saving results

Total time: 40.322505235672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765326
Iteration 2/25 | Loss: 0.00155542
Iteration 3/25 | Loss: 0.00137243
Iteration 4/25 | Loss: 0.00134003
Iteration 5/25 | Loss: 0.00132486
Iteration 6/25 | Loss: 0.00132149
Iteration 7/25 | Loss: 0.00131922
Iteration 8/25 | Loss: 0.00131865
Iteration 9/25 | Loss: 0.00131853
Iteration 10/25 | Loss: 0.00131848
Iteration 11/25 | Loss: 0.00131848
Iteration 12/25 | Loss: 0.00131847
Iteration 13/25 | Loss: 0.00131847
Iteration 14/25 | Loss: 0.00131847
Iteration 15/25 | Loss: 0.00131847
Iteration 16/25 | Loss: 0.00131847
Iteration 17/25 | Loss: 0.00131847
Iteration 18/25 | Loss: 0.00131847
Iteration 19/25 | Loss: 0.00131847
Iteration 20/25 | Loss: 0.00131847
Iteration 21/25 | Loss: 0.00131847
Iteration 22/25 | Loss: 0.00131846
Iteration 23/25 | Loss: 0.00131846
Iteration 24/25 | Loss: 0.00131846
Iteration 25/25 | Loss: 0.00131846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.21127868
Iteration 2/25 | Loss: 0.00103065
Iteration 3/25 | Loss: 0.00102663
Iteration 4/25 | Loss: 0.00102663
Iteration 5/25 | Loss: 0.00102663
Iteration 6/25 | Loss: 0.00102663
Iteration 7/25 | Loss: 0.00102663
Iteration 8/25 | Loss: 0.00102663
Iteration 9/25 | Loss: 0.00102663
Iteration 10/25 | Loss: 0.00102663
Iteration 11/25 | Loss: 0.00102663
Iteration 12/25 | Loss: 0.00102663
Iteration 13/25 | Loss: 0.00102663
Iteration 14/25 | Loss: 0.00102663
Iteration 15/25 | Loss: 0.00102663
Iteration 16/25 | Loss: 0.00102663
Iteration 17/25 | Loss: 0.00102663
Iteration 18/25 | Loss: 0.00102663
Iteration 19/25 | Loss: 0.00102663
Iteration 20/25 | Loss: 0.00102663
Iteration 21/25 | Loss: 0.00102663
Iteration 22/25 | Loss: 0.00102663
Iteration 23/25 | Loss: 0.00102663
Iteration 24/25 | Loss: 0.00102663
Iteration 25/25 | Loss: 0.00102663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102663
Iteration 2/1000 | Loss: 0.00005415
Iteration 3/1000 | Loss: 0.00002902
Iteration 4/1000 | Loss: 0.00002637
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002483
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002440
Iteration 9/1000 | Loss: 0.00002429
Iteration 10/1000 | Loss: 0.00002408
Iteration 11/1000 | Loss: 0.00002400
Iteration 12/1000 | Loss: 0.00002387
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002376
Iteration 15/1000 | Loss: 0.00002376
Iteration 16/1000 | Loss: 0.00002376
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002375
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002366
Iteration 22/1000 | Loss: 0.00002364
Iteration 23/1000 | Loss: 0.00002363
Iteration 24/1000 | Loss: 0.00002362
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002361
Iteration 29/1000 | Loss: 0.00002357
Iteration 30/1000 | Loss: 0.00002357
Iteration 31/1000 | Loss: 0.00002355
Iteration 32/1000 | Loss: 0.00002355
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002347
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002347
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002345
Iteration 49/1000 | Loss: 0.00002345
Iteration 50/1000 | Loss: 0.00002344
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002344
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002341
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002340
Iteration 60/1000 | Loss: 0.00002340
Iteration 61/1000 | Loss: 0.00002340
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002339
Iteration 64/1000 | Loss: 0.00002339
Iteration 65/1000 | Loss: 0.00002339
Iteration 66/1000 | Loss: 0.00002338
Iteration 67/1000 | Loss: 0.00002338
Iteration 68/1000 | Loss: 0.00002338
Iteration 69/1000 | Loss: 0.00002338
Iteration 70/1000 | Loss: 0.00002337
Iteration 71/1000 | Loss: 0.00002337
Iteration 72/1000 | Loss: 0.00002337
Iteration 73/1000 | Loss: 0.00002337
Iteration 74/1000 | Loss: 0.00002336
Iteration 75/1000 | Loss: 0.00002336
Iteration 76/1000 | Loss: 0.00002336
Iteration 77/1000 | Loss: 0.00002335
Iteration 78/1000 | Loss: 0.00002335
Iteration 79/1000 | Loss: 0.00002335
Iteration 80/1000 | Loss: 0.00002335
Iteration 81/1000 | Loss: 0.00002335
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002334
Iteration 84/1000 | Loss: 0.00002334
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002333
Iteration 88/1000 | Loss: 0.00002333
Iteration 89/1000 | Loss: 0.00002333
Iteration 90/1000 | Loss: 0.00002332
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002332
Iteration 93/1000 | Loss: 0.00002332
Iteration 94/1000 | Loss: 0.00002332
Iteration 95/1000 | Loss: 0.00002332
Iteration 96/1000 | Loss: 0.00002332
Iteration 97/1000 | Loss: 0.00002332
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002332
Iteration 100/1000 | Loss: 0.00002332
Iteration 101/1000 | Loss: 0.00002332
Iteration 102/1000 | Loss: 0.00002331
Iteration 103/1000 | Loss: 0.00002331
Iteration 104/1000 | Loss: 0.00002329
Iteration 105/1000 | Loss: 0.00002329
Iteration 106/1000 | Loss: 0.00002329
Iteration 107/1000 | Loss: 0.00002329
Iteration 108/1000 | Loss: 0.00002329
Iteration 109/1000 | Loss: 0.00002329
Iteration 110/1000 | Loss: 0.00002329
Iteration 111/1000 | Loss: 0.00002328
Iteration 112/1000 | Loss: 0.00002328
Iteration 113/1000 | Loss: 0.00002328
Iteration 114/1000 | Loss: 0.00002328
Iteration 115/1000 | Loss: 0.00002328
Iteration 116/1000 | Loss: 0.00002328
Iteration 117/1000 | Loss: 0.00002327
Iteration 118/1000 | Loss: 0.00002327
Iteration 119/1000 | Loss: 0.00002327
Iteration 120/1000 | Loss: 0.00002327
Iteration 121/1000 | Loss: 0.00002327
Iteration 122/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.3274225895875134e-05, 2.3274225895875134e-05, 2.3274225895875134e-05, 2.3274225895875134e-05, 2.3274225895875134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3274225895875134e-05

Optimization complete. Final v2v error: 4.021732330322266 mm

Highest mean error: 4.395403861999512 mm for frame 186

Lowest mean error: 3.8306126594543457 mm for frame 28

Saving results

Total time: 43.97695589065552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477479
Iteration 2/25 | Loss: 0.00126581
Iteration 3/25 | Loss: 0.00120707
Iteration 4/25 | Loss: 0.00119935
Iteration 5/25 | Loss: 0.00119668
Iteration 6/25 | Loss: 0.00119631
Iteration 7/25 | Loss: 0.00119631
Iteration 8/25 | Loss: 0.00119631
Iteration 9/25 | Loss: 0.00119631
Iteration 10/25 | Loss: 0.00119631
Iteration 11/25 | Loss: 0.00119631
Iteration 12/25 | Loss: 0.00119631
Iteration 13/25 | Loss: 0.00119631
Iteration 14/25 | Loss: 0.00119631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001196306082420051, 0.001196306082420051, 0.001196306082420051, 0.001196306082420051, 0.001196306082420051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196306082420051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.85175419
Iteration 2/25 | Loss: 0.00073450
Iteration 3/25 | Loss: 0.00073448
Iteration 4/25 | Loss: 0.00073448
Iteration 5/25 | Loss: 0.00073448
Iteration 6/25 | Loss: 0.00073448
Iteration 7/25 | Loss: 0.00073448
Iteration 8/25 | Loss: 0.00073448
Iteration 9/25 | Loss: 0.00073448
Iteration 10/25 | Loss: 0.00073448
Iteration 11/25 | Loss: 0.00073448
Iteration 12/25 | Loss: 0.00073448
Iteration 13/25 | Loss: 0.00073448
Iteration 14/25 | Loss: 0.00073448
Iteration 15/25 | Loss: 0.00073448
Iteration 16/25 | Loss: 0.00073448
Iteration 17/25 | Loss: 0.00073448
Iteration 18/25 | Loss: 0.00073448
Iteration 19/25 | Loss: 0.00073448
Iteration 20/25 | Loss: 0.00073448
Iteration 21/25 | Loss: 0.00073448
Iteration 22/25 | Loss: 0.00073448
Iteration 23/25 | Loss: 0.00073448
Iteration 24/25 | Loss: 0.00073448
Iteration 25/25 | Loss: 0.00073448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073448
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001359
Iteration 8/1000 | Loss: 0.00001317
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001240
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001228
Iteration 56/1000 | Loss: 0.00001228
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001222
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001217
Iteration 78/1000 | Loss: 0.00001217
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001216
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.2046348274452612e-05, 1.2046348274452612e-05, 1.2046348274452612e-05, 1.2046348274452612e-05, 1.2046348274452612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2046348274452612e-05

Optimization complete. Final v2v error: 2.9833104610443115 mm

Highest mean error: 3.6258232593536377 mm for frame 187

Lowest mean error: 2.748939037322998 mm for frame 70

Saving results

Total time: 39.27836060523987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780921
Iteration 2/25 | Loss: 0.00160775
Iteration 3/25 | Loss: 0.00143363
Iteration 4/25 | Loss: 0.00141391
Iteration 5/25 | Loss: 0.00140934
Iteration 6/25 | Loss: 0.00140821
Iteration 7/25 | Loss: 0.00140821
Iteration 8/25 | Loss: 0.00140821
Iteration 9/25 | Loss: 0.00140821
Iteration 10/25 | Loss: 0.00140821
Iteration 11/25 | Loss: 0.00140821
Iteration 12/25 | Loss: 0.00140821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001408205833286047, 0.001408205833286047, 0.001408205833286047, 0.001408205833286047, 0.001408205833286047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001408205833286047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44027722
Iteration 2/25 | Loss: 0.00091847
Iteration 3/25 | Loss: 0.00091844
Iteration 4/25 | Loss: 0.00091844
Iteration 5/25 | Loss: 0.00091843
Iteration 6/25 | Loss: 0.00091843
Iteration 7/25 | Loss: 0.00091843
Iteration 8/25 | Loss: 0.00091843
Iteration 9/25 | Loss: 0.00091843
Iteration 10/25 | Loss: 0.00091843
Iteration 11/25 | Loss: 0.00091843
Iteration 12/25 | Loss: 0.00091843
Iteration 13/25 | Loss: 0.00091843
Iteration 14/25 | Loss: 0.00091843
Iteration 15/25 | Loss: 0.00091843
Iteration 16/25 | Loss: 0.00091843
Iteration 17/25 | Loss: 0.00091843
Iteration 18/25 | Loss: 0.00091843
Iteration 19/25 | Loss: 0.00091843
Iteration 20/25 | Loss: 0.00091843
Iteration 21/25 | Loss: 0.00091843
Iteration 22/25 | Loss: 0.00091843
Iteration 23/25 | Loss: 0.00091843
Iteration 24/25 | Loss: 0.00091843
Iteration 25/25 | Loss: 0.00091843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009184321970678866, 0.0009184321970678866, 0.0009184321970678866, 0.0009184321970678866, 0.0009184321970678866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009184321970678866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091843
Iteration 2/1000 | Loss: 0.00006167
Iteration 3/1000 | Loss: 0.00003939
Iteration 4/1000 | Loss: 0.00003261
Iteration 5/1000 | Loss: 0.00003087
Iteration 6/1000 | Loss: 0.00002993
Iteration 7/1000 | Loss: 0.00002951
Iteration 8/1000 | Loss: 0.00002907
Iteration 9/1000 | Loss: 0.00002877
Iteration 10/1000 | Loss: 0.00002875
Iteration 11/1000 | Loss: 0.00002859
Iteration 12/1000 | Loss: 0.00002851
Iteration 13/1000 | Loss: 0.00002837
Iteration 14/1000 | Loss: 0.00002828
Iteration 15/1000 | Loss: 0.00002820
Iteration 16/1000 | Loss: 0.00002818
Iteration 17/1000 | Loss: 0.00002816
Iteration 18/1000 | Loss: 0.00002816
Iteration 19/1000 | Loss: 0.00002815
Iteration 20/1000 | Loss: 0.00002815
Iteration 21/1000 | Loss: 0.00002815
Iteration 22/1000 | Loss: 0.00002815
Iteration 23/1000 | Loss: 0.00002812
Iteration 24/1000 | Loss: 0.00002811
Iteration 25/1000 | Loss: 0.00002811
Iteration 26/1000 | Loss: 0.00002807
Iteration 27/1000 | Loss: 0.00002807
Iteration 28/1000 | Loss: 0.00002807
Iteration 29/1000 | Loss: 0.00002807
Iteration 30/1000 | Loss: 0.00002806
Iteration 31/1000 | Loss: 0.00002806
Iteration 32/1000 | Loss: 0.00002806
Iteration 33/1000 | Loss: 0.00002805
Iteration 34/1000 | Loss: 0.00002803
Iteration 35/1000 | Loss: 0.00002800
Iteration 36/1000 | Loss: 0.00002800
Iteration 37/1000 | Loss: 0.00002800
Iteration 38/1000 | Loss: 0.00002799
Iteration 39/1000 | Loss: 0.00002799
Iteration 40/1000 | Loss: 0.00002797
Iteration 41/1000 | Loss: 0.00002792
Iteration 42/1000 | Loss: 0.00002789
Iteration 43/1000 | Loss: 0.00002787
Iteration 44/1000 | Loss: 0.00002786
Iteration 45/1000 | Loss: 0.00002782
Iteration 46/1000 | Loss: 0.00002780
Iteration 47/1000 | Loss: 0.00002779
Iteration 48/1000 | Loss: 0.00002777
Iteration 49/1000 | Loss: 0.00002777
Iteration 50/1000 | Loss: 0.00002777
Iteration 51/1000 | Loss: 0.00002777
Iteration 52/1000 | Loss: 0.00002777
Iteration 53/1000 | Loss: 0.00002777
Iteration 54/1000 | Loss: 0.00002777
Iteration 55/1000 | Loss: 0.00002777
Iteration 56/1000 | Loss: 0.00002776
Iteration 57/1000 | Loss: 0.00002776
Iteration 58/1000 | Loss: 0.00002776
Iteration 59/1000 | Loss: 0.00002776
Iteration 60/1000 | Loss: 0.00002775
Iteration 61/1000 | Loss: 0.00002774
Iteration 62/1000 | Loss: 0.00002774
Iteration 63/1000 | Loss: 0.00002774
Iteration 64/1000 | Loss: 0.00002774
Iteration 65/1000 | Loss: 0.00002773
Iteration 66/1000 | Loss: 0.00002773
Iteration 67/1000 | Loss: 0.00002772
Iteration 68/1000 | Loss: 0.00002772
Iteration 69/1000 | Loss: 0.00002770
Iteration 70/1000 | Loss: 0.00002770
Iteration 71/1000 | Loss: 0.00002770
Iteration 72/1000 | Loss: 0.00002770
Iteration 73/1000 | Loss: 0.00002770
Iteration 74/1000 | Loss: 0.00002770
Iteration 75/1000 | Loss: 0.00002770
Iteration 76/1000 | Loss: 0.00002770
Iteration 77/1000 | Loss: 0.00002769
Iteration 78/1000 | Loss: 0.00002769
Iteration 79/1000 | Loss: 0.00002768
Iteration 80/1000 | Loss: 0.00002768
Iteration 81/1000 | Loss: 0.00002768
Iteration 82/1000 | Loss: 0.00002766
Iteration 83/1000 | Loss: 0.00002766
Iteration 84/1000 | Loss: 0.00002766
Iteration 85/1000 | Loss: 0.00002765
Iteration 86/1000 | Loss: 0.00002765
Iteration 87/1000 | Loss: 0.00002765
Iteration 88/1000 | Loss: 0.00002765
Iteration 89/1000 | Loss: 0.00002765
Iteration 90/1000 | Loss: 0.00002765
Iteration 91/1000 | Loss: 0.00002765
Iteration 92/1000 | Loss: 0.00002765
Iteration 93/1000 | Loss: 0.00002765
Iteration 94/1000 | Loss: 0.00002765
Iteration 95/1000 | Loss: 0.00002765
Iteration 96/1000 | Loss: 0.00002765
Iteration 97/1000 | Loss: 0.00002765
Iteration 98/1000 | Loss: 0.00002764
Iteration 99/1000 | Loss: 0.00002764
Iteration 100/1000 | Loss: 0.00002763
Iteration 101/1000 | Loss: 0.00002763
Iteration 102/1000 | Loss: 0.00002763
Iteration 103/1000 | Loss: 0.00002763
Iteration 104/1000 | Loss: 0.00002763
Iteration 105/1000 | Loss: 0.00002763
Iteration 106/1000 | Loss: 0.00002763
Iteration 107/1000 | Loss: 0.00002763
Iteration 108/1000 | Loss: 0.00002763
Iteration 109/1000 | Loss: 0.00002763
Iteration 110/1000 | Loss: 0.00002763
Iteration 111/1000 | Loss: 0.00002762
Iteration 112/1000 | Loss: 0.00002762
Iteration 113/1000 | Loss: 0.00002761
Iteration 114/1000 | Loss: 0.00002761
Iteration 115/1000 | Loss: 0.00002761
Iteration 116/1000 | Loss: 0.00002761
Iteration 117/1000 | Loss: 0.00002761
Iteration 118/1000 | Loss: 0.00002761
Iteration 119/1000 | Loss: 0.00002761
Iteration 120/1000 | Loss: 0.00002760
Iteration 121/1000 | Loss: 0.00002760
Iteration 122/1000 | Loss: 0.00002760
Iteration 123/1000 | Loss: 0.00002760
Iteration 124/1000 | Loss: 0.00002760
Iteration 125/1000 | Loss: 0.00002759
Iteration 126/1000 | Loss: 0.00002759
Iteration 127/1000 | Loss: 0.00002759
Iteration 128/1000 | Loss: 0.00002759
Iteration 129/1000 | Loss: 0.00002759
Iteration 130/1000 | Loss: 0.00002759
Iteration 131/1000 | Loss: 0.00002759
Iteration 132/1000 | Loss: 0.00002758
Iteration 133/1000 | Loss: 0.00002758
Iteration 134/1000 | Loss: 0.00002758
Iteration 135/1000 | Loss: 0.00002758
Iteration 136/1000 | Loss: 0.00002758
Iteration 137/1000 | Loss: 0.00002757
Iteration 138/1000 | Loss: 0.00002757
Iteration 139/1000 | Loss: 0.00002757
Iteration 140/1000 | Loss: 0.00002757
Iteration 141/1000 | Loss: 0.00002757
Iteration 142/1000 | Loss: 0.00002757
Iteration 143/1000 | Loss: 0.00002756
Iteration 144/1000 | Loss: 0.00002756
Iteration 145/1000 | Loss: 0.00002756
Iteration 146/1000 | Loss: 0.00002756
Iteration 147/1000 | Loss: 0.00002756
Iteration 148/1000 | Loss: 0.00002756
Iteration 149/1000 | Loss: 0.00002756
Iteration 150/1000 | Loss: 0.00002756
Iteration 151/1000 | Loss: 0.00002756
Iteration 152/1000 | Loss: 0.00002756
Iteration 153/1000 | Loss: 0.00002755
Iteration 154/1000 | Loss: 0.00002755
Iteration 155/1000 | Loss: 0.00002755
Iteration 156/1000 | Loss: 0.00002755
Iteration 157/1000 | Loss: 0.00002755
Iteration 158/1000 | Loss: 0.00002755
Iteration 159/1000 | Loss: 0.00002755
Iteration 160/1000 | Loss: 0.00002755
Iteration 161/1000 | Loss: 0.00002755
Iteration 162/1000 | Loss: 0.00002755
Iteration 163/1000 | Loss: 0.00002755
Iteration 164/1000 | Loss: 0.00002755
Iteration 165/1000 | Loss: 0.00002755
Iteration 166/1000 | Loss: 0.00002755
Iteration 167/1000 | Loss: 0.00002755
Iteration 168/1000 | Loss: 0.00002755
Iteration 169/1000 | Loss: 0.00002755
Iteration 170/1000 | Loss: 0.00002755
Iteration 171/1000 | Loss: 0.00002755
Iteration 172/1000 | Loss: 0.00002755
Iteration 173/1000 | Loss: 0.00002755
Iteration 174/1000 | Loss: 0.00002755
Iteration 175/1000 | Loss: 0.00002755
Iteration 176/1000 | Loss: 0.00002755
Iteration 177/1000 | Loss: 0.00002755
Iteration 178/1000 | Loss: 0.00002755
Iteration 179/1000 | Loss: 0.00002755
Iteration 180/1000 | Loss: 0.00002755
Iteration 181/1000 | Loss: 0.00002755
Iteration 182/1000 | Loss: 0.00002755
Iteration 183/1000 | Loss: 0.00002755
Iteration 184/1000 | Loss: 0.00002755
Iteration 185/1000 | Loss: 0.00002755
Iteration 186/1000 | Loss: 0.00002755
Iteration 187/1000 | Loss: 0.00002755
Iteration 188/1000 | Loss: 0.00002755
Iteration 189/1000 | Loss: 0.00002755
Iteration 190/1000 | Loss: 0.00002755
Iteration 191/1000 | Loss: 0.00002755
Iteration 192/1000 | Loss: 0.00002755
Iteration 193/1000 | Loss: 0.00002755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.755231616902165e-05, 2.755231616902165e-05, 2.755231616902165e-05, 2.755231616902165e-05, 2.755231616902165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.755231616902165e-05

Optimization complete. Final v2v error: 4.237748146057129 mm

Highest mean error: 4.573352813720703 mm for frame 86

Lowest mean error: 3.355104684829712 mm for frame 0

Saving results

Total time: 42.269065618515015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479240
Iteration 2/25 | Loss: 0.00130645
Iteration 3/25 | Loss: 0.00122868
Iteration 4/25 | Loss: 0.00121409
Iteration 5/25 | Loss: 0.00120874
Iteration 6/25 | Loss: 0.00120764
Iteration 7/25 | Loss: 0.00120764
Iteration 8/25 | Loss: 0.00120764
Iteration 9/25 | Loss: 0.00120764
Iteration 10/25 | Loss: 0.00120764
Iteration 11/25 | Loss: 0.00120764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001207637251354754, 0.001207637251354754, 0.001207637251354754, 0.001207637251354754, 0.001207637251354754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001207637251354754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38353491
Iteration 2/25 | Loss: 0.00076187
Iteration 3/25 | Loss: 0.00076187
Iteration 4/25 | Loss: 0.00076187
Iteration 5/25 | Loss: 0.00076187
Iteration 6/25 | Loss: 0.00076187
Iteration 7/25 | Loss: 0.00076187
Iteration 8/25 | Loss: 0.00076187
Iteration 9/25 | Loss: 0.00076186
Iteration 10/25 | Loss: 0.00076186
Iteration 11/25 | Loss: 0.00076186
Iteration 12/25 | Loss: 0.00076186
Iteration 13/25 | Loss: 0.00076186
Iteration 14/25 | Loss: 0.00076186
Iteration 15/25 | Loss: 0.00076186
Iteration 16/25 | Loss: 0.00076186
Iteration 17/25 | Loss: 0.00076186
Iteration 18/25 | Loss: 0.00076186
Iteration 19/25 | Loss: 0.00076186
Iteration 20/25 | Loss: 0.00076186
Iteration 21/25 | Loss: 0.00076186
Iteration 22/25 | Loss: 0.00076186
Iteration 23/25 | Loss: 0.00076186
Iteration 24/25 | Loss: 0.00076186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007618644740432501, 0.0007618644740432501, 0.0007618644740432501, 0.0007618644740432501, 0.0007618644740432501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007618644740432501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076186
Iteration 2/1000 | Loss: 0.00003565
Iteration 3/1000 | Loss: 0.00002082
Iteration 4/1000 | Loss: 0.00001814
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00001497
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001449
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001427
Iteration 16/1000 | Loss: 0.00001427
Iteration 17/1000 | Loss: 0.00001426
Iteration 18/1000 | Loss: 0.00001426
Iteration 19/1000 | Loss: 0.00001426
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001400
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001400
Iteration 38/1000 | Loss: 0.00001400
Iteration 39/1000 | Loss: 0.00001400
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001395
Iteration 44/1000 | Loss: 0.00001395
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00001395
Iteration 48/1000 | Loss: 0.00001395
Iteration 49/1000 | Loss: 0.00001395
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001381
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001380
Iteration 86/1000 | Loss: 0.00001380
Iteration 87/1000 | Loss: 0.00001380
Iteration 88/1000 | Loss: 0.00001380
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001368
Iteration 125/1000 | Loss: 0.00001368
Iteration 126/1000 | Loss: 0.00001368
Iteration 127/1000 | Loss: 0.00001368
Iteration 128/1000 | Loss: 0.00001368
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001365
Iteration 142/1000 | Loss: 0.00001365
Iteration 143/1000 | Loss: 0.00001365
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001364
Iteration 147/1000 | Loss: 0.00001364
Iteration 148/1000 | Loss: 0.00001364
Iteration 149/1000 | Loss: 0.00001364
Iteration 150/1000 | Loss: 0.00001364
Iteration 151/1000 | Loss: 0.00001364
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001364
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001364
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001364
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001364
Iteration 161/1000 | Loss: 0.00001364
Iteration 162/1000 | Loss: 0.00001364
Iteration 163/1000 | Loss: 0.00001364
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Iteration 167/1000 | Loss: 0.00001363
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001363
Iteration 180/1000 | Loss: 0.00001363
Iteration 181/1000 | Loss: 0.00001363
Iteration 182/1000 | Loss: 0.00001363
Iteration 183/1000 | Loss: 0.00001363
Iteration 184/1000 | Loss: 0.00001363
Iteration 185/1000 | Loss: 0.00001363
Iteration 186/1000 | Loss: 0.00001363
Iteration 187/1000 | Loss: 0.00001363
Iteration 188/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.3633731214213185e-05, 1.3633731214213185e-05, 1.3633731214213185e-05, 1.3633731214213185e-05, 1.3633731214213185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3633731214213185e-05

Optimization complete. Final v2v error: 3.1818530559539795 mm

Highest mean error: 3.5209522247314453 mm for frame 108

Lowest mean error: 2.9970779418945312 mm for frame 6

Saving results

Total time: 39.91805696487427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020843
Iteration 2/25 | Loss: 0.00255187
Iteration 3/25 | Loss: 0.00177890
Iteration 4/25 | Loss: 0.00169728
Iteration 5/25 | Loss: 0.00181680
Iteration 6/25 | Loss: 0.00168329
Iteration 7/25 | Loss: 0.00166526
Iteration 8/25 | Loss: 0.00141947
Iteration 9/25 | Loss: 0.00149573
Iteration 10/25 | Loss: 0.00141503
Iteration 11/25 | Loss: 0.00137058
Iteration 12/25 | Loss: 0.00136322
Iteration 13/25 | Loss: 0.00132016
Iteration 14/25 | Loss: 0.00130260
Iteration 15/25 | Loss: 0.00127968
Iteration 16/25 | Loss: 0.00132052
Iteration 17/25 | Loss: 0.00126843
Iteration 18/25 | Loss: 0.00126823
Iteration 19/25 | Loss: 0.00126422
Iteration 20/25 | Loss: 0.00126377
Iteration 21/25 | Loss: 0.00126259
Iteration 22/25 | Loss: 0.00125829
Iteration 23/25 | Loss: 0.00125850
Iteration 24/25 | Loss: 0.00125656
Iteration 25/25 | Loss: 0.00125933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53309488
Iteration 2/25 | Loss: 0.00112618
Iteration 3/25 | Loss: 0.00108462
Iteration 4/25 | Loss: 0.00108462
Iteration 5/25 | Loss: 0.00108462
Iteration 6/25 | Loss: 0.00108462
Iteration 7/25 | Loss: 0.00108462
Iteration 8/25 | Loss: 0.00108462
Iteration 9/25 | Loss: 0.00108462
Iteration 10/25 | Loss: 0.00108462
Iteration 11/25 | Loss: 0.00108462
Iteration 12/25 | Loss: 0.00108462
Iteration 13/25 | Loss: 0.00108462
Iteration 14/25 | Loss: 0.00108462
Iteration 15/25 | Loss: 0.00108462
Iteration 16/25 | Loss: 0.00108462
Iteration 17/25 | Loss: 0.00108462
Iteration 18/25 | Loss: 0.00108461
Iteration 19/25 | Loss: 0.00108462
Iteration 20/25 | Loss: 0.00108461
Iteration 21/25 | Loss: 0.00108461
Iteration 22/25 | Loss: 0.00108461
Iteration 23/25 | Loss: 0.00108461
Iteration 24/25 | Loss: 0.00108461
Iteration 25/25 | Loss: 0.00108461
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010846148943528533, 0.0010846148943528533, 0.0010846148943528533, 0.0010846148943528533, 0.0010846148943528533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010846148943528533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108461
Iteration 2/1000 | Loss: 0.00041157
Iteration 3/1000 | Loss: 0.00026972
Iteration 4/1000 | Loss: 0.00045091
Iteration 5/1000 | Loss: 0.00041908
Iteration 6/1000 | Loss: 0.00057576
Iteration 7/1000 | Loss: 0.00033828
Iteration 8/1000 | Loss: 0.00029151
Iteration 9/1000 | Loss: 0.00037766
Iteration 10/1000 | Loss: 0.00038792
Iteration 11/1000 | Loss: 0.00048678
Iteration 12/1000 | Loss: 0.00037347
Iteration 13/1000 | Loss: 0.00041563
Iteration 14/1000 | Loss: 0.00036106
Iteration 15/1000 | Loss: 0.00044347
Iteration 16/1000 | Loss: 0.00080796
Iteration 17/1000 | Loss: 0.00052230
Iteration 18/1000 | Loss: 0.00052615
Iteration 19/1000 | Loss: 0.00046744
Iteration 20/1000 | Loss: 0.00044047
Iteration 21/1000 | Loss: 0.00054434
Iteration 22/1000 | Loss: 0.00056638
Iteration 23/1000 | Loss: 0.00048585
Iteration 24/1000 | Loss: 0.00092999
Iteration 25/1000 | Loss: 0.00048499
Iteration 26/1000 | Loss: 0.00036970
Iteration 27/1000 | Loss: 0.00047879
Iteration 28/1000 | Loss: 0.00054617
Iteration 29/1000 | Loss: 0.00042622
Iteration 30/1000 | Loss: 0.00021691
Iteration 31/1000 | Loss: 0.00046762
Iteration 32/1000 | Loss: 0.00055732
Iteration 33/1000 | Loss: 0.00051635
Iteration 34/1000 | Loss: 0.00053778
Iteration 35/1000 | Loss: 0.00040626
Iteration 36/1000 | Loss: 0.00033979
Iteration 37/1000 | Loss: 0.00040012
Iteration 38/1000 | Loss: 0.00034025
Iteration 39/1000 | Loss: 0.00050973
Iteration 40/1000 | Loss: 0.00046078
Iteration 41/1000 | Loss: 0.00046090
Iteration 42/1000 | Loss: 0.00036180
Iteration 43/1000 | Loss: 0.00051371
Iteration 44/1000 | Loss: 0.00052123
Iteration 45/1000 | Loss: 0.00054550
Iteration 46/1000 | Loss: 0.00047256
Iteration 47/1000 | Loss: 0.00080708
Iteration 48/1000 | Loss: 0.00048175
Iteration 49/1000 | Loss: 0.00076020
Iteration 50/1000 | Loss: 0.00056391
Iteration 51/1000 | Loss: 0.00077821
Iteration 52/1000 | Loss: 0.00056383
Iteration 53/1000 | Loss: 0.00075377
Iteration 54/1000 | Loss: 0.00044772
Iteration 55/1000 | Loss: 0.00066465
Iteration 56/1000 | Loss: 0.00037588
Iteration 57/1000 | Loss: 0.00040544
Iteration 58/1000 | Loss: 0.00058892
Iteration 59/1000 | Loss: 0.00031092
Iteration 60/1000 | Loss: 0.00045589
Iteration 61/1000 | Loss: 0.00027100
Iteration 62/1000 | Loss: 0.00043135
Iteration 63/1000 | Loss: 0.00015112
Iteration 64/1000 | Loss: 0.00036584
Iteration 65/1000 | Loss: 0.00011339
Iteration 66/1000 | Loss: 0.00026809
Iteration 67/1000 | Loss: 0.00009058
Iteration 68/1000 | Loss: 0.00019434
Iteration 69/1000 | Loss: 0.00019832
Iteration 70/1000 | Loss: 0.00016129
Iteration 71/1000 | Loss: 0.00053063
Iteration 72/1000 | Loss: 0.00047184
Iteration 73/1000 | Loss: 0.00020737
Iteration 74/1000 | Loss: 0.00024454
Iteration 75/1000 | Loss: 0.00019463
Iteration 76/1000 | Loss: 0.00026948
Iteration 77/1000 | Loss: 0.00032163
Iteration 78/1000 | Loss: 0.00105140
Iteration 79/1000 | Loss: 0.00070183
Iteration 80/1000 | Loss: 0.00062024
Iteration 81/1000 | Loss: 0.00046951
Iteration 82/1000 | Loss: 0.00025316
Iteration 83/1000 | Loss: 0.00050901
Iteration 84/1000 | Loss: 0.00054748
Iteration 85/1000 | Loss: 0.00038010
Iteration 86/1000 | Loss: 0.00025731
Iteration 87/1000 | Loss: 0.00036277
Iteration 88/1000 | Loss: 0.00047950
Iteration 89/1000 | Loss: 0.00036562
Iteration 90/1000 | Loss: 0.00069273
Iteration 91/1000 | Loss: 0.00025585
Iteration 92/1000 | Loss: 0.00031874
Iteration 93/1000 | Loss: 0.00037914
Iteration 94/1000 | Loss: 0.00031518
Iteration 95/1000 | Loss: 0.00074050
Iteration 96/1000 | Loss: 0.00021315
Iteration 97/1000 | Loss: 0.00046257
Iteration 98/1000 | Loss: 0.00037325
Iteration 99/1000 | Loss: 0.00034698
Iteration 100/1000 | Loss: 0.00037396
Iteration 101/1000 | Loss: 0.00042432
Iteration 102/1000 | Loss: 0.00041635
Iteration 103/1000 | Loss: 0.00019981
Iteration 104/1000 | Loss: 0.00014856
Iteration 105/1000 | Loss: 0.00007251
Iteration 106/1000 | Loss: 0.00008894
Iteration 107/1000 | Loss: 0.00010607
Iteration 108/1000 | Loss: 0.00009917
Iteration 109/1000 | Loss: 0.00017637
Iteration 110/1000 | Loss: 0.00057783
Iteration 111/1000 | Loss: 0.00121260
Iteration 112/1000 | Loss: 0.00012455
Iteration 113/1000 | Loss: 0.00009092
Iteration 114/1000 | Loss: 0.00020033
Iteration 115/1000 | Loss: 0.00036053
Iteration 116/1000 | Loss: 0.00033685
Iteration 117/1000 | Loss: 0.00023513
Iteration 118/1000 | Loss: 0.00064439
Iteration 119/1000 | Loss: 0.00038612
Iteration 120/1000 | Loss: 0.00020325
Iteration 121/1000 | Loss: 0.00009269
Iteration 122/1000 | Loss: 0.00005390
Iteration 123/1000 | Loss: 0.00033458
Iteration 124/1000 | Loss: 0.00058844
Iteration 125/1000 | Loss: 0.00006432
Iteration 126/1000 | Loss: 0.00013181
Iteration 127/1000 | Loss: 0.00036154
Iteration 128/1000 | Loss: 0.00062501
Iteration 129/1000 | Loss: 0.00040749
Iteration 130/1000 | Loss: 0.00035881
Iteration 131/1000 | Loss: 0.00042192
Iteration 132/1000 | Loss: 0.00031779
Iteration 133/1000 | Loss: 0.00012449
Iteration 134/1000 | Loss: 0.00011627
Iteration 135/1000 | Loss: 0.00013367
Iteration 136/1000 | Loss: 0.00009611
Iteration 137/1000 | Loss: 0.00016697
Iteration 138/1000 | Loss: 0.00013746
Iteration 139/1000 | Loss: 0.00014531
Iteration 140/1000 | Loss: 0.00011473
Iteration 141/1000 | Loss: 0.00011190
Iteration 142/1000 | Loss: 0.00008064
Iteration 143/1000 | Loss: 0.00012332
Iteration 144/1000 | Loss: 0.00013419
Iteration 145/1000 | Loss: 0.00018034
Iteration 146/1000 | Loss: 0.00060094
Iteration 147/1000 | Loss: 0.00019709
Iteration 148/1000 | Loss: 0.00005429
Iteration 149/1000 | Loss: 0.00008583
Iteration 150/1000 | Loss: 0.00009322
Iteration 151/1000 | Loss: 0.00009462
Iteration 152/1000 | Loss: 0.00009261
Iteration 153/1000 | Loss: 0.00028960
Iteration 154/1000 | Loss: 0.00015081
Iteration 155/1000 | Loss: 0.00021913
Iteration 156/1000 | Loss: 0.00014387
Iteration 157/1000 | Loss: 0.00055726
Iteration 158/1000 | Loss: 0.00015609
Iteration 159/1000 | Loss: 0.00015852
Iteration 160/1000 | Loss: 0.00012743
Iteration 161/1000 | Loss: 0.00022173
Iteration 162/1000 | Loss: 0.00013323
Iteration 163/1000 | Loss: 0.00017884
Iteration 164/1000 | Loss: 0.00045573
Iteration 165/1000 | Loss: 0.00064384
Iteration 166/1000 | Loss: 0.00032813
Iteration 167/1000 | Loss: 0.00024576
Iteration 168/1000 | Loss: 0.00013026
Iteration 169/1000 | Loss: 0.00026411
Iteration 170/1000 | Loss: 0.00011019
Iteration 171/1000 | Loss: 0.00014620
Iteration 172/1000 | Loss: 0.00012424
Iteration 173/1000 | Loss: 0.00015391
Iteration 174/1000 | Loss: 0.00015768
Iteration 175/1000 | Loss: 0.00008278
Iteration 176/1000 | Loss: 0.00016913
Iteration 177/1000 | Loss: 0.00011160
Iteration 178/1000 | Loss: 0.00007675
Iteration 179/1000 | Loss: 0.00008285
Iteration 180/1000 | Loss: 0.00008580
Iteration 181/1000 | Loss: 0.00010937
Iteration 182/1000 | Loss: 0.00006883
Iteration 183/1000 | Loss: 0.00008885
Iteration 184/1000 | Loss: 0.00008250
Iteration 185/1000 | Loss: 0.00008986
Iteration 186/1000 | Loss: 0.00006193
Iteration 187/1000 | Loss: 0.00002704
Iteration 188/1000 | Loss: 0.00008251
Iteration 189/1000 | Loss: 0.00007792
Iteration 190/1000 | Loss: 0.00009957
Iteration 191/1000 | Loss: 0.00008419
Iteration 192/1000 | Loss: 0.00007466
Iteration 193/1000 | Loss: 0.00007797
Iteration 194/1000 | Loss: 0.00008423
Iteration 195/1000 | Loss: 0.00008017
Iteration 196/1000 | Loss: 0.00008578
Iteration 197/1000 | Loss: 0.00009413
Iteration 198/1000 | Loss: 0.00008915
Iteration 199/1000 | Loss: 0.00008499
Iteration 200/1000 | Loss: 0.00009922
Iteration 201/1000 | Loss: 0.00009082
Iteration 202/1000 | Loss: 0.00008802
Iteration 203/1000 | Loss: 0.00008845
Iteration 204/1000 | Loss: 0.00009099
Iteration 205/1000 | Loss: 0.00009050
Iteration 206/1000 | Loss: 0.00008358
Iteration 207/1000 | Loss: 0.00009933
Iteration 208/1000 | Loss: 0.00003740
Iteration 209/1000 | Loss: 0.00009387
Iteration 210/1000 | Loss: 0.00008432
Iteration 211/1000 | Loss: 0.00008089
Iteration 212/1000 | Loss: 0.00009810
Iteration 213/1000 | Loss: 0.00007443
Iteration 214/1000 | Loss: 0.00010692
Iteration 215/1000 | Loss: 0.00008178
Iteration 216/1000 | Loss: 0.00010829
Iteration 217/1000 | Loss: 0.00007492
Iteration 218/1000 | Loss: 0.00007877
Iteration 219/1000 | Loss: 0.00004380
Iteration 220/1000 | Loss: 0.00006477
Iteration 221/1000 | Loss: 0.00008410
Iteration 222/1000 | Loss: 0.00005840
Iteration 223/1000 | Loss: 0.00009721
Iteration 224/1000 | Loss: 0.00008480
Iteration 225/1000 | Loss: 0.00010224
Iteration 226/1000 | Loss: 0.00009810
Iteration 227/1000 | Loss: 0.00007936
Iteration 228/1000 | Loss: 0.00007741
Iteration 229/1000 | Loss: 0.00008195
Iteration 230/1000 | Loss: 0.00011629
Iteration 231/1000 | Loss: 0.00011084
Iteration 232/1000 | Loss: 0.00012106
Iteration 233/1000 | Loss: 0.00004610
Iteration 234/1000 | Loss: 0.00005492
Iteration 235/1000 | Loss: 0.00005248
Iteration 236/1000 | Loss: 0.00005239
Iteration 237/1000 | Loss: 0.00006414
Iteration 238/1000 | Loss: 0.00005649
Iteration 239/1000 | Loss: 0.00005332
Iteration 240/1000 | Loss: 0.00005556
Iteration 241/1000 | Loss: 0.00005118
Iteration 242/1000 | Loss: 0.00003964
Iteration 243/1000 | Loss: 0.00005900
Iteration 244/1000 | Loss: 0.00003961
Iteration 245/1000 | Loss: 0.00003309
Iteration 246/1000 | Loss: 0.00002388
Iteration 247/1000 | Loss: 0.00002803
Iteration 248/1000 | Loss: 0.00002063
Iteration 249/1000 | Loss: 0.00002836
Iteration 250/1000 | Loss: 0.00002020
Iteration 251/1000 | Loss: 0.00001893
Iteration 252/1000 | Loss: 0.00002647
Iteration 253/1000 | Loss: 0.00003439
Iteration 254/1000 | Loss: 0.00001877
Iteration 255/1000 | Loss: 0.00002817
Iteration 256/1000 | Loss: 0.00001790
Iteration 257/1000 | Loss: 0.00002147
Iteration 258/1000 | Loss: 0.00001795
Iteration 259/1000 | Loss: 0.00001973
Iteration 260/1000 | Loss: 0.00008745
Iteration 261/1000 | Loss: 0.00004192
Iteration 262/1000 | Loss: 0.00001736
Iteration 263/1000 | Loss: 0.00002217
Iteration 264/1000 | Loss: 0.00001783
Iteration 265/1000 | Loss: 0.00002211
Iteration 266/1000 | Loss: 0.00002081
Iteration 267/1000 | Loss: 0.00001782
Iteration 268/1000 | Loss: 0.00001728
Iteration 269/1000 | Loss: 0.00001728
Iteration 270/1000 | Loss: 0.00001728
Iteration 271/1000 | Loss: 0.00001728
Iteration 272/1000 | Loss: 0.00001728
Iteration 273/1000 | Loss: 0.00001728
Iteration 274/1000 | Loss: 0.00001814
Iteration 275/1000 | Loss: 0.00001712
Iteration 276/1000 | Loss: 0.00001712
Iteration 277/1000 | Loss: 0.00001712
Iteration 278/1000 | Loss: 0.00001712
Iteration 279/1000 | Loss: 0.00001711
Iteration 280/1000 | Loss: 0.00001711
Iteration 281/1000 | Loss: 0.00001711
Iteration 282/1000 | Loss: 0.00001711
Iteration 283/1000 | Loss: 0.00001711
Iteration 284/1000 | Loss: 0.00001711
Iteration 285/1000 | Loss: 0.00001710
Iteration 286/1000 | Loss: 0.00001710
Iteration 287/1000 | Loss: 0.00001710
Iteration 288/1000 | Loss: 0.00001710
Iteration 289/1000 | Loss: 0.00001709
Iteration 290/1000 | Loss: 0.00001709
Iteration 291/1000 | Loss: 0.00001708
Iteration 292/1000 | Loss: 0.00001708
Iteration 293/1000 | Loss: 0.00001707
Iteration 294/1000 | Loss: 0.00001707
Iteration 295/1000 | Loss: 0.00001707
Iteration 296/1000 | Loss: 0.00001706
Iteration 297/1000 | Loss: 0.00001706
Iteration 298/1000 | Loss: 0.00001706
Iteration 299/1000 | Loss: 0.00001705
Iteration 300/1000 | Loss: 0.00001705
Iteration 301/1000 | Loss: 0.00001705
Iteration 302/1000 | Loss: 0.00001704
Iteration 303/1000 | Loss: 0.00001704
Iteration 304/1000 | Loss: 0.00001832
Iteration 305/1000 | Loss: 0.00001702
Iteration 306/1000 | Loss: 0.00001702
Iteration 307/1000 | Loss: 0.00001702
Iteration 308/1000 | Loss: 0.00001702
Iteration 309/1000 | Loss: 0.00001702
Iteration 310/1000 | Loss: 0.00001702
Iteration 311/1000 | Loss: 0.00001702
Iteration 312/1000 | Loss: 0.00001702
Iteration 313/1000 | Loss: 0.00001702
Iteration 314/1000 | Loss: 0.00001702
Iteration 315/1000 | Loss: 0.00001762
Iteration 316/1000 | Loss: 0.00001702
Iteration 317/1000 | Loss: 0.00001701
Iteration 318/1000 | Loss: 0.00001701
Iteration 319/1000 | Loss: 0.00001701
Iteration 320/1000 | Loss: 0.00001701
Iteration 321/1000 | Loss: 0.00001701
Iteration 322/1000 | Loss: 0.00001701
Iteration 323/1000 | Loss: 0.00001701
Iteration 324/1000 | Loss: 0.00001701
Iteration 325/1000 | Loss: 0.00001701
Iteration 326/1000 | Loss: 0.00001701
Iteration 327/1000 | Loss: 0.00001701
Iteration 328/1000 | Loss: 0.00001701
Iteration 329/1000 | Loss: 0.00001701
Iteration 330/1000 | Loss: 0.00001701
Iteration 331/1000 | Loss: 0.00001701
Iteration 332/1000 | Loss: 0.00001743
Iteration 333/1000 | Loss: 0.00001711
Iteration 334/1000 | Loss: 0.00001724
Iteration 335/1000 | Loss: 0.00001700
Iteration 336/1000 | Loss: 0.00001699
Iteration 337/1000 | Loss: 0.00001699
Iteration 338/1000 | Loss: 0.00001703
Iteration 339/1000 | Loss: 0.00001702
Iteration 340/1000 | Loss: 0.00001699
Iteration 341/1000 | Loss: 0.00001698
Iteration 342/1000 | Loss: 0.00001698
Iteration 343/1000 | Loss: 0.00001698
Iteration 344/1000 | Loss: 0.00001698
Iteration 345/1000 | Loss: 0.00001698
Iteration 346/1000 | Loss: 0.00001698
Iteration 347/1000 | Loss: 0.00001698
Iteration 348/1000 | Loss: 0.00001698
Iteration 349/1000 | Loss: 0.00001698
Iteration 350/1000 | Loss: 0.00001698
Iteration 351/1000 | Loss: 0.00001698
Iteration 352/1000 | Loss: 0.00001698
Iteration 353/1000 | Loss: 0.00001698
Iteration 354/1000 | Loss: 0.00001698
Iteration 355/1000 | Loss: 0.00001698
Iteration 356/1000 | Loss: 0.00001698
Iteration 357/1000 | Loss: 0.00001698
Iteration 358/1000 | Loss: 0.00001698
Iteration 359/1000 | Loss: 0.00001698
Iteration 360/1000 | Loss: 0.00001698
Iteration 361/1000 | Loss: 0.00001698
Iteration 362/1000 | Loss: 0.00001698
Iteration 363/1000 | Loss: 0.00001698
Iteration 364/1000 | Loss: 0.00001698
Iteration 365/1000 | Loss: 0.00001698
Iteration 366/1000 | Loss: 0.00001698
Iteration 367/1000 | Loss: 0.00001698
Iteration 368/1000 | Loss: 0.00001698
Iteration 369/1000 | Loss: 0.00001698
Iteration 370/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 370. Stopping optimization.
Last 5 losses: [1.697853258519899e-05, 1.697853258519899e-05, 1.697853258519899e-05, 1.697853258519899e-05, 1.697853258519899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.697853258519899e-05

Optimization complete. Final v2v error: 3.445688009262085 mm

Highest mean error: 5.005812644958496 mm for frame 68

Lowest mean error: 2.9526307582855225 mm for frame 104

Saving results

Total time: 415.33439660072327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841872
Iteration 2/25 | Loss: 0.00132665
Iteration 3/25 | Loss: 0.00122586
Iteration 4/25 | Loss: 0.00121513
Iteration 5/25 | Loss: 0.00121276
Iteration 6/25 | Loss: 0.00121276
Iteration 7/25 | Loss: 0.00121276
Iteration 8/25 | Loss: 0.00121276
Iteration 9/25 | Loss: 0.00121276
Iteration 10/25 | Loss: 0.00121276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012127639492973685, 0.0012127639492973685, 0.0012127639492973685, 0.0012127639492973685, 0.0012127639492973685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012127639492973685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.55323982
Iteration 2/25 | Loss: 0.00075162
Iteration 3/25 | Loss: 0.00075161
Iteration 4/25 | Loss: 0.00075160
Iteration 5/25 | Loss: 0.00075160
Iteration 6/25 | Loss: 0.00075160
Iteration 7/25 | Loss: 0.00075160
Iteration 8/25 | Loss: 0.00075160
Iteration 9/25 | Loss: 0.00075160
Iteration 10/25 | Loss: 0.00075160
Iteration 11/25 | Loss: 0.00075160
Iteration 12/25 | Loss: 0.00075160
Iteration 13/25 | Loss: 0.00075160
Iteration 14/25 | Loss: 0.00075160
Iteration 15/25 | Loss: 0.00075160
Iteration 16/25 | Loss: 0.00075160
Iteration 17/25 | Loss: 0.00075160
Iteration 18/25 | Loss: 0.00075160
Iteration 19/25 | Loss: 0.00075160
Iteration 20/25 | Loss: 0.00075160
Iteration 21/25 | Loss: 0.00075160
Iteration 22/25 | Loss: 0.00075160
Iteration 23/25 | Loss: 0.00075160
Iteration 24/25 | Loss: 0.00075160
Iteration 25/25 | Loss: 0.00075160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075160
Iteration 2/1000 | Loss: 0.00003052
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002023
Iteration 5/1000 | Loss: 0.00001910
Iteration 6/1000 | Loss: 0.00001813
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001632
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001593
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001584
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001568
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001564
Iteration 53/1000 | Loss: 0.00001564
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001561
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001559
Iteration 73/1000 | Loss: 0.00001559
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001551
Iteration 105/1000 | Loss: 0.00001551
Iteration 106/1000 | Loss: 0.00001551
Iteration 107/1000 | Loss: 0.00001551
Iteration 108/1000 | Loss: 0.00001551
Iteration 109/1000 | Loss: 0.00001551
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.5510799130424857e-05, 1.5510799130424857e-05, 1.5510799130424857e-05, 1.5510799130424857e-05, 1.5510799130424857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5510799130424857e-05

Optimization complete. Final v2v error: 3.3452205657958984 mm

Highest mean error: 3.691249370574951 mm for frame 111

Lowest mean error: 2.9497623443603516 mm for frame 167

Saving results

Total time: 35.40084099769592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805869
Iteration 2/25 | Loss: 0.00129787
Iteration 3/25 | Loss: 0.00120350
Iteration 4/25 | Loss: 0.00119373
Iteration 5/25 | Loss: 0.00119148
Iteration 6/25 | Loss: 0.00119134
Iteration 7/25 | Loss: 0.00119134
Iteration 8/25 | Loss: 0.00119134
Iteration 9/25 | Loss: 0.00119134
Iteration 10/25 | Loss: 0.00119134
Iteration 11/25 | Loss: 0.00119134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011913430644199252, 0.0011913430644199252, 0.0011913430644199252, 0.0011913430644199252, 0.0011913430644199252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011913430644199252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44078267
Iteration 2/25 | Loss: 0.00070852
Iteration 3/25 | Loss: 0.00070852
Iteration 4/25 | Loss: 0.00070852
Iteration 5/25 | Loss: 0.00070852
Iteration 6/25 | Loss: 0.00070852
Iteration 7/25 | Loss: 0.00070852
Iteration 8/25 | Loss: 0.00070852
Iteration 9/25 | Loss: 0.00070852
Iteration 10/25 | Loss: 0.00070852
Iteration 11/25 | Loss: 0.00070852
Iteration 12/25 | Loss: 0.00070852
Iteration 13/25 | Loss: 0.00070852
Iteration 14/25 | Loss: 0.00070852
Iteration 15/25 | Loss: 0.00070852
Iteration 16/25 | Loss: 0.00070852
Iteration 17/25 | Loss: 0.00070852
Iteration 18/25 | Loss: 0.00070852
Iteration 19/25 | Loss: 0.00070852
Iteration 20/25 | Loss: 0.00070852
Iteration 21/25 | Loss: 0.00070852
Iteration 22/25 | Loss: 0.00070852
Iteration 23/25 | Loss: 0.00070852
Iteration 24/25 | Loss: 0.00070852
Iteration 25/25 | Loss: 0.00070852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070852
Iteration 2/1000 | Loss: 0.00002609
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00001443
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001309
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001219
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001218
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001207
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001188
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001164
Iteration 107/1000 | Loss: 0.00001164
Iteration 108/1000 | Loss: 0.00001164
Iteration 109/1000 | Loss: 0.00001163
Iteration 110/1000 | Loss: 0.00001163
Iteration 111/1000 | Loss: 0.00001163
Iteration 112/1000 | Loss: 0.00001162
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001156
Iteration 136/1000 | Loss: 0.00001156
Iteration 137/1000 | Loss: 0.00001156
Iteration 138/1000 | Loss: 0.00001156
Iteration 139/1000 | Loss: 0.00001156
Iteration 140/1000 | Loss: 0.00001156
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001155
Iteration 144/1000 | Loss: 0.00001155
Iteration 145/1000 | Loss: 0.00001155
Iteration 146/1000 | Loss: 0.00001155
Iteration 147/1000 | Loss: 0.00001155
Iteration 148/1000 | Loss: 0.00001155
Iteration 149/1000 | Loss: 0.00001155
Iteration 150/1000 | Loss: 0.00001155
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001154
Iteration 157/1000 | Loss: 0.00001154
Iteration 158/1000 | Loss: 0.00001154
Iteration 159/1000 | Loss: 0.00001154
Iteration 160/1000 | Loss: 0.00001154
Iteration 161/1000 | Loss: 0.00001154
Iteration 162/1000 | Loss: 0.00001154
Iteration 163/1000 | Loss: 0.00001154
Iteration 164/1000 | Loss: 0.00001154
Iteration 165/1000 | Loss: 0.00001154
Iteration 166/1000 | Loss: 0.00001154
Iteration 167/1000 | Loss: 0.00001154
Iteration 168/1000 | Loss: 0.00001154
Iteration 169/1000 | Loss: 0.00001153
Iteration 170/1000 | Loss: 0.00001153
Iteration 171/1000 | Loss: 0.00001153
Iteration 172/1000 | Loss: 0.00001153
Iteration 173/1000 | Loss: 0.00001153
Iteration 174/1000 | Loss: 0.00001153
Iteration 175/1000 | Loss: 0.00001153
Iteration 176/1000 | Loss: 0.00001153
Iteration 177/1000 | Loss: 0.00001152
Iteration 178/1000 | Loss: 0.00001152
Iteration 179/1000 | Loss: 0.00001152
Iteration 180/1000 | Loss: 0.00001152
Iteration 181/1000 | Loss: 0.00001152
Iteration 182/1000 | Loss: 0.00001152
Iteration 183/1000 | Loss: 0.00001152
Iteration 184/1000 | Loss: 0.00001152
Iteration 185/1000 | Loss: 0.00001151
Iteration 186/1000 | Loss: 0.00001151
Iteration 187/1000 | Loss: 0.00001151
Iteration 188/1000 | Loss: 0.00001151
Iteration 189/1000 | Loss: 0.00001151
Iteration 190/1000 | Loss: 0.00001151
Iteration 191/1000 | Loss: 0.00001151
Iteration 192/1000 | Loss: 0.00001150
Iteration 193/1000 | Loss: 0.00001150
Iteration 194/1000 | Loss: 0.00001150
Iteration 195/1000 | Loss: 0.00001150
Iteration 196/1000 | Loss: 0.00001150
Iteration 197/1000 | Loss: 0.00001150
Iteration 198/1000 | Loss: 0.00001150
Iteration 199/1000 | Loss: 0.00001150
Iteration 200/1000 | Loss: 0.00001150
Iteration 201/1000 | Loss: 0.00001150
Iteration 202/1000 | Loss: 0.00001150
Iteration 203/1000 | Loss: 0.00001150
Iteration 204/1000 | Loss: 0.00001150
Iteration 205/1000 | Loss: 0.00001150
Iteration 206/1000 | Loss: 0.00001150
Iteration 207/1000 | Loss: 0.00001150
Iteration 208/1000 | Loss: 0.00001150
Iteration 209/1000 | Loss: 0.00001150
Iteration 210/1000 | Loss: 0.00001150
Iteration 211/1000 | Loss: 0.00001150
Iteration 212/1000 | Loss: 0.00001149
Iteration 213/1000 | Loss: 0.00001149
Iteration 214/1000 | Loss: 0.00001149
Iteration 215/1000 | Loss: 0.00001149
Iteration 216/1000 | Loss: 0.00001149
Iteration 217/1000 | Loss: 0.00001149
Iteration 218/1000 | Loss: 0.00001149
Iteration 219/1000 | Loss: 0.00001149
Iteration 220/1000 | Loss: 0.00001149
Iteration 221/1000 | Loss: 0.00001149
Iteration 222/1000 | Loss: 0.00001149
Iteration 223/1000 | Loss: 0.00001149
Iteration 224/1000 | Loss: 0.00001149
Iteration 225/1000 | Loss: 0.00001148
Iteration 226/1000 | Loss: 0.00001148
Iteration 227/1000 | Loss: 0.00001148
Iteration 228/1000 | Loss: 0.00001148
Iteration 229/1000 | Loss: 0.00001148
Iteration 230/1000 | Loss: 0.00001148
Iteration 231/1000 | Loss: 0.00001148
Iteration 232/1000 | Loss: 0.00001148
Iteration 233/1000 | Loss: 0.00001148
Iteration 234/1000 | Loss: 0.00001148
Iteration 235/1000 | Loss: 0.00001148
Iteration 236/1000 | Loss: 0.00001148
Iteration 237/1000 | Loss: 0.00001148
Iteration 238/1000 | Loss: 0.00001148
Iteration 239/1000 | Loss: 0.00001148
Iteration 240/1000 | Loss: 0.00001148
Iteration 241/1000 | Loss: 0.00001148
Iteration 242/1000 | Loss: 0.00001147
Iteration 243/1000 | Loss: 0.00001147
Iteration 244/1000 | Loss: 0.00001147
Iteration 245/1000 | Loss: 0.00001147
Iteration 246/1000 | Loss: 0.00001147
Iteration 247/1000 | Loss: 0.00001147
Iteration 248/1000 | Loss: 0.00001147
Iteration 249/1000 | Loss: 0.00001147
Iteration 250/1000 | Loss: 0.00001146
Iteration 251/1000 | Loss: 0.00001146
Iteration 252/1000 | Loss: 0.00001146
Iteration 253/1000 | Loss: 0.00001146
Iteration 254/1000 | Loss: 0.00001146
Iteration 255/1000 | Loss: 0.00001146
Iteration 256/1000 | Loss: 0.00001146
Iteration 257/1000 | Loss: 0.00001146
Iteration 258/1000 | Loss: 0.00001146
Iteration 259/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.1463666851341259e-05, 1.1463666851341259e-05, 1.1463666851341259e-05, 1.1463666851341259e-05, 1.1463666851341259e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1463666851341259e-05

Optimization complete. Final v2v error: 2.883185863494873 mm

Highest mean error: 3.1009817123413086 mm for frame 56

Lowest mean error: 2.7075657844543457 mm for frame 146

Saving results

Total time: 42.85546684265137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796063
Iteration 2/25 | Loss: 0.00138407
Iteration 3/25 | Loss: 0.00121803
Iteration 4/25 | Loss: 0.00119446
Iteration 5/25 | Loss: 0.00118943
Iteration 6/25 | Loss: 0.00118904
Iteration 7/25 | Loss: 0.00118904
Iteration 8/25 | Loss: 0.00118904
Iteration 9/25 | Loss: 0.00118904
Iteration 10/25 | Loss: 0.00118904
Iteration 11/25 | Loss: 0.00118904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011890408350154757, 0.0011890408350154757, 0.0011890408350154757, 0.0011890408350154757, 0.0011890408350154757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011890408350154757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43676996
Iteration 2/25 | Loss: 0.00070939
Iteration 3/25 | Loss: 0.00070939
Iteration 4/25 | Loss: 0.00070939
Iteration 5/25 | Loss: 0.00070939
Iteration 6/25 | Loss: 0.00070939
Iteration 7/25 | Loss: 0.00070939
Iteration 8/25 | Loss: 0.00070939
Iteration 9/25 | Loss: 0.00070939
Iteration 10/25 | Loss: 0.00070939
Iteration 11/25 | Loss: 0.00070939
Iteration 12/25 | Loss: 0.00070939
Iteration 13/25 | Loss: 0.00070939
Iteration 14/25 | Loss: 0.00070939
Iteration 15/25 | Loss: 0.00070939
Iteration 16/25 | Loss: 0.00070939
Iteration 17/25 | Loss: 0.00070939
Iteration 18/25 | Loss: 0.00070939
Iteration 19/25 | Loss: 0.00070939
Iteration 20/25 | Loss: 0.00070939
Iteration 21/25 | Loss: 0.00070939
Iteration 22/25 | Loss: 0.00070939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007093893364071846, 0.0007093893364071846, 0.0007093893364071846, 0.0007093893364071846, 0.0007093893364071846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007093893364071846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070939
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001594
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001376
Iteration 7/1000 | Loss: 0.00001307
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001187
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001182
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001157
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001156
Iteration 42/1000 | Loss: 0.00001156
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001155
Iteration 48/1000 | Loss: 0.00001155
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001151
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001147
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001146
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001144
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001143
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001143
Iteration 119/1000 | Loss: 0.00001143
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001142
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001141
Iteration 141/1000 | Loss: 0.00001141
Iteration 142/1000 | Loss: 0.00001141
Iteration 143/1000 | Loss: 0.00001141
Iteration 144/1000 | Loss: 0.00001141
Iteration 145/1000 | Loss: 0.00001141
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001140
Iteration 148/1000 | Loss: 0.00001140
Iteration 149/1000 | Loss: 0.00001140
Iteration 150/1000 | Loss: 0.00001140
Iteration 151/1000 | Loss: 0.00001140
Iteration 152/1000 | Loss: 0.00001140
Iteration 153/1000 | Loss: 0.00001140
Iteration 154/1000 | Loss: 0.00001140
Iteration 155/1000 | Loss: 0.00001140
Iteration 156/1000 | Loss: 0.00001140
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.1386571713956073e-05, 1.1386571713956073e-05, 1.1386571713956073e-05, 1.1386571713956073e-05, 1.1386571713956073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1386571713956073e-05

Optimization complete. Final v2v error: 2.9054386615753174 mm

Highest mean error: 3.0781874656677246 mm for frame 109

Lowest mean error: 2.7625296115875244 mm for frame 130

Saving results

Total time: 41.167503356933594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027883
Iteration 2/25 | Loss: 0.00200097
Iteration 3/25 | Loss: 0.00200379
Iteration 4/25 | Loss: 0.00143193
Iteration 5/25 | Loss: 0.00140816
Iteration 6/25 | Loss: 0.00139613
Iteration 7/25 | Loss: 0.00136446
Iteration 8/25 | Loss: 0.00130509
Iteration 9/25 | Loss: 0.00127798
Iteration 10/25 | Loss: 0.00126301
Iteration 11/25 | Loss: 0.00124142
Iteration 12/25 | Loss: 0.00123403
Iteration 13/25 | Loss: 0.00123429
Iteration 14/25 | Loss: 0.00123254
Iteration 15/25 | Loss: 0.00123208
Iteration 16/25 | Loss: 0.00123157
Iteration 17/25 | Loss: 0.00122939
Iteration 18/25 | Loss: 0.00122881
Iteration 19/25 | Loss: 0.00122870
Iteration 20/25 | Loss: 0.00122870
Iteration 21/25 | Loss: 0.00122870
Iteration 22/25 | Loss: 0.00122870
Iteration 23/25 | Loss: 0.00122869
Iteration 24/25 | Loss: 0.00122869
Iteration 25/25 | Loss: 0.00122869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47835100
Iteration 2/25 | Loss: 0.00094581
Iteration 3/25 | Loss: 0.00080205
Iteration 4/25 | Loss: 0.00080204
Iteration 5/25 | Loss: 0.00080204
Iteration 6/25 | Loss: 0.00080204
Iteration 7/25 | Loss: 0.00080204
Iteration 8/25 | Loss: 0.00080204
Iteration 9/25 | Loss: 0.00080204
Iteration 10/25 | Loss: 0.00080204
Iteration 11/25 | Loss: 0.00080204
Iteration 12/25 | Loss: 0.00080204
Iteration 13/25 | Loss: 0.00080204
Iteration 14/25 | Loss: 0.00080204
Iteration 15/25 | Loss: 0.00080204
Iteration 16/25 | Loss: 0.00080204
Iteration 17/25 | Loss: 0.00080204
Iteration 18/25 | Loss: 0.00080204
Iteration 19/25 | Loss: 0.00080204
Iteration 20/25 | Loss: 0.00080204
Iteration 21/25 | Loss: 0.00080204
Iteration 22/25 | Loss: 0.00080204
Iteration 23/25 | Loss: 0.00080204
Iteration 24/25 | Loss: 0.00080204
Iteration 25/25 | Loss: 0.00080204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080204
Iteration 2/1000 | Loss: 0.00015308
Iteration 3/1000 | Loss: 0.00002103
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00071780
Iteration 6/1000 | Loss: 0.00005523
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00091742
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001604
Iteration 12/1000 | Loss: 0.00001578
Iteration 13/1000 | Loss: 0.00001566
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001561
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001529
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00005215
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00002654
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001504
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001501
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001501
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001499
Iteration 62/1000 | Loss: 0.00001499
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001498
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001497
Iteration 70/1000 | Loss: 0.00001497
Iteration 71/1000 | Loss: 0.00001497
Iteration 72/1000 | Loss: 0.00001496
Iteration 73/1000 | Loss: 0.00001496
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001495
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001485
Iteration 92/1000 | Loss: 0.00001485
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001484
Iteration 97/1000 | Loss: 0.00001484
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001483
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001480
Iteration 109/1000 | Loss: 0.00001480
Iteration 110/1000 | Loss: 0.00001480
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001479
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001479
Iteration 117/1000 | Loss: 0.00001479
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001473
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001472
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001472
Iteration 155/1000 | Loss: 0.00001472
Iteration 156/1000 | Loss: 0.00001472
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001472
Iteration 161/1000 | Loss: 0.00001472
Iteration 162/1000 | Loss: 0.00001472
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Iteration 165/1000 | Loss: 0.00001472
Iteration 166/1000 | Loss: 0.00001472
Iteration 167/1000 | Loss: 0.00001472
Iteration 168/1000 | Loss: 0.00001472
Iteration 169/1000 | Loss: 0.00001472
Iteration 170/1000 | Loss: 0.00001472
Iteration 171/1000 | Loss: 0.00001472
Iteration 172/1000 | Loss: 0.00001472
Iteration 173/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.4721238585480023e-05, 1.4721238585480023e-05, 1.4721238585480023e-05, 1.4721238585480023e-05, 1.4721238585480023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4721238585480023e-05

Optimization complete. Final v2v error: 3.2466328144073486 mm

Highest mean error: 4.007116794586182 mm for frame 57

Lowest mean error: 2.860786199569702 mm for frame 150

Saving results

Total time: 69.85904693603516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824294
Iteration 2/25 | Loss: 0.00134419
Iteration 3/25 | Loss: 0.00124394
Iteration 4/25 | Loss: 0.00123040
Iteration 5/25 | Loss: 0.00122613
Iteration 6/25 | Loss: 0.00122583
Iteration 7/25 | Loss: 0.00122583
Iteration 8/25 | Loss: 0.00122583
Iteration 9/25 | Loss: 0.00122583
Iteration 10/25 | Loss: 0.00122583
Iteration 11/25 | Loss: 0.00122583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001225834246724844, 0.001225834246724844, 0.001225834246724844, 0.001225834246724844, 0.001225834246724844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225834246724844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81599188
Iteration 2/25 | Loss: 0.00084248
Iteration 3/25 | Loss: 0.00084248
Iteration 4/25 | Loss: 0.00084248
Iteration 5/25 | Loss: 0.00084248
Iteration 6/25 | Loss: 0.00084247
Iteration 7/25 | Loss: 0.00084247
Iteration 8/25 | Loss: 0.00084247
Iteration 9/25 | Loss: 0.00084247
Iteration 10/25 | Loss: 0.00084247
Iteration 11/25 | Loss: 0.00084247
Iteration 12/25 | Loss: 0.00084247
Iteration 13/25 | Loss: 0.00084247
Iteration 14/25 | Loss: 0.00084247
Iteration 15/25 | Loss: 0.00084247
Iteration 16/25 | Loss: 0.00084247
Iteration 17/25 | Loss: 0.00084247
Iteration 18/25 | Loss: 0.00084247
Iteration 19/25 | Loss: 0.00084247
Iteration 20/25 | Loss: 0.00084247
Iteration 21/25 | Loss: 0.00084247
Iteration 22/25 | Loss: 0.00084247
Iteration 23/25 | Loss: 0.00084247
Iteration 24/25 | Loss: 0.00084247
Iteration 25/25 | Loss: 0.00084247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084247
Iteration 2/1000 | Loss: 0.00002474
Iteration 3/1000 | Loss: 0.00001888
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001579
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001356
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001354
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001351
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001330
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001329
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001329
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001326
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001319
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001318
Iteration 124/1000 | Loss: 0.00001318
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001316
Iteration 145/1000 | Loss: 0.00001316
Iteration 146/1000 | Loss: 0.00001316
Iteration 147/1000 | Loss: 0.00001316
Iteration 148/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3162463801563717e-05, 1.3162463801563717e-05, 1.3162463801563717e-05, 1.3162463801563717e-05, 1.3162463801563717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3162463801563717e-05

Optimization complete. Final v2v error: 3.0917885303497314 mm

Highest mean error: 3.82855486869812 mm for frame 105

Lowest mean error: 2.8520359992980957 mm for frame 71

Saving results

Total time: 36.67167019844055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391707
Iteration 2/25 | Loss: 0.00129309
Iteration 3/25 | Loss: 0.00121994
Iteration 4/25 | Loss: 0.00121361
Iteration 5/25 | Loss: 0.00121176
Iteration 6/25 | Loss: 0.00121163
Iteration 7/25 | Loss: 0.00121163
Iteration 8/25 | Loss: 0.00121163
Iteration 9/25 | Loss: 0.00121163
Iteration 10/25 | Loss: 0.00121163
Iteration 11/25 | Loss: 0.00121163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012116309953853488, 0.0012116309953853488, 0.0012116309953853488, 0.0012116309953853488, 0.0012116309953853488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012116309953853488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47117007
Iteration 2/25 | Loss: 0.00080843
Iteration 3/25 | Loss: 0.00080843
Iteration 4/25 | Loss: 0.00080843
Iteration 5/25 | Loss: 0.00080843
Iteration 6/25 | Loss: 0.00080843
Iteration 7/25 | Loss: 0.00080843
Iteration 8/25 | Loss: 0.00080843
Iteration 9/25 | Loss: 0.00080843
Iteration 10/25 | Loss: 0.00080843
Iteration 11/25 | Loss: 0.00080843
Iteration 12/25 | Loss: 0.00080843
Iteration 13/25 | Loss: 0.00080843
Iteration 14/25 | Loss: 0.00080843
Iteration 15/25 | Loss: 0.00080843
Iteration 16/25 | Loss: 0.00080843
Iteration 17/25 | Loss: 0.00080843
Iteration 18/25 | Loss: 0.00080843
Iteration 19/25 | Loss: 0.00080843
Iteration 20/25 | Loss: 0.00080843
Iteration 21/25 | Loss: 0.00080843
Iteration 22/25 | Loss: 0.00080843
Iteration 23/25 | Loss: 0.00080843
Iteration 24/25 | Loss: 0.00080843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008084280416369438, 0.0008084280416369438, 0.0008084280416369438, 0.0008084280416369438, 0.0008084280416369438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008084280416369438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080843
Iteration 2/1000 | Loss: 0.00002720
Iteration 3/1000 | Loss: 0.00001901
Iteration 4/1000 | Loss: 0.00001627
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001460
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001375
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001361
Iteration 17/1000 | Loss: 0.00001360
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001338
Iteration 40/1000 | Loss: 0.00001338
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001337
Iteration 43/1000 | Loss: 0.00001337
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001329
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Iteration 82/1000 | Loss: 0.00001328
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001326
Iteration 92/1000 | Loss: 0.00001326
Iteration 93/1000 | Loss: 0.00001326
Iteration 94/1000 | Loss: 0.00001326
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001322
Iteration 138/1000 | Loss: 0.00001322
Iteration 139/1000 | Loss: 0.00001322
Iteration 140/1000 | Loss: 0.00001322
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001319
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001318
Iteration 157/1000 | Loss: 0.00001318
Iteration 158/1000 | Loss: 0.00001318
Iteration 159/1000 | Loss: 0.00001318
Iteration 160/1000 | Loss: 0.00001318
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001317
Iteration 165/1000 | Loss: 0.00001317
Iteration 166/1000 | Loss: 0.00001317
Iteration 167/1000 | Loss: 0.00001317
Iteration 168/1000 | Loss: 0.00001317
Iteration 169/1000 | Loss: 0.00001316
Iteration 170/1000 | Loss: 0.00001316
Iteration 171/1000 | Loss: 0.00001316
Iteration 172/1000 | Loss: 0.00001316
Iteration 173/1000 | Loss: 0.00001316
Iteration 174/1000 | Loss: 0.00001316
Iteration 175/1000 | Loss: 0.00001316
Iteration 176/1000 | Loss: 0.00001316
Iteration 177/1000 | Loss: 0.00001316
Iteration 178/1000 | Loss: 0.00001316
Iteration 179/1000 | Loss: 0.00001316
Iteration 180/1000 | Loss: 0.00001316
Iteration 181/1000 | Loss: 0.00001315
Iteration 182/1000 | Loss: 0.00001315
Iteration 183/1000 | Loss: 0.00001315
Iteration 184/1000 | Loss: 0.00001315
Iteration 185/1000 | Loss: 0.00001315
Iteration 186/1000 | Loss: 0.00001315
Iteration 187/1000 | Loss: 0.00001315
Iteration 188/1000 | Loss: 0.00001315
Iteration 189/1000 | Loss: 0.00001315
Iteration 190/1000 | Loss: 0.00001315
Iteration 191/1000 | Loss: 0.00001315
Iteration 192/1000 | Loss: 0.00001314
Iteration 193/1000 | Loss: 0.00001314
Iteration 194/1000 | Loss: 0.00001314
Iteration 195/1000 | Loss: 0.00001314
Iteration 196/1000 | Loss: 0.00001314
Iteration 197/1000 | Loss: 0.00001314
Iteration 198/1000 | Loss: 0.00001314
Iteration 199/1000 | Loss: 0.00001314
Iteration 200/1000 | Loss: 0.00001314
Iteration 201/1000 | Loss: 0.00001314
Iteration 202/1000 | Loss: 0.00001314
Iteration 203/1000 | Loss: 0.00001314
Iteration 204/1000 | Loss: 0.00001314
Iteration 205/1000 | Loss: 0.00001314
Iteration 206/1000 | Loss: 0.00001314
Iteration 207/1000 | Loss: 0.00001314
Iteration 208/1000 | Loss: 0.00001313
Iteration 209/1000 | Loss: 0.00001313
Iteration 210/1000 | Loss: 0.00001313
Iteration 211/1000 | Loss: 0.00001313
Iteration 212/1000 | Loss: 0.00001313
Iteration 213/1000 | Loss: 0.00001313
Iteration 214/1000 | Loss: 0.00001313
Iteration 215/1000 | Loss: 0.00001312
Iteration 216/1000 | Loss: 0.00001312
Iteration 217/1000 | Loss: 0.00001312
Iteration 218/1000 | Loss: 0.00001312
Iteration 219/1000 | Loss: 0.00001312
Iteration 220/1000 | Loss: 0.00001312
Iteration 221/1000 | Loss: 0.00001312
Iteration 222/1000 | Loss: 0.00001312
Iteration 223/1000 | Loss: 0.00001311
Iteration 224/1000 | Loss: 0.00001311
Iteration 225/1000 | Loss: 0.00001311
Iteration 226/1000 | Loss: 0.00001311
Iteration 227/1000 | Loss: 0.00001311
Iteration 228/1000 | Loss: 0.00001311
Iteration 229/1000 | Loss: 0.00001311
Iteration 230/1000 | Loss: 0.00001311
Iteration 231/1000 | Loss: 0.00001311
Iteration 232/1000 | Loss: 0.00001311
Iteration 233/1000 | Loss: 0.00001311
Iteration 234/1000 | Loss: 0.00001311
Iteration 235/1000 | Loss: 0.00001311
Iteration 236/1000 | Loss: 0.00001311
Iteration 237/1000 | Loss: 0.00001311
Iteration 238/1000 | Loss: 0.00001311
Iteration 239/1000 | Loss: 0.00001311
Iteration 240/1000 | Loss: 0.00001311
Iteration 241/1000 | Loss: 0.00001310
Iteration 242/1000 | Loss: 0.00001310
Iteration 243/1000 | Loss: 0.00001310
Iteration 244/1000 | Loss: 0.00001310
Iteration 245/1000 | Loss: 0.00001310
Iteration 246/1000 | Loss: 0.00001310
Iteration 247/1000 | Loss: 0.00001310
Iteration 248/1000 | Loss: 0.00001310
Iteration 249/1000 | Loss: 0.00001310
Iteration 250/1000 | Loss: 0.00001310
Iteration 251/1000 | Loss: 0.00001310
Iteration 252/1000 | Loss: 0.00001310
Iteration 253/1000 | Loss: 0.00001310
Iteration 254/1000 | Loss: 0.00001310
Iteration 255/1000 | Loss: 0.00001310
Iteration 256/1000 | Loss: 0.00001310
Iteration 257/1000 | Loss: 0.00001310
Iteration 258/1000 | Loss: 0.00001310
Iteration 259/1000 | Loss: 0.00001310
Iteration 260/1000 | Loss: 0.00001310
Iteration 261/1000 | Loss: 0.00001310
Iteration 262/1000 | Loss: 0.00001310
Iteration 263/1000 | Loss: 0.00001310
Iteration 264/1000 | Loss: 0.00001310
Iteration 265/1000 | Loss: 0.00001310
Iteration 266/1000 | Loss: 0.00001310
Iteration 267/1000 | Loss: 0.00001310
Iteration 268/1000 | Loss: 0.00001310
Iteration 269/1000 | Loss: 0.00001310
Iteration 270/1000 | Loss: 0.00001310
Iteration 271/1000 | Loss: 0.00001310
Iteration 272/1000 | Loss: 0.00001310
Iteration 273/1000 | Loss: 0.00001310
Iteration 274/1000 | Loss: 0.00001310
Iteration 275/1000 | Loss: 0.00001310
Iteration 276/1000 | Loss: 0.00001310
Iteration 277/1000 | Loss: 0.00001310
Iteration 278/1000 | Loss: 0.00001310
Iteration 279/1000 | Loss: 0.00001310
Iteration 280/1000 | Loss: 0.00001310
Iteration 281/1000 | Loss: 0.00001310
Iteration 282/1000 | Loss: 0.00001310
Iteration 283/1000 | Loss: 0.00001310
Iteration 284/1000 | Loss: 0.00001310
Iteration 285/1000 | Loss: 0.00001310
Iteration 286/1000 | Loss: 0.00001310
Iteration 287/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.3101534932502545e-05, 1.3101534932502545e-05, 1.3101534932502545e-05, 1.3101534932502545e-05, 1.3101534932502545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3101534932502545e-05

Optimization complete. Final v2v error: 3.059593677520752 mm

Highest mean error: 3.262831211090088 mm for frame 29

Lowest mean error: 2.8955299854278564 mm for frame 69

Saving results

Total time: 38.99547219276428
