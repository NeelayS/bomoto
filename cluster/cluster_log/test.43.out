Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=43, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2408-2463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972872
Iteration 2/25 | Loss: 0.00188782
Iteration 3/25 | Loss: 0.00164217
Iteration 4/25 | Loss: 0.00155098
Iteration 5/25 | Loss: 0.00155788
Iteration 6/25 | Loss: 0.00161393
Iteration 7/25 | Loss: 0.00143493
Iteration 8/25 | Loss: 0.00146648
Iteration 9/25 | Loss: 0.00143125
Iteration 10/25 | Loss: 0.00141693
Iteration 11/25 | Loss: 0.00140598
Iteration 12/25 | Loss: 0.00140423
Iteration 13/25 | Loss: 0.00140392
Iteration 14/25 | Loss: 0.00140373
Iteration 15/25 | Loss: 0.00140368
Iteration 16/25 | Loss: 0.00140368
Iteration 17/25 | Loss: 0.00140367
Iteration 18/25 | Loss: 0.00140367
Iteration 19/25 | Loss: 0.00140367
Iteration 20/25 | Loss: 0.00140367
Iteration 21/25 | Loss: 0.00140366
Iteration 22/25 | Loss: 0.00140366
Iteration 23/25 | Loss: 0.00140366
Iteration 24/25 | Loss: 0.00140366
Iteration 25/25 | Loss: 0.00140366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78448033
Iteration 2/25 | Loss: 0.00180688
Iteration 3/25 | Loss: 0.00180688
Iteration 4/25 | Loss: 0.00180688
Iteration 5/25 | Loss: 0.00180688
Iteration 6/25 | Loss: 0.00180687
Iteration 7/25 | Loss: 0.00180687
Iteration 8/25 | Loss: 0.00177798
Iteration 9/25 | Loss: 0.00177798
Iteration 10/25 | Loss: 0.00177798
Iteration 11/25 | Loss: 0.00177797
Iteration 12/25 | Loss: 0.00177797
Iteration 13/25 | Loss: 0.00177797
Iteration 14/25 | Loss: 0.00177797
Iteration 15/25 | Loss: 0.00177797
Iteration 16/25 | Loss: 0.00177797
Iteration 17/25 | Loss: 0.00177797
Iteration 18/25 | Loss: 0.00177797
Iteration 19/25 | Loss: 0.00177797
Iteration 20/25 | Loss: 0.00177797
Iteration 21/25 | Loss: 0.00177797
Iteration 22/25 | Loss: 0.00177797
Iteration 23/25 | Loss: 0.00177797
Iteration 24/25 | Loss: 0.00177797
Iteration 25/25 | Loss: 0.00177797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177797
Iteration 2/1000 | Loss: 0.00005730
Iteration 3/1000 | Loss: 0.00002781
Iteration 4/1000 | Loss: 0.00002453
Iteration 5/1000 | Loss: 0.00002323
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002170
Iteration 8/1000 | Loss: 0.00002104
Iteration 9/1000 | Loss: 0.00002045
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00042552
Iteration 12/1000 | Loss: 0.00002578
Iteration 13/1000 | Loss: 0.00002189
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001683
Iteration 22/1000 | Loss: 0.00001677
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001666
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001664
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001658
Iteration 36/1000 | Loss: 0.00001658
Iteration 37/1000 | Loss: 0.00001658
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001646
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001645
Iteration 50/1000 | Loss: 0.00001645
Iteration 51/1000 | Loss: 0.00001645
Iteration 52/1000 | Loss: 0.00001644
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001640
Iteration 56/1000 | Loss: 0.00001640
Iteration 57/1000 | Loss: 0.00001640
Iteration 58/1000 | Loss: 0.00001639
Iteration 59/1000 | Loss: 0.00001639
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001637
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001629
Iteration 79/1000 | Loss: 0.00001629
Iteration 80/1000 | Loss: 0.00001629
Iteration 81/1000 | Loss: 0.00001629
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001628
Iteration 84/1000 | Loss: 0.00001628
Iteration 85/1000 | Loss: 0.00001628
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001627
Iteration 88/1000 | Loss: 0.00001627
Iteration 89/1000 | Loss: 0.00001627
Iteration 90/1000 | Loss: 0.00001627
Iteration 91/1000 | Loss: 0.00001627
Iteration 92/1000 | Loss: 0.00001627
Iteration 93/1000 | Loss: 0.00001626
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001626
Iteration 96/1000 | Loss: 0.00001626
Iteration 97/1000 | Loss: 0.00001626
Iteration 98/1000 | Loss: 0.00001625
Iteration 99/1000 | Loss: 0.00001625
Iteration 100/1000 | Loss: 0.00001624
Iteration 101/1000 | Loss: 0.00001624
Iteration 102/1000 | Loss: 0.00001624
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001623
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001623
Iteration 107/1000 | Loss: 0.00001623
Iteration 108/1000 | Loss: 0.00001623
Iteration 109/1000 | Loss: 0.00001623
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Iteration 127/1000 | Loss: 0.00001620
Iteration 128/1000 | Loss: 0.00001620
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001619
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Iteration 140/1000 | Loss: 0.00001618
Iteration 141/1000 | Loss: 0.00001618
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001618
Iteration 144/1000 | Loss: 0.00001617
Iteration 145/1000 | Loss: 0.00001617
Iteration 146/1000 | Loss: 0.00001617
Iteration 147/1000 | Loss: 0.00001617
Iteration 148/1000 | Loss: 0.00001617
Iteration 149/1000 | Loss: 0.00001617
Iteration 150/1000 | Loss: 0.00001617
Iteration 151/1000 | Loss: 0.00001617
Iteration 152/1000 | Loss: 0.00001616
Iteration 153/1000 | Loss: 0.00001616
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001616
Iteration 159/1000 | Loss: 0.00001616
Iteration 160/1000 | Loss: 0.00001616
Iteration 161/1000 | Loss: 0.00001616
Iteration 162/1000 | Loss: 0.00001616
Iteration 163/1000 | Loss: 0.00001615
Iteration 164/1000 | Loss: 0.00001615
Iteration 165/1000 | Loss: 0.00001615
Iteration 166/1000 | Loss: 0.00001615
Iteration 167/1000 | Loss: 0.00001615
Iteration 168/1000 | Loss: 0.00001615
Iteration 169/1000 | Loss: 0.00001615
Iteration 170/1000 | Loss: 0.00001615
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001615
Iteration 173/1000 | Loss: 0.00001615
Iteration 174/1000 | Loss: 0.00001615
Iteration 175/1000 | Loss: 0.00001615
Iteration 176/1000 | Loss: 0.00001615
Iteration 177/1000 | Loss: 0.00001615
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001615
Iteration 182/1000 | Loss: 0.00001615
Iteration 183/1000 | Loss: 0.00001615
Iteration 184/1000 | Loss: 0.00001615
Iteration 185/1000 | Loss: 0.00001615
Iteration 186/1000 | Loss: 0.00001615
Iteration 187/1000 | Loss: 0.00001615
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001615
Iteration 191/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6149080693139695e-05, 1.6149080693139695e-05, 1.6149080693139695e-05, 1.6149080693139695e-05, 1.6149080693139695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6149080693139695e-05

Optimization complete. Final v2v error: 3.4183411598205566 mm

Highest mean error: 4.1912431716918945 mm for frame 73

Lowest mean error: 3.157683849334717 mm for frame 120

Saving results

Total time: 70.36060810089111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007181
Iteration 2/25 | Loss: 0.01007180
Iteration 3/25 | Loss: 0.01007180
Iteration 4/25 | Loss: 0.01007180
Iteration 5/25 | Loss: 0.01007179
Iteration 6/25 | Loss: 0.01007179
Iteration 7/25 | Loss: 0.01007179
Iteration 8/25 | Loss: 0.00282197
Iteration 9/25 | Loss: 0.00218970
Iteration 10/25 | Loss: 0.00194797
Iteration 11/25 | Loss: 0.00192800
Iteration 12/25 | Loss: 0.00183774
Iteration 13/25 | Loss: 0.00176816
Iteration 14/25 | Loss: 0.00175256
Iteration 15/25 | Loss: 0.00173113
Iteration 16/25 | Loss: 0.00171122
Iteration 17/25 | Loss: 0.00170665
Iteration 18/25 | Loss: 0.00167868
Iteration 19/25 | Loss: 0.00167385
Iteration 20/25 | Loss: 0.00166680
Iteration 21/25 | Loss: 0.00166161
Iteration 22/25 | Loss: 0.00165647
Iteration 23/25 | Loss: 0.00164950
Iteration 24/25 | Loss: 0.00164255
Iteration 25/25 | Loss: 0.00163934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18956399
Iteration 2/25 | Loss: 0.00650126
Iteration 3/25 | Loss: 0.00482287
Iteration 4/25 | Loss: 0.00482285
Iteration 5/25 | Loss: 0.00482285
Iteration 6/25 | Loss: 0.00482285
Iteration 7/25 | Loss: 0.00482284
Iteration 8/25 | Loss: 0.00482284
Iteration 9/25 | Loss: 0.00482284
Iteration 10/25 | Loss: 0.00482284
Iteration 11/25 | Loss: 0.00482284
Iteration 12/25 | Loss: 0.00482284
Iteration 13/25 | Loss: 0.00482284
Iteration 14/25 | Loss: 0.00482284
Iteration 15/25 | Loss: 0.00482284
Iteration 16/25 | Loss: 0.00482284
Iteration 17/25 | Loss: 0.00482284
Iteration 18/25 | Loss: 0.00482284
Iteration 19/25 | Loss: 0.00482284
Iteration 20/25 | Loss: 0.00482284
Iteration 21/25 | Loss: 0.00482284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004822842311114073, 0.004822842311114073, 0.004822842311114073, 0.004822842311114073, 0.004822842311114073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004822842311114073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00482284
Iteration 2/1000 | Loss: 0.00077824
Iteration 3/1000 | Loss: 0.00220812
Iteration 4/1000 | Loss: 0.00029371
Iteration 5/1000 | Loss: 0.00030423
Iteration 6/1000 | Loss: 0.00034327
Iteration 7/1000 | Loss: 0.00126705
Iteration 8/1000 | Loss: 0.00022567
Iteration 9/1000 | Loss: 0.00041706
Iteration 10/1000 | Loss: 0.00042256
Iteration 11/1000 | Loss: 0.00094814
Iteration 12/1000 | Loss: 0.00022985
Iteration 13/1000 | Loss: 0.00065866
Iteration 14/1000 | Loss: 0.00258147
Iteration 15/1000 | Loss: 0.00124691
Iteration 16/1000 | Loss: 0.00153708
Iteration 17/1000 | Loss: 0.00060749
Iteration 18/1000 | Loss: 0.00033113
Iteration 19/1000 | Loss: 0.00022334
Iteration 20/1000 | Loss: 0.00060297
Iteration 21/1000 | Loss: 0.00035227
Iteration 22/1000 | Loss: 0.00052552
Iteration 23/1000 | Loss: 0.00056919
Iteration 24/1000 | Loss: 0.00049639
Iteration 25/1000 | Loss: 0.00042237
Iteration 26/1000 | Loss: 0.00033702
Iteration 27/1000 | Loss: 0.00108633
Iteration 28/1000 | Loss: 0.00069906
Iteration 29/1000 | Loss: 0.00091425
Iteration 30/1000 | Loss: 0.00074949
Iteration 31/1000 | Loss: 0.00034979
Iteration 32/1000 | Loss: 0.00077392
Iteration 33/1000 | Loss: 0.00081620
Iteration 34/1000 | Loss: 0.00102265
Iteration 35/1000 | Loss: 0.00087227
Iteration 36/1000 | Loss: 0.00147033
Iteration 37/1000 | Loss: 0.00090054
Iteration 38/1000 | Loss: 0.00077830
Iteration 39/1000 | Loss: 0.00070559
Iteration 40/1000 | Loss: 0.00056160
Iteration 41/1000 | Loss: 0.00061945
Iteration 42/1000 | Loss: 0.00123819
Iteration 43/1000 | Loss: 0.00099933
Iteration 44/1000 | Loss: 0.00079709
Iteration 45/1000 | Loss: 0.00051771
Iteration 46/1000 | Loss: 0.00017985
Iteration 47/1000 | Loss: 0.00030136
Iteration 48/1000 | Loss: 0.00100785
Iteration 49/1000 | Loss: 0.00058968
Iteration 50/1000 | Loss: 0.00118087
Iteration 51/1000 | Loss: 0.00053872
Iteration 52/1000 | Loss: 0.00038778
Iteration 53/1000 | Loss: 0.00038578
Iteration 54/1000 | Loss: 0.00043937
Iteration 55/1000 | Loss: 0.00119073
Iteration 56/1000 | Loss: 0.00047510
Iteration 57/1000 | Loss: 0.00040085
Iteration 58/1000 | Loss: 0.00161735
Iteration 59/1000 | Loss: 0.00064325
Iteration 60/1000 | Loss: 0.00077528
Iteration 61/1000 | Loss: 0.00024493
Iteration 62/1000 | Loss: 0.00095051
Iteration 63/1000 | Loss: 0.00068323
Iteration 64/1000 | Loss: 0.00044817
Iteration 65/1000 | Loss: 0.00044202
Iteration 66/1000 | Loss: 0.00047333
Iteration 67/1000 | Loss: 0.00031419
Iteration 68/1000 | Loss: 0.00058645
Iteration 69/1000 | Loss: 0.00022177
Iteration 70/1000 | Loss: 0.00014956
Iteration 71/1000 | Loss: 0.00040439
Iteration 72/1000 | Loss: 0.00066854
Iteration 73/1000 | Loss: 0.00038493
Iteration 74/1000 | Loss: 0.00027995
Iteration 75/1000 | Loss: 0.00024026
Iteration 76/1000 | Loss: 0.00019574
Iteration 77/1000 | Loss: 0.00015745
Iteration 78/1000 | Loss: 0.00045595
Iteration 79/1000 | Loss: 0.00178606
Iteration 80/1000 | Loss: 0.00053465
Iteration 81/1000 | Loss: 0.00105512
Iteration 82/1000 | Loss: 0.00028766
Iteration 83/1000 | Loss: 0.00022708
Iteration 84/1000 | Loss: 0.00040648
Iteration 85/1000 | Loss: 0.00202036
Iteration 86/1000 | Loss: 0.00023837
Iteration 87/1000 | Loss: 0.00038340
Iteration 88/1000 | Loss: 0.00019311
Iteration 89/1000 | Loss: 0.00026908
Iteration 90/1000 | Loss: 0.00018003
Iteration 91/1000 | Loss: 0.00012253
Iteration 92/1000 | Loss: 0.00023107
Iteration 93/1000 | Loss: 0.00018439
Iteration 94/1000 | Loss: 0.00022486
Iteration 95/1000 | Loss: 0.00041253
Iteration 96/1000 | Loss: 0.00015792
Iteration 97/1000 | Loss: 0.00011997
Iteration 98/1000 | Loss: 0.00011297
Iteration 99/1000 | Loss: 0.00011193
Iteration 100/1000 | Loss: 0.00017994
Iteration 101/1000 | Loss: 0.00011076
Iteration 102/1000 | Loss: 0.00011003
Iteration 103/1000 | Loss: 0.00034938
Iteration 104/1000 | Loss: 0.00016052
Iteration 105/1000 | Loss: 0.00023479
Iteration 106/1000 | Loss: 0.00011058
Iteration 107/1000 | Loss: 0.00010835
Iteration 108/1000 | Loss: 0.00012023
Iteration 109/1000 | Loss: 0.00010772
Iteration 110/1000 | Loss: 0.00018777
Iteration 111/1000 | Loss: 0.00044162
Iteration 112/1000 | Loss: 0.00011390
Iteration 113/1000 | Loss: 0.00016087
Iteration 114/1000 | Loss: 0.00010636
Iteration 115/1000 | Loss: 0.00010555
Iteration 116/1000 | Loss: 0.00062955
Iteration 117/1000 | Loss: 0.00030588
Iteration 118/1000 | Loss: 0.00019882
Iteration 119/1000 | Loss: 0.00018454
Iteration 120/1000 | Loss: 0.00016739
Iteration 121/1000 | Loss: 0.00057178
Iteration 122/1000 | Loss: 0.00010583
Iteration 123/1000 | Loss: 0.00017528
Iteration 124/1000 | Loss: 0.00029236
Iteration 125/1000 | Loss: 0.00010023
Iteration 126/1000 | Loss: 0.00012787
Iteration 127/1000 | Loss: 0.00011137
Iteration 128/1000 | Loss: 0.00029841
Iteration 129/1000 | Loss: 0.00009480
Iteration 130/1000 | Loss: 0.00030978
Iteration 131/1000 | Loss: 0.00025285
Iteration 132/1000 | Loss: 0.00064180
Iteration 133/1000 | Loss: 0.00010108
Iteration 134/1000 | Loss: 0.00026681
Iteration 135/1000 | Loss: 0.00009413
Iteration 136/1000 | Loss: 0.00025776
Iteration 137/1000 | Loss: 0.00008835
Iteration 138/1000 | Loss: 0.00008716
Iteration 139/1000 | Loss: 0.00062942
Iteration 140/1000 | Loss: 0.00219123
Iteration 141/1000 | Loss: 0.00308978
Iteration 142/1000 | Loss: 0.00053944
Iteration 143/1000 | Loss: 0.00057507
Iteration 144/1000 | Loss: 0.00013917
Iteration 145/1000 | Loss: 0.00043878
Iteration 146/1000 | Loss: 0.00024556
Iteration 147/1000 | Loss: 0.00007296
Iteration 148/1000 | Loss: 0.00011314
Iteration 149/1000 | Loss: 0.00057267
Iteration 150/1000 | Loss: 0.00016854
Iteration 151/1000 | Loss: 0.00026221
Iteration 152/1000 | Loss: 0.00012806
Iteration 153/1000 | Loss: 0.00004335
Iteration 154/1000 | Loss: 0.00003874
Iteration 155/1000 | Loss: 0.00015669
Iteration 156/1000 | Loss: 0.00028526
Iteration 157/1000 | Loss: 0.00009893
Iteration 158/1000 | Loss: 0.00003200
Iteration 159/1000 | Loss: 0.00017511
Iteration 160/1000 | Loss: 0.00012098
Iteration 161/1000 | Loss: 0.00008850
Iteration 162/1000 | Loss: 0.00002689
Iteration 163/1000 | Loss: 0.00011035
Iteration 164/1000 | Loss: 0.00002564
Iteration 165/1000 | Loss: 0.00026887
Iteration 166/1000 | Loss: 0.00002478
Iteration 167/1000 | Loss: 0.00002415
Iteration 168/1000 | Loss: 0.00002374
Iteration 169/1000 | Loss: 0.00002312
Iteration 170/1000 | Loss: 0.00036382
Iteration 171/1000 | Loss: 0.00003505
Iteration 172/1000 | Loss: 0.00020205
Iteration 173/1000 | Loss: 0.00007499
Iteration 174/1000 | Loss: 0.00002902
Iteration 175/1000 | Loss: 0.00027516
Iteration 176/1000 | Loss: 0.00023500
Iteration 177/1000 | Loss: 0.00027269
Iteration 178/1000 | Loss: 0.00024155
Iteration 179/1000 | Loss: 0.00024246
Iteration 180/1000 | Loss: 0.00014282
Iteration 181/1000 | Loss: 0.00002289
Iteration 182/1000 | Loss: 0.00029394
Iteration 183/1000 | Loss: 0.00027455
Iteration 184/1000 | Loss: 0.00024684
Iteration 185/1000 | Loss: 0.00012075
Iteration 186/1000 | Loss: 0.00011575
Iteration 187/1000 | Loss: 0.00008237
Iteration 188/1000 | Loss: 0.00002392
Iteration 189/1000 | Loss: 0.00002162
Iteration 190/1000 | Loss: 0.00002027
Iteration 191/1000 | Loss: 0.00007408
Iteration 192/1000 | Loss: 0.00003798
Iteration 193/1000 | Loss: 0.00009477
Iteration 194/1000 | Loss: 0.00008119
Iteration 195/1000 | Loss: 0.00008810
Iteration 196/1000 | Loss: 0.00010430
Iteration 197/1000 | Loss: 0.00007295
Iteration 198/1000 | Loss: 0.00004769
Iteration 199/1000 | Loss: 0.00007581
Iteration 200/1000 | Loss: 0.00004166
Iteration 201/1000 | Loss: 0.00010139
Iteration 202/1000 | Loss: 0.00009950
Iteration 203/1000 | Loss: 0.00001910
Iteration 204/1000 | Loss: 0.00009704
Iteration 205/1000 | Loss: 0.00002214
Iteration 206/1000 | Loss: 0.00001891
Iteration 207/1000 | Loss: 0.00001824
Iteration 208/1000 | Loss: 0.00001795
Iteration 209/1000 | Loss: 0.00001793
Iteration 210/1000 | Loss: 0.00001760
Iteration 211/1000 | Loss: 0.00001725
Iteration 212/1000 | Loss: 0.00001686
Iteration 213/1000 | Loss: 0.00001668
Iteration 214/1000 | Loss: 0.00001657
Iteration 215/1000 | Loss: 0.00001648
Iteration 216/1000 | Loss: 0.00001646
Iteration 217/1000 | Loss: 0.00001644
Iteration 218/1000 | Loss: 0.00001643
Iteration 219/1000 | Loss: 0.00001641
Iteration 220/1000 | Loss: 0.00001641
Iteration 221/1000 | Loss: 0.00001638
Iteration 222/1000 | Loss: 0.00001637
Iteration 223/1000 | Loss: 0.00001636
Iteration 224/1000 | Loss: 0.00001635
Iteration 225/1000 | Loss: 0.00001635
Iteration 226/1000 | Loss: 0.00001634
Iteration 227/1000 | Loss: 0.00001634
Iteration 228/1000 | Loss: 0.00001633
Iteration 229/1000 | Loss: 0.00001633
Iteration 230/1000 | Loss: 0.00001633
Iteration 231/1000 | Loss: 0.00001633
Iteration 232/1000 | Loss: 0.00001633
Iteration 233/1000 | Loss: 0.00001633
Iteration 234/1000 | Loss: 0.00001632
Iteration 235/1000 | Loss: 0.00001632
Iteration 236/1000 | Loss: 0.00001632
Iteration 237/1000 | Loss: 0.00001632
Iteration 238/1000 | Loss: 0.00001632
Iteration 239/1000 | Loss: 0.00001632
Iteration 240/1000 | Loss: 0.00001632
Iteration 241/1000 | Loss: 0.00001631
Iteration 242/1000 | Loss: 0.00001631
Iteration 243/1000 | Loss: 0.00001631
Iteration 244/1000 | Loss: 0.00001630
Iteration 245/1000 | Loss: 0.00001629
Iteration 246/1000 | Loss: 0.00001628
Iteration 247/1000 | Loss: 0.00001628
Iteration 248/1000 | Loss: 0.00001627
Iteration 249/1000 | Loss: 0.00001626
Iteration 250/1000 | Loss: 0.00001626
Iteration 251/1000 | Loss: 0.00001626
Iteration 252/1000 | Loss: 0.00001626
Iteration 253/1000 | Loss: 0.00001626
Iteration 254/1000 | Loss: 0.00001626
Iteration 255/1000 | Loss: 0.00001626
Iteration 256/1000 | Loss: 0.00001625
Iteration 257/1000 | Loss: 0.00001625
Iteration 258/1000 | Loss: 0.00001625
Iteration 259/1000 | Loss: 0.00001625
Iteration 260/1000 | Loss: 0.00001625
Iteration 261/1000 | Loss: 0.00001625
Iteration 262/1000 | Loss: 0.00001625
Iteration 263/1000 | Loss: 0.00001625
Iteration 264/1000 | Loss: 0.00001624
Iteration 265/1000 | Loss: 0.00001624
Iteration 266/1000 | Loss: 0.00001624
Iteration 267/1000 | Loss: 0.00001624
Iteration 268/1000 | Loss: 0.00001624
Iteration 269/1000 | Loss: 0.00001624
Iteration 270/1000 | Loss: 0.00001623
Iteration 271/1000 | Loss: 0.00001623
Iteration 272/1000 | Loss: 0.00001623
Iteration 273/1000 | Loss: 0.00001623
Iteration 274/1000 | Loss: 0.00001623
Iteration 275/1000 | Loss: 0.00001623
Iteration 276/1000 | Loss: 0.00001623
Iteration 277/1000 | Loss: 0.00001623
Iteration 278/1000 | Loss: 0.00001623
Iteration 279/1000 | Loss: 0.00001623
Iteration 280/1000 | Loss: 0.00001623
Iteration 281/1000 | Loss: 0.00001623
Iteration 282/1000 | Loss: 0.00001623
Iteration 283/1000 | Loss: 0.00001623
Iteration 284/1000 | Loss: 0.00001623
Iteration 285/1000 | Loss: 0.00001623
Iteration 286/1000 | Loss: 0.00001623
Iteration 287/1000 | Loss: 0.00001623
Iteration 288/1000 | Loss: 0.00001623
Iteration 289/1000 | Loss: 0.00001623
Iteration 290/1000 | Loss: 0.00001623
Iteration 291/1000 | Loss: 0.00001623
Iteration 292/1000 | Loss: 0.00001623
Iteration 293/1000 | Loss: 0.00001623
Iteration 294/1000 | Loss: 0.00001623
Iteration 295/1000 | Loss: 0.00001623
Iteration 296/1000 | Loss: 0.00001623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.6232024790951982e-05, 1.6232024790951982e-05, 1.6232024790951982e-05, 1.6232024790951982e-05, 1.6232024790951982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6232024790951982e-05

Optimization complete. Final v2v error: 3.3779914379119873 mm

Highest mean error: 10.67886734008789 mm for frame 188

Lowest mean error: 3.0012409687042236 mm for frame 59

Saving results

Total time: 393.0042109489441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466110
Iteration 2/25 | Loss: 0.00142691
Iteration 3/25 | Loss: 0.00134901
Iteration 4/25 | Loss: 0.00134094
Iteration 5/25 | Loss: 0.00133896
Iteration 6/25 | Loss: 0.00133896
Iteration 7/25 | Loss: 0.00133896
Iteration 8/25 | Loss: 0.00133896
Iteration 9/25 | Loss: 0.00133896
Iteration 10/25 | Loss: 0.00133896
Iteration 11/25 | Loss: 0.00133896
Iteration 12/25 | Loss: 0.00133896
Iteration 13/25 | Loss: 0.00133896
Iteration 14/25 | Loss: 0.00133896
Iteration 15/25 | Loss: 0.00133896
Iteration 16/25 | Loss: 0.00133896
Iteration 17/25 | Loss: 0.00133896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013389630476012826, 0.0013389630476012826, 0.0013389630476012826, 0.0013389630476012826, 0.0013389630476012826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013389630476012826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.00189686
Iteration 2/25 | Loss: 0.00180482
Iteration 3/25 | Loss: 0.00180481
Iteration 4/25 | Loss: 0.00180481
Iteration 5/25 | Loss: 0.00180481
Iteration 6/25 | Loss: 0.00180481
Iteration 7/25 | Loss: 0.00180481
Iteration 8/25 | Loss: 0.00180481
Iteration 9/25 | Loss: 0.00180481
Iteration 10/25 | Loss: 0.00180481
Iteration 11/25 | Loss: 0.00180481
Iteration 12/25 | Loss: 0.00180481
Iteration 13/25 | Loss: 0.00180481
Iteration 14/25 | Loss: 0.00180481
Iteration 15/25 | Loss: 0.00180481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018048088531941175, 0.0018048088531941175, 0.0018048088531941175, 0.0018048088531941175, 0.0018048088531941175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018048088531941175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180481
Iteration 2/1000 | Loss: 0.00002974
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001467
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001394
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001348
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001311
Iteration 22/1000 | Loss: 0.00001309
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001307
Iteration 26/1000 | Loss: 0.00001307
Iteration 27/1000 | Loss: 0.00001306
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00001280
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001271
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001264
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001261
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001260
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001260
Iteration 129/1000 | Loss: 0.00001260
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001260
Iteration 132/1000 | Loss: 0.00001260
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001260
Iteration 135/1000 | Loss: 0.00001260
Iteration 136/1000 | Loss: 0.00001260
Iteration 137/1000 | Loss: 0.00001260
Iteration 138/1000 | Loss: 0.00001260
Iteration 139/1000 | Loss: 0.00001260
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001259
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001259
Iteration 144/1000 | Loss: 0.00001259
Iteration 145/1000 | Loss: 0.00001259
Iteration 146/1000 | Loss: 0.00001259
Iteration 147/1000 | Loss: 0.00001259
Iteration 148/1000 | Loss: 0.00001259
Iteration 149/1000 | Loss: 0.00001259
Iteration 150/1000 | Loss: 0.00001258
Iteration 151/1000 | Loss: 0.00001258
Iteration 152/1000 | Loss: 0.00001258
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001257
Iteration 156/1000 | Loss: 0.00001257
Iteration 157/1000 | Loss: 0.00001257
Iteration 158/1000 | Loss: 0.00001257
Iteration 159/1000 | Loss: 0.00001257
Iteration 160/1000 | Loss: 0.00001257
Iteration 161/1000 | Loss: 0.00001257
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001257
Iteration 164/1000 | Loss: 0.00001257
Iteration 165/1000 | Loss: 0.00001257
Iteration 166/1000 | Loss: 0.00001256
Iteration 167/1000 | Loss: 0.00001256
Iteration 168/1000 | Loss: 0.00001256
Iteration 169/1000 | Loss: 0.00001256
Iteration 170/1000 | Loss: 0.00001256
Iteration 171/1000 | Loss: 0.00001256
Iteration 172/1000 | Loss: 0.00001256
Iteration 173/1000 | Loss: 0.00001256
Iteration 174/1000 | Loss: 0.00001256
Iteration 175/1000 | Loss: 0.00001256
Iteration 176/1000 | Loss: 0.00001256
Iteration 177/1000 | Loss: 0.00001256
Iteration 178/1000 | Loss: 0.00001256
Iteration 179/1000 | Loss: 0.00001255
Iteration 180/1000 | Loss: 0.00001255
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001255
Iteration 184/1000 | Loss: 0.00001255
Iteration 185/1000 | Loss: 0.00001255
Iteration 186/1000 | Loss: 0.00001255
Iteration 187/1000 | Loss: 0.00001255
Iteration 188/1000 | Loss: 0.00001255
Iteration 189/1000 | Loss: 0.00001255
Iteration 190/1000 | Loss: 0.00001255
Iteration 191/1000 | Loss: 0.00001255
Iteration 192/1000 | Loss: 0.00001255
Iteration 193/1000 | Loss: 0.00001255
Iteration 194/1000 | Loss: 0.00001255
Iteration 195/1000 | Loss: 0.00001255
Iteration 196/1000 | Loss: 0.00001255
Iteration 197/1000 | Loss: 0.00001255
Iteration 198/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.255079587281216e-05, 1.255079587281216e-05, 1.255079587281216e-05, 1.255079587281216e-05, 1.255079587281216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.255079587281216e-05

Optimization complete. Final v2v error: 3.0436112880706787 mm

Highest mean error: 3.290675163269043 mm for frame 7

Lowest mean error: 2.826781988143921 mm for frame 135

Saving results

Total time: 45.666738986968994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898110
Iteration 2/25 | Loss: 0.00177491
Iteration 3/25 | Loss: 0.00148458
Iteration 4/25 | Loss: 0.00147240
Iteration 5/25 | Loss: 0.00147629
Iteration 6/25 | Loss: 0.00145108
Iteration 7/25 | Loss: 0.00144233
Iteration 8/25 | Loss: 0.00142789
Iteration 9/25 | Loss: 0.00142305
Iteration 10/25 | Loss: 0.00142414
Iteration 11/25 | Loss: 0.00142215
Iteration 12/25 | Loss: 0.00141855
Iteration 13/25 | Loss: 0.00141351
Iteration 14/25 | Loss: 0.00141084
Iteration 15/25 | Loss: 0.00141009
Iteration 16/25 | Loss: 0.00140970
Iteration 17/25 | Loss: 0.00140953
Iteration 18/25 | Loss: 0.00140938
Iteration 19/25 | Loss: 0.00141021
Iteration 20/25 | Loss: 0.00140873
Iteration 21/25 | Loss: 0.00140837
Iteration 22/25 | Loss: 0.00140816
Iteration 23/25 | Loss: 0.00140814
Iteration 24/25 | Loss: 0.00140814
Iteration 25/25 | Loss: 0.00140814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71780491
Iteration 2/25 | Loss: 0.00172722
Iteration 3/25 | Loss: 0.00172722
Iteration 4/25 | Loss: 0.00172722
Iteration 5/25 | Loss: 0.00172722
Iteration 6/25 | Loss: 0.00172722
Iteration 7/25 | Loss: 0.00172722
Iteration 8/25 | Loss: 0.00172722
Iteration 9/25 | Loss: 0.00172722
Iteration 10/25 | Loss: 0.00172722
Iteration 11/25 | Loss: 0.00172722
Iteration 12/25 | Loss: 0.00172722
Iteration 13/25 | Loss: 0.00172722
Iteration 14/25 | Loss: 0.00172722
Iteration 15/25 | Loss: 0.00172722
Iteration 16/25 | Loss: 0.00172722
Iteration 17/25 | Loss: 0.00172722
Iteration 18/25 | Loss: 0.00172722
Iteration 19/25 | Loss: 0.00172722
Iteration 20/25 | Loss: 0.00172722
Iteration 21/25 | Loss: 0.00172722
Iteration 22/25 | Loss: 0.00172722
Iteration 23/25 | Loss: 0.00172722
Iteration 24/25 | Loss: 0.00172722
Iteration 25/25 | Loss: 0.00172722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172722
Iteration 2/1000 | Loss: 0.00008632
Iteration 3/1000 | Loss: 0.00006009
Iteration 4/1000 | Loss: 0.00005095
Iteration 5/1000 | Loss: 0.00004664
Iteration 6/1000 | Loss: 0.00004450
Iteration 7/1000 | Loss: 0.00004240
Iteration 8/1000 | Loss: 0.00038791
Iteration 9/1000 | Loss: 0.00003987
Iteration 10/1000 | Loss: 0.00003831
Iteration 11/1000 | Loss: 0.00070385
Iteration 12/1000 | Loss: 0.00017532
Iteration 13/1000 | Loss: 0.00004327
Iteration 14/1000 | Loss: 0.00072044
Iteration 15/1000 | Loss: 0.00011457
Iteration 16/1000 | Loss: 0.00003642
Iteration 17/1000 | Loss: 0.00003582
Iteration 18/1000 | Loss: 0.00079322
Iteration 19/1000 | Loss: 0.00222153
Iteration 20/1000 | Loss: 0.00033849
Iteration 21/1000 | Loss: 0.00004595
Iteration 22/1000 | Loss: 0.00031120
Iteration 23/1000 | Loss: 0.00026199
Iteration 24/1000 | Loss: 0.00033746
Iteration 25/1000 | Loss: 0.00029249
Iteration 26/1000 | Loss: 0.00033242
Iteration 27/1000 | Loss: 0.00069794
Iteration 28/1000 | Loss: 0.00019415
Iteration 29/1000 | Loss: 0.00003473
Iteration 30/1000 | Loss: 0.00003192
Iteration 31/1000 | Loss: 0.00007843
Iteration 32/1000 | Loss: 0.00003146
Iteration 33/1000 | Loss: 0.00063674
Iteration 34/1000 | Loss: 0.00013970
Iteration 35/1000 | Loss: 0.00003219
Iteration 36/1000 | Loss: 0.00006635
Iteration 37/1000 | Loss: 0.00003275
Iteration 38/1000 | Loss: 0.00003091
Iteration 39/1000 | Loss: 0.00061325
Iteration 40/1000 | Loss: 0.00017556
Iteration 41/1000 | Loss: 0.00004010
Iteration 42/1000 | Loss: 0.00063526
Iteration 43/1000 | Loss: 0.00018521
Iteration 44/1000 | Loss: 0.00004957
Iteration 45/1000 | Loss: 0.00003027
Iteration 46/1000 | Loss: 0.00061770
Iteration 47/1000 | Loss: 0.00032027
Iteration 48/1000 | Loss: 0.00004903
Iteration 49/1000 | Loss: 0.00070098
Iteration 50/1000 | Loss: 0.00097376
Iteration 51/1000 | Loss: 0.00018781
Iteration 52/1000 | Loss: 0.00011757
Iteration 53/1000 | Loss: 0.00007126
Iteration 54/1000 | Loss: 0.00003644
Iteration 55/1000 | Loss: 0.00003082
Iteration 56/1000 | Loss: 0.00002991
Iteration 57/1000 | Loss: 0.00060484
Iteration 58/1000 | Loss: 0.00008467
Iteration 59/1000 | Loss: 0.00008035
Iteration 60/1000 | Loss: 0.00003259
Iteration 61/1000 | Loss: 0.00003010
Iteration 62/1000 | Loss: 0.00002909
Iteration 63/1000 | Loss: 0.00002875
Iteration 64/1000 | Loss: 0.00014900
Iteration 65/1000 | Loss: 0.00071356
Iteration 66/1000 | Loss: 0.00021144
Iteration 67/1000 | Loss: 0.00003949
Iteration 68/1000 | Loss: 0.00009713
Iteration 69/1000 | Loss: 0.00003223
Iteration 70/1000 | Loss: 0.00003020
Iteration 71/1000 | Loss: 0.00002907
Iteration 72/1000 | Loss: 0.00016619
Iteration 73/1000 | Loss: 0.00003869
Iteration 74/1000 | Loss: 0.00003097
Iteration 75/1000 | Loss: 0.00002814
Iteration 76/1000 | Loss: 0.00002791
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002755
Iteration 79/1000 | Loss: 0.00002749
Iteration 80/1000 | Loss: 0.00002745
Iteration 81/1000 | Loss: 0.00002745
Iteration 82/1000 | Loss: 0.00002744
Iteration 83/1000 | Loss: 0.00002744
Iteration 84/1000 | Loss: 0.00002744
Iteration 85/1000 | Loss: 0.00002744
Iteration 86/1000 | Loss: 0.00002743
Iteration 87/1000 | Loss: 0.00002743
Iteration 88/1000 | Loss: 0.00002743
Iteration 89/1000 | Loss: 0.00002743
Iteration 90/1000 | Loss: 0.00002743
Iteration 91/1000 | Loss: 0.00002743
Iteration 92/1000 | Loss: 0.00002743
Iteration 93/1000 | Loss: 0.00002743
Iteration 94/1000 | Loss: 0.00002743
Iteration 95/1000 | Loss: 0.00002742
Iteration 96/1000 | Loss: 0.00002742
Iteration 97/1000 | Loss: 0.00002742
Iteration 98/1000 | Loss: 0.00002741
Iteration 99/1000 | Loss: 0.00002737
Iteration 100/1000 | Loss: 0.00002736
Iteration 101/1000 | Loss: 0.00002736
Iteration 102/1000 | Loss: 0.00002735
Iteration 103/1000 | Loss: 0.00002734
Iteration 104/1000 | Loss: 0.00002734
Iteration 105/1000 | Loss: 0.00002733
Iteration 106/1000 | Loss: 0.00002733
Iteration 107/1000 | Loss: 0.00002732
Iteration 108/1000 | Loss: 0.00002731
Iteration 109/1000 | Loss: 0.00002731
Iteration 110/1000 | Loss: 0.00002731
Iteration 111/1000 | Loss: 0.00002731
Iteration 112/1000 | Loss: 0.00002731
Iteration 113/1000 | Loss: 0.00002730
Iteration 114/1000 | Loss: 0.00002730
Iteration 115/1000 | Loss: 0.00002730
Iteration 116/1000 | Loss: 0.00002728
Iteration 117/1000 | Loss: 0.00002727
Iteration 118/1000 | Loss: 0.00002727
Iteration 119/1000 | Loss: 0.00002727
Iteration 120/1000 | Loss: 0.00002726
Iteration 121/1000 | Loss: 0.00002726
Iteration 122/1000 | Loss: 0.00002726
Iteration 123/1000 | Loss: 0.00002725
Iteration 124/1000 | Loss: 0.00002725
Iteration 125/1000 | Loss: 0.00002725
Iteration 126/1000 | Loss: 0.00002725
Iteration 127/1000 | Loss: 0.00002724
Iteration 128/1000 | Loss: 0.00002724
Iteration 129/1000 | Loss: 0.00002724
Iteration 130/1000 | Loss: 0.00002724
Iteration 131/1000 | Loss: 0.00002724
Iteration 132/1000 | Loss: 0.00002724
Iteration 133/1000 | Loss: 0.00002724
Iteration 134/1000 | Loss: 0.00002724
Iteration 135/1000 | Loss: 0.00002723
Iteration 136/1000 | Loss: 0.00002723
Iteration 137/1000 | Loss: 0.00002723
Iteration 138/1000 | Loss: 0.00002723
Iteration 139/1000 | Loss: 0.00002723
Iteration 140/1000 | Loss: 0.00002723
Iteration 141/1000 | Loss: 0.00002723
Iteration 142/1000 | Loss: 0.00002723
Iteration 143/1000 | Loss: 0.00002722
Iteration 144/1000 | Loss: 0.00002722
Iteration 145/1000 | Loss: 0.00002722
Iteration 146/1000 | Loss: 0.00002722
Iteration 147/1000 | Loss: 0.00002721
Iteration 148/1000 | Loss: 0.00002721
Iteration 149/1000 | Loss: 0.00002721
Iteration 150/1000 | Loss: 0.00002721
Iteration 151/1000 | Loss: 0.00002721
Iteration 152/1000 | Loss: 0.00002720
Iteration 153/1000 | Loss: 0.00002720
Iteration 154/1000 | Loss: 0.00002720
Iteration 155/1000 | Loss: 0.00002718
Iteration 156/1000 | Loss: 0.00002718
Iteration 157/1000 | Loss: 0.00002718
Iteration 158/1000 | Loss: 0.00002718
Iteration 159/1000 | Loss: 0.00002718
Iteration 160/1000 | Loss: 0.00002718
Iteration 161/1000 | Loss: 0.00002717
Iteration 162/1000 | Loss: 0.00002717
Iteration 163/1000 | Loss: 0.00002717
Iteration 164/1000 | Loss: 0.00002716
Iteration 165/1000 | Loss: 0.00002716
Iteration 166/1000 | Loss: 0.00002716
Iteration 167/1000 | Loss: 0.00002715
Iteration 168/1000 | Loss: 0.00002715
Iteration 169/1000 | Loss: 0.00002715
Iteration 170/1000 | Loss: 0.00002715
Iteration 171/1000 | Loss: 0.00002714
Iteration 172/1000 | Loss: 0.00002714
Iteration 173/1000 | Loss: 0.00002714
Iteration 174/1000 | Loss: 0.00002713
Iteration 175/1000 | Loss: 0.00002713
Iteration 176/1000 | Loss: 0.00002713
Iteration 177/1000 | Loss: 0.00002713
Iteration 178/1000 | Loss: 0.00002712
Iteration 179/1000 | Loss: 0.00002712
Iteration 180/1000 | Loss: 0.00002712
Iteration 181/1000 | Loss: 0.00002711
Iteration 182/1000 | Loss: 0.00002711
Iteration 183/1000 | Loss: 0.00002710
Iteration 184/1000 | Loss: 0.00002708
Iteration 185/1000 | Loss: 0.00002708
Iteration 186/1000 | Loss: 0.00002708
Iteration 187/1000 | Loss: 0.00002708
Iteration 188/1000 | Loss: 0.00002707
Iteration 189/1000 | Loss: 0.00002706
Iteration 190/1000 | Loss: 0.00002705
Iteration 191/1000 | Loss: 0.00002705
Iteration 192/1000 | Loss: 0.00002705
Iteration 193/1000 | Loss: 0.00002704
Iteration 194/1000 | Loss: 0.00002704
Iteration 195/1000 | Loss: 0.00002704
Iteration 196/1000 | Loss: 0.00002703
Iteration 197/1000 | Loss: 0.00002703
Iteration 198/1000 | Loss: 0.00002703
Iteration 199/1000 | Loss: 0.00002703
Iteration 200/1000 | Loss: 0.00002702
Iteration 201/1000 | Loss: 0.00002702
Iteration 202/1000 | Loss: 0.00002702
Iteration 203/1000 | Loss: 0.00002702
Iteration 204/1000 | Loss: 0.00002702
Iteration 205/1000 | Loss: 0.00002702
Iteration 206/1000 | Loss: 0.00002701
Iteration 207/1000 | Loss: 0.00002701
Iteration 208/1000 | Loss: 0.00002701
Iteration 209/1000 | Loss: 0.00002701
Iteration 210/1000 | Loss: 0.00002701
Iteration 211/1000 | Loss: 0.00002701
Iteration 212/1000 | Loss: 0.00002700
Iteration 213/1000 | Loss: 0.00002700
Iteration 214/1000 | Loss: 0.00002700
Iteration 215/1000 | Loss: 0.00002700
Iteration 216/1000 | Loss: 0.00002700
Iteration 217/1000 | Loss: 0.00002700
Iteration 218/1000 | Loss: 0.00002700
Iteration 219/1000 | Loss: 0.00002700
Iteration 220/1000 | Loss: 0.00002700
Iteration 221/1000 | Loss: 0.00002700
Iteration 222/1000 | Loss: 0.00002700
Iteration 223/1000 | Loss: 0.00002699
Iteration 224/1000 | Loss: 0.00002699
Iteration 225/1000 | Loss: 0.00002699
Iteration 226/1000 | Loss: 0.00002699
Iteration 227/1000 | Loss: 0.00002699
Iteration 228/1000 | Loss: 0.00002699
Iteration 229/1000 | Loss: 0.00002699
Iteration 230/1000 | Loss: 0.00002699
Iteration 231/1000 | Loss: 0.00002699
Iteration 232/1000 | Loss: 0.00002699
Iteration 233/1000 | Loss: 0.00002699
Iteration 234/1000 | Loss: 0.00002698
Iteration 235/1000 | Loss: 0.00002698
Iteration 236/1000 | Loss: 0.00002698
Iteration 237/1000 | Loss: 0.00002698
Iteration 238/1000 | Loss: 0.00002698
Iteration 239/1000 | Loss: 0.00002698
Iteration 240/1000 | Loss: 0.00002698
Iteration 241/1000 | Loss: 0.00002698
Iteration 242/1000 | Loss: 0.00002698
Iteration 243/1000 | Loss: 0.00002698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.6984869691659696e-05, 2.6984869691659696e-05, 2.6984869691659696e-05, 2.6984869691659696e-05, 2.6984869691659696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6984869691659696e-05

Optimization complete. Final v2v error: 3.8031914234161377 mm

Highest mean error: 12.74576187133789 mm for frame 141

Lowest mean error: 3.026733636856079 mm for frame 239

Saving results

Total time: 185.81911754608154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954345
Iteration 2/25 | Loss: 0.00954345
Iteration 3/25 | Loss: 0.00954345
Iteration 4/25 | Loss: 0.00954345
Iteration 5/25 | Loss: 0.00954345
Iteration 6/25 | Loss: 0.00954345
Iteration 7/25 | Loss: 0.00954345
Iteration 8/25 | Loss: 0.00954345
Iteration 9/25 | Loss: 0.00954345
Iteration 10/25 | Loss: 0.00954345
Iteration 11/25 | Loss: 0.00954345
Iteration 12/25 | Loss: 0.00954345
Iteration 13/25 | Loss: 0.00954344
Iteration 14/25 | Loss: 0.00954344
Iteration 15/25 | Loss: 0.00954344
Iteration 16/25 | Loss: 0.00954344
Iteration 17/25 | Loss: 0.00954344
Iteration 18/25 | Loss: 0.00954344
Iteration 19/25 | Loss: 0.00954344
Iteration 20/25 | Loss: 0.00954344
Iteration 21/25 | Loss: 0.00954344
Iteration 22/25 | Loss: 0.00954344
Iteration 23/25 | Loss: 0.00954344
Iteration 24/25 | Loss: 0.00954344
Iteration 25/25 | Loss: 0.00954344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52140307
Iteration 2/25 | Loss: 0.15390462
Iteration 3/25 | Loss: 0.14888388
Iteration 4/25 | Loss: 0.14679445
Iteration 5/25 | Loss: 0.14693269
Iteration 6/25 | Loss: 0.14687860
Iteration 7/25 | Loss: 0.14685181
Iteration 8/25 | Loss: 0.14674197
Iteration 9/25 | Loss: 0.14674194
Iteration 10/25 | Loss: 0.14674196
Iteration 11/25 | Loss: 0.14674191
Iteration 12/25 | Loss: 0.14674191
Iteration 13/25 | Loss: 0.14679134
Iteration 14/25 | Loss: 0.14679131
Iteration 15/25 | Loss: 0.14674330
Iteration 16/25 | Loss: 0.14674194
Iteration 17/25 | Loss: 0.14674190
Iteration 18/25 | Loss: 0.14674190
Iteration 19/25 | Loss: 0.14674188
Iteration 20/25 | Loss: 0.14674188
Iteration 21/25 | Loss: 0.14674187
Iteration 22/25 | Loss: 0.14674187
Iteration 23/25 | Loss: 0.14674187
Iteration 24/25 | Loss: 0.14674187
Iteration 25/25 | Loss: 0.14674187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14674187
Iteration 2/1000 | Loss: 0.00722261
Iteration 3/1000 | Loss: 0.00157137
Iteration 4/1000 | Loss: 0.00072981
Iteration 5/1000 | Loss: 0.00330246
Iteration 6/1000 | Loss: 0.00257429
Iteration 7/1000 | Loss: 0.00047925
Iteration 8/1000 | Loss: 0.00072374
Iteration 9/1000 | Loss: 0.00055123
Iteration 10/1000 | Loss: 0.00047016
Iteration 11/1000 | Loss: 0.00222395
Iteration 12/1000 | Loss: 0.00018355
Iteration 13/1000 | Loss: 0.00008526
Iteration 14/1000 | Loss: 0.00026894
Iteration 15/1000 | Loss: 0.00023555
Iteration 16/1000 | Loss: 0.00011462
Iteration 17/1000 | Loss: 0.00009648
Iteration 18/1000 | Loss: 0.00007947
Iteration 19/1000 | Loss: 0.00093688
Iteration 20/1000 | Loss: 0.00061404
Iteration 21/1000 | Loss: 0.00005261
Iteration 22/1000 | Loss: 0.00017660
Iteration 23/1000 | Loss: 0.00004445
Iteration 24/1000 | Loss: 0.00007790
Iteration 25/1000 | Loss: 0.00017296
Iteration 26/1000 | Loss: 0.00026394
Iteration 27/1000 | Loss: 0.00055170
Iteration 28/1000 | Loss: 0.00012026
Iteration 29/1000 | Loss: 0.00022837
Iteration 30/1000 | Loss: 0.00053974
Iteration 31/1000 | Loss: 0.00003774
Iteration 32/1000 | Loss: 0.00004696
Iteration 33/1000 | Loss: 0.00007558
Iteration 34/1000 | Loss: 0.00003120
Iteration 35/1000 | Loss: 0.00026466
Iteration 36/1000 | Loss: 0.00121131
Iteration 37/1000 | Loss: 0.00008910
Iteration 38/1000 | Loss: 0.00009090
Iteration 39/1000 | Loss: 0.00002376
Iteration 40/1000 | Loss: 0.00016898
Iteration 41/1000 | Loss: 0.00002957
Iteration 42/1000 | Loss: 0.00006482
Iteration 43/1000 | Loss: 0.00008992
Iteration 44/1000 | Loss: 0.00012746
Iteration 45/1000 | Loss: 0.00002277
Iteration 46/1000 | Loss: 0.00004151
Iteration 47/1000 | Loss: 0.00003703
Iteration 48/1000 | Loss: 0.00005009
Iteration 49/1000 | Loss: 0.00002370
Iteration 50/1000 | Loss: 0.00002092
Iteration 51/1000 | Loss: 0.00024633
Iteration 52/1000 | Loss: 0.00012303
Iteration 53/1000 | Loss: 0.00002311
Iteration 54/1000 | Loss: 0.00010978
Iteration 55/1000 | Loss: 0.00053711
Iteration 56/1000 | Loss: 0.00005865
Iteration 57/1000 | Loss: 0.00002046
Iteration 58/1000 | Loss: 0.00029245
Iteration 59/1000 | Loss: 0.00008678
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00015519
Iteration 62/1000 | Loss: 0.00004098
Iteration 63/1000 | Loss: 0.00005610
Iteration 64/1000 | Loss: 0.00002978
Iteration 65/1000 | Loss: 0.00028097
Iteration 66/1000 | Loss: 0.00004267
Iteration 67/1000 | Loss: 0.00007058
Iteration 68/1000 | Loss: 0.00003857
Iteration 69/1000 | Loss: 0.00008140
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00003440
Iteration 72/1000 | Loss: 0.00001953
Iteration 73/1000 | Loss: 0.00001940
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00004756
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001909
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001907
Iteration 81/1000 | Loss: 0.00001907
Iteration 82/1000 | Loss: 0.00001907
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00010706
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001882
Iteration 101/1000 | Loss: 0.00001882
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001875
Iteration 108/1000 | Loss: 0.00007100
Iteration 109/1000 | Loss: 0.00001873
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001870
Iteration 112/1000 | Loss: 0.00001869
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001867
Iteration 115/1000 | Loss: 0.00001867
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001866
Iteration 118/1000 | Loss: 0.00001866
Iteration 119/1000 | Loss: 0.00001866
Iteration 120/1000 | Loss: 0.00001866
Iteration 121/1000 | Loss: 0.00001866
Iteration 122/1000 | Loss: 0.00001866
Iteration 123/1000 | Loss: 0.00001866
Iteration 124/1000 | Loss: 0.00001866
Iteration 125/1000 | Loss: 0.00001866
Iteration 126/1000 | Loss: 0.00001865
Iteration 127/1000 | Loss: 0.00001865
Iteration 128/1000 | Loss: 0.00001864
Iteration 129/1000 | Loss: 0.00001864
Iteration 130/1000 | Loss: 0.00001862
Iteration 131/1000 | Loss: 0.00001861
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001861
Iteration 134/1000 | Loss: 0.00001861
Iteration 135/1000 | Loss: 0.00001861
Iteration 136/1000 | Loss: 0.00001861
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001861
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001860
Iteration 143/1000 | Loss: 0.00001860
Iteration 144/1000 | Loss: 0.00001860
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001859
Iteration 147/1000 | Loss: 0.00001859
Iteration 148/1000 | Loss: 0.00001859
Iteration 149/1000 | Loss: 0.00001858
Iteration 150/1000 | Loss: 0.00001858
Iteration 151/1000 | Loss: 0.00001858
Iteration 152/1000 | Loss: 0.00001858
Iteration 153/1000 | Loss: 0.00001858
Iteration 154/1000 | Loss: 0.00001858
Iteration 155/1000 | Loss: 0.00001858
Iteration 156/1000 | Loss: 0.00001858
Iteration 157/1000 | Loss: 0.00001858
Iteration 158/1000 | Loss: 0.00001857
Iteration 159/1000 | Loss: 0.00001857
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001855
Iteration 165/1000 | Loss: 0.00001855
Iteration 166/1000 | Loss: 0.00001855
Iteration 167/1000 | Loss: 0.00001855
Iteration 168/1000 | Loss: 0.00001855
Iteration 169/1000 | Loss: 0.00001855
Iteration 170/1000 | Loss: 0.00001854
Iteration 171/1000 | Loss: 0.00001854
Iteration 172/1000 | Loss: 0.00001854
Iteration 173/1000 | Loss: 0.00001854
Iteration 174/1000 | Loss: 0.00001854
Iteration 175/1000 | Loss: 0.00001854
Iteration 176/1000 | Loss: 0.00001854
Iteration 177/1000 | Loss: 0.00001854
Iteration 178/1000 | Loss: 0.00001853
Iteration 179/1000 | Loss: 0.00001853
Iteration 180/1000 | Loss: 0.00001853
Iteration 181/1000 | Loss: 0.00001853
Iteration 182/1000 | Loss: 0.00001853
Iteration 183/1000 | Loss: 0.00001853
Iteration 184/1000 | Loss: 0.00001853
Iteration 185/1000 | Loss: 0.00001853
Iteration 186/1000 | Loss: 0.00001853
Iteration 187/1000 | Loss: 0.00001852
Iteration 188/1000 | Loss: 0.00001852
Iteration 189/1000 | Loss: 0.00001852
Iteration 190/1000 | Loss: 0.00001852
Iteration 191/1000 | Loss: 0.00001852
Iteration 192/1000 | Loss: 0.00001852
Iteration 193/1000 | Loss: 0.00001851
Iteration 194/1000 | Loss: 0.00001851
Iteration 195/1000 | Loss: 0.00001851
Iteration 196/1000 | Loss: 0.00001851
Iteration 197/1000 | Loss: 0.00001851
Iteration 198/1000 | Loss: 0.00001851
Iteration 199/1000 | Loss: 0.00001851
Iteration 200/1000 | Loss: 0.00001851
Iteration 201/1000 | Loss: 0.00001850
Iteration 202/1000 | Loss: 0.00001850
Iteration 203/1000 | Loss: 0.00006958
Iteration 204/1000 | Loss: 0.00026430
Iteration 205/1000 | Loss: 0.00005418
Iteration 206/1000 | Loss: 0.00002598
Iteration 207/1000 | Loss: 0.00003543
Iteration 208/1000 | Loss: 0.00035139
Iteration 209/1000 | Loss: 0.00002213
Iteration 210/1000 | Loss: 0.00003791
Iteration 211/1000 | Loss: 0.00001959
Iteration 212/1000 | Loss: 0.00001859
Iteration 213/1000 | Loss: 0.00002463
Iteration 214/1000 | Loss: 0.00002182
Iteration 215/1000 | Loss: 0.00001851
Iteration 216/1000 | Loss: 0.00001851
Iteration 217/1000 | Loss: 0.00001851
Iteration 218/1000 | Loss: 0.00001851
Iteration 219/1000 | Loss: 0.00001851
Iteration 220/1000 | Loss: 0.00001851
Iteration 221/1000 | Loss: 0.00001851
Iteration 222/1000 | Loss: 0.00001851
Iteration 223/1000 | Loss: 0.00001851
Iteration 224/1000 | Loss: 0.00001850
Iteration 225/1000 | Loss: 0.00001850
Iteration 226/1000 | Loss: 0.00001850
Iteration 227/1000 | Loss: 0.00001849
Iteration 228/1000 | Loss: 0.00001848
Iteration 229/1000 | Loss: 0.00001848
Iteration 230/1000 | Loss: 0.00001848
Iteration 231/1000 | Loss: 0.00001848
Iteration 232/1000 | Loss: 0.00001848
Iteration 233/1000 | Loss: 0.00001848
Iteration 234/1000 | Loss: 0.00001848
Iteration 235/1000 | Loss: 0.00001847
Iteration 236/1000 | Loss: 0.00001847
Iteration 237/1000 | Loss: 0.00001847
Iteration 238/1000 | Loss: 0.00001847
Iteration 239/1000 | Loss: 0.00001847
Iteration 240/1000 | Loss: 0.00001846
Iteration 241/1000 | Loss: 0.00001846
Iteration 242/1000 | Loss: 0.00001846
Iteration 243/1000 | Loss: 0.00001846
Iteration 244/1000 | Loss: 0.00001846
Iteration 245/1000 | Loss: 0.00001846
Iteration 246/1000 | Loss: 0.00001846
Iteration 247/1000 | Loss: 0.00001846
Iteration 248/1000 | Loss: 0.00001846
Iteration 249/1000 | Loss: 0.00001846
Iteration 250/1000 | Loss: 0.00001846
Iteration 251/1000 | Loss: 0.00001846
Iteration 252/1000 | Loss: 0.00001846
Iteration 253/1000 | Loss: 0.00001845
Iteration 254/1000 | Loss: 0.00001845
Iteration 255/1000 | Loss: 0.00001845
Iteration 256/1000 | Loss: 0.00001845
Iteration 257/1000 | Loss: 0.00001845
Iteration 258/1000 | Loss: 0.00001845
Iteration 259/1000 | Loss: 0.00001845
Iteration 260/1000 | Loss: 0.00001845
Iteration 261/1000 | Loss: 0.00001845
Iteration 262/1000 | Loss: 0.00001845
Iteration 263/1000 | Loss: 0.00001845
Iteration 264/1000 | Loss: 0.00001844
Iteration 265/1000 | Loss: 0.00001844
Iteration 266/1000 | Loss: 0.00001844
Iteration 267/1000 | Loss: 0.00001844
Iteration 268/1000 | Loss: 0.00001844
Iteration 269/1000 | Loss: 0.00001844
Iteration 270/1000 | Loss: 0.00001844
Iteration 271/1000 | Loss: 0.00001844
Iteration 272/1000 | Loss: 0.00001844
Iteration 273/1000 | Loss: 0.00001844
Iteration 274/1000 | Loss: 0.00001844
Iteration 275/1000 | Loss: 0.00001844
Iteration 276/1000 | Loss: 0.00001844
Iteration 277/1000 | Loss: 0.00001844
Iteration 278/1000 | Loss: 0.00001844
Iteration 279/1000 | Loss: 0.00001844
Iteration 280/1000 | Loss: 0.00001843
Iteration 281/1000 | Loss: 0.00001843
Iteration 282/1000 | Loss: 0.00001843
Iteration 283/1000 | Loss: 0.00001843
Iteration 284/1000 | Loss: 0.00001843
Iteration 285/1000 | Loss: 0.00001843
Iteration 286/1000 | Loss: 0.00001843
Iteration 287/1000 | Loss: 0.00001843
Iteration 288/1000 | Loss: 0.00001843
Iteration 289/1000 | Loss: 0.00001843
Iteration 290/1000 | Loss: 0.00001843
Iteration 291/1000 | Loss: 0.00001843
Iteration 292/1000 | Loss: 0.00001843
Iteration 293/1000 | Loss: 0.00001843
Iteration 294/1000 | Loss: 0.00001843
Iteration 295/1000 | Loss: 0.00001843
Iteration 296/1000 | Loss: 0.00001843
Iteration 297/1000 | Loss: 0.00001843
Iteration 298/1000 | Loss: 0.00001843
Iteration 299/1000 | Loss: 0.00001843
Iteration 300/1000 | Loss: 0.00001843
Iteration 301/1000 | Loss: 0.00001843
Iteration 302/1000 | Loss: 0.00001843
Iteration 303/1000 | Loss: 0.00001843
Iteration 304/1000 | Loss: 0.00001843
Iteration 305/1000 | Loss: 0.00001843
Iteration 306/1000 | Loss: 0.00001843
Iteration 307/1000 | Loss: 0.00001843
Iteration 308/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [1.8434093362884596e-05, 1.8434093362884596e-05, 1.8434093362884596e-05, 1.8434093362884596e-05, 1.8434093362884596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8434093362884596e-05

Optimization complete. Final v2v error: 3.690056800842285 mm

Highest mean error: 4.065618515014648 mm for frame 28

Lowest mean error: 3.323751211166382 mm for frame 218

Saving results

Total time: 177.8504180908203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835683
Iteration 2/25 | Loss: 0.00138340
Iteration 3/25 | Loss: 0.00131938
Iteration 4/25 | Loss: 0.00131135
Iteration 5/25 | Loss: 0.00130862
Iteration 6/25 | Loss: 0.00130807
Iteration 7/25 | Loss: 0.00130807
Iteration 8/25 | Loss: 0.00130807
Iteration 9/25 | Loss: 0.00130807
Iteration 10/25 | Loss: 0.00130807
Iteration 11/25 | Loss: 0.00130807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013080660719424486, 0.0013080660719424486, 0.0013080660719424486, 0.0013080660719424486, 0.0013080660719424486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013080660719424486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57028461
Iteration 2/25 | Loss: 0.00211921
Iteration 3/25 | Loss: 0.00211921
Iteration 4/25 | Loss: 0.00211921
Iteration 5/25 | Loss: 0.00211920
Iteration 6/25 | Loss: 0.00211920
Iteration 7/25 | Loss: 0.00211920
Iteration 8/25 | Loss: 0.00211920
Iteration 9/25 | Loss: 0.00211920
Iteration 10/25 | Loss: 0.00211920
Iteration 11/25 | Loss: 0.00211920
Iteration 12/25 | Loss: 0.00211920
Iteration 13/25 | Loss: 0.00211920
Iteration 14/25 | Loss: 0.00211920
Iteration 15/25 | Loss: 0.00211920
Iteration 16/25 | Loss: 0.00211920
Iteration 17/25 | Loss: 0.00211920
Iteration 18/25 | Loss: 0.00211920
Iteration 19/25 | Loss: 0.00211920
Iteration 20/25 | Loss: 0.00211920
Iteration 21/25 | Loss: 0.00211920
Iteration 22/25 | Loss: 0.00211920
Iteration 23/25 | Loss: 0.00211920
Iteration 24/25 | Loss: 0.00211920
Iteration 25/25 | Loss: 0.00211920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211920
Iteration 2/1000 | Loss: 0.00001938
Iteration 3/1000 | Loss: 0.00001496
Iteration 4/1000 | Loss: 0.00001363
Iteration 5/1000 | Loss: 0.00001293
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001170
Iteration 8/1000 | Loss: 0.00001138
Iteration 9/1000 | Loss: 0.00001105
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001061
Iteration 12/1000 | Loss: 0.00001056
Iteration 13/1000 | Loss: 0.00001050
Iteration 14/1000 | Loss: 0.00001044
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001029
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001026
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001017
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001012
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001009
Iteration 32/1000 | Loss: 0.00001009
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001008
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001008
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001007
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001004
Iteration 50/1000 | Loss: 0.00001003
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001003
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00000999
Iteration 61/1000 | Loss: 0.00000999
Iteration 62/1000 | Loss: 0.00000998
Iteration 63/1000 | Loss: 0.00000998
Iteration 64/1000 | Loss: 0.00000998
Iteration 65/1000 | Loss: 0.00000998
Iteration 66/1000 | Loss: 0.00000998
Iteration 67/1000 | Loss: 0.00000998
Iteration 68/1000 | Loss: 0.00000998
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000997
Iteration 71/1000 | Loss: 0.00000997
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000996
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000993
Iteration 85/1000 | Loss: 0.00000993
Iteration 86/1000 | Loss: 0.00000993
Iteration 87/1000 | Loss: 0.00000993
Iteration 88/1000 | Loss: 0.00000993
Iteration 89/1000 | Loss: 0.00000992
Iteration 90/1000 | Loss: 0.00000992
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000992
Iteration 93/1000 | Loss: 0.00000992
Iteration 94/1000 | Loss: 0.00000992
Iteration 95/1000 | Loss: 0.00000991
Iteration 96/1000 | Loss: 0.00000991
Iteration 97/1000 | Loss: 0.00000991
Iteration 98/1000 | Loss: 0.00000991
Iteration 99/1000 | Loss: 0.00000991
Iteration 100/1000 | Loss: 0.00000991
Iteration 101/1000 | Loss: 0.00000991
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000990
Iteration 104/1000 | Loss: 0.00000990
Iteration 105/1000 | Loss: 0.00000990
Iteration 106/1000 | Loss: 0.00000990
Iteration 107/1000 | Loss: 0.00000990
Iteration 108/1000 | Loss: 0.00000990
Iteration 109/1000 | Loss: 0.00000990
Iteration 110/1000 | Loss: 0.00000990
Iteration 111/1000 | Loss: 0.00000990
Iteration 112/1000 | Loss: 0.00000990
Iteration 113/1000 | Loss: 0.00000990
Iteration 114/1000 | Loss: 0.00000990
Iteration 115/1000 | Loss: 0.00000990
Iteration 116/1000 | Loss: 0.00000989
Iteration 117/1000 | Loss: 0.00000989
Iteration 118/1000 | Loss: 0.00000989
Iteration 119/1000 | Loss: 0.00000989
Iteration 120/1000 | Loss: 0.00000989
Iteration 121/1000 | Loss: 0.00000989
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000989
Iteration 124/1000 | Loss: 0.00000988
Iteration 125/1000 | Loss: 0.00000988
Iteration 126/1000 | Loss: 0.00000988
Iteration 127/1000 | Loss: 0.00000988
Iteration 128/1000 | Loss: 0.00000988
Iteration 129/1000 | Loss: 0.00000988
Iteration 130/1000 | Loss: 0.00000988
Iteration 131/1000 | Loss: 0.00000988
Iteration 132/1000 | Loss: 0.00000988
Iteration 133/1000 | Loss: 0.00000988
Iteration 134/1000 | Loss: 0.00000988
Iteration 135/1000 | Loss: 0.00000987
Iteration 136/1000 | Loss: 0.00000987
Iteration 137/1000 | Loss: 0.00000987
Iteration 138/1000 | Loss: 0.00000987
Iteration 139/1000 | Loss: 0.00000987
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000987
Iteration 142/1000 | Loss: 0.00000987
Iteration 143/1000 | Loss: 0.00000987
Iteration 144/1000 | Loss: 0.00000987
Iteration 145/1000 | Loss: 0.00000987
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000986
Iteration 148/1000 | Loss: 0.00000986
Iteration 149/1000 | Loss: 0.00000986
Iteration 150/1000 | Loss: 0.00000986
Iteration 151/1000 | Loss: 0.00000986
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000986
Iteration 156/1000 | Loss: 0.00000986
Iteration 157/1000 | Loss: 0.00000986
Iteration 158/1000 | Loss: 0.00000986
Iteration 159/1000 | Loss: 0.00000986
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000985
Iteration 165/1000 | Loss: 0.00000985
Iteration 166/1000 | Loss: 0.00000985
Iteration 167/1000 | Loss: 0.00000985
Iteration 168/1000 | Loss: 0.00000985
Iteration 169/1000 | Loss: 0.00000985
Iteration 170/1000 | Loss: 0.00000984
Iteration 171/1000 | Loss: 0.00000984
Iteration 172/1000 | Loss: 0.00000984
Iteration 173/1000 | Loss: 0.00000984
Iteration 174/1000 | Loss: 0.00000984
Iteration 175/1000 | Loss: 0.00000984
Iteration 176/1000 | Loss: 0.00000984
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000983
Iteration 182/1000 | Loss: 0.00000983
Iteration 183/1000 | Loss: 0.00000983
Iteration 184/1000 | Loss: 0.00000983
Iteration 185/1000 | Loss: 0.00000983
Iteration 186/1000 | Loss: 0.00000983
Iteration 187/1000 | Loss: 0.00000983
Iteration 188/1000 | Loss: 0.00000982
Iteration 189/1000 | Loss: 0.00000982
Iteration 190/1000 | Loss: 0.00000982
Iteration 191/1000 | Loss: 0.00000982
Iteration 192/1000 | Loss: 0.00000982
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000982
Iteration 195/1000 | Loss: 0.00000982
Iteration 196/1000 | Loss: 0.00000982
Iteration 197/1000 | Loss: 0.00000981
Iteration 198/1000 | Loss: 0.00000981
Iteration 199/1000 | Loss: 0.00000981
Iteration 200/1000 | Loss: 0.00000981
Iteration 201/1000 | Loss: 0.00000980
Iteration 202/1000 | Loss: 0.00000980
Iteration 203/1000 | Loss: 0.00000980
Iteration 204/1000 | Loss: 0.00000980
Iteration 205/1000 | Loss: 0.00000980
Iteration 206/1000 | Loss: 0.00000980
Iteration 207/1000 | Loss: 0.00000980
Iteration 208/1000 | Loss: 0.00000980
Iteration 209/1000 | Loss: 0.00000980
Iteration 210/1000 | Loss: 0.00000979
Iteration 211/1000 | Loss: 0.00000979
Iteration 212/1000 | Loss: 0.00000978
Iteration 213/1000 | Loss: 0.00000978
Iteration 214/1000 | Loss: 0.00000978
Iteration 215/1000 | Loss: 0.00000978
Iteration 216/1000 | Loss: 0.00000978
Iteration 217/1000 | Loss: 0.00000978
Iteration 218/1000 | Loss: 0.00000978
Iteration 219/1000 | Loss: 0.00000978
Iteration 220/1000 | Loss: 0.00000978
Iteration 221/1000 | Loss: 0.00000978
Iteration 222/1000 | Loss: 0.00000977
Iteration 223/1000 | Loss: 0.00000977
Iteration 224/1000 | Loss: 0.00000977
Iteration 225/1000 | Loss: 0.00000977
Iteration 226/1000 | Loss: 0.00000977
Iteration 227/1000 | Loss: 0.00000977
Iteration 228/1000 | Loss: 0.00000977
Iteration 229/1000 | Loss: 0.00000976
Iteration 230/1000 | Loss: 0.00000976
Iteration 231/1000 | Loss: 0.00000976
Iteration 232/1000 | Loss: 0.00000976
Iteration 233/1000 | Loss: 0.00000976
Iteration 234/1000 | Loss: 0.00000976
Iteration 235/1000 | Loss: 0.00000976
Iteration 236/1000 | Loss: 0.00000976
Iteration 237/1000 | Loss: 0.00000976
Iteration 238/1000 | Loss: 0.00000976
Iteration 239/1000 | Loss: 0.00000976
Iteration 240/1000 | Loss: 0.00000976
Iteration 241/1000 | Loss: 0.00000976
Iteration 242/1000 | Loss: 0.00000976
Iteration 243/1000 | Loss: 0.00000976
Iteration 244/1000 | Loss: 0.00000975
Iteration 245/1000 | Loss: 0.00000975
Iteration 246/1000 | Loss: 0.00000975
Iteration 247/1000 | Loss: 0.00000975
Iteration 248/1000 | Loss: 0.00000975
Iteration 249/1000 | Loss: 0.00000975
Iteration 250/1000 | Loss: 0.00000975
Iteration 251/1000 | Loss: 0.00000975
Iteration 252/1000 | Loss: 0.00000975
Iteration 253/1000 | Loss: 0.00000975
Iteration 254/1000 | Loss: 0.00000975
Iteration 255/1000 | Loss: 0.00000975
Iteration 256/1000 | Loss: 0.00000975
Iteration 257/1000 | Loss: 0.00000974
Iteration 258/1000 | Loss: 0.00000974
Iteration 259/1000 | Loss: 0.00000974
Iteration 260/1000 | Loss: 0.00000974
Iteration 261/1000 | Loss: 0.00000974
Iteration 262/1000 | Loss: 0.00000974
Iteration 263/1000 | Loss: 0.00000974
Iteration 264/1000 | Loss: 0.00000974
Iteration 265/1000 | Loss: 0.00000974
Iteration 266/1000 | Loss: 0.00000974
Iteration 267/1000 | Loss: 0.00000974
Iteration 268/1000 | Loss: 0.00000974
Iteration 269/1000 | Loss: 0.00000974
Iteration 270/1000 | Loss: 0.00000974
Iteration 271/1000 | Loss: 0.00000974
Iteration 272/1000 | Loss: 0.00000974
Iteration 273/1000 | Loss: 0.00000974
Iteration 274/1000 | Loss: 0.00000974
Iteration 275/1000 | Loss: 0.00000974
Iteration 276/1000 | Loss: 0.00000974
Iteration 277/1000 | Loss: 0.00000974
Iteration 278/1000 | Loss: 0.00000974
Iteration 279/1000 | Loss: 0.00000974
Iteration 280/1000 | Loss: 0.00000974
Iteration 281/1000 | Loss: 0.00000974
Iteration 282/1000 | Loss: 0.00000974
Iteration 283/1000 | Loss: 0.00000974
Iteration 284/1000 | Loss: 0.00000974
Iteration 285/1000 | Loss: 0.00000974
Iteration 286/1000 | Loss: 0.00000974
Iteration 287/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [9.73623446043348e-06, 9.73623446043348e-06, 9.73623446043348e-06, 9.73623446043348e-06, 9.73623446043348e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.73623446043348e-06

Optimization complete. Final v2v error: 2.7077863216400146 mm

Highest mean error: 3.0558884143829346 mm for frame 58

Lowest mean error: 2.531979560852051 mm for frame 129

Saving results

Total time: 44.95390820503235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594447
Iteration 2/25 | Loss: 0.00138669
Iteration 3/25 | Loss: 0.00132300
Iteration 4/25 | Loss: 0.00131235
Iteration 5/25 | Loss: 0.00130881
Iteration 6/25 | Loss: 0.00130801
Iteration 7/25 | Loss: 0.00130801
Iteration 8/25 | Loss: 0.00130801
Iteration 9/25 | Loss: 0.00130801
Iteration 10/25 | Loss: 0.00130801
Iteration 11/25 | Loss: 0.00130801
Iteration 12/25 | Loss: 0.00130801
Iteration 13/25 | Loss: 0.00130801
Iteration 14/25 | Loss: 0.00130801
Iteration 15/25 | Loss: 0.00130801
Iteration 16/25 | Loss: 0.00130801
Iteration 17/25 | Loss: 0.00130801
Iteration 18/25 | Loss: 0.00130801
Iteration 19/25 | Loss: 0.00130801
Iteration 20/25 | Loss: 0.00130801
Iteration 21/25 | Loss: 0.00130801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013080071657896042, 0.0013080071657896042, 0.0013080071657896042, 0.0013080071657896042, 0.0013080071657896042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013080071657896042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98182511
Iteration 2/25 | Loss: 0.00221044
Iteration 3/25 | Loss: 0.00221044
Iteration 4/25 | Loss: 0.00221044
Iteration 5/25 | Loss: 0.00221044
Iteration 6/25 | Loss: 0.00221044
Iteration 7/25 | Loss: 0.00221044
Iteration 8/25 | Loss: 0.00221044
Iteration 9/25 | Loss: 0.00221044
Iteration 10/25 | Loss: 0.00221044
Iteration 11/25 | Loss: 0.00221044
Iteration 12/25 | Loss: 0.00221044
Iteration 13/25 | Loss: 0.00221044
Iteration 14/25 | Loss: 0.00221044
Iteration 15/25 | Loss: 0.00221044
Iteration 16/25 | Loss: 0.00221044
Iteration 17/25 | Loss: 0.00221044
Iteration 18/25 | Loss: 0.00221044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002210441045463085, 0.002210441045463085, 0.002210441045463085, 0.002210441045463085, 0.002210441045463085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002210441045463085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221044
Iteration 2/1000 | Loss: 0.00002236
Iteration 3/1000 | Loss: 0.00001686
Iteration 4/1000 | Loss: 0.00001523
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001294
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001202
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001130
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001129
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001119
Iteration 29/1000 | Loss: 0.00001117
Iteration 30/1000 | Loss: 0.00001115
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001102
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001091
Iteration 70/1000 | Loss: 0.00001091
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001091
Iteration 73/1000 | Loss: 0.00001091
Iteration 74/1000 | Loss: 0.00001091
Iteration 75/1000 | Loss: 0.00001090
Iteration 76/1000 | Loss: 0.00001090
Iteration 77/1000 | Loss: 0.00001090
Iteration 78/1000 | Loss: 0.00001090
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001087
Iteration 89/1000 | Loss: 0.00001087
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001087
Iteration 95/1000 | Loss: 0.00001087
Iteration 96/1000 | Loss: 0.00001087
Iteration 97/1000 | Loss: 0.00001086
Iteration 98/1000 | Loss: 0.00001086
Iteration 99/1000 | Loss: 0.00001086
Iteration 100/1000 | Loss: 0.00001086
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001085
Iteration 103/1000 | Loss: 0.00001085
Iteration 104/1000 | Loss: 0.00001085
Iteration 105/1000 | Loss: 0.00001085
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001084
Iteration 111/1000 | Loss: 0.00001084
Iteration 112/1000 | Loss: 0.00001084
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001084
Iteration 120/1000 | Loss: 0.00001084
Iteration 121/1000 | Loss: 0.00001084
Iteration 122/1000 | Loss: 0.00001084
Iteration 123/1000 | Loss: 0.00001084
Iteration 124/1000 | Loss: 0.00001084
Iteration 125/1000 | Loss: 0.00001084
Iteration 126/1000 | Loss: 0.00001084
Iteration 127/1000 | Loss: 0.00001084
Iteration 128/1000 | Loss: 0.00001084
Iteration 129/1000 | Loss: 0.00001084
Iteration 130/1000 | Loss: 0.00001084
Iteration 131/1000 | Loss: 0.00001084
Iteration 132/1000 | Loss: 0.00001084
Iteration 133/1000 | Loss: 0.00001084
Iteration 134/1000 | Loss: 0.00001084
Iteration 135/1000 | Loss: 0.00001084
Iteration 136/1000 | Loss: 0.00001084
Iteration 137/1000 | Loss: 0.00001084
Iteration 138/1000 | Loss: 0.00001084
Iteration 139/1000 | Loss: 0.00001084
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.0840854884008877e-05, 1.0840854884008877e-05, 1.0840854884008877e-05, 1.0840854884008877e-05, 1.0840854884008877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0840854884008877e-05

Optimization complete. Final v2v error: 2.8796863555908203 mm

Highest mean error: 3.110938549041748 mm for frame 61

Lowest mean error: 2.7347941398620605 mm for frame 118

Saving results

Total time: 38.43272256851196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794684
Iteration 2/25 | Loss: 0.00144958
Iteration 3/25 | Loss: 0.00133612
Iteration 4/25 | Loss: 0.00131635
Iteration 5/25 | Loss: 0.00131183
Iteration 6/25 | Loss: 0.00131047
Iteration 7/25 | Loss: 0.00131040
Iteration 8/25 | Loss: 0.00131040
Iteration 9/25 | Loss: 0.00131040
Iteration 10/25 | Loss: 0.00131040
Iteration 11/25 | Loss: 0.00131040
Iteration 12/25 | Loss: 0.00131040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013103990349918604, 0.0013103990349918604, 0.0013103990349918604, 0.0013103990349918604, 0.0013103990349918604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013103990349918604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20412576
Iteration 2/25 | Loss: 0.00199175
Iteration 3/25 | Loss: 0.00199175
Iteration 4/25 | Loss: 0.00199175
Iteration 5/25 | Loss: 0.00199175
Iteration 6/25 | Loss: 0.00199175
Iteration 7/25 | Loss: 0.00199175
Iteration 8/25 | Loss: 0.00199175
Iteration 9/25 | Loss: 0.00199175
Iteration 10/25 | Loss: 0.00199175
Iteration 11/25 | Loss: 0.00199175
Iteration 12/25 | Loss: 0.00199175
Iteration 13/25 | Loss: 0.00199175
Iteration 14/25 | Loss: 0.00199175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019917457830160856, 0.0019917457830160856, 0.0019917457830160856, 0.0019917457830160856, 0.0019917457830160856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019917457830160856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199175
Iteration 2/1000 | Loss: 0.00004145
Iteration 3/1000 | Loss: 0.00003187
Iteration 4/1000 | Loss: 0.00002578
Iteration 5/1000 | Loss: 0.00002440
Iteration 6/1000 | Loss: 0.00002342
Iteration 7/1000 | Loss: 0.00002255
Iteration 8/1000 | Loss: 0.00002194
Iteration 9/1000 | Loss: 0.00002139
Iteration 10/1000 | Loss: 0.00002099
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002030
Iteration 13/1000 | Loss: 0.00001996
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001932
Iteration 17/1000 | Loss: 0.00001930
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001918
Iteration 21/1000 | Loss: 0.00001917
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001910
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001908
Iteration 27/1000 | Loss: 0.00001907
Iteration 28/1000 | Loss: 0.00001907
Iteration 29/1000 | Loss: 0.00001907
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001905
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001904
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001895
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001893
Iteration 45/1000 | Loss: 0.00001893
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001892
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001892
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001891
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001883
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001882
Iteration 100/1000 | Loss: 0.00001882
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001880
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001879
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001878
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001877
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001873
Iteration 126/1000 | Loss: 0.00001873
Iteration 127/1000 | Loss: 0.00001873
Iteration 128/1000 | Loss: 0.00001872
Iteration 129/1000 | Loss: 0.00001872
Iteration 130/1000 | Loss: 0.00001872
Iteration 131/1000 | Loss: 0.00001871
Iteration 132/1000 | Loss: 0.00001871
Iteration 133/1000 | Loss: 0.00001871
Iteration 134/1000 | Loss: 0.00001871
Iteration 135/1000 | Loss: 0.00001871
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001870
Iteration 140/1000 | Loss: 0.00001870
Iteration 141/1000 | Loss: 0.00001870
Iteration 142/1000 | Loss: 0.00001870
Iteration 143/1000 | Loss: 0.00001870
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001869
Iteration 148/1000 | Loss: 0.00001869
Iteration 149/1000 | Loss: 0.00001869
Iteration 150/1000 | Loss: 0.00001869
Iteration 151/1000 | Loss: 0.00001869
Iteration 152/1000 | Loss: 0.00001869
Iteration 153/1000 | Loss: 0.00001869
Iteration 154/1000 | Loss: 0.00001869
Iteration 155/1000 | Loss: 0.00001869
Iteration 156/1000 | Loss: 0.00001869
Iteration 157/1000 | Loss: 0.00001869
Iteration 158/1000 | Loss: 0.00001868
Iteration 159/1000 | Loss: 0.00001868
Iteration 160/1000 | Loss: 0.00001868
Iteration 161/1000 | Loss: 0.00001868
Iteration 162/1000 | Loss: 0.00001868
Iteration 163/1000 | Loss: 0.00001868
Iteration 164/1000 | Loss: 0.00001868
Iteration 165/1000 | Loss: 0.00001868
Iteration 166/1000 | Loss: 0.00001868
Iteration 167/1000 | Loss: 0.00001868
Iteration 168/1000 | Loss: 0.00001867
Iteration 169/1000 | Loss: 0.00001867
Iteration 170/1000 | Loss: 0.00001867
Iteration 171/1000 | Loss: 0.00001867
Iteration 172/1000 | Loss: 0.00001867
Iteration 173/1000 | Loss: 0.00001867
Iteration 174/1000 | Loss: 0.00001867
Iteration 175/1000 | Loss: 0.00001867
Iteration 176/1000 | Loss: 0.00001867
Iteration 177/1000 | Loss: 0.00001867
Iteration 178/1000 | Loss: 0.00001866
Iteration 179/1000 | Loss: 0.00001866
Iteration 180/1000 | Loss: 0.00001866
Iteration 181/1000 | Loss: 0.00001866
Iteration 182/1000 | Loss: 0.00001866
Iteration 183/1000 | Loss: 0.00001866
Iteration 184/1000 | Loss: 0.00001866
Iteration 185/1000 | Loss: 0.00001866
Iteration 186/1000 | Loss: 0.00001866
Iteration 187/1000 | Loss: 0.00001866
Iteration 188/1000 | Loss: 0.00001866
Iteration 189/1000 | Loss: 0.00001866
Iteration 190/1000 | Loss: 0.00001866
Iteration 191/1000 | Loss: 0.00001866
Iteration 192/1000 | Loss: 0.00001866
Iteration 193/1000 | Loss: 0.00001865
Iteration 194/1000 | Loss: 0.00001865
Iteration 195/1000 | Loss: 0.00001865
Iteration 196/1000 | Loss: 0.00001865
Iteration 197/1000 | Loss: 0.00001865
Iteration 198/1000 | Loss: 0.00001865
Iteration 199/1000 | Loss: 0.00001865
Iteration 200/1000 | Loss: 0.00001865
Iteration 201/1000 | Loss: 0.00001865
Iteration 202/1000 | Loss: 0.00001864
Iteration 203/1000 | Loss: 0.00001864
Iteration 204/1000 | Loss: 0.00001864
Iteration 205/1000 | Loss: 0.00001864
Iteration 206/1000 | Loss: 0.00001864
Iteration 207/1000 | Loss: 0.00001864
Iteration 208/1000 | Loss: 0.00001864
Iteration 209/1000 | Loss: 0.00001864
Iteration 210/1000 | Loss: 0.00001864
Iteration 211/1000 | Loss: 0.00001864
Iteration 212/1000 | Loss: 0.00001864
Iteration 213/1000 | Loss: 0.00001864
Iteration 214/1000 | Loss: 0.00001864
Iteration 215/1000 | Loss: 0.00001864
Iteration 216/1000 | Loss: 0.00001864
Iteration 217/1000 | Loss: 0.00001864
Iteration 218/1000 | Loss: 0.00001864
Iteration 219/1000 | Loss: 0.00001863
Iteration 220/1000 | Loss: 0.00001863
Iteration 221/1000 | Loss: 0.00001863
Iteration 222/1000 | Loss: 0.00001863
Iteration 223/1000 | Loss: 0.00001863
Iteration 224/1000 | Loss: 0.00001863
Iteration 225/1000 | Loss: 0.00001863
Iteration 226/1000 | Loss: 0.00001863
Iteration 227/1000 | Loss: 0.00001863
Iteration 228/1000 | Loss: 0.00001863
Iteration 229/1000 | Loss: 0.00001863
Iteration 230/1000 | Loss: 0.00001863
Iteration 231/1000 | Loss: 0.00001863
Iteration 232/1000 | Loss: 0.00001863
Iteration 233/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.8632290448294953e-05, 1.8632290448294953e-05, 1.8632290448294953e-05, 1.8632290448294953e-05, 1.8632290448294953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8632290448294953e-05

Optimization complete. Final v2v error: 3.751034736633301 mm

Highest mean error: 4.072347640991211 mm for frame 68

Lowest mean error: 3.465769052505493 mm for frame 52

Saving results

Total time: 49.17812991142273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863965
Iteration 2/25 | Loss: 0.00141194
Iteration 3/25 | Loss: 0.00134458
Iteration 4/25 | Loss: 0.00132953
Iteration 5/25 | Loss: 0.00132505
Iteration 6/25 | Loss: 0.00132426
Iteration 7/25 | Loss: 0.00132426
Iteration 8/25 | Loss: 0.00132426
Iteration 9/25 | Loss: 0.00132426
Iteration 10/25 | Loss: 0.00132426
Iteration 11/25 | Loss: 0.00132426
Iteration 12/25 | Loss: 0.00132426
Iteration 13/25 | Loss: 0.00132426
Iteration 14/25 | Loss: 0.00132426
Iteration 15/25 | Loss: 0.00132426
Iteration 16/25 | Loss: 0.00132426
Iteration 17/25 | Loss: 0.00132426
Iteration 18/25 | Loss: 0.00132426
Iteration 19/25 | Loss: 0.00132426
Iteration 20/25 | Loss: 0.00132426
Iteration 21/25 | Loss: 0.00132426
Iteration 22/25 | Loss: 0.00132426
Iteration 23/25 | Loss: 0.00132426
Iteration 24/25 | Loss: 0.00132426
Iteration 25/25 | Loss: 0.00132426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31950545
Iteration 2/25 | Loss: 0.00206005
Iteration 3/25 | Loss: 0.00206005
Iteration 4/25 | Loss: 0.00206005
Iteration 5/25 | Loss: 0.00206005
Iteration 6/25 | Loss: 0.00206005
Iteration 7/25 | Loss: 0.00206005
Iteration 8/25 | Loss: 0.00206005
Iteration 9/25 | Loss: 0.00206005
Iteration 10/25 | Loss: 0.00206005
Iteration 11/25 | Loss: 0.00206005
Iteration 12/25 | Loss: 0.00206005
Iteration 13/25 | Loss: 0.00206005
Iteration 14/25 | Loss: 0.00206005
Iteration 15/25 | Loss: 0.00206005
Iteration 16/25 | Loss: 0.00206005
Iteration 17/25 | Loss: 0.00206005
Iteration 18/25 | Loss: 0.00206005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002060046885162592, 0.002060046885162592, 0.002060046885162592, 0.002060046885162592, 0.002060046885162592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002060046885162592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206005
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00001631
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001411
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001350
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001345
Iteration 23/1000 | Loss: 0.00001343
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001329
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001325
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001322
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001320
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001317
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001309
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Iteration 117/1000 | Loss: 0.00001308
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001308
Iteration 124/1000 | Loss: 0.00001308
Iteration 125/1000 | Loss: 0.00001308
Iteration 126/1000 | Loss: 0.00001308
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001307
Iteration 129/1000 | Loss: 0.00001307
Iteration 130/1000 | Loss: 0.00001307
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001307
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001306
Iteration 146/1000 | Loss: 0.00001306
Iteration 147/1000 | Loss: 0.00001306
Iteration 148/1000 | Loss: 0.00001306
Iteration 149/1000 | Loss: 0.00001306
Iteration 150/1000 | Loss: 0.00001306
Iteration 151/1000 | Loss: 0.00001306
Iteration 152/1000 | Loss: 0.00001306
Iteration 153/1000 | Loss: 0.00001306
Iteration 154/1000 | Loss: 0.00001306
Iteration 155/1000 | Loss: 0.00001306
Iteration 156/1000 | Loss: 0.00001306
Iteration 157/1000 | Loss: 0.00001306
Iteration 158/1000 | Loss: 0.00001306
Iteration 159/1000 | Loss: 0.00001306
Iteration 160/1000 | Loss: 0.00001306
Iteration 161/1000 | Loss: 0.00001306
Iteration 162/1000 | Loss: 0.00001306
Iteration 163/1000 | Loss: 0.00001306
Iteration 164/1000 | Loss: 0.00001306
Iteration 165/1000 | Loss: 0.00001306
Iteration 166/1000 | Loss: 0.00001306
Iteration 167/1000 | Loss: 0.00001306
Iteration 168/1000 | Loss: 0.00001306
Iteration 169/1000 | Loss: 0.00001306
Iteration 170/1000 | Loss: 0.00001306
Iteration 171/1000 | Loss: 0.00001306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3059657248959411e-05, 1.3059657248959411e-05, 1.3059657248959411e-05, 1.3059657248959411e-05, 1.3059657248959411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3059657248959411e-05

Optimization complete. Final v2v error: 3.1215691566467285 mm

Highest mean error: 3.335529327392578 mm for frame 134

Lowest mean error: 2.8776566982269287 mm for frame 60

Saving results

Total time: 41.227135181427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953448
Iteration 2/25 | Loss: 0.00278986
Iteration 3/25 | Loss: 0.00254840
Iteration 4/25 | Loss: 0.00223196
Iteration 5/25 | Loss: 0.00229441
Iteration 6/25 | Loss: 0.00212491
Iteration 7/25 | Loss: 0.00206526
Iteration 8/25 | Loss: 0.00206413
Iteration 9/25 | Loss: 0.00204970
Iteration 10/25 | Loss: 0.00203907
Iteration 11/25 | Loss: 0.00203696
Iteration 12/25 | Loss: 0.00204431
Iteration 13/25 | Loss: 0.00203403
Iteration 14/25 | Loss: 0.00203343
Iteration 15/25 | Loss: 0.00203334
Iteration 16/25 | Loss: 0.00203334
Iteration 17/25 | Loss: 0.00203334
Iteration 18/25 | Loss: 0.00203334
Iteration 19/25 | Loss: 0.00203334
Iteration 20/25 | Loss: 0.00203334
Iteration 21/25 | Loss: 0.00203334
Iteration 22/25 | Loss: 0.00203334
Iteration 23/25 | Loss: 0.00203333
Iteration 24/25 | Loss: 0.00203333
Iteration 25/25 | Loss: 0.00203333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18109858
Iteration 2/25 | Loss: 0.00934333
Iteration 3/25 | Loss: 0.00533219
Iteration 4/25 | Loss: 0.00533218
Iteration 5/25 | Loss: 0.00533218
Iteration 6/25 | Loss: 0.00533218
Iteration 7/25 | Loss: 0.00533218
Iteration 8/25 | Loss: 0.00533218
Iteration 9/25 | Loss: 0.00533218
Iteration 10/25 | Loss: 0.00533218
Iteration 11/25 | Loss: 0.00533218
Iteration 12/25 | Loss: 0.00533218
Iteration 13/25 | Loss: 0.00533218
Iteration 14/25 | Loss: 0.00533218
Iteration 15/25 | Loss: 0.00533218
Iteration 16/25 | Loss: 0.00533218
Iteration 17/25 | Loss: 0.00533218
Iteration 18/25 | Loss: 0.00533218
Iteration 19/25 | Loss: 0.00533218
Iteration 20/25 | Loss: 0.00533218
Iteration 21/25 | Loss: 0.00533218
Iteration 22/25 | Loss: 0.00533218
Iteration 23/25 | Loss: 0.00533218
Iteration 24/25 | Loss: 0.00533218
Iteration 25/25 | Loss: 0.00533218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00533218
Iteration 2/1000 | Loss: 0.00145396
Iteration 3/1000 | Loss: 0.00138453
Iteration 4/1000 | Loss: 0.00097300
Iteration 5/1000 | Loss: 0.00413193
Iteration 6/1000 | Loss: 0.00072133
Iteration 7/1000 | Loss: 0.00097308
Iteration 8/1000 | Loss: 0.00036191
Iteration 9/1000 | Loss: 0.00052994
Iteration 10/1000 | Loss: 0.00033160
Iteration 11/1000 | Loss: 0.00031686
Iteration 12/1000 | Loss: 0.00030401
Iteration 13/1000 | Loss: 0.00057956
Iteration 14/1000 | Loss: 0.00933521
Iteration 15/1000 | Loss: 0.00831950
Iteration 16/1000 | Loss: 0.00209412
Iteration 17/1000 | Loss: 0.00272073
Iteration 18/1000 | Loss: 0.00268266
Iteration 19/1000 | Loss: 0.00403606
Iteration 20/1000 | Loss: 0.00278482
Iteration 21/1000 | Loss: 0.00431959
Iteration 22/1000 | Loss: 0.00114423
Iteration 23/1000 | Loss: 0.00101106
Iteration 24/1000 | Loss: 0.00074756
Iteration 25/1000 | Loss: 0.00080418
Iteration 26/1000 | Loss: 0.00057077
Iteration 27/1000 | Loss: 0.00306051
Iteration 28/1000 | Loss: 0.00140794
Iteration 29/1000 | Loss: 0.00105133
Iteration 30/1000 | Loss: 0.00054358
Iteration 31/1000 | Loss: 0.00115983
Iteration 32/1000 | Loss: 0.00049391
Iteration 33/1000 | Loss: 0.00061686
Iteration 34/1000 | Loss: 0.00050451
Iteration 35/1000 | Loss: 0.00052436
Iteration 36/1000 | Loss: 0.00119617
Iteration 37/1000 | Loss: 0.00254124
Iteration 38/1000 | Loss: 0.00124738
Iteration 39/1000 | Loss: 0.00174803
Iteration 40/1000 | Loss: 0.00161593
Iteration 41/1000 | Loss: 0.00018141
Iteration 42/1000 | Loss: 0.00014162
Iteration 43/1000 | Loss: 0.00085115
Iteration 44/1000 | Loss: 0.00019006
Iteration 45/1000 | Loss: 0.00022311
Iteration 46/1000 | Loss: 0.00028869
Iteration 47/1000 | Loss: 0.00008688
Iteration 48/1000 | Loss: 0.00018531
Iteration 49/1000 | Loss: 0.00036297
Iteration 50/1000 | Loss: 0.00025385
Iteration 51/1000 | Loss: 0.00033337
Iteration 52/1000 | Loss: 0.00059968
Iteration 53/1000 | Loss: 0.00005804
Iteration 54/1000 | Loss: 0.00047590
Iteration 55/1000 | Loss: 0.00007615
Iteration 56/1000 | Loss: 0.00019169
Iteration 57/1000 | Loss: 0.00004672
Iteration 58/1000 | Loss: 0.00003811
Iteration 59/1000 | Loss: 0.00024802
Iteration 60/1000 | Loss: 0.00181190
Iteration 61/1000 | Loss: 0.00160584
Iteration 62/1000 | Loss: 0.00041597
Iteration 63/1000 | Loss: 0.00024416
Iteration 64/1000 | Loss: 0.00004547
Iteration 65/1000 | Loss: 0.00017021
Iteration 66/1000 | Loss: 0.00279850
Iteration 67/1000 | Loss: 0.00012015
Iteration 68/1000 | Loss: 0.00006025
Iteration 69/1000 | Loss: 0.00032097
Iteration 70/1000 | Loss: 0.00066022
Iteration 71/1000 | Loss: 0.00072975
Iteration 72/1000 | Loss: 0.00026833
Iteration 73/1000 | Loss: 0.00037052
Iteration 74/1000 | Loss: 0.00003634
Iteration 75/1000 | Loss: 0.00008511
Iteration 76/1000 | Loss: 0.00011028
Iteration 77/1000 | Loss: 0.00002573
Iteration 78/1000 | Loss: 0.00009030
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00005370
Iteration 81/1000 | Loss: 0.00003337
Iteration 82/1000 | Loss: 0.00013022
Iteration 83/1000 | Loss: 0.00003347
Iteration 84/1000 | Loss: 0.00003598
Iteration 85/1000 | Loss: 0.00002855
Iteration 86/1000 | Loss: 0.00003590
Iteration 87/1000 | Loss: 0.00006001
Iteration 88/1000 | Loss: 0.00004312
Iteration 89/1000 | Loss: 0.00005052
Iteration 90/1000 | Loss: 0.00003575
Iteration 91/1000 | Loss: 0.00003313
Iteration 92/1000 | Loss: 0.00002831
Iteration 93/1000 | Loss: 0.00002980
Iteration 94/1000 | Loss: 0.00003159
Iteration 95/1000 | Loss: 0.00002746
Iteration 96/1000 | Loss: 0.00006887
Iteration 97/1000 | Loss: 0.00007015
Iteration 98/1000 | Loss: 0.00002654
Iteration 99/1000 | Loss: 0.00009021
Iteration 100/1000 | Loss: 0.00002381
Iteration 101/1000 | Loss: 0.00002640
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002570
Iteration 104/1000 | Loss: 0.00011496
Iteration 105/1000 | Loss: 0.00002817
Iteration 106/1000 | Loss: 0.00002937
Iteration 107/1000 | Loss: 0.00002449
Iteration 108/1000 | Loss: 0.00002339
Iteration 109/1000 | Loss: 0.00003319
Iteration 110/1000 | Loss: 0.00002497
Iteration 111/1000 | Loss: 0.00002421
Iteration 112/1000 | Loss: 0.00002363
Iteration 113/1000 | Loss: 0.00002369
Iteration 114/1000 | Loss: 0.00002377
Iteration 115/1000 | Loss: 0.00002362
Iteration 116/1000 | Loss: 0.00002358
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00007040
Iteration 122/1000 | Loss: 0.00005831
Iteration 123/1000 | Loss: 0.00002029
Iteration 124/1000 | Loss: 0.00002032
Iteration 125/1000 | Loss: 0.00001435
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001510
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001430
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001430
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001425
Iteration 143/1000 | Loss: 0.00001424
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001421
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001418
Iteration 152/1000 | Loss: 0.00001417
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001415
Iteration 155/1000 | Loss: 0.00001413
Iteration 156/1000 | Loss: 0.00004994
Iteration 157/1000 | Loss: 0.00001402
Iteration 158/1000 | Loss: 0.00001402
Iteration 159/1000 | Loss: 0.00006333
Iteration 160/1000 | Loss: 0.00019062
Iteration 161/1000 | Loss: 0.00002225
Iteration 162/1000 | Loss: 0.00005228
Iteration 163/1000 | Loss: 0.00002264
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001388
Iteration 167/1000 | Loss: 0.00001388
Iteration 168/1000 | Loss: 0.00001388
Iteration 169/1000 | Loss: 0.00001387
Iteration 170/1000 | Loss: 0.00001386
Iteration 171/1000 | Loss: 0.00001385
Iteration 172/1000 | Loss: 0.00001385
Iteration 173/1000 | Loss: 0.00001384
Iteration 174/1000 | Loss: 0.00001384
Iteration 175/1000 | Loss: 0.00001383
Iteration 176/1000 | Loss: 0.00001383
Iteration 177/1000 | Loss: 0.00001383
Iteration 178/1000 | Loss: 0.00001383
Iteration 179/1000 | Loss: 0.00001382
Iteration 180/1000 | Loss: 0.00001381
Iteration 181/1000 | Loss: 0.00001381
Iteration 182/1000 | Loss: 0.00001381
Iteration 183/1000 | Loss: 0.00001381
Iteration 184/1000 | Loss: 0.00001380
Iteration 185/1000 | Loss: 0.00001380
Iteration 186/1000 | Loss: 0.00001379
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001378
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001378
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001376
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Iteration 214/1000 | Loss: 0.00001375
Iteration 215/1000 | Loss: 0.00001375
Iteration 216/1000 | Loss: 0.00001375
Iteration 217/1000 | Loss: 0.00001375
Iteration 218/1000 | Loss: 0.00001374
Iteration 219/1000 | Loss: 0.00001374
Iteration 220/1000 | Loss: 0.00001374
Iteration 221/1000 | Loss: 0.00001374
Iteration 222/1000 | Loss: 0.00001374
Iteration 223/1000 | Loss: 0.00001373
Iteration 224/1000 | Loss: 0.00001373
Iteration 225/1000 | Loss: 0.00001373
Iteration 226/1000 | Loss: 0.00001373
Iteration 227/1000 | Loss: 0.00001373
Iteration 228/1000 | Loss: 0.00001373
Iteration 229/1000 | Loss: 0.00001373
Iteration 230/1000 | Loss: 0.00001373
Iteration 231/1000 | Loss: 0.00001373
Iteration 232/1000 | Loss: 0.00001373
Iteration 233/1000 | Loss: 0.00001373
Iteration 234/1000 | Loss: 0.00001373
Iteration 235/1000 | Loss: 0.00001373
Iteration 236/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.3729872080148198e-05, 1.3729872080148198e-05, 1.3729872080148198e-05, 1.3729872080148198e-05, 1.3729872080148198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3729872080148198e-05

Optimization complete. Final v2v error: 3.2139763832092285 mm

Highest mean error: 3.8592889308929443 mm for frame 5

Lowest mean error: 2.98180890083313 mm for frame 4

Saving results

Total time: 251.90468740463257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00572802
Iteration 2/25 | Loss: 0.00155935
Iteration 3/25 | Loss: 0.00142803
Iteration 4/25 | Loss: 0.00139794
Iteration 5/25 | Loss: 0.00139013
Iteration 6/25 | Loss: 0.00138813
Iteration 7/25 | Loss: 0.00138724
Iteration 8/25 | Loss: 0.00138666
Iteration 9/25 | Loss: 0.00139277
Iteration 10/25 | Loss: 0.00138710
Iteration 11/25 | Loss: 0.00138627
Iteration 12/25 | Loss: 0.00138527
Iteration 13/25 | Loss: 0.00138471
Iteration 14/25 | Loss: 0.00138448
Iteration 15/25 | Loss: 0.00138436
Iteration 16/25 | Loss: 0.00138415
Iteration 17/25 | Loss: 0.00138821
Iteration 18/25 | Loss: 0.00138532
Iteration 19/25 | Loss: 0.00138373
Iteration 20/25 | Loss: 0.00138346
Iteration 21/25 | Loss: 0.00138321
Iteration 22/25 | Loss: 0.00138305
Iteration 23/25 | Loss: 0.00138296
Iteration 24/25 | Loss: 0.00138290
Iteration 25/25 | Loss: 0.00138290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07332563
Iteration 2/25 | Loss: 0.00202794
Iteration 3/25 | Loss: 0.00202794
Iteration 4/25 | Loss: 0.00202794
Iteration 5/25 | Loss: 0.00202794
Iteration 6/25 | Loss: 0.00202794
Iteration 7/25 | Loss: 0.00202794
Iteration 8/25 | Loss: 0.00202794
Iteration 9/25 | Loss: 0.00202794
Iteration 10/25 | Loss: 0.00202794
Iteration 11/25 | Loss: 0.00202794
Iteration 12/25 | Loss: 0.00202794
Iteration 13/25 | Loss: 0.00202794
Iteration 14/25 | Loss: 0.00202794
Iteration 15/25 | Loss: 0.00202794
Iteration 16/25 | Loss: 0.00202794
Iteration 17/25 | Loss: 0.00202794
Iteration 18/25 | Loss: 0.00202794
Iteration 19/25 | Loss: 0.00202794
Iteration 20/25 | Loss: 0.00202794
Iteration 21/25 | Loss: 0.00202794
Iteration 22/25 | Loss: 0.00202794
Iteration 23/25 | Loss: 0.00202794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002027935581281781, 0.002027935581281781, 0.002027935581281781, 0.002027935581281781, 0.002027935581281781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002027935581281781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202794
Iteration 2/1000 | Loss: 0.00005621
Iteration 3/1000 | Loss: 0.00004052
Iteration 4/1000 | Loss: 0.00003320
Iteration 5/1000 | Loss: 0.00003124
Iteration 6/1000 | Loss: 0.00002979
Iteration 7/1000 | Loss: 0.00002885
Iteration 8/1000 | Loss: 0.00002806
Iteration 9/1000 | Loss: 0.00002730
Iteration 10/1000 | Loss: 0.00031489
Iteration 11/1000 | Loss: 0.00006380
Iteration 12/1000 | Loss: 0.00003105
Iteration 13/1000 | Loss: 0.00002839
Iteration 14/1000 | Loss: 0.00002673
Iteration 15/1000 | Loss: 0.00002599
Iteration 16/1000 | Loss: 0.00002547
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00002501
Iteration 19/1000 | Loss: 0.00002446
Iteration 20/1000 | Loss: 0.00002412
Iteration 21/1000 | Loss: 0.00002378
Iteration 22/1000 | Loss: 0.00002344
Iteration 23/1000 | Loss: 0.00002307
Iteration 24/1000 | Loss: 0.00002271
Iteration 25/1000 | Loss: 0.00002268
Iteration 26/1000 | Loss: 0.00002258
Iteration 27/1000 | Loss: 0.00002257
Iteration 28/1000 | Loss: 0.00002256
Iteration 29/1000 | Loss: 0.00002250
Iteration 30/1000 | Loss: 0.00002243
Iteration 31/1000 | Loss: 0.00002235
Iteration 32/1000 | Loss: 0.00002226
Iteration 33/1000 | Loss: 0.00002223
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002210
Iteration 36/1000 | Loss: 0.00002207
Iteration 37/1000 | Loss: 0.00002207
Iteration 38/1000 | Loss: 0.00002206
Iteration 39/1000 | Loss: 0.00002206
Iteration 40/1000 | Loss: 0.00002206
Iteration 41/1000 | Loss: 0.00002206
Iteration 42/1000 | Loss: 0.00002206
Iteration 43/1000 | Loss: 0.00002206
Iteration 44/1000 | Loss: 0.00002206
Iteration 45/1000 | Loss: 0.00002206
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00002206
Iteration 48/1000 | Loss: 0.00002205
Iteration 49/1000 | Loss: 0.00002204
Iteration 50/1000 | Loss: 0.00002204
Iteration 51/1000 | Loss: 0.00002204
Iteration 52/1000 | Loss: 0.00002204
Iteration 53/1000 | Loss: 0.00002204
Iteration 54/1000 | Loss: 0.00002204
Iteration 55/1000 | Loss: 0.00002204
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002203
Iteration 58/1000 | Loss: 0.00002203
Iteration 59/1000 | Loss: 0.00002202
Iteration 60/1000 | Loss: 0.00002202
Iteration 61/1000 | Loss: 0.00002202
Iteration 62/1000 | Loss: 0.00002202
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002201
Iteration 65/1000 | Loss: 0.00002201
Iteration 66/1000 | Loss: 0.00002201
Iteration 67/1000 | Loss: 0.00002201
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00002201
Iteration 70/1000 | Loss: 0.00002201
Iteration 71/1000 | Loss: 0.00002201
Iteration 72/1000 | Loss: 0.00002201
Iteration 73/1000 | Loss: 0.00002201
Iteration 74/1000 | Loss: 0.00002200
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002200
Iteration 77/1000 | Loss: 0.00002200
Iteration 78/1000 | Loss: 0.00002200
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002200
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002200
Iteration 83/1000 | Loss: 0.00002200
Iteration 84/1000 | Loss: 0.00002200
Iteration 85/1000 | Loss: 0.00002200
Iteration 86/1000 | Loss: 0.00002199
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002199
Iteration 89/1000 | Loss: 0.00002199
Iteration 90/1000 | Loss: 0.00002199
Iteration 91/1000 | Loss: 0.00002199
Iteration 92/1000 | Loss: 0.00002199
Iteration 93/1000 | Loss: 0.00002199
Iteration 94/1000 | Loss: 0.00002199
Iteration 95/1000 | Loss: 0.00002199
Iteration 96/1000 | Loss: 0.00002199
Iteration 97/1000 | Loss: 0.00002199
Iteration 98/1000 | Loss: 0.00002199
Iteration 99/1000 | Loss: 0.00002199
Iteration 100/1000 | Loss: 0.00002199
Iteration 101/1000 | Loss: 0.00002198
Iteration 102/1000 | Loss: 0.00002198
Iteration 103/1000 | Loss: 0.00002198
Iteration 104/1000 | Loss: 0.00002198
Iteration 105/1000 | Loss: 0.00002198
Iteration 106/1000 | Loss: 0.00002198
Iteration 107/1000 | Loss: 0.00002198
Iteration 108/1000 | Loss: 0.00002198
Iteration 109/1000 | Loss: 0.00002198
Iteration 110/1000 | Loss: 0.00002198
Iteration 111/1000 | Loss: 0.00002198
Iteration 112/1000 | Loss: 0.00002198
Iteration 113/1000 | Loss: 0.00002198
Iteration 114/1000 | Loss: 0.00002198
Iteration 115/1000 | Loss: 0.00002198
Iteration 116/1000 | Loss: 0.00002197
Iteration 117/1000 | Loss: 0.00002197
Iteration 118/1000 | Loss: 0.00002197
Iteration 119/1000 | Loss: 0.00002197
Iteration 120/1000 | Loss: 0.00002197
Iteration 121/1000 | Loss: 0.00002197
Iteration 122/1000 | Loss: 0.00002197
Iteration 123/1000 | Loss: 0.00002197
Iteration 124/1000 | Loss: 0.00002197
Iteration 125/1000 | Loss: 0.00002197
Iteration 126/1000 | Loss: 0.00002197
Iteration 127/1000 | Loss: 0.00002196
Iteration 128/1000 | Loss: 0.00002196
Iteration 129/1000 | Loss: 0.00002196
Iteration 130/1000 | Loss: 0.00002196
Iteration 131/1000 | Loss: 0.00002196
Iteration 132/1000 | Loss: 0.00002196
Iteration 133/1000 | Loss: 0.00002196
Iteration 134/1000 | Loss: 0.00002196
Iteration 135/1000 | Loss: 0.00002196
Iteration 136/1000 | Loss: 0.00002196
Iteration 137/1000 | Loss: 0.00002196
Iteration 138/1000 | Loss: 0.00002196
Iteration 139/1000 | Loss: 0.00002195
Iteration 140/1000 | Loss: 0.00002195
Iteration 141/1000 | Loss: 0.00002195
Iteration 142/1000 | Loss: 0.00002195
Iteration 143/1000 | Loss: 0.00002195
Iteration 144/1000 | Loss: 0.00002195
Iteration 145/1000 | Loss: 0.00002195
Iteration 146/1000 | Loss: 0.00002195
Iteration 147/1000 | Loss: 0.00002195
Iteration 148/1000 | Loss: 0.00002195
Iteration 149/1000 | Loss: 0.00002194
Iteration 150/1000 | Loss: 0.00002194
Iteration 151/1000 | Loss: 0.00002194
Iteration 152/1000 | Loss: 0.00002194
Iteration 153/1000 | Loss: 0.00002194
Iteration 154/1000 | Loss: 0.00002194
Iteration 155/1000 | Loss: 0.00002194
Iteration 156/1000 | Loss: 0.00002194
Iteration 157/1000 | Loss: 0.00002194
Iteration 158/1000 | Loss: 0.00002194
Iteration 159/1000 | Loss: 0.00002194
Iteration 160/1000 | Loss: 0.00002194
Iteration 161/1000 | Loss: 0.00002194
Iteration 162/1000 | Loss: 0.00002194
Iteration 163/1000 | Loss: 0.00002193
Iteration 164/1000 | Loss: 0.00002193
Iteration 165/1000 | Loss: 0.00002193
Iteration 166/1000 | Loss: 0.00002193
Iteration 167/1000 | Loss: 0.00002193
Iteration 168/1000 | Loss: 0.00002193
Iteration 169/1000 | Loss: 0.00002193
Iteration 170/1000 | Loss: 0.00002192
Iteration 171/1000 | Loss: 0.00002192
Iteration 172/1000 | Loss: 0.00002192
Iteration 173/1000 | Loss: 0.00002192
Iteration 174/1000 | Loss: 0.00002192
Iteration 175/1000 | Loss: 0.00002192
Iteration 176/1000 | Loss: 0.00002192
Iteration 177/1000 | Loss: 0.00002192
Iteration 178/1000 | Loss: 0.00002191
Iteration 179/1000 | Loss: 0.00002191
Iteration 180/1000 | Loss: 0.00002191
Iteration 181/1000 | Loss: 0.00002191
Iteration 182/1000 | Loss: 0.00002191
Iteration 183/1000 | Loss: 0.00002191
Iteration 184/1000 | Loss: 0.00002190
Iteration 185/1000 | Loss: 0.00002190
Iteration 186/1000 | Loss: 0.00002190
Iteration 187/1000 | Loss: 0.00002190
Iteration 188/1000 | Loss: 0.00002190
Iteration 189/1000 | Loss: 0.00002189
Iteration 190/1000 | Loss: 0.00002189
Iteration 191/1000 | Loss: 0.00002189
Iteration 192/1000 | Loss: 0.00002189
Iteration 193/1000 | Loss: 0.00002189
Iteration 194/1000 | Loss: 0.00002189
Iteration 195/1000 | Loss: 0.00002189
Iteration 196/1000 | Loss: 0.00002189
Iteration 197/1000 | Loss: 0.00002189
Iteration 198/1000 | Loss: 0.00002189
Iteration 199/1000 | Loss: 0.00002189
Iteration 200/1000 | Loss: 0.00002189
Iteration 201/1000 | Loss: 0.00002189
Iteration 202/1000 | Loss: 0.00002189
Iteration 203/1000 | Loss: 0.00002189
Iteration 204/1000 | Loss: 0.00002189
Iteration 205/1000 | Loss: 0.00002189
Iteration 206/1000 | Loss: 0.00002189
Iteration 207/1000 | Loss: 0.00002189
Iteration 208/1000 | Loss: 0.00002189
Iteration 209/1000 | Loss: 0.00002189
Iteration 210/1000 | Loss: 0.00002189
Iteration 211/1000 | Loss: 0.00002189
Iteration 212/1000 | Loss: 0.00002189
Iteration 213/1000 | Loss: 0.00002189
Iteration 214/1000 | Loss: 0.00002189
Iteration 215/1000 | Loss: 0.00002189
Iteration 216/1000 | Loss: 0.00002189
Iteration 217/1000 | Loss: 0.00002189
Iteration 218/1000 | Loss: 0.00002189
Iteration 219/1000 | Loss: 0.00002189
Iteration 220/1000 | Loss: 0.00002189
Iteration 221/1000 | Loss: 0.00002189
Iteration 222/1000 | Loss: 0.00002189
Iteration 223/1000 | Loss: 0.00002189
Iteration 224/1000 | Loss: 0.00002189
Iteration 225/1000 | Loss: 0.00002189
Iteration 226/1000 | Loss: 0.00002189
Iteration 227/1000 | Loss: 0.00002189
Iteration 228/1000 | Loss: 0.00002189
Iteration 229/1000 | Loss: 0.00002189
Iteration 230/1000 | Loss: 0.00002189
Iteration 231/1000 | Loss: 0.00002189
Iteration 232/1000 | Loss: 0.00002189
Iteration 233/1000 | Loss: 0.00002189
Iteration 234/1000 | Loss: 0.00002189
Iteration 235/1000 | Loss: 0.00002189
Iteration 236/1000 | Loss: 0.00002189
Iteration 237/1000 | Loss: 0.00002189
Iteration 238/1000 | Loss: 0.00002189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.188632970501203e-05, 2.188632970501203e-05, 2.188632970501203e-05, 2.188632970501203e-05, 2.188632970501203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.188632970501203e-05

Optimization complete. Final v2v error: 3.842998743057251 mm

Highest mean error: 6.426916122436523 mm for frame 109

Lowest mean error: 3.231187582015991 mm for frame 0

Saving results

Total time: 95.16140866279602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816914
Iteration 2/25 | Loss: 0.00153544
Iteration 3/25 | Loss: 0.00141509
Iteration 4/25 | Loss: 0.00139808
Iteration 5/25 | Loss: 0.00139406
Iteration 6/25 | Loss: 0.00139406
Iteration 7/25 | Loss: 0.00139406
Iteration 8/25 | Loss: 0.00139406
Iteration 9/25 | Loss: 0.00139406
Iteration 10/25 | Loss: 0.00139406
Iteration 11/25 | Loss: 0.00139406
Iteration 12/25 | Loss: 0.00139406
Iteration 13/25 | Loss: 0.00139406
Iteration 14/25 | Loss: 0.00139406
Iteration 15/25 | Loss: 0.00139406
Iteration 16/25 | Loss: 0.00139406
Iteration 17/25 | Loss: 0.00139406
Iteration 18/25 | Loss: 0.00139406
Iteration 19/25 | Loss: 0.00139406
Iteration 20/25 | Loss: 0.00139406
Iteration 21/25 | Loss: 0.00139406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013940585777163506, 0.0013940585777163506, 0.0013940585777163506, 0.0013940585777163506, 0.0013940585777163506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013940585777163506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20306516
Iteration 2/25 | Loss: 0.00284160
Iteration 3/25 | Loss: 0.00284155
Iteration 4/25 | Loss: 0.00284155
Iteration 5/25 | Loss: 0.00284155
Iteration 6/25 | Loss: 0.00284155
Iteration 7/25 | Loss: 0.00284155
Iteration 8/25 | Loss: 0.00284155
Iteration 9/25 | Loss: 0.00284155
Iteration 10/25 | Loss: 0.00284155
Iteration 11/25 | Loss: 0.00284155
Iteration 12/25 | Loss: 0.00284155
Iteration 13/25 | Loss: 0.00284155
Iteration 14/25 | Loss: 0.00284155
Iteration 15/25 | Loss: 0.00284155
Iteration 16/25 | Loss: 0.00284155
Iteration 17/25 | Loss: 0.00284155
Iteration 18/25 | Loss: 0.00284155
Iteration 19/25 | Loss: 0.00284155
Iteration 20/25 | Loss: 0.00284155
Iteration 21/25 | Loss: 0.00284155
Iteration 22/25 | Loss: 0.00284155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002841549925506115, 0.002841549925506115, 0.002841549925506115, 0.002841549925506115, 0.002841549925506115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002841549925506115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284155
Iteration 2/1000 | Loss: 0.00005304
Iteration 3/1000 | Loss: 0.00003433
Iteration 4/1000 | Loss: 0.00002839
Iteration 5/1000 | Loss: 0.00002558
Iteration 6/1000 | Loss: 0.00002391
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00002194
Iteration 9/1000 | Loss: 0.00002129
Iteration 10/1000 | Loss: 0.00002075
Iteration 11/1000 | Loss: 0.00002034
Iteration 12/1000 | Loss: 0.00002002
Iteration 13/1000 | Loss: 0.00001975
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001930
Iteration 16/1000 | Loss: 0.00001918
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001879
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001855
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001843
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001840
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001830
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001827
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001825
Iteration 54/1000 | Loss: 0.00001824
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001821
Iteration 65/1000 | Loss: 0.00001820
Iteration 66/1000 | Loss: 0.00001820
Iteration 67/1000 | Loss: 0.00001820
Iteration 68/1000 | Loss: 0.00001820
Iteration 69/1000 | Loss: 0.00001819
Iteration 70/1000 | Loss: 0.00001819
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001815
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001815
Iteration 79/1000 | Loss: 0.00001815
Iteration 80/1000 | Loss: 0.00001815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.815237192204222e-05, 1.815237192204222e-05, 1.815237192204222e-05, 1.815237192204222e-05, 1.815237192204222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.815237192204222e-05

Optimization complete. Final v2v error: 3.5863583087921143 mm

Highest mean error: 4.1069488525390625 mm for frame 221

Lowest mean error: 3.2778878211975098 mm for frame 10

Saving results

Total time: 49.71252799034119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897130
Iteration 2/25 | Loss: 0.00145067
Iteration 3/25 | Loss: 0.00137080
Iteration 4/25 | Loss: 0.00135784
Iteration 5/25 | Loss: 0.00135389
Iteration 6/25 | Loss: 0.00135309
Iteration 7/25 | Loss: 0.00135309
Iteration 8/25 | Loss: 0.00135309
Iteration 9/25 | Loss: 0.00135309
Iteration 10/25 | Loss: 0.00135309
Iteration 11/25 | Loss: 0.00135309
Iteration 12/25 | Loss: 0.00135309
Iteration 13/25 | Loss: 0.00135309
Iteration 14/25 | Loss: 0.00135309
Iteration 15/25 | Loss: 0.00135309
Iteration 16/25 | Loss: 0.00135309
Iteration 17/25 | Loss: 0.00135309
Iteration 18/25 | Loss: 0.00135309
Iteration 19/25 | Loss: 0.00135309
Iteration 20/25 | Loss: 0.00135309
Iteration 21/25 | Loss: 0.00135309
Iteration 22/25 | Loss: 0.00135309
Iteration 23/25 | Loss: 0.00135309
Iteration 24/25 | Loss: 0.00135309
Iteration 25/25 | Loss: 0.00135309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25018680
Iteration 2/25 | Loss: 0.00211335
Iteration 3/25 | Loss: 0.00211335
Iteration 4/25 | Loss: 0.00211335
Iteration 5/25 | Loss: 0.00211335
Iteration 6/25 | Loss: 0.00211335
Iteration 7/25 | Loss: 0.00211335
Iteration 8/25 | Loss: 0.00211335
Iteration 9/25 | Loss: 0.00211335
Iteration 10/25 | Loss: 0.00211335
Iteration 11/25 | Loss: 0.00211335
Iteration 12/25 | Loss: 0.00211335
Iteration 13/25 | Loss: 0.00211335
Iteration 14/25 | Loss: 0.00211335
Iteration 15/25 | Loss: 0.00211335
Iteration 16/25 | Loss: 0.00211335
Iteration 17/25 | Loss: 0.00211335
Iteration 18/25 | Loss: 0.00211335
Iteration 19/25 | Loss: 0.00211335
Iteration 20/25 | Loss: 0.00211335
Iteration 21/25 | Loss: 0.00211335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021133471745997667, 0.0021133471745997667, 0.0021133471745997667, 0.0021133471745997667, 0.0021133471745997667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021133471745997667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211335
Iteration 2/1000 | Loss: 0.00004275
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00002502
Iteration 5/1000 | Loss: 0.00002317
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001955
Iteration 12/1000 | Loss: 0.00001931
Iteration 13/1000 | Loss: 0.00001925
Iteration 14/1000 | Loss: 0.00001925
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001905
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001870
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001861
Iteration 27/1000 | Loss: 0.00001857
Iteration 28/1000 | Loss: 0.00001857
Iteration 29/1000 | Loss: 0.00001853
Iteration 30/1000 | Loss: 0.00001852
Iteration 31/1000 | Loss: 0.00001849
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001844
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001841
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001837
Iteration 45/1000 | Loss: 0.00001837
Iteration 46/1000 | Loss: 0.00001837
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001837
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001836
Iteration 61/1000 | Loss: 0.00001836
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001833
Iteration 72/1000 | Loss: 0.00001833
Iteration 73/1000 | Loss: 0.00001833
Iteration 74/1000 | Loss: 0.00001833
Iteration 75/1000 | Loss: 0.00001832
Iteration 76/1000 | Loss: 0.00001832
Iteration 77/1000 | Loss: 0.00001832
Iteration 78/1000 | Loss: 0.00001832
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001825
Iteration 108/1000 | Loss: 0.00001825
Iteration 109/1000 | Loss: 0.00001825
Iteration 110/1000 | Loss: 0.00001824
Iteration 111/1000 | Loss: 0.00001824
Iteration 112/1000 | Loss: 0.00001824
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001824
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00001824
Iteration 118/1000 | Loss: 0.00001824
Iteration 119/1000 | Loss: 0.00001823
Iteration 120/1000 | Loss: 0.00001823
Iteration 121/1000 | Loss: 0.00001823
Iteration 122/1000 | Loss: 0.00001823
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001822
Iteration 125/1000 | Loss: 0.00001822
Iteration 126/1000 | Loss: 0.00001822
Iteration 127/1000 | Loss: 0.00001822
Iteration 128/1000 | Loss: 0.00001822
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001820
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001819
Iteration 147/1000 | Loss: 0.00001819
Iteration 148/1000 | Loss: 0.00001819
Iteration 149/1000 | Loss: 0.00001819
Iteration 150/1000 | Loss: 0.00001819
Iteration 151/1000 | Loss: 0.00001819
Iteration 152/1000 | Loss: 0.00001819
Iteration 153/1000 | Loss: 0.00001819
Iteration 154/1000 | Loss: 0.00001819
Iteration 155/1000 | Loss: 0.00001819
Iteration 156/1000 | Loss: 0.00001819
Iteration 157/1000 | Loss: 0.00001819
Iteration 158/1000 | Loss: 0.00001818
Iteration 159/1000 | Loss: 0.00001818
Iteration 160/1000 | Loss: 0.00001818
Iteration 161/1000 | Loss: 0.00001818
Iteration 162/1000 | Loss: 0.00001818
Iteration 163/1000 | Loss: 0.00001818
Iteration 164/1000 | Loss: 0.00001818
Iteration 165/1000 | Loss: 0.00001818
Iteration 166/1000 | Loss: 0.00001818
Iteration 167/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.817519478208851e-05, 1.817519478208851e-05, 1.817519478208851e-05, 1.817519478208851e-05, 1.817519478208851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.817519478208851e-05

Optimization complete. Final v2v error: 3.5852692127227783 mm

Highest mean error: 5.591629505157471 mm for frame 70

Lowest mean error: 3.1090469360351562 mm for frame 95

Saving results

Total time: 43.77768921852112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753297
Iteration 2/25 | Loss: 0.00144061
Iteration 3/25 | Loss: 0.00136830
Iteration 4/25 | Loss: 0.00134994
Iteration 5/25 | Loss: 0.00136569
Iteration 6/25 | Loss: 0.00132021
Iteration 7/25 | Loss: 0.00131066
Iteration 8/25 | Loss: 0.00130957
Iteration 9/25 | Loss: 0.00130927
Iteration 10/25 | Loss: 0.00130924
Iteration 11/25 | Loss: 0.00130924
Iteration 12/25 | Loss: 0.00130924
Iteration 13/25 | Loss: 0.00130924
Iteration 14/25 | Loss: 0.00130924
Iteration 15/25 | Loss: 0.00130924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013092377921566367, 0.0013092377921566367, 0.0013092377921566367, 0.0013092377921566367, 0.0013092377921566367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013092377921566367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22563195
Iteration 2/25 | Loss: 0.00240892
Iteration 3/25 | Loss: 0.00240891
Iteration 4/25 | Loss: 0.00240891
Iteration 5/25 | Loss: 0.00240891
Iteration 6/25 | Loss: 0.00240891
Iteration 7/25 | Loss: 0.00240891
Iteration 8/25 | Loss: 0.00240891
Iteration 9/25 | Loss: 0.00240891
Iteration 10/25 | Loss: 0.00240891
Iteration 11/25 | Loss: 0.00240891
Iteration 12/25 | Loss: 0.00240891
Iteration 13/25 | Loss: 0.00240891
Iteration 14/25 | Loss: 0.00240891
Iteration 15/25 | Loss: 0.00240891
Iteration 16/25 | Loss: 0.00240891
Iteration 17/25 | Loss: 0.00240891
Iteration 18/25 | Loss: 0.00240891
Iteration 19/25 | Loss: 0.00240891
Iteration 20/25 | Loss: 0.00240891
Iteration 21/25 | Loss: 0.00240891
Iteration 22/25 | Loss: 0.00240891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0024089105427265167, 0.0024089105427265167, 0.0024089105427265167, 0.0024089105427265167, 0.0024089105427265167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024089105427265167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240891
Iteration 2/1000 | Loss: 0.00002771
Iteration 3/1000 | Loss: 0.00002076
Iteration 4/1000 | Loss: 0.00001847
Iteration 5/1000 | Loss: 0.00001701
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001477
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001281
Iteration 56/1000 | Loss: 0.00001281
Iteration 57/1000 | Loss: 0.00001281
Iteration 58/1000 | Loss: 0.00001281
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001274
Iteration 74/1000 | Loss: 0.00001273
Iteration 75/1000 | Loss: 0.00001273
Iteration 76/1000 | Loss: 0.00001273
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001264
Iteration 122/1000 | Loss: 0.00001264
Iteration 123/1000 | Loss: 0.00001264
Iteration 124/1000 | Loss: 0.00001264
Iteration 125/1000 | Loss: 0.00001264
Iteration 126/1000 | Loss: 0.00001264
Iteration 127/1000 | Loss: 0.00001264
Iteration 128/1000 | Loss: 0.00001264
Iteration 129/1000 | Loss: 0.00001264
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001264
Iteration 139/1000 | Loss: 0.00001264
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001264
Iteration 143/1000 | Loss: 0.00001264
Iteration 144/1000 | Loss: 0.00001264
Iteration 145/1000 | Loss: 0.00001264
Iteration 146/1000 | Loss: 0.00001264
Iteration 147/1000 | Loss: 0.00001264
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00001264
Iteration 151/1000 | Loss: 0.00001264
Iteration 152/1000 | Loss: 0.00001264
Iteration 153/1000 | Loss: 0.00001264
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001264
Iteration 157/1000 | Loss: 0.00001264
Iteration 158/1000 | Loss: 0.00001264
Iteration 159/1000 | Loss: 0.00001264
Iteration 160/1000 | Loss: 0.00001264
Iteration 161/1000 | Loss: 0.00001264
Iteration 162/1000 | Loss: 0.00001264
Iteration 163/1000 | Loss: 0.00001264
Iteration 164/1000 | Loss: 0.00001264
Iteration 165/1000 | Loss: 0.00001264
Iteration 166/1000 | Loss: 0.00001264
Iteration 167/1000 | Loss: 0.00001264
Iteration 168/1000 | Loss: 0.00001264
Iteration 169/1000 | Loss: 0.00001264
Iteration 170/1000 | Loss: 0.00001264
Iteration 171/1000 | Loss: 0.00001264
Iteration 172/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.2639970009331591e-05, 1.2639970009331591e-05, 1.2639970009331591e-05, 1.2639970009331591e-05, 1.2639970009331591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2639970009331591e-05

Optimization complete. Final v2v error: 3.083280563354492 mm

Highest mean error: 3.5643341541290283 mm for frame 111

Lowest mean error: 2.567652463912964 mm for frame 203

Saving results

Total time: 56.78965187072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709799
Iteration 2/25 | Loss: 0.00149791
Iteration 3/25 | Loss: 0.00141357
Iteration 4/25 | Loss: 0.00140976
Iteration 5/25 | Loss: 0.00140834
Iteration 6/25 | Loss: 0.00140815
Iteration 7/25 | Loss: 0.00140815
Iteration 8/25 | Loss: 0.00140815
Iteration 9/25 | Loss: 0.00140815
Iteration 10/25 | Loss: 0.00140815
Iteration 11/25 | Loss: 0.00140815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00140814995393157, 0.00140814995393157, 0.00140814995393157, 0.00140814995393157, 0.00140814995393157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00140814995393157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17389405
Iteration 2/25 | Loss: 0.00215269
Iteration 3/25 | Loss: 0.00215268
Iteration 4/25 | Loss: 0.00215268
Iteration 5/25 | Loss: 0.00215267
Iteration 6/25 | Loss: 0.00215267
Iteration 7/25 | Loss: 0.00215267
Iteration 8/25 | Loss: 0.00215267
Iteration 9/25 | Loss: 0.00215267
Iteration 10/25 | Loss: 0.00215267
Iteration 11/25 | Loss: 0.00215267
Iteration 12/25 | Loss: 0.00215267
Iteration 13/25 | Loss: 0.00215267
Iteration 14/25 | Loss: 0.00215267
Iteration 15/25 | Loss: 0.00215267
Iteration 16/25 | Loss: 0.00215267
Iteration 17/25 | Loss: 0.00215267
Iteration 18/25 | Loss: 0.00215267
Iteration 19/25 | Loss: 0.00215267
Iteration 20/25 | Loss: 0.00215267
Iteration 21/25 | Loss: 0.00215267
Iteration 22/25 | Loss: 0.00215267
Iteration 23/25 | Loss: 0.00215267
Iteration 24/25 | Loss: 0.00215267
Iteration 25/25 | Loss: 0.00215267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0021526729688048363, 0.0021526729688048363, 0.0021526729688048363, 0.0021526729688048363, 0.0021526729688048363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021526729688048363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215267
Iteration 2/1000 | Loss: 0.00003330
Iteration 3/1000 | Loss: 0.00002514
Iteration 4/1000 | Loss: 0.00002257
Iteration 5/1000 | Loss: 0.00002144
Iteration 6/1000 | Loss: 0.00002073
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001973
Iteration 9/1000 | Loss: 0.00001950
Iteration 10/1000 | Loss: 0.00001923
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001898
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001883
Iteration 15/1000 | Loss: 0.00001870
Iteration 16/1000 | Loss: 0.00001858
Iteration 17/1000 | Loss: 0.00001857
Iteration 18/1000 | Loss: 0.00001856
Iteration 19/1000 | Loss: 0.00001856
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001854
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00001854
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001846
Iteration 32/1000 | Loss: 0.00001845
Iteration 33/1000 | Loss: 0.00001845
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001844
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001843
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001842
Iteration 48/1000 | Loss: 0.00001842
Iteration 49/1000 | Loss: 0.00001842
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001841
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001841
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001840
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001839
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001838
Iteration 70/1000 | Loss: 0.00001838
Iteration 71/1000 | Loss: 0.00001838
Iteration 72/1000 | Loss: 0.00001838
Iteration 73/1000 | Loss: 0.00001838
Iteration 74/1000 | Loss: 0.00001838
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001838
Iteration 77/1000 | Loss: 0.00001837
Iteration 78/1000 | Loss: 0.00001837
Iteration 79/1000 | Loss: 0.00001837
Iteration 80/1000 | Loss: 0.00001837
Iteration 81/1000 | Loss: 0.00001837
Iteration 82/1000 | Loss: 0.00001837
Iteration 83/1000 | Loss: 0.00001837
Iteration 84/1000 | Loss: 0.00001837
Iteration 85/1000 | Loss: 0.00001837
Iteration 86/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.8370712496107444e-05, 1.8370712496107444e-05, 1.8370712496107444e-05, 1.8370712496107444e-05, 1.8370712496107444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8370712496107444e-05

Optimization complete. Final v2v error: 3.606619119644165 mm

Highest mean error: 3.801205635070801 mm for frame 29

Lowest mean error: 3.450521945953369 mm for frame 78

Saving results

Total time: 32.88200116157532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767752
Iteration 2/25 | Loss: 0.00164805
Iteration 3/25 | Loss: 0.00139261
Iteration 4/25 | Loss: 0.00135673
Iteration 5/25 | Loss: 0.00135677
Iteration 6/25 | Loss: 0.00136227
Iteration 7/25 | Loss: 0.00135024
Iteration 8/25 | Loss: 0.00134073
Iteration 9/25 | Loss: 0.00133618
Iteration 10/25 | Loss: 0.00133578
Iteration 11/25 | Loss: 0.00133573
Iteration 12/25 | Loss: 0.00133572
Iteration 13/25 | Loss: 0.00133572
Iteration 14/25 | Loss: 0.00133572
Iteration 15/25 | Loss: 0.00133572
Iteration 16/25 | Loss: 0.00133572
Iteration 17/25 | Loss: 0.00133572
Iteration 18/25 | Loss: 0.00133572
Iteration 19/25 | Loss: 0.00133572
Iteration 20/25 | Loss: 0.00133571
Iteration 21/25 | Loss: 0.00133571
Iteration 22/25 | Loss: 0.00133571
Iteration 23/25 | Loss: 0.00133571
Iteration 24/25 | Loss: 0.00133571
Iteration 25/25 | Loss: 0.00133571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22403872
Iteration 2/25 | Loss: 0.00176887
Iteration 3/25 | Loss: 0.00176886
Iteration 4/25 | Loss: 0.00176886
Iteration 5/25 | Loss: 0.00176886
Iteration 6/25 | Loss: 0.00176886
Iteration 7/25 | Loss: 0.00176886
Iteration 8/25 | Loss: 0.00176886
Iteration 9/25 | Loss: 0.00176886
Iteration 10/25 | Loss: 0.00176886
Iteration 11/25 | Loss: 0.00176886
Iteration 12/25 | Loss: 0.00176886
Iteration 13/25 | Loss: 0.00176886
Iteration 14/25 | Loss: 0.00176886
Iteration 15/25 | Loss: 0.00176886
Iteration 16/25 | Loss: 0.00176886
Iteration 17/25 | Loss: 0.00176886
Iteration 18/25 | Loss: 0.00176886
Iteration 19/25 | Loss: 0.00176886
Iteration 20/25 | Loss: 0.00176886
Iteration 21/25 | Loss: 0.00176886
Iteration 22/25 | Loss: 0.00176886
Iteration 23/25 | Loss: 0.00176886
Iteration 24/25 | Loss: 0.00176886
Iteration 25/25 | Loss: 0.00176886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176886
Iteration 2/1000 | Loss: 0.00003453
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002090
Iteration 5/1000 | Loss: 0.00001935
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00001514
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001509
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001490
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Iteration 118/1000 | Loss: 0.00001474
Iteration 119/1000 | Loss: 0.00001474
Iteration 120/1000 | Loss: 0.00001474
Iteration 121/1000 | Loss: 0.00001474
Iteration 122/1000 | Loss: 0.00001474
Iteration 123/1000 | Loss: 0.00001474
Iteration 124/1000 | Loss: 0.00001474
Iteration 125/1000 | Loss: 0.00001474
Iteration 126/1000 | Loss: 0.00001474
Iteration 127/1000 | Loss: 0.00001474
Iteration 128/1000 | Loss: 0.00001474
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.47396431202651e-05, 1.47396431202651e-05, 1.47396431202651e-05, 1.47396431202651e-05, 1.47396431202651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.47396431202651e-05

Optimization complete. Final v2v error: 3.268501043319702 mm

Highest mean error: 3.650120735168457 mm for frame 80

Lowest mean error: 3.0521183013916016 mm for frame 104

Saving results

Total time: 50.67924499511719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025474
Iteration 2/25 | Loss: 0.00381775
Iteration 3/25 | Loss: 0.00261875
Iteration 4/25 | Loss: 0.00217154
Iteration 5/25 | Loss: 0.00208789
Iteration 6/25 | Loss: 0.00199173
Iteration 7/25 | Loss: 0.00186882
Iteration 8/25 | Loss: 0.00178104
Iteration 9/25 | Loss: 0.00175156
Iteration 10/25 | Loss: 0.00171209
Iteration 11/25 | Loss: 0.00169809
Iteration 12/25 | Loss: 0.00170090
Iteration 13/25 | Loss: 0.00168114
Iteration 14/25 | Loss: 0.00166179
Iteration 15/25 | Loss: 0.00166409
Iteration 16/25 | Loss: 0.00165869
Iteration 17/25 | Loss: 0.00165116
Iteration 18/25 | Loss: 0.00164744
Iteration 19/25 | Loss: 0.00164725
Iteration 20/25 | Loss: 0.00164643
Iteration 21/25 | Loss: 0.00164764
Iteration 22/25 | Loss: 0.00164768
Iteration 23/25 | Loss: 0.00164692
Iteration 24/25 | Loss: 0.00164714
Iteration 25/25 | Loss: 0.00164886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17598450
Iteration 2/25 | Loss: 0.00487831
Iteration 3/25 | Loss: 0.00432284
Iteration 4/25 | Loss: 0.00432284
Iteration 5/25 | Loss: 0.00432284
Iteration 6/25 | Loss: 0.00432283
Iteration 7/25 | Loss: 0.00432283
Iteration 8/25 | Loss: 0.00432283
Iteration 9/25 | Loss: 0.00432283
Iteration 10/25 | Loss: 0.00432283
Iteration 11/25 | Loss: 0.00432283
Iteration 12/25 | Loss: 0.00432283
Iteration 13/25 | Loss: 0.00432283
Iteration 14/25 | Loss: 0.00432283
Iteration 15/25 | Loss: 0.00432283
Iteration 16/25 | Loss: 0.00432283
Iteration 17/25 | Loss: 0.00432283
Iteration 18/25 | Loss: 0.00432283
Iteration 19/25 | Loss: 0.00432283
Iteration 20/25 | Loss: 0.00432283
Iteration 21/25 | Loss: 0.00432283
Iteration 22/25 | Loss: 0.00432283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0043228319846093655, 0.0043228319846093655, 0.0043228319846093655, 0.0043228319846093655, 0.0043228319846093655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0043228319846093655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00432283
Iteration 2/1000 | Loss: 0.00348478
Iteration 3/1000 | Loss: 0.00805946
Iteration 4/1000 | Loss: 0.00053251
Iteration 5/1000 | Loss: 0.00038254
Iteration 6/1000 | Loss: 0.00039962
Iteration 7/1000 | Loss: 0.00084629
Iteration 8/1000 | Loss: 0.00032218
Iteration 9/1000 | Loss: 0.00039297
Iteration 10/1000 | Loss: 0.00030346
Iteration 11/1000 | Loss: 0.00026652
Iteration 12/1000 | Loss: 0.00027549
Iteration 13/1000 | Loss: 0.00022308
Iteration 14/1000 | Loss: 0.00025916
Iteration 15/1000 | Loss: 0.00024507
Iteration 16/1000 | Loss: 0.00024292
Iteration 17/1000 | Loss: 0.00023332
Iteration 18/1000 | Loss: 0.00036597
Iteration 19/1000 | Loss: 0.00022728
Iteration 20/1000 | Loss: 0.00026651
Iteration 21/1000 | Loss: 0.00168726
Iteration 22/1000 | Loss: 0.00191620
Iteration 23/1000 | Loss: 0.00093478
Iteration 24/1000 | Loss: 0.00107083
Iteration 25/1000 | Loss: 0.00050205
Iteration 26/1000 | Loss: 0.00212483
Iteration 27/1000 | Loss: 0.00117529
Iteration 28/1000 | Loss: 0.00087458
Iteration 29/1000 | Loss: 0.00116619
Iteration 30/1000 | Loss: 0.00096062
Iteration 31/1000 | Loss: 0.00028127
Iteration 32/1000 | Loss: 0.00056851
Iteration 33/1000 | Loss: 0.00024313
Iteration 34/1000 | Loss: 0.00033242
Iteration 35/1000 | Loss: 0.00028168
Iteration 36/1000 | Loss: 0.00036873
Iteration 37/1000 | Loss: 0.00026651
Iteration 38/1000 | Loss: 0.00078089
Iteration 39/1000 | Loss: 0.00049831
Iteration 40/1000 | Loss: 0.00214161
Iteration 41/1000 | Loss: 0.00047820
Iteration 42/1000 | Loss: 0.00076076
Iteration 43/1000 | Loss: 0.00075039
Iteration 44/1000 | Loss: 0.00180858
Iteration 45/1000 | Loss: 0.00098508
Iteration 46/1000 | Loss: 0.00187091
Iteration 47/1000 | Loss: 0.00090348
Iteration 48/1000 | Loss: 0.00166582
Iteration 49/1000 | Loss: 0.00309820
Iteration 50/1000 | Loss: 0.00179846
Iteration 51/1000 | Loss: 0.00253980
Iteration 52/1000 | Loss: 0.00241397
Iteration 53/1000 | Loss: 0.00184199
Iteration 54/1000 | Loss: 0.00153113
Iteration 55/1000 | Loss: 0.00131853
Iteration 56/1000 | Loss: 0.00046782
Iteration 57/1000 | Loss: 0.00118060
Iteration 58/1000 | Loss: 0.00054022
Iteration 59/1000 | Loss: 0.00073055
Iteration 60/1000 | Loss: 0.00200694
Iteration 61/1000 | Loss: 0.00089996
Iteration 62/1000 | Loss: 0.00107428
Iteration 63/1000 | Loss: 0.00080708
Iteration 64/1000 | Loss: 0.00065378
Iteration 65/1000 | Loss: 0.00110659
Iteration 66/1000 | Loss: 0.00131160
Iteration 67/1000 | Loss: 0.00082342
Iteration 68/1000 | Loss: 0.00079497
Iteration 69/1000 | Loss: 0.00089404
Iteration 70/1000 | Loss: 0.00037982
Iteration 71/1000 | Loss: 0.00134373
Iteration 72/1000 | Loss: 0.00155660
Iteration 73/1000 | Loss: 0.00209647
Iteration 74/1000 | Loss: 0.00141773
Iteration 75/1000 | Loss: 0.00091384
Iteration 76/1000 | Loss: 0.00065819
Iteration 77/1000 | Loss: 0.00077722
Iteration 78/1000 | Loss: 0.00049073
Iteration 79/1000 | Loss: 0.00148606
Iteration 80/1000 | Loss: 0.00075986
Iteration 81/1000 | Loss: 0.00112551
Iteration 82/1000 | Loss: 0.00126719
Iteration 83/1000 | Loss: 0.00155369
Iteration 84/1000 | Loss: 0.00188332
Iteration 85/1000 | Loss: 0.00186228
Iteration 86/1000 | Loss: 0.00130329
Iteration 87/1000 | Loss: 0.00126177
Iteration 88/1000 | Loss: 0.00186379
Iteration 89/1000 | Loss: 0.00112625
Iteration 90/1000 | Loss: 0.00107111
Iteration 91/1000 | Loss: 0.00112612
Iteration 92/1000 | Loss: 0.00057600
Iteration 93/1000 | Loss: 0.00091115
Iteration 94/1000 | Loss: 0.00058396
Iteration 95/1000 | Loss: 0.00054631
Iteration 96/1000 | Loss: 0.00036113
Iteration 97/1000 | Loss: 0.00042315
Iteration 98/1000 | Loss: 0.00074693
Iteration 99/1000 | Loss: 0.00069746
Iteration 100/1000 | Loss: 0.00056931
Iteration 101/1000 | Loss: 0.00040660
Iteration 102/1000 | Loss: 0.00081420
Iteration 103/1000 | Loss: 0.00066712
Iteration 104/1000 | Loss: 0.00032933
Iteration 105/1000 | Loss: 0.00058514
Iteration 106/1000 | Loss: 0.00026999
Iteration 107/1000 | Loss: 0.00018147
Iteration 108/1000 | Loss: 0.00026151
Iteration 109/1000 | Loss: 0.00008241
Iteration 110/1000 | Loss: 0.00038878
Iteration 111/1000 | Loss: 0.00284704
Iteration 112/1000 | Loss: 0.00145769
Iteration 113/1000 | Loss: 0.00021931
Iteration 114/1000 | Loss: 0.00043658
Iteration 115/1000 | Loss: 0.00144560
Iteration 116/1000 | Loss: 0.00105182
Iteration 117/1000 | Loss: 0.00103064
Iteration 118/1000 | Loss: 0.00112604
Iteration 119/1000 | Loss: 0.00023862
Iteration 120/1000 | Loss: 0.00075139
Iteration 121/1000 | Loss: 0.00065653
Iteration 122/1000 | Loss: 0.00079194
Iteration 123/1000 | Loss: 0.00031790
Iteration 124/1000 | Loss: 0.00040136
Iteration 125/1000 | Loss: 0.00030628
Iteration 126/1000 | Loss: 0.00203838
Iteration 127/1000 | Loss: 0.00137696
Iteration 128/1000 | Loss: 0.00172129
Iteration 129/1000 | Loss: 0.00016821
Iteration 130/1000 | Loss: 0.00037095
Iteration 131/1000 | Loss: 0.00088756
Iteration 132/1000 | Loss: 0.00029105
Iteration 133/1000 | Loss: 0.00028526
Iteration 134/1000 | Loss: 0.00027245
Iteration 135/1000 | Loss: 0.00023962
Iteration 136/1000 | Loss: 0.00005021
Iteration 137/1000 | Loss: 0.00006007
Iteration 138/1000 | Loss: 0.00034621
Iteration 139/1000 | Loss: 0.00116897
Iteration 140/1000 | Loss: 0.00039171
Iteration 141/1000 | Loss: 0.00006245
Iteration 142/1000 | Loss: 0.00052007
Iteration 143/1000 | Loss: 0.00019462
Iteration 144/1000 | Loss: 0.00017552
Iteration 145/1000 | Loss: 0.00012521
Iteration 146/1000 | Loss: 0.00003999
Iteration 147/1000 | Loss: 0.00012271
Iteration 148/1000 | Loss: 0.00005486
Iteration 149/1000 | Loss: 0.00004285
Iteration 150/1000 | Loss: 0.00003697
Iteration 151/1000 | Loss: 0.00004983
Iteration 152/1000 | Loss: 0.00004540
Iteration 153/1000 | Loss: 0.00004289
Iteration 154/1000 | Loss: 0.00005089
Iteration 155/1000 | Loss: 0.00012223
Iteration 156/1000 | Loss: 0.00005479
Iteration 157/1000 | Loss: 0.00004138
Iteration 158/1000 | Loss: 0.00005437
Iteration 159/1000 | Loss: 0.00005410
Iteration 160/1000 | Loss: 0.00003721
Iteration 161/1000 | Loss: 0.00004771
Iteration 162/1000 | Loss: 0.00004510
Iteration 163/1000 | Loss: 0.00006089
Iteration 164/1000 | Loss: 0.00006723
Iteration 165/1000 | Loss: 0.00004159
Iteration 166/1000 | Loss: 0.00008932
Iteration 167/1000 | Loss: 0.00002915
Iteration 168/1000 | Loss: 0.00004896
Iteration 169/1000 | Loss: 0.00005108
Iteration 170/1000 | Loss: 0.00004503
Iteration 171/1000 | Loss: 0.00004474
Iteration 172/1000 | Loss: 0.00004912
Iteration 173/1000 | Loss: 0.00007168
Iteration 174/1000 | Loss: 0.00005579
Iteration 175/1000 | Loss: 0.00006565
Iteration 176/1000 | Loss: 0.00006564
Iteration 177/1000 | Loss: 0.00009686
Iteration 178/1000 | Loss: 0.00003723
Iteration 179/1000 | Loss: 0.00006670
Iteration 180/1000 | Loss: 0.00003811
Iteration 181/1000 | Loss: 0.00002946
Iteration 182/1000 | Loss: 0.00002481
Iteration 183/1000 | Loss: 0.00002364
Iteration 184/1000 | Loss: 0.00003199
Iteration 185/1000 | Loss: 0.00002144
Iteration 186/1000 | Loss: 0.00004996
Iteration 187/1000 | Loss: 0.00002012
Iteration 188/1000 | Loss: 0.00001960
Iteration 189/1000 | Loss: 0.00001957
Iteration 190/1000 | Loss: 0.00004073
Iteration 191/1000 | Loss: 0.00002057
Iteration 192/1000 | Loss: 0.00001920
Iteration 193/1000 | Loss: 0.00002179
Iteration 194/1000 | Loss: 0.00001849
Iteration 195/1000 | Loss: 0.00001934
Iteration 196/1000 | Loss: 0.00001931
Iteration 197/1000 | Loss: 0.00002834
Iteration 198/1000 | Loss: 0.00002048
Iteration 199/1000 | Loss: 0.00001992
Iteration 200/1000 | Loss: 0.00001823
Iteration 201/1000 | Loss: 0.00001823
Iteration 202/1000 | Loss: 0.00001823
Iteration 203/1000 | Loss: 0.00001820
Iteration 204/1000 | Loss: 0.00002137
Iteration 205/1000 | Loss: 0.00001981
Iteration 206/1000 | Loss: 0.00001818
Iteration 207/1000 | Loss: 0.00001882
Iteration 208/1000 | Loss: 0.00001797
Iteration 209/1000 | Loss: 0.00001797
Iteration 210/1000 | Loss: 0.00001797
Iteration 211/1000 | Loss: 0.00001797
Iteration 212/1000 | Loss: 0.00001796
Iteration 213/1000 | Loss: 0.00001796
Iteration 214/1000 | Loss: 0.00001796
Iteration 215/1000 | Loss: 0.00001796
Iteration 216/1000 | Loss: 0.00001795
Iteration 217/1000 | Loss: 0.00001795
Iteration 218/1000 | Loss: 0.00001793
Iteration 219/1000 | Loss: 0.00001793
Iteration 220/1000 | Loss: 0.00001792
Iteration 221/1000 | Loss: 0.00001792
Iteration 222/1000 | Loss: 0.00002249
Iteration 223/1000 | Loss: 0.00001916
Iteration 224/1000 | Loss: 0.00001823
Iteration 225/1000 | Loss: 0.00001792
Iteration 226/1000 | Loss: 0.00001768
Iteration 227/1000 | Loss: 0.00001768
Iteration 228/1000 | Loss: 0.00001768
Iteration 229/1000 | Loss: 0.00001768
Iteration 230/1000 | Loss: 0.00001768
Iteration 231/1000 | Loss: 0.00001768
Iteration 232/1000 | Loss: 0.00001768
Iteration 233/1000 | Loss: 0.00001768
Iteration 234/1000 | Loss: 0.00001768
Iteration 235/1000 | Loss: 0.00001767
Iteration 236/1000 | Loss: 0.00001767
Iteration 237/1000 | Loss: 0.00001766
Iteration 238/1000 | Loss: 0.00003806
Iteration 239/1000 | Loss: 0.00001740
Iteration 240/1000 | Loss: 0.00001732
Iteration 241/1000 | Loss: 0.00001731
Iteration 242/1000 | Loss: 0.00002005
Iteration 243/1000 | Loss: 0.00002005
Iteration 244/1000 | Loss: 0.00002004
Iteration 245/1000 | Loss: 0.00029657
Iteration 246/1000 | Loss: 0.00010964
Iteration 247/1000 | Loss: 0.00004387
Iteration 248/1000 | Loss: 0.00002038
Iteration 249/1000 | Loss: 0.00002098
Iteration 250/1000 | Loss: 0.00001857
Iteration 251/1000 | Loss: 0.00002087
Iteration 252/1000 | Loss: 0.00001801
Iteration 253/1000 | Loss: 0.00002358
Iteration 254/1000 | Loss: 0.00002280
Iteration 255/1000 | Loss: 0.00001710
Iteration 256/1000 | Loss: 0.00003005
Iteration 257/1000 | Loss: 0.00002376
Iteration 258/1000 | Loss: 0.00002010
Iteration 259/1000 | Loss: 0.00002737
Iteration 260/1000 | Loss: 0.00001693
Iteration 261/1000 | Loss: 0.00025829
Iteration 262/1000 | Loss: 0.00013767
Iteration 263/1000 | Loss: 0.00033278
Iteration 264/1000 | Loss: 0.00006322
Iteration 265/1000 | Loss: 0.00008155
Iteration 266/1000 | Loss: 0.00002019
Iteration 267/1000 | Loss: 0.00006847
Iteration 268/1000 | Loss: 0.00001952
Iteration 269/1000 | Loss: 0.00002256
Iteration 270/1000 | Loss: 0.00001833
Iteration 271/1000 | Loss: 0.00002080
Iteration 272/1000 | Loss: 0.00002580
Iteration 273/1000 | Loss: 0.00002055
Iteration 274/1000 | Loss: 0.00002751
Iteration 275/1000 | Loss: 0.00022857
Iteration 276/1000 | Loss: 0.00016181
Iteration 277/1000 | Loss: 0.00013722
Iteration 278/1000 | Loss: 0.00003952
Iteration 279/1000 | Loss: 0.00021971
Iteration 280/1000 | Loss: 0.00002311
Iteration 281/1000 | Loss: 0.00001972
Iteration 282/1000 | Loss: 0.00001833
Iteration 283/1000 | Loss: 0.00002680
Iteration 284/1000 | Loss: 0.00001954
Iteration 285/1000 | Loss: 0.00001800
Iteration 286/1000 | Loss: 0.00002292
Iteration 287/1000 | Loss: 0.00001823
Iteration 288/1000 | Loss: 0.00001940
Iteration 289/1000 | Loss: 0.00001841
Iteration 290/1000 | Loss: 0.00001670
Iteration 291/1000 | Loss: 0.00001979
Iteration 292/1000 | Loss: 0.00001639
Iteration 293/1000 | Loss: 0.00001634
Iteration 294/1000 | Loss: 0.00001632
Iteration 295/1000 | Loss: 0.00001631
Iteration 296/1000 | Loss: 0.00001630
Iteration 297/1000 | Loss: 0.00002067
Iteration 298/1000 | Loss: 0.00001627
Iteration 299/1000 | Loss: 0.00001625
Iteration 300/1000 | Loss: 0.00001625
Iteration 301/1000 | Loss: 0.00001625
Iteration 302/1000 | Loss: 0.00001625
Iteration 303/1000 | Loss: 0.00001625
Iteration 304/1000 | Loss: 0.00001625
Iteration 305/1000 | Loss: 0.00001625
Iteration 306/1000 | Loss: 0.00001625
Iteration 307/1000 | Loss: 0.00001624
Iteration 308/1000 | Loss: 0.00001624
Iteration 309/1000 | Loss: 0.00001624
Iteration 310/1000 | Loss: 0.00001624
Iteration 311/1000 | Loss: 0.00001624
Iteration 312/1000 | Loss: 0.00001624
Iteration 313/1000 | Loss: 0.00001623
Iteration 314/1000 | Loss: 0.00001623
Iteration 315/1000 | Loss: 0.00001623
Iteration 316/1000 | Loss: 0.00001623
Iteration 317/1000 | Loss: 0.00001622
Iteration 318/1000 | Loss: 0.00001622
Iteration 319/1000 | Loss: 0.00001622
Iteration 320/1000 | Loss: 0.00001622
Iteration 321/1000 | Loss: 0.00001622
Iteration 322/1000 | Loss: 0.00001622
Iteration 323/1000 | Loss: 0.00001622
Iteration 324/1000 | Loss: 0.00001862
Iteration 325/1000 | Loss: 0.00001621
Iteration 326/1000 | Loss: 0.00001621
Iteration 327/1000 | Loss: 0.00001621
Iteration 328/1000 | Loss: 0.00001621
Iteration 329/1000 | Loss: 0.00001620
Iteration 330/1000 | Loss: 0.00001620
Iteration 331/1000 | Loss: 0.00001913
Iteration 332/1000 | Loss: 0.00001652
Iteration 333/1000 | Loss: 0.00001616
Iteration 334/1000 | Loss: 0.00001616
Iteration 335/1000 | Loss: 0.00001616
Iteration 336/1000 | Loss: 0.00001616
Iteration 337/1000 | Loss: 0.00001616
Iteration 338/1000 | Loss: 0.00001628
Iteration 339/1000 | Loss: 0.00001616
Iteration 340/1000 | Loss: 0.00001616
Iteration 341/1000 | Loss: 0.00001616
Iteration 342/1000 | Loss: 0.00001616
Iteration 343/1000 | Loss: 0.00001616
Iteration 344/1000 | Loss: 0.00001616
Iteration 345/1000 | Loss: 0.00001616
Iteration 346/1000 | Loss: 0.00001616
Iteration 347/1000 | Loss: 0.00001616
Iteration 348/1000 | Loss: 0.00001616
Iteration 349/1000 | Loss: 0.00001616
Iteration 350/1000 | Loss: 0.00001616
Iteration 351/1000 | Loss: 0.00001615
Iteration 352/1000 | Loss: 0.00001615
Iteration 353/1000 | Loss: 0.00001615
Iteration 354/1000 | Loss: 0.00001615
Iteration 355/1000 | Loss: 0.00001615
Iteration 356/1000 | Loss: 0.00001615
Iteration 357/1000 | Loss: 0.00001615
Iteration 358/1000 | Loss: 0.00001615
Iteration 359/1000 | Loss: 0.00001615
Iteration 360/1000 | Loss: 0.00001615
Iteration 361/1000 | Loss: 0.00001615
Iteration 362/1000 | Loss: 0.00001615
Iteration 363/1000 | Loss: 0.00001615
Iteration 364/1000 | Loss: 0.00001615
Iteration 365/1000 | Loss: 0.00001615
Iteration 366/1000 | Loss: 0.00001615
Iteration 367/1000 | Loss: 0.00001615
Iteration 368/1000 | Loss: 0.00001615
Iteration 369/1000 | Loss: 0.00001615
Iteration 370/1000 | Loss: 0.00001615
Iteration 371/1000 | Loss: 0.00001615
Iteration 372/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 372. Stopping optimization.
Last 5 losses: [1.614670873095747e-05, 1.614670873095747e-05, 1.614670873095747e-05, 1.614670873095747e-05, 1.614670873095747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.614670873095747e-05

Optimization complete. Final v2v error: 3.322909355163574 mm

Highest mean error: 5.100392818450928 mm for frame 62

Lowest mean error: 2.6814165115356445 mm for frame 92

Saving results

Total time: 439.7233769893646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828245
Iteration 2/25 | Loss: 0.00141949
Iteration 3/25 | Loss: 0.00133113
Iteration 4/25 | Loss: 0.00132186
Iteration 5/25 | Loss: 0.00132009
Iteration 6/25 | Loss: 0.00132005
Iteration 7/25 | Loss: 0.00132005
Iteration 8/25 | Loss: 0.00132005
Iteration 9/25 | Loss: 0.00132005
Iteration 10/25 | Loss: 0.00132005
Iteration 11/25 | Loss: 0.00132005
Iteration 12/25 | Loss: 0.00132005
Iteration 13/25 | Loss: 0.00132005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013200539397075772, 0.0013200539397075772, 0.0013200539397075772, 0.0013200539397075772, 0.0013200539397075772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013200539397075772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23348355
Iteration 2/25 | Loss: 0.00190771
Iteration 3/25 | Loss: 0.00190769
Iteration 4/25 | Loss: 0.00190769
Iteration 5/25 | Loss: 0.00190769
Iteration 6/25 | Loss: 0.00190769
Iteration 7/25 | Loss: 0.00190769
Iteration 8/25 | Loss: 0.00190769
Iteration 9/25 | Loss: 0.00190769
Iteration 10/25 | Loss: 0.00190769
Iteration 11/25 | Loss: 0.00190769
Iteration 12/25 | Loss: 0.00190769
Iteration 13/25 | Loss: 0.00190769
Iteration 14/25 | Loss: 0.00190769
Iteration 15/25 | Loss: 0.00190769
Iteration 16/25 | Loss: 0.00190769
Iteration 17/25 | Loss: 0.00190769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019076864700764418, 0.0019076864700764418, 0.0019076864700764418, 0.0019076864700764418, 0.0019076864700764418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019076864700764418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190769
Iteration 2/1000 | Loss: 0.00002335
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001369
Iteration 5/1000 | Loss: 0.00001247
Iteration 6/1000 | Loss: 0.00001177
Iteration 7/1000 | Loss: 0.00001119
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001060
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000970
Iteration 16/1000 | Loss: 0.00000961
Iteration 17/1000 | Loss: 0.00000959
Iteration 18/1000 | Loss: 0.00000958
Iteration 19/1000 | Loss: 0.00000957
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000956
Iteration 23/1000 | Loss: 0.00000956
Iteration 24/1000 | Loss: 0.00000955
Iteration 25/1000 | Loss: 0.00000954
Iteration 26/1000 | Loss: 0.00000954
Iteration 27/1000 | Loss: 0.00000953
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000953
Iteration 30/1000 | Loss: 0.00000953
Iteration 31/1000 | Loss: 0.00000953
Iteration 32/1000 | Loss: 0.00000953
Iteration 33/1000 | Loss: 0.00000953
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000952
Iteration 36/1000 | Loss: 0.00000952
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000952
Iteration 39/1000 | Loss: 0.00000952
Iteration 40/1000 | Loss: 0.00000952
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000950
Iteration 45/1000 | Loss: 0.00000950
Iteration 46/1000 | Loss: 0.00000949
Iteration 47/1000 | Loss: 0.00000949
Iteration 48/1000 | Loss: 0.00000949
Iteration 49/1000 | Loss: 0.00000948
Iteration 50/1000 | Loss: 0.00000948
Iteration 51/1000 | Loss: 0.00000948
Iteration 52/1000 | Loss: 0.00000947
Iteration 53/1000 | Loss: 0.00000947
Iteration 54/1000 | Loss: 0.00000945
Iteration 55/1000 | Loss: 0.00000944
Iteration 56/1000 | Loss: 0.00000944
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000944
Iteration 59/1000 | Loss: 0.00000944
Iteration 60/1000 | Loss: 0.00000944
Iteration 61/1000 | Loss: 0.00000944
Iteration 62/1000 | Loss: 0.00000944
Iteration 63/1000 | Loss: 0.00000944
Iteration 64/1000 | Loss: 0.00000944
Iteration 65/1000 | Loss: 0.00000943
Iteration 66/1000 | Loss: 0.00000943
Iteration 67/1000 | Loss: 0.00000943
Iteration 68/1000 | Loss: 0.00000943
Iteration 69/1000 | Loss: 0.00000943
Iteration 70/1000 | Loss: 0.00000943
Iteration 71/1000 | Loss: 0.00000943
Iteration 72/1000 | Loss: 0.00000943
Iteration 73/1000 | Loss: 0.00000942
Iteration 74/1000 | Loss: 0.00000941
Iteration 75/1000 | Loss: 0.00000939
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000938
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000938
Iteration 81/1000 | Loss: 0.00000938
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000937
Iteration 84/1000 | Loss: 0.00000937
Iteration 85/1000 | Loss: 0.00000935
Iteration 86/1000 | Loss: 0.00000933
Iteration 87/1000 | Loss: 0.00000933
Iteration 88/1000 | Loss: 0.00000933
Iteration 89/1000 | Loss: 0.00000932
Iteration 90/1000 | Loss: 0.00000932
Iteration 91/1000 | Loss: 0.00000932
Iteration 92/1000 | Loss: 0.00000932
Iteration 93/1000 | Loss: 0.00000932
Iteration 94/1000 | Loss: 0.00000931
Iteration 95/1000 | Loss: 0.00000931
Iteration 96/1000 | Loss: 0.00000931
Iteration 97/1000 | Loss: 0.00000930
Iteration 98/1000 | Loss: 0.00000929
Iteration 99/1000 | Loss: 0.00000929
Iteration 100/1000 | Loss: 0.00000928
Iteration 101/1000 | Loss: 0.00000928
Iteration 102/1000 | Loss: 0.00000928
Iteration 103/1000 | Loss: 0.00000928
Iteration 104/1000 | Loss: 0.00000928
Iteration 105/1000 | Loss: 0.00000927
Iteration 106/1000 | Loss: 0.00000927
Iteration 107/1000 | Loss: 0.00000927
Iteration 108/1000 | Loss: 0.00000926
Iteration 109/1000 | Loss: 0.00000926
Iteration 110/1000 | Loss: 0.00000926
Iteration 111/1000 | Loss: 0.00000925
Iteration 112/1000 | Loss: 0.00000925
Iteration 113/1000 | Loss: 0.00000925
Iteration 114/1000 | Loss: 0.00000925
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000925
Iteration 117/1000 | Loss: 0.00000925
Iteration 118/1000 | Loss: 0.00000925
Iteration 119/1000 | Loss: 0.00000924
Iteration 120/1000 | Loss: 0.00000924
Iteration 121/1000 | Loss: 0.00000924
Iteration 122/1000 | Loss: 0.00000924
Iteration 123/1000 | Loss: 0.00000924
Iteration 124/1000 | Loss: 0.00000924
Iteration 125/1000 | Loss: 0.00000924
Iteration 126/1000 | Loss: 0.00000924
Iteration 127/1000 | Loss: 0.00000924
Iteration 128/1000 | Loss: 0.00000923
Iteration 129/1000 | Loss: 0.00000923
Iteration 130/1000 | Loss: 0.00000923
Iteration 131/1000 | Loss: 0.00000922
Iteration 132/1000 | Loss: 0.00000922
Iteration 133/1000 | Loss: 0.00000922
Iteration 134/1000 | Loss: 0.00000922
Iteration 135/1000 | Loss: 0.00000922
Iteration 136/1000 | Loss: 0.00000922
Iteration 137/1000 | Loss: 0.00000922
Iteration 138/1000 | Loss: 0.00000922
Iteration 139/1000 | Loss: 0.00000922
Iteration 140/1000 | Loss: 0.00000921
Iteration 141/1000 | Loss: 0.00000921
Iteration 142/1000 | Loss: 0.00000921
Iteration 143/1000 | Loss: 0.00000921
Iteration 144/1000 | Loss: 0.00000921
Iteration 145/1000 | Loss: 0.00000921
Iteration 146/1000 | Loss: 0.00000920
Iteration 147/1000 | Loss: 0.00000920
Iteration 148/1000 | Loss: 0.00000920
Iteration 149/1000 | Loss: 0.00000920
Iteration 150/1000 | Loss: 0.00000920
Iteration 151/1000 | Loss: 0.00000920
Iteration 152/1000 | Loss: 0.00000920
Iteration 153/1000 | Loss: 0.00000920
Iteration 154/1000 | Loss: 0.00000920
Iteration 155/1000 | Loss: 0.00000920
Iteration 156/1000 | Loss: 0.00000920
Iteration 157/1000 | Loss: 0.00000920
Iteration 158/1000 | Loss: 0.00000920
Iteration 159/1000 | Loss: 0.00000920
Iteration 160/1000 | Loss: 0.00000919
Iteration 161/1000 | Loss: 0.00000919
Iteration 162/1000 | Loss: 0.00000919
Iteration 163/1000 | Loss: 0.00000919
Iteration 164/1000 | Loss: 0.00000919
Iteration 165/1000 | Loss: 0.00000919
Iteration 166/1000 | Loss: 0.00000919
Iteration 167/1000 | Loss: 0.00000919
Iteration 168/1000 | Loss: 0.00000919
Iteration 169/1000 | Loss: 0.00000919
Iteration 170/1000 | Loss: 0.00000919
Iteration 171/1000 | Loss: 0.00000919
Iteration 172/1000 | Loss: 0.00000919
Iteration 173/1000 | Loss: 0.00000918
Iteration 174/1000 | Loss: 0.00000918
Iteration 175/1000 | Loss: 0.00000918
Iteration 176/1000 | Loss: 0.00000918
Iteration 177/1000 | Loss: 0.00000918
Iteration 178/1000 | Loss: 0.00000918
Iteration 179/1000 | Loss: 0.00000918
Iteration 180/1000 | Loss: 0.00000918
Iteration 181/1000 | Loss: 0.00000918
Iteration 182/1000 | Loss: 0.00000918
Iteration 183/1000 | Loss: 0.00000918
Iteration 184/1000 | Loss: 0.00000918
Iteration 185/1000 | Loss: 0.00000918
Iteration 186/1000 | Loss: 0.00000917
Iteration 187/1000 | Loss: 0.00000917
Iteration 188/1000 | Loss: 0.00000917
Iteration 189/1000 | Loss: 0.00000917
Iteration 190/1000 | Loss: 0.00000917
Iteration 191/1000 | Loss: 0.00000917
Iteration 192/1000 | Loss: 0.00000917
Iteration 193/1000 | Loss: 0.00000917
Iteration 194/1000 | Loss: 0.00000917
Iteration 195/1000 | Loss: 0.00000917
Iteration 196/1000 | Loss: 0.00000916
Iteration 197/1000 | Loss: 0.00000916
Iteration 198/1000 | Loss: 0.00000916
Iteration 199/1000 | Loss: 0.00000916
Iteration 200/1000 | Loss: 0.00000916
Iteration 201/1000 | Loss: 0.00000916
Iteration 202/1000 | Loss: 0.00000916
Iteration 203/1000 | Loss: 0.00000916
Iteration 204/1000 | Loss: 0.00000915
Iteration 205/1000 | Loss: 0.00000915
Iteration 206/1000 | Loss: 0.00000915
Iteration 207/1000 | Loss: 0.00000915
Iteration 208/1000 | Loss: 0.00000915
Iteration 209/1000 | Loss: 0.00000915
Iteration 210/1000 | Loss: 0.00000915
Iteration 211/1000 | Loss: 0.00000915
Iteration 212/1000 | Loss: 0.00000915
Iteration 213/1000 | Loss: 0.00000915
Iteration 214/1000 | Loss: 0.00000915
Iteration 215/1000 | Loss: 0.00000915
Iteration 216/1000 | Loss: 0.00000915
Iteration 217/1000 | Loss: 0.00000915
Iteration 218/1000 | Loss: 0.00000915
Iteration 219/1000 | Loss: 0.00000915
Iteration 220/1000 | Loss: 0.00000915
Iteration 221/1000 | Loss: 0.00000915
Iteration 222/1000 | Loss: 0.00000915
Iteration 223/1000 | Loss: 0.00000915
Iteration 224/1000 | Loss: 0.00000915
Iteration 225/1000 | Loss: 0.00000915
Iteration 226/1000 | Loss: 0.00000915
Iteration 227/1000 | Loss: 0.00000915
Iteration 228/1000 | Loss: 0.00000915
Iteration 229/1000 | Loss: 0.00000915
Iteration 230/1000 | Loss: 0.00000915
Iteration 231/1000 | Loss: 0.00000915
Iteration 232/1000 | Loss: 0.00000915
Iteration 233/1000 | Loss: 0.00000915
Iteration 234/1000 | Loss: 0.00000915
Iteration 235/1000 | Loss: 0.00000915
Iteration 236/1000 | Loss: 0.00000915
Iteration 237/1000 | Loss: 0.00000915
Iteration 238/1000 | Loss: 0.00000915
Iteration 239/1000 | Loss: 0.00000915
Iteration 240/1000 | Loss: 0.00000915
Iteration 241/1000 | Loss: 0.00000915
Iteration 242/1000 | Loss: 0.00000915
Iteration 243/1000 | Loss: 0.00000915
Iteration 244/1000 | Loss: 0.00000915
Iteration 245/1000 | Loss: 0.00000915
Iteration 246/1000 | Loss: 0.00000915
Iteration 247/1000 | Loss: 0.00000915
Iteration 248/1000 | Loss: 0.00000915
Iteration 249/1000 | Loss: 0.00000915
Iteration 250/1000 | Loss: 0.00000915
Iteration 251/1000 | Loss: 0.00000915
Iteration 252/1000 | Loss: 0.00000915
Iteration 253/1000 | Loss: 0.00000915
Iteration 254/1000 | Loss: 0.00000915
Iteration 255/1000 | Loss: 0.00000915
Iteration 256/1000 | Loss: 0.00000915
Iteration 257/1000 | Loss: 0.00000915
Iteration 258/1000 | Loss: 0.00000915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [9.147257514996454e-06, 9.147257514996454e-06, 9.147257514996454e-06, 9.147257514996454e-06, 9.147257514996454e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.147257514996454e-06

Optimization complete. Final v2v error: 2.6461234092712402 mm

Highest mean error: 2.8743956089019775 mm for frame 27

Lowest mean error: 2.535141706466675 mm for frame 87

Saving results

Total time: 43.17194151878357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962743
Iteration 2/25 | Loss: 0.00208320
Iteration 3/25 | Loss: 0.00175092
Iteration 4/25 | Loss: 0.00168407
Iteration 5/25 | Loss: 0.00173849
Iteration 6/25 | Loss: 0.00159161
Iteration 7/25 | Loss: 0.00162804
Iteration 8/25 | Loss: 0.00155792
Iteration 9/25 | Loss: 0.00152565
Iteration 10/25 | Loss: 0.00149347
Iteration 11/25 | Loss: 0.00145218
Iteration 12/25 | Loss: 0.00143174
Iteration 13/25 | Loss: 0.00149880
Iteration 14/25 | Loss: 0.00141976
Iteration 15/25 | Loss: 0.00141647
Iteration 16/25 | Loss: 0.00141720
Iteration 17/25 | Loss: 0.00141575
Iteration 18/25 | Loss: 0.00141442
Iteration 19/25 | Loss: 0.00141382
Iteration 20/25 | Loss: 0.00141367
Iteration 21/25 | Loss: 0.00141365
Iteration 22/25 | Loss: 0.00141365
Iteration 23/25 | Loss: 0.00141364
Iteration 24/25 | Loss: 0.00141364
Iteration 25/25 | Loss: 0.00141364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67477536
Iteration 2/25 | Loss: 0.00277049
Iteration 3/25 | Loss: 0.00277048
Iteration 4/25 | Loss: 0.00277048
Iteration 5/25 | Loss: 0.00277048
Iteration 6/25 | Loss: 0.00277048
Iteration 7/25 | Loss: 0.00277048
Iteration 8/25 | Loss: 0.00277048
Iteration 9/25 | Loss: 0.00277048
Iteration 10/25 | Loss: 0.00277048
Iteration 11/25 | Loss: 0.00277048
Iteration 12/25 | Loss: 0.00277048
Iteration 13/25 | Loss: 0.00277048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002770479768514633, 0.002770479768514633, 0.002770479768514633, 0.002770479768514633, 0.002770479768514633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002770479768514633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277048
Iteration 2/1000 | Loss: 0.00013113
Iteration 3/1000 | Loss: 0.00008981
Iteration 4/1000 | Loss: 0.00006979
Iteration 5/1000 | Loss: 0.00005964
Iteration 6/1000 | Loss: 0.00005234
Iteration 7/1000 | Loss: 0.00004990
Iteration 8/1000 | Loss: 0.00011270
Iteration 9/1000 | Loss: 0.00014395
Iteration 10/1000 | Loss: 0.00004440
Iteration 11/1000 | Loss: 0.00004080
Iteration 12/1000 | Loss: 0.00035978
Iteration 13/1000 | Loss: 0.00005193
Iteration 14/1000 | Loss: 0.00018054
Iteration 15/1000 | Loss: 0.00003579
Iteration 16/1000 | Loss: 0.00003054
Iteration 17/1000 | Loss: 0.00002711
Iteration 18/1000 | Loss: 0.00002473
Iteration 19/1000 | Loss: 0.00002290
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002095
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00022698
Iteration 25/1000 | Loss: 0.00014722
Iteration 26/1000 | Loss: 0.00020666
Iteration 27/1000 | Loss: 0.00013650
Iteration 28/1000 | Loss: 0.00002210
Iteration 29/1000 | Loss: 0.00016110
Iteration 30/1000 | Loss: 0.00016840
Iteration 31/1000 | Loss: 0.00016558
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00025193
Iteration 35/1000 | Loss: 0.00010244
Iteration 36/1000 | Loss: 0.00003447
Iteration 37/1000 | Loss: 0.00002608
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002321
Iteration 40/1000 | Loss: 0.00002260
Iteration 41/1000 | Loss: 0.00002215
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002165
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002131
Iteration 49/1000 | Loss: 0.00002129
Iteration 50/1000 | Loss: 0.00002129
Iteration 51/1000 | Loss: 0.00035714
Iteration 52/1000 | Loss: 0.00060231
Iteration 53/1000 | Loss: 0.00043405
Iteration 54/1000 | Loss: 0.00057124
Iteration 55/1000 | Loss: 0.00036604
Iteration 56/1000 | Loss: 0.00002709
Iteration 57/1000 | Loss: 0.00002364
Iteration 58/1000 | Loss: 0.00033033
Iteration 59/1000 | Loss: 0.00003565
Iteration 60/1000 | Loss: 0.00002448
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001791
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001573
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001515
Iteration 74/1000 | Loss: 0.00001511
Iteration 75/1000 | Loss: 0.00001509
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001508
Iteration 78/1000 | Loss: 0.00001507
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001496
Iteration 82/1000 | Loss: 0.00001496
Iteration 83/1000 | Loss: 0.00001495
Iteration 84/1000 | Loss: 0.00001495
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001491
Iteration 87/1000 | Loss: 0.00001491
Iteration 88/1000 | Loss: 0.00001491
Iteration 89/1000 | Loss: 0.00001491
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001490
Iteration 92/1000 | Loss: 0.00001490
Iteration 93/1000 | Loss: 0.00001490
Iteration 94/1000 | Loss: 0.00001490
Iteration 95/1000 | Loss: 0.00001490
Iteration 96/1000 | Loss: 0.00001490
Iteration 97/1000 | Loss: 0.00001490
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001488
Iteration 113/1000 | Loss: 0.00001488
Iteration 114/1000 | Loss: 0.00001488
Iteration 115/1000 | Loss: 0.00001488
Iteration 116/1000 | Loss: 0.00001488
Iteration 117/1000 | Loss: 0.00001488
Iteration 118/1000 | Loss: 0.00001488
Iteration 119/1000 | Loss: 0.00001488
Iteration 120/1000 | Loss: 0.00001488
Iteration 121/1000 | Loss: 0.00001488
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001488
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001488
Iteration 133/1000 | Loss: 0.00001488
Iteration 134/1000 | Loss: 0.00001488
Iteration 135/1000 | Loss: 0.00001488
Iteration 136/1000 | Loss: 0.00001488
Iteration 137/1000 | Loss: 0.00001488
Iteration 138/1000 | Loss: 0.00001488
Iteration 139/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.487930967414286e-05, 1.487930967414286e-05, 1.487930967414286e-05, 1.487930967414286e-05, 1.487930967414286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.487930967414286e-05

Optimization complete. Final v2v error: 3.268187999725342 mm

Highest mean error: 5.069441318511963 mm for frame 63

Lowest mean error: 2.9199612140655518 mm for frame 23

Saving results

Total time: 130.07251715660095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00225338
Iteration 2/25 | Loss: 0.00141675
Iteration 3/25 | Loss: 0.00132815
Iteration 4/25 | Loss: 0.00130684
Iteration 5/25 | Loss: 0.00129909
Iteration 6/25 | Loss: 0.00129657
Iteration 7/25 | Loss: 0.00129579
Iteration 8/25 | Loss: 0.00129579
Iteration 9/25 | Loss: 0.00129579
Iteration 10/25 | Loss: 0.00129579
Iteration 11/25 | Loss: 0.00129579
Iteration 12/25 | Loss: 0.00129579
Iteration 13/25 | Loss: 0.00129579
Iteration 14/25 | Loss: 0.00129579
Iteration 15/25 | Loss: 0.00129579
Iteration 16/25 | Loss: 0.00129579
Iteration 17/25 | Loss: 0.00129579
Iteration 18/25 | Loss: 0.00129579
Iteration 19/25 | Loss: 0.00129579
Iteration 20/25 | Loss: 0.00129579
Iteration 21/25 | Loss: 0.00129579
Iteration 22/25 | Loss: 0.00129579
Iteration 23/25 | Loss: 0.00129579
Iteration 24/25 | Loss: 0.00129579
Iteration 25/25 | Loss: 0.00129579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001295785536058247, 0.001295785536058247, 0.001295785536058247, 0.001295785536058247, 0.001295785536058247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001295785536058247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20070505
Iteration 2/25 | Loss: 0.00405633
Iteration 3/25 | Loss: 0.00405633
Iteration 4/25 | Loss: 0.00405633
Iteration 5/25 | Loss: 0.00405633
Iteration 6/25 | Loss: 0.00405633
Iteration 7/25 | Loss: 0.00405633
Iteration 8/25 | Loss: 0.00405633
Iteration 9/25 | Loss: 0.00405633
Iteration 10/25 | Loss: 0.00405633
Iteration 11/25 | Loss: 0.00405633
Iteration 12/25 | Loss: 0.00405633
Iteration 13/25 | Loss: 0.00405633
Iteration 14/25 | Loss: 0.00405633
Iteration 15/25 | Loss: 0.00405633
Iteration 16/25 | Loss: 0.00405633
Iteration 17/25 | Loss: 0.00405633
Iteration 18/25 | Loss: 0.00405633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004056329373270273, 0.004056329373270273, 0.004056329373270273, 0.004056329373270273, 0.004056329373270273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004056329373270273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00405633
Iteration 2/1000 | Loss: 0.00004720
Iteration 3/1000 | Loss: 0.00002711
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002043
Iteration 6/1000 | Loss: 0.00001919
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001799
Iteration 9/1000 | Loss: 0.00001760
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001716
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001678
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001647
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001643
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001635
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001620
Iteration 45/1000 | Loss: 0.00001619
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001618
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001606
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001606
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001602
Iteration 88/1000 | Loss: 0.00001602
Iteration 89/1000 | Loss: 0.00001602
Iteration 90/1000 | Loss: 0.00001602
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001601
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001601
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001598
Iteration 118/1000 | Loss: 0.00001598
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001597
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001597
Iteration 144/1000 | Loss: 0.00001597
Iteration 145/1000 | Loss: 0.00001597
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001597
Iteration 150/1000 | Loss: 0.00001597
Iteration 151/1000 | Loss: 0.00001597
Iteration 152/1000 | Loss: 0.00001597
Iteration 153/1000 | Loss: 0.00001597
Iteration 154/1000 | Loss: 0.00001597
Iteration 155/1000 | Loss: 0.00001597
Iteration 156/1000 | Loss: 0.00001597
Iteration 157/1000 | Loss: 0.00001597
Iteration 158/1000 | Loss: 0.00001597
Iteration 159/1000 | Loss: 0.00001597
Iteration 160/1000 | Loss: 0.00001597
Iteration 161/1000 | Loss: 0.00001597
Iteration 162/1000 | Loss: 0.00001597
Iteration 163/1000 | Loss: 0.00001597
Iteration 164/1000 | Loss: 0.00001597
Iteration 165/1000 | Loss: 0.00001597
Iteration 166/1000 | Loss: 0.00001597
Iteration 167/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.5971712855389342e-05, 1.5971712855389342e-05, 1.5971712855389342e-05, 1.5971712855389342e-05, 1.5971712855389342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5971712855389342e-05

Optimization complete. Final v2v error: 3.4118423461914062 mm

Highest mean error: 3.871454954147339 mm for frame 61

Lowest mean error: 3.0622730255126953 mm for frame 118

Saving results

Total time: 43.280447006225586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365029
Iteration 2/25 | Loss: 0.00137212
Iteration 3/25 | Loss: 0.00131191
Iteration 4/25 | Loss: 0.00129959
Iteration 5/25 | Loss: 0.00129560
Iteration 6/25 | Loss: 0.00129486
Iteration 7/25 | Loss: 0.00129486
Iteration 8/25 | Loss: 0.00129486
Iteration 9/25 | Loss: 0.00129486
Iteration 10/25 | Loss: 0.00129486
Iteration 11/25 | Loss: 0.00129486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012948571238666773, 0.0012948571238666773, 0.0012948571238666773, 0.0012948571238666773, 0.0012948571238666773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012948571238666773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28437018
Iteration 2/25 | Loss: 0.00249129
Iteration 3/25 | Loss: 0.00249129
Iteration 4/25 | Loss: 0.00249129
Iteration 5/25 | Loss: 0.00249129
Iteration 6/25 | Loss: 0.00249129
Iteration 7/25 | Loss: 0.00249129
Iteration 8/25 | Loss: 0.00249129
Iteration 9/25 | Loss: 0.00249129
Iteration 10/25 | Loss: 0.00249129
Iteration 11/25 | Loss: 0.00249129
Iteration 12/25 | Loss: 0.00249129
Iteration 13/25 | Loss: 0.00249129
Iteration 14/25 | Loss: 0.00249129
Iteration 15/25 | Loss: 0.00249129
Iteration 16/25 | Loss: 0.00249129
Iteration 17/25 | Loss: 0.00249129
Iteration 18/25 | Loss: 0.00249129
Iteration 19/25 | Loss: 0.00249129
Iteration 20/25 | Loss: 0.00249129
Iteration 21/25 | Loss: 0.00249129
Iteration 22/25 | Loss: 0.00249129
Iteration 23/25 | Loss: 0.00249129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0024912869557738304, 0.0024912869557738304, 0.0024912869557738304, 0.0024912869557738304, 0.0024912869557738304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024912869557738304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249129
Iteration 2/1000 | Loss: 0.00002618
Iteration 3/1000 | Loss: 0.00001725
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001306
Iteration 7/1000 | Loss: 0.00001253
Iteration 8/1000 | Loss: 0.00001215
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001127
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001105
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001092
Iteration 20/1000 | Loss: 0.00001087
Iteration 21/1000 | Loss: 0.00001086
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001069
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001069
Iteration 43/1000 | Loss: 0.00001069
Iteration 44/1000 | Loss: 0.00001069
Iteration 45/1000 | Loss: 0.00001069
Iteration 46/1000 | Loss: 0.00001069
Iteration 47/1000 | Loss: 0.00001068
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001066
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001066
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001064
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001060
Iteration 73/1000 | Loss: 0.00001060
Iteration 74/1000 | Loss: 0.00001060
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001059
Iteration 77/1000 | Loss: 0.00001059
Iteration 78/1000 | Loss: 0.00001059
Iteration 79/1000 | Loss: 0.00001059
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001055
Iteration 92/1000 | Loss: 0.00001055
Iteration 93/1000 | Loss: 0.00001055
Iteration 94/1000 | Loss: 0.00001055
Iteration 95/1000 | Loss: 0.00001054
Iteration 96/1000 | Loss: 0.00001054
Iteration 97/1000 | Loss: 0.00001054
Iteration 98/1000 | Loss: 0.00001054
Iteration 99/1000 | Loss: 0.00001054
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001053
Iteration 105/1000 | Loss: 0.00001053
Iteration 106/1000 | Loss: 0.00001053
Iteration 107/1000 | Loss: 0.00001053
Iteration 108/1000 | Loss: 0.00001052
Iteration 109/1000 | Loss: 0.00001052
Iteration 110/1000 | Loss: 0.00001052
Iteration 111/1000 | Loss: 0.00001052
Iteration 112/1000 | Loss: 0.00001052
Iteration 113/1000 | Loss: 0.00001052
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001052
Iteration 117/1000 | Loss: 0.00001052
Iteration 118/1000 | Loss: 0.00001052
Iteration 119/1000 | Loss: 0.00001052
Iteration 120/1000 | Loss: 0.00001051
Iteration 121/1000 | Loss: 0.00001051
Iteration 122/1000 | Loss: 0.00001051
Iteration 123/1000 | Loss: 0.00001051
Iteration 124/1000 | Loss: 0.00001051
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001050
Iteration 127/1000 | Loss: 0.00001050
Iteration 128/1000 | Loss: 0.00001050
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001049
Iteration 133/1000 | Loss: 0.00001049
Iteration 134/1000 | Loss: 0.00001048
Iteration 135/1000 | Loss: 0.00001048
Iteration 136/1000 | Loss: 0.00001048
Iteration 137/1000 | Loss: 0.00001048
Iteration 138/1000 | Loss: 0.00001048
Iteration 139/1000 | Loss: 0.00001048
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001047
Iteration 142/1000 | Loss: 0.00001047
Iteration 143/1000 | Loss: 0.00001047
Iteration 144/1000 | Loss: 0.00001047
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001046
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001046
Iteration 149/1000 | Loss: 0.00001046
Iteration 150/1000 | Loss: 0.00001046
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001045
Iteration 153/1000 | Loss: 0.00001045
Iteration 154/1000 | Loss: 0.00001045
Iteration 155/1000 | Loss: 0.00001045
Iteration 156/1000 | Loss: 0.00001045
Iteration 157/1000 | Loss: 0.00001045
Iteration 158/1000 | Loss: 0.00001045
Iteration 159/1000 | Loss: 0.00001045
Iteration 160/1000 | Loss: 0.00001045
Iteration 161/1000 | Loss: 0.00001045
Iteration 162/1000 | Loss: 0.00001045
Iteration 163/1000 | Loss: 0.00001045
Iteration 164/1000 | Loss: 0.00001045
Iteration 165/1000 | Loss: 0.00001045
Iteration 166/1000 | Loss: 0.00001045
Iteration 167/1000 | Loss: 0.00001044
Iteration 168/1000 | Loss: 0.00001044
Iteration 169/1000 | Loss: 0.00001044
Iteration 170/1000 | Loss: 0.00001044
Iteration 171/1000 | Loss: 0.00001044
Iteration 172/1000 | Loss: 0.00001044
Iteration 173/1000 | Loss: 0.00001044
Iteration 174/1000 | Loss: 0.00001044
Iteration 175/1000 | Loss: 0.00001044
Iteration 176/1000 | Loss: 0.00001044
Iteration 177/1000 | Loss: 0.00001044
Iteration 178/1000 | Loss: 0.00001044
Iteration 179/1000 | Loss: 0.00001044
Iteration 180/1000 | Loss: 0.00001044
Iteration 181/1000 | Loss: 0.00001044
Iteration 182/1000 | Loss: 0.00001044
Iteration 183/1000 | Loss: 0.00001044
Iteration 184/1000 | Loss: 0.00001044
Iteration 185/1000 | Loss: 0.00001044
Iteration 186/1000 | Loss: 0.00001044
Iteration 187/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0435035619593691e-05, 1.0435035619593691e-05, 1.0435035619593691e-05, 1.0435035619593691e-05, 1.0435035619593691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0435035619593691e-05

Optimization complete. Final v2v error: 2.8208720684051514 mm

Highest mean error: 3.0728371143341064 mm for frame 34

Lowest mean error: 2.5970795154571533 mm for frame 135

Saving results

Total time: 41.590909004211426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809583
Iteration 2/25 | Loss: 0.00162552
Iteration 3/25 | Loss: 0.00146103
Iteration 4/25 | Loss: 0.00143124
Iteration 5/25 | Loss: 0.00142408
Iteration 6/25 | Loss: 0.00142211
Iteration 7/25 | Loss: 0.00142185
Iteration 8/25 | Loss: 0.00142185
Iteration 9/25 | Loss: 0.00142185
Iteration 10/25 | Loss: 0.00142185
Iteration 11/25 | Loss: 0.00142185
Iteration 12/25 | Loss: 0.00142185
Iteration 13/25 | Loss: 0.00142185
Iteration 14/25 | Loss: 0.00142185
Iteration 15/25 | Loss: 0.00142185
Iteration 16/25 | Loss: 0.00142185
Iteration 17/25 | Loss: 0.00142185
Iteration 18/25 | Loss: 0.00142185
Iteration 19/25 | Loss: 0.00142185
Iteration 20/25 | Loss: 0.00142185
Iteration 21/25 | Loss: 0.00142185
Iteration 22/25 | Loss: 0.00142185
Iteration 23/25 | Loss: 0.00142185
Iteration 24/25 | Loss: 0.00142185
Iteration 25/25 | Loss: 0.00142185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60401404
Iteration 2/25 | Loss: 0.00263365
Iteration 3/25 | Loss: 0.00263360
Iteration 4/25 | Loss: 0.00263359
Iteration 5/25 | Loss: 0.00263359
Iteration 6/25 | Loss: 0.00263359
Iteration 7/25 | Loss: 0.00263359
Iteration 8/25 | Loss: 0.00263359
Iteration 9/25 | Loss: 0.00263359
Iteration 10/25 | Loss: 0.00263359
Iteration 11/25 | Loss: 0.00263359
Iteration 12/25 | Loss: 0.00263359
Iteration 13/25 | Loss: 0.00263359
Iteration 14/25 | Loss: 0.00263359
Iteration 15/25 | Loss: 0.00263359
Iteration 16/25 | Loss: 0.00263359
Iteration 17/25 | Loss: 0.00263359
Iteration 18/25 | Loss: 0.00263359
Iteration 19/25 | Loss: 0.00263359
Iteration 20/25 | Loss: 0.00263359
Iteration 21/25 | Loss: 0.00263359
Iteration 22/25 | Loss: 0.00263359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026335923466831446, 0.0026335923466831446, 0.0026335923466831446, 0.0026335923466831446, 0.0026335923466831446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026335923466831446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263359
Iteration 2/1000 | Loss: 0.00007470
Iteration 3/1000 | Loss: 0.00004711
Iteration 4/1000 | Loss: 0.00003727
Iteration 5/1000 | Loss: 0.00003418
Iteration 6/1000 | Loss: 0.00003243
Iteration 7/1000 | Loss: 0.00003144
Iteration 8/1000 | Loss: 0.00003035
Iteration 9/1000 | Loss: 0.00002952
Iteration 10/1000 | Loss: 0.00002900
Iteration 11/1000 | Loss: 0.00002854
Iteration 12/1000 | Loss: 0.00002820
Iteration 13/1000 | Loss: 0.00002792
Iteration 14/1000 | Loss: 0.00002768
Iteration 15/1000 | Loss: 0.00002751
Iteration 16/1000 | Loss: 0.00002733
Iteration 17/1000 | Loss: 0.00002727
Iteration 18/1000 | Loss: 0.00002709
Iteration 19/1000 | Loss: 0.00002706
Iteration 20/1000 | Loss: 0.00002705
Iteration 21/1000 | Loss: 0.00002700
Iteration 22/1000 | Loss: 0.00002696
Iteration 23/1000 | Loss: 0.00002695
Iteration 24/1000 | Loss: 0.00002694
Iteration 25/1000 | Loss: 0.00002694
Iteration 26/1000 | Loss: 0.00002690
Iteration 27/1000 | Loss: 0.00002687
Iteration 28/1000 | Loss: 0.00002686
Iteration 29/1000 | Loss: 0.00002677
Iteration 30/1000 | Loss: 0.00002675
Iteration 31/1000 | Loss: 0.00002675
Iteration 32/1000 | Loss: 0.00002674
Iteration 33/1000 | Loss: 0.00002674
Iteration 34/1000 | Loss: 0.00002674
Iteration 35/1000 | Loss: 0.00002674
Iteration 36/1000 | Loss: 0.00002673
Iteration 37/1000 | Loss: 0.00002673
Iteration 38/1000 | Loss: 0.00002673
Iteration 39/1000 | Loss: 0.00002672
Iteration 40/1000 | Loss: 0.00002672
Iteration 41/1000 | Loss: 0.00002672
Iteration 42/1000 | Loss: 0.00002672
Iteration 43/1000 | Loss: 0.00002672
Iteration 44/1000 | Loss: 0.00002671
Iteration 45/1000 | Loss: 0.00002670
Iteration 46/1000 | Loss: 0.00002670
Iteration 47/1000 | Loss: 0.00002670
Iteration 48/1000 | Loss: 0.00002670
Iteration 49/1000 | Loss: 0.00002670
Iteration 50/1000 | Loss: 0.00002670
Iteration 51/1000 | Loss: 0.00002669
Iteration 52/1000 | Loss: 0.00002669
Iteration 53/1000 | Loss: 0.00002669
Iteration 54/1000 | Loss: 0.00002669
Iteration 55/1000 | Loss: 0.00002669
Iteration 56/1000 | Loss: 0.00002669
Iteration 57/1000 | Loss: 0.00002669
Iteration 58/1000 | Loss: 0.00002669
Iteration 59/1000 | Loss: 0.00002669
Iteration 60/1000 | Loss: 0.00002668
Iteration 61/1000 | Loss: 0.00002668
Iteration 62/1000 | Loss: 0.00002667
Iteration 63/1000 | Loss: 0.00002667
Iteration 64/1000 | Loss: 0.00002666
Iteration 65/1000 | Loss: 0.00002666
Iteration 66/1000 | Loss: 0.00002666
Iteration 67/1000 | Loss: 0.00002666
Iteration 68/1000 | Loss: 0.00002666
Iteration 69/1000 | Loss: 0.00002665
Iteration 70/1000 | Loss: 0.00002665
Iteration 71/1000 | Loss: 0.00002665
Iteration 72/1000 | Loss: 0.00002665
Iteration 73/1000 | Loss: 0.00002665
Iteration 74/1000 | Loss: 0.00002665
Iteration 75/1000 | Loss: 0.00002665
Iteration 76/1000 | Loss: 0.00002665
Iteration 77/1000 | Loss: 0.00002665
Iteration 78/1000 | Loss: 0.00002665
Iteration 79/1000 | Loss: 0.00002665
Iteration 80/1000 | Loss: 0.00002664
Iteration 81/1000 | Loss: 0.00002664
Iteration 82/1000 | Loss: 0.00002664
Iteration 83/1000 | Loss: 0.00002664
Iteration 84/1000 | Loss: 0.00002664
Iteration 85/1000 | Loss: 0.00002664
Iteration 86/1000 | Loss: 0.00002664
Iteration 87/1000 | Loss: 0.00002664
Iteration 88/1000 | Loss: 0.00002664
Iteration 89/1000 | Loss: 0.00002664
Iteration 90/1000 | Loss: 0.00002664
Iteration 91/1000 | Loss: 0.00002663
Iteration 92/1000 | Loss: 0.00002663
Iteration 93/1000 | Loss: 0.00002663
Iteration 94/1000 | Loss: 0.00002663
Iteration 95/1000 | Loss: 0.00002663
Iteration 96/1000 | Loss: 0.00002663
Iteration 97/1000 | Loss: 0.00002663
Iteration 98/1000 | Loss: 0.00002662
Iteration 99/1000 | Loss: 0.00002662
Iteration 100/1000 | Loss: 0.00002662
Iteration 101/1000 | Loss: 0.00002662
Iteration 102/1000 | Loss: 0.00002662
Iteration 103/1000 | Loss: 0.00002662
Iteration 104/1000 | Loss: 0.00002662
Iteration 105/1000 | Loss: 0.00002662
Iteration 106/1000 | Loss: 0.00002662
Iteration 107/1000 | Loss: 0.00002662
Iteration 108/1000 | Loss: 0.00002662
Iteration 109/1000 | Loss: 0.00002661
Iteration 110/1000 | Loss: 0.00002661
Iteration 111/1000 | Loss: 0.00002661
Iteration 112/1000 | Loss: 0.00002661
Iteration 113/1000 | Loss: 0.00002661
Iteration 114/1000 | Loss: 0.00002661
Iteration 115/1000 | Loss: 0.00002661
Iteration 116/1000 | Loss: 0.00002661
Iteration 117/1000 | Loss: 0.00002661
Iteration 118/1000 | Loss: 0.00002661
Iteration 119/1000 | Loss: 0.00002661
Iteration 120/1000 | Loss: 0.00002660
Iteration 121/1000 | Loss: 0.00002660
Iteration 122/1000 | Loss: 0.00002660
Iteration 123/1000 | Loss: 0.00002660
Iteration 124/1000 | Loss: 0.00002660
Iteration 125/1000 | Loss: 0.00002660
Iteration 126/1000 | Loss: 0.00002660
Iteration 127/1000 | Loss: 0.00002659
Iteration 128/1000 | Loss: 0.00002659
Iteration 129/1000 | Loss: 0.00002659
Iteration 130/1000 | Loss: 0.00002659
Iteration 131/1000 | Loss: 0.00002659
Iteration 132/1000 | Loss: 0.00002659
Iteration 133/1000 | Loss: 0.00002659
Iteration 134/1000 | Loss: 0.00002659
Iteration 135/1000 | Loss: 0.00002659
Iteration 136/1000 | Loss: 0.00002659
Iteration 137/1000 | Loss: 0.00002659
Iteration 138/1000 | Loss: 0.00002659
Iteration 139/1000 | Loss: 0.00002658
Iteration 140/1000 | Loss: 0.00002658
Iteration 141/1000 | Loss: 0.00002658
Iteration 142/1000 | Loss: 0.00002658
Iteration 143/1000 | Loss: 0.00002658
Iteration 144/1000 | Loss: 0.00002658
Iteration 145/1000 | Loss: 0.00002658
Iteration 146/1000 | Loss: 0.00002658
Iteration 147/1000 | Loss: 0.00002657
Iteration 148/1000 | Loss: 0.00002657
Iteration 149/1000 | Loss: 0.00002657
Iteration 150/1000 | Loss: 0.00002657
Iteration 151/1000 | Loss: 0.00002657
Iteration 152/1000 | Loss: 0.00002657
Iteration 153/1000 | Loss: 0.00002656
Iteration 154/1000 | Loss: 0.00002656
Iteration 155/1000 | Loss: 0.00002656
Iteration 156/1000 | Loss: 0.00002656
Iteration 157/1000 | Loss: 0.00002656
Iteration 158/1000 | Loss: 0.00002656
Iteration 159/1000 | Loss: 0.00002656
Iteration 160/1000 | Loss: 0.00002656
Iteration 161/1000 | Loss: 0.00002656
Iteration 162/1000 | Loss: 0.00002656
Iteration 163/1000 | Loss: 0.00002655
Iteration 164/1000 | Loss: 0.00002655
Iteration 165/1000 | Loss: 0.00002655
Iteration 166/1000 | Loss: 0.00002655
Iteration 167/1000 | Loss: 0.00002655
Iteration 168/1000 | Loss: 0.00002655
Iteration 169/1000 | Loss: 0.00002655
Iteration 170/1000 | Loss: 0.00002655
Iteration 171/1000 | Loss: 0.00002655
Iteration 172/1000 | Loss: 0.00002655
Iteration 173/1000 | Loss: 0.00002655
Iteration 174/1000 | Loss: 0.00002654
Iteration 175/1000 | Loss: 0.00002654
Iteration 176/1000 | Loss: 0.00002654
Iteration 177/1000 | Loss: 0.00002654
Iteration 178/1000 | Loss: 0.00002654
Iteration 179/1000 | Loss: 0.00002654
Iteration 180/1000 | Loss: 0.00002654
Iteration 181/1000 | Loss: 0.00002654
Iteration 182/1000 | Loss: 0.00002654
Iteration 183/1000 | Loss: 0.00002654
Iteration 184/1000 | Loss: 0.00002654
Iteration 185/1000 | Loss: 0.00002654
Iteration 186/1000 | Loss: 0.00002654
Iteration 187/1000 | Loss: 0.00002654
Iteration 188/1000 | Loss: 0.00002654
Iteration 189/1000 | Loss: 0.00002653
Iteration 190/1000 | Loss: 0.00002653
Iteration 191/1000 | Loss: 0.00002653
Iteration 192/1000 | Loss: 0.00002653
Iteration 193/1000 | Loss: 0.00002653
Iteration 194/1000 | Loss: 0.00002653
Iteration 195/1000 | Loss: 0.00002653
Iteration 196/1000 | Loss: 0.00002653
Iteration 197/1000 | Loss: 0.00002653
Iteration 198/1000 | Loss: 0.00002653
Iteration 199/1000 | Loss: 0.00002653
Iteration 200/1000 | Loss: 0.00002653
Iteration 201/1000 | Loss: 0.00002653
Iteration 202/1000 | Loss: 0.00002653
Iteration 203/1000 | Loss: 0.00002653
Iteration 204/1000 | Loss: 0.00002653
Iteration 205/1000 | Loss: 0.00002653
Iteration 206/1000 | Loss: 0.00002653
Iteration 207/1000 | Loss: 0.00002652
Iteration 208/1000 | Loss: 0.00002652
Iteration 209/1000 | Loss: 0.00002652
Iteration 210/1000 | Loss: 0.00002652
Iteration 211/1000 | Loss: 0.00002652
Iteration 212/1000 | Loss: 0.00002652
Iteration 213/1000 | Loss: 0.00002652
Iteration 214/1000 | Loss: 0.00002652
Iteration 215/1000 | Loss: 0.00002651
Iteration 216/1000 | Loss: 0.00002651
Iteration 217/1000 | Loss: 0.00002651
Iteration 218/1000 | Loss: 0.00002651
Iteration 219/1000 | Loss: 0.00002651
Iteration 220/1000 | Loss: 0.00002651
Iteration 221/1000 | Loss: 0.00002651
Iteration 222/1000 | Loss: 0.00002651
Iteration 223/1000 | Loss: 0.00002651
Iteration 224/1000 | Loss: 0.00002651
Iteration 225/1000 | Loss: 0.00002651
Iteration 226/1000 | Loss: 0.00002651
Iteration 227/1000 | Loss: 0.00002650
Iteration 228/1000 | Loss: 0.00002650
Iteration 229/1000 | Loss: 0.00002650
Iteration 230/1000 | Loss: 0.00002650
Iteration 231/1000 | Loss: 0.00002650
Iteration 232/1000 | Loss: 0.00002650
Iteration 233/1000 | Loss: 0.00002649
Iteration 234/1000 | Loss: 0.00002649
Iteration 235/1000 | Loss: 0.00002649
Iteration 236/1000 | Loss: 0.00002649
Iteration 237/1000 | Loss: 0.00002649
Iteration 238/1000 | Loss: 0.00002649
Iteration 239/1000 | Loss: 0.00002649
Iteration 240/1000 | Loss: 0.00002649
Iteration 241/1000 | Loss: 0.00002648
Iteration 242/1000 | Loss: 0.00002648
Iteration 243/1000 | Loss: 0.00002648
Iteration 244/1000 | Loss: 0.00002648
Iteration 245/1000 | Loss: 0.00002648
Iteration 246/1000 | Loss: 0.00002648
Iteration 247/1000 | Loss: 0.00002648
Iteration 248/1000 | Loss: 0.00002648
Iteration 249/1000 | Loss: 0.00002648
Iteration 250/1000 | Loss: 0.00002648
Iteration 251/1000 | Loss: 0.00002648
Iteration 252/1000 | Loss: 0.00002647
Iteration 253/1000 | Loss: 0.00002647
Iteration 254/1000 | Loss: 0.00002647
Iteration 255/1000 | Loss: 0.00002647
Iteration 256/1000 | Loss: 0.00002647
Iteration 257/1000 | Loss: 0.00002647
Iteration 258/1000 | Loss: 0.00002647
Iteration 259/1000 | Loss: 0.00002647
Iteration 260/1000 | Loss: 0.00002647
Iteration 261/1000 | Loss: 0.00002647
Iteration 262/1000 | Loss: 0.00002647
Iteration 263/1000 | Loss: 0.00002647
Iteration 264/1000 | Loss: 0.00002646
Iteration 265/1000 | Loss: 0.00002646
Iteration 266/1000 | Loss: 0.00002646
Iteration 267/1000 | Loss: 0.00002646
Iteration 268/1000 | Loss: 0.00002646
Iteration 269/1000 | Loss: 0.00002646
Iteration 270/1000 | Loss: 0.00002646
Iteration 271/1000 | Loss: 0.00002646
Iteration 272/1000 | Loss: 0.00002646
Iteration 273/1000 | Loss: 0.00002646
Iteration 274/1000 | Loss: 0.00002646
Iteration 275/1000 | Loss: 0.00002646
Iteration 276/1000 | Loss: 0.00002646
Iteration 277/1000 | Loss: 0.00002646
Iteration 278/1000 | Loss: 0.00002646
Iteration 279/1000 | Loss: 0.00002646
Iteration 280/1000 | Loss: 0.00002645
Iteration 281/1000 | Loss: 0.00002645
Iteration 282/1000 | Loss: 0.00002645
Iteration 283/1000 | Loss: 0.00002645
Iteration 284/1000 | Loss: 0.00002645
Iteration 285/1000 | Loss: 0.00002645
Iteration 286/1000 | Loss: 0.00002645
Iteration 287/1000 | Loss: 0.00002645
Iteration 288/1000 | Loss: 0.00002645
Iteration 289/1000 | Loss: 0.00002645
Iteration 290/1000 | Loss: 0.00002644
Iteration 291/1000 | Loss: 0.00002644
Iteration 292/1000 | Loss: 0.00002644
Iteration 293/1000 | Loss: 0.00002644
Iteration 294/1000 | Loss: 0.00002644
Iteration 295/1000 | Loss: 0.00002644
Iteration 296/1000 | Loss: 0.00002644
Iteration 297/1000 | Loss: 0.00002644
Iteration 298/1000 | Loss: 0.00002644
Iteration 299/1000 | Loss: 0.00002644
Iteration 300/1000 | Loss: 0.00002644
Iteration 301/1000 | Loss: 0.00002644
Iteration 302/1000 | Loss: 0.00002644
Iteration 303/1000 | Loss: 0.00002644
Iteration 304/1000 | Loss: 0.00002644
Iteration 305/1000 | Loss: 0.00002644
Iteration 306/1000 | Loss: 0.00002643
Iteration 307/1000 | Loss: 0.00002643
Iteration 308/1000 | Loss: 0.00002643
Iteration 309/1000 | Loss: 0.00002643
Iteration 310/1000 | Loss: 0.00002643
Iteration 311/1000 | Loss: 0.00002643
Iteration 312/1000 | Loss: 0.00002643
Iteration 313/1000 | Loss: 0.00002643
Iteration 314/1000 | Loss: 0.00002643
Iteration 315/1000 | Loss: 0.00002643
Iteration 316/1000 | Loss: 0.00002643
Iteration 317/1000 | Loss: 0.00002643
Iteration 318/1000 | Loss: 0.00002643
Iteration 319/1000 | Loss: 0.00002643
Iteration 320/1000 | Loss: 0.00002643
Iteration 321/1000 | Loss: 0.00002642
Iteration 322/1000 | Loss: 0.00002642
Iteration 323/1000 | Loss: 0.00002642
Iteration 324/1000 | Loss: 0.00002642
Iteration 325/1000 | Loss: 0.00002642
Iteration 326/1000 | Loss: 0.00002642
Iteration 327/1000 | Loss: 0.00002642
Iteration 328/1000 | Loss: 0.00002642
Iteration 329/1000 | Loss: 0.00002642
Iteration 330/1000 | Loss: 0.00002642
Iteration 331/1000 | Loss: 0.00002642
Iteration 332/1000 | Loss: 0.00002642
Iteration 333/1000 | Loss: 0.00002642
Iteration 334/1000 | Loss: 0.00002642
Iteration 335/1000 | Loss: 0.00002642
Iteration 336/1000 | Loss: 0.00002642
Iteration 337/1000 | Loss: 0.00002641
Iteration 338/1000 | Loss: 0.00002641
Iteration 339/1000 | Loss: 0.00002641
Iteration 340/1000 | Loss: 0.00002641
Iteration 341/1000 | Loss: 0.00002641
Iteration 342/1000 | Loss: 0.00002641
Iteration 343/1000 | Loss: 0.00002641
Iteration 344/1000 | Loss: 0.00002641
Iteration 345/1000 | Loss: 0.00002641
Iteration 346/1000 | Loss: 0.00002641
Iteration 347/1000 | Loss: 0.00002641
Iteration 348/1000 | Loss: 0.00002641
Iteration 349/1000 | Loss: 0.00002641
Iteration 350/1000 | Loss: 0.00002641
Iteration 351/1000 | Loss: 0.00002641
Iteration 352/1000 | Loss: 0.00002641
Iteration 353/1000 | Loss: 0.00002641
Iteration 354/1000 | Loss: 0.00002641
Iteration 355/1000 | Loss: 0.00002641
Iteration 356/1000 | Loss: 0.00002641
Iteration 357/1000 | Loss: 0.00002640
Iteration 358/1000 | Loss: 0.00002640
Iteration 359/1000 | Loss: 0.00002640
Iteration 360/1000 | Loss: 0.00002640
Iteration 361/1000 | Loss: 0.00002640
Iteration 362/1000 | Loss: 0.00002640
Iteration 363/1000 | Loss: 0.00002640
Iteration 364/1000 | Loss: 0.00002640
Iteration 365/1000 | Loss: 0.00002640
Iteration 366/1000 | Loss: 0.00002640
Iteration 367/1000 | Loss: 0.00002640
Iteration 368/1000 | Loss: 0.00002640
Iteration 369/1000 | Loss: 0.00002640
Iteration 370/1000 | Loss: 0.00002640
Iteration 371/1000 | Loss: 0.00002640
Iteration 372/1000 | Loss: 0.00002640
Iteration 373/1000 | Loss: 0.00002640
Iteration 374/1000 | Loss: 0.00002640
Iteration 375/1000 | Loss: 0.00002640
Iteration 376/1000 | Loss: 0.00002640
Iteration 377/1000 | Loss: 0.00002640
Iteration 378/1000 | Loss: 0.00002640
Iteration 379/1000 | Loss: 0.00002640
Iteration 380/1000 | Loss: 0.00002640
Iteration 381/1000 | Loss: 0.00002640
Iteration 382/1000 | Loss: 0.00002640
Iteration 383/1000 | Loss: 0.00002640
Iteration 384/1000 | Loss: 0.00002640
Iteration 385/1000 | Loss: 0.00002640
Iteration 386/1000 | Loss: 0.00002640
Iteration 387/1000 | Loss: 0.00002640
Iteration 388/1000 | Loss: 0.00002640
Iteration 389/1000 | Loss: 0.00002640
Iteration 390/1000 | Loss: 0.00002640
Iteration 391/1000 | Loss: 0.00002640
Iteration 392/1000 | Loss: 0.00002640
Iteration 393/1000 | Loss: 0.00002640
Iteration 394/1000 | Loss: 0.00002640
Iteration 395/1000 | Loss: 0.00002640
Iteration 396/1000 | Loss: 0.00002640
Iteration 397/1000 | Loss: 0.00002640
Iteration 398/1000 | Loss: 0.00002640
Iteration 399/1000 | Loss: 0.00002640
Iteration 400/1000 | Loss: 0.00002640
Iteration 401/1000 | Loss: 0.00002640
Iteration 402/1000 | Loss: 0.00002640
Iteration 403/1000 | Loss: 0.00002640
Iteration 404/1000 | Loss: 0.00002640
Iteration 405/1000 | Loss: 0.00002640
Iteration 406/1000 | Loss: 0.00002640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 406. Stopping optimization.
Last 5 losses: [2.6397514375275932e-05, 2.6397514375275932e-05, 2.6397514375275932e-05, 2.6397514375275932e-05, 2.6397514375275932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6397514375275932e-05

Optimization complete. Final v2v error: 4.251771926879883 mm

Highest mean error: 5.483872413635254 mm for frame 163

Lowest mean error: 3.403510332107544 mm for frame 88

Saving results

Total time: 62.27110934257507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913046
Iteration 2/25 | Loss: 0.00176788
Iteration 3/25 | Loss: 0.00145406
Iteration 4/25 | Loss: 0.00143250
Iteration 5/25 | Loss: 0.00142843
Iteration 6/25 | Loss: 0.00142721
Iteration 7/25 | Loss: 0.00142721
Iteration 8/25 | Loss: 0.00142721
Iteration 9/25 | Loss: 0.00142721
Iteration 10/25 | Loss: 0.00142721
Iteration 11/25 | Loss: 0.00142721
Iteration 12/25 | Loss: 0.00142721
Iteration 13/25 | Loss: 0.00142721
Iteration 14/25 | Loss: 0.00142721
Iteration 15/25 | Loss: 0.00142721
Iteration 16/25 | Loss: 0.00142721
Iteration 17/25 | Loss: 0.00142721
Iteration 18/25 | Loss: 0.00142721
Iteration 19/25 | Loss: 0.00142721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001427214127033949, 0.001427214127033949, 0.001427214127033949, 0.001427214127033949, 0.001427214127033949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001427214127033949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90627146
Iteration 2/25 | Loss: 0.00184027
Iteration 3/25 | Loss: 0.00184026
Iteration 4/25 | Loss: 0.00184026
Iteration 5/25 | Loss: 0.00184026
Iteration 6/25 | Loss: 0.00184026
Iteration 7/25 | Loss: 0.00184026
Iteration 8/25 | Loss: 0.00184026
Iteration 9/25 | Loss: 0.00184026
Iteration 10/25 | Loss: 0.00184026
Iteration 11/25 | Loss: 0.00184026
Iteration 12/25 | Loss: 0.00184026
Iteration 13/25 | Loss: 0.00184026
Iteration 14/25 | Loss: 0.00184026
Iteration 15/25 | Loss: 0.00184026
Iteration 16/25 | Loss: 0.00184026
Iteration 17/25 | Loss: 0.00184026
Iteration 18/25 | Loss: 0.00184026
Iteration 19/25 | Loss: 0.00184026
Iteration 20/25 | Loss: 0.00184026
Iteration 21/25 | Loss: 0.00184026
Iteration 22/25 | Loss: 0.00184026
Iteration 23/25 | Loss: 0.00184026
Iteration 24/25 | Loss: 0.00184026
Iteration 25/25 | Loss: 0.00184026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184026
Iteration 2/1000 | Loss: 0.00004758
Iteration 3/1000 | Loss: 0.00003485
Iteration 4/1000 | Loss: 0.00002873
Iteration 5/1000 | Loss: 0.00002672
Iteration 6/1000 | Loss: 0.00002541
Iteration 7/1000 | Loss: 0.00002455
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002307
Iteration 10/1000 | Loss: 0.00002266
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002169
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00002124
Iteration 16/1000 | Loss: 0.00002117
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002091
Iteration 19/1000 | Loss: 0.00002082
Iteration 20/1000 | Loss: 0.00002079
Iteration 21/1000 | Loss: 0.00002077
Iteration 22/1000 | Loss: 0.00002072
Iteration 23/1000 | Loss: 0.00002066
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002046
Iteration 34/1000 | Loss: 0.00002046
Iteration 35/1000 | Loss: 0.00002046
Iteration 36/1000 | Loss: 0.00002046
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002044
Iteration 47/1000 | Loss: 0.00002044
Iteration 48/1000 | Loss: 0.00002044
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002043
Iteration 56/1000 | Loss: 0.00002043
Iteration 57/1000 | Loss: 0.00002043
Iteration 58/1000 | Loss: 0.00002043
Iteration 59/1000 | Loss: 0.00002042
Iteration 60/1000 | Loss: 0.00002042
Iteration 61/1000 | Loss: 0.00002042
Iteration 62/1000 | Loss: 0.00002042
Iteration 63/1000 | Loss: 0.00002041
Iteration 64/1000 | Loss: 0.00002041
Iteration 65/1000 | Loss: 0.00002041
Iteration 66/1000 | Loss: 0.00002041
Iteration 67/1000 | Loss: 0.00002040
Iteration 68/1000 | Loss: 0.00002040
Iteration 69/1000 | Loss: 0.00002040
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002039
Iteration 72/1000 | Loss: 0.00002039
Iteration 73/1000 | Loss: 0.00002038
Iteration 74/1000 | Loss: 0.00002038
Iteration 75/1000 | Loss: 0.00002038
Iteration 76/1000 | Loss: 0.00002038
Iteration 77/1000 | Loss: 0.00002038
Iteration 78/1000 | Loss: 0.00002038
Iteration 79/1000 | Loss: 0.00002038
Iteration 80/1000 | Loss: 0.00002038
Iteration 81/1000 | Loss: 0.00002038
Iteration 82/1000 | Loss: 0.00002037
Iteration 83/1000 | Loss: 0.00002037
Iteration 84/1000 | Loss: 0.00002037
Iteration 85/1000 | Loss: 0.00002037
Iteration 86/1000 | Loss: 0.00002037
Iteration 87/1000 | Loss: 0.00002037
Iteration 88/1000 | Loss: 0.00002037
Iteration 89/1000 | Loss: 0.00002037
Iteration 90/1000 | Loss: 0.00002037
Iteration 91/1000 | Loss: 0.00002037
Iteration 92/1000 | Loss: 0.00002037
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002037
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002036
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002036
Iteration 103/1000 | Loss: 0.00002036
Iteration 104/1000 | Loss: 0.00002036
Iteration 105/1000 | Loss: 0.00002036
Iteration 106/1000 | Loss: 0.00002035
Iteration 107/1000 | Loss: 0.00002035
Iteration 108/1000 | Loss: 0.00002035
Iteration 109/1000 | Loss: 0.00002035
Iteration 110/1000 | Loss: 0.00002035
Iteration 111/1000 | Loss: 0.00002035
Iteration 112/1000 | Loss: 0.00002035
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002035
Iteration 115/1000 | Loss: 0.00002035
Iteration 116/1000 | Loss: 0.00002035
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002035
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002034
Iteration 126/1000 | Loss: 0.00002034
Iteration 127/1000 | Loss: 0.00002034
Iteration 128/1000 | Loss: 0.00002034
Iteration 129/1000 | Loss: 0.00002034
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.0344339645816945e-05, 2.0344339645816945e-05, 2.0344339645816945e-05, 2.0344339645816945e-05, 2.0344339645816945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0344339645816945e-05

Optimization complete. Final v2v error: 3.8141791820526123 mm

Highest mean error: 4.356543064117432 mm for frame 137

Lowest mean error: 3.1057677268981934 mm for frame 28

Saving results

Total time: 45.818605184555054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946974
Iteration 2/25 | Loss: 0.00946974
Iteration 3/25 | Loss: 0.00946974
Iteration 4/25 | Loss: 0.00946974
Iteration 5/25 | Loss: 0.00946973
Iteration 6/25 | Loss: 0.00946973
Iteration 7/25 | Loss: 0.00946973
Iteration 8/25 | Loss: 0.00946973
Iteration 9/25 | Loss: 0.00946973
Iteration 10/25 | Loss: 0.00946973
Iteration 11/25 | Loss: 0.00946973
Iteration 12/25 | Loss: 0.00946973
Iteration 13/25 | Loss: 0.00946973
Iteration 14/25 | Loss: 0.00946973
Iteration 15/25 | Loss: 0.00946973
Iteration 16/25 | Loss: 0.00946972
Iteration 17/25 | Loss: 0.00946972
Iteration 18/25 | Loss: 0.00946972
Iteration 19/25 | Loss: 0.00946972
Iteration 20/25 | Loss: 0.00946972
Iteration 21/25 | Loss: 0.00946972
Iteration 22/25 | Loss: 0.00946972
Iteration 23/25 | Loss: 0.00946972
Iteration 24/25 | Loss: 0.00946971
Iteration 25/25 | Loss: 0.00946971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58169580
Iteration 2/25 | Loss: 0.17916262
Iteration 3/25 | Loss: 0.17723206
Iteration 4/25 | Loss: 0.17664380
Iteration 5/25 | Loss: 0.17590159
Iteration 6/25 | Loss: 0.17583387
Iteration 7/25 | Loss: 0.17583381
Iteration 8/25 | Loss: 0.17583375
Iteration 9/25 | Loss: 0.17583375
Iteration 10/25 | Loss: 0.17583370
Iteration 11/25 | Loss: 0.17583370
Iteration 12/25 | Loss: 0.17583370
Iteration 13/25 | Loss: 0.17583370
Iteration 14/25 | Loss: 0.17583370
Iteration 15/25 | Loss: 0.17583370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.17583370208740234, 0.17583370208740234, 0.17583370208740234, 0.17583370208740234, 0.17583370208740234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17583370208740234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17583370
Iteration 2/1000 | Loss: 0.01834369
Iteration 3/1000 | Loss: 0.00144856
Iteration 4/1000 | Loss: 0.00082849
Iteration 5/1000 | Loss: 0.00042757
Iteration 6/1000 | Loss: 0.00025839
Iteration 7/1000 | Loss: 0.00015796
Iteration 8/1000 | Loss: 0.00013587
Iteration 9/1000 | Loss: 0.00011700
Iteration 10/1000 | Loss: 0.00009969
Iteration 11/1000 | Loss: 0.00008690
Iteration 12/1000 | Loss: 0.00007645
Iteration 13/1000 | Loss: 0.00006691
Iteration 14/1000 | Loss: 0.00005902
Iteration 15/1000 | Loss: 0.00005205
Iteration 16/1000 | Loss: 0.00004397
Iteration 17/1000 | Loss: 0.00003866
Iteration 18/1000 | Loss: 0.00003577
Iteration 19/1000 | Loss: 0.00003298
Iteration 20/1000 | Loss: 0.00003038
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00002773
Iteration 23/1000 | Loss: 0.00002702
Iteration 24/1000 | Loss: 0.00002650
Iteration 25/1000 | Loss: 0.00002605
Iteration 26/1000 | Loss: 0.00002553
Iteration 27/1000 | Loss: 0.00002513
Iteration 28/1000 | Loss: 0.00002473
Iteration 29/1000 | Loss: 0.00002442
Iteration 30/1000 | Loss: 0.00002440
Iteration 31/1000 | Loss: 0.00002417
Iteration 32/1000 | Loss: 0.00002411
Iteration 33/1000 | Loss: 0.00002397
Iteration 34/1000 | Loss: 0.00002388
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002381
Iteration 37/1000 | Loss: 0.00002378
Iteration 38/1000 | Loss: 0.00002376
Iteration 39/1000 | Loss: 0.00002375
Iteration 40/1000 | Loss: 0.00002375
Iteration 41/1000 | Loss: 0.00002375
Iteration 42/1000 | Loss: 0.00002371
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002370
Iteration 45/1000 | Loss: 0.00002370
Iteration 46/1000 | Loss: 0.00002369
Iteration 47/1000 | Loss: 0.00002367
Iteration 48/1000 | Loss: 0.00002365
Iteration 49/1000 | Loss: 0.00002365
Iteration 50/1000 | Loss: 0.00002365
Iteration 51/1000 | Loss: 0.00002365
Iteration 52/1000 | Loss: 0.00002365
Iteration 53/1000 | Loss: 0.00002365
Iteration 54/1000 | Loss: 0.00002365
Iteration 55/1000 | Loss: 0.00002364
Iteration 56/1000 | Loss: 0.00002364
Iteration 57/1000 | Loss: 0.00002362
Iteration 58/1000 | Loss: 0.00002361
Iteration 59/1000 | Loss: 0.00002360
Iteration 60/1000 | Loss: 0.00002360
Iteration 61/1000 | Loss: 0.00002359
Iteration 62/1000 | Loss: 0.00002357
Iteration 63/1000 | Loss: 0.00002357
Iteration 64/1000 | Loss: 0.00002357
Iteration 65/1000 | Loss: 0.00002356
Iteration 66/1000 | Loss: 0.00002356
Iteration 67/1000 | Loss: 0.00002356
Iteration 68/1000 | Loss: 0.00002356
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002355
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002354
Iteration 73/1000 | Loss: 0.00002354
Iteration 74/1000 | Loss: 0.00002354
Iteration 75/1000 | Loss: 0.00002354
Iteration 76/1000 | Loss: 0.00002353
Iteration 77/1000 | Loss: 0.00002353
Iteration 78/1000 | Loss: 0.00002353
Iteration 79/1000 | Loss: 0.00002353
Iteration 80/1000 | Loss: 0.00002353
Iteration 81/1000 | Loss: 0.00002353
Iteration 82/1000 | Loss: 0.00002353
Iteration 83/1000 | Loss: 0.00002353
Iteration 84/1000 | Loss: 0.00002353
Iteration 85/1000 | Loss: 0.00002353
Iteration 86/1000 | Loss: 0.00002353
Iteration 87/1000 | Loss: 0.00002352
Iteration 88/1000 | Loss: 0.00002352
Iteration 89/1000 | Loss: 0.00002352
Iteration 90/1000 | Loss: 0.00002352
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002352
Iteration 93/1000 | Loss: 0.00002352
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002347
Iteration 96/1000 | Loss: 0.00002346
Iteration 97/1000 | Loss: 0.00002346
Iteration 98/1000 | Loss: 0.00002346
Iteration 99/1000 | Loss: 0.00002346
Iteration 100/1000 | Loss: 0.00002346
Iteration 101/1000 | Loss: 0.00002346
Iteration 102/1000 | Loss: 0.00002346
Iteration 103/1000 | Loss: 0.00002346
Iteration 104/1000 | Loss: 0.00002346
Iteration 105/1000 | Loss: 0.00002346
Iteration 106/1000 | Loss: 0.00002346
Iteration 107/1000 | Loss: 0.00002345
Iteration 108/1000 | Loss: 0.00002345
Iteration 109/1000 | Loss: 0.00002345
Iteration 110/1000 | Loss: 0.00002345
Iteration 111/1000 | Loss: 0.00002344
Iteration 112/1000 | Loss: 0.00002344
Iteration 113/1000 | Loss: 0.00002344
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002344
Iteration 116/1000 | Loss: 0.00002343
Iteration 117/1000 | Loss: 0.00002343
Iteration 118/1000 | Loss: 0.00002343
Iteration 119/1000 | Loss: 0.00002342
Iteration 120/1000 | Loss: 0.00002342
Iteration 121/1000 | Loss: 0.00002342
Iteration 122/1000 | Loss: 0.00002341
Iteration 123/1000 | Loss: 0.00002341
Iteration 124/1000 | Loss: 0.00002341
Iteration 125/1000 | Loss: 0.00002340
Iteration 126/1000 | Loss: 0.00002340
Iteration 127/1000 | Loss: 0.00002340
Iteration 128/1000 | Loss: 0.00002340
Iteration 129/1000 | Loss: 0.00002340
Iteration 130/1000 | Loss: 0.00002340
Iteration 131/1000 | Loss: 0.00002340
Iteration 132/1000 | Loss: 0.00002340
Iteration 133/1000 | Loss: 0.00002340
Iteration 134/1000 | Loss: 0.00002339
Iteration 135/1000 | Loss: 0.00002339
Iteration 136/1000 | Loss: 0.00002339
Iteration 137/1000 | Loss: 0.00002339
Iteration 138/1000 | Loss: 0.00002339
Iteration 139/1000 | Loss: 0.00002339
Iteration 140/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.339472484891303e-05, 2.339472484891303e-05, 2.339472484891303e-05, 2.339472484891303e-05, 2.339472484891303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.339472484891303e-05

Optimization complete. Final v2v error: 4.211146354675293 mm

Highest mean error: 4.512381076812744 mm for frame 124

Lowest mean error: 3.570580244064331 mm for frame 38

Saving results

Total time: 72.33266139030457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852683
Iteration 2/25 | Loss: 0.00138369
Iteration 3/25 | Loss: 0.00131985
Iteration 4/25 | Loss: 0.00130368
Iteration 5/25 | Loss: 0.00129747
Iteration 6/25 | Loss: 0.00129539
Iteration 7/25 | Loss: 0.00129484
Iteration 8/25 | Loss: 0.00129484
Iteration 9/25 | Loss: 0.00129484
Iteration 10/25 | Loss: 0.00129484
Iteration 11/25 | Loss: 0.00129484
Iteration 12/25 | Loss: 0.00129484
Iteration 13/25 | Loss: 0.00129484
Iteration 14/25 | Loss: 0.00129484
Iteration 15/25 | Loss: 0.00129484
Iteration 16/25 | Loss: 0.00129484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012948415242135525, 0.0012948415242135525, 0.0012948415242135525, 0.0012948415242135525, 0.0012948415242135525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012948415242135525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18771207
Iteration 2/25 | Loss: 0.00324891
Iteration 3/25 | Loss: 0.00324890
Iteration 4/25 | Loss: 0.00324890
Iteration 5/25 | Loss: 0.00324890
Iteration 6/25 | Loss: 0.00324890
Iteration 7/25 | Loss: 0.00324890
Iteration 8/25 | Loss: 0.00324890
Iteration 9/25 | Loss: 0.00324890
Iteration 10/25 | Loss: 0.00324890
Iteration 11/25 | Loss: 0.00324890
Iteration 12/25 | Loss: 0.00324890
Iteration 13/25 | Loss: 0.00324890
Iteration 14/25 | Loss: 0.00324890
Iteration 15/25 | Loss: 0.00324890
Iteration 16/25 | Loss: 0.00324890
Iteration 17/25 | Loss: 0.00324890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0032488980796188116, 0.0032488980796188116, 0.0032488980796188116, 0.0032488980796188116, 0.0032488980796188116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032488980796188116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00324890
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00003017
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00002294
Iteration 6/1000 | Loss: 0.00002153
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00001971
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001872
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001817
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001783
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001776
Iteration 17/1000 | Loss: 0.00001765
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001745
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001732
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001720
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001703
Iteration 117/1000 | Loss: 0.00001703
Iteration 118/1000 | Loss: 0.00001703
Iteration 119/1000 | Loss: 0.00001703
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001702
Iteration 123/1000 | Loss: 0.00001702
Iteration 124/1000 | Loss: 0.00001702
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001701
Iteration 131/1000 | Loss: 0.00001701
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001701
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001700
Iteration 137/1000 | Loss: 0.00001700
Iteration 138/1000 | Loss: 0.00001700
Iteration 139/1000 | Loss: 0.00001700
Iteration 140/1000 | Loss: 0.00001700
Iteration 141/1000 | Loss: 0.00001700
Iteration 142/1000 | Loss: 0.00001700
Iteration 143/1000 | Loss: 0.00001700
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001699
Iteration 147/1000 | Loss: 0.00001699
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001697
Iteration 152/1000 | Loss: 0.00001697
Iteration 153/1000 | Loss: 0.00001697
Iteration 154/1000 | Loss: 0.00001697
Iteration 155/1000 | Loss: 0.00001697
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001696
Iteration 158/1000 | Loss: 0.00001696
Iteration 159/1000 | Loss: 0.00001696
Iteration 160/1000 | Loss: 0.00001696
Iteration 161/1000 | Loss: 0.00001696
Iteration 162/1000 | Loss: 0.00001695
Iteration 163/1000 | Loss: 0.00001695
Iteration 164/1000 | Loss: 0.00001695
Iteration 165/1000 | Loss: 0.00001695
Iteration 166/1000 | Loss: 0.00001695
Iteration 167/1000 | Loss: 0.00001695
Iteration 168/1000 | Loss: 0.00001695
Iteration 169/1000 | Loss: 0.00001694
Iteration 170/1000 | Loss: 0.00001694
Iteration 171/1000 | Loss: 0.00001694
Iteration 172/1000 | Loss: 0.00001694
Iteration 173/1000 | Loss: 0.00001694
Iteration 174/1000 | Loss: 0.00001694
Iteration 175/1000 | Loss: 0.00001694
Iteration 176/1000 | Loss: 0.00001694
Iteration 177/1000 | Loss: 0.00001693
Iteration 178/1000 | Loss: 0.00001693
Iteration 179/1000 | Loss: 0.00001693
Iteration 180/1000 | Loss: 0.00001693
Iteration 181/1000 | Loss: 0.00001693
Iteration 182/1000 | Loss: 0.00001693
Iteration 183/1000 | Loss: 0.00001693
Iteration 184/1000 | Loss: 0.00001693
Iteration 185/1000 | Loss: 0.00001692
Iteration 186/1000 | Loss: 0.00001692
Iteration 187/1000 | Loss: 0.00001692
Iteration 188/1000 | Loss: 0.00001692
Iteration 189/1000 | Loss: 0.00001692
Iteration 190/1000 | Loss: 0.00001692
Iteration 191/1000 | Loss: 0.00001692
Iteration 192/1000 | Loss: 0.00001692
Iteration 193/1000 | Loss: 0.00001692
Iteration 194/1000 | Loss: 0.00001692
Iteration 195/1000 | Loss: 0.00001692
Iteration 196/1000 | Loss: 0.00001692
Iteration 197/1000 | Loss: 0.00001692
Iteration 198/1000 | Loss: 0.00001691
Iteration 199/1000 | Loss: 0.00001691
Iteration 200/1000 | Loss: 0.00001691
Iteration 201/1000 | Loss: 0.00001691
Iteration 202/1000 | Loss: 0.00001691
Iteration 203/1000 | Loss: 0.00001691
Iteration 204/1000 | Loss: 0.00001691
Iteration 205/1000 | Loss: 0.00001691
Iteration 206/1000 | Loss: 0.00001691
Iteration 207/1000 | Loss: 0.00001691
Iteration 208/1000 | Loss: 0.00001691
Iteration 209/1000 | Loss: 0.00001691
Iteration 210/1000 | Loss: 0.00001691
Iteration 211/1000 | Loss: 0.00001691
Iteration 212/1000 | Loss: 0.00001691
Iteration 213/1000 | Loss: 0.00001691
Iteration 214/1000 | Loss: 0.00001690
Iteration 215/1000 | Loss: 0.00001690
Iteration 216/1000 | Loss: 0.00001690
Iteration 217/1000 | Loss: 0.00001690
Iteration 218/1000 | Loss: 0.00001690
Iteration 219/1000 | Loss: 0.00001690
Iteration 220/1000 | Loss: 0.00001690
Iteration 221/1000 | Loss: 0.00001690
Iteration 222/1000 | Loss: 0.00001690
Iteration 223/1000 | Loss: 0.00001690
Iteration 224/1000 | Loss: 0.00001690
Iteration 225/1000 | Loss: 0.00001690
Iteration 226/1000 | Loss: 0.00001690
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001690
Iteration 229/1000 | Loss: 0.00001690
Iteration 230/1000 | Loss: 0.00001690
Iteration 231/1000 | Loss: 0.00001690
Iteration 232/1000 | Loss: 0.00001690
Iteration 233/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.6898486137506552e-05, 1.6898486137506552e-05, 1.6898486137506552e-05, 1.6898486137506552e-05, 1.6898486137506552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6898486137506552e-05

Optimization complete. Final v2v error: 3.4894800186157227 mm

Highest mean error: 4.077478885650635 mm for frame 121

Lowest mean error: 2.8033533096313477 mm for frame 69

Saving results

Total time: 49.364219188690186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464644
Iteration 2/25 | Loss: 0.00146021
Iteration 3/25 | Loss: 0.00137858
Iteration 4/25 | Loss: 0.00136056
Iteration 5/25 | Loss: 0.00135484
Iteration 6/25 | Loss: 0.00135377
Iteration 7/25 | Loss: 0.00135377
Iteration 8/25 | Loss: 0.00135377
Iteration 9/25 | Loss: 0.00135377
Iteration 10/25 | Loss: 0.00135377
Iteration 11/25 | Loss: 0.00135377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013537727063521743, 0.0013537727063521743, 0.0013537727063521743, 0.0013537727063521743, 0.0013537727063521743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013537727063521743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66065550
Iteration 2/25 | Loss: 0.00222992
Iteration 3/25 | Loss: 0.00222992
Iteration 4/25 | Loss: 0.00222991
Iteration 5/25 | Loss: 0.00222991
Iteration 6/25 | Loss: 0.00222991
Iteration 7/25 | Loss: 0.00222991
Iteration 8/25 | Loss: 0.00222991
Iteration 9/25 | Loss: 0.00222991
Iteration 10/25 | Loss: 0.00222991
Iteration 11/25 | Loss: 0.00222991
Iteration 12/25 | Loss: 0.00222991
Iteration 13/25 | Loss: 0.00222991
Iteration 14/25 | Loss: 0.00222991
Iteration 15/25 | Loss: 0.00222991
Iteration 16/25 | Loss: 0.00222991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022299112752079964, 0.0022299112752079964, 0.0022299112752079964, 0.0022299112752079964, 0.0022299112752079964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022299112752079964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222991
Iteration 2/1000 | Loss: 0.00002770
Iteration 3/1000 | Loss: 0.00002349
Iteration 4/1000 | Loss: 0.00002231
Iteration 5/1000 | Loss: 0.00002139
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001904
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001819
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001803
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00001784
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001781
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001778
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001770
Iteration 28/1000 | Loss: 0.00001770
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001769
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001766
Iteration 34/1000 | Loss: 0.00001766
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001764
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001762
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001760
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001758
Iteration 49/1000 | Loss: 0.00001758
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001754
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001753
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001753
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001752
Iteration 74/1000 | Loss: 0.00001752
Iteration 75/1000 | Loss: 0.00001751
Iteration 76/1000 | Loss: 0.00001750
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00001750
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001749
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001743
Iteration 110/1000 | Loss: 0.00001743
Iteration 111/1000 | Loss: 0.00001743
Iteration 112/1000 | Loss: 0.00001743
Iteration 113/1000 | Loss: 0.00001743
Iteration 114/1000 | Loss: 0.00001743
Iteration 115/1000 | Loss: 0.00001743
Iteration 116/1000 | Loss: 0.00001743
Iteration 117/1000 | Loss: 0.00001743
Iteration 118/1000 | Loss: 0.00001743
Iteration 119/1000 | Loss: 0.00001743
Iteration 120/1000 | Loss: 0.00001742
Iteration 121/1000 | Loss: 0.00001742
Iteration 122/1000 | Loss: 0.00001742
Iteration 123/1000 | Loss: 0.00001742
Iteration 124/1000 | Loss: 0.00001742
Iteration 125/1000 | Loss: 0.00001742
Iteration 126/1000 | Loss: 0.00001742
Iteration 127/1000 | Loss: 0.00001742
Iteration 128/1000 | Loss: 0.00001742
Iteration 129/1000 | Loss: 0.00001742
Iteration 130/1000 | Loss: 0.00001741
Iteration 131/1000 | Loss: 0.00001741
Iteration 132/1000 | Loss: 0.00001741
Iteration 133/1000 | Loss: 0.00001741
Iteration 134/1000 | Loss: 0.00001741
Iteration 135/1000 | Loss: 0.00001741
Iteration 136/1000 | Loss: 0.00001741
Iteration 137/1000 | Loss: 0.00001741
Iteration 138/1000 | Loss: 0.00001741
Iteration 139/1000 | Loss: 0.00001741
Iteration 140/1000 | Loss: 0.00001741
Iteration 141/1000 | Loss: 0.00001741
Iteration 142/1000 | Loss: 0.00001741
Iteration 143/1000 | Loss: 0.00001741
Iteration 144/1000 | Loss: 0.00001741
Iteration 145/1000 | Loss: 0.00001741
Iteration 146/1000 | Loss: 0.00001741
Iteration 147/1000 | Loss: 0.00001741
Iteration 148/1000 | Loss: 0.00001741
Iteration 149/1000 | Loss: 0.00001741
Iteration 150/1000 | Loss: 0.00001741
Iteration 151/1000 | Loss: 0.00001741
Iteration 152/1000 | Loss: 0.00001741
Iteration 153/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.7414162357454188e-05, 1.7414162357454188e-05, 1.7414162357454188e-05, 1.7414162357454188e-05, 1.7414162357454188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7414162357454188e-05

Optimization complete. Final v2v error: 3.5828537940979004 mm

Highest mean error: 4.097130298614502 mm for frame 229

Lowest mean error: 3.240161895751953 mm for frame 253

Saving results

Total time: 47.50019598007202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037431
Iteration 2/25 | Loss: 0.00641889
Iteration 3/25 | Loss: 0.00472840
Iteration 4/25 | Loss: 0.00248840
Iteration 5/25 | Loss: 0.00223317
Iteration 6/25 | Loss: 0.00270081
Iteration 7/25 | Loss: 0.00227949
Iteration 8/25 | Loss: 0.00183539
Iteration 9/25 | Loss: 0.00170749
Iteration 10/25 | Loss: 0.00165777
Iteration 11/25 | Loss: 0.00163658
Iteration 12/25 | Loss: 0.00164046
Iteration 13/25 | Loss: 0.00161401
Iteration 14/25 | Loss: 0.00161460
Iteration 15/25 | Loss: 0.00159403
Iteration 16/25 | Loss: 0.00158560
Iteration 17/25 | Loss: 0.00158155
Iteration 18/25 | Loss: 0.00160308
Iteration 19/25 | Loss: 0.00158090
Iteration 20/25 | Loss: 0.00157649
Iteration 21/25 | Loss: 0.00158153
Iteration 22/25 | Loss: 0.00158479
Iteration 23/25 | Loss: 0.00157716
Iteration 24/25 | Loss: 0.00157411
Iteration 25/25 | Loss: 0.00157541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40425986
Iteration 2/25 | Loss: 0.00148596
Iteration 3/25 | Loss: 0.00137166
Iteration 4/25 | Loss: 0.00137165
Iteration 5/25 | Loss: 0.00137165
Iteration 6/25 | Loss: 0.00137165
Iteration 7/25 | Loss: 0.00137165
Iteration 8/25 | Loss: 0.00137165
Iteration 9/25 | Loss: 0.00137165
Iteration 10/25 | Loss: 0.00137165
Iteration 11/25 | Loss: 0.00137165
Iteration 12/25 | Loss: 0.00137165
Iteration 13/25 | Loss: 0.00137165
Iteration 14/25 | Loss: 0.00137165
Iteration 15/25 | Loss: 0.00137165
Iteration 16/25 | Loss: 0.00137165
Iteration 17/25 | Loss: 0.00137165
Iteration 18/25 | Loss: 0.00137165
Iteration 19/25 | Loss: 0.00137165
Iteration 20/25 | Loss: 0.00137165
Iteration 21/25 | Loss: 0.00137165
Iteration 22/25 | Loss: 0.00137165
Iteration 23/25 | Loss: 0.00137165
Iteration 24/25 | Loss: 0.00137165
Iteration 25/25 | Loss: 0.00137165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137165
Iteration 2/1000 | Loss: 0.00019633
Iteration 3/1000 | Loss: 0.00027614
Iteration 4/1000 | Loss: 0.00012456
Iteration 5/1000 | Loss: 0.00004922
Iteration 6/1000 | Loss: 0.00004272
Iteration 7/1000 | Loss: 0.00003896
Iteration 8/1000 | Loss: 0.00003665
Iteration 9/1000 | Loss: 0.00003549
Iteration 10/1000 | Loss: 0.00003451
Iteration 11/1000 | Loss: 0.00003552
Iteration 12/1000 | Loss: 0.00003345
Iteration 13/1000 | Loss: 0.00003315
Iteration 14/1000 | Loss: 0.00003421
Iteration 15/1000 | Loss: 0.00003227
Iteration 16/1000 | Loss: 0.00003424
Iteration 17/1000 | Loss: 0.00003183
Iteration 18/1000 | Loss: 0.00003218
Iteration 19/1000 | Loss: 0.00003154
Iteration 20/1000 | Loss: 0.00003218
Iteration 21/1000 | Loss: 0.00003124
Iteration 22/1000 | Loss: 0.00003149
Iteration 23/1000 | Loss: 0.00003105
Iteration 24/1000 | Loss: 0.00003105
Iteration 25/1000 | Loss: 0.00003104
Iteration 26/1000 | Loss: 0.00003104
Iteration 27/1000 | Loss: 0.00003104
Iteration 28/1000 | Loss: 0.00003104
Iteration 29/1000 | Loss: 0.00003104
Iteration 30/1000 | Loss: 0.00003104
Iteration 31/1000 | Loss: 0.00003104
Iteration 32/1000 | Loss: 0.00003103
Iteration 33/1000 | Loss: 0.00003103
Iteration 34/1000 | Loss: 0.00003087
Iteration 35/1000 | Loss: 0.00003085
Iteration 36/1000 | Loss: 0.00003085
Iteration 37/1000 | Loss: 0.00003083
Iteration 38/1000 | Loss: 0.00003076
Iteration 39/1000 | Loss: 0.00003068
Iteration 40/1000 | Loss: 0.00003068
Iteration 41/1000 | Loss: 0.00003068
Iteration 42/1000 | Loss: 0.00003068
Iteration 43/1000 | Loss: 0.00003067
Iteration 44/1000 | Loss: 0.00003067
Iteration 45/1000 | Loss: 0.00003067
Iteration 46/1000 | Loss: 0.00003067
Iteration 47/1000 | Loss: 0.00003066
Iteration 48/1000 | Loss: 0.00003116
Iteration 49/1000 | Loss: 0.00003116
Iteration 50/1000 | Loss: 0.00003060
Iteration 51/1000 | Loss: 0.00003052
Iteration 52/1000 | Loss: 0.00003051
Iteration 53/1000 | Loss: 0.00003050
Iteration 54/1000 | Loss: 0.00003050
Iteration 55/1000 | Loss: 0.00003050
Iteration 56/1000 | Loss: 0.00003050
Iteration 57/1000 | Loss: 0.00003050
Iteration 58/1000 | Loss: 0.00003050
Iteration 59/1000 | Loss: 0.00003050
Iteration 60/1000 | Loss: 0.00003049
Iteration 61/1000 | Loss: 0.00003049
Iteration 62/1000 | Loss: 0.00003049
Iteration 63/1000 | Loss: 0.00003049
Iteration 64/1000 | Loss: 0.00003049
Iteration 65/1000 | Loss: 0.00003049
Iteration 66/1000 | Loss: 0.00003049
Iteration 67/1000 | Loss: 0.00003049
Iteration 68/1000 | Loss: 0.00003049
Iteration 69/1000 | Loss: 0.00003049
Iteration 70/1000 | Loss: 0.00003049
Iteration 71/1000 | Loss: 0.00003049
Iteration 72/1000 | Loss: 0.00003049
Iteration 73/1000 | Loss: 0.00003049
Iteration 74/1000 | Loss: 0.00003049
Iteration 75/1000 | Loss: 0.00003049
Iteration 76/1000 | Loss: 0.00003049
Iteration 77/1000 | Loss: 0.00003049
Iteration 78/1000 | Loss: 0.00003049
Iteration 79/1000 | Loss: 0.00003049
Iteration 80/1000 | Loss: 0.00003049
Iteration 81/1000 | Loss: 0.00003049
Iteration 82/1000 | Loss: 0.00003049
Iteration 83/1000 | Loss: 0.00003049
Iteration 84/1000 | Loss: 0.00003049
Iteration 85/1000 | Loss: 0.00003049
Iteration 86/1000 | Loss: 0.00003049
Iteration 87/1000 | Loss: 0.00003049
Iteration 88/1000 | Loss: 0.00003049
Iteration 89/1000 | Loss: 0.00003049
Iteration 90/1000 | Loss: 0.00003049
Iteration 91/1000 | Loss: 0.00003049
Iteration 92/1000 | Loss: 0.00003049
Iteration 93/1000 | Loss: 0.00003049
Iteration 94/1000 | Loss: 0.00003049
Iteration 95/1000 | Loss: 0.00003049
Iteration 96/1000 | Loss: 0.00003049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.0489838536595926e-05, 3.0489838536595926e-05, 3.0489838536595926e-05, 3.0489838536595926e-05, 3.0489838536595926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0489838536595926e-05

Optimization complete. Final v2v error: 4.621232509613037 mm

Highest mean error: 4.7469305992126465 mm for frame 54

Lowest mean error: 4.17172908782959 mm for frame 8

Saving results

Total time: 88.59611415863037
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956286
Iteration 2/25 | Loss: 0.00318360
Iteration 3/25 | Loss: 0.00258846
Iteration 4/25 | Loss: 0.00207982
Iteration 5/25 | Loss: 0.00205162
Iteration 6/25 | Loss: 0.00193168
Iteration 7/25 | Loss: 0.00176481
Iteration 8/25 | Loss: 0.00174891
Iteration 9/25 | Loss: 0.00171865
Iteration 10/25 | Loss: 0.00159526
Iteration 11/25 | Loss: 0.00155595
Iteration 12/25 | Loss: 0.00152198
Iteration 13/25 | Loss: 0.00150806
Iteration 14/25 | Loss: 0.00149862
Iteration 15/25 | Loss: 0.00147599
Iteration 16/25 | Loss: 0.00147231
Iteration 17/25 | Loss: 0.00146383
Iteration 18/25 | Loss: 0.00145835
Iteration 19/25 | Loss: 0.00145845
Iteration 20/25 | Loss: 0.00145812
Iteration 21/25 | Loss: 0.00145512
Iteration 22/25 | Loss: 0.00145439
Iteration 23/25 | Loss: 0.00145407
Iteration 24/25 | Loss: 0.00145390
Iteration 25/25 | Loss: 0.00145655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21829677
Iteration 2/25 | Loss: 0.00267655
Iteration 3/25 | Loss: 0.00201673
Iteration 4/25 | Loss: 0.00201671
Iteration 5/25 | Loss: 0.00201671
Iteration 6/25 | Loss: 0.00201671
Iteration 7/25 | Loss: 0.00201671
Iteration 8/25 | Loss: 0.00201671
Iteration 9/25 | Loss: 0.00201671
Iteration 10/25 | Loss: 0.00201671
Iteration 11/25 | Loss: 0.00201671
Iteration 12/25 | Loss: 0.00201671
Iteration 13/25 | Loss: 0.00201671
Iteration 14/25 | Loss: 0.00201671
Iteration 15/25 | Loss: 0.00201671
Iteration 16/25 | Loss: 0.00201671
Iteration 17/25 | Loss: 0.00201671
Iteration 18/25 | Loss: 0.00201671
Iteration 19/25 | Loss: 0.00201671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00201670965179801, 0.00201670965179801, 0.00201670965179801, 0.00201670965179801, 0.00201670965179801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00201670965179801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201671
Iteration 2/1000 | Loss: 0.00041041
Iteration 3/1000 | Loss: 0.00086044
Iteration 4/1000 | Loss: 0.00085133
Iteration 5/1000 | Loss: 0.00006518
Iteration 6/1000 | Loss: 0.00013637
Iteration 7/1000 | Loss: 0.00018352
Iteration 8/1000 | Loss: 0.00004982
Iteration 9/1000 | Loss: 0.00004699
Iteration 10/1000 | Loss: 0.00017926
Iteration 11/1000 | Loss: 0.00132707
Iteration 12/1000 | Loss: 0.00242391
Iteration 13/1000 | Loss: 0.00008116
Iteration 14/1000 | Loss: 0.00004941
Iteration 15/1000 | Loss: 0.00029008
Iteration 16/1000 | Loss: 0.00004249
Iteration 17/1000 | Loss: 0.00018174
Iteration 18/1000 | Loss: 0.00003966
Iteration 19/1000 | Loss: 0.00003831
Iteration 20/1000 | Loss: 0.00003756
Iteration 21/1000 | Loss: 0.00014306
Iteration 22/1000 | Loss: 0.00007518
Iteration 23/1000 | Loss: 0.00020802
Iteration 24/1000 | Loss: 0.00016599
Iteration 25/1000 | Loss: 0.00036346
Iteration 26/1000 | Loss: 0.00056969
Iteration 27/1000 | Loss: 0.00012669
Iteration 28/1000 | Loss: 0.00059655
Iteration 29/1000 | Loss: 0.00094034
Iteration 30/1000 | Loss: 0.00003978
Iteration 31/1000 | Loss: 0.00029269
Iteration 32/1000 | Loss: 0.00015240
Iteration 33/1000 | Loss: 0.00017045
Iteration 34/1000 | Loss: 0.00003457
Iteration 35/1000 | Loss: 0.00002971
Iteration 36/1000 | Loss: 0.00002777
Iteration 37/1000 | Loss: 0.00002677
Iteration 38/1000 | Loss: 0.00009784
Iteration 39/1000 | Loss: 0.00070484
Iteration 40/1000 | Loss: 0.00008928
Iteration 41/1000 | Loss: 0.00006951
Iteration 42/1000 | Loss: 0.00002873
Iteration 43/1000 | Loss: 0.00006211
Iteration 44/1000 | Loss: 0.00003041
Iteration 45/1000 | Loss: 0.00003537
Iteration 46/1000 | Loss: 0.00002425
Iteration 47/1000 | Loss: 0.00002287
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00014554
Iteration 51/1000 | Loss: 0.00002090
Iteration 52/1000 | Loss: 0.00002060
Iteration 53/1000 | Loss: 0.00002054
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00006625
Iteration 56/1000 | Loss: 0.00042601
Iteration 57/1000 | Loss: 0.00038421
Iteration 58/1000 | Loss: 0.00003452
Iteration 59/1000 | Loss: 0.00002522
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00015822
Iteration 62/1000 | Loss: 0.00042730
Iteration 63/1000 | Loss: 0.00021987
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00013744
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00025246
Iteration 71/1000 | Loss: 0.00001682
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001609
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001598
Iteration 80/1000 | Loss: 0.00001594
Iteration 81/1000 | Loss: 0.00001594
Iteration 82/1000 | Loss: 0.00001594
Iteration 83/1000 | Loss: 0.00001594
Iteration 84/1000 | Loss: 0.00001594
Iteration 85/1000 | Loss: 0.00001594
Iteration 86/1000 | Loss: 0.00001594
Iteration 87/1000 | Loss: 0.00001594
Iteration 88/1000 | Loss: 0.00001594
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001593
Iteration 93/1000 | Loss: 0.00001593
Iteration 94/1000 | Loss: 0.00001591
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001589
Iteration 97/1000 | Loss: 0.00001589
Iteration 98/1000 | Loss: 0.00001589
Iteration 99/1000 | Loss: 0.00001589
Iteration 100/1000 | Loss: 0.00001589
Iteration 101/1000 | Loss: 0.00001589
Iteration 102/1000 | Loss: 0.00001589
Iteration 103/1000 | Loss: 0.00001589
Iteration 104/1000 | Loss: 0.00001589
Iteration 105/1000 | Loss: 0.00001589
Iteration 106/1000 | Loss: 0.00001589
Iteration 107/1000 | Loss: 0.00001589
Iteration 108/1000 | Loss: 0.00001588
Iteration 109/1000 | Loss: 0.00001588
Iteration 110/1000 | Loss: 0.00001588
Iteration 111/1000 | Loss: 0.00001588
Iteration 112/1000 | Loss: 0.00001588
Iteration 113/1000 | Loss: 0.00001588
Iteration 114/1000 | Loss: 0.00001588
Iteration 115/1000 | Loss: 0.00001588
Iteration 116/1000 | Loss: 0.00001588
Iteration 117/1000 | Loss: 0.00001588
Iteration 118/1000 | Loss: 0.00001588
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001588
Iteration 121/1000 | Loss: 0.00001587
Iteration 122/1000 | Loss: 0.00001587
Iteration 123/1000 | Loss: 0.00001587
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001587
Iteration 127/1000 | Loss: 0.00001587
Iteration 128/1000 | Loss: 0.00001587
Iteration 129/1000 | Loss: 0.00001587
Iteration 130/1000 | Loss: 0.00001587
Iteration 131/1000 | Loss: 0.00001587
Iteration 132/1000 | Loss: 0.00001587
Iteration 133/1000 | Loss: 0.00001586
Iteration 134/1000 | Loss: 0.00001586
Iteration 135/1000 | Loss: 0.00001586
Iteration 136/1000 | Loss: 0.00001586
Iteration 137/1000 | Loss: 0.00001586
Iteration 138/1000 | Loss: 0.00001586
Iteration 139/1000 | Loss: 0.00001586
Iteration 140/1000 | Loss: 0.00001585
Iteration 141/1000 | Loss: 0.00001585
Iteration 142/1000 | Loss: 0.00001585
Iteration 143/1000 | Loss: 0.00001585
Iteration 144/1000 | Loss: 0.00001585
Iteration 145/1000 | Loss: 0.00001585
Iteration 146/1000 | Loss: 0.00001585
Iteration 147/1000 | Loss: 0.00001585
Iteration 148/1000 | Loss: 0.00001585
Iteration 149/1000 | Loss: 0.00001585
Iteration 150/1000 | Loss: 0.00001585
Iteration 151/1000 | Loss: 0.00001584
Iteration 152/1000 | Loss: 0.00001584
Iteration 153/1000 | Loss: 0.00001584
Iteration 154/1000 | Loss: 0.00001584
Iteration 155/1000 | Loss: 0.00001584
Iteration 156/1000 | Loss: 0.00001584
Iteration 157/1000 | Loss: 0.00001584
Iteration 158/1000 | Loss: 0.00001584
Iteration 159/1000 | Loss: 0.00001584
Iteration 160/1000 | Loss: 0.00001584
Iteration 161/1000 | Loss: 0.00001584
Iteration 162/1000 | Loss: 0.00001584
Iteration 163/1000 | Loss: 0.00001584
Iteration 164/1000 | Loss: 0.00001584
Iteration 165/1000 | Loss: 0.00001584
Iteration 166/1000 | Loss: 0.00001584
Iteration 167/1000 | Loss: 0.00001584
Iteration 168/1000 | Loss: 0.00001584
Iteration 169/1000 | Loss: 0.00001584
Iteration 170/1000 | Loss: 0.00001584
Iteration 171/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.5842722859815694e-05, 1.5842722859815694e-05, 1.5842722859815694e-05, 1.5842722859815694e-05, 1.5842722859815694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5842722859815694e-05

Optimization complete. Final v2v error: 3.280128002166748 mm

Highest mean error: 10.199094772338867 mm for frame 152

Lowest mean error: 2.970400333404541 mm for frame 151

Saving results

Total time: 177.305805683136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913050
Iteration 2/25 | Loss: 0.00176751
Iteration 3/25 | Loss: 0.00145367
Iteration 4/25 | Loss: 0.00143227
Iteration 5/25 | Loss: 0.00142822
Iteration 6/25 | Loss: 0.00142711
Iteration 7/25 | Loss: 0.00142711
Iteration 8/25 | Loss: 0.00142711
Iteration 9/25 | Loss: 0.00142711
Iteration 10/25 | Loss: 0.00142711
Iteration 11/25 | Loss: 0.00142711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001427113893441856, 0.001427113893441856, 0.001427113893441856, 0.001427113893441856, 0.001427113893441856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001427113893441856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90609056
Iteration 2/25 | Loss: 0.00183404
Iteration 3/25 | Loss: 0.00183403
Iteration 4/25 | Loss: 0.00183403
Iteration 5/25 | Loss: 0.00183403
Iteration 6/25 | Loss: 0.00183403
Iteration 7/25 | Loss: 0.00183403
Iteration 8/25 | Loss: 0.00183403
Iteration 9/25 | Loss: 0.00183403
Iteration 10/25 | Loss: 0.00183403
Iteration 11/25 | Loss: 0.00183403
Iteration 12/25 | Loss: 0.00183403
Iteration 13/25 | Loss: 0.00183403
Iteration 14/25 | Loss: 0.00183403
Iteration 15/25 | Loss: 0.00183403
Iteration 16/25 | Loss: 0.00183403
Iteration 17/25 | Loss: 0.00183403
Iteration 18/25 | Loss: 0.00183403
Iteration 19/25 | Loss: 0.00183403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018340284004807472, 0.0018340284004807472, 0.0018340284004807472, 0.0018340284004807472, 0.0018340284004807472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018340284004807472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183403
Iteration 2/1000 | Loss: 0.00004437
Iteration 3/1000 | Loss: 0.00003124
Iteration 4/1000 | Loss: 0.00002738
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002471
Iteration 7/1000 | Loss: 0.00002395
Iteration 8/1000 | Loss: 0.00002312
Iteration 9/1000 | Loss: 0.00002276
Iteration 10/1000 | Loss: 0.00002237
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002179
Iteration 13/1000 | Loss: 0.00002156
Iteration 14/1000 | Loss: 0.00002130
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002112
Iteration 17/1000 | Loss: 0.00002096
Iteration 18/1000 | Loss: 0.00002081
Iteration 19/1000 | Loss: 0.00002080
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002061
Iteration 23/1000 | Loss: 0.00002058
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002057
Iteration 26/1000 | Loss: 0.00002057
Iteration 27/1000 | Loss: 0.00002057
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00002057
Iteration 30/1000 | Loss: 0.00002054
Iteration 31/1000 | Loss: 0.00002053
Iteration 32/1000 | Loss: 0.00002051
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00002050
Iteration 35/1000 | Loss: 0.00002050
Iteration 36/1000 | Loss: 0.00002048
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002045
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002044
Iteration 47/1000 | Loss: 0.00002044
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002042
Iteration 54/1000 | Loss: 0.00002042
Iteration 55/1000 | Loss: 0.00002042
Iteration 56/1000 | Loss: 0.00002041
Iteration 57/1000 | Loss: 0.00002041
Iteration 58/1000 | Loss: 0.00002041
Iteration 59/1000 | Loss: 0.00002040
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002040
Iteration 62/1000 | Loss: 0.00002040
Iteration 63/1000 | Loss: 0.00002040
Iteration 64/1000 | Loss: 0.00002039
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002039
Iteration 68/1000 | Loss: 0.00002039
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002038
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002037
Iteration 73/1000 | Loss: 0.00002037
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002037
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002036
Iteration 80/1000 | Loss: 0.00002036
Iteration 81/1000 | Loss: 0.00002036
Iteration 82/1000 | Loss: 0.00002036
Iteration 83/1000 | Loss: 0.00002036
Iteration 84/1000 | Loss: 0.00002036
Iteration 85/1000 | Loss: 0.00002036
Iteration 86/1000 | Loss: 0.00002036
Iteration 87/1000 | Loss: 0.00002036
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002035
Iteration 90/1000 | Loss: 0.00002035
Iteration 91/1000 | Loss: 0.00002035
Iteration 92/1000 | Loss: 0.00002035
Iteration 93/1000 | Loss: 0.00002035
Iteration 94/1000 | Loss: 0.00002035
Iteration 95/1000 | Loss: 0.00002035
Iteration 96/1000 | Loss: 0.00002035
Iteration 97/1000 | Loss: 0.00002035
Iteration 98/1000 | Loss: 0.00002034
Iteration 99/1000 | Loss: 0.00002034
Iteration 100/1000 | Loss: 0.00002034
Iteration 101/1000 | Loss: 0.00002034
Iteration 102/1000 | Loss: 0.00002034
Iteration 103/1000 | Loss: 0.00002034
Iteration 104/1000 | Loss: 0.00002034
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002032
Iteration 123/1000 | Loss: 0.00002031
Iteration 124/1000 | Loss: 0.00002031
Iteration 125/1000 | Loss: 0.00002031
Iteration 126/1000 | Loss: 0.00002031
Iteration 127/1000 | Loss: 0.00002031
Iteration 128/1000 | Loss: 0.00002031
Iteration 129/1000 | Loss: 0.00002031
Iteration 130/1000 | Loss: 0.00002031
Iteration 131/1000 | Loss: 0.00002031
Iteration 132/1000 | Loss: 0.00002031
Iteration 133/1000 | Loss: 0.00002031
Iteration 134/1000 | Loss: 0.00002031
Iteration 135/1000 | Loss: 0.00002031
Iteration 136/1000 | Loss: 0.00002031
Iteration 137/1000 | Loss: 0.00002031
Iteration 138/1000 | Loss: 0.00002030
Iteration 139/1000 | Loss: 0.00002030
Iteration 140/1000 | Loss: 0.00002030
Iteration 141/1000 | Loss: 0.00002030
Iteration 142/1000 | Loss: 0.00002030
Iteration 143/1000 | Loss: 0.00002030
Iteration 144/1000 | Loss: 0.00002030
Iteration 145/1000 | Loss: 0.00002030
Iteration 146/1000 | Loss: 0.00002030
Iteration 147/1000 | Loss: 0.00002030
Iteration 148/1000 | Loss: 0.00002030
Iteration 149/1000 | Loss: 0.00002030
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002030
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002030
Iteration 157/1000 | Loss: 0.00002029
Iteration 158/1000 | Loss: 0.00002029
Iteration 159/1000 | Loss: 0.00002029
Iteration 160/1000 | Loss: 0.00002029
Iteration 161/1000 | Loss: 0.00002029
Iteration 162/1000 | Loss: 0.00002029
Iteration 163/1000 | Loss: 0.00002029
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002029
Iteration 166/1000 | Loss: 0.00002029
Iteration 167/1000 | Loss: 0.00002029
Iteration 168/1000 | Loss: 0.00002029
Iteration 169/1000 | Loss: 0.00002029
Iteration 170/1000 | Loss: 0.00002029
Iteration 171/1000 | Loss: 0.00002029
Iteration 172/1000 | Loss: 0.00002029
Iteration 173/1000 | Loss: 0.00002029
Iteration 174/1000 | Loss: 0.00002029
Iteration 175/1000 | Loss: 0.00002029
Iteration 176/1000 | Loss: 0.00002029
Iteration 177/1000 | Loss: 0.00002029
Iteration 178/1000 | Loss: 0.00002029
Iteration 179/1000 | Loss: 0.00002029
Iteration 180/1000 | Loss: 0.00002029
Iteration 181/1000 | Loss: 0.00002029
Iteration 182/1000 | Loss: 0.00002029
Iteration 183/1000 | Loss: 0.00002029
Iteration 184/1000 | Loss: 0.00002029
Iteration 185/1000 | Loss: 0.00002029
Iteration 186/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.029443930950947e-05, 2.029443930950947e-05, 2.029443930950947e-05, 2.029443930950947e-05, 2.029443930950947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029443930950947e-05

Optimization complete. Final v2v error: 3.808281898498535 mm

Highest mean error: 4.361023426055908 mm for frame 137

Lowest mean error: 3.0879404544830322 mm for frame 27

Saving results

Total time: 45.895575761795044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489145
Iteration 2/25 | Loss: 0.00142443
Iteration 3/25 | Loss: 0.00134297
Iteration 4/25 | Loss: 0.00133180
Iteration 5/25 | Loss: 0.00132871
Iteration 6/25 | Loss: 0.00132814
Iteration 7/25 | Loss: 0.00132814
Iteration 8/25 | Loss: 0.00132814
Iteration 9/25 | Loss: 0.00132814
Iteration 10/25 | Loss: 0.00132814
Iteration 11/25 | Loss: 0.00132814
Iteration 12/25 | Loss: 0.00132814
Iteration 13/25 | Loss: 0.00132814
Iteration 14/25 | Loss: 0.00132814
Iteration 15/25 | Loss: 0.00132814
Iteration 16/25 | Loss: 0.00132814
Iteration 17/25 | Loss: 0.00132814
Iteration 18/25 | Loss: 0.00132814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013281446881592274, 0.0013281446881592274, 0.0013281446881592274, 0.0013281446881592274, 0.0013281446881592274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013281446881592274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24854839
Iteration 2/25 | Loss: 0.00212896
Iteration 3/25 | Loss: 0.00212893
Iteration 4/25 | Loss: 0.00212893
Iteration 5/25 | Loss: 0.00212893
Iteration 6/25 | Loss: 0.00212893
Iteration 7/25 | Loss: 0.00212893
Iteration 8/25 | Loss: 0.00212893
Iteration 9/25 | Loss: 0.00212893
Iteration 10/25 | Loss: 0.00212893
Iteration 11/25 | Loss: 0.00212893
Iteration 12/25 | Loss: 0.00212893
Iteration 13/25 | Loss: 0.00212893
Iteration 14/25 | Loss: 0.00212893
Iteration 15/25 | Loss: 0.00212893
Iteration 16/25 | Loss: 0.00212893
Iteration 17/25 | Loss: 0.00212893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002128926105797291, 0.002128926105797291, 0.002128926105797291, 0.002128926105797291, 0.002128926105797291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002128926105797291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212893
Iteration 2/1000 | Loss: 0.00002632
Iteration 3/1000 | Loss: 0.00001786
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001271
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001167
Iteration 42/1000 | Loss: 0.00001166
Iteration 43/1000 | Loss: 0.00001166
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001164
Iteration 47/1000 | Loss: 0.00001163
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001151
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001148
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001144
Iteration 61/1000 | Loss: 0.00001144
Iteration 62/1000 | Loss: 0.00001143
Iteration 63/1000 | Loss: 0.00001143
Iteration 64/1000 | Loss: 0.00001142
Iteration 65/1000 | Loss: 0.00001142
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001138
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001130
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001129
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001128
Iteration 129/1000 | Loss: 0.00001128
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001127
Iteration 133/1000 | Loss: 0.00001127
Iteration 134/1000 | Loss: 0.00001127
Iteration 135/1000 | Loss: 0.00001127
Iteration 136/1000 | Loss: 0.00001126
Iteration 137/1000 | Loss: 0.00001126
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001126
Iteration 142/1000 | Loss: 0.00001126
Iteration 143/1000 | Loss: 0.00001126
Iteration 144/1000 | Loss: 0.00001126
Iteration 145/1000 | Loss: 0.00001126
Iteration 146/1000 | Loss: 0.00001126
Iteration 147/1000 | Loss: 0.00001126
Iteration 148/1000 | Loss: 0.00001126
Iteration 149/1000 | Loss: 0.00001126
Iteration 150/1000 | Loss: 0.00001125
Iteration 151/1000 | Loss: 0.00001125
Iteration 152/1000 | Loss: 0.00001125
Iteration 153/1000 | Loss: 0.00001125
Iteration 154/1000 | Loss: 0.00001124
Iteration 155/1000 | Loss: 0.00001124
Iteration 156/1000 | Loss: 0.00001124
Iteration 157/1000 | Loss: 0.00001124
Iteration 158/1000 | Loss: 0.00001124
Iteration 159/1000 | Loss: 0.00001124
Iteration 160/1000 | Loss: 0.00001124
Iteration 161/1000 | Loss: 0.00001124
Iteration 162/1000 | Loss: 0.00001124
Iteration 163/1000 | Loss: 0.00001124
Iteration 164/1000 | Loss: 0.00001124
Iteration 165/1000 | Loss: 0.00001124
Iteration 166/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1237758371862583e-05, 1.1237758371862583e-05, 1.1237758371862583e-05, 1.1237758371862583e-05, 1.1237758371862583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1237758371862583e-05

Optimization complete. Final v2v error: 2.870260238647461 mm

Highest mean error: 3.2334036827087402 mm for frame 41

Lowest mean error: 2.5365540981292725 mm for frame 13

Saving results

Total time: 44.85664415359497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403439
Iteration 2/25 | Loss: 0.00138528
Iteration 3/25 | Loss: 0.00131818
Iteration 4/25 | Loss: 0.00130734
Iteration 5/25 | Loss: 0.00130359
Iteration 6/25 | Loss: 0.00130282
Iteration 7/25 | Loss: 0.00130282
Iteration 8/25 | Loss: 0.00130282
Iteration 9/25 | Loss: 0.00130282
Iteration 10/25 | Loss: 0.00130282
Iteration 11/25 | Loss: 0.00130282
Iteration 12/25 | Loss: 0.00130282
Iteration 13/25 | Loss: 0.00130282
Iteration 14/25 | Loss: 0.00130282
Iteration 15/25 | Loss: 0.00130282
Iteration 16/25 | Loss: 0.00130282
Iteration 17/25 | Loss: 0.00130282
Iteration 18/25 | Loss: 0.00130282
Iteration 19/25 | Loss: 0.00130282
Iteration 20/25 | Loss: 0.00130282
Iteration 21/25 | Loss: 0.00130282
Iteration 22/25 | Loss: 0.00130282
Iteration 23/25 | Loss: 0.00130282
Iteration 24/25 | Loss: 0.00130282
Iteration 25/25 | Loss: 0.00130282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77200687
Iteration 2/25 | Loss: 0.00217743
Iteration 3/25 | Loss: 0.00217743
Iteration 4/25 | Loss: 0.00217743
Iteration 5/25 | Loss: 0.00217743
Iteration 6/25 | Loss: 0.00217743
Iteration 7/25 | Loss: 0.00217743
Iteration 8/25 | Loss: 0.00217743
Iteration 9/25 | Loss: 0.00217743
Iteration 10/25 | Loss: 0.00217743
Iteration 11/25 | Loss: 0.00217743
Iteration 12/25 | Loss: 0.00217743
Iteration 13/25 | Loss: 0.00217743
Iteration 14/25 | Loss: 0.00217743
Iteration 15/25 | Loss: 0.00217743
Iteration 16/25 | Loss: 0.00217743
Iteration 17/25 | Loss: 0.00217743
Iteration 18/25 | Loss: 0.00217743
Iteration 19/25 | Loss: 0.00217743
Iteration 20/25 | Loss: 0.00217743
Iteration 21/25 | Loss: 0.00217743
Iteration 22/25 | Loss: 0.00217743
Iteration 23/25 | Loss: 0.00217743
Iteration 24/25 | Loss: 0.00217743
Iteration 25/25 | Loss: 0.00217743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217743
Iteration 2/1000 | Loss: 0.00002828
Iteration 3/1000 | Loss: 0.00002019
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001419
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001113
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001111
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001106
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001097
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001094
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001093
Iteration 61/1000 | Loss: 0.00001093
Iteration 62/1000 | Loss: 0.00001093
Iteration 63/1000 | Loss: 0.00001092
Iteration 64/1000 | Loss: 0.00001092
Iteration 65/1000 | Loss: 0.00001091
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001090
Iteration 70/1000 | Loss: 0.00001090
Iteration 71/1000 | Loss: 0.00001090
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001088
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001087
Iteration 78/1000 | Loss: 0.00001087
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001085
Iteration 84/1000 | Loss: 0.00001085
Iteration 85/1000 | Loss: 0.00001085
Iteration 86/1000 | Loss: 0.00001085
Iteration 87/1000 | Loss: 0.00001085
Iteration 88/1000 | Loss: 0.00001084
Iteration 89/1000 | Loss: 0.00001084
Iteration 90/1000 | Loss: 0.00001084
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001083
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001075
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001073
Iteration 135/1000 | Loss: 0.00001073
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001071
Iteration 142/1000 | Loss: 0.00001071
Iteration 143/1000 | Loss: 0.00001071
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001071
Iteration 147/1000 | Loss: 0.00001070
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001069
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001068
Iteration 158/1000 | Loss: 0.00001068
Iteration 159/1000 | Loss: 0.00001068
Iteration 160/1000 | Loss: 0.00001068
Iteration 161/1000 | Loss: 0.00001068
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001068
Iteration 164/1000 | Loss: 0.00001068
Iteration 165/1000 | Loss: 0.00001068
Iteration 166/1000 | Loss: 0.00001068
Iteration 167/1000 | Loss: 0.00001068
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001068
Iteration 171/1000 | Loss: 0.00001068
Iteration 172/1000 | Loss: 0.00001068
Iteration 173/1000 | Loss: 0.00001068
Iteration 174/1000 | Loss: 0.00001068
Iteration 175/1000 | Loss: 0.00001068
Iteration 176/1000 | Loss: 0.00001068
Iteration 177/1000 | Loss: 0.00001068
Iteration 178/1000 | Loss: 0.00001068
Iteration 179/1000 | Loss: 0.00001068
Iteration 180/1000 | Loss: 0.00001068
Iteration 181/1000 | Loss: 0.00001068
Iteration 182/1000 | Loss: 0.00001068
Iteration 183/1000 | Loss: 0.00001068
Iteration 184/1000 | Loss: 0.00001068
Iteration 185/1000 | Loss: 0.00001068
Iteration 186/1000 | Loss: 0.00001068
Iteration 187/1000 | Loss: 0.00001068
Iteration 188/1000 | Loss: 0.00001068
Iteration 189/1000 | Loss: 0.00001068
Iteration 190/1000 | Loss: 0.00001068
Iteration 191/1000 | Loss: 0.00001068
Iteration 192/1000 | Loss: 0.00001068
Iteration 193/1000 | Loss: 0.00001068
Iteration 194/1000 | Loss: 0.00001068
Iteration 195/1000 | Loss: 0.00001068
Iteration 196/1000 | Loss: 0.00001068
Iteration 197/1000 | Loss: 0.00001068
Iteration 198/1000 | Loss: 0.00001068
Iteration 199/1000 | Loss: 0.00001068
Iteration 200/1000 | Loss: 0.00001068
Iteration 201/1000 | Loss: 0.00001068
Iteration 202/1000 | Loss: 0.00001068
Iteration 203/1000 | Loss: 0.00001068
Iteration 204/1000 | Loss: 0.00001068
Iteration 205/1000 | Loss: 0.00001068
Iteration 206/1000 | Loss: 0.00001068
Iteration 207/1000 | Loss: 0.00001068
Iteration 208/1000 | Loss: 0.00001068
Iteration 209/1000 | Loss: 0.00001068
Iteration 210/1000 | Loss: 0.00001068
Iteration 211/1000 | Loss: 0.00001068
Iteration 212/1000 | Loss: 0.00001068
Iteration 213/1000 | Loss: 0.00001068
Iteration 214/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.0680154446163215e-05, 1.0680154446163215e-05, 1.0680154446163215e-05, 1.0680154446163215e-05, 1.0680154446163215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0680154446163215e-05

Optimization complete. Final v2v error: 2.839014768600464 mm

Highest mean error: 4.123008728027344 mm for frame 78

Lowest mean error: 2.6370999813079834 mm for frame 99

Saving results

Total time: 46.094725608825684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017038
Iteration 2/25 | Loss: 0.00175672
Iteration 3/25 | Loss: 0.00152262
Iteration 4/25 | Loss: 0.00153067
Iteration 5/25 | Loss: 0.00140654
Iteration 6/25 | Loss: 0.00142812
Iteration 7/25 | Loss: 0.00144324
Iteration 8/25 | Loss: 0.00140798
Iteration 9/25 | Loss: 0.00137348
Iteration 10/25 | Loss: 0.00138770
Iteration 11/25 | Loss: 0.00135460
Iteration 12/25 | Loss: 0.00134191
Iteration 13/25 | Loss: 0.00133640
Iteration 14/25 | Loss: 0.00133560
Iteration 15/25 | Loss: 0.00134079
Iteration 16/25 | Loss: 0.00134112
Iteration 17/25 | Loss: 0.00133831
Iteration 18/25 | Loss: 0.00133483
Iteration 19/25 | Loss: 0.00133425
Iteration 20/25 | Loss: 0.00133340
Iteration 21/25 | Loss: 0.00132950
Iteration 22/25 | Loss: 0.00132837
Iteration 23/25 | Loss: 0.00132812
Iteration 24/25 | Loss: 0.00132806
Iteration 25/25 | Loss: 0.00132805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42236316
Iteration 2/25 | Loss: 0.00226375
Iteration 3/25 | Loss: 0.00226375
Iteration 4/25 | Loss: 0.00226375
Iteration 5/25 | Loss: 0.00226374
Iteration 6/25 | Loss: 0.00226374
Iteration 7/25 | Loss: 0.00226374
Iteration 8/25 | Loss: 0.00226374
Iteration 9/25 | Loss: 0.00226374
Iteration 10/25 | Loss: 0.00226374
Iteration 11/25 | Loss: 0.00226374
Iteration 12/25 | Loss: 0.00226374
Iteration 13/25 | Loss: 0.00226374
Iteration 14/25 | Loss: 0.00226374
Iteration 15/25 | Loss: 0.00226374
Iteration 16/25 | Loss: 0.00226374
Iteration 17/25 | Loss: 0.00226374
Iteration 18/25 | Loss: 0.00226374
Iteration 19/25 | Loss: 0.00226374
Iteration 20/25 | Loss: 0.00226374
Iteration 21/25 | Loss: 0.00226374
Iteration 22/25 | Loss: 0.00226374
Iteration 23/25 | Loss: 0.00226374
Iteration 24/25 | Loss: 0.00226374
Iteration 25/25 | Loss: 0.00226374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0022637422662228346, 0.0022637422662228346, 0.0022637422662228346, 0.0022637422662228346, 0.0022637422662228346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022637422662228346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226374
Iteration 2/1000 | Loss: 0.00002272
Iteration 3/1000 | Loss: 0.00028221
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00002661
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001300
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001172
Iteration 29/1000 | Loss: 0.00001171
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001147
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001141
Iteration 70/1000 | Loss: 0.00001141
Iteration 71/1000 | Loss: 0.00001141
Iteration 72/1000 | Loss: 0.00001141
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001141
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001141
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.1413154425099492e-05, 1.1413154425099492e-05, 1.1413154425099492e-05, 1.1413154425099492e-05, 1.1413154425099492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1413154425099492e-05

Optimization complete. Final v2v error: 2.9051473140716553 mm

Highest mean error: 3.304857015609741 mm for frame 61

Lowest mean error: 2.6747071743011475 mm for frame 136

Saving results

Total time: 69.85751843452454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992892
Iteration 2/25 | Loss: 0.00992892
Iteration 3/25 | Loss: 0.00992892
Iteration 4/25 | Loss: 0.00992891
Iteration 5/25 | Loss: 0.00992891
Iteration 6/25 | Loss: 0.00992891
Iteration 7/25 | Loss: 0.00992891
Iteration 8/25 | Loss: 0.00992891
Iteration 9/25 | Loss: 0.00992891
Iteration 10/25 | Loss: 0.00992891
Iteration 11/25 | Loss: 0.00992891
Iteration 12/25 | Loss: 0.00992891
Iteration 13/25 | Loss: 0.00992891
Iteration 14/25 | Loss: 0.00992891
Iteration 15/25 | Loss: 0.00992891
Iteration 16/25 | Loss: 0.00992891
Iteration 17/25 | Loss: 0.00992891
Iteration 18/25 | Loss: 0.00992891
Iteration 19/25 | Loss: 0.00992891
Iteration 20/25 | Loss: 0.00992891
Iteration 21/25 | Loss: 0.00992890
Iteration 22/25 | Loss: 0.00992890
Iteration 23/25 | Loss: 0.00992890
Iteration 24/25 | Loss: 0.00992890
Iteration 25/25 | Loss: 0.00992890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45586228
Iteration 2/25 | Loss: 0.12191722
Iteration 3/25 | Loss: 0.12143683
Iteration 4/25 | Loss: 0.12072312
Iteration 5/25 | Loss: 0.12061859
Iteration 6/25 | Loss: 0.12061854
Iteration 7/25 | Loss: 0.12061854
Iteration 8/25 | Loss: 0.12061854
Iteration 9/25 | Loss: 0.12061851
Iteration 10/25 | Loss: 0.12061851
Iteration 11/25 | Loss: 0.12061852
Iteration 12/25 | Loss: 0.12061852
Iteration 13/25 | Loss: 0.12061851
Iteration 14/25 | Loss: 0.12061851
Iteration 15/25 | Loss: 0.12061851
Iteration 16/25 | Loss: 0.12061851
Iteration 17/25 | Loss: 0.12061851
Iteration 18/25 | Loss: 0.12061851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.12061851471662521, 0.12061851471662521, 0.12061851471662521, 0.12061851471662521, 0.12061851471662521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12061851471662521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12061851
Iteration 2/1000 | Loss: 0.00347708
Iteration 3/1000 | Loss: 0.00342978
Iteration 4/1000 | Loss: 0.00065462
Iteration 5/1000 | Loss: 0.00032359
Iteration 6/1000 | Loss: 0.00086814
Iteration 7/1000 | Loss: 0.00029428
Iteration 8/1000 | Loss: 0.00007344
Iteration 9/1000 | Loss: 0.00005787
Iteration 10/1000 | Loss: 0.00005212
Iteration 11/1000 | Loss: 0.00070089
Iteration 12/1000 | Loss: 0.00004961
Iteration 13/1000 | Loss: 0.00015583
Iteration 14/1000 | Loss: 0.00047830
Iteration 15/1000 | Loss: 0.00003062
Iteration 16/1000 | Loss: 0.00002821
Iteration 17/1000 | Loss: 0.00010830
Iteration 18/1000 | Loss: 0.00006603
Iteration 19/1000 | Loss: 0.00002450
Iteration 20/1000 | Loss: 0.00013210
Iteration 21/1000 | Loss: 0.00004354
Iteration 22/1000 | Loss: 0.00016083
Iteration 23/1000 | Loss: 0.00003933
Iteration 24/1000 | Loss: 0.00026317
Iteration 25/1000 | Loss: 0.00004966
Iteration 26/1000 | Loss: 0.00015205
Iteration 27/1000 | Loss: 0.00009751
Iteration 28/1000 | Loss: 0.00023457
Iteration 29/1000 | Loss: 0.00003217
Iteration 30/1000 | Loss: 0.00002240
Iteration 31/1000 | Loss: 0.00009596
Iteration 32/1000 | Loss: 0.00028856
Iteration 33/1000 | Loss: 0.00121953
Iteration 34/1000 | Loss: 0.00161394
Iteration 35/1000 | Loss: 0.00114027
Iteration 36/1000 | Loss: 0.00025187
Iteration 37/1000 | Loss: 0.00015662
Iteration 38/1000 | Loss: 0.00007520
Iteration 39/1000 | Loss: 0.00003934
Iteration 40/1000 | Loss: 0.00007862
Iteration 41/1000 | Loss: 0.00006448
Iteration 42/1000 | Loss: 0.00002609
Iteration 43/1000 | Loss: 0.00005148
Iteration 44/1000 | Loss: 0.00005984
Iteration 45/1000 | Loss: 0.00046517
Iteration 46/1000 | Loss: 0.00002481
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002544
Iteration 50/1000 | Loss: 0.00002708
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00003508
Iteration 53/1000 | Loss: 0.00002527
Iteration 54/1000 | Loss: 0.00004534
Iteration 55/1000 | Loss: 0.00012482
Iteration 56/1000 | Loss: 0.00002657
Iteration 57/1000 | Loss: 0.00003126
Iteration 58/1000 | Loss: 0.00003548
Iteration 59/1000 | Loss: 0.00009790
Iteration 60/1000 | Loss: 0.00011524
Iteration 61/1000 | Loss: 0.00003438
Iteration 62/1000 | Loss: 0.00006353
Iteration 63/1000 | Loss: 0.00002765
Iteration 64/1000 | Loss: 0.00001801
Iteration 65/1000 | Loss: 0.00007327
Iteration 66/1000 | Loss: 0.00002952
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00002546
Iteration 69/1000 | Loss: 0.00007747
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00002756
Iteration 73/1000 | Loss: 0.00002210
Iteration 74/1000 | Loss: 0.00002252
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001857
Iteration 78/1000 | Loss: 0.00011310
Iteration 79/1000 | Loss: 0.00004241
Iteration 80/1000 | Loss: 0.00002549
Iteration 81/1000 | Loss: 0.00002010
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00006000
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00002357
Iteration 87/1000 | Loss: 0.00012687
Iteration 88/1000 | Loss: 0.00002276
Iteration 89/1000 | Loss: 0.00006895
Iteration 90/1000 | Loss: 0.00002960
Iteration 91/1000 | Loss: 0.00003546
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00002016
Iteration 96/1000 | Loss: 0.00001573
Iteration 97/1000 | Loss: 0.00001817
Iteration 98/1000 | Loss: 0.00003581
Iteration 99/1000 | Loss: 0.00007415
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00003435
Iteration 102/1000 | Loss: 0.00002087
Iteration 103/1000 | Loss: 0.00003285
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00002176
Iteration 106/1000 | Loss: 0.00001590
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00005435
Iteration 109/1000 | Loss: 0.00002537
Iteration 110/1000 | Loss: 0.00014579
Iteration 111/1000 | Loss: 0.00003924
Iteration 112/1000 | Loss: 0.00002463
Iteration 113/1000 | Loss: 0.00002512
Iteration 114/1000 | Loss: 0.00005480
Iteration 115/1000 | Loss: 0.00004352
Iteration 116/1000 | Loss: 0.00001536
Iteration 117/1000 | Loss: 0.00003856
Iteration 118/1000 | Loss: 0.00001992
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00002674
Iteration 123/1000 | Loss: 0.00008850
Iteration 124/1000 | Loss: 0.00004427
Iteration 125/1000 | Loss: 0.00007319
Iteration 126/1000 | Loss: 0.00002387
Iteration 127/1000 | Loss: 0.00007544
Iteration 128/1000 | Loss: 0.00001743
Iteration 129/1000 | Loss: 0.00001646
Iteration 130/1000 | Loss: 0.00001645
Iteration 131/1000 | Loss: 0.00003044
Iteration 132/1000 | Loss: 0.00001631
Iteration 133/1000 | Loss: 0.00001629
Iteration 134/1000 | Loss: 0.00002049
Iteration 135/1000 | Loss: 0.00003253
Iteration 136/1000 | Loss: 0.00001711
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001633
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001594
Iteration 141/1000 | Loss: 0.00001529
Iteration 142/1000 | Loss: 0.00001528
Iteration 143/1000 | Loss: 0.00001528
Iteration 144/1000 | Loss: 0.00001528
Iteration 145/1000 | Loss: 0.00001528
Iteration 146/1000 | Loss: 0.00001528
Iteration 147/1000 | Loss: 0.00001528
Iteration 148/1000 | Loss: 0.00001528
Iteration 149/1000 | Loss: 0.00001528
Iteration 150/1000 | Loss: 0.00001528
Iteration 151/1000 | Loss: 0.00001528
Iteration 152/1000 | Loss: 0.00001528
Iteration 153/1000 | Loss: 0.00001528
Iteration 154/1000 | Loss: 0.00001528
Iteration 155/1000 | Loss: 0.00001528
Iteration 156/1000 | Loss: 0.00001527
Iteration 157/1000 | Loss: 0.00001527
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001527
Iteration 161/1000 | Loss: 0.00001527
Iteration 162/1000 | Loss: 0.00001527
Iteration 163/1000 | Loss: 0.00001527
Iteration 164/1000 | Loss: 0.00001636
Iteration 165/1000 | Loss: 0.00001562
Iteration 166/1000 | Loss: 0.00001527
Iteration 167/1000 | Loss: 0.00001559
Iteration 168/1000 | Loss: 0.00001604
Iteration 169/1000 | Loss: 0.00001525
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001524
Iteration 174/1000 | Loss: 0.00001524
Iteration 175/1000 | Loss: 0.00001524
Iteration 176/1000 | Loss: 0.00001524
Iteration 177/1000 | Loss: 0.00001524
Iteration 178/1000 | Loss: 0.00001524
Iteration 179/1000 | Loss: 0.00001524
Iteration 180/1000 | Loss: 0.00001524
Iteration 181/1000 | Loss: 0.00001524
Iteration 182/1000 | Loss: 0.00001524
Iteration 183/1000 | Loss: 0.00001524
Iteration 184/1000 | Loss: 0.00001524
Iteration 185/1000 | Loss: 0.00001524
Iteration 186/1000 | Loss: 0.00001524
Iteration 187/1000 | Loss: 0.00001524
Iteration 188/1000 | Loss: 0.00001524
Iteration 189/1000 | Loss: 0.00001524
Iteration 190/1000 | Loss: 0.00001524
Iteration 191/1000 | Loss: 0.00001524
Iteration 192/1000 | Loss: 0.00001524
Iteration 193/1000 | Loss: 0.00001524
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001524
Iteration 196/1000 | Loss: 0.00001524
Iteration 197/1000 | Loss: 0.00001524
Iteration 198/1000 | Loss: 0.00001524
Iteration 199/1000 | Loss: 0.00001524
Iteration 200/1000 | Loss: 0.00001524
Iteration 201/1000 | Loss: 0.00001524
Iteration 202/1000 | Loss: 0.00001524
Iteration 203/1000 | Loss: 0.00001524
Iteration 204/1000 | Loss: 0.00001524
Iteration 205/1000 | Loss: 0.00001524
Iteration 206/1000 | Loss: 0.00001524
Iteration 207/1000 | Loss: 0.00001524
Iteration 208/1000 | Loss: 0.00001524
Iteration 209/1000 | Loss: 0.00001524
Iteration 210/1000 | Loss: 0.00001524
Iteration 211/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.5243866982928012e-05, 1.5243866982928012e-05, 1.5243866982928012e-05, 1.5243866982928012e-05, 1.5243866982928012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5243866982928012e-05

Optimization complete. Final v2v error: 3.329195976257324 mm

Highest mean error: 3.5564804077148438 mm for frame 233

Lowest mean error: 3.1984472274780273 mm for frame 226

Saving results

Total time: 214.06127262115479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759888
Iteration 2/25 | Loss: 0.00148498
Iteration 3/25 | Loss: 0.00136610
Iteration 4/25 | Loss: 0.00135668
Iteration 5/25 | Loss: 0.00135508
Iteration 6/25 | Loss: 0.00135508
Iteration 7/25 | Loss: 0.00135508
Iteration 8/25 | Loss: 0.00135508
Iteration 9/25 | Loss: 0.00135508
Iteration 10/25 | Loss: 0.00135508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013550755102187395, 0.0013550755102187395, 0.0013550755102187395, 0.0013550755102187395, 0.0013550755102187395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013550755102187395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19078553
Iteration 2/25 | Loss: 0.00176547
Iteration 3/25 | Loss: 0.00176545
Iteration 4/25 | Loss: 0.00176545
Iteration 5/25 | Loss: 0.00176545
Iteration 6/25 | Loss: 0.00176545
Iteration 7/25 | Loss: 0.00176545
Iteration 8/25 | Loss: 0.00176545
Iteration 9/25 | Loss: 0.00176545
Iteration 10/25 | Loss: 0.00176545
Iteration 11/25 | Loss: 0.00176545
Iteration 12/25 | Loss: 0.00176545
Iteration 13/25 | Loss: 0.00176545
Iteration 14/25 | Loss: 0.00176545
Iteration 15/25 | Loss: 0.00176545
Iteration 16/25 | Loss: 0.00176545
Iteration 17/25 | Loss: 0.00176545
Iteration 18/25 | Loss: 0.00176545
Iteration 19/25 | Loss: 0.00176545
Iteration 20/25 | Loss: 0.00176545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017654465045779943, 0.0017654465045779943, 0.0017654465045779943, 0.0017654465045779943, 0.0017654465045779943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017654465045779943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176545
Iteration 2/1000 | Loss: 0.00002938
Iteration 3/1000 | Loss: 0.00002000
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001683
Iteration 6/1000 | Loss: 0.00001610
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001440
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001386
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001318
Iteration 19/1000 | Loss: 0.00001317
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001314
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001305
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001272
Iteration 43/1000 | Loss: 0.00001272
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001262
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001261
Iteration 54/1000 | Loss: 0.00001261
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001260
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001257
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001256
Iteration 75/1000 | Loss: 0.00001256
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001255
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001255
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001255
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.2551812687888741e-05, 1.2551812687888741e-05, 1.2551812687888741e-05, 1.2551812687888741e-05, 1.2551812687888741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2551812687888741e-05

Optimization complete. Final v2v error: 3.0175023078918457 mm

Highest mean error: 3.2957053184509277 mm for frame 109

Lowest mean error: 2.839383125305176 mm for frame 48

Saving results

Total time: 36.97551918029785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834036
Iteration 2/25 | Loss: 0.00152332
Iteration 3/25 | Loss: 0.00140062
Iteration 4/25 | Loss: 0.00133264
Iteration 5/25 | Loss: 0.00133613
Iteration 6/25 | Loss: 0.00132658
Iteration 7/25 | Loss: 0.00132863
Iteration 8/25 | Loss: 0.00132933
Iteration 9/25 | Loss: 0.00132623
Iteration 10/25 | Loss: 0.00132622
Iteration 11/25 | Loss: 0.00132622
Iteration 12/25 | Loss: 0.00132622
Iteration 13/25 | Loss: 0.00132622
Iteration 14/25 | Loss: 0.00132621
Iteration 15/25 | Loss: 0.00132621
Iteration 16/25 | Loss: 0.00132621
Iteration 17/25 | Loss: 0.00132621
Iteration 18/25 | Loss: 0.00132621
Iteration 19/25 | Loss: 0.00132849
Iteration 20/25 | Loss: 0.00132660
Iteration 21/25 | Loss: 0.00132620
Iteration 22/25 | Loss: 0.00132618
Iteration 23/25 | Loss: 0.00132618
Iteration 24/25 | Loss: 0.00132617
Iteration 25/25 | Loss: 0.00132617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.37263107
Iteration 2/25 | Loss: 0.00216957
Iteration 3/25 | Loss: 0.00216956
Iteration 4/25 | Loss: 0.00216956
Iteration 5/25 | Loss: 0.00216956
Iteration 6/25 | Loss: 0.00216956
Iteration 7/25 | Loss: 0.00216956
Iteration 8/25 | Loss: 0.00216956
Iteration 9/25 | Loss: 0.00216956
Iteration 10/25 | Loss: 0.00216956
Iteration 11/25 | Loss: 0.00216956
Iteration 12/25 | Loss: 0.00216956
Iteration 13/25 | Loss: 0.00216956
Iteration 14/25 | Loss: 0.00216956
Iteration 15/25 | Loss: 0.00216956
Iteration 16/25 | Loss: 0.00216956
Iteration 17/25 | Loss: 0.00216956
Iteration 18/25 | Loss: 0.00216956
Iteration 19/25 | Loss: 0.00216956
Iteration 20/25 | Loss: 0.00216956
Iteration 21/25 | Loss: 0.00216956
Iteration 22/25 | Loss: 0.00216956
Iteration 23/25 | Loss: 0.00216956
Iteration 24/25 | Loss: 0.00216956
Iteration 25/25 | Loss: 0.00216956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216956
Iteration 2/1000 | Loss: 0.00002048
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001315
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001232
Iteration 8/1000 | Loss: 0.00001189
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001111
Iteration 18/1000 | Loss: 0.00001104
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001085
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001080
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001075
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001074
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001070
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001069
Iteration 37/1000 | Loss: 0.00001069
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001068
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001065
Iteration 47/1000 | Loss: 0.00001065
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001064
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001061
Iteration 67/1000 | Loss: 0.00001061
Iteration 68/1000 | Loss: 0.00001061
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001061
Iteration 71/1000 | Loss: 0.00001061
Iteration 72/1000 | Loss: 0.00001061
Iteration 73/1000 | Loss: 0.00001060
Iteration 74/1000 | Loss: 0.00001060
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001060
Iteration 77/1000 | Loss: 0.00001060
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001060
Iteration 80/1000 | Loss: 0.00001060
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001057
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001056
Iteration 96/1000 | Loss: 0.00001056
Iteration 97/1000 | Loss: 0.00001056
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001055
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001054
Iteration 105/1000 | Loss: 0.00001054
Iteration 106/1000 | Loss: 0.00001054
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001053
Iteration 114/1000 | Loss: 0.00001053
Iteration 115/1000 | Loss: 0.00001053
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001052
Iteration 118/1000 | Loss: 0.00001052
Iteration 119/1000 | Loss: 0.00001051
Iteration 120/1000 | Loss: 0.00001051
Iteration 121/1000 | Loss: 0.00001051
Iteration 122/1000 | Loss: 0.00001051
Iteration 123/1000 | Loss: 0.00001051
Iteration 124/1000 | Loss: 0.00001050
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001049
Iteration 127/1000 | Loss: 0.00001049
Iteration 128/1000 | Loss: 0.00001049
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001048
Iteration 133/1000 | Loss: 0.00001048
Iteration 134/1000 | Loss: 0.00001048
Iteration 135/1000 | Loss: 0.00001048
Iteration 136/1000 | Loss: 0.00001048
Iteration 137/1000 | Loss: 0.00001048
Iteration 138/1000 | Loss: 0.00001048
Iteration 139/1000 | Loss: 0.00001048
Iteration 140/1000 | Loss: 0.00001047
Iteration 141/1000 | Loss: 0.00001047
Iteration 142/1000 | Loss: 0.00001047
Iteration 143/1000 | Loss: 0.00001047
Iteration 144/1000 | Loss: 0.00001047
Iteration 145/1000 | Loss: 0.00001047
Iteration 146/1000 | Loss: 0.00001047
Iteration 147/1000 | Loss: 0.00001047
Iteration 148/1000 | Loss: 0.00001047
Iteration 149/1000 | Loss: 0.00001047
Iteration 150/1000 | Loss: 0.00001046
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001046
Iteration 153/1000 | Loss: 0.00001046
Iteration 154/1000 | Loss: 0.00001046
Iteration 155/1000 | Loss: 0.00001046
Iteration 156/1000 | Loss: 0.00001046
Iteration 157/1000 | Loss: 0.00001046
Iteration 158/1000 | Loss: 0.00001046
Iteration 159/1000 | Loss: 0.00001046
Iteration 160/1000 | Loss: 0.00001046
Iteration 161/1000 | Loss: 0.00001046
Iteration 162/1000 | Loss: 0.00001046
Iteration 163/1000 | Loss: 0.00001046
Iteration 164/1000 | Loss: 0.00001046
Iteration 165/1000 | Loss: 0.00001046
Iteration 166/1000 | Loss: 0.00001045
Iteration 167/1000 | Loss: 0.00001045
Iteration 168/1000 | Loss: 0.00001045
Iteration 169/1000 | Loss: 0.00001045
Iteration 170/1000 | Loss: 0.00001045
Iteration 171/1000 | Loss: 0.00001045
Iteration 172/1000 | Loss: 0.00001045
Iteration 173/1000 | Loss: 0.00001045
Iteration 174/1000 | Loss: 0.00001045
Iteration 175/1000 | Loss: 0.00001045
Iteration 176/1000 | Loss: 0.00001045
Iteration 177/1000 | Loss: 0.00001045
Iteration 178/1000 | Loss: 0.00001045
Iteration 179/1000 | Loss: 0.00001045
Iteration 180/1000 | Loss: 0.00001045
Iteration 181/1000 | Loss: 0.00001045
Iteration 182/1000 | Loss: 0.00001045
Iteration 183/1000 | Loss: 0.00001045
Iteration 184/1000 | Loss: 0.00001045
Iteration 185/1000 | Loss: 0.00001045
Iteration 186/1000 | Loss: 0.00001045
Iteration 187/1000 | Loss: 0.00001044
Iteration 188/1000 | Loss: 0.00001044
Iteration 189/1000 | Loss: 0.00001044
Iteration 190/1000 | Loss: 0.00001044
Iteration 191/1000 | Loss: 0.00001044
Iteration 192/1000 | Loss: 0.00001044
Iteration 193/1000 | Loss: 0.00001044
Iteration 194/1000 | Loss: 0.00001044
Iteration 195/1000 | Loss: 0.00001044
Iteration 196/1000 | Loss: 0.00001044
Iteration 197/1000 | Loss: 0.00001044
Iteration 198/1000 | Loss: 0.00001044
Iteration 199/1000 | Loss: 0.00001043
Iteration 200/1000 | Loss: 0.00001043
Iteration 201/1000 | Loss: 0.00001043
Iteration 202/1000 | Loss: 0.00001043
Iteration 203/1000 | Loss: 0.00001043
Iteration 204/1000 | Loss: 0.00001043
Iteration 205/1000 | Loss: 0.00001043
Iteration 206/1000 | Loss: 0.00001043
Iteration 207/1000 | Loss: 0.00001043
Iteration 208/1000 | Loss: 0.00001043
Iteration 209/1000 | Loss: 0.00001043
Iteration 210/1000 | Loss: 0.00001043
Iteration 211/1000 | Loss: 0.00001043
Iteration 212/1000 | Loss: 0.00001043
Iteration 213/1000 | Loss: 0.00001043
Iteration 214/1000 | Loss: 0.00001043
Iteration 215/1000 | Loss: 0.00001043
Iteration 216/1000 | Loss: 0.00001043
Iteration 217/1000 | Loss: 0.00001043
Iteration 218/1000 | Loss: 0.00001043
Iteration 219/1000 | Loss: 0.00001043
Iteration 220/1000 | Loss: 0.00001043
Iteration 221/1000 | Loss: 0.00001043
Iteration 222/1000 | Loss: 0.00001043
Iteration 223/1000 | Loss: 0.00001043
Iteration 224/1000 | Loss: 0.00001043
Iteration 225/1000 | Loss: 0.00001043
Iteration 226/1000 | Loss: 0.00001043
Iteration 227/1000 | Loss: 0.00001043
Iteration 228/1000 | Loss: 0.00001043
Iteration 229/1000 | Loss: 0.00001043
Iteration 230/1000 | Loss: 0.00001043
Iteration 231/1000 | Loss: 0.00001043
Iteration 232/1000 | Loss: 0.00001043
Iteration 233/1000 | Loss: 0.00001043
Iteration 234/1000 | Loss: 0.00001043
Iteration 235/1000 | Loss: 0.00001043
Iteration 236/1000 | Loss: 0.00001043
Iteration 237/1000 | Loss: 0.00001043
Iteration 238/1000 | Loss: 0.00001043
Iteration 239/1000 | Loss: 0.00001043
Iteration 240/1000 | Loss: 0.00001043
Iteration 241/1000 | Loss: 0.00001043
Iteration 242/1000 | Loss: 0.00001043
Iteration 243/1000 | Loss: 0.00001043
Iteration 244/1000 | Loss: 0.00001043
Iteration 245/1000 | Loss: 0.00001043
Iteration 246/1000 | Loss: 0.00001043
Iteration 247/1000 | Loss: 0.00001043
Iteration 248/1000 | Loss: 0.00001043
Iteration 249/1000 | Loss: 0.00001043
Iteration 250/1000 | Loss: 0.00001043
Iteration 251/1000 | Loss: 0.00001043
Iteration 252/1000 | Loss: 0.00001043
Iteration 253/1000 | Loss: 0.00001043
Iteration 254/1000 | Loss: 0.00001043
Iteration 255/1000 | Loss: 0.00001043
Iteration 256/1000 | Loss: 0.00001043
Iteration 257/1000 | Loss: 0.00001043
Iteration 258/1000 | Loss: 0.00001043
Iteration 259/1000 | Loss: 0.00001043
Iteration 260/1000 | Loss: 0.00001043
Iteration 261/1000 | Loss: 0.00001043
Iteration 262/1000 | Loss: 0.00001043
Iteration 263/1000 | Loss: 0.00001043
Iteration 264/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.0429759640828706e-05, 1.0429759640828706e-05, 1.0429759640828706e-05, 1.0429759640828706e-05, 1.0429759640828706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0429759640828706e-05

Optimization complete. Final v2v error: 2.77923321723938 mm

Highest mean error: 3.0775787830352783 mm for frame 237

Lowest mean error: 2.597914218902588 mm for frame 9

Saving results

Total time: 62.49963068962097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901406
Iteration 2/25 | Loss: 0.00179546
Iteration 3/25 | Loss: 0.00144678
Iteration 4/25 | Loss: 0.00142128
Iteration 5/25 | Loss: 0.00141396
Iteration 6/25 | Loss: 0.00141201
Iteration 7/25 | Loss: 0.00141177
Iteration 8/25 | Loss: 0.00141177
Iteration 9/25 | Loss: 0.00141177
Iteration 10/25 | Loss: 0.00141177
Iteration 11/25 | Loss: 0.00141177
Iteration 12/25 | Loss: 0.00141177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001411772915162146, 0.001411772915162146, 0.001411772915162146, 0.001411772915162146, 0.001411772915162146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001411772915162146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07699943
Iteration 2/25 | Loss: 0.00173730
Iteration 3/25 | Loss: 0.00173729
Iteration 4/25 | Loss: 0.00173729
Iteration 5/25 | Loss: 0.00173729
Iteration 6/25 | Loss: 0.00173729
Iteration 7/25 | Loss: 0.00173729
Iteration 8/25 | Loss: 0.00173729
Iteration 9/25 | Loss: 0.00173729
Iteration 10/25 | Loss: 0.00173729
Iteration 11/25 | Loss: 0.00173729
Iteration 12/25 | Loss: 0.00173729
Iteration 13/25 | Loss: 0.00173729
Iteration 14/25 | Loss: 0.00173729
Iteration 15/25 | Loss: 0.00173729
Iteration 16/25 | Loss: 0.00173729
Iteration 17/25 | Loss: 0.00173729
Iteration 18/25 | Loss: 0.00173729
Iteration 19/25 | Loss: 0.00173729
Iteration 20/25 | Loss: 0.00173729
Iteration 21/25 | Loss: 0.00173729
Iteration 22/25 | Loss: 0.00173729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017372919246554375, 0.0017372919246554375, 0.0017372919246554375, 0.0017372919246554375, 0.0017372919246554375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017372919246554375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173729
Iteration 2/1000 | Loss: 0.00005008
Iteration 3/1000 | Loss: 0.00003854
Iteration 4/1000 | Loss: 0.00003246
Iteration 5/1000 | Loss: 0.00003052
Iteration 6/1000 | Loss: 0.00002936
Iteration 7/1000 | Loss: 0.00002860
Iteration 8/1000 | Loss: 0.00002795
Iteration 9/1000 | Loss: 0.00002739
Iteration 10/1000 | Loss: 0.00002700
Iteration 11/1000 | Loss: 0.00002667
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002615
Iteration 14/1000 | Loss: 0.00002590
Iteration 15/1000 | Loss: 0.00002579
Iteration 16/1000 | Loss: 0.00002560
Iteration 17/1000 | Loss: 0.00002541
Iteration 18/1000 | Loss: 0.00002529
Iteration 19/1000 | Loss: 0.00002521
Iteration 20/1000 | Loss: 0.00002513
Iteration 21/1000 | Loss: 0.00002506
Iteration 22/1000 | Loss: 0.00002505
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002499
Iteration 25/1000 | Loss: 0.00002496
Iteration 26/1000 | Loss: 0.00002495
Iteration 27/1000 | Loss: 0.00002494
Iteration 28/1000 | Loss: 0.00002492
Iteration 29/1000 | Loss: 0.00002492
Iteration 30/1000 | Loss: 0.00002492
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00002489
Iteration 33/1000 | Loss: 0.00002488
Iteration 34/1000 | Loss: 0.00002488
Iteration 35/1000 | Loss: 0.00002486
Iteration 36/1000 | Loss: 0.00002486
Iteration 37/1000 | Loss: 0.00002486
Iteration 38/1000 | Loss: 0.00002486
Iteration 39/1000 | Loss: 0.00002485
Iteration 40/1000 | Loss: 0.00002485
Iteration 41/1000 | Loss: 0.00002485
Iteration 42/1000 | Loss: 0.00002485
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00002485
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002484
Iteration 48/1000 | Loss: 0.00002484
Iteration 49/1000 | Loss: 0.00002484
Iteration 50/1000 | Loss: 0.00002484
Iteration 51/1000 | Loss: 0.00002483
Iteration 52/1000 | Loss: 0.00002483
Iteration 53/1000 | Loss: 0.00002483
Iteration 54/1000 | Loss: 0.00002483
Iteration 55/1000 | Loss: 0.00002483
Iteration 56/1000 | Loss: 0.00002482
Iteration 57/1000 | Loss: 0.00002482
Iteration 58/1000 | Loss: 0.00002482
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00002482
Iteration 61/1000 | Loss: 0.00002482
Iteration 62/1000 | Loss: 0.00002482
Iteration 63/1000 | Loss: 0.00002482
Iteration 64/1000 | Loss: 0.00002482
Iteration 65/1000 | Loss: 0.00002481
Iteration 66/1000 | Loss: 0.00002481
Iteration 67/1000 | Loss: 0.00002481
Iteration 68/1000 | Loss: 0.00002480
Iteration 69/1000 | Loss: 0.00002480
Iteration 70/1000 | Loss: 0.00002480
Iteration 71/1000 | Loss: 0.00002480
Iteration 72/1000 | Loss: 0.00002480
Iteration 73/1000 | Loss: 0.00002480
Iteration 74/1000 | Loss: 0.00002480
Iteration 75/1000 | Loss: 0.00002480
Iteration 76/1000 | Loss: 0.00002480
Iteration 77/1000 | Loss: 0.00002480
Iteration 78/1000 | Loss: 0.00002480
Iteration 79/1000 | Loss: 0.00002480
Iteration 80/1000 | Loss: 0.00002480
Iteration 81/1000 | Loss: 0.00002480
Iteration 82/1000 | Loss: 0.00002480
Iteration 83/1000 | Loss: 0.00002480
Iteration 84/1000 | Loss: 0.00002480
Iteration 85/1000 | Loss: 0.00002480
Iteration 86/1000 | Loss: 0.00002480
Iteration 87/1000 | Loss: 0.00002480
Iteration 88/1000 | Loss: 0.00002480
Iteration 89/1000 | Loss: 0.00002480
Iteration 90/1000 | Loss: 0.00002480
Iteration 91/1000 | Loss: 0.00002480
Iteration 92/1000 | Loss: 0.00002480
Iteration 93/1000 | Loss: 0.00002480
Iteration 94/1000 | Loss: 0.00002480
Iteration 95/1000 | Loss: 0.00002480
Iteration 96/1000 | Loss: 0.00002480
Iteration 97/1000 | Loss: 0.00002480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.4796770958346315e-05, 2.4796770958346315e-05, 2.4796770958346315e-05, 2.4796770958346315e-05, 2.4796770958346315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4796770958346315e-05

Optimization complete. Final v2v error: 4.108577251434326 mm

Highest mean error: 5.368161201477051 mm for frame 85

Lowest mean error: 3.2937867641448975 mm for frame 121

Saving results

Total time: 44.51325488090515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00697509
Iteration 2/25 | Loss: 0.00145620
Iteration 3/25 | Loss: 0.00137224
Iteration 4/25 | Loss: 0.00134120
Iteration 5/25 | Loss: 0.00134396
Iteration 6/25 | Loss: 0.00133176
Iteration 7/25 | Loss: 0.00132920
Iteration 8/25 | Loss: 0.00133058
Iteration 9/25 | Loss: 0.00133046
Iteration 10/25 | Loss: 0.00132788
Iteration 11/25 | Loss: 0.00132971
Iteration 12/25 | Loss: 0.00132759
Iteration 13/25 | Loss: 0.00132935
Iteration 14/25 | Loss: 0.00132752
Iteration 15/25 | Loss: 0.00132751
Iteration 16/25 | Loss: 0.00132751
Iteration 17/25 | Loss: 0.00132751
Iteration 18/25 | Loss: 0.00132751
Iteration 19/25 | Loss: 0.00132751
Iteration 20/25 | Loss: 0.00132751
Iteration 21/25 | Loss: 0.00132751
Iteration 22/25 | Loss: 0.00132751
Iteration 23/25 | Loss: 0.00132751
Iteration 24/25 | Loss: 0.00132751
Iteration 25/25 | Loss: 0.00132750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45673060
Iteration 2/25 | Loss: 0.00219191
Iteration 3/25 | Loss: 0.00219191
Iteration 4/25 | Loss: 0.00219191
Iteration 5/25 | Loss: 0.00219191
Iteration 6/25 | Loss: 0.00219191
Iteration 7/25 | Loss: 0.00219191
Iteration 8/25 | Loss: 0.00219191
Iteration 9/25 | Loss: 0.00219191
Iteration 10/25 | Loss: 0.00219191
Iteration 11/25 | Loss: 0.00219191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0021919056307524443, 0.0021919056307524443, 0.0021919056307524443, 0.0021919056307524443, 0.0021919056307524443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021919056307524443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219191
Iteration 2/1000 | Loss: 0.00002224
Iteration 3/1000 | Loss: 0.00001859
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001623
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00023092
Iteration 9/1000 | Loss: 0.00001608
Iteration 10/1000 | Loss: 0.00001462
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001280
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001263
Iteration 24/1000 | Loss: 0.00001253
Iteration 25/1000 | Loss: 0.00001251
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001228
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001218
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001211
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001209
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001205
Iteration 139/1000 | Loss: 0.00001205
Iteration 140/1000 | Loss: 0.00001205
Iteration 141/1000 | Loss: 0.00001205
Iteration 142/1000 | Loss: 0.00001205
Iteration 143/1000 | Loss: 0.00001205
Iteration 144/1000 | Loss: 0.00001205
Iteration 145/1000 | Loss: 0.00001205
Iteration 146/1000 | Loss: 0.00001205
Iteration 147/1000 | Loss: 0.00001205
Iteration 148/1000 | Loss: 0.00001205
Iteration 149/1000 | Loss: 0.00001205
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.2047403288306668e-05, 1.2047403288306668e-05, 1.2047403288306668e-05, 1.2047403288306668e-05, 1.2047403288306668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2047403288306668e-05

Optimization complete. Final v2v error: 3.023947238922119 mm

Highest mean error: 3.8628017902374268 mm for frame 140

Lowest mean error: 2.7806789875030518 mm for frame 33

Saving results

Total time: 65.4457495212555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500087
Iteration 2/25 | Loss: 0.00143109
Iteration 3/25 | Loss: 0.00135063
Iteration 4/25 | Loss: 0.00133729
Iteration 5/25 | Loss: 0.00133155
Iteration 6/25 | Loss: 0.00133145
Iteration 7/25 | Loss: 0.00133145
Iteration 8/25 | Loss: 0.00133145
Iteration 9/25 | Loss: 0.00133145
Iteration 10/25 | Loss: 0.00133145
Iteration 11/25 | Loss: 0.00133145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001331452396698296, 0.001331452396698296, 0.001331452396698296, 0.001331452396698296, 0.001331452396698296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001331452396698296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72605771
Iteration 2/25 | Loss: 0.00184389
Iteration 3/25 | Loss: 0.00184389
Iteration 4/25 | Loss: 0.00184389
Iteration 5/25 | Loss: 0.00184388
Iteration 6/25 | Loss: 0.00184388
Iteration 7/25 | Loss: 0.00184388
Iteration 8/25 | Loss: 0.00184388
Iteration 9/25 | Loss: 0.00184388
Iteration 10/25 | Loss: 0.00184388
Iteration 11/25 | Loss: 0.00184388
Iteration 12/25 | Loss: 0.00184388
Iteration 13/25 | Loss: 0.00184388
Iteration 14/25 | Loss: 0.00184388
Iteration 15/25 | Loss: 0.00184388
Iteration 16/25 | Loss: 0.00184388
Iteration 17/25 | Loss: 0.00184388
Iteration 18/25 | Loss: 0.00184388
Iteration 19/25 | Loss: 0.00184388
Iteration 20/25 | Loss: 0.00184388
Iteration 21/25 | Loss: 0.00184388
Iteration 22/25 | Loss: 0.00184388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00184388249181211, 0.00184388249181211, 0.00184388249181211, 0.00184388249181211, 0.00184388249181211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00184388249181211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184388
Iteration 2/1000 | Loss: 0.00003524
Iteration 3/1000 | Loss: 0.00002467
Iteration 4/1000 | Loss: 0.00002231
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00002026
Iteration 7/1000 | Loss: 0.00001962
Iteration 8/1000 | Loss: 0.00001911
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001838
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001769
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001685
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001636
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001614
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001612
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001611
Iteration 36/1000 | Loss: 0.00001611
Iteration 37/1000 | Loss: 0.00001610
Iteration 38/1000 | Loss: 0.00001610
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001606
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001605
Iteration 57/1000 | Loss: 0.00001605
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001604
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001604
Iteration 69/1000 | Loss: 0.00001604
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001604
Iteration 73/1000 | Loss: 0.00001603
Iteration 74/1000 | Loss: 0.00001603
Iteration 75/1000 | Loss: 0.00001603
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001602
Iteration 82/1000 | Loss: 0.00001602
Iteration 83/1000 | Loss: 0.00001602
Iteration 84/1000 | Loss: 0.00001602
Iteration 85/1000 | Loss: 0.00001601
Iteration 86/1000 | Loss: 0.00001601
Iteration 87/1000 | Loss: 0.00001601
Iteration 88/1000 | Loss: 0.00001601
Iteration 89/1000 | Loss: 0.00001601
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001600
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001600
Iteration 95/1000 | Loss: 0.00001600
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001599
Iteration 99/1000 | Loss: 0.00001599
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001598
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001598
Iteration 106/1000 | Loss: 0.00001598
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001597
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001597
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001597
Iteration 113/1000 | Loss: 0.00001597
Iteration 114/1000 | Loss: 0.00001597
Iteration 115/1000 | Loss: 0.00001597
Iteration 116/1000 | Loss: 0.00001597
Iteration 117/1000 | Loss: 0.00001597
Iteration 118/1000 | Loss: 0.00001597
Iteration 119/1000 | Loss: 0.00001596
Iteration 120/1000 | Loss: 0.00001596
Iteration 121/1000 | Loss: 0.00001596
Iteration 122/1000 | Loss: 0.00001596
Iteration 123/1000 | Loss: 0.00001596
Iteration 124/1000 | Loss: 0.00001596
Iteration 125/1000 | Loss: 0.00001596
Iteration 126/1000 | Loss: 0.00001596
Iteration 127/1000 | Loss: 0.00001596
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001595
Iteration 133/1000 | Loss: 0.00001595
Iteration 134/1000 | Loss: 0.00001595
Iteration 135/1000 | Loss: 0.00001595
Iteration 136/1000 | Loss: 0.00001594
Iteration 137/1000 | Loss: 0.00001594
Iteration 138/1000 | Loss: 0.00001594
Iteration 139/1000 | Loss: 0.00001594
Iteration 140/1000 | Loss: 0.00001594
Iteration 141/1000 | Loss: 0.00001594
Iteration 142/1000 | Loss: 0.00001594
Iteration 143/1000 | Loss: 0.00001594
Iteration 144/1000 | Loss: 0.00001594
Iteration 145/1000 | Loss: 0.00001594
Iteration 146/1000 | Loss: 0.00001594
Iteration 147/1000 | Loss: 0.00001594
Iteration 148/1000 | Loss: 0.00001594
Iteration 149/1000 | Loss: 0.00001594
Iteration 150/1000 | Loss: 0.00001594
Iteration 151/1000 | Loss: 0.00001594
Iteration 152/1000 | Loss: 0.00001594
Iteration 153/1000 | Loss: 0.00001594
Iteration 154/1000 | Loss: 0.00001594
Iteration 155/1000 | Loss: 0.00001594
Iteration 156/1000 | Loss: 0.00001594
Iteration 157/1000 | Loss: 0.00001594
Iteration 158/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.5935087503748946e-05, 1.5935087503748946e-05, 1.5935087503748946e-05, 1.5935087503748946e-05, 1.5935087503748946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5935087503748946e-05

Optimization complete. Final v2v error: 3.4187469482421875 mm

Highest mean error: 4.377168655395508 mm for frame 266

Lowest mean error: 3.3053815364837646 mm for frame 104

Saving results

Total time: 56.25321888923645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713196
Iteration 2/25 | Loss: 0.00144967
Iteration 3/25 | Loss: 0.00136127
Iteration 4/25 | Loss: 0.00133225
Iteration 5/25 | Loss: 0.00132567
Iteration 6/25 | Loss: 0.00132344
Iteration 7/25 | Loss: 0.00132297
Iteration 8/25 | Loss: 0.00132279
Iteration 9/25 | Loss: 0.00132275
Iteration 10/25 | Loss: 0.00132275
Iteration 11/25 | Loss: 0.00132275
Iteration 12/25 | Loss: 0.00132275
Iteration 13/25 | Loss: 0.00132275
Iteration 14/25 | Loss: 0.00132275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013227498857304454, 0.0013227498857304454, 0.0013227498857304454, 0.0013227498857304454, 0.0013227498857304454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013227498857304454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81570542
Iteration 2/25 | Loss: 0.00220281
Iteration 3/25 | Loss: 0.00220281
Iteration 4/25 | Loss: 0.00220281
Iteration 5/25 | Loss: 0.00220281
Iteration 6/25 | Loss: 0.00220281
Iteration 7/25 | Loss: 0.00220281
Iteration 8/25 | Loss: 0.00220281
Iteration 9/25 | Loss: 0.00220281
Iteration 10/25 | Loss: 0.00220281
Iteration 11/25 | Loss: 0.00220281
Iteration 12/25 | Loss: 0.00220281
Iteration 13/25 | Loss: 0.00220281
Iteration 14/25 | Loss: 0.00220281
Iteration 15/25 | Loss: 0.00220281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002202809788286686, 0.002202809788286686, 0.002202809788286686, 0.002202809788286686, 0.002202809788286686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002202809788286686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220281
Iteration 2/1000 | Loss: 0.00002214
Iteration 3/1000 | Loss: 0.00001661
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001430
Iteration 6/1000 | Loss: 0.00001371
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001283
Iteration 9/1000 | Loss: 0.00006085
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00008241
Iteration 21/1000 | Loss: 0.00001171
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001136
Iteration 29/1000 | Loss: 0.00001132
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001129
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001124
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001123
Iteration 45/1000 | Loss: 0.00001123
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001117
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001116
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001116
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001116
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001108
Iteration 131/1000 | Loss: 0.00001108
Iteration 132/1000 | Loss: 0.00001108
Iteration 133/1000 | Loss: 0.00001108
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001105
Iteration 179/1000 | Loss: 0.00001105
Iteration 180/1000 | Loss: 0.00001105
Iteration 181/1000 | Loss: 0.00001105
Iteration 182/1000 | Loss: 0.00001105
Iteration 183/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1046969120798167e-05, 1.1046969120798167e-05, 1.1046969120798167e-05, 1.1046969120798167e-05, 1.1046969120798167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1046969120798167e-05

Optimization complete. Final v2v error: 2.8945374488830566 mm

Highest mean error: 3.1291332244873047 mm for frame 89

Lowest mean error: 2.7535581588745117 mm for frame 58

Saving results

Total time: 57.64244771003723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006227
Iteration 2/25 | Loss: 0.00308065
Iteration 3/25 | Loss: 0.00216458
Iteration 4/25 | Loss: 0.00168506
Iteration 5/25 | Loss: 0.00161742
Iteration 6/25 | Loss: 0.00154927
Iteration 7/25 | Loss: 0.00149136
Iteration 8/25 | Loss: 0.00143069
Iteration 9/25 | Loss: 0.00140715
Iteration 10/25 | Loss: 0.00138628
Iteration 11/25 | Loss: 0.00137745
Iteration 12/25 | Loss: 0.00137438
Iteration 13/25 | Loss: 0.00137053
Iteration 14/25 | Loss: 0.00136828
Iteration 15/25 | Loss: 0.00136793
Iteration 16/25 | Loss: 0.00136785
Iteration 17/25 | Loss: 0.00136785
Iteration 18/25 | Loss: 0.00136785
Iteration 19/25 | Loss: 0.00136785
Iteration 20/25 | Loss: 0.00136785
Iteration 21/25 | Loss: 0.00136785
Iteration 22/25 | Loss: 0.00136785
Iteration 23/25 | Loss: 0.00136785
Iteration 24/25 | Loss: 0.00136785
Iteration 25/25 | Loss: 0.00136785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86614287
Iteration 2/25 | Loss: 0.00249901
Iteration 3/25 | Loss: 0.00249901
Iteration 4/25 | Loss: 0.00249901
Iteration 5/25 | Loss: 0.00249901
Iteration 6/25 | Loss: 0.00249901
Iteration 7/25 | Loss: 0.00249901
Iteration 8/25 | Loss: 0.00249900
Iteration 9/25 | Loss: 0.00249900
Iteration 10/25 | Loss: 0.00249900
Iteration 11/25 | Loss: 0.00249900
Iteration 12/25 | Loss: 0.00249900
Iteration 13/25 | Loss: 0.00249900
Iteration 14/25 | Loss: 0.00249900
Iteration 15/25 | Loss: 0.00249900
Iteration 16/25 | Loss: 0.00249900
Iteration 17/25 | Loss: 0.00249900
Iteration 18/25 | Loss: 0.00249900
Iteration 19/25 | Loss: 0.00249900
Iteration 20/25 | Loss: 0.00249900
Iteration 21/25 | Loss: 0.00249900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024990036617964506, 0.0024990036617964506, 0.0024990036617964506, 0.0024990036617964506, 0.0024990036617964506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024990036617964506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249900
Iteration 2/1000 | Loss: 0.00005325
Iteration 3/1000 | Loss: 0.00003497
Iteration 4/1000 | Loss: 0.00003047
Iteration 5/1000 | Loss: 0.00002881
Iteration 6/1000 | Loss: 0.00023506
Iteration 7/1000 | Loss: 0.00016673
Iteration 8/1000 | Loss: 0.00012782
Iteration 9/1000 | Loss: 0.00003156
Iteration 10/1000 | Loss: 0.00002685
Iteration 11/1000 | Loss: 0.00002530
Iteration 12/1000 | Loss: 0.00002457
Iteration 13/1000 | Loss: 0.00002388
Iteration 14/1000 | Loss: 0.00002338
Iteration 15/1000 | Loss: 0.00002297
Iteration 16/1000 | Loss: 0.00085213
Iteration 17/1000 | Loss: 0.00002707
Iteration 18/1000 | Loss: 0.00002329
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00002005
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001811
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001656
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001617
Iteration 84/1000 | Loss: 0.00001617
Iteration 85/1000 | Loss: 0.00001617
Iteration 86/1000 | Loss: 0.00001617
Iteration 87/1000 | Loss: 0.00001617
Iteration 88/1000 | Loss: 0.00001617
Iteration 89/1000 | Loss: 0.00001617
Iteration 90/1000 | Loss: 0.00001617
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001616
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001615
Iteration 121/1000 | Loss: 0.00001615
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.6146346752066165e-05, 1.6146346752066165e-05, 1.6146346752066165e-05, 1.6146346752066165e-05, 1.6146346752066165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6146346752066165e-05

Optimization complete. Final v2v error: 3.4711546897888184 mm

Highest mean error: 4.115818977355957 mm for frame 189

Lowest mean error: 3.1471548080444336 mm for frame 132

Saving results

Total time: 86.99910521507263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984833
Iteration 2/25 | Loss: 0.00315215
Iteration 3/25 | Loss: 0.00240802
Iteration 4/25 | Loss: 0.00231591
Iteration 5/25 | Loss: 0.00217669
Iteration 6/25 | Loss: 0.00217934
Iteration 7/25 | Loss: 0.00219577
Iteration 8/25 | Loss: 0.00225505
Iteration 9/25 | Loss: 0.00200546
Iteration 10/25 | Loss: 0.00196922
Iteration 11/25 | Loss: 0.00195452
Iteration 12/25 | Loss: 0.00193952
Iteration 13/25 | Loss: 0.00194036
Iteration 14/25 | Loss: 0.00194689
Iteration 15/25 | Loss: 0.00191658
Iteration 16/25 | Loss: 0.00191043
Iteration 17/25 | Loss: 0.00189460
Iteration 18/25 | Loss: 0.00189499
Iteration 19/25 | Loss: 0.00189681
Iteration 20/25 | Loss: 0.00188922
Iteration 21/25 | Loss: 0.00188797
Iteration 22/25 | Loss: 0.00188080
Iteration 23/25 | Loss: 0.00187793
Iteration 24/25 | Loss: 0.00187878
Iteration 25/25 | Loss: 0.00187742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19133961
Iteration 2/25 | Loss: 0.00761566
Iteration 3/25 | Loss: 0.00633992
Iteration 4/25 | Loss: 0.00633991
Iteration 5/25 | Loss: 0.00633991
Iteration 6/25 | Loss: 0.00633991
Iteration 7/25 | Loss: 0.00633991
Iteration 8/25 | Loss: 0.00633991
Iteration 9/25 | Loss: 0.00633991
Iteration 10/25 | Loss: 0.00633991
Iteration 11/25 | Loss: 0.00633991
Iteration 12/25 | Loss: 0.00633991
Iteration 13/25 | Loss: 0.00633991
Iteration 14/25 | Loss: 0.00633991
Iteration 15/25 | Loss: 0.00633991
Iteration 16/25 | Loss: 0.00633991
Iteration 17/25 | Loss: 0.00633991
Iteration 18/25 | Loss: 0.00633991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.006339906714856625, 0.006339906714856625, 0.006339906714856625, 0.006339906714856625, 0.006339906714856625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006339906714856625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00633991
Iteration 2/1000 | Loss: 0.00309608
Iteration 3/1000 | Loss: 0.00067326
Iteration 4/1000 | Loss: 0.00090813
Iteration 5/1000 | Loss: 0.00041670
Iteration 6/1000 | Loss: 0.00050127
Iteration 7/1000 | Loss: 0.00038885
Iteration 8/1000 | Loss: 0.00033627
Iteration 9/1000 | Loss: 0.00049963
Iteration 10/1000 | Loss: 0.00031907
Iteration 11/1000 | Loss: 0.00029271
Iteration 12/1000 | Loss: 0.00031982
Iteration 13/1000 | Loss: 0.00030330
Iteration 14/1000 | Loss: 0.00028728
Iteration 15/1000 | Loss: 0.00033601
Iteration 16/1000 | Loss: 0.00034112
Iteration 17/1000 | Loss: 0.00121023
Iteration 18/1000 | Loss: 0.00156137
Iteration 19/1000 | Loss: 0.00234701
Iteration 20/1000 | Loss: 0.01538549
Iteration 21/1000 | Loss: 0.01101184
Iteration 22/1000 | Loss: 0.00412794
Iteration 23/1000 | Loss: 0.00076777
Iteration 24/1000 | Loss: 0.00076094
Iteration 25/1000 | Loss: 0.00062855
Iteration 26/1000 | Loss: 0.00107360
Iteration 27/1000 | Loss: 0.00169598
Iteration 28/1000 | Loss: 0.00031088
Iteration 29/1000 | Loss: 0.00086398
Iteration 30/1000 | Loss: 0.00015894
Iteration 31/1000 | Loss: 0.00012141
Iteration 32/1000 | Loss: 0.00015874
Iteration 33/1000 | Loss: 0.00011711
Iteration 34/1000 | Loss: 0.00005843
Iteration 35/1000 | Loss: 0.00015046
Iteration 36/1000 | Loss: 0.00005789
Iteration 37/1000 | Loss: 0.00004390
Iteration 38/1000 | Loss: 0.00003586
Iteration 39/1000 | Loss: 0.00007821
Iteration 40/1000 | Loss: 0.00006788
Iteration 41/1000 | Loss: 0.00003939
Iteration 42/1000 | Loss: 0.00005790
Iteration 43/1000 | Loss: 0.00005909
Iteration 44/1000 | Loss: 0.00004524
Iteration 45/1000 | Loss: 0.00004364
Iteration 46/1000 | Loss: 0.00005110
Iteration 47/1000 | Loss: 0.00005938
Iteration 48/1000 | Loss: 0.00004262
Iteration 49/1000 | Loss: 0.00031916
Iteration 50/1000 | Loss: 0.00058559
Iteration 51/1000 | Loss: 0.00016489
Iteration 52/1000 | Loss: 0.00013314
Iteration 53/1000 | Loss: 0.00003591
Iteration 54/1000 | Loss: 0.00009616
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002859
Iteration 57/1000 | Loss: 0.00002715
Iteration 58/1000 | Loss: 0.00003389
Iteration 59/1000 | Loss: 0.00002971
Iteration 60/1000 | Loss: 0.00002366
Iteration 61/1000 | Loss: 0.00004400
Iteration 62/1000 | Loss: 0.00002531
Iteration 63/1000 | Loss: 0.00003191
Iteration 64/1000 | Loss: 0.00047340
Iteration 65/1000 | Loss: 0.00004849
Iteration 66/1000 | Loss: 0.00002098
Iteration 67/1000 | Loss: 0.00003016
Iteration 68/1000 | Loss: 0.00002231
Iteration 69/1000 | Loss: 0.00002504
Iteration 70/1000 | Loss: 0.00002987
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001926
Iteration 73/1000 | Loss: 0.00001871
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002024
Iteration 76/1000 | Loss: 0.00005174
Iteration 77/1000 | Loss: 0.00021868
Iteration 78/1000 | Loss: 0.00002897
Iteration 79/1000 | Loss: 0.00019267
Iteration 80/1000 | Loss: 0.00018137
Iteration 81/1000 | Loss: 0.00002159
Iteration 82/1000 | Loss: 0.00002773
Iteration 83/1000 | Loss: 0.00004087
Iteration 84/1000 | Loss: 0.00002309
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001853
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00001853
Iteration 95/1000 | Loss: 0.00001853
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001852
Iteration 98/1000 | Loss: 0.00001852
Iteration 99/1000 | Loss: 0.00001852
Iteration 100/1000 | Loss: 0.00001852
Iteration 101/1000 | Loss: 0.00001852
Iteration 102/1000 | Loss: 0.00001852
Iteration 103/1000 | Loss: 0.00001852
Iteration 104/1000 | Loss: 0.00001852
Iteration 105/1000 | Loss: 0.00001852
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00001852
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001852
Iteration 129/1000 | Loss: 0.00001852
Iteration 130/1000 | Loss: 0.00001852
Iteration 131/1000 | Loss: 0.00001852
Iteration 132/1000 | Loss: 0.00001852
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Iteration 149/1000 | Loss: 0.00001852
Iteration 150/1000 | Loss: 0.00001852
Iteration 151/1000 | Loss: 0.00001852
Iteration 152/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.8517814169172198e-05, 1.8517814169172198e-05, 1.8517814169172198e-05, 1.8517814169172198e-05, 1.8517814169172198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8517814169172198e-05

Optimization complete. Final v2v error: 3.624450922012329 mm

Highest mean error: 4.051027297973633 mm for frame 90

Lowest mean error: 3.1204118728637695 mm for frame 48

Saving results

Total time: 188.97945547103882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988039
Iteration 2/25 | Loss: 0.00988039
Iteration 3/25 | Loss: 0.00988039
Iteration 4/25 | Loss: 0.00988039
Iteration 5/25 | Loss: 0.00988039
Iteration 6/25 | Loss: 0.00294258
Iteration 7/25 | Loss: 0.00213841
Iteration 8/25 | Loss: 0.00197599
Iteration 9/25 | Loss: 0.00183058
Iteration 10/25 | Loss: 0.00187116
Iteration 11/25 | Loss: 0.00176276
Iteration 12/25 | Loss: 0.00171813
Iteration 13/25 | Loss: 0.00168245
Iteration 14/25 | Loss: 0.00166027
Iteration 15/25 | Loss: 0.00165723
Iteration 16/25 | Loss: 0.00167777
Iteration 17/25 | Loss: 0.00163358
Iteration 18/25 | Loss: 0.00161295
Iteration 19/25 | Loss: 0.00160765
Iteration 20/25 | Loss: 0.00159326
Iteration 21/25 | Loss: 0.00158712
Iteration 22/25 | Loss: 0.00157968
Iteration 23/25 | Loss: 0.00157885
Iteration 24/25 | Loss: 0.00157878
Iteration 25/25 | Loss: 0.00157756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22714436
Iteration 2/25 | Loss: 0.00524642
Iteration 3/25 | Loss: 0.00461616
Iteration 4/25 | Loss: 0.00461609
Iteration 5/25 | Loss: 0.00461609
Iteration 6/25 | Loss: 0.00461609
Iteration 7/25 | Loss: 0.00461609
Iteration 8/25 | Loss: 0.00461609
Iteration 9/25 | Loss: 0.00461609
Iteration 10/25 | Loss: 0.00461609
Iteration 11/25 | Loss: 0.00461609
Iteration 12/25 | Loss: 0.00461609
Iteration 13/25 | Loss: 0.00461609
Iteration 14/25 | Loss: 0.00461609
Iteration 15/25 | Loss: 0.00461609
Iteration 16/25 | Loss: 0.00461609
Iteration 17/25 | Loss: 0.00461609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00461609149351716, 0.00461609149351716, 0.00461609149351716, 0.00461609149351716, 0.00461609149351716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00461609149351716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00461609
Iteration 2/1000 | Loss: 0.00089590
Iteration 3/1000 | Loss: 0.00262500
Iteration 4/1000 | Loss: 0.00032586
Iteration 5/1000 | Loss: 0.00024550
Iteration 6/1000 | Loss: 0.00063566
Iteration 7/1000 | Loss: 0.00034448
Iteration 8/1000 | Loss: 0.00018018
Iteration 9/1000 | Loss: 0.00109141
Iteration 10/1000 | Loss: 0.00091118
Iteration 11/1000 | Loss: 0.00052328
Iteration 12/1000 | Loss: 0.00017723
Iteration 13/1000 | Loss: 0.00015612
Iteration 14/1000 | Loss: 0.00014799
Iteration 15/1000 | Loss: 0.00014862
Iteration 16/1000 | Loss: 0.00015667
Iteration 17/1000 | Loss: 0.00014430
Iteration 18/1000 | Loss: 0.00098506
Iteration 19/1000 | Loss: 0.00018918
Iteration 20/1000 | Loss: 0.00019794
Iteration 21/1000 | Loss: 0.00025984
Iteration 22/1000 | Loss: 0.00027676
Iteration 23/1000 | Loss: 0.00033591
Iteration 24/1000 | Loss: 0.00024908
Iteration 25/1000 | Loss: 0.00012396
Iteration 26/1000 | Loss: 0.00021177
Iteration 27/1000 | Loss: 0.00014101
Iteration 28/1000 | Loss: 0.00031356
Iteration 29/1000 | Loss: 0.00049534
Iteration 30/1000 | Loss: 0.00017402
Iteration 31/1000 | Loss: 0.00044196
Iteration 32/1000 | Loss: 0.00055065
Iteration 33/1000 | Loss: 0.00014331
Iteration 34/1000 | Loss: 0.00013672
Iteration 35/1000 | Loss: 0.00013174
Iteration 36/1000 | Loss: 0.00013211
Iteration 37/1000 | Loss: 0.00012281
Iteration 38/1000 | Loss: 0.00012311
Iteration 39/1000 | Loss: 0.00015400
Iteration 40/1000 | Loss: 0.00014202
Iteration 41/1000 | Loss: 0.00013513
Iteration 42/1000 | Loss: 0.00013568
Iteration 43/1000 | Loss: 0.00012228
Iteration 44/1000 | Loss: 0.00032210
Iteration 45/1000 | Loss: 0.00017835
Iteration 46/1000 | Loss: 0.00036465
Iteration 47/1000 | Loss: 0.00012846
Iteration 48/1000 | Loss: 0.00013010
Iteration 49/1000 | Loss: 0.00014143
Iteration 50/1000 | Loss: 0.00013480
Iteration 51/1000 | Loss: 0.00013003
Iteration 52/1000 | Loss: 0.00012485
Iteration 53/1000 | Loss: 0.00014608
Iteration 54/1000 | Loss: 0.00012439
Iteration 55/1000 | Loss: 0.00012912
Iteration 56/1000 | Loss: 0.00012271
Iteration 57/1000 | Loss: 0.00013143
Iteration 58/1000 | Loss: 0.00013986
Iteration 59/1000 | Loss: 0.00012438
Iteration 60/1000 | Loss: 0.00012220
Iteration 61/1000 | Loss: 0.00012498
Iteration 62/1000 | Loss: 0.00026928
Iteration 63/1000 | Loss: 0.00022683
Iteration 64/1000 | Loss: 0.00012205
Iteration 65/1000 | Loss: 0.00013320
Iteration 66/1000 | Loss: 0.00011337
Iteration 67/1000 | Loss: 0.00013576
Iteration 68/1000 | Loss: 0.00013415
Iteration 69/1000 | Loss: 0.00011534
Iteration 70/1000 | Loss: 0.00028833
Iteration 71/1000 | Loss: 0.00014034
Iteration 72/1000 | Loss: 0.00025363
Iteration 73/1000 | Loss: 0.00060896
Iteration 74/1000 | Loss: 0.00012709
Iteration 75/1000 | Loss: 0.00011768
Iteration 76/1000 | Loss: 0.00028631
Iteration 77/1000 | Loss: 0.00024508
Iteration 78/1000 | Loss: 0.00025284
Iteration 79/1000 | Loss: 0.00013243
Iteration 80/1000 | Loss: 0.00013852
Iteration 81/1000 | Loss: 0.00012126
Iteration 82/1000 | Loss: 0.00014983
Iteration 83/1000 | Loss: 0.00010951
Iteration 84/1000 | Loss: 0.00011345
Iteration 85/1000 | Loss: 0.00010226
Iteration 86/1000 | Loss: 0.00023709
Iteration 87/1000 | Loss: 0.00023158
Iteration 88/1000 | Loss: 0.00010584
Iteration 89/1000 | Loss: 0.00025842
Iteration 90/1000 | Loss: 0.00020157
Iteration 91/1000 | Loss: 0.00060383
Iteration 92/1000 | Loss: 0.00056261
Iteration 93/1000 | Loss: 0.00067344
Iteration 94/1000 | Loss: 0.00050929
Iteration 95/1000 | Loss: 0.00048121
Iteration 96/1000 | Loss: 0.00011108
Iteration 97/1000 | Loss: 0.00010108
Iteration 98/1000 | Loss: 0.00012215
Iteration 99/1000 | Loss: 0.00010082
Iteration 100/1000 | Loss: 0.00010276
Iteration 101/1000 | Loss: 0.00009857
Iteration 102/1000 | Loss: 0.00011026
Iteration 103/1000 | Loss: 0.00010254
Iteration 104/1000 | Loss: 0.00009913
Iteration 105/1000 | Loss: 0.00010027
Iteration 106/1000 | Loss: 0.00010831
Iteration 107/1000 | Loss: 0.00010036
Iteration 108/1000 | Loss: 0.00010361
Iteration 109/1000 | Loss: 0.00009662
Iteration 110/1000 | Loss: 0.00010969
Iteration 111/1000 | Loss: 0.00009872
Iteration 112/1000 | Loss: 0.00010436
Iteration 113/1000 | Loss: 0.00010488
Iteration 114/1000 | Loss: 0.00010316
Iteration 115/1000 | Loss: 0.00010504
Iteration 116/1000 | Loss: 0.00009595
Iteration 117/1000 | Loss: 0.00009088
Iteration 118/1000 | Loss: 0.00009303
Iteration 119/1000 | Loss: 0.00009603
Iteration 120/1000 | Loss: 0.00010353
Iteration 121/1000 | Loss: 0.00010283
Iteration 122/1000 | Loss: 0.00009893
Iteration 123/1000 | Loss: 0.00010879
Iteration 124/1000 | Loss: 0.00010355
Iteration 125/1000 | Loss: 0.00010923
Iteration 126/1000 | Loss: 0.00010199
Iteration 127/1000 | Loss: 0.00011285
Iteration 128/1000 | Loss: 0.00010192
Iteration 129/1000 | Loss: 0.00010325
Iteration 130/1000 | Loss: 0.00010239
Iteration 131/1000 | Loss: 0.00011206
Iteration 132/1000 | Loss: 0.00010363
Iteration 133/1000 | Loss: 0.00010027
Iteration 134/1000 | Loss: 0.00012652
Iteration 135/1000 | Loss: 0.00010758
Iteration 136/1000 | Loss: 0.00010448
Iteration 137/1000 | Loss: 0.00010814
Iteration 138/1000 | Loss: 0.00009868
Iteration 139/1000 | Loss: 0.00012402
Iteration 140/1000 | Loss: 0.00009274
Iteration 141/1000 | Loss: 0.00010622
Iteration 142/1000 | Loss: 0.00010754
Iteration 143/1000 | Loss: 0.00010554
Iteration 144/1000 | Loss: 0.00012013
Iteration 145/1000 | Loss: 0.00009204
Iteration 146/1000 | Loss: 0.00009972
Iteration 147/1000 | Loss: 0.00010099
Iteration 148/1000 | Loss: 0.00010660
Iteration 149/1000 | Loss: 0.00010535
Iteration 150/1000 | Loss: 0.00011316
Iteration 151/1000 | Loss: 0.00010105
Iteration 152/1000 | Loss: 0.00009658
Iteration 153/1000 | Loss: 0.00011800
Iteration 154/1000 | Loss: 0.00016152
Iteration 155/1000 | Loss: 0.00009796
Iteration 156/1000 | Loss: 0.00010142
Iteration 157/1000 | Loss: 0.00011373
Iteration 158/1000 | Loss: 0.00009994
Iteration 159/1000 | Loss: 0.00010320
Iteration 160/1000 | Loss: 0.00009352
Iteration 161/1000 | Loss: 0.00009682
Iteration 162/1000 | Loss: 0.00009220
Iteration 163/1000 | Loss: 0.00010472
Iteration 164/1000 | Loss: 0.00011147
Iteration 165/1000 | Loss: 0.00010649
Iteration 166/1000 | Loss: 0.00010498
Iteration 167/1000 | Loss: 0.00010422
Iteration 168/1000 | Loss: 0.00010772
Iteration 169/1000 | Loss: 0.00010652
Iteration 170/1000 | Loss: 0.00010092
Iteration 171/1000 | Loss: 0.00010292
Iteration 172/1000 | Loss: 0.00009974
Iteration 173/1000 | Loss: 0.00010240
Iteration 174/1000 | Loss: 0.00010409
Iteration 175/1000 | Loss: 0.00010661
Iteration 176/1000 | Loss: 0.00010036
Iteration 177/1000 | Loss: 0.00010261
Iteration 178/1000 | Loss: 0.00009524
Iteration 179/1000 | Loss: 0.00010364
Iteration 180/1000 | Loss: 0.00011395
Iteration 181/1000 | Loss: 0.00009515
Iteration 182/1000 | Loss: 0.00009483
Iteration 183/1000 | Loss: 0.00010276
Iteration 184/1000 | Loss: 0.00009389
Iteration 185/1000 | Loss: 0.00011121
Iteration 186/1000 | Loss: 0.00010407
Iteration 187/1000 | Loss: 0.00010216
Iteration 188/1000 | Loss: 0.00010961
Iteration 189/1000 | Loss: 0.00009922
Iteration 190/1000 | Loss: 0.00010207
Iteration 191/1000 | Loss: 0.00010379
Iteration 192/1000 | Loss: 0.00011076
Iteration 193/1000 | Loss: 0.00010426
Iteration 194/1000 | Loss: 0.00009337
Iteration 195/1000 | Loss: 0.00009906
Iteration 196/1000 | Loss: 0.00009664
Iteration 197/1000 | Loss: 0.00009745
Iteration 198/1000 | Loss: 0.00009906
Iteration 199/1000 | Loss: 0.00008954
Iteration 200/1000 | Loss: 0.00009446
Iteration 201/1000 | Loss: 0.00008983
Iteration 202/1000 | Loss: 0.00008934
Iteration 203/1000 | Loss: 0.00008884
Iteration 204/1000 | Loss: 0.00008884
Iteration 205/1000 | Loss: 0.00008884
Iteration 206/1000 | Loss: 0.00008884
Iteration 207/1000 | Loss: 0.00008884
Iteration 208/1000 | Loss: 0.00008884
Iteration 209/1000 | Loss: 0.00008884
Iteration 210/1000 | Loss: 0.00008884
Iteration 211/1000 | Loss: 0.00008884
Iteration 212/1000 | Loss: 0.00008884
Iteration 213/1000 | Loss: 0.00008882
Iteration 214/1000 | Loss: 0.00008992
Iteration 215/1000 | Loss: 0.00008877
Iteration 216/1000 | Loss: 0.00008877
Iteration 217/1000 | Loss: 0.00008877
Iteration 218/1000 | Loss: 0.00008877
Iteration 219/1000 | Loss: 0.00008877
Iteration 220/1000 | Loss: 0.00008876
Iteration 221/1000 | Loss: 0.00008876
Iteration 222/1000 | Loss: 0.00008876
Iteration 223/1000 | Loss: 0.00008876
Iteration 224/1000 | Loss: 0.00008876
Iteration 225/1000 | Loss: 0.00008876
Iteration 226/1000 | Loss: 0.00008876
Iteration 227/1000 | Loss: 0.00008876
Iteration 228/1000 | Loss: 0.00008876
Iteration 229/1000 | Loss: 0.00008876
Iteration 230/1000 | Loss: 0.00008876
Iteration 231/1000 | Loss: 0.00008876
Iteration 232/1000 | Loss: 0.00008876
Iteration 233/1000 | Loss: 0.00008876
Iteration 234/1000 | Loss: 0.00008876
Iteration 235/1000 | Loss: 0.00008875
Iteration 236/1000 | Loss: 0.00008875
Iteration 237/1000 | Loss: 0.00008875
Iteration 238/1000 | Loss: 0.00008875
Iteration 239/1000 | Loss: 0.00008875
Iteration 240/1000 | Loss: 0.00008875
Iteration 241/1000 | Loss: 0.00008874
Iteration 242/1000 | Loss: 0.00008874
Iteration 243/1000 | Loss: 0.00008874
Iteration 244/1000 | Loss: 0.00008874
Iteration 245/1000 | Loss: 0.00008873
Iteration 246/1000 | Loss: 0.00008873
Iteration 247/1000 | Loss: 0.00008873
Iteration 248/1000 | Loss: 0.00008873
Iteration 249/1000 | Loss: 0.00008873
Iteration 250/1000 | Loss: 0.00008873
Iteration 251/1000 | Loss: 0.00008873
Iteration 252/1000 | Loss: 0.00008873
Iteration 253/1000 | Loss: 0.00008873
Iteration 254/1000 | Loss: 0.00008873
Iteration 255/1000 | Loss: 0.00008873
Iteration 256/1000 | Loss: 0.00008873
Iteration 257/1000 | Loss: 0.00008873
Iteration 258/1000 | Loss: 0.00008873
Iteration 259/1000 | Loss: 0.00008873
Iteration 260/1000 | Loss: 0.00008872
Iteration 261/1000 | Loss: 0.00008872
Iteration 262/1000 | Loss: 0.00008872
Iteration 263/1000 | Loss: 0.00008872
Iteration 264/1000 | Loss: 0.00008872
Iteration 265/1000 | Loss: 0.00008872
Iteration 266/1000 | Loss: 0.00008871
Iteration 267/1000 | Loss: 0.00008871
Iteration 268/1000 | Loss: 0.00008871
Iteration 269/1000 | Loss: 0.00009110
Iteration 270/1000 | Loss: 0.00008869
Iteration 271/1000 | Loss: 0.00008869
Iteration 272/1000 | Loss: 0.00008869
Iteration 273/1000 | Loss: 0.00008869
Iteration 274/1000 | Loss: 0.00008869
Iteration 275/1000 | Loss: 0.00008869
Iteration 276/1000 | Loss: 0.00008869
Iteration 277/1000 | Loss: 0.00008869
Iteration 278/1000 | Loss: 0.00008869
Iteration 279/1000 | Loss: 0.00008869
Iteration 280/1000 | Loss: 0.00008868
Iteration 281/1000 | Loss: 0.00008868
Iteration 282/1000 | Loss: 0.00008868
Iteration 283/1000 | Loss: 0.00008868
Iteration 284/1000 | Loss: 0.00008868
Iteration 285/1000 | Loss: 0.00008868
Iteration 286/1000 | Loss: 0.00008868
Iteration 287/1000 | Loss: 0.00008867
Iteration 288/1000 | Loss: 0.00008867
Iteration 289/1000 | Loss: 0.00008867
Iteration 290/1000 | Loss: 0.00008867
Iteration 291/1000 | Loss: 0.00008867
Iteration 292/1000 | Loss: 0.00008867
Iteration 293/1000 | Loss: 0.00008866
Iteration 294/1000 | Loss: 0.00008866
Iteration 295/1000 | Loss: 0.00008865
Iteration 296/1000 | Loss: 0.00008865
Iteration 297/1000 | Loss: 0.00008865
Iteration 298/1000 | Loss: 0.00008865
Iteration 299/1000 | Loss: 0.00016648
Iteration 300/1000 | Loss: 0.00010560
Iteration 301/1000 | Loss: 0.00016493
Iteration 302/1000 | Loss: 0.00059923
Iteration 303/1000 | Loss: 0.00021599
Iteration 304/1000 | Loss: 0.00032319
Iteration 305/1000 | Loss: 0.00030772
Iteration 306/1000 | Loss: 0.00018851
Iteration 307/1000 | Loss: 0.00013684
Iteration 308/1000 | Loss: 0.00010157
Iteration 309/1000 | Loss: 0.00008796
Iteration 310/1000 | Loss: 0.00009395
Iteration 311/1000 | Loss: 0.00010264
Iteration 312/1000 | Loss: 0.00009538
Iteration 313/1000 | Loss: 0.00008419
Iteration 314/1000 | Loss: 0.00015811
Iteration 315/1000 | Loss: 0.00008372
Iteration 316/1000 | Loss: 0.00008413
Iteration 317/1000 | Loss: 0.00008292
Iteration 318/1000 | Loss: 0.00008387
Iteration 319/1000 | Loss: 0.00015393
Iteration 320/1000 | Loss: 0.00019006
Iteration 321/1000 | Loss: 0.00052929
Iteration 322/1000 | Loss: 0.00013358
Iteration 323/1000 | Loss: 0.00009492
Iteration 324/1000 | Loss: 0.00018004
Iteration 325/1000 | Loss: 0.00008657
Iteration 326/1000 | Loss: 0.00008844
Iteration 327/1000 | Loss: 0.00008346
Iteration 328/1000 | Loss: 0.00008435
Iteration 329/1000 | Loss: 0.00008086
Iteration 330/1000 | Loss: 0.00017510
Iteration 331/1000 | Loss: 0.00008190
Iteration 332/1000 | Loss: 0.00008020
Iteration 333/1000 | Loss: 0.00014458
Iteration 334/1000 | Loss: 0.00008361
Iteration 335/1000 | Loss: 0.00008571
Iteration 336/1000 | Loss: 0.00007980
Iteration 337/1000 | Loss: 0.00011583
Iteration 338/1000 | Loss: 0.00009936
Iteration 339/1000 | Loss: 0.00007950
Iteration 340/1000 | Loss: 0.00007931
Iteration 341/1000 | Loss: 0.00045785
Iteration 342/1000 | Loss: 0.00031195
Iteration 343/1000 | Loss: 0.00009671
Iteration 344/1000 | Loss: 0.00009031
Iteration 345/1000 | Loss: 0.00008498
Iteration 346/1000 | Loss: 0.00008085
Iteration 347/1000 | Loss: 0.00008930
Iteration 348/1000 | Loss: 0.00007934
Iteration 349/1000 | Loss: 0.00044181
Iteration 350/1000 | Loss: 0.00021021
Iteration 351/1000 | Loss: 0.00040337
Iteration 352/1000 | Loss: 0.00022234
Iteration 353/1000 | Loss: 0.00036848
Iteration 354/1000 | Loss: 0.00022373
Iteration 355/1000 | Loss: 0.00036468
Iteration 356/1000 | Loss: 0.00021418
Iteration 357/1000 | Loss: 0.00035487
Iteration 358/1000 | Loss: 0.00020989
Iteration 359/1000 | Loss: 0.00034570
Iteration 360/1000 | Loss: 0.00021367
Iteration 361/1000 | Loss: 0.00026526
Iteration 362/1000 | Loss: 0.00030132
Iteration 363/1000 | Loss: 0.00029733
Iteration 364/1000 | Loss: 0.00026989
Iteration 365/1000 | Loss: 0.00038667
Iteration 366/1000 | Loss: 0.00008065
Iteration 367/1000 | Loss: 0.00008000
Iteration 368/1000 | Loss: 0.00008023
Iteration 369/1000 | Loss: 0.00007959
Iteration 370/1000 | Loss: 0.00007935
Iteration 371/1000 | Loss: 0.00007923
Iteration 372/1000 | Loss: 0.00007922
Iteration 373/1000 | Loss: 0.00007922
Iteration 374/1000 | Loss: 0.00007922
Iteration 375/1000 | Loss: 0.00007922
Iteration 376/1000 | Loss: 0.00007922
Iteration 377/1000 | Loss: 0.00007922
Iteration 378/1000 | Loss: 0.00007922
Iteration 379/1000 | Loss: 0.00007922
Iteration 380/1000 | Loss: 0.00007922
Iteration 381/1000 | Loss: 0.00007922
Iteration 382/1000 | Loss: 0.00007922
Iteration 383/1000 | Loss: 0.00007922
Iteration 384/1000 | Loss: 0.00007921
Iteration 385/1000 | Loss: 0.00007921
Iteration 386/1000 | Loss: 0.00007921
Iteration 387/1000 | Loss: 0.00007921
Iteration 388/1000 | Loss: 0.00007921
Iteration 389/1000 | Loss: 0.00007921
Iteration 390/1000 | Loss: 0.00007920
Iteration 391/1000 | Loss: 0.00007920
Iteration 392/1000 | Loss: 0.00007920
Iteration 393/1000 | Loss: 0.00007918
Iteration 394/1000 | Loss: 0.00007916
Iteration 395/1000 | Loss: 0.00007915
Iteration 396/1000 | Loss: 0.00008158
Iteration 397/1000 | Loss: 0.00008113
Iteration 398/1000 | Loss: 0.00025758
Iteration 399/1000 | Loss: 0.00021292
Iteration 400/1000 | Loss: 0.00008379
Iteration 401/1000 | Loss: 0.00009155
Iteration 402/1000 | Loss: 0.00008124
Iteration 403/1000 | Loss: 0.00007983
Iteration 404/1000 | Loss: 0.00008077
Iteration 405/1000 | Loss: 0.00025872
Iteration 406/1000 | Loss: 0.00030754
Iteration 407/1000 | Loss: 0.00031320
Iteration 408/1000 | Loss: 0.00021699
Iteration 409/1000 | Loss: 0.00039201
Iteration 410/1000 | Loss: 0.00020256
Iteration 411/1000 | Loss: 0.00022474
Iteration 412/1000 | Loss: 0.00029460
Iteration 413/1000 | Loss: 0.00017225
Iteration 414/1000 | Loss: 0.00013704
Iteration 415/1000 | Loss: 0.00008644
Iteration 416/1000 | Loss: 0.00008204
Iteration 417/1000 | Loss: 0.00008075
Iteration 418/1000 | Loss: 0.00007995
Iteration 419/1000 | Loss: 0.00007953
Iteration 420/1000 | Loss: 0.00008155
Iteration 421/1000 | Loss: 0.00009026
Iteration 422/1000 | Loss: 0.00008521
Iteration 423/1000 | Loss: 0.00007859
Iteration 424/1000 | Loss: 0.00007986
Iteration 425/1000 | Loss: 0.00008412
Iteration 426/1000 | Loss: 0.00007910
Iteration 427/1000 | Loss: 0.00007831
Iteration 428/1000 | Loss: 0.00007831
Iteration 429/1000 | Loss: 0.00007831
Iteration 430/1000 | Loss: 0.00007831
Iteration 431/1000 | Loss: 0.00007831
Iteration 432/1000 | Loss: 0.00007831
Iteration 433/1000 | Loss: 0.00007831
Iteration 434/1000 | Loss: 0.00007831
Iteration 435/1000 | Loss: 0.00007831
Iteration 436/1000 | Loss: 0.00007831
Iteration 437/1000 | Loss: 0.00007828
Iteration 438/1000 | Loss: 0.00007828
Iteration 439/1000 | Loss: 0.00007826
Iteration 440/1000 | Loss: 0.00008413
Iteration 441/1000 | Loss: 0.00007804
Iteration 442/1000 | Loss: 0.00007803
Iteration 443/1000 | Loss: 0.00007802
Iteration 444/1000 | Loss: 0.00007801
Iteration 445/1000 | Loss: 0.00007800
Iteration 446/1000 | Loss: 0.00007800
Iteration 447/1000 | Loss: 0.00007979
Iteration 448/1000 | Loss: 0.00009379
Iteration 449/1000 | Loss: 0.00007869
Iteration 450/1000 | Loss: 0.00007821
Iteration 451/1000 | Loss: 0.00007784
Iteration 452/1000 | Loss: 0.00008163
Iteration 453/1000 | Loss: 0.00007764
Iteration 454/1000 | Loss: 0.00007809
Iteration 455/1000 | Loss: 0.00007762
Iteration 456/1000 | Loss: 0.00007762
Iteration 457/1000 | Loss: 0.00007762
Iteration 458/1000 | Loss: 0.00007762
Iteration 459/1000 | Loss: 0.00007762
Iteration 460/1000 | Loss: 0.00007762
Iteration 461/1000 | Loss: 0.00007762
Iteration 462/1000 | Loss: 0.00007762
Iteration 463/1000 | Loss: 0.00007762
Iteration 464/1000 | Loss: 0.00007762
Iteration 465/1000 | Loss: 0.00007762
Iteration 466/1000 | Loss: 0.00007762
Iteration 467/1000 | Loss: 0.00007762
Iteration 468/1000 | Loss: 0.00007762
Iteration 469/1000 | Loss: 0.00007762
Iteration 470/1000 | Loss: 0.00007762
Iteration 471/1000 | Loss: 0.00007762
Iteration 472/1000 | Loss: 0.00007762
Iteration 473/1000 | Loss: 0.00007762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 473. Stopping optimization.
Last 5 losses: [7.761640881653875e-05, 7.761640881653875e-05, 7.761640881653875e-05, 7.761640881653875e-05, 7.761640881653875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.761640881653875e-05

Optimization complete. Final v2v error: 4.479413032531738 mm

Highest mean error: 12.014945030212402 mm for frame 149

Lowest mean error: 2.5887465476989746 mm for frame 134

Saving results

Total time: 566.6772422790527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624963
Iteration 2/25 | Loss: 0.00156841
Iteration 3/25 | Loss: 0.00142594
Iteration 4/25 | Loss: 0.00139867
Iteration 5/25 | Loss: 0.00138572
Iteration 6/25 | Loss: 0.00139017
Iteration 7/25 | Loss: 0.00138558
Iteration 8/25 | Loss: 0.00138198
Iteration 9/25 | Loss: 0.00138026
Iteration 10/25 | Loss: 0.00139006
Iteration 11/25 | Loss: 0.00139075
Iteration 12/25 | Loss: 0.00137894
Iteration 13/25 | Loss: 0.00138029
Iteration 14/25 | Loss: 0.00137397
Iteration 15/25 | Loss: 0.00137188
Iteration 16/25 | Loss: 0.00137172
Iteration 17/25 | Loss: 0.00137867
Iteration 18/25 | Loss: 0.00137867
Iteration 19/25 | Loss: 0.00137867
Iteration 20/25 | Loss: 0.00137867
Iteration 21/25 | Loss: 0.00137799
Iteration 22/25 | Loss: 0.00137170
Iteration 23/25 | Loss: 0.00137166
Iteration 24/25 | Loss: 0.00137166
Iteration 25/25 | Loss: 0.00137166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53315353
Iteration 2/25 | Loss: 0.00340468
Iteration 3/25 | Loss: 0.00340467
Iteration 4/25 | Loss: 0.00340467
Iteration 5/25 | Loss: 0.00340467
Iteration 6/25 | Loss: 0.00340467
Iteration 7/25 | Loss: 0.00340467
Iteration 8/25 | Loss: 0.00340467
Iteration 9/25 | Loss: 0.00340467
Iteration 10/25 | Loss: 0.00340467
Iteration 11/25 | Loss: 0.00340467
Iteration 12/25 | Loss: 0.00340467
Iteration 13/25 | Loss: 0.00340467
Iteration 14/25 | Loss: 0.00340467
Iteration 15/25 | Loss: 0.00340467
Iteration 16/25 | Loss: 0.00340467
Iteration 17/25 | Loss: 0.00340467
Iteration 18/25 | Loss: 0.00340467
Iteration 19/25 | Loss: 0.00340467
Iteration 20/25 | Loss: 0.00340467
Iteration 21/25 | Loss: 0.00340467
Iteration 22/25 | Loss: 0.00340467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0034046696964651346, 0.0034046696964651346, 0.0034046696964651346, 0.0034046696964651346, 0.0034046696964651346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034046696964651346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00340467
Iteration 2/1000 | Loss: 0.00005524
Iteration 3/1000 | Loss: 0.00003607
Iteration 4/1000 | Loss: 0.00003149
Iteration 5/1000 | Loss: 0.00019651
Iteration 6/1000 | Loss: 0.00002867
Iteration 7/1000 | Loss: 0.00002735
Iteration 8/1000 | Loss: 0.00002662
Iteration 9/1000 | Loss: 0.00002587
Iteration 10/1000 | Loss: 0.00002536
Iteration 11/1000 | Loss: 0.00002496
Iteration 12/1000 | Loss: 0.00002465
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002417
Iteration 15/1000 | Loss: 0.00002401
Iteration 16/1000 | Loss: 0.00002383
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002348
Iteration 19/1000 | Loss: 0.00002347
Iteration 20/1000 | Loss: 0.00002344
Iteration 21/1000 | Loss: 0.00002337
Iteration 22/1000 | Loss: 0.00002334
Iteration 23/1000 | Loss: 0.00002333
Iteration 24/1000 | Loss: 0.00002328
Iteration 25/1000 | Loss: 0.00002324
Iteration 26/1000 | Loss: 0.00002323
Iteration 27/1000 | Loss: 0.00002322
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002316
Iteration 30/1000 | Loss: 0.00002315
Iteration 31/1000 | Loss: 0.00002315
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00002311
Iteration 34/1000 | Loss: 0.00002309
Iteration 35/1000 | Loss: 0.00002308
Iteration 36/1000 | Loss: 0.00002307
Iteration 37/1000 | Loss: 0.00002307
Iteration 38/1000 | Loss: 0.00002307
Iteration 39/1000 | Loss: 0.00002305
Iteration 40/1000 | Loss: 0.00002305
Iteration 41/1000 | Loss: 0.00002305
Iteration 42/1000 | Loss: 0.00002305
Iteration 43/1000 | Loss: 0.00002304
Iteration 44/1000 | Loss: 0.00002304
Iteration 45/1000 | Loss: 0.00002304
Iteration 46/1000 | Loss: 0.00002303
Iteration 47/1000 | Loss: 0.00002303
Iteration 48/1000 | Loss: 0.00002302
Iteration 49/1000 | Loss: 0.00002302
Iteration 50/1000 | Loss: 0.00002301
Iteration 51/1000 | Loss: 0.00002301
Iteration 52/1000 | Loss: 0.00002301
Iteration 53/1000 | Loss: 0.00002301
Iteration 54/1000 | Loss: 0.00002301
Iteration 55/1000 | Loss: 0.00002300
Iteration 56/1000 | Loss: 0.00002300
Iteration 57/1000 | Loss: 0.00002300
Iteration 58/1000 | Loss: 0.00002299
Iteration 59/1000 | Loss: 0.00002299
Iteration 60/1000 | Loss: 0.00002298
Iteration 61/1000 | Loss: 0.00002296
Iteration 62/1000 | Loss: 0.00002296
Iteration 63/1000 | Loss: 0.00002296
Iteration 64/1000 | Loss: 0.00002296
Iteration 65/1000 | Loss: 0.00002296
Iteration 66/1000 | Loss: 0.00002296
Iteration 67/1000 | Loss: 0.00002296
Iteration 68/1000 | Loss: 0.00002295
Iteration 69/1000 | Loss: 0.00002295
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002294
Iteration 72/1000 | Loss: 0.00002294
Iteration 73/1000 | Loss: 0.00002293
Iteration 74/1000 | Loss: 0.00002293
Iteration 75/1000 | Loss: 0.00002293
Iteration 76/1000 | Loss: 0.00002293
Iteration 77/1000 | Loss: 0.00002292
Iteration 78/1000 | Loss: 0.00002292
Iteration 79/1000 | Loss: 0.00002292
Iteration 80/1000 | Loss: 0.00002292
Iteration 81/1000 | Loss: 0.00002291
Iteration 82/1000 | Loss: 0.00002291
Iteration 83/1000 | Loss: 0.00002290
Iteration 84/1000 | Loss: 0.00002289
Iteration 85/1000 | Loss: 0.00002289
Iteration 86/1000 | Loss: 0.00002289
Iteration 87/1000 | Loss: 0.00002288
Iteration 88/1000 | Loss: 0.00002288
Iteration 89/1000 | Loss: 0.00002288
Iteration 90/1000 | Loss: 0.00002287
Iteration 91/1000 | Loss: 0.00002287
Iteration 92/1000 | Loss: 0.00002287
Iteration 93/1000 | Loss: 0.00002286
Iteration 94/1000 | Loss: 0.00002286
Iteration 95/1000 | Loss: 0.00002286
Iteration 96/1000 | Loss: 0.00002285
Iteration 97/1000 | Loss: 0.00002285
Iteration 98/1000 | Loss: 0.00002285
Iteration 99/1000 | Loss: 0.00002285
Iteration 100/1000 | Loss: 0.00002284
Iteration 101/1000 | Loss: 0.00002284
Iteration 102/1000 | Loss: 0.00002284
Iteration 103/1000 | Loss: 0.00002284
Iteration 104/1000 | Loss: 0.00002284
Iteration 105/1000 | Loss: 0.00002283
Iteration 106/1000 | Loss: 0.00002283
Iteration 107/1000 | Loss: 0.00002283
Iteration 108/1000 | Loss: 0.00002283
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002282
Iteration 112/1000 | Loss: 0.00002282
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002280
Iteration 118/1000 | Loss: 0.00002280
Iteration 119/1000 | Loss: 0.00002280
Iteration 120/1000 | Loss: 0.00002279
Iteration 121/1000 | Loss: 0.00002279
Iteration 122/1000 | Loss: 0.00002279
Iteration 123/1000 | Loss: 0.00002279
Iteration 124/1000 | Loss: 0.00002279
Iteration 125/1000 | Loss: 0.00002279
Iteration 126/1000 | Loss: 0.00002279
Iteration 127/1000 | Loss: 0.00002279
Iteration 128/1000 | Loss: 0.00002279
Iteration 129/1000 | Loss: 0.00002279
Iteration 130/1000 | Loss: 0.00002278
Iteration 131/1000 | Loss: 0.00002278
Iteration 132/1000 | Loss: 0.00002278
Iteration 133/1000 | Loss: 0.00002278
Iteration 134/1000 | Loss: 0.00002278
Iteration 135/1000 | Loss: 0.00002278
Iteration 136/1000 | Loss: 0.00002278
Iteration 137/1000 | Loss: 0.00002278
Iteration 138/1000 | Loss: 0.00002278
Iteration 139/1000 | Loss: 0.00002278
Iteration 140/1000 | Loss: 0.00002277
Iteration 141/1000 | Loss: 0.00002277
Iteration 142/1000 | Loss: 0.00002277
Iteration 143/1000 | Loss: 0.00002277
Iteration 144/1000 | Loss: 0.00002276
Iteration 145/1000 | Loss: 0.00002276
Iteration 146/1000 | Loss: 0.00002276
Iteration 147/1000 | Loss: 0.00002276
Iteration 148/1000 | Loss: 0.00002276
Iteration 149/1000 | Loss: 0.00002276
Iteration 150/1000 | Loss: 0.00002276
Iteration 151/1000 | Loss: 0.00002276
Iteration 152/1000 | Loss: 0.00002276
Iteration 153/1000 | Loss: 0.00002275
Iteration 154/1000 | Loss: 0.00002275
Iteration 155/1000 | Loss: 0.00002275
Iteration 156/1000 | Loss: 0.00002275
Iteration 157/1000 | Loss: 0.00002274
Iteration 158/1000 | Loss: 0.00002274
Iteration 159/1000 | Loss: 0.00002274
Iteration 160/1000 | Loss: 0.00002274
Iteration 161/1000 | Loss: 0.00002274
Iteration 162/1000 | Loss: 0.00002274
Iteration 163/1000 | Loss: 0.00002274
Iteration 164/1000 | Loss: 0.00002274
Iteration 165/1000 | Loss: 0.00002274
Iteration 166/1000 | Loss: 0.00002274
Iteration 167/1000 | Loss: 0.00002274
Iteration 168/1000 | Loss: 0.00002273
Iteration 169/1000 | Loss: 0.00002273
Iteration 170/1000 | Loss: 0.00002273
Iteration 171/1000 | Loss: 0.00002273
Iteration 172/1000 | Loss: 0.00002273
Iteration 173/1000 | Loss: 0.00002273
Iteration 174/1000 | Loss: 0.00002273
Iteration 175/1000 | Loss: 0.00002273
Iteration 176/1000 | Loss: 0.00002273
Iteration 177/1000 | Loss: 0.00002273
Iteration 178/1000 | Loss: 0.00002272
Iteration 179/1000 | Loss: 0.00002272
Iteration 180/1000 | Loss: 0.00002272
Iteration 181/1000 | Loss: 0.00002272
Iteration 182/1000 | Loss: 0.00002272
Iteration 183/1000 | Loss: 0.00002272
Iteration 184/1000 | Loss: 0.00002272
Iteration 185/1000 | Loss: 0.00002272
Iteration 186/1000 | Loss: 0.00002272
Iteration 187/1000 | Loss: 0.00002272
Iteration 188/1000 | Loss: 0.00002272
Iteration 189/1000 | Loss: 0.00002272
Iteration 190/1000 | Loss: 0.00002272
Iteration 191/1000 | Loss: 0.00002271
Iteration 192/1000 | Loss: 0.00002271
Iteration 193/1000 | Loss: 0.00002271
Iteration 194/1000 | Loss: 0.00002271
Iteration 195/1000 | Loss: 0.00002271
Iteration 196/1000 | Loss: 0.00002271
Iteration 197/1000 | Loss: 0.00002271
Iteration 198/1000 | Loss: 0.00002271
Iteration 199/1000 | Loss: 0.00002271
Iteration 200/1000 | Loss: 0.00002271
Iteration 201/1000 | Loss: 0.00002270
Iteration 202/1000 | Loss: 0.00002270
Iteration 203/1000 | Loss: 0.00002270
Iteration 204/1000 | Loss: 0.00002270
Iteration 205/1000 | Loss: 0.00002270
Iteration 206/1000 | Loss: 0.00002270
Iteration 207/1000 | Loss: 0.00002270
Iteration 208/1000 | Loss: 0.00002270
Iteration 209/1000 | Loss: 0.00002270
Iteration 210/1000 | Loss: 0.00002270
Iteration 211/1000 | Loss: 0.00002270
Iteration 212/1000 | Loss: 0.00002270
Iteration 213/1000 | Loss: 0.00002270
Iteration 214/1000 | Loss: 0.00002270
Iteration 215/1000 | Loss: 0.00002270
Iteration 216/1000 | Loss: 0.00002270
Iteration 217/1000 | Loss: 0.00002270
Iteration 218/1000 | Loss: 0.00002270
Iteration 219/1000 | Loss: 0.00002270
Iteration 220/1000 | Loss: 0.00002270
Iteration 221/1000 | Loss: 0.00002270
Iteration 222/1000 | Loss: 0.00002270
Iteration 223/1000 | Loss: 0.00002270
Iteration 224/1000 | Loss: 0.00002270
Iteration 225/1000 | Loss: 0.00002270
Iteration 226/1000 | Loss: 0.00002270
Iteration 227/1000 | Loss: 0.00002270
Iteration 228/1000 | Loss: 0.00002270
Iteration 229/1000 | Loss: 0.00002270
Iteration 230/1000 | Loss: 0.00002270
Iteration 231/1000 | Loss: 0.00002270
Iteration 232/1000 | Loss: 0.00002270
Iteration 233/1000 | Loss: 0.00002270
Iteration 234/1000 | Loss: 0.00002270
Iteration 235/1000 | Loss: 0.00002270
Iteration 236/1000 | Loss: 0.00002270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.269890501338523e-05, 2.269890501338523e-05, 2.269890501338523e-05, 2.269890501338523e-05, 2.269890501338523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.269890501338523e-05

Optimization complete. Final v2v error: 4.007416725158691 mm

Highest mean error: 4.8961567878723145 mm for frame 59

Lowest mean error: 3.295635223388672 mm for frame 81

Saving results

Total time: 76.69934439659119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393014
Iteration 2/25 | Loss: 0.00138430
Iteration 3/25 | Loss: 0.00132329
Iteration 4/25 | Loss: 0.00131772
Iteration 5/25 | Loss: 0.00131759
Iteration 6/25 | Loss: 0.00131759
Iteration 7/25 | Loss: 0.00131759
Iteration 8/25 | Loss: 0.00131759
Iteration 9/25 | Loss: 0.00131759
Iteration 10/25 | Loss: 0.00131759
Iteration 11/25 | Loss: 0.00131759
Iteration 12/25 | Loss: 0.00131759
Iteration 13/25 | Loss: 0.00131759
Iteration 14/25 | Loss: 0.00131759
Iteration 15/25 | Loss: 0.00131759
Iteration 16/25 | Loss: 0.00131759
Iteration 17/25 | Loss: 0.00131759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013175936182960868, 0.0013175936182960868, 0.0013175936182960868, 0.0013175936182960868, 0.0013175936182960868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013175936182960868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51158512
Iteration 2/25 | Loss: 0.00203581
Iteration 3/25 | Loss: 0.00203581
Iteration 4/25 | Loss: 0.00203581
Iteration 5/25 | Loss: 0.00203581
Iteration 6/25 | Loss: 0.00203581
Iteration 7/25 | Loss: 0.00203581
Iteration 8/25 | Loss: 0.00203581
Iteration 9/25 | Loss: 0.00203581
Iteration 10/25 | Loss: 0.00203581
Iteration 11/25 | Loss: 0.00203581
Iteration 12/25 | Loss: 0.00203581
Iteration 13/25 | Loss: 0.00203581
Iteration 14/25 | Loss: 0.00203581
Iteration 15/25 | Loss: 0.00203581
Iteration 16/25 | Loss: 0.00203581
Iteration 17/25 | Loss: 0.00203581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020358096808195114, 0.0020358096808195114, 0.0020358096808195114, 0.0020358096808195114, 0.0020358096808195114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020358096808195114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203581
Iteration 2/1000 | Loss: 0.00002831
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001559
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001082
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001072
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001072
Iteration 18/1000 | Loss: 0.00001071
Iteration 19/1000 | Loss: 0.00001068
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001064
Iteration 22/1000 | Loss: 0.00001055
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001035
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001029
Iteration 27/1000 | Loss: 0.00001028
Iteration 28/1000 | Loss: 0.00001028
Iteration 29/1000 | Loss: 0.00001026
Iteration 30/1000 | Loss: 0.00001024
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001021
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001020
Iteration 37/1000 | Loss: 0.00001020
Iteration 38/1000 | Loss: 0.00001019
Iteration 39/1000 | Loss: 0.00001019
Iteration 40/1000 | Loss: 0.00001017
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001014
Iteration 43/1000 | Loss: 0.00001013
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001012
Iteration 47/1000 | Loss: 0.00001012
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001003
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000995
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000992
Iteration 84/1000 | Loss: 0.00000992
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000984
Iteration 99/1000 | Loss: 0.00000984
Iteration 100/1000 | Loss: 0.00000984
Iteration 101/1000 | Loss: 0.00000984
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000983
Iteration 107/1000 | Loss: 0.00000983
Iteration 108/1000 | Loss: 0.00000983
Iteration 109/1000 | Loss: 0.00000982
Iteration 110/1000 | Loss: 0.00000982
Iteration 111/1000 | Loss: 0.00000982
Iteration 112/1000 | Loss: 0.00000981
Iteration 113/1000 | Loss: 0.00000981
Iteration 114/1000 | Loss: 0.00000981
Iteration 115/1000 | Loss: 0.00000981
Iteration 116/1000 | Loss: 0.00000981
Iteration 117/1000 | Loss: 0.00000981
Iteration 118/1000 | Loss: 0.00000981
Iteration 119/1000 | Loss: 0.00000981
Iteration 120/1000 | Loss: 0.00000981
Iteration 121/1000 | Loss: 0.00000980
Iteration 122/1000 | Loss: 0.00000980
Iteration 123/1000 | Loss: 0.00000980
Iteration 124/1000 | Loss: 0.00000980
Iteration 125/1000 | Loss: 0.00000980
Iteration 126/1000 | Loss: 0.00000980
Iteration 127/1000 | Loss: 0.00000980
Iteration 128/1000 | Loss: 0.00000980
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000979
Iteration 131/1000 | Loss: 0.00000978
Iteration 132/1000 | Loss: 0.00000978
Iteration 133/1000 | Loss: 0.00000978
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000977
Iteration 136/1000 | Loss: 0.00000977
Iteration 137/1000 | Loss: 0.00000977
Iteration 138/1000 | Loss: 0.00000977
Iteration 139/1000 | Loss: 0.00000977
Iteration 140/1000 | Loss: 0.00000977
Iteration 141/1000 | Loss: 0.00000977
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000977
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [9.768196832737885e-06, 9.768196832737885e-06, 9.768196832737885e-06, 9.768196832737885e-06, 9.768196832737885e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.768196832737885e-06

Optimization complete. Final v2v error: 2.692659854888916 mm

Highest mean error: 2.824908971786499 mm for frame 115

Lowest mean error: 2.5392208099365234 mm for frame 200

Saving results

Total time: 45.29837989807129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730201
Iteration 2/25 | Loss: 0.00148847
Iteration 3/25 | Loss: 0.00135421
Iteration 4/25 | Loss: 0.00133653
Iteration 5/25 | Loss: 0.00132159
Iteration 6/25 | Loss: 0.00132576
Iteration 7/25 | Loss: 0.00131326
Iteration 8/25 | Loss: 0.00131044
Iteration 9/25 | Loss: 0.00130994
Iteration 10/25 | Loss: 0.00130991
Iteration 11/25 | Loss: 0.00130991
Iteration 12/25 | Loss: 0.00130990
Iteration 13/25 | Loss: 0.00130990
Iteration 14/25 | Loss: 0.00130990
Iteration 15/25 | Loss: 0.00130990
Iteration 16/25 | Loss: 0.00130990
Iteration 17/25 | Loss: 0.00130990
Iteration 18/25 | Loss: 0.00130990
Iteration 19/25 | Loss: 0.00130990
Iteration 20/25 | Loss: 0.00130990
Iteration 21/25 | Loss: 0.00130990
Iteration 22/25 | Loss: 0.00130990
Iteration 23/25 | Loss: 0.00130990
Iteration 24/25 | Loss: 0.00130989
Iteration 25/25 | Loss: 0.00130989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66562533
Iteration 2/25 | Loss: 0.00219421
Iteration 3/25 | Loss: 0.00219421
Iteration 4/25 | Loss: 0.00219421
Iteration 5/25 | Loss: 0.00219421
Iteration 6/25 | Loss: 0.00219421
Iteration 7/25 | Loss: 0.00219421
Iteration 8/25 | Loss: 0.00219421
Iteration 9/25 | Loss: 0.00219421
Iteration 10/25 | Loss: 0.00219421
Iteration 11/25 | Loss: 0.00219421
Iteration 12/25 | Loss: 0.00219421
Iteration 13/25 | Loss: 0.00219421
Iteration 14/25 | Loss: 0.00219421
Iteration 15/25 | Loss: 0.00219421
Iteration 16/25 | Loss: 0.00219421
Iteration 17/25 | Loss: 0.00219421
Iteration 18/25 | Loss: 0.00219421
Iteration 19/25 | Loss: 0.00219421
Iteration 20/25 | Loss: 0.00219421
Iteration 21/25 | Loss: 0.00219421
Iteration 22/25 | Loss: 0.00219421
Iteration 23/25 | Loss: 0.00219421
Iteration 24/25 | Loss: 0.00219421
Iteration 25/25 | Loss: 0.00219421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219421
Iteration 2/1000 | Loss: 0.00001955
Iteration 3/1000 | Loss: 0.00001580
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001224
Iteration 9/1000 | Loss: 0.00001188
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001158
Iteration 13/1000 | Loss: 0.00001150
Iteration 14/1000 | Loss: 0.00001148
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001132
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001106
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001097
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001092
Iteration 31/1000 | Loss: 0.00001091
Iteration 32/1000 | Loss: 0.00001091
Iteration 33/1000 | Loss: 0.00001090
Iteration 34/1000 | Loss: 0.00001090
Iteration 35/1000 | Loss: 0.00001089
Iteration 36/1000 | Loss: 0.00001089
Iteration 37/1000 | Loss: 0.00001088
Iteration 38/1000 | Loss: 0.00001087
Iteration 39/1000 | Loss: 0.00001085
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001085
Iteration 42/1000 | Loss: 0.00001085
Iteration 43/1000 | Loss: 0.00001085
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001085
Iteration 46/1000 | Loss: 0.00001085
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001084
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001081
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001077
Iteration 69/1000 | Loss: 0.00001077
Iteration 70/1000 | Loss: 0.00001077
Iteration 71/1000 | Loss: 0.00001077
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001074
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001070
Iteration 95/1000 | Loss: 0.00001070
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001068
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001065
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001065
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001064
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001064
Iteration 129/1000 | Loss: 0.00001064
Iteration 130/1000 | Loss: 0.00001063
Iteration 131/1000 | Loss: 0.00001063
Iteration 132/1000 | Loss: 0.00001063
Iteration 133/1000 | Loss: 0.00001063
Iteration 134/1000 | Loss: 0.00001063
Iteration 135/1000 | Loss: 0.00001063
Iteration 136/1000 | Loss: 0.00001063
Iteration 137/1000 | Loss: 0.00001062
Iteration 138/1000 | Loss: 0.00001062
Iteration 139/1000 | Loss: 0.00001062
Iteration 140/1000 | Loss: 0.00001062
Iteration 141/1000 | Loss: 0.00001062
Iteration 142/1000 | Loss: 0.00001062
Iteration 143/1000 | Loss: 0.00001061
Iteration 144/1000 | Loss: 0.00001061
Iteration 145/1000 | Loss: 0.00001061
Iteration 146/1000 | Loss: 0.00001061
Iteration 147/1000 | Loss: 0.00001061
Iteration 148/1000 | Loss: 0.00001061
Iteration 149/1000 | Loss: 0.00001061
Iteration 150/1000 | Loss: 0.00001061
Iteration 151/1000 | Loss: 0.00001061
Iteration 152/1000 | Loss: 0.00001061
Iteration 153/1000 | Loss: 0.00001060
Iteration 154/1000 | Loss: 0.00001060
Iteration 155/1000 | Loss: 0.00001060
Iteration 156/1000 | Loss: 0.00001060
Iteration 157/1000 | Loss: 0.00001060
Iteration 158/1000 | Loss: 0.00001060
Iteration 159/1000 | Loss: 0.00001060
Iteration 160/1000 | Loss: 0.00001060
Iteration 161/1000 | Loss: 0.00001060
Iteration 162/1000 | Loss: 0.00001059
Iteration 163/1000 | Loss: 0.00001059
Iteration 164/1000 | Loss: 0.00001059
Iteration 165/1000 | Loss: 0.00001059
Iteration 166/1000 | Loss: 0.00001059
Iteration 167/1000 | Loss: 0.00001059
Iteration 168/1000 | Loss: 0.00001059
Iteration 169/1000 | Loss: 0.00001059
Iteration 170/1000 | Loss: 0.00001059
Iteration 171/1000 | Loss: 0.00001059
Iteration 172/1000 | Loss: 0.00001059
Iteration 173/1000 | Loss: 0.00001059
Iteration 174/1000 | Loss: 0.00001059
Iteration 175/1000 | Loss: 0.00001059
Iteration 176/1000 | Loss: 0.00001059
Iteration 177/1000 | Loss: 0.00001059
Iteration 178/1000 | Loss: 0.00001058
Iteration 179/1000 | Loss: 0.00001058
Iteration 180/1000 | Loss: 0.00001058
Iteration 181/1000 | Loss: 0.00001058
Iteration 182/1000 | Loss: 0.00001058
Iteration 183/1000 | Loss: 0.00001058
Iteration 184/1000 | Loss: 0.00001058
Iteration 185/1000 | Loss: 0.00001058
Iteration 186/1000 | Loss: 0.00001058
Iteration 187/1000 | Loss: 0.00001058
Iteration 188/1000 | Loss: 0.00001058
Iteration 189/1000 | Loss: 0.00001058
Iteration 190/1000 | Loss: 0.00001058
Iteration 191/1000 | Loss: 0.00001058
Iteration 192/1000 | Loss: 0.00001058
Iteration 193/1000 | Loss: 0.00001058
Iteration 194/1000 | Loss: 0.00001058
Iteration 195/1000 | Loss: 0.00001058
Iteration 196/1000 | Loss: 0.00001058
Iteration 197/1000 | Loss: 0.00001058
Iteration 198/1000 | Loss: 0.00001058
Iteration 199/1000 | Loss: 0.00001058
Iteration 200/1000 | Loss: 0.00001058
Iteration 201/1000 | Loss: 0.00001058
Iteration 202/1000 | Loss: 0.00001058
Iteration 203/1000 | Loss: 0.00001058
Iteration 204/1000 | Loss: 0.00001058
Iteration 205/1000 | Loss: 0.00001058
Iteration 206/1000 | Loss: 0.00001058
Iteration 207/1000 | Loss: 0.00001058
Iteration 208/1000 | Loss: 0.00001058
Iteration 209/1000 | Loss: 0.00001058
Iteration 210/1000 | Loss: 0.00001058
Iteration 211/1000 | Loss: 0.00001058
Iteration 212/1000 | Loss: 0.00001058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.0580828529782593e-05, 1.0580828529782593e-05, 1.0580828529782593e-05, 1.0580828529782593e-05, 1.0580828529782593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0580828529782593e-05

Optimization complete. Final v2v error: 2.852252244949341 mm

Highest mean error: 3.0556135177612305 mm for frame 104

Lowest mean error: 2.7249701023101807 mm for frame 14

Saving results

Total time: 51.66378045082092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389147
Iteration 2/25 | Loss: 0.00137971
Iteration 3/25 | Loss: 0.00133181
Iteration 4/25 | Loss: 0.00132707
Iteration 5/25 | Loss: 0.00132627
Iteration 6/25 | Loss: 0.00132627
Iteration 7/25 | Loss: 0.00132627
Iteration 8/25 | Loss: 0.00132627
Iteration 9/25 | Loss: 0.00132627
Iteration 10/25 | Loss: 0.00132627
Iteration 11/25 | Loss: 0.00132627
Iteration 12/25 | Loss: 0.00132627
Iteration 13/25 | Loss: 0.00132627
Iteration 14/25 | Loss: 0.00132627
Iteration 15/25 | Loss: 0.00132627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013262664433568716, 0.0013262664433568716, 0.0013262664433568716, 0.0013262664433568716, 0.0013262664433568716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013262664433568716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26500893
Iteration 2/25 | Loss: 0.00204771
Iteration 3/25 | Loss: 0.00204771
Iteration 4/25 | Loss: 0.00204771
Iteration 5/25 | Loss: 0.00204771
Iteration 6/25 | Loss: 0.00204771
Iteration 7/25 | Loss: 0.00204771
Iteration 8/25 | Loss: 0.00204771
Iteration 9/25 | Loss: 0.00204771
Iteration 10/25 | Loss: 0.00204771
Iteration 11/25 | Loss: 0.00204771
Iteration 12/25 | Loss: 0.00204771
Iteration 13/25 | Loss: 0.00204771
Iteration 14/25 | Loss: 0.00204771
Iteration 15/25 | Loss: 0.00204771
Iteration 16/25 | Loss: 0.00204771
Iteration 17/25 | Loss: 0.00204771
Iteration 18/25 | Loss: 0.00204771
Iteration 19/25 | Loss: 0.00204771
Iteration 20/25 | Loss: 0.00204771
Iteration 21/25 | Loss: 0.00204771
Iteration 22/25 | Loss: 0.00204771
Iteration 23/25 | Loss: 0.00204771
Iteration 24/25 | Loss: 0.00204771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020477089565247297, 0.0020477089565247297, 0.0020477089565247297, 0.0020477089565247297, 0.0020477089565247297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020477089565247297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204771
Iteration 2/1000 | Loss: 0.00002502
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001337
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001161
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001157
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001153
Iteration 71/1000 | Loss: 0.00001153
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.152992535935482e-05, 1.152992535935482e-05, 1.152992535935482e-05, 1.152992535935482e-05, 1.152992535935482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.152992535935482e-05

Optimization complete. Final v2v error: 2.917480707168579 mm

Highest mean error: 3.1307480335235596 mm for frame 58

Lowest mean error: 2.7498068809509277 mm for frame 68

Saving results

Total time: 31.907403707504272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424428
Iteration 2/25 | Loss: 0.00153729
Iteration 3/25 | Loss: 0.00140044
Iteration 4/25 | Loss: 0.00138280
Iteration 5/25 | Loss: 0.00137872
Iteration 6/25 | Loss: 0.00137748
Iteration 7/25 | Loss: 0.00137711
Iteration 8/25 | Loss: 0.00137711
Iteration 9/25 | Loss: 0.00137711
Iteration 10/25 | Loss: 0.00137711
Iteration 11/25 | Loss: 0.00137711
Iteration 12/25 | Loss: 0.00137711
Iteration 13/25 | Loss: 0.00137711
Iteration 14/25 | Loss: 0.00137711
Iteration 15/25 | Loss: 0.00137711
Iteration 16/25 | Loss: 0.00137711
Iteration 17/25 | Loss: 0.00137711
Iteration 18/25 | Loss: 0.00137711
Iteration 19/25 | Loss: 0.00137711
Iteration 20/25 | Loss: 0.00137711
Iteration 21/25 | Loss: 0.00137711
Iteration 22/25 | Loss: 0.00137711
Iteration 23/25 | Loss: 0.00137711
Iteration 24/25 | Loss: 0.00137711
Iteration 25/25 | Loss: 0.00137711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.76554823
Iteration 2/25 | Loss: 0.00190102
Iteration 3/25 | Loss: 0.00190100
Iteration 4/25 | Loss: 0.00190100
Iteration 5/25 | Loss: 0.00190100
Iteration 6/25 | Loss: 0.00190100
Iteration 7/25 | Loss: 0.00190100
Iteration 8/25 | Loss: 0.00190100
Iteration 9/25 | Loss: 0.00190100
Iteration 10/25 | Loss: 0.00190100
Iteration 11/25 | Loss: 0.00190100
Iteration 12/25 | Loss: 0.00190100
Iteration 13/25 | Loss: 0.00190100
Iteration 14/25 | Loss: 0.00190100
Iteration 15/25 | Loss: 0.00190100
Iteration 16/25 | Loss: 0.00190100
Iteration 17/25 | Loss: 0.00190100
Iteration 18/25 | Loss: 0.00190100
Iteration 19/25 | Loss: 0.00190100
Iteration 20/25 | Loss: 0.00190100
Iteration 21/25 | Loss: 0.00190100
Iteration 22/25 | Loss: 0.00190100
Iteration 23/25 | Loss: 0.00190100
Iteration 24/25 | Loss: 0.00190100
Iteration 25/25 | Loss: 0.00190100

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190100
Iteration 2/1000 | Loss: 0.00003441
Iteration 3/1000 | Loss: 0.00002570
Iteration 4/1000 | Loss: 0.00002421
Iteration 5/1000 | Loss: 0.00002321
Iteration 6/1000 | Loss: 0.00002254
Iteration 7/1000 | Loss: 0.00002190
Iteration 8/1000 | Loss: 0.00002130
Iteration 9/1000 | Loss: 0.00002084
Iteration 10/1000 | Loss: 0.00002044
Iteration 11/1000 | Loss: 0.00002015
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001943
Iteration 18/1000 | Loss: 0.00001941
Iteration 19/1000 | Loss: 0.00001937
Iteration 20/1000 | Loss: 0.00001937
Iteration 21/1000 | Loss: 0.00001936
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001935
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001933
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001930
Iteration 44/1000 | Loss: 0.00001930
Iteration 45/1000 | Loss: 0.00001930
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001928
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001927
Iteration 53/1000 | Loss: 0.00001927
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001926
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001921
Iteration 82/1000 | Loss: 0.00001921
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001921
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001920
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001920
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001919
Iteration 96/1000 | Loss: 0.00001919
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001919
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001918
Iteration 113/1000 | Loss: 0.00001918
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001917
Iteration 132/1000 | Loss: 0.00001917
Iteration 133/1000 | Loss: 0.00001917
Iteration 134/1000 | Loss: 0.00001917
Iteration 135/1000 | Loss: 0.00001917
Iteration 136/1000 | Loss: 0.00001917
Iteration 137/1000 | Loss: 0.00001917
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001916
Iteration 140/1000 | Loss: 0.00001916
Iteration 141/1000 | Loss: 0.00001916
Iteration 142/1000 | Loss: 0.00001916
Iteration 143/1000 | Loss: 0.00001916
Iteration 144/1000 | Loss: 0.00001916
Iteration 145/1000 | Loss: 0.00001916
Iteration 146/1000 | Loss: 0.00001916
Iteration 147/1000 | Loss: 0.00001916
Iteration 148/1000 | Loss: 0.00001915
Iteration 149/1000 | Loss: 0.00001915
Iteration 150/1000 | Loss: 0.00001915
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001915
Iteration 153/1000 | Loss: 0.00001915
Iteration 154/1000 | Loss: 0.00001915
Iteration 155/1000 | Loss: 0.00001915
Iteration 156/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.9154233086737804e-05, 1.9154233086737804e-05, 1.9154233086737804e-05, 1.9154233086737804e-05, 1.9154233086737804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9154233086737804e-05

Optimization complete. Final v2v error: 3.731994390487671 mm

Highest mean error: 4.336557388305664 mm for frame 49

Lowest mean error: 3.3831613063812256 mm for frame 0

Saving results

Total time: 42.52240967750549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600402
Iteration 2/25 | Loss: 0.00181894
Iteration 3/25 | Loss: 0.00148571
Iteration 4/25 | Loss: 0.00147036
Iteration 5/25 | Loss: 0.00146578
Iteration 6/25 | Loss: 0.00146468
Iteration 7/25 | Loss: 0.00146468
Iteration 8/25 | Loss: 0.00146468
Iteration 9/25 | Loss: 0.00146468
Iteration 10/25 | Loss: 0.00146468
Iteration 11/25 | Loss: 0.00146468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014646841445937753, 0.0014646841445937753, 0.0014646841445937753, 0.0014646841445937753, 0.0014646841445937753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014646841445937753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85827321
Iteration 2/25 | Loss: 0.00204104
Iteration 3/25 | Loss: 0.00204103
Iteration 4/25 | Loss: 0.00204103
Iteration 5/25 | Loss: 0.00204103
Iteration 6/25 | Loss: 0.00204103
Iteration 7/25 | Loss: 0.00204103
Iteration 8/25 | Loss: 0.00204103
Iteration 9/25 | Loss: 0.00204103
Iteration 10/25 | Loss: 0.00204103
Iteration 11/25 | Loss: 0.00204103
Iteration 12/25 | Loss: 0.00204103
Iteration 13/25 | Loss: 0.00204103
Iteration 14/25 | Loss: 0.00204103
Iteration 15/25 | Loss: 0.00204103
Iteration 16/25 | Loss: 0.00204103
Iteration 17/25 | Loss: 0.00204103
Iteration 18/25 | Loss: 0.00204103
Iteration 19/25 | Loss: 0.00204103
Iteration 20/25 | Loss: 0.00204103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002041029278188944, 0.002041029278188944, 0.002041029278188944, 0.002041029278188944, 0.002041029278188944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002041029278188944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204103
Iteration 2/1000 | Loss: 0.00005774
Iteration 3/1000 | Loss: 0.00003708
Iteration 4/1000 | Loss: 0.00003327
Iteration 5/1000 | Loss: 0.00003154
Iteration 6/1000 | Loss: 0.00003068
Iteration 7/1000 | Loss: 0.00003013
Iteration 8/1000 | Loss: 0.00002952
Iteration 9/1000 | Loss: 0.00002910
Iteration 10/1000 | Loss: 0.00002871
Iteration 11/1000 | Loss: 0.00002832
Iteration 12/1000 | Loss: 0.00002800
Iteration 13/1000 | Loss: 0.00002770
Iteration 14/1000 | Loss: 0.00002742
Iteration 15/1000 | Loss: 0.00002716
Iteration 16/1000 | Loss: 0.00002696
Iteration 17/1000 | Loss: 0.00002675
Iteration 18/1000 | Loss: 0.00002659
Iteration 19/1000 | Loss: 0.00002650
Iteration 20/1000 | Loss: 0.00002644
Iteration 21/1000 | Loss: 0.00002643
Iteration 22/1000 | Loss: 0.00002642
Iteration 23/1000 | Loss: 0.00002638
Iteration 24/1000 | Loss: 0.00002632
Iteration 25/1000 | Loss: 0.00002631
Iteration 26/1000 | Loss: 0.00002629
Iteration 27/1000 | Loss: 0.00002628
Iteration 28/1000 | Loss: 0.00002628
Iteration 29/1000 | Loss: 0.00002626
Iteration 30/1000 | Loss: 0.00002626
Iteration 31/1000 | Loss: 0.00002625
Iteration 32/1000 | Loss: 0.00002625
Iteration 33/1000 | Loss: 0.00002625
Iteration 34/1000 | Loss: 0.00002625
Iteration 35/1000 | Loss: 0.00002625
Iteration 36/1000 | Loss: 0.00002625
Iteration 37/1000 | Loss: 0.00002625
Iteration 38/1000 | Loss: 0.00002625
Iteration 39/1000 | Loss: 0.00002625
Iteration 40/1000 | Loss: 0.00002625
Iteration 41/1000 | Loss: 0.00002625
Iteration 42/1000 | Loss: 0.00002625
Iteration 43/1000 | Loss: 0.00002624
Iteration 44/1000 | Loss: 0.00002624
Iteration 45/1000 | Loss: 0.00002624
Iteration 46/1000 | Loss: 0.00002624
Iteration 47/1000 | Loss: 0.00002624
Iteration 48/1000 | Loss: 0.00002624
Iteration 49/1000 | Loss: 0.00002624
Iteration 50/1000 | Loss: 0.00002624
Iteration 51/1000 | Loss: 0.00002623
Iteration 52/1000 | Loss: 0.00002623
Iteration 53/1000 | Loss: 0.00002623
Iteration 54/1000 | Loss: 0.00002623
Iteration 55/1000 | Loss: 0.00002623
Iteration 56/1000 | Loss: 0.00002622
Iteration 57/1000 | Loss: 0.00002622
Iteration 58/1000 | Loss: 0.00002622
Iteration 59/1000 | Loss: 0.00002622
Iteration 60/1000 | Loss: 0.00002622
Iteration 61/1000 | Loss: 0.00002621
Iteration 62/1000 | Loss: 0.00002621
Iteration 63/1000 | Loss: 0.00002621
Iteration 64/1000 | Loss: 0.00002621
Iteration 65/1000 | Loss: 0.00002620
Iteration 66/1000 | Loss: 0.00002620
Iteration 67/1000 | Loss: 0.00002620
Iteration 68/1000 | Loss: 0.00002620
Iteration 69/1000 | Loss: 0.00002620
Iteration 70/1000 | Loss: 0.00002619
Iteration 71/1000 | Loss: 0.00002619
Iteration 72/1000 | Loss: 0.00002619
Iteration 73/1000 | Loss: 0.00002619
Iteration 74/1000 | Loss: 0.00002619
Iteration 75/1000 | Loss: 0.00002619
Iteration 76/1000 | Loss: 0.00002619
Iteration 77/1000 | Loss: 0.00002619
Iteration 78/1000 | Loss: 0.00002619
Iteration 79/1000 | Loss: 0.00002619
Iteration 80/1000 | Loss: 0.00002619
Iteration 81/1000 | Loss: 0.00002618
Iteration 82/1000 | Loss: 0.00002618
Iteration 83/1000 | Loss: 0.00002618
Iteration 84/1000 | Loss: 0.00002618
Iteration 85/1000 | Loss: 0.00002618
Iteration 86/1000 | Loss: 0.00002618
Iteration 87/1000 | Loss: 0.00002618
Iteration 88/1000 | Loss: 0.00002618
Iteration 89/1000 | Loss: 0.00002618
Iteration 90/1000 | Loss: 0.00002618
Iteration 91/1000 | Loss: 0.00002618
Iteration 92/1000 | Loss: 0.00002618
Iteration 93/1000 | Loss: 0.00002618
Iteration 94/1000 | Loss: 0.00002618
Iteration 95/1000 | Loss: 0.00002618
Iteration 96/1000 | Loss: 0.00002618
Iteration 97/1000 | Loss: 0.00002618
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002618
Iteration 100/1000 | Loss: 0.00002618
Iteration 101/1000 | Loss: 0.00002618
Iteration 102/1000 | Loss: 0.00002618
Iteration 103/1000 | Loss: 0.00002618
Iteration 104/1000 | Loss: 0.00002618
Iteration 105/1000 | Loss: 0.00002618
Iteration 106/1000 | Loss: 0.00002618
Iteration 107/1000 | Loss: 0.00002618
Iteration 108/1000 | Loss: 0.00002618
Iteration 109/1000 | Loss: 0.00002618
Iteration 110/1000 | Loss: 0.00002618
Iteration 111/1000 | Loss: 0.00002618
Iteration 112/1000 | Loss: 0.00002618
Iteration 113/1000 | Loss: 0.00002618
Iteration 114/1000 | Loss: 0.00002618
Iteration 115/1000 | Loss: 0.00002618
Iteration 116/1000 | Loss: 0.00002618
Iteration 117/1000 | Loss: 0.00002618
Iteration 118/1000 | Loss: 0.00002618
Iteration 119/1000 | Loss: 0.00002618
Iteration 120/1000 | Loss: 0.00002618
Iteration 121/1000 | Loss: 0.00002618
Iteration 122/1000 | Loss: 0.00002618
Iteration 123/1000 | Loss: 0.00002618
Iteration 124/1000 | Loss: 0.00002618
Iteration 125/1000 | Loss: 0.00002618
Iteration 126/1000 | Loss: 0.00002618
Iteration 127/1000 | Loss: 0.00002618
Iteration 128/1000 | Loss: 0.00002618
Iteration 129/1000 | Loss: 0.00002618
Iteration 130/1000 | Loss: 0.00002618
Iteration 131/1000 | Loss: 0.00002618
Iteration 132/1000 | Loss: 0.00002618
Iteration 133/1000 | Loss: 0.00002618
Iteration 134/1000 | Loss: 0.00002618
Iteration 135/1000 | Loss: 0.00002618
Iteration 136/1000 | Loss: 0.00002618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.6181660359725356e-05, 2.6181660359725356e-05, 2.6181660359725356e-05, 2.6181660359725356e-05, 2.6181660359725356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6181660359725356e-05

Optimization complete. Final v2v error: 3.993924140930176 mm

Highest mean error: 4.820347785949707 mm for frame 97

Lowest mean error: 3.1874983310699463 mm for frame 36

Saving results

Total time: 42.86832022666931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423126
Iteration 2/25 | Loss: 0.00138755
Iteration 3/25 | Loss: 0.00132143
Iteration 4/25 | Loss: 0.00131213
Iteration 5/25 | Loss: 0.00130914
Iteration 6/25 | Loss: 0.00130855
Iteration 7/25 | Loss: 0.00130855
Iteration 8/25 | Loss: 0.00130855
Iteration 9/25 | Loss: 0.00130855
Iteration 10/25 | Loss: 0.00130855
Iteration 11/25 | Loss: 0.00130855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013085544342175126, 0.0013085544342175126, 0.0013085544342175126, 0.0013085544342175126, 0.0013085544342175126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013085544342175126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02256405
Iteration 2/25 | Loss: 0.00201334
Iteration 3/25 | Loss: 0.00201334
Iteration 4/25 | Loss: 0.00201333
Iteration 5/25 | Loss: 0.00201333
Iteration 6/25 | Loss: 0.00201333
Iteration 7/25 | Loss: 0.00201333
Iteration 8/25 | Loss: 0.00201333
Iteration 9/25 | Loss: 0.00201333
Iteration 10/25 | Loss: 0.00201333
Iteration 11/25 | Loss: 0.00201333
Iteration 12/25 | Loss: 0.00201333
Iteration 13/25 | Loss: 0.00201333
Iteration 14/25 | Loss: 0.00201333
Iteration 15/25 | Loss: 0.00201333
Iteration 16/25 | Loss: 0.00201333
Iteration 17/25 | Loss: 0.00201333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002013332676142454, 0.002013332676142454, 0.002013332676142454, 0.002013332676142454, 0.002013332676142454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002013332676142454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201333
Iteration 2/1000 | Loss: 0.00003687
Iteration 3/1000 | Loss: 0.00002242
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001572
Iteration 8/1000 | Loss: 0.00001512
Iteration 9/1000 | Loss: 0.00001467
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001282
Iteration 17/1000 | Loss: 0.00001281
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001274
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001269
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00001262
Iteration 54/1000 | Loss: 0.00001261
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001261
Iteration 57/1000 | Loss: 0.00001260
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001260
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001260
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001251
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001250
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001249
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001246
Iteration 93/1000 | Loss: 0.00001246
Iteration 94/1000 | Loss: 0.00001246
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001245
Iteration 102/1000 | Loss: 0.00001245
Iteration 103/1000 | Loss: 0.00001245
Iteration 104/1000 | Loss: 0.00001245
Iteration 105/1000 | Loss: 0.00001244
Iteration 106/1000 | Loss: 0.00001244
Iteration 107/1000 | Loss: 0.00001243
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001235
Iteration 130/1000 | Loss: 0.00001235
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001234
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001234
Iteration 137/1000 | Loss: 0.00001233
Iteration 138/1000 | Loss: 0.00001233
Iteration 139/1000 | Loss: 0.00001233
Iteration 140/1000 | Loss: 0.00001233
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Iteration 149/1000 | Loss: 0.00001233
Iteration 150/1000 | Loss: 0.00001233
Iteration 151/1000 | Loss: 0.00001232
Iteration 152/1000 | Loss: 0.00001232
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001231
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001231
Iteration 157/1000 | Loss: 0.00001231
Iteration 158/1000 | Loss: 0.00001231
Iteration 159/1000 | Loss: 0.00001231
Iteration 160/1000 | Loss: 0.00001231
Iteration 161/1000 | Loss: 0.00001231
Iteration 162/1000 | Loss: 0.00001230
Iteration 163/1000 | Loss: 0.00001230
Iteration 164/1000 | Loss: 0.00001230
Iteration 165/1000 | Loss: 0.00001230
Iteration 166/1000 | Loss: 0.00001230
Iteration 167/1000 | Loss: 0.00001230
Iteration 168/1000 | Loss: 0.00001230
Iteration 169/1000 | Loss: 0.00001230
Iteration 170/1000 | Loss: 0.00001230
Iteration 171/1000 | Loss: 0.00001230
Iteration 172/1000 | Loss: 0.00001230
Iteration 173/1000 | Loss: 0.00001230
Iteration 174/1000 | Loss: 0.00001229
Iteration 175/1000 | Loss: 0.00001229
Iteration 176/1000 | Loss: 0.00001229
Iteration 177/1000 | Loss: 0.00001229
Iteration 178/1000 | Loss: 0.00001229
Iteration 179/1000 | Loss: 0.00001229
Iteration 180/1000 | Loss: 0.00001229
Iteration 181/1000 | Loss: 0.00001229
Iteration 182/1000 | Loss: 0.00001229
Iteration 183/1000 | Loss: 0.00001229
Iteration 184/1000 | Loss: 0.00001229
Iteration 185/1000 | Loss: 0.00001229
Iteration 186/1000 | Loss: 0.00001228
Iteration 187/1000 | Loss: 0.00001228
Iteration 188/1000 | Loss: 0.00001228
Iteration 189/1000 | Loss: 0.00001228
Iteration 190/1000 | Loss: 0.00001228
Iteration 191/1000 | Loss: 0.00001228
Iteration 192/1000 | Loss: 0.00001228
Iteration 193/1000 | Loss: 0.00001228
Iteration 194/1000 | Loss: 0.00001228
Iteration 195/1000 | Loss: 0.00001228
Iteration 196/1000 | Loss: 0.00001228
Iteration 197/1000 | Loss: 0.00001228
Iteration 198/1000 | Loss: 0.00001227
Iteration 199/1000 | Loss: 0.00001227
Iteration 200/1000 | Loss: 0.00001227
Iteration 201/1000 | Loss: 0.00001227
Iteration 202/1000 | Loss: 0.00001227
Iteration 203/1000 | Loss: 0.00001227
Iteration 204/1000 | Loss: 0.00001227
Iteration 205/1000 | Loss: 0.00001227
Iteration 206/1000 | Loss: 0.00001227
Iteration 207/1000 | Loss: 0.00001227
Iteration 208/1000 | Loss: 0.00001227
Iteration 209/1000 | Loss: 0.00001227
Iteration 210/1000 | Loss: 0.00001227
Iteration 211/1000 | Loss: 0.00001227
Iteration 212/1000 | Loss: 0.00001227
Iteration 213/1000 | Loss: 0.00001227
Iteration 214/1000 | Loss: 0.00001227
Iteration 215/1000 | Loss: 0.00001227
Iteration 216/1000 | Loss: 0.00001227
Iteration 217/1000 | Loss: 0.00001227
Iteration 218/1000 | Loss: 0.00001227
Iteration 219/1000 | Loss: 0.00001227
Iteration 220/1000 | Loss: 0.00001227
Iteration 221/1000 | Loss: 0.00001227
Iteration 222/1000 | Loss: 0.00001227
Iteration 223/1000 | Loss: 0.00001227
Iteration 224/1000 | Loss: 0.00001227
Iteration 225/1000 | Loss: 0.00001227
Iteration 226/1000 | Loss: 0.00001227
Iteration 227/1000 | Loss: 0.00001227
Iteration 228/1000 | Loss: 0.00001227
Iteration 229/1000 | Loss: 0.00001227
Iteration 230/1000 | Loss: 0.00001227
Iteration 231/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.227127813763218e-05, 1.227127813763218e-05, 1.227127813763218e-05, 1.227127813763218e-05, 1.227127813763218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.227127813763218e-05

Optimization complete. Final v2v error: 3.041365623474121 mm

Highest mean error: 3.059107542037964 mm for frame 62

Lowest mean error: 3.0290472507476807 mm for frame 117

Saving results

Total time: 43.728744983673096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930302
Iteration 2/25 | Loss: 0.00173459
Iteration 3/25 | Loss: 0.00144642
Iteration 4/25 | Loss: 0.00141262
Iteration 5/25 | Loss: 0.00142753
Iteration 6/25 | Loss: 0.00141598
Iteration 7/25 | Loss: 0.00142619
Iteration 8/25 | Loss: 0.00140624
Iteration 9/25 | Loss: 0.00138383
Iteration 10/25 | Loss: 0.00139056
Iteration 11/25 | Loss: 0.00138558
Iteration 12/25 | Loss: 0.00137439
Iteration 13/25 | Loss: 0.00137100
Iteration 14/25 | Loss: 0.00137052
Iteration 15/25 | Loss: 0.00137040
Iteration 16/25 | Loss: 0.00137040
Iteration 17/25 | Loss: 0.00137040
Iteration 18/25 | Loss: 0.00137040
Iteration 19/25 | Loss: 0.00137040
Iteration 20/25 | Loss: 0.00137040
Iteration 21/25 | Loss: 0.00137039
Iteration 22/25 | Loss: 0.00137039
Iteration 23/25 | Loss: 0.00137039
Iteration 24/25 | Loss: 0.00137039
Iteration 25/25 | Loss: 0.00137039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81075025
Iteration 2/25 | Loss: 0.00186927
Iteration 3/25 | Loss: 0.00186925
Iteration 4/25 | Loss: 0.00186925
Iteration 5/25 | Loss: 0.00186925
Iteration 6/25 | Loss: 0.00186925
Iteration 7/25 | Loss: 0.00186925
Iteration 8/25 | Loss: 0.00186924
Iteration 9/25 | Loss: 0.00186924
Iteration 10/25 | Loss: 0.00186924
Iteration 11/25 | Loss: 0.00186924
Iteration 12/25 | Loss: 0.00186924
Iteration 13/25 | Loss: 0.00186924
Iteration 14/25 | Loss: 0.00186924
Iteration 15/25 | Loss: 0.00186924
Iteration 16/25 | Loss: 0.00186924
Iteration 17/25 | Loss: 0.00186924
Iteration 18/25 | Loss: 0.00186924
Iteration 19/25 | Loss: 0.00186924
Iteration 20/25 | Loss: 0.00186924
Iteration 21/25 | Loss: 0.00186924
Iteration 22/25 | Loss: 0.00186924
Iteration 23/25 | Loss: 0.00186924
Iteration 24/25 | Loss: 0.00186924
Iteration 25/25 | Loss: 0.00186924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186924
Iteration 2/1000 | Loss: 0.00003027
Iteration 3/1000 | Loss: 0.00002047
Iteration 4/1000 | Loss: 0.00001827
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00070795
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001585
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001470
Iteration 17/1000 | Loss: 0.00001469
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001457
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001442
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001434
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001428
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001426
Iteration 52/1000 | Loss: 0.00001425
Iteration 53/1000 | Loss: 0.00001425
Iteration 54/1000 | Loss: 0.00001425
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001416
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001414
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001413
Iteration 92/1000 | Loss: 0.00001413
Iteration 93/1000 | Loss: 0.00001413
Iteration 94/1000 | Loss: 0.00001413
Iteration 95/1000 | Loss: 0.00001413
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001412
Iteration 100/1000 | Loss: 0.00001412
Iteration 101/1000 | Loss: 0.00001412
Iteration 102/1000 | Loss: 0.00001412
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001411
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001411
Iteration 113/1000 | Loss: 0.00001411
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001410
Iteration 119/1000 | Loss: 0.00001410
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001409
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001409
Iteration 137/1000 | Loss: 0.00001409
Iteration 138/1000 | Loss: 0.00001409
Iteration 139/1000 | Loss: 0.00001409
Iteration 140/1000 | Loss: 0.00001409
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001408
Iteration 144/1000 | Loss: 0.00001408
Iteration 145/1000 | Loss: 0.00001408
Iteration 146/1000 | Loss: 0.00001408
Iteration 147/1000 | Loss: 0.00001408
Iteration 148/1000 | Loss: 0.00001408
Iteration 149/1000 | Loss: 0.00001408
Iteration 150/1000 | Loss: 0.00001408
Iteration 151/1000 | Loss: 0.00001408
Iteration 152/1000 | Loss: 0.00001408
Iteration 153/1000 | Loss: 0.00001408
Iteration 154/1000 | Loss: 0.00001408
Iteration 155/1000 | Loss: 0.00001408
Iteration 156/1000 | Loss: 0.00001408
Iteration 157/1000 | Loss: 0.00001408
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Iteration 160/1000 | Loss: 0.00001407
Iteration 161/1000 | Loss: 0.00001407
Iteration 162/1000 | Loss: 0.00001407
Iteration 163/1000 | Loss: 0.00001407
Iteration 164/1000 | Loss: 0.00001407
Iteration 165/1000 | Loss: 0.00001407
Iteration 166/1000 | Loss: 0.00001407
Iteration 167/1000 | Loss: 0.00001407
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00001407
Iteration 170/1000 | Loss: 0.00001407
Iteration 171/1000 | Loss: 0.00001407
Iteration 172/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4066831681702752e-05, 1.4066831681702752e-05, 1.4066831681702752e-05, 1.4066831681702752e-05, 1.4066831681702752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4066831681702752e-05

Optimization complete. Final v2v error: 3.1663801670074463 mm

Highest mean error: 3.9527876377105713 mm for frame 83

Lowest mean error: 2.6471009254455566 mm for frame 26

Saving results

Total time: 60.8582706451416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033760
Iteration 2/25 | Loss: 0.00168270
Iteration 3/25 | Loss: 0.00171404
Iteration 4/25 | Loss: 0.00157044
Iteration 5/25 | Loss: 0.00152523
Iteration 6/25 | Loss: 0.00133736
Iteration 7/25 | Loss: 0.00132713
Iteration 8/25 | Loss: 0.00132388
Iteration 9/25 | Loss: 0.00132302
Iteration 10/25 | Loss: 0.00132277
Iteration 11/25 | Loss: 0.00132267
Iteration 12/25 | Loss: 0.00132261
Iteration 13/25 | Loss: 0.00132260
Iteration 14/25 | Loss: 0.00132259
Iteration 15/25 | Loss: 0.00132259
Iteration 16/25 | Loss: 0.00132259
Iteration 17/25 | Loss: 0.00132259
Iteration 18/25 | Loss: 0.00132259
Iteration 19/25 | Loss: 0.00132259
Iteration 20/25 | Loss: 0.00132259
Iteration 21/25 | Loss: 0.00132258
Iteration 22/25 | Loss: 0.00132258
Iteration 23/25 | Loss: 0.00132258
Iteration 24/25 | Loss: 0.00132258
Iteration 25/25 | Loss: 0.00132258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.81613302
Iteration 2/25 | Loss: 0.00248823
Iteration 3/25 | Loss: 0.00248823
Iteration 4/25 | Loss: 0.00208617
Iteration 5/25 | Loss: 0.00208617
Iteration 6/25 | Loss: 0.00208616
Iteration 7/25 | Loss: 0.00208616
Iteration 8/25 | Loss: 0.00208616
Iteration 9/25 | Loss: 0.00208616
Iteration 10/25 | Loss: 0.00208616
Iteration 11/25 | Loss: 0.00208616
Iteration 12/25 | Loss: 0.00208616
Iteration 13/25 | Loss: 0.00208616
Iteration 14/25 | Loss: 0.00208616
Iteration 15/25 | Loss: 0.00208616
Iteration 16/25 | Loss: 0.00208616
Iteration 17/25 | Loss: 0.00208616
Iteration 18/25 | Loss: 0.00208616
Iteration 19/25 | Loss: 0.00208616
Iteration 20/25 | Loss: 0.00208616
Iteration 21/25 | Loss: 0.00208616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0020861623343080282, 0.0020861623343080282, 0.0020861623343080282, 0.0020861623343080282, 0.0020861623343080282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020861623343080282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208616
Iteration 2/1000 | Loss: 0.00030147
Iteration 3/1000 | Loss: 0.00039162
Iteration 4/1000 | Loss: 0.00003786
Iteration 5/1000 | Loss: 0.00002989
Iteration 6/1000 | Loss: 0.00005198
Iteration 7/1000 | Loss: 0.00004589
Iteration 8/1000 | Loss: 0.00012065
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00005499
Iteration 11/1000 | Loss: 0.00006066
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001375
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00006612
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00005588
Iteration 19/1000 | Loss: 0.00006432
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001267
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001266
Iteration 36/1000 | Loss: 0.00001266
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001263
Iteration 43/1000 | Loss: 0.00001263
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001262
Iteration 46/1000 | Loss: 0.00004793
Iteration 47/1000 | Loss: 0.00002357
Iteration 48/1000 | Loss: 0.00005070
Iteration 49/1000 | Loss: 0.00060291
Iteration 50/1000 | Loss: 0.00003019
Iteration 51/1000 | Loss: 0.00002301
Iteration 52/1000 | Loss: 0.00007633
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00002102
Iteration 57/1000 | Loss: 0.00002658
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001249
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001248
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00003421
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00002820
Iteration 87/1000 | Loss: 0.00006849
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001229
Iteration 124/1000 | Loss: 0.00001229
Iteration 125/1000 | Loss: 0.00001229
Iteration 126/1000 | Loss: 0.00001229
Iteration 127/1000 | Loss: 0.00001229
Iteration 128/1000 | Loss: 0.00001229
Iteration 129/1000 | Loss: 0.00001229
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Iteration 135/1000 | Loss: 0.00001228
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001226
Iteration 142/1000 | Loss: 0.00001226
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001226
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001225
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001225
Iteration 155/1000 | Loss: 0.00001225
Iteration 156/1000 | Loss: 0.00001225
Iteration 157/1000 | Loss: 0.00001225
Iteration 158/1000 | Loss: 0.00001225
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001225
Iteration 164/1000 | Loss: 0.00001224
Iteration 165/1000 | Loss: 0.00001224
Iteration 166/1000 | Loss: 0.00001224
Iteration 167/1000 | Loss: 0.00001224
Iteration 168/1000 | Loss: 0.00001224
Iteration 169/1000 | Loss: 0.00001224
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001223
Iteration 175/1000 | Loss: 0.00001223
Iteration 176/1000 | Loss: 0.00001223
Iteration 177/1000 | Loss: 0.00001223
Iteration 178/1000 | Loss: 0.00001223
Iteration 179/1000 | Loss: 0.00001223
Iteration 180/1000 | Loss: 0.00001223
Iteration 181/1000 | Loss: 0.00001223
Iteration 182/1000 | Loss: 0.00001223
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001222
Iteration 186/1000 | Loss: 0.00001222
Iteration 187/1000 | Loss: 0.00001222
Iteration 188/1000 | Loss: 0.00001222
Iteration 189/1000 | Loss: 0.00001222
Iteration 190/1000 | Loss: 0.00001222
Iteration 191/1000 | Loss: 0.00001222
Iteration 192/1000 | Loss: 0.00001222
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001221
Iteration 201/1000 | Loss: 0.00001221
Iteration 202/1000 | Loss: 0.00001221
Iteration 203/1000 | Loss: 0.00001221
Iteration 204/1000 | Loss: 0.00001221
Iteration 205/1000 | Loss: 0.00001221
Iteration 206/1000 | Loss: 0.00001221
Iteration 207/1000 | Loss: 0.00001221
Iteration 208/1000 | Loss: 0.00001221
Iteration 209/1000 | Loss: 0.00001221
Iteration 210/1000 | Loss: 0.00001221
Iteration 211/1000 | Loss: 0.00001221
Iteration 212/1000 | Loss: 0.00001221
Iteration 213/1000 | Loss: 0.00001221
Iteration 214/1000 | Loss: 0.00001221
Iteration 215/1000 | Loss: 0.00001221
Iteration 216/1000 | Loss: 0.00001221
Iteration 217/1000 | Loss: 0.00001221
Iteration 218/1000 | Loss: 0.00001221
Iteration 219/1000 | Loss: 0.00001221
Iteration 220/1000 | Loss: 0.00001221
Iteration 221/1000 | Loss: 0.00001221
Iteration 222/1000 | Loss: 0.00001221
Iteration 223/1000 | Loss: 0.00001221
Iteration 224/1000 | Loss: 0.00001220
Iteration 225/1000 | Loss: 0.00001220
Iteration 226/1000 | Loss: 0.00001220
Iteration 227/1000 | Loss: 0.00001220
Iteration 228/1000 | Loss: 0.00001220
Iteration 229/1000 | Loss: 0.00001220
Iteration 230/1000 | Loss: 0.00001220
Iteration 231/1000 | Loss: 0.00001220
Iteration 232/1000 | Loss: 0.00001220
Iteration 233/1000 | Loss: 0.00001220
Iteration 234/1000 | Loss: 0.00001220
Iteration 235/1000 | Loss: 0.00001220
Iteration 236/1000 | Loss: 0.00001220
Iteration 237/1000 | Loss: 0.00001220
Iteration 238/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.2202189282106701e-05, 1.2202189282106701e-05, 1.2202189282106701e-05, 1.2202189282106701e-05, 1.2202189282106701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2202189282106701e-05

Optimization complete. Final v2v error: 3.0152151584625244 mm

Highest mean error: 3.228224039077759 mm for frame 0

Lowest mean error: 2.7223732471466064 mm for frame 31

Saving results

Total time: 86.49548602104187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778572
Iteration 2/25 | Loss: 0.00165705
Iteration 3/25 | Loss: 0.00139624
Iteration 4/25 | Loss: 0.00137729
Iteration 5/25 | Loss: 0.00137424
Iteration 6/25 | Loss: 0.00137424
Iteration 7/25 | Loss: 0.00137424
Iteration 8/25 | Loss: 0.00137424
Iteration 9/25 | Loss: 0.00137424
Iteration 10/25 | Loss: 0.00137424
Iteration 11/25 | Loss: 0.00137424
Iteration 12/25 | Loss: 0.00137424
Iteration 13/25 | Loss: 0.00137424
Iteration 14/25 | Loss: 0.00137424
Iteration 15/25 | Loss: 0.00137424
Iteration 16/25 | Loss: 0.00137424
Iteration 17/25 | Loss: 0.00137424
Iteration 18/25 | Loss: 0.00137424
Iteration 19/25 | Loss: 0.00137424
Iteration 20/25 | Loss: 0.00137424
Iteration 21/25 | Loss: 0.00137424
Iteration 22/25 | Loss: 0.00137424
Iteration 23/25 | Loss: 0.00137424
Iteration 24/25 | Loss: 0.00137424
Iteration 25/25 | Loss: 0.00137424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25062335
Iteration 2/25 | Loss: 0.00222786
Iteration 3/25 | Loss: 0.00222786
Iteration 4/25 | Loss: 0.00222786
Iteration 5/25 | Loss: 0.00222786
Iteration 6/25 | Loss: 0.00222785
Iteration 7/25 | Loss: 0.00222785
Iteration 8/25 | Loss: 0.00222785
Iteration 9/25 | Loss: 0.00222785
Iteration 10/25 | Loss: 0.00222785
Iteration 11/25 | Loss: 0.00222785
Iteration 12/25 | Loss: 0.00222785
Iteration 13/25 | Loss: 0.00222785
Iteration 14/25 | Loss: 0.00222785
Iteration 15/25 | Loss: 0.00222785
Iteration 16/25 | Loss: 0.00222785
Iteration 17/25 | Loss: 0.00222785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002227853750810027, 0.002227853750810027, 0.002227853750810027, 0.002227853750810027, 0.002227853750810027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002227853750810027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222785
Iteration 2/1000 | Loss: 0.00003598
Iteration 3/1000 | Loss: 0.00002459
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001592
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001563
Iteration 27/1000 | Loss: 0.00001562
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001558
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001550
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001539
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001537
Iteration 71/1000 | Loss: 0.00001537
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001536
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001536
Iteration 77/1000 | Loss: 0.00001536
Iteration 78/1000 | Loss: 0.00001536
Iteration 79/1000 | Loss: 0.00001536
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001531
Iteration 89/1000 | Loss: 0.00001531
Iteration 90/1000 | Loss: 0.00001531
Iteration 91/1000 | Loss: 0.00001531
Iteration 92/1000 | Loss: 0.00001531
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001530
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001528
Iteration 99/1000 | Loss: 0.00001528
Iteration 100/1000 | Loss: 0.00001528
Iteration 101/1000 | Loss: 0.00001528
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001527
Iteration 109/1000 | Loss: 0.00001527
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001523
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001522
Iteration 125/1000 | Loss: 0.00001522
Iteration 126/1000 | Loss: 0.00001522
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001522
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001521
Iteration 135/1000 | Loss: 0.00001521
Iteration 136/1000 | Loss: 0.00001521
Iteration 137/1000 | Loss: 0.00001521
Iteration 138/1000 | Loss: 0.00001521
Iteration 139/1000 | Loss: 0.00001521
Iteration 140/1000 | Loss: 0.00001521
Iteration 141/1000 | Loss: 0.00001521
Iteration 142/1000 | Loss: 0.00001521
Iteration 143/1000 | Loss: 0.00001521
Iteration 144/1000 | Loss: 0.00001521
Iteration 145/1000 | Loss: 0.00001521
Iteration 146/1000 | Loss: 0.00001521
Iteration 147/1000 | Loss: 0.00001520
Iteration 148/1000 | Loss: 0.00001520
Iteration 149/1000 | Loss: 0.00001520
Iteration 150/1000 | Loss: 0.00001520
Iteration 151/1000 | Loss: 0.00001520
Iteration 152/1000 | Loss: 0.00001520
Iteration 153/1000 | Loss: 0.00001520
Iteration 154/1000 | Loss: 0.00001520
Iteration 155/1000 | Loss: 0.00001520
Iteration 156/1000 | Loss: 0.00001520
Iteration 157/1000 | Loss: 0.00001520
Iteration 158/1000 | Loss: 0.00001519
Iteration 159/1000 | Loss: 0.00001519
Iteration 160/1000 | Loss: 0.00001519
Iteration 161/1000 | Loss: 0.00001519
Iteration 162/1000 | Loss: 0.00001519
Iteration 163/1000 | Loss: 0.00001519
Iteration 164/1000 | Loss: 0.00001519
Iteration 165/1000 | Loss: 0.00001519
Iteration 166/1000 | Loss: 0.00001519
Iteration 167/1000 | Loss: 0.00001519
Iteration 168/1000 | Loss: 0.00001519
Iteration 169/1000 | Loss: 0.00001519
Iteration 170/1000 | Loss: 0.00001519
Iteration 171/1000 | Loss: 0.00001519
Iteration 172/1000 | Loss: 0.00001519
Iteration 173/1000 | Loss: 0.00001518
Iteration 174/1000 | Loss: 0.00001518
Iteration 175/1000 | Loss: 0.00001518
Iteration 176/1000 | Loss: 0.00001518
Iteration 177/1000 | Loss: 0.00001518
Iteration 178/1000 | Loss: 0.00001518
Iteration 179/1000 | Loss: 0.00001518
Iteration 180/1000 | Loss: 0.00001518
Iteration 181/1000 | Loss: 0.00001518
Iteration 182/1000 | Loss: 0.00001518
Iteration 183/1000 | Loss: 0.00001518
Iteration 184/1000 | Loss: 0.00001518
Iteration 185/1000 | Loss: 0.00001518
Iteration 186/1000 | Loss: 0.00001518
Iteration 187/1000 | Loss: 0.00001518
Iteration 188/1000 | Loss: 0.00001518
Iteration 189/1000 | Loss: 0.00001518
Iteration 190/1000 | Loss: 0.00001518
Iteration 191/1000 | Loss: 0.00001517
Iteration 192/1000 | Loss: 0.00001517
Iteration 193/1000 | Loss: 0.00001517
Iteration 194/1000 | Loss: 0.00001517
Iteration 195/1000 | Loss: 0.00001517
Iteration 196/1000 | Loss: 0.00001517
Iteration 197/1000 | Loss: 0.00001517
Iteration 198/1000 | Loss: 0.00001517
Iteration 199/1000 | Loss: 0.00001517
Iteration 200/1000 | Loss: 0.00001517
Iteration 201/1000 | Loss: 0.00001517
Iteration 202/1000 | Loss: 0.00001517
Iteration 203/1000 | Loss: 0.00001517
Iteration 204/1000 | Loss: 0.00001517
Iteration 205/1000 | Loss: 0.00001517
Iteration 206/1000 | Loss: 0.00001517
Iteration 207/1000 | Loss: 0.00001517
Iteration 208/1000 | Loss: 0.00001517
Iteration 209/1000 | Loss: 0.00001517
Iteration 210/1000 | Loss: 0.00001517
Iteration 211/1000 | Loss: 0.00001517
Iteration 212/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.5173784049693495e-05, 1.5173784049693495e-05, 1.5173784049693495e-05, 1.5173784049693495e-05, 1.5173784049693495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5173784049693495e-05

Optimization complete. Final v2v error: 3.3192145824432373 mm

Highest mean error: 3.6535165309906006 mm for frame 184

Lowest mean error: 3.0018298625946045 mm for frame 221

Saving results

Total time: 49.043567419052124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868922
Iteration 2/25 | Loss: 0.00185166
Iteration 3/25 | Loss: 0.00150139
Iteration 4/25 | Loss: 0.00144977
Iteration 5/25 | Loss: 0.00143670
Iteration 6/25 | Loss: 0.00142429
Iteration 7/25 | Loss: 0.00141268
Iteration 8/25 | Loss: 0.00141733
Iteration 9/25 | Loss: 0.00141605
Iteration 10/25 | Loss: 0.00141132
Iteration 11/25 | Loss: 0.00140743
Iteration 12/25 | Loss: 0.00140178
Iteration 13/25 | Loss: 0.00140043
Iteration 14/25 | Loss: 0.00140201
Iteration 15/25 | Loss: 0.00140009
Iteration 16/25 | Loss: 0.00139982
Iteration 17/25 | Loss: 0.00140474
Iteration 18/25 | Loss: 0.00140319
Iteration 19/25 | Loss: 0.00139873
Iteration 20/25 | Loss: 0.00139726
Iteration 21/25 | Loss: 0.00139695
Iteration 22/25 | Loss: 0.00139684
Iteration 23/25 | Loss: 0.00139683
Iteration 24/25 | Loss: 0.00139683
Iteration 25/25 | Loss: 0.00139682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.59827709
Iteration 2/25 | Loss: 0.00194878
Iteration 3/25 | Loss: 0.00194878
Iteration 4/25 | Loss: 0.00194878
Iteration 5/25 | Loss: 0.00194878
Iteration 6/25 | Loss: 0.00194877
Iteration 7/25 | Loss: 0.00194877
Iteration 8/25 | Loss: 0.00194877
Iteration 9/25 | Loss: 0.00194877
Iteration 10/25 | Loss: 0.00194877
Iteration 11/25 | Loss: 0.00194877
Iteration 12/25 | Loss: 0.00194877
Iteration 13/25 | Loss: 0.00194877
Iteration 14/25 | Loss: 0.00194877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019487730460241437, 0.0019487730460241437, 0.0019487730460241437, 0.0019487730460241437, 0.0019487730460241437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019487730460241437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194877
Iteration 2/1000 | Loss: 0.00004634
Iteration 3/1000 | Loss: 0.00002845
Iteration 4/1000 | Loss: 0.00002461
Iteration 5/1000 | Loss: 0.00002317
Iteration 6/1000 | Loss: 0.00002217
Iteration 7/1000 | Loss: 0.00002150
Iteration 8/1000 | Loss: 0.00002086
Iteration 9/1000 | Loss: 0.00002046
Iteration 10/1000 | Loss: 0.00002012
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001955
Iteration 13/1000 | Loss: 0.00009374
Iteration 14/1000 | Loss: 0.00002341
Iteration 15/1000 | Loss: 0.00002074
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001920
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00006736
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001903
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001902
Iteration 25/1000 | Loss: 0.00001900
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001899
Iteration 28/1000 | Loss: 0.00001899
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001886
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001879
Iteration 43/1000 | Loss: 0.00001879
Iteration 44/1000 | Loss: 0.00001878
Iteration 45/1000 | Loss: 0.00001878
Iteration 46/1000 | Loss: 0.00001878
Iteration 47/1000 | Loss: 0.00001878
Iteration 48/1000 | Loss: 0.00001878
Iteration 49/1000 | Loss: 0.00001877
Iteration 50/1000 | Loss: 0.00001877
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001874
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001870
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001865
Iteration 73/1000 | Loss: 0.00001865
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001864
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001863
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001856
Iteration 105/1000 | Loss: 0.00001855
Iteration 106/1000 | Loss: 0.00001855
Iteration 107/1000 | Loss: 0.00001855
Iteration 108/1000 | Loss: 0.00001855
Iteration 109/1000 | Loss: 0.00001855
Iteration 110/1000 | Loss: 0.00001855
Iteration 111/1000 | Loss: 0.00001855
Iteration 112/1000 | Loss: 0.00001855
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001854
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001852
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001849
Iteration 144/1000 | Loss: 0.00001849
Iteration 145/1000 | Loss: 0.00001849
Iteration 146/1000 | Loss: 0.00001849
Iteration 147/1000 | Loss: 0.00001849
Iteration 148/1000 | Loss: 0.00001849
Iteration 149/1000 | Loss: 0.00001849
Iteration 150/1000 | Loss: 0.00001848
Iteration 151/1000 | Loss: 0.00001848
Iteration 152/1000 | Loss: 0.00001848
Iteration 153/1000 | Loss: 0.00001847
Iteration 154/1000 | Loss: 0.00001847
Iteration 155/1000 | Loss: 0.00001847
Iteration 156/1000 | Loss: 0.00001847
Iteration 157/1000 | Loss: 0.00001847
Iteration 158/1000 | Loss: 0.00010573
Iteration 159/1000 | Loss: 0.00001859
Iteration 160/1000 | Loss: 0.00001846
Iteration 161/1000 | Loss: 0.00001846
Iteration 162/1000 | Loss: 0.00001846
Iteration 163/1000 | Loss: 0.00001845
Iteration 164/1000 | Loss: 0.00001844
Iteration 165/1000 | Loss: 0.00001844
Iteration 166/1000 | Loss: 0.00005042
Iteration 167/1000 | Loss: 0.00003211
Iteration 168/1000 | Loss: 0.00001847
Iteration 169/1000 | Loss: 0.00001847
Iteration 170/1000 | Loss: 0.00001846
Iteration 171/1000 | Loss: 0.00001845
Iteration 172/1000 | Loss: 0.00001845
Iteration 173/1000 | Loss: 0.00001845
Iteration 174/1000 | Loss: 0.00001845
Iteration 175/1000 | Loss: 0.00001844
Iteration 176/1000 | Loss: 0.00001844
Iteration 177/1000 | Loss: 0.00001844
Iteration 178/1000 | Loss: 0.00001843
Iteration 179/1000 | Loss: 0.00001843
Iteration 180/1000 | Loss: 0.00003744
Iteration 181/1000 | Loss: 0.00001845
Iteration 182/1000 | Loss: 0.00001845
Iteration 183/1000 | Loss: 0.00001845
Iteration 184/1000 | Loss: 0.00001844
Iteration 185/1000 | Loss: 0.00001844
Iteration 186/1000 | Loss: 0.00001844
Iteration 187/1000 | Loss: 0.00001844
Iteration 188/1000 | Loss: 0.00001844
Iteration 189/1000 | Loss: 0.00001844
Iteration 190/1000 | Loss: 0.00001844
Iteration 191/1000 | Loss: 0.00001844
Iteration 192/1000 | Loss: 0.00001844
Iteration 193/1000 | Loss: 0.00001844
Iteration 194/1000 | Loss: 0.00001844
Iteration 195/1000 | Loss: 0.00001843
Iteration 196/1000 | Loss: 0.00001843
Iteration 197/1000 | Loss: 0.00001843
Iteration 198/1000 | Loss: 0.00001843
Iteration 199/1000 | Loss: 0.00001843
Iteration 200/1000 | Loss: 0.00001843
Iteration 201/1000 | Loss: 0.00001843
Iteration 202/1000 | Loss: 0.00001843
Iteration 203/1000 | Loss: 0.00001843
Iteration 204/1000 | Loss: 0.00001842
Iteration 205/1000 | Loss: 0.00001842
Iteration 206/1000 | Loss: 0.00001842
Iteration 207/1000 | Loss: 0.00001842
Iteration 208/1000 | Loss: 0.00001842
Iteration 209/1000 | Loss: 0.00001842
Iteration 210/1000 | Loss: 0.00001841
Iteration 211/1000 | Loss: 0.00001841
Iteration 212/1000 | Loss: 0.00001841
Iteration 213/1000 | Loss: 0.00001840
Iteration 214/1000 | Loss: 0.00001840
Iteration 215/1000 | Loss: 0.00001840
Iteration 216/1000 | Loss: 0.00001840
Iteration 217/1000 | Loss: 0.00001840
Iteration 218/1000 | Loss: 0.00001840
Iteration 219/1000 | Loss: 0.00001840
Iteration 220/1000 | Loss: 0.00001840
Iteration 221/1000 | Loss: 0.00001840
Iteration 222/1000 | Loss: 0.00001840
Iteration 223/1000 | Loss: 0.00001840
Iteration 224/1000 | Loss: 0.00001840
Iteration 225/1000 | Loss: 0.00001840
Iteration 226/1000 | Loss: 0.00001840
Iteration 227/1000 | Loss: 0.00001840
Iteration 228/1000 | Loss: 0.00001840
Iteration 229/1000 | Loss: 0.00001840
Iteration 230/1000 | Loss: 0.00001840
Iteration 231/1000 | Loss: 0.00001840
Iteration 232/1000 | Loss: 0.00001840
Iteration 233/1000 | Loss: 0.00001840
Iteration 234/1000 | Loss: 0.00001840
Iteration 235/1000 | Loss: 0.00001840
Iteration 236/1000 | Loss: 0.00001840
Iteration 237/1000 | Loss: 0.00001840
Iteration 238/1000 | Loss: 0.00001840
Iteration 239/1000 | Loss: 0.00001840
Iteration 240/1000 | Loss: 0.00001839
Iteration 241/1000 | Loss: 0.00001839
Iteration 242/1000 | Loss: 0.00001839
Iteration 243/1000 | Loss: 0.00001839
Iteration 244/1000 | Loss: 0.00001839
Iteration 245/1000 | Loss: 0.00001839
Iteration 246/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.8394699509372003e-05, 1.8394699509372003e-05, 1.8394699509372003e-05, 1.8394699509372003e-05, 1.8394699509372003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8394699509372003e-05

Optimization complete. Final v2v error: 3.600250720977783 mm

Highest mean error: 5.719945430755615 mm for frame 99

Lowest mean error: 2.9311392307281494 mm for frame 19

Saving results

Total time: 90.86246466636658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957510
Iteration 2/25 | Loss: 0.00957510
Iteration 3/25 | Loss: 0.00957510
Iteration 4/25 | Loss: 0.00957510
Iteration 5/25 | Loss: 0.00957509
Iteration 6/25 | Loss: 0.00957509
Iteration 7/25 | Loss: 0.00957509
Iteration 8/25 | Loss: 0.00957509
Iteration 9/25 | Loss: 0.00957509
Iteration 10/25 | Loss: 0.00957509
Iteration 11/25 | Loss: 0.00957508
Iteration 12/25 | Loss: 0.00957508
Iteration 13/25 | Loss: 0.00957508
Iteration 14/25 | Loss: 0.00957508
Iteration 15/25 | Loss: 0.00957508
Iteration 16/25 | Loss: 0.00957508
Iteration 17/25 | Loss: 0.00957508
Iteration 18/25 | Loss: 0.00957507
Iteration 19/25 | Loss: 0.00957507
Iteration 20/25 | Loss: 0.00957507
Iteration 21/25 | Loss: 0.00957507
Iteration 22/25 | Loss: 0.00957507
Iteration 23/25 | Loss: 0.00957507
Iteration 24/25 | Loss: 0.00957507
Iteration 25/25 | Loss: 0.00957506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38763666
Iteration 2/25 | Loss: 0.17215507
Iteration 3/25 | Loss: 0.17355123
Iteration 4/25 | Loss: 0.16780747
Iteration 5/25 | Loss: 0.16599151
Iteration 6/25 | Loss: 0.16526854
Iteration 7/25 | Loss: 0.16526854
Iteration 8/25 | Loss: 0.16526853
Iteration 9/25 | Loss: 0.16526850
Iteration 10/25 | Loss: 0.16526848
Iteration 11/25 | Loss: 0.16526848
Iteration 12/25 | Loss: 0.16526848
Iteration 13/25 | Loss: 0.16526848
Iteration 14/25 | Loss: 0.16526848
Iteration 15/25 | Loss: 0.16526848
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.16526848077774048, 0.16526848077774048, 0.16526848077774048, 0.16526848077774048, 0.16526848077774048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16526848077774048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16526848
Iteration 2/1000 | Loss: 0.00766973
Iteration 3/1000 | Loss: 0.00500865
Iteration 4/1000 | Loss: 0.00267252
Iteration 5/1000 | Loss: 0.00135908
Iteration 6/1000 | Loss: 0.00181329
Iteration 7/1000 | Loss: 0.00037391
Iteration 8/1000 | Loss: 0.00021200
Iteration 9/1000 | Loss: 0.00016450
Iteration 10/1000 | Loss: 0.00010297
Iteration 11/1000 | Loss: 0.00007427
Iteration 12/1000 | Loss: 0.00006219
Iteration 13/1000 | Loss: 0.00004909
Iteration 14/1000 | Loss: 0.00004217
Iteration 15/1000 | Loss: 0.00003661
Iteration 16/1000 | Loss: 0.00003346
Iteration 17/1000 | Loss: 0.00003120
Iteration 18/1000 | Loss: 0.00002853
Iteration 19/1000 | Loss: 0.00002585
Iteration 20/1000 | Loss: 0.00002437
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002163
Iteration 25/1000 | Loss: 0.00002126
Iteration 26/1000 | Loss: 0.00002086
Iteration 27/1000 | Loss: 0.00002048
Iteration 28/1000 | Loss: 0.00002019
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001985
Iteration 32/1000 | Loss: 0.00001983
Iteration 33/1000 | Loss: 0.00001980
Iteration 34/1000 | Loss: 0.00001966
Iteration 35/1000 | Loss: 0.00001962
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001942
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001935
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001932
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001926
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001917
Iteration 71/1000 | Loss: 0.00001917
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001910
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001910
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001909
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001905
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001902
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001902
Iteration 143/1000 | Loss: 0.00001902
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001901
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Iteration 153/1000 | Loss: 0.00001901
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001900
Iteration 158/1000 | Loss: 0.00001900
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00001900
Iteration 161/1000 | Loss: 0.00001900
Iteration 162/1000 | Loss: 0.00001900
Iteration 163/1000 | Loss: 0.00001900
Iteration 164/1000 | Loss: 0.00001900
Iteration 165/1000 | Loss: 0.00001900
Iteration 166/1000 | Loss: 0.00001899
Iteration 167/1000 | Loss: 0.00001899
Iteration 168/1000 | Loss: 0.00001899
Iteration 169/1000 | Loss: 0.00001899
Iteration 170/1000 | Loss: 0.00001899
Iteration 171/1000 | Loss: 0.00001899
Iteration 172/1000 | Loss: 0.00001899
Iteration 173/1000 | Loss: 0.00001899
Iteration 174/1000 | Loss: 0.00001899
Iteration 175/1000 | Loss: 0.00001899
Iteration 176/1000 | Loss: 0.00001899
Iteration 177/1000 | Loss: 0.00001898
Iteration 178/1000 | Loss: 0.00001898
Iteration 179/1000 | Loss: 0.00001898
Iteration 180/1000 | Loss: 0.00001898
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001898
Iteration 183/1000 | Loss: 0.00001898
Iteration 184/1000 | Loss: 0.00001898
Iteration 185/1000 | Loss: 0.00001898
Iteration 186/1000 | Loss: 0.00001898
Iteration 187/1000 | Loss: 0.00001898
Iteration 188/1000 | Loss: 0.00001898
Iteration 189/1000 | Loss: 0.00001898
Iteration 190/1000 | Loss: 0.00001898
Iteration 191/1000 | Loss: 0.00001898
Iteration 192/1000 | Loss: 0.00001898
Iteration 193/1000 | Loss: 0.00001898
Iteration 194/1000 | Loss: 0.00001898
Iteration 195/1000 | Loss: 0.00001898
Iteration 196/1000 | Loss: 0.00001898
Iteration 197/1000 | Loss: 0.00001898
Iteration 198/1000 | Loss: 0.00001898
Iteration 199/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.8982485926244408e-05, 1.8982485926244408e-05, 1.8982485926244408e-05, 1.8982485926244408e-05, 1.8982485926244408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8982485926244408e-05

Optimization complete. Final v2v error: 3.7119433879852295 mm

Highest mean error: 4.499774932861328 mm for frame 95

Lowest mean error: 3.4319705963134766 mm for frame 195

Saving results

Total time: 78.33423495292664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003798
Iteration 2/25 | Loss: 0.00215545
Iteration 3/25 | Loss: 0.00161449
Iteration 4/25 | Loss: 0.00161009
Iteration 5/25 | Loss: 0.00156346
Iteration 6/25 | Loss: 0.00158092
Iteration 7/25 | Loss: 0.00150734
Iteration 8/25 | Loss: 0.00145447
Iteration 9/25 | Loss: 0.00138761
Iteration 10/25 | Loss: 0.00144132
Iteration 11/25 | Loss: 0.00137383
Iteration 12/25 | Loss: 0.00135313
Iteration 13/25 | Loss: 0.00134882
Iteration 14/25 | Loss: 0.00134821
Iteration 15/25 | Loss: 0.00134805
Iteration 16/25 | Loss: 0.00134804
Iteration 17/25 | Loss: 0.00134804
Iteration 18/25 | Loss: 0.00134804
Iteration 19/25 | Loss: 0.00134804
Iteration 20/25 | Loss: 0.00134804
Iteration 21/25 | Loss: 0.00134804
Iteration 22/25 | Loss: 0.00134803
Iteration 23/25 | Loss: 0.00134803
Iteration 24/25 | Loss: 0.00134803
Iteration 25/25 | Loss: 0.00134803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30675244
Iteration 2/25 | Loss: 0.00226904
Iteration 3/25 | Loss: 0.00226904
Iteration 4/25 | Loss: 0.00226904
Iteration 5/25 | Loss: 0.00226903
Iteration 6/25 | Loss: 0.00226903
Iteration 7/25 | Loss: 0.00226903
Iteration 8/25 | Loss: 0.00226903
Iteration 9/25 | Loss: 0.00226903
Iteration 10/25 | Loss: 0.00226903
Iteration 11/25 | Loss: 0.00226903
Iteration 12/25 | Loss: 0.00226903
Iteration 13/25 | Loss: 0.00226903
Iteration 14/25 | Loss: 0.00226903
Iteration 15/25 | Loss: 0.00226903
Iteration 16/25 | Loss: 0.00226903
Iteration 17/25 | Loss: 0.00226903
Iteration 18/25 | Loss: 0.00226903
Iteration 19/25 | Loss: 0.00226903
Iteration 20/25 | Loss: 0.00226903
Iteration 21/25 | Loss: 0.00226903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002269032644107938, 0.002269032644107938, 0.002269032644107938, 0.002269032644107938, 0.002269032644107938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002269032644107938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226903
Iteration 2/1000 | Loss: 0.00003154
Iteration 3/1000 | Loss: 0.00002065
Iteration 4/1000 | Loss: 0.00001713
Iteration 5/1000 | Loss: 0.00001582
Iteration 6/1000 | Loss: 0.00006413
Iteration 7/1000 | Loss: 0.00001479
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001293
Iteration 15/1000 | Loss: 0.00006634
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001273
Iteration 21/1000 | Loss: 0.00001273
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00005581
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001242
Iteration 57/1000 | Loss: 0.00001242
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.2411874195095152e-05, 1.2411874195095152e-05, 1.2411874195095152e-05, 1.2411874195095152e-05, 1.2411874195095152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2411874195095152e-05

Optimization complete. Final v2v error: 2.9896950721740723 mm

Highest mean error: 4.117929935455322 mm for frame 64

Lowest mean error: 2.6190078258514404 mm for frame 132

Saving results

Total time: 57.60088658332825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768069
Iteration 2/25 | Loss: 0.00205506
Iteration 3/25 | Loss: 0.00146946
Iteration 4/25 | Loss: 0.00137440
Iteration 5/25 | Loss: 0.00139015
Iteration 6/25 | Loss: 0.00135203
Iteration 7/25 | Loss: 0.00132134
Iteration 8/25 | Loss: 0.00132573
Iteration 9/25 | Loss: 0.00131594
Iteration 10/25 | Loss: 0.00130972
Iteration 11/25 | Loss: 0.00130869
Iteration 12/25 | Loss: 0.00130806
Iteration 13/25 | Loss: 0.00130728
Iteration 14/25 | Loss: 0.00130655
Iteration 15/25 | Loss: 0.00130636
Iteration 16/25 | Loss: 0.00130617
Iteration 17/25 | Loss: 0.00130605
Iteration 18/25 | Loss: 0.00130592
Iteration 19/25 | Loss: 0.00130592
Iteration 20/25 | Loss: 0.00130591
Iteration 21/25 | Loss: 0.00130591
Iteration 22/25 | Loss: 0.00130591
Iteration 23/25 | Loss: 0.00130591
Iteration 24/25 | Loss: 0.00130591
Iteration 25/25 | Loss: 0.00130591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29676747
Iteration 2/25 | Loss: 0.00228792
Iteration 3/25 | Loss: 0.00228791
Iteration 4/25 | Loss: 0.00228791
Iteration 5/25 | Loss: 0.00228791
Iteration 6/25 | Loss: 0.00228791
Iteration 7/25 | Loss: 0.00228791
Iteration 8/25 | Loss: 0.00228791
Iteration 9/25 | Loss: 0.00228791
Iteration 10/25 | Loss: 0.00228791
Iteration 11/25 | Loss: 0.00228791
Iteration 12/25 | Loss: 0.00228791
Iteration 13/25 | Loss: 0.00228791
Iteration 14/25 | Loss: 0.00228791
Iteration 15/25 | Loss: 0.00228791
Iteration 16/25 | Loss: 0.00228791
Iteration 17/25 | Loss: 0.00228791
Iteration 18/25 | Loss: 0.00228791
Iteration 19/25 | Loss: 0.00228791
Iteration 20/25 | Loss: 0.00228791
Iteration 21/25 | Loss: 0.00228791
Iteration 22/25 | Loss: 0.00228791
Iteration 23/25 | Loss: 0.00228791
Iteration 24/25 | Loss: 0.00228791
Iteration 25/25 | Loss: 0.00228791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228791
Iteration 2/1000 | Loss: 0.00002499
Iteration 3/1000 | Loss: 0.00006327
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00021101
Iteration 6/1000 | Loss: 0.00042073
Iteration 7/1000 | Loss: 0.00006036
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001268
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00022718
Iteration 12/1000 | Loss: 0.00042255
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00024376
Iteration 16/1000 | Loss: 0.00069537
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00022133
Iteration 19/1000 | Loss: 0.00003129
Iteration 20/1000 | Loss: 0.00012729
Iteration 21/1000 | Loss: 0.00011635
Iteration 22/1000 | Loss: 0.00001926
Iteration 23/1000 | Loss: 0.00004830
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00006919
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001118
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001103
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00005649
Iteration 33/1000 | Loss: 0.00001088
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001082
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001075
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001074
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001074
Iteration 47/1000 | Loss: 0.00001074
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001073
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00007712
Iteration 57/1000 | Loss: 0.00011626
Iteration 58/1000 | Loss: 0.00010866
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001056
Iteration 61/1000 | Loss: 0.00001049
Iteration 62/1000 | Loss: 0.00001049
Iteration 63/1000 | Loss: 0.00001049
Iteration 64/1000 | Loss: 0.00001048
Iteration 65/1000 | Loss: 0.00001048
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001047
Iteration 70/1000 | Loss: 0.00001047
Iteration 71/1000 | Loss: 0.00001047
Iteration 72/1000 | Loss: 0.00001047
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001047
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001044
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001043
Iteration 91/1000 | Loss: 0.00001043
Iteration 92/1000 | Loss: 0.00001043
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001042
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001041
Iteration 100/1000 | Loss: 0.00001041
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001040
Iteration 105/1000 | Loss: 0.00001040
Iteration 106/1000 | Loss: 0.00001040
Iteration 107/1000 | Loss: 0.00001040
Iteration 108/1000 | Loss: 0.00001040
Iteration 109/1000 | Loss: 0.00001040
Iteration 110/1000 | Loss: 0.00001040
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001040
Iteration 116/1000 | Loss: 0.00001040
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001040
Iteration 119/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.0401567124063149e-05, 1.0401567124063149e-05, 1.0401567124063149e-05, 1.0401567124063149e-05, 1.0401567124063149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0401567124063149e-05

Optimization complete. Final v2v error: 2.8387153148651123 mm

Highest mean error: 3.1564087867736816 mm for frame 57

Lowest mean error: 2.6710762977600098 mm for frame 145

Saving results

Total time: 90.19128251075745
