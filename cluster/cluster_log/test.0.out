Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=222, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12432-12487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1064/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1064. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1023/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1023. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1024/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1024. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1065/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1065. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1069/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1069. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1021/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1021. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1027/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1027. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1004/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1004. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1086/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1086. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1010/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1010. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1007/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1007. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1089/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1089. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1036/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1036. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1054/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1054. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1015/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1015. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1053/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1053. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1037/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1037. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_029/1051/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_029/1051. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1030/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1030. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1078/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1078. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1013/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1013. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1020/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1020. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1009/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1009. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1037/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1037. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1075/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1075. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1051/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1051. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1055/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1055. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1043/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1043. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1085/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1085. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1000/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1000. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1038/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1038. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1025/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1025. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1008/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1008. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1062/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1062. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1032/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1032. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1040/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1040. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1006/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1006. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1077/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072367
Iteration 2/25 | Loss: 0.01072367
Iteration 3/25 | Loss: 0.01072366
Iteration 4/25 | Loss: 0.01072366
Iteration 5/25 | Loss: 0.01072366
Iteration 6/25 | Loss: 0.01072366
Iteration 7/25 | Loss: 0.01072366
Iteration 8/25 | Loss: 0.01072366
Iteration 9/25 | Loss: 0.01072366
Iteration 10/25 | Loss: 0.01072366
Iteration 11/25 | Loss: 0.01072365
Iteration 12/25 | Loss: 0.01072365
Iteration 13/25 | Loss: 0.01072365
Iteration 14/25 | Loss: 0.01072365
Iteration 15/25 | Loss: 0.01072365
Iteration 16/25 | Loss: 0.01072365
Iteration 17/25 | Loss: 0.01072365
Iteration 18/25 | Loss: 0.01072364
Iteration 19/25 | Loss: 0.01072364
Iteration 20/25 | Loss: 0.01072364
Iteration 21/25 | Loss: 0.01072364
Iteration 22/25 | Loss: 0.01072364
Iteration 23/25 | Loss: 0.01072363
Iteration 24/25 | Loss: 0.01072363
Iteration 25/25 | Loss: 0.01072363

Performing global translation and orientation optimization using a vertex loss

Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=237, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13272-13327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1070/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1070. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1045/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1045. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1048/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1048. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0008/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0008. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0014/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0014. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0001/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0001. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0011/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0011. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0000/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0000. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0002/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0002. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0022/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0022. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0021/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0021. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0013/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0013. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0005/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0005. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0010/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0010. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0004/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0004. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0017/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0017. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0020/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0020. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0015/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0015. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0016/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0016. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0006/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0006. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0023/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0023. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0024/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0024. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0012/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0012. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0007/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0007. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0003/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0003. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0018/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0018. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0019/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0019. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0009/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0009. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0008/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0008. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0014/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0014. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0001/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0001. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0011/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0011. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0000/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0000. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0002/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0002. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0022/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0022. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0021/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0021. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0013/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0013. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0005/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0005. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0010/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0010. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0004/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=237, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13272-13327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1070/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1070. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1045/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1045. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1048/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1048. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0008/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0008. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0014/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0014. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0001/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0001. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0011/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0011. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0000/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0000. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0002/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0002. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0022/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0022. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0021/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0021. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0013/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0013. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0005/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0005. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0010/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0010. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0004/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0004. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0017/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0017. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0020/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0020. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0015/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0015. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0016/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0016. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0006/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0006. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0023/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0023. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0024/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0024. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0012/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0012. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0007/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0007. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0003/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0003. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0018/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0018. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0019/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0019. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0009/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0009. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0008/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0008. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0014/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0014. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0001/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0001. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0011/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0011. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0000/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0000. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0002/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0002. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0022/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0022. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0021/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0021. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0013/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0013. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0005/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0005. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0010/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0010. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0004/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 2.22712135
Iteration 2/25 | Loss: 0.11373260
Iteration 3/25 | Loss: 0.11145642
Iteration 4/25 | Loss: 0.11041819
Iteration 5/25 | Loss: 0.11027756
Iteration 6/25 | Loss: 0.11022315
Iteration 7/25 | Loss: 0.11022313
Iteration 8/25 | Loss: 0.11022313
Iteration 9/25 | Loss: 0.11022313
Iteration 10/25 | Loss: 0.11022311
Iteration 11/25 | Loss: 0.11022311
Iteration 12/25 | Loss: 0.11022311
Iteration 13/25 | Loss: 0.11022311
Iteration 14/25 | Loss: 0.11022311
Iteration 15/25 | Loss: 0.11022311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.11022311449050903, 0.11022311449050903, 0.11022311449050903, 0.11022311449050903, 0.11022311449050903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11022311449050903

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00968559
Iteration 2/25 | Loss: 0.00117597
Iteration 3/25 | Loss: 0.00075239
Iteration 4/25 | Loss: 0.00070972
Iteration 5/25 | Loss: 0.00069193
Iteration 6/25 | Loss: 0.00069038
Iteration 7/25 | Loss: 0.00069001
Iteration 8/25 | Loss: 0.00069001
Iteration 9/25 | Loss: 0.00069001
Iteration 10/25 | Loss: 0.00069001
Iteration 11/25 | Loss: 0.00069001
Iteration 12/25 | Loss: 0.00069001
Iteration 13/25 | Loss: 0.00069001
Iteration 14/25 | Loss: 0.00069001
Iteration 15/25 | Loss: 0.00069001
Iteration 16/25 | Loss: 0.00069001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006900069420225918, 0.0006900069420225918, 0.0006900069420225918, 0.0006900069420225918, 0.0006900069420225918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006900069420225918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97352755
Iteration 2/25 | Loss: 0.00018661
Iteration 3/25 | Loss: 0.00018661
Iteration 4/25 | Loss: 0.00018661
Iteration 5/25 | Loss: 0.00018661
Iteration 6/25 | Loss: 0.00018661
Iteration 7/25 | Loss: 0.00018661
Iteration 8/25 | Loss: 0.00018661
Iteration 9/25 | Loss: 0.00018661
Iteration 10/25 | Loss: 0.00018661
Iteration 11/25 | Loss: 0.00018661
Iteration 12/25 | Loss: 0.00018661
Iteration 13/25 | Loss: 0.00018661
Iteration 14/25 | Loss: 0.00018661
Iteration 15/25 | Loss: 0.00018661
Iteration 16/25 | Loss: 0.00018661
Iteration 17/25 | Loss: 0.00018661
Iteration 18/25 | Loss: 0.00018661
Iteration 19/25 | Loss: 0.00018661
Iteration 20/25 | Loss: 0.00018661
Iteration 21/25 | Loss: 0.00018661
Iteration 22/25 | Loss: 0.00018661
Iteration 23/25 | Loss: 0.00018661
Iteration 24/25 | Loss: 0.00018661
Iteration 25/25 | Loss: 0.00018661

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00968559
Iteration 2/25 | Loss: 0.00117592
Iteration 3/25 | Loss: 0.00075593
Iteration 4/25 | Loss: 0.00071312
Iteration 5/25 | Loss: 0.00069335
Iteration 6/25 | Loss: 0.00069065
Iteration 7/25 | Loss: 0.00069008
Iteration 8/25 | Loss: 0.00069008
Iteration 9/25 | Loss: 0.00069008
Iteration 10/25 | Loss: 0.00069008
Iteration 11/25 | Loss: 0.00069008
Iteration 12/25 | Loss: 0.00069008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006900831358507276, 0.0006900831358507276, 0.0006900831358507276, 0.0006900831358507276, 0.0006900831358507276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006900831358507276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97372985
Iteration 2/25 | Loss: 0.00018655
Iteration 3/25 | Loss: 0.00018655
Iteration 4/25 | Loss: 0.00018655
Iteration 5/25 | Loss: 0.00018655
Iteration 6/25 | Loss: 0.00018655
Iteration 7/25 | Loss: 0.00018655
Iteration 8/25 | Loss: 0.00018655
Iteration 9/25 | Loss: 0.00018655
Iteration 10/25 | Loss: 0.00018655
Iteration 11/25 | Loss: 0.00018655
Iteration 12/25 | Loss: 0.00018655
Iteration 13/25 | Loss: 0.00018655
Iteration 14/25 | Loss: 0.00018655
Iteration 15/25 | Loss: 0.00018655
Iteration 16/25 | Loss: 0.00018655
Iteration 17/25 | Loss: 0.00018655
Iteration 18/25 | Loss: 0.00018655
Iteration 19/25 | Loss: 0.00018655
Iteration 20/25 | Loss: 0.00018655
Iteration 21/25 | Loss: 0.00018655
Iteration 22/25 | Loss: 0.00018655
Iteration 23/25 | Loss: 0.00018655
Iteration 24/25 | Loss: 0.00018655
Iteration 25/25 | Loss: 0.00018655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018655
Iteration 2/1000 | Loss: 0.00004074
Iteration 3/1000 | Loss: 0.00003115
Iteration 4/1000 | Loss: 0.00002889
Iteration 5/1000 | Loss: 0.00002794
Iteration 6/1000 | Loss: 0.00002724
Iteration 7/1000 | Loss: 0.00002686
Iteration 8/1000 | Loss: 0.00002649
Iteration 9/1000 | Loss: 0.00002630
Iteration 10/1000 | Loss: 0.00002623
Iteration 11/1000 | Loss: 0.00002604
Iteration 12/1000 | Loss: 0.00002594
Iteration 13/1000 | Loss: 0.00002587
Iteration 14/1000 | Loss: 0.00002580
Iteration 15/1000 | Loss: 0.00002580
Iteration 16/1000 | Loss: 0.00002577
Iteration 17/1000 | Loss: 0.00002576
Iteration 18/1000 | Loss: 0.00002575
Iteration 19/1000 | Loss: 0.00002575
Iteration 20/1000 | Loss: 0.00002574
Iteration 21/1000 | Loss: 0.00002573
Iteration 22/1000 | Loss: 0.00002573
Iteration 23/1000 | Loss: 0.00002572
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002570
Iteration 27/1000 | Loss: 0.00002569
Iteration 28/1000 | Loss: 0.00002569
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002568
Iteration 31/1000 | Loss: 0.00002566
Iteration 32/1000 | Loss: 0.00002566
Iteration 33/1000 | Loss: 0.00002566
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002566
Iteration 36/1000 | Loss: 0.00002566
Iteration 37/1000 | Loss: 0.00002566
Iteration 38/1000 | Loss: 0.00002566
Iteration 39/1000 | Loss: 0.00002566
Iteration 40/1000 | Loss: 0.00002566
Iteration 41/1000 | Loss: 0.00002566
Iteration 42/1000 | Loss: 0.00002565
Iteration 43/1000 | Loss: 0.00002565
Iteration 44/1000 | Loss: 0.00002565
Iteration 45/1000 | Loss: 0.00002564
Iteration 46/1000 | Loss: 0.00002564
Iteration 47/1000 | Loss: 0.00002564
Iteration 48/1000 | Loss: 0.00002563
Iteration 49/1000 | Loss: 0.00002563
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00002563
Iteration 52/1000 | Loss: 0.00002562
Iteration 53/1000 | Loss: 0.00002562
Iteration 54/1000 | Loss: 0.00002562
Iteration 55/1000 | Loss: 0.00002561
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002561
Iteration 58/1000 | Loss: 0.00002560
Iteration 59/1000 | Loss: 0.00002560
Iteration 60/1000 | Loss: 0.00002559
Iteration 61/1000 | Loss: 0.00002558
Iteration 62/1000 | Loss: 0.00002558
Iteration 63/1000 | Loss: 0.00002558
Iteration 64/1000 | Loss: 0.00002558
Iteration 65/1000 | Loss: 0.00002557
Iteration 66/1000 | Loss: 0.00002557
Iteration 67/1000 | Loss: 0.00002554
Iteration 68/1000 | Loss: 0.00002554
Iteration 69/1000 | Loss: 0.00002554
Iteration 70/1000 | Loss: 0.00002554
Iteration 71/1000 | Loss: 0.00002554
Iteration 72/1000 | Loss: 0.00002554
Iteration 73/1000 | Loss: 0.00002553
Iteration 74/1000 | Loss: 0.00002553
Iteration 75/1000 | Loss: 0.00002553
Iteration 76/1000 | Loss: 0.00002553
Iteration 77/1000 | Loss: 0.00002552
Iteration 78/1000 | Loss: 0.00002551
Iteration 79/1000 | Loss: 0.00002551
Iteration 80/1000 | Loss: 0.00002550
Iteration 81/1000 | Loss: 0.00002550
Iteration 82/1000 | Loss: 0.00002550
Iteration 83/1000 | Loss: 0.00002550
Iteration 84/1000 | Loss: 0.00002550
Iteration 85/1000 | Loss: 0.00002550
Iteration 86/1000 | Loss: 0.00002550
Iteration 87/1000 | Loss: 0.00002550
Iteration 88/1000 | Loss: 0.00002550
Iteration 89/1000 | Loss: 0.00002550
Iteration 90/1000 | Loss: 0.00002549
Iteration 91/1000 | Loss: 0.00002549
Iteration 92/1000 | Loss: 0.00002548
Iteration 93/1000 | Loss: 0.00002548
Iteration 94/1000 | Loss: 0.00002548
Iteration 95/1000 | Loss: 0.00002548
Iteration 96/1000 | Loss: 0.00002547
Iteration 97/1000 | Loss: 0.00002547
Iteration 98/1000 | Loss: 0.00002547
Iteration 99/1000 | Loss: 0.00002545
Iteration 100/1000 | Loss: 0.00002545
Iteration 101/1000 | Loss: 0.00002545
Iteration 102/1000 | Loss: 0.00002544
Iteration 103/1000 | Loss: 0.00002544
Iteration 104/1000 | Loss: 0.00002544
Iteration 105/1000 | Loss: 0.00002544
Iteration 106/1000 | Loss: 0.00002544
Iteration 107/1000 | Loss: 0.00002544
Iteration 108/1000 | Loss: 0.00002544
Iteration 109/1000 | Loss: 0.00002544
Iteration 110/1000 | Loss: 0.00002544
Iteration 111/1000 | Loss: 0.00002544
Iteration 112/1000 | Loss: 0.00002542
Iteration 113/1000 | Loss: 0.00002542
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002542
Iteration 116/1000 | Loss: 0.00002542
Iteration 117/1000 | Loss: 0.00002542
Iteration 118/1000 | Loss: 0.00002542
Iteration 119/1000 | Loss: 0.00002542
Iteration 120/1000 | Loss: 0.00002542
Iteration 121/1000 | Loss: 0.00002542
Iteration 122/1000 | Loss: 0.00002542
Iteration 123/1000 | Loss: 0.00002542
Iteration 124/1000 | Loss: 0.00002542
Iteration 125/1000 | Loss: 0.00002541
Iteration 126/1000 | Loss: 0.00002541
Iteration 127/1000 | Loss: 0.00002541
Iteration 128/1000 | Loss: 0.00002541
Iteration 129/1000 | Loss: 0.00002541
Iteration 130/1000 | Loss: 0.00002541
Iteration 131/1000 | Loss: 0.00002541
Iteration 132/1000 | Loss: 0.00002541
Iteration 133/1000 | Loss: 0.00002541
Iteration 134/1000 | Loss: 0.00002541
Iteration 135/1000 | Loss: 0.00002541
Iteration 136/1000 | Loss: 0.00002541
Iteration 137/1000 | Loss: 0.00002541
Iteration 138/1000 | Loss: 0.00002541
Iteration 139/1000 | Loss: 0.00002541
Iteration 140/1000 | Loss: 0.00002541
Iteration 141/1000 | Loss: 0.00002541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.5413970433874056e-05, 2.5413970433874056e-05, 2.5413970433874056e-05, 2.5413970433874056e-05, 2.5413970433874056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5413970433874056e-05

Optimization complete. Final v2v error: 4.177934169769287 mm

Highest mean error: 5.028397560119629 mm for frame 79

Lowest mean error: 3.597590684890747 mm for frame 170

Saving results

Total time: 39.451677322387695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00018661
Iteration 2/1000 | Loss: 0.00004174
Iteration 3/1000 | Loss: 0.00003164
Iteration 4/1000 | Loss: 0.00002936
Iteration 5/1000 | Loss: 0.00002849
Iteration 6/1000 | Loss: 0.00002758
Iteration 7/1000 | Loss: 0.00002717
Iteration 8/1000 | Loss: 0.00002688
Iteration 9/1000 | Loss: 0.00002647
Iteration 10/1000 | Loss: 0.00002625
Iteration 11/1000 | Loss: 0.00002604
Iteration 12/1000 | Loss: 0.00002589
Iteration 13/1000 | Loss: 0.00002584
Iteration 14/1000 | Loss: 0.00002584
Iteration 15/1000 | Loss: 0.00002583
Iteration 16/1000 | Loss: 0.00002582
Iteration 17/1000 | Loss: 0.00002576
Iteration 18/1000 | Loss: 0.00002574
Iteration 19/1000 | Loss: 0.00002574
Iteration 20/1000 | Loss: 0.00002574
Iteration 21/1000 | Loss: 0.00002574
Iteration 22/1000 | Loss: 0.00002574
Iteration 23/1000 | Loss: 0.00002572
Iteration 24/1000 | Loss: 0.00002571
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002570
Iteration 27/1000 | Loss: 0.00002568
Iteration 28/1000 | Loss: 0.00002568
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002568
Iteration 31/1000 | Loss: 0.00002567
Iteration 32/1000 | Loss: 0.00002567
Iteration 33/1000 | Loss: 0.00002567
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002566
Iteration 36/1000 | Loss: 0.00002566
Iteration 37/1000 | Loss: 0.00002565
Iteration 38/1000 | Loss: 0.00002565
Iteration 39/1000 | Loss: 0.00002565
Iteration 40/1000 | Loss: 0.00002565
Iteration 41/1000 | Loss: 0.00002564
Iteration 42/1000 | Loss: 0.00002564
Iteration 43/1000 | Loss: 0.00002564
Iteration 44/1000 | Loss: 0.00002563
Iteration 45/1000 | Loss: 0.00002563
Iteration 46/1000 | Loss: 0.00002563
Iteration 47/1000 | Loss: 0.00002563
Iteration 48/1000 | Loss: 0.00002563
Iteration 49/1000 | Loss: 0.00002562
Iteration 50/1000 | Loss: 0.00002562
Iteration 51/1000 | Loss: 0.00002562
Iteration 52/1000 | Loss: 0.00002562
Iteration 53/1000 | Loss: 0.00002562
Iteration 54/1000 | Loss: 0.00002562
Iteration 55/1000 | Loss: 0.00002562
Iteration 56/1000 | Loss: 0.00002562
Iteration 57/1000 | Loss: 0.00002562
Iteration 58/1000 | Loss: 0.00002562
Iteration 59/1000 | Loss: 0.00002562
Iteration 60/1000 | Loss: 0.00002561
Iteration 61/1000 | Loss: 0.00002561
Iteration 62/1000 | Loss: 0.00002560
Iteration 63/1000 | Loss: 0.00002560
Iteration 64/1000 | Loss: 0.00002559
Iteration 65/1000 | Loss: 0.00002558
Iteration 66/1000 | Loss: 0.00002558
Iteration 67/1000 | Loss: 0.00002558
Iteration 68/1000 | Loss: 0.00002557
Iteration 69/1000 | Loss: 0.00002556
Iteration 70/1000 | Loss: 0.00002556
Iteration 71/1000 | Loss: 0.00002555
Iteration 72/1000 | Loss: 0.00002554
Iteration 73/1000 | Loss: 0.00002554
Iteration 74/1000 | Loss: 0.00002553
Iteration 75/1000 | Loss: 0.00002553
Iteration 76/1000 | Loss: 0.00002553
Iteration 77/1000 | Loss: 0.00002552
Iteration 78/1000 | Loss: 0.00002552
Iteration 79/1000 | Loss: 0.00002551
Iteration 80/1000 | Loss: 0.00002551
Iteration 81/1000 | Loss: 0.00002551
Iteration 82/1000 | Loss: 0.00002550
Iteration 83/1000 | Loss: 0.00002550
Iteration 84/1000 | Loss: 0.00002550
Iteration 85/1000 | Loss: 0.00002550
Iteration 86/1000 | Loss: 0.00002550
Iteration 87/1000 | Loss: 0.00002549
Iteration 88/1000 | Loss: 0.00002549
Iteration 89/1000 | Loss: 0.00002548
Iteration 90/1000 | Loss: 0.00002548
Iteration 91/1000 | Loss: 0.00002547
Iteration 92/1000 | Loss: 0.00002547
Iteration 93/1000 | Loss: 0.00002547
Iteration 94/1000 | Loss: 0.00002547
Iteration 95/1000 | Loss: 0.00002546
Iteration 96/1000 | Loss: 0.00002545
Iteration 97/1000 | Loss: 0.00002545
Iteration 98/1000 | Loss: 0.00002545
Iteration 99/1000 | Loss: 0.00002544
Iteration 100/1000 | Loss: 0.00002544
Iteration 101/1000 | Loss: 0.00002544
Iteration 102/1000 | Loss: 0.00002543
Iteration 103/1000 | Loss: 0.00002543
Iteration 104/1000 | Loss: 0.00002542
Iteration 105/1000 | Loss: 0.00002542
Iteration 106/1000 | Loss: 0.00002542
Iteration 107/1000 | Loss: 0.00002542
Iteration 108/1000 | Loss: 0.00002542
Iteration 109/1000 | Loss: 0.00002542
Iteration 110/1000 | Loss: 0.00002542
Iteration 111/1000 | Loss: 0.00002542
Iteration 112/1000 | Loss: 0.00002542
Iteration 113/1000 | Loss: 0.00002542
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002541
Iteration 116/1000 | Loss: 0.00002541
Iteration 117/1000 | Loss: 0.00002541
Iteration 118/1000 | Loss: 0.00002541
Iteration 119/1000 | Loss: 0.00002541
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002541
Iteration 123/1000 | Loss: 0.00002540
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002539
Iteration 127/1000 | Loss: 0.00002539
Iteration 128/1000 | Loss: 0.00002539
Iteration 129/1000 | Loss: 0.00002539
Iteration 130/1000 | Loss: 0.00002539
Iteration 131/1000 | Loss: 0.00002539
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002539
Iteration 134/1000 | Loss: 0.00002539
Iteration 135/1000 | Loss: 0.00002539
Iteration 136/1000 | Loss: 0.00002538
Iteration 137/1000 | Loss: 0.00002538
Iteration 138/1000 | Loss: 0.00002538
Iteration 139/1000 | Loss: 0.00002538
Iteration 140/1000 | Loss: 0.00002538
Iteration 141/1000 | Loss: 0.00002538
Iteration 142/1000 | Loss: 0.00002538
Iteration 143/1000 | Loss: 0.00002538
Iteration 144/1000 | Loss: 0.00002538
Iteration 145/1000 | Loss: 0.00002538
Iteration 146/1000 | Loss: 0.00002537
Iteration 147/1000 | Loss: 0.00002537
Iteration 148/1000 | Loss: 0.00002537
Iteration 149/1000 | Loss: 0.00002537
Iteration 150/1000 | Loss: 0.00002537
Iteration 151/1000 | Loss: 0.00002537
Iteration 152/1000 | Loss: 0.00002537
Iteration 153/1000 | Loss: 0.00002536
Iteration 154/1000 | Loss: 0.00002536
Iteration 155/1000 | Loss: 0.00002536
Iteration 156/1000 | Loss: 0.00002536
Iteration 157/1000 | Loss: 0.00002536
Iteration 158/1000 | Loss: 0.00002535
Iteration 159/1000 | Loss: 0.00002535
Iteration 160/1000 | Loss: 0.00002535
Iteration 161/1000 | Loss: 0.00002535
Iteration 162/1000 | Loss: 0.00002535
Iteration 163/1000 | Loss: 0.00002534
Iteration 164/1000 | Loss: 0.00002534
Iteration 165/1000 | Loss: 0.00002534
Iteration 166/1000 | Loss: 0.00002534
Iteration 167/1000 | Loss: 0.00002534
Iteration 168/1000 | Loss: 0.00002534
Iteration 169/1000 | Loss: 0.00002534
Iteration 170/1000 | Loss: 0.00002534
Iteration 171/1000 | Loss: 0.00002534
Iteration 172/1000 | Loss: 0.00002534
Iteration 173/1000 | Loss: 0.00002534
Iteration 174/1000 | Loss: 0.00002534
Iteration 175/1000 | Loss: 0.00002533
Iteration 176/1000 | Loss: 0.00002533
Iteration 177/1000 | Loss: 0.00002533
Iteration 178/1000 | Loss: 0.00002533
Iteration 179/1000 | Loss: 0.00002533
Iteration 180/1000 | Loss: 0.00002533
Iteration 181/1000 | Loss: 0.00002533
Iteration 182/1000 | Loss: 0.00002533
Iteration 183/1000 | Loss: 0.00002533
Iteration 184/1000 | Loss: 0.00002533
Iteration 185/1000 | Loss: 0.00002533
Iteration 186/1000 | Loss: 0.00002533
Iteration 187/1000 | Loss: 0.00002533
Iteration 188/1000 | Loss: 0.00002532
Iteration 189/1000 | Loss: 0.00002532
Iteration 190/1000 | Loss: 0.00002532
Iteration 191/1000 | Loss: 0.00002532
Iteration 192/1000 | Loss: 0.00002532
Iteration 193/1000 | Loss: 0.00002532
Iteration 194/1000 | Loss: 0.00002532
Iteration 195/1000 | Loss: 0.00002532
Iteration 196/1000 | Loss: 0.00002532
Iteration 197/1000 | Loss: 0.00002532
Iteration 198/1000 | Loss: 0.00002531
Iteration 199/1000 | Loss: 0.00002531
Iteration 200/1000 | Loss: 0.00002531
Iteration 201/1000 | Loss: 0.00002531
Iteration 202/1000 | Loss: 0.00002531
Iteration 203/1000 | Loss: 0.00002531
Iteration 204/1000 | Loss: 0.00002531
Iteration 205/1000 | Loss: 0.00002531
Iteration 206/1000 | Loss: 0.00002531
Iteration 207/1000 | Loss: 0.00002531
Iteration 208/1000 | Loss: 0.00002531
Iteration 209/1000 | Loss: 0.00002531
Iteration 210/1000 | Loss: 0.00002531
Iteration 211/1000 | Loss: 0.00002531
Iteration 212/1000 | Loss: 0.00002531
Iteration 213/1000 | Loss: 0.00002531
Iteration 214/1000 | Loss: 0.00002531
Iteration 215/1000 | Loss: 0.00002531
Iteration 216/1000 | Loss: 0.00002531
Iteration 217/1000 | Loss: 0.00002531
Iteration 218/1000 | Loss: 0.00002531
Iteration 219/1000 | Loss: 0.00002531
Iteration 220/1000 | Loss: 0.00002531
Iteration 221/1000 | Loss: 0.00002531
Iteration 222/1000 | Loss: 0.00002531
Iteration 223/1000 | Loss: 0.00002531
Iteration 224/1000 | Loss: 0.00002530
Iteration 225/1000 | Loss: 0.00002530
Iteration 226/1000 | Loss: 0.00002530
Iteration 227/1000 | Loss: 0.00002530
Iteration 228/1000 | Loss: 0.00002530
Iteration 229/1000 | Loss: 0.00002530
Iteration 230/1000 | Loss: 0.00002530
Iteration 231/1000 | Loss: 0.00002530
Iteration 232/1000 | Loss: 0.00002530
Iteration 233/1000 | Loss: 0.00002530
Iteration 234/1000 | Loss: 0.00002530
Iteration 235/1000 | Loss: 0.00002530
Iteration 236/1000 | Loss: 0.00002530
Iteration 237/1000 | Loss: 0.00002530
Iteration 238/1000 | Loss: 0.00002530
Iteration 239/1000 | Loss: 0.00002530
Iteration 240/1000 | Loss: 0.00002530
Iteration 241/1000 | Loss: 0.00002530
Iteration 242/1000 | Loss: 0.00002530
Iteration 243/1000 | Loss: 0.00002530
Iteration 244/1000 | Loss: 0.00002530
Iteration 245/1000 | Loss: 0.00002530
Iteration 246/1000 | Loss: 0.00002530
Iteration 247/1000 | Loss: 0.00002530
Iteration 248/1000 | Loss: 0.00002530
Iteration 249/1000 | Loss: 0.00002530
Iteration 250/1000 | Loss: 0.00002530
Iteration 251/1000 | Loss: 0.00002530
Iteration 252/1000 | Loss: 0.00002530
Iteration 253/1000 | Loss: 0.00002530
Iteration 254/1000 | Loss: 0.00002530
Iteration 255/1000 | Loss: 0.00002530
Iteration 256/1000 | Loss: 0.00002530
Iteration 257/1000 | Loss: 0.00002530
Iteration 258/1000 | Loss: 0.00002530
Iteration 259/1000 | Loss: 0.00002530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [2.5303739676019177e-05, 2.5303739676019177e-05, 2.5303739676019177e-05, 2.5303739676019177e-05, 2.5303739676019177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5303739676019177e-05

Optimization complete. Final v2v error: 4.167535781860352 mm

Highest mean error: 5.006252765655518 mm for frame 79

Lowest mean error: 3.580247402191162 mm for frame 170

Saving results

Total time: 47.26761555671692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0017/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875585
Iteration 2/25 | Loss: 0.00076034
Iteration 3/25 | Loss: 0.00064437
Iteration 4/25 | Loss: 0.00061658
Iteration 5/25 | Loss: 0.00060602
Iteration 6/25 | Loss: 0.00060410
Iteration 7/25 | Loss: 0.00060401
Iteration 8/25 | Loss: 0.00060401
Iteration 9/25 | Loss: 0.00060401
Iteration 10/25 | Loss: 0.00060401
Iteration 11/25 | Loss: 0.00060399
Iteration 12/25 | Loss: 0.00060399
Iteration 13/25 | Loss: 0.00060399
Iteration 14/25 | Loss: 0.00060399
Iteration 15/25 | Loss: 0.00060399
Iteration 16/25 | Loss: 0.00060399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006039942963980138, 0.0006039942963980138, 0.0006039942963980138, 0.0006039942963980138, 0.0006039942963980138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006039942963980138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39195597
Iteration 2/25 | Loss: 0.00013925
Iteration 3/25 | Loss: 0.00013924
Iteration 4/25 | Loss: 0.00013924
Iteration 5/25 | Loss: 0.00013924
Iteration 6/25 | Loss: 0.00013924
Iteration 7/25 | Loss: 0.00013924
Iteration 8/25 | Loss: 0.00013924
Iteration 9/25 | Loss: 0.00013924
Iteration 10/25 | Loss: 0.00013924
Iteration 11/25 | Loss: 0.00013924
Iteration 12/25 | Loss: 0.00013924
Iteration 13/25 | Loss: 0.00013924
Iteration 14/25 | Loss: 0.00013924
Iteration 15/25 | Loss: 0.00013924
Iteration 16/25 | Loss: 0.00013924
Iteration 17/25 | Loss: 0.00013924
Iteration 18/25 | Loss: 0.00013924
Iteration 19/25 | Loss: 0.00013924
Iteration 20/25 | Loss: 0.00013924
Iteration 21/25 | Loss: 0.00013924
Iteration 22/25 | Loss: 0.00013924
Iteration 23/25 | Loss: 0.00013924
Iteration 24/25 | Loss: 0.00013924
Iteration 25/25 | Loss: 0.00013924

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00875585
Iteration 2/25 | Loss: 0.00076019
Iteration 3/25 | Loss: 0.00063660
Iteration 4/25 | Loss: 0.00061304
Iteration 5/25 | Loss: 0.00060535
Iteration 6/25 | Loss: 0.00060399
Iteration 7/25 | Loss: 0.00060389
Iteration 8/25 | Loss: 0.00060389
Iteration 9/25 | Loss: 0.00060389
Iteration 10/25 | Loss: 0.00060389
Iteration 11/25 | Loss: 0.00060389
Iteration 12/25 | Loss: 0.00060389
Iteration 13/25 | Loss: 0.00060389
Iteration 14/25 | Loss: 0.00060389
Iteration 15/25 | Loss: 0.00060389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006038880674168468, 0.0006038880674168468, 0.0006038880674168468, 0.0006038880674168468, 0.0006038880674168468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006038880674168468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39189982
Iteration 2/25 | Loss: 0.00013692
Iteration 3/25 | Loss: 0.00013691
Iteration 4/25 | Loss: 0.00013691
Iteration 5/25 | Loss: 0.00013691
Iteration 6/25 | Loss: 0.00013691
Iteration 7/25 | Loss: 0.00013691
Iteration 8/25 | Loss: 0.00013691
Iteration 9/25 | Loss: 0.00013691
Iteration 10/25 | Loss: 0.00013691
Iteration 11/25 | Loss: 0.00013691
Iteration 12/25 | Loss: 0.00013691
Iteration 13/25 | Loss: 0.00013691
Iteration 14/25 | Loss: 0.00013691
Iteration 15/25 | Loss: 0.00013691
Iteration 16/25 | Loss: 0.00013691
Iteration 17/25 | Loss: 0.00013691
Iteration 18/25 | Loss: 0.00013691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0001369120873278007, 0.0001369120873278007, 0.0001369120873278007, 0.0001369120873278007, 0.0001369120873278007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001369120873278007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125610
Iteration 2/1000 | Loss: 0.00019893
Iteration 3/1000 | Loss: 0.00015764
Iteration 4/1000 | Loss: 0.00009658
Iteration 5/1000 | Loss: 0.00030125
Iteration 6/1000 | Loss: 0.00014599
Iteration 7/1000 | Loss: 0.00018821
Iteration 8/1000 | Loss: 0.00026584
Iteration 9/1000 | Loss: 0.00028710
Iteration 10/1000 | Loss: 0.00031293
Iteration 11/1000 | Loss: 0.00029200
Iteration 12/1000 | Loss: 0.00032610
Iteration 13/1000 | Loss: 0.00016862
Iteration 14/1000 | Loss: 0.00011604
Iteration 15/1000 | Loss: 0.00010110
Iteration 16/1000 | Loss: 0.00030781
Iteration 17/1000 | Loss: 0.00027867
Iteration 18/1000 | Loss: 0.00014571
Iteration 19/1000 | Loss: 0.00017451
Iteration 20/1000 | Loss: 0.00050237
Iteration 21/1000 | Loss: 0.00017118
Iteration 22/1000 | Loss: 0.00020009
Iteration 23/1000 | Loss: 0.00007909
Iteration 24/1000 | Loss: 0.00014784
Iteration 25/1000 | Loss: 0.00004591
Iteration 26/1000 | Loss: 0.00019234
Iteration 27/1000 | Loss: 0.00016050
Iteration 28/1000 | Loss: 0.00015599
Iteration 29/1000 | Loss: 0.00017461
Iteration 30/1000 | Loss: 0.00014904
Iteration 31/1000 | Loss: 0.00014221
Iteration 32/1000 | Loss: 0.00014985
Iteration 33/1000 | Loss: 0.00012218
Iteration 34/1000 | Loss: 0.00014731
Iteration 35/1000 | Loss: 0.00013527
Iteration 36/1000 | Loss: 0.00011111
Iteration 37/1000 | Loss: 0.00010676
Iteration 38/1000 | Loss: 0.00010473
Iteration 39/1000 | Loss: 0.00011163
Iteration 40/1000 | Loss: 0.00020787
Iteration 41/1000 | Loss: 0.00017138
Iteration 42/1000 | Loss: 0.00009199
Iteration 43/1000 | Loss: 0.00007858
Iteration 44/1000 | Loss: 0.00011964
Iteration 45/1000 | Loss: 0.00012846
Iteration 46/1000 | Loss: 0.00025570
Iteration 47/1000 | Loss: 0.00028751
Iteration 48/1000 | Loss: 0.00035792
Iteration 49/1000 | Loss: 0.00009009
Iteration 50/1000 | Loss: 0.00017397
Iteration 51/1000 | Loss: 0.00026918
Iteration 52/1000 | Loss: 0.00013015
Iteration 53/1000 | Loss: 0.00015268
Iteration 54/1000 | Loss: 0.00021386
Iteration 55/1000 | Loss: 0.00016012
Iteration 56/1000 | Loss: 0.00074416
Iteration 57/1000 | Loss: 0.00036070
Iteration 58/1000 | Loss: 0.00049785
Iteration 59/1000 | Loss: 0.00035171
Iteration 60/1000 | Loss: 0.00046594
Iteration 61/1000 | Loss: 0.00033746
Iteration 62/1000 | Loss: 0.00046969
Iteration 63/1000 | Loss: 0.00032277
Iteration 64/1000 | Loss: 0.00012828
Iteration 65/1000 | Loss: 0.00018138
Iteration 66/1000 | Loss: 0.00015180
Iteration 67/1000 | Loss: 0.00015971
Iteration 68/1000 | Loss: 0.00050572
Iteration 69/1000 | Loss: 0.00023832
Iteration 70/1000 | Loss: 0.00071373
Iteration 71/1000 | Loss: 0.00025244
Iteration 72/1000 | Loss: 0.00024061
Iteration 73/1000 | Loss: 0.00041136
Iteration 74/1000 | Loss: 0.00064030
Iteration 75/1000 | Loss: 0.00052777
Iteration 76/1000 | Loss: 0.00016762
Iteration 77/1000 | Loss: 0.00017043
Iteration 78/1000 | Loss: 0.00018425
Iteration 79/1000 | Loss: 0.00023887
Iteration 80/1000 | Loss: 0.00037864
Iteration 81/1000 | Loss: 0.00033203
Iteration 82/1000 | Loss: 0.00012433
Iteration 83/1000 | Loss: 0.00006116
Iteration 84/1000 | Loss: 0.00024525
Iteration 85/1000 | Loss: 0.00049662
Iteration 86/1000 | Loss: 0.00033251
Iteration 87/1000 | Loss: 0.00033366
Iteration 88/1000 | Loss: 0.00009322
Iteration 89/1000 | Loss: 0.00026652
Iteration 90/1000 | Loss: 0.00070700
Iteration 91/1000 | Loss: 0.00044286
Iteration 92/1000 | Loss: 0.00034215
Iteration 93/1000 | Loss: 0.00031751
Iteration 94/1000 | Loss: 0.00031362
Iteration 95/1000 | Loss: 0.00080954
Iteration 96/1000 | Loss: 0.00031012
Iteration 97/1000 | Loss: 0.00032247
Iteration 98/1000 | Loss: 0.00027319
Iteration 99/1000 | Loss: 0.00037273
Iteration 100/1000 | Loss: 0.00028182
Iteration 101/1000 | Loss: 0.00027382
Iteration 102/1000 | Loss: 0.00025274
Iteration 103/1000 | Loss: 0.00010282
Iteration 104/1000 | Loss: 0.00025198
Iteration 105/1000 | Loss: 0.00023490
Iteration 106/1000 | Loss: 0.00005590
Iteration 107/1000 | Loss: 0.00023210
Iteration 108/1000 | Loss: 0.00026040
Iteration 109/1000 | Loss: 0.00052575
Iteration 110/1000 | Loss: 0.00032265
Iteration 111/1000 | Loss: 0.00034699
Iteration 112/1000 | Loss: 0.00024165
Iteration 113/1000 | Loss: 0.00019712
Iteration 114/1000 | Loss: 0.00011930
Iteration 115/1000 | Loss: 0.00009330
Iteration 116/1000 | Loss: 0.00017796
Iteration 117/1000 | Loss: 0.00016546
Iteration 118/1000 | Loss: 0.00017442
Iteration 119/1000 | Loss: 0.00026231
Iteration 120/1000 | Loss: 0.00048448
Iteration 121/1000 | Loss: 0.00011029
Iteration 122/1000 | Loss: 0.00006062
Iteration 123/1000 | Loss: 0.00018152
Iteration 124/1000 | Loss: 0.00009718
Iteration 125/1000 | Loss: 0.00007837
Iteration 126/1000 | Loss: 0.00003566
Iteration 127/1000 | Loss: 0.00003583
Iteration 128/1000 | Loss: 0.00003079
Iteration 129/1000 | Loss: 0.00008641
Iteration 130/1000 | Loss: 0.00011076
Iteration 131/1000 | Loss: 0.00004683
Iteration 132/1000 | Loss: 0.00008140
Iteration 133/1000 | Loss: 0.00009104
Iteration 134/1000 | Loss: 0.00013809
Iteration 135/1000 | Loss: 0.00003230
Iteration 136/1000 | Loss: 0.00003428
Iteration 137/1000 | Loss: 0.00003162
Iteration 138/1000 | Loss: 0.00003686
Iteration 139/1000 | Loss: 0.00003248
Iteration 140/1000 | Loss: 0.00002388
Iteration 141/1000 | Loss: 0.00002288
Iteration 142/1000 | Loss: 0.00040527
Iteration 143/1000 | Loss: 0.00016203
Iteration 144/1000 | Loss: 0.00006829
Iteration 145/1000 | Loss: 0.00009671
Iteration 146/1000 | Loss: 0.00008755
Iteration 147/1000 | Loss: 0.00004086
Iteration 148/1000 | Loss: 0.00003681
Iteration 149/1000 | Loss: 0.00054207
Iteration 150/1000 | Loss: 0.00064258
Iteration 151/1000 | Loss: 0.00055037
Iteration 152/1000 | Loss: 0.00039343
Iteration 153/1000 | Loss: 0.00053178
Iteration 154/1000 | Loss: 0.00066228
Iteration 155/1000 | Loss: 0.00063998
Iteration 156/1000 | Loss: 0.00055106
Iteration 157/1000 | Loss: 0.00054171
Iteration 158/1000 | Loss: 0.00063508
Iteration 159/1000 | Loss: 0.00051346
Iteration 160/1000 | Loss: 0.00060017
Iteration 161/1000 | Loss: 0.00044206
Iteration 162/1000 | Loss: 0.00062159
Iteration 163/1000 | Loss: 0.00040135
Iteration 164/1000 | Loss: 0.00037973
Iteration 165/1000 | Loss: 0.00004032
Iteration 166/1000 | Loss: 0.00003185
Iteration 167/1000 | Loss: 0.00002668
Iteration 168/1000 | Loss: 0.00003910
Iteration 169/1000 | Loss: 0.00003347
Iteration 170/1000 | Loss: 0.00003318
Iteration 171/1000 | Loss: 0.00002119
Iteration 172/1000 | Loss: 0.00001986
Iteration 173/1000 | Loss: 0.00008487
Iteration 174/1000 | Loss: 0.00007372
Iteration 175/1000 | Loss: 0.00002212
Iteration 176/1000 | Loss: 0.00001951
Iteration 177/1000 | Loss: 0.00001805
Iteration 178/1000 | Loss: 0.00001743
Iteration 179/1000 | Loss: 0.00001677
Iteration 180/1000 | Loss: 0.00001627
Iteration 181/1000 | Loss: 0.00013617
Iteration 182/1000 | Loss: 0.00006442
Iteration 183/1000 | Loss: 0.00001576
Iteration 184/1000 | Loss: 0.00001553
Iteration 185/1000 | Loss: 0.00001553
Iteration 186/1000 | Loss: 0.00001535
Iteration 187/1000 | Loss: 0.00001535
Iteration 188/1000 | Loss: 0.00001533
Iteration 189/1000 | Loss: 0.00001532
Iteration 190/1000 | Loss: 0.00001531
Iteration 191/1000 | Loss: 0.00001531
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001524
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00001523
Iteration 198/1000 | Loss: 0.00001522
Iteration 199/1000 | Loss: 0.00001522
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001522
Iteration 204/1000 | Loss: 0.00001522
Iteration 205/1000 | Loss: 0.00001522
Iteration 206/1000 | Loss: 0.00001522
Iteration 207/1000 | Loss: 0.00001522
Iteration 208/1000 | Loss: 0.00001521
Iteration 209/1000 | Loss: 0.00001521
Iteration 210/1000 | Loss: 0.00001520
Iteration 211/1000 | Loss: 0.00001519
Iteration 212/1000 | Loss: 0.00001518
Iteration 213/1000 | Loss: 0.00001518
Iteration 214/1000 | Loss: 0.00001518
Iteration 215/1000 | Loss: 0.00001518
Iteration 216/1000 | Loss: 0.00001517
Iteration 217/1000 | Loss: 0.00001517
Iteration 218/1000 | Loss: 0.00001517
Iteration 219/1000 | Loss: 0.00001516
Iteration 220/1000 | Loss: 0.00001516
Iteration 221/1000 | Loss: 0.00001516
Iteration 222/1000 | Loss: 0.00001516
Iteration 223/1000 | Loss: 0.00001516
Iteration 224/1000 | Loss: 0.00001516
Iteration 225/1000 | Loss: 0.00001516
Iteration 226/1000 | Loss: 0.00001516
Iteration 227/1000 | Loss: 0.00001515
Iteration 228/1000 | Loss: 0.00001515
Iteration 229/1000 | Loss: 0.00001514
Iteration 230/1000 | Loss: 0.00001514
Iteration 231/1000 | Loss: 0.00001514
Iteration 232/1000 | Loss: 0.00001514
Iteration 233/1000 | Loss: 0.00001513
Iteration 234/1000 | Loss: 0.00001512
Iteration 235/1000 | Loss: 0.00001512
Iteration 236/1000 | Loss: 0.00001511
Iteration 237/1000 | Loss: 0.00001511
Iteration 238/1000 | Loss: 0.00001511
Iteration 239/1000 | Loss: 0.00001511
Iteration 240/1000 | Loss: 0.00001510
Iteration 241/1000 | Loss: 0.00001510
Iteration 242/1000 | Loss: 0.00001509
Iteration 243/1000 | Loss: 0.00001509
Iteration 244/1000 | Loss: 0.00001509
Iteration 245/1000 | Loss: 0.00001509
Iteration 246/1000 | Loss: 0.00001509
Iteration 247/1000 | Loss: 0.00001509
Iteration 248/1000 | Loss: 0.00001509
Iteration 249/1000 | Loss: 0.00001508
Iteration 250/1000 | Loss: 0.00001508
Iteration 251/1000 | Loss: 0.00001508
Iteration 252/1000 | Loss: 0.00001508
Iteration 253/1000 | Loss: 0.00001508
Iteration 254/1000 | Loss: 0.00001508
Iteration 255/1000 | Loss: 0.00001507
Iteration 256/1000 | Loss: 0.00001507
Iteration 257/1000 | Loss: 0.00001507
Iteration 258/1000 | Loss: 0.00001506
Iteration 259/1000 | Loss: 0.00001506
Iteration 260/1000 | Loss: 0.00001506
Iteration 261/1000 | Loss: 0.00001506
Iteration 262/1000 | Loss: 0.00001505
Iteration 263/1000 | Loss: 0.00001505
Iteration 264/1000 | Loss: 0.00001505
Iteration 265/1000 | Loss: 0.00001504
Iteration 266/1000 | Loss: 0.00001504
Iteration 267/1000 | Loss: 0.00001504
Iteration 268/1000 | Loss: 0.00001504
Iteration 269/1000 | Loss: 0.00001504
Iteration 270/1000 | Loss: 0.00001504
Iteration 271/1000 | Loss: 0.00001503
Iteration 272/1000 | Loss: 0.00001503
Iteration 273/1000 | Loss: 0.00001503
Iteration 274/1000 | Loss: 0.00001502
Iteration 275/1000 | Loss: 0.00001502
Iteration 276/1000 | Loss: 0.00001502
Iteration 277/1000 | Loss: 0.00001502
Iteration 278/1000 | Loss: 0.00001501
Iteration 279/1000 | Loss: 0.00001501
Iteration 280/1000 | Loss: 0.00001501
Iteration 281/1000 | Loss: 0.00001501
Iteration 282/1000 | Loss: 0.00001500
Iteration 283/1000 | Loss: 0.00001500
Iteration 284/1000 | Loss: 0.00001500
Iteration 285/1000 | Loss: 0.00001500
Iteration 286/1000 | Loss: 0.00001500
Iteration 287/1000 | Loss: 0.00001500
Iteration 288/1000 | Loss: 0.00001500
Iteration 289/1000 | Loss: 0.00001500
Iteration 290/1000 | Loss: 0.00001499
Iteration 291/1000 | Loss: 0.00001499
Iteration 292/1000 | Loss: 0.00001498
Iteration 293/1000 | Loss: 0.00007759
Iteration 294/1000 | Loss: 0.00020802
Iteration 295/1000 | Loss: 0.00011936
Iteration 296/1000 | Loss: 0.00001890
Iteration 297/1000 | Loss: 0.00001699
Iteration 298/1000 | Loss: 0.00001593
Iteration 299/1000 | Loss: 0.00001499
Iteration 300/1000 | Loss: 0.00001426
Iteration 301/1000 | Loss: 0.00001398
Iteration 302/1000 | Loss: 0.00001388
Iteration 303/1000 | Loss: 0.00001385
Iteration 304/1000 | Loss: 0.00001382
Iteration 305/1000 | Loss: 0.00001381
Iteration 306/1000 | Loss: 0.00001379
Iteration 307/1000 | Loss: 0.00001378
Iteration 308/1000 | Loss: 0.00001377
Iteration 309/1000 | Loss: 0.00001376
Iteration 310/1000 | Loss: 0.00001376
Iteration 311/1000 | Loss: 0.00001375
Iteration 312/1000 | Loss: 0.00001375
Iteration 313/1000 | Loss: 0.00001374
Iteration 314/1000 | Loss: 0.00001374
Iteration 315/1000 | Loss: 0.00001373
Iteration 316/1000 | Loss: 0.00001372
Iteration 317/1000 | Loss: 0.00001371
Iteration 318/1000 | Loss: 0.00001371
Iteration 319/1000 | Loss: 0.00001370
Iteration 320/1000 | Loss: 0.00001370
Iteration 321/1000 | Loss: 0.00001369
Iteration 322/1000 | Loss: 0.00001369
Iteration 323/1000 | Loss: 0.00001368
Iteration 324/1000 | Loss: 0.00001368
Iteration 325/1000 | Loss: 0.00001367
Iteration 326/1000 | Loss: 0.00001367
Iteration 327/1000 | Loss: 0.00001367
Iteration 328/1000 | Loss: 0.00001366
Iteration 329/1000 | Loss: 0.00001366
Iteration 330/1000 | Loss: 0.00001365
Iteration 331/1000 | Loss: 0.00001365
Iteration 332/1000 | Loss: 0.00001363
Iteration 333/1000 | Loss: 0.00001363
Iteration 334/1000 | Loss: 0.00001363
Iteration 335/1000 | Loss: 0.00001363
Iteration 336/1000 | Loss: 0.00001363
Iteration 337/1000 | Loss: 0.00001363
Iteration 338/1000 | Loss: 0.00001363
Iteration 339/1000 | Loss: 0.00001363
Iteration 340/1000 | Loss: 0.00001363
Iteration 341/1000 | Loss: 0.00001363
Iteration 342/1000 | Loss: 0.00001363
Iteration 343/1000 | Loss: 0.00001362
Iteration 344/1000 | Loss: 0.00001362
Iteration 345/1000 | Loss: 0.00001362
Iteration 346/1000 | Loss: 0.00001362
Iteration 347/1000 | Loss: 0.00001362
Iteration 348/1000 | Loss: 0.00001362
Iteration 349/1000 | Loss: 0.00001362
Iteration 350/1000 | Loss: 0.00001362
Iteration 351/1000 | Loss: 0.00001361
Iteration 352/1000 | Loss: 0.00001361
Iteration 353/1000 | Loss: 0.00001361
Iteration 354/1000 | Loss: 0.00001358
Iteration 355/1000 | Loss: 0.00001357
Iteration 356/1000 | Loss: 0.00001355
Iteration 357/1000 | Loss: 0.00001354
Iteration 358/1000 | Loss: 0.00001354
Iteration 359/1000 | Loss: 0.00001353
Iteration 360/1000 | Loss: 0.00001353
Iteration 361/1000 | Loss: 0.00001353
Iteration 362/1000 | Loss: 0.00001353
Iteration 363/1000 | Loss: 0.00001352
Iteration 364/1000 | Loss: 0.00001352
Iteration 365/1000 | Loss: 0.00001352
Iteration 366/1000 | Loss: 0.00001352
Iteration 367/1000 | Loss: 0.00001352
Iteration 368/1000 | Loss: 0.00001352
Iteration 369/1000 | Loss: 0.00001352
Iteration 370/1000 | Loss: 0.00001351
Iteration 371/1000 | Loss: 0.00001351
Iteration 372/1000 | Loss: 0.00001351
Iteration 373/1000 | Loss: 0.00001351
Iteration 374/1000 | Loss: 0.00001351
Iteration 375/1000 | Loss: 0.00001351
Iteration 376/1000 | Loss: 0.00001351
Iteration 377/1000 | Loss: 0.00001351
Iteration 378/1000 | Loss: 0.00001351
Iteration 379/1000 | Loss: 0.00001350
Iteration 380/1000 | Loss: 0.00001350
Iteration 381/1000 | Loss: 0.00001350
Iteration 382/1000 | Loss: 0.00001350
Iteration 383/1000 | Loss: 0.00001350
Iteration 384/1000 | Loss: 0.00001350
Iteration 385/1000 | Loss: 0.00001350
Iteration 386/1000 | Loss: 0.00001350
Iteration 387/1000 | Loss: 0.00001350
Iteration 388/1000 | Loss: 0.00001350
Iteration 389/1000 | Loss: 0.00001350
Iteration 390/1000 | Loss: 0.00001349
Iteration 391/1000 | Loss: 0.00001349
Iteration 392/1000 | Loss: 0.00001349
Iteration 393/1000 | Loss: 0.00001349
Iteration 394/1000 | Loss: 0.00001349
Iteration 395/1000 | Loss: 0.00001349
Iteration 396/1000 | Loss: 0.00001349
Iteration 397/1000 | Loss: 0.00001349
Iteration 398/1000 | Loss: 0.00001349
Iteration 399/1000 | Loss: 0.00001349
Iteration 400/1000 | Loss: 0.00001349
Iteration 401/1000 | Loss: 0.00001349
Iteration 402/1000 | Loss: 0.00001349
Iteration 403/1000 | Loss: 0.00001349
Iteration 404/1000 | Loss: 0.00001348
Iteration 405/1000 | Loss: 0.00001348
Iteration 406/1000 | Loss: 0.00001348
Iteration 407/1000 | Loss: 0.00001348
Iteration 408/1000 | Loss: 0.00001348
Iteration 409/1000 | Loss: 0.00001348
Iteration 410/1000 | Loss: 0.00001348
Iteration 411/1000 | Loss: 0.00001348
Iteration 412/1000 | Loss: 0.00001348
Iteration 413/1000 | Loss: 0.00001348
Iteration 414/1000 | Loss: 0.00001348
Iteration 415/1000 | Loss: 0.00001348
Iteration 416/1000 | Loss: 0.00001348
Iteration 417/1000 | Loss: 0.00001347
Iteration 418/1000 | Loss: 0.00001347
Iteration 419/1000 | Loss: 0.00001347
Iteration 420/1000 | Loss: 0.00001347
Iteration 421/1000 | Loss: 0.00001347
Iteration 422/1000 | Loss: 0.00001347
Iteration 423/1000 | Loss: 0.00001347
Iteration 424/1000 | Loss: 0.00001347
Iteration 425/1000 | Loss: 0.00001347
Iteration 426/1000 | Loss: 0.00001347
Iteration 427/1000 | Loss: 0.00001346
Iteration 428/1000 | Loss: 0.00001346
Iteration 429/1000 | Loss: 0.00001346
Iteration 430/1000 | Loss: 0.00001346
Iteration 431/1000 | Loss: 0.00001346
Iteration 432/1000 | Loss: 0.00001346
Iteration 433/1000 | Loss: 0.00001346
Iteration 434/1000 | Loss: 0.00001346
Iteration 435/1000 | Loss: 0.00001346
Iteration 436/1000 | Loss: 0.00001346
Iteration 437/1000 | Loss: 0.00001346
Iteration 438/1000 | Loss: 0.00001345
Iteration 439/1000 | Loss: 0.00001345
Iteration 440/1000 | Loss: 0.00001345
Iteration 441/1000 | Loss: 0.00001345
Iteration 442/1000 | Loss: 0.00001345
Iteration 443/1000 | Loss: 0.00001345
Iteration 444/1000 | Loss: 0.00001345
Iteration 445/1000 | Loss: 0.00001345
Iteration 446/1000 | Loss: 0.00001345
Iteration 447/1000 | Loss: 0.00001345
Iteration 448/1000 | Loss: 0.00001345
Iteration 449/1000 | Loss: 0.00001345
Iteration 450/1000 | Loss: 0.00001345
Iteration 451/1000 | Loss: 0.00001344
Iteration 452/1000 | Loss: 0.00001344
Iteration 453/1000 | Loss: 0.00001344
Iteration 454/1000 | Loss: 0.00001344
Iteration 455/1000 | Loss: 0.00001344
Iteration 456/1000 | Loss: 0.00001344
Iteration 457/1000 | Loss: 0.00001344
Iteration 458/1000 | Loss: 0.00001344
Iteration 459/1000 | Loss: 0.00001344
Iteration 460/1000 | Loss: 0.00001344
Iteration 461/1000 | Loss: 0.00001344
Iteration 462/1000 | Loss: 0.00001344
Iteration 463/1000 | Loss: 0.00001344
Iteration 464/1000 | Loss: 0.00001344
Iteration 465/1000 | Loss: 0.00001344
Iteration 466/1000 | Loss: 0.00001344
Iteration 467/1000 | Loss: 0.00001343
Iteration 468/1000 | Loss: 0.00001343
Iteration 469/1000 | Loss: 0.00001343
Iteration 470/1000 | Loss: 0.00001343
Iteration 471/1000 | Loss: 0.00001343
Iteration 472/1000 | Loss: 0.00001343
Iteration 473/1000 | Loss: 0.00001343
Iteration 474/1000 | Loss: 0.00001343
Iteration 475/1000 | Loss: 0.00001343
Iteration 476/1000 | Loss: 0.00001343
Iteration 477/1000 | Loss: 0.00001343
Iteration 478/1000 | Loss: 0.00001343
Iteration 479/1000 | Loss: 0.00001343
Iteration 480/1000 | Loss: 0.00001343
Iteration 481/1000 | Loss: 0.00001343
Iteration 482/1000 | Loss: 0.00001343
Iteration 483/1000 | Loss: 0.00001343
Iteration 484/1000 | Loss: 0.00001343
Iteration 485/1000 | Loss: 0.00001343
Iteration 486/1000 | Loss: 0.00001343
Iteration 487/1000 | Loss: 0.00001343
Iteration 488/1000 | Loss: 0.00001343
Iteration 489/1000 | Loss: 0.00001343
Iteration 490/1000 | Loss: 0.00001343
Iteration 491/1000 | Loss: 0.00001343
Iteration 492/1000 | Loss: 0.00001343
Iteration 493/1000 | Loss: 0.00001343
Iteration 494/1000 | Loss: 0.00001343
Iteration 495/1000 | Loss: 0.00001343
Iteration 496/1000 | Loss: 0.00001343
Iteration 497/1000 | Loss: 0.00001343
Iteration 498/1000 | Loss: 0.00001343
Iteration 499/1000 | Loss: 0.00001343
Iteration 500/1000 | Loss: 0.00001343
Iteration 501/1000 | Loss: 0.00001343
Iteration 502/1000 | Loss: 0.00001343
Iteration 503/1000 | Loss: 0.00001343
Iteration 504/1000 | Loss: 0.00001343
Iteration 505/1000 | Loss: 0.00001343
Iteration 506/1000 | Loss: 0.00001343
Iteration 507/1000 | Loss: 0.00001343
Iteration 508/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 508. Stopping optimization.
Last 5 losses: [1.3429075806925539e-05, 1.3429075806925539e-05, 1.3429075806925539e-05, 1.3429075806925539e-05, 1.3429075806925539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3429075806925539e-05

Optimization complete. Final v2v error: 2.8448636531829834 mm

Highest mean error: 5.84337854385376 mm for frame 199

Lowest mean error: 2.341747283935547 mm for frame 114

Saving results

Total time: 381.45730423927307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00013924
Iteration 2/1000 | Loss: 0.00003940
Iteration 3/1000 | Loss: 0.00003061
Iteration 4/1000 | Loss: 0.00002858
Iteration 5/1000 | Loss: 0.00002662
Iteration 6/1000 | Loss: 0.00002552
Iteration 7/1000 | Loss: 0.00002424
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002345
Iteration 10/1000 | Loss: 0.00002335
Iteration 11/1000 | Loss: 0.00002311
Iteration 12/1000 | Loss: 0.00002305
Iteration 13/1000 | Loss: 0.00002301
Iteration 14/1000 | Loss: 0.00002295
Iteration 15/1000 | Loss: 0.00002294
Iteration 16/1000 | Loss: 0.00002294
Iteration 17/1000 | Loss: 0.00002293
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002292
Iteration 20/1000 | Loss: 0.00002289
Iteration 21/1000 | Loss: 0.00002283
Iteration 22/1000 | Loss: 0.00002277
Iteration 23/1000 | Loss: 0.00002273
Iteration 24/1000 | Loss: 0.00002273
Iteration 25/1000 | Loss: 0.00002272
Iteration 26/1000 | Loss: 0.00002272
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002270
Iteration 29/1000 | Loss: 0.00002270
Iteration 30/1000 | Loss: 0.00002269
Iteration 31/1000 | Loss: 0.00002268
Iteration 32/1000 | Loss: 0.00002268
Iteration 33/1000 | Loss: 0.00002268
Iteration 34/1000 | Loss: 0.00002268
Iteration 35/1000 | Loss: 0.00002268
Iteration 36/1000 | Loss: 0.00002268
Iteration 37/1000 | Loss: 0.00002268
Iteration 38/1000 | Loss: 0.00002267
Iteration 39/1000 | Loss: 0.00002267
Iteration 40/1000 | Loss: 0.00002267
Iteration 41/1000 | Loss: 0.00002267
Iteration 42/1000 | Loss: 0.00002267
Iteration 43/1000 | Loss: 0.00002267
Iteration 44/1000 | Loss: 0.00002267
Iteration 45/1000 | Loss: 0.00002267
Iteration 46/1000 | Loss: 0.00002267
Iteration 47/1000 | Loss: 0.00002267
Iteration 48/1000 | Loss: 0.00002267
Iteration 49/1000 | Loss: 0.00002267
Iteration 50/1000 | Loss: 0.00002267
Iteration 51/1000 | Loss: 0.00002267
Iteration 52/1000 | Loss: 0.00002267
Iteration 53/1000 | Loss: 0.00002267
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002264
Iteration 57/1000 | Loss: 0.00002263
Iteration 58/1000 | Loss: 0.00002263
Iteration 59/1000 | Loss: 0.00002262
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002260
Iteration 65/1000 | Loss: 0.00002260
Iteration 66/1000 | Loss: 0.00002260
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002258
Iteration 74/1000 | Loss: 0.00002258
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00002258
Iteration 77/1000 | Loss: 0.00002257
Iteration 78/1000 | Loss: 0.00002257
Iteration 79/1000 | Loss: 0.00002257
Iteration 80/1000 | Loss: 0.00002257
Iteration 81/1000 | Loss: 0.00002257
Iteration 82/1000 | Loss: 0.00002257
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002256
Iteration 85/1000 | Loss: 0.00002256
Iteration 86/1000 | Loss: 0.00002256
Iteration 87/1000 | Loss: 0.00002256
Iteration 88/1000 | Loss: 0.00002256
Iteration 89/1000 | Loss: 0.00002256
Iteration 90/1000 | Loss: 0.00002256
Iteration 91/1000 | Loss: 0.00002256
Iteration 92/1000 | Loss: 0.00002256
Iteration 93/1000 | Loss: 0.00002255
Iteration 94/1000 | Loss: 0.00002255
Iteration 95/1000 | Loss: 0.00002255
Iteration 96/1000 | Loss: 0.00002255
Iteration 97/1000 | Loss: 0.00002255
Iteration 98/1000 | Loss: 0.00002255
Iteration 99/1000 | Loss: 0.00002255
Iteration 100/1000 | Loss: 0.00002254
Iteration 101/1000 | Loss: 0.00002254
Iteration 102/1000 | Loss: 0.00002254
Iteration 103/1000 | Loss: 0.00002254
Iteration 104/1000 | Loss: 0.00002254
Iteration 105/1000 | Loss: 0.00002254
Iteration 106/1000 | Loss: 0.00002254
Iteration 107/1000 | Loss: 0.00002254
Iteration 108/1000 | Loss: 0.00002254
Iteration 109/1000 | Loss: 0.00002254
Iteration 110/1000 | Loss: 0.00002254
Iteration 111/1000 | Loss: 0.00002254
Iteration 112/1000 | Loss: 0.00002254
Iteration 113/1000 | Loss: 0.00002254
Iteration 114/1000 | Loss: 0.00002254
Iteration 115/1000 | Loss: 0.00002254
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Iteration 118/1000 | Loss: 0.00002253
Iteration 119/1000 | Loss: 0.00002253
Iteration 120/1000 | Loss: 0.00002253
Iteration 121/1000 | Loss: 0.00002253
Iteration 122/1000 | Loss: 0.00002253
Iteration 123/1000 | Loss: 0.00002253
Iteration 124/1000 | Loss: 0.00002253
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002252
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002252
Iteration 135/1000 | Loss: 0.00002252
Iteration 136/1000 | Loss: 0.00002252
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002251
Iteration 141/1000 | Loss: 0.00002251
Iteration 142/1000 | Loss: 0.00002251
Iteration 143/1000 | Loss: 0.00002250
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002250
Iteration 146/1000 | Loss: 0.00002250
Iteration 147/1000 | Loss: 0.00002250
Iteration 148/1000 | Loss: 0.00002250
Iteration 149/1000 | Loss: 0.00002250
Iteration 150/1000 | Loss: 0.00002250
Iteration 151/1000 | Loss: 0.00002250
Iteration 152/1000 | Loss: 0.00002250
Iteration 153/1000 | Loss: 0.00002250
Iteration 154/1000 | Loss: 0.00002250
Iteration 155/1000 | Loss: 0.00002250
Iteration 156/1000 | Loss: 0.00002249
Iteration 157/1000 | Loss: 0.00002249
Iteration 158/1000 | Loss: 0.00002249
Iteration 159/1000 | Loss: 0.00002249
Iteration 160/1000 | Loss: 0.00002249
Iteration 161/1000 | Loss: 0.00002249
Iteration 162/1000 | Loss: 0.00002249
Iteration 163/1000 | Loss: 0.00002249
Iteration 164/1000 | Loss: 0.00002249
Iteration 165/1000 | Loss: 0.00002249
Iteration 166/1000 | Loss: 0.00002249
Iteration 167/1000 | Loss: 0.00002249
Iteration 168/1000 | Loss: 0.00002249
Iteration 169/1000 | Loss: 0.00002249
Iteration 170/1000 | Loss: 0.00002249
Iteration 171/1000 | Loss: 0.00002249
Iteration 172/1000 | Loss: 0.00002249
Iteration 173/1000 | Loss: 0.00002249
Iteration 174/1000 | Loss: 0.00002249
Iteration 175/1000 | Loss: 0.00002249
Iteration 176/1000 | Loss: 0.00002249
Iteration 177/1000 | Loss: 0.00002249
Iteration 178/1000 | Loss: 0.00002249
Iteration 179/1000 | Loss: 0.00002248
Iteration 180/1000 | Loss: 0.00002248
Iteration 181/1000 | Loss: 0.00002248
Iteration 182/1000 | Loss: 0.00002248
Iteration 183/1000 | Loss: 0.00002248
Iteration 184/1000 | Loss: 0.00002248
Iteration 185/1000 | Loss: 0.00002248
Iteration 186/1000 | Loss: 0.00002248
Iteration 187/1000 | Loss: 0.00002248
Iteration 188/1000 | Loss: 0.00002248
Iteration 189/1000 | Loss: 0.00002248
Iteration 190/1000 | Loss: 0.00002248
Iteration 191/1000 | Loss: 0.00002248
Iteration 192/1000 | Loss: 0.00002248
Iteration 193/1000 | Loss: 0.00002248
Iteration 194/1000 | Loss: 0.00002248
Iteration 195/1000 | Loss: 0.00002248
Iteration 196/1000 | Loss: 0.00002248
Iteration 197/1000 | Loss: 0.00002248
Iteration 198/1000 | Loss: 0.00002248
Iteration 199/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.2480040570371784e-05, 2.2480040570371784e-05, 2.2480040570371784e-05, 2.2480040570371784e-05, 2.2480040570371784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2480040570371784e-05

Optimization complete. Final v2v error: 3.957625389099121 mm

Highest mean error: 4.4332404136657715 mm for frame 99

Lowest mean error: 3.6935675144195557 mm for frame 45

Saving results

Total time: 41.378976583480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00013691
Iteration 2/1000 | Loss: 0.00004387
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00002967
Iteration 5/1000 | Loss: 0.00002806
Iteration 6/1000 | Loss: 0.00002633
Iteration 7/1000 | Loss: 0.00002506
Iteration 8/1000 | Loss: 0.00002406
Iteration 9/1000 | Loss: 0.00002364
Iteration 10/1000 | Loss: 0.00002339
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002302
Iteration 13/1000 | Loss: 0.00002301
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00002300
Iteration 16/1000 | Loss: 0.00002300
Iteration 17/1000 | Loss: 0.00002294
Iteration 18/1000 | Loss: 0.00002294
Iteration 19/1000 | Loss: 0.00002294
Iteration 20/1000 | Loss: 0.00002291
Iteration 21/1000 | Loss: 0.00002287
Iteration 22/1000 | Loss: 0.00002287
Iteration 23/1000 | Loss: 0.00002286
Iteration 24/1000 | Loss: 0.00002283
Iteration 25/1000 | Loss: 0.00002282
Iteration 26/1000 | Loss: 0.00002281
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00002280
Iteration 29/1000 | Loss: 0.00002280
Iteration 30/1000 | Loss: 0.00002279
Iteration 31/1000 | Loss: 0.00002279
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002276
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00002275
Iteration 39/1000 | Loss: 0.00002274
Iteration 40/1000 | Loss: 0.00002274
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002272
Iteration 43/1000 | Loss: 0.00002272
Iteration 44/1000 | Loss: 0.00002272
Iteration 45/1000 | Loss: 0.00002271
Iteration 46/1000 | Loss: 0.00002271
Iteration 47/1000 | Loss: 0.00002270
Iteration 48/1000 | Loss: 0.00002268
Iteration 49/1000 | Loss: 0.00002268
Iteration 50/1000 | Loss: 0.00002268
Iteration 51/1000 | Loss: 0.00002267
Iteration 52/1000 | Loss: 0.00002266
Iteration 53/1000 | Loss: 0.00002266
Iteration 54/1000 | Loss: 0.00002265
Iteration 55/1000 | Loss: 0.00002265
Iteration 56/1000 | Loss: 0.00002265
Iteration 57/1000 | Loss: 0.00002264
Iteration 58/1000 | Loss: 0.00002263
Iteration 59/1000 | Loss: 0.00002261
Iteration 60/1000 | Loss: 0.00002261
Iteration 61/1000 | Loss: 0.00002261
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002257
Iteration 71/1000 | Loss: 0.00002257
Iteration 72/1000 | Loss: 0.00002257
Iteration 73/1000 | Loss: 0.00002257
Iteration 74/1000 | Loss: 0.00002256
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002256
Iteration 77/1000 | Loss: 0.00002256
Iteration 78/1000 | Loss: 0.00002256
Iteration 79/1000 | Loss: 0.00002256
Iteration 80/1000 | Loss: 0.00002256
Iteration 81/1000 | Loss: 0.00002255
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002254
Iteration 90/1000 | Loss: 0.00002254
Iteration 91/1000 | Loss: 0.00002254
Iteration 92/1000 | Loss: 0.00002254
Iteration 93/1000 | Loss: 0.00002254
Iteration 94/1000 | Loss: 0.00002254
Iteration 95/1000 | Loss: 0.00002254
Iteration 96/1000 | Loss: 0.00002254
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002254
Iteration 99/1000 | Loss: 0.00002254
Iteration 100/1000 | Loss: 0.00002254
Iteration 101/1000 | Loss: 0.00002253
Iteration 102/1000 | Loss: 0.00002253
Iteration 103/1000 | Loss: 0.00002253
Iteration 104/1000 | Loss: 0.00002253
Iteration 105/1000 | Loss: 0.00002253
Iteration 106/1000 | Loss: 0.00002253
Iteration 107/1000 | Loss: 0.00002253
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002253
Iteration 112/1000 | Loss: 0.00002252
Iteration 113/1000 | Loss: 0.00002252
Iteration 114/1000 | Loss: 0.00002252
Iteration 115/1000 | Loss: 0.00002252
Iteration 116/1000 | Loss: 0.00002252
Iteration 117/1000 | Loss: 0.00002252
Iteration 118/1000 | Loss: 0.00002252
Iteration 119/1000 | Loss: 0.00002252
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002252
Iteration 122/1000 | Loss: 0.00002252
Iteration 123/1000 | Loss: 0.00002252
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002251
Iteration 130/1000 | Loss: 0.00002251
Iteration 131/1000 | Loss: 0.00002251
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002250
Iteration 137/1000 | Loss: 0.00002250
Iteration 138/1000 | Loss: 0.00002250
Iteration 139/1000 | Loss: 0.00002250
Iteration 140/1000 | Loss: 0.00002250
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002250
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002249
Iteration 146/1000 | Loss: 0.00002249
Iteration 147/1000 | Loss: 0.00002249
Iteration 148/1000 | Loss: 0.00002249
Iteration 149/1000 | Loss: 0.00002249
Iteration 150/1000 | Loss: 0.00002249
Iteration 151/1000 | Loss: 0.00002249
Iteration 152/1000 | Loss: 0.00002249
Iteration 153/1000 | Loss: 0.00002249
Iteration 154/1000 | Loss: 0.00002248
Iteration 155/1000 | Loss: 0.00002248
Iteration 156/1000 | Loss: 0.00002248
Iteration 157/1000 | Loss: 0.00002248
Iteration 158/1000 | Loss: 0.00002248
Iteration 159/1000 | Loss: 0.00002248
Iteration 160/1000 | Loss: 0.00002248
Iteration 161/1000 | Loss: 0.00002248
Iteration 162/1000 | Loss: 0.00002248
Iteration 163/1000 | Loss: 0.00002248
Iteration 164/1000 | Loss: 0.00002248
Iteration 165/1000 | Loss: 0.00002248
Iteration 166/1000 | Loss: 0.00002248
Iteration 167/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.2481292035081424e-05, 2.2481292035081424e-05, 2.2481292035081424e-05, 2.2481292035081424e-05, 2.2481292035081424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2481292035081424e-05

Optimization complete. Final v2v error: 3.9572081565856934 mm

Highest mean error: 4.431321144104004 mm for frame 99

Lowest mean error: 3.6927077770233154 mm for frame 45

Saving results

Total time: 41.19610643386841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0020/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344221
Iteration 2/25 | Loss: 0.00137841
Iteration 3/25 | Loss: 0.00111735
Iteration 4/25 | Loss: 0.00107669
Iteration 5/25 | Loss: 0.00106953
Iteration 6/25 | Loss: 0.00106750
Iteration 7/25 | Loss: 0.00106684
Iteration 8/25 | Loss: 0.00106684
Iteration 9/25 | Loss: 0.00106684
Iteration 10/25 | Loss: 0.00106684
Iteration 11/25 | Loss: 0.00106684
Iteration 12/25 | Loss: 0.00106684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001066837809048593, 0.001066837809048593, 0.001066837809048593, 0.001066837809048593, 0.001066837809048593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001066837809048593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40751529
Iteration 2/25 | Loss: 0.00071974
Iteration 3/25 | Loss: 0.00071974
Iteration 4/25 | Loss: 0.00071974
Iteration 5/25 | Loss: 0.00071974
Iteration 6/25 | Loss: 0.00071974
Iteration 7/25 | Loss: 0.00071974
Iteration 8/25 | Loss: 0.00071974
Iteration 9/25 | Loss: 0.00071974
Iteration 10/25 | Loss: 0.00071974
Iteration 11/25 | Loss: 0.00071974
Iteration 12/25 | Loss: 0.00071974
Iteration 13/25 | Loss: 0.00071974
Iteration 14/25 | Loss: 0.00071974
Iteration 15/25 | Loss: 0.00071974
Iteration 16/25 | Loss: 0.00071974
Iteration 17/25 | Loss: 0.00071974
Iteration 18/25 | Loss: 0.00071974
Iteration 19/25 | Loss: 0.00071974
Iteration 20/25 | Loss: 0.00071974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007197422091849148, 0.0007197422091849148, 0.0007197422091849148, 0.0007197422091849148, 0.0007197422091849148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007197422091849148

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00404320
Iteration 2/25 | Loss: 0.00073537
Iteration 3/25 | Loss: 0.00061519
Iteration 4/25 | Loss: 0.00058690
Iteration 5/25 | Loss: 0.00057738
Iteration 6/25 | Loss: 0.00057530
Iteration 7/25 | Loss: 0.00057512
Iteration 8/25 | Loss: 0.00057512
Iteration 9/25 | Loss: 0.00057512
Iteration 10/25 | Loss: 0.00057512
Iteration 11/25 | Loss: 0.00057512
Iteration 12/25 | Loss: 0.00057512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005751207354478538, 0.0005751207354478538, 0.0005751207354478538, 0.0005751207354478538, 0.0005751207354478538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005751207354478538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38314724
Iteration 2/25 | Loss: 0.00012260
Iteration 3/25 | Loss: 0.00012260
Iteration 4/25 | Loss: 0.00012260
Iteration 5/25 | Loss: 0.00012260
Iteration 6/25 | Loss: 0.00012260
Iteration 7/25 | Loss: 0.00012260
Iteration 8/25 | Loss: 0.00012260
Iteration 9/25 | Loss: 0.00012259
Iteration 10/25 | Loss: 0.00012259
Iteration 11/25 | Loss: 0.00012259
Iteration 12/25 | Loss: 0.00012259
Iteration 13/25 | Loss: 0.00012259
Iteration 14/25 | Loss: 0.00012259
Iteration 15/25 | Loss: 0.00012259
Iteration 16/25 | Loss: 0.00012259
Iteration 17/25 | Loss: 0.00012259
Iteration 18/25 | Loss: 0.00012259
Iteration 19/25 | Loss: 0.00012259
Iteration 20/25 | Loss: 0.00012259
Iteration 21/25 | Loss: 0.00012259
Iteration 22/25 | Loss: 0.00012259
Iteration 23/25 | Loss: 0.00012259
Iteration 24/25 | Loss: 0.00012259
Iteration 25/25 | Loss: 0.00012259

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00404320
Iteration 2/25 | Loss: 0.00073577
Iteration 3/25 | Loss: 0.00060610
Iteration 4/25 | Loss: 0.00058158
Iteration 5/25 | Loss: 0.00057621
Iteration 6/25 | Loss: 0.00057513
Iteration 7/25 | Loss: 0.00057512
Iteration 8/25 | Loss: 0.00057512
Iteration 9/25 | Loss: 0.00057512
Iteration 10/25 | Loss: 0.00057512
Iteration 11/25 | Loss: 0.00057512
Iteration 12/25 | Loss: 0.00057512
Iteration 13/25 | Loss: 0.00057512
Iteration 14/25 | Loss: 0.00057512
Iteration 15/25 | Loss: 0.00057512
Iteration 16/25 | Loss: 0.00057512
Iteration 17/25 | Loss: 0.00057512
Iteration 18/25 | Loss: 0.00057512
Iteration 19/25 | Loss: 0.00057512
Iteration 20/25 | Loss: 0.00057512
Iteration 21/25 | Loss: 0.00057512
Iteration 22/25 | Loss: 0.00057512
Iteration 23/25 | Loss: 0.00057512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005751196877099574, 0.0005751196877099574, 0.0005751196877099574, 0.0005751196877099574, 0.0005751196877099574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005751196877099574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38308144
Iteration 2/25 | Loss: 0.00012240
Iteration 3/25 | Loss: 0.00012240
Iteration 4/25 | Loss: 0.00012240
Iteration 5/25 | Loss: 0.00012240
Iteration 6/25 | Loss: 0.00012240
Iteration 7/25 | Loss: 0.00012240
Iteration 8/25 | Loss: 0.00012240
Iteration 9/25 | Loss: 0.00012240
Iteration 10/25 | Loss: 0.00012240
Iteration 11/25 | Loss: 0.00012240
Iteration 12/25 | Loss: 0.00012240
Iteration 13/25 | Loss: 0.00012240
Iteration 14/25 | Loss: 0.00012240
Iteration 15/25 | Loss: 0.00012240
Iteration 16/25 | Loss: 0.00012240
Iteration 17/25 | Loss: 0.00012240
Iteration 18/25 | Loss: 0.00012240
Iteration 19/25 | Loss: 0.00012240
Iteration 20/25 | Loss: 0.00012240
Iteration 21/25 | Loss: 0.00012240
Iteration 22/25 | Loss: 0.00012240
Iteration 23/25 | Loss: 0.00012240
Iteration 24/25 | Loss: 0.00012240
Iteration 25/25 | Loss: 0.00012240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11022311
Iteration 2/1000 | Loss: 0.00176546
Iteration 3/1000 | Loss: 0.00227911
Iteration 4/1000 | Loss: 0.00050814
Iteration 5/1000 | Loss: 0.00021369
Iteration 6/1000 | Loss: 0.00068180
Iteration 7/1000 | Loss: 0.00017728
Iteration 8/1000 | Loss: 0.00027094
Iteration 9/1000 | Loss: 0.00008753
Iteration 10/1000 | Loss: 0.00013046
Iteration 11/1000 | Loss: 0.00007390
Iteration 12/1000 | Loss: 0.00048865
Iteration 13/1000 | Loss: 0.00021612
Iteration 14/1000 | Loss: 0.00004747
Iteration 15/1000 | Loss: 0.00009895
Iteration 16/1000 | Loss: 0.00011526
Iteration 17/1000 | Loss: 0.00007788
Iteration 18/1000 | Loss: 0.00022065
Iteration 19/1000 | Loss: 0.00009821
Iteration 20/1000 | Loss: 0.00008756
Iteration 21/1000 | Loss: 0.00019100
Iteration 22/1000 | Loss: 0.00011147
Iteration 23/1000 | Loss: 0.00004584
Iteration 24/1000 | Loss: 0.00004263
Iteration 25/1000 | Loss: 0.00004818
Iteration 26/1000 | Loss: 0.00004599
Iteration 27/1000 | Loss: 0.00017297
Iteration 28/1000 | Loss: 0.00003429
Iteration 29/1000 | Loss: 0.00003362
Iteration 30/1000 | Loss: 0.00003429
Iteration 31/1000 | Loss: 0.00020449
Iteration 32/1000 | Loss: 0.00011967
Iteration 33/1000 | Loss: 0.00003453
Iteration 34/1000 | Loss: 0.00003411
Iteration 35/1000 | Loss: 0.00009162
Iteration 36/1000 | Loss: 0.00003705
Iteration 37/1000 | Loss: 0.00003372
Iteration 38/1000 | Loss: 0.00002790
Iteration 39/1000 | Loss: 0.00004912
Iteration 40/1000 | Loss: 0.00015095
Iteration 41/1000 | Loss: 0.00005927
Iteration 42/1000 | Loss: 0.00004263
Iteration 43/1000 | Loss: 0.00007105
Iteration 44/1000 | Loss: 0.00002862
Iteration 45/1000 | Loss: 0.00002627
Iteration 46/1000 | Loss: 0.00002596
Iteration 47/1000 | Loss: 0.00002583
Iteration 48/1000 | Loss: 0.00004065
Iteration 49/1000 | Loss: 0.00011797
Iteration 50/1000 | Loss: 0.00002733
Iteration 51/1000 | Loss: 0.00009298
Iteration 52/1000 | Loss: 0.00008930
Iteration 53/1000 | Loss: 0.00002766
Iteration 54/1000 | Loss: 0.00004000
Iteration 55/1000 | Loss: 0.00004573
Iteration 56/1000 | Loss: 0.00002566
Iteration 57/1000 | Loss: 0.00002528
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002528
Iteration 60/1000 | Loss: 0.00002527
Iteration 61/1000 | Loss: 0.00002527
Iteration 62/1000 | Loss: 0.00002527
Iteration 63/1000 | Loss: 0.00002527
Iteration 64/1000 | Loss: 0.00002526
Iteration 65/1000 | Loss: 0.00002526
Iteration 66/1000 | Loss: 0.00002526
Iteration 67/1000 | Loss: 0.00002525
Iteration 68/1000 | Loss: 0.00002525
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002529
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002526
Iteration 73/1000 | Loss: 0.00003040
Iteration 74/1000 | Loss: 0.00002528
Iteration 75/1000 | Loss: 0.00002513
Iteration 76/1000 | Loss: 0.00002513
Iteration 77/1000 | Loss: 0.00002513
Iteration 78/1000 | Loss: 0.00002512
Iteration 79/1000 | Loss: 0.00002512
Iteration 80/1000 | Loss: 0.00002512
Iteration 81/1000 | Loss: 0.00002512
Iteration 82/1000 | Loss: 0.00002512
Iteration 83/1000 | Loss: 0.00002512
Iteration 84/1000 | Loss: 0.00002511
Iteration 85/1000 | Loss: 0.00002511
Iteration 86/1000 | Loss: 0.00002511
Iteration 87/1000 | Loss: 0.00002511
Iteration 88/1000 | Loss: 0.00002510
Iteration 89/1000 | Loss: 0.00014379
Iteration 90/1000 | Loss: 0.00003232
Iteration 91/1000 | Loss: 0.00002511
Iteration 92/1000 | Loss: 0.00002511
Iteration 93/1000 | Loss: 0.00002517
Iteration 94/1000 | Loss: 0.00002517
Iteration 95/1000 | Loss: 0.00002502
Iteration 96/1000 | Loss: 0.00002502
Iteration 97/1000 | Loss: 0.00002502
Iteration 98/1000 | Loss: 0.00002502
Iteration 99/1000 | Loss: 0.00002501
Iteration 100/1000 | Loss: 0.00002501
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00002500
Iteration 103/1000 | Loss: 0.00002500
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002500
Iteration 107/1000 | Loss: 0.00002500
Iteration 108/1000 | Loss: 0.00002500
Iteration 109/1000 | Loss: 0.00002500
Iteration 110/1000 | Loss: 0.00002500
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002499
Iteration 113/1000 | Loss: 0.00002499
Iteration 114/1000 | Loss: 0.00002499
Iteration 115/1000 | Loss: 0.00002504
Iteration 116/1000 | Loss: 0.00002500
Iteration 117/1000 | Loss: 0.00002500
Iteration 118/1000 | Loss: 0.00002500
Iteration 119/1000 | Loss: 0.00002500
Iteration 120/1000 | Loss: 0.00002500
Iteration 121/1000 | Loss: 0.00002500
Iteration 122/1000 | Loss: 0.00002500
Iteration 123/1000 | Loss: 0.00002500
Iteration 124/1000 | Loss: 0.00002500
Iteration 125/1000 | Loss: 0.00002500
Iteration 126/1000 | Loss: 0.00002500
Iteration 127/1000 | Loss: 0.00002500
Iteration 128/1000 | Loss: 0.00002500
Iteration 129/1000 | Loss: 0.00002500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.4996432330226526e-05, 2.4996432330226526e-05, 2.4996432330226526e-05, 2.4996432330226526e-05, 2.4996432330226526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4996432330226526e-05

Optimization complete. Final v2v error: 4.171133518218994 mm

Highest mean error: 5.825627326965332 mm for frame 192

Lowest mean error: 3.301953077316284 mm for frame 180

Saving results

Total time: 116.00480794906616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00071974
Iteration 2/1000 | Loss: 0.00004701
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00001641
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001365
Iteration 7/1000 | Loss: 0.00001315
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001193
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001169
Iteration 21/1000 | Loss: 0.00001169
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001167
Iteration 24/1000 | Loss: 0.00001167
Iteration 25/1000 | Loss: 0.00001167
Iteration 26/1000 | Loss: 0.00001167
Iteration 27/1000 | Loss: 0.00001167
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001165
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001164
Iteration 40/1000 | Loss: 0.00001164
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001161
Iteration 51/1000 | Loss: 0.00001161
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001160
Iteration 55/1000 | Loss: 0.00001160
Iteration 56/1000 | Loss: 0.00001159
Iteration 57/1000 | Loss: 0.00001159
Iteration 58/1000 | Loss: 0.00001158
Iteration 59/1000 | Loss: 0.00001158
Iteration 60/1000 | Loss: 0.00001158
Iteration 61/1000 | Loss: 0.00001158
Iteration 62/1000 | Loss: 0.00001157
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001157
Iteration 66/1000 | Loss: 0.00001157
Iteration 67/1000 | Loss: 0.00001157
Iteration 68/1000 | Loss: 0.00001156
Iteration 69/1000 | Loss: 0.00001156
Iteration 70/1000 | Loss: 0.00001156
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001155
Iteration 73/1000 | Loss: 0.00001155
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001152
Iteration 83/1000 | Loss: 0.00001152
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.1514088328112848e-05, 1.1514088328112848e-05, 1.1514088328112848e-05, 1.1514088328112848e-05, 1.1514088328112848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1514088328112848e-05

Optimization complete. Final v2v error: 2.9092020988464355 mm

Highest mean error: 3.312778949737549 mm for frame 28

Lowest mean error: 2.449800729751587 mm for frame 12

Saving results

Total time: 32.87953495979309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983900
Iteration 2/25 | Loss: 0.00121631
Iteration 3/25 | Loss: 0.00079235
Iteration 4/25 | Loss: 0.00075375
Iteration 5/25 | Loss: 0.00074361
Iteration 6/25 | Loss: 0.00074098
Iteration 7/25 | Loss: 0.00074020
Iteration 8/25 | Loss: 0.00074020
Iteration 9/25 | Loss: 0.00074020
Iteration 10/25 | Loss: 0.00074019
Iteration 11/25 | Loss: 0.00074019
Iteration 12/25 | Loss: 0.00074019
Iteration 13/25 | Loss: 0.00074019
Iteration 14/25 | Loss: 0.00074019
Iteration 15/25 | Loss: 0.00074019
Iteration 16/25 | Loss: 0.00074019
Iteration 17/25 | Loss: 0.00074019
Iteration 18/25 | Loss: 0.00074019
Iteration 19/25 | Loss: 0.00074019
Iteration 20/25 | Loss: 0.00074019
Iteration 21/25 | Loss: 0.00074019
Iteration 22/25 | Loss: 0.00074019
Iteration 23/25 | Loss: 0.00074019
Iteration 24/25 | Loss: 0.00074019
Iteration 25/25 | Loss: 0.00074019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.78482270
Iteration 2/25 | Loss: 0.00040300
Iteration 3/25 | Loss: 0.00040300
Iteration 4/25 | Loss: 0.00040300
Iteration 5/25 | Loss: 0.00040300
Iteration 6/25 | Loss: 0.00040300
Iteration 7/25 | Loss: 0.00040299
Iteration 8/25 | Loss: 0.00040299
Iteration 9/25 | Loss: 0.00040299
Iteration 10/25 | Loss: 0.00040299
Iteration 11/25 | Loss: 0.00040299
Iteration 12/25 | Loss: 0.00040299
Iteration 13/25 | Loss: 0.00040299
Iteration 14/25 | Loss: 0.00040299
Iteration 15/25 | Loss: 0.00040299
Iteration 16/25 | Loss: 0.00040299
Iteration 17/25 | Loss: 0.00040299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004029939591418952, 0.0004029939591418952, 0.0004029939591418952, 0.0004029939591418952, 0.0004029939591418952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004029939591418952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012259
Iteration 2/1000 | Loss: 0.00003175
Iteration 3/1000 | Loss: 0.00002283
Iteration 4/1000 | Loss: 0.00002104
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001737
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001729
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001725
Iteration 14/1000 | Loss: 0.00001720
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001706
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001700
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001699
Iteration 23/1000 | Loss: 0.00001698
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001686
Iteration 30/1000 | Loss: 0.00001686
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001681
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001677
Iteration 43/1000 | Loss: 0.00001677
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001676
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001673
Iteration 58/1000 | Loss: 0.00001672
Iteration 59/1000 | Loss: 0.00001672
Iteration 60/1000 | Loss: 0.00001672
Iteration 61/1000 | Loss: 0.00001672
Iteration 62/1000 | Loss: 0.00001672
Iteration 63/1000 | Loss: 0.00001672
Iteration 64/1000 | Loss: 0.00001671
Iteration 65/1000 | Loss: 0.00001671
Iteration 66/1000 | Loss: 0.00001671
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001670
Iteration 69/1000 | Loss: 0.00001670
Iteration 70/1000 | Loss: 0.00001669
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001668
Iteration 73/1000 | Loss: 0.00001668
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001667
Iteration 76/1000 | Loss: 0.00001667
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001662
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001662
Iteration 93/1000 | Loss: 0.00001662
Iteration 94/1000 | Loss: 0.00001661
Iteration 95/1000 | Loss: 0.00001661
Iteration 96/1000 | Loss: 0.00001661
Iteration 97/1000 | Loss: 0.00001661
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001656
Iteration 123/1000 | Loss: 0.00001656
Iteration 124/1000 | Loss: 0.00001656
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001655
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001655
Iteration 130/1000 | Loss: 0.00001655
Iteration 131/1000 | Loss: 0.00001655
Iteration 132/1000 | Loss: 0.00001655
Iteration 133/1000 | Loss: 0.00001655
Iteration 134/1000 | Loss: 0.00001655
Iteration 135/1000 | Loss: 0.00001655
Iteration 136/1000 | Loss: 0.00001655
Iteration 137/1000 | Loss: 0.00001654
Iteration 138/1000 | Loss: 0.00001654
Iteration 139/1000 | Loss: 0.00001654
Iteration 140/1000 | Loss: 0.00001654
Iteration 141/1000 | Loss: 0.00001654
Iteration 142/1000 | Loss: 0.00001654
Iteration 143/1000 | Loss: 0.00001654
Iteration 144/1000 | Loss: 0.00001654
Iteration 145/1000 | Loss: 0.00001654
Iteration 146/1000 | Loss: 0.00001654
Iteration 147/1000 | Loss: 0.00001654
Iteration 148/1000 | Loss: 0.00001654
Iteration 149/1000 | Loss: 0.00001654
Iteration 150/1000 | Loss: 0.00001654
Iteration 151/1000 | Loss: 0.00001654
Iteration 152/1000 | Loss: 0.00001654
Iteration 153/1000 | Loss: 0.00001654
Iteration 154/1000 | Loss: 0.00001654
Iteration 155/1000 | Loss: 0.00001654
Iteration 156/1000 | Loss: 0.00001654
Iteration 157/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.6536012481083162e-05, 1.6536012481083162e-05, 1.6536012481083162e-05, 1.6536012481083162e-05, 1.6536012481083162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6536012481083162e-05

Optimization complete. Final v2v error: 3.4327268600463867 mm

Highest mean error: 3.9747910499572754 mm for frame 51

Lowest mean error: 3.263531446456909 mm for frame 0

Saving results

Total time: 37.09295845031738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00278594
Iteration 2/25 | Loss: 0.00128850
Iteration 3/25 | Loss: 0.00109406
Iteration 4/25 | Loss: 0.00106366
Iteration 5/25 | Loss: 0.00105265
Iteration 6/25 | Loss: 0.00104943
Iteration 7/25 | Loss: 0.00104897
Iteration 8/25 | Loss: 0.00104897
Iteration 9/25 | Loss: 0.00104897
Iteration 10/25 | Loss: 0.00104897
Iteration 11/25 | Loss: 0.00104897
Iteration 12/25 | Loss: 0.00104897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010489735286682844, 0.0010489735286682844, 0.0010489735286682844, 0.0010489735286682844, 0.0010489735286682844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010489735286682844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36248231
Iteration 2/25 | Loss: 0.00078973
Iteration 3/25 | Loss: 0.00078973
Iteration 4/25 | Loss: 0.00078973
Iteration 5/25 | Loss: 0.00078973
Iteration 6/25 | Loss: 0.00078972
Iteration 7/25 | Loss: 0.00078972
Iteration 8/25 | Loss: 0.00078972
Iteration 9/25 | Loss: 0.00078972
Iteration 10/25 | Loss: 0.00078972
Iteration 11/25 | Loss: 0.00078972
Iteration 12/25 | Loss: 0.00078972
Iteration 13/25 | Loss: 0.00078972
Iteration 14/25 | Loss: 0.00078972
Iteration 15/25 | Loss: 0.00078972
Iteration 16/25 | Loss: 0.00078972
Iteration 17/25 | Loss: 0.00078972
Iteration 18/25 | Loss: 0.00078972
Iteration 19/25 | Loss: 0.00078972
Iteration 20/25 | Loss: 0.00078972
Iteration 21/25 | Loss: 0.00078972
Iteration 22/25 | Loss: 0.00078972
Iteration 23/25 | Loss: 0.00078972
Iteration 24/25 | Loss: 0.00078972
Iteration 25/25 | Loss: 0.00078972

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01103868
Iteration 2/25 | Loss: 0.01103868
Iteration 3/25 | Loss: 0.01103868
Iteration 4/25 | Loss: 0.01103867
Iteration 5/25 | Loss: 0.01103867
Iteration 6/25 | Loss: 0.01103867
Iteration 7/25 | Loss: 0.01103867
Iteration 8/25 | Loss: 0.01103867
Iteration 9/25 | Loss: 0.01103867
Iteration 10/25 | Loss: 0.01103866
Iteration 11/25 | Loss: 0.01103866
Iteration 12/25 | Loss: 0.01103866
Iteration 13/25 | Loss: 0.01103866
Iteration 14/25 | Loss: 0.01103866
Iteration 15/25 | Loss: 0.01103866
Iteration 16/25 | Loss: 0.01103865
Iteration 17/25 | Loss: 0.01103865
Iteration 18/25 | Loss: 0.01103865
Iteration 19/25 | Loss: 0.01103865
Iteration 20/25 | Loss: 0.01103864
Iteration 21/25 | Loss: 0.01103864
Iteration 22/25 | Loss: 0.01103864
Iteration 23/25 | Loss: 0.01103864
Iteration 24/25 | Loss: 0.01103864
Iteration 25/25 | Loss: 0.01103863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/1000 | Loss: 0.00012240
Iteration 2/1000 | Loss: 0.00003222
Iteration 3/1000 | Loss: 0.00002354
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001773
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001720
Iteration 19/1000 | Loss: 0.00001719
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001698
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001689
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001681
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001676
Iteration 40/1000 | Loss: 0.00001676
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001676
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001675
Iteration 51/1000 | Loss: 0.00001675
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001673
Iteration 59/1000 | Loss: 0.00001672
Iteration 60/1000 | Loss: 0.00001672
Iteration 61/1000 | Loss: 0.00001672
Iteration 62/1000 | Loss: 0.00001672
Iteration 63/1000 | Loss: 0.00001671
Iteration 64/1000 | Loss: 0.00001671
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001670
Iteration 67/1000 | Loss: 0.00001669
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001669
Iteration 70/1000 | Loss: 0.00001668
Iteration 71/1000 | Loss: 0.00001668
Iteration 72/1000 | Loss: 0.00001668
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001664
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001663
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001662
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001661
Iteration 91/1000 | Loss: 0.00001660
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001659
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001658
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001658
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Iteration 151/1000 | Loss: 0.00001655
Iteration 152/1000 | Loss: 0.00001654
Iteration 153/1000 | Loss: 0.00001654
Iteration 154/1000 | Loss: 0.00001654
Iteration 155/1000 | Loss: 0.00001654
Iteration 156/1000 | Loss: 0.00001654
Iteration 157/1000 | Loss: 0.00001654
Iteration 158/1000 | Loss: 0.00001654
Iteration 159/1000 | Loss: 0.00001654
Iteration 160/1000 | Loss: 0.00001654
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001654
Iteration 166/1000 | Loss: 0.00001654
Iteration 167/1000 | Loss: 0.00001654
Iteration 168/1000 | Loss: 0.00001654
Iteration 169/1000 | Loss: 0.00001654
Iteration 170/1000 | Loss: 0.00001654
Iteration 171/1000 | Loss: 0.00001654
Iteration 172/1000 | Loss: 0.00001654
Iteration 173/1000 | Loss: 0.00001654
Iteration 174/1000 | Loss: 0.00001654
Iteration 175/1000 | Loss: 0.00001654
Iteration 176/1000 | Loss: 0.00001654
Iteration 177/1000 | Loss: 0.00001654
Iteration 178/1000 | Loss: 0.00001654
Iteration 179/1000 | Loss: 0.00001654
Iteration 180/1000 | Loss: 0.00001654
Iteration 181/1000 | Loss: 0.00001654
Iteration 182/1000 | Loss: 0.00001654
Iteration 183/1000 | Loss: 0.00001654
Iteration 184/1000 | Loss: 0.00001654
Iteration 185/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.654148036323022e-05, 1.654148036323022e-05, 1.654148036323022e-05, 1.654148036323022e-05, 1.654148036323022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.654148036323022e-05

Optimization complete. Final v2v error: 3.43471360206604 mm

Highest mean error: 3.9776861667633057 mm for frame 51

Lowest mean error: 3.2629740238189697 mm for frame 0

Saving results

Total time: 37.519009828567505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0015/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 1.39677429
Iteration 2/25 | Loss: 0.15482725
Iteration 3/25 | Loss: 0.15117219
Iteration 4/25 | Loss: 0.14967604
Iteration 5/25 | Loss: 0.14961798
Iteration 6/25 | Loss: 0.14961796
Iteration 7/25 | Loss: 0.14961794
Iteration 8/25 | Loss: 0.14961794
Iteration 9/25 | Loss: 0.14961793
Iteration 10/25 | Loss: 0.14961794
Iteration 11/25 | Loss: 0.14961794
Iteration 12/25 | Loss: 0.14961793
Iteration 13/25 | Loss: 0.14961793
Iteration 14/25 | Loss: 0.14961793
Iteration 15/25 | Loss: 0.14961793
Iteration 16/25 | Loss: 0.14961791
Iteration 17/25 | Loss: 0.14961791
Iteration 18/25 | Loss: 0.14961791
Iteration 19/25 | Loss: 0.14961791
Iteration 20/25 | Loss: 0.14961791
Iteration 21/25 | Loss: 0.14961791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.14961791038513184, 0.14961791038513184, 0.14961791038513184, 0.14961791038513184, 0.14961791038513184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14961791038513184

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01103868
Iteration 2/25 | Loss: 0.01103868
Iteration 3/25 | Loss: 0.01103868
Iteration 4/25 | Loss: 0.01103867
Iteration 5/25 | Loss: 0.01103867
Iteration 6/25 | Loss: 0.01103867
Iteration 7/25 | Loss: 0.01103867
Iteration 8/25 | Loss: 0.01103867
Iteration 9/25 | Loss: 0.01103867
Iteration 10/25 | Loss: 0.01103866
Iteration 11/25 | Loss: 0.01103866
Iteration 12/25 | Loss: 0.01103866
Iteration 13/25 | Loss: 0.01103866
Iteration 14/25 | Loss: 0.01103866
Iteration 15/25 | Loss: 0.01103866
Iteration 16/25 | Loss: 0.01103865
Iteration 17/25 | Loss: 0.01103865
Iteration 18/25 | Loss: 0.01103865
Iteration 19/25 | Loss: 0.01103865
Iteration 20/25 | Loss: 0.01103864
Iteration 21/25 | Loss: 0.01103864
Iteration 22/25 | Loss: 0.01103864
Iteration 23/25 | Loss: 0.01103864
Iteration 24/25 | Loss: 0.01103864
Iteration 25/25 | Loss: 0.01103863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39677429
Iteration 2/25 | Loss: 0.15462966
Iteration 3/25 | Loss: 0.15117213
Iteration 4/25 | Loss: 0.14967583
Iteration 5/25 | Loss: 0.14967974
Iteration 6/25 | Loss: 0.14961798
Iteration 7/25 | Loss: 0.14961796
Iteration 8/25 | Loss: 0.14961794
Iteration 9/25 | Loss: 0.14961794
Iteration 10/25 | Loss: 0.14961794
Iteration 11/25 | Loss: 0.14961793
Iteration 12/25 | Loss: 0.14961793
Iteration 13/25 | Loss: 0.14961793
Iteration 14/25 | Loss: 0.14961793
Iteration 15/25 | Loss: 0.14961793
Iteration 16/25 | Loss: 0.14961793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.14961792528629303, 0.14961792528629303, 0.14961792528629303, 0.14961792528629303, 0.14961792528629303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14961792528629303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040299
Iteration 2/1000 | Loss: 0.00002454
Iteration 3/1000 | Loss: 0.00001906
Iteration 4/1000 | Loss: 0.00001817
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001630
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001573
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001573
Iteration 21/1000 | Loss: 0.00001573
Iteration 22/1000 | Loss: 0.00001572
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001567
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001565
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001564
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001562
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001560
Iteration 39/1000 | Loss: 0.00001560
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001550
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001544
Iteration 70/1000 | Loss: 0.00001544
Iteration 71/1000 | Loss: 0.00001543
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001541
Iteration 83/1000 | Loss: 0.00001541
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001540
Iteration 86/1000 | Loss: 0.00001540
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001537
Iteration 93/1000 | Loss: 0.00001537
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001533
Iteration 107/1000 | Loss: 0.00001533
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001533
Iteration 120/1000 | Loss: 0.00001533
Iteration 121/1000 | Loss: 0.00001533
Iteration 122/1000 | Loss: 0.00001533
Iteration 123/1000 | Loss: 0.00001533
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001533
Iteration 132/1000 | Loss: 0.00001533
Iteration 133/1000 | Loss: 0.00001533
Iteration 134/1000 | Loss: 0.00001533
Iteration 135/1000 | Loss: 0.00001533
Iteration 136/1000 | Loss: 0.00001533
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001533
Iteration 142/1000 | Loss: 0.00001533
Iteration 143/1000 | Loss: 0.00001533
Iteration 144/1000 | Loss: 0.00001533
Iteration 145/1000 | Loss: 0.00001533
Iteration 146/1000 | Loss: 0.00001533
Iteration 147/1000 | Loss: 0.00001533
Iteration 148/1000 | Loss: 0.00001533
Iteration 149/1000 | Loss: 0.00001533
Iteration 150/1000 | Loss: 0.00001533
Iteration 151/1000 | Loss: 0.00001533
Iteration 152/1000 | Loss: 0.00001533
Iteration 153/1000 | Loss: 0.00001533
Iteration 154/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.5331122995121405e-05, 1.5331122995121405e-05, 1.5331122995121405e-05, 1.5331122995121405e-05, 1.5331122995121405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5331122995121405e-05

Optimization complete. Final v2v error: 3.3178815841674805 mm

Highest mean error: 3.5267627239227295 mm for frame 76

Lowest mean error: 3.1363394260406494 mm for frame 9

Saving results

Total time: 35.3647403717041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00078972
Iteration 2/1000 | Loss: 0.00004090
Iteration 3/1000 | Loss: 0.00002114
Iteration 4/1000 | Loss: 0.00001815
Iteration 5/1000 | Loss: 0.00001698
Iteration 6/1000 | Loss: 0.00001606
Iteration 7/1000 | Loss: 0.00001543
Iteration 8/1000 | Loss: 0.00001502
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001371
Iteration 46/1000 | Loss: 0.00001371
Iteration 47/1000 | Loss: 0.00001371
Iteration 48/1000 | Loss: 0.00001371
Iteration 49/1000 | Loss: 0.00001371
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001370
Iteration 52/1000 | Loss: 0.00001370
Iteration 53/1000 | Loss: 0.00001369
Iteration 54/1000 | Loss: 0.00001369
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001366
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001366
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001365
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001360
Iteration 80/1000 | Loss: 0.00001360
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.3559109902416822e-05, 1.3559109902416822e-05, 1.3559109902416822e-05, 1.3559109902416822e-05, 1.3559109902416822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3559109902416822e-05

Optimization complete. Final v2v error: 3.0963356494903564 mm

Highest mean error: 3.67519211769104 mm for frame 57

Lowest mean error: 2.7187845706939697 mm for frame 0

Saving results

Total time: 38.24245238304138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805082
Iteration 2/25 | Loss: 0.00135777
Iteration 3/25 | Loss: 0.00099565
Iteration 4/25 | Loss: 0.00094663
Iteration 5/25 | Loss: 0.00093871
Iteration 6/25 | Loss: 0.00093770
Iteration 7/25 | Loss: 0.00093763
Iteration 8/25 | Loss: 0.00093763
Iteration 9/25 | Loss: 0.00093763
Iteration 10/25 | Loss: 0.00093763
Iteration 11/25 | Loss: 0.00093758
Iteration 12/25 | Loss: 0.00093758
Iteration 13/25 | Loss: 0.00093758
Iteration 14/25 | Loss: 0.00093758
Iteration 15/25 | Loss: 0.00093758
Iteration 16/25 | Loss: 0.00093758
Iteration 17/25 | Loss: 0.00093758
Iteration 18/25 | Loss: 0.00093758
Iteration 19/25 | Loss: 0.00093758
Iteration 20/25 | Loss: 0.00093758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009375824010930955, 0.0009375824010930955, 0.0009375824010930955, 0.0009375824010930955, 0.0009375824010930955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009375824010930955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48297668
Iteration 2/25 | Loss: 0.00039137
Iteration 3/25 | Loss: 0.00039133
Iteration 4/25 | Loss: 0.00039133
Iteration 5/25 | Loss: 0.00039133
Iteration 6/25 | Loss: 0.00039133
Iteration 7/25 | Loss: 0.00039133
Iteration 8/25 | Loss: 0.00039133
Iteration 9/25 | Loss: 0.00039133
Iteration 10/25 | Loss: 0.00039133
Iteration 11/25 | Loss: 0.00039133
Iteration 12/25 | Loss: 0.00039133
Iteration 13/25 | Loss: 0.00039133
Iteration 14/25 | Loss: 0.00039133
Iteration 15/25 | Loss: 0.00039133
Iteration 16/25 | Loss: 0.00039133
Iteration 17/25 | Loss: 0.00039133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00039132568053901196, 0.00039132568053901196, 0.00039132568053901196, 0.00039132568053901196, 0.00039132568053901196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039132568053901196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039133
Iteration 2/1000 | Loss: 0.00004816
Iteration 3/1000 | Loss: 0.00003407
Iteration 4/1000 | Loss: 0.00003134
Iteration 5/1000 | Loss: 0.00003002
Iteration 6/1000 | Loss: 0.00002936
Iteration 7/1000 | Loss: 0.00002885
Iteration 8/1000 | Loss: 0.00002857
Iteration 9/1000 | Loss: 0.00002834
Iteration 10/1000 | Loss: 0.00002819
Iteration 11/1000 | Loss: 0.00002817
Iteration 12/1000 | Loss: 0.00002813
Iteration 13/1000 | Loss: 0.00002813
Iteration 14/1000 | Loss: 0.00002810
Iteration 15/1000 | Loss: 0.00002807
Iteration 16/1000 | Loss: 0.00002798
Iteration 17/1000 | Loss: 0.00002797
Iteration 18/1000 | Loss: 0.00002795
Iteration 19/1000 | Loss: 0.00002795
Iteration 20/1000 | Loss: 0.00002795
Iteration 21/1000 | Loss: 0.00002795
Iteration 22/1000 | Loss: 0.00002795
Iteration 23/1000 | Loss: 0.00002794
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002794
Iteration 27/1000 | Loss: 0.00002794
Iteration 28/1000 | Loss: 0.00002794
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00002793
Iteration 31/1000 | Loss: 0.00002793
Iteration 32/1000 | Loss: 0.00002793
Iteration 33/1000 | Loss: 0.00002793
Iteration 34/1000 | Loss: 0.00002793
Iteration 35/1000 | Loss: 0.00002793
Iteration 36/1000 | Loss: 0.00002793
Iteration 37/1000 | Loss: 0.00002793
Iteration 38/1000 | Loss: 0.00002793
Iteration 39/1000 | Loss: 0.00002793
Iteration 40/1000 | Loss: 0.00002792
Iteration 41/1000 | Loss: 0.00002792
Iteration 42/1000 | Loss: 0.00002792
Iteration 43/1000 | Loss: 0.00002792
Iteration 44/1000 | Loss: 0.00002792
Iteration 45/1000 | Loss: 0.00002791
Iteration 46/1000 | Loss: 0.00002791
Iteration 47/1000 | Loss: 0.00002791
Iteration 48/1000 | Loss: 0.00002791
Iteration 49/1000 | Loss: 0.00002791
Iteration 50/1000 | Loss: 0.00002791
Iteration 51/1000 | Loss: 0.00002791
Iteration 52/1000 | Loss: 0.00002791
Iteration 53/1000 | Loss: 0.00002791
Iteration 54/1000 | Loss: 0.00002791
Iteration 55/1000 | Loss: 0.00002790
Iteration 56/1000 | Loss: 0.00002790
Iteration 57/1000 | Loss: 0.00002790
Iteration 58/1000 | Loss: 0.00002790
Iteration 59/1000 | Loss: 0.00002790
Iteration 60/1000 | Loss: 0.00002790
Iteration 61/1000 | Loss: 0.00002790
Iteration 62/1000 | Loss: 0.00002790
Iteration 63/1000 | Loss: 0.00002789
Iteration 64/1000 | Loss: 0.00002789
Iteration 65/1000 | Loss: 0.00002789
Iteration 66/1000 | Loss: 0.00002789
Iteration 67/1000 | Loss: 0.00002789
Iteration 68/1000 | Loss: 0.00002789
Iteration 69/1000 | Loss: 0.00002789
Iteration 70/1000 | Loss: 0.00002789
Iteration 71/1000 | Loss: 0.00002789
Iteration 72/1000 | Loss: 0.00002789
Iteration 73/1000 | Loss: 0.00002789
Iteration 74/1000 | Loss: 0.00002788
Iteration 75/1000 | Loss: 0.00002788
Iteration 76/1000 | Loss: 0.00002788
Iteration 77/1000 | Loss: 0.00002788
Iteration 78/1000 | Loss: 0.00002788
Iteration 79/1000 | Loss: 0.00002788
Iteration 80/1000 | Loss: 0.00002788
Iteration 81/1000 | Loss: 0.00002787
Iteration 82/1000 | Loss: 0.00002787
Iteration 83/1000 | Loss: 0.00002787
Iteration 84/1000 | Loss: 0.00002787
Iteration 85/1000 | Loss: 0.00002787
Iteration 86/1000 | Loss: 0.00002787
Iteration 87/1000 | Loss: 0.00002786
Iteration 88/1000 | Loss: 0.00002786
Iteration 89/1000 | Loss: 0.00002786
Iteration 90/1000 | Loss: 0.00002786
Iteration 91/1000 | Loss: 0.00002785
Iteration 92/1000 | Loss: 0.00002785
Iteration 93/1000 | Loss: 0.00002784
Iteration 94/1000 | Loss: 0.00002783
Iteration 95/1000 | Loss: 0.00002783
Iteration 96/1000 | Loss: 0.00002783
Iteration 97/1000 | Loss: 0.00002783
Iteration 98/1000 | Loss: 0.00002783
Iteration 99/1000 | Loss: 0.00002783
Iteration 100/1000 | Loss: 0.00002783
Iteration 101/1000 | Loss: 0.00002783
Iteration 102/1000 | Loss: 0.00002783
Iteration 103/1000 | Loss: 0.00002782
Iteration 104/1000 | Loss: 0.00002782
Iteration 105/1000 | Loss: 0.00002780
Iteration 106/1000 | Loss: 0.00002780
Iteration 107/1000 | Loss: 0.00002780
Iteration 108/1000 | Loss: 0.00002780
Iteration 109/1000 | Loss: 0.00002780
Iteration 110/1000 | Loss: 0.00002779
Iteration 111/1000 | Loss: 0.00002779
Iteration 112/1000 | Loss: 0.00002779
Iteration 113/1000 | Loss: 0.00002779
Iteration 114/1000 | Loss: 0.00002779
Iteration 115/1000 | Loss: 0.00002779
Iteration 116/1000 | Loss: 0.00002779
Iteration 117/1000 | Loss: 0.00002779
Iteration 118/1000 | Loss: 0.00002779
Iteration 119/1000 | Loss: 0.00002779
Iteration 120/1000 | Loss: 0.00002779
Iteration 121/1000 | Loss: 0.00002779
Iteration 122/1000 | Loss: 0.00002777
Iteration 123/1000 | Loss: 0.00002777
Iteration 124/1000 | Loss: 0.00002777
Iteration 125/1000 | Loss: 0.00002777
Iteration 126/1000 | Loss: 0.00002776
Iteration 127/1000 | Loss: 0.00002776
Iteration 128/1000 | Loss: 0.00002776
Iteration 129/1000 | Loss: 0.00002775
Iteration 130/1000 | Loss: 0.00002775
Iteration 131/1000 | Loss: 0.00002774
Iteration 132/1000 | Loss: 0.00002774
Iteration 133/1000 | Loss: 0.00002774
Iteration 134/1000 | Loss: 0.00002774
Iteration 135/1000 | Loss: 0.00002774
Iteration 136/1000 | Loss: 0.00002774
Iteration 137/1000 | Loss: 0.00002774
Iteration 138/1000 | Loss: 0.00002774
Iteration 139/1000 | Loss: 0.00002773
Iteration 140/1000 | Loss: 0.00002772
Iteration 141/1000 | Loss: 0.00002772
Iteration 142/1000 | Loss: 0.00002771
Iteration 143/1000 | Loss: 0.00002771
Iteration 144/1000 | Loss: 0.00002771
Iteration 145/1000 | Loss: 0.00002770
Iteration 146/1000 | Loss: 0.00002770
Iteration 147/1000 | Loss: 0.00002770
Iteration 148/1000 | Loss: 0.00002770
Iteration 149/1000 | Loss: 0.00002769
Iteration 150/1000 | Loss: 0.00002769
Iteration 151/1000 | Loss: 0.00002769
Iteration 152/1000 | Loss: 0.00002769
Iteration 153/1000 | Loss: 0.00002769
Iteration 154/1000 | Loss: 0.00002769
Iteration 155/1000 | Loss: 0.00002768
Iteration 156/1000 | Loss: 0.00002768
Iteration 157/1000 | Loss: 0.00002767
Iteration 158/1000 | Loss: 0.00002767
Iteration 159/1000 | Loss: 0.00002767
Iteration 160/1000 | Loss: 0.00002767
Iteration 161/1000 | Loss: 0.00002767
Iteration 162/1000 | Loss: 0.00002767
Iteration 163/1000 | Loss: 0.00002767
Iteration 164/1000 | Loss: 0.00002767
Iteration 165/1000 | Loss: 0.00002767
Iteration 166/1000 | Loss: 0.00002767
Iteration 167/1000 | Loss: 0.00002767
Iteration 168/1000 | Loss: 0.00002767
Iteration 169/1000 | Loss: 0.00002767
Iteration 170/1000 | Loss: 0.00002767
Iteration 171/1000 | Loss: 0.00002767
Iteration 172/1000 | Loss: 0.00002767
Iteration 173/1000 | Loss: 0.00002767
Iteration 174/1000 | Loss: 0.00002767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.7666315872920677e-05, 2.7666315872920677e-05, 2.7666315872920677e-05, 2.7666315872920677e-05, 2.7666315872920677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7666315872920677e-05

Optimization complete. Final v2v error: 4.188741207122803 mm

Highest mean error: 4.572576999664307 mm for frame 80

Lowest mean error: 3.2056527137756348 mm for frame 4

Saving results

Total time: 38.46315336227417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.14961793
Iteration 2/1000 | Loss: 0.01382218
Iteration 3/1000 | Loss: 0.00285057
Iteration 4/1000 | Loss: 0.00099604
Iteration 5/1000 | Loss: 0.00746372
Iteration 6/1000 | Loss: 0.00049997
Iteration 7/1000 | Loss: 0.00022684
Iteration 8/1000 | Loss: 0.00013294
Iteration 9/1000 | Loss: 0.00075073
Iteration 10/1000 | Loss: 0.00040416
Iteration 11/1000 | Loss: 0.00007778
Iteration 12/1000 | Loss: 0.00004125
Iteration 13/1000 | Loss: 0.00003835
Iteration 14/1000 | Loss: 0.00003563
Iteration 15/1000 | Loss: 0.00003411
Iteration 16/1000 | Loss: 0.00038560
Iteration 17/1000 | Loss: 0.00094691
Iteration 18/1000 | Loss: 0.00043204
Iteration 19/1000 | Loss: 0.00017518
Iteration 20/1000 | Loss: 0.00005363
Iteration 21/1000 | Loss: 0.00003077
Iteration 22/1000 | Loss: 0.00002959
Iteration 23/1000 | Loss: 0.00002810
Iteration 24/1000 | Loss: 0.00002689
Iteration 25/1000 | Loss: 0.00002601
Iteration 26/1000 | Loss: 0.00002522
Iteration 27/1000 | Loss: 0.00002478
Iteration 28/1000 | Loss: 0.00002452
Iteration 29/1000 | Loss: 0.00002428
Iteration 30/1000 | Loss: 0.00002412
Iteration 31/1000 | Loss: 0.00002411
Iteration 32/1000 | Loss: 0.00002411
Iteration 33/1000 | Loss: 0.00002410
Iteration 34/1000 | Loss: 0.00002410
Iteration 35/1000 | Loss: 0.00002410
Iteration 36/1000 | Loss: 0.00002404
Iteration 37/1000 | Loss: 0.00002404
Iteration 38/1000 | Loss: 0.00002404
Iteration 39/1000 | Loss: 0.00002404
Iteration 40/1000 | Loss: 0.00002404
Iteration 41/1000 | Loss: 0.00002404
Iteration 42/1000 | Loss: 0.00002403
Iteration 43/1000 | Loss: 0.00002403
Iteration 44/1000 | Loss: 0.00002403
Iteration 45/1000 | Loss: 0.00002403
Iteration 46/1000 | Loss: 0.00002403
Iteration 47/1000 | Loss: 0.00002403
Iteration 48/1000 | Loss: 0.00002403
Iteration 49/1000 | Loss: 0.00002402
Iteration 50/1000 | Loss: 0.00002402
Iteration 51/1000 | Loss: 0.00002401
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002400
Iteration 54/1000 | Loss: 0.00002400
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002400
Iteration 57/1000 | Loss: 0.00002399
Iteration 58/1000 | Loss: 0.00002399
Iteration 59/1000 | Loss: 0.00002399
Iteration 60/1000 | Loss: 0.00002399
Iteration 61/1000 | Loss: 0.00002398
Iteration 62/1000 | Loss: 0.00002398
Iteration 63/1000 | Loss: 0.00002398
Iteration 64/1000 | Loss: 0.00002397
Iteration 65/1000 | Loss: 0.00002397
Iteration 66/1000 | Loss: 0.00002397
Iteration 67/1000 | Loss: 0.00002397
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002396
Iteration 72/1000 | Loss: 0.00002395
Iteration 73/1000 | Loss: 0.00002395
Iteration 74/1000 | Loss: 0.00002395
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002395
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002395
Iteration 82/1000 | Loss: 0.00002395
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002395
Iteration 85/1000 | Loss: 0.00002395
Iteration 86/1000 | Loss: 0.00002395
Iteration 87/1000 | Loss: 0.00002395
Iteration 88/1000 | Loss: 0.00002395
Iteration 89/1000 | Loss: 0.00002395
Iteration 90/1000 | Loss: 0.00002395
Iteration 91/1000 | Loss: 0.00002395
Iteration 92/1000 | Loss: 0.00002395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.3946042347233742e-05, 2.3946042347233742e-05, 2.3946042347233742e-05, 2.3946042347233742e-05, 2.3946042347233742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3946042347233742e-05

Optimization complete. Final v2v error: 4.085761547088623 mm

Highest mean error: 4.367049694061279 mm for frame 137

Lowest mean error: 3.750884771347046 mm for frame 221

Saving results

Total time: 63.02363920211792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025220
Iteration 2/25 | Loss: 0.00241154
Iteration 3/25 | Loss: 0.00214113
Iteration 4/25 | Loss: 0.00180997
Iteration 5/25 | Loss: 0.00188766
Iteration 6/25 | Loss: 0.00158295
Iteration 7/25 | Loss: 0.00127699
Iteration 8/25 | Loss: 0.00122358
Iteration 9/25 | Loss: 0.00121186
Iteration 10/25 | Loss: 0.00120984
Iteration 11/25 | Loss: 0.00120959
Iteration 12/25 | Loss: 0.00120948
Iteration 13/25 | Loss: 0.00120937
Iteration 14/25 | Loss: 0.00120925
Iteration 15/25 | Loss: 0.00120910
Iteration 16/25 | Loss: 0.00121559
Iteration 17/25 | Loss: 0.00121578
Iteration 18/25 | Loss: 0.00121656
Iteration 19/25 | Loss: 0.00121447
Iteration 20/25 | Loss: 0.00121542
Iteration 21/25 | Loss: 0.00121356
Iteration 22/25 | Loss: 0.00120864
Iteration 23/25 | Loss: 0.00120816
Iteration 24/25 | Loss: 0.00120786
Iteration 25/25 | Loss: 0.00120784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41877270
Iteration 2/25 | Loss: 0.00058796
Iteration 3/25 | Loss: 0.00058795
Iteration 4/25 | Loss: 0.00058795
Iteration 5/25 | Loss: 0.00058795
Iteration 6/25 | Loss: 0.00058795
Iteration 7/25 | Loss: 0.00058795
Iteration 8/25 | Loss: 0.00058795
Iteration 9/25 | Loss: 0.00058795
Iteration 10/25 | Loss: 0.00058795
Iteration 11/25 | Loss: 0.00058795
Iteration 12/25 | Loss: 0.00058795
Iteration 13/25 | Loss: 0.00058795
Iteration 14/25 | Loss: 0.00058795
Iteration 15/25 | Loss: 0.00058795
Iteration 16/25 | Loss: 0.00058795
Iteration 17/25 | Loss: 0.00058795
Iteration 18/25 | Loss: 0.00058795
Iteration 19/25 | Loss: 0.00058795
Iteration 20/25 | Loss: 0.00058795
Iteration 21/25 | Loss: 0.00058795
Iteration 22/25 | Loss: 0.00058795
Iteration 23/25 | Loss: 0.00058795
Iteration 24/25 | Loss: 0.00058795
Iteration 25/25 | Loss: 0.00058795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14961791
Iteration 2/1000 | Loss: 0.00836962
Iteration 3/1000 | Loss: 0.00092933
Iteration 4/1000 | Loss: 0.00092614
Iteration 5/1000 | Loss: 0.00104615
Iteration 6/1000 | Loss: 0.00024053
Iteration 7/1000 | Loss: 0.00016369
Iteration 8/1000 | Loss: 0.00012801
Iteration 9/1000 | Loss: 0.00022781
Iteration 10/1000 | Loss: 0.00073522
Iteration 11/1000 | Loss: 0.00006927
Iteration 12/1000 | Loss: 0.00004283
Iteration 13/1000 | Loss: 0.00003924
Iteration 14/1000 | Loss: 0.00003611
Iteration 15/1000 | Loss: 0.00003410
Iteration 16/1000 | Loss: 0.00049606
Iteration 17/1000 | Loss: 0.00050047
Iteration 18/1000 | Loss: 0.00289687
Iteration 19/1000 | Loss: 0.00163192
Iteration 20/1000 | Loss: 0.00151486
Iteration 21/1000 | Loss: 0.00196860
Iteration 22/1000 | Loss: 0.00052288
Iteration 23/1000 | Loss: 0.00152721
Iteration 24/1000 | Loss: 0.00174332
Iteration 25/1000 | Loss: 0.00092713
Iteration 26/1000 | Loss: 0.00019425
Iteration 27/1000 | Loss: 0.00012080
Iteration 28/1000 | Loss: 0.00004640
Iteration 29/1000 | Loss: 0.00022097
Iteration 30/1000 | Loss: 0.00004053
Iteration 31/1000 | Loss: 0.00003507
Iteration 32/1000 | Loss: 0.00003240
Iteration 33/1000 | Loss: 0.00016230
Iteration 34/1000 | Loss: 0.00002993
Iteration 35/1000 | Loss: 0.00002898
Iteration 36/1000 | Loss: 0.00002780
Iteration 37/1000 | Loss: 0.00002652
Iteration 38/1000 | Loss: 0.00023363
Iteration 39/1000 | Loss: 0.00002606
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002495
Iteration 42/1000 | Loss: 0.00002468
Iteration 43/1000 | Loss: 0.00002453
Iteration 44/1000 | Loss: 0.00002430
Iteration 45/1000 | Loss: 0.00002430
Iteration 46/1000 | Loss: 0.00002426
Iteration 47/1000 | Loss: 0.00002425
Iteration 48/1000 | Loss: 0.00002424
Iteration 49/1000 | Loss: 0.00002420
Iteration 50/1000 | Loss: 0.00002411
Iteration 51/1000 | Loss: 0.00002410
Iteration 52/1000 | Loss: 0.00002408
Iteration 53/1000 | Loss: 0.00002407
Iteration 54/1000 | Loss: 0.00002407
Iteration 55/1000 | Loss: 0.00002406
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00002405
Iteration 58/1000 | Loss: 0.00002405
Iteration 59/1000 | Loss: 0.00002405
Iteration 60/1000 | Loss: 0.00002402
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002401
Iteration 64/1000 | Loss: 0.00002400
Iteration 65/1000 | Loss: 0.00002399
Iteration 66/1000 | Loss: 0.00002396
Iteration 67/1000 | Loss: 0.00002396
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002396
Iteration 72/1000 | Loss: 0.00002396
Iteration 73/1000 | Loss: 0.00002396
Iteration 74/1000 | Loss: 0.00002396
Iteration 75/1000 | Loss: 0.00002396
Iteration 76/1000 | Loss: 0.00002396
Iteration 77/1000 | Loss: 0.00002396
Iteration 78/1000 | Loss: 0.00002396
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002394
Iteration 82/1000 | Loss: 0.00002394
Iteration 83/1000 | Loss: 0.00002394
Iteration 84/1000 | Loss: 0.00002393
Iteration 85/1000 | Loss: 0.00002393
Iteration 86/1000 | Loss: 0.00002393
Iteration 87/1000 | Loss: 0.00002393
Iteration 88/1000 | Loss: 0.00002392
Iteration 89/1000 | Loss: 0.00002392
Iteration 90/1000 | Loss: 0.00002392
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002391
Iteration 95/1000 | Loss: 0.00002391
Iteration 96/1000 | Loss: 0.00002391
Iteration 97/1000 | Loss: 0.00002390
Iteration 98/1000 | Loss: 0.00002390
Iteration 99/1000 | Loss: 0.00002390
Iteration 100/1000 | Loss: 0.00002389
Iteration 101/1000 | Loss: 0.00002389
Iteration 102/1000 | Loss: 0.00002389
Iteration 103/1000 | Loss: 0.00002389
Iteration 104/1000 | Loss: 0.00002388
Iteration 105/1000 | Loss: 0.00002388
Iteration 106/1000 | Loss: 0.00002388
Iteration 107/1000 | Loss: 0.00002387
Iteration 108/1000 | Loss: 0.00002387
Iteration 109/1000 | Loss: 0.00002386
Iteration 110/1000 | Loss: 0.00002386
Iteration 111/1000 | Loss: 0.00002386
Iteration 112/1000 | Loss: 0.00002386
Iteration 113/1000 | Loss: 0.00002386
Iteration 114/1000 | Loss: 0.00002386
Iteration 115/1000 | Loss: 0.00002386
Iteration 116/1000 | Loss: 0.00002386
Iteration 117/1000 | Loss: 0.00002386
Iteration 118/1000 | Loss: 0.00002386
Iteration 119/1000 | Loss: 0.00002385
Iteration 120/1000 | Loss: 0.00002385
Iteration 121/1000 | Loss: 0.00002385
Iteration 122/1000 | Loss: 0.00002385
Iteration 123/1000 | Loss: 0.00002385
Iteration 124/1000 | Loss: 0.00002385
Iteration 125/1000 | Loss: 0.00002385
Iteration 126/1000 | Loss: 0.00002385
Iteration 127/1000 | Loss: 0.00002385
Iteration 128/1000 | Loss: 0.00002385
Iteration 129/1000 | Loss: 0.00002385
Iteration 130/1000 | Loss: 0.00002385
Iteration 131/1000 | Loss: 0.00002385
Iteration 132/1000 | Loss: 0.00002385
Iteration 133/1000 | Loss: 0.00002385
Iteration 134/1000 | Loss: 0.00002384
Iteration 135/1000 | Loss: 0.00002384
Iteration 136/1000 | Loss: 0.00002384
Iteration 137/1000 | Loss: 0.00002384
Iteration 138/1000 | Loss: 0.00002384
Iteration 139/1000 | Loss: 0.00002384
Iteration 140/1000 | Loss: 0.00002384
Iteration 141/1000 | Loss: 0.00002384
Iteration 142/1000 | Loss: 0.00002384
Iteration 143/1000 | Loss: 0.00002384
Iteration 144/1000 | Loss: 0.00002383
Iteration 145/1000 | Loss: 0.00002383
Iteration 146/1000 | Loss: 0.00002383
Iteration 147/1000 | Loss: 0.00002383
Iteration 148/1000 | Loss: 0.00002383
Iteration 149/1000 | Loss: 0.00002383
Iteration 150/1000 | Loss: 0.00002383
Iteration 151/1000 | Loss: 0.00002383
Iteration 152/1000 | Loss: 0.00002383
Iteration 153/1000 | Loss: 0.00002383
Iteration 154/1000 | Loss: 0.00002383
Iteration 155/1000 | Loss: 0.00002383
Iteration 156/1000 | Loss: 0.00002383
Iteration 157/1000 | Loss: 0.00002383
Iteration 158/1000 | Loss: 0.00002382
Iteration 159/1000 | Loss: 0.00002382
Iteration 160/1000 | Loss: 0.00002382
Iteration 161/1000 | Loss: 0.00002382
Iteration 162/1000 | Loss: 0.00002382
Iteration 163/1000 | Loss: 0.00002382
Iteration 164/1000 | Loss: 0.00002382
Iteration 165/1000 | Loss: 0.00002382
Iteration 166/1000 | Loss: 0.00002382
Iteration 167/1000 | Loss: 0.00002382
Iteration 168/1000 | Loss: 0.00002382
Iteration 169/1000 | Loss: 0.00002382
Iteration 170/1000 | Loss: 0.00002382
Iteration 171/1000 | Loss: 0.00002382
Iteration 172/1000 | Loss: 0.00002382
Iteration 173/1000 | Loss: 0.00002382
Iteration 174/1000 | Loss: 0.00002382
Iteration 175/1000 | Loss: 0.00002382
Iteration 176/1000 | Loss: 0.00002382
Iteration 177/1000 | Loss: 0.00002382
Iteration 178/1000 | Loss: 0.00002382
Iteration 179/1000 | Loss: 0.00002382
Iteration 180/1000 | Loss: 0.00002382
Iteration 181/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.382497586950194e-05, 2.382497586950194e-05, 2.382497586950194e-05, 2.382497586950194e-05, 2.382497586950194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.382497586950194e-05

Optimization complete. Final v2v error: 4.0662407875061035 mm

Highest mean error: 5.104693412780762 mm for frame 136

Lowest mean error: 3.7288260459899902 mm for frame 221

Saving results

Total time: 91.26724195480347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0016/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00058795
Iteration 2/1000 | Loss: 0.00004435
Iteration 3/1000 | Loss: 0.00003161
Iteration 4/1000 | Loss: 0.00002882
Iteration 5/1000 | Loss: 0.00002773
Iteration 6/1000 | Loss: 0.00002686
Iteration 7/1000 | Loss: 0.00002631
Iteration 8/1000 | Loss: 0.00002601
Iteration 9/1000 | Loss: 0.00002572
Iteration 10/1000 | Loss: 0.00002553
Iteration 11/1000 | Loss: 0.00002531
Iteration 12/1000 | Loss: 0.00002523
Iteration 13/1000 | Loss: 0.00020379
Iteration 14/1000 | Loss: 0.00002689
Iteration 15/1000 | Loss: 0.00002507
Iteration 16/1000 | Loss: 0.00002438
Iteration 17/1000 | Loss: 0.00002377
Iteration 18/1000 | Loss: 0.00002337
Iteration 19/1000 | Loss: 0.00002336
Iteration 20/1000 | Loss: 0.00002330
Iteration 21/1000 | Loss: 0.00002330
Iteration 22/1000 | Loss: 0.00002328
Iteration 23/1000 | Loss: 0.00002323
Iteration 24/1000 | Loss: 0.00002314
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002312
Iteration 27/1000 | Loss: 0.00002309
Iteration 28/1000 | Loss: 0.00002307
Iteration 29/1000 | Loss: 0.00002307
Iteration 30/1000 | Loss: 0.00002306
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002302
Iteration 33/1000 | Loss: 0.00002301
Iteration 34/1000 | Loss: 0.00002300
Iteration 35/1000 | Loss: 0.00002300
Iteration 36/1000 | Loss: 0.00002299
Iteration 37/1000 | Loss: 0.00002298
Iteration 38/1000 | Loss: 0.00002298
Iteration 39/1000 | Loss: 0.00002297
Iteration 40/1000 | Loss: 0.00002297
Iteration 41/1000 | Loss: 0.00002297
Iteration 42/1000 | Loss: 0.00002297
Iteration 43/1000 | Loss: 0.00002296
Iteration 44/1000 | Loss: 0.00002296
Iteration 45/1000 | Loss: 0.00002296
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002295
Iteration 48/1000 | Loss: 0.00002294
Iteration 49/1000 | Loss: 0.00002294
Iteration 50/1000 | Loss: 0.00002294
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002293
Iteration 53/1000 | Loss: 0.00002293
Iteration 54/1000 | Loss: 0.00002292
Iteration 55/1000 | Loss: 0.00002292
Iteration 56/1000 | Loss: 0.00002291
Iteration 57/1000 | Loss: 0.00002290
Iteration 58/1000 | Loss: 0.00002290
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002289
Iteration 61/1000 | Loss: 0.00002289
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002287
Iteration 65/1000 | Loss: 0.00002287
Iteration 66/1000 | Loss: 0.00002287
Iteration 67/1000 | Loss: 0.00002287
Iteration 68/1000 | Loss: 0.00002286
Iteration 69/1000 | Loss: 0.00002286
Iteration 70/1000 | Loss: 0.00002286
Iteration 71/1000 | Loss: 0.00002286
Iteration 72/1000 | Loss: 0.00002286
Iteration 73/1000 | Loss: 0.00002286
Iteration 74/1000 | Loss: 0.00002286
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002286
Iteration 79/1000 | Loss: 0.00002285
Iteration 80/1000 | Loss: 0.00002285
Iteration 81/1000 | Loss: 0.00002285
Iteration 82/1000 | Loss: 0.00002284
Iteration 83/1000 | Loss: 0.00002284
Iteration 84/1000 | Loss: 0.00002283
Iteration 85/1000 | Loss: 0.00002283
Iteration 86/1000 | Loss: 0.00002283
Iteration 87/1000 | Loss: 0.00002283
Iteration 88/1000 | Loss: 0.00002283
Iteration 89/1000 | Loss: 0.00002283
Iteration 90/1000 | Loss: 0.00002282
Iteration 91/1000 | Loss: 0.00002282
Iteration 92/1000 | Loss: 0.00002281
Iteration 93/1000 | Loss: 0.00002281
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002280
Iteration 96/1000 | Loss: 0.00002280
Iteration 97/1000 | Loss: 0.00002280
Iteration 98/1000 | Loss: 0.00002280
Iteration 99/1000 | Loss: 0.00002279
Iteration 100/1000 | Loss: 0.00002279
Iteration 101/1000 | Loss: 0.00002279
Iteration 102/1000 | Loss: 0.00002279
Iteration 103/1000 | Loss: 0.00002279
Iteration 104/1000 | Loss: 0.00002279
Iteration 105/1000 | Loss: 0.00002279
Iteration 106/1000 | Loss: 0.00002279
Iteration 107/1000 | Loss: 0.00002279
Iteration 108/1000 | Loss: 0.00002279
Iteration 109/1000 | Loss: 0.00002279
Iteration 110/1000 | Loss: 0.00002278
Iteration 111/1000 | Loss: 0.00002278
Iteration 112/1000 | Loss: 0.00002278
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00002278
Iteration 115/1000 | Loss: 0.00002278
Iteration 116/1000 | Loss: 0.00002277
Iteration 117/1000 | Loss: 0.00002277
Iteration 118/1000 | Loss: 0.00002277
Iteration 119/1000 | Loss: 0.00002277
Iteration 120/1000 | Loss: 0.00002277
Iteration 121/1000 | Loss: 0.00002277
Iteration 122/1000 | Loss: 0.00002277
Iteration 123/1000 | Loss: 0.00002277
Iteration 124/1000 | Loss: 0.00002276
Iteration 125/1000 | Loss: 0.00002276
Iteration 126/1000 | Loss: 0.00002276
Iteration 127/1000 | Loss: 0.00002276
Iteration 128/1000 | Loss: 0.00002276
Iteration 129/1000 | Loss: 0.00002276
Iteration 130/1000 | Loss: 0.00002276
Iteration 131/1000 | Loss: 0.00002276
Iteration 132/1000 | Loss: 0.00002276
Iteration 133/1000 | Loss: 0.00002276
Iteration 134/1000 | Loss: 0.00002275
Iteration 135/1000 | Loss: 0.00002275
Iteration 136/1000 | Loss: 0.00002275
Iteration 137/1000 | Loss: 0.00002275
Iteration 138/1000 | Loss: 0.00002275
Iteration 139/1000 | Loss: 0.00002275
Iteration 140/1000 | Loss: 0.00002275
Iteration 141/1000 | Loss: 0.00002275
Iteration 142/1000 | Loss: 0.00002275
Iteration 143/1000 | Loss: 0.00002275
Iteration 144/1000 | Loss: 0.00002275
Iteration 145/1000 | Loss: 0.00002275
Iteration 146/1000 | Loss: 0.00002275
Iteration 147/1000 | Loss: 0.00002275
Iteration 148/1000 | Loss: 0.00002275
Iteration 149/1000 | Loss: 0.00002275
Iteration 150/1000 | Loss: 0.00002275
Iteration 151/1000 | Loss: 0.00002275
Iteration 152/1000 | Loss: 0.00002275
Iteration 153/1000 | Loss: 0.00002275
Iteration 154/1000 | Loss: 0.00002275
Iteration 155/1000 | Loss: 0.00002275
Iteration 156/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.2746335162082687e-05, 2.2746335162082687e-05, 2.2746335162082687e-05, 2.2746335162082687e-05, 2.2746335162082687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2746335162082687e-05

Optimization complete. Final v2v error: 4.093013763427734 mm

Highest mean error: 4.394333839416504 mm for frame 6

Lowest mean error: 3.6038806438446045 mm for frame 4

Saving results

Total time: 79.26766347885132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064528
Iteration 2/25 | Loss: 0.00215435
Iteration 3/25 | Loss: 0.00153883
Iteration 4/25 | Loss: 0.00167480
Iteration 5/25 | Loss: 0.00194085
Iteration 6/25 | Loss: 0.00166992
Iteration 7/25 | Loss: 0.00167270
Iteration 8/25 | Loss: 0.00169269
Iteration 9/25 | Loss: 0.00162527
Iteration 10/25 | Loss: 0.00145060
Iteration 11/25 | Loss: 0.00136945
Iteration 12/25 | Loss: 0.00133930
Iteration 13/25 | Loss: 0.00120652
Iteration 14/25 | Loss: 0.00109873
Iteration 15/25 | Loss: 0.00105019
Iteration 16/25 | Loss: 0.00099836
Iteration 17/25 | Loss: 0.00098073
Iteration 18/25 | Loss: 0.00094661
Iteration 19/25 | Loss: 0.00095401
Iteration 20/25 | Loss: 0.00094282
Iteration 21/25 | Loss: 0.00093571
Iteration 22/25 | Loss: 0.00091869
Iteration 23/25 | Loss: 0.00090327
Iteration 24/25 | Loss: 0.00088902
Iteration 25/25 | Loss: 0.00088877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.00970480
Iteration 2/25 | Loss: 0.00244053
Iteration 3/25 | Loss: 0.00165760
Iteration 4/25 | Loss: 0.00141134
Iteration 5/25 | Loss: 0.00130842
Iteration 6/25 | Loss: 0.00120102
Iteration 7/25 | Loss: 0.00118987
Iteration 8/25 | Loss: 0.00104949
Iteration 9/25 | Loss: 0.00100594
Iteration 10/25 | Loss: 0.00096326
Iteration 11/25 | Loss: 0.00095010
Iteration 12/25 | Loss: 0.00089738
Iteration 13/25 | Loss: 0.00089388
Iteration 14/25 | Loss: 0.00085160
Iteration 15/25 | Loss: 0.00082643
Iteration 16/25 | Loss: 0.00081236
Iteration 17/25 | Loss: 0.00080468
Iteration 18/25 | Loss: 0.00079168
Iteration 19/25 | Loss: 0.00078487
Iteration 20/25 | Loss: 0.00077265
Iteration 21/25 | Loss: 0.00075862
Iteration 22/25 | Loss: 0.00075320
Iteration 23/25 | Loss: 0.00075166
Iteration 24/25 | Loss: 0.00075079
Iteration 25/25 | Loss: 0.00075050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37729645
Iteration 2/25 | Loss: 0.00038546
Iteration 3/25 | Loss: 0.00038546
Iteration 4/25 | Loss: 0.00038546
Iteration 5/25 | Loss: 0.00038546
Iteration 6/25 | Loss: 0.00038546
Iteration 7/25 | Loss: 0.00038546
Iteration 8/25 | Loss: 0.00038546
Iteration 9/25 | Loss: 0.00038546
Iteration 10/25 | Loss: 0.00038546
Iteration 11/25 | Loss: 0.00038546
Iteration 12/25 | Loss: 0.00038546
Iteration 13/25 | Loss: 0.00038546
Iteration 14/25 | Loss: 0.00038546
Iteration 15/25 | Loss: 0.00038546
Iteration 16/25 | Loss: 0.00038546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00038546021096408367, 0.00038546021096408367, 0.00038546021096408367, 0.00038546021096408367, 0.00038546021096408367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038546021096408367

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 1.46018302
Iteration 2/25 | Loss: 0.00155063
Iteration 3/25 | Loss: 0.00145769
Iteration 4/25 | Loss: 0.00145769
Iteration 5/25 | Loss: 0.00145769
Iteration 6/25 | Loss: 0.00145769
Iteration 7/25 | Loss: 0.00145769
Iteration 8/25 | Loss: 0.00145769
Iteration 9/25 | Loss: 0.00145769
Iteration 10/25 | Loss: 0.00145769
Iteration 11/25 | Loss: 0.00145769
Iteration 12/25 | Loss: 0.00145769
Iteration 13/25 | Loss: 0.00145769
Iteration 14/25 | Loss: 0.00145769
Iteration 15/25 | Loss: 0.00145769
Iteration 16/25 | Loss: 0.00145769
Iteration 17/25 | Loss: 0.00145769
Iteration 18/25 | Loss: 0.00145769
Iteration 19/25 | Loss: 0.00145769
Iteration 20/25 | Loss: 0.00145769
Iteration 21/25 | Loss: 0.00145769
Iteration 22/25 | Loss: 0.00145769
Iteration 23/25 | Loss: 0.00145769
Iteration 24/25 | Loss: 0.00145769
Iteration 25/25 | Loss: 0.00145769

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00836910
Iteration 2/25 | Loss: 0.00129385
Iteration 3/25 | Loss: 0.00106920
Iteration 4/25 | Loss: 0.00104744
Iteration 5/25 | Loss: 0.00104195
Iteration 6/25 | Loss: 0.00104074
Iteration 7/25 | Loss: 0.00104074
Iteration 8/25 | Loss: 0.00104074
Iteration 9/25 | Loss: 0.00104074
Iteration 10/25 | Loss: 0.00104074
Iteration 11/25 | Loss: 0.00104074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010407406371086836, 0.0010407406371086836, 0.0010407406371086836, 0.0010407406371086836, 0.0010407406371086836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010407406371086836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85007977
Iteration 2/25 | Loss: 0.00069304
Iteration 3/25 | Loss: 0.00069304
Iteration 4/25 | Loss: 0.00069304
Iteration 5/25 | Loss: 0.00069304
Iteration 6/25 | Loss: 0.00069304
Iteration 7/25 | Loss: 0.00069304
Iteration 8/25 | Loss: 0.00069304
Iteration 9/25 | Loss: 0.00069304
Iteration 10/25 | Loss: 0.00069304
Iteration 11/25 | Loss: 0.00069304
Iteration 12/25 | Loss: 0.00069304
Iteration 13/25 | Loss: 0.00069304
Iteration 14/25 | Loss: 0.00069304
Iteration 15/25 | Loss: 0.00069304
Iteration 16/25 | Loss: 0.00069304
Iteration 17/25 | Loss: 0.00069304
Iteration 18/25 | Loss: 0.00069304
Iteration 19/25 | Loss: 0.00069304
Iteration 20/25 | Loss: 0.00069304
Iteration 21/25 | Loss: 0.00069304
Iteration 22/25 | Loss: 0.00069304
Iteration 23/25 | Loss: 0.00069304
Iteration 24/25 | Loss: 0.00069304
Iteration 25/25 | Loss: 0.00069304

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00970480
Iteration 2/25 | Loss: 0.00244053
Iteration 3/25 | Loss: 0.00165760
Iteration 4/25 | Loss: 0.00141134
Iteration 5/25 | Loss: 0.00130842
Iteration 6/25 | Loss: 0.00120102
Iteration 7/25 | Loss: 0.00118987
Iteration 8/25 | Loss: 0.00104949
Iteration 9/25 | Loss: 0.00100594
Iteration 10/25 | Loss: 0.00096326
Iteration 11/25 | Loss: 0.00095010
Iteration 12/25 | Loss: 0.00089738
Iteration 13/25 | Loss: 0.00089388
Iteration 14/25 | Loss: 0.00085160
Iteration 15/25 | Loss: 0.00082643
Iteration 16/25 | Loss: 0.00081236
Iteration 17/25 | Loss: 0.00080468
Iteration 18/25 | Loss: 0.00079168
Iteration 19/25 | Loss: 0.00078487
Iteration 20/25 | Loss: 0.00077265
Iteration 21/25 | Loss: 0.00075862
Iteration 22/25 | Loss: 0.00075320
Iteration 23/25 | Loss: 0.00075166
Iteration 24/25 | Loss: 0.00075079
Iteration 25/25 | Loss: 0.00075050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37729645
Iteration 2/25 | Loss: 0.00038546
Iteration 3/25 | Loss: 0.00038546
Iteration 4/25 | Loss: 0.00038546
Iteration 5/25 | Loss: 0.00038546
Iteration 6/25 | Loss: 0.00038546
Iteration 7/25 | Loss: 0.00038546
Iteration 8/25 | Loss: 0.00038546
Iteration 9/25 | Loss: 0.00038546
Iteration 10/25 | Loss: 0.00038546
Iteration 11/25 | Loss: 0.00038546
Iteration 12/25 | Loss: 0.00038546
Iteration 13/25 | Loss: 0.00038546
Iteration 14/25 | Loss: 0.00038546
Iteration 15/25 | Loss: 0.00038546
Iteration 16/25 | Loss: 0.00038546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00038546021096408367, 0.00038546021096408367, 0.00038546021096408367, 0.00038546021096408367, 0.00038546021096408367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038546021096408367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069304
Iteration 2/1000 | Loss: 0.00002084
Iteration 3/1000 | Loss: 0.00001399
Iteration 4/1000 | Loss: 0.00001261
Iteration 5/1000 | Loss: 0.00001200
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001124
Iteration 8/1000 | Loss: 0.00001123
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001052
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001050
Iteration 18/1000 | Loss: 0.00001050
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001045
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001043
Iteration 27/1000 | Loss: 0.00001042
Iteration 28/1000 | Loss: 0.00001042
Iteration 29/1000 | Loss: 0.00001041
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001040
Iteration 32/1000 | Loss: 0.00001040
Iteration 33/1000 | Loss: 0.00001039
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001035
Iteration 39/1000 | Loss: 0.00001035
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001033
Iteration 43/1000 | Loss: 0.00001033
Iteration 44/1000 | Loss: 0.00001032
Iteration 45/1000 | Loss: 0.00001031
Iteration 46/1000 | Loss: 0.00001031
Iteration 47/1000 | Loss: 0.00001031
Iteration 48/1000 | Loss: 0.00001030
Iteration 49/1000 | Loss: 0.00001030
Iteration 50/1000 | Loss: 0.00001030
Iteration 51/1000 | Loss: 0.00001030
Iteration 52/1000 | Loss: 0.00001029
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001025
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001020
Iteration 66/1000 | Loss: 0.00001019
Iteration 67/1000 | Loss: 0.00001019
Iteration 68/1000 | Loss: 0.00001019
Iteration 69/1000 | Loss: 0.00001018
Iteration 70/1000 | Loss: 0.00001017
Iteration 71/1000 | Loss: 0.00001017
Iteration 72/1000 | Loss: 0.00001017
Iteration 73/1000 | Loss: 0.00001017
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001016
Iteration 76/1000 | Loss: 0.00001016
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001016
Iteration 80/1000 | Loss: 0.00001016
Iteration 81/1000 | Loss: 0.00001016
Iteration 82/1000 | Loss: 0.00001015
Iteration 83/1000 | Loss: 0.00001015
Iteration 84/1000 | Loss: 0.00001015
Iteration 85/1000 | Loss: 0.00001015
Iteration 86/1000 | Loss: 0.00001015
Iteration 87/1000 | Loss: 0.00001015
Iteration 88/1000 | Loss: 0.00001014
Iteration 89/1000 | Loss: 0.00001014
Iteration 90/1000 | Loss: 0.00001014
Iteration 91/1000 | Loss: 0.00001014
Iteration 92/1000 | Loss: 0.00001014
Iteration 93/1000 | Loss: 0.00001014
Iteration 94/1000 | Loss: 0.00001014
Iteration 95/1000 | Loss: 0.00001013
Iteration 96/1000 | Loss: 0.00001013
Iteration 97/1000 | Loss: 0.00001013
Iteration 98/1000 | Loss: 0.00001013
Iteration 99/1000 | Loss: 0.00001013
Iteration 100/1000 | Loss: 0.00001013
Iteration 101/1000 | Loss: 0.00001012
Iteration 102/1000 | Loss: 0.00001012
Iteration 103/1000 | Loss: 0.00001012
Iteration 104/1000 | Loss: 0.00001012
Iteration 105/1000 | Loss: 0.00001012
Iteration 106/1000 | Loss: 0.00001011
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001011
Iteration 109/1000 | Loss: 0.00001010
Iteration 110/1000 | Loss: 0.00001010
Iteration 111/1000 | Loss: 0.00001010
Iteration 112/1000 | Loss: 0.00001010
Iteration 113/1000 | Loss: 0.00001010
Iteration 114/1000 | Loss: 0.00001009
Iteration 115/1000 | Loss: 0.00001009
Iteration 116/1000 | Loss: 0.00001009
Iteration 117/1000 | Loss: 0.00001009
Iteration 118/1000 | Loss: 0.00001009
Iteration 119/1000 | Loss: 0.00001008
Iteration 120/1000 | Loss: 0.00001008
Iteration 121/1000 | Loss: 0.00001008
Iteration 122/1000 | Loss: 0.00001007
Iteration 123/1000 | Loss: 0.00001007
Iteration 124/1000 | Loss: 0.00001007
Iteration 125/1000 | Loss: 0.00001006
Iteration 126/1000 | Loss: 0.00001006
Iteration 127/1000 | Loss: 0.00001006
Iteration 128/1000 | Loss: 0.00001005
Iteration 129/1000 | Loss: 0.00001005
Iteration 130/1000 | Loss: 0.00001005
Iteration 131/1000 | Loss: 0.00001004
Iteration 132/1000 | Loss: 0.00001004
Iteration 133/1000 | Loss: 0.00001004
Iteration 134/1000 | Loss: 0.00001004
Iteration 135/1000 | Loss: 0.00001004
Iteration 136/1000 | Loss: 0.00001003
Iteration 137/1000 | Loss: 0.00001003
Iteration 138/1000 | Loss: 0.00001003
Iteration 139/1000 | Loss: 0.00001002
Iteration 140/1000 | Loss: 0.00001002
Iteration 141/1000 | Loss: 0.00001002
Iteration 142/1000 | Loss: 0.00001002
Iteration 143/1000 | Loss: 0.00001002
Iteration 144/1000 | Loss: 0.00001002
Iteration 145/1000 | Loss: 0.00001002
Iteration 146/1000 | Loss: 0.00001002
Iteration 147/1000 | Loss: 0.00001002
Iteration 148/1000 | Loss: 0.00001002
Iteration 149/1000 | Loss: 0.00001002
Iteration 150/1000 | Loss: 0.00001002
Iteration 151/1000 | Loss: 0.00001002
Iteration 152/1000 | Loss: 0.00001001
Iteration 153/1000 | Loss: 0.00001001
Iteration 154/1000 | Loss: 0.00001001
Iteration 155/1000 | Loss: 0.00001001
Iteration 156/1000 | Loss: 0.00001001
Iteration 157/1000 | Loss: 0.00001001
Iteration 158/1000 | Loss: 0.00001001
Iteration 159/1000 | Loss: 0.00001001
Iteration 160/1000 | Loss: 0.00001001
Iteration 161/1000 | Loss: 0.00001001
Iteration 162/1000 | Loss: 0.00001000
Iteration 163/1000 | Loss: 0.00001000
Iteration 164/1000 | Loss: 0.00001000
Iteration 165/1000 | Loss: 0.00001000
Iteration 166/1000 | Loss: 0.00001000
Iteration 167/1000 | Loss: 0.00001000
Iteration 168/1000 | Loss: 0.00001000
Iteration 169/1000 | Loss: 0.00000999
Iteration 170/1000 | Loss: 0.00000999
Iteration 171/1000 | Loss: 0.00000999
Iteration 172/1000 | Loss: 0.00000999
Iteration 173/1000 | Loss: 0.00000999
Iteration 174/1000 | Loss: 0.00000999
Iteration 175/1000 | Loss: 0.00000998
Iteration 176/1000 | Loss: 0.00000998
Iteration 177/1000 | Loss: 0.00000998
Iteration 178/1000 | Loss: 0.00000998
Iteration 179/1000 | Loss: 0.00000998
Iteration 180/1000 | Loss: 0.00000998
Iteration 181/1000 | Loss: 0.00000998
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000997
Iteration 184/1000 | Loss: 0.00000997
Iteration 185/1000 | Loss: 0.00000997
Iteration 186/1000 | Loss: 0.00000997
Iteration 187/1000 | Loss: 0.00000997
Iteration 188/1000 | Loss: 0.00000996
Iteration 189/1000 | Loss: 0.00000996
Iteration 190/1000 | Loss: 0.00000996
Iteration 191/1000 | Loss: 0.00000996
Iteration 192/1000 | Loss: 0.00000996
Iteration 193/1000 | Loss: 0.00000996
Iteration 194/1000 | Loss: 0.00000996
Iteration 195/1000 | Loss: 0.00000996
Iteration 196/1000 | Loss: 0.00000996
Iteration 197/1000 | Loss: 0.00000996
Iteration 198/1000 | Loss: 0.00000996
Iteration 199/1000 | Loss: 0.00000996
Iteration 200/1000 | Loss: 0.00000996
Iteration 201/1000 | Loss: 0.00000996
Iteration 202/1000 | Loss: 0.00000996
Iteration 203/1000 | Loss: 0.00000995
Iteration 204/1000 | Loss: 0.00000995
Iteration 205/1000 | Loss: 0.00000995
Iteration 206/1000 | Loss: 0.00000995
Iteration 207/1000 | Loss: 0.00000995
Iteration 208/1000 | Loss: 0.00000995
Iteration 209/1000 | Loss: 0.00000995
Iteration 210/1000 | Loss: 0.00000995
Iteration 211/1000 | Loss: 0.00000995
Iteration 212/1000 | Loss: 0.00000995
Iteration 213/1000 | Loss: 0.00000995
Iteration 214/1000 | Loss: 0.00000995
Iteration 215/1000 | Loss: 0.00000995
Iteration 216/1000 | Loss: 0.00000995
Iteration 217/1000 | Loss: 0.00000995
Iteration 218/1000 | Loss: 0.00000995
Iteration 219/1000 | Loss: 0.00000995
Iteration 220/1000 | Loss: 0.00000995
Iteration 221/1000 | Loss: 0.00000995
Iteration 222/1000 | Loss: 0.00000995
Iteration 223/1000 | Loss: 0.00000995
Iteration 224/1000 | Loss: 0.00000995
Iteration 225/1000 | Loss: 0.00000995
Iteration 226/1000 | Loss: 0.00000995
Iteration 227/1000 | Loss: 0.00000995
Iteration 228/1000 | Loss: 0.00000995
Iteration 229/1000 | Loss: 0.00000995
Iteration 230/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [9.953951121133287e-06, 9.953951121133287e-06, 9.953951121133287e-06, 9.953951121133287e-06, 9.953951121133287e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.953951121133287e-06

Optimization complete. Final v2v error: 2.720957040786743 mm

Highest mean error: 3.0815999507904053 mm for frame 172

Lowest mean error: 2.3845860958099365 mm for frame 0

Saving results

Total time: 43.882731437683105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370820
Iteration 2/25 | Loss: 0.00098732
Iteration 3/25 | Loss: 0.00090601
Iteration 4/25 | Loss: 0.00089365
Iteration 5/25 | Loss: 0.00088884
Iteration 6/25 | Loss: 0.00088810
Iteration 7/25 | Loss: 0.00088810
Iteration 8/25 | Loss: 0.00088810
Iteration 9/25 | Loss: 0.00088810
Iteration 10/25 | Loss: 0.00088810
Iteration 11/25 | Loss: 0.00088810
Iteration 12/25 | Loss: 0.00088810
Iteration 13/25 | Loss: 0.00088810
Iteration 14/25 | Loss: 0.00088810
Iteration 15/25 | Loss: 0.00088810
Iteration 16/25 | Loss: 0.00088810
Iteration 17/25 | Loss: 0.00088810
Iteration 18/25 | Loss: 0.00088810
Iteration 19/25 | Loss: 0.00088810
Iteration 20/25 | Loss: 0.00088810
Iteration 21/25 | Loss: 0.00088810
Iteration 22/25 | Loss: 0.00088810
Iteration 23/25 | Loss: 0.00088810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008880982641130686, 0.0008880982641130686, 0.0008880982641130686, 0.0008880982641130686, 0.0008880982641130686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008880982641130686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69014323
Iteration 2/25 | Loss: 0.00028605
Iteration 3/25 | Loss: 0.00028605
Iteration 4/25 | Loss: 0.00028605
Iteration 5/25 | Loss: 0.00028605
Iteration 6/25 | Loss: 0.00028605
Iteration 7/25 | Loss: 0.00028605
Iteration 8/25 | Loss: 0.00028605
Iteration 9/25 | Loss: 0.00028605
Iteration 10/25 | Loss: 0.00028605
Iteration 11/25 | Loss: 0.00028605
Iteration 12/25 | Loss: 0.00028605
Iteration 13/25 | Loss: 0.00028605
Iteration 14/25 | Loss: 0.00028605
Iteration 15/25 | Loss: 0.00028605
Iteration 16/25 | Loss: 0.00028605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002860496169887483, 0.0002860496169887483, 0.0002860496169887483, 0.0002860496169887483, 0.0002860496169887483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002860496169887483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038546
Iteration 2/1000 | Loss: 0.00084151
Iteration 3/1000 | Loss: 0.00009713
Iteration 4/1000 | Loss: 0.00007481
Iteration 5/1000 | Loss: 0.00073710
Iteration 6/1000 | Loss: 0.00007787
Iteration 7/1000 | Loss: 0.00073772
Iteration 8/1000 | Loss: 0.00008859
Iteration 9/1000 | Loss: 0.00013586
Iteration 10/1000 | Loss: 0.00008238
Iteration 11/1000 | Loss: 0.00073403
Iteration 12/1000 | Loss: 0.00034304
Iteration 13/1000 | Loss: 0.00055464
Iteration 14/1000 | Loss: 0.00072039
Iteration 15/1000 | Loss: 0.00042470
Iteration 16/1000 | Loss: 0.00012786
Iteration 17/1000 | Loss: 0.00008489
Iteration 18/1000 | Loss: 0.00007904
Iteration 19/1000 | Loss: 0.00007411
Iteration 20/1000 | Loss: 0.00007185
Iteration 21/1000 | Loss: 0.00036422
Iteration 22/1000 | Loss: 0.00092765
Iteration 23/1000 | Loss: 0.00032126
Iteration 24/1000 | Loss: 0.00146974
Iteration 25/1000 | Loss: 0.00020379
Iteration 26/1000 | Loss: 0.00018631
Iteration 27/1000 | Loss: 0.00006893
Iteration 28/1000 | Loss: 0.00010643
Iteration 29/1000 | Loss: 0.00006611
Iteration 30/1000 | Loss: 0.00006118
Iteration 31/1000 | Loss: 0.00005770
Iteration 32/1000 | Loss: 0.00005171
Iteration 33/1000 | Loss: 0.00004649
Iteration 34/1000 | Loss: 0.00004431
Iteration 35/1000 | Loss: 0.00004253
Iteration 36/1000 | Loss: 0.00004155
Iteration 37/1000 | Loss: 0.00004086
Iteration 38/1000 | Loss: 0.00004044
Iteration 39/1000 | Loss: 0.00004005
Iteration 40/1000 | Loss: 0.00003972
Iteration 41/1000 | Loss: 0.00003947
Iteration 42/1000 | Loss: 0.00003946
Iteration 43/1000 | Loss: 0.00003935
Iteration 44/1000 | Loss: 0.00003924
Iteration 45/1000 | Loss: 0.00003923
Iteration 46/1000 | Loss: 0.00003923
Iteration 47/1000 | Loss: 0.00003922
Iteration 48/1000 | Loss: 0.00003921
Iteration 49/1000 | Loss: 0.00003918
Iteration 50/1000 | Loss: 0.00003916
Iteration 51/1000 | Loss: 0.00003916
Iteration 52/1000 | Loss: 0.00003916
Iteration 53/1000 | Loss: 0.00003916
Iteration 54/1000 | Loss: 0.00003915
Iteration 55/1000 | Loss: 0.00003915
Iteration 56/1000 | Loss: 0.00003915
Iteration 57/1000 | Loss: 0.00003915
Iteration 58/1000 | Loss: 0.00003915
Iteration 59/1000 | Loss: 0.00003915
Iteration 60/1000 | Loss: 0.00003914
Iteration 61/1000 | Loss: 0.00003914
Iteration 62/1000 | Loss: 0.00003914
Iteration 63/1000 | Loss: 0.00003914
Iteration 64/1000 | Loss: 0.00003914
Iteration 65/1000 | Loss: 0.00003913
Iteration 66/1000 | Loss: 0.00003913
Iteration 67/1000 | Loss: 0.00003913
Iteration 68/1000 | Loss: 0.00003913
Iteration 69/1000 | Loss: 0.00003913
Iteration 70/1000 | Loss: 0.00003913
Iteration 71/1000 | Loss: 0.00003913
Iteration 72/1000 | Loss: 0.00003912
Iteration 73/1000 | Loss: 0.00003912
Iteration 74/1000 | Loss: 0.00003912
Iteration 75/1000 | Loss: 0.00003911
Iteration 76/1000 | Loss: 0.00003911
Iteration 77/1000 | Loss: 0.00003911
Iteration 78/1000 | Loss: 0.00003911
Iteration 79/1000 | Loss: 0.00003910
Iteration 80/1000 | Loss: 0.00003910
Iteration 81/1000 | Loss: 0.00003910
Iteration 82/1000 | Loss: 0.00003909
Iteration 83/1000 | Loss: 0.00003909
Iteration 84/1000 | Loss: 0.00003909
Iteration 85/1000 | Loss: 0.00003909
Iteration 86/1000 | Loss: 0.00003909
Iteration 87/1000 | Loss: 0.00003909
Iteration 88/1000 | Loss: 0.00003909
Iteration 89/1000 | Loss: 0.00003909
Iteration 90/1000 | Loss: 0.00003909
Iteration 91/1000 | Loss: 0.00003908
Iteration 92/1000 | Loss: 0.00003908
Iteration 93/1000 | Loss: 0.00003908
Iteration 94/1000 | Loss: 0.00003908
Iteration 95/1000 | Loss: 0.00003908
Iteration 96/1000 | Loss: 0.00003908
Iteration 97/1000 | Loss: 0.00003907
Iteration 98/1000 | Loss: 0.00003907
Iteration 99/1000 | Loss: 0.00003907
Iteration 100/1000 | Loss: 0.00003907
Iteration 101/1000 | Loss: 0.00003907
Iteration 102/1000 | Loss: 0.00003907
Iteration 103/1000 | Loss: 0.00003907
Iteration 104/1000 | Loss: 0.00003907
Iteration 105/1000 | Loss: 0.00003907
Iteration 106/1000 | Loss: 0.00003907
Iteration 107/1000 | Loss: 0.00003907
Iteration 108/1000 | Loss: 0.00003907
Iteration 109/1000 | Loss: 0.00003907
Iteration 110/1000 | Loss: 0.00003907
Iteration 111/1000 | Loss: 0.00003907
Iteration 112/1000 | Loss: 0.00003907
Iteration 113/1000 | Loss: 0.00003907
Iteration 114/1000 | Loss: 0.00003907
Iteration 115/1000 | Loss: 0.00003907
Iteration 116/1000 | Loss: 0.00003907
Iteration 117/1000 | Loss: 0.00003907
Iteration 118/1000 | Loss: 0.00003907
Iteration 119/1000 | Loss: 0.00003907
Iteration 120/1000 | Loss: 0.00003907
Iteration 121/1000 | Loss: 0.00003907
Iteration 122/1000 | Loss: 0.00003907
Iteration 123/1000 | Loss: 0.00003907
Iteration 124/1000 | Loss: 0.00003907
Iteration 125/1000 | Loss: 0.00003907
Iteration 126/1000 | Loss: 0.00003907
Iteration 127/1000 | Loss: 0.00003907
Iteration 128/1000 | Loss: 0.00003907
Iteration 129/1000 | Loss: 0.00003907
Iteration 130/1000 | Loss: 0.00003907
Iteration 131/1000 | Loss: 0.00003907
Iteration 132/1000 | Loss: 0.00003907
Iteration 133/1000 | Loss: 0.00003907
Iteration 134/1000 | Loss: 0.00003907
Iteration 135/1000 | Loss: 0.00003907
Iteration 136/1000 | Loss: 0.00003907
Iteration 137/1000 | Loss: 0.00003907
Iteration 138/1000 | Loss: 0.00003907
Iteration 139/1000 | Loss: 0.00003907
Iteration 140/1000 | Loss: 0.00003907
Iteration 141/1000 | Loss: 0.00003907
Iteration 142/1000 | Loss: 0.00003907
Iteration 143/1000 | Loss: 0.00003907
Iteration 144/1000 | Loss: 0.00003907
Iteration 145/1000 | Loss: 0.00003907
Iteration 146/1000 | Loss: 0.00003907
Iteration 147/1000 | Loss: 0.00003907
Iteration 148/1000 | Loss: 0.00003907
Iteration 149/1000 | Loss: 0.00003907
Iteration 150/1000 | Loss: 0.00003907
Iteration 151/1000 | Loss: 0.00003907
Iteration 152/1000 | Loss: 0.00003907
Iteration 153/1000 | Loss: 0.00003907
Iteration 154/1000 | Loss: 0.00003907
Iteration 155/1000 | Loss: 0.00003907
Iteration 156/1000 | Loss: 0.00003907
Iteration 157/1000 | Loss: 0.00003907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.907080099452287e-05, 3.907080099452287e-05, 3.907080099452287e-05, 3.907080099452287e-05, 3.907080099452287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.907080099452287e-05

Optimization complete. Final v2v error: 4.402443885803223 mm

Highest mean error: 6.47429895401001 mm for frame 107

Lowest mean error: 3.488978862762451 mm for frame 119

Saving results

Total time: 107.1029794216156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00028605
Iteration 2/1000 | Loss: 0.00002999
Iteration 3/1000 | Loss: 0.00002297
Iteration 4/1000 | Loss: 0.00002142
Iteration 5/1000 | Loss: 0.00002067
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001972
Iteration 8/1000 | Loss: 0.00001958
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00001952
Iteration 12/1000 | Loss: 0.00001951
Iteration 13/1000 | Loss: 0.00001950
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001946
Iteration 22/1000 | Loss: 0.00001945
Iteration 23/1000 | Loss: 0.00001944
Iteration 24/1000 | Loss: 0.00001944
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001942
Iteration 28/1000 | Loss: 0.00001942
Iteration 29/1000 | Loss: 0.00001942
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001940
Iteration 34/1000 | Loss: 0.00001940
Iteration 35/1000 | Loss: 0.00001940
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001939
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001936
Iteration 66/1000 | Loss: 0.00001936
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001936
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001936
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001936
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001936
Iteration 87/1000 | Loss: 0.00001936
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00001936
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001936
Iteration 100/1000 | Loss: 0.00001936
Iteration 101/1000 | Loss: 0.00001936
Iteration 102/1000 | Loss: 0.00001936
Iteration 103/1000 | Loss: 0.00001936
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00001936
Iteration 110/1000 | Loss: 0.00001936
Iteration 111/1000 | Loss: 0.00001936
Iteration 112/1000 | Loss: 0.00001936
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001936
Iteration 115/1000 | Loss: 0.00001936
Iteration 116/1000 | Loss: 0.00001936
Iteration 117/1000 | Loss: 0.00001936
Iteration 118/1000 | Loss: 0.00001936
Iteration 119/1000 | Loss: 0.00001936
Iteration 120/1000 | Loss: 0.00001936
Iteration 121/1000 | Loss: 0.00001936
Iteration 122/1000 | Loss: 0.00001936
Iteration 123/1000 | Loss: 0.00001936
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.9355393305886537e-05, 1.9355393305886537e-05, 1.9355393305886537e-05, 1.9355393305886537e-05, 1.9355393305886537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9355393305886537e-05

Optimization complete. Final v2v error: 3.7948758602142334 mm

Highest mean error: 4.0577073097229 mm for frame 130

Lowest mean error: 3.5963633060455322 mm for frame 251

Saving results

Total time: 28.384730100631714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00038546
Iteration 2/1000 | Loss: 0.00084151
Iteration 3/1000 | Loss: 0.00009713
Iteration 4/1000 | Loss: 0.00007481
Iteration 5/1000 | Loss: 0.00073710
Iteration 6/1000 | Loss: 0.00007787
Iteration 7/1000 | Loss: 0.00073772
Iteration 8/1000 | Loss: 0.00008859
Iteration 9/1000 | Loss: 0.00013586
Iteration 10/1000 | Loss: 0.00008238
Iteration 11/1000 | Loss: 0.00073403
Iteration 12/1000 | Loss: 0.00034304
Iteration 13/1000 | Loss: 0.00055464
Iteration 14/1000 | Loss: 0.00072039
Iteration 15/1000 | Loss: 0.00042470
Iteration 16/1000 | Loss: 0.00012786
Iteration 17/1000 | Loss: 0.00008489
Iteration 18/1000 | Loss: 0.00007904
Iteration 19/1000 | Loss: 0.00007411
Iteration 20/1000 | Loss: 0.00007185
Iteration 21/1000 | Loss: 0.00036422
Iteration 22/1000 | Loss: 0.00092765
Iteration 23/1000 | Loss: 0.00032126
Iteration 24/1000 | Loss: 0.00146974
Iteration 25/1000 | Loss: 0.00020379
Iteration 26/1000 | Loss: 0.00018631
Iteration 27/1000 | Loss: 0.00006893
Iteration 28/1000 | Loss: 0.00010643
Iteration 29/1000 | Loss: 0.00006611
Iteration 30/1000 | Loss: 0.00006118
Iteration 31/1000 | Loss: 0.00005770
Iteration 32/1000 | Loss: 0.00005171
Iteration 33/1000 | Loss: 0.00004649
Iteration 34/1000 | Loss: 0.00004431
Iteration 35/1000 | Loss: 0.00004253
Iteration 36/1000 | Loss: 0.00004155
Iteration 37/1000 | Loss: 0.00004086
Iteration 38/1000 | Loss: 0.00004044
Iteration 39/1000 | Loss: 0.00004005
Iteration 40/1000 | Loss: 0.00003972
Iteration 41/1000 | Loss: 0.00003947
Iteration 42/1000 | Loss: 0.00003946
Iteration 43/1000 | Loss: 0.00003935
Iteration 44/1000 | Loss: 0.00003924
Iteration 45/1000 | Loss: 0.00003923
Iteration 46/1000 | Loss: 0.00003923
Iteration 47/1000 | Loss: 0.00003922
Iteration 48/1000 | Loss: 0.00003921
Iteration 49/1000 | Loss: 0.00003918
Iteration 50/1000 | Loss: 0.00003916
Iteration 51/1000 | Loss: 0.00003916
Iteration 52/1000 | Loss: 0.00003916
Iteration 53/1000 | Loss: 0.00003916
Iteration 54/1000 | Loss: 0.00003915
Iteration 55/1000 | Loss: 0.00003915
Iteration 56/1000 | Loss: 0.00003915
Iteration 57/1000 | Loss: 0.00003915
Iteration 58/1000 | Loss: 0.00003915
Iteration 59/1000 | Loss: 0.00003915
Iteration 60/1000 | Loss: 0.00003914
Iteration 61/1000 | Loss: 0.00003914
Iteration 62/1000 | Loss: 0.00003914
Iteration 63/1000 | Loss: 0.00003914
Iteration 64/1000 | Loss: 0.00003914
Iteration 65/1000 | Loss: 0.00003913
Iteration 66/1000 | Loss: 0.00003913
Iteration 67/1000 | Loss: 0.00003913
Iteration 68/1000 | Loss: 0.00003913
Iteration 69/1000 | Loss: 0.00003913
Iteration 70/1000 | Loss: 0.00003913
Iteration 71/1000 | Loss: 0.00003913
Iteration 72/1000 | Loss: 0.00003912
Iteration 73/1000 | Loss: 0.00003912
Iteration 74/1000 | Loss: 0.00003912
Iteration 75/1000 | Loss: 0.00003911
Iteration 76/1000 | Loss: 0.00003911
Iteration 77/1000 | Loss: 0.00003911
Iteration 78/1000 | Loss: 0.00003911
Iteration 79/1000 | Loss: 0.00003910
Iteration 80/1000 | Loss: 0.00003910
Iteration 81/1000 | Loss: 0.00003910
Iteration 82/1000 | Loss: 0.00003909
Iteration 83/1000 | Loss: 0.00003909
Iteration 84/1000 | Loss: 0.00003909
Iteration 85/1000 | Loss: 0.00003909
Iteration 86/1000 | Loss: 0.00003909
Iteration 87/1000 | Loss: 0.00003909
Iteration 88/1000 | Loss: 0.00003909
Iteration 89/1000 | Loss: 0.00003909
Iteration 90/1000 | Loss: 0.00003909
Iteration 91/1000 | Loss: 0.00003908
Iteration 92/1000 | Loss: 0.00003908
Iteration 93/1000 | Loss: 0.00003908
Iteration 94/1000 | Loss: 0.00003908
Iteration 95/1000 | Loss: 0.00003908
Iteration 96/1000 | Loss: 0.00003908
Iteration 97/1000 | Loss: 0.00003907
Iteration 98/1000 | Loss: 0.00003907
Iteration 99/1000 | Loss: 0.00003907
Iteration 100/1000 | Loss: 0.00003907
Iteration 101/1000 | Loss: 0.00003907
Iteration 102/1000 | Loss: 0.00003907
Iteration 103/1000 | Loss: 0.00003907
Iteration 104/1000 | Loss: 0.00003907
Iteration 105/1000 | Loss: 0.00003907
Iteration 106/1000 | Loss: 0.00003907
Iteration 107/1000 | Loss: 0.00003907
Iteration 108/1000 | Loss: 0.00003907
Iteration 109/1000 | Loss: 0.00003907
Iteration 110/1000 | Loss: 0.00003907
Iteration 111/1000 | Loss: 0.00003907
Iteration 112/1000 | Loss: 0.00003907
Iteration 113/1000 | Loss: 0.00003907
Iteration 114/1000 | Loss: 0.00003907
Iteration 115/1000 | Loss: 0.00003907
Iteration 116/1000 | Loss: 0.00003907
Iteration 117/1000 | Loss: 0.00003907
Iteration 118/1000 | Loss: 0.00003907
Iteration 119/1000 | Loss: 0.00003907
Iteration 120/1000 | Loss: 0.00003907
Iteration 121/1000 | Loss: 0.00003907
Iteration 122/1000 | Loss: 0.00003907
Iteration 123/1000 | Loss: 0.00003907
Iteration 124/1000 | Loss: 0.00003907
Iteration 125/1000 | Loss: 0.00003907
Iteration 126/1000 | Loss: 0.00003907
Iteration 127/1000 | Loss: 0.00003907
Iteration 128/1000 | Loss: 0.00003907
Iteration 129/1000 | Loss: 0.00003907
Iteration 130/1000 | Loss: 0.00003907
Iteration 131/1000 | Loss: 0.00003907
Iteration 132/1000 | Loss: 0.00003907
Iteration 133/1000 | Loss: 0.00003907
Iteration 134/1000 | Loss: 0.00003907
Iteration 135/1000 | Loss: 0.00003907
Iteration 136/1000 | Loss: 0.00003907
Iteration 137/1000 | Loss: 0.00003907
Iteration 138/1000 | Loss: 0.00003907
Iteration 139/1000 | Loss: 0.00003907
Iteration 140/1000 | Loss: 0.00003907
Iteration 141/1000 | Loss: 0.00003907
Iteration 142/1000 | Loss: 0.00003907
Iteration 143/1000 | Loss: 0.00003907
Iteration 144/1000 | Loss: 0.00003907
Iteration 145/1000 | Loss: 0.00003907
Iteration 146/1000 | Loss: 0.00003907
Iteration 147/1000 | Loss: 0.00003907
Iteration 148/1000 | Loss: 0.00003907
Iteration 149/1000 | Loss: 0.00003907
Iteration 150/1000 | Loss: 0.00003907
Iteration 151/1000 | Loss: 0.00003907
Iteration 152/1000 | Loss: 0.00003907
Iteration 153/1000 | Loss: 0.00003907
Iteration 154/1000 | Loss: 0.00003907
Iteration 155/1000 | Loss: 0.00003907
Iteration 156/1000 | Loss: 0.00003907
Iteration 157/1000 | Loss: 0.00003907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.907080099452287e-05, 3.907080099452287e-05, 3.907080099452287e-05, 3.907080099452287e-05, 3.907080099452287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.907080099452287e-05

Optimization complete. Final v2v error: 4.402443885803223 mm

Highest mean error: 6.47429895401001 mm for frame 107

Lowest mean error: 3.488978862762451 mm for frame 119

Saving results

Total time: 106.76891231536865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0006/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130940
Iteration 2/25 | Loss: 0.00257666
Iteration 3/25 | Loss: 0.00145917
Iteration 4/25 | Loss: 0.00131572
Iteration 5/25 | Loss: 0.00118619
Iteration 6/25 | Loss: 0.00113243
Iteration 7/25 | Loss: 0.00109471
Iteration 8/25 | Loss: 0.00100678
Iteration 9/25 | Loss: 0.00100866
Iteration 10/25 | Loss: 0.00094194
Iteration 11/25 | Loss: 0.00092797
Iteration 12/25 | Loss: 0.00091915
Iteration 13/25 | Loss: 0.00088364
Iteration 14/25 | Loss: 0.00085220
Iteration 15/25 | Loss: 0.00085005
Iteration 16/25 | Loss: 0.00082958
Iteration 17/25 | Loss: 0.00082300
Iteration 18/25 | Loss: 0.00080946
Iteration 19/25 | Loss: 0.00080627
Iteration 20/25 | Loss: 0.00080622
Iteration 21/25 | Loss: 0.00080487
Iteration 22/25 | Loss: 0.00080534
Iteration 23/25 | Loss: 0.00080497
Iteration 24/25 | Loss: 0.00080466
Iteration 25/25 | Loss: 0.00080384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09167910
Iteration 2/25 | Loss: 0.00069414
Iteration 3/25 | Loss: 0.00069413
Iteration 4/25 | Loss: 0.00069413
Iteration 5/25 | Loss: 0.00069413
Iteration 6/25 | Loss: 0.00069413
Iteration 7/25 | Loss: 0.00069413
Iteration 8/25 | Loss: 0.00069413
Iteration 9/25 | Loss: 0.00069413
Iteration 10/25 | Loss: 0.00069413
Iteration 11/25 | Loss: 0.00069413
Iteration 12/25 | Loss: 0.00069413
Iteration 13/25 | Loss: 0.00069413
Iteration 14/25 | Loss: 0.00069413
Iteration 15/25 | Loss: 0.00069413
Iteration 16/25 | Loss: 0.00069413
Iteration 17/25 | Loss: 0.00069413
Iteration 18/25 | Loss: 0.00069413
Iteration 19/25 | Loss: 0.00069413
Iteration 20/25 | Loss: 0.00069413
Iteration 21/25 | Loss: 0.00069413
Iteration 22/25 | Loss: 0.00069413
Iteration 23/25 | Loss: 0.00069413
Iteration 24/25 | Loss: 0.00069413
Iteration 25/25 | Loss: 0.00069413

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01134711
Iteration 2/25 | Loss: 0.00194943
Iteration 3/25 | Loss: 0.00139714
Iteration 4/25 | Loss: 0.00128690
Iteration 5/25 | Loss: 0.00126421
Iteration 6/25 | Loss: 0.00117756
Iteration 7/25 | Loss: 0.00112883
Iteration 8/25 | Loss: 0.00110099
Iteration 9/25 | Loss: 0.00106154
Iteration 10/25 | Loss: 0.00104786
Iteration 11/25 | Loss: 0.00105436
Iteration 12/25 | Loss: 0.00103333
Iteration 13/25 | Loss: 0.00101788
Iteration 14/25 | Loss: 0.00101122
Iteration 15/25 | Loss: 0.00101485
Iteration 16/25 | Loss: 0.00100934
Iteration 17/25 | Loss: 0.00100902
Iteration 18/25 | Loss: 0.00100870
Iteration 19/25 | Loss: 0.00100854
Iteration 20/25 | Loss: 0.00100841
Iteration 21/25 | Loss: 0.00100831
Iteration 22/25 | Loss: 0.00100823
Iteration 23/25 | Loss: 0.00100822
Iteration 24/25 | Loss: 0.00100815
Iteration 25/25 | Loss: 0.00100815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42135108
Iteration 2/25 | Loss: 0.00063447
Iteration 3/25 | Loss: 0.00063447
Iteration 4/25 | Loss: 0.00063447
Iteration 5/25 | Loss: 0.00063447
Iteration 6/25 | Loss: 0.00063447
Iteration 7/25 | Loss: 0.00063447
Iteration 8/25 | Loss: 0.00063447
Iteration 9/25 | Loss: 0.00063447
Iteration 10/25 | Loss: 0.00063447
Iteration 11/25 | Loss: 0.00063447
Iteration 12/25 | Loss: 0.00063447
Iteration 13/25 | Loss: 0.00063447
Iteration 14/25 | Loss: 0.00063447
Iteration 15/25 | Loss: 0.00063447
Iteration 16/25 | Loss: 0.00063447
Iteration 17/25 | Loss: 0.00063447
Iteration 18/25 | Loss: 0.00063447
Iteration 19/25 | Loss: 0.00063447
Iteration 20/25 | Loss: 0.00063447
Iteration 21/25 | Loss: 0.00063447
Iteration 22/25 | Loss: 0.00063447
Iteration 23/25 | Loss: 0.00063447
Iteration 24/25 | Loss: 0.00063447
Iteration 25/25 | Loss: 0.00063447

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01130940
Iteration 2/25 | Loss: 0.00257666
Iteration 3/25 | Loss: 0.00145917
Iteration 4/25 | Loss: 0.00131572
Iteration 5/25 | Loss: 0.00118619
Iteration 6/25 | Loss: 0.00113243
Iteration 7/25 | Loss: 0.00109471
Iteration 8/25 | Loss: 0.00100678
Iteration 9/25 | Loss: 0.00100866
Iteration 10/25 | Loss: 0.00094194
Iteration 11/25 | Loss: 0.00092797
Iteration 12/25 | Loss: 0.00091915
Iteration 13/25 | Loss: 0.00088364
Iteration 14/25 | Loss: 0.00085220
Iteration 15/25 | Loss: 0.00085005
Iteration 16/25 | Loss: 0.00082958
Iteration 17/25 | Loss: 0.00082300
Iteration 18/25 | Loss: 0.00080946
Iteration 19/25 | Loss: 0.00080627
Iteration 20/25 | Loss: 0.00080622
Iteration 21/25 | Loss: 0.00080487
Iteration 22/25 | Loss: 0.00080534
Iteration 23/25 | Loss: 0.00080497
Iteration 24/25 | Loss: 0.00080466
Iteration 25/25 | Loss: 0.00080384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09167910
Iteration 2/25 | Loss: 0.00069414
Iteration 3/25 | Loss: 0.00069413
Iteration 4/25 | Loss: 0.00069413
Iteration 5/25 | Loss: 0.00069413
Iteration 6/25 | Loss: 0.00069413
Iteration 7/25 | Loss: 0.00069413
Iteration 8/25 | Loss: 0.00069413
Iteration 9/25 | Loss: 0.00069413
Iteration 10/25 | Loss: 0.00069413
Iteration 11/25 | Loss: 0.00069413
Iteration 12/25 | Loss: 0.00069413
Iteration 13/25 | Loss: 0.00069413
Iteration 14/25 | Loss: 0.00069413
Iteration 15/25 | Loss: 0.00069413
Iteration 16/25 | Loss: 0.00069413
Iteration 17/25 | Loss: 0.00069413
Iteration 18/25 | Loss: 0.00069413
Iteration 19/25 | Loss: 0.00069413
Iteration 20/25 | Loss: 0.00069413
Iteration 21/25 | Loss: 0.00069413
Iteration 22/25 | Loss: 0.00069413
Iteration 23/25 | Loss: 0.00069413
Iteration 24/25 | Loss: 0.00069413
Iteration 25/25 | Loss: 0.00069413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063447
Iteration 2/1000 | Loss: 0.00042789
Iteration 3/1000 | Loss: 0.00010126
Iteration 4/1000 | Loss: 0.00007813
Iteration 5/1000 | Loss: 0.00006817
Iteration 6/1000 | Loss: 0.00006105
Iteration 7/1000 | Loss: 0.00083013
Iteration 8/1000 | Loss: 0.00202371
Iteration 9/1000 | Loss: 0.00006629
Iteration 10/1000 | Loss: 0.00004647
Iteration 11/1000 | Loss: 0.00003529
Iteration 12/1000 | Loss: 0.00003030
Iteration 13/1000 | Loss: 0.00002838
Iteration 14/1000 | Loss: 0.00002728
Iteration 15/1000 | Loss: 0.00002655
Iteration 16/1000 | Loss: 0.00002597
Iteration 17/1000 | Loss: 0.00002543
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002481
Iteration 20/1000 | Loss: 0.00002461
Iteration 21/1000 | Loss: 0.00002456
Iteration 22/1000 | Loss: 0.00002455
Iteration 23/1000 | Loss: 0.00002453
Iteration 24/1000 | Loss: 0.00002451
Iteration 25/1000 | Loss: 0.00002451
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00002450
Iteration 28/1000 | Loss: 0.00002450
Iteration 29/1000 | Loss: 0.00002450
Iteration 30/1000 | Loss: 0.00002450
Iteration 31/1000 | Loss: 0.00002450
Iteration 32/1000 | Loss: 0.00002449
Iteration 33/1000 | Loss: 0.00002448
Iteration 34/1000 | Loss: 0.00002447
Iteration 35/1000 | Loss: 0.00002446
Iteration 36/1000 | Loss: 0.00002446
Iteration 37/1000 | Loss: 0.00002445
Iteration 38/1000 | Loss: 0.00002444
Iteration 39/1000 | Loss: 0.00002443
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002442
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002442
Iteration 46/1000 | Loss: 0.00002442
Iteration 47/1000 | Loss: 0.00002442
Iteration 48/1000 | Loss: 0.00002442
Iteration 49/1000 | Loss: 0.00002442
Iteration 50/1000 | Loss: 0.00002442
Iteration 51/1000 | Loss: 0.00002442
Iteration 52/1000 | Loss: 0.00002442
Iteration 53/1000 | Loss: 0.00002442
Iteration 54/1000 | Loss: 0.00002442
Iteration 55/1000 | Loss: 0.00002442
Iteration 56/1000 | Loss: 0.00002442
Iteration 57/1000 | Loss: 0.00002442
Iteration 58/1000 | Loss: 0.00002442
Iteration 59/1000 | Loss: 0.00002442
Iteration 60/1000 | Loss: 0.00002442
Iteration 61/1000 | Loss: 0.00002442
Iteration 62/1000 | Loss: 0.00002442
Iteration 63/1000 | Loss: 0.00002442
Iteration 64/1000 | Loss: 0.00002442
Iteration 65/1000 | Loss: 0.00002442
Iteration 66/1000 | Loss: 0.00002442
Iteration 67/1000 | Loss: 0.00002442
Iteration 68/1000 | Loss: 0.00002442
Iteration 69/1000 | Loss: 0.00002442
Iteration 70/1000 | Loss: 0.00002442
Iteration 71/1000 | Loss: 0.00002442
Iteration 72/1000 | Loss: 0.00002442
Iteration 73/1000 | Loss: 0.00002442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.4416471205768175e-05, 2.4416471205768175e-05, 2.4416471205768175e-05, 2.4416471205768175e-05, 2.4416471205768175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4416471205768175e-05

Optimization complete. Final v2v error: 4.133725166320801 mm

Highest mean error: 10.231609344482422 mm for frame 110

Lowest mean error: 3.777658462524414 mm for frame 229

Saving results

Total time: 81.29602932929993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400845
Iteration 2/25 | Loss: 0.00098268
Iteration 3/25 | Loss: 0.00089143
Iteration 4/25 | Loss: 0.00087902
Iteration 5/25 | Loss: 0.00087478
Iteration 6/25 | Loss: 0.00087386
Iteration 7/25 | Loss: 0.00087367
Iteration 8/25 | Loss: 0.00087367
Iteration 9/25 | Loss: 0.00087367
Iteration 10/25 | Loss: 0.00087367
Iteration 11/25 | Loss: 0.00087367
Iteration 12/25 | Loss: 0.00087367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008736749296076596, 0.0008736749296076596, 0.0008736749296076596, 0.0008736749296076596, 0.0008736749296076596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008736749296076596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46448827
Iteration 2/25 | Loss: 0.00030787
Iteration 3/25 | Loss: 0.00030787
Iteration 4/25 | Loss: 0.00030787
Iteration 5/25 | Loss: 0.00030787
Iteration 6/25 | Loss: 0.00030787
Iteration 7/25 | Loss: 0.00030787
Iteration 8/25 | Loss: 0.00030787
Iteration 9/25 | Loss: 0.00030787
Iteration 10/25 | Loss: 0.00030787
Iteration 11/25 | Loss: 0.00030787
Iteration 12/25 | Loss: 0.00030787
Iteration 13/25 | Loss: 0.00030787
Iteration 14/25 | Loss: 0.00030787
Iteration 15/25 | Loss: 0.00030787
Iteration 16/25 | Loss: 0.00030787
Iteration 17/25 | Loss: 0.00030787
Iteration 18/25 | Loss: 0.00030787
Iteration 19/25 | Loss: 0.00030787
Iteration 20/25 | Loss: 0.00030787
Iteration 21/25 | Loss: 0.00030787
Iteration 22/25 | Loss: 0.00030787
Iteration 23/25 | Loss: 0.00030787
Iteration 24/25 | Loss: 0.00030787
Iteration 25/25 | Loss: 0.00030787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030787
Iteration 2/1000 | Loss: 0.00003100
Iteration 3/1000 | Loss: 0.00002070
Iteration 4/1000 | Loss: 0.00001837
Iteration 5/1000 | Loss: 0.00001761
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001694
Iteration 8/1000 | Loss: 0.00001675
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001666
Iteration 12/1000 | Loss: 0.00001665
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001664
Iteration 15/1000 | Loss: 0.00001663
Iteration 16/1000 | Loss: 0.00001662
Iteration 17/1000 | Loss: 0.00001662
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001661
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001656
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001656
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001655
Iteration 36/1000 | Loss: 0.00001655
Iteration 37/1000 | Loss: 0.00001655
Iteration 38/1000 | Loss: 0.00001655
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001654
Iteration 41/1000 | Loss: 0.00001654
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001653
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001652
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001649
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001648
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001642
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001641
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001640
Iteration 95/1000 | Loss: 0.00001640
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001639
Iteration 103/1000 | Loss: 0.00001639
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001638
Iteration 126/1000 | Loss: 0.00001638
Iteration 127/1000 | Loss: 0.00001638
Iteration 128/1000 | Loss: 0.00001638
Iteration 129/1000 | Loss: 0.00001638
Iteration 130/1000 | Loss: 0.00001638
Iteration 131/1000 | Loss: 0.00001637
Iteration 132/1000 | Loss: 0.00001637
Iteration 133/1000 | Loss: 0.00001637
Iteration 134/1000 | Loss: 0.00001637
Iteration 135/1000 | Loss: 0.00001637
Iteration 136/1000 | Loss: 0.00001637
Iteration 137/1000 | Loss: 0.00001637
Iteration 138/1000 | Loss: 0.00001637
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001637
Iteration 141/1000 | Loss: 0.00001637
Iteration 142/1000 | Loss: 0.00001637
Iteration 143/1000 | Loss: 0.00001637
Iteration 144/1000 | Loss: 0.00001637
Iteration 145/1000 | Loss: 0.00001637
Iteration 146/1000 | Loss: 0.00001637
Iteration 147/1000 | Loss: 0.00001637
Iteration 148/1000 | Loss: 0.00001637
Iteration 149/1000 | Loss: 0.00001637
Iteration 150/1000 | Loss: 0.00001637
Iteration 151/1000 | Loss: 0.00001637
Iteration 152/1000 | Loss: 0.00001637
Iteration 153/1000 | Loss: 0.00001637
Iteration 154/1000 | Loss: 0.00001637
Iteration 155/1000 | Loss: 0.00001637
Iteration 156/1000 | Loss: 0.00001637
Iteration 157/1000 | Loss: 0.00001637
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Iteration 170/1000 | Loss: 0.00001637
Iteration 171/1000 | Loss: 0.00001637
Iteration 172/1000 | Loss: 0.00001637
Iteration 173/1000 | Loss: 0.00001637
Iteration 174/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.6368760043405928e-05, 1.6368760043405928e-05, 1.6368760043405928e-05, 1.6368760043405928e-05, 1.6368760043405928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6368760043405928e-05

Optimization complete. Final v2v error: 3.526801824569702 mm

Highest mean error: 4.011188983917236 mm for frame 13

Lowest mean error: 3.0572283267974854 mm for frame 133

Saving results

Total time: 31.750711679458618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00069413
Iteration 2/1000 | Loss: 0.00087674
Iteration 3/1000 | Loss: 0.00016624
Iteration 4/1000 | Loss: 0.00012164
Iteration 5/1000 | Loss: 0.00009492
Iteration 6/1000 | Loss: 0.00009538
Iteration 7/1000 | Loss: 0.00010623
Iteration 8/1000 | Loss: 0.00007369
Iteration 9/1000 | Loss: 0.00008423
Iteration 10/1000 | Loss: 0.00008867
Iteration 11/1000 | Loss: 0.00020028
Iteration 12/1000 | Loss: 0.00010885
Iteration 13/1000 | Loss: 0.00008257
Iteration 14/1000 | Loss: 0.00008210
Iteration 15/1000 | Loss: 0.00024149
Iteration 16/1000 | Loss: 0.00062019
Iteration 17/1000 | Loss: 0.00007704
Iteration 18/1000 | Loss: 0.00008006
Iteration 19/1000 | Loss: 0.00008230
Iteration 20/1000 | Loss: 0.00007488
Iteration 21/1000 | Loss: 0.00007332
Iteration 22/1000 | Loss: 0.00014356
Iteration 23/1000 | Loss: 0.00008338
Iteration 24/1000 | Loss: 0.00005623
Iteration 25/1000 | Loss: 0.00006865
Iteration 26/1000 | Loss: 0.00006652
Iteration 27/1000 | Loss: 0.00007227
Iteration 28/1000 | Loss: 0.00004768
Iteration 29/1000 | Loss: 0.00006026
Iteration 30/1000 | Loss: 0.00008487
Iteration 31/1000 | Loss: 0.00008686
Iteration 32/1000 | Loss: 0.00007659
Iteration 33/1000 | Loss: 0.00009175
Iteration 34/1000 | Loss: 0.00007500
Iteration 35/1000 | Loss: 0.00008832
Iteration 36/1000 | Loss: 0.00007496
Iteration 37/1000 | Loss: 0.00008286
Iteration 38/1000 | Loss: 0.00007483
Iteration 39/1000 | Loss: 0.00007762
Iteration 40/1000 | Loss: 0.00014131
Iteration 41/1000 | Loss: 0.00009985
Iteration 42/1000 | Loss: 0.00007110
Iteration 43/1000 | Loss: 0.00006820
Iteration 44/1000 | Loss: 0.00006902
Iteration 45/1000 | Loss: 0.00006843
Iteration 46/1000 | Loss: 0.00007678
Iteration 47/1000 | Loss: 0.00006893
Iteration 48/1000 | Loss: 0.00006764
Iteration 49/1000 | Loss: 0.00007244
Iteration 50/1000 | Loss: 0.00006776
Iteration 51/1000 | Loss: 0.00006537
Iteration 52/1000 | Loss: 0.00006817
Iteration 53/1000 | Loss: 0.00023768
Iteration 54/1000 | Loss: 0.00014197
Iteration 55/1000 | Loss: 0.00067917
Iteration 56/1000 | Loss: 0.00020899
Iteration 57/1000 | Loss: 0.00005484
Iteration 58/1000 | Loss: 0.00004474
Iteration 59/1000 | Loss: 0.00004184
Iteration 60/1000 | Loss: 0.00004053
Iteration 61/1000 | Loss: 0.00003964
Iteration 62/1000 | Loss: 0.00003919
Iteration 63/1000 | Loss: 0.00003890
Iteration 64/1000 | Loss: 0.00003870
Iteration 65/1000 | Loss: 0.00003868
Iteration 66/1000 | Loss: 0.00003864
Iteration 67/1000 | Loss: 0.00003863
Iteration 68/1000 | Loss: 0.00003862
Iteration 69/1000 | Loss: 0.00003861
Iteration 70/1000 | Loss: 0.00003861
Iteration 71/1000 | Loss: 0.00003860
Iteration 72/1000 | Loss: 0.00003859
Iteration 73/1000 | Loss: 0.00003859
Iteration 74/1000 | Loss: 0.00003859
Iteration 75/1000 | Loss: 0.00003855
Iteration 76/1000 | Loss: 0.00003854
Iteration 77/1000 | Loss: 0.00003847
Iteration 78/1000 | Loss: 0.00003846
Iteration 79/1000 | Loss: 0.00003846
Iteration 80/1000 | Loss: 0.00003845
Iteration 81/1000 | Loss: 0.00003844
Iteration 82/1000 | Loss: 0.00003843
Iteration 83/1000 | Loss: 0.00003841
Iteration 84/1000 | Loss: 0.00003841
Iteration 85/1000 | Loss: 0.00003841
Iteration 86/1000 | Loss: 0.00003841
Iteration 87/1000 | Loss: 0.00003841
Iteration 88/1000 | Loss: 0.00003841
Iteration 89/1000 | Loss: 0.00003841
Iteration 90/1000 | Loss: 0.00003841
Iteration 91/1000 | Loss: 0.00003841
Iteration 92/1000 | Loss: 0.00003840
Iteration 93/1000 | Loss: 0.00003839
Iteration 94/1000 | Loss: 0.00003834
Iteration 95/1000 | Loss: 0.00003833
Iteration 96/1000 | Loss: 0.00003833
Iteration 97/1000 | Loss: 0.00003832
Iteration 98/1000 | Loss: 0.00003832
Iteration 99/1000 | Loss: 0.00003831
Iteration 100/1000 | Loss: 0.00003831
Iteration 101/1000 | Loss: 0.00003831
Iteration 102/1000 | Loss: 0.00003831
Iteration 103/1000 | Loss: 0.00003831
Iteration 104/1000 | Loss: 0.00003831
Iteration 105/1000 | Loss: 0.00003830
Iteration 106/1000 | Loss: 0.00003830
Iteration 107/1000 | Loss: 0.00003830
Iteration 108/1000 | Loss: 0.00003830
Iteration 109/1000 | Loss: 0.00003830
Iteration 110/1000 | Loss: 0.00003830
Iteration 111/1000 | Loss: 0.00003830
Iteration 112/1000 | Loss: 0.00003830
Iteration 113/1000 | Loss: 0.00003829
Iteration 114/1000 | Loss: 0.00003829
Iteration 115/1000 | Loss: 0.00003829
Iteration 116/1000 | Loss: 0.00003828
Iteration 117/1000 | Loss: 0.00003828
Iteration 118/1000 | Loss: 0.00003828
Iteration 119/1000 | Loss: 0.00003828
Iteration 120/1000 | Loss: 0.00003828
Iteration 121/1000 | Loss: 0.00003828
Iteration 122/1000 | Loss: 0.00003828
Iteration 123/1000 | Loss: 0.00003828
Iteration 124/1000 | Loss: 0.00003827
Iteration 125/1000 | Loss: 0.00003827
Iteration 126/1000 | Loss: 0.00003827
Iteration 127/1000 | Loss: 0.00003827
Iteration 128/1000 | Loss: 0.00003827
Iteration 129/1000 | Loss: 0.00003827
Iteration 130/1000 | Loss: 0.00003827
Iteration 131/1000 | Loss: 0.00003827
Iteration 132/1000 | Loss: 0.00003827
Iteration 133/1000 | Loss: 0.00003827
Iteration 134/1000 | Loss: 0.00003827
Iteration 135/1000 | Loss: 0.00003827
Iteration 136/1000 | Loss: 0.00003827
Iteration 137/1000 | Loss: 0.00003827
Iteration 138/1000 | Loss: 0.00003827
Iteration 139/1000 | Loss: 0.00003827
Iteration 140/1000 | Loss: 0.00003827
Iteration 141/1000 | Loss: 0.00003827
Iteration 142/1000 | Loss: 0.00003827
Iteration 143/1000 | Loss: 0.00003827
Iteration 144/1000 | Loss: 0.00003827
Iteration 145/1000 | Loss: 0.00003827
Iteration 146/1000 | Loss: 0.00003827
Iteration 147/1000 | Loss: 0.00003827
Iteration 148/1000 | Loss: 0.00003827
Iteration 149/1000 | Loss: 0.00003827
Iteration 150/1000 | Loss: 0.00003827
Iteration 151/1000 | Loss: 0.00003827
Iteration 152/1000 | Loss: 0.00003827
Iteration 153/1000 | Loss: 0.00003827
Iteration 154/1000 | Loss: 0.00003827
Iteration 155/1000 | Loss: 0.00003827
Iteration 156/1000 | Loss: 0.00003827
Iteration 157/1000 | Loss: 0.00003827
Iteration 158/1000 | Loss: 0.00003827
Iteration 159/1000 | Loss: 0.00003827
Iteration 160/1000 | Loss: 0.00003827
Iteration 161/1000 | Loss: 0.00003827
Iteration 162/1000 | Loss: 0.00003827
Iteration 163/1000 | Loss: 0.00003827
Iteration 164/1000 | Loss: 0.00003827
Iteration 165/1000 | Loss: 0.00003827
Iteration 166/1000 | Loss: 0.00003827
Iteration 167/1000 | Loss: 0.00003827
Iteration 168/1000 | Loss: 0.00003827
Iteration 169/1000 | Loss: 0.00003827
Iteration 170/1000 | Loss: 0.00003827
Iteration 171/1000 | Loss: 0.00003827
Iteration 172/1000 | Loss: 0.00003827
Iteration 173/1000 | Loss: 0.00003827
Iteration 174/1000 | Loss: 0.00003827
Iteration 175/1000 | Loss: 0.00003827
Iteration 176/1000 | Loss: 0.00003827
Iteration 177/1000 | Loss: 0.00003827
Iteration 178/1000 | Loss: 0.00003827
Iteration 179/1000 | Loss: 0.00003827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.826664396910928e-05, 3.826664396910928e-05, 3.826664396910928e-05, 3.826664396910928e-05, 3.826664396910928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.826664396910928e-05

Optimization complete. Final v2v error: 4.805593490600586 mm

Highest mean error: 19.56760597229004 mm for frame 16

Lowest mean error: 4.088514804840088 mm for frame 0

Saving results

Total time: 142.41341304779053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818268
Iteration 2/25 | Loss: 0.00110518
Iteration 3/25 | Loss: 0.00069080
Iteration 4/25 | Loss: 0.00061439
Iteration 5/25 | Loss: 0.00060314
Iteration 6/25 | Loss: 0.00060073
Iteration 7/25 | Loss: 0.00060068
Iteration 8/25 | Loss: 0.00060068
Iteration 9/25 | Loss: 0.00060068
Iteration 10/25 | Loss: 0.00060068
Iteration 11/25 | Loss: 0.00060068
Iteration 12/25 | Loss: 0.00060068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006006843759678304, 0.0006006843759678304, 0.0006006843759678304, 0.0006006843759678304, 0.0006006843759678304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006006843759678304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.01162850
Iteration 2/25 | Loss: 0.00230863
Iteration 3/25 | Loss: 0.00136501
Iteration 4/25 | Loss: 0.00119505
Iteration 5/25 | Loss: 0.00113046
Iteration 6/25 | Loss: 0.00111663
Iteration 7/25 | Loss: 0.00109450
Iteration 8/25 | Loss: 0.00108732
Iteration 9/25 | Loss: 0.00108550
Iteration 10/25 | Loss: 0.00108506
Iteration 11/25 | Loss: 0.00108479
Iteration 12/25 | Loss: 0.00108517
Iteration 13/25 | Loss: 0.00108320
Iteration 14/25 | Loss: 0.00108293
Iteration 15/25 | Loss: 0.00108284
Iteration 16/25 | Loss: 0.00108281
Iteration 17/25 | Loss: 0.00108280
Iteration 18/25 | Loss: 0.00108280
Iteration 19/25 | Loss: 0.00108280
Iteration 20/25 | Loss: 0.00108280
Iteration 21/25 | Loss: 0.00108280
Iteration 22/25 | Loss: 0.00108280
Iteration 23/25 | Loss: 0.00108280
Iteration 24/25 | Loss: 0.00108280
Iteration 25/25 | Loss: 0.00108280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37384653
Iteration 2/25 | Loss: 0.00013487
Iteration 3/25 | Loss: 0.00013486
Iteration 4/25 | Loss: 0.00013486
Iteration 5/25 | Loss: 0.00013486
Iteration 6/25 | Loss: 0.00013486
Iteration 7/25 | Loss: 0.00013486
Iteration 8/25 | Loss: 0.00013486
Iteration 9/25 | Loss: 0.00013486
Iteration 10/25 | Loss: 0.00013486
Iteration 11/25 | Loss: 0.00013486
Iteration 12/25 | Loss: 0.00013486
Iteration 13/25 | Loss: 0.00013486
Iteration 14/25 | Loss: 0.00013486
Iteration 15/25 | Loss: 0.00013486
Iteration 16/25 | Loss: 0.00013486
Iteration 17/25 | Loss: 0.00013486
Iteration 18/25 | Loss: 0.00013486
Iteration 19/25 | Loss: 0.00013486
Iteration 20/25 | Loss: 0.00013486
Iteration 21/25 | Loss: 0.00013486
Iteration 22/25 | Loss: 0.00013486
Iteration 23/25 | Loss: 0.00013486
Iteration 24/25 | Loss: 0.00013486
Iteration 25/25 | Loss: 0.00013486

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 3.71048832
Iteration 2/25 | Loss: 0.00058253
Iteration 3/25 | Loss: 0.00058244
Iteration 4/25 | Loss: 0.00058244
Iteration 5/25 | Loss: 0.00058244
Iteration 6/25 | Loss: 0.00058244
Iteration 7/25 | Loss: 0.00058244
Iteration 8/25 | Loss: 0.00058244
Iteration 9/25 | Loss: 0.00058244
Iteration 10/25 | Loss: 0.00058244
Iteration 11/25 | Loss: 0.00058244
Iteration 12/25 | Loss: 0.00058244
Iteration 13/25 | Loss: 0.00058244
Iteration 14/25 | Loss: 0.00058244
Iteration 15/25 | Loss: 0.00058244
Iteration 16/25 | Loss: 0.00058244
Iteration 17/25 | Loss: 0.00058244
Iteration 18/25 | Loss: 0.00058244
Iteration 19/25 | Loss: 0.00058244
Iteration 20/25 | Loss: 0.00058244
Iteration 21/25 | Loss: 0.00058244
Iteration 22/25 | Loss: 0.00058244
Iteration 23/25 | Loss: 0.00058244
Iteration 24/25 | Loss: 0.00058244
Iteration 25/25 | Loss: 0.00058244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069413
Iteration 2/1000 | Loss: 0.00087674
Iteration 3/1000 | Loss: 0.00016624
Iteration 4/1000 | Loss: 0.00012164
Iteration 5/1000 | Loss: 0.00009492
Iteration 6/1000 | Loss: 0.00009538
Iteration 7/1000 | Loss: 0.00010623
Iteration 8/1000 | Loss: 0.00007369
Iteration 9/1000 | Loss: 0.00008423
Iteration 10/1000 | Loss: 0.00008867
Iteration 11/1000 | Loss: 0.00020028
Iteration 12/1000 | Loss: 0.00010885
Iteration 13/1000 | Loss: 0.00008257
Iteration 14/1000 | Loss: 0.00008210
Iteration 15/1000 | Loss: 0.00024149
Iteration 16/1000 | Loss: 0.00062019
Iteration 17/1000 | Loss: 0.00007704
Iteration 18/1000 | Loss: 0.00008006
Iteration 19/1000 | Loss: 0.00008230
Iteration 20/1000 | Loss: 0.00007488
Iteration 21/1000 | Loss: 0.00007332
Iteration 22/1000 | Loss: 0.00014356
Iteration 23/1000 | Loss: 0.00008338
Iteration 24/1000 | Loss: 0.00005623
Iteration 25/1000 | Loss: 0.00006865
Iteration 26/1000 | Loss: 0.00006652
Iteration 27/1000 | Loss: 0.00007227
Iteration 28/1000 | Loss: 0.00004768
Iteration 29/1000 | Loss: 0.00006026
Iteration 30/1000 | Loss: 0.00008487
Iteration 31/1000 | Loss: 0.00008686
Iteration 32/1000 | Loss: 0.00007659
Iteration 33/1000 | Loss: 0.00009175
Iteration 34/1000 | Loss: 0.00007500
Iteration 35/1000 | Loss: 0.00008832
Iteration 36/1000 | Loss: 0.00007496
Iteration 37/1000 | Loss: 0.00008286
Iteration 38/1000 | Loss: 0.00007483
Iteration 39/1000 | Loss: 0.00007762
Iteration 40/1000 | Loss: 0.00014131
Iteration 41/1000 | Loss: 0.00009985
Iteration 42/1000 | Loss: 0.00007110
Iteration 43/1000 | Loss: 0.00006820
Iteration 44/1000 | Loss: 0.00006902
Iteration 45/1000 | Loss: 0.00006843
Iteration 46/1000 | Loss: 0.00007678
Iteration 47/1000 | Loss: 0.00006893
Iteration 48/1000 | Loss: 0.00006764
Iteration 49/1000 | Loss: 0.00007244
Iteration 50/1000 | Loss: 0.00006776
Iteration 51/1000 | Loss: 0.00006537
Iteration 52/1000 | Loss: 0.00006817
Iteration 53/1000 | Loss: 0.00023768
Iteration 54/1000 | Loss: 0.00014197
Iteration 55/1000 | Loss: 0.00067917
Iteration 56/1000 | Loss: 0.00020899
Iteration 57/1000 | Loss: 0.00005484
Iteration 58/1000 | Loss: 0.00004474
Iteration 59/1000 | Loss: 0.00004184
Iteration 60/1000 | Loss: 0.00004053
Iteration 61/1000 | Loss: 0.00003964
Iteration 62/1000 | Loss: 0.00003919
Iteration 63/1000 | Loss: 0.00003890
Iteration 64/1000 | Loss: 0.00003870
Iteration 65/1000 | Loss: 0.00003868
Iteration 66/1000 | Loss: 0.00003864
Iteration 67/1000 | Loss: 0.00003863
Iteration 68/1000 | Loss: 0.00003862
Iteration 69/1000 | Loss: 0.00003861
Iteration 70/1000 | Loss: 0.00003861
Iteration 71/1000 | Loss: 0.00003860
Iteration 72/1000 | Loss: 0.00003859
Iteration 73/1000 | Loss: 0.00003859
Iteration 74/1000 | Loss: 0.00003859
Iteration 75/1000 | Loss: 0.00003855
Iteration 76/1000 | Loss: 0.00003854
Iteration 77/1000 | Loss: 0.00003847
Iteration 78/1000 | Loss: 0.00003846
Iteration 79/1000 | Loss: 0.00003846
Iteration 80/1000 | Loss: 0.00003845
Iteration 81/1000 | Loss: 0.00003844
Iteration 82/1000 | Loss: 0.00003843
Iteration 83/1000 | Loss: 0.00003841
Iteration 84/1000 | Loss: 0.00003841
Iteration 85/1000 | Loss: 0.00003841
Iteration 86/1000 | Loss: 0.00003841
Iteration 87/1000 | Loss: 0.00003841
Iteration 88/1000 | Loss: 0.00003841
Iteration 89/1000 | Loss: 0.00003841
Iteration 90/1000 | Loss: 0.00003841
Iteration 91/1000 | Loss: 0.00003841
Iteration 92/1000 | Loss: 0.00003840
Iteration 93/1000 | Loss: 0.00003839
Iteration 94/1000 | Loss: 0.00003834
Iteration 95/1000 | Loss: 0.00003833
Iteration 96/1000 | Loss: 0.00003833
Iteration 97/1000 | Loss: 0.00003832
Iteration 98/1000 | Loss: 0.00003832
Iteration 99/1000 | Loss: 0.00003831
Iteration 100/1000 | Loss: 0.00003831
Iteration 101/1000 | Loss: 0.00003831
Iteration 102/1000 | Loss: 0.00003831
Iteration 103/1000 | Loss: 0.00003831
Iteration 104/1000 | Loss: 0.00003831
Iteration 105/1000 | Loss: 0.00003830
Iteration 106/1000 | Loss: 0.00003830
Iteration 107/1000 | Loss: 0.00003830
Iteration 108/1000 | Loss: 0.00003830
Iteration 109/1000 | Loss: 0.00003830
Iteration 110/1000 | Loss: 0.00003830
Iteration 111/1000 | Loss: 0.00003830
Iteration 112/1000 | Loss: 0.00003830
Iteration 113/1000 | Loss: 0.00003829
Iteration 114/1000 | Loss: 0.00003829
Iteration 115/1000 | Loss: 0.00003829
Iteration 116/1000 | Loss: 0.00003828
Iteration 117/1000 | Loss: 0.00003828
Iteration 118/1000 | Loss: 0.00003828
Iteration 119/1000 | Loss: 0.00003828
Iteration 120/1000 | Loss: 0.00003828
Iteration 121/1000 | Loss: 0.00003828
Iteration 122/1000 | Loss: 0.00003828
Iteration 123/1000 | Loss: 0.00003828
Iteration 124/1000 | Loss: 0.00003827
Iteration 125/1000 | Loss: 0.00003827
Iteration 126/1000 | Loss: 0.00003827
Iteration 127/1000 | Loss: 0.00003827
Iteration 128/1000 | Loss: 0.00003827
Iteration 129/1000 | Loss: 0.00003827
Iteration 130/1000 | Loss: 0.00003827
Iteration 131/1000 | Loss: 0.00003827
Iteration 132/1000 | Loss: 0.00003827
Iteration 133/1000 | Loss: 0.00003827
Iteration 134/1000 | Loss: 0.00003827
Iteration 135/1000 | Loss: 0.00003827
Iteration 136/1000 | Loss: 0.00003827
Iteration 137/1000 | Loss: 0.00003827
Iteration 138/1000 | Loss: 0.00003827
Iteration 139/1000 | Loss: 0.00003827
Iteration 140/1000 | Loss: 0.00003827
Iteration 141/1000 | Loss: 0.00003827
Iteration 142/1000 | Loss: 0.00003827
Iteration 143/1000 | Loss: 0.00003827
Iteration 144/1000 | Loss: 0.00003827
Iteration 145/1000 | Loss: 0.00003827
Iteration 146/1000 | Loss: 0.00003827
Iteration 147/1000 | Loss: 0.00003827
Iteration 148/1000 | Loss: 0.00003827
Iteration 149/1000 | Loss: 0.00003827
Iteration 150/1000 | Loss: 0.00003827
Iteration 151/1000 | Loss: 0.00003827
Iteration 152/1000 | Loss: 0.00003827
Iteration 153/1000 | Loss: 0.00003827
Iteration 154/1000 | Loss: 0.00003827
Iteration 155/1000 | Loss: 0.00003827
Iteration 156/1000 | Loss: 0.00003827
Iteration 157/1000 | Loss: 0.00003827
Iteration 158/1000 | Loss: 0.00003827
Iteration 159/1000 | Loss: 0.00003827
Iteration 160/1000 | Loss: 0.00003827
Iteration 161/1000 | Loss: 0.00003827
Iteration 162/1000 | Loss: 0.00003827
Iteration 163/1000 | Loss: 0.00003827
Iteration 164/1000 | Loss: 0.00003827
Iteration 165/1000 | Loss: 0.00003827
Iteration 166/1000 | Loss: 0.00003827
Iteration 167/1000 | Loss: 0.00003827
Iteration 168/1000 | Loss: 0.00003827
Iteration 169/1000 | Loss: 0.00003827
Iteration 170/1000 | Loss: 0.00003827
Iteration 171/1000 | Loss: 0.00003827
Iteration 172/1000 | Loss: 0.00003827
Iteration 173/1000 | Loss: 0.00003827
Iteration 174/1000 | Loss: 0.00003827
Iteration 175/1000 | Loss: 0.00003827
Iteration 176/1000 | Loss: 0.00003827
Iteration 177/1000 | Loss: 0.00003827
Iteration 178/1000 | Loss: 0.00003827
Iteration 179/1000 | Loss: 0.00003827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.826664396910928e-05, 3.826664396910928e-05, 3.826664396910928e-05, 3.826664396910928e-05, 3.826664396910928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.826664396910928e-05

Optimization complete. Final v2v error: 4.805593490600586 mm

Highest mean error: 19.56760597229004 mm for frame 16

Lowest mean error: 4.088514804840088 mm for frame 0

Saving results

Total time: 141.07694578170776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0023/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818268
Iteration 2/25 | Loss: 0.00110487
Iteration 3/25 | Loss: 0.00068763
Iteration 4/25 | Loss: 0.00061398
Iteration 5/25 | Loss: 0.00060299
Iteration 6/25 | Loss: 0.00060071
Iteration 7/25 | Loss: 0.00060068
Iteration 8/25 | Loss: 0.00060068
Iteration 9/25 | Loss: 0.00060068
Iteration 10/25 | Loss: 0.00060068
Iteration 11/25 | Loss: 0.00060068
Iteration 12/25 | Loss: 0.00060068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006006823386996984, 0.0006006823386996984, 0.0006006823386996984, 0.0006006823386996984, 0.0006006823386996984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006006823386996984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37384987
Iteration 2/25 | Loss: 0.00013507
Iteration 3/25 | Loss: 0.00013506
Iteration 4/25 | Loss: 0.00013506
Iteration 5/25 | Loss: 0.00013506
Iteration 6/25 | Loss: 0.00013506
Iteration 7/25 | Loss: 0.00013506
Iteration 8/25 | Loss: 0.00013506
Iteration 9/25 | Loss: 0.00013506
Iteration 10/25 | Loss: 0.00013506
Iteration 11/25 | Loss: 0.00013506
Iteration 12/25 | Loss: 0.00013506
Iteration 13/25 | Loss: 0.00013506
Iteration 14/25 | Loss: 0.00013506
Iteration 15/25 | Loss: 0.00013506
Iteration 16/25 | Loss: 0.00013506
Iteration 17/25 | Loss: 0.00013506
Iteration 18/25 | Loss: 0.00013506
Iteration 19/25 | Loss: 0.00013506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00013505936658475548, 0.00013505936658475548, 0.00013505936658475548, 0.00013505936658475548, 0.00013505936658475548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013505936658475548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013486
Iteration 2/1000 | Loss: 0.00003741
Iteration 3/1000 | Loss: 0.00003064
Iteration 4/1000 | Loss: 0.00002761
Iteration 5/1000 | Loss: 0.00002572
Iteration 6/1000 | Loss: 0.00002457
Iteration 7/1000 | Loss: 0.00002368
Iteration 8/1000 | Loss: 0.00002299
Iteration 9/1000 | Loss: 0.00002264
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002225
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002201
Iteration 14/1000 | Loss: 0.00002189
Iteration 15/1000 | Loss: 0.00002180
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002175
Iteration 18/1000 | Loss: 0.00002171
Iteration 19/1000 | Loss: 0.00002170
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002166
Iteration 25/1000 | Loss: 0.00002166
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002165
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002165
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002162
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002160
Iteration 39/1000 | Loss: 0.00002159
Iteration 40/1000 | Loss: 0.00002159
Iteration 41/1000 | Loss: 0.00002158
Iteration 42/1000 | Loss: 0.00002158
Iteration 43/1000 | Loss: 0.00002158
Iteration 44/1000 | Loss: 0.00002158
Iteration 45/1000 | Loss: 0.00002158
Iteration 46/1000 | Loss: 0.00002158
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002154
Iteration 51/1000 | Loss: 0.00002154
Iteration 52/1000 | Loss: 0.00002154
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002153
Iteration 56/1000 | Loss: 0.00002153
Iteration 57/1000 | Loss: 0.00002150
Iteration 58/1000 | Loss: 0.00002150
Iteration 59/1000 | Loss: 0.00002150
Iteration 60/1000 | Loss: 0.00002150
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00002150
Iteration 63/1000 | Loss: 0.00002150
Iteration 64/1000 | Loss: 0.00002150
Iteration 65/1000 | Loss: 0.00002150
Iteration 66/1000 | Loss: 0.00002149
Iteration 67/1000 | Loss: 0.00002149
Iteration 68/1000 | Loss: 0.00002149
Iteration 69/1000 | Loss: 0.00002149
Iteration 70/1000 | Loss: 0.00002149
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002148
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002147
Iteration 79/1000 | Loss: 0.00002147
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002146
Iteration 82/1000 | Loss: 0.00002146
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002144
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00002143
Iteration 93/1000 | Loss: 0.00002143
Iteration 94/1000 | Loss: 0.00002143
Iteration 95/1000 | Loss: 0.00002143
Iteration 96/1000 | Loss: 0.00002143
Iteration 97/1000 | Loss: 0.00002143
Iteration 98/1000 | Loss: 0.00002143
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002142
Iteration 104/1000 | Loss: 0.00002142
Iteration 105/1000 | Loss: 0.00002142
Iteration 106/1000 | Loss: 0.00002142
Iteration 107/1000 | Loss: 0.00002142
Iteration 108/1000 | Loss: 0.00002142
Iteration 109/1000 | Loss: 0.00002142
Iteration 110/1000 | Loss: 0.00002142
Iteration 111/1000 | Loss: 0.00002142
Iteration 112/1000 | Loss: 0.00002142
Iteration 113/1000 | Loss: 0.00002141
Iteration 114/1000 | Loss: 0.00002141
Iteration 115/1000 | Loss: 0.00002141
Iteration 116/1000 | Loss: 0.00002141
Iteration 117/1000 | Loss: 0.00002141
Iteration 118/1000 | Loss: 0.00002141
Iteration 119/1000 | Loss: 0.00002141
Iteration 120/1000 | Loss: 0.00002141
Iteration 121/1000 | Loss: 0.00002141
Iteration 122/1000 | Loss: 0.00002140
Iteration 123/1000 | Loss: 0.00002140
Iteration 124/1000 | Loss: 0.00002140
Iteration 125/1000 | Loss: 0.00002140
Iteration 126/1000 | Loss: 0.00002140
Iteration 127/1000 | Loss: 0.00002140
Iteration 128/1000 | Loss: 0.00002140
Iteration 129/1000 | Loss: 0.00002140
Iteration 130/1000 | Loss: 0.00002140
Iteration 131/1000 | Loss: 0.00002140
Iteration 132/1000 | Loss: 0.00002140
Iteration 133/1000 | Loss: 0.00002140
Iteration 134/1000 | Loss: 0.00002140
Iteration 135/1000 | Loss: 0.00002140
Iteration 136/1000 | Loss: 0.00002140
Iteration 137/1000 | Loss: 0.00002140
Iteration 138/1000 | Loss: 0.00002140
Iteration 139/1000 | Loss: 0.00002140
Iteration 140/1000 | Loss: 0.00002140
Iteration 141/1000 | Loss: 0.00002140
Iteration 142/1000 | Loss: 0.00002140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.1397529053501785e-05, 2.1397529053501785e-05, 2.1397529053501785e-05, 2.1397529053501785e-05, 2.1397529053501785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1397529053501785e-05

Optimization complete. Final v2v error: 3.8935916423797607 mm

Highest mean error: 4.291258811950684 mm for frame 88

Lowest mean error: 3.4196746349334717 mm for frame 123

Saving results

Total time: 44.30432415008545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00013506
Iteration 2/1000 | Loss: 0.00003515
Iteration 3/1000 | Loss: 0.00002845
Iteration 4/1000 | Loss: 0.00002678
Iteration 5/1000 | Loss: 0.00002515
Iteration 6/1000 | Loss: 0.00002414
Iteration 7/1000 | Loss: 0.00002323
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002224
Iteration 11/1000 | Loss: 0.00002200
Iteration 12/1000 | Loss: 0.00002184
Iteration 13/1000 | Loss: 0.00002180
Iteration 14/1000 | Loss: 0.00002179
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002175
Iteration 21/1000 | Loss: 0.00002174
Iteration 22/1000 | Loss: 0.00002173
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002165
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002161
Iteration 36/1000 | Loss: 0.00002161
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002160
Iteration 39/1000 | Loss: 0.00002160
Iteration 40/1000 | Loss: 0.00002159
Iteration 41/1000 | Loss: 0.00002157
Iteration 42/1000 | Loss: 0.00002157
Iteration 43/1000 | Loss: 0.00002157
Iteration 44/1000 | Loss: 0.00002157
Iteration 45/1000 | Loss: 0.00002157
Iteration 46/1000 | Loss: 0.00002157
Iteration 47/1000 | Loss: 0.00002157
Iteration 48/1000 | Loss: 0.00002157
Iteration 49/1000 | Loss: 0.00002156
Iteration 50/1000 | Loss: 0.00002154
Iteration 51/1000 | Loss: 0.00002154
Iteration 52/1000 | Loss: 0.00002153
Iteration 53/1000 | Loss: 0.00002153
Iteration 54/1000 | Loss: 0.00002152
Iteration 55/1000 | Loss: 0.00002152
Iteration 56/1000 | Loss: 0.00002150
Iteration 57/1000 | Loss: 0.00002150
Iteration 58/1000 | Loss: 0.00002150
Iteration 59/1000 | Loss: 0.00002150
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002148
Iteration 65/1000 | Loss: 0.00002148
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002147
Iteration 68/1000 | Loss: 0.00002147
Iteration 69/1000 | Loss: 0.00002147
Iteration 70/1000 | Loss: 0.00002147
Iteration 71/1000 | Loss: 0.00002146
Iteration 72/1000 | Loss: 0.00002146
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00002144
Iteration 77/1000 | Loss: 0.00002144
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002143
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002143
Iteration 84/1000 | Loss: 0.00002143
Iteration 85/1000 | Loss: 0.00002143
Iteration 86/1000 | Loss: 0.00002143
Iteration 87/1000 | Loss: 0.00002143
Iteration 88/1000 | Loss: 0.00002143
Iteration 89/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.1431673303595744e-05, 2.1431673303595744e-05, 2.1431673303595744e-05, 2.1431673303595744e-05, 2.1431673303595744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1431673303595744e-05

Optimization complete. Final v2v error: 3.9001083374023438 mm

Highest mean error: 4.297884464263916 mm for frame 88

Lowest mean error: 3.424259662628174 mm for frame 123

Saving results

Total time: 37.92405128479004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0024/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00058244
Iteration 2/1000 | Loss: 0.00006587
Iteration 3/1000 | Loss: 0.00005117
Iteration 4/1000 | Loss: 0.00004812
Iteration 5/1000 | Loss: 0.00004638
Iteration 6/1000 | Loss: 0.00004544
Iteration 7/1000 | Loss: 0.00004489
Iteration 8/1000 | Loss: 0.00004431
Iteration 9/1000 | Loss: 0.00004385
Iteration 10/1000 | Loss: 0.00004350
Iteration 11/1000 | Loss: 0.00004325
Iteration 12/1000 | Loss: 0.00004305
Iteration 13/1000 | Loss: 0.00004284
Iteration 14/1000 | Loss: 0.00004267
Iteration 15/1000 | Loss: 0.00004247
Iteration 16/1000 | Loss: 0.00004231
Iteration 17/1000 | Loss: 0.00004223
Iteration 18/1000 | Loss: 0.00004215
Iteration 19/1000 | Loss: 0.00004199
Iteration 20/1000 | Loss: 0.00004197
Iteration 21/1000 | Loss: 0.00004197
Iteration 22/1000 | Loss: 0.00004190
Iteration 23/1000 | Loss: 0.00004187
Iteration 24/1000 | Loss: 0.00004185
Iteration 25/1000 | Loss: 0.00004176
Iteration 26/1000 | Loss: 0.00004176
Iteration 27/1000 | Loss: 0.00004161
Iteration 28/1000 | Loss: 0.00004147
Iteration 29/1000 | Loss: 0.00004136
Iteration 30/1000 | Loss: 0.00004135
Iteration 31/1000 | Loss: 0.00004130
Iteration 32/1000 | Loss: 0.00004130
Iteration 33/1000 | Loss: 0.00004126
Iteration 34/1000 | Loss: 0.00004126
Iteration 35/1000 | Loss: 0.00004122
Iteration 36/1000 | Loss: 0.00004120
Iteration 37/1000 | Loss: 0.00004119
Iteration 38/1000 | Loss: 0.00004119
Iteration 39/1000 | Loss: 0.00004118
Iteration 40/1000 | Loss: 0.00004118
Iteration 41/1000 | Loss: 0.00004117
Iteration 42/1000 | Loss: 0.00004116
Iteration 43/1000 | Loss: 0.00004115
Iteration 44/1000 | Loss: 0.00004115
Iteration 45/1000 | Loss: 0.00004114
Iteration 46/1000 | Loss: 0.00004113
Iteration 47/1000 | Loss: 0.00004110
Iteration 48/1000 | Loss: 0.00004110
Iteration 49/1000 | Loss: 0.00004109
Iteration 50/1000 | Loss: 0.00004107
Iteration 51/1000 | Loss: 0.00004107
Iteration 52/1000 | Loss: 0.00004106
Iteration 53/1000 | Loss: 0.00004106
Iteration 54/1000 | Loss: 0.00004106
Iteration 55/1000 | Loss: 0.00004106
Iteration 56/1000 | Loss: 0.00004106
Iteration 57/1000 | Loss: 0.00004106
Iteration 58/1000 | Loss: 0.00004105
Iteration 59/1000 | Loss: 0.00004104
Iteration 60/1000 | Loss: 0.00004103
Iteration 61/1000 | Loss: 0.00004101
Iteration 62/1000 | Loss: 0.00004101
Iteration 63/1000 | Loss: 0.00004101
Iteration 64/1000 | Loss: 0.00004100
Iteration 65/1000 | Loss: 0.00004100
Iteration 66/1000 | Loss: 0.00004100
Iteration 67/1000 | Loss: 0.00004100
Iteration 68/1000 | Loss: 0.00004100
Iteration 69/1000 | Loss: 0.00004100
Iteration 70/1000 | Loss: 0.00004100
Iteration 71/1000 | Loss: 0.00004100
Iteration 72/1000 | Loss: 0.00004100
Iteration 73/1000 | Loss: 0.00004098
Iteration 74/1000 | Loss: 0.00004098
Iteration 75/1000 | Loss: 0.00004098
Iteration 76/1000 | Loss: 0.00004098
Iteration 77/1000 | Loss: 0.00004098
Iteration 78/1000 | Loss: 0.00004098
Iteration 79/1000 | Loss: 0.00004097
Iteration 80/1000 | Loss: 0.00004097
Iteration 81/1000 | Loss: 0.00004097
Iteration 82/1000 | Loss: 0.00004097
Iteration 83/1000 | Loss: 0.00004097
Iteration 84/1000 | Loss: 0.00004097
Iteration 85/1000 | Loss: 0.00004097
Iteration 86/1000 | Loss: 0.00004097
Iteration 87/1000 | Loss: 0.00004097
Iteration 88/1000 | Loss: 0.00004097
Iteration 89/1000 | Loss: 0.00004096
Iteration 90/1000 | Loss: 0.00004096
Iteration 91/1000 | Loss: 0.00004095
Iteration 92/1000 | Loss: 0.00004094
Iteration 93/1000 | Loss: 0.00004094
Iteration 94/1000 | Loss: 0.00004094
Iteration 95/1000 | Loss: 0.00004094
Iteration 96/1000 | Loss: 0.00004094
Iteration 97/1000 | Loss: 0.00004093
Iteration 98/1000 | Loss: 0.00004093
Iteration 99/1000 | Loss: 0.00004093
Iteration 100/1000 | Loss: 0.00004093
Iteration 101/1000 | Loss: 0.00004093
Iteration 102/1000 | Loss: 0.00004092
Iteration 103/1000 | Loss: 0.00004092
Iteration 104/1000 | Loss: 0.00004091
Iteration 105/1000 | Loss: 0.00004091
Iteration 106/1000 | Loss: 0.00004091
Iteration 107/1000 | Loss: 0.00004091
Iteration 108/1000 | Loss: 0.00004090
Iteration 109/1000 | Loss: 0.00004090
Iteration 110/1000 | Loss: 0.00004090
Iteration 111/1000 | Loss: 0.00004090
Iteration 112/1000 | Loss: 0.00004090
Iteration 113/1000 | Loss: 0.00004089
Iteration 114/1000 | Loss: 0.00004089
Iteration 115/1000 | Loss: 0.00004089
Iteration 116/1000 | Loss: 0.00004089
Iteration 117/1000 | Loss: 0.00004089
Iteration 118/1000 | Loss: 0.00004089
Iteration 119/1000 | Loss: 0.00004089
Iteration 120/1000 | Loss: 0.00004089
Iteration 121/1000 | Loss: 0.00004089
Iteration 122/1000 | Loss: 0.00004089
Iteration 123/1000 | Loss: 0.00004089
Iteration 124/1000 | Loss: 0.00004089
Iteration 125/1000 | Loss: 0.00004089
Iteration 126/1000 | Loss: 0.00004089
Iteration 127/1000 | Loss: 0.00004089
Iteration 128/1000 | Loss: 0.00004089
Iteration 129/1000 | Loss: 0.00004089
Iteration 130/1000 | Loss: 0.00004089
Iteration 131/1000 | Loss: 0.00004089
Iteration 132/1000 | Loss: 0.00004089
Iteration 133/1000 | Loss: 0.00004089
Iteration 134/1000 | Loss: 0.00004089
Iteration 135/1000 | Loss: 0.00004089
Iteration 136/1000 | Loss: 0.00004089
Iteration 137/1000 | Loss: 0.00004089
Iteration 138/1000 | Loss: 0.00004089
Iteration 139/1000 | Loss: 0.00004089
Iteration 140/1000 | Loss: 0.00004089
Iteration 141/1000 | Loss: 0.00004089
Iteration 142/1000 | Loss: 0.00004089
Iteration 143/1000 | Loss: 0.00004089
Iteration 144/1000 | Loss: 0.00004089
Iteration 145/1000 | Loss: 0.00004089
Iteration 146/1000 | Loss: 0.00004089
Iteration 147/1000 | Loss: 0.00004089
Iteration 148/1000 | Loss: 0.00004089
Iteration 149/1000 | Loss: 0.00004089
Iteration 150/1000 | Loss: 0.00004089
Iteration 151/1000 | Loss: 0.00004089
Iteration 152/1000 | Loss: 0.00004089
Iteration 153/1000 | Loss: 0.00004089
Iteration 154/1000 | Loss: 0.00004089
Iteration 155/1000 | Loss: 0.00004089
Iteration 156/1000 | Loss: 0.00004089
Iteration 157/1000 | Loss: 0.00004089
Iteration 158/1000 | Loss: 0.00004089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [4.088923378731124e-05, 4.088923378731124e-05, 4.088923378731124e-05, 4.088923378731124e-05, 4.088923378731124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.088923378731124e-05

Optimization complete. Final v2v error: 5.122690677642822 mm

Highest mean error: 7.084239959716797 mm for frame 75

Lowest mean error: 3.583916425704956 mm for frame 24

Saving results

Total time: 80.49022817611694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855265
Iteration 2/25 | Loss: 0.00125040
Iteration 3/25 | Loss: 0.00102928
Iteration 4/25 | Loss: 0.00099993
Iteration 5/25 | Loss: 0.00099015
Iteration 6/25 | Loss: 0.00098708
Iteration 7/25 | Loss: 0.00098623
Iteration 8/25 | Loss: 0.00098623
Iteration 9/25 | Loss: 0.00098623
Iteration 10/25 | Loss: 0.00098623
Iteration 11/25 | Loss: 0.00098623
Iteration 12/25 | Loss: 0.00098623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009862290462478995, 0.0009862290462478995, 0.0009862290462478995, 0.0009862290462478995, 0.0009862290462478995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009862290462478995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40080273
Iteration 2/25 | Loss: 0.00045350
Iteration 3/25 | Loss: 0.00045345
Iteration 4/25 | Loss: 0.00045345
Iteration 5/25 | Loss: 0.00045345
Iteration 6/25 | Loss: 0.00045345
Iteration 7/25 | Loss: 0.00045345
Iteration 8/25 | Loss: 0.00045345
Iteration 9/25 | Loss: 0.00045345
Iteration 10/25 | Loss: 0.00045345
Iteration 11/25 | Loss: 0.00045344
Iteration 12/25 | Loss: 0.00045344
Iteration 13/25 | Loss: 0.00045344
Iteration 14/25 | Loss: 0.00045344
Iteration 15/25 | Loss: 0.00045344
Iteration 16/25 | Loss: 0.00045344
Iteration 17/25 | Loss: 0.00045344
Iteration 18/25 | Loss: 0.00045344
Iteration 19/25 | Loss: 0.00045344
Iteration 20/25 | Loss: 0.00045344
Iteration 21/25 | Loss: 0.00045344
Iteration 22/25 | Loss: 0.00045344
Iteration 23/25 | Loss: 0.00045344
Iteration 24/25 | Loss: 0.00045344
Iteration 25/25 | Loss: 0.00045344

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01090503
Iteration 2/25 | Loss: 0.00282456
Iteration 3/25 | Loss: 0.00148355
Iteration 4/25 | Loss: 0.00143888
Iteration 5/25 | Loss: 0.00119705
Iteration 6/25 | Loss: 0.00106028
Iteration 7/25 | Loss: 0.00117007
Iteration 8/25 | Loss: 0.00124185
Iteration 9/25 | Loss: 0.00115782
Iteration 10/25 | Loss: 0.00100672
Iteration 11/25 | Loss: 0.00090452
Iteration 12/25 | Loss: 0.00084559
Iteration 13/25 | Loss: 0.00077929
Iteration 14/25 | Loss: 0.00076291
Iteration 15/25 | Loss: 0.00075419
Iteration 16/25 | Loss: 0.00072801
Iteration 17/25 | Loss: 0.00072061
Iteration 18/25 | Loss: 0.00071400
Iteration 19/25 | Loss: 0.00070752
Iteration 20/25 | Loss: 0.00070015
Iteration 21/25 | Loss: 0.00068423
Iteration 22/25 | Loss: 0.00067511
Iteration 23/25 | Loss: 0.00066975
Iteration 24/25 | Loss: 0.00066730
Iteration 25/25 | Loss: 0.00065683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44922352
Iteration 2/25 | Loss: 0.00164815
Iteration 3/25 | Loss: 0.00100151
Iteration 4/25 | Loss: 0.00100151
Iteration 5/25 | Loss: 0.00100151
Iteration 6/25 | Loss: 0.00100151
Iteration 7/25 | Loss: 0.00100151
Iteration 8/25 | Loss: 0.00100151
Iteration 9/25 | Loss: 0.00100151
Iteration 10/25 | Loss: 0.00100151
Iteration 11/25 | Loss: 0.00100151
Iteration 12/25 | Loss: 0.00100151
Iteration 13/25 | Loss: 0.00100151
Iteration 14/25 | Loss: 0.00100151
Iteration 15/25 | Loss: 0.00100151
Iteration 16/25 | Loss: 0.00100151
Iteration 17/25 | Loss: 0.00100151
Iteration 18/25 | Loss: 0.00100151
Iteration 19/25 | Loss: 0.00100151
Iteration 20/25 | Loss: 0.00100151
Iteration 21/25 | Loss: 0.00100151
Iteration 22/25 | Loss: 0.00100151
Iteration 23/25 | Loss: 0.00100151
Iteration 24/25 | Loss: 0.00100151
Iteration 25/25 | Loss: 0.00100151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045344
Iteration 2/1000 | Loss: 0.00003387
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001992
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001941
Iteration 9/1000 | Loss: 0.00001916
Iteration 10/1000 | Loss: 0.00001914
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001906
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001889
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001885
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001880
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001877
Iteration 41/1000 | Loss: 0.00001877
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001872
Iteration 55/1000 | Loss: 0.00001872
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001869
Iteration 65/1000 | Loss: 0.00001869
Iteration 66/1000 | Loss: 0.00001869
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001869
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001868
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001868
Iteration 92/1000 | Loss: 0.00001868
Iteration 93/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.867750324890949e-05, 1.867750324890949e-05, 1.867750324890949e-05, 1.867750324890949e-05, 1.867750324890949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.867750324890949e-05

Optimization complete. Final v2v error: 3.7108047008514404 mm

Highest mean error: 4.003905296325684 mm for frame 47

Lowest mean error: 3.5703940391540527 mm for frame 123

Saving results

Total time: 32.48221397399902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090503
Iteration 2/25 | Loss: 0.00234263
Iteration 3/25 | Loss: 0.00152084
Iteration 4/25 | Loss: 0.00137215
Iteration 5/25 | Loss: 0.00098944
Iteration 6/25 | Loss: 0.00095717
Iteration 7/25 | Loss: 0.00102778
Iteration 8/25 | Loss: 0.00105294
Iteration 9/25 | Loss: 0.00092279
Iteration 10/25 | Loss: 0.00086042
Iteration 11/25 | Loss: 0.00081452
Iteration 12/25 | Loss: 0.00077526
Iteration 13/25 | Loss: 0.00074871
Iteration 14/25 | Loss: 0.00071943
Iteration 15/25 | Loss: 0.00071341
Iteration 16/25 | Loss: 0.00068907
Iteration 17/25 | Loss: 0.00067590
Iteration 18/25 | Loss: 0.00068058
Iteration 19/25 | Loss: 0.00065436
Iteration 20/25 | Loss: 0.00065711
Iteration 21/25 | Loss: 0.00066349
Iteration 22/25 | Loss: 0.00065211
Iteration 23/25 | Loss: 0.00065475
Iteration 24/25 | Loss: 0.00065570
Iteration 25/25 | Loss: 0.00064712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44859064
Iteration 2/25 | Loss: 0.00089515
Iteration 3/25 | Loss: 0.00047922
Iteration 4/25 | Loss: 0.00047922
Iteration 5/25 | Loss: 0.00047922
Iteration 6/25 | Loss: 0.00047922
Iteration 7/25 | Loss: 0.00047922
Iteration 8/25 | Loss: 0.00047922
Iteration 9/25 | Loss: 0.00047922
Iteration 10/25 | Loss: 0.00047921
Iteration 11/25 | Loss: 0.00047921
Iteration 12/25 | Loss: 0.00047921
Iteration 13/25 | Loss: 0.00047921
Iteration 14/25 | Loss: 0.00047921
Iteration 15/25 | Loss: 0.00047921
Iteration 16/25 | Loss: 0.00047921
Iteration 17/25 | Loss: 0.00047921
Iteration 18/25 | Loss: 0.00047921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00047921453369781375, 0.00047921453369781375, 0.00047921453369781375, 0.00047921453369781375, 0.00047921453369781375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047921453369781375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145769
Iteration 2/1000 | Loss: 0.00128594
Iteration 3/1000 | Loss: 0.00068528
Iteration 4/1000 | Loss: 0.00093893
Iteration 5/1000 | Loss: 0.00121845
Iteration 6/1000 | Loss: 0.00159531
Iteration 7/1000 | Loss: 0.00109384
Iteration 8/1000 | Loss: 0.00139639
Iteration 9/1000 | Loss: 0.00099718
Iteration 10/1000 | Loss: 0.00072507
Iteration 11/1000 | Loss: 0.00077881
Iteration 12/1000 | Loss: 0.00112810
Iteration 13/1000 | Loss: 0.00168761
Iteration 14/1000 | Loss: 0.00088185
Iteration 15/1000 | Loss: 0.00041398
Iteration 16/1000 | Loss: 0.00054405
Iteration 17/1000 | Loss: 0.00038342
Iteration 18/1000 | Loss: 0.00048545
Iteration 19/1000 | Loss: 0.00115561
Iteration 20/1000 | Loss: 0.00115326
Iteration 21/1000 | Loss: 0.00094751
Iteration 22/1000 | Loss: 0.00067927
Iteration 23/1000 | Loss: 0.00127302
Iteration 24/1000 | Loss: 0.00112131
Iteration 25/1000 | Loss: 0.00048591
Iteration 26/1000 | Loss: 0.00139168
Iteration 27/1000 | Loss: 0.00131533
Iteration 28/1000 | Loss: 0.00142855
Iteration 29/1000 | Loss: 0.00092598
Iteration 30/1000 | Loss: 0.00075812
Iteration 31/1000 | Loss: 0.00074855
Iteration 32/1000 | Loss: 0.00097351
Iteration 33/1000 | Loss: 0.00045709
Iteration 34/1000 | Loss: 0.00031083
Iteration 35/1000 | Loss: 0.00045794
Iteration 36/1000 | Loss: 0.00034837
Iteration 37/1000 | Loss: 0.00040847
Iteration 38/1000 | Loss: 0.00045238
Iteration 39/1000 | Loss: 0.00082816
Iteration 40/1000 | Loss: 0.00106851
Iteration 41/1000 | Loss: 0.00037524
Iteration 42/1000 | Loss: 0.00058273
Iteration 43/1000 | Loss: 0.00035821
Iteration 44/1000 | Loss: 0.00017860
Iteration 45/1000 | Loss: 0.00027991
Iteration 46/1000 | Loss: 0.00013767
Iteration 47/1000 | Loss: 0.00030030
Iteration 48/1000 | Loss: 0.00030919
Iteration 49/1000 | Loss: 0.00045347
Iteration 50/1000 | Loss: 0.00075193
Iteration 51/1000 | Loss: 0.00033150
Iteration 52/1000 | Loss: 0.00024649
Iteration 53/1000 | Loss: 0.00011341
Iteration 54/1000 | Loss: 0.00022599
Iteration 55/1000 | Loss: 0.00022485
Iteration 56/1000 | Loss: 0.00010816
Iteration 57/1000 | Loss: 0.00036617
Iteration 58/1000 | Loss: 0.00030739
Iteration 59/1000 | Loss: 0.00025497
Iteration 60/1000 | Loss: 0.00085961
Iteration 61/1000 | Loss: 0.00041875
Iteration 62/1000 | Loss: 0.00029093
Iteration 63/1000 | Loss: 0.00032619
Iteration 64/1000 | Loss: 0.00029700
Iteration 65/1000 | Loss: 0.00014193
Iteration 66/1000 | Loss: 0.00036404
Iteration 67/1000 | Loss: 0.00043953
Iteration 68/1000 | Loss: 0.00045237
Iteration 69/1000 | Loss: 0.00020880
Iteration 70/1000 | Loss: 0.00050839
Iteration 71/1000 | Loss: 0.00057523
Iteration 72/1000 | Loss: 0.00070548
Iteration 73/1000 | Loss: 0.00052231
Iteration 74/1000 | Loss: 0.00018690
Iteration 75/1000 | Loss: 0.00045822
Iteration 76/1000 | Loss: 0.00095910
Iteration 77/1000 | Loss: 0.00020153
Iteration 78/1000 | Loss: 0.00031536
Iteration 79/1000 | Loss: 0.00020695
Iteration 80/1000 | Loss: 0.00070656
Iteration 81/1000 | Loss: 0.00020911
Iteration 82/1000 | Loss: 0.00088435
Iteration 83/1000 | Loss: 0.00029079
Iteration 84/1000 | Loss: 0.00060765
Iteration 85/1000 | Loss: 0.00091343
Iteration 86/1000 | Loss: 0.00070941
Iteration 87/1000 | Loss: 0.00082514
Iteration 88/1000 | Loss: 0.00076721
Iteration 89/1000 | Loss: 0.00012210
Iteration 90/1000 | Loss: 0.00012933
Iteration 91/1000 | Loss: 0.00025427
Iteration 92/1000 | Loss: 0.00088395
Iteration 93/1000 | Loss: 0.00047890
Iteration 94/1000 | Loss: 0.00075035
Iteration 95/1000 | Loss: 0.00081200
Iteration 96/1000 | Loss: 0.00082197
Iteration 97/1000 | Loss: 0.00059046
Iteration 98/1000 | Loss: 0.00071358
Iteration 99/1000 | Loss: 0.00084701
Iteration 100/1000 | Loss: 0.00076370
Iteration 101/1000 | Loss: 0.00080255
Iteration 102/1000 | Loss: 0.00069509
Iteration 103/1000 | Loss: 0.00030299
Iteration 104/1000 | Loss: 0.00074039
Iteration 105/1000 | Loss: 0.00034866
Iteration 106/1000 | Loss: 0.00028864
Iteration 107/1000 | Loss: 0.00010934
Iteration 108/1000 | Loss: 0.00034345
Iteration 109/1000 | Loss: 0.00010739
Iteration 110/1000 | Loss: 0.00014922
Iteration 111/1000 | Loss: 0.00011943
Iteration 112/1000 | Loss: 0.00013278
Iteration 113/1000 | Loss: 0.00010882
Iteration 114/1000 | Loss: 0.00012570
Iteration 115/1000 | Loss: 0.00010300
Iteration 116/1000 | Loss: 0.00011384
Iteration 117/1000 | Loss: 0.00010131
Iteration 118/1000 | Loss: 0.00011610
Iteration 119/1000 | Loss: 0.00011010
Iteration 120/1000 | Loss: 0.00012558
Iteration 121/1000 | Loss: 0.00010920
Iteration 122/1000 | Loss: 0.00012247
Iteration 123/1000 | Loss: 0.00010828
Iteration 124/1000 | Loss: 0.00010829
Iteration 125/1000 | Loss: 0.00011564
Iteration 126/1000 | Loss: 0.00010043
Iteration 127/1000 | Loss: 0.00010132
Iteration 128/1000 | Loss: 0.00009474
Iteration 129/1000 | Loss: 0.00010036
Iteration 130/1000 | Loss: 0.00031300
Iteration 131/1000 | Loss: 0.00030670
Iteration 132/1000 | Loss: 0.00013782
Iteration 133/1000 | Loss: 0.00004524
Iteration 134/1000 | Loss: 0.00011667
Iteration 135/1000 | Loss: 0.00019673
Iteration 136/1000 | Loss: 0.00007281
Iteration 137/1000 | Loss: 0.00014637
Iteration 138/1000 | Loss: 0.00008159
Iteration 139/1000 | Loss: 0.00016219
Iteration 140/1000 | Loss: 0.00002569
Iteration 141/1000 | Loss: 0.00002285
Iteration 142/1000 | Loss: 0.00004057
Iteration 143/1000 | Loss: 0.00003030
Iteration 144/1000 | Loss: 0.00003253
Iteration 145/1000 | Loss: 0.00001940
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001800
Iteration 148/1000 | Loss: 0.00001767
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001708
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001689
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001682
Iteration 156/1000 | Loss: 0.00001680
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001675
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001665
Iteration 161/1000 | Loss: 0.00001665
Iteration 162/1000 | Loss: 0.00001660
Iteration 163/1000 | Loss: 0.00001659
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001658
Iteration 166/1000 | Loss: 0.00001657
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001652
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001650
Iteration 171/1000 | Loss: 0.00001649
Iteration 172/1000 | Loss: 0.00001649
Iteration 173/1000 | Loss: 0.00001649
Iteration 174/1000 | Loss: 0.00001649
Iteration 175/1000 | Loss: 0.00001649
Iteration 176/1000 | Loss: 0.00001649
Iteration 177/1000 | Loss: 0.00001648
Iteration 178/1000 | Loss: 0.00001648
Iteration 179/1000 | Loss: 0.00001648
Iteration 180/1000 | Loss: 0.00001648
Iteration 181/1000 | Loss: 0.00001648
Iteration 182/1000 | Loss: 0.00001648
Iteration 183/1000 | Loss: 0.00001648
Iteration 184/1000 | Loss: 0.00001648
Iteration 185/1000 | Loss: 0.00022767
Iteration 186/1000 | Loss: 0.00002314
Iteration 187/1000 | Loss: 0.00002053
Iteration 188/1000 | Loss: 0.00001871
Iteration 189/1000 | Loss: 0.00001802
Iteration 190/1000 | Loss: 0.00001781
Iteration 191/1000 | Loss: 0.00001757
Iteration 192/1000 | Loss: 0.00001736
Iteration 193/1000 | Loss: 0.00001709
Iteration 194/1000 | Loss: 0.00001692
Iteration 195/1000 | Loss: 0.00022582
Iteration 196/1000 | Loss: 0.00022582
Iteration 197/1000 | Loss: 0.00003075
Iteration 198/1000 | Loss: 0.00002307
Iteration 199/1000 | Loss: 0.00002097
Iteration 200/1000 | Loss: 0.00001938
Iteration 201/1000 | Loss: 0.00001849
Iteration 202/1000 | Loss: 0.00001819
Iteration 203/1000 | Loss: 0.00001777
Iteration 204/1000 | Loss: 0.00001748
Iteration 205/1000 | Loss: 0.00001725
Iteration 206/1000 | Loss: 0.00001714
Iteration 207/1000 | Loss: 0.00021990
Iteration 208/1000 | Loss: 0.00002182
Iteration 209/1000 | Loss: 0.00001983
Iteration 210/1000 | Loss: 0.00001866
Iteration 211/1000 | Loss: 0.00001839
Iteration 212/1000 | Loss: 0.00001807
Iteration 213/1000 | Loss: 0.00022439
Iteration 214/1000 | Loss: 0.00002389
Iteration 215/1000 | Loss: 0.00002154
Iteration 216/1000 | Loss: 0.00001934
Iteration 217/1000 | Loss: 0.00001867
Iteration 218/1000 | Loss: 0.00001837
Iteration 219/1000 | Loss: 0.00001799
Iteration 220/1000 | Loss: 0.00001762
Iteration 221/1000 | Loss: 0.00001734
Iteration 222/1000 | Loss: 0.00001713
Iteration 223/1000 | Loss: 0.00001711
Iteration 224/1000 | Loss: 0.00001703
Iteration 225/1000 | Loss: 0.00001702
Iteration 226/1000 | Loss: 0.00001699
Iteration 227/1000 | Loss: 0.00001695
Iteration 228/1000 | Loss: 0.00001694
Iteration 229/1000 | Loss: 0.00001692
Iteration 230/1000 | Loss: 0.00001691
Iteration 231/1000 | Loss: 0.00001691
Iteration 232/1000 | Loss: 0.00001691
Iteration 233/1000 | Loss: 0.00001689
Iteration 234/1000 | Loss: 0.00001685
Iteration 235/1000 | Loss: 0.00001685
Iteration 236/1000 | Loss: 0.00001684
Iteration 237/1000 | Loss: 0.00001682
Iteration 238/1000 | Loss: 0.00001682
Iteration 239/1000 | Loss: 0.00001680
Iteration 240/1000 | Loss: 0.00001679
Iteration 241/1000 | Loss: 0.00001679
Iteration 242/1000 | Loss: 0.00001679
Iteration 243/1000 | Loss: 0.00001678
Iteration 244/1000 | Loss: 0.00001672
Iteration 245/1000 | Loss: 0.00001669
Iteration 246/1000 | Loss: 0.00001669
Iteration 247/1000 | Loss: 0.00001669
Iteration 248/1000 | Loss: 0.00001669
Iteration 249/1000 | Loss: 0.00001669
Iteration 250/1000 | Loss: 0.00001666
Iteration 251/1000 | Loss: 0.00001663
Iteration 252/1000 | Loss: 0.00001662
Iteration 253/1000 | Loss: 0.00001662
Iteration 254/1000 | Loss: 0.00001662
Iteration 255/1000 | Loss: 0.00001662
Iteration 256/1000 | Loss: 0.00001662
Iteration 257/1000 | Loss: 0.00001662
Iteration 258/1000 | Loss: 0.00001662
Iteration 259/1000 | Loss: 0.00001662
Iteration 260/1000 | Loss: 0.00001662
Iteration 261/1000 | Loss: 0.00001662
Iteration 262/1000 | Loss: 0.00001662
Iteration 263/1000 | Loss: 0.00001662
Iteration 264/1000 | Loss: 0.00001661
Iteration 265/1000 | Loss: 0.00001661
Iteration 266/1000 | Loss: 0.00001661
Iteration 267/1000 | Loss: 0.00001661
Iteration 268/1000 | Loss: 0.00001661
Iteration 269/1000 | Loss: 0.00001661
Iteration 270/1000 | Loss: 0.00001661
Iteration 271/1000 | Loss: 0.00001661
Iteration 272/1000 | Loss: 0.00001659
Iteration 273/1000 | Loss: 0.00001659
Iteration 274/1000 | Loss: 0.00001658
Iteration 275/1000 | Loss: 0.00001658
Iteration 276/1000 | Loss: 0.00001655
Iteration 277/1000 | Loss: 0.00001654
Iteration 278/1000 | Loss: 0.00001653
Iteration 279/1000 | Loss: 0.00001653
Iteration 280/1000 | Loss: 0.00001652
Iteration 281/1000 | Loss: 0.00001651
Iteration 282/1000 | Loss: 0.00001651
Iteration 283/1000 | Loss: 0.00001651
Iteration 284/1000 | Loss: 0.00022343
Iteration 285/1000 | Loss: 0.00041027
Iteration 286/1000 | Loss: 0.00019984
Iteration 287/1000 | Loss: 0.00009201
Iteration 288/1000 | Loss: 0.00002639
Iteration 289/1000 | Loss: 0.00002093
Iteration 290/1000 | Loss: 0.00001843
Iteration 291/1000 | Loss: 0.00001706
Iteration 292/1000 | Loss: 0.00001633
Iteration 293/1000 | Loss: 0.00001583
Iteration 294/1000 | Loss: 0.00001539
Iteration 295/1000 | Loss: 0.00001508
Iteration 296/1000 | Loss: 0.00001488
Iteration 297/1000 | Loss: 0.00001471
Iteration 298/1000 | Loss: 0.00001465
Iteration 299/1000 | Loss: 0.00001463
Iteration 300/1000 | Loss: 0.00001460
Iteration 301/1000 | Loss: 0.00001456
Iteration 302/1000 | Loss: 0.00001448
Iteration 303/1000 | Loss: 0.00001445
Iteration 304/1000 | Loss: 0.00001444
Iteration 305/1000 | Loss: 0.00001443
Iteration 306/1000 | Loss: 0.00001443
Iteration 307/1000 | Loss: 0.00001443
Iteration 308/1000 | Loss: 0.00001443
Iteration 309/1000 | Loss: 0.00001442
Iteration 310/1000 | Loss: 0.00001442
Iteration 311/1000 | Loss: 0.00001440
Iteration 312/1000 | Loss: 0.00001440
Iteration 313/1000 | Loss: 0.00001440
Iteration 314/1000 | Loss: 0.00001440
Iteration 315/1000 | Loss: 0.00001440
Iteration 316/1000 | Loss: 0.00001440
Iteration 317/1000 | Loss: 0.00001439
Iteration 318/1000 | Loss: 0.00001438
Iteration 319/1000 | Loss: 0.00001437
Iteration 320/1000 | Loss: 0.00001437
Iteration 321/1000 | Loss: 0.00001436
Iteration 322/1000 | Loss: 0.00001436
Iteration 323/1000 | Loss: 0.00001435
Iteration 324/1000 | Loss: 0.00001435
Iteration 325/1000 | Loss: 0.00001435
Iteration 326/1000 | Loss: 0.00001435
Iteration 327/1000 | Loss: 0.00001435
Iteration 328/1000 | Loss: 0.00001435
Iteration 329/1000 | Loss: 0.00001435
Iteration 330/1000 | Loss: 0.00001435
Iteration 331/1000 | Loss: 0.00001435
Iteration 332/1000 | Loss: 0.00001435
Iteration 333/1000 | Loss: 0.00001435
Iteration 334/1000 | Loss: 0.00001434
Iteration 335/1000 | Loss: 0.00001434
Iteration 336/1000 | Loss: 0.00001434
Iteration 337/1000 | Loss: 0.00001434
Iteration 338/1000 | Loss: 0.00001434
Iteration 339/1000 | Loss: 0.00001434
Iteration 340/1000 | Loss: 0.00001434
Iteration 341/1000 | Loss: 0.00001434
Iteration 342/1000 | Loss: 0.00001434
Iteration 343/1000 | Loss: 0.00001433
Iteration 344/1000 | Loss: 0.00001432
Iteration 345/1000 | Loss: 0.00001432
Iteration 346/1000 | Loss: 0.00001432
Iteration 347/1000 | Loss: 0.00001432
Iteration 348/1000 | Loss: 0.00001432
Iteration 349/1000 | Loss: 0.00001432
Iteration 350/1000 | Loss: 0.00001431
Iteration 351/1000 | Loss: 0.00001431
Iteration 352/1000 | Loss: 0.00001431
Iteration 353/1000 | Loss: 0.00001431
Iteration 354/1000 | Loss: 0.00001431
Iteration 355/1000 | Loss: 0.00001431
Iteration 356/1000 | Loss: 0.00001431
Iteration 357/1000 | Loss: 0.00001431
Iteration 358/1000 | Loss: 0.00001431
Iteration 359/1000 | Loss: 0.00001431
Iteration 360/1000 | Loss: 0.00001431
Iteration 361/1000 | Loss: 0.00001431
Iteration 362/1000 | Loss: 0.00001430
Iteration 363/1000 | Loss: 0.00001430
Iteration 364/1000 | Loss: 0.00001429
Iteration 365/1000 | Loss: 0.00001429
Iteration 366/1000 | Loss: 0.00001429
Iteration 367/1000 | Loss: 0.00001429
Iteration 368/1000 | Loss: 0.00001429
Iteration 369/1000 | Loss: 0.00001429
Iteration 370/1000 | Loss: 0.00001428
Iteration 371/1000 | Loss: 0.00001428
Iteration 372/1000 | Loss: 0.00001428
Iteration 373/1000 | Loss: 0.00001428
Iteration 374/1000 | Loss: 0.00001428
Iteration 375/1000 | Loss: 0.00001428
Iteration 376/1000 | Loss: 0.00001428
Iteration 377/1000 | Loss: 0.00001428
Iteration 378/1000 | Loss: 0.00001427
Iteration 379/1000 | Loss: 0.00001427
Iteration 380/1000 | Loss: 0.00001427
Iteration 381/1000 | Loss: 0.00001427
Iteration 382/1000 | Loss: 0.00001427
Iteration 383/1000 | Loss: 0.00001427
Iteration 384/1000 | Loss: 0.00001427
Iteration 385/1000 | Loss: 0.00001427
Iteration 386/1000 | Loss: 0.00001427
Iteration 387/1000 | Loss: 0.00001426
Iteration 388/1000 | Loss: 0.00001426
Iteration 389/1000 | Loss: 0.00001426
Iteration 390/1000 | Loss: 0.00001426
Iteration 391/1000 | Loss: 0.00001426
Iteration 392/1000 | Loss: 0.00001426
Iteration 393/1000 | Loss: 0.00001426
Iteration 394/1000 | Loss: 0.00001426
Iteration 395/1000 | Loss: 0.00001426
Iteration 396/1000 | Loss: 0.00001426
Iteration 397/1000 | Loss: 0.00001426
Iteration 398/1000 | Loss: 0.00001426
Iteration 399/1000 | Loss: 0.00001426
Iteration 400/1000 | Loss: 0.00001426
Iteration 401/1000 | Loss: 0.00001426
Iteration 402/1000 | Loss: 0.00001426
Iteration 403/1000 | Loss: 0.00001426
Iteration 404/1000 | Loss: 0.00001426
Iteration 405/1000 | Loss: 0.00001426
Iteration 406/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 406. Stopping optimization.
Last 5 losses: [1.4258335795602761e-05, 1.4258335795602761e-05, 1.4258335795602761e-05, 1.4258335795602761e-05, 1.4258335795602761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4258335795602761e-05

Optimization complete. Final v2v error: 3.2085864543914795 mm

Highest mean error: 4.163824558258057 mm for frame 57

Lowest mean error: 3.0735743045806885 mm for frame 139

Saving results

Total time: 372.19154930114746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387705
Iteration 2/25 | Loss: 0.00100307
Iteration 3/25 | Loss: 0.00083550
Iteration 4/25 | Loss: 0.00080136
Iteration 5/25 | Loss: 0.00078942
Iteration 6/25 | Loss: 0.00078669
Iteration 7/25 | Loss: 0.00078585
Iteration 8/25 | Loss: 0.00078585
Iteration 9/25 | Loss: 0.00078585
Iteration 10/25 | Loss: 0.00078585
Iteration 11/25 | Loss: 0.00078585
Iteration 12/25 | Loss: 0.00078585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007858534227125347, 0.0007858534227125347, 0.0007858534227125347, 0.0007858534227125347, 0.0007858534227125347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007858534227125347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51549768
Iteration 2/25 | Loss: 0.00053149
Iteration 3/25 | Loss: 0.00053147
Iteration 4/25 | Loss: 0.00053147
Iteration 5/25 | Loss: 0.00053147
Iteration 6/25 | Loss: 0.00053147
Iteration 7/25 | Loss: 0.00053147
Iteration 8/25 | Loss: 0.00053147
Iteration 9/25 | Loss: 0.00053147
Iteration 10/25 | Loss: 0.00053147
Iteration 11/25 | Loss: 0.00053147
Iteration 12/25 | Loss: 0.00053147
Iteration 13/25 | Loss: 0.00053147
Iteration 14/25 | Loss: 0.00053147
Iteration 15/25 | Loss: 0.00053147
Iteration 16/25 | Loss: 0.00053147
Iteration 17/25 | Loss: 0.00053147
Iteration 18/25 | Loss: 0.00053147
Iteration 19/25 | Loss: 0.00053147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005314697627909482, 0.0005314697627909482, 0.0005314697627909482, 0.0005314697627909482, 0.0005314697627909482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005314697627909482

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01061290
Iteration 2/25 | Loss: 0.00406953
Iteration 3/25 | Loss: 0.00238036
Iteration 4/25 | Loss: 0.00201039
Iteration 5/25 | Loss: 0.00191838
Iteration 6/25 | Loss: 0.00173412
Iteration 7/25 | Loss: 0.00163464
Iteration 8/25 | Loss: 0.00155823
Iteration 9/25 | Loss: 0.00144275
Iteration 10/25 | Loss: 0.00133807
Iteration 11/25 | Loss: 0.00131596
Iteration 12/25 | Loss: 0.00129974
Iteration 13/25 | Loss: 0.00126198
Iteration 14/25 | Loss: 0.00124787
Iteration 15/25 | Loss: 0.00124227
Iteration 16/25 | Loss: 0.00123543
Iteration 17/25 | Loss: 0.00122863
Iteration 18/25 | Loss: 0.00122870
Iteration 19/25 | Loss: 0.00122223
Iteration 20/25 | Loss: 0.00121628
Iteration 21/25 | Loss: 0.00121282
Iteration 22/25 | Loss: 0.00120549
Iteration 23/25 | Loss: 0.00120725
Iteration 24/25 | Loss: 0.00120959
Iteration 25/25 | Loss: 0.00120378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46122575
Iteration 2/25 | Loss: 0.00405093
Iteration 3/25 | Loss: 0.00365075
Iteration 4/25 | Loss: 0.00365068
Iteration 5/25 | Loss: 0.00365068
Iteration 6/25 | Loss: 0.00365068
Iteration 7/25 | Loss: 0.00365068
Iteration 8/25 | Loss: 0.00365068
Iteration 9/25 | Loss: 0.00365068
Iteration 10/25 | Loss: 0.00365068
Iteration 11/25 | Loss: 0.00365068
Iteration 12/25 | Loss: 0.00365068
Iteration 13/25 | Loss: 0.00365068
Iteration 14/25 | Loss: 0.00365068
Iteration 15/25 | Loss: 0.00365068
Iteration 16/25 | Loss: 0.00365068
Iteration 17/25 | Loss: 0.00365068
Iteration 18/25 | Loss: 0.00365068
Iteration 19/25 | Loss: 0.00365068
Iteration 20/25 | Loss: 0.00365068
Iteration 21/25 | Loss: 0.00365068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003650681348517537, 0.003650681348517537, 0.003650681348517537, 0.003650681348517537, 0.003650681348517537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003650681348517537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053147
Iteration 2/1000 | Loss: 0.00004188
Iteration 3/1000 | Loss: 0.00002884
Iteration 4/1000 | Loss: 0.00002428
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002121
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001918
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001853
Iteration 13/1000 | Loss: 0.00001845
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00001807
Iteration 20/1000 | Loss: 0.00001806
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001804
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001802
Iteration 29/1000 | Loss: 0.00001802
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001800
Iteration 33/1000 | Loss: 0.00001800
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001798
Iteration 40/1000 | Loss: 0.00001797
Iteration 41/1000 | Loss: 0.00001797
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001795
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001794
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001793
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001788
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001787
Iteration 78/1000 | Loss: 0.00001787
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.784441883501131e-05, 1.784441883501131e-05, 1.784441883501131e-05, 1.784441883501131e-05, 1.784441883501131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.784441883501131e-05

Optimization complete. Final v2v error: 3.428785562515259 mm

Highest mean error: 4.010130882263184 mm for frame 47

Lowest mean error: 2.8541786670684814 mm for frame 100

Saving results

Total time: 43.455503940582275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817245
Iteration 2/25 | Loss: 0.00180977
Iteration 3/25 | Loss: 0.00091757
Iteration 4/25 | Loss: 0.00078664
Iteration 5/25 | Loss: 0.00075942
Iteration 6/25 | Loss: 0.00074989
Iteration 7/25 | Loss: 0.00075977
Iteration 8/25 | Loss: 0.00075224
Iteration 9/25 | Loss: 0.00075080
Iteration 10/25 | Loss: 0.00074367
Iteration 11/25 | Loss: 0.00073959
Iteration 12/25 | Loss: 0.00073784
Iteration 13/25 | Loss: 0.00073688
Iteration 14/25 | Loss: 0.00073656
Iteration 15/25 | Loss: 0.00073646
Iteration 16/25 | Loss: 0.00073646
Iteration 17/25 | Loss: 0.00073645
Iteration 18/25 | Loss: 0.00073645
Iteration 19/25 | Loss: 0.00073645
Iteration 20/25 | Loss: 0.00073645
Iteration 21/25 | Loss: 0.00073645
Iteration 22/25 | Loss: 0.00073645
Iteration 23/25 | Loss: 0.00073645
Iteration 24/25 | Loss: 0.00073645
Iteration 25/25 | Loss: 0.00073645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04804540
Iteration 2/25 | Loss: 0.00051431
Iteration 3/25 | Loss: 0.00046796
Iteration 4/25 | Loss: 0.00046796
Iteration 5/25 | Loss: 0.00046796
Iteration 6/25 | Loss: 0.00046796
Iteration 7/25 | Loss: 0.00046796
Iteration 8/25 | Loss: 0.00046796
Iteration 9/25 | Loss: 0.00046796
Iteration 10/25 | Loss: 0.00046796
Iteration 11/25 | Loss: 0.00046796
Iteration 12/25 | Loss: 0.00046796
Iteration 13/25 | Loss: 0.00046796
Iteration 14/25 | Loss: 0.00046796
Iteration 15/25 | Loss: 0.00046796
Iteration 16/25 | Loss: 0.00046796
Iteration 17/25 | Loss: 0.00046796
Iteration 18/25 | Loss: 0.00046796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004679561825469136, 0.0004679561825469136, 0.0004679561825469136, 0.0004679561825469136, 0.0004679561825469136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004679561825469136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047921
Iteration 2/1000 | Loss: 0.00058332
Iteration 3/1000 | Loss: 0.00051840
Iteration 4/1000 | Loss: 0.00026587
Iteration 5/1000 | Loss: 0.00025242
Iteration 6/1000 | Loss: 0.00007444
Iteration 7/1000 | Loss: 0.00037680
Iteration 8/1000 | Loss: 0.00032059
Iteration 9/1000 | Loss: 0.00018385
Iteration 10/1000 | Loss: 0.00070409
Iteration 11/1000 | Loss: 0.00035675
Iteration 12/1000 | Loss: 0.00028229
Iteration 13/1000 | Loss: 0.00008272
Iteration 14/1000 | Loss: 0.00056581
Iteration 15/1000 | Loss: 0.00023057
Iteration 16/1000 | Loss: 0.00020441
Iteration 17/1000 | Loss: 0.00036098
Iteration 18/1000 | Loss: 0.00007265
Iteration 19/1000 | Loss: 0.00089889
Iteration 20/1000 | Loss: 0.00046857
Iteration 21/1000 | Loss: 0.00006615
Iteration 22/1000 | Loss: 0.00005115
Iteration 23/1000 | Loss: 0.00030022
Iteration 24/1000 | Loss: 0.00004803
Iteration 25/1000 | Loss: 0.00004090
Iteration 26/1000 | Loss: 0.00029194
Iteration 27/1000 | Loss: 0.00014329
Iteration 28/1000 | Loss: 0.00011884
Iteration 29/1000 | Loss: 0.00035828
Iteration 30/1000 | Loss: 0.00019220
Iteration 31/1000 | Loss: 0.00034567
Iteration 32/1000 | Loss: 0.00022316
Iteration 33/1000 | Loss: 0.00004945
Iteration 34/1000 | Loss: 0.00038821
Iteration 35/1000 | Loss: 0.00009640
Iteration 36/1000 | Loss: 0.00012892
Iteration 37/1000 | Loss: 0.00013316
Iteration 38/1000 | Loss: 0.00023601
Iteration 39/1000 | Loss: 0.00017159
Iteration 40/1000 | Loss: 0.00018297
Iteration 41/1000 | Loss: 0.00011540
Iteration 42/1000 | Loss: 0.00011020
Iteration 43/1000 | Loss: 0.00014110
Iteration 44/1000 | Loss: 0.00022922
Iteration 45/1000 | Loss: 0.00028199
Iteration 46/1000 | Loss: 0.00012158
Iteration 47/1000 | Loss: 0.00022734
Iteration 48/1000 | Loss: 0.00011428
Iteration 49/1000 | Loss: 0.00011751
Iteration 50/1000 | Loss: 0.00016200
Iteration 51/1000 | Loss: 0.00010981
Iteration 52/1000 | Loss: 0.00003859
Iteration 53/1000 | Loss: 0.00030485
Iteration 54/1000 | Loss: 0.00026091
Iteration 55/1000 | Loss: 0.00031888
Iteration 56/1000 | Loss: 0.00030716
Iteration 57/1000 | Loss: 0.00020563
Iteration 58/1000 | Loss: 0.00026935
Iteration 59/1000 | Loss: 0.00066120
Iteration 60/1000 | Loss: 0.00089064
Iteration 61/1000 | Loss: 0.00131484
Iteration 62/1000 | Loss: 0.00084089
Iteration 63/1000 | Loss: 0.00029182
Iteration 64/1000 | Loss: 0.00025142
Iteration 65/1000 | Loss: 0.00045380
Iteration 66/1000 | Loss: 0.00011883
Iteration 67/1000 | Loss: 0.00003674
Iteration 68/1000 | Loss: 0.00003120
Iteration 69/1000 | Loss: 0.00002560
Iteration 70/1000 | Loss: 0.00002248
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001898
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001754
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001716
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001703
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001700
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001700
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001699
Iteration 97/1000 | Loss: 0.00001699
Iteration 98/1000 | Loss: 0.00001699
Iteration 99/1000 | Loss: 0.00001699
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001697
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001697
Iteration 117/1000 | Loss: 0.00001697
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001695
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001695
Iteration 126/1000 | Loss: 0.00001695
Iteration 127/1000 | Loss: 0.00001695
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001694
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001694
Iteration 136/1000 | Loss: 0.00001694
Iteration 137/1000 | Loss: 0.00001694
Iteration 138/1000 | Loss: 0.00001694
Iteration 139/1000 | Loss: 0.00001694
Iteration 140/1000 | Loss: 0.00001694
Iteration 141/1000 | Loss: 0.00001694
Iteration 142/1000 | Loss: 0.00001693
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001693
Iteration 147/1000 | Loss: 0.00001693
Iteration 148/1000 | Loss: 0.00001693
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001693
Iteration 155/1000 | Loss: 0.00001693
Iteration 156/1000 | Loss: 0.00001692
Iteration 157/1000 | Loss: 0.00001692
Iteration 158/1000 | Loss: 0.00001692
Iteration 159/1000 | Loss: 0.00001692
Iteration 160/1000 | Loss: 0.00001692
Iteration 161/1000 | Loss: 0.00001692
Iteration 162/1000 | Loss: 0.00001692
Iteration 163/1000 | Loss: 0.00001692
Iteration 164/1000 | Loss: 0.00001692
Iteration 165/1000 | Loss: 0.00001692
Iteration 166/1000 | Loss: 0.00001692
Iteration 167/1000 | Loss: 0.00001692
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001692
Iteration 171/1000 | Loss: 0.00001691
Iteration 172/1000 | Loss: 0.00001691
Iteration 173/1000 | Loss: 0.00001691
Iteration 174/1000 | Loss: 0.00001691
Iteration 175/1000 | Loss: 0.00001691
Iteration 176/1000 | Loss: 0.00001691
Iteration 177/1000 | Loss: 0.00001691
Iteration 178/1000 | Loss: 0.00001691
Iteration 179/1000 | Loss: 0.00001691
Iteration 180/1000 | Loss: 0.00001691
Iteration 181/1000 | Loss: 0.00001691
Iteration 182/1000 | Loss: 0.00001691
Iteration 183/1000 | Loss: 0.00001691
Iteration 184/1000 | Loss: 0.00001691
Iteration 185/1000 | Loss: 0.00001690
Iteration 186/1000 | Loss: 0.00001690
Iteration 187/1000 | Loss: 0.00001690
Iteration 188/1000 | Loss: 0.00001690
Iteration 189/1000 | Loss: 0.00001690
Iteration 190/1000 | Loss: 0.00001690
Iteration 191/1000 | Loss: 0.00001690
Iteration 192/1000 | Loss: 0.00001690
Iteration 193/1000 | Loss: 0.00001689
Iteration 194/1000 | Loss: 0.00001689
Iteration 195/1000 | Loss: 0.00001689
Iteration 196/1000 | Loss: 0.00001689
Iteration 197/1000 | Loss: 0.00001689
Iteration 198/1000 | Loss: 0.00001689
Iteration 199/1000 | Loss: 0.00001689
Iteration 200/1000 | Loss: 0.00001689
Iteration 201/1000 | Loss: 0.00001689
Iteration 202/1000 | Loss: 0.00001689
Iteration 203/1000 | Loss: 0.00001689
Iteration 204/1000 | Loss: 0.00001689
Iteration 205/1000 | Loss: 0.00001689
Iteration 206/1000 | Loss: 0.00001689
Iteration 207/1000 | Loss: 0.00001689
Iteration 208/1000 | Loss: 0.00001689
Iteration 209/1000 | Loss: 0.00001689
Iteration 210/1000 | Loss: 0.00001689
Iteration 211/1000 | Loss: 0.00001689
Iteration 212/1000 | Loss: 0.00001689
Iteration 213/1000 | Loss: 0.00001689
Iteration 214/1000 | Loss: 0.00001689
Iteration 215/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.688546035438776e-05, 1.688546035438776e-05, 1.688546035438776e-05, 1.688546035438776e-05, 1.688546035438776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.688546035438776e-05

Optimization complete. Final v2v error: 3.443643569946289 mm

Highest mean error: 4.396946430206299 mm for frame 58

Lowest mean error: 2.910618305206299 mm for frame 6

Saving results

Total time: 161.2073619365692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00046796
Iteration 2/1000 | Loss: 0.00002487
Iteration 3/1000 | Loss: 0.00006617
Iteration 4/1000 | Loss: 0.00003018
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001648
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00007319
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00005159
Iteration 16/1000 | Loss: 0.00018481
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00004300
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001450
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001449
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001449
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001442
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001442
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001432
Iteration 55/1000 | Loss: 0.00001431
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001421
Iteration 78/1000 | Loss: 0.00001421
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001418
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001418
Iteration 93/1000 | Loss: 0.00001418
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001417
Iteration 99/1000 | Loss: 0.00001417
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001414
Iteration 104/1000 | Loss: 0.00001414
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001414
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001412
Iteration 116/1000 | Loss: 0.00001412
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001410
Iteration 130/1000 | Loss: 0.00001410
Iteration 131/1000 | Loss: 0.00001410
Iteration 132/1000 | Loss: 0.00001410
Iteration 133/1000 | Loss: 0.00001410
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001410
Iteration 136/1000 | Loss: 0.00001410
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001410
Iteration 142/1000 | Loss: 0.00001410
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001409
Iteration 146/1000 | Loss: 0.00001409
Iteration 147/1000 | Loss: 0.00001409
Iteration 148/1000 | Loss: 0.00001409
Iteration 149/1000 | Loss: 0.00001409
Iteration 150/1000 | Loss: 0.00001409
Iteration 151/1000 | Loss: 0.00001409
Iteration 152/1000 | Loss: 0.00001408
Iteration 153/1000 | Loss: 0.00001408
Iteration 154/1000 | Loss: 0.00001408
Iteration 155/1000 | Loss: 0.00001408
Iteration 156/1000 | Loss: 0.00001408
Iteration 157/1000 | Loss: 0.00001408
Iteration 158/1000 | Loss: 0.00001408
Iteration 159/1000 | Loss: 0.00001408
Iteration 160/1000 | Loss: 0.00001408
Iteration 161/1000 | Loss: 0.00001408
Iteration 162/1000 | Loss: 0.00001408
Iteration 163/1000 | Loss: 0.00001408
Iteration 164/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4077995729167014e-05, 1.4077995729167014e-05, 1.4077995729167014e-05, 1.4077995729167014e-05, 1.4077995729167014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4077995729167014e-05

Optimization complete. Final v2v error: 3.183701515197754 mm

Highest mean error: 3.5472068786621094 mm for frame 115

Lowest mean error: 2.9983785152435303 mm for frame 141

Saving results

Total time: 65.84132099151611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812242
Iteration 2/25 | Loss: 0.00110138
Iteration 3/25 | Loss: 0.00066534
Iteration 4/25 | Loss: 0.00058187
Iteration 5/25 | Loss: 0.00056776
Iteration 6/25 | Loss: 0.00056382
Iteration 7/25 | Loss: 0.00056234
Iteration 8/25 | Loss: 0.00056229
Iteration 9/25 | Loss: 0.00056229
Iteration 10/25 | Loss: 0.00056229
Iteration 11/25 | Loss: 0.00056229
Iteration 12/25 | Loss: 0.00056229
Iteration 13/25 | Loss: 0.00056229
Iteration 14/25 | Loss: 0.00056229
Iteration 15/25 | Loss: 0.00056229
Iteration 16/25 | Loss: 0.00056229
Iteration 17/25 | Loss: 0.00056229
Iteration 18/25 | Loss: 0.00056229
Iteration 19/25 | Loss: 0.00056229
Iteration 20/25 | Loss: 0.00056229
Iteration 21/25 | Loss: 0.00056229
Iteration 22/25 | Loss: 0.00056229
Iteration 23/25 | Loss: 0.00056229
Iteration 24/25 | Loss: 0.00056229
Iteration 25/25 | Loss: 0.00056229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38357508
Iteration 2/25 | Loss: 0.00010889
Iteration 3/25 | Loss: 0.00010889
Iteration 4/25 | Loss: 0.00010889
Iteration 5/25 | Loss: 0.00010889
Iteration 6/25 | Loss: 0.00010889
Iteration 7/25 | Loss: 0.00010889
Iteration 8/25 | Loss: 0.00010889
Iteration 9/25 | Loss: 0.00010889
Iteration 10/25 | Loss: 0.00010889
Iteration 11/25 | Loss: 0.00010889
Iteration 12/25 | Loss: 0.00010889
Iteration 13/25 | Loss: 0.00010889
Iteration 14/25 | Loss: 0.00010889
Iteration 15/25 | Loss: 0.00010889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00010888942051678896, 0.00010888942051678896, 0.00010888942051678896, 0.00010888942051678896, 0.00010888942051678896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010888942051678896

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01037389
Iteration 2/25 | Loss: 0.00230725
Iteration 3/25 | Loss: 0.00177833
Iteration 4/25 | Loss: 0.00177136
Iteration 5/25 | Loss: 0.00139842
Iteration 6/25 | Loss: 0.00139542
Iteration 7/25 | Loss: 0.00137763
Iteration 8/25 | Loss: 0.00107188
Iteration 9/25 | Loss: 0.00091917
Iteration 10/25 | Loss: 0.00088195
Iteration 11/25 | Loss: 0.00087623
Iteration 12/25 | Loss: 0.00087356
Iteration 13/25 | Loss: 0.00087063
Iteration 14/25 | Loss: 0.00086999
Iteration 15/25 | Loss: 0.00086980
Iteration 16/25 | Loss: 0.00086971
Iteration 17/25 | Loss: 0.00086970
Iteration 18/25 | Loss: 0.00086970
Iteration 19/25 | Loss: 0.00086970
Iteration 20/25 | Loss: 0.00086970
Iteration 21/25 | Loss: 0.00086970
Iteration 22/25 | Loss: 0.00086970
Iteration 23/25 | Loss: 0.00086970
Iteration 24/25 | Loss: 0.00086970
Iteration 25/25 | Loss: 0.00086970

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50064504
Iteration 2/25 | Loss: 0.00046512
Iteration 3/25 | Loss: 0.00046511
Iteration 4/25 | Loss: 0.00046511
Iteration 5/25 | Loss: 0.00046511
Iteration 6/25 | Loss: 0.00046511
Iteration 7/25 | Loss: 0.00046511
Iteration 8/25 | Loss: 0.00046511
Iteration 9/25 | Loss: 0.00046511
Iteration 10/25 | Loss: 0.00046511
Iteration 11/25 | Loss: 0.00046511
Iteration 12/25 | Loss: 0.00046511
Iteration 13/25 | Loss: 0.00046511
Iteration 14/25 | Loss: 0.00046511
Iteration 15/25 | Loss: 0.00046511
Iteration 16/25 | Loss: 0.00046511
Iteration 17/25 | Loss: 0.00046511
Iteration 18/25 | Loss: 0.00046511
Iteration 19/25 | Loss: 0.00046511
Iteration 20/25 | Loss: 0.00046511
Iteration 21/25 | Loss: 0.00046511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004651091294363141, 0.0004651091294363141, 0.0004651091294363141, 0.0004651091294363141, 0.0004651091294363141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004651091294363141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010889
Iteration 2/1000 | Loss: 0.00002251
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001395
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001319
Iteration 8/1000 | Loss: 0.00001293
Iteration 9/1000 | Loss: 0.00001274
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001242
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001240
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001232
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001226
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001225
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001225
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001221
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001220
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001218
Iteration 104/1000 | Loss: 0.00001218
Iteration 105/1000 | Loss: 0.00001218
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001217
Iteration 110/1000 | Loss: 0.00001217
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001216
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.215952852362534e-05, 1.215952852362534e-05, 1.215952852362534e-05, 1.215952852362534e-05, 1.215952852362534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.215952852362534e-05

Optimization complete. Final v2v error: 2.9675533771514893 mm

Highest mean error: 3.2467775344848633 mm for frame 109

Lowest mean error: 2.769448757171631 mm for frame 24

Saving results

Total time: 39.8404221534729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00613787
Iteration 2/25 | Loss: 0.00103381
Iteration 3/25 | Loss: 0.00077475
Iteration 4/25 | Loss: 0.00073734
Iteration 5/25 | Loss: 0.00072656
Iteration 6/25 | Loss: 0.00072559
Iteration 7/25 | Loss: 0.00072557
Iteration 8/25 | Loss: 0.00072557
Iteration 9/25 | Loss: 0.00072557
Iteration 10/25 | Loss: 0.00072557
Iteration 11/25 | Loss: 0.00072557
Iteration 12/25 | Loss: 0.00072557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007255688542500138, 0.0007255688542500138, 0.0007255688542500138, 0.0007255688542500138, 0.0007255688542500138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007255688542500138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38176048
Iteration 2/25 | Loss: 0.00010966
Iteration 3/25 | Loss: 0.00010964
Iteration 4/25 | Loss: 0.00010964
Iteration 5/25 | Loss: 0.00010964
Iteration 6/25 | Loss: 0.00010964
Iteration 7/25 | Loss: 0.00010964
Iteration 8/25 | Loss: 0.00010964
Iteration 9/25 | Loss: 0.00010964
Iteration 10/25 | Loss: 0.00010964
Iteration 11/25 | Loss: 0.00010964
Iteration 12/25 | Loss: 0.00010964
Iteration 13/25 | Loss: 0.00010964
Iteration 14/25 | Loss: 0.00010964
Iteration 15/25 | Loss: 0.00010964
Iteration 16/25 | Loss: 0.00010964
Iteration 17/25 | Loss: 0.00010964
Iteration 18/25 | Loss: 0.00010964
Iteration 19/25 | Loss: 0.00010964
Iteration 20/25 | Loss: 0.00010964
Iteration 21/25 | Loss: 0.00010964
Iteration 22/25 | Loss: 0.00010964
Iteration 23/25 | Loss: 0.00010964
Iteration 24/25 | Loss: 0.00010964
Iteration 25/25 | Loss: 0.00010964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046511
Iteration 2/1000 | Loss: 0.00003127
Iteration 3/1000 | Loss: 0.00002394
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001954
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001929
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001929
Iteration 13/1000 | Loss: 0.00001919
Iteration 14/1000 | Loss: 0.00001918
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001918
Iteration 17/1000 | Loss: 0.00001917
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001914
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001914
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001913
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001913
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001909
Iteration 34/1000 | Loss: 0.00001908
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001908
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001907
Iteration 41/1000 | Loss: 0.00001907
Iteration 42/1000 | Loss: 0.00001907
Iteration 43/1000 | Loss: 0.00001907
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001904
Iteration 68/1000 | Loss: 0.00001904
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001898
Iteration 92/1000 | Loss: 0.00001898
Iteration 93/1000 | Loss: 0.00001898
Iteration 94/1000 | Loss: 0.00001898
Iteration 95/1000 | Loss: 0.00001898
Iteration 96/1000 | Loss: 0.00001898
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001898
Iteration 100/1000 | Loss: 0.00001898
Iteration 101/1000 | Loss: 0.00001898
Iteration 102/1000 | Loss: 0.00001898
Iteration 103/1000 | Loss: 0.00001898
Iteration 104/1000 | Loss: 0.00001898
Iteration 105/1000 | Loss: 0.00001898
Iteration 106/1000 | Loss: 0.00001898
Iteration 107/1000 | Loss: 0.00001898
Iteration 108/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8984295820700936e-05, 1.8984295820700936e-05, 1.8984295820700936e-05, 1.8984295820700936e-05, 1.8984295820700936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8984295820700936e-05

Optimization complete. Final v2v error: 3.647984743118286 mm

Highest mean error: 3.7571394443511963 mm for frame 194

Lowest mean error: 3.5614073276519775 mm for frame 183

Saving results

Total time: 56.001511573791504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467592
Iteration 2/25 | Loss: 0.00101359
Iteration 3/25 | Loss: 0.00083117
Iteration 4/25 | Loss: 0.00078465
Iteration 5/25 | Loss: 0.00077670
Iteration 6/25 | Loss: 0.00077452
Iteration 7/25 | Loss: 0.00077429
Iteration 8/25 | Loss: 0.00077429
Iteration 9/25 | Loss: 0.00077429
Iteration 10/25 | Loss: 0.00077429
Iteration 11/25 | Loss: 0.00077429
Iteration 12/25 | Loss: 0.00077429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007742855232208967, 0.0007742855232208967, 0.0007742855232208967, 0.0007742855232208967, 0.0007742855232208967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007742855232208967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44158840
Iteration 2/25 | Loss: 0.00047870
Iteration 3/25 | Loss: 0.00047865
Iteration 4/25 | Loss: 0.00047865
Iteration 5/25 | Loss: 0.00047865
Iteration 6/25 | Loss: 0.00047865
Iteration 7/25 | Loss: 0.00047865
Iteration 8/25 | Loss: 0.00047865
Iteration 9/25 | Loss: 0.00047865
Iteration 10/25 | Loss: 0.00047865
Iteration 11/25 | Loss: 0.00047865
Iteration 12/25 | Loss: 0.00047865
Iteration 13/25 | Loss: 0.00047865
Iteration 14/25 | Loss: 0.00047865
Iteration 15/25 | Loss: 0.00047865
Iteration 16/25 | Loss: 0.00047865
Iteration 17/25 | Loss: 0.00047865
Iteration 18/25 | Loss: 0.00047865
Iteration 19/25 | Loss: 0.00047865
Iteration 20/25 | Loss: 0.00047865
Iteration 21/25 | Loss: 0.00047865
Iteration 22/25 | Loss: 0.00047865
Iteration 23/25 | Loss: 0.00047865
Iteration 24/25 | Loss: 0.00047865
Iteration 25/25 | Loss: 0.00047865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010964
Iteration 2/1000 | Loss: 0.00003686
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002762
Iteration 5/1000 | Loss: 0.00002644
Iteration 6/1000 | Loss: 0.00002570
Iteration 7/1000 | Loss: 0.00002514
Iteration 8/1000 | Loss: 0.00002467
Iteration 9/1000 | Loss: 0.00002429
Iteration 10/1000 | Loss: 0.00002413
Iteration 11/1000 | Loss: 0.00002410
Iteration 12/1000 | Loss: 0.00002398
Iteration 13/1000 | Loss: 0.00002396
Iteration 14/1000 | Loss: 0.00002387
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002386
Iteration 17/1000 | Loss: 0.00002385
Iteration 18/1000 | Loss: 0.00002384
Iteration 19/1000 | Loss: 0.00002384
Iteration 20/1000 | Loss: 0.00002383
Iteration 21/1000 | Loss: 0.00002383
Iteration 22/1000 | Loss: 0.00002383
Iteration 23/1000 | Loss: 0.00002380
Iteration 24/1000 | Loss: 0.00002379
Iteration 25/1000 | Loss: 0.00002378
Iteration 26/1000 | Loss: 0.00002377
Iteration 27/1000 | Loss: 0.00002377
Iteration 28/1000 | Loss: 0.00002376
Iteration 29/1000 | Loss: 0.00002376
Iteration 30/1000 | Loss: 0.00002376
Iteration 31/1000 | Loss: 0.00002376
Iteration 32/1000 | Loss: 0.00002375
Iteration 33/1000 | Loss: 0.00002375
Iteration 34/1000 | Loss: 0.00002375
Iteration 35/1000 | Loss: 0.00002374
Iteration 36/1000 | Loss: 0.00002374
Iteration 37/1000 | Loss: 0.00002374
Iteration 38/1000 | Loss: 0.00002374
Iteration 39/1000 | Loss: 0.00002373
Iteration 40/1000 | Loss: 0.00002373
Iteration 41/1000 | Loss: 0.00002373
Iteration 42/1000 | Loss: 0.00002373
Iteration 43/1000 | Loss: 0.00002373
Iteration 44/1000 | Loss: 0.00002372
Iteration 45/1000 | Loss: 0.00002372
Iteration 46/1000 | Loss: 0.00002372
Iteration 47/1000 | Loss: 0.00002371
Iteration 48/1000 | Loss: 0.00002370
Iteration 49/1000 | Loss: 0.00002370
Iteration 50/1000 | Loss: 0.00002370
Iteration 51/1000 | Loss: 0.00002370
Iteration 52/1000 | Loss: 0.00002370
Iteration 53/1000 | Loss: 0.00002370
Iteration 54/1000 | Loss: 0.00002369
Iteration 55/1000 | Loss: 0.00002369
Iteration 56/1000 | Loss: 0.00002369
Iteration 57/1000 | Loss: 0.00002368
Iteration 58/1000 | Loss: 0.00002368
Iteration 59/1000 | Loss: 0.00002368
Iteration 60/1000 | Loss: 0.00002367
Iteration 61/1000 | Loss: 0.00002366
Iteration 62/1000 | Loss: 0.00002366
Iteration 63/1000 | Loss: 0.00002366
Iteration 64/1000 | Loss: 0.00002366
Iteration 65/1000 | Loss: 0.00002365
Iteration 66/1000 | Loss: 0.00002365
Iteration 67/1000 | Loss: 0.00002365
Iteration 68/1000 | Loss: 0.00002365
Iteration 69/1000 | Loss: 0.00002365
Iteration 70/1000 | Loss: 0.00002365
Iteration 71/1000 | Loss: 0.00002365
Iteration 72/1000 | Loss: 0.00002365
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002364
Iteration 75/1000 | Loss: 0.00002364
Iteration 76/1000 | Loss: 0.00002364
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002363
Iteration 79/1000 | Loss: 0.00002363
Iteration 80/1000 | Loss: 0.00002363
Iteration 81/1000 | Loss: 0.00002363
Iteration 82/1000 | Loss: 0.00002363
Iteration 83/1000 | Loss: 0.00002363
Iteration 84/1000 | Loss: 0.00002362
Iteration 85/1000 | Loss: 0.00002362
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002362
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002361
Iteration 91/1000 | Loss: 0.00002361
Iteration 92/1000 | Loss: 0.00002361
Iteration 93/1000 | Loss: 0.00002361
Iteration 94/1000 | Loss: 0.00002361
Iteration 95/1000 | Loss: 0.00002361
Iteration 96/1000 | Loss: 0.00002360
Iteration 97/1000 | Loss: 0.00002360
Iteration 98/1000 | Loss: 0.00002360
Iteration 99/1000 | Loss: 0.00002359
Iteration 100/1000 | Loss: 0.00002359
Iteration 101/1000 | Loss: 0.00002359
Iteration 102/1000 | Loss: 0.00002359
Iteration 103/1000 | Loss: 0.00002359
Iteration 104/1000 | Loss: 0.00002359
Iteration 105/1000 | Loss: 0.00002359
Iteration 106/1000 | Loss: 0.00002358
Iteration 107/1000 | Loss: 0.00002358
Iteration 108/1000 | Loss: 0.00002358
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002358
Iteration 111/1000 | Loss: 0.00002358
Iteration 112/1000 | Loss: 0.00002358
Iteration 113/1000 | Loss: 0.00002358
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00002357
Iteration 116/1000 | Loss: 0.00002357
Iteration 117/1000 | Loss: 0.00002357
Iteration 118/1000 | Loss: 0.00002357
Iteration 119/1000 | Loss: 0.00002357
Iteration 120/1000 | Loss: 0.00002357
Iteration 121/1000 | Loss: 0.00002357
Iteration 122/1000 | Loss: 0.00002357
Iteration 123/1000 | Loss: 0.00002357
Iteration 124/1000 | Loss: 0.00002357
Iteration 125/1000 | Loss: 0.00002357
Iteration 126/1000 | Loss: 0.00002357
Iteration 127/1000 | Loss: 0.00002357
Iteration 128/1000 | Loss: 0.00002357
Iteration 129/1000 | Loss: 0.00002357
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002356
Iteration 132/1000 | Loss: 0.00002356
Iteration 133/1000 | Loss: 0.00002356
Iteration 134/1000 | Loss: 0.00002356
Iteration 135/1000 | Loss: 0.00002356
Iteration 136/1000 | Loss: 0.00002356
Iteration 137/1000 | Loss: 0.00002356
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002356
Iteration 140/1000 | Loss: 0.00002356
Iteration 141/1000 | Loss: 0.00002356
Iteration 142/1000 | Loss: 0.00002356
Iteration 143/1000 | Loss: 0.00002356
Iteration 144/1000 | Loss: 0.00002356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.3557360691484064e-05, 2.3557360691484064e-05, 2.3557360691484064e-05, 2.3557360691484064e-05, 2.3557360691484064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3557360691484064e-05

Optimization complete. Final v2v error: 4.159266471862793 mm

Highest mean error: 4.811224460601807 mm for frame 61

Lowest mean error: 3.7594056129455566 mm for frame 116

Saving results

Total time: 40.688387632369995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598223
Iteration 2/25 | Loss: 0.00074489
Iteration 3/25 | Loss: 0.00061091
Iteration 4/25 | Loss: 0.00058657
Iteration 5/25 | Loss: 0.00057771
Iteration 6/25 | Loss: 0.00057586
Iteration 7/25 | Loss: 0.00057570
Iteration 8/25 | Loss: 0.00057570
Iteration 9/25 | Loss: 0.00057570
Iteration 10/25 | Loss: 0.00057570
Iteration 11/25 | Loss: 0.00057570
Iteration 12/25 | Loss: 0.00057570
Iteration 13/25 | Loss: 0.00057570
Iteration 14/25 | Loss: 0.00057570
Iteration 15/25 | Loss: 0.00057570
Iteration 16/25 | Loss: 0.00057570
Iteration 17/25 | Loss: 0.00057570
Iteration 18/25 | Loss: 0.00057570
Iteration 19/25 | Loss: 0.00057570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005756961763836443, 0.0005756961763836443, 0.0005756961763836443, 0.0005756961763836443, 0.0005756961763836443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005756961763836443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71983242
Iteration 2/25 | Loss: 0.00010960
Iteration 3/25 | Loss: 0.00010960
Iteration 4/25 | Loss: 0.00010959
Iteration 5/25 | Loss: 0.00010959
Iteration 6/25 | Loss: 0.00010959
Iteration 7/25 | Loss: 0.00010959
Iteration 8/25 | Loss: 0.00010959
Iteration 9/25 | Loss: 0.00010959
Iteration 10/25 | Loss: 0.00010959
Iteration 11/25 | Loss: 0.00010959
Iteration 12/25 | Loss: 0.00010959
Iteration 13/25 | Loss: 0.00010959
Iteration 14/25 | Loss: 0.00010959
Iteration 15/25 | Loss: 0.00010959
Iteration 16/25 | Loss: 0.00010959
Iteration 17/25 | Loss: 0.00010959
Iteration 18/25 | Loss: 0.00010959
Iteration 19/25 | Loss: 0.00010959
Iteration 20/25 | Loss: 0.00010959
Iteration 21/25 | Loss: 0.00010959
Iteration 22/25 | Loss: 0.00010959
Iteration 23/25 | Loss: 0.00010959
Iteration 24/25 | Loss: 0.00010959
Iteration 25/25 | Loss: 0.00010959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00010959245992125943, 0.00010959245992125943, 0.00010959245992125943, 0.00010959245992125943, 0.00010959245992125943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010959245992125943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047865
Iteration 2/1000 | Loss: 0.00003633
Iteration 3/1000 | Loss: 0.00002440
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001869
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00001778
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001724
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001712
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001711
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001708
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001707
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001704
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001696
Iteration 86/1000 | Loss: 0.00001696
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.687836083874572e-05, 1.687836083874572e-05, 1.687836083874572e-05, 1.687836083874572e-05, 1.687836083874572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.687836083874572e-05

Optimization complete. Final v2v error: 3.4396822452545166 mm

Highest mean error: 4.450560569763184 mm for frame 56

Lowest mean error: 2.8847291469573975 mm for frame 103

Saving results

Total time: 44.90941882133484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00010959
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002364
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002095
Iteration 6/1000 | Loss: 0.00002003
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001913
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001875
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001856
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001853
Iteration 20/1000 | Loss: 0.00001852
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001840
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001839
Iteration 30/1000 | Loss: 0.00001839
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001838
Iteration 33/1000 | Loss: 0.00001837
Iteration 34/1000 | Loss: 0.00001837
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001836
Iteration 38/1000 | Loss: 0.00001836
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001831
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001828
Iteration 55/1000 | Loss: 0.00001828
Iteration 56/1000 | Loss: 0.00001828
Iteration 57/1000 | Loss: 0.00001828
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001828
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00001828
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001827
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001826
Iteration 69/1000 | Loss: 0.00001825
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00001825
Iteration 72/1000 | Loss: 0.00001825
Iteration 73/1000 | Loss: 0.00001825
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001824
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001823
Iteration 95/1000 | Loss: 0.00001823
Iteration 96/1000 | Loss: 0.00001823
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001822
Iteration 102/1000 | Loss: 0.00001822
Iteration 103/1000 | Loss: 0.00001822
Iteration 104/1000 | Loss: 0.00001822
Iteration 105/1000 | Loss: 0.00001822
Iteration 106/1000 | Loss: 0.00001822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.8223669030703604e-05, 1.8223669030703604e-05, 1.8223669030703604e-05, 1.8223669030703604e-05, 1.8223669030703604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8223669030703604e-05

Optimization complete. Final v2v error: 3.6234750747680664 mm

Highest mean error: 3.819432020187378 mm for frame 142

Lowest mean error: 3.357297658920288 mm for frame 85

Saving results

Total time: 34.11659479141235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912267
Iteration 2/25 | Loss: 0.00136685
Iteration 3/25 | Loss: 0.00103263
Iteration 4/25 | Loss: 0.00094981
Iteration 5/25 | Loss: 0.00092847
Iteration 6/25 | Loss: 0.00092579
Iteration 7/25 | Loss: 0.00092353
Iteration 8/25 | Loss: 0.00090887
Iteration 9/25 | Loss: 0.00089806
Iteration 10/25 | Loss: 0.00088291
Iteration 11/25 | Loss: 0.00087538
Iteration 12/25 | Loss: 0.00086755
Iteration 13/25 | Loss: 0.00086717
Iteration 14/25 | Loss: 0.00086737
Iteration 15/25 | Loss: 0.00086294
Iteration 16/25 | Loss: 0.00085653
Iteration 17/25 | Loss: 0.00085433
Iteration 18/25 | Loss: 0.00085407
Iteration 19/25 | Loss: 0.00085399
Iteration 20/25 | Loss: 0.00085398
Iteration 21/25 | Loss: 0.00085398
Iteration 22/25 | Loss: 0.00085398
Iteration 23/25 | Loss: 0.00085398
Iteration 24/25 | Loss: 0.00085398
Iteration 25/25 | Loss: 0.00085398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.34710121
Iteration 2/25 | Loss: 0.00088952
Iteration 3/25 | Loss: 0.00088952
Iteration 4/25 | Loss: 0.00088952
Iteration 5/25 | Loss: 0.00088952
Iteration 6/25 | Loss: 0.00088952
Iteration 7/25 | Loss: 0.00088952
Iteration 8/25 | Loss: 0.00088952
Iteration 9/25 | Loss: 0.00088952
Iteration 10/25 | Loss: 0.00088952
Iteration 11/25 | Loss: 0.00088952
Iteration 12/25 | Loss: 0.00088952
Iteration 13/25 | Loss: 0.00088952
Iteration 14/25 | Loss: 0.00088952
Iteration 15/25 | Loss: 0.00088952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008895205683074892, 0.0008895205683074892, 0.0008895205683074892, 0.0008895205683074892, 0.0008895205683074892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008895205683074892

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01084029
Iteration 2/25 | Loss: 0.00224421
Iteration 3/25 | Loss: 0.00177794
Iteration 4/25 | Loss: 0.00166223
Iteration 5/25 | Loss: 0.00155739
Iteration 6/25 | Loss: 0.00155513
Iteration 7/25 | Loss: 0.00139043
Iteration 8/25 | Loss: 0.00114727
Iteration 9/25 | Loss: 0.00101951
Iteration 10/25 | Loss: 0.00093786
Iteration 11/25 | Loss: 0.00089036
Iteration 12/25 | Loss: 0.00086035
Iteration 13/25 | Loss: 0.00082996
Iteration 14/25 | Loss: 0.00083182
Iteration 15/25 | Loss: 0.00082163
Iteration 16/25 | Loss: 0.00081490
Iteration 17/25 | Loss: 0.00081799
Iteration 18/25 | Loss: 0.00081619
Iteration 19/25 | Loss: 0.00081171
Iteration 20/25 | Loss: 0.00081078
Iteration 21/25 | Loss: 0.00081051
Iteration 22/25 | Loss: 0.00081036
Iteration 23/25 | Loss: 0.00081025
Iteration 24/25 | Loss: 0.00081015
Iteration 25/25 | Loss: 0.00081004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42279410
Iteration 2/25 | Loss: 0.00098510
Iteration 3/25 | Loss: 0.00098510
Iteration 4/25 | Loss: 0.00098510
Iteration 5/25 | Loss: 0.00098510
Iteration 6/25 | Loss: 0.00098510
Iteration 7/25 | Loss: 0.00098510
Iteration 8/25 | Loss: 0.00098510
Iteration 9/25 | Loss: 0.00098510
Iteration 10/25 | Loss: 0.00098510
Iteration 11/25 | Loss: 0.00098510
Iteration 12/25 | Loss: 0.00098510
Iteration 13/25 | Loss: 0.00098510
Iteration 14/25 | Loss: 0.00098510
Iteration 15/25 | Loss: 0.00098510
Iteration 16/25 | Loss: 0.00098510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009851005161181092, 0.0009851005161181092, 0.0009851005161181092, 0.0009851005161181092, 0.0009851005161181092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009851005161181092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100151
Iteration 2/1000 | Loss: 0.00073227
Iteration 3/1000 | Loss: 0.00097632
Iteration 4/1000 | Loss: 0.00069198
Iteration 5/1000 | Loss: 0.00088602
Iteration 6/1000 | Loss: 0.00054563
Iteration 7/1000 | Loss: 0.00099988
Iteration 8/1000 | Loss: 0.00129240
Iteration 9/1000 | Loss: 0.00096538
Iteration 10/1000 | Loss: 0.00045514
Iteration 11/1000 | Loss: 0.00084497
Iteration 12/1000 | Loss: 0.00127434
Iteration 13/1000 | Loss: 0.00120797
Iteration 14/1000 | Loss: 0.00122437
Iteration 15/1000 | Loss: 0.00173915
Iteration 16/1000 | Loss: 0.00138811
Iteration 17/1000 | Loss: 0.00085164
Iteration 18/1000 | Loss: 0.00094089
Iteration 19/1000 | Loss: 0.00094899
Iteration 20/1000 | Loss: 0.00151741
Iteration 21/1000 | Loss: 0.00152788
Iteration 22/1000 | Loss: 0.00238591
Iteration 23/1000 | Loss: 0.00206311
Iteration 24/1000 | Loss: 0.00090682
Iteration 25/1000 | Loss: 0.00085942
Iteration 26/1000 | Loss: 0.00080298
Iteration 27/1000 | Loss: 0.00145607
Iteration 28/1000 | Loss: 0.00078749
Iteration 29/1000 | Loss: 0.00094644
Iteration 30/1000 | Loss: 0.00057984
Iteration 31/1000 | Loss: 0.00069598
Iteration 32/1000 | Loss: 0.00069405
Iteration 33/1000 | Loss: 0.00065562
Iteration 34/1000 | Loss: 0.00163453
Iteration 35/1000 | Loss: 0.00063255
Iteration 36/1000 | Loss: 0.00061113
Iteration 37/1000 | Loss: 0.00069318
Iteration 38/1000 | Loss: 0.00076680
Iteration 39/1000 | Loss: 0.00090716
Iteration 40/1000 | Loss: 0.00087359
Iteration 41/1000 | Loss: 0.00076226
Iteration 42/1000 | Loss: 0.00090863
Iteration 43/1000 | Loss: 0.00116297
Iteration 44/1000 | Loss: 0.00087704
Iteration 45/1000 | Loss: 0.00156239
Iteration 46/1000 | Loss: 0.00178072
Iteration 47/1000 | Loss: 0.00102703
Iteration 48/1000 | Loss: 0.00195347
Iteration 49/1000 | Loss: 0.00137932
Iteration 50/1000 | Loss: 0.00071338
Iteration 51/1000 | Loss: 0.00131648
Iteration 52/1000 | Loss: 0.00194676
Iteration 53/1000 | Loss: 0.00138043
Iteration 54/1000 | Loss: 0.00153202
Iteration 55/1000 | Loss: 0.00181759
Iteration 56/1000 | Loss: 0.00187195
Iteration 57/1000 | Loss: 0.00078658
Iteration 58/1000 | Loss: 0.00026282
Iteration 59/1000 | Loss: 0.00097371
Iteration 60/1000 | Loss: 0.00125909
Iteration 61/1000 | Loss: 0.00148579
Iteration 62/1000 | Loss: 0.00165425
Iteration 63/1000 | Loss: 0.00192731
Iteration 64/1000 | Loss: 0.00102547
Iteration 65/1000 | Loss: 0.00092847
Iteration 66/1000 | Loss: 0.00069828
Iteration 67/1000 | Loss: 0.00085874
Iteration 68/1000 | Loss: 0.00040742
Iteration 69/1000 | Loss: 0.00053549
Iteration 70/1000 | Loss: 0.00101316
Iteration 71/1000 | Loss: 0.00085472
Iteration 72/1000 | Loss: 0.00110603
Iteration 73/1000 | Loss: 0.00113156
Iteration 74/1000 | Loss: 0.00059216
Iteration 75/1000 | Loss: 0.00092592
Iteration 76/1000 | Loss: 0.00088246
Iteration 77/1000 | Loss: 0.00072484
Iteration 78/1000 | Loss: 0.00033218
Iteration 79/1000 | Loss: 0.00014346
Iteration 80/1000 | Loss: 0.00014828
Iteration 81/1000 | Loss: 0.00026804
Iteration 82/1000 | Loss: 0.00058656
Iteration 83/1000 | Loss: 0.00043978
Iteration 84/1000 | Loss: 0.00036202
Iteration 85/1000 | Loss: 0.00023316
Iteration 86/1000 | Loss: 0.00040528
Iteration 87/1000 | Loss: 0.00033578
Iteration 88/1000 | Loss: 0.00056350
Iteration 89/1000 | Loss: 0.00088421
Iteration 90/1000 | Loss: 0.00068432
Iteration 91/1000 | Loss: 0.00052363
Iteration 92/1000 | Loss: 0.00034667
Iteration 93/1000 | Loss: 0.00033488
Iteration 94/1000 | Loss: 0.00046887
Iteration 95/1000 | Loss: 0.00046093
Iteration 96/1000 | Loss: 0.00043032
Iteration 97/1000 | Loss: 0.00094584
Iteration 98/1000 | Loss: 0.00038494
Iteration 99/1000 | Loss: 0.00045657
Iteration 100/1000 | Loss: 0.00098329
Iteration 101/1000 | Loss: 0.00065265
Iteration 102/1000 | Loss: 0.00049953
Iteration 103/1000 | Loss: 0.00079943
Iteration 104/1000 | Loss: 0.00058447
Iteration 105/1000 | Loss: 0.00057779
Iteration 106/1000 | Loss: 0.00060480
Iteration 107/1000 | Loss: 0.00120858
Iteration 108/1000 | Loss: 0.00063129
Iteration 109/1000 | Loss: 0.00203658
Iteration 110/1000 | Loss: 0.00170435
Iteration 111/1000 | Loss: 0.00081611
Iteration 112/1000 | Loss: 0.00035366
Iteration 113/1000 | Loss: 0.00100527
Iteration 114/1000 | Loss: 0.00048717
Iteration 115/1000 | Loss: 0.00088119
Iteration 116/1000 | Loss: 0.00076249
Iteration 117/1000 | Loss: 0.00052922
Iteration 118/1000 | Loss: 0.00075972
Iteration 119/1000 | Loss: 0.00050313
Iteration 120/1000 | Loss: 0.00047244
Iteration 121/1000 | Loss: 0.00085519
Iteration 122/1000 | Loss: 0.00057332
Iteration 123/1000 | Loss: 0.00037132
Iteration 124/1000 | Loss: 0.00069413
Iteration 125/1000 | Loss: 0.00090171
Iteration 126/1000 | Loss: 0.00052485
Iteration 127/1000 | Loss: 0.00092091
Iteration 128/1000 | Loss: 0.00079023
Iteration 129/1000 | Loss: 0.00084776
Iteration 130/1000 | Loss: 0.00048114
Iteration 131/1000 | Loss: 0.00048529
Iteration 132/1000 | Loss: 0.00006184
Iteration 133/1000 | Loss: 0.00018933
Iteration 134/1000 | Loss: 0.00018964
Iteration 135/1000 | Loss: 0.00016246
Iteration 136/1000 | Loss: 0.00019093
Iteration 137/1000 | Loss: 0.00042632
Iteration 138/1000 | Loss: 0.00038024
Iteration 139/1000 | Loss: 0.00031769
Iteration 140/1000 | Loss: 0.00038907
Iteration 141/1000 | Loss: 0.00050801
Iteration 142/1000 | Loss: 0.00044159
Iteration 143/1000 | Loss: 0.00084061
Iteration 144/1000 | Loss: 0.00013900
Iteration 145/1000 | Loss: 0.00039967
Iteration 146/1000 | Loss: 0.00027918
Iteration 147/1000 | Loss: 0.00042126
Iteration 148/1000 | Loss: 0.00022661
Iteration 149/1000 | Loss: 0.00018048
Iteration 150/1000 | Loss: 0.00010793
Iteration 151/1000 | Loss: 0.00016581
Iteration 152/1000 | Loss: 0.00030649
Iteration 153/1000 | Loss: 0.00028674
Iteration 154/1000 | Loss: 0.00024116
Iteration 155/1000 | Loss: 0.00013757
Iteration 156/1000 | Loss: 0.00019956
Iteration 157/1000 | Loss: 0.00021241
Iteration 158/1000 | Loss: 0.00028408
Iteration 159/1000 | Loss: 0.00017339
Iteration 160/1000 | Loss: 0.00042184
Iteration 161/1000 | Loss: 0.00035384
Iteration 162/1000 | Loss: 0.00058732
Iteration 163/1000 | Loss: 0.00052734
Iteration 164/1000 | Loss: 0.00020431
Iteration 165/1000 | Loss: 0.00038398
Iteration 166/1000 | Loss: 0.00034600
Iteration 167/1000 | Loss: 0.00015731
Iteration 168/1000 | Loss: 0.00028734
Iteration 169/1000 | Loss: 0.00038602
Iteration 170/1000 | Loss: 0.00034790
Iteration 171/1000 | Loss: 0.00048086
Iteration 172/1000 | Loss: 0.00044035
Iteration 173/1000 | Loss: 0.00031594
Iteration 174/1000 | Loss: 0.00039790
Iteration 175/1000 | Loss: 0.00043103
Iteration 176/1000 | Loss: 0.00024678
Iteration 177/1000 | Loss: 0.00038216
Iteration 178/1000 | Loss: 0.00068120
Iteration 179/1000 | Loss: 0.00042605
Iteration 180/1000 | Loss: 0.00050383
Iteration 181/1000 | Loss: 0.00033408
Iteration 182/1000 | Loss: 0.00027024
Iteration 183/1000 | Loss: 0.00007517
Iteration 184/1000 | Loss: 0.00041896
Iteration 185/1000 | Loss: 0.00014827
Iteration 186/1000 | Loss: 0.00019143
Iteration 187/1000 | Loss: 0.00031766
Iteration 188/1000 | Loss: 0.00035587
Iteration 189/1000 | Loss: 0.00046335
Iteration 190/1000 | Loss: 0.00042256
Iteration 191/1000 | Loss: 0.00012054
Iteration 192/1000 | Loss: 0.00035194
Iteration 193/1000 | Loss: 0.00025497
Iteration 194/1000 | Loss: 0.00034123
Iteration 195/1000 | Loss: 0.00041299
Iteration 196/1000 | Loss: 0.00006694
Iteration 197/1000 | Loss: 0.00052040
Iteration 198/1000 | Loss: 0.00053740
Iteration 199/1000 | Loss: 0.00028715
Iteration 200/1000 | Loss: 0.00025151
Iteration 201/1000 | Loss: 0.00023975
Iteration 202/1000 | Loss: 0.00017108
Iteration 203/1000 | Loss: 0.00028803
Iteration 204/1000 | Loss: 0.00015658
Iteration 205/1000 | Loss: 0.00024633
Iteration 206/1000 | Loss: 0.00010991
Iteration 207/1000 | Loss: 0.00021646
Iteration 208/1000 | Loss: 0.00021602
Iteration 209/1000 | Loss: 0.00004199
Iteration 210/1000 | Loss: 0.00014338
Iteration 211/1000 | Loss: 0.00011527
Iteration 212/1000 | Loss: 0.00033822
Iteration 213/1000 | Loss: 0.00022514
Iteration 214/1000 | Loss: 0.00037849
Iteration 215/1000 | Loss: 0.00016486
Iteration 216/1000 | Loss: 0.00041869
Iteration 217/1000 | Loss: 0.00004714
Iteration 218/1000 | Loss: 0.00024293
Iteration 219/1000 | Loss: 0.00063838
Iteration 220/1000 | Loss: 0.00061005
Iteration 221/1000 | Loss: 0.00020038
Iteration 222/1000 | Loss: 0.00011830
Iteration 223/1000 | Loss: 0.00013721
Iteration 224/1000 | Loss: 0.00033617
Iteration 225/1000 | Loss: 0.00032877
Iteration 226/1000 | Loss: 0.00013772
Iteration 227/1000 | Loss: 0.00027684
Iteration 228/1000 | Loss: 0.00024534
Iteration 229/1000 | Loss: 0.00021123
Iteration 230/1000 | Loss: 0.00021959
Iteration 231/1000 | Loss: 0.00019856
Iteration 232/1000 | Loss: 0.00014120
Iteration 233/1000 | Loss: 0.00013950
Iteration 234/1000 | Loss: 0.00009210
Iteration 235/1000 | Loss: 0.00012979
Iteration 236/1000 | Loss: 0.00021279
Iteration 237/1000 | Loss: 0.00018318
Iteration 238/1000 | Loss: 0.00012258
Iteration 239/1000 | Loss: 0.00013609
Iteration 240/1000 | Loss: 0.00021072
Iteration 241/1000 | Loss: 0.00003960
Iteration 242/1000 | Loss: 0.00010430
Iteration 243/1000 | Loss: 0.00031692
Iteration 244/1000 | Loss: 0.00007196
Iteration 245/1000 | Loss: 0.00015628
Iteration 246/1000 | Loss: 0.00028086
Iteration 247/1000 | Loss: 0.00008250
Iteration 248/1000 | Loss: 0.00012644
Iteration 249/1000 | Loss: 0.00031313
Iteration 250/1000 | Loss: 0.00002879
Iteration 251/1000 | Loss: 0.00002430
Iteration 252/1000 | Loss: 0.00002289
Iteration 253/1000 | Loss: 0.00002160
Iteration 254/1000 | Loss: 0.00002079
Iteration 255/1000 | Loss: 0.00063011
Iteration 256/1000 | Loss: 0.00003230
Iteration 257/1000 | Loss: 0.00002077
Iteration 258/1000 | Loss: 0.00001883
Iteration 259/1000 | Loss: 0.00001805
Iteration 260/1000 | Loss: 0.00001759
Iteration 261/1000 | Loss: 0.00001733
Iteration 262/1000 | Loss: 0.00001732
Iteration 263/1000 | Loss: 0.00001731
Iteration 264/1000 | Loss: 0.00001731
Iteration 265/1000 | Loss: 0.00001730
Iteration 266/1000 | Loss: 0.00001730
Iteration 267/1000 | Loss: 0.00001730
Iteration 268/1000 | Loss: 0.00001729
Iteration 269/1000 | Loss: 0.00001729
Iteration 270/1000 | Loss: 0.00001727
Iteration 271/1000 | Loss: 0.00001727
Iteration 272/1000 | Loss: 0.00001726
Iteration 273/1000 | Loss: 0.00001721
Iteration 274/1000 | Loss: 0.00001721
Iteration 275/1000 | Loss: 0.00001721
Iteration 276/1000 | Loss: 0.00001721
Iteration 277/1000 | Loss: 0.00001721
Iteration 278/1000 | Loss: 0.00001721
Iteration 279/1000 | Loss: 0.00001721
Iteration 280/1000 | Loss: 0.00001721
Iteration 281/1000 | Loss: 0.00001721
Iteration 282/1000 | Loss: 0.00001721
Iteration 283/1000 | Loss: 0.00001721
Iteration 284/1000 | Loss: 0.00001720
Iteration 285/1000 | Loss: 0.00001720
Iteration 286/1000 | Loss: 0.00001720
Iteration 287/1000 | Loss: 0.00001720
Iteration 288/1000 | Loss: 0.00001718
Iteration 289/1000 | Loss: 0.00001717
Iteration 290/1000 | Loss: 0.00001717
Iteration 291/1000 | Loss: 0.00001717
Iteration 292/1000 | Loss: 0.00001717
Iteration 293/1000 | Loss: 0.00001717
Iteration 294/1000 | Loss: 0.00001717
Iteration 295/1000 | Loss: 0.00001717
Iteration 296/1000 | Loss: 0.00001717
Iteration 297/1000 | Loss: 0.00001715
Iteration 298/1000 | Loss: 0.00001709
Iteration 299/1000 | Loss: 0.00001709
Iteration 300/1000 | Loss: 0.00001709
Iteration 301/1000 | Loss: 0.00001708
Iteration 302/1000 | Loss: 0.00001707
Iteration 303/1000 | Loss: 0.00001706
Iteration 304/1000 | Loss: 0.00001706
Iteration 305/1000 | Loss: 0.00001705
Iteration 306/1000 | Loss: 0.00001705
Iteration 307/1000 | Loss: 0.00001704
Iteration 308/1000 | Loss: 0.00001704
Iteration 309/1000 | Loss: 0.00001704
Iteration 310/1000 | Loss: 0.00001703
Iteration 311/1000 | Loss: 0.00001703
Iteration 312/1000 | Loss: 0.00001703
Iteration 313/1000 | Loss: 0.00001703
Iteration 314/1000 | Loss: 0.00001702
Iteration 315/1000 | Loss: 0.00001702
Iteration 316/1000 | Loss: 0.00001702
Iteration 317/1000 | Loss: 0.00001701
Iteration 318/1000 | Loss: 0.00001701
Iteration 319/1000 | Loss: 0.00001701
Iteration 320/1000 | Loss: 0.00001701
Iteration 321/1000 | Loss: 0.00001700
Iteration 322/1000 | Loss: 0.00001700
Iteration 323/1000 | Loss: 0.00001700
Iteration 324/1000 | Loss: 0.00001700
Iteration 325/1000 | Loss: 0.00001700
Iteration 326/1000 | Loss: 0.00001700
Iteration 327/1000 | Loss: 0.00001700
Iteration 328/1000 | Loss: 0.00001699
Iteration 329/1000 | Loss: 0.00001699
Iteration 330/1000 | Loss: 0.00001699
Iteration 331/1000 | Loss: 0.00001699
Iteration 332/1000 | Loss: 0.00001699
Iteration 333/1000 | Loss: 0.00001699
Iteration 334/1000 | Loss: 0.00001699
Iteration 335/1000 | Loss: 0.00001698
Iteration 336/1000 | Loss: 0.00001698
Iteration 337/1000 | Loss: 0.00001698
Iteration 338/1000 | Loss: 0.00001698
Iteration 339/1000 | Loss: 0.00001697
Iteration 340/1000 | Loss: 0.00001697
Iteration 341/1000 | Loss: 0.00001697
Iteration 342/1000 | Loss: 0.00001697
Iteration 343/1000 | Loss: 0.00001696
Iteration 344/1000 | Loss: 0.00001696
Iteration 345/1000 | Loss: 0.00001696
Iteration 346/1000 | Loss: 0.00001696
Iteration 347/1000 | Loss: 0.00001696
Iteration 348/1000 | Loss: 0.00001696
Iteration 349/1000 | Loss: 0.00001696
Iteration 350/1000 | Loss: 0.00001696
Iteration 351/1000 | Loss: 0.00001695
Iteration 352/1000 | Loss: 0.00001695
Iteration 353/1000 | Loss: 0.00001695
Iteration 354/1000 | Loss: 0.00001695
Iteration 355/1000 | Loss: 0.00001695
Iteration 356/1000 | Loss: 0.00001695
Iteration 357/1000 | Loss: 0.00001695
Iteration 358/1000 | Loss: 0.00001695
Iteration 359/1000 | Loss: 0.00001694
Iteration 360/1000 | Loss: 0.00001694
Iteration 361/1000 | Loss: 0.00001694
Iteration 362/1000 | Loss: 0.00001694
Iteration 363/1000 | Loss: 0.00001694
Iteration 364/1000 | Loss: 0.00001694
Iteration 365/1000 | Loss: 0.00001694
Iteration 366/1000 | Loss: 0.00001694
Iteration 367/1000 | Loss: 0.00001694
Iteration 368/1000 | Loss: 0.00001694
Iteration 369/1000 | Loss: 0.00001694
Iteration 370/1000 | Loss: 0.00001694
Iteration 371/1000 | Loss: 0.00001694
Iteration 372/1000 | Loss: 0.00001694
Iteration 373/1000 | Loss: 0.00001694
Iteration 374/1000 | Loss: 0.00001693
Iteration 375/1000 | Loss: 0.00001693
Iteration 376/1000 | Loss: 0.00001693
Iteration 377/1000 | Loss: 0.00001693
Iteration 378/1000 | Loss: 0.00001693
Iteration 379/1000 | Loss: 0.00001693
Iteration 380/1000 | Loss: 0.00001693
Iteration 381/1000 | Loss: 0.00001693
Iteration 382/1000 | Loss: 0.00001693
Iteration 383/1000 | Loss: 0.00001693
Iteration 384/1000 | Loss: 0.00001693
Iteration 385/1000 | Loss: 0.00001693
Iteration 386/1000 | Loss: 0.00001693
Iteration 387/1000 | Loss: 0.00001693
Iteration 388/1000 | Loss: 0.00001693
Iteration 389/1000 | Loss: 0.00001693
Iteration 390/1000 | Loss: 0.00001693
Iteration 391/1000 | Loss: 0.00001692
Iteration 392/1000 | Loss: 0.00001692
Iteration 393/1000 | Loss: 0.00001692
Iteration 394/1000 | Loss: 0.00001692
Iteration 395/1000 | Loss: 0.00001692
Iteration 396/1000 | Loss: 0.00001692
Iteration 397/1000 | Loss: 0.00001692
Iteration 398/1000 | Loss: 0.00001692
Iteration 399/1000 | Loss: 0.00001692
Iteration 400/1000 | Loss: 0.00001691
Iteration 401/1000 | Loss: 0.00001691
Iteration 402/1000 | Loss: 0.00001691
Iteration 403/1000 | Loss: 0.00001691
Iteration 404/1000 | Loss: 0.00001691
Iteration 405/1000 | Loss: 0.00001691
Iteration 406/1000 | Loss: 0.00001691
Iteration 407/1000 | Loss: 0.00001691
Iteration 408/1000 | Loss: 0.00001691
Iteration 409/1000 | Loss: 0.00001691
Iteration 410/1000 | Loss: 0.00001691
Iteration 411/1000 | Loss: 0.00001691
Iteration 412/1000 | Loss: 0.00001691
Iteration 413/1000 | Loss: 0.00001691
Iteration 414/1000 | Loss: 0.00001690
Iteration 415/1000 | Loss: 0.00001690
Iteration 416/1000 | Loss: 0.00001690
Iteration 417/1000 | Loss: 0.00001690
Iteration 418/1000 | Loss: 0.00001690
Iteration 419/1000 | Loss: 0.00001690
Iteration 420/1000 | Loss: 0.00001690
Iteration 421/1000 | Loss: 0.00001690
Iteration 422/1000 | Loss: 0.00001690
Iteration 423/1000 | Loss: 0.00001690
Iteration 424/1000 | Loss: 0.00001690
Iteration 425/1000 | Loss: 0.00001690
Iteration 426/1000 | Loss: 0.00001690
Iteration 427/1000 | Loss: 0.00001690
Iteration 428/1000 | Loss: 0.00001690
Iteration 429/1000 | Loss: 0.00001690
Iteration 430/1000 | Loss: 0.00001690
Iteration 431/1000 | Loss: 0.00001690
Iteration 432/1000 | Loss: 0.00001690
Iteration 433/1000 | Loss: 0.00001690
Iteration 434/1000 | Loss: 0.00001690
Iteration 435/1000 | Loss: 0.00001690
Iteration 436/1000 | Loss: 0.00001690
Iteration 437/1000 | Loss: 0.00001690
Iteration 438/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 438. Stopping optimization.
Last 5 losses: [1.6898149624466896e-05, 1.6898149624466896e-05, 1.6898149624466896e-05, 1.6898149624466896e-05, 1.6898149624466896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6898149624466896e-05

Optimization complete. Final v2v error: 3.4408788681030273 mm

Highest mean error: 4.393189907073975 mm for frame 58

Lowest mean error: 2.90649676322937 mm for frame 6

Saving results

Total time: 424.9889323711395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0012/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0012. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0007/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0007. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0003/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0003. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0018/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00098510
Iteration 2/1000 | Loss: 0.00020394
Iteration 3/1000 | Loss: 0.00016023
Iteration 4/1000 | Loss: 0.00014717
Iteration 5/1000 | Loss: 0.00013506
Iteration 6/1000 | Loss: 0.00012360
Iteration 7/1000 | Loss: 0.00011266
Iteration 8/1000 | Loss: 0.00010518
Iteration 9/1000 | Loss: 0.00009731
Iteration 10/1000 | Loss: 0.00042947
Iteration 11/1000 | Loss: 0.00026943
Iteration 12/1000 | Loss: 0.00039506
Iteration 13/1000 | Loss: 0.00028270
Iteration 14/1000 | Loss: 0.00010299
Iteration 15/1000 | Loss: 0.00025724
Iteration 16/1000 | Loss: 0.00010515
Iteration 17/1000 | Loss: 0.00007740
Iteration 18/1000 | Loss: 0.00023644
Iteration 19/1000 | Loss: 0.00007644
Iteration 20/1000 | Loss: 0.00023820
Iteration 21/1000 | Loss: 0.00027175
Iteration 22/1000 | Loss: 0.00062606
Iteration 23/1000 | Loss: 0.00056931
Iteration 24/1000 | Loss: 0.00052936
Iteration 25/1000 | Loss: 0.00015277
Iteration 26/1000 | Loss: 0.00011031
Iteration 27/1000 | Loss: 0.00008217
Iteration 28/1000 | Loss: 0.00006489
Iteration 29/1000 | Loss: 0.00023164
Iteration 30/1000 | Loss: 0.00006627
Iteration 31/1000 | Loss: 0.00005537
Iteration 32/1000 | Loss: 0.00005039
Iteration 33/1000 | Loss: 0.00004574
Iteration 34/1000 | Loss: 0.00022265
Iteration 35/1000 | Loss: 0.00004921
Iteration 36/1000 | Loss: 0.00004269
Iteration 37/1000 | Loss: 0.00055884
Iteration 38/1000 | Loss: 0.00008037
Iteration 39/1000 | Loss: 0.00022897
Iteration 40/1000 | Loss: 0.00015138
Iteration 41/1000 | Loss: 0.00004036
Iteration 42/1000 | Loss: 0.00020369
Iteration 43/1000 | Loss: 0.00008458
Iteration 44/1000 | Loss: 0.00017087
Iteration 45/1000 | Loss: 0.00005171
Iteration 46/1000 | Loss: 0.00011477
Iteration 47/1000 | Loss: 0.00020428
Iteration 48/1000 | Loss: 0.00004491
Iteration 49/1000 | Loss: 0.00003297
Iteration 50/1000 | Loss: 0.00002916
Iteration 51/1000 | Loss: 0.00002679
Iteration 52/1000 | Loss: 0.00002578
Iteration 53/1000 | Loss: 0.00002507
Iteration 54/1000 | Loss: 0.00002462
Iteration 55/1000 | Loss: 0.00002429
Iteration 56/1000 | Loss: 0.00002402
Iteration 57/1000 | Loss: 0.00002394
Iteration 58/1000 | Loss: 0.00002387
Iteration 59/1000 | Loss: 0.00002380
Iteration 60/1000 | Loss: 0.00002379
Iteration 61/1000 | Loss: 0.00002378
Iteration 62/1000 | Loss: 0.00002378
Iteration 63/1000 | Loss: 0.00002377
Iteration 64/1000 | Loss: 0.00002377
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002375
Iteration 67/1000 | Loss: 0.00002374
Iteration 68/1000 | Loss: 0.00002374
Iteration 69/1000 | Loss: 0.00002373
Iteration 70/1000 | Loss: 0.00002372
Iteration 71/1000 | Loss: 0.00002372
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002368
Iteration 74/1000 | Loss: 0.00002367
Iteration 75/1000 | Loss: 0.00002367
Iteration 76/1000 | Loss: 0.00002366
Iteration 77/1000 | Loss: 0.00002366
Iteration 78/1000 | Loss: 0.00002365
Iteration 79/1000 | Loss: 0.00002365
Iteration 80/1000 | Loss: 0.00002365
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002362
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002362
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002362
Iteration 91/1000 | Loss: 0.00002361
Iteration 92/1000 | Loss: 0.00002361
Iteration 93/1000 | Loss: 0.00002361
Iteration 94/1000 | Loss: 0.00002361
Iteration 95/1000 | Loss: 0.00002361
Iteration 96/1000 | Loss: 0.00002361
Iteration 97/1000 | Loss: 0.00002361
Iteration 98/1000 | Loss: 0.00002361
Iteration 99/1000 | Loss: 0.00002360
Iteration 100/1000 | Loss: 0.00002360
Iteration 101/1000 | Loss: 0.00002360
Iteration 102/1000 | Loss: 0.00002360
Iteration 103/1000 | Loss: 0.00002360
Iteration 104/1000 | Loss: 0.00002360
Iteration 105/1000 | Loss: 0.00002359
Iteration 106/1000 | Loss: 0.00002359
Iteration 107/1000 | Loss: 0.00002359
Iteration 108/1000 | Loss: 0.00002359
Iteration 109/1000 | Loss: 0.00002359
Iteration 110/1000 | Loss: 0.00002359
Iteration 111/1000 | Loss: 0.00002359
Iteration 112/1000 | Loss: 0.00002359
Iteration 113/1000 | Loss: 0.00002358
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00002358
Iteration 116/1000 | Loss: 0.00002358
Iteration 117/1000 | Loss: 0.00002358
Iteration 118/1000 | Loss: 0.00002358
Iteration 119/1000 | Loss: 0.00002358
Iteration 120/1000 | Loss: 0.00002357
Iteration 121/1000 | Loss: 0.00002357
Iteration 122/1000 | Loss: 0.00002357
Iteration 123/1000 | Loss: 0.00002357
Iteration 124/1000 | Loss: 0.00002357
Iteration 125/1000 | Loss: 0.00002357
Iteration 126/1000 | Loss: 0.00002357
Iteration 127/1000 | Loss: 0.00002357
Iteration 128/1000 | Loss: 0.00002357
Iteration 129/1000 | Loss: 0.00002357
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002357
Iteration 136/1000 | Loss: 0.00002357
Iteration 137/1000 | Loss: 0.00002357
Iteration 138/1000 | Loss: 0.00002357
Iteration 139/1000 | Loss: 0.00002357
Iteration 140/1000 | Loss: 0.00002357
Iteration 141/1000 | Loss: 0.00002357
Iteration 142/1000 | Loss: 0.00002356
Iteration 143/1000 | Loss: 0.00002356
Iteration 144/1000 | Loss: 0.00002356
Iteration 145/1000 | Loss: 0.00002356
Iteration 146/1000 | Loss: 0.00002356
Iteration 147/1000 | Loss: 0.00002356
Iteration 148/1000 | Loss: 0.00002356
Iteration 149/1000 | Loss: 0.00002356
Iteration 150/1000 | Loss: 0.00002356
Iteration 151/1000 | Loss: 0.00002356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.3564665752928704e-05, 2.3564665752928704e-05, 2.3564665752928704e-05, 2.3564665752928704e-05, 2.3564665752928704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3564665752928704e-05

Optimization complete. Final v2v error: 3.944929599761963 mm

Highest mean error: 11.81348991394043 mm for frame 119

Lowest mean error: 3.620216131210327 mm for frame 41

Saving results

Total time: 133.2473373413086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084029
Iteration 2/25 | Loss: 0.00224394
Iteration 3/25 | Loss: 0.00177706
Iteration 4/25 | Loss: 0.00167256
Iteration 5/25 | Loss: 0.00155419
Iteration 6/25 | Loss: 0.00150245
Iteration 7/25 | Loss: 0.00131753
Iteration 8/25 | Loss: 0.00117184
Iteration 9/25 | Loss: 0.00100024
Iteration 10/25 | Loss: 0.00090908
Iteration 11/25 | Loss: 0.00085712
Iteration 12/25 | Loss: 0.00084258
Iteration 13/25 | Loss: 0.00082813
Iteration 14/25 | Loss: 0.00081796
Iteration 15/25 | Loss: 0.00081931
Iteration 16/25 | Loss: 0.00081831
Iteration 17/25 | Loss: 0.00081554
Iteration 18/25 | Loss: 0.00081235
Iteration 19/25 | Loss: 0.00081124
Iteration 20/25 | Loss: 0.00081088
Iteration 21/25 | Loss: 0.00081056
Iteration 22/25 | Loss: 0.00081024
Iteration 23/25 | Loss: 0.00080851
Iteration 24/25 | Loss: 0.00080705
Iteration 25/25 | Loss: 0.00081140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/1000 | Loss: 0.00365068
Iteration 2/1000 | Loss: 0.00158421
Iteration 3/1000 | Loss: 0.00091517
Iteration 4/1000 | Loss: 0.00203215
Iteration 5/1000 | Loss: 0.00215083
Iteration 6/1000 | Loss: 0.00162339
Iteration 7/1000 | Loss: 0.00184253
Iteration 8/1000 | Loss: 0.00135917
Iteration 9/1000 | Loss: 0.00131353
Iteration 10/1000 | Loss: 0.00162006
Iteration 11/1000 | Loss: 0.00030695
Iteration 12/1000 | Loss: 0.00046349
Iteration 13/1000 | Loss: 0.00233373
Iteration 14/1000 | Loss: 0.00090907
Iteration 15/1000 | Loss: 0.00247672
Iteration 16/1000 | Loss: 0.00218470
Iteration 17/1000 | Loss: 0.00101912
Iteration 18/1000 | Loss: 0.00165470
Iteration 19/1000 | Loss: 0.00098703
Iteration 20/1000 | Loss: 0.00023937
Iteration 21/1000 | Loss: 0.00059516
Iteration 22/1000 | Loss: 0.00017514
Iteration 23/1000 | Loss: 0.00044324
Iteration 24/1000 | Loss: 0.00036865
Iteration 25/1000 | Loss: 0.00050988
Iteration 26/1000 | Loss: 0.00034807
Iteration 27/1000 | Loss: 0.00052528
Iteration 28/1000 | Loss: 0.00014412
Iteration 29/1000 | Loss: 0.00089333
Iteration 30/1000 | Loss: 0.00105033
Iteration 31/1000 | Loss: 0.00098766
Iteration 32/1000 | Loss: 0.00035822
Iteration 33/1000 | Loss: 0.00019520
Iteration 34/1000 | Loss: 0.00037354
Iteration 35/1000 | Loss: 0.00109927
Iteration 36/1000 | Loss: 0.00075738
Iteration 37/1000 | Loss: 0.00055917
Iteration 38/1000 | Loss: 0.00089303
Iteration 39/1000 | Loss: 0.00069982
Iteration 40/1000 | Loss: 0.00059464
Iteration 41/1000 | Loss: 0.00100451
Iteration 42/1000 | Loss: 0.00053694
Iteration 43/1000 | Loss: 0.00080103
Iteration 44/1000 | Loss: 0.00015753
Iteration 45/1000 | Loss: 0.00019752
Iteration 46/1000 | Loss: 0.00109125
Iteration 47/1000 | Loss: 0.00051315
Iteration 48/1000 | Loss: 0.00059813
Iteration 49/1000 | Loss: 0.00019718
Iteration 50/1000 | Loss: 0.00015328
Iteration 51/1000 | Loss: 0.00011133
Iteration 52/1000 | Loss: 0.00007668
Iteration 53/1000 | Loss: 0.00016631
Iteration 54/1000 | Loss: 0.00019637
Iteration 55/1000 | Loss: 0.00012393
Iteration 56/1000 | Loss: 0.00006750
Iteration 57/1000 | Loss: 0.00011914
Iteration 58/1000 | Loss: 0.00019794
Iteration 59/1000 | Loss: 0.00011669
Iteration 60/1000 | Loss: 0.00011750
Iteration 61/1000 | Loss: 0.00015062
Iteration 62/1000 | Loss: 0.00010456
Iteration 63/1000 | Loss: 0.00008233
Iteration 64/1000 | Loss: 0.00011256
Iteration 65/1000 | Loss: 0.00010943
Iteration 66/1000 | Loss: 0.00011331
Iteration 67/1000 | Loss: 0.00010079
Iteration 68/1000 | Loss: 0.00018131
Iteration 69/1000 | Loss: 0.00013583
Iteration 70/1000 | Loss: 0.00015993
Iteration 71/1000 | Loss: 0.00010580
Iteration 72/1000 | Loss: 0.00007182
Iteration 73/1000 | Loss: 0.00009607
Iteration 74/1000 | Loss: 0.00010706
Iteration 75/1000 | Loss: 0.00009796
Iteration 76/1000 | Loss: 0.00009696
Iteration 77/1000 | Loss: 0.00009459
Iteration 78/1000 | Loss: 0.00009467
Iteration 79/1000 | Loss: 0.00033445
Iteration 80/1000 | Loss: 0.00054309
Iteration 81/1000 | Loss: 0.00026603
Iteration 82/1000 | Loss: 0.00023921
Iteration 83/1000 | Loss: 0.00039508
Iteration 84/1000 | Loss: 0.00008545
Iteration 85/1000 | Loss: 0.00008506
Iteration 86/1000 | Loss: 0.00007251
Iteration 87/1000 | Loss: 0.00012369
Iteration 88/1000 | Loss: 0.00007605
Iteration 89/1000 | Loss: 0.00008072
Iteration 90/1000 | Loss: 0.00013233
Iteration 91/1000 | Loss: 0.00014924
Iteration 92/1000 | Loss: 0.00011675
Iteration 93/1000 | Loss: 0.00008946
Iteration 94/1000 | Loss: 0.00007926
Iteration 95/1000 | Loss: 0.00009747
Iteration 96/1000 | Loss: 0.00008678
Iteration 97/1000 | Loss: 0.00009950
Iteration 98/1000 | Loss: 0.00010534
Iteration 99/1000 | Loss: 0.00011810
Iteration 100/1000 | Loss: 0.00010655
Iteration 101/1000 | Loss: 0.00011165
Iteration 102/1000 | Loss: 0.00009556
Iteration 103/1000 | Loss: 0.00009886
Iteration 104/1000 | Loss: 0.00009875
Iteration 105/1000 | Loss: 0.00007121
Iteration 106/1000 | Loss: 0.00011360
Iteration 107/1000 | Loss: 0.00009103
Iteration 108/1000 | Loss: 0.00008619
Iteration 109/1000 | Loss: 0.00010210
Iteration 110/1000 | Loss: 0.00010672
Iteration 111/1000 | Loss: 0.00009738
Iteration 112/1000 | Loss: 0.00011429
Iteration 113/1000 | Loss: 0.00009756
Iteration 114/1000 | Loss: 0.00012027
Iteration 115/1000 | Loss: 0.00009935
Iteration 116/1000 | Loss: 0.00010227
Iteration 117/1000 | Loss: 0.00009394
Iteration 118/1000 | Loss: 0.00009653
Iteration 119/1000 | Loss: 0.00009641
Iteration 120/1000 | Loss: 0.00010070
Iteration 121/1000 | Loss: 0.00009159
Iteration 122/1000 | Loss: 0.00006574
Iteration 123/1000 | Loss: 0.00007706
Iteration 124/1000 | Loss: 0.00014148
Iteration 125/1000 | Loss: 0.00010902
Iteration 126/1000 | Loss: 0.00009645
Iteration 127/1000 | Loss: 0.00008415
Iteration 128/1000 | Loss: 0.00009318
Iteration 129/1000 | Loss: 0.00009143
Iteration 130/1000 | Loss: 0.00009288
Iteration 131/1000 | Loss: 0.00025098
Iteration 132/1000 | Loss: 0.00013371
Iteration 133/1000 | Loss: 0.00013663
Iteration 134/1000 | Loss: 0.00010640
Iteration 135/1000 | Loss: 0.00010834
Iteration 136/1000 | Loss: 0.00009289
Iteration 137/1000 | Loss: 0.00015248
Iteration 138/1000 | Loss: 0.00010003
Iteration 139/1000 | Loss: 0.00009232
Iteration 140/1000 | Loss: 0.00009697
Iteration 141/1000 | Loss: 0.00009137
Iteration 142/1000 | Loss: 0.00009739
Iteration 143/1000 | Loss: 0.00004845
Iteration 144/1000 | Loss: 0.00011006
Iteration 145/1000 | Loss: 0.00004094
Iteration 146/1000 | Loss: 0.00007200
Iteration 147/1000 | Loss: 0.00005018
Iteration 148/1000 | Loss: 0.00004318
Iteration 149/1000 | Loss: 0.00004286
Iteration 150/1000 | Loss: 0.00003850
Iteration 151/1000 | Loss: 0.00003184
Iteration 152/1000 | Loss: 0.00003185
Iteration 153/1000 | Loss: 0.00003889
Iteration 154/1000 | Loss: 0.00003855
Iteration 155/1000 | Loss: 0.00004245
Iteration 156/1000 | Loss: 0.00003751
Iteration 157/1000 | Loss: 0.00004202
Iteration 158/1000 | Loss: 0.00003728
Iteration 159/1000 | Loss: 0.00003005
Iteration 160/1000 | Loss: 0.00003076
Iteration 161/1000 | Loss: 0.00003038
Iteration 162/1000 | Loss: 0.00003914
Iteration 163/1000 | Loss: 0.00003590
Iteration 164/1000 | Loss: 0.00003630
Iteration 165/1000 | Loss: 0.00003786
Iteration 166/1000 | Loss: 0.00003767
Iteration 167/1000 | Loss: 0.00047332
Iteration 168/1000 | Loss: 0.00003299
Iteration 169/1000 | Loss: 0.00004348
Iteration 170/1000 | Loss: 0.00009899
Iteration 171/1000 | Loss: 0.00004228
Iteration 172/1000 | Loss: 0.00003956
Iteration 173/1000 | Loss: 0.00003612
Iteration 174/1000 | Loss: 0.00003690
Iteration 175/1000 | Loss: 0.00003689
Iteration 176/1000 | Loss: 0.00002667
Iteration 177/1000 | Loss: 0.00004212
Iteration 178/1000 | Loss: 0.00003710
Iteration 179/1000 | Loss: 0.00004309
Iteration 180/1000 | Loss: 0.00003815
Iteration 181/1000 | Loss: 0.00004437
Iteration 182/1000 | Loss: 0.00002655
Iteration 183/1000 | Loss: 0.00003700
Iteration 184/1000 | Loss: 0.00003845
Iteration 185/1000 | Loss: 0.00004238
Iteration 186/1000 | Loss: 0.00003342
Iteration 187/1000 | Loss: 0.00003350
Iteration 188/1000 | Loss: 0.00003324
Iteration 189/1000 | Loss: 0.00003761
Iteration 190/1000 | Loss: 0.00003631
Iteration 191/1000 | Loss: 0.00004752
Iteration 192/1000 | Loss: 0.00006192
Iteration 193/1000 | Loss: 0.00004560
Iteration 194/1000 | Loss: 0.00003162
Iteration 195/1000 | Loss: 0.00003739
Iteration 196/1000 | Loss: 0.00003045
Iteration 197/1000 | Loss: 0.00003560
Iteration 198/1000 | Loss: 0.00004071
Iteration 199/1000 | Loss: 0.00003945
Iteration 200/1000 | Loss: 0.00003772
Iteration 201/1000 | Loss: 0.00003610
Iteration 202/1000 | Loss: 0.00003766
Iteration 203/1000 | Loss: 0.00003987
Iteration 204/1000 | Loss: 0.00003771
Iteration 205/1000 | Loss: 0.00003336
Iteration 206/1000 | Loss: 0.00003529
Iteration 207/1000 | Loss: 0.00004296
Iteration 208/1000 | Loss: 0.00003512
Iteration 209/1000 | Loss: 0.00002994
Iteration 210/1000 | Loss: 0.00003703
Iteration 211/1000 | Loss: 0.00003572
Iteration 212/1000 | Loss: 0.00005765
Iteration 213/1000 | Loss: 0.00006675
Iteration 214/1000 | Loss: 0.00005568
Iteration 215/1000 | Loss: 0.00002328
Iteration 216/1000 | Loss: 0.00004072
Iteration 217/1000 | Loss: 0.00006322
Iteration 218/1000 | Loss: 0.00001950
Iteration 219/1000 | Loss: 0.00001909
Iteration 220/1000 | Loss: 0.00005377
Iteration 221/1000 | Loss: 0.00001875
Iteration 222/1000 | Loss: 0.00001866
Iteration 223/1000 | Loss: 0.00001866
Iteration 224/1000 | Loss: 0.00003508
Iteration 225/1000 | Loss: 0.00001868
Iteration 226/1000 | Loss: 0.00001860
Iteration 227/1000 | Loss: 0.00001858
Iteration 228/1000 | Loss: 0.00001857
Iteration 229/1000 | Loss: 0.00001857
Iteration 230/1000 | Loss: 0.00001857
Iteration 231/1000 | Loss: 0.00001857
Iteration 232/1000 | Loss: 0.00001856
Iteration 233/1000 | Loss: 0.00001856
Iteration 234/1000 | Loss: 0.00001856
Iteration 235/1000 | Loss: 0.00001856
Iteration 236/1000 | Loss: 0.00001856
Iteration 237/1000 | Loss: 0.00001856
Iteration 238/1000 | Loss: 0.00001855
Iteration 239/1000 | Loss: 0.00001855
Iteration 240/1000 | Loss: 0.00001855
Iteration 241/1000 | Loss: 0.00001855
Iteration 242/1000 | Loss: 0.00001855
Iteration 243/1000 | Loss: 0.00001855
Iteration 244/1000 | Loss: 0.00001855
Iteration 245/1000 | Loss: 0.00001855
Iteration 246/1000 | Loss: 0.00001854
Iteration 247/1000 | Loss: 0.00001854
Iteration 248/1000 | Loss: 0.00001854
Iteration 249/1000 | Loss: 0.00001854
Iteration 250/1000 | Loss: 0.00001854
Iteration 251/1000 | Loss: 0.00001854
Iteration 252/1000 | Loss: 0.00001853
Iteration 253/1000 | Loss: 0.00001853
Iteration 254/1000 | Loss: 0.00001852
Iteration 255/1000 | Loss: 0.00001852
Iteration 256/1000 | Loss: 0.00001852
Iteration 257/1000 | Loss: 0.00001852
Iteration 258/1000 | Loss: 0.00001852
Iteration 259/1000 | Loss: 0.00001851
Iteration 260/1000 | Loss: 0.00001851
Iteration 261/1000 | Loss: 0.00001851
Iteration 262/1000 | Loss: 0.00001850
Iteration 263/1000 | Loss: 0.00004928
Iteration 264/1000 | Loss: 0.00002739
Iteration 265/1000 | Loss: 0.00003634
Iteration 266/1000 | Loss: 0.00003634
Iteration 267/1000 | Loss: 0.00016185
Iteration 268/1000 | Loss: 0.00001909
Iteration 269/1000 | Loss: 0.00004775
Iteration 270/1000 | Loss: 0.00001848
Iteration 271/1000 | Loss: 0.00001837
Iteration 272/1000 | Loss: 0.00001835
Iteration 273/1000 | Loss: 0.00001835
Iteration 274/1000 | Loss: 0.00001835
Iteration 275/1000 | Loss: 0.00001834
Iteration 276/1000 | Loss: 0.00001834
Iteration 277/1000 | Loss: 0.00001834
Iteration 278/1000 | Loss: 0.00001833
Iteration 279/1000 | Loss: 0.00001833
Iteration 280/1000 | Loss: 0.00001833
Iteration 281/1000 | Loss: 0.00001833
Iteration 282/1000 | Loss: 0.00001832
Iteration 283/1000 | Loss: 0.00001832
Iteration 284/1000 | Loss: 0.00001832
Iteration 285/1000 | Loss: 0.00001832
Iteration 286/1000 | Loss: 0.00001831
Iteration 287/1000 | Loss: 0.00001831
Iteration 288/1000 | Loss: 0.00001831
Iteration 289/1000 | Loss: 0.00001831
Iteration 290/1000 | Loss: 0.00001831
Iteration 291/1000 | Loss: 0.00001831
Iteration 292/1000 | Loss: 0.00001831
Iteration 293/1000 | Loss: 0.00001830
Iteration 294/1000 | Loss: 0.00001830
Iteration 295/1000 | Loss: 0.00001830
Iteration 296/1000 | Loss: 0.00001830
Iteration 297/1000 | Loss: 0.00001830
Iteration 298/1000 | Loss: 0.00001830
Iteration 299/1000 | Loss: 0.00001830
Iteration 300/1000 | Loss: 0.00001830
Iteration 301/1000 | Loss: 0.00001830
Iteration 302/1000 | Loss: 0.00001830
Iteration 303/1000 | Loss: 0.00001830
Iteration 304/1000 | Loss: 0.00001829
Iteration 305/1000 | Loss: 0.00001829
Iteration 306/1000 | Loss: 0.00001829
Iteration 307/1000 | Loss: 0.00001829
Iteration 308/1000 | Loss: 0.00001829
Iteration 309/1000 | Loss: 0.00001829
Iteration 310/1000 | Loss: 0.00001829
Iteration 311/1000 | Loss: 0.00001829
Iteration 312/1000 | Loss: 0.00001829
Iteration 313/1000 | Loss: 0.00001829
Iteration 314/1000 | Loss: 0.00001829
Iteration 315/1000 | Loss: 0.00001829
Iteration 316/1000 | Loss: 0.00001829
Iteration 317/1000 | Loss: 0.00001829
Iteration 318/1000 | Loss: 0.00001829
Iteration 319/1000 | Loss: 0.00001829
Iteration 320/1000 | Loss: 0.00001828
Iteration 321/1000 | Loss: 0.00001828
Iteration 322/1000 | Loss: 0.00001828
Iteration 323/1000 | Loss: 0.00001828
Iteration 324/1000 | Loss: 0.00001828
Iteration 325/1000 | Loss: 0.00001828
Iteration 326/1000 | Loss: 0.00001828
Iteration 327/1000 | Loss: 0.00001828
Iteration 328/1000 | Loss: 0.00001828
Iteration 329/1000 | Loss: 0.00001828
Iteration 330/1000 | Loss: 0.00001828
Iteration 331/1000 | Loss: 0.00001828
Iteration 332/1000 | Loss: 0.00001828
Iteration 333/1000 | Loss: 0.00001828
Iteration 334/1000 | Loss: 0.00001828
Iteration 335/1000 | Loss: 0.00001828
Iteration 336/1000 | Loss: 0.00001828
Iteration 337/1000 | Loss: 0.00001828
Iteration 338/1000 | Loss: 0.00001827
Iteration 339/1000 | Loss: 0.00001827
Iteration 340/1000 | Loss: 0.00001827
Iteration 341/1000 | Loss: 0.00001827
Iteration 342/1000 | Loss: 0.00001827
Iteration 343/1000 | Loss: 0.00001827
Iteration 344/1000 | Loss: 0.00001827
Iteration 345/1000 | Loss: 0.00001827
Iteration 346/1000 | Loss: 0.00001827
Iteration 347/1000 | Loss: 0.00001827
Iteration 348/1000 | Loss: 0.00001827
Iteration 349/1000 | Loss: 0.00001826
Iteration 350/1000 | Loss: 0.00001826
Iteration 351/1000 | Loss: 0.00001826
Iteration 352/1000 | Loss: 0.00001826
Iteration 353/1000 | Loss: 0.00001826
Iteration 354/1000 | Loss: 0.00001826
Iteration 355/1000 | Loss: 0.00001826
Iteration 356/1000 | Loss: 0.00001826
Iteration 357/1000 | Loss: 0.00001826
Iteration 358/1000 | Loss: 0.00001826
Iteration 359/1000 | Loss: 0.00001826
Iteration 360/1000 | Loss: 0.00001826
Iteration 361/1000 | Loss: 0.00001825
Iteration 362/1000 | Loss: 0.00001825
Iteration 363/1000 | Loss: 0.00001825
Iteration 364/1000 | Loss: 0.00001825
Iteration 365/1000 | Loss: 0.00001825
Iteration 366/1000 | Loss: 0.00001825
Iteration 367/1000 | Loss: 0.00001825
Iteration 368/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [1.8254486349178478e-05, 1.8254486349178478e-05, 1.8254486349178478e-05, 1.8254486349178478e-05, 1.8254486349178478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8254486349178478e-05

Optimization complete. Final v2v error: 3.6992321014404297 mm

Highest mean error: 4.5885701179504395 mm for frame 91

Lowest mean error: 3.229365587234497 mm for frame 0

Saving results

Total time: 422.44985461235046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 1.42287767
Iteration 2/25 | Loss: 0.00095820
Iteration 3/25 | Loss: 0.00095820
Iteration 4/25 | Loss: 0.00095820
Iteration 5/25 | Loss: 0.00095820
Iteration 6/25 | Loss: 0.00095820
Iteration 7/25 | Loss: 0.00095820
Iteration 8/25 | Loss: 0.00095820
Iteration 9/25 | Loss: 0.00095820
Iteration 10/25 | Loss: 0.00095820
Iteration 11/25 | Loss: 0.00095820
Iteration 12/25 | Loss: 0.00095820
Iteration 13/25 | Loss: 0.00095820
Iteration 14/25 | Loss: 0.00095820
Iteration 15/25 | Loss: 0.00095820
Iteration 16/25 | Loss: 0.00095820
Iteration 17/25 | Loss: 0.00095820
Iteration 18/25 | Loss: 0.00095820
Iteration 19/25 | Loss: 0.00095820
Iteration 20/25 | Loss: 0.00095820
Iteration 21/25 | Loss: 0.00095820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009582014754414558, 0.0009582014754414558, 0.0009582014754414558, 0.0009582014754414558, 0.0009582014754414558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009582014754414558

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.01124881
Iteration 2/25 | Loss: 0.01124881
Iteration 3/25 | Loss: 0.01124881
Iteration 4/25 | Loss: 0.01124881
Iteration 5/25 | Loss: 0.01124881
Iteration 6/25 | Loss: 0.01124881
Iteration 7/25 | Loss: 0.01124880
Iteration 8/25 | Loss: 0.01124880
Iteration 9/25 | Loss: 0.01124880
Iteration 10/25 | Loss: 0.01124880
Iteration 11/25 | Loss: 0.01124880
Iteration 12/25 | Loss: 0.01124880
Iteration 13/25 | Loss: 0.01124880
Iteration 14/25 | Loss: 0.01124880
Iteration 15/25 | Loss: 0.01124880
Iteration 16/25 | Loss: 0.01124880
Iteration 17/25 | Loss: 0.01124879
Iteration 18/25 | Loss: 0.01124879
Iteration 19/25 | Loss: 0.01124879
Iteration 20/25 | Loss: 0.01124879
Iteration 21/25 | Loss: 0.01124879
Iteration 22/25 | Loss: 0.01124879
Iteration 23/25 | Loss: 0.01124879
Iteration 24/25 | Loss: 0.01124879
Iteration 25/25 | Loss: 0.01124879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.01017799
Iteration 2/25 | Loss: 0.00146391
Iteration 3/25 | Loss: 0.00092392
Iteration 4/25 | Loss: 0.00073310
Iteration 5/25 | Loss: 0.00071536
Iteration 6/25 | Loss: 0.00070125
Iteration 7/25 | Loss: 0.00068844
Iteration 8/25 | Loss: 0.00068359
Iteration 9/25 | Loss: 0.00068809
Iteration 10/25 | Loss: 0.00068198
Iteration 11/25 | Loss: 0.00067925
Iteration 12/25 | Loss: 0.00067826
Iteration 13/25 | Loss: 0.00067457
Iteration 14/25 | Loss: 0.00067297
Iteration 15/25 | Loss: 0.00066831
Iteration 16/25 | Loss: 0.00066620
Iteration 17/25 | Loss: 0.00066620
Iteration 18/25 | Loss: 0.00066717
Iteration 19/25 | Loss: 0.00066639
Iteration 20/25 | Loss: 0.00066464
Iteration 21/25 | Loss: 0.00066422
Iteration 22/25 | Loss: 0.00066559
Iteration 23/25 | Loss: 0.00066489
Iteration 24/25 | Loss: 0.00066635
Iteration 25/25 | Loss: 0.00066561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98348236
Iteration 2/25 | Loss: 0.05576997
Iteration 3/25 | Loss: 0.05575813
Iteration 4/25 | Loss: 0.05575800
Iteration 5/25 | Loss: 0.05575798
Iteration 6/25 | Loss: 0.05575798
Iteration 7/25 | Loss: 0.05575798
Iteration 8/25 | Loss: 0.05575798
Iteration 9/25 | Loss: 0.05575798
Iteration 10/25 | Loss: 0.05575798
Iteration 11/25 | Loss: 0.05575798
Iteration 12/25 | Loss: 0.05575798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.055757977068424225, 0.055757977068424225, 0.055757977068424225, 0.055757977068424225, 0.055757977068424225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.055757977068424225

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 1.54521334
Iteration 2/25 | Loss: 0.00020921
Iteration 3/25 | Loss: 0.00020921
Iteration 4/25 | Loss: 0.00020921
Iteration 5/25 | Loss: 0.00020921
Iteration 6/25 | Loss: 0.00020921
Iteration 7/25 | Loss: 0.00020921
Iteration 8/25 | Loss: 0.00020921
Iteration 9/25 | Loss: 0.00020921
Iteration 10/25 | Loss: 0.00020921
Iteration 11/25 | Loss: 0.00020921
Iteration 12/25 | Loss: 0.00020921
Iteration 13/25 | Loss: 0.00020921
Iteration 14/25 | Loss: 0.00020921
Iteration 15/25 | Loss: 0.00020921
Iteration 16/25 | Loss: 0.00020921
Iteration 17/25 | Loss: 0.00020921
Iteration 18/25 | Loss: 0.00020921
Iteration 19/25 | Loss: 0.00020921
Iteration 20/25 | Loss: 0.00020921
Iteration 21/25 | Loss: 0.00020921
Iteration 22/25 | Loss: 0.00020921
Iteration 23/25 | Loss: 0.00020921
Iteration 24/25 | Loss: 0.00020921
Iteration 25/25 | Loss: 0.00020921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095820
Iteration 2/1000 | Loss: 0.00020149
Iteration 3/1000 | Loss: 0.00015929
Iteration 4/1000 | Loss: 0.00014274
Iteration 5/1000 | Loss: 0.00013210
Iteration 6/1000 | Loss: 0.00012106
Iteration 7/1000 | Loss: 0.00010927
Iteration 8/1000 | Loss: 0.00010149
Iteration 9/1000 | Loss: 0.00009384
Iteration 10/1000 | Loss: 0.00042427
Iteration 11/1000 | Loss: 0.00023015
Iteration 12/1000 | Loss: 0.00008939
Iteration 13/1000 | Loss: 0.00021741
Iteration 14/1000 | Loss: 0.00007970
Iteration 15/1000 | Loss: 0.00007307
Iteration 16/1000 | Loss: 0.00006802
Iteration 17/1000 | Loss: 0.00082275
Iteration 18/1000 | Loss: 0.00060391
Iteration 19/1000 | Loss: 0.00044833
Iteration 20/1000 | Loss: 0.00045688
Iteration 21/1000 | Loss: 0.00027762
Iteration 22/1000 | Loss: 0.00010138
Iteration 23/1000 | Loss: 0.00007210
Iteration 24/1000 | Loss: 0.00005839
Iteration 25/1000 | Loss: 0.00019100
Iteration 26/1000 | Loss: 0.00005488
Iteration 27/1000 | Loss: 0.00018115
Iteration 28/1000 | Loss: 0.00005605
Iteration 29/1000 | Loss: 0.00037819
Iteration 30/1000 | Loss: 0.00022872
Iteration 31/1000 | Loss: 0.00005637
Iteration 32/1000 | Loss: 0.00004418
Iteration 33/1000 | Loss: 0.00003741
Iteration 34/1000 | Loss: 0.00003263
Iteration 35/1000 | Loss: 0.00003065
Iteration 36/1000 | Loss: 0.00020396
Iteration 37/1000 | Loss: 0.00003767
Iteration 38/1000 | Loss: 0.00003048
Iteration 39/1000 | Loss: 0.00002772
Iteration 40/1000 | Loss: 0.00002607
Iteration 41/1000 | Loss: 0.00002511
Iteration 42/1000 | Loss: 0.00002465
Iteration 43/1000 | Loss: 0.00002431
Iteration 44/1000 | Loss: 0.00002407
Iteration 45/1000 | Loss: 0.00002392
Iteration 46/1000 | Loss: 0.00002390
Iteration 47/1000 | Loss: 0.00002377
Iteration 48/1000 | Loss: 0.00002376
Iteration 49/1000 | Loss: 0.00002376
Iteration 50/1000 | Loss: 0.00002376
Iteration 51/1000 | Loss: 0.00002376
Iteration 52/1000 | Loss: 0.00002376
Iteration 53/1000 | Loss: 0.00002376
Iteration 54/1000 | Loss: 0.00002376
Iteration 55/1000 | Loss: 0.00002376
Iteration 56/1000 | Loss: 0.00002376
Iteration 57/1000 | Loss: 0.00002376
Iteration 58/1000 | Loss: 0.00002375
Iteration 59/1000 | Loss: 0.00002375
Iteration 60/1000 | Loss: 0.00002375
Iteration 61/1000 | Loss: 0.00002375
Iteration 62/1000 | Loss: 0.00002375
Iteration 63/1000 | Loss: 0.00002375
Iteration 64/1000 | Loss: 0.00002375
Iteration 65/1000 | Loss: 0.00002375
Iteration 66/1000 | Loss: 0.00002374
Iteration 67/1000 | Loss: 0.00002374
Iteration 68/1000 | Loss: 0.00002374
Iteration 69/1000 | Loss: 0.00002374
Iteration 70/1000 | Loss: 0.00002374
Iteration 71/1000 | Loss: 0.00002374
Iteration 72/1000 | Loss: 0.00002374
Iteration 73/1000 | Loss: 0.00002374
Iteration 74/1000 | Loss: 0.00002374
Iteration 75/1000 | Loss: 0.00002373
Iteration 76/1000 | Loss: 0.00002373
Iteration 77/1000 | Loss: 0.00002372
Iteration 78/1000 | Loss: 0.00002372
Iteration 79/1000 | Loss: 0.00002371
Iteration 80/1000 | Loss: 0.00002369
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002365
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002364
Iteration 85/1000 | Loss: 0.00002364
Iteration 86/1000 | Loss: 0.00002364
Iteration 87/1000 | Loss: 0.00002364
Iteration 88/1000 | Loss: 0.00002364
Iteration 89/1000 | Loss: 0.00002363
Iteration 90/1000 | Loss: 0.00002363
Iteration 91/1000 | Loss: 0.00002363
Iteration 92/1000 | Loss: 0.00002363
Iteration 93/1000 | Loss: 0.00002363
Iteration 94/1000 | Loss: 0.00002363
Iteration 95/1000 | Loss: 0.00002363
Iteration 96/1000 | Loss: 0.00002363
Iteration 97/1000 | Loss: 0.00002363
Iteration 98/1000 | Loss: 0.00002362
Iteration 99/1000 | Loss: 0.00002362
Iteration 100/1000 | Loss: 0.00002362
Iteration 101/1000 | Loss: 0.00002362
Iteration 102/1000 | Loss: 0.00002362
Iteration 103/1000 | Loss: 0.00002362
Iteration 104/1000 | Loss: 0.00002362
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002362
Iteration 107/1000 | Loss: 0.00002362
Iteration 108/1000 | Loss: 0.00002362
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002361
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002360
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002360
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002360
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002359
Iteration 127/1000 | Loss: 0.00002359
Iteration 128/1000 | Loss: 0.00002358
Iteration 129/1000 | Loss: 0.00002358
Iteration 130/1000 | Loss: 0.00002358
Iteration 131/1000 | Loss: 0.00002358
Iteration 132/1000 | Loss: 0.00002358
Iteration 133/1000 | Loss: 0.00002358
Iteration 134/1000 | Loss: 0.00002358
Iteration 135/1000 | Loss: 0.00002358
Iteration 136/1000 | Loss: 0.00002358
Iteration 137/1000 | Loss: 0.00002358
Iteration 138/1000 | Loss: 0.00002358
Iteration 139/1000 | Loss: 0.00002358
Iteration 140/1000 | Loss: 0.00002358
Iteration 141/1000 | Loss: 0.00002358
Iteration 142/1000 | Loss: 0.00002358
Iteration 143/1000 | Loss: 0.00002358
Iteration 144/1000 | Loss: 0.00002358
Iteration 145/1000 | Loss: 0.00002357
Iteration 146/1000 | Loss: 0.00002357
Iteration 147/1000 | Loss: 0.00002357
Iteration 148/1000 | Loss: 0.00002357
Iteration 149/1000 | Loss: 0.00002357
Iteration 150/1000 | Loss: 0.00002357
Iteration 151/1000 | Loss: 0.00002357
Iteration 152/1000 | Loss: 0.00002357
Iteration 153/1000 | Loss: 0.00002357
Iteration 154/1000 | Loss: 0.00002357
Iteration 155/1000 | Loss: 0.00002357
Iteration 156/1000 | Loss: 0.00002357
Iteration 157/1000 | Loss: 0.00002357
Iteration 158/1000 | Loss: 0.00002357
Iteration 159/1000 | Loss: 0.00002357
Iteration 160/1000 | Loss: 0.00002357
Iteration 161/1000 | Loss: 0.00002357
Iteration 162/1000 | Loss: 0.00002357
Iteration 163/1000 | Loss: 0.00002357
Iteration 164/1000 | Loss: 0.00002357
Iteration 165/1000 | Loss: 0.00002357
Iteration 166/1000 | Loss: 0.00002357
Iteration 167/1000 | Loss: 0.00002357
Iteration 168/1000 | Loss: 0.00002357
Iteration 169/1000 | Loss: 0.00002357
Iteration 170/1000 | Loss: 0.00002357
Iteration 171/1000 | Loss: 0.00002357
Iteration 172/1000 | Loss: 0.00002357
Iteration 173/1000 | Loss: 0.00002357
Iteration 174/1000 | Loss: 0.00002357
Iteration 175/1000 | Loss: 0.00002357
Iteration 176/1000 | Loss: 0.00002357
Iteration 177/1000 | Loss: 0.00002357
Iteration 178/1000 | Loss: 0.00002357
Iteration 179/1000 | Loss: 0.00002357
Iteration 180/1000 | Loss: 0.00002357
Iteration 181/1000 | Loss: 0.00002357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.3569433324155398e-05, 2.3569433324155398e-05, 2.3569433324155398e-05, 2.3569433324155398e-05, 2.3569433324155398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3569433324155398e-05

Optimization complete. Final v2v error: 3.945486068725586 mm

Highest mean error: 12.09470272064209 mm for frame 119

Lowest mean error: 3.6207733154296875 mm for frame 41

Saving results

Total time: 120.01860690116882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0019/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00020921
Iteration 2/1000 | Loss: 0.00017409
Iteration 3/1000 | Loss: 0.00013876
Iteration 4/1000 | Loss: 0.00011337
Iteration 5/1000 | Loss: 0.00008801
Iteration 6/1000 | Loss: 0.00003664
Iteration 7/1000 | Loss: 0.00008618
Iteration 8/1000 | Loss: 0.00008211
Iteration 9/1000 | Loss: 0.00008427
Iteration 10/1000 | Loss: 0.00007745
Iteration 11/1000 | Loss: 0.00008617
Iteration 12/1000 | Loss: 0.00008160
Iteration 13/1000 | Loss: 0.00010897
Iteration 14/1000 | Loss: 0.00014357
Iteration 15/1000 | Loss: 0.00012423
Iteration 16/1000 | Loss: 0.00009533
Iteration 17/1000 | Loss: 0.00017269
Iteration 18/1000 | Loss: 0.00009925
Iteration 19/1000 | Loss: 0.00011667
Iteration 20/1000 | Loss: 0.00007491
Iteration 21/1000 | Loss: 0.00009488
Iteration 22/1000 | Loss: 0.00008524
Iteration 23/1000 | Loss: 0.00008545
Iteration 24/1000 | Loss: 0.00014815
Iteration 25/1000 | Loss: 0.00011658
Iteration 26/1000 | Loss: 0.00010026
Iteration 27/1000 | Loss: 0.00015960
Iteration 28/1000 | Loss: 0.00003170
Iteration 29/1000 | Loss: 0.00002686
Iteration 30/1000 | Loss: 0.00002473
Iteration 31/1000 | Loss: 0.00002280
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002041
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00001983
Iteration 37/1000 | Loss: 0.00001970
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001941
Iteration 54/1000 | Loss: 0.00001940
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001938
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001929
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001926
Iteration 92/1000 | Loss: 0.00001926
Iteration 93/1000 | Loss: 0.00001926
Iteration 94/1000 | Loss: 0.00001926
Iteration 95/1000 | Loss: 0.00001926
Iteration 96/1000 | Loss: 0.00001926
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001924
Iteration 103/1000 | Loss: 0.00001924
Iteration 104/1000 | Loss: 0.00001924
Iteration 105/1000 | Loss: 0.00001923
Iteration 106/1000 | Loss: 0.00001923
Iteration 107/1000 | Loss: 0.00001923
Iteration 108/1000 | Loss: 0.00001922
Iteration 109/1000 | Loss: 0.00001922
Iteration 110/1000 | Loss: 0.00001922
Iteration 111/1000 | Loss: 0.00001922
Iteration 112/1000 | Loss: 0.00001922
Iteration 113/1000 | Loss: 0.00001922
Iteration 114/1000 | Loss: 0.00001922
Iteration 115/1000 | Loss: 0.00001922
Iteration 116/1000 | Loss: 0.00001922
Iteration 117/1000 | Loss: 0.00001921
Iteration 118/1000 | Loss: 0.00001921
Iteration 119/1000 | Loss: 0.00001921
Iteration 120/1000 | Loss: 0.00001921
Iteration 121/1000 | Loss: 0.00001921
Iteration 122/1000 | Loss: 0.00001921
Iteration 123/1000 | Loss: 0.00001921
Iteration 124/1000 | Loss: 0.00001921
Iteration 125/1000 | Loss: 0.00001921
Iteration 126/1000 | Loss: 0.00001921
Iteration 127/1000 | Loss: 0.00001920
Iteration 128/1000 | Loss: 0.00001920
Iteration 129/1000 | Loss: 0.00001920
Iteration 130/1000 | Loss: 0.00001920
Iteration 131/1000 | Loss: 0.00001920
Iteration 132/1000 | Loss: 0.00001920
Iteration 133/1000 | Loss: 0.00001919
Iteration 134/1000 | Loss: 0.00001919
Iteration 135/1000 | Loss: 0.00001919
Iteration 136/1000 | Loss: 0.00001918
Iteration 137/1000 | Loss: 0.00001918
Iteration 138/1000 | Loss: 0.00001918
Iteration 139/1000 | Loss: 0.00001918
Iteration 140/1000 | Loss: 0.00001918
Iteration 141/1000 | Loss: 0.00001918
Iteration 142/1000 | Loss: 0.00001918
Iteration 143/1000 | Loss: 0.00001917
Iteration 144/1000 | Loss: 0.00001917
Iteration 145/1000 | Loss: 0.00001917
Iteration 146/1000 | Loss: 0.00001917
Iteration 147/1000 | Loss: 0.00001917
Iteration 148/1000 | Loss: 0.00001917
Iteration 149/1000 | Loss: 0.00001917
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001917
Iteration 152/1000 | Loss: 0.00001916
Iteration 153/1000 | Loss: 0.00001916
Iteration 154/1000 | Loss: 0.00001916
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001915
Iteration 160/1000 | Loss: 0.00001915
Iteration 161/1000 | Loss: 0.00001915
Iteration 162/1000 | Loss: 0.00001915
Iteration 163/1000 | Loss: 0.00001915
Iteration 164/1000 | Loss: 0.00001915
Iteration 165/1000 | Loss: 0.00001915
Iteration 166/1000 | Loss: 0.00001915
Iteration 167/1000 | Loss: 0.00001914
Iteration 168/1000 | Loss: 0.00001914
Iteration 169/1000 | Loss: 0.00001914
Iteration 170/1000 | Loss: 0.00001914
Iteration 171/1000 | Loss: 0.00001914
Iteration 172/1000 | Loss: 0.00001914
Iteration 173/1000 | Loss: 0.00001914
Iteration 174/1000 | Loss: 0.00001914
Iteration 175/1000 | Loss: 0.00001914
Iteration 176/1000 | Loss: 0.00001914
Iteration 177/1000 | Loss: 0.00001914
Iteration 178/1000 | Loss: 0.00001914
Iteration 179/1000 | Loss: 0.00001914
Iteration 180/1000 | Loss: 0.00001914
Iteration 181/1000 | Loss: 0.00001913
Iteration 182/1000 | Loss: 0.00001913
Iteration 183/1000 | Loss: 0.00001913
Iteration 184/1000 | Loss: 0.00001913
Iteration 185/1000 | Loss: 0.00001913
Iteration 186/1000 | Loss: 0.00001913
Iteration 187/1000 | Loss: 0.00001913
Iteration 188/1000 | Loss: 0.00001913
Iteration 189/1000 | Loss: 0.00001913
Iteration 190/1000 | Loss: 0.00001913
Iteration 191/1000 | Loss: 0.00001913
Iteration 192/1000 | Loss: 0.00001913
Iteration 193/1000 | Loss: 0.00001913
Iteration 194/1000 | Loss: 0.00001913
Iteration 195/1000 | Loss: 0.00001913
Iteration 196/1000 | Loss: 0.00001913
Iteration 197/1000 | Loss: 0.00001913
Iteration 198/1000 | Loss: 0.00001913
Iteration 199/1000 | Loss: 0.00001913
Iteration 200/1000 | Loss: 0.00001912
Iteration 201/1000 | Loss: 0.00001912
Iteration 202/1000 | Loss: 0.00001912
Iteration 203/1000 | Loss: 0.00001912
Iteration 204/1000 | Loss: 0.00001912
Iteration 205/1000 | Loss: 0.00001912
Iteration 206/1000 | Loss: 0.00001912
Iteration 207/1000 | Loss: 0.00001912
Iteration 208/1000 | Loss: 0.00001912
Iteration 209/1000 | Loss: 0.00001912
Iteration 210/1000 | Loss: 0.00001911
Iteration 211/1000 | Loss: 0.00001911
Iteration 212/1000 | Loss: 0.00001911
Iteration 213/1000 | Loss: 0.00001911
Iteration 214/1000 | Loss: 0.00001911
Iteration 215/1000 | Loss: 0.00001911
Iteration 216/1000 | Loss: 0.00001911
Iteration 217/1000 | Loss: 0.00001911
Iteration 218/1000 | Loss: 0.00001911
Iteration 219/1000 | Loss: 0.00001910
Iteration 220/1000 | Loss: 0.00001910
Iteration 221/1000 | Loss: 0.00001910
Iteration 222/1000 | Loss: 0.00001910
Iteration 223/1000 | Loss: 0.00001910
Iteration 224/1000 | Loss: 0.00001910
Iteration 225/1000 | Loss: 0.00001909
Iteration 226/1000 | Loss: 0.00001909
Iteration 227/1000 | Loss: 0.00001909
Iteration 228/1000 | Loss: 0.00001909
Iteration 229/1000 | Loss: 0.00001909
Iteration 230/1000 | Loss: 0.00001909
Iteration 231/1000 | Loss: 0.00001909
Iteration 232/1000 | Loss: 0.00001909
Iteration 233/1000 | Loss: 0.00001909
Iteration 234/1000 | Loss: 0.00001909
Iteration 235/1000 | Loss: 0.00001909
Iteration 236/1000 | Loss: 0.00001909
Iteration 237/1000 | Loss: 0.00001909
Iteration 238/1000 | Loss: 0.00001909
Iteration 239/1000 | Loss: 0.00001909
Iteration 240/1000 | Loss: 0.00001909
Iteration 241/1000 | Loss: 0.00001909
Iteration 242/1000 | Loss: 0.00001909
Iteration 243/1000 | Loss: 0.00001909
Iteration 244/1000 | Loss: 0.00001909
Iteration 245/1000 | Loss: 0.00001909
Iteration 246/1000 | Loss: 0.00001909
Iteration 247/1000 | Loss: 0.00001909
Iteration 248/1000 | Loss: 0.00001909
Iteration 249/1000 | Loss: 0.00001909
Iteration 250/1000 | Loss: 0.00001909
Iteration 251/1000 | Loss: 0.00001909
Iteration 252/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.9086519387201406e-05, 1.9086519387201406e-05, 1.9086519387201406e-05, 1.9086519387201406e-05, 1.9086519387201406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9086519387201406e-05

Optimization complete. Final v2v error: 3.7940149307250977 mm

Highest mean error: 4.997410297393799 mm for frame 166

Lowest mean error: 3.2312026023864746 mm for frame 199

Saving results

Total time: 129.47672986984253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395356
Iteration 2/25 | Loss: 0.00070703
Iteration 3/25 | Loss: 0.00059802
Iteration 4/25 | Loss: 0.00057870
Iteration 5/25 | Loss: 0.00057269
Iteration 6/25 | Loss: 0.00057118
Iteration 7/25 | Loss: 0.00057111
Iteration 8/25 | Loss: 0.00057111
Iteration 9/25 | Loss: 0.00057111
Iteration 10/25 | Loss: 0.00057111
Iteration 11/25 | Loss: 0.00057111
Iteration 12/25 | Loss: 0.00057111
Iteration 13/25 | Loss: 0.00057111
Iteration 14/25 | Loss: 0.00057111
Iteration 15/25 | Loss: 0.00057111
Iteration 16/25 | Loss: 0.00057111
Iteration 17/25 | Loss: 0.00057111
Iteration 18/25 | Loss: 0.00057111
Iteration 19/25 | Loss: 0.00057111
Iteration 20/25 | Loss: 0.00057111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005711087142117321, 0.0005711087142117321, 0.0005711087142117321, 0.0005711087142117321, 0.0005711087142117321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005711087142117321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91405547
Iteration 2/25 | Loss: 0.00010608
Iteration 3/25 | Loss: 0.00010608
Iteration 4/25 | Loss: 0.00010608
Iteration 5/25 | Loss: 0.00010608
Iteration 6/25 | Loss: 0.00010608
Iteration 7/25 | Loss: 0.00010608
Iteration 8/25 | Loss: 0.00010608
Iteration 9/25 | Loss: 0.00010608
Iteration 10/25 | Loss: 0.00010608
Iteration 11/25 | Loss: 0.00010608
Iteration 12/25 | Loss: 0.00010608
Iteration 13/25 | Loss: 0.00010608
Iteration 14/25 | Loss: 0.00010608
Iteration 15/25 | Loss: 0.00010608
Iteration 16/25 | Loss: 0.00010608
Iteration 17/25 | Loss: 0.00010608
Iteration 18/25 | Loss: 0.00010608
Iteration 19/25 | Loss: 0.00010608
Iteration 20/25 | Loss: 0.00010608
Iteration 21/25 | Loss: 0.00010608
Iteration 22/25 | Loss: 0.00010608
Iteration 23/25 | Loss: 0.00010608
Iteration 24/25 | Loss: 0.00010608
Iteration 25/25 | Loss: 0.00010608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088952
Iteration 2/1000 | Loss: 0.00072804
Iteration 3/1000 | Loss: 0.00027295
Iteration 4/1000 | Loss: 0.00007540
Iteration 5/1000 | Loss: 0.00005029
Iteration 6/1000 | Loss: 0.00004140
Iteration 7/1000 | Loss: 0.00003889
Iteration 8/1000 | Loss: 0.00018298
Iteration 9/1000 | Loss: 0.00024263
Iteration 10/1000 | Loss: 0.00006276
Iteration 11/1000 | Loss: 0.00005257
Iteration 12/1000 | Loss: 0.00017897
Iteration 13/1000 | Loss: 0.00033404
Iteration 14/1000 | Loss: 0.00006054
Iteration 15/1000 | Loss: 0.00006272
Iteration 16/1000 | Loss: 0.00004706
Iteration 17/1000 | Loss: 0.00008521
Iteration 18/1000 | Loss: 0.00016383
Iteration 19/1000 | Loss: 0.00021812
Iteration 20/1000 | Loss: 0.00004977
Iteration 21/1000 | Loss: 0.00004203
Iteration 22/1000 | Loss: 0.00003992
Iteration 23/1000 | Loss: 0.00003909
Iteration 24/1000 | Loss: 0.00003813
Iteration 25/1000 | Loss: 0.00010625
Iteration 26/1000 | Loss: 0.00004726
Iteration 27/1000 | Loss: 0.00014384
Iteration 28/1000 | Loss: 0.00004705
Iteration 29/1000 | Loss: 0.00009094
Iteration 30/1000 | Loss: 0.00005946
Iteration 31/1000 | Loss: 0.00005191
Iteration 32/1000 | Loss: 0.00003714
Iteration 33/1000 | Loss: 0.00013835
Iteration 34/1000 | Loss: 0.00006880
Iteration 35/1000 | Loss: 0.00006417
Iteration 36/1000 | Loss: 0.00011185
Iteration 37/1000 | Loss: 0.00005731
Iteration 38/1000 | Loss: 0.00003620
Iteration 39/1000 | Loss: 0.00003286
Iteration 40/1000 | Loss: 0.00003148
Iteration 41/1000 | Loss: 0.00003067
Iteration 42/1000 | Loss: 0.00002998
Iteration 43/1000 | Loss: 0.00002942
Iteration 44/1000 | Loss: 0.00002897
Iteration 45/1000 | Loss: 0.00002865
Iteration 46/1000 | Loss: 0.00002836
Iteration 47/1000 | Loss: 0.00002798
Iteration 48/1000 | Loss: 0.00005127
Iteration 49/1000 | Loss: 0.00004564
Iteration 50/1000 | Loss: 0.00005088
Iteration 51/1000 | Loss: 0.00003756
Iteration 52/1000 | Loss: 0.00003197
Iteration 53/1000 | Loss: 0.00002967
Iteration 54/1000 | Loss: 0.00002848
Iteration 55/1000 | Loss: 0.00002769
Iteration 56/1000 | Loss: 0.00002742
Iteration 57/1000 | Loss: 0.00002717
Iteration 58/1000 | Loss: 0.00002697
Iteration 59/1000 | Loss: 0.00002675
Iteration 60/1000 | Loss: 0.00002674
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00024746
Iteration 63/1000 | Loss: 0.00011452
Iteration 64/1000 | Loss: 0.00002667
Iteration 65/1000 | Loss: 0.00024659
Iteration 66/1000 | Loss: 0.00007434
Iteration 67/1000 | Loss: 0.00003079
Iteration 68/1000 | Loss: 0.00002773
Iteration 69/1000 | Loss: 0.00002673
Iteration 70/1000 | Loss: 0.00024157
Iteration 71/1000 | Loss: 0.00003529
Iteration 72/1000 | Loss: 0.00002904
Iteration 73/1000 | Loss: 0.00002802
Iteration 74/1000 | Loss: 0.00002780
Iteration 75/1000 | Loss: 0.00002761
Iteration 76/1000 | Loss: 0.00002971
Iteration 77/1000 | Loss: 0.00002742
Iteration 78/1000 | Loss: 0.00002672
Iteration 79/1000 | Loss: 0.00002655
Iteration 80/1000 | Loss: 0.00002651
Iteration 81/1000 | Loss: 0.00026458
Iteration 82/1000 | Loss: 0.00004464
Iteration 83/1000 | Loss: 0.00002861
Iteration 84/1000 | Loss: 0.00002760
Iteration 85/1000 | Loss: 0.00003034
Iteration 86/1000 | Loss: 0.00002719
Iteration 87/1000 | Loss: 0.00002660
Iteration 88/1000 | Loss: 0.00002647
Iteration 89/1000 | Loss: 0.00025916
Iteration 90/1000 | Loss: 0.00004597
Iteration 91/1000 | Loss: 0.00002919
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00003661
Iteration 94/1000 | Loss: 0.00002788
Iteration 95/1000 | Loss: 0.00002669
Iteration 96/1000 | Loss: 0.00002642
Iteration 97/1000 | Loss: 0.00025135
Iteration 98/1000 | Loss: 0.00003702
Iteration 99/1000 | Loss: 0.00003030
Iteration 100/1000 | Loss: 0.00002656
Iteration 101/1000 | Loss: 0.00025267
Iteration 102/1000 | Loss: 0.00004198
Iteration 103/1000 | Loss: 0.00002759
Iteration 104/1000 | Loss: 0.00003311
Iteration 105/1000 | Loss: 0.00002645
Iteration 106/1000 | Loss: 0.00002635
Iteration 107/1000 | Loss: 0.00002635
Iteration 108/1000 | Loss: 0.00002635
Iteration 109/1000 | Loss: 0.00002635
Iteration 110/1000 | Loss: 0.00002635
Iteration 111/1000 | Loss: 0.00002635
Iteration 112/1000 | Loss: 0.00002634
Iteration 113/1000 | Loss: 0.00002634
Iteration 114/1000 | Loss: 0.00002634
Iteration 115/1000 | Loss: 0.00002634
Iteration 116/1000 | Loss: 0.00002634
Iteration 117/1000 | Loss: 0.00002634
Iteration 118/1000 | Loss: 0.00002634
Iteration 119/1000 | Loss: 0.00002634
Iteration 120/1000 | Loss: 0.00002633
Iteration 121/1000 | Loss: 0.00002633
Iteration 122/1000 | Loss: 0.00002633
Iteration 123/1000 | Loss: 0.00002632
Iteration 124/1000 | Loss: 0.00002631
Iteration 125/1000 | Loss: 0.00002631
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002630
Iteration 129/1000 | Loss: 0.00002629
Iteration 130/1000 | Loss: 0.00002629
Iteration 131/1000 | Loss: 0.00002629
Iteration 132/1000 | Loss: 0.00002629
Iteration 133/1000 | Loss: 0.00002628
Iteration 134/1000 | Loss: 0.00002628
Iteration 135/1000 | Loss: 0.00002627
Iteration 136/1000 | Loss: 0.00002627
Iteration 137/1000 | Loss: 0.00024409
Iteration 138/1000 | Loss: 0.00003288
Iteration 139/1000 | Loss: 0.00002809
Iteration 140/1000 | Loss: 0.00003589
Iteration 141/1000 | Loss: 0.00003158
Iteration 142/1000 | Loss: 0.00003906
Iteration 143/1000 | Loss: 0.00002870
Iteration 144/1000 | Loss: 0.00024810
Iteration 145/1000 | Loss: 0.00003030
Iteration 146/1000 | Loss: 0.00002747
Iteration 147/1000 | Loss: 0.00004363
Iteration 148/1000 | Loss: 0.00003066
Iteration 149/1000 | Loss: 0.00002735
Iteration 150/1000 | Loss: 0.00002645
Iteration 151/1000 | Loss: 0.00002626
Iteration 152/1000 | Loss: 0.00002625
Iteration 153/1000 | Loss: 0.00002623
Iteration 154/1000 | Loss: 0.00002623
Iteration 155/1000 | Loss: 0.00002622
Iteration 156/1000 | Loss: 0.00002622
Iteration 157/1000 | Loss: 0.00002622
Iteration 158/1000 | Loss: 0.00002621
Iteration 159/1000 | Loss: 0.00002620
Iteration 160/1000 | Loss: 0.00002619
Iteration 161/1000 | Loss: 0.00002619
Iteration 162/1000 | Loss: 0.00002618
Iteration 163/1000 | Loss: 0.00002618
Iteration 164/1000 | Loss: 0.00002618
Iteration 165/1000 | Loss: 0.00002618
Iteration 166/1000 | Loss: 0.00002617
Iteration 167/1000 | Loss: 0.00002617
Iteration 168/1000 | Loss: 0.00002617
Iteration 169/1000 | Loss: 0.00002616
Iteration 170/1000 | Loss: 0.00002615
Iteration 171/1000 | Loss: 0.00002614
Iteration 172/1000 | Loss: 0.00002614
Iteration 173/1000 | Loss: 0.00002614
Iteration 174/1000 | Loss: 0.00002614
Iteration 175/1000 | Loss: 0.00002613
Iteration 176/1000 | Loss: 0.00002613
Iteration 177/1000 | Loss: 0.00021000
Iteration 178/1000 | Loss: 0.00003182
Iteration 179/1000 | Loss: 0.00002847
Iteration 180/1000 | Loss: 0.00002789
Iteration 181/1000 | Loss: 0.00002785
Iteration 182/1000 | Loss: 0.00002676
Iteration 183/1000 | Loss: 0.00022277
Iteration 184/1000 | Loss: 0.00003025
Iteration 185/1000 | Loss: 0.00002772
Iteration 186/1000 | Loss: 0.00002946
Iteration 187/1000 | Loss: 0.00002625
Iteration 188/1000 | Loss: 0.00002617
Iteration 189/1000 | Loss: 0.00002617
Iteration 190/1000 | Loss: 0.00002615
Iteration 191/1000 | Loss: 0.00019909
Iteration 192/1000 | Loss: 0.00003571
Iteration 193/1000 | Loss: 0.00002807
Iteration 194/1000 | Loss: 0.00002927
Iteration 195/1000 | Loss: 0.00002639
Iteration 196/1000 | Loss: 0.00019792
Iteration 197/1000 | Loss: 0.00002950
Iteration 198/1000 | Loss: 0.00002765
Iteration 199/1000 | Loss: 0.00003292
Iteration 200/1000 | Loss: 0.00002613
Iteration 201/1000 | Loss: 0.00002605
Iteration 202/1000 | Loss: 0.00002604
Iteration 203/1000 | Loss: 0.00018720
Iteration 204/1000 | Loss: 0.00002955
Iteration 205/1000 | Loss: 0.00003320
Iteration 206/1000 | Loss: 0.00002675
Iteration 207/1000 | Loss: 0.00002604
Iteration 208/1000 | Loss: 0.00002604
Iteration 209/1000 | Loss: 0.00002603
Iteration 210/1000 | Loss: 0.00002602
Iteration 211/1000 | Loss: 0.00002602
Iteration 212/1000 | Loss: 0.00002602
Iteration 213/1000 | Loss: 0.00002602
Iteration 214/1000 | Loss: 0.00002602
Iteration 215/1000 | Loss: 0.00002602
Iteration 216/1000 | Loss: 0.00002602
Iteration 217/1000 | Loss: 0.00002602
Iteration 218/1000 | Loss: 0.00002601
Iteration 219/1000 | Loss: 0.00002601
Iteration 220/1000 | Loss: 0.00002601
Iteration 221/1000 | Loss: 0.00002601
Iteration 222/1000 | Loss: 0.00002601
Iteration 223/1000 | Loss: 0.00002601
Iteration 224/1000 | Loss: 0.00002601
Iteration 225/1000 | Loss: 0.00002600
Iteration 226/1000 | Loss: 0.00002600
Iteration 227/1000 | Loss: 0.00002600
Iteration 228/1000 | Loss: 0.00002600
Iteration 229/1000 | Loss: 0.00002600
Iteration 230/1000 | Loss: 0.00002600
Iteration 231/1000 | Loss: 0.00002600
Iteration 232/1000 | Loss: 0.00002600
Iteration 233/1000 | Loss: 0.00002599
Iteration 234/1000 | Loss: 0.00002599
Iteration 235/1000 | Loss: 0.00002599
Iteration 236/1000 | Loss: 0.00002598
Iteration 237/1000 | Loss: 0.00002598
Iteration 238/1000 | Loss: 0.00002598
Iteration 239/1000 | Loss: 0.00002598
Iteration 240/1000 | Loss: 0.00002597
Iteration 241/1000 | Loss: 0.00002597
Iteration 242/1000 | Loss: 0.00002597
Iteration 243/1000 | Loss: 0.00002596
Iteration 244/1000 | Loss: 0.00002596
Iteration 245/1000 | Loss: 0.00002596
Iteration 246/1000 | Loss: 0.00002595
Iteration 247/1000 | Loss: 0.00002595
Iteration 248/1000 | Loss: 0.00002595
Iteration 249/1000 | Loss: 0.00002595
Iteration 250/1000 | Loss: 0.00002595
Iteration 251/1000 | Loss: 0.00002595
Iteration 252/1000 | Loss: 0.00002595
Iteration 253/1000 | Loss: 0.00016471
Iteration 254/1000 | Loss: 0.00005440
Iteration 255/1000 | Loss: 0.00002603
Iteration 256/1000 | Loss: 0.00002594
Iteration 257/1000 | Loss: 0.00002593
Iteration 258/1000 | Loss: 0.00002593
Iteration 259/1000 | Loss: 0.00002592
Iteration 260/1000 | Loss: 0.00002592
Iteration 261/1000 | Loss: 0.00002592
Iteration 262/1000 | Loss: 0.00002592
Iteration 263/1000 | Loss: 0.00002592
Iteration 264/1000 | Loss: 0.00002592
Iteration 265/1000 | Loss: 0.00002592
Iteration 266/1000 | Loss: 0.00002591
Iteration 267/1000 | Loss: 0.00002591
Iteration 268/1000 | Loss: 0.00002591
Iteration 269/1000 | Loss: 0.00002591
Iteration 270/1000 | Loss: 0.00002591
Iteration 271/1000 | Loss: 0.00002591
Iteration 272/1000 | Loss: 0.00002591
Iteration 273/1000 | Loss: 0.00002591
Iteration 274/1000 | Loss: 0.00002590
Iteration 275/1000 | Loss: 0.00002590
Iteration 276/1000 | Loss: 0.00002590
Iteration 277/1000 | Loss: 0.00002590
Iteration 278/1000 | Loss: 0.00002590
Iteration 279/1000 | Loss: 0.00002590
Iteration 280/1000 | Loss: 0.00002590
Iteration 281/1000 | Loss: 0.00002590
Iteration 282/1000 | Loss: 0.00002590
Iteration 283/1000 | Loss: 0.00002590
Iteration 284/1000 | Loss: 0.00002589
Iteration 285/1000 | Loss: 0.00002589
Iteration 286/1000 | Loss: 0.00002589
Iteration 287/1000 | Loss: 0.00002589
Iteration 288/1000 | Loss: 0.00002589
Iteration 289/1000 | Loss: 0.00002589
Iteration 290/1000 | Loss: 0.00002589
Iteration 291/1000 | Loss: 0.00014808
Iteration 292/1000 | Loss: 0.00011660
Iteration 293/1000 | Loss: 0.00003461
Iteration 294/1000 | Loss: 0.00003182
Iteration 295/1000 | Loss: 0.00013109
Iteration 296/1000 | Loss: 0.00014019
Iteration 297/1000 | Loss: 0.00003413
Iteration 298/1000 | Loss: 0.00003160
Iteration 299/1000 | Loss: 0.00003103
Iteration 300/1000 | Loss: 0.00003062
Iteration 301/1000 | Loss: 0.00003022
Iteration 302/1000 | Loss: 0.00003005
Iteration 303/1000 | Loss: 0.00002997
Iteration 304/1000 | Loss: 0.00002982
Iteration 305/1000 | Loss: 0.00002981
Iteration 306/1000 | Loss: 0.00002963
Iteration 307/1000 | Loss: 0.00002962
Iteration 308/1000 | Loss: 0.00002960
Iteration 309/1000 | Loss: 0.00002959
Iteration 310/1000 | Loss: 0.00002959
Iteration 311/1000 | Loss: 0.00002958
Iteration 312/1000 | Loss: 0.00002958
Iteration 313/1000 | Loss: 0.00002958
Iteration 314/1000 | Loss: 0.00002958
Iteration 315/1000 | Loss: 0.00002957
Iteration 316/1000 | Loss: 0.00002957
Iteration 317/1000 | Loss: 0.00002957
Iteration 318/1000 | Loss: 0.00002957
Iteration 319/1000 | Loss: 0.00002956
Iteration 320/1000 | Loss: 0.00002956
Iteration 321/1000 | Loss: 0.00002956
Iteration 322/1000 | Loss: 0.00002956
Iteration 323/1000 | Loss: 0.00002956
Iteration 324/1000 | Loss: 0.00002956
Iteration 325/1000 | Loss: 0.00002956
Iteration 326/1000 | Loss: 0.00002956
Iteration 327/1000 | Loss: 0.00002956
Iteration 328/1000 | Loss: 0.00002955
Iteration 329/1000 | Loss: 0.00002955
Iteration 330/1000 | Loss: 0.00002955
Iteration 331/1000 | Loss: 0.00002955
Iteration 332/1000 | Loss: 0.00002955
Iteration 333/1000 | Loss: 0.00002955
Iteration 334/1000 | Loss: 0.00002955
Iteration 335/1000 | Loss: 0.00002954
Iteration 336/1000 | Loss: 0.00002954
Iteration 337/1000 | Loss: 0.00002953
Iteration 338/1000 | Loss: 0.00002953
Iteration 339/1000 | Loss: 0.00002949
Iteration 340/1000 | Loss: 0.00002949
Iteration 341/1000 | Loss: 0.00002948
Iteration 342/1000 | Loss: 0.00002948
Iteration 343/1000 | Loss: 0.00002931
Iteration 344/1000 | Loss: 0.00002915
Iteration 345/1000 | Loss: 0.00002909
Iteration 346/1000 | Loss: 0.00002902
Iteration 347/1000 | Loss: 0.00002891
Iteration 348/1000 | Loss: 0.00002888
Iteration 349/1000 | Loss: 0.00002888
Iteration 350/1000 | Loss: 0.00002885
Iteration 351/1000 | Loss: 0.00002883
Iteration 352/1000 | Loss: 0.00002882
Iteration 353/1000 | Loss: 0.00002881
Iteration 354/1000 | Loss: 0.00002880
Iteration 355/1000 | Loss: 0.00002880
Iteration 356/1000 | Loss: 0.00002879
Iteration 357/1000 | Loss: 0.00002879
Iteration 358/1000 | Loss: 0.00002878
Iteration 359/1000 | Loss: 0.00002878
Iteration 360/1000 | Loss: 0.00002878
Iteration 361/1000 | Loss: 0.00002877
Iteration 362/1000 | Loss: 0.00002877
Iteration 363/1000 | Loss: 0.00002875
Iteration 364/1000 | Loss: 0.00002874
Iteration 365/1000 | Loss: 0.00002874
Iteration 366/1000 | Loss: 0.00002873
Iteration 367/1000 | Loss: 0.00002873
Iteration 368/1000 | Loss: 0.00002864
Iteration 369/1000 | Loss: 0.00002863
Iteration 370/1000 | Loss: 0.00002858
Iteration 371/1000 | Loss: 0.00002844
Iteration 372/1000 | Loss: 0.00002842
Iteration 373/1000 | Loss: 0.00002813
Iteration 374/1000 | Loss: 0.00002740
Iteration 375/1000 | Loss: 0.00002657
Iteration 376/1000 | Loss: 0.00002612
Iteration 377/1000 | Loss: 0.00002585
Iteration 378/1000 | Loss: 0.00002568
Iteration 379/1000 | Loss: 0.00002566
Iteration 380/1000 | Loss: 0.00002564
Iteration 381/1000 | Loss: 0.00002563
Iteration 382/1000 | Loss: 0.00002562
Iteration 383/1000 | Loss: 0.00002561
Iteration 384/1000 | Loss: 0.00002561
Iteration 385/1000 | Loss: 0.00002560
Iteration 386/1000 | Loss: 0.00002558
Iteration 387/1000 | Loss: 0.00002558
Iteration 388/1000 | Loss: 0.00002558
Iteration 389/1000 | Loss: 0.00002557
Iteration 390/1000 | Loss: 0.00002557
Iteration 391/1000 | Loss: 0.00002556
Iteration 392/1000 | Loss: 0.00002556
Iteration 393/1000 | Loss: 0.00002556
Iteration 394/1000 | Loss: 0.00002555
Iteration 395/1000 | Loss: 0.00002555
Iteration 396/1000 | Loss: 0.00002555
Iteration 397/1000 | Loss: 0.00002555
Iteration 398/1000 | Loss: 0.00002555
Iteration 399/1000 | Loss: 0.00002554
Iteration 400/1000 | Loss: 0.00002554
Iteration 401/1000 | Loss: 0.00002554
Iteration 402/1000 | Loss: 0.00002554
Iteration 403/1000 | Loss: 0.00002553
Iteration 404/1000 | Loss: 0.00002553
Iteration 405/1000 | Loss: 0.00002553
Iteration 406/1000 | Loss: 0.00002553
Iteration 407/1000 | Loss: 0.00002553
Iteration 408/1000 | Loss: 0.00002553
Iteration 409/1000 | Loss: 0.00002553
Iteration 410/1000 | Loss: 0.00002552
Iteration 411/1000 | Loss: 0.00002552
Iteration 412/1000 | Loss: 0.00002552
Iteration 413/1000 | Loss: 0.00002552
Iteration 414/1000 | Loss: 0.00002552
Iteration 415/1000 | Loss: 0.00002552
Iteration 416/1000 | Loss: 0.00002552
Iteration 417/1000 | Loss: 0.00002552
Iteration 418/1000 | Loss: 0.00002552
Iteration 419/1000 | Loss: 0.00002551
Iteration 420/1000 | Loss: 0.00002551
Iteration 421/1000 | Loss: 0.00002551
Iteration 422/1000 | Loss: 0.00002551
Iteration 423/1000 | Loss: 0.00002551
Iteration 424/1000 | Loss: 0.00002551
Iteration 425/1000 | Loss: 0.00002551
Iteration 426/1000 | Loss: 0.00002551
Iteration 427/1000 | Loss: 0.00002550
Iteration 428/1000 | Loss: 0.00002550
Iteration 429/1000 | Loss: 0.00002550
Iteration 430/1000 | Loss: 0.00002550
Iteration 431/1000 | Loss: 0.00002550
Iteration 432/1000 | Loss: 0.00002550
Iteration 433/1000 | Loss: 0.00002550
Iteration 434/1000 | Loss: 0.00002550
Iteration 435/1000 | Loss: 0.00002550
Iteration 436/1000 | Loss: 0.00002550
Iteration 437/1000 | Loss: 0.00002550
Iteration 438/1000 | Loss: 0.00002550
Iteration 439/1000 | Loss: 0.00002550
Iteration 440/1000 | Loss: 0.00002550
Iteration 441/1000 | Loss: 0.00002550
Iteration 442/1000 | Loss: 0.00002550
Iteration 443/1000 | Loss: 0.00002550
Iteration 444/1000 | Loss: 0.00002550
Iteration 445/1000 | Loss: 0.00002550
Iteration 446/1000 | Loss: 0.00002550
Iteration 447/1000 | Loss: 0.00002550
Iteration 448/1000 | Loss: 0.00002550
Iteration 449/1000 | Loss: 0.00002550
Iteration 450/1000 | Loss: 0.00002550
Iteration 451/1000 | Loss: 0.00002550
Iteration 452/1000 | Loss: 0.00002550
Iteration 453/1000 | Loss: 0.00002550
Iteration 454/1000 | Loss: 0.00002550
Iteration 455/1000 | Loss: 0.00002550
Iteration 456/1000 | Loss: 0.00002550
Iteration 457/1000 | Loss: 0.00002550
Iteration 458/1000 | Loss: 0.00002550
Iteration 459/1000 | Loss: 0.00002550
Iteration 460/1000 | Loss: 0.00002550
Iteration 461/1000 | Loss: 0.00002550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 461. Stopping optimization.
Last 5 losses: [2.5496559828752652e-05, 2.5496559828752652e-05, 2.5496559828752652e-05, 2.5496559828752652e-05, 2.5496559828752652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5496559828752652e-05

Optimization complete. Final v2v error: 4.1552205085754395 mm

Highest mean error: 6.821497917175293 mm for frame 95

Lowest mean error: 3.4159646034240723 mm for frame 134

Saving results

Total time: 314.98241782188416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017799
Iteration 2/25 | Loss: 0.00146242
Iteration 3/25 | Loss: 0.00092226
Iteration 4/25 | Loss: 0.00074482
Iteration 5/25 | Loss: 0.00071458
Iteration 6/25 | Loss: 0.00069336
Iteration 7/25 | Loss: 0.00069260
Iteration 8/25 | Loss: 0.00068362
Iteration 9/25 | Loss: 0.00068612
Iteration 10/25 | Loss: 0.00067921
Iteration 11/25 | Loss: 0.00067838
Iteration 12/25 | Loss: 0.00068260
Iteration 13/25 | Loss: 0.00068057
Iteration 14/25 | Loss: 0.00068149
Iteration 15/25 | Loss: 0.00067788
Iteration 16/25 | Loss: 0.00067926
Iteration 17/25 | Loss: 0.00067859
Iteration 18/25 | Loss: 0.00067642
Iteration 19/25 | Loss: 0.00067270
Iteration 20/25 | Loss: 0.00067583
Iteration 21/25 | Loss: 0.00067407
Iteration 22/25 | Loss: 0.00067451
Iteration 23/25 | Loss: 0.00067615
Iteration 24/25 | Loss: 0.00067414
Iteration 25/25 | Loss: 0.00067902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54507077
Iteration 2/25 | Loss: 0.00043100
Iteration 3/25 | Loss: 0.00043100
Iteration 4/25 | Loss: 0.00043100
Iteration 5/25 | Loss: 0.00043100
Iteration 6/25 | Loss: 0.00043100
Iteration 7/25 | Loss: 0.00043100
Iteration 8/25 | Loss: 0.00043100
Iteration 9/25 | Loss: 0.00043100
Iteration 10/25 | Loss: 0.00043100
Iteration 11/25 | Loss: 0.00043100
Iteration 12/25 | Loss: 0.00043100
Iteration 13/25 | Loss: 0.00043100
Iteration 14/25 | Loss: 0.00043100
Iteration 15/25 | Loss: 0.00043100
Iteration 16/25 | Loss: 0.00043100
Iteration 17/25 | Loss: 0.00043100
Iteration 18/25 | Loss: 0.00043100
Iteration 19/25 | Loss: 0.00043100
Iteration 20/25 | Loss: 0.00043100
Iteration 21/25 | Loss: 0.00043100
Iteration 22/25 | Loss: 0.00043100
Iteration 23/25 | Loss: 0.00043100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004309966752771288, 0.0004309966752771288, 0.0004309966752771288, 0.0004309966752771288, 0.0004309966752771288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004309966752771288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010608
Iteration 2/1000 | Loss: 0.00003436
Iteration 3/1000 | Loss: 0.00002412
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00002096
Iteration 6/1000 | Loss: 0.00002021
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001809
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00001802
Iteration 20/1000 | Loss: 0.00001801
Iteration 21/1000 | Loss: 0.00001801
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001800
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001796
Iteration 27/1000 | Loss: 0.00001795
Iteration 28/1000 | Loss: 0.00001795
Iteration 29/1000 | Loss: 0.00001794
Iteration 30/1000 | Loss: 0.00001794
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001793
Iteration 33/1000 | Loss: 0.00001791
Iteration 34/1000 | Loss: 0.00001791
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001776
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001774
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001773
Iteration 82/1000 | Loss: 0.00001773
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001768
Iteration 108/1000 | Loss: 0.00001768
Iteration 109/1000 | Loss: 0.00001768
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001766
Iteration 122/1000 | Loss: 0.00001766
Iteration 123/1000 | Loss: 0.00001766
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001765
Iteration 126/1000 | Loss: 0.00001765
Iteration 127/1000 | Loss: 0.00001765
Iteration 128/1000 | Loss: 0.00001765
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001765
Iteration 131/1000 | Loss: 0.00001765
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001764
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001763
Iteration 157/1000 | Loss: 0.00001763
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001763
Iteration 165/1000 | Loss: 0.00001763
Iteration 166/1000 | Loss: 0.00001763
Iteration 167/1000 | Loss: 0.00001763
Iteration 168/1000 | Loss: 0.00001763
Iteration 169/1000 | Loss: 0.00001763
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00001762
Iteration 172/1000 | Loss: 0.00001762
Iteration 173/1000 | Loss: 0.00001762
Iteration 174/1000 | Loss: 0.00001762
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001762
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001762
Iteration 179/1000 | Loss: 0.00001762
Iteration 180/1000 | Loss: 0.00001762
Iteration 181/1000 | Loss: 0.00001762
Iteration 182/1000 | Loss: 0.00001762
Iteration 183/1000 | Loss: 0.00001762
Iteration 184/1000 | Loss: 0.00001762
Iteration 185/1000 | Loss: 0.00001762
Iteration 186/1000 | Loss: 0.00001762
Iteration 187/1000 | Loss: 0.00001762
Iteration 188/1000 | Loss: 0.00001762
Iteration 189/1000 | Loss: 0.00001762
Iteration 190/1000 | Loss: 0.00001762
Iteration 191/1000 | Loss: 0.00001762
Iteration 192/1000 | Loss: 0.00001762
Iteration 193/1000 | Loss: 0.00001762
Iteration 194/1000 | Loss: 0.00001762
Iteration 195/1000 | Loss: 0.00001762
Iteration 196/1000 | Loss: 0.00001762
Iteration 197/1000 | Loss: 0.00001762
Iteration 198/1000 | Loss: 0.00001762
Iteration 199/1000 | Loss: 0.00001762
Iteration 200/1000 | Loss: 0.00001762
Iteration 201/1000 | Loss: 0.00001762
Iteration 202/1000 | Loss: 0.00001762
Iteration 203/1000 | Loss: 0.00001762
Iteration 204/1000 | Loss: 0.00001762
Iteration 205/1000 | Loss: 0.00001762
Iteration 206/1000 | Loss: 0.00001762
Iteration 207/1000 | Loss: 0.00001762
Iteration 208/1000 | Loss: 0.00001762
Iteration 209/1000 | Loss: 0.00001762
Iteration 210/1000 | Loss: 0.00001762
Iteration 211/1000 | Loss: 0.00001762
Iteration 212/1000 | Loss: 0.00001762
Iteration 213/1000 | Loss: 0.00001762
Iteration 214/1000 | Loss: 0.00001762
Iteration 215/1000 | Loss: 0.00001762
Iteration 216/1000 | Loss: 0.00001762
Iteration 217/1000 | Loss: 0.00001762
Iteration 218/1000 | Loss: 0.00001762
Iteration 219/1000 | Loss: 0.00001762
Iteration 220/1000 | Loss: 0.00001762
Iteration 221/1000 | Loss: 0.00001762
Iteration 222/1000 | Loss: 0.00001762
Iteration 223/1000 | Loss: 0.00001762
Iteration 224/1000 | Loss: 0.00001762
Iteration 225/1000 | Loss: 0.00001762
Iteration 226/1000 | Loss: 0.00001762
Iteration 227/1000 | Loss: 0.00001762
Iteration 228/1000 | Loss: 0.00001762
Iteration 229/1000 | Loss: 0.00001762
Iteration 230/1000 | Loss: 0.00001762
Iteration 231/1000 | Loss: 0.00001762
Iteration 232/1000 | Loss: 0.00001762
Iteration 233/1000 | Loss: 0.00001762
Iteration 234/1000 | Loss: 0.00001762
Iteration 235/1000 | Loss: 0.00001762
Iteration 236/1000 | Loss: 0.00001762
Iteration 237/1000 | Loss: 0.00001762
Iteration 238/1000 | Loss: 0.00001762
Iteration 239/1000 | Loss: 0.00001762
Iteration 240/1000 | Loss: 0.00001762
Iteration 241/1000 | Loss: 0.00001762
Iteration 242/1000 | Loss: 0.00001762
Iteration 243/1000 | Loss: 0.00001762
Iteration 244/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.7616257537156343e-05, 1.7616257537156343e-05, 1.7616257537156343e-05, 1.7616257537156343e-05, 1.7616257537156343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7616257537156343e-05

Optimization complete. Final v2v error: 3.555227756500244 mm

Highest mean error: 4.020944118499756 mm for frame 73

Lowest mean error: 3.077155590057373 mm for frame 85

Saving results

Total time: 42.37318563461304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400047
Iteration 2/25 | Loss: 0.00096837
Iteration 3/25 | Loss: 0.00080983
Iteration 4/25 | Loss: 0.00078970
Iteration 5/25 | Loss: 0.00078281
Iteration 6/25 | Loss: 0.00078039
Iteration 7/25 | Loss: 0.00077972
Iteration 8/25 | Loss: 0.00077965
Iteration 9/25 | Loss: 0.00077965
Iteration 10/25 | Loss: 0.00077965
Iteration 11/25 | Loss: 0.00077965
Iteration 12/25 | Loss: 0.00077965
Iteration 13/25 | Loss: 0.00077965
Iteration 14/25 | Loss: 0.00077965
Iteration 15/25 | Loss: 0.00077965
Iteration 16/25 | Loss: 0.00077965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007796463323757052, 0.0007796463323757052, 0.0007796463323757052, 0.0007796463323757052, 0.0007796463323757052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007796463323757052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57504904
Iteration 2/25 | Loss: 0.00057095
Iteration 3/25 | Loss: 0.00057094
Iteration 4/25 | Loss: 0.00057094
Iteration 5/25 | Loss: 0.00057094
Iteration 6/25 | Loss: 0.00057094
Iteration 7/25 | Loss: 0.00057094
Iteration 8/25 | Loss: 0.00057094
Iteration 9/25 | Loss: 0.00057094
Iteration 10/25 | Loss: 0.00057094
Iteration 11/25 | Loss: 0.00057094
Iteration 12/25 | Loss: 0.00057094
Iteration 13/25 | Loss: 0.00057094
Iteration 14/25 | Loss: 0.00057094
Iteration 15/25 | Loss: 0.00057094
Iteration 16/25 | Loss: 0.00057094
Iteration 17/25 | Loss: 0.00057094
Iteration 18/25 | Loss: 0.00057094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005709409015253186, 0.0005709409015253186, 0.0005709409015253186, 0.0005709409015253186, 0.0005709409015253186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005709409015253186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057094
Iteration 2/1000 | Loss: 0.00004184
Iteration 3/1000 | Loss: 0.00002345
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001630
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001475
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001390
Iteration 12/1000 | Loss: 0.00001388
Iteration 13/1000 | Loss: 0.00001388
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001337
Iteration 104/1000 | Loss: 0.00001337
Iteration 105/1000 | Loss: 0.00001337
Iteration 106/1000 | Loss: 0.00001337
Iteration 107/1000 | Loss: 0.00001337
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001337
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001336
Iteration 116/1000 | Loss: 0.00001336
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00001336
Iteration 127/1000 | Loss: 0.00001336
Iteration 128/1000 | Loss: 0.00001336
Iteration 129/1000 | Loss: 0.00001336
Iteration 130/1000 | Loss: 0.00001336
Iteration 131/1000 | Loss: 0.00001336
Iteration 132/1000 | Loss: 0.00001336
Iteration 133/1000 | Loss: 0.00001336
Iteration 134/1000 | Loss: 0.00001336
Iteration 135/1000 | Loss: 0.00001336
Iteration 136/1000 | Loss: 0.00001336
Iteration 137/1000 | Loss: 0.00001336
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3355279406823684e-05, 1.3355279406823684e-05, 1.3355279406823684e-05, 1.3355279406823684e-05, 1.3355279406823684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3355279406823684e-05

Optimization complete. Final v2v error: 3.0737805366516113 mm

Highest mean error: 4.375341415405273 mm for frame 67

Lowest mean error: 2.7034354209899902 mm for frame 116

Saving results

Total time: 37.667311906814575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031392
Iteration 2/25 | Loss: 0.00264020
Iteration 3/25 | Loss: 0.00212199
Iteration 4/25 | Loss: 0.00194399
Iteration 5/25 | Loss: 0.00199395
Iteration 6/25 | Loss: 0.00203782
Iteration 7/25 | Loss: 0.00191464
Iteration 8/25 | Loss: 0.00177287
Iteration 9/25 | Loss: 0.00160495
Iteration 10/25 | Loss: 0.00143678
Iteration 11/25 | Loss: 0.00133717
Iteration 12/25 | Loss: 0.00129940
Iteration 13/25 | Loss: 0.00129206
Iteration 14/25 | Loss: 0.00128838
Iteration 15/25 | Loss: 0.00126644
Iteration 16/25 | Loss: 0.00123700
Iteration 17/25 | Loss: 0.00122996
Iteration 18/25 | Loss: 0.00123482
Iteration 19/25 | Loss: 0.00121914
Iteration 20/25 | Loss: 0.00121183
Iteration 21/25 | Loss: 0.00120830
Iteration 22/25 | Loss: 0.00120653
Iteration 23/25 | Loss: 0.00120764
Iteration 24/25 | Loss: 0.00120761
Iteration 25/25 | Loss: 0.00120736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41072321
Iteration 2/25 | Loss: 0.00243630
Iteration 3/25 | Loss: 0.00243629
Iteration 4/25 | Loss: 0.00243629
Iteration 5/25 | Loss: 0.00243629
Iteration 6/25 | Loss: 0.00243629
Iteration 7/25 | Loss: 0.00243629
Iteration 8/25 | Loss: 0.00243629
Iteration 9/25 | Loss: 0.00243629
Iteration 10/25 | Loss: 0.00243629
Iteration 11/25 | Loss: 0.00243629
Iteration 12/25 | Loss: 0.00243629
Iteration 13/25 | Loss: 0.00243629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002436293289065361, 0.002436293289065361, 0.002436293289065361, 0.002436293289065361, 0.002436293289065361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002436293289065361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05575798
Iteration 2/1000 | Loss: 0.00865875
Iteration 3/1000 | Loss: 0.00267143
Iteration 4/1000 | Loss: 0.00097166
Iteration 5/1000 | Loss: 0.00046627
Iteration 6/1000 | Loss: 0.00530685
Iteration 7/1000 | Loss: 0.00192762
Iteration 8/1000 | Loss: 0.00152190
Iteration 9/1000 | Loss: 0.00113547
Iteration 10/1000 | Loss: 0.00014170
Iteration 11/1000 | Loss: 0.00102770
Iteration 12/1000 | Loss: 0.00010029
Iteration 13/1000 | Loss: 0.00017127
Iteration 14/1000 | Loss: 0.00007564
Iteration 15/1000 | Loss: 0.00008324
Iteration 16/1000 | Loss: 0.00012792
Iteration 17/1000 | Loss: 0.00006237
Iteration 18/1000 | Loss: 0.00015340
Iteration 19/1000 | Loss: 0.00005770
Iteration 20/1000 | Loss: 0.00006895
Iteration 21/1000 | Loss: 0.00009782
Iteration 22/1000 | Loss: 0.00013996
Iteration 23/1000 | Loss: 0.00011045
Iteration 24/1000 | Loss: 0.00004630
Iteration 25/1000 | Loss: 0.00058032
Iteration 26/1000 | Loss: 0.00029041
Iteration 27/1000 | Loss: 0.00010830
Iteration 28/1000 | Loss: 0.00024780
Iteration 29/1000 | Loss: 0.00028449
Iteration 30/1000 | Loss: 0.00020049
Iteration 31/1000 | Loss: 0.00007348
Iteration 32/1000 | Loss: 0.00018510
Iteration 33/1000 | Loss: 0.00022051
Iteration 34/1000 | Loss: 0.00021561
Iteration 35/1000 | Loss: 0.00009592
Iteration 36/1000 | Loss: 0.00005135
Iteration 37/1000 | Loss: 0.00015216
Iteration 38/1000 | Loss: 0.00004245
Iteration 39/1000 | Loss: 0.00008117
Iteration 40/1000 | Loss: 0.00005931
Iteration 41/1000 | Loss: 0.00022977
Iteration 42/1000 | Loss: 0.00005146
Iteration 43/1000 | Loss: 0.00004410
Iteration 44/1000 | Loss: 0.00025663
Iteration 45/1000 | Loss: 0.00081539
Iteration 46/1000 | Loss: 0.00035967
Iteration 47/1000 | Loss: 0.00027625
Iteration 48/1000 | Loss: 0.00019722
Iteration 49/1000 | Loss: 0.00005694
Iteration 50/1000 | Loss: 0.00004130
Iteration 51/1000 | Loss: 0.00047883
Iteration 52/1000 | Loss: 0.00022242
Iteration 53/1000 | Loss: 0.00020475
Iteration 54/1000 | Loss: 0.00024166
Iteration 55/1000 | Loss: 0.00009635
Iteration 56/1000 | Loss: 0.00008318
Iteration 57/1000 | Loss: 0.00015295
Iteration 58/1000 | Loss: 0.00009370
Iteration 59/1000 | Loss: 0.00005284
Iteration 60/1000 | Loss: 0.00011424
Iteration 61/1000 | Loss: 0.00003742
Iteration 62/1000 | Loss: 0.00006668
Iteration 63/1000 | Loss: 0.00003345
Iteration 64/1000 | Loss: 0.00003282
Iteration 65/1000 | Loss: 0.00009337
Iteration 66/1000 | Loss: 0.00003488
Iteration 67/1000 | Loss: 0.00003505
Iteration 68/1000 | Loss: 0.00024386
Iteration 69/1000 | Loss: 0.00004853
Iteration 70/1000 | Loss: 0.00006252
Iteration 71/1000 | Loss: 0.00010490
Iteration 72/1000 | Loss: 0.00003367
Iteration 73/1000 | Loss: 0.00003724
Iteration 74/1000 | Loss: 0.00005181
Iteration 75/1000 | Loss: 0.00004207
Iteration 76/1000 | Loss: 0.00008086
Iteration 77/1000 | Loss: 0.00021939
Iteration 78/1000 | Loss: 0.00006321
Iteration 79/1000 | Loss: 0.00004345
Iteration 80/1000 | Loss: 0.00048881
Iteration 81/1000 | Loss: 0.00030080
Iteration 82/1000 | Loss: 0.00024959
Iteration 83/1000 | Loss: 0.00004201
Iteration 84/1000 | Loss: 0.00011950
Iteration 85/1000 | Loss: 0.00004616
Iteration 86/1000 | Loss: 0.00006375
Iteration 87/1000 | Loss: 0.00006005
Iteration 88/1000 | Loss: 0.00004636
Iteration 89/1000 | Loss: 0.00003483
Iteration 90/1000 | Loss: 0.00023054
Iteration 91/1000 | Loss: 0.00008240
Iteration 92/1000 | Loss: 0.00005821
Iteration 93/1000 | Loss: 0.00003826
Iteration 94/1000 | Loss: 0.00003618
Iteration 95/1000 | Loss: 0.00003155
Iteration 96/1000 | Loss: 0.00007993
Iteration 97/1000 | Loss: 0.00003132
Iteration 98/1000 | Loss: 0.00003440
Iteration 99/1000 | Loss: 0.00003830
Iteration 100/1000 | Loss: 0.00002916
Iteration 101/1000 | Loss: 0.00002898
Iteration 102/1000 | Loss: 0.00002858
Iteration 103/1000 | Loss: 0.00010768
Iteration 104/1000 | Loss: 0.00004382
Iteration 105/1000 | Loss: 0.00005652
Iteration 106/1000 | Loss: 0.00010807
Iteration 107/1000 | Loss: 0.00002808
Iteration 108/1000 | Loss: 0.00002768
Iteration 109/1000 | Loss: 0.00004854
Iteration 110/1000 | Loss: 0.00006885
Iteration 111/1000 | Loss: 0.00003552
Iteration 112/1000 | Loss: 0.00003017
Iteration 113/1000 | Loss: 0.00002740
Iteration 114/1000 | Loss: 0.00002740
Iteration 115/1000 | Loss: 0.00002740
Iteration 116/1000 | Loss: 0.00002740
Iteration 117/1000 | Loss: 0.00002739
Iteration 118/1000 | Loss: 0.00002739
Iteration 119/1000 | Loss: 0.00002739
Iteration 120/1000 | Loss: 0.00002739
Iteration 121/1000 | Loss: 0.00002739
Iteration 122/1000 | Loss: 0.00002739
Iteration 123/1000 | Loss: 0.00002739
Iteration 124/1000 | Loss: 0.00004328
Iteration 125/1000 | Loss: 0.00003200
Iteration 126/1000 | Loss: 0.00002988
Iteration 127/1000 | Loss: 0.00002739
Iteration 128/1000 | Loss: 0.00002739
Iteration 129/1000 | Loss: 0.00002738
Iteration 130/1000 | Loss: 0.00002738
Iteration 131/1000 | Loss: 0.00002738
Iteration 132/1000 | Loss: 0.00002738
Iteration 133/1000 | Loss: 0.00002738
Iteration 134/1000 | Loss: 0.00002738
Iteration 135/1000 | Loss: 0.00002738
Iteration 136/1000 | Loss: 0.00002875
Iteration 137/1000 | Loss: 0.00002736
Iteration 138/1000 | Loss: 0.00002736
Iteration 139/1000 | Loss: 0.00002736
Iteration 140/1000 | Loss: 0.00002736
Iteration 141/1000 | Loss: 0.00002736
Iteration 142/1000 | Loss: 0.00002736
Iteration 143/1000 | Loss: 0.00002736
Iteration 144/1000 | Loss: 0.00002736
Iteration 145/1000 | Loss: 0.00002735
Iteration 146/1000 | Loss: 0.00002735
Iteration 147/1000 | Loss: 0.00002735
Iteration 148/1000 | Loss: 0.00002735
Iteration 149/1000 | Loss: 0.00002735
Iteration 150/1000 | Loss: 0.00002735
Iteration 151/1000 | Loss: 0.00002735
Iteration 152/1000 | Loss: 0.00002735
Iteration 153/1000 | Loss: 0.00002735
Iteration 154/1000 | Loss: 0.00002735
Iteration 155/1000 | Loss: 0.00002734
Iteration 156/1000 | Loss: 0.00002734
Iteration 157/1000 | Loss: 0.00002732
Iteration 158/1000 | Loss: 0.00002732
Iteration 159/1000 | Loss: 0.00002732
Iteration 160/1000 | Loss: 0.00002732
Iteration 161/1000 | Loss: 0.00002732
Iteration 162/1000 | Loss: 0.00002732
Iteration 163/1000 | Loss: 0.00002732
Iteration 164/1000 | Loss: 0.00002732
Iteration 165/1000 | Loss: 0.00002732
Iteration 166/1000 | Loss: 0.00002732
Iteration 167/1000 | Loss: 0.00002732
Iteration 168/1000 | Loss: 0.00002731
Iteration 169/1000 | Loss: 0.00002731
Iteration 170/1000 | Loss: 0.00002731
Iteration 171/1000 | Loss: 0.00002731
Iteration 172/1000 | Loss: 0.00002731
Iteration 173/1000 | Loss: 0.00002731
Iteration 174/1000 | Loss: 0.00002731
Iteration 175/1000 | Loss: 0.00002731
Iteration 176/1000 | Loss: 0.00002731
Iteration 177/1000 | Loss: 0.00002731
Iteration 178/1000 | Loss: 0.00002730
Iteration 179/1000 | Loss: 0.00002730
Iteration 180/1000 | Loss: 0.00003586
Iteration 181/1000 | Loss: 0.00002726
Iteration 182/1000 | Loss: 0.00002726
Iteration 183/1000 | Loss: 0.00002726
Iteration 184/1000 | Loss: 0.00002726
Iteration 185/1000 | Loss: 0.00002726
Iteration 186/1000 | Loss: 0.00002726
Iteration 187/1000 | Loss: 0.00002726
Iteration 188/1000 | Loss: 0.00002726
Iteration 189/1000 | Loss: 0.00002726
Iteration 190/1000 | Loss: 0.00002725
Iteration 191/1000 | Loss: 0.00002725
Iteration 192/1000 | Loss: 0.00002725
Iteration 193/1000 | Loss: 0.00002725
Iteration 194/1000 | Loss: 0.00002725
Iteration 195/1000 | Loss: 0.00002725
Iteration 196/1000 | Loss: 0.00004472
Iteration 197/1000 | Loss: 0.00005659
Iteration 198/1000 | Loss: 0.00003373
Iteration 199/1000 | Loss: 0.00004586
Iteration 200/1000 | Loss: 0.00002722
Iteration 201/1000 | Loss: 0.00002722
Iteration 202/1000 | Loss: 0.00002722
Iteration 203/1000 | Loss: 0.00003595
Iteration 204/1000 | Loss: 0.00002722
Iteration 205/1000 | Loss: 0.00002720
Iteration 206/1000 | Loss: 0.00002720
Iteration 207/1000 | Loss: 0.00002720
Iteration 208/1000 | Loss: 0.00002720
Iteration 209/1000 | Loss: 0.00002720
Iteration 210/1000 | Loss: 0.00002720
Iteration 211/1000 | Loss: 0.00002720
Iteration 212/1000 | Loss: 0.00002720
Iteration 213/1000 | Loss: 0.00002720
Iteration 214/1000 | Loss: 0.00002720
Iteration 215/1000 | Loss: 0.00002720
Iteration 216/1000 | Loss: 0.00002720
Iteration 217/1000 | Loss: 0.00002720
Iteration 218/1000 | Loss: 0.00002720
Iteration 219/1000 | Loss: 0.00002719
Iteration 220/1000 | Loss: 0.00002719
Iteration 221/1000 | Loss: 0.00002719
Iteration 222/1000 | Loss: 0.00002719
Iteration 223/1000 | Loss: 0.00002719
Iteration 224/1000 | Loss: 0.00002719
Iteration 225/1000 | Loss: 0.00002719
Iteration 226/1000 | Loss: 0.00002719
Iteration 227/1000 | Loss: 0.00002719
Iteration 228/1000 | Loss: 0.00002719
Iteration 229/1000 | Loss: 0.00002719
Iteration 230/1000 | Loss: 0.00002719
Iteration 231/1000 | Loss: 0.00002719
Iteration 232/1000 | Loss: 0.00002719
Iteration 233/1000 | Loss: 0.00002719
Iteration 234/1000 | Loss: 0.00002719
Iteration 235/1000 | Loss: 0.00002719
Iteration 236/1000 | Loss: 0.00002719
Iteration 237/1000 | Loss: 0.00002719
Iteration 238/1000 | Loss: 0.00002719
Iteration 239/1000 | Loss: 0.00002719
Iteration 240/1000 | Loss: 0.00002719
Iteration 241/1000 | Loss: 0.00002719
Iteration 242/1000 | Loss: 0.00002719
Iteration 243/1000 | Loss: 0.00002719
Iteration 244/1000 | Loss: 0.00002719
Iteration 245/1000 | Loss: 0.00002719
Iteration 246/1000 | Loss: 0.00002719
Iteration 247/1000 | Loss: 0.00002719
Iteration 248/1000 | Loss: 0.00002719
Iteration 249/1000 | Loss: 0.00002719
Iteration 250/1000 | Loss: 0.00002719
Iteration 251/1000 | Loss: 0.00002719
Iteration 252/1000 | Loss: 0.00002719
Iteration 253/1000 | Loss: 0.00002719
Iteration 254/1000 | Loss: 0.00002719
Iteration 255/1000 | Loss: 0.00002719
Iteration 256/1000 | Loss: 0.00002719
Iteration 257/1000 | Loss: 0.00002719
Iteration 258/1000 | Loss: 0.00002719
Iteration 259/1000 | Loss: 0.00002719
Iteration 260/1000 | Loss: 0.00002719
Iteration 261/1000 | Loss: 0.00002719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.718772338994313e-05, 2.718772338994313e-05, 2.718772338994313e-05, 2.718772338994313e-05, 2.718772338994313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.718772338994313e-05

Optimization complete. Final v2v error: 3.636409044265747 mm

Highest mean error: 17.667896270751953 mm for frame 190

Lowest mean error: 2.706746816635132 mm for frame 154

Saving results

Total time: 206.6173391342163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780865
Iteration 2/25 | Loss: 0.00141840
Iteration 3/25 | Loss: 0.00103514
Iteration 4/25 | Loss: 0.00096055
Iteration 5/25 | Loss: 0.00093032
Iteration 6/25 | Loss: 0.00089944
Iteration 7/25 | Loss: 0.00089285
Iteration 8/25 | Loss: 0.00087421
Iteration 9/25 | Loss: 0.00087104
Iteration 10/25 | Loss: 0.00086983
Iteration 11/25 | Loss: 0.00087022
Iteration 12/25 | Loss: 0.00086930
Iteration 13/25 | Loss: 0.00086919
Iteration 14/25 | Loss: 0.00086888
Iteration 15/25 | Loss: 0.00086834
Iteration 16/25 | Loss: 0.00086805
Iteration 17/25 | Loss: 0.00086881
Iteration 18/25 | Loss: 0.00086712
Iteration 19/25 | Loss: 0.00086665
Iteration 20/25 | Loss: 0.00086947
Iteration 21/25 | Loss: 0.00087044
Iteration 22/25 | Loss: 0.00087002
Iteration 23/25 | Loss: 0.00086820
Iteration 24/25 | Loss: 0.00086888
Iteration 25/25 | Loss: 0.00086970

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.00916435
Iteration 2/25 | Loss: 0.00189238
Iteration 3/25 | Loss: 0.00117983
Iteration 4/25 | Loss: 0.00107705
Iteration 5/25 | Loss: 0.00106603
Iteration 6/25 | Loss: 0.00106522
Iteration 7/25 | Loss: 0.00106522
Iteration 8/25 | Loss: 0.00106522
Iteration 9/25 | Loss: 0.00106522
Iteration 10/25 | Loss: 0.00106522
Iteration 11/25 | Loss: 0.00106522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001065224059857428, 0.001065224059857428, 0.001065224059857428, 0.001065224059857428, 0.001065224059857428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001065224059857428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07445168
Iteration 2/25 | Loss: 0.00111412
Iteration 3/25 | Loss: 0.00111405
Iteration 4/25 | Loss: 0.00111405
Iteration 5/25 | Loss: 0.00111405
Iteration 6/25 | Loss: 0.00111405
Iteration 7/25 | Loss: 0.00111405
Iteration 8/25 | Loss: 0.00111405
Iteration 9/25 | Loss: 0.00111405
Iteration 10/25 | Loss: 0.00111405
Iteration 11/25 | Loss: 0.00111405
Iteration 12/25 | Loss: 0.00111405
Iteration 13/25 | Loss: 0.00111405
Iteration 14/25 | Loss: 0.00111405
Iteration 15/25 | Loss: 0.00111405
Iteration 16/25 | Loss: 0.00111405
Iteration 17/25 | Loss: 0.00111405
Iteration 18/25 | Loss: 0.00111405
Iteration 19/25 | Loss: 0.00111405
Iteration 20/25 | Loss: 0.00111405
Iteration 21/25 | Loss: 0.00111405
Iteration 22/25 | Loss: 0.00111405
Iteration 23/25 | Loss: 0.00111405
Iteration 24/25 | Loss: 0.00111405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001114053069613874, 0.001114053069613874, 0.001114053069613874, 0.001114053069613874, 0.001114053069613874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001114053069613874

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 1.27875209
Iteration 2/25 | Loss: 0.00040137
Iteration 3/25 | Loss: 0.00040137
Iteration 4/25 | Loss: 0.00040137
Iteration 5/25 | Loss: 0.00040137
Iteration 6/25 | Loss: 0.00040137
Iteration 7/25 | Loss: 0.00040137
Iteration 8/25 | Loss: 0.00040137
Iteration 9/25 | Loss: 0.00040137
Iteration 10/25 | Loss: 0.00040137
Iteration 11/25 | Loss: 0.00040137
Iteration 12/25 | Loss: 0.00040137
Iteration 13/25 | Loss: 0.00040137
Iteration 14/25 | Loss: 0.00040137
Iteration 15/25 | Loss: 0.00040137
Iteration 16/25 | Loss: 0.00040137
Iteration 17/25 | Loss: 0.00040137
Iteration 18/25 | Loss: 0.00040137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00040136705501936376, 0.00040136705501936376, 0.00040136705501936376, 0.00040136705501936376, 0.00040136705501936376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040136705501936376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040137
Iteration 2/1000 | Loss: 0.00004948
Iteration 3/1000 | Loss: 0.00003311
Iteration 4/1000 | Loss: 0.00003069
Iteration 5/1000 | Loss: 0.00002950
Iteration 6/1000 | Loss: 0.00002886
Iteration 7/1000 | Loss: 0.00002844
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002770
Iteration 10/1000 | Loss: 0.00002749
Iteration 11/1000 | Loss: 0.00002730
Iteration 12/1000 | Loss: 0.00002720
Iteration 13/1000 | Loss: 0.00002714
Iteration 14/1000 | Loss: 0.00002713
Iteration 15/1000 | Loss: 0.00002707
Iteration 16/1000 | Loss: 0.00002701
Iteration 17/1000 | Loss: 0.00002695
Iteration 18/1000 | Loss: 0.00002694
Iteration 19/1000 | Loss: 0.00002694
Iteration 20/1000 | Loss: 0.00002693
Iteration 21/1000 | Loss: 0.00002693
Iteration 22/1000 | Loss: 0.00002693
Iteration 23/1000 | Loss: 0.00002693
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00002691
Iteration 26/1000 | Loss: 0.00002690
Iteration 27/1000 | Loss: 0.00002690
Iteration 28/1000 | Loss: 0.00002690
Iteration 29/1000 | Loss: 0.00002688
Iteration 30/1000 | Loss: 0.00002688
Iteration 31/1000 | Loss: 0.00002688
Iteration 32/1000 | Loss: 0.00002688
Iteration 33/1000 | Loss: 0.00002688
Iteration 34/1000 | Loss: 0.00002688
Iteration 35/1000 | Loss: 0.00002688
Iteration 36/1000 | Loss: 0.00002688
Iteration 37/1000 | Loss: 0.00002688
Iteration 38/1000 | Loss: 0.00002687
Iteration 39/1000 | Loss: 0.00002687
Iteration 40/1000 | Loss: 0.00002687
Iteration 41/1000 | Loss: 0.00002687
Iteration 42/1000 | Loss: 0.00002687
Iteration 43/1000 | Loss: 0.00002686
Iteration 44/1000 | Loss: 0.00002686
Iteration 45/1000 | Loss: 0.00002686
Iteration 46/1000 | Loss: 0.00002686
Iteration 47/1000 | Loss: 0.00002686
Iteration 48/1000 | Loss: 0.00002686
Iteration 49/1000 | Loss: 0.00002686
Iteration 50/1000 | Loss: 0.00002686
Iteration 51/1000 | Loss: 0.00002685
Iteration 52/1000 | Loss: 0.00002685
Iteration 53/1000 | Loss: 0.00002685
Iteration 54/1000 | Loss: 0.00002684
Iteration 55/1000 | Loss: 0.00002684
Iteration 56/1000 | Loss: 0.00002684
Iteration 57/1000 | Loss: 0.00002684
Iteration 58/1000 | Loss: 0.00002684
Iteration 59/1000 | Loss: 0.00002684
Iteration 60/1000 | Loss: 0.00002684
Iteration 61/1000 | Loss: 0.00002684
Iteration 62/1000 | Loss: 0.00002683
Iteration 63/1000 | Loss: 0.00002683
Iteration 64/1000 | Loss: 0.00002683
Iteration 65/1000 | Loss: 0.00002683
Iteration 66/1000 | Loss: 0.00002683
Iteration 67/1000 | Loss: 0.00002683
Iteration 68/1000 | Loss: 0.00002683
Iteration 69/1000 | Loss: 0.00002683
Iteration 70/1000 | Loss: 0.00002683
Iteration 71/1000 | Loss: 0.00002683
Iteration 72/1000 | Loss: 0.00002683
Iteration 73/1000 | Loss: 0.00002682
Iteration 74/1000 | Loss: 0.00002682
Iteration 75/1000 | Loss: 0.00002682
Iteration 76/1000 | Loss: 0.00002681
Iteration 77/1000 | Loss: 0.00002681
Iteration 78/1000 | Loss: 0.00002681
Iteration 79/1000 | Loss: 0.00002681
Iteration 80/1000 | Loss: 0.00002681
Iteration 81/1000 | Loss: 0.00002681
Iteration 82/1000 | Loss: 0.00002680
Iteration 83/1000 | Loss: 0.00002680
Iteration 84/1000 | Loss: 0.00002680
Iteration 85/1000 | Loss: 0.00002680
Iteration 86/1000 | Loss: 0.00002680
Iteration 87/1000 | Loss: 0.00002680
Iteration 88/1000 | Loss: 0.00002680
Iteration 89/1000 | Loss: 0.00002680
Iteration 90/1000 | Loss: 0.00002679
Iteration 91/1000 | Loss: 0.00002679
Iteration 92/1000 | Loss: 0.00002679
Iteration 93/1000 | Loss: 0.00002679
Iteration 94/1000 | Loss: 0.00002679
Iteration 95/1000 | Loss: 0.00002679
Iteration 96/1000 | Loss: 0.00002679
Iteration 97/1000 | Loss: 0.00002679
Iteration 98/1000 | Loss: 0.00002679
Iteration 99/1000 | Loss: 0.00002679
Iteration 100/1000 | Loss: 0.00002679
Iteration 101/1000 | Loss: 0.00002679
Iteration 102/1000 | Loss: 0.00002679
Iteration 103/1000 | Loss: 0.00002679
Iteration 104/1000 | Loss: 0.00002679
Iteration 105/1000 | Loss: 0.00002679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.6785706722876057e-05, 2.6785706722876057e-05, 2.6785706722876057e-05, 2.6785706722876057e-05, 2.6785706722876057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6785706722876057e-05

Optimization complete. Final v2v error: 4.483856678009033 mm

Highest mean error: 4.738962650299072 mm for frame 160

Lowest mean error: 4.318632125854492 mm for frame 198

Saving results

Total time: 38.67312264442444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0574/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0574/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095846
Iteration 2/25 | Loss: 0.00253743
Iteration 3/25 | Loss: 0.00179563
Iteration 4/25 | Loss: 0.00180646
Iteration 5/25 | Loss: 0.00140716
Iteration 6/25 | Loss: 0.00121429
Iteration 7/25 | Loss: 0.00109701
Iteration 8/25 | Loss: 0.00104295
Iteration 9/25 | Loss: 0.00103225
Iteration 10/25 | Loss: 0.00102065
Iteration 11/25 | Loss: 0.00101875
Iteration 12/25 | Loss: 0.00101843
Iteration 13/25 | Loss: 0.00101521
Iteration 14/25 | Loss: 0.00101809
Iteration 15/25 | Loss: 0.00101508
Iteration 16/25 | Loss: 0.00101586
Iteration 17/25 | Loss: 0.00101330
Iteration 18/25 | Loss: 0.00100935
Iteration 19/25 | Loss: 0.00100773
Iteration 20/25 | Loss: 0.00100596
Iteration 21/25 | Loss: 0.00100663
Iteration 22/25 | Loss: 0.00100612
Iteration 23/25 | Loss: 0.00100441
Iteration 24/25 | Loss: 0.00100497
Iteration 25/25 | Loss: 0.00100505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43695641
Iteration 2/25 | Loss: 0.00050137
Iteration 3/25 | Loss: 0.00050137
Iteration 4/25 | Loss: 0.00050137
Iteration 5/25 | Loss: 0.00050137
Iteration 6/25 | Loss: 0.00050137
Iteration 7/25 | Loss: 0.00050137
Iteration 8/25 | Loss: 0.00050137
Iteration 9/25 | Loss: 0.00050137
Iteration 10/25 | Loss: 0.00050137
Iteration 11/25 | Loss: 0.00050137
Iteration 12/25 | Loss: 0.00050137
Iteration 13/25 | Loss: 0.00050137
Iteration 14/25 | Loss: 0.00050137
Iteration 15/25 | Loss: 0.00050137
Iteration 16/25 | Loss: 0.00050137
Iteration 17/25 | Loss: 0.00050137
Iteration 18/25 | Loss: 0.00050137
Iteration 19/25 | Loss: 0.00050137
Iteration 20/25 | Loss: 0.00050137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005013702902942896, 0.0005013702902942896, 0.0005013702902942896, 0.0005013702902942896, 0.0005013702902942896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005013702902942896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243629
Iteration 2/1000 | Loss: 0.00026460
Iteration 3/1000 | Loss: 0.00021393
Iteration 4/1000 | Loss: 0.00017947
Iteration 5/1000 | Loss: 0.00014662
Iteration 6/1000 | Loss: 0.00012096
Iteration 7/1000 | Loss: 0.00011848
Iteration 8/1000 | Loss: 0.00011081
Iteration 9/1000 | Loss: 0.00009011
Iteration 10/1000 | Loss: 0.00009341
Iteration 11/1000 | Loss: 0.00255451
Iteration 12/1000 | Loss: 0.00173262
Iteration 13/1000 | Loss: 0.00022029
Iteration 14/1000 | Loss: 0.00015265
Iteration 15/1000 | Loss: 0.00011201
Iteration 16/1000 | Loss: 0.00008788
Iteration 17/1000 | Loss: 0.00005841
Iteration 18/1000 | Loss: 0.00005390
Iteration 19/1000 | Loss: 0.00004694
Iteration 20/1000 | Loss: 0.00005893
Iteration 21/1000 | Loss: 0.00007469
Iteration 22/1000 | Loss: 0.00004812
Iteration 23/1000 | Loss: 0.00003800
Iteration 24/1000 | Loss: 0.00004324
Iteration 25/1000 | Loss: 0.00003324
Iteration 26/1000 | Loss: 0.00004694
Iteration 27/1000 | Loss: 0.00002754
Iteration 28/1000 | Loss: 0.00004198
Iteration 29/1000 | Loss: 0.00004822
Iteration 30/1000 | Loss: 0.00004027
Iteration 31/1000 | Loss: 0.00004571
Iteration 32/1000 | Loss: 0.00003833
Iteration 33/1000 | Loss: 0.00004519
Iteration 34/1000 | Loss: 0.00004627
Iteration 35/1000 | Loss: 0.00004532
Iteration 36/1000 | Loss: 0.00004815
Iteration 37/1000 | Loss: 0.00004687
Iteration 38/1000 | Loss: 0.00004758
Iteration 39/1000 | Loss: 0.00004556
Iteration 40/1000 | Loss: 0.00004530
Iteration 41/1000 | Loss: 0.00004344
Iteration 42/1000 | Loss: 0.00004728
Iteration 43/1000 | Loss: 0.00004663
Iteration 44/1000 | Loss: 0.00004529
Iteration 45/1000 | Loss: 0.00004645
Iteration 46/1000 | Loss: 0.00004748
Iteration 47/1000 | Loss: 0.00004783
Iteration 48/1000 | Loss: 0.00003828
Iteration 49/1000 | Loss: 0.00005653
Iteration 50/1000 | Loss: 0.00003536
Iteration 51/1000 | Loss: 0.00004629
Iteration 52/1000 | Loss: 0.00003513
Iteration 53/1000 | Loss: 0.00004388
Iteration 54/1000 | Loss: 0.00003408
Iteration 55/1000 | Loss: 0.00004101
Iteration 56/1000 | Loss: 0.00003909
Iteration 57/1000 | Loss: 0.00004278
Iteration 58/1000 | Loss: 0.00004695
Iteration 59/1000 | Loss: 0.00003783
Iteration 60/1000 | Loss: 0.00003309
Iteration 61/1000 | Loss: 0.00004948
Iteration 62/1000 | Loss: 0.00004739
Iteration 63/1000 | Loss: 0.00005089
Iteration 64/1000 | Loss: 0.00004778
Iteration 65/1000 | Loss: 0.00004407
Iteration 66/1000 | Loss: 0.00003777
Iteration 67/1000 | Loss: 0.00003603
Iteration 68/1000 | Loss: 0.00004239
Iteration 69/1000 | Loss: 0.00004564
Iteration 70/1000 | Loss: 0.00004709
Iteration 71/1000 | Loss: 0.00004576
Iteration 72/1000 | Loss: 0.00004711
Iteration 73/1000 | Loss: 0.00004496
Iteration 74/1000 | Loss: 0.00004744
Iteration 75/1000 | Loss: 0.00004365
Iteration 76/1000 | Loss: 0.00003132
Iteration 77/1000 | Loss: 0.00004335
Iteration 78/1000 | Loss: 0.00004096
Iteration 79/1000 | Loss: 0.00005016
Iteration 80/1000 | Loss: 0.00003842
Iteration 81/1000 | Loss: 0.00004427
Iteration 82/1000 | Loss: 0.00004072
Iteration 83/1000 | Loss: 0.00005070
Iteration 84/1000 | Loss: 0.00003515
Iteration 85/1000 | Loss: 0.00003937
Iteration 86/1000 | Loss: 0.00004694
Iteration 87/1000 | Loss: 0.00004074
Iteration 88/1000 | Loss: 0.00003338
Iteration 89/1000 | Loss: 0.00004014
Iteration 90/1000 | Loss: 0.00002534
Iteration 91/1000 | Loss: 0.00002469
Iteration 92/1000 | Loss: 0.00002407
Iteration 93/1000 | Loss: 0.00002367
Iteration 94/1000 | Loss: 0.00002364
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002358
Iteration 97/1000 | Loss: 0.00002355
Iteration 98/1000 | Loss: 0.00002354
Iteration 99/1000 | Loss: 0.00002354
Iteration 100/1000 | Loss: 0.00002352
Iteration 101/1000 | Loss: 0.00002352
Iteration 102/1000 | Loss: 0.00002351
Iteration 103/1000 | Loss: 0.00002351
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002347
Iteration 106/1000 | Loss: 0.00002344
Iteration 107/1000 | Loss: 0.00002336
Iteration 108/1000 | Loss: 0.00002332
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002324
Iteration 111/1000 | Loss: 0.00002324
Iteration 112/1000 | Loss: 0.00002324
Iteration 113/1000 | Loss: 0.00002324
Iteration 114/1000 | Loss: 0.00002324
Iteration 115/1000 | Loss: 0.00002324
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002323
Iteration 120/1000 | Loss: 0.00002323
Iteration 121/1000 | Loss: 0.00002323
Iteration 122/1000 | Loss: 0.00002323
Iteration 123/1000 | Loss: 0.00002323
Iteration 124/1000 | Loss: 0.00002323
Iteration 125/1000 | Loss: 0.00002323
Iteration 126/1000 | Loss: 0.00002323
Iteration 127/1000 | Loss: 0.00002323
Iteration 128/1000 | Loss: 0.00002322
Iteration 129/1000 | Loss: 0.00002322
Iteration 130/1000 | Loss: 0.00002322
Iteration 131/1000 | Loss: 0.00002322
Iteration 132/1000 | Loss: 0.00002322
Iteration 133/1000 | Loss: 0.00002321
Iteration 134/1000 | Loss: 0.00002321
Iteration 135/1000 | Loss: 0.00002321
Iteration 136/1000 | Loss: 0.00002321
Iteration 137/1000 | Loss: 0.00002320
Iteration 138/1000 | Loss: 0.00002320
Iteration 139/1000 | Loss: 0.00002320
Iteration 140/1000 | Loss: 0.00002320
Iteration 141/1000 | Loss: 0.00002320
Iteration 142/1000 | Loss: 0.00002320
Iteration 143/1000 | Loss: 0.00002320
Iteration 144/1000 | Loss: 0.00002320
Iteration 145/1000 | Loss: 0.00002320
Iteration 146/1000 | Loss: 0.00002320
Iteration 147/1000 | Loss: 0.00002319
Iteration 148/1000 | Loss: 0.00002319
Iteration 149/1000 | Loss: 0.00002319
Iteration 150/1000 | Loss: 0.00002319
Iteration 151/1000 | Loss: 0.00002319
Iteration 152/1000 | Loss: 0.00002319
Iteration 153/1000 | Loss: 0.00002319
Iteration 154/1000 | Loss: 0.00002319
Iteration 155/1000 | Loss: 0.00002319
Iteration 156/1000 | Loss: 0.00002319
Iteration 157/1000 | Loss: 0.00002319
Iteration 158/1000 | Loss: 0.00002319
Iteration 159/1000 | Loss: 0.00002319
Iteration 160/1000 | Loss: 0.00002319
Iteration 161/1000 | Loss: 0.00002319
Iteration 162/1000 | Loss: 0.00002319
Iteration 163/1000 | Loss: 0.00002319
Iteration 164/1000 | Loss: 0.00002319
Iteration 165/1000 | Loss: 0.00002319
Iteration 166/1000 | Loss: 0.00002319
Iteration 167/1000 | Loss: 0.00002319
Iteration 168/1000 | Loss: 0.00002319
Iteration 169/1000 | Loss: 0.00002319
Iteration 170/1000 | Loss: 0.00002319
Iteration 171/1000 | Loss: 0.00002319
Iteration 172/1000 | Loss: 0.00002319
Iteration 173/1000 | Loss: 0.00002319
Iteration 174/1000 | Loss: 0.00002319
Iteration 175/1000 | Loss: 0.00002319
Iteration 176/1000 | Loss: 0.00002319
Iteration 177/1000 | Loss: 0.00002319
Iteration 178/1000 | Loss: 0.00002319
Iteration 179/1000 | Loss: 0.00002319
Iteration 180/1000 | Loss: 0.00002319
Iteration 181/1000 | Loss: 0.00002319
Iteration 182/1000 | Loss: 0.00002319
Iteration 183/1000 | Loss: 0.00002319
Iteration 184/1000 | Loss: 0.00002319
Iteration 185/1000 | Loss: 0.00002319
Iteration 186/1000 | Loss: 0.00002319
Iteration 187/1000 | Loss: 0.00002319
Iteration 188/1000 | Loss: 0.00002319
Iteration 189/1000 | Loss: 0.00002319
Iteration 190/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.3189737476059236e-05, 2.3189737476059236e-05, 2.3189737476059236e-05, 2.3189737476059236e-05, 2.3189737476059236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3189737476059236e-05

Optimization complete. Final v2v error: 4.203113555908203 mm

Highest mean error: 4.811579704284668 mm for frame 71

Lowest mean error: 3.80285906791687 mm for frame 17

Saving results

Total time: 177.2108826637268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00043100
Iteration 2/1000 | Loss: 0.00021888
Iteration 3/1000 | Loss: 0.00012545
Iteration 4/1000 | Loss: 0.00022706
Iteration 5/1000 | Loss: 0.00029906
Iteration 6/1000 | Loss: 0.00031734
Iteration 7/1000 | Loss: 0.00039061
Iteration 8/1000 | Loss: 0.00028418
Iteration 9/1000 | Loss: 0.00028998
Iteration 10/1000 | Loss: 0.00028593
Iteration 11/1000 | Loss: 0.00028678
Iteration 12/1000 | Loss: 0.00030919
Iteration 13/1000 | Loss: 0.00027832
Iteration 14/1000 | Loss: 0.00026262
Iteration 15/1000 | Loss: 0.00032456
Iteration 16/1000 | Loss: 0.00030013
Iteration 17/1000 | Loss: 0.00052978
Iteration 18/1000 | Loss: 0.00029469
Iteration 19/1000 | Loss: 0.00030376
Iteration 20/1000 | Loss: 0.00027605
Iteration 21/1000 | Loss: 0.00024009
Iteration 22/1000 | Loss: 0.00028136
Iteration 23/1000 | Loss: 0.00021139
Iteration 24/1000 | Loss: 0.00039273
Iteration 25/1000 | Loss: 0.00025354
Iteration 26/1000 | Loss: 0.00026731
Iteration 27/1000 | Loss: 0.00024331
Iteration 28/1000 | Loss: 0.00033212
Iteration 29/1000 | Loss: 0.00028912
Iteration 30/1000 | Loss: 0.00037823
Iteration 31/1000 | Loss: 0.00023177
Iteration 32/1000 | Loss: 0.00016773
Iteration 33/1000 | Loss: 0.00023775
Iteration 34/1000 | Loss: 0.00025745
Iteration 35/1000 | Loss: 0.00024436
Iteration 36/1000 | Loss: 0.00024215
Iteration 37/1000 | Loss: 0.00031837
Iteration 38/1000 | Loss: 0.00027303
Iteration 39/1000 | Loss: 0.00027300
Iteration 40/1000 | Loss: 0.00018249
Iteration 41/1000 | Loss: 0.00024300
Iteration 42/1000 | Loss: 0.00028022
Iteration 43/1000 | Loss: 0.00032574
Iteration 44/1000 | Loss: 0.00029784
Iteration 45/1000 | Loss: 0.00050232
Iteration 46/1000 | Loss: 0.00022674
Iteration 47/1000 | Loss: 0.00026242
Iteration 48/1000 | Loss: 0.00027622
Iteration 49/1000 | Loss: 0.00028434
Iteration 50/1000 | Loss: 0.00053350
Iteration 51/1000 | Loss: 0.00041006
Iteration 52/1000 | Loss: 0.00022654
Iteration 53/1000 | Loss: 0.00019508
Iteration 54/1000 | Loss: 0.00015792
Iteration 55/1000 | Loss: 0.00028038
Iteration 56/1000 | Loss: 0.00039111
Iteration 57/1000 | Loss: 0.00034447
Iteration 58/1000 | Loss: 0.00009832
Iteration 59/1000 | Loss: 0.00027738
Iteration 60/1000 | Loss: 0.00034019
Iteration 61/1000 | Loss: 0.00034246
Iteration 62/1000 | Loss: 0.00039223
Iteration 63/1000 | Loss: 0.00019473
Iteration 64/1000 | Loss: 0.00058163
Iteration 65/1000 | Loss: 0.00041167
Iteration 66/1000 | Loss: 0.00062516
Iteration 67/1000 | Loss: 0.00040656
Iteration 68/1000 | Loss: 0.00036539
Iteration 69/1000 | Loss: 0.00012816
Iteration 70/1000 | Loss: 0.00016687
Iteration 71/1000 | Loss: 0.00024647
Iteration 72/1000 | Loss: 0.00047725
Iteration 73/1000 | Loss: 0.00029505
Iteration 74/1000 | Loss: 0.00028647
Iteration 75/1000 | Loss: 0.00028966
Iteration 76/1000 | Loss: 0.00014267
Iteration 77/1000 | Loss: 0.00039029
Iteration 78/1000 | Loss: 0.00043333
Iteration 79/1000 | Loss: 0.00041014
Iteration 80/1000 | Loss: 0.00012062
Iteration 81/1000 | Loss: 0.00010129
Iteration 82/1000 | Loss: 0.00008125
Iteration 83/1000 | Loss: 0.00005150
Iteration 84/1000 | Loss: 0.00009786
Iteration 85/1000 | Loss: 0.00010345
Iteration 86/1000 | Loss: 0.00011069
Iteration 87/1000 | Loss: 0.00008673
Iteration 88/1000 | Loss: 0.00009097
Iteration 89/1000 | Loss: 0.00008126
Iteration 90/1000 | Loss: 0.00008458
Iteration 91/1000 | Loss: 0.00010035
Iteration 92/1000 | Loss: 0.00004757
Iteration 93/1000 | Loss: 0.00008152
Iteration 94/1000 | Loss: 0.00007880
Iteration 95/1000 | Loss: 0.00008479
Iteration 96/1000 | Loss: 0.00014008
Iteration 97/1000 | Loss: 0.00008224
Iteration 98/1000 | Loss: 0.00002669
Iteration 99/1000 | Loss: 0.00002325
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002117
Iteration 102/1000 | Loss: 0.00002073
Iteration 103/1000 | Loss: 0.00002034
Iteration 104/1000 | Loss: 0.00002004
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001944
Iteration 111/1000 | Loss: 0.00001944
Iteration 112/1000 | Loss: 0.00001942
Iteration 113/1000 | Loss: 0.00001941
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001939
Iteration 118/1000 | Loss: 0.00001936
Iteration 119/1000 | Loss: 0.00001935
Iteration 120/1000 | Loss: 0.00001934
Iteration 121/1000 | Loss: 0.00001933
Iteration 122/1000 | Loss: 0.00001932
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001930
Iteration 125/1000 | Loss: 0.00001930
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001927
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001924
Iteration 139/1000 | Loss: 0.00001924
Iteration 140/1000 | Loss: 0.00001923
Iteration 141/1000 | Loss: 0.00001923
Iteration 142/1000 | Loss: 0.00001923
Iteration 143/1000 | Loss: 0.00001923
Iteration 144/1000 | Loss: 0.00001923
Iteration 145/1000 | Loss: 0.00001923
Iteration 146/1000 | Loss: 0.00001923
Iteration 147/1000 | Loss: 0.00001923
Iteration 148/1000 | Loss: 0.00001923
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001922
Iteration 151/1000 | Loss: 0.00001922
Iteration 152/1000 | Loss: 0.00001922
Iteration 153/1000 | Loss: 0.00001921
Iteration 154/1000 | Loss: 0.00001921
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001920
Iteration 157/1000 | Loss: 0.00001920
Iteration 158/1000 | Loss: 0.00001919
Iteration 159/1000 | Loss: 0.00001919
Iteration 160/1000 | Loss: 0.00001919
Iteration 161/1000 | Loss: 0.00001919
Iteration 162/1000 | Loss: 0.00001919
Iteration 163/1000 | Loss: 0.00001919
Iteration 164/1000 | Loss: 0.00001918
Iteration 165/1000 | Loss: 0.00001918
Iteration 166/1000 | Loss: 0.00001918
Iteration 167/1000 | Loss: 0.00001918
Iteration 168/1000 | Loss: 0.00001918
Iteration 169/1000 | Loss: 0.00001918
Iteration 170/1000 | Loss: 0.00001917
Iteration 171/1000 | Loss: 0.00001917
Iteration 172/1000 | Loss: 0.00001917
Iteration 173/1000 | Loss: 0.00001917
Iteration 174/1000 | Loss: 0.00001917
Iteration 175/1000 | Loss: 0.00001917
Iteration 176/1000 | Loss: 0.00001917
Iteration 177/1000 | Loss: 0.00001917
Iteration 178/1000 | Loss: 0.00001917
Iteration 179/1000 | Loss: 0.00001917
Iteration 180/1000 | Loss: 0.00001917
Iteration 181/1000 | Loss: 0.00001916
Iteration 182/1000 | Loss: 0.00001916
Iteration 183/1000 | Loss: 0.00001916
Iteration 184/1000 | Loss: 0.00001916
Iteration 185/1000 | Loss: 0.00001916
Iteration 186/1000 | Loss: 0.00001916
Iteration 187/1000 | Loss: 0.00001916
Iteration 188/1000 | Loss: 0.00001916
Iteration 189/1000 | Loss: 0.00001916
Iteration 190/1000 | Loss: 0.00001916
Iteration 191/1000 | Loss: 0.00001916
Iteration 192/1000 | Loss: 0.00001916
Iteration 193/1000 | Loss: 0.00001916
Iteration 194/1000 | Loss: 0.00001916
Iteration 195/1000 | Loss: 0.00001916
Iteration 196/1000 | Loss: 0.00001916
Iteration 197/1000 | Loss: 0.00001915
Iteration 198/1000 | Loss: 0.00001915
Iteration 199/1000 | Loss: 0.00001915
Iteration 200/1000 | Loss: 0.00001915
Iteration 201/1000 | Loss: 0.00001915
Iteration 202/1000 | Loss: 0.00001915
Iteration 203/1000 | Loss: 0.00001915
Iteration 204/1000 | Loss: 0.00001915
Iteration 205/1000 | Loss: 0.00001915
Iteration 206/1000 | Loss: 0.00001915
Iteration 207/1000 | Loss: 0.00001915
Iteration 208/1000 | Loss: 0.00001915
Iteration 209/1000 | Loss: 0.00001915
Iteration 210/1000 | Loss: 0.00001915
Iteration 211/1000 | Loss: 0.00001915
Iteration 212/1000 | Loss: 0.00001915
Iteration 213/1000 | Loss: 0.00001915
Iteration 214/1000 | Loss: 0.00001915
Iteration 215/1000 | Loss: 0.00001914
Iteration 216/1000 | Loss: 0.00001914
Iteration 217/1000 | Loss: 0.00001914
Iteration 218/1000 | Loss: 0.00001914
Iteration 219/1000 | Loss: 0.00001914
Iteration 220/1000 | Loss: 0.00001914
Iteration 221/1000 | Loss: 0.00001914
Iteration 222/1000 | Loss: 0.00001914
Iteration 223/1000 | Loss: 0.00001914
Iteration 224/1000 | Loss: 0.00001914
Iteration 225/1000 | Loss: 0.00001914
Iteration 226/1000 | Loss: 0.00001914
Iteration 227/1000 | Loss: 0.00001914
Iteration 228/1000 | Loss: 0.00001914
Iteration 229/1000 | Loss: 0.00001914
Iteration 230/1000 | Loss: 0.00001914
Iteration 231/1000 | Loss: 0.00001914
Iteration 232/1000 | Loss: 0.00001914
Iteration 233/1000 | Loss: 0.00001914
Iteration 234/1000 | Loss: 0.00001914
Iteration 235/1000 | Loss: 0.00001914
Iteration 236/1000 | Loss: 0.00001913
Iteration 237/1000 | Loss: 0.00001913
Iteration 238/1000 | Loss: 0.00001913
Iteration 239/1000 | Loss: 0.00001913
Iteration 240/1000 | Loss: 0.00001913
Iteration 241/1000 | Loss: 0.00001913
Iteration 242/1000 | Loss: 0.00001913
Iteration 243/1000 | Loss: 0.00001913
Iteration 244/1000 | Loss: 0.00001913
Iteration 245/1000 | Loss: 0.00001913
Iteration 246/1000 | Loss: 0.00001912
Iteration 247/1000 | Loss: 0.00001912
Iteration 248/1000 | Loss: 0.00001912
Iteration 249/1000 | Loss: 0.00001912
Iteration 250/1000 | Loss: 0.00001911
Iteration 251/1000 | Loss: 0.00001911
Iteration 252/1000 | Loss: 0.00001911
Iteration 253/1000 | Loss: 0.00001911
Iteration 254/1000 | Loss: 0.00001911
Iteration 255/1000 | Loss: 0.00001910
Iteration 256/1000 | Loss: 0.00001910
Iteration 257/1000 | Loss: 0.00001910
Iteration 258/1000 | Loss: 0.00001910
Iteration 259/1000 | Loss: 0.00001910
Iteration 260/1000 | Loss: 0.00001910
Iteration 261/1000 | Loss: 0.00001910
Iteration 262/1000 | Loss: 0.00001910
Iteration 263/1000 | Loss: 0.00001910
Iteration 264/1000 | Loss: 0.00001910
Iteration 265/1000 | Loss: 0.00001910
Iteration 266/1000 | Loss: 0.00001910
Iteration 267/1000 | Loss: 0.00001910
Iteration 268/1000 | Loss: 0.00001910
Iteration 269/1000 | Loss: 0.00001910
Iteration 270/1000 | Loss: 0.00001910
Iteration 271/1000 | Loss: 0.00001910
Iteration 272/1000 | Loss: 0.00001910
Iteration 273/1000 | Loss: 0.00001910
Iteration 274/1000 | Loss: 0.00001910
Iteration 275/1000 | Loss: 0.00001910
Iteration 276/1000 | Loss: 0.00001910
Iteration 277/1000 | Loss: 0.00001910
Iteration 278/1000 | Loss: 0.00001910
Iteration 279/1000 | Loss: 0.00001910
Iteration 280/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [1.9099728888249956e-05, 1.9099728888249956e-05, 1.9099728888249956e-05, 1.9099728888249956e-05, 1.9099728888249956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9099728888249956e-05

Optimization complete. Final v2v error: 3.795022964477539 mm

Highest mean error: 4.99516487121582 mm for frame 166

Lowest mean error: 3.2299017906188965 mm for frame 199

Saving results

Total time: 238.7805576324463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0009/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0009. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0008/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0008. Skipping.
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0014/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442213
Iteration 2/25 | Loss: 0.00123451
Iteration 3/25 | Loss: 0.00115280
Iteration 4/25 | Loss: 0.00112726
Iteration 5/25 | Loss: 0.00111966
Iteration 6/25 | Loss: 0.00111883
Iteration 7/25 | Loss: 0.00111883
Iteration 8/25 | Loss: 0.00111883
Iteration 9/25 | Loss: 0.00111883
Iteration 10/25 | Loss: 0.00111883
Iteration 11/25 | Loss: 0.00111883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001118829008191824, 0.001118829008191824, 0.001118829008191824, 0.001118829008191824, 0.001118829008191824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118829008191824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39940464
Iteration 2/25 | Loss: 0.00148332
Iteration 3/25 | Loss: 0.00148332
Iteration 4/25 | Loss: 0.00148332
Iteration 5/25 | Loss: 0.00148332
Iteration 6/25 | Loss: 0.00148332
Iteration 7/25 | Loss: 0.00148332
Iteration 8/25 | Loss: 0.00148332
Iteration 9/25 | Loss: 0.00148332
Iteration 10/25 | Loss: 0.00148332
Iteration 11/25 | Loss: 0.00148332
Iteration 12/25 | Loss: 0.00148332
Iteration 13/25 | Loss: 0.00148332
Iteration 14/25 | Loss: 0.00148332
Iteration 15/25 | Loss: 0.00148332
Iteration 16/25 | Loss: 0.00148332
Iteration 17/25 | Loss: 0.00148332
Iteration 18/25 | Loss: 0.00148332
Iteration 19/25 | Loss: 0.00148332
Iteration 20/25 | Loss: 0.00148332
Iteration 21/25 | Loss: 0.00148332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014833220047876239, 0.0014833220047876239, 0.0014833220047876239, 0.0014833220047876239, 0.0014833220047876239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014833220047876239

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00442213
Iteration 2/25 | Loss: 0.00125056
Iteration 3/25 | Loss: 0.00115230
Iteration 4/25 | Loss: 0.00112472
Iteration 5/25 | Loss: 0.00111931
Iteration 6/25 | Loss: 0.00111872
Iteration 7/25 | Loss: 0.00111872
Iteration 8/25 | Loss: 0.00111872
Iteration 9/25 | Loss: 0.00111872
Iteration 10/25 | Loss: 0.00111872
Iteration 11/25 | Loss: 0.00111872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011187208583578467, 0.0011187208583578467, 0.0011187208583578467, 0.0011187208583578467, 0.0011187208583578467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011187208583578467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39949715
Iteration 2/25 | Loss: 0.00148952
Iteration 3/25 | Loss: 0.00148952
Iteration 4/25 | Loss: 0.00148952
Iteration 5/25 | Loss: 0.00148952
Iteration 6/25 | Loss: 0.00148952
Iteration 7/25 | Loss: 0.00148952
Iteration 8/25 | Loss: 0.00148952
Iteration 9/25 | Loss: 0.00148952
Iteration 10/25 | Loss: 0.00148952
Iteration 11/25 | Loss: 0.00148952
Iteration 12/25 | Loss: 0.00148952
Iteration 13/25 | Loss: 0.00148952
Iteration 14/25 | Loss: 0.00148952
Iteration 15/25 | Loss: 0.00148952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014895220519974828, 0.0014895220519974828, 0.0014895220519974828, 0.0014895220519974828, 0.0014895220519974828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014895220519974828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111405
Iteration 2/1000 | Loss: 0.00064565
Iteration 3/1000 | Loss: 0.00058704
Iteration 4/1000 | Loss: 0.00027913
Iteration 5/1000 | Loss: 0.00008977
Iteration 6/1000 | Loss: 0.00006211
Iteration 7/1000 | Loss: 0.00054204
Iteration 8/1000 | Loss: 0.00050722
Iteration 9/1000 | Loss: 0.00021316
Iteration 10/1000 | Loss: 0.00061702
Iteration 11/1000 | Loss: 0.00005107
Iteration 12/1000 | Loss: 0.00004434
Iteration 13/1000 | Loss: 0.00041499
Iteration 14/1000 | Loss: 0.00018131
Iteration 15/1000 | Loss: 0.00005133
Iteration 16/1000 | Loss: 0.00003400
Iteration 17/1000 | Loss: 0.00003294
Iteration 18/1000 | Loss: 0.00003234
Iteration 19/1000 | Loss: 0.00003159
Iteration 20/1000 | Loss: 0.00004762
Iteration 21/1000 | Loss: 0.00003135
Iteration 22/1000 | Loss: 0.00003044
Iteration 23/1000 | Loss: 0.00003009
Iteration 24/1000 | Loss: 0.00047795
Iteration 25/1000 | Loss: 0.00041380
Iteration 26/1000 | Loss: 0.00034046
Iteration 27/1000 | Loss: 0.00024877
Iteration 28/1000 | Loss: 0.00044322
Iteration 29/1000 | Loss: 0.00013323
Iteration 30/1000 | Loss: 0.00004616
Iteration 31/1000 | Loss: 0.00061389
Iteration 32/1000 | Loss: 0.00063052
Iteration 33/1000 | Loss: 0.00021929
Iteration 34/1000 | Loss: 0.00043207
Iteration 35/1000 | Loss: 0.00015905
Iteration 36/1000 | Loss: 0.00003729
Iteration 37/1000 | Loss: 0.00008209
Iteration 38/1000 | Loss: 0.00003325
Iteration 39/1000 | Loss: 0.00009537
Iteration 40/1000 | Loss: 0.00008431
Iteration 41/1000 | Loss: 0.00008934
Iteration 42/1000 | Loss: 0.00005836
Iteration 43/1000 | Loss: 0.00038960
Iteration 44/1000 | Loss: 0.00013221
Iteration 45/1000 | Loss: 0.00019859
Iteration 46/1000 | Loss: 0.00037525
Iteration 47/1000 | Loss: 0.00067649
Iteration 48/1000 | Loss: 0.00006812
Iteration 49/1000 | Loss: 0.00058221
Iteration 50/1000 | Loss: 0.00033283
Iteration 51/1000 | Loss: 0.00003860
Iteration 52/1000 | Loss: 0.00006256
Iteration 53/1000 | Loss: 0.00009777
Iteration 54/1000 | Loss: 0.00004228
Iteration 55/1000 | Loss: 0.00034815
Iteration 56/1000 | Loss: 0.00066930
Iteration 57/1000 | Loss: 0.00050148
Iteration 58/1000 | Loss: 0.00005756
Iteration 59/1000 | Loss: 0.00004088
Iteration 60/1000 | Loss: 0.00003552
Iteration 61/1000 | Loss: 0.00003275
Iteration 62/1000 | Loss: 0.00003166
Iteration 63/1000 | Loss: 0.00003034
Iteration 64/1000 | Loss: 0.00071103
Iteration 65/1000 | Loss: 0.00005260
Iteration 66/1000 | Loss: 0.00003548
Iteration 67/1000 | Loss: 0.00002850
Iteration 68/1000 | Loss: 0.00002781
Iteration 69/1000 | Loss: 0.00002986
Iteration 70/1000 | Loss: 0.00002668
Iteration 71/1000 | Loss: 0.00002613
Iteration 72/1000 | Loss: 0.00002583
Iteration 73/1000 | Loss: 0.00002566
Iteration 74/1000 | Loss: 0.00002556
Iteration 75/1000 | Loss: 0.00002541
Iteration 76/1000 | Loss: 0.00002531
Iteration 77/1000 | Loss: 0.00003454
Iteration 78/1000 | Loss: 0.00002518
Iteration 79/1000 | Loss: 0.00002517
Iteration 80/1000 | Loss: 0.00002516
Iteration 81/1000 | Loss: 0.00002514
Iteration 82/1000 | Loss: 0.00002514
Iteration 83/1000 | Loss: 0.00002512
Iteration 84/1000 | Loss: 0.00002512
Iteration 85/1000 | Loss: 0.00002512
Iteration 86/1000 | Loss: 0.00002512
Iteration 87/1000 | Loss: 0.00002512
Iteration 88/1000 | Loss: 0.00002512
Iteration 89/1000 | Loss: 0.00002512
Iteration 90/1000 | Loss: 0.00002511
Iteration 91/1000 | Loss: 0.00002511
Iteration 92/1000 | Loss: 0.00002511
Iteration 93/1000 | Loss: 0.00002511
Iteration 94/1000 | Loss: 0.00002511
Iteration 95/1000 | Loss: 0.00002510
Iteration 96/1000 | Loss: 0.00002510
Iteration 97/1000 | Loss: 0.00002510
Iteration 98/1000 | Loss: 0.00002509
Iteration 99/1000 | Loss: 0.00002509
Iteration 100/1000 | Loss: 0.00002509
Iteration 101/1000 | Loss: 0.00002508
Iteration 102/1000 | Loss: 0.00002508
Iteration 103/1000 | Loss: 0.00002508
Iteration 104/1000 | Loss: 0.00003115
Iteration 105/1000 | Loss: 0.00003114
Iteration 106/1000 | Loss: 0.00003114
Iteration 107/1000 | Loss: 0.00003114
Iteration 108/1000 | Loss: 0.00003114
Iteration 109/1000 | Loss: 0.00003114
Iteration 110/1000 | Loss: 0.00003113
Iteration 111/1000 | Loss: 0.00010995
Iteration 112/1000 | Loss: 0.00002506
Iteration 113/1000 | Loss: 0.00002505
Iteration 114/1000 | Loss: 0.00002505
Iteration 115/1000 | Loss: 0.00002505
Iteration 116/1000 | Loss: 0.00002505
Iteration 117/1000 | Loss: 0.00002505
Iteration 118/1000 | Loss: 0.00002505
Iteration 119/1000 | Loss: 0.00002505
Iteration 120/1000 | Loss: 0.00002505
Iteration 121/1000 | Loss: 0.00002505
Iteration 122/1000 | Loss: 0.00002504
Iteration 123/1000 | Loss: 0.00002504
Iteration 124/1000 | Loss: 0.00002504
Iteration 125/1000 | Loss: 0.00002504
Iteration 126/1000 | Loss: 0.00002504
Iteration 127/1000 | Loss: 0.00002503
Iteration 128/1000 | Loss: 0.00002503
Iteration 129/1000 | Loss: 0.00002503
Iteration 130/1000 | Loss: 0.00002503
Iteration 131/1000 | Loss: 0.00002503
Iteration 132/1000 | Loss: 0.00002503
Iteration 133/1000 | Loss: 0.00002503
Iteration 134/1000 | Loss: 0.00002503
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.503275936760474e-05, 2.503275936760474e-05, 2.503275936760474e-05, 2.503275936760474e-05, 2.503275936760474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.503275936760474e-05

Optimization complete. Final v2v error: 4.156522274017334 mm

Highest mean error: 5.961162567138672 mm for frame 147

Lowest mean error: 3.74198317527771 mm for frame 212

Saving results

Total time: 182.64475226402283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00148332
Iteration 2/1000 | Loss: 0.00007941
Iteration 3/1000 | Loss: 0.00004153
Iteration 4/1000 | Loss: 0.00003317
Iteration 5/1000 | Loss: 0.00003023
Iteration 6/1000 | Loss: 0.00002828
Iteration 7/1000 | Loss: 0.00002735
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002618
Iteration 10/1000 | Loss: 0.00002579
Iteration 11/1000 | Loss: 0.00002544
Iteration 12/1000 | Loss: 0.00002522
Iteration 13/1000 | Loss: 0.00002511
Iteration 14/1000 | Loss: 0.00002498
Iteration 15/1000 | Loss: 0.00002493
Iteration 16/1000 | Loss: 0.00002492
Iteration 17/1000 | Loss: 0.00002491
Iteration 18/1000 | Loss: 0.00002489
Iteration 19/1000 | Loss: 0.00002488
Iteration 20/1000 | Loss: 0.00002487
Iteration 21/1000 | Loss: 0.00002487
Iteration 22/1000 | Loss: 0.00002487
Iteration 23/1000 | Loss: 0.00002487
Iteration 24/1000 | Loss: 0.00002486
Iteration 25/1000 | Loss: 0.00002486
Iteration 26/1000 | Loss: 0.00002486
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002484
Iteration 29/1000 | Loss: 0.00002484
Iteration 30/1000 | Loss: 0.00002483
Iteration 31/1000 | Loss: 0.00002482
Iteration 32/1000 | Loss: 0.00002480
Iteration 33/1000 | Loss: 0.00002480
Iteration 34/1000 | Loss: 0.00002480
Iteration 35/1000 | Loss: 0.00002480
Iteration 36/1000 | Loss: 0.00002480
Iteration 37/1000 | Loss: 0.00002479
Iteration 38/1000 | Loss: 0.00002479
Iteration 39/1000 | Loss: 0.00002478
Iteration 40/1000 | Loss: 0.00002475
Iteration 41/1000 | Loss: 0.00002474
Iteration 42/1000 | Loss: 0.00002473
Iteration 43/1000 | Loss: 0.00002473
Iteration 44/1000 | Loss: 0.00002473
Iteration 45/1000 | Loss: 0.00002473
Iteration 46/1000 | Loss: 0.00002472
Iteration 47/1000 | Loss: 0.00002472
Iteration 48/1000 | Loss: 0.00002472
Iteration 49/1000 | Loss: 0.00002471
Iteration 50/1000 | Loss: 0.00002471
Iteration 51/1000 | Loss: 0.00002470
Iteration 52/1000 | Loss: 0.00002470
Iteration 53/1000 | Loss: 0.00002469
Iteration 54/1000 | Loss: 0.00002469
Iteration 55/1000 | Loss: 0.00002468
Iteration 56/1000 | Loss: 0.00002468
Iteration 57/1000 | Loss: 0.00002468
Iteration 58/1000 | Loss: 0.00002467
Iteration 59/1000 | Loss: 0.00002467
Iteration 60/1000 | Loss: 0.00002467
Iteration 61/1000 | Loss: 0.00002466
Iteration 62/1000 | Loss: 0.00002466
Iteration 63/1000 | Loss: 0.00002466
Iteration 64/1000 | Loss: 0.00002466
Iteration 65/1000 | Loss: 0.00002465
Iteration 66/1000 | Loss: 0.00002465
Iteration 67/1000 | Loss: 0.00002465
Iteration 68/1000 | Loss: 0.00002464
Iteration 69/1000 | Loss: 0.00002464
Iteration 70/1000 | Loss: 0.00002464
Iteration 71/1000 | Loss: 0.00002463
Iteration 72/1000 | Loss: 0.00002463
Iteration 73/1000 | Loss: 0.00002463
Iteration 74/1000 | Loss: 0.00002463
Iteration 75/1000 | Loss: 0.00002463
Iteration 76/1000 | Loss: 0.00002462
Iteration 77/1000 | Loss: 0.00002462
Iteration 78/1000 | Loss: 0.00002462
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002461
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002461
Iteration 84/1000 | Loss: 0.00002461
Iteration 85/1000 | Loss: 0.00002460
Iteration 86/1000 | Loss: 0.00002460
Iteration 87/1000 | Loss: 0.00002460
Iteration 88/1000 | Loss: 0.00002460
Iteration 89/1000 | Loss: 0.00002459
Iteration 90/1000 | Loss: 0.00002459
Iteration 91/1000 | Loss: 0.00002459
Iteration 92/1000 | Loss: 0.00002459
Iteration 93/1000 | Loss: 0.00002459
Iteration 94/1000 | Loss: 0.00002459
Iteration 95/1000 | Loss: 0.00002459
Iteration 96/1000 | Loss: 0.00002459
Iteration 97/1000 | Loss: 0.00002459
Iteration 98/1000 | Loss: 0.00002459
Iteration 99/1000 | Loss: 0.00002459
Iteration 100/1000 | Loss: 0.00002459
Iteration 101/1000 | Loss: 0.00002459
Iteration 102/1000 | Loss: 0.00002459
Iteration 103/1000 | Loss: 0.00002459
Iteration 104/1000 | Loss: 0.00002459
Iteration 105/1000 | Loss: 0.00002459
Iteration 106/1000 | Loss: 0.00002459
Iteration 107/1000 | Loss: 0.00002459
Iteration 108/1000 | Loss: 0.00002459
Iteration 109/1000 | Loss: 0.00002459
Iteration 110/1000 | Loss: 0.00002459
Iteration 111/1000 | Loss: 0.00002459
Iteration 112/1000 | Loss: 0.00002459
Iteration 113/1000 | Loss: 0.00002459
Iteration 114/1000 | Loss: 0.00002459
Iteration 115/1000 | Loss: 0.00002459
Iteration 116/1000 | Loss: 0.00002459
Iteration 117/1000 | Loss: 0.00002459
Iteration 118/1000 | Loss: 0.00002459
Iteration 119/1000 | Loss: 0.00002459
Iteration 120/1000 | Loss: 0.00002459
Iteration 121/1000 | Loss: 0.00002459
Iteration 122/1000 | Loss: 0.00002459
Iteration 123/1000 | Loss: 0.00002459
Iteration 124/1000 | Loss: 0.00002459
Iteration 125/1000 | Loss: 0.00002459
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00002459
Iteration 128/1000 | Loss: 0.00002459
Iteration 129/1000 | Loss: 0.00002459
Iteration 130/1000 | Loss: 0.00002459
Iteration 131/1000 | Loss: 0.00002459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.458619565004483e-05, 2.458619565004483e-05, 2.458619565004483e-05, 2.458619565004483e-05, 2.458619565004483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.458619565004483e-05

Optimization complete. Final v2v error: 4.179051876068115 mm

Highest mean error: 5.013859748840332 mm for frame 181

Lowest mean error: 3.6997737884521484 mm for frame 230

Saving results

Total time: 39.81251764297485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/1000 | Loss: 0.00148952
Iteration 2/1000 | Loss: 0.00008840
Iteration 3/1000 | Loss: 0.00004096
Iteration 4/1000 | Loss: 0.00003252
Iteration 5/1000 | Loss: 0.00002961
Iteration 6/1000 | Loss: 0.00002796
Iteration 7/1000 | Loss: 0.00002710
Iteration 8/1000 | Loss: 0.00002643
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002530
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002522
Iteration 14/1000 | Loss: 0.00002520
Iteration 15/1000 | Loss: 0.00002504
Iteration 16/1000 | Loss: 0.00002494
Iteration 17/1000 | Loss: 0.00002492
Iteration 18/1000 | Loss: 0.00002491
Iteration 19/1000 | Loss: 0.00002489
Iteration 20/1000 | Loss: 0.00002488
Iteration 21/1000 | Loss: 0.00002487
Iteration 22/1000 | Loss: 0.00002486
Iteration 23/1000 | Loss: 0.00002486
Iteration 24/1000 | Loss: 0.00002485
Iteration 25/1000 | Loss: 0.00002485
Iteration 26/1000 | Loss: 0.00002485
Iteration 27/1000 | Loss: 0.00002484
Iteration 28/1000 | Loss: 0.00002483
Iteration 29/1000 | Loss: 0.00002483
Iteration 30/1000 | Loss: 0.00002482
Iteration 31/1000 | Loss: 0.00002482
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002481
Iteration 34/1000 | Loss: 0.00002480
Iteration 35/1000 | Loss: 0.00002476
Iteration 36/1000 | Loss: 0.00002474
Iteration 37/1000 | Loss: 0.00002473
Iteration 38/1000 | Loss: 0.00002473
Iteration 39/1000 | Loss: 0.00002473
Iteration 40/1000 | Loss: 0.00002472
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002471
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00002471
Iteration 45/1000 | Loss: 0.00002470
Iteration 46/1000 | Loss: 0.00002470
Iteration 47/1000 | Loss: 0.00002470
Iteration 48/1000 | Loss: 0.00002469
Iteration 49/1000 | Loss: 0.00002469
Iteration 50/1000 | Loss: 0.00002468
Iteration 51/1000 | Loss: 0.00002468
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002467
Iteration 54/1000 | Loss: 0.00002467
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002465
Iteration 59/1000 | Loss: 0.00002465
Iteration 60/1000 | Loss: 0.00002464
Iteration 61/1000 | Loss: 0.00002464
Iteration 62/1000 | Loss: 0.00002464
Iteration 63/1000 | Loss: 0.00002463
Iteration 64/1000 | Loss: 0.00002463
Iteration 65/1000 | Loss: 0.00002463
Iteration 66/1000 | Loss: 0.00002462
Iteration 67/1000 | Loss: 0.00002462
Iteration 68/1000 | Loss: 0.00002462
Iteration 69/1000 | Loss: 0.00002462
Iteration 70/1000 | Loss: 0.00002462
Iteration 71/1000 | Loss: 0.00002462
Iteration 72/1000 | Loss: 0.00002461
Iteration 73/1000 | Loss: 0.00002461
Iteration 74/1000 | Loss: 0.00002461
Iteration 75/1000 | Loss: 0.00002461
Iteration 76/1000 | Loss: 0.00002461
Iteration 77/1000 | Loss: 0.00002461
Iteration 78/1000 | Loss: 0.00002461
Iteration 79/1000 | Loss: 0.00002461
Iteration 80/1000 | Loss: 0.00002461
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002460
Iteration 84/1000 | Loss: 0.00002460
Iteration 85/1000 | Loss: 0.00002460
Iteration 86/1000 | Loss: 0.00002460
Iteration 87/1000 | Loss: 0.00002460
Iteration 88/1000 | Loss: 0.00002460
Iteration 89/1000 | Loss: 0.00002460
Iteration 90/1000 | Loss: 0.00002460
Iteration 91/1000 | Loss: 0.00002460
Iteration 92/1000 | Loss: 0.00002460
Iteration 93/1000 | Loss: 0.00002460
Iteration 94/1000 | Loss: 0.00002460
Iteration 95/1000 | Loss: 0.00002460
Iteration 96/1000 | Loss: 0.00002460
Iteration 97/1000 | Loss: 0.00002459
Iteration 98/1000 | Loss: 0.00002459
Iteration 99/1000 | Loss: 0.00002459
Iteration 100/1000 | Loss: 0.00002459
Iteration 101/1000 | Loss: 0.00002459
Iteration 102/1000 | Loss: 0.00002459
Iteration 103/1000 | Loss: 0.00002459
Iteration 104/1000 | Loss: 0.00002459
Iteration 105/1000 | Loss: 0.00002459
Iteration 106/1000 | Loss: 0.00002459
Iteration 107/1000 | Loss: 0.00002459
Iteration 108/1000 | Loss: 0.00002459
Iteration 109/1000 | Loss: 0.00002459
Iteration 110/1000 | Loss: 0.00002459
Iteration 111/1000 | Loss: 0.00002459
Iteration 112/1000 | Loss: 0.00002459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.4588860469521023e-05, 2.4588860469521023e-05, 2.4588860469521023e-05, 2.4588860469521023e-05, 2.4588860469521023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4588860469521023e-05

Optimization complete. Final v2v error: 4.179227352142334 mm

Highest mean error: 5.015650749206543 mm for frame 181

Lowest mean error: 3.7015106678009033 mm for frame 230

Saving results

Total time: 41.42368125915527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0156/0001/motion_seq.npz
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0156/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885692
Iteration 2/25 | Loss: 0.00164616
Iteration 3/25 | Loss: 0.00108491
Iteration 4/25 | Loss: 0.00104815
Iteration 5/25 | Loss: 0.00104326
Iteration 6/25 | Loss: 0.00104297
Iteration 7/25 | Loss: 0.00104297
Iteration 8/25 | Loss: 0.00104297
Iteration 9/25 | Loss: 0.00104297
Iteration 10/25 | Loss: 0.00104297
Iteration 11/25 | Loss: 0.00104297
Iteration 12/25 | Loss: 0.00104297
Iteration 13/25 | Loss: 0.00104297
Iteration 14/25 | Loss: 0.00104297
Iteration 15/25 | Loss: 0.00104297
Iteration 16/25 | Loss: 0.00104297
Iteration 17/25 | Loss: 0.00104297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001042973599396646, 0.001042973599396646, 0.001042973599396646, 0.001042973599396646, 0.001042973599396646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001042973599396646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76036549
Iteration 2/25 | Loss: 0.00082826
Iteration 3/25 | Loss: 0.00082825
Iteration 4/25 | Loss: 0.00082825
Iteration 5/25 | Loss: 0.00082825
Iteration 6/25 | Loss: 0.00082825
Iteration 7/25 | Loss: 0.00082825
Iteration 8/25 | Loss: 0.00082825
Iteration 9/25 | Loss: 0.00082825
Iteration 10/25 | Loss: 0.00082825
Iteration 11/25 | Loss: 0.00082825
Iteration 12/25 | Loss: 0.00082825
Iteration 13/25 | Loss: 0.00082825
Iteration 14/25 | Loss: 0.00082825
Iteration 15/25 | Loss: 0.00082825
Iteration 16/25 | Loss: 0.00082825
Iteration 17/25 | Loss: 0.00082825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008282513590529561, 0.0008282513590529561, 0.0008282513590529561, 0.0008282513590529561, 0.0008282513590529561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008282513590529561

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00940091
Iteration 2/25 | Loss: 0.00153387
Iteration 3/25 | Loss: 0.00102242
Iteration 4/25 | Loss: 0.00095233
Iteration 5/25 | Loss: 0.00093373
Iteration 6/25 | Loss: 0.00092961
Iteration 7/25 | Loss: 0.00092882
Iteration 8/25 | Loss: 0.00092882
Iteration 9/25 | Loss: 0.00092882
Iteration 10/25 | Loss: 0.00092882
Iteration 11/25 | Loss: 0.00092882
Iteration 12/25 | Loss: 0.00092882
Iteration 13/25 | Loss: 0.00092882
Iteration 14/25 | Loss: 0.00092882
Iteration 15/25 | Loss: 0.00092882
Iteration 16/25 | Loss: 0.00092882
Iteration 17/25 | Loss: 0.00092882
Iteration 18/25 | Loss: 0.00092882
Iteration 19/25 | Loss: 0.00092882
Iteration 20/25 | Loss: 0.00092882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009288177243433893, 0.0009288177243433893, 0.0009288177243433893, 0.0009288177243433893, 0.0009288177243433893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009288177243433893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18940604
Iteration 2/25 | Loss: 0.00044858
Iteration 3/25 | Loss: 0.00044858
Iteration 4/25 | Loss: 0.00044858
Iteration 5/25 | Loss: 0.00044858
Iteration 6/25 | Loss: 0.00044858
Iteration 7/25 | Loss: 0.00044857
Iteration 8/25 | Loss: 0.00044857
Iteration 9/25 | Loss: 0.00044857
Iteration 10/25 | Loss: 0.00044857
Iteration 11/25 | Loss: 0.00044857
Iteration 12/25 | Loss: 0.00044857
Iteration 13/25 | Loss: 0.00044857
Iteration 14/25 | Loss: 0.00044857
Iteration 15/25 | Loss: 0.00044857
Iteration 16/25 | Loss: 0.00044857
Iteration 17/25 | Loss: 0.00044857
Iteration 18/25 | Loss: 0.00044857
Iteration 19/25 | Loss: 0.00044857
Iteration 20/25 | Loss: 0.00044857
Iteration 21/25 | Loss: 0.00044857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004485737590584904, 0.0004485737590584904, 0.0004485737590584904, 0.0004485737590584904, 0.0004485737590584904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004485737590584904

Optimizing all parameters using a vertex loss

Iteration 1/25 | Loss: 0.00885692
Iteration 2/25 | Loss: 0.00164745
Iteration 3/25 | Loss: 0.00109011
Iteration 4/25 | Loss: 0.00104998
Iteration 5/25 | Loss: 0.00104356
Iteration 6/25 | Loss: 0.00104252
Iteration 7/25 | Loss: 0.00104252
Iteration 8/25 | Loss: 0.00104252
Iteration 9/25 | Loss: 0.00104252
Iteration 10/25 | Loss: 0.00104252
Iteration 11/25 | Loss: 0.00104252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010425243526697159, 0.0010425243526697159, 0.0010425243526697159, 0.0010425243526697159, 0.0010425243526697159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010425243526697159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76035523
Iteration 2/25 | Loss: 0.00082199
Iteration 3/25 | Loss: 0.00082198
Iteration 4/25 | Loss: 0.00082198
Iteration 5/25 | Loss: 0.00082198
Iteration 6/25 | Loss: 0.00082198
Iteration 7/25 | Loss: 0.00082198
Iteration 8/25 | Loss: 0.00082198
Iteration 9/25 | Loss: 0.00082198
Iteration 10/25 | Loss: 0.00082198
Iteration 11/25 | Loss: 0.00082198
Iteration 12/25 | Loss: 0.00082198
Iteration 13/25 | Loss: 0.00082198
Iteration 14/25 | Loss: 0.00082198
Iteration 15/25 | Loss: 0.00082198
Iteration 16/25 | Loss: 0.00082198
Iteration 17/25 | Loss: 0.00082198
Iteration 18/25 | Loss: 0.00082198
Iteration 19/25 | Loss: 0.00082198
Iteration 20/25 | Loss: 0.00082198
Iteration 21/25 | Loss: 0.00082198
Iteration 22/25 | Loss: 0.00082198
Iteration 23/25 | Loss: 0.00082198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008219799492508173, 0.0008219799492508173, 0.0008219799492508173, 0.0008219799492508173, 0.0008219799492508173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008219799492508173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050137
Iteration 2/1000 | Loss: 0.00008220
Iteration 3/1000 | Loss: 0.00008593
Iteration 4/1000 | Loss: 0.00007260
Iteration 5/1000 | Loss: 0.00005835
Iteration 6/1000 | Loss: 0.00006936
Iteration 7/1000 | Loss: 0.00006949
Iteration 8/1000 | Loss: 0.00006851
Iteration 9/1000 | Loss: 0.00007595
Iteration 10/1000 | Loss: 0.00009002
Iteration 11/1000 | Loss: 0.00008262
Iteration 12/1000 | Loss: 0.00007732
Iteration 13/1000 | Loss: 0.00008153
Iteration 14/1000 | Loss: 0.00006293
Iteration 15/1000 | Loss: 0.00010331
Iteration 16/1000 | Loss: 0.00009527
Iteration 17/1000 | Loss: 0.00010151
Iteration 18/1000 | Loss: 0.00010713
Iteration 19/1000 | Loss: 0.00010427
Iteration 20/1000 | Loss: 0.00010905
Iteration 21/1000 | Loss: 0.00009400
Iteration 22/1000 | Loss: 0.00036560
Iteration 23/1000 | Loss: 0.00038735
Iteration 24/1000 | Loss: 0.00009230
Iteration 25/1000 | Loss: 0.00008629
Iteration 26/1000 | Loss: 0.00007240
Iteration 27/1000 | Loss: 0.00005555
Iteration 28/1000 | Loss: 0.00005235
Iteration 29/1000 | Loss: 0.00007731
Iteration 30/1000 | Loss: 0.00008282
Iteration 31/1000 | Loss: 0.00006407
Iteration 32/1000 | Loss: 0.00007460
Iteration 33/1000 | Loss: 0.00005725
Iteration 34/1000 | Loss: 0.00005793
Iteration 35/1000 | Loss: 0.00007515
Iteration 36/1000 | Loss: 0.00007469
Iteration 37/1000 | Loss: 0.00008022
Iteration 38/1000 | Loss: 0.00009681
Iteration 39/1000 | Loss: 0.00007830
Iteration 40/1000 | Loss: 0.00009067
Iteration 41/1000 | Loss: 0.00007739
Iteration 42/1000 | Loss: 0.00008673
Iteration 43/1000 | Loss: 0.00007017
Iteration 44/1000 | Loss: 0.00004291
Iteration 45/1000 | Loss: 0.00003423
Iteration 46/1000 | Loss: 0.00002923
Iteration 47/1000 | Loss: 0.00002728
Iteration 48/1000 | Loss: 0.00002654
Iteration 49/1000 | Loss: 0.00002592
Iteration 50/1000 | Loss: 0.00002548
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002511
Iteration 53/1000 | Loss: 0.00002498
Iteration 54/1000 | Loss: 0.00002498
Iteration 55/1000 | Loss: 0.00002496
Iteration 56/1000 | Loss: 0.00002495
Iteration 57/1000 | Loss: 0.00002493
Iteration 58/1000 | Loss: 0.00002491
Iteration 59/1000 | Loss: 0.00002491
Iteration 60/1000 | Loss: 0.00002490
Iteration 61/1000 | Loss: 0.00002489
Iteration 62/1000 | Loss: 0.00002489
Iteration 63/1000 | Loss: 0.00002489
Iteration 64/1000 | Loss: 0.00002488
Iteration 65/1000 | Loss: 0.00002488
Iteration 66/1000 | Loss: 0.00002487
Iteration 67/1000 | Loss: 0.00002487
Iteration 68/1000 | Loss: 0.00002487
Iteration 69/1000 | Loss: 0.00002487
Iteration 70/1000 | Loss: 0.00002486
Iteration 71/1000 | Loss: 0.00002485
Iteration 72/1000 | Loss: 0.00002485
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002483
Iteration 77/1000 | Loss: 0.00002483
Iteration 78/1000 | Loss: 0.00002483
Iteration 79/1000 | Loss: 0.00002482
Iteration 80/1000 | Loss: 0.00002482
Iteration 81/1000 | Loss: 0.00002482
Iteration 82/1000 | Loss: 0.00002482
Iteration 83/1000 | Loss: 0.00002482
Iteration 84/1000 | Loss: 0.00002482
Iteration 85/1000 | Loss: 0.00002481
Iteration 86/1000 | Loss: 0.00002481
Iteration 87/1000 | Loss: 0.00002481
Iteration 88/1000 | Loss: 0.00002481
Iteration 89/1000 | Loss: 0.00002481
Iteration 90/1000 | Loss: 0.00002481
Iteration 91/1000 | Loss: 0.00002481
Iteration 92/1000 | Loss: 0.00002481
Iteration 93/1000 | Loss: 0.00002481
Iteration 94/1000 | Loss: 0.00002481
Iteration 95/1000 | Loss: 0.00002481
Iteration 96/1000 | Loss: 0.00002481
Iteration 97/1000 | Loss: 0.00002480
Iteration 98/1000 | Loss: 0.00002480
Iteration 99/1000 | Loss: 0.00002480
Iteration 100/1000 | Loss: 0.00002477
Iteration 101/1000 | Loss: 0.00002477
Iteration 102/1000 | Loss: 0.00002477
Iteration 103/1000 | Loss: 0.00002477
Iteration 104/1000 | Loss: 0.00002477
Iteration 105/1000 | Loss: 0.00002477
Iteration 106/1000 | Loss: 0.00002476
Iteration 107/1000 | Loss: 0.00002476
Iteration 108/1000 | Loss: 0.00002476
Iteration 109/1000 | Loss: 0.00002476
Iteration 110/1000 | Loss: 0.00002475
Iteration 111/1000 | Loss: 0.00002474
Iteration 112/1000 | Loss: 0.00002474
Iteration 113/1000 | Loss: 0.00002474
Iteration 114/1000 | Loss: 0.00002474
Iteration 115/1000 | Loss: 0.00002474
Iteration 116/1000 | Loss: 0.00002474
Iteration 117/1000 | Loss: 0.00002474
Iteration 118/1000 | Loss: 0.00002474
Iteration 119/1000 | Loss: 0.00002474
Iteration 120/1000 | Loss: 0.00002474
Iteration 121/1000 | Loss: 0.00002474
Iteration 122/1000 | Loss: 0.00002473
Iteration 123/1000 | Loss: 0.00002473
Iteration 124/1000 | Loss: 0.00002473
Iteration 125/1000 | Loss: 0.00002473
Iteration 126/1000 | Loss: 0.00002472
Iteration 127/1000 | Loss: 0.00002472
Iteration 128/1000 | Loss: 0.00002472
Iteration 129/1000 | Loss: 0.00002472
Iteration 130/1000 | Loss: 0.00002472
Iteration 131/1000 | Loss: 0.00002471
Iteration 132/1000 | Loss: 0.00002471
Iteration 133/1000 | Loss: 0.00002471
Iteration 134/1000 | Loss: 0.00002471
Iteration 135/1000 | Loss: 0.00002471
Iteration 136/1000 | Loss: 0.00002471
Iteration 137/1000 | Loss: 0.00002471
Iteration 138/1000 | Loss: 0.00002471
Iteration 139/1000 | Loss: 0.00002471
Iteration 140/1000 | Loss: 0.00002470
Iteration 141/1000 | Loss: 0.00002470
Iteration 142/1000 | Loss: 0.00002470
Iteration 143/1000 | Loss: 0.00002470
Iteration 144/1000 | Loss: 0.00002470
Iteration 145/1000 | Loss: 0.00002470
Iteration 146/1000 | Loss: 0.00002470
Iteration 147/1000 | Loss: 0.00002470
Iteration 148/1000 | Loss: 0.00002469
Iteration 149/1000 | Loss: 0.00002469
Iteration 150/1000 | Loss: 0.00002469
Iteration 151/1000 | Loss: 0.00002469
Iteration 152/1000 | Loss: 0.00002469
Iteration 153/1000 | Loss: 0.00002469
Iteration 154/1000 | Loss: 0.00002469
Iteration 155/1000 | Loss: 0.00002468
Iteration 156/1000 | Loss: 0.00002468
Iteration 157/1000 | Loss: 0.00002468
Iteration 158/1000 | Loss: 0.00002468
Iteration 159/1000 | Loss: 0.00002468
Iteration 160/1000 | Loss: 0.00002468
Iteration 161/1000 | Loss: 0.00002468
Iteration 162/1000 | Loss: 0.00002468
Iteration 163/1000 | Loss: 0.00002467
Iteration 164/1000 | Loss: 0.00002467
Iteration 165/1000 | Loss: 0.00002467
Iteration 166/1000 | Loss: 0.00002467
Iteration 167/1000 | Loss: 0.00002467
Iteration 168/1000 | Loss: 0.00002467
Iteration 169/1000 | Loss: 0.00002467
Iteration 170/1000 | Loss: 0.00002467
Iteration 171/1000 | Loss: 0.00002467
Iteration 172/1000 | Loss: 0.00002467
Iteration 173/1000 | Loss: 0.00002467
Iteration 174/1000 | Loss: 0.00002467
Iteration 175/1000 | Loss: 0.00002467
Iteration 176/1000 | Loss: 0.00002466
Iteration 177/1000 | Loss: 0.00002466
Iteration 178/1000 | Loss: 0.00002466
Iteration 179/1000 | Loss: 0.00002466
Iteration 180/1000 | Loss: 0.00002466
Iteration 181/1000 | Loss: 0.00002466
Iteration 182/1000 | Loss: 0.00002466
Iteration 183/1000 | Loss: 0.00002466
Iteration 184/1000 | Loss: 0.00002466
Iteration 185/1000 | Loss: 0.00002466
Iteration 186/1000 | Loss: 0.00002466
Iteration 187/1000 | Loss: 0.00002466
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002466
Iteration 192/1000 | Loss: 0.00002466
Iteration 193/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.4663433578098193e-05, 2.4663433578098193e-05, 2.4663433578098193e-05, 2.4663433578098193e-05, 2.4663433578098193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4663433578098193e-05

Optimization complete. Final v2v error: 4.2183308601379395 mm

Highest mean error: 9.523388862609863 mm for frame 3

Lowest mean error: 3.882864236831665 mm for frame 48

Saving results

Total time: 125.3901195526123
Iteration 1/1000 | Loss: 0.00082825
Iteration 2/1000 | Loss: 0.00004791
Iteration 3/1000 | Loss: 0.00002934
Iteration 4/1000 | Loss: 0.00002275
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00002015
Iteration 7/1000 | Loss: 0.00001959
Iteration 8/1000 | Loss: 0.00001918
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001860
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001818
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001813
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001790
Iteration 26/1000 | Loss: 0.00001790
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001789
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001785
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001784
Iteration 36/1000 | Loss: 0.00001784
Iteration 37/1000 | Loss: 0.00001783
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001771
Iteration 84/1000 | Loss: 0.00001771
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001771
Iteration 96/1000 | Loss: 0.00001771
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001771
Iteration 115/1000 | Loss: 0.00001771
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001771
Iteration 118/1000 | Loss: 0.00001771
Iteration 119/1000 | Loss: 0.00001771
Iteration 120/1000 | Loss: 0.00001771
Iteration 121/1000 | Loss: 0.00001771
Iteration 122/1000 | Loss: 0.00001771
Iteration 123/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.7706841390463524e-05, 1.7706841390463524e-05, 1.7706841390463524e-05, 1.7706841390463524e-05, 1.7706841390463524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7706841390463524e-05

Optimization complete. Final v2v error: 3.7705016136169434 mm

Highest mean error: 4.0471510887146 mm for frame 170

Lowest mean error: 3.481365442276001 mm for frame 123

Saving results

Total time: 38.1069118976593
Iteration 1/1000 | Loss: 0.00082198
Iteration 2/1000 | Loss: 0.00004788
Iteration 3/1000 | Loss: 0.00002944
Iteration 4/1000 | Loss: 0.00002335
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002035
Iteration 7/1000 | Loss: 0.00001973
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001812
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001799
Iteration 16/1000 | Loss: 0.00001791
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001790
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001784
Iteration 21/1000 | Loss: 0.00001782
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001778
Iteration 29/1000 | Loss: 0.00001778
Iteration 30/1000 | Loss: 0.00001778
Iteration 31/1000 | Loss: 0.00001778
Iteration 32/1000 | Loss: 0.00001778
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001777
Iteration 35/1000 | Loss: 0.00001777
Iteration 36/1000 | Loss: 0.00001777
Iteration 37/1000 | Loss: 0.00001777
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001774
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001773
Iteration 66/1000 | Loss: 0.00001773
Iteration 67/1000 | Loss: 0.00001773
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001773
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.7731104890117422e-05, 1.7731104890117422e-05, 1.7731104890117422e-05, 1.7731104890117422e-05, 1.7731104890117422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7731104890117422e-05

Optimization complete. Final v2v error: 3.7736728191375732 mm

Highest mean error: 4.049363136291504 mm for frame 170

Lowest mean error: 3.4854576587677 mm for frame 123

Saving results

Total time: 37.64714288711548
Iteration 1/1000 | Loss: 0.00044857
Iteration 2/1000 | Loss: 0.00008024
Iteration 3/1000 | Loss: 0.00005328
Iteration 4/1000 | Loss: 0.00004769
Iteration 5/1000 | Loss: 0.00004522
Iteration 6/1000 | Loss: 0.00004387
Iteration 7/1000 | Loss: 0.00004291
Iteration 8/1000 | Loss: 0.00004209
Iteration 9/1000 | Loss: 0.00004161
Iteration 10/1000 | Loss: 0.00004122
Iteration 11/1000 | Loss: 0.00004088
Iteration 12/1000 | Loss: 0.00004066
Iteration 13/1000 | Loss: 0.00004044
Iteration 14/1000 | Loss: 0.00004021
Iteration 15/1000 | Loss: 0.00004013
Iteration 16/1000 | Loss: 0.00003997
Iteration 17/1000 | Loss: 0.00003995
Iteration 18/1000 | Loss: 0.00003982
Iteration 19/1000 | Loss: 0.00003977
Iteration 20/1000 | Loss: 0.00003976
Iteration 21/1000 | Loss: 0.00003972
Iteration 22/1000 | Loss: 0.00003962
Iteration 23/1000 | Loss: 0.00003958
Iteration 24/1000 | Loss: 0.00003955
Iteration 25/1000 | Loss: 0.00003953
Iteration 26/1000 | Loss: 0.00003950
Iteration 27/1000 | Loss: 0.00003946
Iteration 28/1000 | Loss: 0.00003946
Iteration 29/1000 | Loss: 0.00003946
Iteration 30/1000 | Loss: 0.00003946
Iteration 31/1000 | Loss: 0.00003946
Iteration 32/1000 | Loss: 0.00003945
Iteration 33/1000 | Loss: 0.00003945
Iteration 34/1000 | Loss: 0.00003944
Iteration 35/1000 | Loss: 0.00003944
Iteration 36/1000 | Loss: 0.00003941
Iteration 37/1000 | Loss: 0.00003941
Iteration 38/1000 | Loss: 0.00003941
Iteration 39/1000 | Loss: 0.00003941
Iteration 40/1000 | Loss: 0.00003940
Iteration 41/1000 | Loss: 0.00003940
Iteration 42/1000 | Loss: 0.00003940
Iteration 43/1000 | Loss: 0.00003939
Iteration 44/1000 | Loss: 0.00003938
Iteration 45/1000 | Loss: 0.00003938
Iteration 46/1000 | Loss: 0.00003937
Iteration 47/1000 | Loss: 0.00003937
Iteration 48/1000 | Loss: 0.00003937
Iteration 49/1000 | Loss: 0.00003937
Iteration 50/1000 | Loss: 0.00003935
Iteration 51/1000 | Loss: 0.00003935
Iteration 52/1000 | Loss: 0.00003935
Iteration 53/1000 | Loss: 0.00003935
Iteration 54/1000 | Loss: 0.00003935
Iteration 55/1000 | Loss: 0.00003935
Iteration 56/1000 | Loss: 0.00003935
Iteration 57/1000 | Loss: 0.00003935
Iteration 58/1000 | Loss: 0.00003935
Iteration 59/1000 | Loss: 0.00003934
Iteration 60/1000 | Loss: 0.00003934
Iteration 61/1000 | Loss: 0.00003934
Iteration 62/1000 | Loss: 0.00003934
Iteration 63/1000 | Loss: 0.00003934
Iteration 64/1000 | Loss: 0.00003933
Iteration 65/1000 | Loss: 0.00003933
Iteration 66/1000 | Loss: 0.00003933
Iteration 67/1000 | Loss: 0.00003933
Iteration 68/1000 | Loss: 0.00003933
Iteration 69/1000 | Loss: 0.00003933
Iteration 70/1000 | Loss: 0.00003933
Iteration 71/1000 | Loss: 0.00003933
Iteration 72/1000 | Loss: 0.00003933
Iteration 73/1000 | Loss: 0.00003933
Iteration 74/1000 | Loss: 0.00003933
Iteration 75/1000 | Loss: 0.00003933
Iteration 76/1000 | Loss: 0.00003933
Iteration 77/1000 | Loss: 0.00003932
Iteration 78/1000 | Loss: 0.00003932
Iteration 79/1000 | Loss: 0.00003932
Iteration 80/1000 | Loss: 0.00003932
Iteration 81/1000 | Loss: 0.00003932
Iteration 82/1000 | Loss: 0.00003931
Iteration 83/1000 | Loss: 0.00003931
Iteration 84/1000 | Loss: 0.00003930
Iteration 85/1000 | Loss: 0.00003930
Iteration 86/1000 | Loss: 0.00003930
Iteration 87/1000 | Loss: 0.00003930
Iteration 88/1000 | Loss: 0.00003930
Iteration 89/1000 | Loss: 0.00003930
Iteration 90/1000 | Loss: 0.00003929
Iteration 91/1000 | Loss: 0.00003929
Iteration 92/1000 | Loss: 0.00003929
Iteration 93/1000 | Loss: 0.00003929
Iteration 94/1000 | Loss: 0.00003928
Iteration 95/1000 | Loss: 0.00003928
Iteration 96/1000 | Loss: 0.00003928
Iteration 97/1000 | Loss: 0.00003927
Iteration 98/1000 | Loss: 0.00003927
Iteration 99/1000 | Loss: 0.00003926
Iteration 100/1000 | Loss: 0.00003926
Iteration 101/1000 | Loss: 0.00003925
Iteration 102/1000 | Loss: 0.00003925
Iteration 103/1000 | Loss: 0.00003924
Iteration 104/1000 | Loss: 0.00003924
Iteration 105/1000 | Loss: 0.00003924
Iteration 106/1000 | Loss: 0.00003924
Iteration 107/1000 | Loss: 0.00003924
Iteration 108/1000 | Loss: 0.00003924
Iteration 109/1000 | Loss: 0.00003924
Iteration 110/1000 | Loss: 0.00003924
Iteration 111/1000 | Loss: 0.00003923
Iteration 112/1000 | Loss: 0.00003923
Iteration 113/1000 | Loss: 0.00003923
Iteration 114/1000 | Loss: 0.00003923
Iteration 115/1000 | Loss: 0.00003922
Iteration 116/1000 | Loss: 0.00003922
Iteration 117/1000 | Loss: 0.00003922
Iteration 118/1000 | Loss: 0.00003921
Iteration 119/1000 | Loss: 0.00003921
Iteration 120/1000 | Loss: 0.00003921
Iteration 121/1000 | Loss: 0.00003921
Iteration 122/1000 | Loss: 0.00003921
Iteration 123/1000 | Loss: 0.00003921
Iteration 124/1000 | Loss: 0.00003921
Iteration 125/1000 | Loss: 0.00003921
Iteration 126/1000 | Loss: 0.00003920
Iteration 127/1000 | Loss: 0.00003920
Iteration 128/1000 | Loss: 0.00003920
Iteration 129/1000 | Loss: 0.00003920
Iteration 130/1000 | Loss: 0.00003920
Iteration 131/1000 | Loss: 0.00003920
Iteration 132/1000 | Loss: 0.00003920
Iteration 133/1000 | Loss: 0.00003920
Iteration 134/1000 | Loss: 0.00003920
Iteration 135/1000 | Loss: 0.00003919
Iteration 136/1000 | Loss: 0.00003919
Iteration 137/1000 | Loss: 0.00003919
Iteration 138/1000 | Loss: 0.00003919
Iteration 139/1000 | Loss: 0.00003919
Iteration 140/1000 | Loss: 0.00003919
Iteration 141/1000 | Loss: 0.00003919
Iteration 142/1000 | Loss: 0.00003919
Iteration 143/1000 | Loss: 0.00003919
Iteration 144/1000 | Loss: 0.00003919
Iteration 145/1000 | Loss: 0.00003919
Iteration 146/1000 | Loss: 0.00003919
Iteration 147/1000 | Loss: 0.00003919
Iteration 148/1000 | Loss: 0.00003919
Iteration 149/1000 | Loss: 0.00003919
Iteration 150/1000 | Loss: 0.00003919
Iteration 151/1000 | Loss: 0.00003919
Iteration 152/1000 | Loss: 0.00003919
Iteration 153/1000 | Loss: 0.00003919
Iteration 154/1000 | Loss: 0.00003919
Iteration 155/1000 | Loss: 0.00003919
Iteration 156/1000 | Loss: 0.00003919
Iteration 157/1000 | Loss: 0.00003919
Iteration 158/1000 | Loss: 0.00003919
Iteration 159/1000 | Loss: 0.00003919
Iteration 160/1000 | Loss: 0.00003919
Iteration 161/1000 | Loss: 0.00003919
Iteration 162/1000 | Loss: 0.00003919
Iteration 163/1000 | Loss: 0.00003919
Iteration 164/1000 | Loss: 0.00003919
Iteration 165/1000 | Loss: 0.00003919
Iteration 166/1000 | Loss: 0.00003919
Iteration 167/1000 | Loss: 0.00003919
Iteration 168/1000 | Loss: 0.00003919
Iteration 169/1000 | Loss: 0.00003919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.9193408156279474e-05, 3.9193408156279474e-05, 3.9193408156279474e-05, 3.9193408156279474e-05, 3.9193408156279474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9193408156279474e-05

Optimization complete. Final v2v error: 5.137462139129639 mm

Highest mean error: 6.338201522827148 mm for frame 144

Lowest mean error: 4.50214147567749 mm for frame 27

Saving results

Total time: 55.87188124656677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390363
Iteration 2/25 | Loss: 0.00092089
Iteration 3/25 | Loss: 0.00075080
Iteration 4/25 | Loss: 0.00072895
Iteration 5/25 | Loss: 0.00072279
Iteration 6/25 | Loss: 0.00072114
Iteration 7/25 | Loss: 0.00072074
Iteration 8/25 | Loss: 0.00072074
Iteration 9/25 | Loss: 0.00072074
Iteration 10/25 | Loss: 0.00072074
Iteration 11/25 | Loss: 0.00072074
Iteration 12/25 | Loss: 0.00072074
Iteration 13/25 | Loss: 0.00072074
Iteration 14/25 | Loss: 0.00072074
Iteration 15/25 | Loss: 0.00072074
Iteration 16/25 | Loss: 0.00072074
Iteration 17/25 | Loss: 0.00072074
Iteration 18/25 | Loss: 0.00072074
Iteration 19/25 | Loss: 0.00072074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007207388407550752, 0.0007207388407550752, 0.0007207388407550752, 0.0007207388407550752, 0.0007207388407550752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007207388407550752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49718785
Iteration 2/25 | Loss: 0.00042751
Iteration 3/25 | Loss: 0.00042751
Iteration 4/25 | Loss: 0.00042751
Iteration 5/25 | Loss: 0.00042751
Iteration 6/25 | Loss: 0.00042751
Iteration 7/25 | Loss: 0.00042751
Iteration 8/25 | Loss: 0.00042751
Iteration 9/25 | Loss: 0.00042751
Iteration 10/25 | Loss: 0.00042751
Iteration 11/25 | Loss: 0.00042751
Iteration 12/25 | Loss: 0.00042751
Iteration 13/25 | Loss: 0.00042751
Iteration 14/25 | Loss: 0.00042751
Iteration 15/25 | Loss: 0.00042751
Iteration 16/25 | Loss: 0.00042751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00042750727152451873, 0.00042750727152451873, 0.00042750727152451873, 0.00042750727152451873, 0.00042750727152451873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042750727152451873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042751
Iteration 2/1000 | Loss: 0.00002578
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001659
Iteration 6/1000 | Loss: 0.00001594
Iteration 7/1000 | Loss: 0.00001552
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001471
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001444
Iteration 14/1000 | Loss: 0.00001443
Iteration 15/1000 | Loss: 0.00001441
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001438
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001416
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001408
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001404
Iteration 61/1000 | Loss: 0.00001404
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001400
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001400
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001400
Iteration 101/1000 | Loss: 0.00001400
Iteration 102/1000 | Loss: 0.00001400
Iteration 103/1000 | Loss: 0.00001400
Iteration 104/1000 | Loss: 0.00001400
Iteration 105/1000 | Loss: 0.00001400
Iteration 106/1000 | Loss: 0.00001400
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.3999711882206611e-05, 1.3999711882206611e-05, 1.3999711882206611e-05, 1.3999711882206611e-05, 1.3999711882206611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3999711882206611e-05

Optimization complete. Final v2v error: 3.1070210933685303 mm

Highest mean error: 3.454143524169922 mm for frame 126

Lowest mean error: 2.791363477706909 mm for frame 13

Saving results

Total time: 36.55212759971619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433748
Iteration 2/25 | Loss: 0.00110438
Iteration 3/25 | Loss: 0.00081171
Iteration 4/25 | Loss: 0.00078327
Iteration 5/25 | Loss: 0.00077425
Iteration 6/25 | Loss: 0.00077207
Iteration 7/25 | Loss: 0.00077159
Iteration 8/25 | Loss: 0.00077159
Iteration 9/25 | Loss: 0.00077159
Iteration 10/25 | Loss: 0.00077159
Iteration 11/25 | Loss: 0.00077159
Iteration 12/25 | Loss: 0.00077159
Iteration 13/25 | Loss: 0.00077159
Iteration 14/25 | Loss: 0.00077159
Iteration 15/25 | Loss: 0.00077159
Iteration 16/25 | Loss: 0.00077159
Iteration 17/25 | Loss: 0.00077159
Iteration 18/25 | Loss: 0.00077159
Iteration 19/25 | Loss: 0.00077159
Iteration 20/25 | Loss: 0.00077159
Iteration 21/25 | Loss: 0.00077159
Iteration 22/25 | Loss: 0.00077159
Iteration 23/25 | Loss: 0.00077159
Iteration 24/25 | Loss: 0.00077159
Iteration 25/25 | Loss: 0.00077159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50297737
Iteration 2/25 | Loss: 0.00053628
Iteration 3/25 | Loss: 0.00053628
Iteration 4/25 | Loss: 0.00053628
Iteration 5/25 | Loss: 0.00053628
Iteration 6/25 | Loss: 0.00053628
Iteration 7/25 | Loss: 0.00053628
Iteration 8/25 | Loss: 0.00053628
Iteration 9/25 | Loss: 0.00053628
Iteration 10/25 | Loss: 0.00053628
Iteration 11/25 | Loss: 0.00053628
Iteration 12/25 | Loss: 0.00053628
Iteration 13/25 | Loss: 0.00053628
Iteration 14/25 | Loss: 0.00053628
Iteration 15/25 | Loss: 0.00053628
Iteration 16/25 | Loss: 0.00053628
Iteration 17/25 | Loss: 0.00053628
Iteration 18/25 | Loss: 0.00053628
Iteration 19/25 | Loss: 0.00053628
Iteration 20/25 | Loss: 0.00053628
Iteration 21/25 | Loss: 0.00053628
Iteration 22/25 | Loss: 0.00053628
Iteration 23/25 | Loss: 0.00053628
Iteration 24/25 | Loss: 0.00053628
Iteration 25/25 | Loss: 0.00053628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053628
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001764
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001390
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001295
Iteration 16/1000 | Loss: 0.00001292
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001289
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001285
Iteration 31/1000 | Loss: 0.00001284
Iteration 32/1000 | Loss: 0.00001284
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00001282
Iteration 37/1000 | Loss: 0.00001282
Iteration 38/1000 | Loss: 0.00001282
Iteration 39/1000 | Loss: 0.00001281
Iteration 40/1000 | Loss: 0.00001281
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001277
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001276
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001274
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001273
Iteration 73/1000 | Loss: 0.00001273
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001271
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001271
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001271
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001270
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001270
Iteration 91/1000 | Loss: 0.00001270
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001269
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001269
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001268
Iteration 98/1000 | Loss: 0.00001268
Iteration 99/1000 | Loss: 0.00001268
Iteration 100/1000 | Loss: 0.00001268
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001268
Iteration 107/1000 | Loss: 0.00001268
Iteration 108/1000 | Loss: 0.00001268
Iteration 109/1000 | Loss: 0.00001268
Iteration 110/1000 | Loss: 0.00001268
Iteration 111/1000 | Loss: 0.00001268
Iteration 112/1000 | Loss: 0.00001268
Iteration 113/1000 | Loss: 0.00001268
Iteration 114/1000 | Loss: 0.00001268
Iteration 115/1000 | Loss: 0.00001268
Iteration 116/1000 | Loss: 0.00001268
Iteration 117/1000 | Loss: 0.00001268
Iteration 118/1000 | Loss: 0.00001268
Iteration 119/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.2676004189415835e-05, 1.2676004189415835e-05, 1.2676004189415835e-05, 1.2676004189415835e-05, 1.2676004189415835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2676004189415835e-05

Optimization complete. Final v2v error: 2.9313137531280518 mm

Highest mean error: 4.449460983276367 mm for frame 239

Lowest mean error: 2.6652984619140625 mm for frame 68

Saving results

Total time: 41.18044948577881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452568
Iteration 2/25 | Loss: 0.00106446
Iteration 3/25 | Loss: 0.00079835
Iteration 4/25 | Loss: 0.00076291
Iteration 5/25 | Loss: 0.00075305
Iteration 6/25 | Loss: 0.00075029
Iteration 7/25 | Loss: 0.00074975
Iteration 8/25 | Loss: 0.00074975
Iteration 9/25 | Loss: 0.00074975
Iteration 10/25 | Loss: 0.00074975
Iteration 11/25 | Loss: 0.00074975
Iteration 12/25 | Loss: 0.00074975
Iteration 13/25 | Loss: 0.00074975
Iteration 14/25 | Loss: 0.00074975
Iteration 15/25 | Loss: 0.00074975
Iteration 16/25 | Loss: 0.00074975
Iteration 17/25 | Loss: 0.00074975
Iteration 18/25 | Loss: 0.00074975
Iteration 19/25 | Loss: 0.00074975
Iteration 20/25 | Loss: 0.00074975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007497539627365768, 0.0007497539627365768, 0.0007497539627365768, 0.0007497539627365768, 0.0007497539627365768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007497539627365768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49773049
Iteration 2/25 | Loss: 0.00042843
Iteration 3/25 | Loss: 0.00042842
Iteration 4/25 | Loss: 0.00042842
Iteration 5/25 | Loss: 0.00042842
Iteration 6/25 | Loss: 0.00042842
Iteration 7/25 | Loss: 0.00042842
Iteration 8/25 | Loss: 0.00042842
Iteration 9/25 | Loss: 0.00042842
Iteration 10/25 | Loss: 0.00042842
Iteration 11/25 | Loss: 0.00042842
Iteration 12/25 | Loss: 0.00042842
Iteration 13/25 | Loss: 0.00042842
Iteration 14/25 | Loss: 0.00042842
Iteration 15/25 | Loss: 0.00042842
Iteration 16/25 | Loss: 0.00042842
Iteration 17/25 | Loss: 0.00042842
Iteration 18/25 | Loss: 0.00042842
Iteration 19/25 | Loss: 0.00042842
Iteration 20/25 | Loss: 0.00042842
Iteration 21/25 | Loss: 0.00042842
Iteration 22/25 | Loss: 0.00042842
Iteration 23/25 | Loss: 0.00042842
Iteration 24/25 | Loss: 0.00042842
Iteration 25/25 | Loss: 0.00042842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042842
Iteration 2/1000 | Loss: 0.00003676
Iteration 3/1000 | Loss: 0.00002281
Iteration 4/1000 | Loss: 0.00002114
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001826
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001765
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001758
Iteration 14/1000 | Loss: 0.00001757
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001745
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001739
Iteration 26/1000 | Loss: 0.00001736
Iteration 27/1000 | Loss: 0.00001736
Iteration 28/1000 | Loss: 0.00001733
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001719
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001717
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001716
Iteration 89/1000 | Loss: 0.00001716
Iteration 90/1000 | Loss: 0.00001716
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001715
Iteration 96/1000 | Loss: 0.00001715
Iteration 97/1000 | Loss: 0.00001715
Iteration 98/1000 | Loss: 0.00001715
Iteration 99/1000 | Loss: 0.00001715
Iteration 100/1000 | Loss: 0.00001715
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001714
Iteration 104/1000 | Loss: 0.00001714
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001714
Iteration 113/1000 | Loss: 0.00001714
Iteration 114/1000 | Loss: 0.00001714
Iteration 115/1000 | Loss: 0.00001714
Iteration 116/1000 | Loss: 0.00001714
Iteration 117/1000 | Loss: 0.00001714
Iteration 118/1000 | Loss: 0.00001714
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001714
Iteration 124/1000 | Loss: 0.00001714
Iteration 125/1000 | Loss: 0.00001714
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.7140162526629865e-05, 1.7140162526629865e-05, 1.7140162526629865e-05, 1.7140162526629865e-05, 1.7140162526629865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7140162526629865e-05

Optimization complete. Final v2v error: 3.482222318649292 mm

Highest mean error: 4.339384078979492 mm for frame 115

Lowest mean error: 2.8318164348602295 mm for frame 199

Saving results

Total time: 41.48001217842102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769620
Iteration 2/25 | Loss: 0.00100575
Iteration 3/25 | Loss: 0.00086702
Iteration 4/25 | Loss: 0.00083289
Iteration 5/25 | Loss: 0.00082446
Iteration 6/25 | Loss: 0.00082303
Iteration 7/25 | Loss: 0.00082275
Iteration 8/25 | Loss: 0.00082275
Iteration 9/25 | Loss: 0.00082275
Iteration 10/25 | Loss: 0.00082275
Iteration 11/25 | Loss: 0.00082275
Iteration 12/25 | Loss: 0.00082275
Iteration 13/25 | Loss: 0.00082275
Iteration 14/25 | Loss: 0.00082275
Iteration 15/25 | Loss: 0.00082275
Iteration 16/25 | Loss: 0.00082275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008227465441450477, 0.0008227465441450477, 0.0008227465441450477, 0.0008227465441450477, 0.0008227465441450477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008227465441450477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50940907
Iteration 2/25 | Loss: 0.00040466
Iteration 3/25 | Loss: 0.00040457
Iteration 4/25 | Loss: 0.00040457
Iteration 5/25 | Loss: 0.00040457
Iteration 6/25 | Loss: 0.00040457
Iteration 7/25 | Loss: 0.00040457
Iteration 8/25 | Loss: 0.00040457
Iteration 9/25 | Loss: 0.00040457
Iteration 10/25 | Loss: 0.00040457
Iteration 11/25 | Loss: 0.00040457
Iteration 12/25 | Loss: 0.00040457
Iteration 13/25 | Loss: 0.00040457
Iteration 14/25 | Loss: 0.00040457
Iteration 15/25 | Loss: 0.00040457
Iteration 16/25 | Loss: 0.00040457
Iteration 17/25 | Loss: 0.00040457
Iteration 18/25 | Loss: 0.00040457
Iteration 19/25 | Loss: 0.00040457
Iteration 20/25 | Loss: 0.00040457
Iteration 21/25 | Loss: 0.00040457
Iteration 22/25 | Loss: 0.00040457
Iteration 23/25 | Loss: 0.00040457
Iteration 24/25 | Loss: 0.00040457
Iteration 25/25 | Loss: 0.00040457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040457
Iteration 2/1000 | Loss: 0.00005231
Iteration 3/1000 | Loss: 0.00003504
Iteration 4/1000 | Loss: 0.00003023
Iteration 5/1000 | Loss: 0.00002813
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002643
Iteration 8/1000 | Loss: 0.00002575
Iteration 9/1000 | Loss: 0.00002549
Iteration 10/1000 | Loss: 0.00002521
Iteration 11/1000 | Loss: 0.00002509
Iteration 12/1000 | Loss: 0.00002505
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002480
Iteration 15/1000 | Loss: 0.00002476
Iteration 16/1000 | Loss: 0.00002475
Iteration 17/1000 | Loss: 0.00002468
Iteration 18/1000 | Loss: 0.00002467
Iteration 19/1000 | Loss: 0.00002466
Iteration 20/1000 | Loss: 0.00002465
Iteration 21/1000 | Loss: 0.00002465
Iteration 22/1000 | Loss: 0.00002461
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002460
Iteration 26/1000 | Loss: 0.00002459
Iteration 27/1000 | Loss: 0.00002456
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002456
Iteration 30/1000 | Loss: 0.00002456
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002455
Iteration 35/1000 | Loss: 0.00002455
Iteration 36/1000 | Loss: 0.00002455
Iteration 37/1000 | Loss: 0.00002455
Iteration 38/1000 | Loss: 0.00002455
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002455
Iteration 41/1000 | Loss: 0.00002454
Iteration 42/1000 | Loss: 0.00002454
Iteration 43/1000 | Loss: 0.00002453
Iteration 44/1000 | Loss: 0.00002453
Iteration 45/1000 | Loss: 0.00002453
Iteration 46/1000 | Loss: 0.00002453
Iteration 47/1000 | Loss: 0.00002452
Iteration 48/1000 | Loss: 0.00002452
Iteration 49/1000 | Loss: 0.00002452
Iteration 50/1000 | Loss: 0.00002452
Iteration 51/1000 | Loss: 0.00002452
Iteration 52/1000 | Loss: 0.00002452
Iteration 53/1000 | Loss: 0.00002451
Iteration 54/1000 | Loss: 0.00002451
Iteration 55/1000 | Loss: 0.00002451
Iteration 56/1000 | Loss: 0.00002451
Iteration 57/1000 | Loss: 0.00002451
Iteration 58/1000 | Loss: 0.00002451
Iteration 59/1000 | Loss: 0.00002451
Iteration 60/1000 | Loss: 0.00002451
Iteration 61/1000 | Loss: 0.00002450
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00002450
Iteration 65/1000 | Loss: 0.00002450
Iteration 66/1000 | Loss: 0.00002450
Iteration 67/1000 | Loss: 0.00002450
Iteration 68/1000 | Loss: 0.00002450
Iteration 69/1000 | Loss: 0.00002449
Iteration 70/1000 | Loss: 0.00002449
Iteration 71/1000 | Loss: 0.00002449
Iteration 72/1000 | Loss: 0.00002449
Iteration 73/1000 | Loss: 0.00002449
Iteration 74/1000 | Loss: 0.00002449
Iteration 75/1000 | Loss: 0.00002449
Iteration 76/1000 | Loss: 0.00002449
Iteration 77/1000 | Loss: 0.00002449
Iteration 78/1000 | Loss: 0.00002449
Iteration 79/1000 | Loss: 0.00002449
Iteration 80/1000 | Loss: 0.00002449
Iteration 81/1000 | Loss: 0.00002449
Iteration 82/1000 | Loss: 0.00002449
Iteration 83/1000 | Loss: 0.00002449
Iteration 84/1000 | Loss: 0.00002449
Iteration 85/1000 | Loss: 0.00002449
Iteration 86/1000 | Loss: 0.00002449
Iteration 87/1000 | Loss: 0.00002449
Iteration 88/1000 | Loss: 0.00002449
Iteration 89/1000 | Loss: 0.00002449
Iteration 90/1000 | Loss: 0.00002449
Iteration 91/1000 | Loss: 0.00002449
Iteration 92/1000 | Loss: 0.00002449
Iteration 93/1000 | Loss: 0.00002449
Iteration 94/1000 | Loss: 0.00002449
Iteration 95/1000 | Loss: 0.00002449
Iteration 96/1000 | Loss: 0.00002449
Iteration 97/1000 | Loss: 0.00002449
Iteration 98/1000 | Loss: 0.00002449
Iteration 99/1000 | Loss: 0.00002449
Iteration 100/1000 | Loss: 0.00002449
Iteration 101/1000 | Loss: 0.00002449
Iteration 102/1000 | Loss: 0.00002449
Iteration 103/1000 | Loss: 0.00002449
Iteration 104/1000 | Loss: 0.00002449
Iteration 105/1000 | Loss: 0.00002449
Iteration 106/1000 | Loss: 0.00002449
Iteration 107/1000 | Loss: 0.00002449
Iteration 108/1000 | Loss: 0.00002449
Iteration 109/1000 | Loss: 0.00002449
Iteration 110/1000 | Loss: 0.00002449
Iteration 111/1000 | Loss: 0.00002449
Iteration 112/1000 | Loss: 0.00002449
Iteration 113/1000 | Loss: 0.00002449
Iteration 114/1000 | Loss: 0.00002449
Iteration 115/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.4487275368301198e-05, 2.4487275368301198e-05, 2.4487275368301198e-05, 2.4487275368301198e-05, 2.4487275368301198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4487275368301198e-05

Optimization complete. Final v2v error: 4.222609996795654 mm

Highest mean error: 4.627519130706787 mm for frame 46

Lowest mean error: 3.8967607021331787 mm for frame 13

Saving results

Total time: 34.32660126686096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770395
Iteration 2/25 | Loss: 0.00146204
Iteration 3/25 | Loss: 0.00100485
Iteration 4/25 | Loss: 0.00092531
Iteration 5/25 | Loss: 0.00084137
Iteration 6/25 | Loss: 0.00076473
Iteration 7/25 | Loss: 0.00075613
Iteration 8/25 | Loss: 0.00074824
Iteration 9/25 | Loss: 0.00074742
Iteration 10/25 | Loss: 0.00074676
Iteration 11/25 | Loss: 0.00074636
Iteration 12/25 | Loss: 0.00074615
Iteration 13/25 | Loss: 0.00074599
Iteration 14/25 | Loss: 0.00074592
Iteration 15/25 | Loss: 0.00074592
Iteration 16/25 | Loss: 0.00074591
Iteration 17/25 | Loss: 0.00074591
Iteration 18/25 | Loss: 0.00074591
Iteration 19/25 | Loss: 0.00074591
Iteration 20/25 | Loss: 0.00074591
Iteration 21/25 | Loss: 0.00074591
Iteration 22/25 | Loss: 0.00074591
Iteration 23/25 | Loss: 0.00074591
Iteration 24/25 | Loss: 0.00074591
Iteration 25/25 | Loss: 0.00074591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98980343
Iteration 2/25 | Loss: 0.00053953
Iteration 3/25 | Loss: 0.00053952
Iteration 4/25 | Loss: 0.00053952
Iteration 5/25 | Loss: 0.00053952
Iteration 6/25 | Loss: 0.00053952
Iteration 7/25 | Loss: 0.00053952
Iteration 8/25 | Loss: 0.00053952
Iteration 9/25 | Loss: 0.00053952
Iteration 10/25 | Loss: 0.00053952
Iteration 11/25 | Loss: 0.00053952
Iteration 12/25 | Loss: 0.00053952
Iteration 13/25 | Loss: 0.00053952
Iteration 14/25 | Loss: 0.00053952
Iteration 15/25 | Loss: 0.00053952
Iteration 16/25 | Loss: 0.00053952
Iteration 17/25 | Loss: 0.00053952
Iteration 18/25 | Loss: 0.00053952
Iteration 19/25 | Loss: 0.00053952
Iteration 20/25 | Loss: 0.00053952
Iteration 21/25 | Loss: 0.00053952
Iteration 22/25 | Loss: 0.00053952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005395215121097863, 0.0005395215121097863, 0.0005395215121097863, 0.0005395215121097863, 0.0005395215121097863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005395215121097863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053952
Iteration 2/1000 | Loss: 0.00017715
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001453
Iteration 8/1000 | Loss: 0.00010215
Iteration 9/1000 | Loss: 0.00001419
Iteration 10/1000 | Loss: 0.00007295
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001389
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00009338
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001367
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001366
Iteration 34/1000 | Loss: 0.00001366
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001355
Iteration 60/1000 | Loss: 0.00001355
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001353
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001343
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001341
Iteration 83/1000 | Loss: 0.00001341
Iteration 84/1000 | Loss: 0.00001341
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001333
Iteration 106/1000 | Loss: 0.00001333
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001332
Iteration 112/1000 | Loss: 0.00001332
Iteration 113/1000 | Loss: 0.00001332
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001331
Iteration 117/1000 | Loss: 0.00001331
Iteration 118/1000 | Loss: 0.00001331
Iteration 119/1000 | Loss: 0.00001331
Iteration 120/1000 | Loss: 0.00001331
Iteration 121/1000 | Loss: 0.00001331
Iteration 122/1000 | Loss: 0.00001331
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001329
Iteration 152/1000 | Loss: 0.00001329
Iteration 153/1000 | Loss: 0.00001329
Iteration 154/1000 | Loss: 0.00001329
Iteration 155/1000 | Loss: 0.00001329
Iteration 156/1000 | Loss: 0.00001329
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001329
Iteration 159/1000 | Loss: 0.00001329
Iteration 160/1000 | Loss: 0.00001329
Iteration 161/1000 | Loss: 0.00001329
Iteration 162/1000 | Loss: 0.00001329
Iteration 163/1000 | Loss: 0.00001329
Iteration 164/1000 | Loss: 0.00001329
Iteration 165/1000 | Loss: 0.00001329
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.3287286492413841e-05, 1.3287286492413841e-05, 1.3287286492413841e-05, 1.3287286492413841e-05, 1.3287286492413841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3287286492413841e-05

Optimization complete. Final v2v error: 3.0572314262390137 mm

Highest mean error: 3.471766471862793 mm for frame 64

Lowest mean error: 2.782806158065796 mm for frame 21

Saving results

Total time: 57.31041741371155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390271
Iteration 2/25 | Loss: 0.00095007
Iteration 3/25 | Loss: 0.00076656
Iteration 4/25 | Loss: 0.00075146
Iteration 5/25 | Loss: 0.00074477
Iteration 6/25 | Loss: 0.00074253
Iteration 7/25 | Loss: 0.00074217
Iteration 8/25 | Loss: 0.00074217
Iteration 9/25 | Loss: 0.00074217
Iteration 10/25 | Loss: 0.00074217
Iteration 11/25 | Loss: 0.00074217
Iteration 12/25 | Loss: 0.00074217
Iteration 13/25 | Loss: 0.00074217
Iteration 14/25 | Loss: 0.00074217
Iteration 15/25 | Loss: 0.00074217
Iteration 16/25 | Loss: 0.00074217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000742165429983288, 0.000742165429983288, 0.000742165429983288, 0.000742165429983288, 0.000742165429983288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000742165429983288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78167534
Iteration 2/25 | Loss: 0.00052219
Iteration 3/25 | Loss: 0.00052219
Iteration 4/25 | Loss: 0.00052219
Iteration 5/25 | Loss: 0.00052219
Iteration 6/25 | Loss: 0.00052219
Iteration 7/25 | Loss: 0.00052219
Iteration 8/25 | Loss: 0.00052219
Iteration 9/25 | Loss: 0.00052219
Iteration 10/25 | Loss: 0.00052219
Iteration 11/25 | Loss: 0.00052219
Iteration 12/25 | Loss: 0.00052219
Iteration 13/25 | Loss: 0.00052219
Iteration 14/25 | Loss: 0.00052219
Iteration 15/25 | Loss: 0.00052219
Iteration 16/25 | Loss: 0.00052219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005221887258812785, 0.0005221887258812785, 0.0005221887258812785, 0.0005221887258812785, 0.0005221887258812785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005221887258812785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052219
Iteration 2/1000 | Loss: 0.00002193
Iteration 3/1000 | Loss: 0.00001528
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001160
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001125
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001107
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001095
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001094
Iteration 23/1000 | Loss: 0.00001094
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001092
Iteration 26/1000 | Loss: 0.00001091
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001086
Iteration 31/1000 | Loss: 0.00001085
Iteration 32/1000 | Loss: 0.00001085
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001084
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001083
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001083
Iteration 48/1000 | Loss: 0.00001083
Iteration 49/1000 | Loss: 0.00001083
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001082
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001081
Iteration 57/1000 | Loss: 0.00001081
Iteration 58/1000 | Loss: 0.00001081
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001080
Iteration 64/1000 | Loss: 0.00001080
Iteration 65/1000 | Loss: 0.00001080
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001079
Iteration 69/1000 | Loss: 0.00001079
Iteration 70/1000 | Loss: 0.00001079
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001079
Iteration 73/1000 | Loss: 0.00001079
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001077
Iteration 100/1000 | Loss: 0.00001077
Iteration 101/1000 | Loss: 0.00001077
Iteration 102/1000 | Loss: 0.00001077
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001074
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001073
Iteration 148/1000 | Loss: 0.00001073
Iteration 149/1000 | Loss: 0.00001073
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001073
Iteration 159/1000 | Loss: 0.00001073
Iteration 160/1000 | Loss: 0.00001073
Iteration 161/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.0729458153946325e-05, 1.0729458153946325e-05, 1.0729458153946325e-05, 1.0729458153946325e-05, 1.0729458153946325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0729458153946325e-05

Optimization complete. Final v2v error: 2.781022310256958 mm

Highest mean error: 2.987826347351074 mm for frame 53

Lowest mean error: 2.651620388031006 mm for frame 4

Saving results

Total time: 41.92519450187683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568268
Iteration 2/25 | Loss: 0.00095775
Iteration 3/25 | Loss: 0.00080906
Iteration 4/25 | Loss: 0.00078349
Iteration 5/25 | Loss: 0.00077746
Iteration 6/25 | Loss: 0.00077684
Iteration 7/25 | Loss: 0.00077684
Iteration 8/25 | Loss: 0.00077684
Iteration 9/25 | Loss: 0.00077684
Iteration 10/25 | Loss: 0.00077684
Iteration 11/25 | Loss: 0.00077684
Iteration 12/25 | Loss: 0.00077684
Iteration 13/25 | Loss: 0.00077684
Iteration 14/25 | Loss: 0.00077684
Iteration 15/25 | Loss: 0.00077684
Iteration 16/25 | Loss: 0.00077684
Iteration 17/25 | Loss: 0.00077684
Iteration 18/25 | Loss: 0.00077684
Iteration 19/25 | Loss: 0.00077684
Iteration 20/25 | Loss: 0.00077684
Iteration 21/25 | Loss: 0.00077684
Iteration 22/25 | Loss: 0.00077684
Iteration 23/25 | Loss: 0.00077684
Iteration 24/25 | Loss: 0.00077684
Iteration 25/25 | Loss: 0.00077684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46869278
Iteration 2/25 | Loss: 0.00039600
Iteration 3/25 | Loss: 0.00039595
Iteration 4/25 | Loss: 0.00039595
Iteration 5/25 | Loss: 0.00039595
Iteration 6/25 | Loss: 0.00039595
Iteration 7/25 | Loss: 0.00039595
Iteration 8/25 | Loss: 0.00039595
Iteration 9/25 | Loss: 0.00039595
Iteration 10/25 | Loss: 0.00039595
Iteration 11/25 | Loss: 0.00039595
Iteration 12/25 | Loss: 0.00039595
Iteration 13/25 | Loss: 0.00039595
Iteration 14/25 | Loss: 0.00039595
Iteration 15/25 | Loss: 0.00039595
Iteration 16/25 | Loss: 0.00039595
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003959473979193717, 0.0003959473979193717, 0.0003959473979193717, 0.0003959473979193717, 0.0003959473979193717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003959473979193717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039595
Iteration 2/1000 | Loss: 0.00003116
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001510
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001296
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001266
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001247
Iteration 13/1000 | Loss: 0.00001238
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001222
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001192
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.1923844795092009e-05, 1.1923844795092009e-05, 1.1923844795092009e-05, 1.1923844795092009e-05, 1.1923844795092009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1923844795092009e-05

Optimization complete. Final v2v error: 2.9117276668548584 mm

Highest mean error: 3.001206159591675 mm for frame 149

Lowest mean error: 2.8484933376312256 mm for frame 67

Saving results

Total time: 32.29907560348511
