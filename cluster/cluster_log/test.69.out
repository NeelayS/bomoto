Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=69, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3864-3919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383969
Iteration 2/25 | Loss: 0.00143005
Iteration 3/25 | Loss: 0.00135503
Iteration 4/25 | Loss: 0.00134615
Iteration 5/25 | Loss: 0.00134369
Iteration 6/25 | Loss: 0.00134330
Iteration 7/25 | Loss: 0.00134330
Iteration 8/25 | Loss: 0.00134330
Iteration 9/25 | Loss: 0.00134330
Iteration 10/25 | Loss: 0.00134330
Iteration 11/25 | Loss: 0.00134330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013432953273877501, 0.0013432953273877501, 0.0013432953273877501, 0.0013432953273877501, 0.0013432953273877501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013432953273877501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55718732
Iteration 2/25 | Loss: 0.00215561
Iteration 3/25 | Loss: 0.00215561
Iteration 4/25 | Loss: 0.00215561
Iteration 5/25 | Loss: 0.00215561
Iteration 6/25 | Loss: 0.00215561
Iteration 7/25 | Loss: 0.00215561
Iteration 8/25 | Loss: 0.00215561
Iteration 9/25 | Loss: 0.00215561
Iteration 10/25 | Loss: 0.00215561
Iteration 11/25 | Loss: 0.00215561
Iteration 12/25 | Loss: 0.00215561
Iteration 13/25 | Loss: 0.00215561
Iteration 14/25 | Loss: 0.00215561
Iteration 15/25 | Loss: 0.00215561
Iteration 16/25 | Loss: 0.00215561
Iteration 17/25 | Loss: 0.00215561
Iteration 18/25 | Loss: 0.00215561
Iteration 19/25 | Loss: 0.00215561
Iteration 20/25 | Loss: 0.00215561
Iteration 21/25 | Loss: 0.00215561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002155609428882599, 0.002155609428882599, 0.002155609428882599, 0.002155609428882599, 0.002155609428882599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002155609428882599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215561
Iteration 2/1000 | Loss: 0.00002743
Iteration 3/1000 | Loss: 0.00001872
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001281
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001139
Iteration 10/1000 | Loss: 0.00001107
Iteration 11/1000 | Loss: 0.00001088
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001064
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001059
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001056
Iteration 20/1000 | Loss: 0.00001055
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001032
Iteration 26/1000 | Loss: 0.00001032
Iteration 27/1000 | Loss: 0.00001025
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001017
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001013
Iteration 34/1000 | Loss: 0.00001013
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001005
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001004
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000999
Iteration 80/1000 | Loss: 0.00000999
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000997
Iteration 86/1000 | Loss: 0.00000997
Iteration 87/1000 | Loss: 0.00000997
Iteration 88/1000 | Loss: 0.00000997
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000996
Iteration 92/1000 | Loss: 0.00000996
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000995
Iteration 95/1000 | Loss: 0.00000995
Iteration 96/1000 | Loss: 0.00000995
Iteration 97/1000 | Loss: 0.00000995
Iteration 98/1000 | Loss: 0.00000995
Iteration 99/1000 | Loss: 0.00000995
Iteration 100/1000 | Loss: 0.00000995
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000994
Iteration 104/1000 | Loss: 0.00000994
Iteration 105/1000 | Loss: 0.00000994
Iteration 106/1000 | Loss: 0.00000994
Iteration 107/1000 | Loss: 0.00000994
Iteration 108/1000 | Loss: 0.00000994
Iteration 109/1000 | Loss: 0.00000993
Iteration 110/1000 | Loss: 0.00000993
Iteration 111/1000 | Loss: 0.00000993
Iteration 112/1000 | Loss: 0.00000993
Iteration 113/1000 | Loss: 0.00000993
Iteration 114/1000 | Loss: 0.00000993
Iteration 115/1000 | Loss: 0.00000993
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000992
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000992
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000991
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000991
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000990
Iteration 137/1000 | Loss: 0.00000990
Iteration 138/1000 | Loss: 0.00000990
Iteration 139/1000 | Loss: 0.00000989
Iteration 140/1000 | Loss: 0.00000989
Iteration 141/1000 | Loss: 0.00000989
Iteration 142/1000 | Loss: 0.00000989
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000989
Iteration 147/1000 | Loss: 0.00000989
Iteration 148/1000 | Loss: 0.00000988
Iteration 149/1000 | Loss: 0.00000988
Iteration 150/1000 | Loss: 0.00000988
Iteration 151/1000 | Loss: 0.00000988
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Iteration 154/1000 | Loss: 0.00000987
Iteration 155/1000 | Loss: 0.00000987
Iteration 156/1000 | Loss: 0.00000987
Iteration 157/1000 | Loss: 0.00000987
Iteration 158/1000 | Loss: 0.00000987
Iteration 159/1000 | Loss: 0.00000987
Iteration 160/1000 | Loss: 0.00000987
Iteration 161/1000 | Loss: 0.00000986
Iteration 162/1000 | Loss: 0.00000986
Iteration 163/1000 | Loss: 0.00000986
Iteration 164/1000 | Loss: 0.00000986
Iteration 165/1000 | Loss: 0.00000986
Iteration 166/1000 | Loss: 0.00000986
Iteration 167/1000 | Loss: 0.00000986
Iteration 168/1000 | Loss: 0.00000986
Iteration 169/1000 | Loss: 0.00000986
Iteration 170/1000 | Loss: 0.00000986
Iteration 171/1000 | Loss: 0.00000986
Iteration 172/1000 | Loss: 0.00000986
Iteration 173/1000 | Loss: 0.00000986
Iteration 174/1000 | Loss: 0.00000985
Iteration 175/1000 | Loss: 0.00000985
Iteration 176/1000 | Loss: 0.00000985
Iteration 177/1000 | Loss: 0.00000985
Iteration 178/1000 | Loss: 0.00000985
Iteration 179/1000 | Loss: 0.00000985
Iteration 180/1000 | Loss: 0.00000985
Iteration 181/1000 | Loss: 0.00000985
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000984
Iteration 187/1000 | Loss: 0.00000984
Iteration 188/1000 | Loss: 0.00000984
Iteration 189/1000 | Loss: 0.00000984
Iteration 190/1000 | Loss: 0.00000984
Iteration 191/1000 | Loss: 0.00000984
Iteration 192/1000 | Loss: 0.00000984
Iteration 193/1000 | Loss: 0.00000984
Iteration 194/1000 | Loss: 0.00000984
Iteration 195/1000 | Loss: 0.00000984
Iteration 196/1000 | Loss: 0.00000983
Iteration 197/1000 | Loss: 0.00000983
Iteration 198/1000 | Loss: 0.00000983
Iteration 199/1000 | Loss: 0.00000983
Iteration 200/1000 | Loss: 0.00000982
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000982
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000982
Iteration 210/1000 | Loss: 0.00000982
Iteration 211/1000 | Loss: 0.00000981
Iteration 212/1000 | Loss: 0.00000981
Iteration 213/1000 | Loss: 0.00000981
Iteration 214/1000 | Loss: 0.00000981
Iteration 215/1000 | Loss: 0.00000981
Iteration 216/1000 | Loss: 0.00000981
Iteration 217/1000 | Loss: 0.00000980
Iteration 218/1000 | Loss: 0.00000980
Iteration 219/1000 | Loss: 0.00000980
Iteration 220/1000 | Loss: 0.00000980
Iteration 221/1000 | Loss: 0.00000980
Iteration 222/1000 | Loss: 0.00000980
Iteration 223/1000 | Loss: 0.00000980
Iteration 224/1000 | Loss: 0.00000980
Iteration 225/1000 | Loss: 0.00000979
Iteration 226/1000 | Loss: 0.00000979
Iteration 227/1000 | Loss: 0.00000979
Iteration 228/1000 | Loss: 0.00000979
Iteration 229/1000 | Loss: 0.00000978
Iteration 230/1000 | Loss: 0.00000978
Iteration 231/1000 | Loss: 0.00000978
Iteration 232/1000 | Loss: 0.00000978
Iteration 233/1000 | Loss: 0.00000977
Iteration 234/1000 | Loss: 0.00000977
Iteration 235/1000 | Loss: 0.00000977
Iteration 236/1000 | Loss: 0.00000977
Iteration 237/1000 | Loss: 0.00000977
Iteration 238/1000 | Loss: 0.00000977
Iteration 239/1000 | Loss: 0.00000977
Iteration 240/1000 | Loss: 0.00000977
Iteration 241/1000 | Loss: 0.00000977
Iteration 242/1000 | Loss: 0.00000977
Iteration 243/1000 | Loss: 0.00000977
Iteration 244/1000 | Loss: 0.00000977
Iteration 245/1000 | Loss: 0.00000977
Iteration 246/1000 | Loss: 0.00000977
Iteration 247/1000 | Loss: 0.00000976
Iteration 248/1000 | Loss: 0.00000976
Iteration 249/1000 | Loss: 0.00000976
Iteration 250/1000 | Loss: 0.00000975
Iteration 251/1000 | Loss: 0.00000975
Iteration 252/1000 | Loss: 0.00000975
Iteration 253/1000 | Loss: 0.00000975
Iteration 254/1000 | Loss: 0.00000975
Iteration 255/1000 | Loss: 0.00000975
Iteration 256/1000 | Loss: 0.00000975
Iteration 257/1000 | Loss: 0.00000975
Iteration 258/1000 | Loss: 0.00000975
Iteration 259/1000 | Loss: 0.00000975
Iteration 260/1000 | Loss: 0.00000975
Iteration 261/1000 | Loss: 0.00000975
Iteration 262/1000 | Loss: 0.00000975
Iteration 263/1000 | Loss: 0.00000975
Iteration 264/1000 | Loss: 0.00000975
Iteration 265/1000 | Loss: 0.00000975
Iteration 266/1000 | Loss: 0.00000975
Iteration 267/1000 | Loss: 0.00000975
Iteration 268/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [9.746551768330391e-06, 9.746551768330391e-06, 9.746551768330391e-06, 9.746551768330391e-06, 9.746551768330391e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.746551768330391e-06

Optimization complete. Final v2v error: 2.6992592811584473 mm

Highest mean error: 3.076158285140991 mm for frame 83

Lowest mean error: 2.6080453395843506 mm for frame 29

Saving results

Total time: 49.75598955154419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498085
Iteration 2/25 | Loss: 0.00153978
Iteration 3/25 | Loss: 0.00140109
Iteration 4/25 | Loss: 0.00139309
Iteration 5/25 | Loss: 0.00139134
Iteration 6/25 | Loss: 0.00139120
Iteration 7/25 | Loss: 0.00139120
Iteration 8/25 | Loss: 0.00139120
Iteration 9/25 | Loss: 0.00139120
Iteration 10/25 | Loss: 0.00139120
Iteration 11/25 | Loss: 0.00139120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013911979040130973, 0.0013911979040130973, 0.0013911979040130973, 0.0013911979040130973, 0.0013911979040130973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013911979040130973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26368630
Iteration 2/25 | Loss: 0.00192034
Iteration 3/25 | Loss: 0.00192034
Iteration 4/25 | Loss: 0.00192034
Iteration 5/25 | Loss: 0.00192033
Iteration 6/25 | Loss: 0.00192033
Iteration 7/25 | Loss: 0.00192033
Iteration 8/25 | Loss: 0.00192033
Iteration 9/25 | Loss: 0.00192033
Iteration 10/25 | Loss: 0.00192033
Iteration 11/25 | Loss: 0.00192033
Iteration 12/25 | Loss: 0.00192033
Iteration 13/25 | Loss: 0.00192033
Iteration 14/25 | Loss: 0.00192033
Iteration 15/25 | Loss: 0.00192033
Iteration 16/25 | Loss: 0.00192033
Iteration 17/25 | Loss: 0.00192033
Iteration 18/25 | Loss: 0.00192033
Iteration 19/25 | Loss: 0.00192033
Iteration 20/25 | Loss: 0.00192033
Iteration 21/25 | Loss: 0.00192033
Iteration 22/25 | Loss: 0.00192033
Iteration 23/25 | Loss: 0.00192033
Iteration 24/25 | Loss: 0.00192033
Iteration 25/25 | Loss: 0.00192033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192033
Iteration 2/1000 | Loss: 0.00004049
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002203
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001927
Iteration 7/1000 | Loss: 0.00001854
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001678
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001650
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001634
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001622
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001615
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001585
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001575
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001571
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001568
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001566
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001565
Iteration 73/1000 | Loss: 0.00001565
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.5644118320778944e-05, 1.5644118320778944e-05, 1.5644118320778944e-05, 1.5644118320778944e-05, 1.5644118320778944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5644118320778944e-05

Optimization complete. Final v2v error: 3.236193895339966 mm

Highest mean error: 4.4628496170043945 mm for frame 69

Lowest mean error: 2.8293750286102295 mm for frame 9

Saving results

Total time: 40.146222829818726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370443
Iteration 2/25 | Loss: 0.00153558
Iteration 3/25 | Loss: 0.00139696
Iteration 4/25 | Loss: 0.00138683
Iteration 5/25 | Loss: 0.00138343
Iteration 6/25 | Loss: 0.00138208
Iteration 7/25 | Loss: 0.00138164
Iteration 8/25 | Loss: 0.00138164
Iteration 9/25 | Loss: 0.00138164
Iteration 10/25 | Loss: 0.00138164
Iteration 11/25 | Loss: 0.00138164
Iteration 12/25 | Loss: 0.00138164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013816433493047953, 0.0013816433493047953, 0.0013816433493047953, 0.0013816433493047953, 0.0013816433493047953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013816433493047953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22516072
Iteration 2/25 | Loss: 0.00297223
Iteration 3/25 | Loss: 0.00297223
Iteration 4/25 | Loss: 0.00297223
Iteration 5/25 | Loss: 0.00297223
Iteration 6/25 | Loss: 0.00297223
Iteration 7/25 | Loss: 0.00297223
Iteration 8/25 | Loss: 0.00297223
Iteration 9/25 | Loss: 0.00297223
Iteration 10/25 | Loss: 0.00297223
Iteration 11/25 | Loss: 0.00297223
Iteration 12/25 | Loss: 0.00297223
Iteration 13/25 | Loss: 0.00297223
Iteration 14/25 | Loss: 0.00297223
Iteration 15/25 | Loss: 0.00297223
Iteration 16/25 | Loss: 0.00297223
Iteration 17/25 | Loss: 0.00297223
Iteration 18/25 | Loss: 0.00297223
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002972227521240711, 0.002972227521240711, 0.002972227521240711, 0.002972227521240711, 0.002972227521240711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002972227521240711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00297223
Iteration 2/1000 | Loss: 0.00004312
Iteration 3/1000 | Loss: 0.00002548
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001468
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001384
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001360
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001350
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001327
Iteration 42/1000 | Loss: 0.00001327
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001326
Iteration 45/1000 | Loss: 0.00001326
Iteration 46/1000 | Loss: 0.00001326
Iteration 47/1000 | Loss: 0.00001326
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001322
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001321
Iteration 53/1000 | Loss: 0.00001321
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.3122724340064451e-05, 1.3122724340064451e-05, 1.3122724340064451e-05, 1.3122724340064451e-05, 1.3122724340064451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3122724340064451e-05

Optimization complete. Final v2v error: 3.1482174396514893 mm

Highest mean error: 3.3921549320220947 mm for frame 93

Lowest mean error: 2.8450632095336914 mm for frame 9

Saving results

Total time: 37.0236234664917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960086
Iteration 2/25 | Loss: 0.00212777
Iteration 3/25 | Loss: 0.00180099
Iteration 4/25 | Loss: 0.00172349
Iteration 5/25 | Loss: 0.00186044
Iteration 6/25 | Loss: 0.00177532
Iteration 7/25 | Loss: 0.00166860
Iteration 8/25 | Loss: 0.00154697
Iteration 9/25 | Loss: 0.00159232
Iteration 10/25 | Loss: 0.00153088
Iteration 11/25 | Loss: 0.00149788
Iteration 12/25 | Loss: 0.00154879
Iteration 13/25 | Loss: 0.00149202
Iteration 14/25 | Loss: 0.00149050
Iteration 15/25 | Loss: 0.00147305
Iteration 16/25 | Loss: 0.00146284
Iteration 17/25 | Loss: 0.00146716
Iteration 18/25 | Loss: 0.00146638
Iteration 19/25 | Loss: 0.00146337
Iteration 20/25 | Loss: 0.00146052
Iteration 21/25 | Loss: 0.00145565
Iteration 22/25 | Loss: 0.00145524
Iteration 23/25 | Loss: 0.00146128
Iteration 24/25 | Loss: 0.00145958
Iteration 25/25 | Loss: 0.00146607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68192029
Iteration 2/25 | Loss: 0.00268129
Iteration 3/25 | Loss: 0.00268129
Iteration 4/25 | Loss: 0.00268129
Iteration 5/25 | Loss: 0.00268129
Iteration 6/25 | Loss: 0.00268129
Iteration 7/25 | Loss: 0.00268129
Iteration 8/25 | Loss: 0.00268129
Iteration 9/25 | Loss: 0.00268129
Iteration 10/25 | Loss: 0.00268129
Iteration 11/25 | Loss: 0.00268129
Iteration 12/25 | Loss: 0.00268128
Iteration 13/25 | Loss: 0.00268128
Iteration 14/25 | Loss: 0.00268128
Iteration 15/25 | Loss: 0.00268128
Iteration 16/25 | Loss: 0.00268128
Iteration 17/25 | Loss: 0.00268128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026812846772372723, 0.0026812846772372723, 0.0026812846772372723, 0.0026812846772372723, 0.0026812846772372723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026812846772372723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268128
Iteration 2/1000 | Loss: 0.00011327
Iteration 3/1000 | Loss: 0.00021984
Iteration 4/1000 | Loss: 0.00016917
Iteration 5/1000 | Loss: 0.00018415
Iteration 6/1000 | Loss: 0.00008449
Iteration 7/1000 | Loss: 0.00006625
Iteration 8/1000 | Loss: 0.00030346
Iteration 9/1000 | Loss: 0.00025649
Iteration 10/1000 | Loss: 0.00017693
Iteration 11/1000 | Loss: 0.00018201
Iteration 12/1000 | Loss: 0.00029357
Iteration 13/1000 | Loss: 0.00026313
Iteration 14/1000 | Loss: 0.00031386
Iteration 15/1000 | Loss: 0.00025713
Iteration 16/1000 | Loss: 0.00034139
Iteration 17/1000 | Loss: 0.00029147
Iteration 18/1000 | Loss: 0.00021822
Iteration 19/1000 | Loss: 0.00021592
Iteration 20/1000 | Loss: 0.00019952
Iteration 21/1000 | Loss: 0.00017827
Iteration 22/1000 | Loss: 0.00014715
Iteration 23/1000 | Loss: 0.00015216
Iteration 24/1000 | Loss: 0.00055265
Iteration 25/1000 | Loss: 0.00017297
Iteration 26/1000 | Loss: 0.00018787
Iteration 27/1000 | Loss: 0.00005159
Iteration 28/1000 | Loss: 0.00016611
Iteration 29/1000 | Loss: 0.00011037
Iteration 30/1000 | Loss: 0.00004373
Iteration 31/1000 | Loss: 0.00027582
Iteration 32/1000 | Loss: 0.00021983
Iteration 33/1000 | Loss: 0.00011422
Iteration 34/1000 | Loss: 0.00007294
Iteration 35/1000 | Loss: 0.00004775
Iteration 36/1000 | Loss: 0.00019779
Iteration 37/1000 | Loss: 0.00003990
Iteration 38/1000 | Loss: 0.00003479
Iteration 39/1000 | Loss: 0.00003102
Iteration 40/1000 | Loss: 0.00002827
Iteration 41/1000 | Loss: 0.00002627
Iteration 42/1000 | Loss: 0.00002462
Iteration 43/1000 | Loss: 0.00002340
Iteration 44/1000 | Loss: 0.00002254
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001910
Iteration 51/1000 | Loss: 0.00063814
Iteration 52/1000 | Loss: 0.00002561
Iteration 53/1000 | Loss: 0.00002281
Iteration 54/1000 | Loss: 0.00002041
Iteration 55/1000 | Loss: 0.00001798
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001594
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001587
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001580
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001568
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001565
Iteration 73/1000 | Loss: 0.00001565
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001562
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001558
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001557
Iteration 88/1000 | Loss: 0.00001557
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001556
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001555
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001554
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001553
Iteration 122/1000 | Loss: 0.00001553
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001552
Iteration 142/1000 | Loss: 0.00001552
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001552
Iteration 147/1000 | Loss: 0.00001552
Iteration 148/1000 | Loss: 0.00001552
Iteration 149/1000 | Loss: 0.00001552
Iteration 150/1000 | Loss: 0.00001552
Iteration 151/1000 | Loss: 0.00001552
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001551
Iteration 155/1000 | Loss: 0.00001551
Iteration 156/1000 | Loss: 0.00001551
Iteration 157/1000 | Loss: 0.00001551
Iteration 158/1000 | Loss: 0.00001551
Iteration 159/1000 | Loss: 0.00001551
Iteration 160/1000 | Loss: 0.00001551
Iteration 161/1000 | Loss: 0.00001551
Iteration 162/1000 | Loss: 0.00001551
Iteration 163/1000 | Loss: 0.00001551
Iteration 164/1000 | Loss: 0.00001551
Iteration 165/1000 | Loss: 0.00001551
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001551
Iteration 168/1000 | Loss: 0.00001551
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Iteration 171/1000 | Loss: 0.00001550
Iteration 172/1000 | Loss: 0.00001550
Iteration 173/1000 | Loss: 0.00001550
Iteration 174/1000 | Loss: 0.00001550
Iteration 175/1000 | Loss: 0.00001550
Iteration 176/1000 | Loss: 0.00001550
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001550
Iteration 184/1000 | Loss: 0.00001550
Iteration 185/1000 | Loss: 0.00001550
Iteration 186/1000 | Loss: 0.00001550
Iteration 187/1000 | Loss: 0.00001550
Iteration 188/1000 | Loss: 0.00001550
Iteration 189/1000 | Loss: 0.00001550
Iteration 190/1000 | Loss: 0.00001550
Iteration 191/1000 | Loss: 0.00001550
Iteration 192/1000 | Loss: 0.00001550
Iteration 193/1000 | Loss: 0.00001550
Iteration 194/1000 | Loss: 0.00001550
Iteration 195/1000 | Loss: 0.00001550
Iteration 196/1000 | Loss: 0.00001550
Iteration 197/1000 | Loss: 0.00001550
Iteration 198/1000 | Loss: 0.00001550
Iteration 199/1000 | Loss: 0.00001550
Iteration 200/1000 | Loss: 0.00001550
Iteration 201/1000 | Loss: 0.00001550
Iteration 202/1000 | Loss: 0.00001550
Iteration 203/1000 | Loss: 0.00001550
Iteration 204/1000 | Loss: 0.00001550
Iteration 205/1000 | Loss: 0.00001550
Iteration 206/1000 | Loss: 0.00001550
Iteration 207/1000 | Loss: 0.00001550
Iteration 208/1000 | Loss: 0.00001550
Iteration 209/1000 | Loss: 0.00001550
Iteration 210/1000 | Loss: 0.00001550
Iteration 211/1000 | Loss: 0.00001550
Iteration 212/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.54959452629555e-05, 1.54959452629555e-05, 1.54959452629555e-05, 1.54959452629555e-05, 1.54959452629555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.54959452629555e-05

Optimization complete. Final v2v error: 3.3443338871002197 mm

Highest mean error: 4.7456536293029785 mm for frame 63

Lowest mean error: 2.9777426719665527 mm for frame 106

Saving results

Total time: 135.22075653076172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855238
Iteration 2/25 | Loss: 0.00149869
Iteration 3/25 | Loss: 0.00138859
Iteration 4/25 | Loss: 0.00137725
Iteration 5/25 | Loss: 0.00137440
Iteration 6/25 | Loss: 0.00137380
Iteration 7/25 | Loss: 0.00137380
Iteration 8/25 | Loss: 0.00137380
Iteration 9/25 | Loss: 0.00137380
Iteration 10/25 | Loss: 0.00137380
Iteration 11/25 | Loss: 0.00137380
Iteration 12/25 | Loss: 0.00137380
Iteration 13/25 | Loss: 0.00137380
Iteration 14/25 | Loss: 0.00137380
Iteration 15/25 | Loss: 0.00137380
Iteration 16/25 | Loss: 0.00137380
Iteration 17/25 | Loss: 0.00137380
Iteration 18/25 | Loss: 0.00137380
Iteration 19/25 | Loss: 0.00137380
Iteration 20/25 | Loss: 0.00137380
Iteration 21/25 | Loss: 0.00137380
Iteration 22/25 | Loss: 0.00137380
Iteration 23/25 | Loss: 0.00137380
Iteration 24/25 | Loss: 0.00137380
Iteration 25/25 | Loss: 0.00137380

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33720171
Iteration 2/25 | Loss: 0.00205677
Iteration 3/25 | Loss: 0.00205677
Iteration 4/25 | Loss: 0.00205677
Iteration 5/25 | Loss: 0.00205677
Iteration 6/25 | Loss: 0.00205677
Iteration 7/25 | Loss: 0.00205677
Iteration 8/25 | Loss: 0.00205677
Iteration 9/25 | Loss: 0.00205677
Iteration 10/25 | Loss: 0.00205677
Iteration 11/25 | Loss: 0.00205677
Iteration 12/25 | Loss: 0.00205677
Iteration 13/25 | Loss: 0.00205677
Iteration 14/25 | Loss: 0.00205677
Iteration 15/25 | Loss: 0.00205677
Iteration 16/25 | Loss: 0.00205677
Iteration 17/25 | Loss: 0.00205677
Iteration 18/25 | Loss: 0.00205677
Iteration 19/25 | Loss: 0.00205677
Iteration 20/25 | Loss: 0.00205677
Iteration 21/25 | Loss: 0.00205677
Iteration 22/25 | Loss: 0.00205677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0020567672327160835, 0.0020567672327160835, 0.0020567672327160835, 0.0020567672327160835, 0.0020567672327160835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020567672327160835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205677
Iteration 2/1000 | Loss: 0.00003357
Iteration 3/1000 | Loss: 0.00002420
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00001952
Iteration 6/1000 | Loss: 0.00001874
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001740
Iteration 9/1000 | Loss: 0.00001700
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001592
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001539
Iteration 19/1000 | Loss: 0.00001536
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001516
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001513
Iteration 32/1000 | Loss: 0.00001512
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001509
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001506
Iteration 41/1000 | Loss: 0.00001505
Iteration 42/1000 | Loss: 0.00001505
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001504
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001501
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001498
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001497
Iteration 66/1000 | Loss: 0.00001497
Iteration 67/1000 | Loss: 0.00001497
Iteration 68/1000 | Loss: 0.00001496
Iteration 69/1000 | Loss: 0.00001496
Iteration 70/1000 | Loss: 0.00001496
Iteration 71/1000 | Loss: 0.00001496
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001495
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001494
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001494
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001493
Iteration 91/1000 | Loss: 0.00001493
Iteration 92/1000 | Loss: 0.00001493
Iteration 93/1000 | Loss: 0.00001493
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001492
Iteration 98/1000 | Loss: 0.00001492
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001491
Iteration 109/1000 | Loss: 0.00001491
Iteration 110/1000 | Loss: 0.00001491
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001491
Iteration 114/1000 | Loss: 0.00001491
Iteration 115/1000 | Loss: 0.00001491
Iteration 116/1000 | Loss: 0.00001491
Iteration 117/1000 | Loss: 0.00001491
Iteration 118/1000 | Loss: 0.00001491
Iteration 119/1000 | Loss: 0.00001491
Iteration 120/1000 | Loss: 0.00001491
Iteration 121/1000 | Loss: 0.00001491
Iteration 122/1000 | Loss: 0.00001491
Iteration 123/1000 | Loss: 0.00001491
Iteration 124/1000 | Loss: 0.00001491
Iteration 125/1000 | Loss: 0.00001491
Iteration 126/1000 | Loss: 0.00001491
Iteration 127/1000 | Loss: 0.00001491
Iteration 128/1000 | Loss: 0.00001491
Iteration 129/1000 | Loss: 0.00001491
Iteration 130/1000 | Loss: 0.00001491
Iteration 131/1000 | Loss: 0.00001491
Iteration 132/1000 | Loss: 0.00001491
Iteration 133/1000 | Loss: 0.00001491
Iteration 134/1000 | Loss: 0.00001491
Iteration 135/1000 | Loss: 0.00001491
Iteration 136/1000 | Loss: 0.00001491
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001491
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4906733667885419e-05, 1.4906733667885419e-05, 1.4906733667885419e-05, 1.4906733667885419e-05, 1.4906733667885419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4906733667885419e-05

Optimization complete. Final v2v error: 3.2353105545043945 mm

Highest mean error: 4.5853190422058105 mm for frame 55

Lowest mean error: 2.9002745151519775 mm for frame 20

Saving results

Total time: 40.439478397369385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945755
Iteration 2/25 | Loss: 0.00945755
Iteration 3/25 | Loss: 0.00332467
Iteration 4/25 | Loss: 0.00227520
Iteration 5/25 | Loss: 0.00204868
Iteration 6/25 | Loss: 0.00198480
Iteration 7/25 | Loss: 0.00205199
Iteration 8/25 | Loss: 0.00188082
Iteration 9/25 | Loss: 0.00176625
Iteration 10/25 | Loss: 0.00172113
Iteration 11/25 | Loss: 0.00170191
Iteration 12/25 | Loss: 0.00168269
Iteration 13/25 | Loss: 0.00166169
Iteration 14/25 | Loss: 0.00164695
Iteration 15/25 | Loss: 0.00163555
Iteration 16/25 | Loss: 0.00163588
Iteration 17/25 | Loss: 0.00162833
Iteration 18/25 | Loss: 0.00162664
Iteration 19/25 | Loss: 0.00162620
Iteration 20/25 | Loss: 0.00162516
Iteration 21/25 | Loss: 0.00162419
Iteration 22/25 | Loss: 0.00162302
Iteration 23/25 | Loss: 0.00162218
Iteration 24/25 | Loss: 0.00162313
Iteration 25/25 | Loss: 0.00162546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22767532
Iteration 2/25 | Loss: 0.00566596
Iteration 3/25 | Loss: 0.00522398
Iteration 4/25 | Loss: 0.00522379
Iteration 5/25 | Loss: 0.00522384
Iteration 6/25 | Loss: 0.00522386
Iteration 7/25 | Loss: 0.00522386
Iteration 8/25 | Loss: 0.00522321
Iteration 9/25 | Loss: 0.00522321
Iteration 10/25 | Loss: 0.00522321
Iteration 11/25 | Loss: 0.00522321
Iteration 12/25 | Loss: 0.00522321
Iteration 13/25 | Loss: 0.00522321
Iteration 14/25 | Loss: 0.00522321
Iteration 15/25 | Loss: 0.00522321
Iteration 16/25 | Loss: 0.00522321
Iteration 17/25 | Loss: 0.00522321
Iteration 18/25 | Loss: 0.00522320
Iteration 19/25 | Loss: 0.00522320
Iteration 20/25 | Loss: 0.00522320
Iteration 21/25 | Loss: 0.00522321
Iteration 22/25 | Loss: 0.00522320
Iteration 23/25 | Loss: 0.00522321
Iteration 24/25 | Loss: 0.00522320
Iteration 25/25 | Loss: 0.00522320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00522321
Iteration 2/1000 | Loss: 0.00092311
Iteration 3/1000 | Loss: 0.00077615
Iteration 4/1000 | Loss: 0.00246973
Iteration 5/1000 | Loss: 0.00030261
Iteration 6/1000 | Loss: 0.00031727
Iteration 7/1000 | Loss: 0.00080100
Iteration 8/1000 | Loss: 0.00135336
Iteration 9/1000 | Loss: 0.00033204
Iteration 10/1000 | Loss: 0.00012094
Iteration 11/1000 | Loss: 0.00077513
Iteration 12/1000 | Loss: 0.00010646
Iteration 13/1000 | Loss: 0.00019236
Iteration 14/1000 | Loss: 0.00006186
Iteration 15/1000 | Loss: 0.00011606
Iteration 16/1000 | Loss: 0.00005420
Iteration 17/1000 | Loss: 0.00005026
Iteration 18/1000 | Loss: 0.00014696
Iteration 19/1000 | Loss: 0.00007794
Iteration 20/1000 | Loss: 0.00006473
Iteration 21/1000 | Loss: 0.00005869
Iteration 22/1000 | Loss: 0.00010711
Iteration 23/1000 | Loss: 0.00003045
Iteration 24/1000 | Loss: 0.00011929
Iteration 25/1000 | Loss: 0.00007274
Iteration 26/1000 | Loss: 0.00002962
Iteration 27/1000 | Loss: 0.00007745
Iteration 28/1000 | Loss: 0.00086170
Iteration 29/1000 | Loss: 0.00006705
Iteration 30/1000 | Loss: 0.00003886
Iteration 31/1000 | Loss: 0.00003922
Iteration 32/1000 | Loss: 0.00003743
Iteration 33/1000 | Loss: 0.00005773
Iteration 34/1000 | Loss: 0.00002460
Iteration 35/1000 | Loss: 0.00002365
Iteration 36/1000 | Loss: 0.00024187
Iteration 37/1000 | Loss: 0.00005421
Iteration 38/1000 | Loss: 0.00002525
Iteration 39/1000 | Loss: 0.00002831
Iteration 40/1000 | Loss: 0.00003925
Iteration 41/1000 | Loss: 0.00003242
Iteration 42/1000 | Loss: 0.00004539
Iteration 43/1000 | Loss: 0.00002688
Iteration 44/1000 | Loss: 0.00022417
Iteration 45/1000 | Loss: 0.00006049
Iteration 46/1000 | Loss: 0.00006029
Iteration 47/1000 | Loss: 0.00003298
Iteration 48/1000 | Loss: 0.00003095
Iteration 49/1000 | Loss: 0.00002167
Iteration 50/1000 | Loss: 0.00003278
Iteration 51/1000 | Loss: 0.00002199
Iteration 52/1000 | Loss: 0.00002034
Iteration 53/1000 | Loss: 0.00002298
Iteration 54/1000 | Loss: 0.00001997
Iteration 55/1000 | Loss: 0.00002435
Iteration 56/1000 | Loss: 0.00003388
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001969
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00002812
Iteration 66/1000 | Loss: 0.00001957
Iteration 67/1000 | Loss: 0.00001957
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00002215
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001954
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001952
Iteration 77/1000 | Loss: 0.00001951
Iteration 78/1000 | Loss: 0.00001951
Iteration 79/1000 | Loss: 0.00001950
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00002144
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001945
Iteration 91/1000 | Loss: 0.00003546
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00001944
Iteration 96/1000 | Loss: 0.00001944
Iteration 97/1000 | Loss: 0.00001944
Iteration 98/1000 | Loss: 0.00001944
Iteration 99/1000 | Loss: 0.00001944
Iteration 100/1000 | Loss: 0.00001943
Iteration 101/1000 | Loss: 0.00001943
Iteration 102/1000 | Loss: 0.00001943
Iteration 103/1000 | Loss: 0.00001943
Iteration 104/1000 | Loss: 0.00001957
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001942
Iteration 109/1000 | Loss: 0.00001942
Iteration 110/1000 | Loss: 0.00001942
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001941
Iteration 114/1000 | Loss: 0.00001941
Iteration 115/1000 | Loss: 0.00001941
Iteration 116/1000 | Loss: 0.00001941
Iteration 117/1000 | Loss: 0.00001941
Iteration 118/1000 | Loss: 0.00001941
Iteration 119/1000 | Loss: 0.00001939
Iteration 120/1000 | Loss: 0.00001939
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001938
Iteration 123/1000 | Loss: 0.00001936
Iteration 124/1000 | Loss: 0.00001935
Iteration 125/1000 | Loss: 0.00002323
Iteration 126/1000 | Loss: 0.00003502
Iteration 127/1000 | Loss: 0.00001928
Iteration 128/1000 | Loss: 0.00001926
Iteration 129/1000 | Loss: 0.00001926
Iteration 130/1000 | Loss: 0.00001926
Iteration 131/1000 | Loss: 0.00001926
Iteration 132/1000 | Loss: 0.00001926
Iteration 133/1000 | Loss: 0.00001926
Iteration 134/1000 | Loss: 0.00001926
Iteration 135/1000 | Loss: 0.00001925
Iteration 136/1000 | Loss: 0.00001925
Iteration 137/1000 | Loss: 0.00001925
Iteration 138/1000 | Loss: 0.00001925
Iteration 139/1000 | Loss: 0.00001925
Iteration 140/1000 | Loss: 0.00001925
Iteration 141/1000 | Loss: 0.00001925
Iteration 142/1000 | Loss: 0.00001925
Iteration 143/1000 | Loss: 0.00001925
Iteration 144/1000 | Loss: 0.00001925
Iteration 145/1000 | Loss: 0.00001924
Iteration 146/1000 | Loss: 0.00001924
Iteration 147/1000 | Loss: 0.00001924
Iteration 148/1000 | Loss: 0.00001924
Iteration 149/1000 | Loss: 0.00002139
Iteration 150/1000 | Loss: 0.00002139
Iteration 151/1000 | Loss: 0.00001922
Iteration 152/1000 | Loss: 0.00001922
Iteration 153/1000 | Loss: 0.00001922
Iteration 154/1000 | Loss: 0.00001922
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001921
Iteration 159/1000 | Loss: 0.00001921
Iteration 160/1000 | Loss: 0.00001921
Iteration 161/1000 | Loss: 0.00001921
Iteration 162/1000 | Loss: 0.00001921
Iteration 163/1000 | Loss: 0.00001921
Iteration 164/1000 | Loss: 0.00001921
Iteration 165/1000 | Loss: 0.00001921
Iteration 166/1000 | Loss: 0.00001921
Iteration 167/1000 | Loss: 0.00001921
Iteration 168/1000 | Loss: 0.00001921
Iteration 169/1000 | Loss: 0.00001921
Iteration 170/1000 | Loss: 0.00001921
Iteration 171/1000 | Loss: 0.00001921
Iteration 172/1000 | Loss: 0.00001921
Iteration 173/1000 | Loss: 0.00001921
Iteration 174/1000 | Loss: 0.00001920
Iteration 175/1000 | Loss: 0.00001920
Iteration 176/1000 | Loss: 0.00001920
Iteration 177/1000 | Loss: 0.00001920
Iteration 178/1000 | Loss: 0.00001920
Iteration 179/1000 | Loss: 0.00001920
Iteration 180/1000 | Loss: 0.00001920
Iteration 181/1000 | Loss: 0.00001920
Iteration 182/1000 | Loss: 0.00001920
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.9204477212042548e-05, 1.9204477212042548e-05, 1.9204477212042548e-05, 1.9204477212042548e-05, 1.9204477212042548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9204477212042548e-05

Optimization complete. Final v2v error: 3.2832963466644287 mm

Highest mean error: 11.075942993164062 mm for frame 9

Lowest mean error: 2.821824073791504 mm for frame 96

Saving results

Total time: 164.5951156616211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767095
Iteration 2/25 | Loss: 0.00170462
Iteration 3/25 | Loss: 0.00154858
Iteration 4/25 | Loss: 0.00153257
Iteration 5/25 | Loss: 0.00152874
Iteration 6/25 | Loss: 0.00152777
Iteration 7/25 | Loss: 0.00152777
Iteration 8/25 | Loss: 0.00152777
Iteration 9/25 | Loss: 0.00152777
Iteration 10/25 | Loss: 0.00152777
Iteration 11/25 | Loss: 0.00152777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001527768443338573, 0.001527768443338573, 0.001527768443338573, 0.001527768443338573, 0.001527768443338573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001527768443338573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26134050
Iteration 2/25 | Loss: 0.00196241
Iteration 3/25 | Loss: 0.00196241
Iteration 4/25 | Loss: 0.00196241
Iteration 5/25 | Loss: 0.00196241
Iteration 6/25 | Loss: 0.00196241
Iteration 7/25 | Loss: 0.00196241
Iteration 8/25 | Loss: 0.00196241
Iteration 9/25 | Loss: 0.00196241
Iteration 10/25 | Loss: 0.00196241
Iteration 11/25 | Loss: 0.00196241
Iteration 12/25 | Loss: 0.00196241
Iteration 13/25 | Loss: 0.00196241
Iteration 14/25 | Loss: 0.00196241
Iteration 15/25 | Loss: 0.00196241
Iteration 16/25 | Loss: 0.00196241
Iteration 17/25 | Loss: 0.00196241
Iteration 18/25 | Loss: 0.00196241
Iteration 19/25 | Loss: 0.00196241
Iteration 20/25 | Loss: 0.00196241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019624053966253996, 0.0019624053966253996, 0.0019624053966253996, 0.0019624053966253996, 0.0019624053966253996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019624053966253996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196241
Iteration 2/1000 | Loss: 0.00005675
Iteration 3/1000 | Loss: 0.00003782
Iteration 4/1000 | Loss: 0.00003163
Iteration 5/1000 | Loss: 0.00003010
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00002873
Iteration 8/1000 | Loss: 0.00002829
Iteration 9/1000 | Loss: 0.00002792
Iteration 10/1000 | Loss: 0.00002763
Iteration 11/1000 | Loss: 0.00002740
Iteration 12/1000 | Loss: 0.00002726
Iteration 13/1000 | Loss: 0.00002707
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00002693
Iteration 16/1000 | Loss: 0.00002692
Iteration 17/1000 | Loss: 0.00002691
Iteration 18/1000 | Loss: 0.00002690
Iteration 19/1000 | Loss: 0.00002685
Iteration 20/1000 | Loss: 0.00002681
Iteration 21/1000 | Loss: 0.00002679
Iteration 22/1000 | Loss: 0.00002678
Iteration 23/1000 | Loss: 0.00002675
Iteration 24/1000 | Loss: 0.00002675
Iteration 25/1000 | Loss: 0.00002674
Iteration 26/1000 | Loss: 0.00002674
Iteration 27/1000 | Loss: 0.00002672
Iteration 28/1000 | Loss: 0.00002670
Iteration 29/1000 | Loss: 0.00002667
Iteration 30/1000 | Loss: 0.00002661
Iteration 31/1000 | Loss: 0.00002660
Iteration 32/1000 | Loss: 0.00002659
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002658
Iteration 35/1000 | Loss: 0.00002656
Iteration 36/1000 | Loss: 0.00002655
Iteration 37/1000 | Loss: 0.00002655
Iteration 38/1000 | Loss: 0.00002654
Iteration 39/1000 | Loss: 0.00002653
Iteration 40/1000 | Loss: 0.00002652
Iteration 41/1000 | Loss: 0.00002652
Iteration 42/1000 | Loss: 0.00002652
Iteration 43/1000 | Loss: 0.00002649
Iteration 44/1000 | Loss: 0.00002649
Iteration 45/1000 | Loss: 0.00002649
Iteration 46/1000 | Loss: 0.00002649
Iteration 47/1000 | Loss: 0.00002649
Iteration 48/1000 | Loss: 0.00002649
Iteration 49/1000 | Loss: 0.00002649
Iteration 50/1000 | Loss: 0.00002649
Iteration 51/1000 | Loss: 0.00002649
Iteration 52/1000 | Loss: 0.00002649
Iteration 53/1000 | Loss: 0.00002649
Iteration 54/1000 | Loss: 0.00002649
Iteration 55/1000 | Loss: 0.00002649
Iteration 56/1000 | Loss: 0.00002648
Iteration 57/1000 | Loss: 0.00002648
Iteration 58/1000 | Loss: 0.00002648
Iteration 59/1000 | Loss: 0.00002645
Iteration 60/1000 | Loss: 0.00002644
Iteration 61/1000 | Loss: 0.00002644
Iteration 62/1000 | Loss: 0.00002644
Iteration 63/1000 | Loss: 0.00002643
Iteration 64/1000 | Loss: 0.00002642
Iteration 65/1000 | Loss: 0.00002642
Iteration 66/1000 | Loss: 0.00002639
Iteration 67/1000 | Loss: 0.00002638
Iteration 68/1000 | Loss: 0.00002638
Iteration 69/1000 | Loss: 0.00002637
Iteration 70/1000 | Loss: 0.00002637
Iteration 71/1000 | Loss: 0.00002636
Iteration 72/1000 | Loss: 0.00002635
Iteration 73/1000 | Loss: 0.00002635
Iteration 74/1000 | Loss: 0.00002635
Iteration 75/1000 | Loss: 0.00002635
Iteration 76/1000 | Loss: 0.00002634
Iteration 77/1000 | Loss: 0.00002634
Iteration 78/1000 | Loss: 0.00002634
Iteration 79/1000 | Loss: 0.00002634
Iteration 80/1000 | Loss: 0.00002633
Iteration 81/1000 | Loss: 0.00002633
Iteration 82/1000 | Loss: 0.00002633
Iteration 83/1000 | Loss: 0.00002633
Iteration 84/1000 | Loss: 0.00002633
Iteration 85/1000 | Loss: 0.00002633
Iteration 86/1000 | Loss: 0.00002633
Iteration 87/1000 | Loss: 0.00002633
Iteration 88/1000 | Loss: 0.00002633
Iteration 89/1000 | Loss: 0.00002633
Iteration 90/1000 | Loss: 0.00002633
Iteration 91/1000 | Loss: 0.00002632
Iteration 92/1000 | Loss: 0.00002632
Iteration 93/1000 | Loss: 0.00002632
Iteration 94/1000 | Loss: 0.00002632
Iteration 95/1000 | Loss: 0.00002632
Iteration 96/1000 | Loss: 0.00002632
Iteration 97/1000 | Loss: 0.00002632
Iteration 98/1000 | Loss: 0.00002632
Iteration 99/1000 | Loss: 0.00002632
Iteration 100/1000 | Loss: 0.00002632
Iteration 101/1000 | Loss: 0.00002632
Iteration 102/1000 | Loss: 0.00002631
Iteration 103/1000 | Loss: 0.00002631
Iteration 104/1000 | Loss: 0.00002631
Iteration 105/1000 | Loss: 0.00002631
Iteration 106/1000 | Loss: 0.00002630
Iteration 107/1000 | Loss: 0.00002630
Iteration 108/1000 | Loss: 0.00002630
Iteration 109/1000 | Loss: 0.00002630
Iteration 110/1000 | Loss: 0.00002630
Iteration 111/1000 | Loss: 0.00002630
Iteration 112/1000 | Loss: 0.00002630
Iteration 113/1000 | Loss: 0.00002630
Iteration 114/1000 | Loss: 0.00002629
Iteration 115/1000 | Loss: 0.00002629
Iteration 116/1000 | Loss: 0.00002629
Iteration 117/1000 | Loss: 0.00002629
Iteration 118/1000 | Loss: 0.00002629
Iteration 119/1000 | Loss: 0.00002629
Iteration 120/1000 | Loss: 0.00002628
Iteration 121/1000 | Loss: 0.00002628
Iteration 122/1000 | Loss: 0.00002628
Iteration 123/1000 | Loss: 0.00002628
Iteration 124/1000 | Loss: 0.00002628
Iteration 125/1000 | Loss: 0.00002628
Iteration 126/1000 | Loss: 0.00002628
Iteration 127/1000 | Loss: 0.00002628
Iteration 128/1000 | Loss: 0.00002628
Iteration 129/1000 | Loss: 0.00002628
Iteration 130/1000 | Loss: 0.00002628
Iteration 131/1000 | Loss: 0.00002628
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00002628
Iteration 134/1000 | Loss: 0.00002627
Iteration 135/1000 | Loss: 0.00002627
Iteration 136/1000 | Loss: 0.00002627
Iteration 137/1000 | Loss: 0.00002627
Iteration 138/1000 | Loss: 0.00002627
Iteration 139/1000 | Loss: 0.00002627
Iteration 140/1000 | Loss: 0.00002627
Iteration 141/1000 | Loss: 0.00002627
Iteration 142/1000 | Loss: 0.00002627
Iteration 143/1000 | Loss: 0.00002627
Iteration 144/1000 | Loss: 0.00002627
Iteration 145/1000 | Loss: 0.00002627
Iteration 146/1000 | Loss: 0.00002627
Iteration 147/1000 | Loss: 0.00002627
Iteration 148/1000 | Loss: 0.00002627
Iteration 149/1000 | Loss: 0.00002627
Iteration 150/1000 | Loss: 0.00002626
Iteration 151/1000 | Loss: 0.00002626
Iteration 152/1000 | Loss: 0.00002626
Iteration 153/1000 | Loss: 0.00002626
Iteration 154/1000 | Loss: 0.00002626
Iteration 155/1000 | Loss: 0.00002626
Iteration 156/1000 | Loss: 0.00002626
Iteration 157/1000 | Loss: 0.00002626
Iteration 158/1000 | Loss: 0.00002626
Iteration 159/1000 | Loss: 0.00002626
Iteration 160/1000 | Loss: 0.00002626
Iteration 161/1000 | Loss: 0.00002626
Iteration 162/1000 | Loss: 0.00002625
Iteration 163/1000 | Loss: 0.00002625
Iteration 164/1000 | Loss: 0.00002625
Iteration 165/1000 | Loss: 0.00002625
Iteration 166/1000 | Loss: 0.00002625
Iteration 167/1000 | Loss: 0.00002625
Iteration 168/1000 | Loss: 0.00002625
Iteration 169/1000 | Loss: 0.00002625
Iteration 170/1000 | Loss: 0.00002625
Iteration 171/1000 | Loss: 0.00002625
Iteration 172/1000 | Loss: 0.00002625
Iteration 173/1000 | Loss: 0.00002625
Iteration 174/1000 | Loss: 0.00002624
Iteration 175/1000 | Loss: 0.00002624
Iteration 176/1000 | Loss: 0.00002624
Iteration 177/1000 | Loss: 0.00002624
Iteration 178/1000 | Loss: 0.00002624
Iteration 179/1000 | Loss: 0.00002624
Iteration 180/1000 | Loss: 0.00002624
Iteration 181/1000 | Loss: 0.00002624
Iteration 182/1000 | Loss: 0.00002624
Iteration 183/1000 | Loss: 0.00002624
Iteration 184/1000 | Loss: 0.00002624
Iteration 185/1000 | Loss: 0.00002624
Iteration 186/1000 | Loss: 0.00002624
Iteration 187/1000 | Loss: 0.00002624
Iteration 188/1000 | Loss: 0.00002624
Iteration 189/1000 | Loss: 0.00002624
Iteration 190/1000 | Loss: 0.00002624
Iteration 191/1000 | Loss: 0.00002624
Iteration 192/1000 | Loss: 0.00002624
Iteration 193/1000 | Loss: 0.00002624
Iteration 194/1000 | Loss: 0.00002624
Iteration 195/1000 | Loss: 0.00002624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.6235040422761813e-05, 2.6235040422761813e-05, 2.6235040422761813e-05, 2.6235040422761813e-05, 2.6235040422761813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6235040422761813e-05

Optimization complete. Final v2v error: 4.186789512634277 mm

Highest mean error: 4.568192958831787 mm for frame 76

Lowest mean error: 3.313645839691162 mm for frame 0

Saving results

Total time: 43.99964261054993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474157
Iteration 2/25 | Loss: 0.00144376
Iteration 3/25 | Loss: 0.00136491
Iteration 4/25 | Loss: 0.00135450
Iteration 5/25 | Loss: 0.00135187
Iteration 6/25 | Loss: 0.00135174
Iteration 7/25 | Loss: 0.00135174
Iteration 8/25 | Loss: 0.00135174
Iteration 9/25 | Loss: 0.00135174
Iteration 10/25 | Loss: 0.00135174
Iteration 11/25 | Loss: 0.00135174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013517362531274557, 0.0013517362531274557, 0.0013517362531274557, 0.0013517362531274557, 0.0013517362531274557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013517362531274557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.99797583
Iteration 2/25 | Loss: 0.00204978
Iteration 3/25 | Loss: 0.00204977
Iteration 4/25 | Loss: 0.00204977
Iteration 5/25 | Loss: 0.00204977
Iteration 6/25 | Loss: 0.00204977
Iteration 7/25 | Loss: 0.00204977
Iteration 8/25 | Loss: 0.00204977
Iteration 9/25 | Loss: 0.00204977
Iteration 10/25 | Loss: 0.00204977
Iteration 11/25 | Loss: 0.00204977
Iteration 12/25 | Loss: 0.00204977
Iteration 13/25 | Loss: 0.00204977
Iteration 14/25 | Loss: 0.00204977
Iteration 15/25 | Loss: 0.00204977
Iteration 16/25 | Loss: 0.00204977
Iteration 17/25 | Loss: 0.00204977
Iteration 18/25 | Loss: 0.00204977
Iteration 19/25 | Loss: 0.00204977
Iteration 20/25 | Loss: 0.00204977
Iteration 21/25 | Loss: 0.00204977
Iteration 22/25 | Loss: 0.00204977
Iteration 23/25 | Loss: 0.00204977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002049769042059779, 0.002049769042059779, 0.002049769042059779, 0.002049769042059779, 0.002049769042059779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002049769042059779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204977
Iteration 2/1000 | Loss: 0.00002607
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001678
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001355
Iteration 9/1000 | Loss: 0.00001339
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001238
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001229
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001192
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001165
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001163
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001161
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001159
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001158
Iteration 59/1000 | Loss: 0.00001158
Iteration 60/1000 | Loss: 0.00001158
Iteration 61/1000 | Loss: 0.00001157
Iteration 62/1000 | Loss: 0.00001157
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001156
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001142
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001142
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001142
Iteration 151/1000 | Loss: 0.00001142
Iteration 152/1000 | Loss: 0.00001142
Iteration 153/1000 | Loss: 0.00001142
Iteration 154/1000 | Loss: 0.00001142
Iteration 155/1000 | Loss: 0.00001142
Iteration 156/1000 | Loss: 0.00001142
Iteration 157/1000 | Loss: 0.00001142
Iteration 158/1000 | Loss: 0.00001142
Iteration 159/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.1422362149460241e-05, 1.1422362149460241e-05, 1.1422362149460241e-05, 1.1422362149460241e-05, 1.1422362149460241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1422362149460241e-05

Optimization complete. Final v2v error: 2.936461925506592 mm

Highest mean error: 3.2946174144744873 mm for frame 75

Lowest mean error: 2.7520201206207275 mm for frame 30

Saving results

Total time: 41.08632040023804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563912
Iteration 2/25 | Loss: 0.00151229
Iteration 3/25 | Loss: 0.00142204
Iteration 4/25 | Loss: 0.00140609
Iteration 5/25 | Loss: 0.00140106
Iteration 6/25 | Loss: 0.00139968
Iteration 7/25 | Loss: 0.00139948
Iteration 8/25 | Loss: 0.00139948
Iteration 9/25 | Loss: 0.00139948
Iteration 10/25 | Loss: 0.00139948
Iteration 11/25 | Loss: 0.00139948
Iteration 12/25 | Loss: 0.00139948
Iteration 13/25 | Loss: 0.00139948
Iteration 14/25 | Loss: 0.00139948
Iteration 15/25 | Loss: 0.00139948
Iteration 16/25 | Loss: 0.00139948
Iteration 17/25 | Loss: 0.00139948
Iteration 18/25 | Loss: 0.00139948
Iteration 19/25 | Loss: 0.00139948
Iteration 20/25 | Loss: 0.00139948
Iteration 21/25 | Loss: 0.00139948
Iteration 22/25 | Loss: 0.00139948
Iteration 23/25 | Loss: 0.00139948
Iteration 24/25 | Loss: 0.00139948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001399481319822371, 0.001399481319822371, 0.001399481319822371, 0.001399481319822371, 0.001399481319822371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001399481319822371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.68920231
Iteration 2/25 | Loss: 0.00244435
Iteration 3/25 | Loss: 0.00244435
Iteration 4/25 | Loss: 0.00244435
Iteration 5/25 | Loss: 0.00244435
Iteration 6/25 | Loss: 0.00244435
Iteration 7/25 | Loss: 0.00244435
Iteration 8/25 | Loss: 0.00244435
Iteration 9/25 | Loss: 0.00244435
Iteration 10/25 | Loss: 0.00244435
Iteration 11/25 | Loss: 0.00244435
Iteration 12/25 | Loss: 0.00244435
Iteration 13/25 | Loss: 0.00244435
Iteration 14/25 | Loss: 0.00244435
Iteration 15/25 | Loss: 0.00244435
Iteration 16/25 | Loss: 0.00244435
Iteration 17/25 | Loss: 0.00244435
Iteration 18/25 | Loss: 0.00244435
Iteration 19/25 | Loss: 0.00244435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0024443489965051413, 0.0024443489965051413, 0.0024443489965051413, 0.0024443489965051413, 0.0024443489965051413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024443489965051413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244435
Iteration 2/1000 | Loss: 0.00003421
Iteration 3/1000 | Loss: 0.00002681
Iteration 4/1000 | Loss: 0.00002298
Iteration 5/1000 | Loss: 0.00002182
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00001974
Iteration 10/1000 | Loss: 0.00001950
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001883
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001868
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001847
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001843
Iteration 23/1000 | Loss: 0.00001842
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001838
Iteration 26/1000 | Loss: 0.00001836
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001832
Iteration 32/1000 | Loss: 0.00001832
Iteration 33/1000 | Loss: 0.00001830
Iteration 34/1000 | Loss: 0.00001830
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001830
Iteration 38/1000 | Loss: 0.00001830
Iteration 39/1000 | Loss: 0.00001830
Iteration 40/1000 | Loss: 0.00001830
Iteration 41/1000 | Loss: 0.00001830
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001830
Iteration 44/1000 | Loss: 0.00001830
Iteration 45/1000 | Loss: 0.00001830
Iteration 46/1000 | Loss: 0.00001830
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001825
Iteration 57/1000 | Loss: 0.00001824
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001823
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001821
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001817
Iteration 68/1000 | Loss: 0.00001816
Iteration 69/1000 | Loss: 0.00001816
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001814
Iteration 76/1000 | Loss: 0.00001814
Iteration 77/1000 | Loss: 0.00001814
Iteration 78/1000 | Loss: 0.00001813
Iteration 79/1000 | Loss: 0.00001813
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001812
Iteration 89/1000 | Loss: 0.00001812
Iteration 90/1000 | Loss: 0.00001812
Iteration 91/1000 | Loss: 0.00001812
Iteration 92/1000 | Loss: 0.00001812
Iteration 93/1000 | Loss: 0.00001812
Iteration 94/1000 | Loss: 0.00001811
Iteration 95/1000 | Loss: 0.00001811
Iteration 96/1000 | Loss: 0.00001811
Iteration 97/1000 | Loss: 0.00001810
Iteration 98/1000 | Loss: 0.00001810
Iteration 99/1000 | Loss: 0.00001810
Iteration 100/1000 | Loss: 0.00001809
Iteration 101/1000 | Loss: 0.00001809
Iteration 102/1000 | Loss: 0.00001809
Iteration 103/1000 | Loss: 0.00001808
Iteration 104/1000 | Loss: 0.00001808
Iteration 105/1000 | Loss: 0.00001807
Iteration 106/1000 | Loss: 0.00001807
Iteration 107/1000 | Loss: 0.00001807
Iteration 108/1000 | Loss: 0.00001807
Iteration 109/1000 | Loss: 0.00001807
Iteration 110/1000 | Loss: 0.00001807
Iteration 111/1000 | Loss: 0.00001807
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001806
Iteration 115/1000 | Loss: 0.00001806
Iteration 116/1000 | Loss: 0.00001806
Iteration 117/1000 | Loss: 0.00001806
Iteration 118/1000 | Loss: 0.00001806
Iteration 119/1000 | Loss: 0.00001806
Iteration 120/1000 | Loss: 0.00001806
Iteration 121/1000 | Loss: 0.00001806
Iteration 122/1000 | Loss: 0.00001806
Iteration 123/1000 | Loss: 0.00001806
Iteration 124/1000 | Loss: 0.00001805
Iteration 125/1000 | Loss: 0.00001805
Iteration 126/1000 | Loss: 0.00001805
Iteration 127/1000 | Loss: 0.00001805
Iteration 128/1000 | Loss: 0.00001805
Iteration 129/1000 | Loss: 0.00001805
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Iteration 133/1000 | Loss: 0.00001804
Iteration 134/1000 | Loss: 0.00001804
Iteration 135/1000 | Loss: 0.00001804
Iteration 136/1000 | Loss: 0.00001804
Iteration 137/1000 | Loss: 0.00001804
Iteration 138/1000 | Loss: 0.00001804
Iteration 139/1000 | Loss: 0.00001804
Iteration 140/1000 | Loss: 0.00001804
Iteration 141/1000 | Loss: 0.00001803
Iteration 142/1000 | Loss: 0.00001803
Iteration 143/1000 | Loss: 0.00001803
Iteration 144/1000 | Loss: 0.00001802
Iteration 145/1000 | Loss: 0.00001802
Iteration 146/1000 | Loss: 0.00001802
Iteration 147/1000 | Loss: 0.00001802
Iteration 148/1000 | Loss: 0.00001802
Iteration 149/1000 | Loss: 0.00001802
Iteration 150/1000 | Loss: 0.00001802
Iteration 151/1000 | Loss: 0.00001802
Iteration 152/1000 | Loss: 0.00001802
Iteration 153/1000 | Loss: 0.00001802
Iteration 154/1000 | Loss: 0.00001802
Iteration 155/1000 | Loss: 0.00001802
Iteration 156/1000 | Loss: 0.00001802
Iteration 157/1000 | Loss: 0.00001802
Iteration 158/1000 | Loss: 0.00001802
Iteration 159/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.80194456333993e-05, 1.80194456333993e-05, 1.80194456333993e-05, 1.80194456333993e-05, 1.80194456333993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.80194456333993e-05

Optimization complete. Final v2v error: 3.573237180709839 mm

Highest mean error: 5.032404899597168 mm for frame 97

Lowest mean error: 2.967129707336426 mm for frame 146

Saving results

Total time: 41.832555055618286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388771
Iteration 2/25 | Loss: 0.00140861
Iteration 3/25 | Loss: 0.00134543
Iteration 4/25 | Loss: 0.00133590
Iteration 5/25 | Loss: 0.00133244
Iteration 6/25 | Loss: 0.00133213
Iteration 7/25 | Loss: 0.00133213
Iteration 8/25 | Loss: 0.00133213
Iteration 9/25 | Loss: 0.00133213
Iteration 10/25 | Loss: 0.00133213
Iteration 11/25 | Loss: 0.00133213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001332126441411674, 0.001332126441411674, 0.001332126441411674, 0.001332126441411674, 0.001332126441411674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001332126441411674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33473289
Iteration 2/25 | Loss: 0.00232026
Iteration 3/25 | Loss: 0.00232026
Iteration 4/25 | Loss: 0.00232026
Iteration 5/25 | Loss: 0.00232025
Iteration 6/25 | Loss: 0.00232025
Iteration 7/25 | Loss: 0.00232025
Iteration 8/25 | Loss: 0.00232025
Iteration 9/25 | Loss: 0.00232025
Iteration 10/25 | Loss: 0.00232025
Iteration 11/25 | Loss: 0.00232025
Iteration 12/25 | Loss: 0.00232025
Iteration 13/25 | Loss: 0.00232025
Iteration 14/25 | Loss: 0.00232025
Iteration 15/25 | Loss: 0.00232025
Iteration 16/25 | Loss: 0.00232025
Iteration 17/25 | Loss: 0.00232025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023202523589134216, 0.0023202523589134216, 0.0023202523589134216, 0.0023202523589134216, 0.0023202523589134216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023202523589134216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232025
Iteration 2/1000 | Loss: 0.00001981
Iteration 3/1000 | Loss: 0.00001431
Iteration 4/1000 | Loss: 0.00001274
Iteration 5/1000 | Loss: 0.00001200
Iteration 6/1000 | Loss: 0.00001138
Iteration 7/1000 | Loss: 0.00001106
Iteration 8/1000 | Loss: 0.00001071
Iteration 9/1000 | Loss: 0.00001034
Iteration 10/1000 | Loss: 0.00001018
Iteration 11/1000 | Loss: 0.00001010
Iteration 12/1000 | Loss: 0.00001003
Iteration 13/1000 | Loss: 0.00000999
Iteration 14/1000 | Loss: 0.00000993
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000990
Iteration 17/1000 | Loss: 0.00000984
Iteration 18/1000 | Loss: 0.00000973
Iteration 19/1000 | Loss: 0.00000972
Iteration 20/1000 | Loss: 0.00000971
Iteration 21/1000 | Loss: 0.00000966
Iteration 22/1000 | Loss: 0.00000952
Iteration 23/1000 | Loss: 0.00000950
Iteration 24/1000 | Loss: 0.00000947
Iteration 25/1000 | Loss: 0.00000947
Iteration 26/1000 | Loss: 0.00000946
Iteration 27/1000 | Loss: 0.00000944
Iteration 28/1000 | Loss: 0.00000943
Iteration 29/1000 | Loss: 0.00000940
Iteration 30/1000 | Loss: 0.00000939
Iteration 31/1000 | Loss: 0.00000937
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000936
Iteration 34/1000 | Loss: 0.00000935
Iteration 35/1000 | Loss: 0.00000927
Iteration 36/1000 | Loss: 0.00000926
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000923
Iteration 41/1000 | Loss: 0.00000923
Iteration 42/1000 | Loss: 0.00000923
Iteration 43/1000 | Loss: 0.00000922
Iteration 44/1000 | Loss: 0.00000922
Iteration 45/1000 | Loss: 0.00000922
Iteration 46/1000 | Loss: 0.00000921
Iteration 47/1000 | Loss: 0.00000921
Iteration 48/1000 | Loss: 0.00000921
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000920
Iteration 51/1000 | Loss: 0.00000920
Iteration 52/1000 | Loss: 0.00000920
Iteration 53/1000 | Loss: 0.00000919
Iteration 54/1000 | Loss: 0.00000919
Iteration 55/1000 | Loss: 0.00000919
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000918
Iteration 60/1000 | Loss: 0.00000918
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000918
Iteration 63/1000 | Loss: 0.00000918
Iteration 64/1000 | Loss: 0.00000918
Iteration 65/1000 | Loss: 0.00000918
Iteration 66/1000 | Loss: 0.00000918
Iteration 67/1000 | Loss: 0.00000918
Iteration 68/1000 | Loss: 0.00000918
Iteration 69/1000 | Loss: 0.00000918
Iteration 70/1000 | Loss: 0.00000918
Iteration 71/1000 | Loss: 0.00000918
Iteration 72/1000 | Loss: 0.00000918
Iteration 73/1000 | Loss: 0.00000918
Iteration 74/1000 | Loss: 0.00000918
Iteration 75/1000 | Loss: 0.00000918
Iteration 76/1000 | Loss: 0.00000918
Iteration 77/1000 | Loss: 0.00000918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [9.18335990718333e-06, 9.18335990718333e-06, 9.18335990718333e-06, 9.18335990718333e-06, 9.18335990718333e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.18335990718333e-06

Optimization complete. Final v2v error: 2.6686882972717285 mm

Highest mean error: 2.755284547805786 mm for frame 134

Lowest mean error: 2.622361183166504 mm for frame 5

Saving results

Total time: 34.02037024497986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966122
Iteration 2/25 | Loss: 0.00966122
Iteration 3/25 | Loss: 0.00966122
Iteration 4/25 | Loss: 0.00966121
Iteration 5/25 | Loss: 0.00966121
Iteration 6/25 | Loss: 0.00966121
Iteration 7/25 | Loss: 0.00966121
Iteration 8/25 | Loss: 0.00966121
Iteration 9/25 | Loss: 0.00966121
Iteration 10/25 | Loss: 0.00966121
Iteration 11/25 | Loss: 0.00966121
Iteration 12/25 | Loss: 0.00966121
Iteration 13/25 | Loss: 0.00966120
Iteration 14/25 | Loss: 0.00966120
Iteration 15/25 | Loss: 0.00966120
Iteration 16/25 | Loss: 0.00966120
Iteration 17/25 | Loss: 0.00966120
Iteration 18/25 | Loss: 0.00966120
Iteration 19/25 | Loss: 0.00966120
Iteration 20/25 | Loss: 0.00966120
Iteration 21/25 | Loss: 0.00966119
Iteration 22/25 | Loss: 0.00966119
Iteration 23/25 | Loss: 0.00966119
Iteration 24/25 | Loss: 0.00966119
Iteration 25/25 | Loss: 0.00966119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37501514
Iteration 2/25 | Loss: 0.16751541
Iteration 3/25 | Loss: 0.16751418
Iteration 4/25 | Loss: 0.16751418
Iteration 5/25 | Loss: 0.16751418
Iteration 6/25 | Loss: 0.16751413
Iteration 7/25 | Loss: 0.16751413
Iteration 8/25 | Loss: 0.16751413
Iteration 9/25 | Loss: 0.16751413
Iteration 10/25 | Loss: 0.16751413
Iteration 11/25 | Loss: 0.16751413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.1675141304731369, 0.1675141304731369, 0.1675141304731369, 0.1675141304731369, 0.1675141304731369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1675141304731369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16751413
Iteration 2/1000 | Loss: 0.00278601
Iteration 3/1000 | Loss: 0.00085839
Iteration 4/1000 | Loss: 0.00049942
Iteration 5/1000 | Loss: 0.00060179
Iteration 6/1000 | Loss: 0.00047639
Iteration 7/1000 | Loss: 0.00013181
Iteration 8/1000 | Loss: 0.00018620
Iteration 9/1000 | Loss: 0.00009978
Iteration 10/1000 | Loss: 0.00009357
Iteration 11/1000 | Loss: 0.00004969
Iteration 12/1000 | Loss: 0.00004713
Iteration 13/1000 | Loss: 0.00017261
Iteration 14/1000 | Loss: 0.00012991
Iteration 15/1000 | Loss: 0.00009389
Iteration 16/1000 | Loss: 0.00004243
Iteration 17/1000 | Loss: 0.00008797
Iteration 18/1000 | Loss: 0.00002616
Iteration 19/1000 | Loss: 0.00009949
Iteration 20/1000 | Loss: 0.00010085
Iteration 21/1000 | Loss: 0.00006595
Iteration 22/1000 | Loss: 0.00004800
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00003993
Iteration 25/1000 | Loss: 0.00002954
Iteration 26/1000 | Loss: 0.00002596
Iteration 27/1000 | Loss: 0.00002286
Iteration 28/1000 | Loss: 0.00004145
Iteration 29/1000 | Loss: 0.00009233
Iteration 30/1000 | Loss: 0.00002330
Iteration 31/1000 | Loss: 0.00004305
Iteration 32/1000 | Loss: 0.00005046
Iteration 33/1000 | Loss: 0.00025417
Iteration 34/1000 | Loss: 0.00003710
Iteration 35/1000 | Loss: 0.00002689
Iteration 36/1000 | Loss: 0.00003783
Iteration 37/1000 | Loss: 0.00002450
Iteration 38/1000 | Loss: 0.00002562
Iteration 39/1000 | Loss: 0.00011697
Iteration 40/1000 | Loss: 0.00003155
Iteration 41/1000 | Loss: 0.00002453
Iteration 42/1000 | Loss: 0.00016857
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00002939
Iteration 45/1000 | Loss: 0.00001970
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001864
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001864
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.863060606410727e-05, 1.863060606410727e-05, 1.863060606410727e-05, 1.863060606410727e-05, 1.863060606410727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.863060606410727e-05

Optimization complete. Final v2v error: 3.7199320793151855 mm

Highest mean error: 4.034168243408203 mm for frame 229

Lowest mean error: 3.6699929237365723 mm for frame 120

Saving results

Total time: 79.48397731781006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752097
Iteration 2/25 | Loss: 0.00173809
Iteration 3/25 | Loss: 0.00146005
Iteration 4/25 | Loss: 0.00139088
Iteration 5/25 | Loss: 0.00138942
Iteration 6/25 | Loss: 0.00138315
Iteration 7/25 | Loss: 0.00136956
Iteration 8/25 | Loss: 0.00136060
Iteration 9/25 | Loss: 0.00135814
Iteration 10/25 | Loss: 0.00135743
Iteration 11/25 | Loss: 0.00135713
Iteration 12/25 | Loss: 0.00135705
Iteration 13/25 | Loss: 0.00135705
Iteration 14/25 | Loss: 0.00135704
Iteration 15/25 | Loss: 0.00135704
Iteration 16/25 | Loss: 0.00135704
Iteration 17/25 | Loss: 0.00135704
Iteration 18/25 | Loss: 0.00135704
Iteration 19/25 | Loss: 0.00135703
Iteration 20/25 | Loss: 0.00135703
Iteration 21/25 | Loss: 0.00135703
Iteration 22/25 | Loss: 0.00135703
Iteration 23/25 | Loss: 0.00135703
Iteration 24/25 | Loss: 0.00135703
Iteration 25/25 | Loss: 0.00135703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73164690
Iteration 2/25 | Loss: 0.00209021
Iteration 3/25 | Loss: 0.00209021
Iteration 4/25 | Loss: 0.00209021
Iteration 5/25 | Loss: 0.00209021
Iteration 6/25 | Loss: 0.00209021
Iteration 7/25 | Loss: 0.00209021
Iteration 8/25 | Loss: 0.00209021
Iteration 9/25 | Loss: 0.00209021
Iteration 10/25 | Loss: 0.00209021
Iteration 11/25 | Loss: 0.00209021
Iteration 12/25 | Loss: 0.00209021
Iteration 13/25 | Loss: 0.00209021
Iteration 14/25 | Loss: 0.00209021
Iteration 15/25 | Loss: 0.00209021
Iteration 16/25 | Loss: 0.00209021
Iteration 17/25 | Loss: 0.00209020
Iteration 18/25 | Loss: 0.00209020
Iteration 19/25 | Loss: 0.00209020
Iteration 20/25 | Loss: 0.00209020
Iteration 21/25 | Loss: 0.00209020
Iteration 22/25 | Loss: 0.00209020
Iteration 23/25 | Loss: 0.00209020
Iteration 24/25 | Loss: 0.00209020
Iteration 25/25 | Loss: 0.00209020

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209020
Iteration 2/1000 | Loss: 0.00002526
Iteration 3/1000 | Loss: 0.00002113
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001495
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001425
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001406
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001389
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001383
Iteration 49/1000 | Loss: 0.00001383
Iteration 50/1000 | Loss: 0.00001383
Iteration 51/1000 | Loss: 0.00001382
Iteration 52/1000 | Loss: 0.00001382
Iteration 53/1000 | Loss: 0.00001382
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001381
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001373
Iteration 80/1000 | Loss: 0.00001373
Iteration 81/1000 | Loss: 0.00001372
Iteration 82/1000 | Loss: 0.00001372
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001367
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001366
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001365
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001364
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001363
Iteration 117/1000 | Loss: 0.00001363
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001363
Iteration 120/1000 | Loss: 0.00001362
Iteration 121/1000 | Loss: 0.00001362
Iteration 122/1000 | Loss: 0.00001362
Iteration 123/1000 | Loss: 0.00001362
Iteration 124/1000 | Loss: 0.00001362
Iteration 125/1000 | Loss: 0.00001362
Iteration 126/1000 | Loss: 0.00001362
Iteration 127/1000 | Loss: 0.00001362
Iteration 128/1000 | Loss: 0.00001362
Iteration 129/1000 | Loss: 0.00001362
Iteration 130/1000 | Loss: 0.00001362
Iteration 131/1000 | Loss: 0.00001362
Iteration 132/1000 | Loss: 0.00001362
Iteration 133/1000 | Loss: 0.00001362
Iteration 134/1000 | Loss: 0.00001362
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001362
Iteration 139/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.3618478988064453e-05, 1.3618478988064453e-05, 1.3618478988064453e-05, 1.3618478988064453e-05, 1.3618478988064453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3618478988064453e-05

Optimization complete. Final v2v error: 3.2205348014831543 mm

Highest mean error: 3.504549980163574 mm for frame 87

Lowest mean error: 2.9308366775512695 mm for frame 192

Saving results

Total time: 54.60829830169678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00577162
Iteration 2/25 | Loss: 0.00160854
Iteration 3/25 | Loss: 0.00143705
Iteration 4/25 | Loss: 0.00141709
Iteration 5/25 | Loss: 0.00141489
Iteration 6/25 | Loss: 0.00141454
Iteration 7/25 | Loss: 0.00141454
Iteration 8/25 | Loss: 0.00141454
Iteration 9/25 | Loss: 0.00141454
Iteration 10/25 | Loss: 0.00141454
Iteration 11/25 | Loss: 0.00141454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014145368477329612, 0.0014145368477329612, 0.0014145368477329612, 0.0014145368477329612, 0.0014145368477329612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014145368477329612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20380402
Iteration 2/25 | Loss: 0.00176663
Iteration 3/25 | Loss: 0.00176659
Iteration 4/25 | Loss: 0.00176659
Iteration 5/25 | Loss: 0.00176659
Iteration 6/25 | Loss: 0.00176659
Iteration 7/25 | Loss: 0.00176659
Iteration 8/25 | Loss: 0.00176659
Iteration 9/25 | Loss: 0.00176659
Iteration 10/25 | Loss: 0.00176659
Iteration 11/25 | Loss: 0.00176659
Iteration 12/25 | Loss: 0.00176659
Iteration 13/25 | Loss: 0.00176659
Iteration 14/25 | Loss: 0.00176659
Iteration 15/25 | Loss: 0.00176659
Iteration 16/25 | Loss: 0.00176659
Iteration 17/25 | Loss: 0.00176659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001766585628502071, 0.001766585628502071, 0.001766585628502071, 0.001766585628502071, 0.001766585628502071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001766585628502071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176659
Iteration 2/1000 | Loss: 0.00004093
Iteration 3/1000 | Loss: 0.00003121
Iteration 4/1000 | Loss: 0.00002459
Iteration 5/1000 | Loss: 0.00002258
Iteration 6/1000 | Loss: 0.00002125
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00001893
Iteration 11/1000 | Loss: 0.00001860
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001806
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001753
Iteration 16/1000 | Loss: 0.00001731
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001710
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001684
Iteration 28/1000 | Loss: 0.00001684
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001678
Iteration 36/1000 | Loss: 0.00001676
Iteration 37/1000 | Loss: 0.00001676
Iteration 38/1000 | Loss: 0.00001675
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001671
Iteration 47/1000 | Loss: 0.00001671
Iteration 48/1000 | Loss: 0.00001671
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001671
Iteration 51/1000 | Loss: 0.00001671
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001671
Iteration 57/1000 | Loss: 0.00001671
Iteration 58/1000 | Loss: 0.00001670
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001666
Iteration 81/1000 | Loss: 0.00001666
Iteration 82/1000 | Loss: 0.00001666
Iteration 83/1000 | Loss: 0.00001666
Iteration 84/1000 | Loss: 0.00001665
Iteration 85/1000 | Loss: 0.00001665
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001664
Iteration 92/1000 | Loss: 0.00001664
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001662
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001662
Iteration 110/1000 | Loss: 0.00001662
Iteration 111/1000 | Loss: 0.00001662
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001662
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001662
Iteration 117/1000 | Loss: 0.00001661
Iteration 118/1000 | Loss: 0.00001661
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001661
Iteration 126/1000 | Loss: 0.00001661
Iteration 127/1000 | Loss: 0.00001661
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001659
Iteration 136/1000 | Loss: 0.00001659
Iteration 137/1000 | Loss: 0.00001659
Iteration 138/1000 | Loss: 0.00001659
Iteration 139/1000 | Loss: 0.00001659
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001659
Iteration 143/1000 | Loss: 0.00001659
Iteration 144/1000 | Loss: 0.00001659
Iteration 145/1000 | Loss: 0.00001658
Iteration 146/1000 | Loss: 0.00001658
Iteration 147/1000 | Loss: 0.00001658
Iteration 148/1000 | Loss: 0.00001658
Iteration 149/1000 | Loss: 0.00001658
Iteration 150/1000 | Loss: 0.00001658
Iteration 151/1000 | Loss: 0.00001658
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001657
Iteration 154/1000 | Loss: 0.00001657
Iteration 155/1000 | Loss: 0.00001657
Iteration 156/1000 | Loss: 0.00001657
Iteration 157/1000 | Loss: 0.00001656
Iteration 158/1000 | Loss: 0.00001656
Iteration 159/1000 | Loss: 0.00001656
Iteration 160/1000 | Loss: 0.00001656
Iteration 161/1000 | Loss: 0.00001656
Iteration 162/1000 | Loss: 0.00001656
Iteration 163/1000 | Loss: 0.00001656
Iteration 164/1000 | Loss: 0.00001656
Iteration 165/1000 | Loss: 0.00001656
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001655
Iteration 168/1000 | Loss: 0.00001655
Iteration 169/1000 | Loss: 0.00001655
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001654
Iteration 184/1000 | Loss: 0.00001654
Iteration 185/1000 | Loss: 0.00001654
Iteration 186/1000 | Loss: 0.00001654
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Iteration 189/1000 | Loss: 0.00001654
Iteration 190/1000 | Loss: 0.00001654
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001654
Iteration 193/1000 | Loss: 0.00001654
Iteration 194/1000 | Loss: 0.00001654
Iteration 195/1000 | Loss: 0.00001654
Iteration 196/1000 | Loss: 0.00001654
Iteration 197/1000 | Loss: 0.00001654
Iteration 198/1000 | Loss: 0.00001654
Iteration 199/1000 | Loss: 0.00001654
Iteration 200/1000 | Loss: 0.00001654
Iteration 201/1000 | Loss: 0.00001654
Iteration 202/1000 | Loss: 0.00001654
Iteration 203/1000 | Loss: 0.00001654
Iteration 204/1000 | Loss: 0.00001654
Iteration 205/1000 | Loss: 0.00001654
Iteration 206/1000 | Loss: 0.00001654
Iteration 207/1000 | Loss: 0.00001654
Iteration 208/1000 | Loss: 0.00001654
Iteration 209/1000 | Loss: 0.00001654
Iteration 210/1000 | Loss: 0.00001654
Iteration 211/1000 | Loss: 0.00001654
Iteration 212/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.6541524018975906e-05, 1.6541524018975906e-05, 1.6541524018975906e-05, 1.6541524018975906e-05, 1.6541524018975906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6541524018975906e-05

Optimization complete. Final v2v error: 3.4581031799316406 mm

Highest mean error: 3.7497425079345703 mm for frame 66

Lowest mean error: 3.1065382957458496 mm for frame 16

Saving results

Total time: 45.594045639038086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418744
Iteration 2/25 | Loss: 0.00141036
Iteration 3/25 | Loss: 0.00135753
Iteration 4/25 | Loss: 0.00135002
Iteration 5/25 | Loss: 0.00134828
Iteration 6/25 | Loss: 0.00134828
Iteration 7/25 | Loss: 0.00134828
Iteration 8/25 | Loss: 0.00134828
Iteration 9/25 | Loss: 0.00134828
Iteration 10/25 | Loss: 0.00134821
Iteration 11/25 | Loss: 0.00134821
Iteration 12/25 | Loss: 0.00134821
Iteration 13/25 | Loss: 0.00134821
Iteration 14/25 | Loss: 0.00134821
Iteration 15/25 | Loss: 0.00134821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013482067734003067, 0.0013482067734003067, 0.0013482067734003067, 0.0013482067734003067, 0.0013482067734003067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013482067734003067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.05779839
Iteration 2/25 | Loss: 0.00198277
Iteration 3/25 | Loss: 0.00198277
Iteration 4/25 | Loss: 0.00198277
Iteration 5/25 | Loss: 0.00198277
Iteration 6/25 | Loss: 0.00198277
Iteration 7/25 | Loss: 0.00198277
Iteration 8/25 | Loss: 0.00198277
Iteration 9/25 | Loss: 0.00198277
Iteration 10/25 | Loss: 0.00198277
Iteration 11/25 | Loss: 0.00198277
Iteration 12/25 | Loss: 0.00198277
Iteration 13/25 | Loss: 0.00198277
Iteration 14/25 | Loss: 0.00198277
Iteration 15/25 | Loss: 0.00198277
Iteration 16/25 | Loss: 0.00198277
Iteration 17/25 | Loss: 0.00198277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001982768764719367, 0.001982768764719367, 0.001982768764719367, 0.001982768764719367, 0.001982768764719367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001982768764719367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198277
Iteration 2/1000 | Loss: 0.00002237
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001762
Iteration 5/1000 | Loss: 0.00001651
Iteration 6/1000 | Loss: 0.00001574
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001460
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001346
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001289
Iteration 30/1000 | Loss: 0.00001288
Iteration 31/1000 | Loss: 0.00001288
Iteration 32/1000 | Loss: 0.00001287
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001284
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001281
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001276
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001274
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001270
Iteration 76/1000 | Loss: 0.00001270
Iteration 77/1000 | Loss: 0.00001270
Iteration 78/1000 | Loss: 0.00001270
Iteration 79/1000 | Loss: 0.00001269
Iteration 80/1000 | Loss: 0.00001269
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001269
Iteration 83/1000 | Loss: 0.00001268
Iteration 84/1000 | Loss: 0.00001268
Iteration 85/1000 | Loss: 0.00001268
Iteration 86/1000 | Loss: 0.00001268
Iteration 87/1000 | Loss: 0.00001267
Iteration 88/1000 | Loss: 0.00001267
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001265
Iteration 114/1000 | Loss: 0.00001265
Iteration 115/1000 | Loss: 0.00001265
Iteration 116/1000 | Loss: 0.00001265
Iteration 117/1000 | Loss: 0.00001265
Iteration 118/1000 | Loss: 0.00001265
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001264
Iteration 123/1000 | Loss: 0.00001264
Iteration 124/1000 | Loss: 0.00001264
Iteration 125/1000 | Loss: 0.00001264
Iteration 126/1000 | Loss: 0.00001264
Iteration 127/1000 | Loss: 0.00001264
Iteration 128/1000 | Loss: 0.00001264
Iteration 129/1000 | Loss: 0.00001264
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001264
Iteration 139/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.2635919119929895e-05, 1.2635919119929895e-05, 1.2635919119929895e-05, 1.2635919119929895e-05, 1.2635919119929895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2635919119929895e-05

Optimization complete. Final v2v error: 3.1080896854400635 mm

Highest mean error: 3.3262343406677246 mm for frame 117

Lowest mean error: 2.9420080184936523 mm for frame 11

Saving results

Total time: 43.08238244056702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753989
Iteration 2/25 | Loss: 0.00151761
Iteration 3/25 | Loss: 0.00141846
Iteration 4/25 | Loss: 0.00141247
Iteration 5/25 | Loss: 0.00141247
Iteration 6/25 | Loss: 0.00141247
Iteration 7/25 | Loss: 0.00141247
Iteration 8/25 | Loss: 0.00141247
Iteration 9/25 | Loss: 0.00141247
Iteration 10/25 | Loss: 0.00141247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001412472571246326, 0.001412472571246326, 0.001412472571246326, 0.001412472571246326, 0.001412472571246326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001412472571246326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23743856
Iteration 2/25 | Loss: 0.00196233
Iteration 3/25 | Loss: 0.00196224
Iteration 4/25 | Loss: 0.00196224
Iteration 5/25 | Loss: 0.00196224
Iteration 6/25 | Loss: 0.00196224
Iteration 7/25 | Loss: 0.00196224
Iteration 8/25 | Loss: 0.00196224
Iteration 9/25 | Loss: 0.00196224
Iteration 10/25 | Loss: 0.00196224
Iteration 11/25 | Loss: 0.00196224
Iteration 12/25 | Loss: 0.00196224
Iteration 13/25 | Loss: 0.00196224
Iteration 14/25 | Loss: 0.00196224
Iteration 15/25 | Loss: 0.00196224
Iteration 16/25 | Loss: 0.00196224
Iteration 17/25 | Loss: 0.00196224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019622407853603363, 0.0019622407853603363, 0.0019622407853603363, 0.0019622407853603363, 0.0019622407853603363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019622407853603363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196224
Iteration 2/1000 | Loss: 0.00003956
Iteration 3/1000 | Loss: 0.00002319
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00001759
Iteration 8/1000 | Loss: 0.00001717
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001591
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001500
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001461
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001450
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001435
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001434
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001421
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001420
Iteration 47/1000 | Loss: 0.00001419
Iteration 48/1000 | Loss: 0.00001419
Iteration 49/1000 | Loss: 0.00001419
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001419
Iteration 52/1000 | Loss: 0.00001419
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001418
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001400
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001399
Iteration 86/1000 | Loss: 0.00001399
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001397
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001396
Iteration 106/1000 | Loss: 0.00001396
Iteration 107/1000 | Loss: 0.00001396
Iteration 108/1000 | Loss: 0.00001396
Iteration 109/1000 | Loss: 0.00001395
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001395
Iteration 112/1000 | Loss: 0.00001395
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001393
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001392
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001391
Iteration 138/1000 | Loss: 0.00001391
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001391
Iteration 141/1000 | Loss: 0.00001391
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001391
Iteration 149/1000 | Loss: 0.00001391
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001391
Iteration 154/1000 | Loss: 0.00001391
Iteration 155/1000 | Loss: 0.00001390
Iteration 156/1000 | Loss: 0.00001390
Iteration 157/1000 | Loss: 0.00001390
Iteration 158/1000 | Loss: 0.00001390
Iteration 159/1000 | Loss: 0.00001390
Iteration 160/1000 | Loss: 0.00001390
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001390
Iteration 168/1000 | Loss: 0.00001390
Iteration 169/1000 | Loss: 0.00001390
Iteration 170/1000 | Loss: 0.00001390
Iteration 171/1000 | Loss: 0.00001390
Iteration 172/1000 | Loss: 0.00001390
Iteration 173/1000 | Loss: 0.00001390
Iteration 174/1000 | Loss: 0.00001390
Iteration 175/1000 | Loss: 0.00001390
Iteration 176/1000 | Loss: 0.00001390
Iteration 177/1000 | Loss: 0.00001390
Iteration 178/1000 | Loss: 0.00001390
Iteration 179/1000 | Loss: 0.00001390
Iteration 180/1000 | Loss: 0.00001390
Iteration 181/1000 | Loss: 0.00001390
Iteration 182/1000 | Loss: 0.00001390
Iteration 183/1000 | Loss: 0.00001390
Iteration 184/1000 | Loss: 0.00001390
Iteration 185/1000 | Loss: 0.00001390
Iteration 186/1000 | Loss: 0.00001390
Iteration 187/1000 | Loss: 0.00001390
Iteration 188/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.3899087207391858e-05, 1.3899087207391858e-05, 1.3899087207391858e-05, 1.3899087207391858e-05, 1.3899087207391858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3899087207391858e-05

Optimization complete. Final v2v error: 3.166959524154663 mm

Highest mean error: 3.597644805908203 mm for frame 20

Lowest mean error: 2.867593288421631 mm for frame 186

Saving results

Total time: 51.11357402801514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671152
Iteration 2/25 | Loss: 0.00143404
Iteration 3/25 | Loss: 0.00136497
Iteration 4/25 | Loss: 0.00135566
Iteration 5/25 | Loss: 0.00135263
Iteration 6/25 | Loss: 0.00135177
Iteration 7/25 | Loss: 0.00135175
Iteration 8/25 | Loss: 0.00135175
Iteration 9/25 | Loss: 0.00135175
Iteration 10/25 | Loss: 0.00135175
Iteration 11/25 | Loss: 0.00135175
Iteration 12/25 | Loss: 0.00135175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013517494080588222, 0.0013517494080588222, 0.0013517494080588222, 0.0013517494080588222, 0.0013517494080588222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013517494080588222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31775272
Iteration 2/25 | Loss: 0.00210389
Iteration 3/25 | Loss: 0.00210389
Iteration 4/25 | Loss: 0.00210389
Iteration 5/25 | Loss: 0.00210389
Iteration 6/25 | Loss: 0.00210388
Iteration 7/25 | Loss: 0.00210388
Iteration 8/25 | Loss: 0.00210388
Iteration 9/25 | Loss: 0.00210388
Iteration 10/25 | Loss: 0.00210388
Iteration 11/25 | Loss: 0.00210388
Iteration 12/25 | Loss: 0.00210388
Iteration 13/25 | Loss: 0.00210388
Iteration 14/25 | Loss: 0.00210388
Iteration 15/25 | Loss: 0.00210388
Iteration 16/25 | Loss: 0.00210388
Iteration 17/25 | Loss: 0.00210388
Iteration 18/25 | Loss: 0.00210388
Iteration 19/25 | Loss: 0.00210388
Iteration 20/25 | Loss: 0.00210388
Iteration 21/25 | Loss: 0.00210388
Iteration 22/25 | Loss: 0.00210388
Iteration 23/25 | Loss: 0.00210388
Iteration 24/25 | Loss: 0.00210388
Iteration 25/25 | Loss: 0.00210388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210388
Iteration 2/1000 | Loss: 0.00002636
Iteration 3/1000 | Loss: 0.00001889
Iteration 4/1000 | Loss: 0.00001520
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001127
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001099
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001088
Iteration 18/1000 | Loss: 0.00001081
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001077
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001076
Iteration 24/1000 | Loss: 0.00001073
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001067
Iteration 27/1000 | Loss: 0.00001067
Iteration 28/1000 | Loss: 0.00001065
Iteration 29/1000 | Loss: 0.00001064
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001063
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001062
Iteration 35/1000 | Loss: 0.00001062
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001061
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001055
Iteration 55/1000 | Loss: 0.00001054
Iteration 56/1000 | Loss: 0.00001053
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001051
Iteration 63/1000 | Loss: 0.00001050
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001048
Iteration 66/1000 | Loss: 0.00001048
Iteration 67/1000 | Loss: 0.00001048
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001047
Iteration 71/1000 | Loss: 0.00001047
Iteration 72/1000 | Loss: 0.00001047
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001046
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001045
Iteration 78/1000 | Loss: 0.00001045
Iteration 79/1000 | Loss: 0.00001045
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001044
Iteration 82/1000 | Loss: 0.00001044
Iteration 83/1000 | Loss: 0.00001044
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001042
Iteration 95/1000 | Loss: 0.00001042
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001041
Iteration 98/1000 | Loss: 0.00001041
Iteration 99/1000 | Loss: 0.00001041
Iteration 100/1000 | Loss: 0.00001041
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001041
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001041
Iteration 108/1000 | Loss: 0.00001041
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001041
Iteration 111/1000 | Loss: 0.00001041
Iteration 112/1000 | Loss: 0.00001041
Iteration 113/1000 | Loss: 0.00001041
Iteration 114/1000 | Loss: 0.00001041
Iteration 115/1000 | Loss: 0.00001041
Iteration 116/1000 | Loss: 0.00001040
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001040
Iteration 119/1000 | Loss: 0.00001040
Iteration 120/1000 | Loss: 0.00001040
Iteration 121/1000 | Loss: 0.00001040
Iteration 122/1000 | Loss: 0.00001040
Iteration 123/1000 | Loss: 0.00001040
Iteration 124/1000 | Loss: 0.00001040
Iteration 125/1000 | Loss: 0.00001040
Iteration 126/1000 | Loss: 0.00001040
Iteration 127/1000 | Loss: 0.00001040
Iteration 128/1000 | Loss: 0.00001040
Iteration 129/1000 | Loss: 0.00001040
Iteration 130/1000 | Loss: 0.00001040
Iteration 131/1000 | Loss: 0.00001040
Iteration 132/1000 | Loss: 0.00001040
Iteration 133/1000 | Loss: 0.00001040
Iteration 134/1000 | Loss: 0.00001040
Iteration 135/1000 | Loss: 0.00001040
Iteration 136/1000 | Loss: 0.00001040
Iteration 137/1000 | Loss: 0.00001040
Iteration 138/1000 | Loss: 0.00001040
Iteration 139/1000 | Loss: 0.00001040
Iteration 140/1000 | Loss: 0.00001040
Iteration 141/1000 | Loss: 0.00001040
Iteration 142/1000 | Loss: 0.00001040
Iteration 143/1000 | Loss: 0.00001040
Iteration 144/1000 | Loss: 0.00001040
Iteration 145/1000 | Loss: 0.00001040
Iteration 146/1000 | Loss: 0.00001040
Iteration 147/1000 | Loss: 0.00001040
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001040
Iteration 152/1000 | Loss: 0.00001040
Iteration 153/1000 | Loss: 0.00001040
Iteration 154/1000 | Loss: 0.00001040
Iteration 155/1000 | Loss: 0.00001040
Iteration 156/1000 | Loss: 0.00001040
Iteration 157/1000 | Loss: 0.00001040
Iteration 158/1000 | Loss: 0.00001040
Iteration 159/1000 | Loss: 0.00001040
Iteration 160/1000 | Loss: 0.00001040
Iteration 161/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.0399900929769501e-05, 1.0399900929769501e-05, 1.0399900929769501e-05, 1.0399900929769501e-05, 1.0399900929769501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0399900929769501e-05

Optimization complete. Final v2v error: 2.782503366470337 mm

Highest mean error: 3.429109573364258 mm for frame 72

Lowest mean error: 2.5638887882232666 mm for frame 13

Saving results

Total time: 38.699002742767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809546
Iteration 2/25 | Loss: 0.00144532
Iteration 3/25 | Loss: 0.00136104
Iteration 4/25 | Loss: 0.00135161
Iteration 5/25 | Loss: 0.00134974
Iteration 6/25 | Loss: 0.00134968
Iteration 7/25 | Loss: 0.00134968
Iteration 8/25 | Loss: 0.00134968
Iteration 9/25 | Loss: 0.00134968
Iteration 10/25 | Loss: 0.00134968
Iteration 11/25 | Loss: 0.00134968
Iteration 12/25 | Loss: 0.00134968
Iteration 13/25 | Loss: 0.00134968
Iteration 14/25 | Loss: 0.00134968
Iteration 15/25 | Loss: 0.00134968
Iteration 16/25 | Loss: 0.00134968
Iteration 17/25 | Loss: 0.00134968
Iteration 18/25 | Loss: 0.00134968
Iteration 19/25 | Loss: 0.00134968
Iteration 20/25 | Loss: 0.00134968
Iteration 21/25 | Loss: 0.00134968
Iteration 22/25 | Loss: 0.00134968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013496777974069118, 0.0013496777974069118, 0.0013496777974069118, 0.0013496777974069118, 0.0013496777974069118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013496777974069118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24363804
Iteration 2/25 | Loss: 0.00192056
Iteration 3/25 | Loss: 0.00192054
Iteration 4/25 | Loss: 0.00192054
Iteration 5/25 | Loss: 0.00192054
Iteration 6/25 | Loss: 0.00192054
Iteration 7/25 | Loss: 0.00192054
Iteration 8/25 | Loss: 0.00192054
Iteration 9/25 | Loss: 0.00192054
Iteration 10/25 | Loss: 0.00192054
Iteration 11/25 | Loss: 0.00192054
Iteration 12/25 | Loss: 0.00192054
Iteration 13/25 | Loss: 0.00192054
Iteration 14/25 | Loss: 0.00192054
Iteration 15/25 | Loss: 0.00192054
Iteration 16/25 | Loss: 0.00192054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00192054093349725, 0.00192054093349725, 0.00192054093349725, 0.00192054093349725, 0.00192054093349725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00192054093349725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192054
Iteration 2/1000 | Loss: 0.00002200
Iteration 3/1000 | Loss: 0.00001637
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001179
Iteration 7/1000 | Loss: 0.00001124
Iteration 8/1000 | Loss: 0.00001091
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001035
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001026
Iteration 17/1000 | Loss: 0.00001025
Iteration 18/1000 | Loss: 0.00001015
Iteration 19/1000 | Loss: 0.00001014
Iteration 20/1000 | Loss: 0.00001009
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001007
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001001
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000998
Iteration 29/1000 | Loss: 0.00000997
Iteration 30/1000 | Loss: 0.00000995
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000989
Iteration 33/1000 | Loss: 0.00000989
Iteration 34/1000 | Loss: 0.00000989
Iteration 35/1000 | Loss: 0.00000989
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000989
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000986
Iteration 41/1000 | Loss: 0.00000985
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000985
Iteration 44/1000 | Loss: 0.00000984
Iteration 45/1000 | Loss: 0.00000984
Iteration 46/1000 | Loss: 0.00000984
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000983
Iteration 49/1000 | Loss: 0.00000983
Iteration 50/1000 | Loss: 0.00000983
Iteration 51/1000 | Loss: 0.00000983
Iteration 52/1000 | Loss: 0.00000983
Iteration 53/1000 | Loss: 0.00000983
Iteration 54/1000 | Loss: 0.00000982
Iteration 55/1000 | Loss: 0.00000981
Iteration 56/1000 | Loss: 0.00000980
Iteration 57/1000 | Loss: 0.00000980
Iteration 58/1000 | Loss: 0.00000980
Iteration 59/1000 | Loss: 0.00000980
Iteration 60/1000 | Loss: 0.00000980
Iteration 61/1000 | Loss: 0.00000980
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000980
Iteration 64/1000 | Loss: 0.00000980
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000979
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000978
Iteration 70/1000 | Loss: 0.00000978
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000975
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000974
Iteration 85/1000 | Loss: 0.00000974
Iteration 86/1000 | Loss: 0.00000973
Iteration 87/1000 | Loss: 0.00000973
Iteration 88/1000 | Loss: 0.00000972
Iteration 89/1000 | Loss: 0.00000971
Iteration 90/1000 | Loss: 0.00000971
Iteration 91/1000 | Loss: 0.00000970
Iteration 92/1000 | Loss: 0.00000970
Iteration 93/1000 | Loss: 0.00000969
Iteration 94/1000 | Loss: 0.00000969
Iteration 95/1000 | Loss: 0.00000968
Iteration 96/1000 | Loss: 0.00000968
Iteration 97/1000 | Loss: 0.00000965
Iteration 98/1000 | Loss: 0.00000964
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000958
Iteration 111/1000 | Loss: 0.00000958
Iteration 112/1000 | Loss: 0.00000958
Iteration 113/1000 | Loss: 0.00000958
Iteration 114/1000 | Loss: 0.00000958
Iteration 115/1000 | Loss: 0.00000957
Iteration 116/1000 | Loss: 0.00000957
Iteration 117/1000 | Loss: 0.00000957
Iteration 118/1000 | Loss: 0.00000957
Iteration 119/1000 | Loss: 0.00000956
Iteration 120/1000 | Loss: 0.00000956
Iteration 121/1000 | Loss: 0.00000956
Iteration 122/1000 | Loss: 0.00000956
Iteration 123/1000 | Loss: 0.00000956
Iteration 124/1000 | Loss: 0.00000956
Iteration 125/1000 | Loss: 0.00000956
Iteration 126/1000 | Loss: 0.00000956
Iteration 127/1000 | Loss: 0.00000956
Iteration 128/1000 | Loss: 0.00000956
Iteration 129/1000 | Loss: 0.00000955
Iteration 130/1000 | Loss: 0.00000954
Iteration 131/1000 | Loss: 0.00000954
Iteration 132/1000 | Loss: 0.00000954
Iteration 133/1000 | Loss: 0.00000954
Iteration 134/1000 | Loss: 0.00000954
Iteration 135/1000 | Loss: 0.00000954
Iteration 136/1000 | Loss: 0.00000954
Iteration 137/1000 | Loss: 0.00000954
Iteration 138/1000 | Loss: 0.00000954
Iteration 139/1000 | Loss: 0.00000954
Iteration 140/1000 | Loss: 0.00000954
Iteration 141/1000 | Loss: 0.00000954
Iteration 142/1000 | Loss: 0.00000953
Iteration 143/1000 | Loss: 0.00000953
Iteration 144/1000 | Loss: 0.00000953
Iteration 145/1000 | Loss: 0.00000953
Iteration 146/1000 | Loss: 0.00000953
Iteration 147/1000 | Loss: 0.00000953
Iteration 148/1000 | Loss: 0.00000953
Iteration 149/1000 | Loss: 0.00000952
Iteration 150/1000 | Loss: 0.00000952
Iteration 151/1000 | Loss: 0.00000952
Iteration 152/1000 | Loss: 0.00000952
Iteration 153/1000 | Loss: 0.00000952
Iteration 154/1000 | Loss: 0.00000952
Iteration 155/1000 | Loss: 0.00000952
Iteration 156/1000 | Loss: 0.00000952
Iteration 157/1000 | Loss: 0.00000951
Iteration 158/1000 | Loss: 0.00000951
Iteration 159/1000 | Loss: 0.00000951
Iteration 160/1000 | Loss: 0.00000951
Iteration 161/1000 | Loss: 0.00000951
Iteration 162/1000 | Loss: 0.00000951
Iteration 163/1000 | Loss: 0.00000951
Iteration 164/1000 | Loss: 0.00000951
Iteration 165/1000 | Loss: 0.00000951
Iteration 166/1000 | Loss: 0.00000951
Iteration 167/1000 | Loss: 0.00000951
Iteration 168/1000 | Loss: 0.00000951
Iteration 169/1000 | Loss: 0.00000950
Iteration 170/1000 | Loss: 0.00000950
Iteration 171/1000 | Loss: 0.00000950
Iteration 172/1000 | Loss: 0.00000950
Iteration 173/1000 | Loss: 0.00000950
Iteration 174/1000 | Loss: 0.00000950
Iteration 175/1000 | Loss: 0.00000950
Iteration 176/1000 | Loss: 0.00000950
Iteration 177/1000 | Loss: 0.00000950
Iteration 178/1000 | Loss: 0.00000950
Iteration 179/1000 | Loss: 0.00000950
Iteration 180/1000 | Loss: 0.00000950
Iteration 181/1000 | Loss: 0.00000950
Iteration 182/1000 | Loss: 0.00000950
Iteration 183/1000 | Loss: 0.00000950
Iteration 184/1000 | Loss: 0.00000950
Iteration 185/1000 | Loss: 0.00000950
Iteration 186/1000 | Loss: 0.00000950
Iteration 187/1000 | Loss: 0.00000949
Iteration 188/1000 | Loss: 0.00000949
Iteration 189/1000 | Loss: 0.00000949
Iteration 190/1000 | Loss: 0.00000949
Iteration 191/1000 | Loss: 0.00000949
Iteration 192/1000 | Loss: 0.00000949
Iteration 193/1000 | Loss: 0.00000949
Iteration 194/1000 | Loss: 0.00000948
Iteration 195/1000 | Loss: 0.00000948
Iteration 196/1000 | Loss: 0.00000948
Iteration 197/1000 | Loss: 0.00000948
Iteration 198/1000 | Loss: 0.00000948
Iteration 199/1000 | Loss: 0.00000948
Iteration 200/1000 | Loss: 0.00000947
Iteration 201/1000 | Loss: 0.00000947
Iteration 202/1000 | Loss: 0.00000947
Iteration 203/1000 | Loss: 0.00000947
Iteration 204/1000 | Loss: 0.00000947
Iteration 205/1000 | Loss: 0.00000947
Iteration 206/1000 | Loss: 0.00000947
Iteration 207/1000 | Loss: 0.00000946
Iteration 208/1000 | Loss: 0.00000946
Iteration 209/1000 | Loss: 0.00000946
Iteration 210/1000 | Loss: 0.00000946
Iteration 211/1000 | Loss: 0.00000946
Iteration 212/1000 | Loss: 0.00000946
Iteration 213/1000 | Loss: 0.00000946
Iteration 214/1000 | Loss: 0.00000946
Iteration 215/1000 | Loss: 0.00000946
Iteration 216/1000 | Loss: 0.00000946
Iteration 217/1000 | Loss: 0.00000946
Iteration 218/1000 | Loss: 0.00000946
Iteration 219/1000 | Loss: 0.00000946
Iteration 220/1000 | Loss: 0.00000946
Iteration 221/1000 | Loss: 0.00000946
Iteration 222/1000 | Loss: 0.00000946
Iteration 223/1000 | Loss: 0.00000946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [9.46250838751439e-06, 9.46250838751439e-06, 9.46250838751439e-06, 9.46250838751439e-06, 9.46250838751439e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.46250838751439e-06

Optimization complete. Final v2v error: 2.6808278560638428 mm

Highest mean error: 2.814920425415039 mm for frame 22

Lowest mean error: 2.5691769123077393 mm for frame 62

Saving results

Total time: 40.235578536987305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869014
Iteration 2/25 | Loss: 0.00180720
Iteration 3/25 | Loss: 0.00154238
Iteration 4/25 | Loss: 0.00149739
Iteration 5/25 | Loss: 0.00149858
Iteration 6/25 | Loss: 0.00148713
Iteration 7/25 | Loss: 0.00147033
Iteration 8/25 | Loss: 0.00145134
Iteration 9/25 | Loss: 0.00145392
Iteration 10/25 | Loss: 0.00144520
Iteration 11/25 | Loss: 0.00144160
Iteration 12/25 | Loss: 0.00143912
Iteration 13/25 | Loss: 0.00144337
Iteration 14/25 | Loss: 0.00144061
Iteration 15/25 | Loss: 0.00143354
Iteration 16/25 | Loss: 0.00142918
Iteration 17/25 | Loss: 0.00142778
Iteration 18/25 | Loss: 0.00142743
Iteration 19/25 | Loss: 0.00142724
Iteration 20/25 | Loss: 0.00142707
Iteration 21/25 | Loss: 0.00142912
Iteration 22/25 | Loss: 0.00142761
Iteration 23/25 | Loss: 0.00142625
Iteration 24/25 | Loss: 0.00142598
Iteration 25/25 | Loss: 0.00142589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22201920
Iteration 2/25 | Loss: 0.00227843
Iteration 3/25 | Loss: 0.00227843
Iteration 4/25 | Loss: 0.00227843
Iteration 5/25 | Loss: 0.00227843
Iteration 6/25 | Loss: 0.00227843
Iteration 7/25 | Loss: 0.00227842
Iteration 8/25 | Loss: 0.00227842
Iteration 9/25 | Loss: 0.00227842
Iteration 10/25 | Loss: 0.00227842
Iteration 11/25 | Loss: 0.00227842
Iteration 12/25 | Loss: 0.00227842
Iteration 13/25 | Loss: 0.00227842
Iteration 14/25 | Loss: 0.00227842
Iteration 15/25 | Loss: 0.00227842
Iteration 16/25 | Loss: 0.00227842
Iteration 17/25 | Loss: 0.00227842
Iteration 18/25 | Loss: 0.00227842
Iteration 19/25 | Loss: 0.00227842
Iteration 20/25 | Loss: 0.00227842
Iteration 21/25 | Loss: 0.00227842
Iteration 22/25 | Loss: 0.00227842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022784224711358547, 0.0022784224711358547, 0.0022784224711358547, 0.0022784224711358547, 0.0022784224711358547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022784224711358547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227842
Iteration 2/1000 | Loss: 0.00014792
Iteration 3/1000 | Loss: 0.00029867
Iteration 4/1000 | Loss: 0.00004846
Iteration 5/1000 | Loss: 0.00003354
Iteration 6/1000 | Loss: 0.00002751
Iteration 7/1000 | Loss: 0.00002597
Iteration 8/1000 | Loss: 0.00002493
Iteration 9/1000 | Loss: 0.00002401
Iteration 10/1000 | Loss: 0.00002309
Iteration 11/1000 | Loss: 0.00002253
Iteration 12/1000 | Loss: 0.00002216
Iteration 13/1000 | Loss: 0.00002183
Iteration 14/1000 | Loss: 0.00002150
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002100
Iteration 18/1000 | Loss: 0.00002083
Iteration 19/1000 | Loss: 0.00002080
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00002067
Iteration 22/1000 | Loss: 0.00002066
Iteration 23/1000 | Loss: 0.00002061
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002057
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00002055
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002051
Iteration 33/1000 | Loss: 0.00002051
Iteration 34/1000 | Loss: 0.00002051
Iteration 35/1000 | Loss: 0.00002050
Iteration 36/1000 | Loss: 0.00002048
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002047
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002045
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002042
Iteration 47/1000 | Loss: 0.00002042
Iteration 48/1000 | Loss: 0.00002042
Iteration 49/1000 | Loss: 0.00002041
Iteration 50/1000 | Loss: 0.00002041
Iteration 51/1000 | Loss: 0.00002040
Iteration 52/1000 | Loss: 0.00002040
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002039
Iteration 55/1000 | Loss: 0.00002039
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002035
Iteration 70/1000 | Loss: 0.00002035
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00002034
Iteration 74/1000 | Loss: 0.00002034
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002033
Iteration 77/1000 | Loss: 0.00002032
Iteration 78/1000 | Loss: 0.00002032
Iteration 79/1000 | Loss: 0.00002032
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002031
Iteration 82/1000 | Loss: 0.00002031
Iteration 83/1000 | Loss: 0.00002031
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002030
Iteration 90/1000 | Loss: 0.00002030
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002027
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002027
Iteration 116/1000 | Loss: 0.00002027
Iteration 117/1000 | Loss: 0.00002027
Iteration 118/1000 | Loss: 0.00002027
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002026
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002024
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002023
Iteration 132/1000 | Loss: 0.00002023
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00002023
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00002022
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002022
Iteration 139/1000 | Loss: 0.00002021
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002021
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00002020
Iteration 148/1000 | Loss: 0.00002020
Iteration 149/1000 | Loss: 0.00002020
Iteration 150/1000 | Loss: 0.00002020
Iteration 151/1000 | Loss: 0.00002020
Iteration 152/1000 | Loss: 0.00002019
Iteration 153/1000 | Loss: 0.00002019
Iteration 154/1000 | Loss: 0.00002019
Iteration 155/1000 | Loss: 0.00002019
Iteration 156/1000 | Loss: 0.00002019
Iteration 157/1000 | Loss: 0.00002019
Iteration 158/1000 | Loss: 0.00002019
Iteration 159/1000 | Loss: 0.00002019
Iteration 160/1000 | Loss: 0.00002019
Iteration 161/1000 | Loss: 0.00002018
Iteration 162/1000 | Loss: 0.00002018
Iteration 163/1000 | Loss: 0.00002018
Iteration 164/1000 | Loss: 0.00002018
Iteration 165/1000 | Loss: 0.00002018
Iteration 166/1000 | Loss: 0.00002018
Iteration 167/1000 | Loss: 0.00002018
Iteration 168/1000 | Loss: 0.00002017
Iteration 169/1000 | Loss: 0.00002017
Iteration 170/1000 | Loss: 0.00002017
Iteration 171/1000 | Loss: 0.00002017
Iteration 172/1000 | Loss: 0.00002017
Iteration 173/1000 | Loss: 0.00002017
Iteration 174/1000 | Loss: 0.00002017
Iteration 175/1000 | Loss: 0.00002017
Iteration 176/1000 | Loss: 0.00002017
Iteration 177/1000 | Loss: 0.00002017
Iteration 178/1000 | Loss: 0.00002017
Iteration 179/1000 | Loss: 0.00002017
Iteration 180/1000 | Loss: 0.00002016
Iteration 181/1000 | Loss: 0.00002016
Iteration 182/1000 | Loss: 0.00002016
Iteration 183/1000 | Loss: 0.00002016
Iteration 184/1000 | Loss: 0.00002016
Iteration 185/1000 | Loss: 0.00002016
Iteration 186/1000 | Loss: 0.00002016
Iteration 187/1000 | Loss: 0.00002016
Iteration 188/1000 | Loss: 0.00002016
Iteration 189/1000 | Loss: 0.00002016
Iteration 190/1000 | Loss: 0.00002015
Iteration 191/1000 | Loss: 0.00002015
Iteration 192/1000 | Loss: 0.00002015
Iteration 193/1000 | Loss: 0.00002015
Iteration 194/1000 | Loss: 0.00002014
Iteration 195/1000 | Loss: 0.00002014
Iteration 196/1000 | Loss: 0.00002014
Iteration 197/1000 | Loss: 0.00002014
Iteration 198/1000 | Loss: 0.00002014
Iteration 199/1000 | Loss: 0.00002013
Iteration 200/1000 | Loss: 0.00002013
Iteration 201/1000 | Loss: 0.00002013
Iteration 202/1000 | Loss: 0.00002013
Iteration 203/1000 | Loss: 0.00002012
Iteration 204/1000 | Loss: 0.00002012
Iteration 205/1000 | Loss: 0.00002012
Iteration 206/1000 | Loss: 0.00002012
Iteration 207/1000 | Loss: 0.00002012
Iteration 208/1000 | Loss: 0.00002012
Iteration 209/1000 | Loss: 0.00002012
Iteration 210/1000 | Loss: 0.00002011
Iteration 211/1000 | Loss: 0.00002011
Iteration 212/1000 | Loss: 0.00002011
Iteration 213/1000 | Loss: 0.00002011
Iteration 214/1000 | Loss: 0.00002011
Iteration 215/1000 | Loss: 0.00002011
Iteration 216/1000 | Loss: 0.00002011
Iteration 217/1000 | Loss: 0.00002010
Iteration 218/1000 | Loss: 0.00002010
Iteration 219/1000 | Loss: 0.00002010
Iteration 220/1000 | Loss: 0.00002010
Iteration 221/1000 | Loss: 0.00002010
Iteration 222/1000 | Loss: 0.00002010
Iteration 223/1000 | Loss: 0.00002010
Iteration 224/1000 | Loss: 0.00002009
Iteration 225/1000 | Loss: 0.00002009
Iteration 226/1000 | Loss: 0.00002009
Iteration 227/1000 | Loss: 0.00002009
Iteration 228/1000 | Loss: 0.00002009
Iteration 229/1000 | Loss: 0.00002009
Iteration 230/1000 | Loss: 0.00002009
Iteration 231/1000 | Loss: 0.00002009
Iteration 232/1000 | Loss: 0.00002009
Iteration 233/1000 | Loss: 0.00002009
Iteration 234/1000 | Loss: 0.00002009
Iteration 235/1000 | Loss: 0.00002009
Iteration 236/1000 | Loss: 0.00002009
Iteration 237/1000 | Loss: 0.00002009
Iteration 238/1000 | Loss: 0.00002008
Iteration 239/1000 | Loss: 0.00002008
Iteration 240/1000 | Loss: 0.00002008
Iteration 241/1000 | Loss: 0.00002008
Iteration 242/1000 | Loss: 0.00002008
Iteration 243/1000 | Loss: 0.00002008
Iteration 244/1000 | Loss: 0.00002008
Iteration 245/1000 | Loss: 0.00002008
Iteration 246/1000 | Loss: 0.00002008
Iteration 247/1000 | Loss: 0.00002008
Iteration 248/1000 | Loss: 0.00002008
Iteration 249/1000 | Loss: 0.00002008
Iteration 250/1000 | Loss: 0.00002008
Iteration 251/1000 | Loss: 0.00002008
Iteration 252/1000 | Loss: 0.00002008
Iteration 253/1000 | Loss: 0.00002008
Iteration 254/1000 | Loss: 0.00002008
Iteration 255/1000 | Loss: 0.00002008
Iteration 256/1000 | Loss: 0.00002008
Iteration 257/1000 | Loss: 0.00002008
Iteration 258/1000 | Loss: 0.00002008
Iteration 259/1000 | Loss: 0.00002008
Iteration 260/1000 | Loss: 0.00002008
Iteration 261/1000 | Loss: 0.00002008
Iteration 262/1000 | Loss: 0.00002008
Iteration 263/1000 | Loss: 0.00002008
Iteration 264/1000 | Loss: 0.00002008
Iteration 265/1000 | Loss: 0.00002008
Iteration 266/1000 | Loss: 0.00002008
Iteration 267/1000 | Loss: 0.00002008
Iteration 268/1000 | Loss: 0.00002008
Iteration 269/1000 | Loss: 0.00002008
Iteration 270/1000 | Loss: 0.00002008
Iteration 271/1000 | Loss: 0.00002008
Iteration 272/1000 | Loss: 0.00002008
Iteration 273/1000 | Loss: 0.00002008
Iteration 274/1000 | Loss: 0.00002008
Iteration 275/1000 | Loss: 0.00002008
Iteration 276/1000 | Loss: 0.00002008
Iteration 277/1000 | Loss: 0.00002008
Iteration 278/1000 | Loss: 0.00002008
Iteration 279/1000 | Loss: 0.00002008
Iteration 280/1000 | Loss: 0.00002008
Iteration 281/1000 | Loss: 0.00002008
Iteration 282/1000 | Loss: 0.00002008
Iteration 283/1000 | Loss: 0.00002008
Iteration 284/1000 | Loss: 0.00002008
Iteration 285/1000 | Loss: 0.00002008
Iteration 286/1000 | Loss: 0.00002008
Iteration 287/1000 | Loss: 0.00002008
Iteration 288/1000 | Loss: 0.00002008
Iteration 289/1000 | Loss: 0.00002008
Iteration 290/1000 | Loss: 0.00002008
Iteration 291/1000 | Loss: 0.00002008
Iteration 292/1000 | Loss: 0.00002008
Iteration 293/1000 | Loss: 0.00002008
Iteration 294/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.007542207138613e-05, 2.007542207138613e-05, 2.007542207138613e-05, 2.007542207138613e-05, 2.007542207138613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.007542207138613e-05

Optimization complete. Final v2v error: 3.757201910018921 mm

Highest mean error: 5.709357738494873 mm for frame 94

Lowest mean error: 3.2027082443237305 mm for frame 67

Saving results

Total time: 92.5348539352417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998260
Iteration 2/25 | Loss: 0.00276058
Iteration 3/25 | Loss: 0.00230858
Iteration 4/25 | Loss: 0.00197319
Iteration 5/25 | Loss: 0.00262744
Iteration 6/25 | Loss: 0.00273163
Iteration 7/25 | Loss: 0.00181524
Iteration 8/25 | Loss: 0.00160970
Iteration 9/25 | Loss: 0.00153369
Iteration 10/25 | Loss: 0.00149248
Iteration 11/25 | Loss: 0.00148395
Iteration 12/25 | Loss: 0.00146749
Iteration 13/25 | Loss: 0.00146429
Iteration 14/25 | Loss: 0.00146375
Iteration 15/25 | Loss: 0.00146040
Iteration 16/25 | Loss: 0.00146235
Iteration 17/25 | Loss: 0.00145848
Iteration 18/25 | Loss: 0.00146296
Iteration 19/25 | Loss: 0.00145978
Iteration 20/25 | Loss: 0.00145282
Iteration 21/25 | Loss: 0.00145550
Iteration 22/25 | Loss: 0.00145489
Iteration 23/25 | Loss: 0.00145044
Iteration 24/25 | Loss: 0.00145043
Iteration 25/25 | Loss: 0.00145043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14452672
Iteration 2/25 | Loss: 0.00171418
Iteration 3/25 | Loss: 0.00170230
Iteration 4/25 | Loss: 0.00170230
Iteration 5/25 | Loss: 0.00170230
Iteration 6/25 | Loss: 0.00170230
Iteration 7/25 | Loss: 0.00170230
Iteration 8/25 | Loss: 0.00170230
Iteration 9/25 | Loss: 0.00170230
Iteration 10/25 | Loss: 0.00170230
Iteration 11/25 | Loss: 0.00170230
Iteration 12/25 | Loss: 0.00170230
Iteration 13/25 | Loss: 0.00170230
Iteration 14/25 | Loss: 0.00170230
Iteration 15/25 | Loss: 0.00170230
Iteration 16/25 | Loss: 0.00170230
Iteration 17/25 | Loss: 0.00170230
Iteration 18/25 | Loss: 0.00170230
Iteration 19/25 | Loss: 0.00170230
Iteration 20/25 | Loss: 0.00170230
Iteration 21/25 | Loss: 0.00170230
Iteration 22/25 | Loss: 0.00170230
Iteration 23/25 | Loss: 0.00170230
Iteration 24/25 | Loss: 0.00170230
Iteration 25/25 | Loss: 0.00170230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170230
Iteration 2/1000 | Loss: 0.00006078
Iteration 3/1000 | Loss: 0.00011549
Iteration 4/1000 | Loss: 0.00007254
Iteration 5/1000 | Loss: 0.00002580
Iteration 6/1000 | Loss: 0.00005147
Iteration 7/1000 | Loss: 0.00002411
Iteration 8/1000 | Loss: 0.00008793
Iteration 9/1000 | Loss: 0.00020355
Iteration 10/1000 | Loss: 0.00003120
Iteration 11/1000 | Loss: 0.00004209
Iteration 12/1000 | Loss: 0.00003485
Iteration 13/1000 | Loss: 0.00004746
Iteration 14/1000 | Loss: 0.00002823
Iteration 15/1000 | Loss: 0.00002642
Iteration 16/1000 | Loss: 0.00002857
Iteration 17/1000 | Loss: 0.00002163
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00009139
Iteration 20/1000 | Loss: 0.00015181
Iteration 21/1000 | Loss: 0.00004728
Iteration 22/1000 | Loss: 0.00005655
Iteration 23/1000 | Loss: 0.00002062
Iteration 24/1000 | Loss: 0.00002039
Iteration 25/1000 | Loss: 0.00011564
Iteration 26/1000 | Loss: 0.00005545
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001973
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001963
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001962
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001957
Iteration 42/1000 | Loss: 0.00001956
Iteration 43/1000 | Loss: 0.00001956
Iteration 44/1000 | Loss: 0.00001956
Iteration 45/1000 | Loss: 0.00001954
Iteration 46/1000 | Loss: 0.00001954
Iteration 47/1000 | Loss: 0.00001954
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001954
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001953
Iteration 53/1000 | Loss: 0.00001953
Iteration 54/1000 | Loss: 0.00001953
Iteration 55/1000 | Loss: 0.00001953
Iteration 56/1000 | Loss: 0.00001953
Iteration 57/1000 | Loss: 0.00001953
Iteration 58/1000 | Loss: 0.00001953
Iteration 59/1000 | Loss: 0.00001952
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001951
Iteration 62/1000 | Loss: 0.00001951
Iteration 63/1000 | Loss: 0.00001950
Iteration 64/1000 | Loss: 0.00001950
Iteration 65/1000 | Loss: 0.00001950
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00004240
Iteration 73/1000 | Loss: 0.00009868
Iteration 74/1000 | Loss: 0.00008936
Iteration 75/1000 | Loss: 0.00002695
Iteration 76/1000 | Loss: 0.00002590
Iteration 77/1000 | Loss: 0.00001958
Iteration 78/1000 | Loss: 0.00003844
Iteration 79/1000 | Loss: 0.00002078
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002733
Iteration 82/1000 | Loss: 0.00002933
Iteration 83/1000 | Loss: 0.00002354
Iteration 84/1000 | Loss: 0.00001946
Iteration 85/1000 | Loss: 0.00001946
Iteration 86/1000 | Loss: 0.00001946
Iteration 87/1000 | Loss: 0.00001946
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001945
Iteration 91/1000 | Loss: 0.00001945
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00001944
Iteration 96/1000 | Loss: 0.00001944
Iteration 97/1000 | Loss: 0.00001944
Iteration 98/1000 | Loss: 0.00001944
Iteration 99/1000 | Loss: 0.00001944
Iteration 100/1000 | Loss: 0.00001944
Iteration 101/1000 | Loss: 0.00001944
Iteration 102/1000 | Loss: 0.00001944
Iteration 103/1000 | Loss: 0.00001944
Iteration 104/1000 | Loss: 0.00001944
Iteration 105/1000 | Loss: 0.00001944
Iteration 106/1000 | Loss: 0.00001944
Iteration 107/1000 | Loss: 0.00001944
Iteration 108/1000 | Loss: 0.00001944
Iteration 109/1000 | Loss: 0.00001944
Iteration 110/1000 | Loss: 0.00001944
Iteration 111/1000 | Loss: 0.00001944
Iteration 112/1000 | Loss: 0.00001944
Iteration 113/1000 | Loss: 0.00001943
Iteration 114/1000 | Loss: 0.00001943
Iteration 115/1000 | Loss: 0.00001943
Iteration 116/1000 | Loss: 0.00001943
Iteration 117/1000 | Loss: 0.00001943
Iteration 118/1000 | Loss: 0.00001943
Iteration 119/1000 | Loss: 0.00001943
Iteration 120/1000 | Loss: 0.00001943
Iteration 121/1000 | Loss: 0.00001943
Iteration 122/1000 | Loss: 0.00001943
Iteration 123/1000 | Loss: 0.00001943
Iteration 124/1000 | Loss: 0.00001943
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001943
Iteration 127/1000 | Loss: 0.00001943
Iteration 128/1000 | Loss: 0.00001943
Iteration 129/1000 | Loss: 0.00001942
Iteration 130/1000 | Loss: 0.00001942
Iteration 131/1000 | Loss: 0.00001942
Iteration 132/1000 | Loss: 0.00001942
Iteration 133/1000 | Loss: 0.00001941
Iteration 134/1000 | Loss: 0.00001941
Iteration 135/1000 | Loss: 0.00001941
Iteration 136/1000 | Loss: 0.00007409
Iteration 137/1000 | Loss: 0.00003976
Iteration 138/1000 | Loss: 0.00003163
Iteration 139/1000 | Loss: 0.00002865
Iteration 140/1000 | Loss: 0.00001944
Iteration 141/1000 | Loss: 0.00001944
Iteration 142/1000 | Loss: 0.00001944
Iteration 143/1000 | Loss: 0.00001943
Iteration 144/1000 | Loss: 0.00001943
Iteration 145/1000 | Loss: 0.00001943
Iteration 146/1000 | Loss: 0.00001942
Iteration 147/1000 | Loss: 0.00001942
Iteration 148/1000 | Loss: 0.00001942
Iteration 149/1000 | Loss: 0.00001941
Iteration 150/1000 | Loss: 0.00001941
Iteration 151/1000 | Loss: 0.00001941
Iteration 152/1000 | Loss: 0.00001941
Iteration 153/1000 | Loss: 0.00001941
Iteration 154/1000 | Loss: 0.00001941
Iteration 155/1000 | Loss: 0.00001941
Iteration 156/1000 | Loss: 0.00001941
Iteration 157/1000 | Loss: 0.00001941
Iteration 158/1000 | Loss: 0.00001941
Iteration 159/1000 | Loss: 0.00001941
Iteration 160/1000 | Loss: 0.00001941
Iteration 161/1000 | Loss: 0.00001941
Iteration 162/1000 | Loss: 0.00001941
Iteration 163/1000 | Loss: 0.00001941
Iteration 164/1000 | Loss: 0.00001941
Iteration 165/1000 | Loss: 0.00001941
Iteration 166/1000 | Loss: 0.00001941
Iteration 167/1000 | Loss: 0.00001941
Iteration 168/1000 | Loss: 0.00001941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.940852962434292e-05, 1.940852962434292e-05, 1.940852962434292e-05, 1.940852962434292e-05, 1.940852962434292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.940852962434292e-05

Optimization complete. Final v2v error: 3.7278683185577393 mm

Highest mean error: 3.878305435180664 mm for frame 0

Lowest mean error: 3.6180062294006348 mm for frame 126

Saving results

Total time: 104.76218032836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811369
Iteration 2/25 | Loss: 0.00146310
Iteration 3/25 | Loss: 0.00137801
Iteration 4/25 | Loss: 0.00136510
Iteration 5/25 | Loss: 0.00136135
Iteration 6/25 | Loss: 0.00136099
Iteration 7/25 | Loss: 0.00136099
Iteration 8/25 | Loss: 0.00136093
Iteration 9/25 | Loss: 0.00136093
Iteration 10/25 | Loss: 0.00136093
Iteration 11/25 | Loss: 0.00136093
Iteration 12/25 | Loss: 0.00136093
Iteration 13/25 | Loss: 0.00136093
Iteration 14/25 | Loss: 0.00136093
Iteration 15/25 | Loss: 0.00136093
Iteration 16/25 | Loss: 0.00136093
Iteration 17/25 | Loss: 0.00136093
Iteration 18/25 | Loss: 0.00136093
Iteration 19/25 | Loss: 0.00136093
Iteration 20/25 | Loss: 0.00136093
Iteration 21/25 | Loss: 0.00136093
Iteration 22/25 | Loss: 0.00136093
Iteration 23/25 | Loss: 0.00136093
Iteration 24/25 | Loss: 0.00136093
Iteration 25/25 | Loss: 0.00136093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19818830
Iteration 2/25 | Loss: 0.00217102
Iteration 3/25 | Loss: 0.00217102
Iteration 4/25 | Loss: 0.00217102
Iteration 5/25 | Loss: 0.00217102
Iteration 6/25 | Loss: 0.00217102
Iteration 7/25 | Loss: 0.00217102
Iteration 8/25 | Loss: 0.00217102
Iteration 9/25 | Loss: 0.00217102
Iteration 10/25 | Loss: 0.00217102
Iteration 11/25 | Loss: 0.00217102
Iteration 12/25 | Loss: 0.00217102
Iteration 13/25 | Loss: 0.00217102
Iteration 14/25 | Loss: 0.00217102
Iteration 15/25 | Loss: 0.00217102
Iteration 16/25 | Loss: 0.00217102
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002171015366911888, 0.002171015366911888, 0.002171015366911888, 0.002171015366911888, 0.002171015366911888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002171015366911888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217102
Iteration 2/1000 | Loss: 0.00002544
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001535
Iteration 6/1000 | Loss: 0.00001480
Iteration 7/1000 | Loss: 0.00001415
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001304
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001275
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001274
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001271
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001233
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001219
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001218
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001213
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001213
Iteration 97/1000 | Loss: 0.00001213
Iteration 98/1000 | Loss: 0.00001213
Iteration 99/1000 | Loss: 0.00001213
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001212
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001212
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001212
Iteration 106/1000 | Loss: 0.00001212
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001210
Iteration 120/1000 | Loss: 0.00001210
Iteration 121/1000 | Loss: 0.00001210
Iteration 122/1000 | Loss: 0.00001210
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001210
Iteration 125/1000 | Loss: 0.00001210
Iteration 126/1000 | Loss: 0.00001210
Iteration 127/1000 | Loss: 0.00001210
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001209
Iteration 130/1000 | Loss: 0.00001209
Iteration 131/1000 | Loss: 0.00001209
Iteration 132/1000 | Loss: 0.00001209
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001208
Iteration 138/1000 | Loss: 0.00001208
Iteration 139/1000 | Loss: 0.00001208
Iteration 140/1000 | Loss: 0.00001208
Iteration 141/1000 | Loss: 0.00001208
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001205
Iteration 160/1000 | Loss: 0.00001205
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001204
Iteration 169/1000 | Loss: 0.00001204
Iteration 170/1000 | Loss: 0.00001204
Iteration 171/1000 | Loss: 0.00001204
Iteration 172/1000 | Loss: 0.00001203
Iteration 173/1000 | Loss: 0.00001203
Iteration 174/1000 | Loss: 0.00001203
Iteration 175/1000 | Loss: 0.00001203
Iteration 176/1000 | Loss: 0.00001203
Iteration 177/1000 | Loss: 0.00001203
Iteration 178/1000 | Loss: 0.00001203
Iteration 179/1000 | Loss: 0.00001202
Iteration 180/1000 | Loss: 0.00001202
Iteration 181/1000 | Loss: 0.00001202
Iteration 182/1000 | Loss: 0.00001202
Iteration 183/1000 | Loss: 0.00001202
Iteration 184/1000 | Loss: 0.00001202
Iteration 185/1000 | Loss: 0.00001202
Iteration 186/1000 | Loss: 0.00001202
Iteration 187/1000 | Loss: 0.00001202
Iteration 188/1000 | Loss: 0.00001202
Iteration 189/1000 | Loss: 0.00001201
Iteration 190/1000 | Loss: 0.00001201
Iteration 191/1000 | Loss: 0.00001201
Iteration 192/1000 | Loss: 0.00001201
Iteration 193/1000 | Loss: 0.00001201
Iteration 194/1000 | Loss: 0.00001201
Iteration 195/1000 | Loss: 0.00001201
Iteration 196/1000 | Loss: 0.00001201
Iteration 197/1000 | Loss: 0.00001201
Iteration 198/1000 | Loss: 0.00001201
Iteration 199/1000 | Loss: 0.00001201
Iteration 200/1000 | Loss: 0.00001201
Iteration 201/1000 | Loss: 0.00001201
Iteration 202/1000 | Loss: 0.00001200
Iteration 203/1000 | Loss: 0.00001200
Iteration 204/1000 | Loss: 0.00001200
Iteration 205/1000 | Loss: 0.00001200
Iteration 206/1000 | Loss: 0.00001200
Iteration 207/1000 | Loss: 0.00001200
Iteration 208/1000 | Loss: 0.00001200
Iteration 209/1000 | Loss: 0.00001200
Iteration 210/1000 | Loss: 0.00001200
Iteration 211/1000 | Loss: 0.00001200
Iteration 212/1000 | Loss: 0.00001200
Iteration 213/1000 | Loss: 0.00001200
Iteration 214/1000 | Loss: 0.00001200
Iteration 215/1000 | Loss: 0.00001200
Iteration 216/1000 | Loss: 0.00001200
Iteration 217/1000 | Loss: 0.00001200
Iteration 218/1000 | Loss: 0.00001200
Iteration 219/1000 | Loss: 0.00001199
Iteration 220/1000 | Loss: 0.00001199
Iteration 221/1000 | Loss: 0.00001199
Iteration 222/1000 | Loss: 0.00001199
Iteration 223/1000 | Loss: 0.00001199
Iteration 224/1000 | Loss: 0.00001199
Iteration 225/1000 | Loss: 0.00001199
Iteration 226/1000 | Loss: 0.00001199
Iteration 227/1000 | Loss: 0.00001199
Iteration 228/1000 | Loss: 0.00001199
Iteration 229/1000 | Loss: 0.00001199
Iteration 230/1000 | Loss: 0.00001199
Iteration 231/1000 | Loss: 0.00001199
Iteration 232/1000 | Loss: 0.00001199
Iteration 233/1000 | Loss: 0.00001199
Iteration 234/1000 | Loss: 0.00001199
Iteration 235/1000 | Loss: 0.00001199
Iteration 236/1000 | Loss: 0.00001199
Iteration 237/1000 | Loss: 0.00001199
Iteration 238/1000 | Loss: 0.00001199
Iteration 239/1000 | Loss: 0.00001199
Iteration 240/1000 | Loss: 0.00001199
Iteration 241/1000 | Loss: 0.00001199
Iteration 242/1000 | Loss: 0.00001198
Iteration 243/1000 | Loss: 0.00001198
Iteration 244/1000 | Loss: 0.00001198
Iteration 245/1000 | Loss: 0.00001198
Iteration 246/1000 | Loss: 0.00001198
Iteration 247/1000 | Loss: 0.00001198
Iteration 248/1000 | Loss: 0.00001198
Iteration 249/1000 | Loss: 0.00001198
Iteration 250/1000 | Loss: 0.00001198
Iteration 251/1000 | Loss: 0.00001198
Iteration 252/1000 | Loss: 0.00001198
Iteration 253/1000 | Loss: 0.00001198
Iteration 254/1000 | Loss: 0.00001198
Iteration 255/1000 | Loss: 0.00001198
Iteration 256/1000 | Loss: 0.00001198
Iteration 257/1000 | Loss: 0.00001198
Iteration 258/1000 | Loss: 0.00001198
Iteration 259/1000 | Loss: 0.00001198
Iteration 260/1000 | Loss: 0.00001198
Iteration 261/1000 | Loss: 0.00001198
Iteration 262/1000 | Loss: 0.00001198
Iteration 263/1000 | Loss: 0.00001198
Iteration 264/1000 | Loss: 0.00001198
Iteration 265/1000 | Loss: 0.00001198
Iteration 266/1000 | Loss: 0.00001197
Iteration 267/1000 | Loss: 0.00001197
Iteration 268/1000 | Loss: 0.00001197
Iteration 269/1000 | Loss: 0.00001197
Iteration 270/1000 | Loss: 0.00001197
Iteration 271/1000 | Loss: 0.00001197
Iteration 272/1000 | Loss: 0.00001197
Iteration 273/1000 | Loss: 0.00001197
Iteration 274/1000 | Loss: 0.00001197
Iteration 275/1000 | Loss: 0.00001197
Iteration 276/1000 | Loss: 0.00001197
Iteration 277/1000 | Loss: 0.00001197
Iteration 278/1000 | Loss: 0.00001197
Iteration 279/1000 | Loss: 0.00001197
Iteration 280/1000 | Loss: 0.00001197
Iteration 281/1000 | Loss: 0.00001197
Iteration 282/1000 | Loss: 0.00001197
Iteration 283/1000 | Loss: 0.00001197
Iteration 284/1000 | Loss: 0.00001197
Iteration 285/1000 | Loss: 0.00001197
Iteration 286/1000 | Loss: 0.00001197
Iteration 287/1000 | Loss: 0.00001197
Iteration 288/1000 | Loss: 0.00001197
Iteration 289/1000 | Loss: 0.00001197
Iteration 290/1000 | Loss: 0.00001197
Iteration 291/1000 | Loss: 0.00001197
Iteration 292/1000 | Loss: 0.00001197
Iteration 293/1000 | Loss: 0.00001197
Iteration 294/1000 | Loss: 0.00001197
Iteration 295/1000 | Loss: 0.00001197
Iteration 296/1000 | Loss: 0.00001197
Iteration 297/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.197481560666347e-05, 1.197481560666347e-05, 1.197481560666347e-05, 1.197481560666347e-05, 1.197481560666347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.197481560666347e-05

Optimization complete. Final v2v error: 2.938819169998169 mm

Highest mean error: 4.044788837432861 mm for frame 56

Lowest mean error: 2.631394386291504 mm for frame 107

Saving results

Total time: 46.87825679779053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958748
Iteration 2/25 | Loss: 0.00182897
Iteration 3/25 | Loss: 0.00158358
Iteration 4/25 | Loss: 0.00156384
Iteration 5/25 | Loss: 0.00155757
Iteration 6/25 | Loss: 0.00155664
Iteration 7/25 | Loss: 0.00155664
Iteration 8/25 | Loss: 0.00155664
Iteration 9/25 | Loss: 0.00155664
Iteration 10/25 | Loss: 0.00155664
Iteration 11/25 | Loss: 0.00155664
Iteration 12/25 | Loss: 0.00155664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015566409565508366, 0.0015566409565508366, 0.0015566409565508366, 0.0015566409565508366, 0.0015566409565508366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015566409565508366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48348561
Iteration 2/25 | Loss: 0.00220821
Iteration 3/25 | Loss: 0.00220821
Iteration 4/25 | Loss: 0.00220821
Iteration 5/25 | Loss: 0.00220821
Iteration 6/25 | Loss: 0.00220821
Iteration 7/25 | Loss: 0.00220821
Iteration 8/25 | Loss: 0.00220821
Iteration 9/25 | Loss: 0.00220821
Iteration 10/25 | Loss: 0.00220821
Iteration 11/25 | Loss: 0.00220821
Iteration 12/25 | Loss: 0.00220821
Iteration 13/25 | Loss: 0.00220821
Iteration 14/25 | Loss: 0.00220821
Iteration 15/25 | Loss: 0.00220821
Iteration 16/25 | Loss: 0.00220821
Iteration 17/25 | Loss: 0.00220821
Iteration 18/25 | Loss: 0.00220821
Iteration 19/25 | Loss: 0.00220821
Iteration 20/25 | Loss: 0.00220821
Iteration 21/25 | Loss: 0.00220821
Iteration 22/25 | Loss: 0.00220821
Iteration 23/25 | Loss: 0.00220821
Iteration 24/25 | Loss: 0.00220821
Iteration 25/25 | Loss: 0.00220821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220821
Iteration 2/1000 | Loss: 0.00007164
Iteration 3/1000 | Loss: 0.00004872
Iteration 4/1000 | Loss: 0.00004189
Iteration 5/1000 | Loss: 0.00003945
Iteration 6/1000 | Loss: 0.00003810
Iteration 7/1000 | Loss: 0.00003714
Iteration 8/1000 | Loss: 0.00003652
Iteration 9/1000 | Loss: 0.00003598
Iteration 10/1000 | Loss: 0.00003553
Iteration 11/1000 | Loss: 0.00003512
Iteration 12/1000 | Loss: 0.00003478
Iteration 13/1000 | Loss: 0.00003437
Iteration 14/1000 | Loss: 0.00003401
Iteration 15/1000 | Loss: 0.00003376
Iteration 16/1000 | Loss: 0.00003350
Iteration 17/1000 | Loss: 0.00003322
Iteration 18/1000 | Loss: 0.00003296
Iteration 19/1000 | Loss: 0.00003281
Iteration 20/1000 | Loss: 0.00003268
Iteration 21/1000 | Loss: 0.00003260
Iteration 22/1000 | Loss: 0.00003247
Iteration 23/1000 | Loss: 0.00003247
Iteration 24/1000 | Loss: 0.00003240
Iteration 25/1000 | Loss: 0.00003238
Iteration 26/1000 | Loss: 0.00003230
Iteration 27/1000 | Loss: 0.00003230
Iteration 28/1000 | Loss: 0.00003229
Iteration 29/1000 | Loss: 0.00003228
Iteration 30/1000 | Loss: 0.00003228
Iteration 31/1000 | Loss: 0.00003228
Iteration 32/1000 | Loss: 0.00003227
Iteration 33/1000 | Loss: 0.00003227
Iteration 34/1000 | Loss: 0.00003227
Iteration 35/1000 | Loss: 0.00003226
Iteration 36/1000 | Loss: 0.00003226
Iteration 37/1000 | Loss: 0.00003226
Iteration 38/1000 | Loss: 0.00003226
Iteration 39/1000 | Loss: 0.00003226
Iteration 40/1000 | Loss: 0.00003225
Iteration 41/1000 | Loss: 0.00003225
Iteration 42/1000 | Loss: 0.00003225
Iteration 43/1000 | Loss: 0.00003224
Iteration 44/1000 | Loss: 0.00003224
Iteration 45/1000 | Loss: 0.00003224
Iteration 46/1000 | Loss: 0.00003224
Iteration 47/1000 | Loss: 0.00003224
Iteration 48/1000 | Loss: 0.00003223
Iteration 49/1000 | Loss: 0.00003223
Iteration 50/1000 | Loss: 0.00003223
Iteration 51/1000 | Loss: 0.00003223
Iteration 52/1000 | Loss: 0.00003222
Iteration 53/1000 | Loss: 0.00003222
Iteration 54/1000 | Loss: 0.00003222
Iteration 55/1000 | Loss: 0.00003221
Iteration 56/1000 | Loss: 0.00003221
Iteration 57/1000 | Loss: 0.00003221
Iteration 58/1000 | Loss: 0.00003221
Iteration 59/1000 | Loss: 0.00003220
Iteration 60/1000 | Loss: 0.00003220
Iteration 61/1000 | Loss: 0.00003220
Iteration 62/1000 | Loss: 0.00003220
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003220
Iteration 66/1000 | Loss: 0.00003220
Iteration 67/1000 | Loss: 0.00003220
Iteration 68/1000 | Loss: 0.00003220
Iteration 69/1000 | Loss: 0.00003220
Iteration 70/1000 | Loss: 0.00003220
Iteration 71/1000 | Loss: 0.00003220
Iteration 72/1000 | Loss: 0.00003220
Iteration 73/1000 | Loss: 0.00003220
Iteration 74/1000 | Loss: 0.00003220
Iteration 75/1000 | Loss: 0.00003220
Iteration 76/1000 | Loss: 0.00003220
Iteration 77/1000 | Loss: 0.00003220
Iteration 78/1000 | Loss: 0.00003220
Iteration 79/1000 | Loss: 0.00003220
Iteration 80/1000 | Loss: 0.00003220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [3.219575228285976e-05, 3.219575228285976e-05, 3.219575228285976e-05, 3.219575228285976e-05, 3.219575228285976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.219575228285976e-05

Optimization complete. Final v2v error: 4.788527965545654 mm

Highest mean error: 5.640981674194336 mm for frame 16

Lowest mean error: 4.459811210632324 mm for frame 43

Saving results

Total time: 46.27119278907776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005293
Iteration 2/25 | Loss: 0.00260519
Iteration 3/25 | Loss: 0.00208214
Iteration 4/25 | Loss: 0.00188467
Iteration 5/25 | Loss: 0.00176384
Iteration 6/25 | Loss: 0.00171407
Iteration 7/25 | Loss: 0.00169896
Iteration 8/25 | Loss: 0.00167156
Iteration 9/25 | Loss: 0.00166219
Iteration 10/25 | Loss: 0.00166858
Iteration 11/25 | Loss: 0.00167602
Iteration 12/25 | Loss: 0.00165897
Iteration 13/25 | Loss: 0.00164393
Iteration 14/25 | Loss: 0.00163726
Iteration 15/25 | Loss: 0.00163503
Iteration 16/25 | Loss: 0.00163364
Iteration 17/25 | Loss: 0.00163715
Iteration 18/25 | Loss: 0.00163190
Iteration 19/25 | Loss: 0.00162743
Iteration 20/25 | Loss: 0.00162589
Iteration 21/25 | Loss: 0.00162519
Iteration 22/25 | Loss: 0.00162468
Iteration 23/25 | Loss: 0.00162437
Iteration 24/25 | Loss: 0.00162416
Iteration 25/25 | Loss: 0.00162413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71496105
Iteration 2/25 | Loss: 0.00361396
Iteration 3/25 | Loss: 0.00361396
Iteration 4/25 | Loss: 0.00361396
Iteration 5/25 | Loss: 0.00361396
Iteration 6/25 | Loss: 0.00361396
Iteration 7/25 | Loss: 0.00361396
Iteration 8/25 | Loss: 0.00361396
Iteration 9/25 | Loss: 0.00361396
Iteration 10/25 | Loss: 0.00361396
Iteration 11/25 | Loss: 0.00361396
Iteration 12/25 | Loss: 0.00361396
Iteration 13/25 | Loss: 0.00361396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0036139620933681726, 0.0036139620933681726, 0.0036139620933681726, 0.0036139620933681726, 0.0036139620933681726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036139620933681726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361396
Iteration 2/1000 | Loss: 0.00038198
Iteration 3/1000 | Loss: 0.00041411
Iteration 4/1000 | Loss: 0.00042786
Iteration 5/1000 | Loss: 0.00041329
Iteration 6/1000 | Loss: 0.00014666
Iteration 7/1000 | Loss: 0.00047197
Iteration 8/1000 | Loss: 0.00013783
Iteration 9/1000 | Loss: 0.00012448
Iteration 10/1000 | Loss: 0.00011449
Iteration 11/1000 | Loss: 0.00010728
Iteration 12/1000 | Loss: 0.00010319
Iteration 13/1000 | Loss: 0.00010053
Iteration 14/1000 | Loss: 0.00009873
Iteration 15/1000 | Loss: 0.00009678
Iteration 16/1000 | Loss: 0.00009549
Iteration 17/1000 | Loss: 0.00013009
Iteration 18/1000 | Loss: 0.00028365
Iteration 19/1000 | Loss: 0.00010215
Iteration 20/1000 | Loss: 0.00009828
Iteration 21/1000 | Loss: 0.00031509
Iteration 22/1000 | Loss: 0.00027371
Iteration 23/1000 | Loss: 0.00031925
Iteration 24/1000 | Loss: 0.00010870
Iteration 25/1000 | Loss: 0.00010130
Iteration 26/1000 | Loss: 0.00009752
Iteration 27/1000 | Loss: 0.00009586
Iteration 28/1000 | Loss: 0.00009434
Iteration 29/1000 | Loss: 0.00032245
Iteration 30/1000 | Loss: 0.00009528
Iteration 31/1000 | Loss: 0.00009300
Iteration 32/1000 | Loss: 0.00034701
Iteration 33/1000 | Loss: 0.00030947
Iteration 34/1000 | Loss: 0.00009873
Iteration 35/1000 | Loss: 0.00009155
Iteration 36/1000 | Loss: 0.00008897
Iteration 37/1000 | Loss: 0.00008761
Iteration 38/1000 | Loss: 0.00008648
Iteration 39/1000 | Loss: 0.00008557
Iteration 40/1000 | Loss: 0.00008497
Iteration 41/1000 | Loss: 0.00008431
Iteration 42/1000 | Loss: 0.00008393
Iteration 43/1000 | Loss: 0.00008367
Iteration 44/1000 | Loss: 0.00008342
Iteration 45/1000 | Loss: 0.00008326
Iteration 46/1000 | Loss: 0.00008305
Iteration 47/1000 | Loss: 0.00008289
Iteration 48/1000 | Loss: 0.00008274
Iteration 49/1000 | Loss: 0.00008269
Iteration 50/1000 | Loss: 0.00008269
Iteration 51/1000 | Loss: 0.00008263
Iteration 52/1000 | Loss: 0.00008259
Iteration 53/1000 | Loss: 0.00008258
Iteration 54/1000 | Loss: 0.00008256
Iteration 55/1000 | Loss: 0.00008253
Iteration 56/1000 | Loss: 0.00008252
Iteration 57/1000 | Loss: 0.00008248
Iteration 58/1000 | Loss: 0.00008246
Iteration 59/1000 | Loss: 0.00008245
Iteration 60/1000 | Loss: 0.00008244
Iteration 61/1000 | Loss: 0.00008242
Iteration 62/1000 | Loss: 0.00008242
Iteration 63/1000 | Loss: 0.00008241
Iteration 64/1000 | Loss: 0.00008240
Iteration 65/1000 | Loss: 0.00008240
Iteration 66/1000 | Loss: 0.00008239
Iteration 67/1000 | Loss: 0.00008238
Iteration 68/1000 | Loss: 0.00008238
Iteration 69/1000 | Loss: 0.00008237
Iteration 70/1000 | Loss: 0.00008237
Iteration 71/1000 | Loss: 0.00008236
Iteration 72/1000 | Loss: 0.00008235
Iteration 73/1000 | Loss: 0.00008234
Iteration 74/1000 | Loss: 0.00008233
Iteration 75/1000 | Loss: 0.00008233
Iteration 76/1000 | Loss: 0.00008232
Iteration 77/1000 | Loss: 0.00008232
Iteration 78/1000 | Loss: 0.00008232
Iteration 79/1000 | Loss: 0.00008230
Iteration 80/1000 | Loss: 0.00008230
Iteration 81/1000 | Loss: 0.00008229
Iteration 82/1000 | Loss: 0.00008229
Iteration 83/1000 | Loss: 0.00031233
Iteration 84/1000 | Loss: 0.00025452
Iteration 85/1000 | Loss: 0.00009598
Iteration 86/1000 | Loss: 0.00008506
Iteration 87/1000 | Loss: 0.00008275
Iteration 88/1000 | Loss: 0.00008235
Iteration 89/1000 | Loss: 0.00031194
Iteration 90/1000 | Loss: 0.00086284
Iteration 91/1000 | Loss: 0.00012361
Iteration 92/1000 | Loss: 0.00009081
Iteration 93/1000 | Loss: 0.00008375
Iteration 94/1000 | Loss: 0.00008262
Iteration 95/1000 | Loss: 0.00008154
Iteration 96/1000 | Loss: 0.00008097
Iteration 97/1000 | Loss: 0.00008051
Iteration 98/1000 | Loss: 0.00008028
Iteration 99/1000 | Loss: 0.00008005
Iteration 100/1000 | Loss: 0.00007999
Iteration 101/1000 | Loss: 0.00007994
Iteration 102/1000 | Loss: 0.00007989
Iteration 103/1000 | Loss: 0.00007988
Iteration 104/1000 | Loss: 0.00007988
Iteration 105/1000 | Loss: 0.00007988
Iteration 106/1000 | Loss: 0.00007988
Iteration 107/1000 | Loss: 0.00007988
Iteration 108/1000 | Loss: 0.00007988
Iteration 109/1000 | Loss: 0.00007988
Iteration 110/1000 | Loss: 0.00007988
Iteration 111/1000 | Loss: 0.00007988
Iteration 112/1000 | Loss: 0.00007988
Iteration 113/1000 | Loss: 0.00007987
Iteration 114/1000 | Loss: 0.00007987
Iteration 115/1000 | Loss: 0.00007987
Iteration 116/1000 | Loss: 0.00007987
Iteration 117/1000 | Loss: 0.00007987
Iteration 118/1000 | Loss: 0.00007987
Iteration 119/1000 | Loss: 0.00007987
Iteration 120/1000 | Loss: 0.00007987
Iteration 121/1000 | Loss: 0.00007987
Iteration 122/1000 | Loss: 0.00007987
Iteration 123/1000 | Loss: 0.00007987
Iteration 124/1000 | Loss: 0.00007986
Iteration 125/1000 | Loss: 0.00007986
Iteration 126/1000 | Loss: 0.00007986
Iteration 127/1000 | Loss: 0.00007985
Iteration 128/1000 | Loss: 0.00007985
Iteration 129/1000 | Loss: 0.00007985
Iteration 130/1000 | Loss: 0.00007984
Iteration 131/1000 | Loss: 0.00007984
Iteration 132/1000 | Loss: 0.00007984
Iteration 133/1000 | Loss: 0.00007984
Iteration 134/1000 | Loss: 0.00007984
Iteration 135/1000 | Loss: 0.00007984
Iteration 136/1000 | Loss: 0.00007983
Iteration 137/1000 | Loss: 0.00007983
Iteration 138/1000 | Loss: 0.00007982
Iteration 139/1000 | Loss: 0.00007982
Iteration 140/1000 | Loss: 0.00007982
Iteration 141/1000 | Loss: 0.00007982
Iteration 142/1000 | Loss: 0.00007981
Iteration 143/1000 | Loss: 0.00007981
Iteration 144/1000 | Loss: 0.00007980
Iteration 145/1000 | Loss: 0.00007980
Iteration 146/1000 | Loss: 0.00007980
Iteration 147/1000 | Loss: 0.00007979
Iteration 148/1000 | Loss: 0.00007979
Iteration 149/1000 | Loss: 0.00007979
Iteration 150/1000 | Loss: 0.00007979
Iteration 151/1000 | Loss: 0.00007979
Iteration 152/1000 | Loss: 0.00007979
Iteration 153/1000 | Loss: 0.00007979
Iteration 154/1000 | Loss: 0.00007978
Iteration 155/1000 | Loss: 0.00007978
Iteration 156/1000 | Loss: 0.00007978
Iteration 157/1000 | Loss: 0.00007978
Iteration 158/1000 | Loss: 0.00007978
Iteration 159/1000 | Loss: 0.00007978
Iteration 160/1000 | Loss: 0.00007977
Iteration 161/1000 | Loss: 0.00007977
Iteration 162/1000 | Loss: 0.00007977
Iteration 163/1000 | Loss: 0.00007977
Iteration 164/1000 | Loss: 0.00007977
Iteration 165/1000 | Loss: 0.00007977
Iteration 166/1000 | Loss: 0.00007976
Iteration 167/1000 | Loss: 0.00007975
Iteration 168/1000 | Loss: 0.00007975
Iteration 169/1000 | Loss: 0.00007975
Iteration 170/1000 | Loss: 0.00007974
Iteration 171/1000 | Loss: 0.00007974
Iteration 172/1000 | Loss: 0.00007974
Iteration 173/1000 | Loss: 0.00007974
Iteration 174/1000 | Loss: 0.00007973
Iteration 175/1000 | Loss: 0.00007973
Iteration 176/1000 | Loss: 0.00007973
Iteration 177/1000 | Loss: 0.00007973
Iteration 178/1000 | Loss: 0.00007973
Iteration 179/1000 | Loss: 0.00007973
Iteration 180/1000 | Loss: 0.00007972
Iteration 181/1000 | Loss: 0.00007972
Iteration 182/1000 | Loss: 0.00007972
Iteration 183/1000 | Loss: 0.00007971
Iteration 184/1000 | Loss: 0.00007971
Iteration 185/1000 | Loss: 0.00007971
Iteration 186/1000 | Loss: 0.00007970
Iteration 187/1000 | Loss: 0.00007970
Iteration 188/1000 | Loss: 0.00007970
Iteration 189/1000 | Loss: 0.00007970
Iteration 190/1000 | Loss: 0.00007969
Iteration 191/1000 | Loss: 0.00007969
Iteration 192/1000 | Loss: 0.00007969
Iteration 193/1000 | Loss: 0.00007969
Iteration 194/1000 | Loss: 0.00007969
Iteration 195/1000 | Loss: 0.00007969
Iteration 196/1000 | Loss: 0.00007969
Iteration 197/1000 | Loss: 0.00007969
Iteration 198/1000 | Loss: 0.00007968
Iteration 199/1000 | Loss: 0.00007968
Iteration 200/1000 | Loss: 0.00007968
Iteration 201/1000 | Loss: 0.00007968
Iteration 202/1000 | Loss: 0.00007968
Iteration 203/1000 | Loss: 0.00007968
Iteration 204/1000 | Loss: 0.00007968
Iteration 205/1000 | Loss: 0.00007968
Iteration 206/1000 | Loss: 0.00007968
Iteration 207/1000 | Loss: 0.00007967
Iteration 208/1000 | Loss: 0.00007967
Iteration 209/1000 | Loss: 0.00007967
Iteration 210/1000 | Loss: 0.00007967
Iteration 211/1000 | Loss: 0.00007967
Iteration 212/1000 | Loss: 0.00007967
Iteration 213/1000 | Loss: 0.00007967
Iteration 214/1000 | Loss: 0.00007967
Iteration 215/1000 | Loss: 0.00007967
Iteration 216/1000 | Loss: 0.00007967
Iteration 217/1000 | Loss: 0.00007967
Iteration 218/1000 | Loss: 0.00007967
Iteration 219/1000 | Loss: 0.00007967
Iteration 220/1000 | Loss: 0.00007967
Iteration 221/1000 | Loss: 0.00007966
Iteration 222/1000 | Loss: 0.00007966
Iteration 223/1000 | Loss: 0.00007966
Iteration 224/1000 | Loss: 0.00007966
Iteration 225/1000 | Loss: 0.00007966
Iteration 226/1000 | Loss: 0.00007966
Iteration 227/1000 | Loss: 0.00007966
Iteration 228/1000 | Loss: 0.00007966
Iteration 229/1000 | Loss: 0.00007966
Iteration 230/1000 | Loss: 0.00007966
Iteration 231/1000 | Loss: 0.00007966
Iteration 232/1000 | Loss: 0.00007966
Iteration 233/1000 | Loss: 0.00007966
Iteration 234/1000 | Loss: 0.00007966
Iteration 235/1000 | Loss: 0.00007966
Iteration 236/1000 | Loss: 0.00007966
Iteration 237/1000 | Loss: 0.00007966
Iteration 238/1000 | Loss: 0.00007966
Iteration 239/1000 | Loss: 0.00007966
Iteration 240/1000 | Loss: 0.00007966
Iteration 241/1000 | Loss: 0.00007966
Iteration 242/1000 | Loss: 0.00007966
Iteration 243/1000 | Loss: 0.00007966
Iteration 244/1000 | Loss: 0.00007966
Iteration 245/1000 | Loss: 0.00007966
Iteration 246/1000 | Loss: 0.00007966
Iteration 247/1000 | Loss: 0.00007966
Iteration 248/1000 | Loss: 0.00007966
Iteration 249/1000 | Loss: 0.00007966
Iteration 250/1000 | Loss: 0.00007966
Iteration 251/1000 | Loss: 0.00007966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [7.966093835420907e-05, 7.966093835420907e-05, 7.966093835420907e-05, 7.966093835420907e-05, 7.966093835420907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.966093835420907e-05

Optimization complete. Final v2v error: 4.978547096252441 mm

Highest mean error: 11.705574989318848 mm for frame 51

Lowest mean error: 3.1682803630828857 mm for frame 5

Saving results

Total time: 144.22879815101624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777328
Iteration 2/25 | Loss: 0.00152079
Iteration 3/25 | Loss: 0.00140370
Iteration 4/25 | Loss: 0.00139056
Iteration 5/25 | Loss: 0.00138910
Iteration 6/25 | Loss: 0.00138910
Iteration 7/25 | Loss: 0.00138910
Iteration 8/25 | Loss: 0.00138910
Iteration 9/25 | Loss: 0.00138910
Iteration 10/25 | Loss: 0.00138910
Iteration 11/25 | Loss: 0.00138910
Iteration 12/25 | Loss: 0.00138910
Iteration 13/25 | Loss: 0.00138910
Iteration 14/25 | Loss: 0.00138910
Iteration 15/25 | Loss: 0.00138910
Iteration 16/25 | Loss: 0.00138910
Iteration 17/25 | Loss: 0.00138910
Iteration 18/25 | Loss: 0.00138910
Iteration 19/25 | Loss: 0.00138910
Iteration 20/25 | Loss: 0.00138910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013890981208533049, 0.0013890981208533049, 0.0013890981208533049, 0.0013890981208533049, 0.0013890981208533049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013890981208533049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32119441
Iteration 2/25 | Loss: 0.00200944
Iteration 3/25 | Loss: 0.00200942
Iteration 4/25 | Loss: 0.00200942
Iteration 5/25 | Loss: 0.00200942
Iteration 6/25 | Loss: 0.00200942
Iteration 7/25 | Loss: 0.00200942
Iteration 8/25 | Loss: 0.00200942
Iteration 9/25 | Loss: 0.00200942
Iteration 10/25 | Loss: 0.00200942
Iteration 11/25 | Loss: 0.00200942
Iteration 12/25 | Loss: 0.00200942
Iteration 13/25 | Loss: 0.00200942
Iteration 14/25 | Loss: 0.00200942
Iteration 15/25 | Loss: 0.00200942
Iteration 16/25 | Loss: 0.00200942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002009421354159713, 0.002009421354159713, 0.002009421354159713, 0.002009421354159713, 0.002009421354159713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002009421354159713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200942
Iteration 2/1000 | Loss: 0.00002346
Iteration 3/1000 | Loss: 0.00001744
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001430
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001262
Iteration 29/1000 | Loss: 0.00001261
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001260
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001235
Iteration 52/1000 | Loss: 0.00001235
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001232
Iteration 66/1000 | Loss: 0.00001232
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001231
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001229
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001226
Iteration 93/1000 | Loss: 0.00001226
Iteration 94/1000 | Loss: 0.00001226
Iteration 95/1000 | Loss: 0.00001226
Iteration 96/1000 | Loss: 0.00001226
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.2262260497664101e-05, 1.2262260497664101e-05, 1.2262260497664101e-05, 1.2262260497664101e-05, 1.2262260497664101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2262260497664101e-05

Optimization complete. Final v2v error: 2.964763641357422 mm

Highest mean error: 3.4097297191619873 mm for frame 81

Lowest mean error: 2.649596691131592 mm for frame 47

Saving results

Total time: 37.364421129226685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013153
Iteration 2/25 | Loss: 0.00275308
Iteration 3/25 | Loss: 0.00248014
Iteration 4/25 | Loss: 0.00197240
Iteration 5/25 | Loss: 0.00184464
Iteration 6/25 | Loss: 0.00177136
Iteration 7/25 | Loss: 0.00163824
Iteration 8/25 | Loss: 0.00161153
Iteration 9/25 | Loss: 0.00157074
Iteration 10/25 | Loss: 0.00151339
Iteration 11/25 | Loss: 0.00154786
Iteration 12/25 | Loss: 0.00153014
Iteration 13/25 | Loss: 0.00150887
Iteration 14/25 | Loss: 0.00150280
Iteration 15/25 | Loss: 0.00149618
Iteration 16/25 | Loss: 0.00149231
Iteration 17/25 | Loss: 0.00149627
Iteration 18/25 | Loss: 0.00149166
Iteration 19/25 | Loss: 0.00148089
Iteration 20/25 | Loss: 0.00148000
Iteration 21/25 | Loss: 0.00148137
Iteration 22/25 | Loss: 0.00148075
Iteration 23/25 | Loss: 0.00147986
Iteration 24/25 | Loss: 0.00147865
Iteration 25/25 | Loss: 0.00147848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24519455
Iteration 2/25 | Loss: 0.00223913
Iteration 3/25 | Loss: 0.00214842
Iteration 4/25 | Loss: 0.00214842
Iteration 5/25 | Loss: 0.00214841
Iteration 6/25 | Loss: 0.00214841
Iteration 7/25 | Loss: 0.00214841
Iteration 8/25 | Loss: 0.00214841
Iteration 9/25 | Loss: 0.00214841
Iteration 10/25 | Loss: 0.00214841
Iteration 11/25 | Loss: 0.00214841
Iteration 12/25 | Loss: 0.00214841
Iteration 13/25 | Loss: 0.00214841
Iteration 14/25 | Loss: 0.00214841
Iteration 15/25 | Loss: 0.00214841
Iteration 16/25 | Loss: 0.00214841
Iteration 17/25 | Loss: 0.00214841
Iteration 18/25 | Loss: 0.00214841
Iteration 19/25 | Loss: 0.00214841
Iteration 20/25 | Loss: 0.00214841
Iteration 21/25 | Loss: 0.00214841
Iteration 22/25 | Loss: 0.00214841
Iteration 23/25 | Loss: 0.00214841
Iteration 24/25 | Loss: 0.00214841
Iteration 25/25 | Loss: 0.00214841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214841
Iteration 2/1000 | Loss: 0.00051184
Iteration 3/1000 | Loss: 0.00017718
Iteration 4/1000 | Loss: 0.00036887
Iteration 5/1000 | Loss: 0.00008256
Iteration 6/1000 | Loss: 0.00009301
Iteration 7/1000 | Loss: 0.00005253
Iteration 8/1000 | Loss: 0.00007048
Iteration 9/1000 | Loss: 0.00004499
Iteration 10/1000 | Loss: 0.00016507
Iteration 11/1000 | Loss: 0.00016504
Iteration 12/1000 | Loss: 0.00005650
Iteration 13/1000 | Loss: 0.00004631
Iteration 14/1000 | Loss: 0.00004183
Iteration 15/1000 | Loss: 0.00004085
Iteration 16/1000 | Loss: 0.00013681
Iteration 17/1000 | Loss: 0.00012729
Iteration 18/1000 | Loss: 0.00004098
Iteration 19/1000 | Loss: 0.00015772
Iteration 20/1000 | Loss: 0.00005507
Iteration 21/1000 | Loss: 0.00006234
Iteration 22/1000 | Loss: 0.00028808
Iteration 23/1000 | Loss: 0.00018244
Iteration 24/1000 | Loss: 0.00020837
Iteration 25/1000 | Loss: 0.00018288
Iteration 26/1000 | Loss: 0.00011359
Iteration 27/1000 | Loss: 0.00018679
Iteration 28/1000 | Loss: 0.00008111
Iteration 29/1000 | Loss: 0.00004126
Iteration 30/1000 | Loss: 0.00003637
Iteration 31/1000 | Loss: 0.00005528
Iteration 32/1000 | Loss: 0.00003477
Iteration 33/1000 | Loss: 0.00003428
Iteration 34/1000 | Loss: 0.00006673
Iteration 35/1000 | Loss: 0.00004424
Iteration 36/1000 | Loss: 0.00003367
Iteration 37/1000 | Loss: 0.00003351
Iteration 38/1000 | Loss: 0.00003324
Iteration 39/1000 | Loss: 0.00003305
Iteration 40/1000 | Loss: 0.00003303
Iteration 41/1000 | Loss: 0.00007569
Iteration 42/1000 | Loss: 0.00016254
Iteration 43/1000 | Loss: 0.00015962
Iteration 44/1000 | Loss: 0.00009314
Iteration 45/1000 | Loss: 0.00004517
Iteration 46/1000 | Loss: 0.00003302
Iteration 47/1000 | Loss: 0.00003990
Iteration 48/1000 | Loss: 0.00003290
Iteration 49/1000 | Loss: 0.00003287
Iteration 50/1000 | Loss: 0.00003287
Iteration 51/1000 | Loss: 0.00003287
Iteration 52/1000 | Loss: 0.00003286
Iteration 53/1000 | Loss: 0.00003286
Iteration 54/1000 | Loss: 0.00003286
Iteration 55/1000 | Loss: 0.00003286
Iteration 56/1000 | Loss: 0.00003285
Iteration 57/1000 | Loss: 0.00003285
Iteration 58/1000 | Loss: 0.00003284
Iteration 59/1000 | Loss: 0.00003284
Iteration 60/1000 | Loss: 0.00003283
Iteration 61/1000 | Loss: 0.00003283
Iteration 62/1000 | Loss: 0.00003283
Iteration 63/1000 | Loss: 0.00003283
Iteration 64/1000 | Loss: 0.00003283
Iteration 65/1000 | Loss: 0.00003282
Iteration 66/1000 | Loss: 0.00003282
Iteration 67/1000 | Loss: 0.00003282
Iteration 68/1000 | Loss: 0.00003281
Iteration 69/1000 | Loss: 0.00003281
Iteration 70/1000 | Loss: 0.00003281
Iteration 71/1000 | Loss: 0.00003280
Iteration 72/1000 | Loss: 0.00003279
Iteration 73/1000 | Loss: 0.00003279
Iteration 74/1000 | Loss: 0.00003279
Iteration 75/1000 | Loss: 0.00003279
Iteration 76/1000 | Loss: 0.00003279
Iteration 77/1000 | Loss: 0.00003278
Iteration 78/1000 | Loss: 0.00003278
Iteration 79/1000 | Loss: 0.00003278
Iteration 80/1000 | Loss: 0.00003278
Iteration 81/1000 | Loss: 0.00003278
Iteration 82/1000 | Loss: 0.00003278
Iteration 83/1000 | Loss: 0.00003277
Iteration 84/1000 | Loss: 0.00003277
Iteration 85/1000 | Loss: 0.00003277
Iteration 86/1000 | Loss: 0.00003277
Iteration 87/1000 | Loss: 0.00003277
Iteration 88/1000 | Loss: 0.00003277
Iteration 89/1000 | Loss: 0.00003277
Iteration 90/1000 | Loss: 0.00003276
Iteration 91/1000 | Loss: 0.00003276
Iteration 92/1000 | Loss: 0.00003276
Iteration 93/1000 | Loss: 0.00003276
Iteration 94/1000 | Loss: 0.00003276
Iteration 95/1000 | Loss: 0.00003275
Iteration 96/1000 | Loss: 0.00003275
Iteration 97/1000 | Loss: 0.00003275
Iteration 98/1000 | Loss: 0.00003275
Iteration 99/1000 | Loss: 0.00003275
Iteration 100/1000 | Loss: 0.00003274
Iteration 101/1000 | Loss: 0.00003274
Iteration 102/1000 | Loss: 0.00003274
Iteration 103/1000 | Loss: 0.00015603
Iteration 104/1000 | Loss: 0.00003929
Iteration 105/1000 | Loss: 0.00003279
Iteration 106/1000 | Loss: 0.00005693
Iteration 107/1000 | Loss: 0.00003279
Iteration 108/1000 | Loss: 0.00003273
Iteration 109/1000 | Loss: 0.00003273
Iteration 110/1000 | Loss: 0.00003273
Iteration 111/1000 | Loss: 0.00003273
Iteration 112/1000 | Loss: 0.00003273
Iteration 113/1000 | Loss: 0.00003272
Iteration 114/1000 | Loss: 0.00003272
Iteration 115/1000 | Loss: 0.00003272
Iteration 116/1000 | Loss: 0.00003272
Iteration 117/1000 | Loss: 0.00003272
Iteration 118/1000 | Loss: 0.00003272
Iteration 119/1000 | Loss: 0.00003272
Iteration 120/1000 | Loss: 0.00003271
Iteration 121/1000 | Loss: 0.00003271
Iteration 122/1000 | Loss: 0.00003270
Iteration 123/1000 | Loss: 0.00003270
Iteration 124/1000 | Loss: 0.00003270
Iteration 125/1000 | Loss: 0.00003269
Iteration 126/1000 | Loss: 0.00003269
Iteration 127/1000 | Loss: 0.00003268
Iteration 128/1000 | Loss: 0.00003268
Iteration 129/1000 | Loss: 0.00003268
Iteration 130/1000 | Loss: 0.00003268
Iteration 131/1000 | Loss: 0.00003267
Iteration 132/1000 | Loss: 0.00003267
Iteration 133/1000 | Loss: 0.00003267
Iteration 134/1000 | Loss: 0.00003267
Iteration 135/1000 | Loss: 0.00003267
Iteration 136/1000 | Loss: 0.00003267
Iteration 137/1000 | Loss: 0.00003267
Iteration 138/1000 | Loss: 0.00003266
Iteration 139/1000 | Loss: 0.00003266
Iteration 140/1000 | Loss: 0.00003265
Iteration 141/1000 | Loss: 0.00003265
Iteration 142/1000 | Loss: 0.00003265
Iteration 143/1000 | Loss: 0.00003265
Iteration 144/1000 | Loss: 0.00003264
Iteration 145/1000 | Loss: 0.00003264
Iteration 146/1000 | Loss: 0.00003264
Iteration 147/1000 | Loss: 0.00003264
Iteration 148/1000 | Loss: 0.00003263
Iteration 149/1000 | Loss: 0.00003262
Iteration 150/1000 | Loss: 0.00003262
Iteration 151/1000 | Loss: 0.00003262
Iteration 152/1000 | Loss: 0.00003262
Iteration 153/1000 | Loss: 0.00003262
Iteration 154/1000 | Loss: 0.00003261
Iteration 155/1000 | Loss: 0.00003261
Iteration 156/1000 | Loss: 0.00003261
Iteration 157/1000 | Loss: 0.00003261
Iteration 158/1000 | Loss: 0.00003260
Iteration 159/1000 | Loss: 0.00003260
Iteration 160/1000 | Loss: 0.00003260
Iteration 161/1000 | Loss: 0.00003260
Iteration 162/1000 | Loss: 0.00003260
Iteration 163/1000 | Loss: 0.00003259
Iteration 164/1000 | Loss: 0.00003259
Iteration 165/1000 | Loss: 0.00003259
Iteration 166/1000 | Loss: 0.00003259
Iteration 167/1000 | Loss: 0.00003259
Iteration 168/1000 | Loss: 0.00003259
Iteration 169/1000 | Loss: 0.00003259
Iteration 170/1000 | Loss: 0.00003259
Iteration 171/1000 | Loss: 0.00003259
Iteration 172/1000 | Loss: 0.00003258
Iteration 173/1000 | Loss: 0.00003258
Iteration 174/1000 | Loss: 0.00003258
Iteration 175/1000 | Loss: 0.00003258
Iteration 176/1000 | Loss: 0.00003258
Iteration 177/1000 | Loss: 0.00003258
Iteration 178/1000 | Loss: 0.00003258
Iteration 179/1000 | Loss: 0.00003258
Iteration 180/1000 | Loss: 0.00003258
Iteration 181/1000 | Loss: 0.00003258
Iteration 182/1000 | Loss: 0.00003258
Iteration 183/1000 | Loss: 0.00003258
Iteration 184/1000 | Loss: 0.00003258
Iteration 185/1000 | Loss: 0.00003258
Iteration 186/1000 | Loss: 0.00003258
Iteration 187/1000 | Loss: 0.00003258
Iteration 188/1000 | Loss: 0.00003257
Iteration 189/1000 | Loss: 0.00003257
Iteration 190/1000 | Loss: 0.00003257
Iteration 191/1000 | Loss: 0.00003257
Iteration 192/1000 | Loss: 0.00003257
Iteration 193/1000 | Loss: 0.00003257
Iteration 194/1000 | Loss: 0.00003257
Iteration 195/1000 | Loss: 0.00003257
Iteration 196/1000 | Loss: 0.00003257
Iteration 197/1000 | Loss: 0.00003257
Iteration 198/1000 | Loss: 0.00003257
Iteration 199/1000 | Loss: 0.00003257
Iteration 200/1000 | Loss: 0.00003257
Iteration 201/1000 | Loss: 0.00003257
Iteration 202/1000 | Loss: 0.00003257
Iteration 203/1000 | Loss: 0.00003257
Iteration 204/1000 | Loss: 0.00003257
Iteration 205/1000 | Loss: 0.00003257
Iteration 206/1000 | Loss: 0.00003257
Iteration 207/1000 | Loss: 0.00003257
Iteration 208/1000 | Loss: 0.00003257
Iteration 209/1000 | Loss: 0.00003257
Iteration 210/1000 | Loss: 0.00003257
Iteration 211/1000 | Loss: 0.00003257
Iteration 212/1000 | Loss: 0.00003257
Iteration 213/1000 | Loss: 0.00003257
Iteration 214/1000 | Loss: 0.00003257
Iteration 215/1000 | Loss: 0.00003257
Iteration 216/1000 | Loss: 0.00003257
Iteration 217/1000 | Loss: 0.00003257
Iteration 218/1000 | Loss: 0.00003257
Iteration 219/1000 | Loss: 0.00003257
Iteration 220/1000 | Loss: 0.00003257
Iteration 221/1000 | Loss: 0.00003257
Iteration 222/1000 | Loss: 0.00003257
Iteration 223/1000 | Loss: 0.00003257
Iteration 224/1000 | Loss: 0.00003257
Iteration 225/1000 | Loss: 0.00003257
Iteration 226/1000 | Loss: 0.00003257
Iteration 227/1000 | Loss: 0.00003257
Iteration 228/1000 | Loss: 0.00003257
Iteration 229/1000 | Loss: 0.00003257
Iteration 230/1000 | Loss: 0.00003257
Iteration 231/1000 | Loss: 0.00003257
Iteration 232/1000 | Loss: 0.00003257
Iteration 233/1000 | Loss: 0.00003257
Iteration 234/1000 | Loss: 0.00003257
Iteration 235/1000 | Loss: 0.00003257
Iteration 236/1000 | Loss: 0.00003257
Iteration 237/1000 | Loss: 0.00003257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [3.256936906836927e-05, 3.256936906836927e-05, 3.256936906836927e-05, 3.256936906836927e-05, 3.256936906836927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.256936906836927e-05

Optimization complete. Final v2v error: 4.0389180183410645 mm

Highest mean error: 11.766220092773438 mm for frame 215

Lowest mean error: 3.1430811882019043 mm for frame 153

Saving results

Total time: 144.98223662376404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848591
Iteration 2/25 | Loss: 0.00235603
Iteration 3/25 | Loss: 0.00202305
Iteration 4/25 | Loss: 0.00190331
Iteration 5/25 | Loss: 0.00173111
Iteration 6/25 | Loss: 0.00159084
Iteration 7/25 | Loss: 0.00156358
Iteration 8/25 | Loss: 0.00154542
Iteration 9/25 | Loss: 0.00154103
Iteration 10/25 | Loss: 0.00153796
Iteration 11/25 | Loss: 0.00153626
Iteration 12/25 | Loss: 0.00153665
Iteration 13/25 | Loss: 0.00153626
Iteration 14/25 | Loss: 0.00153643
Iteration 15/25 | Loss: 0.00153585
Iteration 16/25 | Loss: 0.00153626
Iteration 17/25 | Loss: 0.00153625
Iteration 18/25 | Loss: 0.00153625
Iteration 19/25 | Loss: 0.00153625
Iteration 20/25 | Loss: 0.00153625
Iteration 21/25 | Loss: 0.00153612
Iteration 22/25 | Loss: 0.00153617
Iteration 23/25 | Loss: 0.00153660
Iteration 24/25 | Loss: 0.00153596
Iteration 25/25 | Loss: 0.00153455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21675861
Iteration 2/25 | Loss: 0.00157172
Iteration 3/25 | Loss: 0.00157169
Iteration 4/25 | Loss: 0.00157169
Iteration 5/25 | Loss: 0.00157169
Iteration 6/25 | Loss: 0.00157169
Iteration 7/25 | Loss: 0.00157169
Iteration 8/25 | Loss: 0.00157169
Iteration 9/25 | Loss: 0.00157169
Iteration 10/25 | Loss: 0.00157169
Iteration 11/25 | Loss: 0.00157169
Iteration 12/25 | Loss: 0.00157169
Iteration 13/25 | Loss: 0.00157169
Iteration 14/25 | Loss: 0.00157169
Iteration 15/25 | Loss: 0.00157169
Iteration 16/25 | Loss: 0.00157169
Iteration 17/25 | Loss: 0.00157169
Iteration 18/25 | Loss: 0.00157169
Iteration 19/25 | Loss: 0.00157169
Iteration 20/25 | Loss: 0.00157169
Iteration 21/25 | Loss: 0.00157169
Iteration 22/25 | Loss: 0.00157169
Iteration 23/25 | Loss: 0.00157169
Iteration 24/25 | Loss: 0.00157169
Iteration 25/25 | Loss: 0.00157169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157169
Iteration 2/1000 | Loss: 0.00004240
Iteration 3/1000 | Loss: 0.00002838
Iteration 4/1000 | Loss: 0.00002539
Iteration 5/1000 | Loss: 0.00002405
Iteration 6/1000 | Loss: 0.00002319
Iteration 7/1000 | Loss: 0.00002258
Iteration 8/1000 | Loss: 0.00002233
Iteration 9/1000 | Loss: 0.00002200
Iteration 10/1000 | Loss: 0.00002179
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002148
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002144
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002143
Iteration 17/1000 | Loss: 0.00002141
Iteration 18/1000 | Loss: 0.00002140
Iteration 19/1000 | Loss: 0.00002140
Iteration 20/1000 | Loss: 0.00002139
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00002133
Iteration 23/1000 | Loss: 0.00002131
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00002129
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002128
Iteration 30/1000 | Loss: 0.00002128
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002128
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002127
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002126
Iteration 41/1000 | Loss: 0.00002125
Iteration 42/1000 | Loss: 0.00002122
Iteration 43/1000 | Loss: 0.00002122
Iteration 44/1000 | Loss: 0.00002122
Iteration 45/1000 | Loss: 0.00002122
Iteration 46/1000 | Loss: 0.00002122
Iteration 47/1000 | Loss: 0.00002122
Iteration 48/1000 | Loss: 0.00002122
Iteration 49/1000 | Loss: 0.00002122
Iteration 50/1000 | Loss: 0.00002122
Iteration 51/1000 | Loss: 0.00002122
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002119
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002114
Iteration 68/1000 | Loss: 0.00002114
Iteration 69/1000 | Loss: 0.00002114
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00002112
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002111
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002110
Iteration 84/1000 | Loss: 0.00002110
Iteration 85/1000 | Loss: 0.00002110
Iteration 86/1000 | Loss: 0.00002110
Iteration 87/1000 | Loss: 0.00002110
Iteration 88/1000 | Loss: 0.00002110
Iteration 89/1000 | Loss: 0.00002110
Iteration 90/1000 | Loss: 0.00002110
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002109
Iteration 93/1000 | Loss: 0.00002109
Iteration 94/1000 | Loss: 0.00002109
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002109
Iteration 99/1000 | Loss: 0.00002109
Iteration 100/1000 | Loss: 0.00002108
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002107
Iteration 105/1000 | Loss: 0.00002107
Iteration 106/1000 | Loss: 0.00002107
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002107
Iteration 109/1000 | Loss: 0.00002107
Iteration 110/1000 | Loss: 0.00002107
Iteration 111/1000 | Loss: 0.00002107
Iteration 112/1000 | Loss: 0.00002106
Iteration 113/1000 | Loss: 0.00002106
Iteration 114/1000 | Loss: 0.00002106
Iteration 115/1000 | Loss: 0.00002106
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002105
Iteration 119/1000 | Loss: 0.00002105
Iteration 120/1000 | Loss: 0.00002105
Iteration 121/1000 | Loss: 0.00002105
Iteration 122/1000 | Loss: 0.00002105
Iteration 123/1000 | Loss: 0.00002105
Iteration 124/1000 | Loss: 0.00002104
Iteration 125/1000 | Loss: 0.00002104
Iteration 126/1000 | Loss: 0.00002104
Iteration 127/1000 | Loss: 0.00002104
Iteration 128/1000 | Loss: 0.00002104
Iteration 129/1000 | Loss: 0.00002103
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002103
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002102
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002101
Iteration 147/1000 | Loss: 0.00002101
Iteration 148/1000 | Loss: 0.00002101
Iteration 149/1000 | Loss: 0.00002101
Iteration 150/1000 | Loss: 0.00002101
Iteration 151/1000 | Loss: 0.00002101
Iteration 152/1000 | Loss: 0.00002101
Iteration 153/1000 | Loss: 0.00002101
Iteration 154/1000 | Loss: 0.00002101
Iteration 155/1000 | Loss: 0.00002101
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002101
Iteration 158/1000 | Loss: 0.00002101
Iteration 159/1000 | Loss: 0.00002101
Iteration 160/1000 | Loss: 0.00002101
Iteration 161/1000 | Loss: 0.00002101
Iteration 162/1000 | Loss: 0.00002101
Iteration 163/1000 | Loss: 0.00002101
Iteration 164/1000 | Loss: 0.00002101
Iteration 165/1000 | Loss: 0.00002101
Iteration 166/1000 | Loss: 0.00002101
Iteration 167/1000 | Loss: 0.00002101
Iteration 168/1000 | Loss: 0.00002101
Iteration 169/1000 | Loss: 0.00002101
Iteration 170/1000 | Loss: 0.00002101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.1006928363931365e-05, 2.1006928363931365e-05, 2.1006928363931365e-05, 2.1006928363931365e-05, 2.1006928363931365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1006928363931365e-05

Optimization complete. Final v2v error: 3.910421848297119 mm

Highest mean error: 4.446750164031982 mm for frame 99

Lowest mean error: 3.721010208129883 mm for frame 114

Saving results

Total time: 74.7277557849884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038031
Iteration 2/25 | Loss: 0.00151964
Iteration 3/25 | Loss: 0.00138347
Iteration 4/25 | Loss: 0.00136589
Iteration 5/25 | Loss: 0.00136360
Iteration 6/25 | Loss: 0.00136360
Iteration 7/25 | Loss: 0.00136360
Iteration 8/25 | Loss: 0.00136360
Iteration 9/25 | Loss: 0.00136360
Iteration 10/25 | Loss: 0.00136360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013635973446071148, 0.0013635973446071148, 0.0013635973446071148, 0.0013635973446071148, 0.0013635973446071148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013635973446071148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03341746
Iteration 2/25 | Loss: 0.00232041
Iteration 3/25 | Loss: 0.00232041
Iteration 4/25 | Loss: 0.00232041
Iteration 5/25 | Loss: 0.00232041
Iteration 6/25 | Loss: 0.00232041
Iteration 7/25 | Loss: 0.00232041
Iteration 8/25 | Loss: 0.00232041
Iteration 9/25 | Loss: 0.00232041
Iteration 10/25 | Loss: 0.00232041
Iteration 11/25 | Loss: 0.00232041
Iteration 12/25 | Loss: 0.00232041
Iteration 13/25 | Loss: 0.00232041
Iteration 14/25 | Loss: 0.00232041
Iteration 15/25 | Loss: 0.00232041
Iteration 16/25 | Loss: 0.00232041
Iteration 17/25 | Loss: 0.00232041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002320407424122095, 0.002320407424122095, 0.002320407424122095, 0.002320407424122095, 0.002320407424122095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002320407424122095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232041
Iteration 2/1000 | Loss: 0.00003293
Iteration 3/1000 | Loss: 0.00002132
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00001692
Iteration 6/1000 | Loss: 0.00001549
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001367
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001317
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001226
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001208
Iteration 37/1000 | Loss: 0.00001208
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001191
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001189
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001186
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001184
Iteration 82/1000 | Loss: 0.00001184
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001179
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001177
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001172
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001167
Iteration 139/1000 | Loss: 0.00001167
Iteration 140/1000 | Loss: 0.00001167
Iteration 141/1000 | Loss: 0.00001167
Iteration 142/1000 | Loss: 0.00001167
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001165
Iteration 153/1000 | Loss: 0.00001165
Iteration 154/1000 | Loss: 0.00001165
Iteration 155/1000 | Loss: 0.00001165
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001162
Iteration 162/1000 | Loss: 0.00001162
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001162
Iteration 165/1000 | Loss: 0.00001162
Iteration 166/1000 | Loss: 0.00001161
Iteration 167/1000 | Loss: 0.00001161
Iteration 168/1000 | Loss: 0.00001161
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001161
Iteration 171/1000 | Loss: 0.00001161
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001159
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001159
Iteration 181/1000 | Loss: 0.00001159
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001158
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Iteration 205/1000 | Loss: 0.00001158
Iteration 206/1000 | Loss: 0.00001158
Iteration 207/1000 | Loss: 0.00001158
Iteration 208/1000 | Loss: 0.00001157
Iteration 209/1000 | Loss: 0.00001157
Iteration 210/1000 | Loss: 0.00001157
Iteration 211/1000 | Loss: 0.00001157
Iteration 212/1000 | Loss: 0.00001157
Iteration 213/1000 | Loss: 0.00001157
Iteration 214/1000 | Loss: 0.00001156
Iteration 215/1000 | Loss: 0.00001156
Iteration 216/1000 | Loss: 0.00001156
Iteration 217/1000 | Loss: 0.00001156
Iteration 218/1000 | Loss: 0.00001156
Iteration 219/1000 | Loss: 0.00001156
Iteration 220/1000 | Loss: 0.00001156
Iteration 221/1000 | Loss: 0.00001156
Iteration 222/1000 | Loss: 0.00001156
Iteration 223/1000 | Loss: 0.00001156
Iteration 224/1000 | Loss: 0.00001156
Iteration 225/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1561021892703138e-05, 1.1561021892703138e-05, 1.1561021892703138e-05, 1.1561021892703138e-05, 1.1561021892703138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1561021892703138e-05

Optimization complete. Final v2v error: 2.9373362064361572 mm

Highest mean error: 3.103834867477417 mm for frame 0

Lowest mean error: 2.82133412361145 mm for frame 239

Saving results

Total time: 52.30620765686035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806499
Iteration 2/25 | Loss: 0.00143616
Iteration 3/25 | Loss: 0.00135742
Iteration 4/25 | Loss: 0.00134848
Iteration 5/25 | Loss: 0.00134725
Iteration 6/25 | Loss: 0.00134725
Iteration 7/25 | Loss: 0.00134725
Iteration 8/25 | Loss: 0.00134725
Iteration 9/25 | Loss: 0.00134725
Iteration 10/25 | Loss: 0.00134725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013472466962412, 0.0013472466962412, 0.0013472466962412, 0.0013472466962412, 0.0013472466962412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013472466962412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24745250
Iteration 2/25 | Loss: 0.00211212
Iteration 3/25 | Loss: 0.00211212
Iteration 4/25 | Loss: 0.00211212
Iteration 5/25 | Loss: 0.00211212
Iteration 6/25 | Loss: 0.00211212
Iteration 7/25 | Loss: 0.00211212
Iteration 8/25 | Loss: 0.00211212
Iteration 9/25 | Loss: 0.00211211
Iteration 10/25 | Loss: 0.00211211
Iteration 11/25 | Loss: 0.00211211
Iteration 12/25 | Loss: 0.00211211
Iteration 13/25 | Loss: 0.00211211
Iteration 14/25 | Loss: 0.00211211
Iteration 15/25 | Loss: 0.00211211
Iteration 16/25 | Loss: 0.00211211
Iteration 17/25 | Loss: 0.00211211
Iteration 18/25 | Loss: 0.00211211
Iteration 19/25 | Loss: 0.00211211
Iteration 20/25 | Loss: 0.00211211
Iteration 21/25 | Loss: 0.00211211
Iteration 22/25 | Loss: 0.00211211
Iteration 23/25 | Loss: 0.00211211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002112112706527114, 0.002112112706527114, 0.002112112706527114, 0.002112112706527114, 0.002112112706527114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002112112706527114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211211
Iteration 2/1000 | Loss: 0.00002098
Iteration 3/1000 | Loss: 0.00001625
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001241
Iteration 8/1000 | Loss: 0.00001208
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001147
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001119
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001086
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001064
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001059
Iteration 29/1000 | Loss: 0.00001058
Iteration 30/1000 | Loss: 0.00001057
Iteration 31/1000 | Loss: 0.00001057
Iteration 32/1000 | Loss: 0.00001054
Iteration 33/1000 | Loss: 0.00001053
Iteration 34/1000 | Loss: 0.00001052
Iteration 35/1000 | Loss: 0.00001052
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001043
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001042
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001035
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001034
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001033
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001032
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001031
Iteration 71/1000 | Loss: 0.00001031
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001026
Iteration 87/1000 | Loss: 0.00001026
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001025
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001023
Iteration 92/1000 | Loss: 0.00001022
Iteration 93/1000 | Loss: 0.00001022
Iteration 94/1000 | Loss: 0.00001022
Iteration 95/1000 | Loss: 0.00001021
Iteration 96/1000 | Loss: 0.00001021
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001018
Iteration 102/1000 | Loss: 0.00001018
Iteration 103/1000 | Loss: 0.00001018
Iteration 104/1000 | Loss: 0.00001018
Iteration 105/1000 | Loss: 0.00001018
Iteration 106/1000 | Loss: 0.00001018
Iteration 107/1000 | Loss: 0.00001018
Iteration 108/1000 | Loss: 0.00001017
Iteration 109/1000 | Loss: 0.00001017
Iteration 110/1000 | Loss: 0.00001017
Iteration 111/1000 | Loss: 0.00001017
Iteration 112/1000 | Loss: 0.00001017
Iteration 113/1000 | Loss: 0.00001017
Iteration 114/1000 | Loss: 0.00001017
Iteration 115/1000 | Loss: 0.00001017
Iteration 116/1000 | Loss: 0.00001017
Iteration 117/1000 | Loss: 0.00001017
Iteration 118/1000 | Loss: 0.00001017
Iteration 119/1000 | Loss: 0.00001016
Iteration 120/1000 | Loss: 0.00001016
Iteration 121/1000 | Loss: 0.00001016
Iteration 122/1000 | Loss: 0.00001016
Iteration 123/1000 | Loss: 0.00001016
Iteration 124/1000 | Loss: 0.00001015
Iteration 125/1000 | Loss: 0.00001015
Iteration 126/1000 | Loss: 0.00001015
Iteration 127/1000 | Loss: 0.00001014
Iteration 128/1000 | Loss: 0.00001014
Iteration 129/1000 | Loss: 0.00001013
Iteration 130/1000 | Loss: 0.00001013
Iteration 131/1000 | Loss: 0.00001013
Iteration 132/1000 | Loss: 0.00001013
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001013
Iteration 137/1000 | Loss: 0.00001013
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001012
Iteration 140/1000 | Loss: 0.00001012
Iteration 141/1000 | Loss: 0.00001011
Iteration 142/1000 | Loss: 0.00001011
Iteration 143/1000 | Loss: 0.00001011
Iteration 144/1000 | Loss: 0.00001011
Iteration 145/1000 | Loss: 0.00001011
Iteration 146/1000 | Loss: 0.00001011
Iteration 147/1000 | Loss: 0.00001011
Iteration 148/1000 | Loss: 0.00001011
Iteration 149/1000 | Loss: 0.00001011
Iteration 150/1000 | Loss: 0.00001011
Iteration 151/1000 | Loss: 0.00001011
Iteration 152/1000 | Loss: 0.00001011
Iteration 153/1000 | Loss: 0.00001011
Iteration 154/1000 | Loss: 0.00001011
Iteration 155/1000 | Loss: 0.00001010
Iteration 156/1000 | Loss: 0.00001010
Iteration 157/1000 | Loss: 0.00001010
Iteration 158/1000 | Loss: 0.00001010
Iteration 159/1000 | Loss: 0.00001010
Iteration 160/1000 | Loss: 0.00001010
Iteration 161/1000 | Loss: 0.00001010
Iteration 162/1000 | Loss: 0.00001010
Iteration 163/1000 | Loss: 0.00001010
Iteration 164/1000 | Loss: 0.00001010
Iteration 165/1000 | Loss: 0.00001010
Iteration 166/1000 | Loss: 0.00001009
Iteration 167/1000 | Loss: 0.00001009
Iteration 168/1000 | Loss: 0.00001009
Iteration 169/1000 | Loss: 0.00001009
Iteration 170/1000 | Loss: 0.00001009
Iteration 171/1000 | Loss: 0.00001009
Iteration 172/1000 | Loss: 0.00001009
Iteration 173/1000 | Loss: 0.00001009
Iteration 174/1000 | Loss: 0.00001009
Iteration 175/1000 | Loss: 0.00001009
Iteration 176/1000 | Loss: 0.00001009
Iteration 177/1000 | Loss: 0.00001009
Iteration 178/1000 | Loss: 0.00001009
Iteration 179/1000 | Loss: 0.00001008
Iteration 180/1000 | Loss: 0.00001008
Iteration 181/1000 | Loss: 0.00001008
Iteration 182/1000 | Loss: 0.00001008
Iteration 183/1000 | Loss: 0.00001008
Iteration 184/1000 | Loss: 0.00001008
Iteration 185/1000 | Loss: 0.00001008
Iteration 186/1000 | Loss: 0.00001008
Iteration 187/1000 | Loss: 0.00001008
Iteration 188/1000 | Loss: 0.00001008
Iteration 189/1000 | Loss: 0.00001008
Iteration 190/1000 | Loss: 0.00001008
Iteration 191/1000 | Loss: 0.00001008
Iteration 192/1000 | Loss: 0.00001007
Iteration 193/1000 | Loss: 0.00001007
Iteration 194/1000 | Loss: 0.00001007
Iteration 195/1000 | Loss: 0.00001007
Iteration 196/1000 | Loss: 0.00001007
Iteration 197/1000 | Loss: 0.00001007
Iteration 198/1000 | Loss: 0.00001007
Iteration 199/1000 | Loss: 0.00001007
Iteration 200/1000 | Loss: 0.00001007
Iteration 201/1000 | Loss: 0.00001007
Iteration 202/1000 | Loss: 0.00001007
Iteration 203/1000 | Loss: 0.00001007
Iteration 204/1000 | Loss: 0.00001006
Iteration 205/1000 | Loss: 0.00001006
Iteration 206/1000 | Loss: 0.00001006
Iteration 207/1000 | Loss: 0.00001006
Iteration 208/1000 | Loss: 0.00001006
Iteration 209/1000 | Loss: 0.00001006
Iteration 210/1000 | Loss: 0.00001006
Iteration 211/1000 | Loss: 0.00001006
Iteration 212/1000 | Loss: 0.00001006
Iteration 213/1000 | Loss: 0.00001006
Iteration 214/1000 | Loss: 0.00001006
Iteration 215/1000 | Loss: 0.00001006
Iteration 216/1000 | Loss: 0.00001005
Iteration 217/1000 | Loss: 0.00001005
Iteration 218/1000 | Loss: 0.00001005
Iteration 219/1000 | Loss: 0.00001005
Iteration 220/1000 | Loss: 0.00001005
Iteration 221/1000 | Loss: 0.00001005
Iteration 222/1000 | Loss: 0.00001005
Iteration 223/1000 | Loss: 0.00001005
Iteration 224/1000 | Loss: 0.00001005
Iteration 225/1000 | Loss: 0.00001005
Iteration 226/1000 | Loss: 0.00001005
Iteration 227/1000 | Loss: 0.00001005
Iteration 228/1000 | Loss: 0.00001005
Iteration 229/1000 | Loss: 0.00001005
Iteration 230/1000 | Loss: 0.00001005
Iteration 231/1000 | Loss: 0.00001005
Iteration 232/1000 | Loss: 0.00001005
Iteration 233/1000 | Loss: 0.00001005
Iteration 234/1000 | Loss: 0.00001005
Iteration 235/1000 | Loss: 0.00001005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.0048274816654157e-05, 1.0048274816654157e-05, 1.0048274816654157e-05, 1.0048274816654157e-05, 1.0048274816654157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0048274816654157e-05

Optimization complete. Final v2v error: 2.71795916557312 mm

Highest mean error: 2.9004814624786377 mm for frame 84

Lowest mean error: 2.594862937927246 mm for frame 214

Saving results

Total time: 51.488821506500244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879023
Iteration 2/25 | Loss: 0.00236044
Iteration 3/25 | Loss: 0.00211745
Iteration 4/25 | Loss: 0.00214382
Iteration 5/25 | Loss: 0.00201964
Iteration 6/25 | Loss: 0.00177490
Iteration 7/25 | Loss: 0.00156163
Iteration 8/25 | Loss: 0.00147618
Iteration 9/25 | Loss: 0.00145350
Iteration 10/25 | Loss: 0.00144148
Iteration 11/25 | Loss: 0.00143483
Iteration 12/25 | Loss: 0.00143136
Iteration 13/25 | Loss: 0.00142619
Iteration 14/25 | Loss: 0.00142414
Iteration 15/25 | Loss: 0.00142340
Iteration 16/25 | Loss: 0.00142310
Iteration 17/25 | Loss: 0.00142285
Iteration 18/25 | Loss: 0.00142260
Iteration 19/25 | Loss: 0.00142918
Iteration 20/25 | Loss: 0.00142495
Iteration 21/25 | Loss: 0.00142141
Iteration 22/25 | Loss: 0.00141884
Iteration 23/25 | Loss: 0.00141800
Iteration 24/25 | Loss: 0.00141780
Iteration 25/25 | Loss: 0.00141779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21126211
Iteration 2/25 | Loss: 0.00200105
Iteration 3/25 | Loss: 0.00200104
Iteration 4/25 | Loss: 0.00200104
Iteration 5/25 | Loss: 0.00200104
Iteration 6/25 | Loss: 0.00200104
Iteration 7/25 | Loss: 0.00200104
Iteration 8/25 | Loss: 0.00200104
Iteration 9/25 | Loss: 0.00200104
Iteration 10/25 | Loss: 0.00200104
Iteration 11/25 | Loss: 0.00200104
Iteration 12/25 | Loss: 0.00200104
Iteration 13/25 | Loss: 0.00200104
Iteration 14/25 | Loss: 0.00200104
Iteration 15/25 | Loss: 0.00200104
Iteration 16/25 | Loss: 0.00200104
Iteration 17/25 | Loss: 0.00200104
Iteration 18/25 | Loss: 0.00200104
Iteration 19/25 | Loss: 0.00200104
Iteration 20/25 | Loss: 0.00200104
Iteration 21/25 | Loss: 0.00200104
Iteration 22/25 | Loss: 0.00200104
Iteration 23/25 | Loss: 0.00200104
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00200103921815753, 0.00200103921815753, 0.00200103921815753, 0.00200103921815753, 0.00200103921815753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00200103921815753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200104
Iteration 2/1000 | Loss: 0.00006214
Iteration 3/1000 | Loss: 0.00004238
Iteration 4/1000 | Loss: 0.00003345
Iteration 5/1000 | Loss: 0.00002937
Iteration 6/1000 | Loss: 0.00002758
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002580
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002464
Iteration 11/1000 | Loss: 0.00002428
Iteration 12/1000 | Loss: 0.00026554
Iteration 13/1000 | Loss: 0.00008518
Iteration 14/1000 | Loss: 0.00014071
Iteration 15/1000 | Loss: 0.00002465
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002151
Iteration 18/1000 | Loss: 0.00002073
Iteration 19/1000 | Loss: 0.00002036
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001948
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001928
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001926
Iteration 40/1000 | Loss: 0.00001926
Iteration 41/1000 | Loss: 0.00001926
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001920
Iteration 55/1000 | Loss: 0.00001920
Iteration 56/1000 | Loss: 0.00001919
Iteration 57/1000 | Loss: 0.00001919
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001914
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001912
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001911
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001910
Iteration 73/1000 | Loss: 0.00001909
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001908
Iteration 77/1000 | Loss: 0.00001908
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001906
Iteration 94/1000 | Loss: 0.00001906
Iteration 95/1000 | Loss: 0.00001906
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001906
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001905
Iteration 111/1000 | Loss: 0.00001905
Iteration 112/1000 | Loss: 0.00001905
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001904
Iteration 135/1000 | Loss: 0.00001904
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001904
Iteration 138/1000 | Loss: 0.00001904
Iteration 139/1000 | Loss: 0.00001904
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001904
Iteration 142/1000 | Loss: 0.00001904
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001903
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001903
Iteration 153/1000 | Loss: 0.00001903
Iteration 154/1000 | Loss: 0.00001903
Iteration 155/1000 | Loss: 0.00001903
Iteration 156/1000 | Loss: 0.00001903
Iteration 157/1000 | Loss: 0.00001903
Iteration 158/1000 | Loss: 0.00001903
Iteration 159/1000 | Loss: 0.00001903
Iteration 160/1000 | Loss: 0.00001903
Iteration 161/1000 | Loss: 0.00001903
Iteration 162/1000 | Loss: 0.00001903
Iteration 163/1000 | Loss: 0.00001903
Iteration 164/1000 | Loss: 0.00001903
Iteration 165/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.903232987388037e-05, 1.903232987388037e-05, 1.903232987388037e-05, 1.903232987388037e-05, 1.903232987388037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.903232987388037e-05

Optimization complete. Final v2v error: 3.7668373584747314 mm

Highest mean error: 5.272687911987305 mm for frame 27

Lowest mean error: 3.4435012340545654 mm for frame 41

Saving results

Total time: 83.48733592033386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865567
Iteration 2/25 | Loss: 0.00272450
Iteration 3/25 | Loss: 0.00198215
Iteration 4/25 | Loss: 0.00176440
Iteration 5/25 | Loss: 0.00173016
Iteration 6/25 | Loss: 0.00171005
Iteration 7/25 | Loss: 0.00168950
Iteration 8/25 | Loss: 0.00170356
Iteration 9/25 | Loss: 0.00167481
Iteration 10/25 | Loss: 0.00168866
Iteration 11/25 | Loss: 0.00167133
Iteration 12/25 | Loss: 0.00166609
Iteration 13/25 | Loss: 0.00166799
Iteration 14/25 | Loss: 0.00166196
Iteration 15/25 | Loss: 0.00166427
Iteration 16/25 | Loss: 0.00166554
Iteration 17/25 | Loss: 0.00166791
Iteration 18/25 | Loss: 0.00167815
Iteration 19/25 | Loss: 0.00166112
Iteration 20/25 | Loss: 0.00166660
Iteration 21/25 | Loss: 0.00166235
Iteration 22/25 | Loss: 0.00166990
Iteration 23/25 | Loss: 0.00166017
Iteration 24/25 | Loss: 0.00165538
Iteration 25/25 | Loss: 0.00165803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.93454933
Iteration 2/25 | Loss: 0.00584709
Iteration 3/25 | Loss: 0.00584708
Iteration 4/25 | Loss: 0.00584708
Iteration 5/25 | Loss: 0.00547475
Iteration 6/25 | Loss: 0.00547453
Iteration 7/25 | Loss: 0.00547453
Iteration 8/25 | Loss: 0.00547453
Iteration 9/25 | Loss: 0.00547453
Iteration 10/25 | Loss: 0.00547453
Iteration 11/25 | Loss: 0.00547453
Iteration 12/25 | Loss: 0.00547453
Iteration 13/25 | Loss: 0.00547453
Iteration 14/25 | Loss: 0.00547453
Iteration 15/25 | Loss: 0.00547453
Iteration 16/25 | Loss: 0.00547453
Iteration 17/25 | Loss: 0.00547453
Iteration 18/25 | Loss: 0.00547453
Iteration 19/25 | Loss: 0.00547453
Iteration 20/25 | Loss: 0.00547453
Iteration 21/25 | Loss: 0.00547453
Iteration 22/25 | Loss: 0.00547453
Iteration 23/25 | Loss: 0.00547453
Iteration 24/25 | Loss: 0.00547453
Iteration 25/25 | Loss: 0.00547453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00547453
Iteration 2/1000 | Loss: 0.00076191
Iteration 3/1000 | Loss: 0.00070169
Iteration 4/1000 | Loss: 0.00114794
Iteration 5/1000 | Loss: 0.00113614
Iteration 6/1000 | Loss: 0.00450487
Iteration 7/1000 | Loss: 0.00097373
Iteration 8/1000 | Loss: 0.00114387
Iteration 9/1000 | Loss: 0.00137889
Iteration 10/1000 | Loss: 0.00038118
Iteration 11/1000 | Loss: 0.00027150
Iteration 12/1000 | Loss: 0.00105268
Iteration 13/1000 | Loss: 0.00082938
Iteration 14/1000 | Loss: 0.00072108
Iteration 15/1000 | Loss: 0.00018294
Iteration 16/1000 | Loss: 0.00091588
Iteration 17/1000 | Loss: 0.00119406
Iteration 18/1000 | Loss: 0.00058537
Iteration 19/1000 | Loss: 0.00102293
Iteration 20/1000 | Loss: 0.00054432
Iteration 21/1000 | Loss: 0.00036685
Iteration 22/1000 | Loss: 0.00025309
Iteration 23/1000 | Loss: 0.00022286
Iteration 24/1000 | Loss: 0.00008891
Iteration 25/1000 | Loss: 0.00054432
Iteration 26/1000 | Loss: 0.00010535
Iteration 27/1000 | Loss: 0.00075014
Iteration 28/1000 | Loss: 0.00013261
Iteration 29/1000 | Loss: 0.00096873
Iteration 30/1000 | Loss: 0.00010447
Iteration 31/1000 | Loss: 0.00008093
Iteration 32/1000 | Loss: 0.00006829
Iteration 33/1000 | Loss: 0.00006298
Iteration 34/1000 | Loss: 0.00006020
Iteration 35/1000 | Loss: 0.00015465
Iteration 36/1000 | Loss: 0.00015933
Iteration 37/1000 | Loss: 0.00020687
Iteration 38/1000 | Loss: 0.00031828
Iteration 39/1000 | Loss: 0.00027236
Iteration 40/1000 | Loss: 0.00013921
Iteration 41/1000 | Loss: 0.00008804
Iteration 42/1000 | Loss: 0.00008733
Iteration 43/1000 | Loss: 0.00020911
Iteration 44/1000 | Loss: 0.00006112
Iteration 45/1000 | Loss: 0.00027056
Iteration 46/1000 | Loss: 0.00038240
Iteration 47/1000 | Loss: 0.00030938
Iteration 48/1000 | Loss: 0.00022685
Iteration 49/1000 | Loss: 0.00019034
Iteration 50/1000 | Loss: 0.00012200
Iteration 51/1000 | Loss: 0.00021244
Iteration 52/1000 | Loss: 0.00014971
Iteration 53/1000 | Loss: 0.00016684
Iteration 54/1000 | Loss: 0.00013312
Iteration 55/1000 | Loss: 0.00014792
Iteration 56/1000 | Loss: 0.00013708
Iteration 57/1000 | Loss: 0.00006809
Iteration 58/1000 | Loss: 0.00035007
Iteration 59/1000 | Loss: 0.00022834
Iteration 60/1000 | Loss: 0.00007663
Iteration 61/1000 | Loss: 0.00027963
Iteration 62/1000 | Loss: 0.00023560
Iteration 63/1000 | Loss: 0.00008861
Iteration 64/1000 | Loss: 0.00005341
Iteration 65/1000 | Loss: 0.00005175
Iteration 66/1000 | Loss: 0.00013903
Iteration 67/1000 | Loss: 0.00005043
Iteration 68/1000 | Loss: 0.00005069
Iteration 69/1000 | Loss: 0.00004945
Iteration 70/1000 | Loss: 0.00004897
Iteration 71/1000 | Loss: 0.00011918
Iteration 72/1000 | Loss: 0.00054006
Iteration 73/1000 | Loss: 0.00026709
Iteration 74/1000 | Loss: 0.00026612
Iteration 75/1000 | Loss: 0.00021725
Iteration 76/1000 | Loss: 0.00045375
Iteration 77/1000 | Loss: 0.00030298
Iteration 78/1000 | Loss: 0.00035655
Iteration 79/1000 | Loss: 0.00038384
Iteration 80/1000 | Loss: 0.00038870
Iteration 81/1000 | Loss: 0.00022514
Iteration 82/1000 | Loss: 0.00005897
Iteration 83/1000 | Loss: 0.00005371
Iteration 84/1000 | Loss: 0.00012833
Iteration 85/1000 | Loss: 0.00010440
Iteration 86/1000 | Loss: 0.00013643
Iteration 87/1000 | Loss: 0.00004834
Iteration 88/1000 | Loss: 0.00009509
Iteration 89/1000 | Loss: 0.00004725
Iteration 90/1000 | Loss: 0.00004667
Iteration 91/1000 | Loss: 0.00004981
Iteration 92/1000 | Loss: 0.00004578
Iteration 93/1000 | Loss: 0.00004522
Iteration 94/1000 | Loss: 0.00011754
Iteration 95/1000 | Loss: 0.00005153
Iteration 96/1000 | Loss: 0.00005150
Iteration 97/1000 | Loss: 0.00047822
Iteration 98/1000 | Loss: 0.00005258
Iteration 99/1000 | Loss: 0.00004493
Iteration 100/1000 | Loss: 0.00009857
Iteration 101/1000 | Loss: 0.00004253
Iteration 102/1000 | Loss: 0.00004182
Iteration 103/1000 | Loss: 0.00004133
Iteration 104/1000 | Loss: 0.00010949
Iteration 105/1000 | Loss: 0.00004981
Iteration 106/1000 | Loss: 0.00004843
Iteration 107/1000 | Loss: 0.00004058
Iteration 108/1000 | Loss: 0.00006119
Iteration 109/1000 | Loss: 0.00004023
Iteration 110/1000 | Loss: 0.00004016
Iteration 111/1000 | Loss: 0.00003995
Iteration 112/1000 | Loss: 0.00003983
Iteration 113/1000 | Loss: 0.00003981
Iteration 114/1000 | Loss: 0.00003980
Iteration 115/1000 | Loss: 0.00003979
Iteration 116/1000 | Loss: 0.00010208
Iteration 117/1000 | Loss: 0.00009469
Iteration 118/1000 | Loss: 0.00004076
Iteration 119/1000 | Loss: 0.00005391
Iteration 120/1000 | Loss: 0.00016773
Iteration 121/1000 | Loss: 0.00006386
Iteration 122/1000 | Loss: 0.00005921
Iteration 123/1000 | Loss: 0.00003972
Iteration 124/1000 | Loss: 0.00003971
Iteration 125/1000 | Loss: 0.00003960
Iteration 126/1000 | Loss: 0.00003948
Iteration 127/1000 | Loss: 0.00004754
Iteration 128/1000 | Loss: 0.00003959
Iteration 129/1000 | Loss: 0.00004107
Iteration 130/1000 | Loss: 0.00007797
Iteration 131/1000 | Loss: 0.00003937
Iteration 132/1000 | Loss: 0.00003909
Iteration 133/1000 | Loss: 0.00003889
Iteration 134/1000 | Loss: 0.00003875
Iteration 135/1000 | Loss: 0.00003864
Iteration 136/1000 | Loss: 0.00003856
Iteration 137/1000 | Loss: 0.00003850
Iteration 138/1000 | Loss: 0.00010446
Iteration 139/1000 | Loss: 0.00003854
Iteration 140/1000 | Loss: 0.00003822
Iteration 141/1000 | Loss: 0.00003821
Iteration 142/1000 | Loss: 0.00009596
Iteration 143/1000 | Loss: 0.00009596
Iteration 144/1000 | Loss: 0.00090218
Iteration 145/1000 | Loss: 0.00005855
Iteration 146/1000 | Loss: 0.00004121
Iteration 147/1000 | Loss: 0.00010365
Iteration 148/1000 | Loss: 0.00004242
Iteration 149/1000 | Loss: 0.00006026
Iteration 150/1000 | Loss: 0.00003748
Iteration 151/1000 | Loss: 0.00003656
Iteration 152/1000 | Loss: 0.00003603
Iteration 153/1000 | Loss: 0.00003564
Iteration 154/1000 | Loss: 0.00003537
Iteration 155/1000 | Loss: 0.00003498
Iteration 156/1000 | Loss: 0.00008218
Iteration 157/1000 | Loss: 0.00008161
Iteration 158/1000 | Loss: 0.00003462
Iteration 159/1000 | Loss: 0.00003417
Iteration 160/1000 | Loss: 0.00037118
Iteration 161/1000 | Loss: 0.00053363
Iteration 162/1000 | Loss: 0.00028009
Iteration 163/1000 | Loss: 0.00036800
Iteration 164/1000 | Loss: 0.00036777
Iteration 165/1000 | Loss: 0.00010530
Iteration 166/1000 | Loss: 0.00005513
Iteration 167/1000 | Loss: 0.00021188
Iteration 168/1000 | Loss: 0.00032467
Iteration 169/1000 | Loss: 0.00015429
Iteration 170/1000 | Loss: 0.00004948
Iteration 171/1000 | Loss: 0.00020062
Iteration 172/1000 | Loss: 0.00021774
Iteration 173/1000 | Loss: 0.00018497
Iteration 174/1000 | Loss: 0.00006698
Iteration 175/1000 | Loss: 0.00005640
Iteration 176/1000 | Loss: 0.00013705
Iteration 177/1000 | Loss: 0.00009885
Iteration 178/1000 | Loss: 0.00031809
Iteration 179/1000 | Loss: 0.00026282
Iteration 180/1000 | Loss: 0.00030410
Iteration 181/1000 | Loss: 0.00035507
Iteration 182/1000 | Loss: 0.00022145
Iteration 183/1000 | Loss: 0.00024672
Iteration 184/1000 | Loss: 0.00024070
Iteration 185/1000 | Loss: 0.00038514
Iteration 186/1000 | Loss: 0.00035867
Iteration 187/1000 | Loss: 0.00034615
Iteration 188/1000 | Loss: 0.00031183
Iteration 189/1000 | Loss: 0.00017401
Iteration 190/1000 | Loss: 0.00018142
Iteration 191/1000 | Loss: 0.00021506
Iteration 192/1000 | Loss: 0.00003656
Iteration 193/1000 | Loss: 0.00003395
Iteration 194/1000 | Loss: 0.00004532
Iteration 195/1000 | Loss: 0.00016835
Iteration 196/1000 | Loss: 0.00021274
Iteration 197/1000 | Loss: 0.00020590
Iteration 198/1000 | Loss: 0.00018417
Iteration 199/1000 | Loss: 0.00022496
Iteration 200/1000 | Loss: 0.00003262
Iteration 201/1000 | Loss: 0.00003062
Iteration 202/1000 | Loss: 0.00002944
Iteration 203/1000 | Loss: 0.00002840
Iteration 204/1000 | Loss: 0.00002775
Iteration 205/1000 | Loss: 0.00021019
Iteration 206/1000 | Loss: 0.00013890
Iteration 207/1000 | Loss: 0.00023262
Iteration 208/1000 | Loss: 0.00013735
Iteration 209/1000 | Loss: 0.00018208
Iteration 210/1000 | Loss: 0.00003911
Iteration 211/1000 | Loss: 0.00003029
Iteration 212/1000 | Loss: 0.00004381
Iteration 213/1000 | Loss: 0.00043153
Iteration 214/1000 | Loss: 0.00025265
Iteration 215/1000 | Loss: 0.00011072
Iteration 216/1000 | Loss: 0.00041641
Iteration 217/1000 | Loss: 0.00005192
Iteration 218/1000 | Loss: 0.00017992
Iteration 219/1000 | Loss: 0.00012944
Iteration 220/1000 | Loss: 0.00003096
Iteration 221/1000 | Loss: 0.00003623
Iteration 222/1000 | Loss: 0.00003138
Iteration 223/1000 | Loss: 0.00002651
Iteration 224/1000 | Loss: 0.00003921
Iteration 225/1000 | Loss: 0.00002602
Iteration 226/1000 | Loss: 0.00027993
Iteration 227/1000 | Loss: 0.00011112
Iteration 228/1000 | Loss: 0.00047154
Iteration 229/1000 | Loss: 0.00035312
Iteration 230/1000 | Loss: 0.00025970
Iteration 231/1000 | Loss: 0.00014229
Iteration 232/1000 | Loss: 0.00003165
Iteration 233/1000 | Loss: 0.00006240
Iteration 234/1000 | Loss: 0.00012064
Iteration 235/1000 | Loss: 0.00002567
Iteration 236/1000 | Loss: 0.00002476
Iteration 237/1000 | Loss: 0.00002404
Iteration 238/1000 | Loss: 0.00002347
Iteration 239/1000 | Loss: 0.00002275
Iteration 240/1000 | Loss: 0.00017785
Iteration 241/1000 | Loss: 0.00002811
Iteration 242/1000 | Loss: 0.00010772
Iteration 243/1000 | Loss: 0.00002311
Iteration 244/1000 | Loss: 0.00005380
Iteration 245/1000 | Loss: 0.00023947
Iteration 246/1000 | Loss: 0.00002601
Iteration 247/1000 | Loss: 0.00003851
Iteration 248/1000 | Loss: 0.00002209
Iteration 249/1000 | Loss: 0.00003858
Iteration 250/1000 | Loss: 0.00002459
Iteration 251/1000 | Loss: 0.00002226
Iteration 252/1000 | Loss: 0.00002167
Iteration 253/1000 | Loss: 0.00002123
Iteration 254/1000 | Loss: 0.00002076
Iteration 255/1000 | Loss: 0.00002032
Iteration 256/1000 | Loss: 0.00002007
Iteration 257/1000 | Loss: 0.00002002
Iteration 258/1000 | Loss: 0.00002000
Iteration 259/1000 | Loss: 0.00001998
Iteration 260/1000 | Loss: 0.00001994
Iteration 261/1000 | Loss: 0.00001991
Iteration 262/1000 | Loss: 0.00001980
Iteration 263/1000 | Loss: 0.00001977
Iteration 264/1000 | Loss: 0.00001972
Iteration 265/1000 | Loss: 0.00001970
Iteration 266/1000 | Loss: 0.00001969
Iteration 267/1000 | Loss: 0.00001969
Iteration 268/1000 | Loss: 0.00001968
Iteration 269/1000 | Loss: 0.00001968
Iteration 270/1000 | Loss: 0.00001967
Iteration 271/1000 | Loss: 0.00001967
Iteration 272/1000 | Loss: 0.00001967
Iteration 273/1000 | Loss: 0.00001966
Iteration 274/1000 | Loss: 0.00001965
Iteration 275/1000 | Loss: 0.00001964
Iteration 276/1000 | Loss: 0.00001963
Iteration 277/1000 | Loss: 0.00001962
Iteration 278/1000 | Loss: 0.00001962
Iteration 279/1000 | Loss: 0.00001960
Iteration 280/1000 | Loss: 0.00001960
Iteration 281/1000 | Loss: 0.00001959
Iteration 282/1000 | Loss: 0.00001958
Iteration 283/1000 | Loss: 0.00001957
Iteration 284/1000 | Loss: 0.00001957
Iteration 285/1000 | Loss: 0.00001957
Iteration 286/1000 | Loss: 0.00001957
Iteration 287/1000 | Loss: 0.00001956
Iteration 288/1000 | Loss: 0.00001956
Iteration 289/1000 | Loss: 0.00001956
Iteration 290/1000 | Loss: 0.00001955
Iteration 291/1000 | Loss: 0.00001955
Iteration 292/1000 | Loss: 0.00001955
Iteration 293/1000 | Loss: 0.00001955
Iteration 294/1000 | Loss: 0.00001955
Iteration 295/1000 | Loss: 0.00001955
Iteration 296/1000 | Loss: 0.00001954
Iteration 297/1000 | Loss: 0.00001954
Iteration 298/1000 | Loss: 0.00001953
Iteration 299/1000 | Loss: 0.00001953
Iteration 300/1000 | Loss: 0.00001952
Iteration 301/1000 | Loss: 0.00001952
Iteration 302/1000 | Loss: 0.00001952
Iteration 303/1000 | Loss: 0.00001952
Iteration 304/1000 | Loss: 0.00001952
Iteration 305/1000 | Loss: 0.00001951
Iteration 306/1000 | Loss: 0.00001951
Iteration 307/1000 | Loss: 0.00001951
Iteration 308/1000 | Loss: 0.00001950
Iteration 309/1000 | Loss: 0.00001950
Iteration 310/1000 | Loss: 0.00001950
Iteration 311/1000 | Loss: 0.00001949
Iteration 312/1000 | Loss: 0.00001949
Iteration 313/1000 | Loss: 0.00001949
Iteration 314/1000 | Loss: 0.00001949
Iteration 315/1000 | Loss: 0.00001949
Iteration 316/1000 | Loss: 0.00001949
Iteration 317/1000 | Loss: 0.00001949
Iteration 318/1000 | Loss: 0.00001948
Iteration 319/1000 | Loss: 0.00001948
Iteration 320/1000 | Loss: 0.00001948
Iteration 321/1000 | Loss: 0.00001948
Iteration 322/1000 | Loss: 0.00001947
Iteration 323/1000 | Loss: 0.00001947
Iteration 324/1000 | Loss: 0.00001947
Iteration 325/1000 | Loss: 0.00001946
Iteration 326/1000 | Loss: 0.00001944
Iteration 327/1000 | Loss: 0.00001944
Iteration 328/1000 | Loss: 0.00001943
Iteration 329/1000 | Loss: 0.00001943
Iteration 330/1000 | Loss: 0.00001943
Iteration 331/1000 | Loss: 0.00001942
Iteration 332/1000 | Loss: 0.00001942
Iteration 333/1000 | Loss: 0.00001942
Iteration 334/1000 | Loss: 0.00001941
Iteration 335/1000 | Loss: 0.00001941
Iteration 336/1000 | Loss: 0.00001940
Iteration 337/1000 | Loss: 0.00001940
Iteration 338/1000 | Loss: 0.00001940
Iteration 339/1000 | Loss: 0.00001939
Iteration 340/1000 | Loss: 0.00001939
Iteration 341/1000 | Loss: 0.00001939
Iteration 342/1000 | Loss: 0.00001938
Iteration 343/1000 | Loss: 0.00001938
Iteration 344/1000 | Loss: 0.00001938
Iteration 345/1000 | Loss: 0.00001938
Iteration 346/1000 | Loss: 0.00001938
Iteration 347/1000 | Loss: 0.00001938
Iteration 348/1000 | Loss: 0.00001938
Iteration 349/1000 | Loss: 0.00001938
Iteration 350/1000 | Loss: 0.00001938
Iteration 351/1000 | Loss: 0.00001937
Iteration 352/1000 | Loss: 0.00001937
Iteration 353/1000 | Loss: 0.00001937
Iteration 354/1000 | Loss: 0.00001937
Iteration 355/1000 | Loss: 0.00001937
Iteration 356/1000 | Loss: 0.00001936
Iteration 357/1000 | Loss: 0.00001936
Iteration 358/1000 | Loss: 0.00001936
Iteration 359/1000 | Loss: 0.00001936
Iteration 360/1000 | Loss: 0.00001936
Iteration 361/1000 | Loss: 0.00001935
Iteration 362/1000 | Loss: 0.00001935
Iteration 363/1000 | Loss: 0.00001935
Iteration 364/1000 | Loss: 0.00001935
Iteration 365/1000 | Loss: 0.00001935
Iteration 366/1000 | Loss: 0.00001935
Iteration 367/1000 | Loss: 0.00001935
Iteration 368/1000 | Loss: 0.00001935
Iteration 369/1000 | Loss: 0.00001935
Iteration 370/1000 | Loss: 0.00001935
Iteration 371/1000 | Loss: 0.00001935
Iteration 372/1000 | Loss: 0.00001935
Iteration 373/1000 | Loss: 0.00001934
Iteration 374/1000 | Loss: 0.00001934
Iteration 375/1000 | Loss: 0.00001934
Iteration 376/1000 | Loss: 0.00001934
Iteration 377/1000 | Loss: 0.00001934
Iteration 378/1000 | Loss: 0.00001934
Iteration 379/1000 | Loss: 0.00001934
Iteration 380/1000 | Loss: 0.00001934
Iteration 381/1000 | Loss: 0.00001934
Iteration 382/1000 | Loss: 0.00001934
Iteration 383/1000 | Loss: 0.00001934
Iteration 384/1000 | Loss: 0.00001933
Iteration 385/1000 | Loss: 0.00001933
Iteration 386/1000 | Loss: 0.00001933
Iteration 387/1000 | Loss: 0.00001933
Iteration 388/1000 | Loss: 0.00001933
Iteration 389/1000 | Loss: 0.00001933
Iteration 390/1000 | Loss: 0.00001933
Iteration 391/1000 | Loss: 0.00001933
Iteration 392/1000 | Loss: 0.00001933
Iteration 393/1000 | Loss: 0.00001933
Iteration 394/1000 | Loss: 0.00001933
Iteration 395/1000 | Loss: 0.00001933
Iteration 396/1000 | Loss: 0.00001932
Iteration 397/1000 | Loss: 0.00001932
Iteration 398/1000 | Loss: 0.00001932
Iteration 399/1000 | Loss: 0.00001932
Iteration 400/1000 | Loss: 0.00001932
Iteration 401/1000 | Loss: 0.00001931
Iteration 402/1000 | Loss: 0.00001931
Iteration 403/1000 | Loss: 0.00001931
Iteration 404/1000 | Loss: 0.00001931
Iteration 405/1000 | Loss: 0.00001931
Iteration 406/1000 | Loss: 0.00001931
Iteration 407/1000 | Loss: 0.00001931
Iteration 408/1000 | Loss: 0.00001931
Iteration 409/1000 | Loss: 0.00001931
Iteration 410/1000 | Loss: 0.00001931
Iteration 411/1000 | Loss: 0.00001931
Iteration 412/1000 | Loss: 0.00001931
Iteration 413/1000 | Loss: 0.00001931
Iteration 414/1000 | Loss: 0.00001930
Iteration 415/1000 | Loss: 0.00001930
Iteration 416/1000 | Loss: 0.00001930
Iteration 417/1000 | Loss: 0.00001930
Iteration 418/1000 | Loss: 0.00001930
Iteration 419/1000 | Loss: 0.00001930
Iteration 420/1000 | Loss: 0.00001930
Iteration 421/1000 | Loss: 0.00001930
Iteration 422/1000 | Loss: 0.00001930
Iteration 423/1000 | Loss: 0.00001930
Iteration 424/1000 | Loss: 0.00001930
Iteration 425/1000 | Loss: 0.00001930
Iteration 426/1000 | Loss: 0.00001930
Iteration 427/1000 | Loss: 0.00001930
Iteration 428/1000 | Loss: 0.00001930
Iteration 429/1000 | Loss: 0.00001929
Iteration 430/1000 | Loss: 0.00001929
Iteration 431/1000 | Loss: 0.00001929
Iteration 432/1000 | Loss: 0.00001929
Iteration 433/1000 | Loss: 0.00001929
Iteration 434/1000 | Loss: 0.00001929
Iteration 435/1000 | Loss: 0.00001929
Iteration 436/1000 | Loss: 0.00001929
Iteration 437/1000 | Loss: 0.00001929
Iteration 438/1000 | Loss: 0.00001929
Iteration 439/1000 | Loss: 0.00001929
Iteration 440/1000 | Loss: 0.00001929
Iteration 441/1000 | Loss: 0.00001929
Iteration 442/1000 | Loss: 0.00001929
Iteration 443/1000 | Loss: 0.00001929
Iteration 444/1000 | Loss: 0.00001929
Iteration 445/1000 | Loss: 0.00001929
Iteration 446/1000 | Loss: 0.00001929
Iteration 447/1000 | Loss: 0.00001929
Iteration 448/1000 | Loss: 0.00001929
Iteration 449/1000 | Loss: 0.00001929
Iteration 450/1000 | Loss: 0.00001929
Iteration 451/1000 | Loss: 0.00001929
Iteration 452/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 452. Stopping optimization.
Last 5 losses: [1.9285322196083143e-05, 1.9285322196083143e-05, 1.9285322196083143e-05, 1.9285322196083143e-05, 1.9285322196083143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9285322196083143e-05

Optimization complete. Final v2v error: 3.387526035308838 mm

Highest mean error: 10.23624324798584 mm for frame 77

Lowest mean error: 2.7833054065704346 mm for frame 112

Saving results

Total time: 421.2637062072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403370
Iteration 2/25 | Loss: 0.00150131
Iteration 3/25 | Loss: 0.00140039
Iteration 4/25 | Loss: 0.00138811
Iteration 5/25 | Loss: 0.00138556
Iteration 6/25 | Loss: 0.00138553
Iteration 7/25 | Loss: 0.00138553
Iteration 8/25 | Loss: 0.00138553
Iteration 9/25 | Loss: 0.00138553
Iteration 10/25 | Loss: 0.00138553
Iteration 11/25 | Loss: 0.00138553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013855252182111144, 0.0013855252182111144, 0.0013855252182111144, 0.0013855252182111144, 0.0013855252182111144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013855252182111144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25346625
Iteration 2/25 | Loss: 0.00210769
Iteration 3/25 | Loss: 0.00210769
Iteration 4/25 | Loss: 0.00210769
Iteration 5/25 | Loss: 0.00210769
Iteration 6/25 | Loss: 0.00210769
Iteration 7/25 | Loss: 0.00210769
Iteration 8/25 | Loss: 0.00210768
Iteration 9/25 | Loss: 0.00210768
Iteration 10/25 | Loss: 0.00210768
Iteration 11/25 | Loss: 0.00210768
Iteration 12/25 | Loss: 0.00210768
Iteration 13/25 | Loss: 0.00210768
Iteration 14/25 | Loss: 0.00210768
Iteration 15/25 | Loss: 0.00210768
Iteration 16/25 | Loss: 0.00210768
Iteration 17/25 | Loss: 0.00210768
Iteration 18/25 | Loss: 0.00210768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021076840348541737, 0.0021076840348541737, 0.0021076840348541737, 0.0021076840348541737, 0.0021076840348541737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021076840348541737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210768
Iteration 2/1000 | Loss: 0.00003176
Iteration 3/1000 | Loss: 0.00002157
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001651
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001590
Iteration 13/1000 | Loss: 0.00001569
Iteration 14/1000 | Loss: 0.00001551
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001540
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001533
Iteration 19/1000 | Loss: 0.00001533
Iteration 20/1000 | Loss: 0.00001532
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001527
Iteration 24/1000 | Loss: 0.00001527
Iteration 25/1000 | Loss: 0.00001526
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001514
Iteration 28/1000 | Loss: 0.00001514
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001501
Iteration 43/1000 | Loss: 0.00001501
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001497
Iteration 47/1000 | Loss: 0.00001496
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00001492
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001483
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001473
Iteration 109/1000 | Loss: 0.00001473
Iteration 110/1000 | Loss: 0.00001473
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001473
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001471
Iteration 124/1000 | Loss: 0.00001471
Iteration 125/1000 | Loss: 0.00001471
Iteration 126/1000 | Loss: 0.00001471
Iteration 127/1000 | Loss: 0.00001471
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001470
Iteration 131/1000 | Loss: 0.00001470
Iteration 132/1000 | Loss: 0.00001470
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001469
Iteration 135/1000 | Loss: 0.00001469
Iteration 136/1000 | Loss: 0.00001469
Iteration 137/1000 | Loss: 0.00001469
Iteration 138/1000 | Loss: 0.00001469
Iteration 139/1000 | Loss: 0.00001469
Iteration 140/1000 | Loss: 0.00001469
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Iteration 155/1000 | Loss: 0.00001466
Iteration 156/1000 | Loss: 0.00001466
Iteration 157/1000 | Loss: 0.00001466
Iteration 158/1000 | Loss: 0.00001466
Iteration 159/1000 | Loss: 0.00001465
Iteration 160/1000 | Loss: 0.00001465
Iteration 161/1000 | Loss: 0.00001465
Iteration 162/1000 | Loss: 0.00001465
Iteration 163/1000 | Loss: 0.00001465
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001464
Iteration 166/1000 | Loss: 0.00001464
Iteration 167/1000 | Loss: 0.00001464
Iteration 168/1000 | Loss: 0.00001464
Iteration 169/1000 | Loss: 0.00001464
Iteration 170/1000 | Loss: 0.00001464
Iteration 171/1000 | Loss: 0.00001464
Iteration 172/1000 | Loss: 0.00001464
Iteration 173/1000 | Loss: 0.00001464
Iteration 174/1000 | Loss: 0.00001464
Iteration 175/1000 | Loss: 0.00001464
Iteration 176/1000 | Loss: 0.00001464
Iteration 177/1000 | Loss: 0.00001464
Iteration 178/1000 | Loss: 0.00001464
Iteration 179/1000 | Loss: 0.00001464
Iteration 180/1000 | Loss: 0.00001464
Iteration 181/1000 | Loss: 0.00001463
Iteration 182/1000 | Loss: 0.00001463
Iteration 183/1000 | Loss: 0.00001463
Iteration 184/1000 | Loss: 0.00001463
Iteration 185/1000 | Loss: 0.00001463
Iteration 186/1000 | Loss: 0.00001463
Iteration 187/1000 | Loss: 0.00001463
Iteration 188/1000 | Loss: 0.00001463
Iteration 189/1000 | Loss: 0.00001463
Iteration 190/1000 | Loss: 0.00001463
Iteration 191/1000 | Loss: 0.00001463
Iteration 192/1000 | Loss: 0.00001463
Iteration 193/1000 | Loss: 0.00001462
Iteration 194/1000 | Loss: 0.00001462
Iteration 195/1000 | Loss: 0.00001462
Iteration 196/1000 | Loss: 0.00001462
Iteration 197/1000 | Loss: 0.00001462
Iteration 198/1000 | Loss: 0.00001462
Iteration 199/1000 | Loss: 0.00001462
Iteration 200/1000 | Loss: 0.00001462
Iteration 201/1000 | Loss: 0.00001462
Iteration 202/1000 | Loss: 0.00001462
Iteration 203/1000 | Loss: 0.00001462
Iteration 204/1000 | Loss: 0.00001462
Iteration 205/1000 | Loss: 0.00001462
Iteration 206/1000 | Loss: 0.00001462
Iteration 207/1000 | Loss: 0.00001462
Iteration 208/1000 | Loss: 0.00001461
Iteration 209/1000 | Loss: 0.00001461
Iteration 210/1000 | Loss: 0.00001461
Iteration 211/1000 | Loss: 0.00001461
Iteration 212/1000 | Loss: 0.00001461
Iteration 213/1000 | Loss: 0.00001461
Iteration 214/1000 | Loss: 0.00001461
Iteration 215/1000 | Loss: 0.00001461
Iteration 216/1000 | Loss: 0.00001461
Iteration 217/1000 | Loss: 0.00001461
Iteration 218/1000 | Loss: 0.00001461
Iteration 219/1000 | Loss: 0.00001461
Iteration 220/1000 | Loss: 0.00001461
Iteration 221/1000 | Loss: 0.00001461
Iteration 222/1000 | Loss: 0.00001461
Iteration 223/1000 | Loss: 0.00001461
Iteration 224/1000 | Loss: 0.00001461
Iteration 225/1000 | Loss: 0.00001461
Iteration 226/1000 | Loss: 0.00001461
Iteration 227/1000 | Loss: 0.00001460
Iteration 228/1000 | Loss: 0.00001460
Iteration 229/1000 | Loss: 0.00001460
Iteration 230/1000 | Loss: 0.00001460
Iteration 231/1000 | Loss: 0.00001460
Iteration 232/1000 | Loss: 0.00001460
Iteration 233/1000 | Loss: 0.00001460
Iteration 234/1000 | Loss: 0.00001460
Iteration 235/1000 | Loss: 0.00001460
Iteration 236/1000 | Loss: 0.00001460
Iteration 237/1000 | Loss: 0.00001460
Iteration 238/1000 | Loss: 0.00001460
Iteration 239/1000 | Loss: 0.00001460
Iteration 240/1000 | Loss: 0.00001460
Iteration 241/1000 | Loss: 0.00001460
Iteration 242/1000 | Loss: 0.00001460
Iteration 243/1000 | Loss: 0.00001460
Iteration 244/1000 | Loss: 0.00001460
Iteration 245/1000 | Loss: 0.00001460
Iteration 246/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.4602399460272864e-05, 1.4602399460272864e-05, 1.4602399460272864e-05, 1.4602399460272864e-05, 1.4602399460272864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4602399460272864e-05

Optimization complete. Final v2v error: 3.201298952102661 mm

Highest mean error: 3.4330241680145264 mm for frame 43

Lowest mean error: 2.894658088684082 mm for frame 65

Saving results

Total time: 47.371700286865234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985507
Iteration 2/25 | Loss: 0.00195023
Iteration 3/25 | Loss: 0.00166251
Iteration 4/25 | Loss: 0.00160983
Iteration 5/25 | Loss: 0.00165258
Iteration 6/25 | Loss: 0.00158729
Iteration 7/25 | Loss: 0.00151828
Iteration 8/25 | Loss: 0.00148151
Iteration 9/25 | Loss: 0.00146470
Iteration 10/25 | Loss: 0.00145064
Iteration 11/25 | Loss: 0.00144188
Iteration 12/25 | Loss: 0.00142804
Iteration 13/25 | Loss: 0.00141612
Iteration 14/25 | Loss: 0.00141475
Iteration 15/25 | Loss: 0.00141148
Iteration 16/25 | Loss: 0.00141079
Iteration 17/25 | Loss: 0.00140288
Iteration 18/25 | Loss: 0.00140264
Iteration 19/25 | Loss: 0.00140076
Iteration 20/25 | Loss: 0.00139963
Iteration 21/25 | Loss: 0.00139929
Iteration 22/25 | Loss: 0.00140188
Iteration 23/25 | Loss: 0.00139919
Iteration 24/25 | Loss: 0.00139931
Iteration 25/25 | Loss: 0.00140065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26114905
Iteration 2/25 | Loss: 0.00239351
Iteration 3/25 | Loss: 0.00219761
Iteration 4/25 | Loss: 0.00219760
Iteration 5/25 | Loss: 0.00219760
Iteration 6/25 | Loss: 0.00219760
Iteration 7/25 | Loss: 0.00219760
Iteration 8/25 | Loss: 0.00219760
Iteration 9/25 | Loss: 0.00219760
Iteration 10/25 | Loss: 0.00219760
Iteration 11/25 | Loss: 0.00219760
Iteration 12/25 | Loss: 0.00219760
Iteration 13/25 | Loss: 0.00219760
Iteration 14/25 | Loss: 0.00219760
Iteration 15/25 | Loss: 0.00219760
Iteration 16/25 | Loss: 0.00219760
Iteration 17/25 | Loss: 0.00219760
Iteration 18/25 | Loss: 0.00219760
Iteration 19/25 | Loss: 0.00219760
Iteration 20/25 | Loss: 0.00219760
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0021976011339575052, 0.0021976011339575052, 0.0021976011339575052, 0.0021976011339575052, 0.0021976011339575052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021976011339575052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219760
Iteration 2/1000 | Loss: 0.00033892
Iteration 3/1000 | Loss: 0.00064580
Iteration 4/1000 | Loss: 0.00034298
Iteration 5/1000 | Loss: 0.00039905
Iteration 6/1000 | Loss: 0.00033669
Iteration 7/1000 | Loss: 0.00046474
Iteration 8/1000 | Loss: 0.00011304
Iteration 9/1000 | Loss: 0.00028110
Iteration 10/1000 | Loss: 0.00018977
Iteration 11/1000 | Loss: 0.00019026
Iteration 12/1000 | Loss: 0.00024229
Iteration 13/1000 | Loss: 0.00010116
Iteration 14/1000 | Loss: 0.00030608
Iteration 15/1000 | Loss: 0.00017383
Iteration 16/1000 | Loss: 0.00014743
Iteration 17/1000 | Loss: 0.00066098
Iteration 18/1000 | Loss: 0.00040481
Iteration 19/1000 | Loss: 0.00038227
Iteration 20/1000 | Loss: 0.00018868
Iteration 21/1000 | Loss: 0.00016330
Iteration 22/1000 | Loss: 0.00022485
Iteration 23/1000 | Loss: 0.00016409
Iteration 24/1000 | Loss: 0.00019770
Iteration 25/1000 | Loss: 0.00016323
Iteration 26/1000 | Loss: 0.00016805
Iteration 27/1000 | Loss: 0.00020167
Iteration 28/1000 | Loss: 0.00033301
Iteration 29/1000 | Loss: 0.00018581
Iteration 30/1000 | Loss: 0.00008902
Iteration 31/1000 | Loss: 0.00006232
Iteration 32/1000 | Loss: 0.00015546
Iteration 33/1000 | Loss: 0.00004224
Iteration 34/1000 | Loss: 0.00002964
Iteration 35/1000 | Loss: 0.00011422
Iteration 36/1000 | Loss: 0.00006088
Iteration 37/1000 | Loss: 0.00011292
Iteration 38/1000 | Loss: 0.00005727
Iteration 39/1000 | Loss: 0.00003718
Iteration 40/1000 | Loss: 0.00003079
Iteration 41/1000 | Loss: 0.00002860
Iteration 42/1000 | Loss: 0.00003766
Iteration 43/1000 | Loss: 0.00020861
Iteration 44/1000 | Loss: 0.00028898
Iteration 45/1000 | Loss: 0.00004117
Iteration 46/1000 | Loss: 0.00004487
Iteration 47/1000 | Loss: 0.00003159
Iteration 48/1000 | Loss: 0.00017743
Iteration 49/1000 | Loss: 0.00002826
Iteration 50/1000 | Loss: 0.00004386
Iteration 51/1000 | Loss: 0.00002355
Iteration 52/1000 | Loss: 0.00002921
Iteration 53/1000 | Loss: 0.00002464
Iteration 54/1000 | Loss: 0.00002273
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00037793
Iteration 60/1000 | Loss: 0.00021249
Iteration 61/1000 | Loss: 0.00004406
Iteration 62/1000 | Loss: 0.00002769
Iteration 63/1000 | Loss: 0.00033067
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001584
Iteration 86/1000 | Loss: 0.00001584
Iteration 87/1000 | Loss: 0.00001583
Iteration 88/1000 | Loss: 0.00001582
Iteration 89/1000 | Loss: 0.00001582
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001575
Iteration 97/1000 | Loss: 0.00001574
Iteration 98/1000 | Loss: 0.00001573
Iteration 99/1000 | Loss: 0.00001572
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00007989
Iteration 102/1000 | Loss: 0.00007989
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001568
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001555
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001553
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001552
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001545
Iteration 124/1000 | Loss: 0.00001545
Iteration 125/1000 | Loss: 0.00001544
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Iteration 130/1000 | Loss: 0.00001544
Iteration 131/1000 | Loss: 0.00001544
Iteration 132/1000 | Loss: 0.00001544
Iteration 133/1000 | Loss: 0.00001544
Iteration 134/1000 | Loss: 0.00001544
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001543
Iteration 137/1000 | Loss: 0.00001542
Iteration 138/1000 | Loss: 0.00001542
Iteration 139/1000 | Loss: 0.00001542
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001540
Iteration 143/1000 | Loss: 0.00001539
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001539
Iteration 146/1000 | Loss: 0.00001539
Iteration 147/1000 | Loss: 0.00001539
Iteration 148/1000 | Loss: 0.00001538
Iteration 149/1000 | Loss: 0.00001538
Iteration 150/1000 | Loss: 0.00001538
Iteration 151/1000 | Loss: 0.00001538
Iteration 152/1000 | Loss: 0.00001538
Iteration 153/1000 | Loss: 0.00001538
Iteration 154/1000 | Loss: 0.00001538
Iteration 155/1000 | Loss: 0.00001538
Iteration 156/1000 | Loss: 0.00001538
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001536
Iteration 163/1000 | Loss: 0.00001536
Iteration 164/1000 | Loss: 0.00001536
Iteration 165/1000 | Loss: 0.00001536
Iteration 166/1000 | Loss: 0.00001535
Iteration 167/1000 | Loss: 0.00012029
Iteration 168/1000 | Loss: 0.00002901
Iteration 169/1000 | Loss: 0.00004806
Iteration 170/1000 | Loss: 0.00003701
Iteration 171/1000 | Loss: 0.00003733
Iteration 172/1000 | Loss: 0.00001538
Iteration 173/1000 | Loss: 0.00001536
Iteration 174/1000 | Loss: 0.00001535
Iteration 175/1000 | Loss: 0.00001535
Iteration 176/1000 | Loss: 0.00001534
Iteration 177/1000 | Loss: 0.00001534
Iteration 178/1000 | Loss: 0.00001534
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Iteration 181/1000 | Loss: 0.00001533
Iteration 182/1000 | Loss: 0.00001533
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001532
Iteration 186/1000 | Loss: 0.00001532
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001532
Iteration 189/1000 | Loss: 0.00001532
Iteration 190/1000 | Loss: 0.00001532
Iteration 191/1000 | Loss: 0.00001531
Iteration 192/1000 | Loss: 0.00001531
Iteration 193/1000 | Loss: 0.00001531
Iteration 194/1000 | Loss: 0.00001531
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001531
Iteration 197/1000 | Loss: 0.00001531
Iteration 198/1000 | Loss: 0.00001531
Iteration 199/1000 | Loss: 0.00001531
Iteration 200/1000 | Loss: 0.00001531
Iteration 201/1000 | Loss: 0.00001531
Iteration 202/1000 | Loss: 0.00001531
Iteration 203/1000 | Loss: 0.00001530
Iteration 204/1000 | Loss: 0.00001530
Iteration 205/1000 | Loss: 0.00001530
Iteration 206/1000 | Loss: 0.00001530
Iteration 207/1000 | Loss: 0.00001530
Iteration 208/1000 | Loss: 0.00001530
Iteration 209/1000 | Loss: 0.00001530
Iteration 210/1000 | Loss: 0.00001530
Iteration 211/1000 | Loss: 0.00001530
Iteration 212/1000 | Loss: 0.00001530
Iteration 213/1000 | Loss: 0.00001530
Iteration 214/1000 | Loss: 0.00001530
Iteration 215/1000 | Loss: 0.00001530
Iteration 216/1000 | Loss: 0.00001530
Iteration 217/1000 | Loss: 0.00001530
Iteration 218/1000 | Loss: 0.00001530
Iteration 219/1000 | Loss: 0.00001530
Iteration 220/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.529948895040434e-05, 1.529948895040434e-05, 1.529948895040434e-05, 1.529948895040434e-05, 1.529948895040434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.529948895040434e-05

Optimization complete. Final v2v error: 3.170579671859741 mm

Highest mean error: 6.376943588256836 mm for frame 49

Lowest mean error: 2.749764919281006 mm for frame 12

Saving results

Total time: 169.51018857955933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393730
Iteration 2/25 | Loss: 0.00143451
Iteration 3/25 | Loss: 0.00137454
Iteration 4/25 | Loss: 0.00136830
Iteration 5/25 | Loss: 0.00136830
Iteration 6/25 | Loss: 0.00136830
Iteration 7/25 | Loss: 0.00136830
Iteration 8/25 | Loss: 0.00136830
Iteration 9/25 | Loss: 0.00136830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0013682959834113717, 0.0013682959834113717, 0.0013682959834113717, 0.0013682959834113717, 0.0013682959834113717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013682959834113717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25901401
Iteration 2/25 | Loss: 0.00196980
Iteration 3/25 | Loss: 0.00196980
Iteration 4/25 | Loss: 0.00196980
Iteration 5/25 | Loss: 0.00196980
Iteration 6/25 | Loss: 0.00196980
Iteration 7/25 | Loss: 0.00196980
Iteration 8/25 | Loss: 0.00196980
Iteration 9/25 | Loss: 0.00196980
Iteration 10/25 | Loss: 0.00196980
Iteration 11/25 | Loss: 0.00196980
Iteration 12/25 | Loss: 0.00196980
Iteration 13/25 | Loss: 0.00196980
Iteration 14/25 | Loss: 0.00196980
Iteration 15/25 | Loss: 0.00196980
Iteration 16/25 | Loss: 0.00196980
Iteration 17/25 | Loss: 0.00196980
Iteration 18/25 | Loss: 0.00196980
Iteration 19/25 | Loss: 0.00196980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019697989337146282, 0.0019697989337146282, 0.0019697989337146282, 0.0019697989337146282, 0.0019697989337146282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019697989337146282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196980
Iteration 2/1000 | Loss: 0.00002002
Iteration 3/1000 | Loss: 0.00001613
Iteration 4/1000 | Loss: 0.00001493
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001181
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001118
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001104
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001095
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001081
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001077
Iteration 33/1000 | Loss: 0.00001077
Iteration 34/1000 | Loss: 0.00001076
Iteration 35/1000 | Loss: 0.00001076
Iteration 36/1000 | Loss: 0.00001075
Iteration 37/1000 | Loss: 0.00001072
Iteration 38/1000 | Loss: 0.00001071
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001064
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001062
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001059
Iteration 54/1000 | Loss: 0.00001059
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001059
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001058
Iteration 65/1000 | Loss: 0.00001058
Iteration 66/1000 | Loss: 0.00001058
Iteration 67/1000 | Loss: 0.00001058
Iteration 68/1000 | Loss: 0.00001058
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001058
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001057
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001057
Iteration 81/1000 | Loss: 0.00001057
Iteration 82/1000 | Loss: 0.00001057
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001057
Iteration 85/1000 | Loss: 0.00001056
Iteration 86/1000 | Loss: 0.00001056
Iteration 87/1000 | Loss: 0.00001056
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001055
Iteration 90/1000 | Loss: 0.00001055
Iteration 91/1000 | Loss: 0.00001055
Iteration 92/1000 | Loss: 0.00001055
Iteration 93/1000 | Loss: 0.00001055
Iteration 94/1000 | Loss: 0.00001055
Iteration 95/1000 | Loss: 0.00001055
Iteration 96/1000 | Loss: 0.00001055
Iteration 97/1000 | Loss: 0.00001055
Iteration 98/1000 | Loss: 0.00001055
Iteration 99/1000 | Loss: 0.00001055
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001054
Iteration 105/1000 | Loss: 0.00001054
Iteration 106/1000 | Loss: 0.00001054
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001054
Iteration 111/1000 | Loss: 0.00001054
Iteration 112/1000 | Loss: 0.00001054
Iteration 113/1000 | Loss: 0.00001054
Iteration 114/1000 | Loss: 0.00001054
Iteration 115/1000 | Loss: 0.00001054
Iteration 116/1000 | Loss: 0.00001054
Iteration 117/1000 | Loss: 0.00001054
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.0543967619014438e-05, 1.0543967619014438e-05, 1.0543967619014438e-05, 1.0543967619014438e-05, 1.0543967619014438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0543967619014438e-05

Optimization complete. Final v2v error: 2.8463010787963867 mm

Highest mean error: 2.885796308517456 mm for frame 93

Lowest mean error: 2.8130624294281006 mm for frame 198

Saving results

Total time: 39.904884815216064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996917
Iteration 2/25 | Loss: 0.00244351
Iteration 3/25 | Loss: 0.00181514
Iteration 4/25 | Loss: 0.00173863
Iteration 5/25 | Loss: 0.00166061
Iteration 6/25 | Loss: 0.00162583
Iteration 7/25 | Loss: 0.00157645
Iteration 8/25 | Loss: 0.00164532
Iteration 9/25 | Loss: 0.00171579
Iteration 10/25 | Loss: 0.00157062
Iteration 11/25 | Loss: 0.00155649
Iteration 12/25 | Loss: 0.00149412
Iteration 13/25 | Loss: 0.00149147
Iteration 14/25 | Loss: 0.00147512
Iteration 15/25 | Loss: 0.00146109
Iteration 16/25 | Loss: 0.00142914
Iteration 17/25 | Loss: 0.00141964
Iteration 18/25 | Loss: 0.00141492
Iteration 19/25 | Loss: 0.00141545
Iteration 20/25 | Loss: 0.00142233
Iteration 21/25 | Loss: 0.00142116
Iteration 22/25 | Loss: 0.00141326
Iteration 23/25 | Loss: 0.00140832
Iteration 24/25 | Loss: 0.00140127
Iteration 25/25 | Loss: 0.00139806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50151134
Iteration 2/25 | Loss: 0.00292847
Iteration 3/25 | Loss: 0.00292847
Iteration 4/25 | Loss: 0.00205804
Iteration 5/25 | Loss: 0.00205803
Iteration 6/25 | Loss: 0.00205803
Iteration 7/25 | Loss: 0.00205803
Iteration 8/25 | Loss: 0.00205803
Iteration 9/25 | Loss: 0.00205803
Iteration 10/25 | Loss: 0.00205803
Iteration 11/25 | Loss: 0.00205803
Iteration 12/25 | Loss: 0.00205803
Iteration 13/25 | Loss: 0.00205803
Iteration 14/25 | Loss: 0.00205803
Iteration 15/25 | Loss: 0.00205803
Iteration 16/25 | Loss: 0.00205803
Iteration 17/25 | Loss: 0.00205803
Iteration 18/25 | Loss: 0.00205803
Iteration 19/25 | Loss: 0.00205803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020580270793288946, 0.0020580270793288946, 0.0020580270793288946, 0.0020580270793288946, 0.0020580270793288946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020580270793288946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205803
Iteration 2/1000 | Loss: 0.00003195
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001808
Iteration 5/1000 | Loss: 0.00016654
Iteration 6/1000 | Loss: 0.00003831
Iteration 7/1000 | Loss: 0.00002171
Iteration 8/1000 | Loss: 0.00002224
Iteration 9/1000 | Loss: 0.00001655
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00014758
Iteration 12/1000 | Loss: 0.00002895
Iteration 13/1000 | Loss: 0.00002301
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00007903
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00003685
Iteration 18/1000 | Loss: 0.00002221
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00002291
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00008635
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001397
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00013009
Iteration 37/1000 | Loss: 0.00002649
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001376
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001372
Iteration 47/1000 | Loss: 0.00001371
Iteration 48/1000 | Loss: 0.00001371
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001364
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001352
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001351
Iteration 76/1000 | Loss: 0.00001351
Iteration 77/1000 | Loss: 0.00001351
Iteration 78/1000 | Loss: 0.00001351
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001348
Iteration 100/1000 | Loss: 0.00001348
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001346
Iteration 109/1000 | Loss: 0.00001346
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001346
Iteration 118/1000 | Loss: 0.00001346
Iteration 119/1000 | Loss: 0.00001346
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001345
Iteration 126/1000 | Loss: 0.00001345
Iteration 127/1000 | Loss: 0.00001345
Iteration 128/1000 | Loss: 0.00001345
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001344
Iteration 135/1000 | Loss: 0.00001344
Iteration 136/1000 | Loss: 0.00001344
Iteration 137/1000 | Loss: 0.00001344
Iteration 138/1000 | Loss: 0.00001344
Iteration 139/1000 | Loss: 0.00001344
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001342
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001339
Iteration 175/1000 | Loss: 0.00001339
Iteration 176/1000 | Loss: 0.00001339
Iteration 177/1000 | Loss: 0.00001338
Iteration 178/1000 | Loss: 0.00001338
Iteration 179/1000 | Loss: 0.00001338
Iteration 180/1000 | Loss: 0.00001338
Iteration 181/1000 | Loss: 0.00001338
Iteration 182/1000 | Loss: 0.00001338
Iteration 183/1000 | Loss: 0.00001337
Iteration 184/1000 | Loss: 0.00001337
Iteration 185/1000 | Loss: 0.00001337
Iteration 186/1000 | Loss: 0.00001337
Iteration 187/1000 | Loss: 0.00001337
Iteration 188/1000 | Loss: 0.00001337
Iteration 189/1000 | Loss: 0.00001337
Iteration 190/1000 | Loss: 0.00001337
Iteration 191/1000 | Loss: 0.00001337
Iteration 192/1000 | Loss: 0.00001337
Iteration 193/1000 | Loss: 0.00001337
Iteration 194/1000 | Loss: 0.00001337
Iteration 195/1000 | Loss: 0.00001337
Iteration 196/1000 | Loss: 0.00001337
Iteration 197/1000 | Loss: 0.00001337
Iteration 198/1000 | Loss: 0.00001337
Iteration 199/1000 | Loss: 0.00001337
Iteration 200/1000 | Loss: 0.00001337
Iteration 201/1000 | Loss: 0.00001337
Iteration 202/1000 | Loss: 0.00001337
Iteration 203/1000 | Loss: 0.00001337
Iteration 204/1000 | Loss: 0.00001337
Iteration 205/1000 | Loss: 0.00001337
Iteration 206/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.337069261353463e-05, 1.337069261353463e-05, 1.337069261353463e-05, 1.337069261353463e-05, 1.337069261353463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.337069261353463e-05

Optimization complete. Final v2v error: 3.151198148727417 mm

Highest mean error: 3.8056387901306152 mm for frame 71

Lowest mean error: 2.7512903213500977 mm for frame 41

Saving results

Total time: 98.89763927459717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424255
Iteration 2/25 | Loss: 0.00142783
Iteration 3/25 | Loss: 0.00135642
Iteration 4/25 | Loss: 0.00134505
Iteration 5/25 | Loss: 0.00134124
Iteration 6/25 | Loss: 0.00134034
Iteration 7/25 | Loss: 0.00134034
Iteration 8/25 | Loss: 0.00134034
Iteration 9/25 | Loss: 0.00134034
Iteration 10/25 | Loss: 0.00134034
Iteration 11/25 | Loss: 0.00134034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013403380289673805, 0.0013403380289673805, 0.0013403380289673805, 0.0013403380289673805, 0.0013403380289673805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013403380289673805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03290844
Iteration 2/25 | Loss: 0.00195268
Iteration 3/25 | Loss: 0.00195267
Iteration 4/25 | Loss: 0.00195267
Iteration 5/25 | Loss: 0.00195267
Iteration 6/25 | Loss: 0.00195267
Iteration 7/25 | Loss: 0.00195267
Iteration 8/25 | Loss: 0.00195267
Iteration 9/25 | Loss: 0.00195267
Iteration 10/25 | Loss: 0.00195267
Iteration 11/25 | Loss: 0.00195267
Iteration 12/25 | Loss: 0.00195267
Iteration 13/25 | Loss: 0.00195267
Iteration 14/25 | Loss: 0.00195267
Iteration 15/25 | Loss: 0.00195267
Iteration 16/25 | Loss: 0.00195267
Iteration 17/25 | Loss: 0.00195267
Iteration 18/25 | Loss: 0.00195267
Iteration 19/25 | Loss: 0.00195267
Iteration 20/25 | Loss: 0.00195267
Iteration 21/25 | Loss: 0.00195267
Iteration 22/25 | Loss: 0.00195267
Iteration 23/25 | Loss: 0.00195267
Iteration 24/25 | Loss: 0.00195267
Iteration 25/25 | Loss: 0.00195267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195267
Iteration 2/1000 | Loss: 0.00003872
Iteration 3/1000 | Loss: 0.00002287
Iteration 4/1000 | Loss: 0.00001978
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001623
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001471
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001343
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001325
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001316
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001302
Iteration 89/1000 | Loss: 0.00001302
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001301
Iteration 92/1000 | Loss: 0.00001301
Iteration 93/1000 | Loss: 0.00001301
Iteration 94/1000 | Loss: 0.00001301
Iteration 95/1000 | Loss: 0.00001301
Iteration 96/1000 | Loss: 0.00001301
Iteration 97/1000 | Loss: 0.00001301
Iteration 98/1000 | Loss: 0.00001301
Iteration 99/1000 | Loss: 0.00001301
Iteration 100/1000 | Loss: 0.00001301
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001300
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001298
Iteration 111/1000 | Loss: 0.00001298
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001295
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001290
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001289
Iteration 155/1000 | Loss: 0.00001289
Iteration 156/1000 | Loss: 0.00001289
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001288
Iteration 166/1000 | Loss: 0.00001288
Iteration 167/1000 | Loss: 0.00001288
Iteration 168/1000 | Loss: 0.00001288
Iteration 169/1000 | Loss: 0.00001288
Iteration 170/1000 | Loss: 0.00001288
Iteration 171/1000 | Loss: 0.00001288
Iteration 172/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.288115709030535e-05, 1.288115709030535e-05, 1.288115709030535e-05, 1.288115709030535e-05, 1.288115709030535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.288115709030535e-05

Optimization complete. Final v2v error: 3.1195318698883057 mm

Highest mean error: 3.137030839920044 mm for frame 41

Lowest mean error: 3.106689929962158 mm for frame 77

Saving results

Total time: 39.84661626815796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438030
Iteration 2/25 | Loss: 0.00146717
Iteration 3/25 | Loss: 0.00137848
Iteration 4/25 | Loss: 0.00136993
Iteration 5/25 | Loss: 0.00136739
Iteration 6/25 | Loss: 0.00136734
Iteration 7/25 | Loss: 0.00136734
Iteration 8/25 | Loss: 0.00136734
Iteration 9/25 | Loss: 0.00136734
Iteration 10/25 | Loss: 0.00136734
Iteration 11/25 | Loss: 0.00136734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013673402136191726, 0.0013673402136191726, 0.0013673402136191726, 0.0013673402136191726, 0.0013673402136191726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013673402136191726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23040462
Iteration 2/25 | Loss: 0.00218987
Iteration 3/25 | Loss: 0.00218986
Iteration 4/25 | Loss: 0.00218985
Iteration 5/25 | Loss: 0.00218985
Iteration 6/25 | Loss: 0.00218985
Iteration 7/25 | Loss: 0.00218985
Iteration 8/25 | Loss: 0.00218985
Iteration 9/25 | Loss: 0.00218985
Iteration 10/25 | Loss: 0.00218985
Iteration 11/25 | Loss: 0.00218985
Iteration 12/25 | Loss: 0.00218985
Iteration 13/25 | Loss: 0.00218985
Iteration 14/25 | Loss: 0.00218985
Iteration 15/25 | Loss: 0.00218985
Iteration 16/25 | Loss: 0.00218985
Iteration 17/25 | Loss: 0.00218985
Iteration 18/25 | Loss: 0.00218985
Iteration 19/25 | Loss: 0.00218985
Iteration 20/25 | Loss: 0.00218985
Iteration 21/25 | Loss: 0.00218985
Iteration 22/25 | Loss: 0.00218985
Iteration 23/25 | Loss: 0.00218985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0021898518316447735, 0.0021898518316447735, 0.0021898518316447735, 0.0021898518316447735, 0.0021898518316447735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021898518316447735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218985
Iteration 2/1000 | Loss: 0.00002929
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001561
Iteration 6/1000 | Loss: 0.00001490
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001392
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001313
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001290
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001260
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001232
Iteration 61/1000 | Loss: 0.00001232
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001223
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001219
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001217
Iteration 115/1000 | Loss: 0.00001217
Iteration 116/1000 | Loss: 0.00001217
Iteration 117/1000 | Loss: 0.00001217
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001215
Iteration 125/1000 | Loss: 0.00001215
Iteration 126/1000 | Loss: 0.00001215
Iteration 127/1000 | Loss: 0.00001215
Iteration 128/1000 | Loss: 0.00001215
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001213
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00001212
Iteration 146/1000 | Loss: 0.00001212
Iteration 147/1000 | Loss: 0.00001212
Iteration 148/1000 | Loss: 0.00001212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2123297892685514e-05, 1.2123297892685514e-05, 1.2123297892685514e-05, 1.2123297892685514e-05, 1.2123297892685514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2123297892685514e-05

Optimization complete. Final v2v error: 3.0085513591766357 mm

Highest mean error: 3.8440358638763428 mm for frame 62

Lowest mean error: 2.590467929840088 mm for frame 29

Saving results

Total time: 41.54818892478943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415323
Iteration 2/25 | Loss: 0.00143764
Iteration 3/25 | Loss: 0.00137488
Iteration 4/25 | Loss: 0.00136441
Iteration 5/25 | Loss: 0.00136168
Iteration 6/25 | Loss: 0.00136150
Iteration 7/25 | Loss: 0.00136150
Iteration 8/25 | Loss: 0.00136150
Iteration 9/25 | Loss: 0.00136150
Iteration 10/25 | Loss: 0.00136150
Iteration 11/25 | Loss: 0.00136150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013615009374916553, 0.0013615009374916553, 0.0013615009374916553, 0.0013615009374916553, 0.0013615009374916553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013615009374916553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25068045
Iteration 2/25 | Loss: 0.00200965
Iteration 3/25 | Loss: 0.00200964
Iteration 4/25 | Loss: 0.00200964
Iteration 5/25 | Loss: 0.00200964
Iteration 6/25 | Loss: 0.00200964
Iteration 7/25 | Loss: 0.00200964
Iteration 8/25 | Loss: 0.00200964
Iteration 9/25 | Loss: 0.00200964
Iteration 10/25 | Loss: 0.00200964
Iteration 11/25 | Loss: 0.00200964
Iteration 12/25 | Loss: 0.00200964
Iteration 13/25 | Loss: 0.00200964
Iteration 14/25 | Loss: 0.00200964
Iteration 15/25 | Loss: 0.00200964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002009641146287322, 0.002009641146287322, 0.002009641146287322, 0.002009641146287322, 0.002009641146287322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002009641146287322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200964
Iteration 2/1000 | Loss: 0.00002546
Iteration 3/1000 | Loss: 0.00001865
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001647
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001558
Iteration 8/1000 | Loss: 0.00001500
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001332
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001319
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001317
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001309
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001302
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001301
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001301
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001300
Iteration 104/1000 | Loss: 0.00001300
Iteration 105/1000 | Loss: 0.00001300
Iteration 106/1000 | Loss: 0.00001300
Iteration 107/1000 | Loss: 0.00001300
Iteration 108/1000 | Loss: 0.00001300
Iteration 109/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.299873383686645e-05, 1.299873383686645e-05, 1.299873383686645e-05, 1.299873383686645e-05, 1.299873383686645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.299873383686645e-05

Optimization complete. Final v2v error: 3.1655895709991455 mm

Highest mean error: 3.1961982250213623 mm for frame 138

Lowest mean error: 3.0948634147644043 mm for frame 121

Saving results

Total time: 34.88987421989441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791959
Iteration 2/25 | Loss: 0.00181320
Iteration 3/25 | Loss: 0.00158650
Iteration 4/25 | Loss: 0.00156604
Iteration 5/25 | Loss: 0.00156230
Iteration 6/25 | Loss: 0.00156200
Iteration 7/25 | Loss: 0.00156200
Iteration 8/25 | Loss: 0.00156199
Iteration 9/25 | Loss: 0.00156200
Iteration 10/25 | Loss: 0.00156199
Iteration 11/25 | Loss: 0.00156199
Iteration 12/25 | Loss: 0.00156199
Iteration 13/25 | Loss: 0.00156199
Iteration 14/25 | Loss: 0.00156199
Iteration 15/25 | Loss: 0.00156200
Iteration 16/25 | Loss: 0.00156200
Iteration 17/25 | Loss: 0.00156199
Iteration 18/25 | Loss: 0.00156199
Iteration 19/25 | Loss: 0.00156199
Iteration 20/25 | Loss: 0.00156199
Iteration 21/25 | Loss: 0.00156199
Iteration 22/25 | Loss: 0.00156199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015619948972016573, 0.0015619948972016573, 0.0015619948972016573, 0.0015619948972016573, 0.0015619948972016573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015619948972016573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.24606419
Iteration 2/25 | Loss: 0.00257243
Iteration 3/25 | Loss: 0.00257243
Iteration 4/25 | Loss: 0.00257243
Iteration 5/25 | Loss: 0.00257242
Iteration 6/25 | Loss: 0.00257242
Iteration 7/25 | Loss: 0.00257242
Iteration 8/25 | Loss: 0.00257242
Iteration 9/25 | Loss: 0.00257242
Iteration 10/25 | Loss: 0.00257242
Iteration 11/25 | Loss: 0.00257242
Iteration 12/25 | Loss: 0.00257242
Iteration 13/25 | Loss: 0.00257242
Iteration 14/25 | Loss: 0.00257242
Iteration 15/25 | Loss: 0.00257242
Iteration 16/25 | Loss: 0.00257242
Iteration 17/25 | Loss: 0.00257242
Iteration 18/25 | Loss: 0.00257242
Iteration 19/25 | Loss: 0.00257242
Iteration 20/25 | Loss: 0.00257242
Iteration 21/25 | Loss: 0.00257242
Iteration 22/25 | Loss: 0.00257242
Iteration 23/25 | Loss: 0.00257242
Iteration 24/25 | Loss: 0.00257242
Iteration 25/25 | Loss: 0.00257242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257242
Iteration 2/1000 | Loss: 0.00006539
Iteration 3/1000 | Loss: 0.00004435
Iteration 4/1000 | Loss: 0.00003583
Iteration 5/1000 | Loss: 0.00003168
Iteration 6/1000 | Loss: 0.00002979
Iteration 7/1000 | Loss: 0.00002868
Iteration 8/1000 | Loss: 0.00002763
Iteration 9/1000 | Loss: 0.00002712
Iteration 10/1000 | Loss: 0.00002676
Iteration 11/1000 | Loss: 0.00002649
Iteration 12/1000 | Loss: 0.00002628
Iteration 13/1000 | Loss: 0.00002604
Iteration 14/1000 | Loss: 0.00002590
Iteration 15/1000 | Loss: 0.00002590
Iteration 16/1000 | Loss: 0.00002587
Iteration 17/1000 | Loss: 0.00002585
Iteration 18/1000 | Loss: 0.00002574
Iteration 19/1000 | Loss: 0.00002572
Iteration 20/1000 | Loss: 0.00002568
Iteration 21/1000 | Loss: 0.00002567
Iteration 22/1000 | Loss: 0.00002565
Iteration 23/1000 | Loss: 0.00002564
Iteration 24/1000 | Loss: 0.00002564
Iteration 25/1000 | Loss: 0.00002564
Iteration 26/1000 | Loss: 0.00002563
Iteration 27/1000 | Loss: 0.00002563
Iteration 28/1000 | Loss: 0.00002561
Iteration 29/1000 | Loss: 0.00002560
Iteration 30/1000 | Loss: 0.00002560
Iteration 31/1000 | Loss: 0.00002559
Iteration 32/1000 | Loss: 0.00002559
Iteration 33/1000 | Loss: 0.00002559
Iteration 34/1000 | Loss: 0.00002558
Iteration 35/1000 | Loss: 0.00002558
Iteration 36/1000 | Loss: 0.00002558
Iteration 37/1000 | Loss: 0.00002557
Iteration 38/1000 | Loss: 0.00002557
Iteration 39/1000 | Loss: 0.00002556
Iteration 40/1000 | Loss: 0.00002556
Iteration 41/1000 | Loss: 0.00002556
Iteration 42/1000 | Loss: 0.00002556
Iteration 43/1000 | Loss: 0.00002556
Iteration 44/1000 | Loss: 0.00002555
Iteration 45/1000 | Loss: 0.00002555
Iteration 46/1000 | Loss: 0.00002555
Iteration 47/1000 | Loss: 0.00002555
Iteration 48/1000 | Loss: 0.00002554
Iteration 49/1000 | Loss: 0.00002554
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002553
Iteration 52/1000 | Loss: 0.00002553
Iteration 53/1000 | Loss: 0.00002552
Iteration 54/1000 | Loss: 0.00002552
Iteration 55/1000 | Loss: 0.00002552
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002551
Iteration 58/1000 | Loss: 0.00002551
Iteration 59/1000 | Loss: 0.00002551
Iteration 60/1000 | Loss: 0.00002551
Iteration 61/1000 | Loss: 0.00002550
Iteration 62/1000 | Loss: 0.00002550
Iteration 63/1000 | Loss: 0.00002549
Iteration 64/1000 | Loss: 0.00002549
Iteration 65/1000 | Loss: 0.00002548
Iteration 66/1000 | Loss: 0.00002548
Iteration 67/1000 | Loss: 0.00002548
Iteration 68/1000 | Loss: 0.00002547
Iteration 69/1000 | Loss: 0.00002547
Iteration 70/1000 | Loss: 0.00002547
Iteration 71/1000 | Loss: 0.00002547
Iteration 72/1000 | Loss: 0.00002547
Iteration 73/1000 | Loss: 0.00002547
Iteration 74/1000 | Loss: 0.00002547
Iteration 75/1000 | Loss: 0.00002547
Iteration 76/1000 | Loss: 0.00002546
Iteration 77/1000 | Loss: 0.00002546
Iteration 78/1000 | Loss: 0.00002546
Iteration 79/1000 | Loss: 0.00002546
Iteration 80/1000 | Loss: 0.00002546
Iteration 81/1000 | Loss: 0.00002545
Iteration 82/1000 | Loss: 0.00002545
Iteration 83/1000 | Loss: 0.00002545
Iteration 84/1000 | Loss: 0.00002545
Iteration 85/1000 | Loss: 0.00002545
Iteration 86/1000 | Loss: 0.00002545
Iteration 87/1000 | Loss: 0.00002545
Iteration 88/1000 | Loss: 0.00002545
Iteration 89/1000 | Loss: 0.00002545
Iteration 90/1000 | Loss: 0.00002545
Iteration 91/1000 | Loss: 0.00002544
Iteration 92/1000 | Loss: 0.00002543
Iteration 93/1000 | Loss: 0.00002543
Iteration 94/1000 | Loss: 0.00002543
Iteration 95/1000 | Loss: 0.00002543
Iteration 96/1000 | Loss: 0.00002543
Iteration 97/1000 | Loss: 0.00002542
Iteration 98/1000 | Loss: 0.00002542
Iteration 99/1000 | Loss: 0.00002542
Iteration 100/1000 | Loss: 0.00002542
Iteration 101/1000 | Loss: 0.00002541
Iteration 102/1000 | Loss: 0.00002541
Iteration 103/1000 | Loss: 0.00002541
Iteration 104/1000 | Loss: 0.00002541
Iteration 105/1000 | Loss: 0.00002540
Iteration 106/1000 | Loss: 0.00002540
Iteration 107/1000 | Loss: 0.00002540
Iteration 108/1000 | Loss: 0.00002540
Iteration 109/1000 | Loss: 0.00002540
Iteration 110/1000 | Loss: 0.00002540
Iteration 111/1000 | Loss: 0.00002539
Iteration 112/1000 | Loss: 0.00002539
Iteration 113/1000 | Loss: 0.00002539
Iteration 114/1000 | Loss: 0.00002539
Iteration 115/1000 | Loss: 0.00002539
Iteration 116/1000 | Loss: 0.00002538
Iteration 117/1000 | Loss: 0.00002538
Iteration 118/1000 | Loss: 0.00002538
Iteration 119/1000 | Loss: 0.00002538
Iteration 120/1000 | Loss: 0.00002538
Iteration 121/1000 | Loss: 0.00002538
Iteration 122/1000 | Loss: 0.00002538
Iteration 123/1000 | Loss: 0.00002538
Iteration 124/1000 | Loss: 0.00002537
Iteration 125/1000 | Loss: 0.00002537
Iteration 126/1000 | Loss: 0.00002537
Iteration 127/1000 | Loss: 0.00002537
Iteration 128/1000 | Loss: 0.00002536
Iteration 129/1000 | Loss: 0.00002536
Iteration 130/1000 | Loss: 0.00002536
Iteration 131/1000 | Loss: 0.00002536
Iteration 132/1000 | Loss: 0.00002536
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002536
Iteration 135/1000 | Loss: 0.00002535
Iteration 136/1000 | Loss: 0.00002535
Iteration 137/1000 | Loss: 0.00002535
Iteration 138/1000 | Loss: 0.00002535
Iteration 139/1000 | Loss: 0.00002535
Iteration 140/1000 | Loss: 0.00002535
Iteration 141/1000 | Loss: 0.00002535
Iteration 142/1000 | Loss: 0.00002535
Iteration 143/1000 | Loss: 0.00002535
Iteration 144/1000 | Loss: 0.00002535
Iteration 145/1000 | Loss: 0.00002535
Iteration 146/1000 | Loss: 0.00002535
Iteration 147/1000 | Loss: 0.00002534
Iteration 148/1000 | Loss: 0.00002534
Iteration 149/1000 | Loss: 0.00002534
Iteration 150/1000 | Loss: 0.00002533
Iteration 151/1000 | Loss: 0.00002533
Iteration 152/1000 | Loss: 0.00002533
Iteration 153/1000 | Loss: 0.00002533
Iteration 154/1000 | Loss: 0.00002533
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00002532
Iteration 157/1000 | Loss: 0.00002532
Iteration 158/1000 | Loss: 0.00002532
Iteration 159/1000 | Loss: 0.00002532
Iteration 160/1000 | Loss: 0.00002531
Iteration 161/1000 | Loss: 0.00002531
Iteration 162/1000 | Loss: 0.00002531
Iteration 163/1000 | Loss: 0.00002531
Iteration 164/1000 | Loss: 0.00002530
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002530
Iteration 170/1000 | Loss: 0.00002530
Iteration 171/1000 | Loss: 0.00002530
Iteration 172/1000 | Loss: 0.00002529
Iteration 173/1000 | Loss: 0.00002529
Iteration 174/1000 | Loss: 0.00002529
Iteration 175/1000 | Loss: 0.00002529
Iteration 176/1000 | Loss: 0.00002529
Iteration 177/1000 | Loss: 0.00002529
Iteration 178/1000 | Loss: 0.00002529
Iteration 179/1000 | Loss: 0.00002529
Iteration 180/1000 | Loss: 0.00002528
Iteration 181/1000 | Loss: 0.00002528
Iteration 182/1000 | Loss: 0.00002528
Iteration 183/1000 | Loss: 0.00002528
Iteration 184/1000 | Loss: 0.00002528
Iteration 185/1000 | Loss: 0.00002528
Iteration 186/1000 | Loss: 0.00002528
Iteration 187/1000 | Loss: 0.00002527
Iteration 188/1000 | Loss: 0.00002527
Iteration 189/1000 | Loss: 0.00002527
Iteration 190/1000 | Loss: 0.00002527
Iteration 191/1000 | Loss: 0.00002527
Iteration 192/1000 | Loss: 0.00002527
Iteration 193/1000 | Loss: 0.00002527
Iteration 194/1000 | Loss: 0.00002527
Iteration 195/1000 | Loss: 0.00002526
Iteration 196/1000 | Loss: 0.00002526
Iteration 197/1000 | Loss: 0.00002526
Iteration 198/1000 | Loss: 0.00002526
Iteration 199/1000 | Loss: 0.00002526
Iteration 200/1000 | Loss: 0.00002526
Iteration 201/1000 | Loss: 0.00002526
Iteration 202/1000 | Loss: 0.00002526
Iteration 203/1000 | Loss: 0.00002526
Iteration 204/1000 | Loss: 0.00002525
Iteration 205/1000 | Loss: 0.00002525
Iteration 206/1000 | Loss: 0.00002525
Iteration 207/1000 | Loss: 0.00002525
Iteration 208/1000 | Loss: 0.00002525
Iteration 209/1000 | Loss: 0.00002525
Iteration 210/1000 | Loss: 0.00002525
Iteration 211/1000 | Loss: 0.00002525
Iteration 212/1000 | Loss: 0.00002525
Iteration 213/1000 | Loss: 0.00002525
Iteration 214/1000 | Loss: 0.00002525
Iteration 215/1000 | Loss: 0.00002525
Iteration 216/1000 | Loss: 0.00002525
Iteration 217/1000 | Loss: 0.00002525
Iteration 218/1000 | Loss: 0.00002525
Iteration 219/1000 | Loss: 0.00002525
Iteration 220/1000 | Loss: 0.00002525
Iteration 221/1000 | Loss: 0.00002524
Iteration 222/1000 | Loss: 0.00002524
Iteration 223/1000 | Loss: 0.00002524
Iteration 224/1000 | Loss: 0.00002524
Iteration 225/1000 | Loss: 0.00002524
Iteration 226/1000 | Loss: 0.00002524
Iteration 227/1000 | Loss: 0.00002524
Iteration 228/1000 | Loss: 0.00002524
Iteration 229/1000 | Loss: 0.00002524
Iteration 230/1000 | Loss: 0.00002524
Iteration 231/1000 | Loss: 0.00002524
Iteration 232/1000 | Loss: 0.00002524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [2.5243520212825388e-05, 2.5243520212825388e-05, 2.5243520212825388e-05, 2.5243520212825388e-05, 2.5243520212825388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5243520212825388e-05

Optimization complete. Final v2v error: 4.105621814727783 mm

Highest mean error: 4.843567848205566 mm for frame 5

Lowest mean error: 3.5582144260406494 mm for frame 139

Saving results

Total time: 45.420448303222656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833061
Iteration 2/25 | Loss: 0.00173479
Iteration 3/25 | Loss: 0.00148956
Iteration 4/25 | Loss: 0.00146260
Iteration 5/25 | Loss: 0.00145782
Iteration 6/25 | Loss: 0.00145782
Iteration 7/25 | Loss: 0.00145782
Iteration 8/25 | Loss: 0.00145782
Iteration 9/25 | Loss: 0.00145782
Iteration 10/25 | Loss: 0.00145782
Iteration 11/25 | Loss: 0.00145782
Iteration 12/25 | Loss: 0.00145782
Iteration 13/25 | Loss: 0.00145782
Iteration 14/25 | Loss: 0.00145782
Iteration 15/25 | Loss: 0.00145782
Iteration 16/25 | Loss: 0.00145782
Iteration 17/25 | Loss: 0.00145782
Iteration 18/25 | Loss: 0.00145782
Iteration 19/25 | Loss: 0.00145782
Iteration 20/25 | Loss: 0.00145782
Iteration 21/25 | Loss: 0.00145782
Iteration 22/25 | Loss: 0.00145782
Iteration 23/25 | Loss: 0.00145782
Iteration 24/25 | Loss: 0.00145782
Iteration 25/25 | Loss: 0.00145782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.14853652
Iteration 2/25 | Loss: 0.00186690
Iteration 3/25 | Loss: 0.00186688
Iteration 4/25 | Loss: 0.00186688
Iteration 5/25 | Loss: 0.00186688
Iteration 6/25 | Loss: 0.00186688
Iteration 7/25 | Loss: 0.00186688
Iteration 8/25 | Loss: 0.00186688
Iteration 9/25 | Loss: 0.00186688
Iteration 10/25 | Loss: 0.00186688
Iteration 11/25 | Loss: 0.00186688
Iteration 12/25 | Loss: 0.00186688
Iteration 13/25 | Loss: 0.00186688
Iteration 14/25 | Loss: 0.00186688
Iteration 15/25 | Loss: 0.00186688
Iteration 16/25 | Loss: 0.00186688
Iteration 17/25 | Loss: 0.00186688
Iteration 18/25 | Loss: 0.00186688
Iteration 19/25 | Loss: 0.00186688
Iteration 20/25 | Loss: 0.00186688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018668784759938717, 0.0018668784759938717, 0.0018668784759938717, 0.0018668784759938717, 0.0018668784759938717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018668784759938717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186688
Iteration 2/1000 | Loss: 0.00007382
Iteration 3/1000 | Loss: 0.00003832
Iteration 4/1000 | Loss: 0.00002950
Iteration 5/1000 | Loss: 0.00002740
Iteration 6/1000 | Loss: 0.00002589
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002461
Iteration 9/1000 | Loss: 0.00002417
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002352
Iteration 12/1000 | Loss: 0.00002320
Iteration 13/1000 | Loss: 0.00002299
Iteration 14/1000 | Loss: 0.00002284
Iteration 15/1000 | Loss: 0.00002283
Iteration 16/1000 | Loss: 0.00002283
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002280
Iteration 19/1000 | Loss: 0.00002279
Iteration 20/1000 | Loss: 0.00002278
Iteration 21/1000 | Loss: 0.00002278
Iteration 22/1000 | Loss: 0.00002277
Iteration 23/1000 | Loss: 0.00002277
Iteration 24/1000 | Loss: 0.00002277
Iteration 25/1000 | Loss: 0.00002274
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002267
Iteration 28/1000 | Loss: 0.00002267
Iteration 29/1000 | Loss: 0.00002267
Iteration 30/1000 | Loss: 0.00002266
Iteration 31/1000 | Loss: 0.00002260
Iteration 32/1000 | Loss: 0.00002260
Iteration 33/1000 | Loss: 0.00002259
Iteration 34/1000 | Loss: 0.00002259
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002257
Iteration 40/1000 | Loss: 0.00002257
Iteration 41/1000 | Loss: 0.00002257
Iteration 42/1000 | Loss: 0.00002256
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002256
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002255
Iteration 51/1000 | Loss: 0.00002255
Iteration 52/1000 | Loss: 0.00002255
Iteration 53/1000 | Loss: 0.00002255
Iteration 54/1000 | Loss: 0.00002255
Iteration 55/1000 | Loss: 0.00002255
Iteration 56/1000 | Loss: 0.00002255
Iteration 57/1000 | Loss: 0.00002255
Iteration 58/1000 | Loss: 0.00002254
Iteration 59/1000 | Loss: 0.00002254
Iteration 60/1000 | Loss: 0.00002254
Iteration 61/1000 | Loss: 0.00002254
Iteration 62/1000 | Loss: 0.00002251
Iteration 63/1000 | Loss: 0.00002250
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002250
Iteration 66/1000 | Loss: 0.00002249
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002248
Iteration 70/1000 | Loss: 0.00002248
Iteration 71/1000 | Loss: 0.00002248
Iteration 72/1000 | Loss: 0.00002248
Iteration 73/1000 | Loss: 0.00002248
Iteration 74/1000 | Loss: 0.00002248
Iteration 75/1000 | Loss: 0.00002248
Iteration 76/1000 | Loss: 0.00002248
Iteration 77/1000 | Loss: 0.00002248
Iteration 78/1000 | Loss: 0.00002248
Iteration 79/1000 | Loss: 0.00002248
Iteration 80/1000 | Loss: 0.00002248
Iteration 81/1000 | Loss: 0.00002248
Iteration 82/1000 | Loss: 0.00002247
Iteration 83/1000 | Loss: 0.00002247
Iteration 84/1000 | Loss: 0.00002247
Iteration 85/1000 | Loss: 0.00002246
Iteration 86/1000 | Loss: 0.00002246
Iteration 87/1000 | Loss: 0.00002246
Iteration 88/1000 | Loss: 0.00002245
Iteration 89/1000 | Loss: 0.00002245
Iteration 90/1000 | Loss: 0.00002245
Iteration 91/1000 | Loss: 0.00002244
Iteration 92/1000 | Loss: 0.00002244
Iteration 93/1000 | Loss: 0.00002244
Iteration 94/1000 | Loss: 0.00002244
Iteration 95/1000 | Loss: 0.00002244
Iteration 96/1000 | Loss: 0.00002244
Iteration 97/1000 | Loss: 0.00002243
Iteration 98/1000 | Loss: 0.00002243
Iteration 99/1000 | Loss: 0.00002243
Iteration 100/1000 | Loss: 0.00002243
Iteration 101/1000 | Loss: 0.00002243
Iteration 102/1000 | Loss: 0.00002243
Iteration 103/1000 | Loss: 0.00002243
Iteration 104/1000 | Loss: 0.00002243
Iteration 105/1000 | Loss: 0.00002243
Iteration 106/1000 | Loss: 0.00002243
Iteration 107/1000 | Loss: 0.00002243
Iteration 108/1000 | Loss: 0.00002242
Iteration 109/1000 | Loss: 0.00002242
Iteration 110/1000 | Loss: 0.00002242
Iteration 111/1000 | Loss: 0.00002242
Iteration 112/1000 | Loss: 0.00002242
Iteration 113/1000 | Loss: 0.00002242
Iteration 114/1000 | Loss: 0.00002242
Iteration 115/1000 | Loss: 0.00002242
Iteration 116/1000 | Loss: 0.00002242
Iteration 117/1000 | Loss: 0.00002242
Iteration 118/1000 | Loss: 0.00002242
Iteration 119/1000 | Loss: 0.00002242
Iteration 120/1000 | Loss: 0.00002242
Iteration 121/1000 | Loss: 0.00002242
Iteration 122/1000 | Loss: 0.00002242
Iteration 123/1000 | Loss: 0.00002242
Iteration 124/1000 | Loss: 0.00002242
Iteration 125/1000 | Loss: 0.00002242
Iteration 126/1000 | Loss: 0.00002242
Iteration 127/1000 | Loss: 0.00002242
Iteration 128/1000 | Loss: 0.00002242
Iteration 129/1000 | Loss: 0.00002241
Iteration 130/1000 | Loss: 0.00002241
Iteration 131/1000 | Loss: 0.00002241
Iteration 132/1000 | Loss: 0.00002241
Iteration 133/1000 | Loss: 0.00002241
Iteration 134/1000 | Loss: 0.00002241
Iteration 135/1000 | Loss: 0.00002241
Iteration 136/1000 | Loss: 0.00002241
Iteration 137/1000 | Loss: 0.00002241
Iteration 138/1000 | Loss: 0.00002241
Iteration 139/1000 | Loss: 0.00002241
Iteration 140/1000 | Loss: 0.00002241
Iteration 141/1000 | Loss: 0.00002241
Iteration 142/1000 | Loss: 0.00002240
Iteration 143/1000 | Loss: 0.00002240
Iteration 144/1000 | Loss: 0.00002240
Iteration 145/1000 | Loss: 0.00002240
Iteration 146/1000 | Loss: 0.00002240
Iteration 147/1000 | Loss: 0.00002240
Iteration 148/1000 | Loss: 0.00002240
Iteration 149/1000 | Loss: 0.00002240
Iteration 150/1000 | Loss: 0.00002239
Iteration 151/1000 | Loss: 0.00002239
Iteration 152/1000 | Loss: 0.00002239
Iteration 153/1000 | Loss: 0.00002239
Iteration 154/1000 | Loss: 0.00002238
Iteration 155/1000 | Loss: 0.00002238
Iteration 156/1000 | Loss: 0.00002238
Iteration 157/1000 | Loss: 0.00002238
Iteration 158/1000 | Loss: 0.00002238
Iteration 159/1000 | Loss: 0.00002238
Iteration 160/1000 | Loss: 0.00002238
Iteration 161/1000 | Loss: 0.00002238
Iteration 162/1000 | Loss: 0.00002238
Iteration 163/1000 | Loss: 0.00002238
Iteration 164/1000 | Loss: 0.00002238
Iteration 165/1000 | Loss: 0.00002238
Iteration 166/1000 | Loss: 0.00002238
Iteration 167/1000 | Loss: 0.00002238
Iteration 168/1000 | Loss: 0.00002238
Iteration 169/1000 | Loss: 0.00002238
Iteration 170/1000 | Loss: 0.00002238
Iteration 171/1000 | Loss: 0.00002238
Iteration 172/1000 | Loss: 0.00002238
Iteration 173/1000 | Loss: 0.00002238
Iteration 174/1000 | Loss: 0.00002238
Iteration 175/1000 | Loss: 0.00002238
Iteration 176/1000 | Loss: 0.00002238
Iteration 177/1000 | Loss: 0.00002238
Iteration 178/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.2375446860678494e-05, 2.2375446860678494e-05, 2.2375446860678494e-05, 2.2375446860678494e-05, 2.2375446860678494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2375446860678494e-05

Optimization complete. Final v2v error: 4.06822395324707 mm

Highest mean error: 4.229303359985352 mm for frame 163

Lowest mean error: 3.657726526260376 mm for frame 102

Saving results

Total time: 45.24731802940369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920436
Iteration 2/25 | Loss: 0.00188540
Iteration 3/25 | Loss: 0.00153543
Iteration 4/25 | Loss: 0.00151565
Iteration 5/25 | Loss: 0.00150951
Iteration 6/25 | Loss: 0.00150887
Iteration 7/25 | Loss: 0.00150887
Iteration 8/25 | Loss: 0.00150887
Iteration 9/25 | Loss: 0.00150887
Iteration 10/25 | Loss: 0.00150887
Iteration 11/25 | Loss: 0.00150887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015088743530213833, 0.0015088743530213833, 0.0015088743530213833, 0.0015088743530213833, 0.0015088743530213833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015088743530213833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82705635
Iteration 2/25 | Loss: 0.00230941
Iteration 3/25 | Loss: 0.00230941
Iteration 4/25 | Loss: 0.00230941
Iteration 5/25 | Loss: 0.00230941
Iteration 6/25 | Loss: 0.00230941
Iteration 7/25 | Loss: 0.00230941
Iteration 8/25 | Loss: 0.00230941
Iteration 9/25 | Loss: 0.00230941
Iteration 10/25 | Loss: 0.00230941
Iteration 11/25 | Loss: 0.00230941
Iteration 12/25 | Loss: 0.00230941
Iteration 13/25 | Loss: 0.00230941
Iteration 14/25 | Loss: 0.00230941
Iteration 15/25 | Loss: 0.00230941
Iteration 16/25 | Loss: 0.00230941
Iteration 17/25 | Loss: 0.00230941
Iteration 18/25 | Loss: 0.00230941
Iteration 19/25 | Loss: 0.00230941
Iteration 20/25 | Loss: 0.00230941
Iteration 21/25 | Loss: 0.00230941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002309407340362668, 0.002309407340362668, 0.002309407340362668, 0.002309407340362668, 0.002309407340362668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002309407340362668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230941
Iteration 2/1000 | Loss: 0.00005751
Iteration 3/1000 | Loss: 0.00003991
Iteration 4/1000 | Loss: 0.00003613
Iteration 5/1000 | Loss: 0.00003434
Iteration 6/1000 | Loss: 0.00003317
Iteration 7/1000 | Loss: 0.00003228
Iteration 8/1000 | Loss: 0.00003175
Iteration 9/1000 | Loss: 0.00003117
Iteration 10/1000 | Loss: 0.00003073
Iteration 11/1000 | Loss: 0.00003047
Iteration 12/1000 | Loss: 0.00003011
Iteration 13/1000 | Loss: 0.00002973
Iteration 14/1000 | Loss: 0.00002953
Iteration 15/1000 | Loss: 0.00002924
Iteration 16/1000 | Loss: 0.00002895
Iteration 17/1000 | Loss: 0.00002873
Iteration 18/1000 | Loss: 0.00002862
Iteration 19/1000 | Loss: 0.00002847
Iteration 20/1000 | Loss: 0.00002846
Iteration 21/1000 | Loss: 0.00002835
Iteration 22/1000 | Loss: 0.00002827
Iteration 23/1000 | Loss: 0.00002827
Iteration 24/1000 | Loss: 0.00002822
Iteration 25/1000 | Loss: 0.00002822
Iteration 26/1000 | Loss: 0.00002822
Iteration 27/1000 | Loss: 0.00002822
Iteration 28/1000 | Loss: 0.00002822
Iteration 29/1000 | Loss: 0.00002822
Iteration 30/1000 | Loss: 0.00002821
Iteration 31/1000 | Loss: 0.00002821
Iteration 32/1000 | Loss: 0.00002816
Iteration 33/1000 | Loss: 0.00002816
Iteration 34/1000 | Loss: 0.00002815
Iteration 35/1000 | Loss: 0.00002814
Iteration 36/1000 | Loss: 0.00002813
Iteration 37/1000 | Loss: 0.00002812
Iteration 38/1000 | Loss: 0.00002812
Iteration 39/1000 | Loss: 0.00002812
Iteration 40/1000 | Loss: 0.00002812
Iteration 41/1000 | Loss: 0.00002812
Iteration 42/1000 | Loss: 0.00002812
Iteration 43/1000 | Loss: 0.00002812
Iteration 44/1000 | Loss: 0.00002812
Iteration 45/1000 | Loss: 0.00002811
Iteration 46/1000 | Loss: 0.00002811
Iteration 47/1000 | Loss: 0.00002810
Iteration 48/1000 | Loss: 0.00002810
Iteration 49/1000 | Loss: 0.00002808
Iteration 50/1000 | Loss: 0.00002808
Iteration 51/1000 | Loss: 0.00002808
Iteration 52/1000 | Loss: 0.00002808
Iteration 53/1000 | Loss: 0.00002808
Iteration 54/1000 | Loss: 0.00002807
Iteration 55/1000 | Loss: 0.00002807
Iteration 56/1000 | Loss: 0.00002806
Iteration 57/1000 | Loss: 0.00002806
Iteration 58/1000 | Loss: 0.00002806
Iteration 59/1000 | Loss: 0.00002806
Iteration 60/1000 | Loss: 0.00002806
Iteration 61/1000 | Loss: 0.00002806
Iteration 62/1000 | Loss: 0.00002806
Iteration 63/1000 | Loss: 0.00002806
Iteration 64/1000 | Loss: 0.00002805
Iteration 65/1000 | Loss: 0.00002805
Iteration 66/1000 | Loss: 0.00002805
Iteration 67/1000 | Loss: 0.00002804
Iteration 68/1000 | Loss: 0.00002804
Iteration 69/1000 | Loss: 0.00002804
Iteration 70/1000 | Loss: 0.00002804
Iteration 71/1000 | Loss: 0.00002803
Iteration 72/1000 | Loss: 0.00002803
Iteration 73/1000 | Loss: 0.00002803
Iteration 74/1000 | Loss: 0.00002803
Iteration 75/1000 | Loss: 0.00002803
Iteration 76/1000 | Loss: 0.00002803
Iteration 77/1000 | Loss: 0.00002803
Iteration 78/1000 | Loss: 0.00002803
Iteration 79/1000 | Loss: 0.00002803
Iteration 80/1000 | Loss: 0.00002803
Iteration 81/1000 | Loss: 0.00002802
Iteration 82/1000 | Loss: 0.00002802
Iteration 83/1000 | Loss: 0.00002801
Iteration 84/1000 | Loss: 0.00002801
Iteration 85/1000 | Loss: 0.00002801
Iteration 86/1000 | Loss: 0.00002800
Iteration 87/1000 | Loss: 0.00002800
Iteration 88/1000 | Loss: 0.00002800
Iteration 89/1000 | Loss: 0.00002800
Iteration 90/1000 | Loss: 0.00002799
Iteration 91/1000 | Loss: 0.00002799
Iteration 92/1000 | Loss: 0.00002799
Iteration 93/1000 | Loss: 0.00002799
Iteration 94/1000 | Loss: 0.00002799
Iteration 95/1000 | Loss: 0.00002799
Iteration 96/1000 | Loss: 0.00002799
Iteration 97/1000 | Loss: 0.00002799
Iteration 98/1000 | Loss: 0.00002799
Iteration 99/1000 | Loss: 0.00002799
Iteration 100/1000 | Loss: 0.00002798
Iteration 101/1000 | Loss: 0.00002798
Iteration 102/1000 | Loss: 0.00002798
Iteration 103/1000 | Loss: 0.00002798
Iteration 104/1000 | Loss: 0.00002798
Iteration 105/1000 | Loss: 0.00002798
Iteration 106/1000 | Loss: 0.00002798
Iteration 107/1000 | Loss: 0.00002797
Iteration 108/1000 | Loss: 0.00002797
Iteration 109/1000 | Loss: 0.00002797
Iteration 110/1000 | Loss: 0.00002797
Iteration 111/1000 | Loss: 0.00002797
Iteration 112/1000 | Loss: 0.00002796
Iteration 113/1000 | Loss: 0.00002796
Iteration 114/1000 | Loss: 0.00002796
Iteration 115/1000 | Loss: 0.00002796
Iteration 116/1000 | Loss: 0.00002796
Iteration 117/1000 | Loss: 0.00002796
Iteration 118/1000 | Loss: 0.00002796
Iteration 119/1000 | Loss: 0.00002796
Iteration 120/1000 | Loss: 0.00002796
Iteration 121/1000 | Loss: 0.00002796
Iteration 122/1000 | Loss: 0.00002796
Iteration 123/1000 | Loss: 0.00002796
Iteration 124/1000 | Loss: 0.00002796
Iteration 125/1000 | Loss: 0.00002796
Iteration 126/1000 | Loss: 0.00002796
Iteration 127/1000 | Loss: 0.00002796
Iteration 128/1000 | Loss: 0.00002796
Iteration 129/1000 | Loss: 0.00002796
Iteration 130/1000 | Loss: 0.00002796
Iteration 131/1000 | Loss: 0.00002796
Iteration 132/1000 | Loss: 0.00002796
Iteration 133/1000 | Loss: 0.00002796
Iteration 134/1000 | Loss: 0.00002796
Iteration 135/1000 | Loss: 0.00002796
Iteration 136/1000 | Loss: 0.00002796
Iteration 137/1000 | Loss: 0.00002796
Iteration 138/1000 | Loss: 0.00002796
Iteration 139/1000 | Loss: 0.00002796
Iteration 140/1000 | Loss: 0.00002796
Iteration 141/1000 | Loss: 0.00002796
Iteration 142/1000 | Loss: 0.00002796
Iteration 143/1000 | Loss: 0.00002796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.796223634504713e-05, 2.796223634504713e-05, 2.796223634504713e-05, 2.796223634504713e-05, 2.796223634504713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.796223634504713e-05

Optimization complete. Final v2v error: 4.4215240478515625 mm

Highest mean error: 5.696157455444336 mm for frame 120

Lowest mean error: 3.5719854831695557 mm for frame 0

Saving results

Total time: 50.48120737075806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475708
Iteration 2/25 | Loss: 0.00145539
Iteration 3/25 | Loss: 0.00138257
Iteration 4/25 | Loss: 0.00137351
Iteration 5/25 | Loss: 0.00137205
Iteration 6/25 | Loss: 0.00137205
Iteration 7/25 | Loss: 0.00137205
Iteration 8/25 | Loss: 0.00137205
Iteration 9/25 | Loss: 0.00137205
Iteration 10/25 | Loss: 0.00137205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013720494462177157, 0.0013720494462177157, 0.0013720494462177157, 0.0013720494462177157, 0.0013720494462177157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013720494462177157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.88563156
Iteration 2/25 | Loss: 0.00185771
Iteration 3/25 | Loss: 0.00185771
Iteration 4/25 | Loss: 0.00185771
Iteration 5/25 | Loss: 0.00185771
Iteration 6/25 | Loss: 0.00185771
Iteration 7/25 | Loss: 0.00185771
Iteration 8/25 | Loss: 0.00185771
Iteration 9/25 | Loss: 0.00185770
Iteration 10/25 | Loss: 0.00185770
Iteration 11/25 | Loss: 0.00185770
Iteration 12/25 | Loss: 0.00185770
Iteration 13/25 | Loss: 0.00185770
Iteration 14/25 | Loss: 0.00185770
Iteration 15/25 | Loss: 0.00185770
Iteration 16/25 | Loss: 0.00185770
Iteration 17/25 | Loss: 0.00185770
Iteration 18/25 | Loss: 0.00185770
Iteration 19/25 | Loss: 0.00185770
Iteration 20/25 | Loss: 0.00185770
Iteration 21/25 | Loss: 0.00185770
Iteration 22/25 | Loss: 0.00185770
Iteration 23/25 | Loss: 0.00185770
Iteration 24/25 | Loss: 0.00185770
Iteration 25/25 | Loss: 0.00185770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185770
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00002014
Iteration 4/1000 | Loss: 0.00001846
Iteration 5/1000 | Loss: 0.00001735
Iteration 6/1000 | Loss: 0.00001665
Iteration 7/1000 | Loss: 0.00001610
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001433
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001356
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001347
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001343
Iteration 78/1000 | Loss: 0.00001343
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001342
Iteration 96/1000 | Loss: 0.00001342
Iteration 97/1000 | Loss: 0.00001342
Iteration 98/1000 | Loss: 0.00001342
Iteration 99/1000 | Loss: 0.00001342
Iteration 100/1000 | Loss: 0.00001342
Iteration 101/1000 | Loss: 0.00001341
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001341
Iteration 104/1000 | Loss: 0.00001341
Iteration 105/1000 | Loss: 0.00001341
Iteration 106/1000 | Loss: 0.00001341
Iteration 107/1000 | Loss: 0.00001341
Iteration 108/1000 | Loss: 0.00001341
Iteration 109/1000 | Loss: 0.00001341
Iteration 110/1000 | Loss: 0.00001341
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001340
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001340
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001340
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Iteration 159/1000 | Loss: 0.00001340
Iteration 160/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.3401406249613501e-05, 1.3401406249613501e-05, 1.3401406249613501e-05, 1.3401406249613501e-05, 1.3401406249613501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3401406249613501e-05

Optimization complete. Final v2v error: 3.165411949157715 mm

Highest mean error: 3.45027494430542 mm for frame 212

Lowest mean error: 3.036226987838745 mm for frame 47

Saving results

Total time: 42.01394248008728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818233
Iteration 2/25 | Loss: 0.00145900
Iteration 3/25 | Loss: 0.00137287
Iteration 4/25 | Loss: 0.00136320
Iteration 5/25 | Loss: 0.00136103
Iteration 6/25 | Loss: 0.00136094
Iteration 7/25 | Loss: 0.00136094
Iteration 8/25 | Loss: 0.00136094
Iteration 9/25 | Loss: 0.00136094
Iteration 10/25 | Loss: 0.00136094
Iteration 11/25 | Loss: 0.00136094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013609363231807947, 0.0013609363231807947, 0.0013609363231807947, 0.0013609363231807947, 0.0013609363231807947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013609363231807947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24041176
Iteration 2/25 | Loss: 0.00189795
Iteration 3/25 | Loss: 0.00189793
Iteration 4/25 | Loss: 0.00189793
Iteration 5/25 | Loss: 0.00189793
Iteration 6/25 | Loss: 0.00189793
Iteration 7/25 | Loss: 0.00189793
Iteration 8/25 | Loss: 0.00189793
Iteration 9/25 | Loss: 0.00189792
Iteration 10/25 | Loss: 0.00189792
Iteration 11/25 | Loss: 0.00189792
Iteration 12/25 | Loss: 0.00189792
Iteration 13/25 | Loss: 0.00189792
Iteration 14/25 | Loss: 0.00189792
Iteration 15/25 | Loss: 0.00189792
Iteration 16/25 | Loss: 0.00189792
Iteration 17/25 | Loss: 0.00189792
Iteration 18/25 | Loss: 0.00189792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001897923881188035, 0.001897923881188035, 0.001897923881188035, 0.001897923881188035, 0.001897923881188035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001897923881188035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189792
Iteration 2/1000 | Loss: 0.00002240
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00001169
Iteration 8/1000 | Loss: 0.00001142
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001051
Iteration 16/1000 | Loss: 0.00001047
Iteration 17/1000 | Loss: 0.00001043
Iteration 18/1000 | Loss: 0.00001042
Iteration 19/1000 | Loss: 0.00001041
Iteration 20/1000 | Loss: 0.00001040
Iteration 21/1000 | Loss: 0.00001039
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001038
Iteration 24/1000 | Loss: 0.00001037
Iteration 25/1000 | Loss: 0.00001037
Iteration 26/1000 | Loss: 0.00001036
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001034
Iteration 31/1000 | Loss: 0.00001031
Iteration 32/1000 | Loss: 0.00001031
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001029
Iteration 38/1000 | Loss: 0.00001029
Iteration 39/1000 | Loss: 0.00001028
Iteration 40/1000 | Loss: 0.00001028
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001026
Iteration 45/1000 | Loss: 0.00001026
Iteration 46/1000 | Loss: 0.00001025
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001025
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001022
Iteration 60/1000 | Loss: 0.00001022
Iteration 61/1000 | Loss: 0.00001021
Iteration 62/1000 | Loss: 0.00001021
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001020
Iteration 66/1000 | Loss: 0.00001019
Iteration 67/1000 | Loss: 0.00001018
Iteration 68/1000 | Loss: 0.00001018
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001017
Iteration 71/1000 | Loss: 0.00001016
Iteration 72/1000 | Loss: 0.00001016
Iteration 73/1000 | Loss: 0.00001016
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001016
Iteration 76/1000 | Loss: 0.00001016
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001014
Iteration 81/1000 | Loss: 0.00001014
Iteration 82/1000 | Loss: 0.00001014
Iteration 83/1000 | Loss: 0.00001014
Iteration 84/1000 | Loss: 0.00001013
Iteration 85/1000 | Loss: 0.00001013
Iteration 86/1000 | Loss: 0.00001013
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001010
Iteration 93/1000 | Loss: 0.00001009
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001008
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001008
Iteration 100/1000 | Loss: 0.00001008
Iteration 101/1000 | Loss: 0.00001007
Iteration 102/1000 | Loss: 0.00001007
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001006
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001005
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001004
Iteration 112/1000 | Loss: 0.00001004
Iteration 113/1000 | Loss: 0.00001004
Iteration 114/1000 | Loss: 0.00001004
Iteration 115/1000 | Loss: 0.00001004
Iteration 116/1000 | Loss: 0.00001004
Iteration 117/1000 | Loss: 0.00001004
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001001
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001001
Iteration 129/1000 | Loss: 0.00001001
Iteration 130/1000 | Loss: 0.00001001
Iteration 131/1000 | Loss: 0.00001001
Iteration 132/1000 | Loss: 0.00001001
Iteration 133/1000 | Loss: 0.00001001
Iteration 134/1000 | Loss: 0.00001001
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Iteration 143/1000 | Loss: 0.00001000
Iteration 144/1000 | Loss: 0.00001000
Iteration 145/1000 | Loss: 0.00000999
Iteration 146/1000 | Loss: 0.00000999
Iteration 147/1000 | Loss: 0.00000999
Iteration 148/1000 | Loss: 0.00000999
Iteration 149/1000 | Loss: 0.00000999
Iteration 150/1000 | Loss: 0.00000999
Iteration 151/1000 | Loss: 0.00000999
Iteration 152/1000 | Loss: 0.00000999
Iteration 153/1000 | Loss: 0.00000999
Iteration 154/1000 | Loss: 0.00000999
Iteration 155/1000 | Loss: 0.00000999
Iteration 156/1000 | Loss: 0.00000998
Iteration 157/1000 | Loss: 0.00000998
Iteration 158/1000 | Loss: 0.00000998
Iteration 159/1000 | Loss: 0.00000998
Iteration 160/1000 | Loss: 0.00000998
Iteration 161/1000 | Loss: 0.00000998
Iteration 162/1000 | Loss: 0.00000998
Iteration 163/1000 | Loss: 0.00000998
Iteration 164/1000 | Loss: 0.00000998
Iteration 165/1000 | Loss: 0.00000998
Iteration 166/1000 | Loss: 0.00000998
Iteration 167/1000 | Loss: 0.00000997
Iteration 168/1000 | Loss: 0.00000997
Iteration 169/1000 | Loss: 0.00000997
Iteration 170/1000 | Loss: 0.00000997
Iteration 171/1000 | Loss: 0.00000997
Iteration 172/1000 | Loss: 0.00000997
Iteration 173/1000 | Loss: 0.00000997
Iteration 174/1000 | Loss: 0.00000997
Iteration 175/1000 | Loss: 0.00000997
Iteration 176/1000 | Loss: 0.00000997
Iteration 177/1000 | Loss: 0.00000997
Iteration 178/1000 | Loss: 0.00000997
Iteration 179/1000 | Loss: 0.00000997
Iteration 180/1000 | Loss: 0.00000997
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000996
Iteration 184/1000 | Loss: 0.00000996
Iteration 185/1000 | Loss: 0.00000996
Iteration 186/1000 | Loss: 0.00000996
Iteration 187/1000 | Loss: 0.00000996
Iteration 188/1000 | Loss: 0.00000996
Iteration 189/1000 | Loss: 0.00000996
Iteration 190/1000 | Loss: 0.00000995
Iteration 191/1000 | Loss: 0.00000995
Iteration 192/1000 | Loss: 0.00000995
Iteration 193/1000 | Loss: 0.00000995
Iteration 194/1000 | Loss: 0.00000995
Iteration 195/1000 | Loss: 0.00000995
Iteration 196/1000 | Loss: 0.00000995
Iteration 197/1000 | Loss: 0.00000995
Iteration 198/1000 | Loss: 0.00000995
Iteration 199/1000 | Loss: 0.00000995
Iteration 200/1000 | Loss: 0.00000995
Iteration 201/1000 | Loss: 0.00000995
Iteration 202/1000 | Loss: 0.00000995
Iteration 203/1000 | Loss: 0.00000995
Iteration 204/1000 | Loss: 0.00000994
Iteration 205/1000 | Loss: 0.00000994
Iteration 206/1000 | Loss: 0.00000994
Iteration 207/1000 | Loss: 0.00000994
Iteration 208/1000 | Loss: 0.00000994
Iteration 209/1000 | Loss: 0.00000994
Iteration 210/1000 | Loss: 0.00000994
Iteration 211/1000 | Loss: 0.00000994
Iteration 212/1000 | Loss: 0.00000994
Iteration 213/1000 | Loss: 0.00000994
Iteration 214/1000 | Loss: 0.00000994
Iteration 215/1000 | Loss: 0.00000994
Iteration 216/1000 | Loss: 0.00000994
Iteration 217/1000 | Loss: 0.00000994
Iteration 218/1000 | Loss: 0.00000994
Iteration 219/1000 | Loss: 0.00000994
Iteration 220/1000 | Loss: 0.00000994
Iteration 221/1000 | Loss: 0.00000994
Iteration 222/1000 | Loss: 0.00000993
Iteration 223/1000 | Loss: 0.00000993
Iteration 224/1000 | Loss: 0.00000993
Iteration 225/1000 | Loss: 0.00000993
Iteration 226/1000 | Loss: 0.00000993
Iteration 227/1000 | Loss: 0.00000993
Iteration 228/1000 | Loss: 0.00000993
Iteration 229/1000 | Loss: 0.00000993
Iteration 230/1000 | Loss: 0.00000993
Iteration 231/1000 | Loss: 0.00000993
Iteration 232/1000 | Loss: 0.00000993
Iteration 233/1000 | Loss: 0.00000993
Iteration 234/1000 | Loss: 0.00000993
Iteration 235/1000 | Loss: 0.00000993
Iteration 236/1000 | Loss: 0.00000993
Iteration 237/1000 | Loss: 0.00000993
Iteration 238/1000 | Loss: 0.00000993
Iteration 239/1000 | Loss: 0.00000993
Iteration 240/1000 | Loss: 0.00000993
Iteration 241/1000 | Loss: 0.00000993
Iteration 242/1000 | Loss: 0.00000993
Iteration 243/1000 | Loss: 0.00000993
Iteration 244/1000 | Loss: 0.00000993
Iteration 245/1000 | Loss: 0.00000993
Iteration 246/1000 | Loss: 0.00000993
Iteration 247/1000 | Loss: 0.00000993
Iteration 248/1000 | Loss: 0.00000993
Iteration 249/1000 | Loss: 0.00000993
Iteration 250/1000 | Loss: 0.00000993
Iteration 251/1000 | Loss: 0.00000993
Iteration 252/1000 | Loss: 0.00000993
Iteration 253/1000 | Loss: 0.00000993
Iteration 254/1000 | Loss: 0.00000993
Iteration 255/1000 | Loss: 0.00000993
Iteration 256/1000 | Loss: 0.00000993
Iteration 257/1000 | Loss: 0.00000993
Iteration 258/1000 | Loss: 0.00000993
Iteration 259/1000 | Loss: 0.00000993
Iteration 260/1000 | Loss: 0.00000993
Iteration 261/1000 | Loss: 0.00000993
Iteration 262/1000 | Loss: 0.00000993
Iteration 263/1000 | Loss: 0.00000993
Iteration 264/1000 | Loss: 0.00000993
Iteration 265/1000 | Loss: 0.00000993
Iteration 266/1000 | Loss: 0.00000993
Iteration 267/1000 | Loss: 0.00000993
Iteration 268/1000 | Loss: 0.00000993
Iteration 269/1000 | Loss: 0.00000993
Iteration 270/1000 | Loss: 0.00000993
Iteration 271/1000 | Loss: 0.00000993
Iteration 272/1000 | Loss: 0.00000993
Iteration 273/1000 | Loss: 0.00000993
Iteration 274/1000 | Loss: 0.00000993
Iteration 275/1000 | Loss: 0.00000993
Iteration 276/1000 | Loss: 0.00000993
Iteration 277/1000 | Loss: 0.00000993
Iteration 278/1000 | Loss: 0.00000993
Iteration 279/1000 | Loss: 0.00000993
Iteration 280/1000 | Loss: 0.00000993
Iteration 281/1000 | Loss: 0.00000993
Iteration 282/1000 | Loss: 0.00000993
Iteration 283/1000 | Loss: 0.00000993
Iteration 284/1000 | Loss: 0.00000993
Iteration 285/1000 | Loss: 0.00000993
Iteration 286/1000 | Loss: 0.00000993
Iteration 287/1000 | Loss: 0.00000993
Iteration 288/1000 | Loss: 0.00000993
Iteration 289/1000 | Loss: 0.00000993
Iteration 290/1000 | Loss: 0.00000993
Iteration 291/1000 | Loss: 0.00000993
Iteration 292/1000 | Loss: 0.00000993
Iteration 293/1000 | Loss: 0.00000993
Iteration 294/1000 | Loss: 0.00000993
Iteration 295/1000 | Loss: 0.00000993
Iteration 296/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [9.931235581461806e-06, 9.931235581461806e-06, 9.931235581461806e-06, 9.931235581461806e-06, 9.931235581461806e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.931235581461806e-06

Optimization complete. Final v2v error: 2.747194528579712 mm

Highest mean error: 3.002614974975586 mm for frame 24

Lowest mean error: 2.6260292530059814 mm for frame 109

Saving results

Total time: 42.91435384750366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356038
Iteration 2/25 | Loss: 0.00140084
Iteration 3/25 | Loss: 0.00133748
Iteration 4/25 | Loss: 0.00132774
Iteration 5/25 | Loss: 0.00132370
Iteration 6/25 | Loss: 0.00132229
Iteration 7/25 | Loss: 0.00132229
Iteration 8/25 | Loss: 0.00132229
Iteration 9/25 | Loss: 0.00132229
Iteration 10/25 | Loss: 0.00132229
Iteration 11/25 | Loss: 0.00132229
Iteration 12/25 | Loss: 0.00132229
Iteration 13/25 | Loss: 0.00132229
Iteration 14/25 | Loss: 0.00132229
Iteration 15/25 | Loss: 0.00132229
Iteration 16/25 | Loss: 0.00132229
Iteration 17/25 | Loss: 0.00132229
Iteration 18/25 | Loss: 0.00132229
Iteration 19/25 | Loss: 0.00132229
Iteration 20/25 | Loss: 0.00132229
Iteration 21/25 | Loss: 0.00132229
Iteration 22/25 | Loss: 0.00132229
Iteration 23/25 | Loss: 0.00132229
Iteration 24/25 | Loss: 0.00132229
Iteration 25/25 | Loss: 0.00132229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25831175
Iteration 2/25 | Loss: 0.00252856
Iteration 3/25 | Loss: 0.00252856
Iteration 4/25 | Loss: 0.00252856
Iteration 5/25 | Loss: 0.00252856
Iteration 6/25 | Loss: 0.00252856
Iteration 7/25 | Loss: 0.00252856
Iteration 8/25 | Loss: 0.00252856
Iteration 9/25 | Loss: 0.00252856
Iteration 10/25 | Loss: 0.00252856
Iteration 11/25 | Loss: 0.00252856
Iteration 12/25 | Loss: 0.00252856
Iteration 13/25 | Loss: 0.00252856
Iteration 14/25 | Loss: 0.00252856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002528558485209942, 0.002528558485209942, 0.002528558485209942, 0.002528558485209942, 0.002528558485209942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002528558485209942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252856
Iteration 2/1000 | Loss: 0.00002906
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001652
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001437
Iteration 8/1000 | Loss: 0.00001376
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001347
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001264
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001247
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001244
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001221
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001209
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001208
Iteration 120/1000 | Loss: 0.00001208
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001205
Iteration 139/1000 | Loss: 0.00001205
Iteration 140/1000 | Loss: 0.00001204
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001204
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001203
Iteration 147/1000 | Loss: 0.00001203
Iteration 148/1000 | Loss: 0.00001203
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Iteration 151/1000 | Loss: 0.00001203
Iteration 152/1000 | Loss: 0.00001203
Iteration 153/1000 | Loss: 0.00001203
Iteration 154/1000 | Loss: 0.00001203
Iteration 155/1000 | Loss: 0.00001203
Iteration 156/1000 | Loss: 0.00001203
Iteration 157/1000 | Loss: 0.00001203
Iteration 158/1000 | Loss: 0.00001203
Iteration 159/1000 | Loss: 0.00001203
Iteration 160/1000 | Loss: 0.00001203
Iteration 161/1000 | Loss: 0.00001203
Iteration 162/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.2028832315991167e-05, 1.2028832315991167e-05, 1.2028832315991167e-05, 1.2028832315991167e-05, 1.2028832315991167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2028832315991167e-05

Optimization complete. Final v2v error: 2.9743492603302 mm

Highest mean error: 3.7138805389404297 mm for frame 25

Lowest mean error: 2.5510618686676025 mm for frame 132

Saving results

Total time: 39.705031394958496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440426
Iteration 2/25 | Loss: 0.00149138
Iteration 3/25 | Loss: 0.00139846
Iteration 4/25 | Loss: 0.00138280
Iteration 5/25 | Loss: 0.00137925
Iteration 6/25 | Loss: 0.00137862
Iteration 7/25 | Loss: 0.00137862
Iteration 8/25 | Loss: 0.00137862
Iteration 9/25 | Loss: 0.00137862
Iteration 10/25 | Loss: 0.00137862
Iteration 11/25 | Loss: 0.00137862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013786237686872482, 0.0013786237686872482, 0.0013786237686872482, 0.0013786237686872482, 0.0013786237686872482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013786237686872482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23626995
Iteration 2/25 | Loss: 0.00183817
Iteration 3/25 | Loss: 0.00183816
Iteration 4/25 | Loss: 0.00183816
Iteration 5/25 | Loss: 0.00183816
Iteration 6/25 | Loss: 0.00183816
Iteration 7/25 | Loss: 0.00183816
Iteration 8/25 | Loss: 0.00183816
Iteration 9/25 | Loss: 0.00183816
Iteration 10/25 | Loss: 0.00183816
Iteration 11/25 | Loss: 0.00183816
Iteration 12/25 | Loss: 0.00183816
Iteration 13/25 | Loss: 0.00183816
Iteration 14/25 | Loss: 0.00183816
Iteration 15/25 | Loss: 0.00183816
Iteration 16/25 | Loss: 0.00183816
Iteration 17/25 | Loss: 0.00183816
Iteration 18/25 | Loss: 0.00183816
Iteration 19/25 | Loss: 0.00183816
Iteration 20/25 | Loss: 0.00183816
Iteration 21/25 | Loss: 0.00183816
Iteration 22/25 | Loss: 0.00183816
Iteration 23/25 | Loss: 0.00183816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001838158117607236, 0.001838158117607236, 0.001838158117607236, 0.001838158117607236, 0.001838158117607236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001838158117607236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183816
Iteration 2/1000 | Loss: 0.00002725
Iteration 3/1000 | Loss: 0.00002143
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001879
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001748
Iteration 8/1000 | Loss: 0.00001711
Iteration 9/1000 | Loss: 0.00001664
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001622
Iteration 12/1000 | Loss: 0.00001599
Iteration 13/1000 | Loss: 0.00001590
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001556
Iteration 18/1000 | Loss: 0.00001553
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001550
Iteration 22/1000 | Loss: 0.00001544
Iteration 23/1000 | Loss: 0.00001543
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001541
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001537
Iteration 28/1000 | Loss: 0.00001536
Iteration 29/1000 | Loss: 0.00001535
Iteration 30/1000 | Loss: 0.00001535
Iteration 31/1000 | Loss: 0.00001535
Iteration 32/1000 | Loss: 0.00001535
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001527
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001510
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001508
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001501
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001501
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001501
Iteration 113/1000 | Loss: 0.00001500
Iteration 114/1000 | Loss: 0.00001500
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001500
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001500
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001499
Iteration 132/1000 | Loss: 0.00001499
Iteration 133/1000 | Loss: 0.00001499
Iteration 134/1000 | Loss: 0.00001499
Iteration 135/1000 | Loss: 0.00001499
Iteration 136/1000 | Loss: 0.00001499
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.4991830539656803e-05, 1.4991830539656803e-05, 1.4991830539656803e-05, 1.4991830539656803e-05, 1.4991830539656803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4991830539656803e-05

Optimization complete. Final v2v error: 3.341825485229492 mm

Highest mean error: 3.484840154647827 mm for frame 177

Lowest mean error: 3.02582049369812 mm for frame 216

Saving results

Total time: 45.77832889556885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506639
Iteration 2/25 | Loss: 0.00152653
Iteration 3/25 | Loss: 0.00143761
Iteration 4/25 | Loss: 0.00143213
Iteration 5/25 | Loss: 0.00143108
Iteration 6/25 | Loss: 0.00143108
Iteration 7/25 | Loss: 0.00143108
Iteration 8/25 | Loss: 0.00143108
Iteration 9/25 | Loss: 0.00143108
Iteration 10/25 | Loss: 0.00143108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001431075856089592, 0.001431075856089592, 0.001431075856089592, 0.001431075856089592, 0.001431075856089592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001431075856089592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25420356
Iteration 2/25 | Loss: 0.00237160
Iteration 3/25 | Loss: 0.00237160
Iteration 4/25 | Loss: 0.00237159
Iteration 5/25 | Loss: 0.00237159
Iteration 6/25 | Loss: 0.00237159
Iteration 7/25 | Loss: 0.00237159
Iteration 8/25 | Loss: 0.00237159
Iteration 9/25 | Loss: 0.00237159
Iteration 10/25 | Loss: 0.00237159
Iteration 11/25 | Loss: 0.00237159
Iteration 12/25 | Loss: 0.00237159
Iteration 13/25 | Loss: 0.00237159
Iteration 14/25 | Loss: 0.00237159
Iteration 15/25 | Loss: 0.00237159
Iteration 16/25 | Loss: 0.00237159
Iteration 17/25 | Loss: 0.00237159
Iteration 18/25 | Loss: 0.00237159
Iteration 19/25 | Loss: 0.00237159
Iteration 20/25 | Loss: 0.00237159
Iteration 21/25 | Loss: 0.00237159
Iteration 22/25 | Loss: 0.00237159
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002371592679992318, 0.002371592679992318, 0.002371592679992318, 0.002371592679992318, 0.002371592679992318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002371592679992318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237159
Iteration 2/1000 | Loss: 0.00004052
Iteration 3/1000 | Loss: 0.00002695
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002225
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00001992
Iteration 9/1000 | Loss: 0.00001959
Iteration 10/1000 | Loss: 0.00001917
Iteration 11/1000 | Loss: 0.00001883
Iteration 12/1000 | Loss: 0.00001851
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001806
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001778
Iteration 18/1000 | Loss: 0.00001776
Iteration 19/1000 | Loss: 0.00001775
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001764
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001757
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001752
Iteration 47/1000 | Loss: 0.00001752
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001751
Iteration 50/1000 | Loss: 0.00001751
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001750
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001747
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001746
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001745
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001740
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001739
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001737
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001737
Iteration 83/1000 | Loss: 0.00001736
Iteration 84/1000 | Loss: 0.00001736
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001735
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001733
Iteration 103/1000 | Loss: 0.00001733
Iteration 104/1000 | Loss: 0.00001733
Iteration 105/1000 | Loss: 0.00001733
Iteration 106/1000 | Loss: 0.00001733
Iteration 107/1000 | Loss: 0.00001733
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001732
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001730
Iteration 118/1000 | Loss: 0.00001730
Iteration 119/1000 | Loss: 0.00001730
Iteration 120/1000 | Loss: 0.00001730
Iteration 121/1000 | Loss: 0.00001730
Iteration 122/1000 | Loss: 0.00001730
Iteration 123/1000 | Loss: 0.00001730
Iteration 124/1000 | Loss: 0.00001730
Iteration 125/1000 | Loss: 0.00001730
Iteration 126/1000 | Loss: 0.00001729
Iteration 127/1000 | Loss: 0.00001729
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001729
Iteration 130/1000 | Loss: 0.00001729
Iteration 131/1000 | Loss: 0.00001729
Iteration 132/1000 | Loss: 0.00001729
Iteration 133/1000 | Loss: 0.00001729
Iteration 134/1000 | Loss: 0.00001728
Iteration 135/1000 | Loss: 0.00001728
Iteration 136/1000 | Loss: 0.00001728
Iteration 137/1000 | Loss: 0.00001728
Iteration 138/1000 | Loss: 0.00001728
Iteration 139/1000 | Loss: 0.00001728
Iteration 140/1000 | Loss: 0.00001728
Iteration 141/1000 | Loss: 0.00001728
Iteration 142/1000 | Loss: 0.00001728
Iteration 143/1000 | Loss: 0.00001728
Iteration 144/1000 | Loss: 0.00001728
Iteration 145/1000 | Loss: 0.00001727
Iteration 146/1000 | Loss: 0.00001727
Iteration 147/1000 | Loss: 0.00001727
Iteration 148/1000 | Loss: 0.00001727
Iteration 149/1000 | Loss: 0.00001727
Iteration 150/1000 | Loss: 0.00001727
Iteration 151/1000 | Loss: 0.00001727
Iteration 152/1000 | Loss: 0.00001727
Iteration 153/1000 | Loss: 0.00001727
Iteration 154/1000 | Loss: 0.00001727
Iteration 155/1000 | Loss: 0.00001727
Iteration 156/1000 | Loss: 0.00001727
Iteration 157/1000 | Loss: 0.00001727
Iteration 158/1000 | Loss: 0.00001727
Iteration 159/1000 | Loss: 0.00001727
Iteration 160/1000 | Loss: 0.00001727
Iteration 161/1000 | Loss: 0.00001727
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001727
Iteration 165/1000 | Loss: 0.00001727
Iteration 166/1000 | Loss: 0.00001727
Iteration 167/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.7266107533941977e-05, 1.7266107533941977e-05, 1.7266107533941977e-05, 1.7266107533941977e-05, 1.7266107533941977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7266107533941977e-05

Optimization complete. Final v2v error: 3.4582080841064453 mm

Highest mean error: 4.1249589920043945 mm for frame 207

Lowest mean error: 2.9519593715667725 mm for frame 102

Saving results

Total time: 47.01487159729004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413681
Iteration 2/25 | Loss: 0.00153808
Iteration 3/25 | Loss: 0.00139108
Iteration 4/25 | Loss: 0.00137738
Iteration 5/25 | Loss: 0.00137531
Iteration 6/25 | Loss: 0.00137512
Iteration 7/25 | Loss: 0.00137512
Iteration 8/25 | Loss: 0.00137512
Iteration 9/25 | Loss: 0.00137512
Iteration 10/25 | Loss: 0.00137512
Iteration 11/25 | Loss: 0.00137512
Iteration 12/25 | Loss: 0.00137512
Iteration 13/25 | Loss: 0.00137512
Iteration 14/25 | Loss: 0.00137512
Iteration 15/25 | Loss: 0.00137512
Iteration 16/25 | Loss: 0.00137512
Iteration 17/25 | Loss: 0.00137512
Iteration 18/25 | Loss: 0.00137512
Iteration 19/25 | Loss: 0.00137512
Iteration 20/25 | Loss: 0.00137512
Iteration 21/25 | Loss: 0.00137512
Iteration 22/25 | Loss: 0.00137512
Iteration 23/25 | Loss: 0.00137512
Iteration 24/25 | Loss: 0.00137512
Iteration 25/25 | Loss: 0.00137512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24456298
Iteration 2/25 | Loss: 0.00217739
Iteration 3/25 | Loss: 0.00217739
Iteration 4/25 | Loss: 0.00217739
Iteration 5/25 | Loss: 0.00217739
Iteration 6/25 | Loss: 0.00217739
Iteration 7/25 | Loss: 0.00217739
Iteration 8/25 | Loss: 0.00217739
Iteration 9/25 | Loss: 0.00217739
Iteration 10/25 | Loss: 0.00217739
Iteration 11/25 | Loss: 0.00217739
Iteration 12/25 | Loss: 0.00217739
Iteration 13/25 | Loss: 0.00217739
Iteration 14/25 | Loss: 0.00217739
Iteration 15/25 | Loss: 0.00217739
Iteration 16/25 | Loss: 0.00217739
Iteration 17/25 | Loss: 0.00217739
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00217738701030612, 0.00217738701030612, 0.00217738701030612, 0.00217738701030612, 0.00217738701030612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00217738701030612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217739
Iteration 2/1000 | Loss: 0.00002929
Iteration 3/1000 | Loss: 0.00001995
Iteration 4/1000 | Loss: 0.00001802
Iteration 5/1000 | Loss: 0.00001639
Iteration 6/1000 | Loss: 0.00001526
Iteration 7/1000 | Loss: 0.00001464
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001256
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001222
Iteration 52/1000 | Loss: 0.00001222
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001211
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001201
Iteration 115/1000 | Loss: 0.00001201
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001200
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001199
Iteration 123/1000 | Loss: 0.00001199
Iteration 124/1000 | Loss: 0.00001199
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001199
Iteration 128/1000 | Loss: 0.00001199
Iteration 129/1000 | Loss: 0.00001199
Iteration 130/1000 | Loss: 0.00001199
Iteration 131/1000 | Loss: 0.00001198
Iteration 132/1000 | Loss: 0.00001198
Iteration 133/1000 | Loss: 0.00001198
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001197
Iteration 136/1000 | Loss: 0.00001197
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001192
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001189
Iteration 170/1000 | Loss: 0.00001189
Iteration 171/1000 | Loss: 0.00001189
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001188
Iteration 180/1000 | Loss: 0.00001188
Iteration 181/1000 | Loss: 0.00001188
Iteration 182/1000 | Loss: 0.00001188
Iteration 183/1000 | Loss: 0.00001188
Iteration 184/1000 | Loss: 0.00001188
Iteration 185/1000 | Loss: 0.00001188
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001187
Iteration 193/1000 | Loss: 0.00001187
Iteration 194/1000 | Loss: 0.00001187
Iteration 195/1000 | Loss: 0.00001187
Iteration 196/1000 | Loss: 0.00001187
Iteration 197/1000 | Loss: 0.00001187
Iteration 198/1000 | Loss: 0.00001187
Iteration 199/1000 | Loss: 0.00001187
Iteration 200/1000 | Loss: 0.00001187
Iteration 201/1000 | Loss: 0.00001187
Iteration 202/1000 | Loss: 0.00001187
Iteration 203/1000 | Loss: 0.00001186
Iteration 204/1000 | Loss: 0.00001186
Iteration 205/1000 | Loss: 0.00001186
Iteration 206/1000 | Loss: 0.00001186
Iteration 207/1000 | Loss: 0.00001186
Iteration 208/1000 | Loss: 0.00001186
Iteration 209/1000 | Loss: 0.00001186
Iteration 210/1000 | Loss: 0.00001186
Iteration 211/1000 | Loss: 0.00001186
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001185
Iteration 215/1000 | Loss: 0.00001185
Iteration 216/1000 | Loss: 0.00001185
Iteration 217/1000 | Loss: 0.00001185
Iteration 218/1000 | Loss: 0.00001185
Iteration 219/1000 | Loss: 0.00001185
Iteration 220/1000 | Loss: 0.00001185
Iteration 221/1000 | Loss: 0.00001185
Iteration 222/1000 | Loss: 0.00001185
Iteration 223/1000 | Loss: 0.00001185
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Iteration 226/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1847924724861514e-05, 1.1847924724861514e-05, 1.1847924724861514e-05, 1.1847924724861514e-05, 1.1847924724861514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1847924724861514e-05

Optimization complete. Final v2v error: 2.9644153118133545 mm

Highest mean error: 3.5745608806610107 mm for frame 97

Lowest mean error: 2.7391223907470703 mm for frame 44

Saving results

Total time: 47.331223249435425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022610
Iteration 2/25 | Loss: 0.00378886
Iteration 3/25 | Loss: 0.00266497
Iteration 4/25 | Loss: 0.00223599
Iteration 5/25 | Loss: 0.00207704
Iteration 6/25 | Loss: 0.00197178
Iteration 7/25 | Loss: 0.00186806
Iteration 8/25 | Loss: 0.00184316
Iteration 9/25 | Loss: 0.00177587
Iteration 10/25 | Loss: 0.00174790
Iteration 11/25 | Loss: 0.00173739
Iteration 12/25 | Loss: 0.00172041
Iteration 13/25 | Loss: 0.00170020
Iteration 14/25 | Loss: 0.00169820
Iteration 15/25 | Loss: 0.00169823
Iteration 16/25 | Loss: 0.00169079
Iteration 17/25 | Loss: 0.00168250
Iteration 18/25 | Loss: 0.00168026
Iteration 19/25 | Loss: 0.00168151
Iteration 20/25 | Loss: 0.00167975
Iteration 21/25 | Loss: 0.00167956
Iteration 22/25 | Loss: 0.00168009
Iteration 23/25 | Loss: 0.00168041
Iteration 24/25 | Loss: 0.00167937
Iteration 25/25 | Loss: 0.00168440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18528676
Iteration 2/25 | Loss: 0.00500272
Iteration 3/25 | Loss: 0.00428438
Iteration 4/25 | Loss: 0.00428438
Iteration 5/25 | Loss: 0.00428438
Iteration 6/25 | Loss: 0.00428438
Iteration 7/25 | Loss: 0.00428438
Iteration 8/25 | Loss: 0.00428438
Iteration 9/25 | Loss: 0.00428438
Iteration 10/25 | Loss: 0.00428438
Iteration 11/25 | Loss: 0.00428438
Iteration 12/25 | Loss: 0.00428438
Iteration 13/25 | Loss: 0.00428438
Iteration 14/25 | Loss: 0.00428438
Iteration 15/25 | Loss: 0.00428438
Iteration 16/25 | Loss: 0.00428438
Iteration 17/25 | Loss: 0.00428438
Iteration 18/25 | Loss: 0.00428438
Iteration 19/25 | Loss: 0.00428438
Iteration 20/25 | Loss: 0.00428438
Iteration 21/25 | Loss: 0.00428438
Iteration 22/25 | Loss: 0.00428438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004284377209842205, 0.004284377209842205, 0.004284377209842205, 0.004284377209842205, 0.004284377209842205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004284377209842205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00428438
Iteration 2/1000 | Loss: 0.00392617
Iteration 3/1000 | Loss: 0.00935502
Iteration 4/1000 | Loss: 0.00101989
Iteration 5/1000 | Loss: 0.00034676
Iteration 6/1000 | Loss: 0.00051836
Iteration 7/1000 | Loss: 0.00112401
Iteration 8/1000 | Loss: 0.00043125
Iteration 9/1000 | Loss: 0.00037343
Iteration 10/1000 | Loss: 0.00025509
Iteration 11/1000 | Loss: 0.00021976
Iteration 12/1000 | Loss: 0.00037928
Iteration 13/1000 | Loss: 0.00041106
Iteration 14/1000 | Loss: 0.00044119
Iteration 15/1000 | Loss: 0.00052938
Iteration 16/1000 | Loss: 0.00029501
Iteration 17/1000 | Loss: 0.00040729
Iteration 18/1000 | Loss: 0.00037972
Iteration 19/1000 | Loss: 0.00060557
Iteration 20/1000 | Loss: 0.00060185
Iteration 21/1000 | Loss: 0.00757985
Iteration 22/1000 | Loss: 0.00939015
Iteration 23/1000 | Loss: 0.00935084
Iteration 24/1000 | Loss: 0.00627212
Iteration 25/1000 | Loss: 0.00361034
Iteration 26/1000 | Loss: 0.00520088
Iteration 27/1000 | Loss: 0.00354475
Iteration 28/1000 | Loss: 0.00336705
Iteration 29/1000 | Loss: 0.00024712
Iteration 30/1000 | Loss: 0.00076871
Iteration 31/1000 | Loss: 0.00078286
Iteration 32/1000 | Loss: 0.00015974
Iteration 33/1000 | Loss: 0.00014181
Iteration 34/1000 | Loss: 0.00012418
Iteration 35/1000 | Loss: 0.00041796
Iteration 36/1000 | Loss: 0.00014416
Iteration 37/1000 | Loss: 0.00023949
Iteration 38/1000 | Loss: 0.00005180
Iteration 39/1000 | Loss: 0.00014754
Iteration 40/1000 | Loss: 0.00005508
Iteration 41/1000 | Loss: 0.00010027
Iteration 42/1000 | Loss: 0.00003874
Iteration 43/1000 | Loss: 0.00009148
Iteration 44/1000 | Loss: 0.00012176
Iteration 45/1000 | Loss: 0.00009488
Iteration 46/1000 | Loss: 0.00004918
Iteration 47/1000 | Loss: 0.00004329
Iteration 48/1000 | Loss: 0.00007874
Iteration 49/1000 | Loss: 0.00003074
Iteration 50/1000 | Loss: 0.00130094
Iteration 51/1000 | Loss: 0.00061113
Iteration 52/1000 | Loss: 0.00004108
Iteration 53/1000 | Loss: 0.00131053
Iteration 54/1000 | Loss: 0.00056245
Iteration 55/1000 | Loss: 0.00003550
Iteration 56/1000 | Loss: 0.00072895
Iteration 57/1000 | Loss: 0.00027782
Iteration 58/1000 | Loss: 0.00201849
Iteration 59/1000 | Loss: 0.00052490
Iteration 60/1000 | Loss: 0.00005512
Iteration 61/1000 | Loss: 0.00003117
Iteration 62/1000 | Loss: 0.00061890
Iteration 63/1000 | Loss: 0.00043922
Iteration 64/1000 | Loss: 0.00003683
Iteration 65/1000 | Loss: 0.00065621
Iteration 66/1000 | Loss: 0.00005386
Iteration 67/1000 | Loss: 0.00054648
Iteration 68/1000 | Loss: 0.00270458
Iteration 69/1000 | Loss: 0.00040033
Iteration 70/1000 | Loss: 0.00061785
Iteration 71/1000 | Loss: 0.00025652
Iteration 72/1000 | Loss: 0.00003923
Iteration 73/1000 | Loss: 0.00062491
Iteration 74/1000 | Loss: 0.00116986
Iteration 75/1000 | Loss: 0.00069407
Iteration 76/1000 | Loss: 0.00004987
Iteration 77/1000 | Loss: 0.00003214
Iteration 78/1000 | Loss: 0.00066242
Iteration 79/1000 | Loss: 0.00087872
Iteration 80/1000 | Loss: 0.00023204
Iteration 81/1000 | Loss: 0.00003232
Iteration 82/1000 | Loss: 0.00002896
Iteration 83/1000 | Loss: 0.00002886
Iteration 84/1000 | Loss: 0.00017346
Iteration 85/1000 | Loss: 0.00072586
Iteration 86/1000 | Loss: 0.00093064
Iteration 87/1000 | Loss: 0.00026372
Iteration 88/1000 | Loss: 0.00065320
Iteration 89/1000 | Loss: 0.00017860
Iteration 90/1000 | Loss: 0.00004456
Iteration 91/1000 | Loss: 0.00003791
Iteration 92/1000 | Loss: 0.00002757
Iteration 93/1000 | Loss: 0.00054591
Iteration 94/1000 | Loss: 0.00036521
Iteration 95/1000 | Loss: 0.00051224
Iteration 96/1000 | Loss: 0.00069185
Iteration 97/1000 | Loss: 0.00042794
Iteration 98/1000 | Loss: 0.00002432
Iteration 99/1000 | Loss: 0.00002668
Iteration 100/1000 | Loss: 0.00002213
Iteration 101/1000 | Loss: 0.00002050
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001833
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00002236
Iteration 107/1000 | Loss: 0.00007532
Iteration 108/1000 | Loss: 0.00004911
Iteration 109/1000 | Loss: 0.00005148
Iteration 110/1000 | Loss: 0.00001809
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00002314
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00006281
Iteration 118/1000 | Loss: 0.00002372
Iteration 119/1000 | Loss: 0.00001695
Iteration 120/1000 | Loss: 0.00003818
Iteration 121/1000 | Loss: 0.00002157
Iteration 122/1000 | Loss: 0.00002099
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001685
Iteration 126/1000 | Loss: 0.00001685
Iteration 127/1000 | Loss: 0.00001685
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001683
Iteration 137/1000 | Loss: 0.00001683
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001683
Iteration 140/1000 | Loss: 0.00001683
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001682
Iteration 145/1000 | Loss: 0.00001682
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001682
Iteration 148/1000 | Loss: 0.00001682
Iteration 149/1000 | Loss: 0.00001681
Iteration 150/1000 | Loss: 0.00001681
Iteration 151/1000 | Loss: 0.00001681
Iteration 152/1000 | Loss: 0.00001680
Iteration 153/1000 | Loss: 0.00001680
Iteration 154/1000 | Loss: 0.00001680
Iteration 155/1000 | Loss: 0.00001680
Iteration 156/1000 | Loss: 0.00001680
Iteration 157/1000 | Loss: 0.00001680
Iteration 158/1000 | Loss: 0.00001680
Iteration 159/1000 | Loss: 0.00001680
Iteration 160/1000 | Loss: 0.00001680
Iteration 161/1000 | Loss: 0.00001680
Iteration 162/1000 | Loss: 0.00001685
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001679
Iteration 165/1000 | Loss: 0.00001679
Iteration 166/1000 | Loss: 0.00001679
Iteration 167/1000 | Loss: 0.00001678
Iteration 168/1000 | Loss: 0.00001678
Iteration 169/1000 | Loss: 0.00001678
Iteration 170/1000 | Loss: 0.00001678
Iteration 171/1000 | Loss: 0.00001678
Iteration 172/1000 | Loss: 0.00001678
Iteration 173/1000 | Loss: 0.00001678
Iteration 174/1000 | Loss: 0.00001678
Iteration 175/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.6784271792857908e-05, 1.6784271792857908e-05, 1.6784271792857908e-05, 1.6784271792857908e-05, 1.6784271792857908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6784271792857908e-05

Optimization complete. Final v2v error: 3.408860206604004 mm

Highest mean error: 5.272049427032471 mm for frame 67

Lowest mean error: 2.8212544918060303 mm for frame 103

Saving results

Total time: 224.96982312202454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448123
Iteration 2/25 | Loss: 0.00143579
Iteration 3/25 | Loss: 0.00138647
Iteration 4/25 | Loss: 0.00137681
Iteration 5/25 | Loss: 0.00137432
Iteration 6/25 | Loss: 0.00137432
Iteration 7/25 | Loss: 0.00137432
Iteration 8/25 | Loss: 0.00137432
Iteration 9/25 | Loss: 0.00137432
Iteration 10/25 | Loss: 0.00137432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013743238523602486, 0.0013743238523602486, 0.0013743238523602486, 0.0013743238523602486, 0.0013743238523602486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013743238523602486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26364458
Iteration 2/25 | Loss: 0.00217694
Iteration 3/25 | Loss: 0.00217694
Iteration 4/25 | Loss: 0.00217694
Iteration 5/25 | Loss: 0.00217694
Iteration 6/25 | Loss: 0.00217694
Iteration 7/25 | Loss: 0.00217694
Iteration 8/25 | Loss: 0.00217694
Iteration 9/25 | Loss: 0.00217694
Iteration 10/25 | Loss: 0.00217694
Iteration 11/25 | Loss: 0.00217694
Iteration 12/25 | Loss: 0.00217694
Iteration 13/25 | Loss: 0.00217694
Iteration 14/25 | Loss: 0.00217694
Iteration 15/25 | Loss: 0.00217694
Iteration 16/25 | Loss: 0.00217694
Iteration 17/25 | Loss: 0.00217694
Iteration 18/25 | Loss: 0.00217694
Iteration 19/25 | Loss: 0.00217694
Iteration 20/25 | Loss: 0.00217694
Iteration 21/25 | Loss: 0.00217694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002176935551688075, 0.002176935551688075, 0.002176935551688075, 0.002176935551688075, 0.002176935551688075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002176935551688075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217694
Iteration 2/1000 | Loss: 0.00002209
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001678
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001518
Iteration 8/1000 | Loss: 0.00001479
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001423
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001397
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001382
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001361
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001340
Iteration 72/1000 | Loss: 0.00001340
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001339
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.3270225281303283e-05, 1.3270225281303283e-05, 1.3270225281303283e-05, 1.3270225281303283e-05, 1.3270225281303283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3270225281303283e-05

Optimization complete. Final v2v error: 3.0970377922058105 mm

Highest mean error: 3.411926507949829 mm for frame 97

Lowest mean error: 2.8617475032806396 mm for frame 1

Saving results

Total time: 39.92771005630493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430238
Iteration 2/25 | Loss: 0.00144389
Iteration 3/25 | Loss: 0.00137049
Iteration 4/25 | Loss: 0.00135618
Iteration 5/25 | Loss: 0.00135169
Iteration 6/25 | Loss: 0.00135078
Iteration 7/25 | Loss: 0.00135078
Iteration 8/25 | Loss: 0.00135078
Iteration 9/25 | Loss: 0.00135078
Iteration 10/25 | Loss: 0.00135078
Iteration 11/25 | Loss: 0.00135078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013507771072909236, 0.0013507771072909236, 0.0013507771072909236, 0.0013507771072909236, 0.0013507771072909236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013507771072909236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46554708
Iteration 2/25 | Loss: 0.00221252
Iteration 3/25 | Loss: 0.00221252
Iteration 4/25 | Loss: 0.00221252
Iteration 5/25 | Loss: 0.00221252
Iteration 6/25 | Loss: 0.00221251
Iteration 7/25 | Loss: 0.00221251
Iteration 8/25 | Loss: 0.00221251
Iteration 9/25 | Loss: 0.00221251
Iteration 10/25 | Loss: 0.00221251
Iteration 11/25 | Loss: 0.00221251
Iteration 12/25 | Loss: 0.00221251
Iteration 13/25 | Loss: 0.00221251
Iteration 14/25 | Loss: 0.00221251
Iteration 15/25 | Loss: 0.00221251
Iteration 16/25 | Loss: 0.00221251
Iteration 17/25 | Loss: 0.00221251
Iteration 18/25 | Loss: 0.00221251
Iteration 19/25 | Loss: 0.00221251
Iteration 20/25 | Loss: 0.00221251
Iteration 21/25 | Loss: 0.00221251
Iteration 22/25 | Loss: 0.00221251
Iteration 23/25 | Loss: 0.00221251
Iteration 24/25 | Loss: 0.00221251
Iteration 25/25 | Loss: 0.00221251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221251
Iteration 2/1000 | Loss: 0.00002531
Iteration 3/1000 | Loss: 0.00001830
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001590
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001464
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001288
Iteration 18/1000 | Loss: 0.00001287
Iteration 19/1000 | Loss: 0.00001284
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001275
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001264
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001259
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001233
Iteration 78/1000 | Loss: 0.00001233
Iteration 79/1000 | Loss: 0.00001233
Iteration 80/1000 | Loss: 0.00001233
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001224
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Iteration 157/1000 | Loss: 0.00001223
Iteration 158/1000 | Loss: 0.00001223
Iteration 159/1000 | Loss: 0.00001223
Iteration 160/1000 | Loss: 0.00001223
Iteration 161/1000 | Loss: 0.00001223
Iteration 162/1000 | Loss: 0.00001223
Iteration 163/1000 | Loss: 0.00001223
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001222
Iteration 170/1000 | Loss: 0.00001222
Iteration 171/1000 | Loss: 0.00001222
Iteration 172/1000 | Loss: 0.00001222
Iteration 173/1000 | Loss: 0.00001222
Iteration 174/1000 | Loss: 0.00001222
Iteration 175/1000 | Loss: 0.00001222
Iteration 176/1000 | Loss: 0.00001222
Iteration 177/1000 | Loss: 0.00001222
Iteration 178/1000 | Loss: 0.00001222
Iteration 179/1000 | Loss: 0.00001222
Iteration 180/1000 | Loss: 0.00001221
Iteration 181/1000 | Loss: 0.00001221
Iteration 182/1000 | Loss: 0.00001221
Iteration 183/1000 | Loss: 0.00001221
Iteration 184/1000 | Loss: 0.00001221
Iteration 185/1000 | Loss: 0.00001221
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001221
Iteration 190/1000 | Loss: 0.00001221
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001220
Iteration 193/1000 | Loss: 0.00001220
Iteration 194/1000 | Loss: 0.00001220
Iteration 195/1000 | Loss: 0.00001220
Iteration 196/1000 | Loss: 0.00001220
Iteration 197/1000 | Loss: 0.00001220
Iteration 198/1000 | Loss: 0.00001220
Iteration 199/1000 | Loss: 0.00001220
Iteration 200/1000 | Loss: 0.00001220
Iteration 201/1000 | Loss: 0.00001220
Iteration 202/1000 | Loss: 0.00001220
Iteration 203/1000 | Loss: 0.00001220
Iteration 204/1000 | Loss: 0.00001220
Iteration 205/1000 | Loss: 0.00001220
Iteration 206/1000 | Loss: 0.00001220
Iteration 207/1000 | Loss: 0.00001220
Iteration 208/1000 | Loss: 0.00001220
Iteration 209/1000 | Loss: 0.00001220
Iteration 210/1000 | Loss: 0.00001220
Iteration 211/1000 | Loss: 0.00001220
Iteration 212/1000 | Loss: 0.00001220
Iteration 213/1000 | Loss: 0.00001220
Iteration 214/1000 | Loss: 0.00001220
Iteration 215/1000 | Loss: 0.00001220
Iteration 216/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.2195765521028079e-05, 1.2195765521028079e-05, 1.2195765521028079e-05, 1.2195765521028079e-05, 1.2195765521028079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2195765521028079e-05

Optimization complete. Final v2v error: 3.0375657081604004 mm

Highest mean error: 3.3537099361419678 mm for frame 90

Lowest mean error: 2.7803430557250977 mm for frame 111

Saving results

Total time: 44.706011056900024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961026
Iteration 2/25 | Loss: 0.00961026
Iteration 3/25 | Loss: 0.00961025
Iteration 4/25 | Loss: 0.00961025
Iteration 5/25 | Loss: 0.00298539
Iteration 6/25 | Loss: 0.00209942
Iteration 7/25 | Loss: 0.00190923
Iteration 8/25 | Loss: 0.00187606
Iteration 9/25 | Loss: 0.00191877
Iteration 10/25 | Loss: 0.00176883
Iteration 11/25 | Loss: 0.00159125
Iteration 12/25 | Loss: 0.00154665
Iteration 13/25 | Loss: 0.00150319
Iteration 14/25 | Loss: 0.00149980
Iteration 15/25 | Loss: 0.00148072
Iteration 16/25 | Loss: 0.00146112
Iteration 17/25 | Loss: 0.00142883
Iteration 18/25 | Loss: 0.00142597
Iteration 19/25 | Loss: 0.00141453
Iteration 20/25 | Loss: 0.00141509
Iteration 21/25 | Loss: 0.00140629
Iteration 22/25 | Loss: 0.00140934
Iteration 23/25 | Loss: 0.00140398
Iteration 24/25 | Loss: 0.00141160
Iteration 25/25 | Loss: 0.00140649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25049531
Iteration 2/25 | Loss: 0.00233691
Iteration 3/25 | Loss: 0.00217080
Iteration 4/25 | Loss: 0.00217080
Iteration 5/25 | Loss: 0.00217080
Iteration 6/25 | Loss: 0.00217080
Iteration 7/25 | Loss: 0.00217080
Iteration 8/25 | Loss: 0.00217080
Iteration 9/25 | Loss: 0.00217080
Iteration 10/25 | Loss: 0.00217080
Iteration 11/25 | Loss: 0.00217080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00217079883441329, 0.00217079883441329, 0.00217079883441329, 0.00217079883441329, 0.00217079883441329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00217079883441329

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217080
Iteration 2/1000 | Loss: 0.00007572
Iteration 3/1000 | Loss: 0.00035863
Iteration 4/1000 | Loss: 0.00029238
Iteration 5/1000 | Loss: 0.00005246
Iteration 6/1000 | Loss: 0.00003920
Iteration 7/1000 | Loss: 0.00005577
Iteration 8/1000 | Loss: 0.00003449
Iteration 9/1000 | Loss: 0.00003313
Iteration 10/1000 | Loss: 0.00027761
Iteration 11/1000 | Loss: 0.00003201
Iteration 12/1000 | Loss: 0.00003143
Iteration 13/1000 | Loss: 0.00013086
Iteration 14/1000 | Loss: 0.00003095
Iteration 15/1000 | Loss: 0.00021228
Iteration 16/1000 | Loss: 0.00043132
Iteration 17/1000 | Loss: 0.00003468
Iteration 18/1000 | Loss: 0.00011865
Iteration 19/1000 | Loss: 0.00003022
Iteration 20/1000 | Loss: 0.00002993
Iteration 21/1000 | Loss: 0.00002961
Iteration 22/1000 | Loss: 0.00002933
Iteration 23/1000 | Loss: 0.00028867
Iteration 24/1000 | Loss: 0.00002997
Iteration 25/1000 | Loss: 0.00002906
Iteration 26/1000 | Loss: 0.00002875
Iteration 27/1000 | Loss: 0.00002857
Iteration 28/1000 | Loss: 0.00002844
Iteration 29/1000 | Loss: 0.00002843
Iteration 30/1000 | Loss: 0.00002840
Iteration 31/1000 | Loss: 0.00002839
Iteration 32/1000 | Loss: 0.00002839
Iteration 33/1000 | Loss: 0.00002833
Iteration 34/1000 | Loss: 0.00052625
Iteration 35/1000 | Loss: 0.00011876
Iteration 36/1000 | Loss: 0.00010484
Iteration 37/1000 | Loss: 0.00003079
Iteration 38/1000 | Loss: 0.00011170
Iteration 39/1000 | Loss: 0.00002642
Iteration 40/1000 | Loss: 0.00002585
Iteration 41/1000 | Loss: 0.00002556
Iteration 42/1000 | Loss: 0.00002540
Iteration 43/1000 | Loss: 0.00017820
Iteration 44/1000 | Loss: 0.00002777
Iteration 45/1000 | Loss: 0.00006678
Iteration 46/1000 | Loss: 0.00002530
Iteration 47/1000 | Loss: 0.00002523
Iteration 48/1000 | Loss: 0.00002519
Iteration 49/1000 | Loss: 0.00002518
Iteration 50/1000 | Loss: 0.00002517
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002507
Iteration 53/1000 | Loss: 0.00002507
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002502
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002501
Iteration 58/1000 | Loss: 0.00002499
Iteration 59/1000 | Loss: 0.00002495
Iteration 60/1000 | Loss: 0.00002493
Iteration 61/1000 | Loss: 0.00002493
Iteration 62/1000 | Loss: 0.00002492
Iteration 63/1000 | Loss: 0.00002492
Iteration 64/1000 | Loss: 0.00002492
Iteration 65/1000 | Loss: 0.00002492
Iteration 66/1000 | Loss: 0.00002491
Iteration 67/1000 | Loss: 0.00002491
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00002490
Iteration 70/1000 | Loss: 0.00002490
Iteration 71/1000 | Loss: 0.00002489
Iteration 72/1000 | Loss: 0.00002489
Iteration 73/1000 | Loss: 0.00002489
Iteration 74/1000 | Loss: 0.00002489
Iteration 75/1000 | Loss: 0.00002488
Iteration 76/1000 | Loss: 0.00002488
Iteration 77/1000 | Loss: 0.00002488
Iteration 78/1000 | Loss: 0.00002488
Iteration 79/1000 | Loss: 0.00002488
Iteration 80/1000 | Loss: 0.00002488
Iteration 81/1000 | Loss: 0.00002488
Iteration 82/1000 | Loss: 0.00002488
Iteration 83/1000 | Loss: 0.00002488
Iteration 84/1000 | Loss: 0.00002487
Iteration 85/1000 | Loss: 0.00002487
Iteration 86/1000 | Loss: 0.00002487
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002487
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002487
Iteration 95/1000 | Loss: 0.00002487
Iteration 96/1000 | Loss: 0.00002487
Iteration 97/1000 | Loss: 0.00002487
Iteration 98/1000 | Loss: 0.00002486
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002486
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00002486
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002485
Iteration 113/1000 | Loss: 0.00002485
Iteration 114/1000 | Loss: 0.00002485
Iteration 115/1000 | Loss: 0.00002485
Iteration 116/1000 | Loss: 0.00002485
Iteration 117/1000 | Loss: 0.00002485
Iteration 118/1000 | Loss: 0.00002485
Iteration 119/1000 | Loss: 0.00002484
Iteration 120/1000 | Loss: 0.00002484
Iteration 121/1000 | Loss: 0.00002484
Iteration 122/1000 | Loss: 0.00002484
Iteration 123/1000 | Loss: 0.00002484
Iteration 124/1000 | Loss: 0.00002484
Iteration 125/1000 | Loss: 0.00002484
Iteration 126/1000 | Loss: 0.00002483
Iteration 127/1000 | Loss: 0.00002483
Iteration 128/1000 | Loss: 0.00002482
Iteration 129/1000 | Loss: 0.00002482
Iteration 130/1000 | Loss: 0.00002482
Iteration 131/1000 | Loss: 0.00002482
Iteration 132/1000 | Loss: 0.00002482
Iteration 133/1000 | Loss: 0.00002481
Iteration 134/1000 | Loss: 0.00002481
Iteration 135/1000 | Loss: 0.00002481
Iteration 136/1000 | Loss: 0.00002481
Iteration 137/1000 | Loss: 0.00002481
Iteration 138/1000 | Loss: 0.00002481
Iteration 139/1000 | Loss: 0.00002480
Iteration 140/1000 | Loss: 0.00002480
Iteration 141/1000 | Loss: 0.00002480
Iteration 142/1000 | Loss: 0.00002480
Iteration 143/1000 | Loss: 0.00002480
Iteration 144/1000 | Loss: 0.00002480
Iteration 145/1000 | Loss: 0.00002480
Iteration 146/1000 | Loss: 0.00002480
Iteration 147/1000 | Loss: 0.00002479
Iteration 148/1000 | Loss: 0.00002479
Iteration 149/1000 | Loss: 0.00002479
Iteration 150/1000 | Loss: 0.00002478
Iteration 151/1000 | Loss: 0.00002478
Iteration 152/1000 | Loss: 0.00002478
Iteration 153/1000 | Loss: 0.00002478
Iteration 154/1000 | Loss: 0.00002478
Iteration 155/1000 | Loss: 0.00002478
Iteration 156/1000 | Loss: 0.00002477
Iteration 157/1000 | Loss: 0.00002477
Iteration 158/1000 | Loss: 0.00002477
Iteration 159/1000 | Loss: 0.00002477
Iteration 160/1000 | Loss: 0.00002477
Iteration 161/1000 | Loss: 0.00002477
Iteration 162/1000 | Loss: 0.00002476
Iteration 163/1000 | Loss: 0.00002476
Iteration 164/1000 | Loss: 0.00002476
Iteration 165/1000 | Loss: 0.00002476
Iteration 166/1000 | Loss: 0.00002476
Iteration 167/1000 | Loss: 0.00002476
Iteration 168/1000 | Loss: 0.00020789
Iteration 169/1000 | Loss: 0.00002501
Iteration 170/1000 | Loss: 0.00002477
Iteration 171/1000 | Loss: 0.00002474
Iteration 172/1000 | Loss: 0.00002474
Iteration 173/1000 | Loss: 0.00002474
Iteration 174/1000 | Loss: 0.00002474
Iteration 175/1000 | Loss: 0.00002474
Iteration 176/1000 | Loss: 0.00002474
Iteration 177/1000 | Loss: 0.00002474
Iteration 178/1000 | Loss: 0.00002474
Iteration 179/1000 | Loss: 0.00002473
Iteration 180/1000 | Loss: 0.00002473
Iteration 181/1000 | Loss: 0.00002473
Iteration 182/1000 | Loss: 0.00002473
Iteration 183/1000 | Loss: 0.00002473
Iteration 184/1000 | Loss: 0.00002473
Iteration 185/1000 | Loss: 0.00002473
Iteration 186/1000 | Loss: 0.00002473
Iteration 187/1000 | Loss: 0.00002473
Iteration 188/1000 | Loss: 0.00002473
Iteration 189/1000 | Loss: 0.00002473
Iteration 190/1000 | Loss: 0.00002473
Iteration 191/1000 | Loss: 0.00002473
Iteration 192/1000 | Loss: 0.00002473
Iteration 193/1000 | Loss: 0.00002473
Iteration 194/1000 | Loss: 0.00002473
Iteration 195/1000 | Loss: 0.00002473
Iteration 196/1000 | Loss: 0.00002473
Iteration 197/1000 | Loss: 0.00002473
Iteration 198/1000 | Loss: 0.00002473
Iteration 199/1000 | Loss: 0.00002473
Iteration 200/1000 | Loss: 0.00002473
Iteration 201/1000 | Loss: 0.00002473
Iteration 202/1000 | Loss: 0.00002473
Iteration 203/1000 | Loss: 0.00002473
Iteration 204/1000 | Loss: 0.00002473
Iteration 205/1000 | Loss: 0.00002473
Iteration 206/1000 | Loss: 0.00002473
Iteration 207/1000 | Loss: 0.00002473
Iteration 208/1000 | Loss: 0.00002473
Iteration 209/1000 | Loss: 0.00002473
Iteration 210/1000 | Loss: 0.00002473
Iteration 211/1000 | Loss: 0.00002473
Iteration 212/1000 | Loss: 0.00002473
Iteration 213/1000 | Loss: 0.00002473
Iteration 214/1000 | Loss: 0.00002473
Iteration 215/1000 | Loss: 0.00002473
Iteration 216/1000 | Loss: 0.00002473
Iteration 217/1000 | Loss: 0.00002473
Iteration 218/1000 | Loss: 0.00002473
Iteration 219/1000 | Loss: 0.00002473
Iteration 220/1000 | Loss: 0.00002473
Iteration 221/1000 | Loss: 0.00002473
Iteration 222/1000 | Loss: 0.00002473
Iteration 223/1000 | Loss: 0.00002473
Iteration 224/1000 | Loss: 0.00002473
Iteration 225/1000 | Loss: 0.00002473
Iteration 226/1000 | Loss: 0.00002473
Iteration 227/1000 | Loss: 0.00002473
Iteration 228/1000 | Loss: 0.00002473
Iteration 229/1000 | Loss: 0.00002473
Iteration 230/1000 | Loss: 0.00002473
Iteration 231/1000 | Loss: 0.00002473
Iteration 232/1000 | Loss: 0.00002473
Iteration 233/1000 | Loss: 0.00002473
Iteration 234/1000 | Loss: 0.00002473
Iteration 235/1000 | Loss: 0.00002473
Iteration 236/1000 | Loss: 0.00002473
Iteration 237/1000 | Loss: 0.00002473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.4728373318794183e-05, 2.4728373318794183e-05, 2.4728373318794183e-05, 2.4728373318794183e-05, 2.4728373318794183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4728373318794183e-05

Optimization complete. Final v2v error: 4.307365894317627 mm

Highest mean error: 4.919078826904297 mm for frame 45

Lowest mean error: 3.6049044132232666 mm for frame 97

Saving results

Total time: 112.6374819278717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053117
Iteration 2/25 | Loss: 0.01053117
Iteration 3/25 | Loss: 0.01053116
Iteration 4/25 | Loss: 0.01053116
Iteration 5/25 | Loss: 0.01053116
Iteration 6/25 | Loss: 0.01053116
Iteration 7/25 | Loss: 0.01053116
Iteration 8/25 | Loss: 0.01053116
Iteration 9/25 | Loss: 0.01053116
Iteration 10/25 | Loss: 0.01053116
Iteration 11/25 | Loss: 0.01053116
Iteration 12/25 | Loss: 0.01053116
Iteration 13/25 | Loss: 0.01053116
Iteration 14/25 | Loss: 0.01053115
Iteration 15/25 | Loss: 0.01053115
Iteration 16/25 | Loss: 0.01053115
Iteration 17/25 | Loss: 0.01053115
Iteration 18/25 | Loss: 0.01053115
Iteration 19/25 | Loss: 0.01053115
Iteration 20/25 | Loss: 0.01053115
Iteration 21/25 | Loss: 0.01053115
Iteration 22/25 | Loss: 0.01053115
Iteration 23/25 | Loss: 0.01053114
Iteration 24/25 | Loss: 0.01053114
Iteration 25/25 | Loss: 0.01053114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57737601
Iteration 2/25 | Loss: 0.06788760
Iteration 3/25 | Loss: 0.06762130
Iteration 4/25 | Loss: 0.06762130
Iteration 5/25 | Loss: 0.06762130
Iteration 6/25 | Loss: 0.06762129
Iteration 7/25 | Loss: 0.06762129
Iteration 8/25 | Loss: 0.06762129
Iteration 9/25 | Loss: 0.06762129
Iteration 10/25 | Loss: 0.06762129
Iteration 11/25 | Loss: 0.06762129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06762129068374634, 0.06762129068374634, 0.06762129068374634, 0.06762129068374634, 0.06762129068374634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06762129068374634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06762129
Iteration 2/1000 | Loss: 0.01497877
Iteration 3/1000 | Loss: 0.00565244
Iteration 4/1000 | Loss: 0.01016391
Iteration 5/1000 | Loss: 0.00102843
Iteration 6/1000 | Loss: 0.00234360
Iteration 7/1000 | Loss: 0.00084932
Iteration 8/1000 | Loss: 0.00052175
Iteration 9/1000 | Loss: 0.00032609
Iteration 10/1000 | Loss: 0.00020109
Iteration 11/1000 | Loss: 0.00007470
Iteration 12/1000 | Loss: 0.00023603
Iteration 13/1000 | Loss: 0.00008214
Iteration 14/1000 | Loss: 0.00027208
Iteration 15/1000 | Loss: 0.00049977
Iteration 16/1000 | Loss: 0.00154184
Iteration 17/1000 | Loss: 0.00129917
Iteration 18/1000 | Loss: 0.00041148
Iteration 19/1000 | Loss: 0.00038015
Iteration 20/1000 | Loss: 0.00022997
Iteration 21/1000 | Loss: 0.00014437
Iteration 22/1000 | Loss: 0.00034378
Iteration 23/1000 | Loss: 0.00013843
Iteration 24/1000 | Loss: 0.00006770
Iteration 25/1000 | Loss: 0.00016475
Iteration 26/1000 | Loss: 0.00005121
Iteration 27/1000 | Loss: 0.00016295
Iteration 28/1000 | Loss: 0.00003686
Iteration 29/1000 | Loss: 0.00016607
Iteration 30/1000 | Loss: 0.00004138
Iteration 31/1000 | Loss: 0.00022445
Iteration 32/1000 | Loss: 0.00031109
Iteration 33/1000 | Loss: 0.00011064
Iteration 34/1000 | Loss: 0.00003314
Iteration 35/1000 | Loss: 0.00003873
Iteration 36/1000 | Loss: 0.00013044
Iteration 37/1000 | Loss: 0.00010035
Iteration 38/1000 | Loss: 0.00002301
Iteration 39/1000 | Loss: 0.00009577
Iteration 40/1000 | Loss: 0.00007235
Iteration 41/1000 | Loss: 0.00005918
Iteration 42/1000 | Loss: 0.00004547
Iteration 43/1000 | Loss: 0.00003617
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00004704
Iteration 46/1000 | Loss: 0.00006814
Iteration 47/1000 | Loss: 0.00011395
Iteration 48/1000 | Loss: 0.00030855
Iteration 49/1000 | Loss: 0.00110418
Iteration 50/1000 | Loss: 0.00011537
Iteration 51/1000 | Loss: 0.00016812
Iteration 52/1000 | Loss: 0.00002100
Iteration 53/1000 | Loss: 0.00006836
Iteration 54/1000 | Loss: 0.00022903
Iteration 55/1000 | Loss: 0.00002169
Iteration 56/1000 | Loss: 0.00002916
Iteration 57/1000 | Loss: 0.00002008
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00005002
Iteration 60/1000 | Loss: 0.00004799
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00005932
Iteration 63/1000 | Loss: 0.00002493
Iteration 64/1000 | Loss: 0.00004318
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00004837
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00002781
Iteration 70/1000 | Loss: 0.00001944
Iteration 71/1000 | Loss: 0.00001853
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00009341
Iteration 75/1000 | Loss: 0.00120400
Iteration 76/1000 | Loss: 0.00003510
Iteration 77/1000 | Loss: 0.00013281
Iteration 78/1000 | Loss: 0.00005155
Iteration 79/1000 | Loss: 0.00002745
Iteration 80/1000 | Loss: 0.00002543
Iteration 81/1000 | Loss: 0.00002428
Iteration 82/1000 | Loss: 0.00001837
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00006756
Iteration 86/1000 | Loss: 0.00014777
Iteration 87/1000 | Loss: 0.00003638
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00004278
Iteration 90/1000 | Loss: 0.00003879
Iteration 91/1000 | Loss: 0.00003052
Iteration 92/1000 | Loss: 0.00019995
Iteration 93/1000 | Loss: 0.00001809
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00011001
Iteration 96/1000 | Loss: 0.00008313
Iteration 97/1000 | Loss: 0.00003935
Iteration 98/1000 | Loss: 0.00004858
Iteration 99/1000 | Loss: 0.00004732
Iteration 100/1000 | Loss: 0.00003267
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00002176
Iteration 104/1000 | Loss: 0.00001759
Iteration 105/1000 | Loss: 0.00001759
Iteration 106/1000 | Loss: 0.00001759
Iteration 107/1000 | Loss: 0.00001759
Iteration 108/1000 | Loss: 0.00001759
Iteration 109/1000 | Loss: 0.00001759
Iteration 110/1000 | Loss: 0.00001759
Iteration 111/1000 | Loss: 0.00001759
Iteration 112/1000 | Loss: 0.00001759
Iteration 113/1000 | Loss: 0.00001759
Iteration 114/1000 | Loss: 0.00001758
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001757
Iteration 119/1000 | Loss: 0.00003374
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001752
Iteration 126/1000 | Loss: 0.00001752
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00003811
Iteration 131/1000 | Loss: 0.00002092
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00003545
Iteration 134/1000 | Loss: 0.00013300
Iteration 135/1000 | Loss: 0.00005560
Iteration 136/1000 | Loss: 0.00001946
Iteration 137/1000 | Loss: 0.00002599
Iteration 138/1000 | Loss: 0.00001737
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001734
Iteration 141/1000 | Loss: 0.00001734
Iteration 142/1000 | Loss: 0.00001731
Iteration 143/1000 | Loss: 0.00002111
Iteration 144/1000 | Loss: 0.00001730
Iteration 145/1000 | Loss: 0.00001730
Iteration 146/1000 | Loss: 0.00001730
Iteration 147/1000 | Loss: 0.00001730
Iteration 148/1000 | Loss: 0.00001730
Iteration 149/1000 | Loss: 0.00001730
Iteration 150/1000 | Loss: 0.00001730
Iteration 151/1000 | Loss: 0.00001730
Iteration 152/1000 | Loss: 0.00001730
Iteration 153/1000 | Loss: 0.00001730
Iteration 154/1000 | Loss: 0.00001730
Iteration 155/1000 | Loss: 0.00001729
Iteration 156/1000 | Loss: 0.00001729
Iteration 157/1000 | Loss: 0.00001728
Iteration 158/1000 | Loss: 0.00001728
Iteration 159/1000 | Loss: 0.00001728
Iteration 160/1000 | Loss: 0.00001728
Iteration 161/1000 | Loss: 0.00001728
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001727
Iteration 165/1000 | Loss: 0.00001727
Iteration 166/1000 | Loss: 0.00001727
Iteration 167/1000 | Loss: 0.00001726
Iteration 168/1000 | Loss: 0.00001726
Iteration 169/1000 | Loss: 0.00001726
Iteration 170/1000 | Loss: 0.00001726
Iteration 171/1000 | Loss: 0.00001726
Iteration 172/1000 | Loss: 0.00001726
Iteration 173/1000 | Loss: 0.00001726
Iteration 174/1000 | Loss: 0.00001726
Iteration 175/1000 | Loss: 0.00001726
Iteration 176/1000 | Loss: 0.00001726
Iteration 177/1000 | Loss: 0.00001725
Iteration 178/1000 | Loss: 0.00001725
Iteration 179/1000 | Loss: 0.00001725
Iteration 180/1000 | Loss: 0.00001725
Iteration 181/1000 | Loss: 0.00001725
Iteration 182/1000 | Loss: 0.00001725
Iteration 183/1000 | Loss: 0.00001725
Iteration 184/1000 | Loss: 0.00001725
Iteration 185/1000 | Loss: 0.00001725
Iteration 186/1000 | Loss: 0.00001725
Iteration 187/1000 | Loss: 0.00001725
Iteration 188/1000 | Loss: 0.00001724
Iteration 189/1000 | Loss: 0.00001724
Iteration 190/1000 | Loss: 0.00001724
Iteration 191/1000 | Loss: 0.00001724
Iteration 192/1000 | Loss: 0.00001724
Iteration 193/1000 | Loss: 0.00001724
Iteration 194/1000 | Loss: 0.00001723
Iteration 195/1000 | Loss: 0.00001723
Iteration 196/1000 | Loss: 0.00001723
Iteration 197/1000 | Loss: 0.00001723
Iteration 198/1000 | Loss: 0.00001723
Iteration 199/1000 | Loss: 0.00001723
Iteration 200/1000 | Loss: 0.00001723
Iteration 201/1000 | Loss: 0.00001723
Iteration 202/1000 | Loss: 0.00001722
Iteration 203/1000 | Loss: 0.00001722
Iteration 204/1000 | Loss: 0.00001722
Iteration 205/1000 | Loss: 0.00001722
Iteration 206/1000 | Loss: 0.00001722
Iteration 207/1000 | Loss: 0.00001722
Iteration 208/1000 | Loss: 0.00001722
Iteration 209/1000 | Loss: 0.00001722
Iteration 210/1000 | Loss: 0.00001721
Iteration 211/1000 | Loss: 0.00001721
Iteration 212/1000 | Loss: 0.00001721
Iteration 213/1000 | Loss: 0.00001721
Iteration 214/1000 | Loss: 0.00001721
Iteration 215/1000 | Loss: 0.00001721
Iteration 216/1000 | Loss: 0.00001721
Iteration 217/1000 | Loss: 0.00001721
Iteration 218/1000 | Loss: 0.00001721
Iteration 219/1000 | Loss: 0.00001721
Iteration 220/1000 | Loss: 0.00001721
Iteration 221/1000 | Loss: 0.00001720
Iteration 222/1000 | Loss: 0.00001720
Iteration 223/1000 | Loss: 0.00001719
Iteration 224/1000 | Loss: 0.00001719
Iteration 225/1000 | Loss: 0.00001719
Iteration 226/1000 | Loss: 0.00001719
Iteration 227/1000 | Loss: 0.00001719
Iteration 228/1000 | Loss: 0.00001719
Iteration 229/1000 | Loss: 0.00001719
Iteration 230/1000 | Loss: 0.00001719
Iteration 231/1000 | Loss: 0.00001719
Iteration 232/1000 | Loss: 0.00001719
Iteration 233/1000 | Loss: 0.00001719
Iteration 234/1000 | Loss: 0.00001718
Iteration 235/1000 | Loss: 0.00004516
Iteration 236/1000 | Loss: 0.00011112
Iteration 237/1000 | Loss: 0.00004157
Iteration 238/1000 | Loss: 0.00001725
Iteration 239/1000 | Loss: 0.00004202
Iteration 240/1000 | Loss: 0.00001719
Iteration 241/1000 | Loss: 0.00001719
Iteration 242/1000 | Loss: 0.00001718
Iteration 243/1000 | Loss: 0.00001718
Iteration 244/1000 | Loss: 0.00001717
Iteration 245/1000 | Loss: 0.00001717
Iteration 246/1000 | Loss: 0.00001717
Iteration 247/1000 | Loss: 0.00001716
Iteration 248/1000 | Loss: 0.00001716
Iteration 249/1000 | Loss: 0.00001715
Iteration 250/1000 | Loss: 0.00001715
Iteration 251/1000 | Loss: 0.00001715
Iteration 252/1000 | Loss: 0.00001714
Iteration 253/1000 | Loss: 0.00001714
Iteration 254/1000 | Loss: 0.00001713
Iteration 255/1000 | Loss: 0.00001713
Iteration 256/1000 | Loss: 0.00001713
Iteration 257/1000 | Loss: 0.00001713
Iteration 258/1000 | Loss: 0.00001712
Iteration 259/1000 | Loss: 0.00001712
Iteration 260/1000 | Loss: 0.00001712
Iteration 261/1000 | Loss: 0.00001712
Iteration 262/1000 | Loss: 0.00001711
Iteration 263/1000 | Loss: 0.00001711
Iteration 264/1000 | Loss: 0.00001711
Iteration 265/1000 | Loss: 0.00001711
Iteration 266/1000 | Loss: 0.00001710
Iteration 267/1000 | Loss: 0.00001710
Iteration 268/1000 | Loss: 0.00001710
Iteration 269/1000 | Loss: 0.00001710
Iteration 270/1000 | Loss: 0.00001710
Iteration 271/1000 | Loss: 0.00001710
Iteration 272/1000 | Loss: 0.00001710
Iteration 273/1000 | Loss: 0.00001710
Iteration 274/1000 | Loss: 0.00001710
Iteration 275/1000 | Loss: 0.00001710
Iteration 276/1000 | Loss: 0.00001710
Iteration 277/1000 | Loss: 0.00001710
Iteration 278/1000 | Loss: 0.00001710
Iteration 279/1000 | Loss: 0.00001710
Iteration 280/1000 | Loss: 0.00001710
Iteration 281/1000 | Loss: 0.00001710
Iteration 282/1000 | Loss: 0.00001710
Iteration 283/1000 | Loss: 0.00001710
Iteration 284/1000 | Loss: 0.00001710
Iteration 285/1000 | Loss: 0.00001710
Iteration 286/1000 | Loss: 0.00001710
Iteration 287/1000 | Loss: 0.00001710
Iteration 288/1000 | Loss: 0.00001710
Iteration 289/1000 | Loss: 0.00001710
Iteration 290/1000 | Loss: 0.00001710
Iteration 291/1000 | Loss: 0.00001710
Iteration 292/1000 | Loss: 0.00001710
Iteration 293/1000 | Loss: 0.00001710
Iteration 294/1000 | Loss: 0.00001710
Iteration 295/1000 | Loss: 0.00001710
Iteration 296/1000 | Loss: 0.00001710
Iteration 297/1000 | Loss: 0.00001710
Iteration 298/1000 | Loss: 0.00001710
Iteration 299/1000 | Loss: 0.00001710
Iteration 300/1000 | Loss: 0.00001710
Iteration 301/1000 | Loss: 0.00001710
Iteration 302/1000 | Loss: 0.00001710
Iteration 303/1000 | Loss: 0.00001710
Iteration 304/1000 | Loss: 0.00001710
Iteration 305/1000 | Loss: 0.00001710
Iteration 306/1000 | Loss: 0.00001710
Iteration 307/1000 | Loss: 0.00001710
Iteration 308/1000 | Loss: 0.00001710
Iteration 309/1000 | Loss: 0.00001710
Iteration 310/1000 | Loss: 0.00001710
Iteration 311/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.7102580386563204e-05, 1.7102580386563204e-05, 1.7102580386563204e-05, 1.7102580386563204e-05, 1.7102580386563204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7102580386563204e-05

Optimization complete. Final v2v error: 3.4217026233673096 mm

Highest mean error: 4.64513635635376 mm for frame 165

Lowest mean error: 2.751805067062378 mm for frame 240

Saving results

Total time: 195.11112117767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834219
Iteration 2/25 | Loss: 0.00157978
Iteration 3/25 | Loss: 0.00140661
Iteration 4/25 | Loss: 0.00139049
Iteration 5/25 | Loss: 0.00138771
Iteration 6/25 | Loss: 0.00138737
Iteration 7/25 | Loss: 0.00138737
Iteration 8/25 | Loss: 0.00138737
Iteration 9/25 | Loss: 0.00138737
Iteration 10/25 | Loss: 0.00138737
Iteration 11/25 | Loss: 0.00138737
Iteration 12/25 | Loss: 0.00138737
Iteration 13/25 | Loss: 0.00138737
Iteration 14/25 | Loss: 0.00138737
Iteration 15/25 | Loss: 0.00138737
Iteration 16/25 | Loss: 0.00138737
Iteration 17/25 | Loss: 0.00138737
Iteration 18/25 | Loss: 0.00138737
Iteration 19/25 | Loss: 0.00138737
Iteration 20/25 | Loss: 0.00138737
Iteration 21/25 | Loss: 0.00138737
Iteration 22/25 | Loss: 0.00138737
Iteration 23/25 | Loss: 0.00138737
Iteration 24/25 | Loss: 0.00138737
Iteration 25/25 | Loss: 0.00138737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84138083
Iteration 2/25 | Loss: 0.00139046
Iteration 3/25 | Loss: 0.00139045
Iteration 4/25 | Loss: 0.00139045
Iteration 5/25 | Loss: 0.00139045
Iteration 6/25 | Loss: 0.00139045
Iteration 7/25 | Loss: 0.00139045
Iteration 8/25 | Loss: 0.00139045
Iteration 9/25 | Loss: 0.00139045
Iteration 10/25 | Loss: 0.00139045
Iteration 11/25 | Loss: 0.00139045
Iteration 12/25 | Loss: 0.00139045
Iteration 13/25 | Loss: 0.00139045
Iteration 14/25 | Loss: 0.00139045
Iteration 15/25 | Loss: 0.00139045
Iteration 16/25 | Loss: 0.00139045
Iteration 17/25 | Loss: 0.00139045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013904465595260262, 0.0013904465595260262, 0.0013904465595260262, 0.0013904465595260262, 0.0013904465595260262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013904465595260262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139045
Iteration 2/1000 | Loss: 0.00003791
Iteration 3/1000 | Loss: 0.00002873
Iteration 4/1000 | Loss: 0.00002584
Iteration 5/1000 | Loss: 0.00002475
Iteration 6/1000 | Loss: 0.00002374
Iteration 7/1000 | Loss: 0.00002290
Iteration 8/1000 | Loss: 0.00002231
Iteration 9/1000 | Loss: 0.00002175
Iteration 10/1000 | Loss: 0.00002126
Iteration 11/1000 | Loss: 0.00002098
Iteration 12/1000 | Loss: 0.00002078
Iteration 13/1000 | Loss: 0.00002073
Iteration 14/1000 | Loss: 0.00002057
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002049
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002046
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00002045
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002043
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002039
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002038
Iteration 32/1000 | Loss: 0.00002037
Iteration 33/1000 | Loss: 0.00002036
Iteration 34/1000 | Loss: 0.00002036
Iteration 35/1000 | Loss: 0.00002036
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002036
Iteration 38/1000 | Loss: 0.00002036
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002035
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002031
Iteration 45/1000 | Loss: 0.00002031
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002030
Iteration 50/1000 | Loss: 0.00002030
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002030
Iteration 53/1000 | Loss: 0.00002029
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002029
Iteration 56/1000 | Loss: 0.00002028
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002027
Iteration 59/1000 | Loss: 0.00002027
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002026
Iteration 63/1000 | Loss: 0.00002026
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00002024
Iteration 66/1000 | Loss: 0.00002022
Iteration 67/1000 | Loss: 0.00002022
Iteration 68/1000 | Loss: 0.00002022
Iteration 69/1000 | Loss: 0.00002022
Iteration 70/1000 | Loss: 0.00002022
Iteration 71/1000 | Loss: 0.00002020
Iteration 72/1000 | Loss: 0.00002020
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00002018
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002018
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002017
Iteration 84/1000 | Loss: 0.00002017
Iteration 85/1000 | Loss: 0.00002017
Iteration 86/1000 | Loss: 0.00002017
Iteration 87/1000 | Loss: 0.00002017
Iteration 88/1000 | Loss: 0.00002017
Iteration 89/1000 | Loss: 0.00002016
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002016
Iteration 92/1000 | Loss: 0.00002016
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002011
Iteration 100/1000 | Loss: 0.00002011
Iteration 101/1000 | Loss: 0.00002010
Iteration 102/1000 | Loss: 0.00002010
Iteration 103/1000 | Loss: 0.00002010
Iteration 104/1000 | Loss: 0.00002010
Iteration 105/1000 | Loss: 0.00002010
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002010
Iteration 108/1000 | Loss: 0.00002010
Iteration 109/1000 | Loss: 0.00002010
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002009
Iteration 112/1000 | Loss: 0.00002009
Iteration 113/1000 | Loss: 0.00002009
Iteration 114/1000 | Loss: 0.00002009
Iteration 115/1000 | Loss: 0.00002009
Iteration 116/1000 | Loss: 0.00002008
Iteration 117/1000 | Loss: 0.00002008
Iteration 118/1000 | Loss: 0.00002008
Iteration 119/1000 | Loss: 0.00002008
Iteration 120/1000 | Loss: 0.00002008
Iteration 121/1000 | Loss: 0.00002008
Iteration 122/1000 | Loss: 0.00002008
Iteration 123/1000 | Loss: 0.00002008
Iteration 124/1000 | Loss: 0.00002008
Iteration 125/1000 | Loss: 0.00002008
Iteration 126/1000 | Loss: 0.00002008
Iteration 127/1000 | Loss: 0.00002008
Iteration 128/1000 | Loss: 0.00002008
Iteration 129/1000 | Loss: 0.00002008
Iteration 130/1000 | Loss: 0.00002008
Iteration 131/1000 | Loss: 0.00002008
Iteration 132/1000 | Loss: 0.00002008
Iteration 133/1000 | Loss: 0.00002007
Iteration 134/1000 | Loss: 0.00002007
Iteration 135/1000 | Loss: 0.00002007
Iteration 136/1000 | Loss: 0.00002007
Iteration 137/1000 | Loss: 0.00002007
Iteration 138/1000 | Loss: 0.00002007
Iteration 139/1000 | Loss: 0.00002007
Iteration 140/1000 | Loss: 0.00002007
Iteration 141/1000 | Loss: 0.00002007
Iteration 142/1000 | Loss: 0.00002007
Iteration 143/1000 | Loss: 0.00002007
Iteration 144/1000 | Loss: 0.00002007
Iteration 145/1000 | Loss: 0.00002007
Iteration 146/1000 | Loss: 0.00002007
Iteration 147/1000 | Loss: 0.00002007
Iteration 148/1000 | Loss: 0.00002007
Iteration 149/1000 | Loss: 0.00002007
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002007
Iteration 152/1000 | Loss: 0.00002007
Iteration 153/1000 | Loss: 0.00002007
Iteration 154/1000 | Loss: 0.00002007
Iteration 155/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0069244783371687e-05, 2.0069244783371687e-05, 2.0069244783371687e-05, 2.0069244783371687e-05, 2.0069244783371687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0069244783371687e-05

Optimization complete. Final v2v error: 3.8391594886779785 mm

Highest mean error: 3.9792792797088623 mm for frame 40

Lowest mean error: 3.744478225708008 mm for frame 66

Saving results

Total time: 39.27645564079285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720123
Iteration 2/25 | Loss: 0.00146533
Iteration 3/25 | Loss: 0.00141026
Iteration 4/25 | Loss: 0.00140155
Iteration 5/25 | Loss: 0.00139798
Iteration 6/25 | Loss: 0.00139693
Iteration 7/25 | Loss: 0.00139693
Iteration 8/25 | Loss: 0.00139693
Iteration 9/25 | Loss: 0.00139693
Iteration 10/25 | Loss: 0.00139693
Iteration 11/25 | Loss: 0.00139693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013969329884275794, 0.0013969329884275794, 0.0013969329884275794, 0.0013969329884275794, 0.0013969329884275794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013969329884275794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.37212038
Iteration 2/25 | Loss: 0.00202332
Iteration 3/25 | Loss: 0.00202331
Iteration 4/25 | Loss: 0.00202331
Iteration 5/25 | Loss: 0.00202331
Iteration 6/25 | Loss: 0.00202331
Iteration 7/25 | Loss: 0.00202331
Iteration 8/25 | Loss: 0.00202331
Iteration 9/25 | Loss: 0.00202331
Iteration 10/25 | Loss: 0.00202331
Iteration 11/25 | Loss: 0.00202331
Iteration 12/25 | Loss: 0.00202331
Iteration 13/25 | Loss: 0.00202331
Iteration 14/25 | Loss: 0.00202331
Iteration 15/25 | Loss: 0.00202331
Iteration 16/25 | Loss: 0.00202331
Iteration 17/25 | Loss: 0.00202331
Iteration 18/25 | Loss: 0.00202331
Iteration 19/25 | Loss: 0.00202331
Iteration 20/25 | Loss: 0.00202331
Iteration 21/25 | Loss: 0.00202331
Iteration 22/25 | Loss: 0.00202331
Iteration 23/25 | Loss: 0.00202331
Iteration 24/25 | Loss: 0.00202331
Iteration 25/25 | Loss: 0.00202331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202331
Iteration 2/1000 | Loss: 0.00002885
Iteration 3/1000 | Loss: 0.00001855
Iteration 4/1000 | Loss: 0.00001677
Iteration 5/1000 | Loss: 0.00001580
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001468
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001406
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001312
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001284
Iteration 29/1000 | Loss: 0.00001284
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001284
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001280
Iteration 35/1000 | Loss: 0.00001279
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001278
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001277
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001255
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001251
Iteration 68/1000 | Loss: 0.00001251
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001249
Iteration 73/1000 | Loss: 0.00001249
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001243
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001241
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001240
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001240
Iteration 115/1000 | Loss: 0.00001240
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001239
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001238
Iteration 125/1000 | Loss: 0.00001238
Iteration 126/1000 | Loss: 0.00001238
Iteration 127/1000 | Loss: 0.00001238
Iteration 128/1000 | Loss: 0.00001238
Iteration 129/1000 | Loss: 0.00001238
Iteration 130/1000 | Loss: 0.00001238
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001237
Iteration 135/1000 | Loss: 0.00001237
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Iteration 141/1000 | Loss: 0.00001237
Iteration 142/1000 | Loss: 0.00001237
Iteration 143/1000 | Loss: 0.00001237
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001236
Iteration 153/1000 | Loss: 0.00001236
Iteration 154/1000 | Loss: 0.00001236
Iteration 155/1000 | Loss: 0.00001236
Iteration 156/1000 | Loss: 0.00001236
Iteration 157/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2364408576104324e-05, 1.2364408576104324e-05, 1.2364408576104324e-05, 1.2364408576104324e-05, 1.2364408576104324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2364408576104324e-05

Optimization complete. Final v2v error: 3.0105650424957275 mm

Highest mean error: 3.2507903575897217 mm for frame 37

Lowest mean error: 2.814791202545166 mm for frame 82

Saving results

Total time: 41.3098828792572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730600
Iteration 2/25 | Loss: 0.00207421
Iteration 3/25 | Loss: 0.00180074
Iteration 4/25 | Loss: 0.00176581
Iteration 5/25 | Loss: 0.00170286
Iteration 6/25 | Loss: 0.00169177
Iteration 7/25 | Loss: 0.00168927
Iteration 8/25 | Loss: 0.00167708
Iteration 9/25 | Loss: 0.00168071
Iteration 10/25 | Loss: 0.00166115
Iteration 11/25 | Loss: 0.00165610
Iteration 12/25 | Loss: 0.00165577
Iteration 13/25 | Loss: 0.00165492
Iteration 14/25 | Loss: 0.00165834
Iteration 15/25 | Loss: 0.00164969
Iteration 16/25 | Loss: 0.00164905
Iteration 17/25 | Loss: 0.00164903
Iteration 18/25 | Loss: 0.00164902
Iteration 19/25 | Loss: 0.00164902
Iteration 20/25 | Loss: 0.00164902
Iteration 21/25 | Loss: 0.00164902
Iteration 22/25 | Loss: 0.00164902
Iteration 23/25 | Loss: 0.00164902
Iteration 24/25 | Loss: 0.00164902
Iteration 25/25 | Loss: 0.00164902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55933082
Iteration 2/25 | Loss: 0.00281046
Iteration 3/25 | Loss: 0.00281014
Iteration 4/25 | Loss: 0.00281014
Iteration 5/25 | Loss: 0.00281014
Iteration 6/25 | Loss: 0.00281014
Iteration 7/25 | Loss: 0.00281014
Iteration 8/25 | Loss: 0.00281014
Iteration 9/25 | Loss: 0.00281014
Iteration 10/25 | Loss: 0.00281014
Iteration 11/25 | Loss: 0.00281014
Iteration 12/25 | Loss: 0.00281014
Iteration 13/25 | Loss: 0.00281014
Iteration 14/25 | Loss: 0.00281014
Iteration 15/25 | Loss: 0.00281014
Iteration 16/25 | Loss: 0.00281014
Iteration 17/25 | Loss: 0.00281014
Iteration 18/25 | Loss: 0.00281014
Iteration 19/25 | Loss: 0.00281014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002810137113556266, 0.002810137113556266, 0.002810137113556266, 0.002810137113556266, 0.002810137113556266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002810137113556266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00281014
Iteration 2/1000 | Loss: 0.00250686
Iteration 3/1000 | Loss: 0.00015137
Iteration 4/1000 | Loss: 0.00008516
Iteration 5/1000 | Loss: 0.00006103
Iteration 6/1000 | Loss: 0.00005153
Iteration 7/1000 | Loss: 0.00004733
Iteration 8/1000 | Loss: 0.00004443
Iteration 9/1000 | Loss: 0.00004240
Iteration 10/1000 | Loss: 0.00004099
Iteration 11/1000 | Loss: 0.00003979
Iteration 12/1000 | Loss: 0.00003870
Iteration 13/1000 | Loss: 0.00003792
Iteration 14/1000 | Loss: 0.00003731
Iteration 15/1000 | Loss: 0.00003680
Iteration 16/1000 | Loss: 0.00003636
Iteration 17/1000 | Loss: 0.00003600
Iteration 18/1000 | Loss: 0.00003570
Iteration 19/1000 | Loss: 0.00003540
Iteration 20/1000 | Loss: 0.00003523
Iteration 21/1000 | Loss: 0.00003520
Iteration 22/1000 | Loss: 0.00003513
Iteration 23/1000 | Loss: 0.00003506
Iteration 24/1000 | Loss: 0.00003504
Iteration 25/1000 | Loss: 0.00003500
Iteration 26/1000 | Loss: 0.00003493
Iteration 27/1000 | Loss: 0.00004669
Iteration 28/1000 | Loss: 0.00004669
Iteration 29/1000 | Loss: 0.00005839
Iteration 30/1000 | Loss: 0.00004707
Iteration 31/1000 | Loss: 0.00003685
Iteration 32/1000 | Loss: 0.00005215
Iteration 33/1000 | Loss: 0.00005344
Iteration 34/1000 | Loss: 0.00005161
Iteration 35/1000 | Loss: 0.00004614
Iteration 36/1000 | Loss: 0.00005010
Iteration 37/1000 | Loss: 0.00004386
Iteration 38/1000 | Loss: 0.00003699
Iteration 39/1000 | Loss: 0.00003633
Iteration 40/1000 | Loss: 0.00003546
Iteration 41/1000 | Loss: 0.00003511
Iteration 42/1000 | Loss: 0.00003498
Iteration 43/1000 | Loss: 0.00003490
Iteration 44/1000 | Loss: 0.00003486
Iteration 45/1000 | Loss: 0.00003486
Iteration 46/1000 | Loss: 0.00003735
Iteration 47/1000 | Loss: 0.00003552
Iteration 48/1000 | Loss: 0.00003516
Iteration 49/1000 | Loss: 0.00003498
Iteration 50/1000 | Loss: 0.00003591
Iteration 51/1000 | Loss: 0.00003591
Iteration 52/1000 | Loss: 0.00003497
Iteration 53/1000 | Loss: 0.00003553
Iteration 54/1000 | Loss: 0.00003492
Iteration 55/1000 | Loss: 0.00003547
Iteration 56/1000 | Loss: 0.00003492
Iteration 57/1000 | Loss: 0.00003492
Iteration 58/1000 | Loss: 0.00003525
Iteration 59/1000 | Loss: 0.00003483
Iteration 60/1000 | Loss: 0.00003483
Iteration 61/1000 | Loss: 0.00003483
Iteration 62/1000 | Loss: 0.00003494
Iteration 63/1000 | Loss: 0.00003478
Iteration 64/1000 | Loss: 0.00003477
Iteration 65/1000 | Loss: 0.00003476
Iteration 66/1000 | Loss: 0.00003476
Iteration 67/1000 | Loss: 0.00003476
Iteration 68/1000 | Loss: 0.00003476
Iteration 69/1000 | Loss: 0.00003475
Iteration 70/1000 | Loss: 0.00003475
Iteration 71/1000 | Loss: 0.00003475
Iteration 72/1000 | Loss: 0.00003475
Iteration 73/1000 | Loss: 0.00003474
Iteration 74/1000 | Loss: 0.00003474
Iteration 75/1000 | Loss: 0.00003474
Iteration 76/1000 | Loss: 0.00003474
Iteration 77/1000 | Loss: 0.00003473
Iteration 78/1000 | Loss: 0.00003473
Iteration 79/1000 | Loss: 0.00003473
Iteration 80/1000 | Loss: 0.00003472
Iteration 81/1000 | Loss: 0.00003472
Iteration 82/1000 | Loss: 0.00003472
Iteration 83/1000 | Loss: 0.00003472
Iteration 84/1000 | Loss: 0.00003472
Iteration 85/1000 | Loss: 0.00003472
Iteration 86/1000 | Loss: 0.00003471
Iteration 87/1000 | Loss: 0.00003471
Iteration 88/1000 | Loss: 0.00003471
Iteration 89/1000 | Loss: 0.00003471
Iteration 90/1000 | Loss: 0.00003470
Iteration 91/1000 | Loss: 0.00003470
Iteration 92/1000 | Loss: 0.00003470
Iteration 93/1000 | Loss: 0.00003470
Iteration 94/1000 | Loss: 0.00003470
Iteration 95/1000 | Loss: 0.00003469
Iteration 96/1000 | Loss: 0.00003469
Iteration 97/1000 | Loss: 0.00003469
Iteration 98/1000 | Loss: 0.00003469
Iteration 99/1000 | Loss: 0.00003469
Iteration 100/1000 | Loss: 0.00003468
Iteration 101/1000 | Loss: 0.00003468
Iteration 102/1000 | Loss: 0.00003467
Iteration 103/1000 | Loss: 0.00003467
Iteration 104/1000 | Loss: 0.00003467
Iteration 105/1000 | Loss: 0.00003466
Iteration 106/1000 | Loss: 0.00003465
Iteration 107/1000 | Loss: 0.00003465
Iteration 108/1000 | Loss: 0.00003464
Iteration 109/1000 | Loss: 0.00003464
Iteration 110/1000 | Loss: 0.00003464
Iteration 111/1000 | Loss: 0.00003464
Iteration 112/1000 | Loss: 0.00003463
Iteration 113/1000 | Loss: 0.00003463
Iteration 114/1000 | Loss: 0.00003462
Iteration 115/1000 | Loss: 0.00003462
Iteration 116/1000 | Loss: 0.00003461
Iteration 117/1000 | Loss: 0.00003461
Iteration 118/1000 | Loss: 0.00003461
Iteration 119/1000 | Loss: 0.00003460
Iteration 120/1000 | Loss: 0.00003460
Iteration 121/1000 | Loss: 0.00003459
Iteration 122/1000 | Loss: 0.00003459
Iteration 123/1000 | Loss: 0.00003459
Iteration 124/1000 | Loss: 0.00003459
Iteration 125/1000 | Loss: 0.00003458
Iteration 126/1000 | Loss: 0.00003458
Iteration 127/1000 | Loss: 0.00003458
Iteration 128/1000 | Loss: 0.00003458
Iteration 129/1000 | Loss: 0.00003458
Iteration 130/1000 | Loss: 0.00003458
Iteration 131/1000 | Loss: 0.00003458
Iteration 132/1000 | Loss: 0.00003457
Iteration 133/1000 | Loss: 0.00003457
Iteration 134/1000 | Loss: 0.00003456
Iteration 135/1000 | Loss: 0.00003456
Iteration 136/1000 | Loss: 0.00003456
Iteration 137/1000 | Loss: 0.00003456
Iteration 138/1000 | Loss: 0.00003456
Iteration 139/1000 | Loss: 0.00003456
Iteration 140/1000 | Loss: 0.00003456
Iteration 141/1000 | Loss: 0.00003455
Iteration 142/1000 | Loss: 0.00003455
Iteration 143/1000 | Loss: 0.00003455
Iteration 144/1000 | Loss: 0.00003455
Iteration 145/1000 | Loss: 0.00003455
Iteration 146/1000 | Loss: 0.00003455
Iteration 147/1000 | Loss: 0.00003454
Iteration 148/1000 | Loss: 0.00003454
Iteration 149/1000 | Loss: 0.00003454
Iteration 150/1000 | Loss: 0.00003454
Iteration 151/1000 | Loss: 0.00003454
Iteration 152/1000 | Loss: 0.00003454
Iteration 153/1000 | Loss: 0.00003454
Iteration 154/1000 | Loss: 0.00003454
Iteration 155/1000 | Loss: 0.00003454
Iteration 156/1000 | Loss: 0.00003454
Iteration 157/1000 | Loss: 0.00003454
Iteration 158/1000 | Loss: 0.00003454
Iteration 159/1000 | Loss: 0.00003454
Iteration 160/1000 | Loss: 0.00003454
Iteration 161/1000 | Loss: 0.00003454
Iteration 162/1000 | Loss: 0.00003454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.45392654708121e-05, 3.45392654708121e-05, 3.45392654708121e-05, 3.45392654708121e-05, 3.45392654708121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.45392654708121e-05

Optimization complete. Final v2v error: 4.710464954376221 mm

Highest mean error: 7.819668292999268 mm for frame 179

Lowest mean error: 3.124626636505127 mm for frame 239

Saving results

Total time: 120.21761798858643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703001
Iteration 2/25 | Loss: 0.00175321
Iteration 3/25 | Loss: 0.00152091
Iteration 4/25 | Loss: 0.00149373
Iteration 5/25 | Loss: 0.00148672
Iteration 6/25 | Loss: 0.00148406
Iteration 7/25 | Loss: 0.00148283
Iteration 8/25 | Loss: 0.00148204
Iteration 9/25 | Loss: 0.00148158
Iteration 10/25 | Loss: 0.00148286
Iteration 11/25 | Loss: 0.00148185
Iteration 12/25 | Loss: 0.00148009
Iteration 13/25 | Loss: 0.00147848
Iteration 14/25 | Loss: 0.00147828
Iteration 15/25 | Loss: 0.00147816
Iteration 16/25 | Loss: 0.00147815
Iteration 17/25 | Loss: 0.00147815
Iteration 18/25 | Loss: 0.00147815
Iteration 19/25 | Loss: 0.00147815
Iteration 20/25 | Loss: 0.00147815
Iteration 21/25 | Loss: 0.00147815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014781453646719456, 0.0014781453646719456, 0.0014781453646719456, 0.0014781453646719456, 0.0014781453646719456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014781453646719456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16194594
Iteration 2/25 | Loss: 0.00235803
Iteration 3/25 | Loss: 0.00235800
Iteration 4/25 | Loss: 0.00235800
Iteration 5/25 | Loss: 0.00235800
Iteration 6/25 | Loss: 0.00235800
Iteration 7/25 | Loss: 0.00235800
Iteration 8/25 | Loss: 0.00235800
Iteration 9/25 | Loss: 0.00235800
Iteration 10/25 | Loss: 0.00235800
Iteration 11/25 | Loss: 0.00235800
Iteration 12/25 | Loss: 0.00235800
Iteration 13/25 | Loss: 0.00235800
Iteration 14/25 | Loss: 0.00235800
Iteration 15/25 | Loss: 0.00235800
Iteration 16/25 | Loss: 0.00235800
Iteration 17/25 | Loss: 0.00235800
Iteration 18/25 | Loss: 0.00235800
Iteration 19/25 | Loss: 0.00235800
Iteration 20/25 | Loss: 0.00235800
Iteration 21/25 | Loss: 0.00235800
Iteration 22/25 | Loss: 0.00235800
Iteration 23/25 | Loss: 0.00235800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002357999561354518, 0.002357999561354518, 0.002357999561354518, 0.002357999561354518, 0.002357999561354518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002357999561354518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235800
Iteration 2/1000 | Loss: 0.00009417
Iteration 3/1000 | Loss: 0.00006409
Iteration 4/1000 | Loss: 0.00005651
Iteration 5/1000 | Loss: 0.00005178
Iteration 6/1000 | Loss: 0.00004948
Iteration 7/1000 | Loss: 0.00004806
Iteration 8/1000 | Loss: 0.00004719
Iteration 9/1000 | Loss: 0.00004657
Iteration 10/1000 | Loss: 0.00004612
Iteration 11/1000 | Loss: 0.00004566
Iteration 12/1000 | Loss: 0.00004535
Iteration 13/1000 | Loss: 0.00004500
Iteration 14/1000 | Loss: 0.00004469
Iteration 15/1000 | Loss: 0.00004447
Iteration 16/1000 | Loss: 0.00004430
Iteration 17/1000 | Loss: 0.00004415
Iteration 18/1000 | Loss: 0.00004398
Iteration 19/1000 | Loss: 0.00004397
Iteration 20/1000 | Loss: 0.00004389
Iteration 21/1000 | Loss: 0.00004385
Iteration 22/1000 | Loss: 0.00004383
Iteration 23/1000 | Loss: 0.00004382
Iteration 24/1000 | Loss: 0.00004380
Iteration 25/1000 | Loss: 0.00004378
Iteration 26/1000 | Loss: 0.00004374
Iteration 27/1000 | Loss: 0.00004374
Iteration 28/1000 | Loss: 0.00004373
Iteration 29/1000 | Loss: 0.00004373
Iteration 30/1000 | Loss: 0.00004372
Iteration 31/1000 | Loss: 0.00004370
Iteration 32/1000 | Loss: 0.00004368
Iteration 33/1000 | Loss: 0.00004361
Iteration 34/1000 | Loss: 0.00004359
Iteration 35/1000 | Loss: 0.00004359
Iteration 36/1000 | Loss: 0.00004358
Iteration 37/1000 | Loss: 0.00004356
Iteration 38/1000 | Loss: 0.00004356
Iteration 39/1000 | Loss: 0.00004356
Iteration 40/1000 | Loss: 0.00004356
Iteration 41/1000 | Loss: 0.00004355
Iteration 42/1000 | Loss: 0.00004355
Iteration 43/1000 | Loss: 0.00004355
Iteration 44/1000 | Loss: 0.00004354
Iteration 45/1000 | Loss: 0.00004354
Iteration 46/1000 | Loss: 0.00004354
Iteration 47/1000 | Loss: 0.00004353
Iteration 48/1000 | Loss: 0.00004353
Iteration 49/1000 | Loss: 0.00004352
Iteration 50/1000 | Loss: 0.00004352
Iteration 51/1000 | Loss: 0.00004351
Iteration 52/1000 | Loss: 0.00004351
Iteration 53/1000 | Loss: 0.00004351
Iteration 54/1000 | Loss: 0.00004349
Iteration 55/1000 | Loss: 0.00004348
Iteration 56/1000 | Loss: 0.00004347
Iteration 57/1000 | Loss: 0.00004346
Iteration 58/1000 | Loss: 0.00004346
Iteration 59/1000 | Loss: 0.00004345
Iteration 60/1000 | Loss: 0.00004345
Iteration 61/1000 | Loss: 0.00004345
Iteration 62/1000 | Loss: 0.00004345
Iteration 63/1000 | Loss: 0.00004345
Iteration 64/1000 | Loss: 0.00004345
Iteration 65/1000 | Loss: 0.00004345
Iteration 66/1000 | Loss: 0.00004344
Iteration 67/1000 | Loss: 0.00004343
Iteration 68/1000 | Loss: 0.00004343
Iteration 69/1000 | Loss: 0.00004343
Iteration 70/1000 | Loss: 0.00004342
Iteration 71/1000 | Loss: 0.00004342
Iteration 72/1000 | Loss: 0.00004342
Iteration 73/1000 | Loss: 0.00004341
Iteration 74/1000 | Loss: 0.00004341
Iteration 75/1000 | Loss: 0.00004341
Iteration 76/1000 | Loss: 0.00004341
Iteration 77/1000 | Loss: 0.00004341
Iteration 78/1000 | Loss: 0.00004341
Iteration 79/1000 | Loss: 0.00004341
Iteration 80/1000 | Loss: 0.00004341
Iteration 81/1000 | Loss: 0.00004341
Iteration 82/1000 | Loss: 0.00004340
Iteration 83/1000 | Loss: 0.00004340
Iteration 84/1000 | Loss: 0.00004340
Iteration 85/1000 | Loss: 0.00004340
Iteration 86/1000 | Loss: 0.00004340
Iteration 87/1000 | Loss: 0.00004340
Iteration 88/1000 | Loss: 0.00004340
Iteration 89/1000 | Loss: 0.00004340
Iteration 90/1000 | Loss: 0.00004340
Iteration 91/1000 | Loss: 0.00004340
Iteration 92/1000 | Loss: 0.00004340
Iteration 93/1000 | Loss: 0.00004339
Iteration 94/1000 | Loss: 0.00004339
Iteration 95/1000 | Loss: 0.00004339
Iteration 96/1000 | Loss: 0.00004338
Iteration 97/1000 | Loss: 0.00004338
Iteration 98/1000 | Loss: 0.00004338
Iteration 99/1000 | Loss: 0.00004338
Iteration 100/1000 | Loss: 0.00004338
Iteration 101/1000 | Loss: 0.00004338
Iteration 102/1000 | Loss: 0.00004338
Iteration 103/1000 | Loss: 0.00004338
Iteration 104/1000 | Loss: 0.00004338
Iteration 105/1000 | Loss: 0.00004338
Iteration 106/1000 | Loss: 0.00004338
Iteration 107/1000 | Loss: 0.00004338
Iteration 108/1000 | Loss: 0.00004338
Iteration 109/1000 | Loss: 0.00004337
Iteration 110/1000 | Loss: 0.00004337
Iteration 111/1000 | Loss: 0.00004337
Iteration 112/1000 | Loss: 0.00004337
Iteration 113/1000 | Loss: 0.00004337
Iteration 114/1000 | Loss: 0.00004336
Iteration 115/1000 | Loss: 0.00004336
Iteration 116/1000 | Loss: 0.00004336
Iteration 117/1000 | Loss: 0.00004335
Iteration 118/1000 | Loss: 0.00004335
Iteration 119/1000 | Loss: 0.00004335
Iteration 120/1000 | Loss: 0.00004335
Iteration 121/1000 | Loss: 0.00004335
Iteration 122/1000 | Loss: 0.00004335
Iteration 123/1000 | Loss: 0.00004335
Iteration 124/1000 | Loss: 0.00004334
Iteration 125/1000 | Loss: 0.00004334
Iteration 126/1000 | Loss: 0.00004334
Iteration 127/1000 | Loss: 0.00004333
Iteration 128/1000 | Loss: 0.00004333
Iteration 129/1000 | Loss: 0.00004333
Iteration 130/1000 | Loss: 0.00004332
Iteration 131/1000 | Loss: 0.00004332
Iteration 132/1000 | Loss: 0.00004332
Iteration 133/1000 | Loss: 0.00004332
Iteration 134/1000 | Loss: 0.00004332
Iteration 135/1000 | Loss: 0.00004332
Iteration 136/1000 | Loss: 0.00004332
Iteration 137/1000 | Loss: 0.00004332
Iteration 138/1000 | Loss: 0.00004332
Iteration 139/1000 | Loss: 0.00004332
Iteration 140/1000 | Loss: 0.00004331
Iteration 141/1000 | Loss: 0.00004331
Iteration 142/1000 | Loss: 0.00004331
Iteration 143/1000 | Loss: 0.00004330
Iteration 144/1000 | Loss: 0.00004330
Iteration 145/1000 | Loss: 0.00004330
Iteration 146/1000 | Loss: 0.00004330
Iteration 147/1000 | Loss: 0.00004329
Iteration 148/1000 | Loss: 0.00004329
Iteration 149/1000 | Loss: 0.00004329
Iteration 150/1000 | Loss: 0.00004329
Iteration 151/1000 | Loss: 0.00004328
Iteration 152/1000 | Loss: 0.00004328
Iteration 153/1000 | Loss: 0.00004327
Iteration 154/1000 | Loss: 0.00004327
Iteration 155/1000 | Loss: 0.00004327
Iteration 156/1000 | Loss: 0.00004327
Iteration 157/1000 | Loss: 0.00004326
Iteration 158/1000 | Loss: 0.00004326
Iteration 159/1000 | Loss: 0.00004326
Iteration 160/1000 | Loss: 0.00004326
Iteration 161/1000 | Loss: 0.00004326
Iteration 162/1000 | Loss: 0.00004326
Iteration 163/1000 | Loss: 0.00004325
Iteration 164/1000 | Loss: 0.00004325
Iteration 165/1000 | Loss: 0.00004325
Iteration 166/1000 | Loss: 0.00004325
Iteration 167/1000 | Loss: 0.00004325
Iteration 168/1000 | Loss: 0.00004325
Iteration 169/1000 | Loss: 0.00004325
Iteration 170/1000 | Loss: 0.00004324
Iteration 171/1000 | Loss: 0.00004324
Iteration 172/1000 | Loss: 0.00004324
Iteration 173/1000 | Loss: 0.00004323
Iteration 174/1000 | Loss: 0.00004323
Iteration 175/1000 | Loss: 0.00004322
Iteration 176/1000 | Loss: 0.00004322
Iteration 177/1000 | Loss: 0.00004322
Iteration 178/1000 | Loss: 0.00004321
Iteration 179/1000 | Loss: 0.00004321
Iteration 180/1000 | Loss: 0.00004321
Iteration 181/1000 | Loss: 0.00004320
Iteration 182/1000 | Loss: 0.00004320
Iteration 183/1000 | Loss: 0.00004319
Iteration 184/1000 | Loss: 0.00004319
Iteration 185/1000 | Loss: 0.00004319
Iteration 186/1000 | Loss: 0.00004318
Iteration 187/1000 | Loss: 0.00004318
Iteration 188/1000 | Loss: 0.00004318
Iteration 189/1000 | Loss: 0.00004318
Iteration 190/1000 | Loss: 0.00004318
Iteration 191/1000 | Loss: 0.00004317
Iteration 192/1000 | Loss: 0.00004317
Iteration 193/1000 | Loss: 0.00004317
Iteration 194/1000 | Loss: 0.00004317
Iteration 195/1000 | Loss: 0.00004317
Iteration 196/1000 | Loss: 0.00004317
Iteration 197/1000 | Loss: 0.00004317
Iteration 198/1000 | Loss: 0.00004316
Iteration 199/1000 | Loss: 0.00004316
Iteration 200/1000 | Loss: 0.00004316
Iteration 201/1000 | Loss: 0.00004316
Iteration 202/1000 | Loss: 0.00004316
Iteration 203/1000 | Loss: 0.00004316
Iteration 204/1000 | Loss: 0.00004316
Iteration 205/1000 | Loss: 0.00004316
Iteration 206/1000 | Loss: 0.00004316
Iteration 207/1000 | Loss: 0.00004315
Iteration 208/1000 | Loss: 0.00004315
Iteration 209/1000 | Loss: 0.00004315
Iteration 210/1000 | Loss: 0.00004315
Iteration 211/1000 | Loss: 0.00004315
Iteration 212/1000 | Loss: 0.00004315
Iteration 213/1000 | Loss: 0.00004315
Iteration 214/1000 | Loss: 0.00004315
Iteration 215/1000 | Loss: 0.00004315
Iteration 216/1000 | Loss: 0.00004314
Iteration 217/1000 | Loss: 0.00004314
Iteration 218/1000 | Loss: 0.00004314
Iteration 219/1000 | Loss: 0.00004314
Iteration 220/1000 | Loss: 0.00004314
Iteration 221/1000 | Loss: 0.00004314
Iteration 222/1000 | Loss: 0.00004314
Iteration 223/1000 | Loss: 0.00004313
Iteration 224/1000 | Loss: 0.00004313
Iteration 225/1000 | Loss: 0.00004313
Iteration 226/1000 | Loss: 0.00004313
Iteration 227/1000 | Loss: 0.00004312
Iteration 228/1000 | Loss: 0.00004312
Iteration 229/1000 | Loss: 0.00004312
Iteration 230/1000 | Loss: 0.00004312
Iteration 231/1000 | Loss: 0.00004311
Iteration 232/1000 | Loss: 0.00004311
Iteration 233/1000 | Loss: 0.00004311
Iteration 234/1000 | Loss: 0.00004311
Iteration 235/1000 | Loss: 0.00004311
Iteration 236/1000 | Loss: 0.00004310
Iteration 237/1000 | Loss: 0.00004310
Iteration 238/1000 | Loss: 0.00004310
Iteration 239/1000 | Loss: 0.00004310
Iteration 240/1000 | Loss: 0.00004310
Iteration 241/1000 | Loss: 0.00004310
Iteration 242/1000 | Loss: 0.00004310
Iteration 243/1000 | Loss: 0.00004310
Iteration 244/1000 | Loss: 0.00004310
Iteration 245/1000 | Loss: 0.00004310
Iteration 246/1000 | Loss: 0.00004310
Iteration 247/1000 | Loss: 0.00004309
Iteration 248/1000 | Loss: 0.00004309
Iteration 249/1000 | Loss: 0.00004309
Iteration 250/1000 | Loss: 0.00004309
Iteration 251/1000 | Loss: 0.00004309
Iteration 252/1000 | Loss: 0.00004309
Iteration 253/1000 | Loss: 0.00004309
Iteration 254/1000 | Loss: 0.00004309
Iteration 255/1000 | Loss: 0.00004309
Iteration 256/1000 | Loss: 0.00004308
Iteration 257/1000 | Loss: 0.00004308
Iteration 258/1000 | Loss: 0.00004308
Iteration 259/1000 | Loss: 0.00004308
Iteration 260/1000 | Loss: 0.00004308
Iteration 261/1000 | Loss: 0.00004308
Iteration 262/1000 | Loss: 0.00004307
Iteration 263/1000 | Loss: 0.00004307
Iteration 264/1000 | Loss: 0.00004307
Iteration 265/1000 | Loss: 0.00004307
Iteration 266/1000 | Loss: 0.00004307
Iteration 267/1000 | Loss: 0.00004307
Iteration 268/1000 | Loss: 0.00004306
Iteration 269/1000 | Loss: 0.00004306
Iteration 270/1000 | Loss: 0.00004306
Iteration 271/1000 | Loss: 0.00004305
Iteration 272/1000 | Loss: 0.00004305
Iteration 273/1000 | Loss: 0.00004305
Iteration 274/1000 | Loss: 0.00004305
Iteration 275/1000 | Loss: 0.00004305
Iteration 276/1000 | Loss: 0.00004305
Iteration 277/1000 | Loss: 0.00004305
Iteration 278/1000 | Loss: 0.00004305
Iteration 279/1000 | Loss: 0.00004305
Iteration 280/1000 | Loss: 0.00004304
Iteration 281/1000 | Loss: 0.00004304
Iteration 282/1000 | Loss: 0.00004304
Iteration 283/1000 | Loss: 0.00004304
Iteration 284/1000 | Loss: 0.00004303
Iteration 285/1000 | Loss: 0.00004303
Iteration 286/1000 | Loss: 0.00004303
Iteration 287/1000 | Loss: 0.00004303
Iteration 288/1000 | Loss: 0.00004303
Iteration 289/1000 | Loss: 0.00004302
Iteration 290/1000 | Loss: 0.00004302
Iteration 291/1000 | Loss: 0.00004302
Iteration 292/1000 | Loss: 0.00004302
Iteration 293/1000 | Loss: 0.00004301
Iteration 294/1000 | Loss: 0.00004301
Iteration 295/1000 | Loss: 0.00004301
Iteration 296/1000 | Loss: 0.00004301
Iteration 297/1000 | Loss: 0.00004300
Iteration 298/1000 | Loss: 0.00004300
Iteration 299/1000 | Loss: 0.00004300
Iteration 300/1000 | Loss: 0.00004300
Iteration 301/1000 | Loss: 0.00004300
Iteration 302/1000 | Loss: 0.00004299
Iteration 303/1000 | Loss: 0.00004299
Iteration 304/1000 | Loss: 0.00004299
Iteration 305/1000 | Loss: 0.00004299
Iteration 306/1000 | Loss: 0.00004299
Iteration 307/1000 | Loss: 0.00004299
Iteration 308/1000 | Loss: 0.00004299
Iteration 309/1000 | Loss: 0.00004299
Iteration 310/1000 | Loss: 0.00004299
Iteration 311/1000 | Loss: 0.00004299
Iteration 312/1000 | Loss: 0.00004299
Iteration 313/1000 | Loss: 0.00004299
Iteration 314/1000 | Loss: 0.00004299
Iteration 315/1000 | Loss: 0.00004299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [4.299030115362257e-05, 4.299030115362257e-05, 4.299030115362257e-05, 4.299030115362257e-05, 4.299030115362257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.299030115362257e-05

Optimization complete. Final v2v error: 4.165722370147705 mm

Highest mean error: 11.891047477722168 mm for frame 20

Lowest mean error: 3.0283613204956055 mm for frame 237

Saving results

Total time: 87.71646857261658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070823
Iteration 2/25 | Loss: 0.00198524
Iteration 3/25 | Loss: 0.00153550
Iteration 4/25 | Loss: 0.00150558
Iteration 5/25 | Loss: 0.00149818
Iteration 6/25 | Loss: 0.00149692
Iteration 7/25 | Loss: 0.00149692
Iteration 8/25 | Loss: 0.00149692
Iteration 9/25 | Loss: 0.00149692
Iteration 10/25 | Loss: 0.00149692
Iteration 11/25 | Loss: 0.00149692
Iteration 12/25 | Loss: 0.00149692
Iteration 13/25 | Loss: 0.00149692
Iteration 14/25 | Loss: 0.00149692
Iteration 15/25 | Loss: 0.00149692
Iteration 16/25 | Loss: 0.00149692
Iteration 17/25 | Loss: 0.00149692
Iteration 18/25 | Loss: 0.00149692
Iteration 19/25 | Loss: 0.00149692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014969166368246078, 0.0014969166368246078, 0.0014969166368246078, 0.0014969166368246078, 0.0014969166368246078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014969166368246078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12328899
Iteration 2/25 | Loss: 0.00220867
Iteration 3/25 | Loss: 0.00220866
Iteration 4/25 | Loss: 0.00220866
Iteration 5/25 | Loss: 0.00220866
Iteration 6/25 | Loss: 0.00220866
Iteration 7/25 | Loss: 0.00220866
Iteration 8/25 | Loss: 0.00220866
Iteration 9/25 | Loss: 0.00220866
Iteration 10/25 | Loss: 0.00220866
Iteration 11/25 | Loss: 0.00220866
Iteration 12/25 | Loss: 0.00220866
Iteration 13/25 | Loss: 0.00220866
Iteration 14/25 | Loss: 0.00220866
Iteration 15/25 | Loss: 0.00220866
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022086594253778458, 0.0022086594253778458, 0.0022086594253778458, 0.0022086594253778458, 0.0022086594253778458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022086594253778458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220866
Iteration 2/1000 | Loss: 0.00007799
Iteration 3/1000 | Loss: 0.00004764
Iteration 4/1000 | Loss: 0.00003503
Iteration 5/1000 | Loss: 0.00003243
Iteration 6/1000 | Loss: 0.00003052
Iteration 7/1000 | Loss: 0.00002941
Iteration 8/1000 | Loss: 0.00002867
Iteration 9/1000 | Loss: 0.00002823
Iteration 10/1000 | Loss: 0.00002778
Iteration 11/1000 | Loss: 0.00002746
Iteration 12/1000 | Loss: 0.00002724
Iteration 13/1000 | Loss: 0.00002699
Iteration 14/1000 | Loss: 0.00002681
Iteration 15/1000 | Loss: 0.00002666
Iteration 16/1000 | Loss: 0.00002652
Iteration 17/1000 | Loss: 0.00002634
Iteration 18/1000 | Loss: 0.00002630
Iteration 19/1000 | Loss: 0.00002619
Iteration 20/1000 | Loss: 0.00002615
Iteration 21/1000 | Loss: 0.00002615
Iteration 22/1000 | Loss: 0.00002613
Iteration 23/1000 | Loss: 0.00002609
Iteration 24/1000 | Loss: 0.00002606
Iteration 25/1000 | Loss: 0.00002605
Iteration 26/1000 | Loss: 0.00002605
Iteration 27/1000 | Loss: 0.00002603
Iteration 28/1000 | Loss: 0.00002603
Iteration 29/1000 | Loss: 0.00002603
Iteration 30/1000 | Loss: 0.00002602
Iteration 31/1000 | Loss: 0.00002602
Iteration 32/1000 | Loss: 0.00002602
Iteration 33/1000 | Loss: 0.00002601
Iteration 34/1000 | Loss: 0.00002600
Iteration 35/1000 | Loss: 0.00002599
Iteration 36/1000 | Loss: 0.00002598
Iteration 37/1000 | Loss: 0.00002598
Iteration 38/1000 | Loss: 0.00002597
Iteration 39/1000 | Loss: 0.00002596
Iteration 40/1000 | Loss: 0.00002596
Iteration 41/1000 | Loss: 0.00002596
Iteration 42/1000 | Loss: 0.00002595
Iteration 43/1000 | Loss: 0.00002595
Iteration 44/1000 | Loss: 0.00002594
Iteration 45/1000 | Loss: 0.00002594
Iteration 46/1000 | Loss: 0.00002593
Iteration 47/1000 | Loss: 0.00002592
Iteration 48/1000 | Loss: 0.00002592
Iteration 49/1000 | Loss: 0.00002592
Iteration 50/1000 | Loss: 0.00002590
Iteration 51/1000 | Loss: 0.00002590
Iteration 52/1000 | Loss: 0.00002590
Iteration 53/1000 | Loss: 0.00002589
Iteration 54/1000 | Loss: 0.00002589
Iteration 55/1000 | Loss: 0.00002588
Iteration 56/1000 | Loss: 0.00002587
Iteration 57/1000 | Loss: 0.00002587
Iteration 58/1000 | Loss: 0.00002587
Iteration 59/1000 | Loss: 0.00002587
Iteration 60/1000 | Loss: 0.00002587
Iteration 61/1000 | Loss: 0.00002587
Iteration 62/1000 | Loss: 0.00002587
Iteration 63/1000 | Loss: 0.00002587
Iteration 64/1000 | Loss: 0.00002586
Iteration 65/1000 | Loss: 0.00002586
Iteration 66/1000 | Loss: 0.00002586
Iteration 67/1000 | Loss: 0.00002586
Iteration 68/1000 | Loss: 0.00002585
Iteration 69/1000 | Loss: 0.00002585
Iteration 70/1000 | Loss: 0.00002585
Iteration 71/1000 | Loss: 0.00002584
Iteration 72/1000 | Loss: 0.00002584
Iteration 73/1000 | Loss: 0.00002584
Iteration 74/1000 | Loss: 0.00002584
Iteration 75/1000 | Loss: 0.00002583
Iteration 76/1000 | Loss: 0.00002583
Iteration 77/1000 | Loss: 0.00002583
Iteration 78/1000 | Loss: 0.00002582
Iteration 79/1000 | Loss: 0.00002582
Iteration 80/1000 | Loss: 0.00002582
Iteration 81/1000 | Loss: 0.00002582
Iteration 82/1000 | Loss: 0.00002581
Iteration 83/1000 | Loss: 0.00002581
Iteration 84/1000 | Loss: 0.00002581
Iteration 85/1000 | Loss: 0.00002581
Iteration 86/1000 | Loss: 0.00002581
Iteration 87/1000 | Loss: 0.00002581
Iteration 88/1000 | Loss: 0.00002580
Iteration 89/1000 | Loss: 0.00002580
Iteration 90/1000 | Loss: 0.00002580
Iteration 91/1000 | Loss: 0.00002579
Iteration 92/1000 | Loss: 0.00002579
Iteration 93/1000 | Loss: 0.00002579
Iteration 94/1000 | Loss: 0.00002579
Iteration 95/1000 | Loss: 0.00002579
Iteration 96/1000 | Loss: 0.00002579
Iteration 97/1000 | Loss: 0.00002579
Iteration 98/1000 | Loss: 0.00002579
Iteration 99/1000 | Loss: 0.00002579
Iteration 100/1000 | Loss: 0.00002578
Iteration 101/1000 | Loss: 0.00002578
Iteration 102/1000 | Loss: 0.00002578
Iteration 103/1000 | Loss: 0.00002578
Iteration 104/1000 | Loss: 0.00002578
Iteration 105/1000 | Loss: 0.00002578
Iteration 106/1000 | Loss: 0.00002578
Iteration 107/1000 | Loss: 0.00002578
Iteration 108/1000 | Loss: 0.00002578
Iteration 109/1000 | Loss: 0.00002578
Iteration 110/1000 | Loss: 0.00002578
Iteration 111/1000 | Loss: 0.00002578
Iteration 112/1000 | Loss: 0.00002578
Iteration 113/1000 | Loss: 0.00002578
Iteration 114/1000 | Loss: 0.00002577
Iteration 115/1000 | Loss: 0.00002577
Iteration 116/1000 | Loss: 0.00002577
Iteration 117/1000 | Loss: 0.00002577
Iteration 118/1000 | Loss: 0.00002577
Iteration 119/1000 | Loss: 0.00002577
Iteration 120/1000 | Loss: 0.00002577
Iteration 121/1000 | Loss: 0.00002577
Iteration 122/1000 | Loss: 0.00002577
Iteration 123/1000 | Loss: 0.00002577
Iteration 124/1000 | Loss: 0.00002577
Iteration 125/1000 | Loss: 0.00002577
Iteration 126/1000 | Loss: 0.00002577
Iteration 127/1000 | Loss: 0.00002577
Iteration 128/1000 | Loss: 0.00002577
Iteration 129/1000 | Loss: 0.00002577
Iteration 130/1000 | Loss: 0.00002577
Iteration 131/1000 | Loss: 0.00002576
Iteration 132/1000 | Loss: 0.00002576
Iteration 133/1000 | Loss: 0.00002576
Iteration 134/1000 | Loss: 0.00002576
Iteration 135/1000 | Loss: 0.00002576
Iteration 136/1000 | Loss: 0.00002576
Iteration 137/1000 | Loss: 0.00002576
Iteration 138/1000 | Loss: 0.00002576
Iteration 139/1000 | Loss: 0.00002576
Iteration 140/1000 | Loss: 0.00002576
Iteration 141/1000 | Loss: 0.00002576
Iteration 142/1000 | Loss: 0.00002576
Iteration 143/1000 | Loss: 0.00002576
Iteration 144/1000 | Loss: 0.00002576
Iteration 145/1000 | Loss: 0.00002576
Iteration 146/1000 | Loss: 0.00002576
Iteration 147/1000 | Loss: 0.00002576
Iteration 148/1000 | Loss: 0.00002576
Iteration 149/1000 | Loss: 0.00002576
Iteration 150/1000 | Loss: 0.00002576
Iteration 151/1000 | Loss: 0.00002576
Iteration 152/1000 | Loss: 0.00002576
Iteration 153/1000 | Loss: 0.00002576
Iteration 154/1000 | Loss: 0.00002576
Iteration 155/1000 | Loss: 0.00002576
Iteration 156/1000 | Loss: 0.00002576
Iteration 157/1000 | Loss: 0.00002576
Iteration 158/1000 | Loss: 0.00002576
Iteration 159/1000 | Loss: 0.00002576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.5759942218428478e-05, 2.5759942218428478e-05, 2.5759942218428478e-05, 2.5759942218428478e-05, 2.5759942218428478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5759942218428478e-05

Optimization complete. Final v2v error: 4.194530963897705 mm

Highest mean error: 5.549475193023682 mm for frame 149

Lowest mean error: 3.447066068649292 mm for frame 54

Saving results

Total time: 51.97655439376831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781826
Iteration 2/25 | Loss: 0.00156424
Iteration 3/25 | Loss: 0.00139438
Iteration 4/25 | Loss: 0.00137561
Iteration 5/25 | Loss: 0.00137188
Iteration 6/25 | Loss: 0.00137146
Iteration 7/25 | Loss: 0.00137146
Iteration 8/25 | Loss: 0.00137146
Iteration 9/25 | Loss: 0.00137146
Iteration 10/25 | Loss: 0.00137146
Iteration 11/25 | Loss: 0.00137146
Iteration 12/25 | Loss: 0.00137146
Iteration 13/25 | Loss: 0.00137146
Iteration 14/25 | Loss: 0.00137146
Iteration 15/25 | Loss: 0.00137146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013714644592255354, 0.0013714644592255354, 0.0013714644592255354, 0.0013714644592255354, 0.0013714644592255354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013714644592255354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56933665
Iteration 2/25 | Loss: 0.00217439
Iteration 3/25 | Loss: 0.00217439
Iteration 4/25 | Loss: 0.00217439
Iteration 5/25 | Loss: 0.00217439
Iteration 6/25 | Loss: 0.00217439
Iteration 7/25 | Loss: 0.00217439
Iteration 8/25 | Loss: 0.00217439
Iteration 9/25 | Loss: 0.00217439
Iteration 10/25 | Loss: 0.00217439
Iteration 11/25 | Loss: 0.00217439
Iteration 12/25 | Loss: 0.00217439
Iteration 13/25 | Loss: 0.00217439
Iteration 14/25 | Loss: 0.00217439
Iteration 15/25 | Loss: 0.00217439
Iteration 16/25 | Loss: 0.00217439
Iteration 17/25 | Loss: 0.00217439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021743858233094215, 0.0021743858233094215, 0.0021743858233094215, 0.0021743858233094215, 0.0021743858233094215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021743858233094215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217439
Iteration 2/1000 | Loss: 0.00004921
Iteration 3/1000 | Loss: 0.00003371
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002396
Iteration 7/1000 | Loss: 0.00002306
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002195
Iteration 10/1000 | Loss: 0.00002162
Iteration 11/1000 | Loss: 0.00002120
Iteration 12/1000 | Loss: 0.00002079
Iteration 13/1000 | Loss: 0.00002053
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002026
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002018
Iteration 18/1000 | Loss: 0.00002018
Iteration 19/1000 | Loss: 0.00002017
Iteration 20/1000 | Loss: 0.00002016
Iteration 21/1000 | Loss: 0.00002015
Iteration 22/1000 | Loss: 0.00002014
Iteration 23/1000 | Loss: 0.00002013
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00002006
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001995
Iteration 28/1000 | Loss: 0.00001988
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001968
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001964
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001963
Iteration 40/1000 | Loss: 0.00001963
Iteration 41/1000 | Loss: 0.00001962
Iteration 42/1000 | Loss: 0.00001961
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001959
Iteration 46/1000 | Loss: 0.00001958
Iteration 47/1000 | Loss: 0.00001958
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001957
Iteration 50/1000 | Loss: 0.00001957
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001955
Iteration 53/1000 | Loss: 0.00001955
Iteration 54/1000 | Loss: 0.00001955
Iteration 55/1000 | Loss: 0.00001955
Iteration 56/1000 | Loss: 0.00001955
Iteration 57/1000 | Loss: 0.00001954
Iteration 58/1000 | Loss: 0.00001954
Iteration 59/1000 | Loss: 0.00001954
Iteration 60/1000 | Loss: 0.00001953
Iteration 61/1000 | Loss: 0.00001953
Iteration 62/1000 | Loss: 0.00001953
Iteration 63/1000 | Loss: 0.00001953
Iteration 64/1000 | Loss: 0.00001953
Iteration 65/1000 | Loss: 0.00001953
Iteration 66/1000 | Loss: 0.00001952
Iteration 67/1000 | Loss: 0.00001952
Iteration 68/1000 | Loss: 0.00001952
Iteration 69/1000 | Loss: 0.00001952
Iteration 70/1000 | Loss: 0.00001952
Iteration 71/1000 | Loss: 0.00001952
Iteration 72/1000 | Loss: 0.00001952
Iteration 73/1000 | Loss: 0.00001952
Iteration 74/1000 | Loss: 0.00001952
Iteration 75/1000 | Loss: 0.00001952
Iteration 76/1000 | Loss: 0.00001951
Iteration 77/1000 | Loss: 0.00001951
Iteration 78/1000 | Loss: 0.00001950
Iteration 79/1000 | Loss: 0.00001950
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001947
Iteration 88/1000 | Loss: 0.00001947
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001945
Iteration 102/1000 | Loss: 0.00001944
Iteration 103/1000 | Loss: 0.00001944
Iteration 104/1000 | Loss: 0.00001944
Iteration 105/1000 | Loss: 0.00001944
Iteration 106/1000 | Loss: 0.00001943
Iteration 107/1000 | Loss: 0.00001943
Iteration 108/1000 | Loss: 0.00001943
Iteration 109/1000 | Loss: 0.00001943
Iteration 110/1000 | Loss: 0.00001942
Iteration 111/1000 | Loss: 0.00001942
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001941
Iteration 114/1000 | Loss: 0.00001941
Iteration 115/1000 | Loss: 0.00001941
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001940
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001940
Iteration 125/1000 | Loss: 0.00001940
Iteration 126/1000 | Loss: 0.00001940
Iteration 127/1000 | Loss: 0.00001940
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001939
Iteration 130/1000 | Loss: 0.00001938
Iteration 131/1000 | Loss: 0.00001938
Iteration 132/1000 | Loss: 0.00001938
Iteration 133/1000 | Loss: 0.00001938
Iteration 134/1000 | Loss: 0.00001937
Iteration 135/1000 | Loss: 0.00001937
Iteration 136/1000 | Loss: 0.00001937
Iteration 137/1000 | Loss: 0.00001936
Iteration 138/1000 | Loss: 0.00001936
Iteration 139/1000 | Loss: 0.00001936
Iteration 140/1000 | Loss: 0.00001936
Iteration 141/1000 | Loss: 0.00001936
Iteration 142/1000 | Loss: 0.00001935
Iteration 143/1000 | Loss: 0.00001935
Iteration 144/1000 | Loss: 0.00001935
Iteration 145/1000 | Loss: 0.00001935
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001935
Iteration 148/1000 | Loss: 0.00001935
Iteration 149/1000 | Loss: 0.00001935
Iteration 150/1000 | Loss: 0.00001935
Iteration 151/1000 | Loss: 0.00001934
Iteration 152/1000 | Loss: 0.00001934
Iteration 153/1000 | Loss: 0.00001934
Iteration 154/1000 | Loss: 0.00001934
Iteration 155/1000 | Loss: 0.00001934
Iteration 156/1000 | Loss: 0.00001934
Iteration 157/1000 | Loss: 0.00001934
Iteration 158/1000 | Loss: 0.00001934
Iteration 159/1000 | Loss: 0.00001934
Iteration 160/1000 | Loss: 0.00001934
Iteration 161/1000 | Loss: 0.00001934
Iteration 162/1000 | Loss: 0.00001934
Iteration 163/1000 | Loss: 0.00001933
Iteration 164/1000 | Loss: 0.00001933
Iteration 165/1000 | Loss: 0.00001933
Iteration 166/1000 | Loss: 0.00001933
Iteration 167/1000 | Loss: 0.00001933
Iteration 168/1000 | Loss: 0.00001933
Iteration 169/1000 | Loss: 0.00001933
Iteration 170/1000 | Loss: 0.00001932
Iteration 171/1000 | Loss: 0.00001932
Iteration 172/1000 | Loss: 0.00001932
Iteration 173/1000 | Loss: 0.00001932
Iteration 174/1000 | Loss: 0.00001932
Iteration 175/1000 | Loss: 0.00001932
Iteration 176/1000 | Loss: 0.00001932
Iteration 177/1000 | Loss: 0.00001931
Iteration 178/1000 | Loss: 0.00001931
Iteration 179/1000 | Loss: 0.00001931
Iteration 180/1000 | Loss: 0.00001931
Iteration 181/1000 | Loss: 0.00001931
Iteration 182/1000 | Loss: 0.00001931
Iteration 183/1000 | Loss: 0.00001931
Iteration 184/1000 | Loss: 0.00001931
Iteration 185/1000 | Loss: 0.00001931
Iteration 186/1000 | Loss: 0.00001931
Iteration 187/1000 | Loss: 0.00001931
Iteration 188/1000 | Loss: 0.00001931
Iteration 189/1000 | Loss: 0.00001931
Iteration 190/1000 | Loss: 0.00001931
Iteration 191/1000 | Loss: 0.00001931
Iteration 192/1000 | Loss: 0.00001931
Iteration 193/1000 | Loss: 0.00001931
Iteration 194/1000 | Loss: 0.00001931
Iteration 195/1000 | Loss: 0.00001931
Iteration 196/1000 | Loss: 0.00001931
Iteration 197/1000 | Loss: 0.00001931
Iteration 198/1000 | Loss: 0.00001931
Iteration 199/1000 | Loss: 0.00001931
Iteration 200/1000 | Loss: 0.00001931
Iteration 201/1000 | Loss: 0.00001931
Iteration 202/1000 | Loss: 0.00001931
Iteration 203/1000 | Loss: 0.00001931
Iteration 204/1000 | Loss: 0.00001931
Iteration 205/1000 | Loss: 0.00001931
Iteration 206/1000 | Loss: 0.00001931
Iteration 207/1000 | Loss: 0.00001931
Iteration 208/1000 | Loss: 0.00001931
Iteration 209/1000 | Loss: 0.00001931
Iteration 210/1000 | Loss: 0.00001931
Iteration 211/1000 | Loss: 0.00001931
Iteration 212/1000 | Loss: 0.00001931
Iteration 213/1000 | Loss: 0.00001931
Iteration 214/1000 | Loss: 0.00001931
Iteration 215/1000 | Loss: 0.00001931
Iteration 216/1000 | Loss: 0.00001931
Iteration 217/1000 | Loss: 0.00001931
Iteration 218/1000 | Loss: 0.00001931
Iteration 219/1000 | Loss: 0.00001931
Iteration 220/1000 | Loss: 0.00001931
Iteration 221/1000 | Loss: 0.00001931
Iteration 222/1000 | Loss: 0.00001931
Iteration 223/1000 | Loss: 0.00001931
Iteration 224/1000 | Loss: 0.00001931
Iteration 225/1000 | Loss: 0.00001931
Iteration 226/1000 | Loss: 0.00001931
Iteration 227/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.930810321937315e-05, 1.930810321937315e-05, 1.930810321937315e-05, 1.930810321937315e-05, 1.930810321937315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.930810321937315e-05

Optimization complete. Final v2v error: 3.6882503032684326 mm

Highest mean error: 4.879305839538574 mm for frame 153

Lowest mean error: 2.8539164066314697 mm for frame 49

Saving results

Total time: 53.080970287323
