Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=207, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11592-11647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744206
Iteration 2/25 | Loss: 0.00144541
Iteration 3/25 | Loss: 0.00132607
Iteration 4/25 | Loss: 0.00129714
Iteration 5/25 | Loss: 0.00128672
Iteration 6/25 | Loss: 0.00128457
Iteration 7/25 | Loss: 0.00128457
Iteration 8/25 | Loss: 0.00128457
Iteration 9/25 | Loss: 0.00128457
Iteration 10/25 | Loss: 0.00128457
Iteration 11/25 | Loss: 0.00128457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012845683377236128, 0.0012845683377236128, 0.0012845683377236128, 0.0012845683377236128, 0.0012845683377236128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012845683377236128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37981880
Iteration 2/25 | Loss: 0.00109648
Iteration 3/25 | Loss: 0.00109647
Iteration 4/25 | Loss: 0.00109647
Iteration 5/25 | Loss: 0.00109647
Iteration 6/25 | Loss: 0.00109647
Iteration 7/25 | Loss: 0.00109647
Iteration 8/25 | Loss: 0.00109647
Iteration 9/25 | Loss: 0.00109647
Iteration 10/25 | Loss: 0.00109647
Iteration 11/25 | Loss: 0.00109647
Iteration 12/25 | Loss: 0.00109647
Iteration 13/25 | Loss: 0.00109647
Iteration 14/25 | Loss: 0.00109647
Iteration 15/25 | Loss: 0.00109647
Iteration 16/25 | Loss: 0.00109647
Iteration 17/25 | Loss: 0.00109647
Iteration 18/25 | Loss: 0.00109647
Iteration 19/25 | Loss: 0.00109647
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010964723769575357, 0.0010964723769575357, 0.0010964723769575357, 0.0010964723769575357, 0.0010964723769575357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010964723769575357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109647
Iteration 2/1000 | Loss: 0.00004760
Iteration 3/1000 | Loss: 0.00003290
Iteration 4/1000 | Loss: 0.00002631
Iteration 5/1000 | Loss: 0.00002471
Iteration 6/1000 | Loss: 0.00002340
Iteration 7/1000 | Loss: 0.00002255
Iteration 8/1000 | Loss: 0.00002186
Iteration 9/1000 | Loss: 0.00002142
Iteration 10/1000 | Loss: 0.00002104
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002029
Iteration 14/1000 | Loss: 0.00002023
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002014
Iteration 17/1000 | Loss: 0.00002010
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002005
Iteration 21/1000 | Loss: 0.00001999
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001995
Iteration 25/1000 | Loss: 0.00001995
Iteration 26/1000 | Loss: 0.00001994
Iteration 27/1000 | Loss: 0.00001994
Iteration 28/1000 | Loss: 0.00001993
Iteration 29/1000 | Loss: 0.00001993
Iteration 30/1000 | Loss: 0.00001992
Iteration 31/1000 | Loss: 0.00001992
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001991
Iteration 34/1000 | Loss: 0.00001991
Iteration 35/1000 | Loss: 0.00001990
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001989
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001985
Iteration 48/1000 | Loss: 0.00001985
Iteration 49/1000 | Loss: 0.00001985
Iteration 50/1000 | Loss: 0.00001985
Iteration 51/1000 | Loss: 0.00001985
Iteration 52/1000 | Loss: 0.00001984
Iteration 53/1000 | Loss: 0.00001984
Iteration 54/1000 | Loss: 0.00001984
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00001982
Iteration 61/1000 | Loss: 0.00001982
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001980
Iteration 64/1000 | Loss: 0.00001980
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001980
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001979
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001978
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001977
Iteration 74/1000 | Loss: 0.00001977
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001976
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00001976
Iteration 80/1000 | Loss: 0.00001975
Iteration 81/1000 | Loss: 0.00001975
Iteration 82/1000 | Loss: 0.00001975
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001975
Iteration 85/1000 | Loss: 0.00001975
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001974
Iteration 90/1000 | Loss: 0.00001974
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001973
Iteration 95/1000 | Loss: 0.00001973
Iteration 96/1000 | Loss: 0.00001973
Iteration 97/1000 | Loss: 0.00001973
Iteration 98/1000 | Loss: 0.00001973
Iteration 99/1000 | Loss: 0.00001973
Iteration 100/1000 | Loss: 0.00001973
Iteration 101/1000 | Loss: 0.00001973
Iteration 102/1000 | Loss: 0.00001973
Iteration 103/1000 | Loss: 0.00001973
Iteration 104/1000 | Loss: 0.00001973
Iteration 105/1000 | Loss: 0.00001973
Iteration 106/1000 | Loss: 0.00001973
Iteration 107/1000 | Loss: 0.00001973
Iteration 108/1000 | Loss: 0.00001973
Iteration 109/1000 | Loss: 0.00001973
Iteration 110/1000 | Loss: 0.00001973
Iteration 111/1000 | Loss: 0.00001973
Iteration 112/1000 | Loss: 0.00001973
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.9733221051865257e-05, 1.9733221051865257e-05, 1.9733221051865257e-05, 1.9733221051865257e-05, 1.9733221051865257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9733221051865257e-05

Optimization complete. Final v2v error: 3.725539207458496 mm

Highest mean error: 4.537781238555908 mm for frame 67

Lowest mean error: 3.0751192569732666 mm for frame 121

Saving results

Total time: 44.17596936225891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406752
Iteration 2/25 | Loss: 0.00136526
Iteration 3/25 | Loss: 0.00130445
Iteration 4/25 | Loss: 0.00130100
Iteration 5/25 | Loss: 0.00130100
Iteration 6/25 | Loss: 0.00130100
Iteration 7/25 | Loss: 0.00130100
Iteration 8/25 | Loss: 0.00130100
Iteration 9/25 | Loss: 0.00130100
Iteration 10/25 | Loss: 0.00130100
Iteration 11/25 | Loss: 0.00130100
Iteration 12/25 | Loss: 0.00130100
Iteration 13/25 | Loss: 0.00130100
Iteration 14/25 | Loss: 0.00130100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013010023394599557, 0.0013010023394599557, 0.0013010023394599557, 0.0013010023394599557, 0.0013010023394599557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013010023394599557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66379356
Iteration 2/25 | Loss: 0.00078016
Iteration 3/25 | Loss: 0.00078015
Iteration 4/25 | Loss: 0.00078015
Iteration 5/25 | Loss: 0.00078015
Iteration 6/25 | Loss: 0.00078015
Iteration 7/25 | Loss: 0.00078015
Iteration 8/25 | Loss: 0.00078015
Iteration 9/25 | Loss: 0.00078015
Iteration 10/25 | Loss: 0.00078015
Iteration 11/25 | Loss: 0.00078015
Iteration 12/25 | Loss: 0.00078015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007801539031788707, 0.0007801539031788707, 0.0007801539031788707, 0.0007801539031788707, 0.0007801539031788707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007801539031788707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078015
Iteration 2/1000 | Loss: 0.00003160
Iteration 3/1000 | Loss: 0.00001884
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001573
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001410
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001357
Iteration 13/1000 | Loss: 0.00001353
Iteration 14/1000 | Loss: 0.00001341
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001334
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001324
Iteration 24/1000 | Loss: 0.00001324
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001319
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001284
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001280
Iteration 53/1000 | Loss: 0.00001280
Iteration 54/1000 | Loss: 0.00001279
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001278
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001276
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001274
Iteration 72/1000 | Loss: 0.00001274
Iteration 73/1000 | Loss: 0.00001274
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001270
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001269
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001266
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001265
Iteration 113/1000 | Loss: 0.00001265
Iteration 114/1000 | Loss: 0.00001265
Iteration 115/1000 | Loss: 0.00001265
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001264
Iteration 122/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.2642122783290688e-05, 1.2642122783290688e-05, 1.2642122783290688e-05, 1.2642122783290688e-05, 1.2642122783290688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2642122783290688e-05

Optimization complete. Final v2v error: 3.0288503170013428 mm

Highest mean error: 3.204787015914917 mm for frame 131

Lowest mean error: 2.879347801208496 mm for frame 43

Saving results

Total time: 43.55297613143921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586276
Iteration 2/25 | Loss: 0.00141865
Iteration 3/25 | Loss: 0.00134826
Iteration 4/25 | Loss: 0.00134201
Iteration 5/25 | Loss: 0.00134097
Iteration 6/25 | Loss: 0.00134097
Iteration 7/25 | Loss: 0.00134097
Iteration 8/25 | Loss: 0.00134097
Iteration 9/25 | Loss: 0.00134097
Iteration 10/25 | Loss: 0.00134097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013409708626568317, 0.0013409708626568317, 0.0013409708626568317, 0.0013409708626568317, 0.0013409708626568317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013409708626568317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.77317762
Iteration 2/25 | Loss: 0.00092196
Iteration 3/25 | Loss: 0.00092193
Iteration 4/25 | Loss: 0.00092193
Iteration 5/25 | Loss: 0.00092193
Iteration 6/25 | Loss: 0.00092193
Iteration 7/25 | Loss: 0.00092193
Iteration 8/25 | Loss: 0.00092193
Iteration 9/25 | Loss: 0.00092193
Iteration 10/25 | Loss: 0.00092193
Iteration 11/25 | Loss: 0.00092193
Iteration 12/25 | Loss: 0.00092193
Iteration 13/25 | Loss: 0.00092193
Iteration 14/25 | Loss: 0.00092193
Iteration 15/25 | Loss: 0.00092193
Iteration 16/25 | Loss: 0.00092193
Iteration 17/25 | Loss: 0.00092193
Iteration 18/25 | Loss: 0.00092193
Iteration 19/25 | Loss: 0.00092193
Iteration 20/25 | Loss: 0.00092193
Iteration 21/25 | Loss: 0.00092193
Iteration 22/25 | Loss: 0.00092193
Iteration 23/25 | Loss: 0.00092193
Iteration 24/25 | Loss: 0.00092193
Iteration 25/25 | Loss: 0.00092193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092193
Iteration 2/1000 | Loss: 0.00003065
Iteration 3/1000 | Loss: 0.00002220
Iteration 4/1000 | Loss: 0.00002038
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001908
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001804
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001789
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001773
Iteration 17/1000 | Loss: 0.00001771
Iteration 18/1000 | Loss: 0.00001764
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001755
Iteration 22/1000 | Loss: 0.00001754
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001748
Iteration 28/1000 | Loss: 0.00001743
Iteration 29/1000 | Loss: 0.00001742
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001735
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001733
Iteration 36/1000 | Loss: 0.00001733
Iteration 37/1000 | Loss: 0.00001731
Iteration 38/1000 | Loss: 0.00001731
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001731
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001731
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001725
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001720
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.7203448805958033e-05, 1.7203448805958033e-05, 1.7203448805958033e-05, 1.7203448805958033e-05, 1.7203448805958033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7203448805958033e-05

Optimization complete. Final v2v error: 3.504290819168091 mm

Highest mean error: 3.83573842048645 mm for frame 177

Lowest mean error: 3.2786669731140137 mm for frame 201

Saving results

Total time: 34.26616597175598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691464
Iteration 2/25 | Loss: 0.00159170
Iteration 3/25 | Loss: 0.00141023
Iteration 4/25 | Loss: 0.00132132
Iteration 5/25 | Loss: 0.00132937
Iteration 6/25 | Loss: 0.00130326
Iteration 7/25 | Loss: 0.00129906
Iteration 8/25 | Loss: 0.00129642
Iteration 9/25 | Loss: 0.00129573
Iteration 10/25 | Loss: 0.00129545
Iteration 11/25 | Loss: 0.00129534
Iteration 12/25 | Loss: 0.00129533
Iteration 13/25 | Loss: 0.00129531
Iteration 14/25 | Loss: 0.00129530
Iteration 15/25 | Loss: 0.00129530
Iteration 16/25 | Loss: 0.00129530
Iteration 17/25 | Loss: 0.00129530
Iteration 18/25 | Loss: 0.00129530
Iteration 19/25 | Loss: 0.00129530
Iteration 20/25 | Loss: 0.00129530
Iteration 21/25 | Loss: 0.00129530
Iteration 22/25 | Loss: 0.00129529
Iteration 23/25 | Loss: 0.00129529
Iteration 24/25 | Loss: 0.00129529
Iteration 25/25 | Loss: 0.00129529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71384180
Iteration 2/25 | Loss: 0.00102745
Iteration 3/25 | Loss: 0.00101022
Iteration 4/25 | Loss: 0.00101022
Iteration 5/25 | Loss: 0.00101022
Iteration 6/25 | Loss: 0.00101022
Iteration 7/25 | Loss: 0.00101022
Iteration 8/25 | Loss: 0.00101022
Iteration 9/25 | Loss: 0.00101022
Iteration 10/25 | Loss: 0.00101022
Iteration 11/25 | Loss: 0.00101022
Iteration 12/25 | Loss: 0.00101022
Iteration 13/25 | Loss: 0.00101022
Iteration 14/25 | Loss: 0.00101022
Iteration 15/25 | Loss: 0.00101022
Iteration 16/25 | Loss: 0.00101022
Iteration 17/25 | Loss: 0.00101022
Iteration 18/25 | Loss: 0.00101022
Iteration 19/25 | Loss: 0.00101022
Iteration 20/25 | Loss: 0.00101022
Iteration 21/25 | Loss: 0.00101022
Iteration 22/25 | Loss: 0.00101022
Iteration 23/25 | Loss: 0.00101022
Iteration 24/25 | Loss: 0.00101022
Iteration 25/25 | Loss: 0.00101022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101022
Iteration 2/1000 | Loss: 0.00003136
Iteration 3/1000 | Loss: 0.00005422
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001765
Iteration 6/1000 | Loss: 0.00001706
Iteration 7/1000 | Loss: 0.00004409
Iteration 8/1000 | Loss: 0.00001658
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001549
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00004715
Iteration 19/1000 | Loss: 0.00001527
Iteration 20/1000 | Loss: 0.00001526
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001518
Iteration 24/1000 | Loss: 0.00001518
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001516
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001515
Iteration 31/1000 | Loss: 0.00001513
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001505
Iteration 36/1000 | Loss: 0.00001505
Iteration 37/1000 | Loss: 0.00001505
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001500
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001498
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001490
Iteration 74/1000 | Loss: 0.00001489
Iteration 75/1000 | Loss: 0.00001489
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001488
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001488
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001488
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001488
Iteration 113/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.4879131413181312e-05, 1.4879131413181312e-05, 1.4879131413181312e-05, 1.4879131413181312e-05, 1.4879131413181312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4879131413181312e-05

Optimization complete. Final v2v error: 3.279113292694092 mm

Highest mean error: 3.8048763275146484 mm for frame 84

Lowest mean error: 2.9285404682159424 mm for frame 13

Saving results

Total time: 49.22722029685974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793433
Iteration 2/25 | Loss: 0.00149604
Iteration 3/25 | Loss: 0.00130873
Iteration 4/25 | Loss: 0.00129464
Iteration 5/25 | Loss: 0.00128947
Iteration 6/25 | Loss: 0.00128821
Iteration 7/25 | Loss: 0.00128821
Iteration 8/25 | Loss: 0.00128821
Iteration 9/25 | Loss: 0.00128821
Iteration 10/25 | Loss: 0.00128821
Iteration 11/25 | Loss: 0.00128821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001288213417865336, 0.001288213417865336, 0.001288213417865336, 0.001288213417865336, 0.001288213417865336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001288213417865336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20647693
Iteration 2/25 | Loss: 0.00095998
Iteration 3/25 | Loss: 0.00095997
Iteration 4/25 | Loss: 0.00095997
Iteration 5/25 | Loss: 0.00095997
Iteration 6/25 | Loss: 0.00095997
Iteration 7/25 | Loss: 0.00095997
Iteration 8/25 | Loss: 0.00095997
Iteration 9/25 | Loss: 0.00095997
Iteration 10/25 | Loss: 0.00095997
Iteration 11/25 | Loss: 0.00095997
Iteration 12/25 | Loss: 0.00095997
Iteration 13/25 | Loss: 0.00095997
Iteration 14/25 | Loss: 0.00095997
Iteration 15/25 | Loss: 0.00095997
Iteration 16/25 | Loss: 0.00095997
Iteration 17/25 | Loss: 0.00095997
Iteration 18/25 | Loss: 0.00095997
Iteration 19/25 | Loss: 0.00095997
Iteration 20/25 | Loss: 0.00095997
Iteration 21/25 | Loss: 0.00095997
Iteration 22/25 | Loss: 0.00095997
Iteration 23/25 | Loss: 0.00095997
Iteration 24/25 | Loss: 0.00095997
Iteration 25/25 | Loss: 0.00095997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095997
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00002746
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00001932
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001718
Iteration 10/1000 | Loss: 0.00001683
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001612
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001595
Iteration 17/1000 | Loss: 0.00001591
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001561
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001559
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001557
Iteration 27/1000 | Loss: 0.00001556
Iteration 28/1000 | Loss: 0.00001556
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001552
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001533
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001532
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001514
Iteration 133/1000 | Loss: 0.00001514
Iteration 134/1000 | Loss: 0.00001514
Iteration 135/1000 | Loss: 0.00001514
Iteration 136/1000 | Loss: 0.00001514
Iteration 137/1000 | Loss: 0.00001514
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001514
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001513
Iteration 149/1000 | Loss: 0.00001513
Iteration 150/1000 | Loss: 0.00001513
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001512
Iteration 156/1000 | Loss: 0.00001512
Iteration 157/1000 | Loss: 0.00001512
Iteration 158/1000 | Loss: 0.00001512
Iteration 159/1000 | Loss: 0.00001512
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001512
Iteration 162/1000 | Loss: 0.00001512
Iteration 163/1000 | Loss: 0.00001512
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001512
Iteration 167/1000 | Loss: 0.00001512
Iteration 168/1000 | Loss: 0.00001512
Iteration 169/1000 | Loss: 0.00001512
Iteration 170/1000 | Loss: 0.00001512
Iteration 171/1000 | Loss: 0.00001512
Iteration 172/1000 | Loss: 0.00001512
Iteration 173/1000 | Loss: 0.00001512
Iteration 174/1000 | Loss: 0.00001512
Iteration 175/1000 | Loss: 0.00001512
Iteration 176/1000 | Loss: 0.00001512
Iteration 177/1000 | Loss: 0.00001512
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001512
Iteration 180/1000 | Loss: 0.00001512
Iteration 181/1000 | Loss: 0.00001512
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001512
Iteration 184/1000 | Loss: 0.00001512
Iteration 185/1000 | Loss: 0.00001512
Iteration 186/1000 | Loss: 0.00001512
Iteration 187/1000 | Loss: 0.00001512
Iteration 188/1000 | Loss: 0.00001512
Iteration 189/1000 | Loss: 0.00001512
Iteration 190/1000 | Loss: 0.00001512
Iteration 191/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.5123079720069654e-05, 1.5123079720069654e-05, 1.5123079720069654e-05, 1.5123079720069654e-05, 1.5123079720069654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5123079720069654e-05

Optimization complete. Final v2v error: 3.2367684841156006 mm

Highest mean error: 4.473511695861816 mm for frame 62

Lowest mean error: 2.791633129119873 mm for frame 93

Saving results

Total time: 45.203102827072144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578972
Iteration 2/25 | Loss: 0.00155712
Iteration 3/25 | Loss: 0.00140943
Iteration 4/25 | Loss: 0.00137883
Iteration 5/25 | Loss: 0.00136954
Iteration 6/25 | Loss: 0.00136729
Iteration 7/25 | Loss: 0.00136637
Iteration 8/25 | Loss: 0.00136590
Iteration 9/25 | Loss: 0.00137166
Iteration 10/25 | Loss: 0.00136709
Iteration 11/25 | Loss: 0.00136583
Iteration 12/25 | Loss: 0.00136549
Iteration 13/25 | Loss: 0.00136460
Iteration 14/25 | Loss: 0.00136423
Iteration 15/25 | Loss: 0.00136404
Iteration 16/25 | Loss: 0.00136392
Iteration 17/25 | Loss: 0.00136374
Iteration 18/25 | Loss: 0.00136795
Iteration 19/25 | Loss: 0.00136401
Iteration 20/25 | Loss: 0.00136759
Iteration 21/25 | Loss: 0.00136546
Iteration 22/25 | Loss: 0.00136378
Iteration 23/25 | Loss: 0.00136247
Iteration 24/25 | Loss: 0.00136142
Iteration 25/25 | Loss: 0.00136088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20518017
Iteration 2/25 | Loss: 0.00087286
Iteration 3/25 | Loss: 0.00087280
Iteration 4/25 | Loss: 0.00087280
Iteration 5/25 | Loss: 0.00087279
Iteration 6/25 | Loss: 0.00087279
Iteration 7/25 | Loss: 0.00087279
Iteration 8/25 | Loss: 0.00087279
Iteration 9/25 | Loss: 0.00087279
Iteration 10/25 | Loss: 0.00087279
Iteration 11/25 | Loss: 0.00087279
Iteration 12/25 | Loss: 0.00087279
Iteration 13/25 | Loss: 0.00087279
Iteration 14/25 | Loss: 0.00087279
Iteration 15/25 | Loss: 0.00087279
Iteration 16/25 | Loss: 0.00087279
Iteration 17/25 | Loss: 0.00087279
Iteration 18/25 | Loss: 0.00087279
Iteration 19/25 | Loss: 0.00087279
Iteration 20/25 | Loss: 0.00087279
Iteration 21/25 | Loss: 0.00087279
Iteration 22/25 | Loss: 0.00087279
Iteration 23/25 | Loss: 0.00087279
Iteration 24/25 | Loss: 0.00087279
Iteration 25/25 | Loss: 0.00087279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087279
Iteration 2/1000 | Loss: 0.00005456
Iteration 3/1000 | Loss: 0.00003575
Iteration 4/1000 | Loss: 0.00003133
Iteration 5/1000 | Loss: 0.00002970
Iteration 6/1000 | Loss: 0.00002887
Iteration 7/1000 | Loss: 0.00002825
Iteration 8/1000 | Loss: 0.00002766
Iteration 9/1000 | Loss: 0.00002715
Iteration 10/1000 | Loss: 0.00002684
Iteration 11/1000 | Loss: 0.00002651
Iteration 12/1000 | Loss: 0.00002627
Iteration 13/1000 | Loss: 0.00002606
Iteration 14/1000 | Loss: 0.00002602
Iteration 15/1000 | Loss: 0.00002589
Iteration 16/1000 | Loss: 0.00002571
Iteration 17/1000 | Loss: 0.00002556
Iteration 18/1000 | Loss: 0.00002552
Iteration 19/1000 | Loss: 0.00002551
Iteration 20/1000 | Loss: 0.00002550
Iteration 21/1000 | Loss: 0.00002544
Iteration 22/1000 | Loss: 0.00002544
Iteration 23/1000 | Loss: 0.00002539
Iteration 24/1000 | Loss: 0.00002536
Iteration 25/1000 | Loss: 0.00002536
Iteration 26/1000 | Loss: 0.00002534
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002530
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002530
Iteration 31/1000 | Loss: 0.00002530
Iteration 32/1000 | Loss: 0.00002530
Iteration 33/1000 | Loss: 0.00002529
Iteration 34/1000 | Loss: 0.00002529
Iteration 35/1000 | Loss: 0.00002529
Iteration 36/1000 | Loss: 0.00002527
Iteration 37/1000 | Loss: 0.00002527
Iteration 38/1000 | Loss: 0.00002526
Iteration 39/1000 | Loss: 0.00002525
Iteration 40/1000 | Loss: 0.00002524
Iteration 41/1000 | Loss: 0.00002524
Iteration 42/1000 | Loss: 0.00002523
Iteration 43/1000 | Loss: 0.00002523
Iteration 44/1000 | Loss: 0.00002522
Iteration 45/1000 | Loss: 0.00002520
Iteration 46/1000 | Loss: 0.00002520
Iteration 47/1000 | Loss: 0.00002520
Iteration 48/1000 | Loss: 0.00002519
Iteration 49/1000 | Loss: 0.00002518
Iteration 50/1000 | Loss: 0.00002518
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002517
Iteration 54/1000 | Loss: 0.00002516
Iteration 55/1000 | Loss: 0.00002515
Iteration 56/1000 | Loss: 0.00002515
Iteration 57/1000 | Loss: 0.00002514
Iteration 58/1000 | Loss: 0.00002513
Iteration 59/1000 | Loss: 0.00002513
Iteration 60/1000 | Loss: 0.00002513
Iteration 61/1000 | Loss: 0.00002513
Iteration 62/1000 | Loss: 0.00002512
Iteration 63/1000 | Loss: 0.00002512
Iteration 64/1000 | Loss: 0.00002511
Iteration 65/1000 | Loss: 0.00002511
Iteration 66/1000 | Loss: 0.00002511
Iteration 67/1000 | Loss: 0.00002511
Iteration 68/1000 | Loss: 0.00002511
Iteration 69/1000 | Loss: 0.00002510
Iteration 70/1000 | Loss: 0.00002510
Iteration 71/1000 | Loss: 0.00002510
Iteration 72/1000 | Loss: 0.00002510
Iteration 73/1000 | Loss: 0.00002510
Iteration 74/1000 | Loss: 0.00002510
Iteration 75/1000 | Loss: 0.00002510
Iteration 76/1000 | Loss: 0.00002510
Iteration 77/1000 | Loss: 0.00002509
Iteration 78/1000 | Loss: 0.00002509
Iteration 79/1000 | Loss: 0.00002509
Iteration 80/1000 | Loss: 0.00002509
Iteration 81/1000 | Loss: 0.00002509
Iteration 82/1000 | Loss: 0.00002508
Iteration 83/1000 | Loss: 0.00002508
Iteration 84/1000 | Loss: 0.00002508
Iteration 85/1000 | Loss: 0.00002508
Iteration 86/1000 | Loss: 0.00002507
Iteration 87/1000 | Loss: 0.00002507
Iteration 88/1000 | Loss: 0.00002507
Iteration 89/1000 | Loss: 0.00002507
Iteration 90/1000 | Loss: 0.00002507
Iteration 91/1000 | Loss: 0.00002507
Iteration 92/1000 | Loss: 0.00002507
Iteration 93/1000 | Loss: 0.00002507
Iteration 94/1000 | Loss: 0.00002507
Iteration 95/1000 | Loss: 0.00002506
Iteration 96/1000 | Loss: 0.00002506
Iteration 97/1000 | Loss: 0.00002506
Iteration 98/1000 | Loss: 0.00002506
Iteration 99/1000 | Loss: 0.00002506
Iteration 100/1000 | Loss: 0.00002506
Iteration 101/1000 | Loss: 0.00002506
Iteration 102/1000 | Loss: 0.00002506
Iteration 103/1000 | Loss: 0.00002506
Iteration 104/1000 | Loss: 0.00002506
Iteration 105/1000 | Loss: 0.00002506
Iteration 106/1000 | Loss: 0.00002506
Iteration 107/1000 | Loss: 0.00002506
Iteration 108/1000 | Loss: 0.00002506
Iteration 109/1000 | Loss: 0.00002506
Iteration 110/1000 | Loss: 0.00002506
Iteration 111/1000 | Loss: 0.00002506
Iteration 112/1000 | Loss: 0.00002506
Iteration 113/1000 | Loss: 0.00002506
Iteration 114/1000 | Loss: 0.00002506
Iteration 115/1000 | Loss: 0.00002506
Iteration 116/1000 | Loss: 0.00002506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.5060340703930706e-05, 2.5060340703930706e-05, 2.5060340703930706e-05, 2.5060340703930706e-05, 2.5060340703930706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5060340703930706e-05

Optimization complete. Final v2v error: 4.092257499694824 mm

Highest mean error: 6.041772842407227 mm for frame 114

Lowest mean error: 3.425813674926758 mm for frame 0

Saving results

Total time: 78.91292786598206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841902
Iteration 2/25 | Loss: 0.00169416
Iteration 3/25 | Loss: 0.00141551
Iteration 4/25 | Loss: 0.00137498
Iteration 5/25 | Loss: 0.00136909
Iteration 6/25 | Loss: 0.00136843
Iteration 7/25 | Loss: 0.00136843
Iteration 8/25 | Loss: 0.00136843
Iteration 9/25 | Loss: 0.00136843
Iteration 10/25 | Loss: 0.00136843
Iteration 11/25 | Loss: 0.00136843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013684318400919437, 0.0013684318400919437, 0.0013684318400919437, 0.0013684318400919437, 0.0013684318400919437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013684318400919437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29607129
Iteration 2/25 | Loss: 0.00065739
Iteration 3/25 | Loss: 0.00065735
Iteration 4/25 | Loss: 0.00065735
Iteration 5/25 | Loss: 0.00065735
Iteration 6/25 | Loss: 0.00065735
Iteration 7/25 | Loss: 0.00065735
Iteration 8/25 | Loss: 0.00065735
Iteration 9/25 | Loss: 0.00065735
Iteration 10/25 | Loss: 0.00065735
Iteration 11/25 | Loss: 0.00065735
Iteration 12/25 | Loss: 0.00065735
Iteration 13/25 | Loss: 0.00065735
Iteration 14/25 | Loss: 0.00065735
Iteration 15/25 | Loss: 0.00065735
Iteration 16/25 | Loss: 0.00065735
Iteration 17/25 | Loss: 0.00065735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006573484279215336, 0.0006573484279215336, 0.0006573484279215336, 0.0006573484279215336, 0.0006573484279215336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006573484279215336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065735
Iteration 2/1000 | Loss: 0.00006406
Iteration 3/1000 | Loss: 0.00004684
Iteration 4/1000 | Loss: 0.00003989
Iteration 5/1000 | Loss: 0.00003668
Iteration 6/1000 | Loss: 0.00003501
Iteration 7/1000 | Loss: 0.00003375
Iteration 8/1000 | Loss: 0.00003305
Iteration 9/1000 | Loss: 0.00003259
Iteration 10/1000 | Loss: 0.00003221
Iteration 11/1000 | Loss: 0.00003181
Iteration 12/1000 | Loss: 0.00003144
Iteration 13/1000 | Loss: 0.00003111
Iteration 14/1000 | Loss: 0.00003086
Iteration 15/1000 | Loss: 0.00003066
Iteration 16/1000 | Loss: 0.00003051
Iteration 17/1000 | Loss: 0.00003038
Iteration 18/1000 | Loss: 0.00003030
Iteration 19/1000 | Loss: 0.00003029
Iteration 20/1000 | Loss: 0.00003028
Iteration 21/1000 | Loss: 0.00003021
Iteration 22/1000 | Loss: 0.00003020
Iteration 23/1000 | Loss: 0.00003020
Iteration 24/1000 | Loss: 0.00003019
Iteration 25/1000 | Loss: 0.00003016
Iteration 26/1000 | Loss: 0.00003014
Iteration 27/1000 | Loss: 0.00003013
Iteration 28/1000 | Loss: 0.00003013
Iteration 29/1000 | Loss: 0.00003012
Iteration 30/1000 | Loss: 0.00003012
Iteration 31/1000 | Loss: 0.00003010
Iteration 32/1000 | Loss: 0.00003009
Iteration 33/1000 | Loss: 0.00003008
Iteration 34/1000 | Loss: 0.00003007
Iteration 35/1000 | Loss: 0.00003007
Iteration 36/1000 | Loss: 0.00003006
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00003004
Iteration 39/1000 | Loss: 0.00003003
Iteration 40/1000 | Loss: 0.00003003
Iteration 41/1000 | Loss: 0.00003003
Iteration 42/1000 | Loss: 0.00003002
Iteration 43/1000 | Loss: 0.00003002
Iteration 44/1000 | Loss: 0.00003001
Iteration 45/1000 | Loss: 0.00003001
Iteration 46/1000 | Loss: 0.00002999
Iteration 47/1000 | Loss: 0.00002998
Iteration 48/1000 | Loss: 0.00002998
Iteration 49/1000 | Loss: 0.00002997
Iteration 50/1000 | Loss: 0.00002997
Iteration 51/1000 | Loss: 0.00002997
Iteration 52/1000 | Loss: 0.00002994
Iteration 53/1000 | Loss: 0.00002994
Iteration 54/1000 | Loss: 0.00002994
Iteration 55/1000 | Loss: 0.00002994
Iteration 56/1000 | Loss: 0.00002994
Iteration 57/1000 | Loss: 0.00002993
Iteration 58/1000 | Loss: 0.00002993
Iteration 59/1000 | Loss: 0.00002993
Iteration 60/1000 | Loss: 0.00002993
Iteration 61/1000 | Loss: 0.00002993
Iteration 62/1000 | Loss: 0.00002993
Iteration 63/1000 | Loss: 0.00002993
Iteration 64/1000 | Loss: 0.00002992
Iteration 65/1000 | Loss: 0.00002989
Iteration 66/1000 | Loss: 0.00002989
Iteration 67/1000 | Loss: 0.00002988
Iteration 68/1000 | Loss: 0.00002988
Iteration 69/1000 | Loss: 0.00002987
Iteration 70/1000 | Loss: 0.00002986
Iteration 71/1000 | Loss: 0.00002985
Iteration 72/1000 | Loss: 0.00002985
Iteration 73/1000 | Loss: 0.00002984
Iteration 74/1000 | Loss: 0.00002984
Iteration 75/1000 | Loss: 0.00002984
Iteration 76/1000 | Loss: 0.00002984
Iteration 77/1000 | Loss: 0.00002984
Iteration 78/1000 | Loss: 0.00002984
Iteration 79/1000 | Loss: 0.00002984
Iteration 80/1000 | Loss: 0.00002984
Iteration 81/1000 | Loss: 0.00002984
Iteration 82/1000 | Loss: 0.00002984
Iteration 83/1000 | Loss: 0.00002983
Iteration 84/1000 | Loss: 0.00002982
Iteration 85/1000 | Loss: 0.00002982
Iteration 86/1000 | Loss: 0.00002982
Iteration 87/1000 | Loss: 0.00002981
Iteration 88/1000 | Loss: 0.00002981
Iteration 89/1000 | Loss: 0.00002981
Iteration 90/1000 | Loss: 0.00002981
Iteration 91/1000 | Loss: 0.00002981
Iteration 92/1000 | Loss: 0.00002981
Iteration 93/1000 | Loss: 0.00002981
Iteration 94/1000 | Loss: 0.00002981
Iteration 95/1000 | Loss: 0.00002980
Iteration 96/1000 | Loss: 0.00002980
Iteration 97/1000 | Loss: 0.00002979
Iteration 98/1000 | Loss: 0.00002979
Iteration 99/1000 | Loss: 0.00002979
Iteration 100/1000 | Loss: 0.00002978
Iteration 101/1000 | Loss: 0.00002978
Iteration 102/1000 | Loss: 0.00002978
Iteration 103/1000 | Loss: 0.00002978
Iteration 104/1000 | Loss: 0.00002978
Iteration 105/1000 | Loss: 0.00002978
Iteration 106/1000 | Loss: 0.00002978
Iteration 107/1000 | Loss: 0.00002978
Iteration 108/1000 | Loss: 0.00002977
Iteration 109/1000 | Loss: 0.00002977
Iteration 110/1000 | Loss: 0.00002977
Iteration 111/1000 | Loss: 0.00002977
Iteration 112/1000 | Loss: 0.00002977
Iteration 113/1000 | Loss: 0.00002977
Iteration 114/1000 | Loss: 0.00002976
Iteration 115/1000 | Loss: 0.00002976
Iteration 116/1000 | Loss: 0.00002975
Iteration 117/1000 | Loss: 0.00002975
Iteration 118/1000 | Loss: 0.00002975
Iteration 119/1000 | Loss: 0.00002975
Iteration 120/1000 | Loss: 0.00002975
Iteration 121/1000 | Loss: 0.00002975
Iteration 122/1000 | Loss: 0.00002975
Iteration 123/1000 | Loss: 0.00002975
Iteration 124/1000 | Loss: 0.00002975
Iteration 125/1000 | Loss: 0.00002975
Iteration 126/1000 | Loss: 0.00002975
Iteration 127/1000 | Loss: 0.00002975
Iteration 128/1000 | Loss: 0.00002975
Iteration 129/1000 | Loss: 0.00002975
Iteration 130/1000 | Loss: 0.00002975
Iteration 131/1000 | Loss: 0.00002975
Iteration 132/1000 | Loss: 0.00002975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.9749911846010946e-05, 2.9749911846010946e-05, 2.9749911846010946e-05, 2.9749911846010946e-05, 2.9749911846010946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9749911846010946e-05

Optimization complete. Final v2v error: 4.513670921325684 mm

Highest mean error: 4.838522911071777 mm for frame 3

Lowest mean error: 4.316789627075195 mm for frame 136

Saving results

Total time: 43.38610076904297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961674
Iteration 2/25 | Loss: 0.00186183
Iteration 3/25 | Loss: 0.00154417
Iteration 4/25 | Loss: 0.00150830
Iteration 5/25 | Loss: 0.00150204
Iteration 6/25 | Loss: 0.00150151
Iteration 7/25 | Loss: 0.00150151
Iteration 8/25 | Loss: 0.00150151
Iteration 9/25 | Loss: 0.00150151
Iteration 10/25 | Loss: 0.00150151
Iteration 11/25 | Loss: 0.00150151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015015080571174622, 0.0015015080571174622, 0.0015015080571174622, 0.0015015080571174622, 0.0015015080571174622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015015080571174622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05313289
Iteration 2/25 | Loss: 0.00112604
Iteration 3/25 | Loss: 0.00112602
Iteration 4/25 | Loss: 0.00112602
Iteration 5/25 | Loss: 0.00112602
Iteration 6/25 | Loss: 0.00112602
Iteration 7/25 | Loss: 0.00112602
Iteration 8/25 | Loss: 0.00112602
Iteration 9/25 | Loss: 0.00112602
Iteration 10/25 | Loss: 0.00112602
Iteration 11/25 | Loss: 0.00112602
Iteration 12/25 | Loss: 0.00112602
Iteration 13/25 | Loss: 0.00112602
Iteration 14/25 | Loss: 0.00112602
Iteration 15/25 | Loss: 0.00112602
Iteration 16/25 | Loss: 0.00112602
Iteration 17/25 | Loss: 0.00112602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011260175378993154, 0.0011260175378993154, 0.0011260175378993154, 0.0011260175378993154, 0.0011260175378993154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011260175378993154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112602
Iteration 2/1000 | Loss: 0.00006577
Iteration 3/1000 | Loss: 0.00004424
Iteration 4/1000 | Loss: 0.00003779
Iteration 5/1000 | Loss: 0.00003511
Iteration 6/1000 | Loss: 0.00003380
Iteration 7/1000 | Loss: 0.00003284
Iteration 8/1000 | Loss: 0.00003193
Iteration 9/1000 | Loss: 0.00003137
Iteration 10/1000 | Loss: 0.00003096
Iteration 11/1000 | Loss: 0.00003064
Iteration 12/1000 | Loss: 0.00003039
Iteration 13/1000 | Loss: 0.00003014
Iteration 14/1000 | Loss: 0.00003010
Iteration 15/1000 | Loss: 0.00002988
Iteration 16/1000 | Loss: 0.00002970
Iteration 17/1000 | Loss: 0.00002961
Iteration 18/1000 | Loss: 0.00002956
Iteration 19/1000 | Loss: 0.00002950
Iteration 20/1000 | Loss: 0.00002947
Iteration 21/1000 | Loss: 0.00002943
Iteration 22/1000 | Loss: 0.00002942
Iteration 23/1000 | Loss: 0.00002941
Iteration 24/1000 | Loss: 0.00002940
Iteration 25/1000 | Loss: 0.00002940
Iteration 26/1000 | Loss: 0.00002936
Iteration 27/1000 | Loss: 0.00002936
Iteration 28/1000 | Loss: 0.00002934
Iteration 29/1000 | Loss: 0.00002934
Iteration 30/1000 | Loss: 0.00002933
Iteration 31/1000 | Loss: 0.00002933
Iteration 32/1000 | Loss: 0.00002933
Iteration 33/1000 | Loss: 0.00002932
Iteration 34/1000 | Loss: 0.00002932
Iteration 35/1000 | Loss: 0.00002932
Iteration 36/1000 | Loss: 0.00002932
Iteration 37/1000 | Loss: 0.00002932
Iteration 38/1000 | Loss: 0.00002932
Iteration 39/1000 | Loss: 0.00002932
Iteration 40/1000 | Loss: 0.00002932
Iteration 41/1000 | Loss: 0.00002932
Iteration 42/1000 | Loss: 0.00002932
Iteration 43/1000 | Loss: 0.00002932
Iteration 44/1000 | Loss: 0.00002932
Iteration 45/1000 | Loss: 0.00002932
Iteration 46/1000 | Loss: 0.00002932
Iteration 47/1000 | Loss: 0.00002932
Iteration 48/1000 | Loss: 0.00002932
Iteration 49/1000 | Loss: 0.00002932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [2.9322360205696896e-05, 2.9322360205696896e-05, 2.9322360205696896e-05, 2.9322360205696896e-05, 2.9322360205696896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9322360205696896e-05

Optimization complete. Final v2v error: 4.414048194885254 mm

Highest mean error: 5.112960338592529 mm for frame 81

Lowest mean error: 3.6133034229278564 mm for frame 26

Saving results

Total time: 38.26264142990112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879972
Iteration 2/25 | Loss: 0.00879972
Iteration 3/25 | Loss: 0.00879972
Iteration 4/25 | Loss: 0.00879972
Iteration 5/25 | Loss: 0.00879972
Iteration 6/25 | Loss: 0.00879972
Iteration 7/25 | Loss: 0.00879972
Iteration 8/25 | Loss: 0.00879972
Iteration 9/25 | Loss: 0.00879971
Iteration 10/25 | Loss: 0.00879971
Iteration 11/25 | Loss: 0.00879971
Iteration 12/25 | Loss: 0.00879971
Iteration 13/25 | Loss: 0.00879971
Iteration 14/25 | Loss: 0.00879971
Iteration 15/25 | Loss: 0.00879971
Iteration 16/25 | Loss: 0.00879970
Iteration 17/25 | Loss: 0.00879970
Iteration 18/25 | Loss: 0.00879970
Iteration 19/25 | Loss: 0.00879970
Iteration 20/25 | Loss: 0.00879970
Iteration 21/25 | Loss: 0.00879970
Iteration 22/25 | Loss: 0.00879970
Iteration 23/25 | Loss: 0.00879970
Iteration 24/25 | Loss: 0.00879969
Iteration 25/25 | Loss: 0.00879969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47880101
Iteration 2/25 | Loss: 0.08591877
Iteration 3/25 | Loss: 0.08510458
Iteration 4/25 | Loss: 0.08498877
Iteration 5/25 | Loss: 0.08490598
Iteration 6/25 | Loss: 0.08490598
Iteration 7/25 | Loss: 0.08490597
Iteration 8/25 | Loss: 0.08486677
Iteration 9/25 | Loss: 0.08486675
Iteration 10/25 | Loss: 0.08486674
Iteration 11/25 | Loss: 0.08486672
Iteration 12/25 | Loss: 0.08486672
Iteration 13/25 | Loss: 0.08486672
Iteration 14/25 | Loss: 0.08486672
Iteration 15/25 | Loss: 0.08486672
Iteration 16/25 | Loss: 0.08486672
Iteration 17/25 | Loss: 0.08486672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0848667174577713, 0.0848667174577713, 0.0848667174577713, 0.0848667174577713, 0.0848667174577713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0848667174577713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08486672
Iteration 2/1000 | Loss: 0.00281712
Iteration 3/1000 | Loss: 0.00222141
Iteration 4/1000 | Loss: 0.00192006
Iteration 5/1000 | Loss: 0.00459053
Iteration 6/1000 | Loss: 0.00153812
Iteration 7/1000 | Loss: 0.00504491
Iteration 8/1000 | Loss: 0.00220469
Iteration 9/1000 | Loss: 0.00061166
Iteration 10/1000 | Loss: 0.00046111
Iteration 11/1000 | Loss: 0.00037050
Iteration 12/1000 | Loss: 0.00035376
Iteration 13/1000 | Loss: 0.00039073
Iteration 14/1000 | Loss: 0.00038090
Iteration 15/1000 | Loss: 0.00025706
Iteration 16/1000 | Loss: 0.00028054
Iteration 17/1000 | Loss: 0.00042049
Iteration 18/1000 | Loss: 0.00058331
Iteration 19/1000 | Loss: 0.00106706
Iteration 20/1000 | Loss: 0.00037423
Iteration 21/1000 | Loss: 0.00025597
Iteration 22/1000 | Loss: 0.00024117
Iteration 23/1000 | Loss: 0.00023013
Iteration 24/1000 | Loss: 0.00030442
Iteration 25/1000 | Loss: 0.00022335
Iteration 26/1000 | Loss: 0.00023418
Iteration 27/1000 | Loss: 0.00021579
Iteration 28/1000 | Loss: 0.00028895
Iteration 29/1000 | Loss: 0.00020874
Iteration 30/1000 | Loss: 0.00020403
Iteration 31/1000 | Loss: 0.00027386
Iteration 32/1000 | Loss: 0.00024689
Iteration 33/1000 | Loss: 0.00037820
Iteration 34/1000 | Loss: 0.00028237
Iteration 35/1000 | Loss: 0.00019667
Iteration 36/1000 | Loss: 0.00018917
Iteration 37/1000 | Loss: 0.00023399
Iteration 38/1000 | Loss: 0.00018231
Iteration 39/1000 | Loss: 0.00017945
Iteration 40/1000 | Loss: 0.00017729
Iteration 41/1000 | Loss: 0.00025223
Iteration 42/1000 | Loss: 0.00017712
Iteration 43/1000 | Loss: 0.00017363
Iteration 44/1000 | Loss: 0.00017288
Iteration 45/1000 | Loss: 0.00017196
Iteration 46/1000 | Loss: 0.00020552
Iteration 47/1000 | Loss: 0.00049032
Iteration 48/1000 | Loss: 0.00017057
Iteration 49/1000 | Loss: 0.00016976
Iteration 50/1000 | Loss: 0.00016893
Iteration 51/1000 | Loss: 0.00017259
Iteration 52/1000 | Loss: 0.00016820
Iteration 53/1000 | Loss: 0.00016727
Iteration 54/1000 | Loss: 0.00016774
Iteration 55/1000 | Loss: 0.00022911
Iteration 56/1000 | Loss: 0.00019446
Iteration 57/1000 | Loss: 0.00017472
Iteration 58/1000 | Loss: 0.00152071
Iteration 59/1000 | Loss: 0.00205514
Iteration 60/1000 | Loss: 0.00019054
Iteration 61/1000 | Loss: 0.00024242
Iteration 62/1000 | Loss: 0.00020741
Iteration 63/1000 | Loss: 0.00023865
Iteration 64/1000 | Loss: 0.00019819
Iteration 65/1000 | Loss: 0.00021737
Iteration 66/1000 | Loss: 0.00018767
Iteration 67/1000 | Loss: 0.00017465
Iteration 68/1000 | Loss: 0.00017096
Iteration 69/1000 | Loss: 0.00047696
Iteration 70/1000 | Loss: 0.00028871
Iteration 71/1000 | Loss: 0.00024626
Iteration 72/1000 | Loss: 0.00020138
Iteration 73/1000 | Loss: 0.00016867
Iteration 74/1000 | Loss: 0.00016611
Iteration 75/1000 | Loss: 0.00144786
Iteration 76/1000 | Loss: 0.00068968
Iteration 77/1000 | Loss: 0.00125824
Iteration 78/1000 | Loss: 0.00017917
Iteration 79/1000 | Loss: 0.00172446
Iteration 80/1000 | Loss: 0.00025597
Iteration 81/1000 | Loss: 0.00055304
Iteration 82/1000 | Loss: 0.00023918
Iteration 83/1000 | Loss: 0.00020637
Iteration 84/1000 | Loss: 0.00016695
Iteration 85/1000 | Loss: 0.00096790
Iteration 86/1000 | Loss: 0.00037089
Iteration 87/1000 | Loss: 0.00021331
Iteration 88/1000 | Loss: 0.00052534
Iteration 89/1000 | Loss: 0.00018905
Iteration 90/1000 | Loss: 0.00017193
Iteration 91/1000 | Loss: 0.00016852
Iteration 92/1000 | Loss: 0.00026630
Iteration 93/1000 | Loss: 0.00016838
Iteration 94/1000 | Loss: 0.00016572
Iteration 95/1000 | Loss: 0.00031124
Iteration 96/1000 | Loss: 0.00020348
Iteration 97/1000 | Loss: 0.00016466
Iteration 98/1000 | Loss: 0.00103892
Iteration 99/1000 | Loss: 0.00102956
Iteration 100/1000 | Loss: 0.00029578
Iteration 101/1000 | Loss: 0.00027466
Iteration 102/1000 | Loss: 0.00017783
Iteration 103/1000 | Loss: 0.00016767
Iteration 104/1000 | Loss: 0.00016537
Iteration 105/1000 | Loss: 0.00123734
Iteration 106/1000 | Loss: 0.00157686
Iteration 107/1000 | Loss: 0.00021722
Iteration 108/1000 | Loss: 0.00018455
Iteration 109/1000 | Loss: 0.00018461
Iteration 110/1000 | Loss: 0.00017607
Iteration 111/1000 | Loss: 0.00056601
Iteration 112/1000 | Loss: 0.00110795
Iteration 113/1000 | Loss: 0.00064424
Iteration 114/1000 | Loss: 0.00016697
Iteration 115/1000 | Loss: 0.00016447
Iteration 116/1000 | Loss: 0.00142989
Iteration 117/1000 | Loss: 0.00040663
Iteration 118/1000 | Loss: 0.00016918
Iteration 119/1000 | Loss: 0.00130661
Iteration 120/1000 | Loss: 0.00198819
Iteration 121/1000 | Loss: 0.00213896
Iteration 122/1000 | Loss: 0.00150370
Iteration 123/1000 | Loss: 0.00194398
Iteration 124/1000 | Loss: 0.00156148
Iteration 125/1000 | Loss: 0.00216733
Iteration 126/1000 | Loss: 0.00166594
Iteration 127/1000 | Loss: 0.00240245
Iteration 128/1000 | Loss: 0.00157025
Iteration 129/1000 | Loss: 0.00205309
Iteration 130/1000 | Loss: 0.00248801
Iteration 131/1000 | Loss: 0.00170467
Iteration 132/1000 | Loss: 0.00231679
Iteration 133/1000 | Loss: 0.00158276
Iteration 134/1000 | Loss: 0.00123549
Iteration 135/1000 | Loss: 0.00142756
Iteration 136/1000 | Loss: 0.00170460
Iteration 137/1000 | Loss: 0.00241288
Iteration 138/1000 | Loss: 0.00134989
Iteration 139/1000 | Loss: 0.00146404
Iteration 140/1000 | Loss: 0.00132414
Iteration 141/1000 | Loss: 0.00137537
Iteration 142/1000 | Loss: 0.00119483
Iteration 143/1000 | Loss: 0.00111302
Iteration 144/1000 | Loss: 0.00112429
Iteration 145/1000 | Loss: 0.00164157
Iteration 146/1000 | Loss: 0.00106520
Iteration 147/1000 | Loss: 0.00138705
Iteration 148/1000 | Loss: 0.00105409
Iteration 149/1000 | Loss: 0.00345139
Iteration 150/1000 | Loss: 0.00118404
Iteration 151/1000 | Loss: 0.00124777
Iteration 152/1000 | Loss: 0.00101727
Iteration 153/1000 | Loss: 0.00107059
Iteration 154/1000 | Loss: 0.00100169
Iteration 155/1000 | Loss: 0.00106311
Iteration 156/1000 | Loss: 0.00130034
Iteration 157/1000 | Loss: 0.00159581
Iteration 158/1000 | Loss: 0.00105174
Iteration 159/1000 | Loss: 0.00103949
Iteration 160/1000 | Loss: 0.00098894
Iteration 161/1000 | Loss: 0.00100017
Iteration 162/1000 | Loss: 0.00150413
Iteration 163/1000 | Loss: 0.00098787
Iteration 164/1000 | Loss: 0.00122210
Iteration 165/1000 | Loss: 0.00107901
Iteration 166/1000 | Loss: 0.00127716
Iteration 167/1000 | Loss: 0.00105982
Iteration 168/1000 | Loss: 0.00117173
Iteration 169/1000 | Loss: 0.00112456
Iteration 170/1000 | Loss: 0.00114792
Iteration 171/1000 | Loss: 0.00109522
Iteration 172/1000 | Loss: 0.00094971
Iteration 173/1000 | Loss: 0.00100313
Iteration 174/1000 | Loss: 0.00095016
Iteration 175/1000 | Loss: 0.00092067
Iteration 176/1000 | Loss: 0.00097573
Iteration 177/1000 | Loss: 0.00095503
Iteration 178/1000 | Loss: 0.00098894
Iteration 179/1000 | Loss: 0.00086801
Iteration 180/1000 | Loss: 0.00092396
Iteration 181/1000 | Loss: 0.00095170
Iteration 182/1000 | Loss: 0.00088681
Iteration 183/1000 | Loss: 0.00094862
Iteration 184/1000 | Loss: 0.00089180
Iteration 185/1000 | Loss: 0.00096692
Iteration 186/1000 | Loss: 0.00088089
Iteration 187/1000 | Loss: 0.00096015
Iteration 188/1000 | Loss: 0.00111452
Iteration 189/1000 | Loss: 0.00087895
Iteration 190/1000 | Loss: 0.00085591
Iteration 191/1000 | Loss: 0.00085584
Iteration 192/1000 | Loss: 0.00189780
Iteration 193/1000 | Loss: 0.00171573
Iteration 194/1000 | Loss: 0.00155981
Iteration 195/1000 | Loss: 0.00169207
Iteration 196/1000 | Loss: 0.00167261
Iteration 197/1000 | Loss: 0.00171270
Iteration 198/1000 | Loss: 0.00168229
Iteration 199/1000 | Loss: 0.00177643
Iteration 200/1000 | Loss: 0.00156456
Iteration 201/1000 | Loss: 0.00092758
Iteration 202/1000 | Loss: 0.00087050
Iteration 203/1000 | Loss: 0.00119165
Iteration 204/1000 | Loss: 0.00135693
Iteration 205/1000 | Loss: 0.00091794
Iteration 206/1000 | Loss: 0.00099691
Iteration 207/1000 | Loss: 0.00205566
Iteration 208/1000 | Loss: 0.00089058
Iteration 209/1000 | Loss: 0.00159815
Iteration 210/1000 | Loss: 0.00087905
Iteration 211/1000 | Loss: 0.00122137
Iteration 212/1000 | Loss: 0.00094254
Iteration 213/1000 | Loss: 0.00106741
Iteration 214/1000 | Loss: 0.00088776
Iteration 215/1000 | Loss: 0.00106119
Iteration 216/1000 | Loss: 0.00086831
Iteration 217/1000 | Loss: 0.00129223
Iteration 218/1000 | Loss: 0.00082467
Iteration 219/1000 | Loss: 0.00112908
Iteration 220/1000 | Loss: 0.00089028
Iteration 221/1000 | Loss: 0.00107733
Iteration 222/1000 | Loss: 0.00093576
Iteration 223/1000 | Loss: 0.00126839
Iteration 224/1000 | Loss: 0.00161553
Iteration 225/1000 | Loss: 0.00125119
Iteration 226/1000 | Loss: 0.00125654
Iteration 227/1000 | Loss: 0.00114000
Iteration 228/1000 | Loss: 0.00104934
Iteration 229/1000 | Loss: 0.00109310
Iteration 230/1000 | Loss: 0.00125637
Iteration 231/1000 | Loss: 0.00112176
Iteration 232/1000 | Loss: 0.00101694
Iteration 233/1000 | Loss: 0.00102943
Iteration 234/1000 | Loss: 0.00178084
Iteration 235/1000 | Loss: 0.00119064
Iteration 236/1000 | Loss: 0.00140637
Iteration 237/1000 | Loss: 0.00126610
Iteration 238/1000 | Loss: 0.00144509
Iteration 239/1000 | Loss: 0.00156952
Iteration 240/1000 | Loss: 0.00158585
Iteration 241/1000 | Loss: 0.00085539
Iteration 242/1000 | Loss: 0.00084284
Iteration 243/1000 | Loss: 0.00123447
Iteration 244/1000 | Loss: 0.00104074
Iteration 245/1000 | Loss: 0.00098672
Iteration 246/1000 | Loss: 0.00082986
Iteration 247/1000 | Loss: 0.00096094
Iteration 248/1000 | Loss: 0.00137003
Iteration 249/1000 | Loss: 0.00117906
Iteration 250/1000 | Loss: 0.00123226
Iteration 251/1000 | Loss: 0.00115347
Iteration 252/1000 | Loss: 0.00126807
Iteration 253/1000 | Loss: 0.00164470
Iteration 254/1000 | Loss: 0.00165036
Iteration 255/1000 | Loss: 0.00157496
Iteration 256/1000 | Loss: 0.00083782
Iteration 257/1000 | Loss: 0.00101762
Iteration 258/1000 | Loss: 0.00083271
Iteration 259/1000 | Loss: 0.00097376
Iteration 260/1000 | Loss: 0.00088869
Iteration 261/1000 | Loss: 0.00094506
Iteration 262/1000 | Loss: 0.00102830
Iteration 263/1000 | Loss: 0.00100993
Iteration 264/1000 | Loss: 0.00149222
Iteration 265/1000 | Loss: 0.00167300
Iteration 266/1000 | Loss: 0.00196658
Iteration 267/1000 | Loss: 0.00091120
Iteration 268/1000 | Loss: 0.00186218
Iteration 269/1000 | Loss: 0.00220393
Iteration 270/1000 | Loss: 0.00159685
Iteration 271/1000 | Loss: 0.00101568
Iteration 272/1000 | Loss: 0.00088718
Iteration 273/1000 | Loss: 0.00080791
Iteration 274/1000 | Loss: 0.00104224
Iteration 275/1000 | Loss: 0.00099157
Iteration 276/1000 | Loss: 0.00084281
Iteration 277/1000 | Loss: 0.00202624
Iteration 278/1000 | Loss: 0.00114686
Iteration 279/1000 | Loss: 0.00082928
Iteration 280/1000 | Loss: 0.00119937
Iteration 281/1000 | Loss: 0.00087429
Iteration 282/1000 | Loss: 0.00087150
Iteration 283/1000 | Loss: 0.00080776
Iteration 284/1000 | Loss: 0.00084837
Iteration 285/1000 | Loss: 0.00119872
Iteration 286/1000 | Loss: 0.00093374
Iteration 287/1000 | Loss: 0.00092599
Iteration 288/1000 | Loss: 0.00090560
Iteration 289/1000 | Loss: 0.00081435
Iteration 290/1000 | Loss: 0.00079400
Iteration 291/1000 | Loss: 0.00099371
Iteration 292/1000 | Loss: 0.00092162
Iteration 293/1000 | Loss: 0.00092659
Iteration 294/1000 | Loss: 0.00088158
Iteration 295/1000 | Loss: 0.00092447
Iteration 296/1000 | Loss: 0.00078940
Iteration 297/1000 | Loss: 0.00092293
Iteration 298/1000 | Loss: 0.00092756
Iteration 299/1000 | Loss: 0.00082859
Iteration 300/1000 | Loss: 0.00084256
Iteration 301/1000 | Loss: 0.00079949
Iteration 302/1000 | Loss: 0.00091971
Iteration 303/1000 | Loss: 0.00080509
Iteration 304/1000 | Loss: 0.00096523
Iteration 305/1000 | Loss: 0.00097861
Iteration 306/1000 | Loss: 0.00099983
Iteration 307/1000 | Loss: 0.00087639
Iteration 308/1000 | Loss: 0.00079842
Iteration 309/1000 | Loss: 0.00090337
Iteration 310/1000 | Loss: 0.00105658
Iteration 311/1000 | Loss: 0.00086086
Iteration 312/1000 | Loss: 0.00096283
Iteration 313/1000 | Loss: 0.00083455
Iteration 314/1000 | Loss: 0.00114453
Iteration 315/1000 | Loss: 0.00100880
Iteration 316/1000 | Loss: 0.00100609
Iteration 317/1000 | Loss: 0.00102593
Iteration 318/1000 | Loss: 0.00086966
Iteration 319/1000 | Loss: 0.00088182
Iteration 320/1000 | Loss: 0.00078983
Iteration 321/1000 | Loss: 0.00085605
Iteration 322/1000 | Loss: 0.00078984
Iteration 323/1000 | Loss: 0.00087996
Iteration 324/1000 | Loss: 0.00107913
Iteration 325/1000 | Loss: 0.00088121
Iteration 326/1000 | Loss: 0.00110032
Iteration 327/1000 | Loss: 0.00085617
Iteration 328/1000 | Loss: 0.00102140
Iteration 329/1000 | Loss: 0.00095109
Iteration 330/1000 | Loss: 0.00129498
Iteration 331/1000 | Loss: 0.00095714
Iteration 332/1000 | Loss: 0.00090525
Iteration 333/1000 | Loss: 0.00092373
Iteration 334/1000 | Loss: 0.00082764
Iteration 335/1000 | Loss: 0.00093505
Iteration 336/1000 | Loss: 0.00128674
Iteration 337/1000 | Loss: 0.00104787
Iteration 338/1000 | Loss: 0.00138044
Iteration 339/1000 | Loss: 0.00121710
Iteration 340/1000 | Loss: 0.00094160
Iteration 341/1000 | Loss: 0.00104910
Iteration 342/1000 | Loss: 0.00172662
Iteration 343/1000 | Loss: 0.00124094
Iteration 344/1000 | Loss: 0.00152089
Iteration 345/1000 | Loss: 0.00109915
Iteration 346/1000 | Loss: 0.00081071
Iteration 347/1000 | Loss: 0.00126410
Iteration 348/1000 | Loss: 0.00080467
Iteration 349/1000 | Loss: 0.00087033
Iteration 350/1000 | Loss: 0.00155840
Iteration 351/1000 | Loss: 0.00165830
Iteration 352/1000 | Loss: 0.00102066
Iteration 353/1000 | Loss: 0.00103578
Iteration 354/1000 | Loss: 0.00125144
Iteration 355/1000 | Loss: 0.00144279
Iteration 356/1000 | Loss: 0.00083606
Iteration 357/1000 | Loss: 0.00083270
Iteration 358/1000 | Loss: 0.00111041
Iteration 359/1000 | Loss: 0.00081743
Iteration 360/1000 | Loss: 0.00076950
Iteration 361/1000 | Loss: 0.00082648
Iteration 362/1000 | Loss: 0.00080334
Iteration 363/1000 | Loss: 0.00165145
Iteration 364/1000 | Loss: 0.00089141
Iteration 365/1000 | Loss: 0.00115725
Iteration 366/1000 | Loss: 0.00090202
Iteration 367/1000 | Loss: 0.00113892
Iteration 368/1000 | Loss: 0.00080193
Iteration 369/1000 | Loss: 0.00097967
Iteration 370/1000 | Loss: 0.00079813
Iteration 371/1000 | Loss: 0.00115176
Iteration 372/1000 | Loss: 0.00096507
Iteration 373/1000 | Loss: 0.00103288
Iteration 374/1000 | Loss: 0.00091411
Iteration 375/1000 | Loss: 0.00091389
Iteration 376/1000 | Loss: 0.00090836
Iteration 377/1000 | Loss: 0.00086700
Iteration 378/1000 | Loss: 0.00093111
Iteration 379/1000 | Loss: 0.00084329
Iteration 380/1000 | Loss: 0.00096460
Iteration 381/1000 | Loss: 0.00091691
Iteration 382/1000 | Loss: 0.00091524
Iteration 383/1000 | Loss: 0.00082026
Iteration 384/1000 | Loss: 0.00107916
Iteration 385/1000 | Loss: 0.00097036
Iteration 386/1000 | Loss: 0.00093894
Iteration 387/1000 | Loss: 0.00090495
Iteration 388/1000 | Loss: 0.00090256
Iteration 389/1000 | Loss: 0.00082424
Iteration 390/1000 | Loss: 0.00090879
Iteration 391/1000 | Loss: 0.00119109
Iteration 392/1000 | Loss: 0.00075444
Iteration 393/1000 | Loss: 0.00095170
Iteration 394/1000 | Loss: 0.00104081
Iteration 395/1000 | Loss: 0.00084267
Iteration 396/1000 | Loss: 0.00098627
Iteration 397/1000 | Loss: 0.00082055
Iteration 398/1000 | Loss: 0.00095124
Iteration 399/1000 | Loss: 0.00077952
Iteration 400/1000 | Loss: 0.00105482
Iteration 401/1000 | Loss: 0.00079069
Iteration 402/1000 | Loss: 0.00098505
Iteration 403/1000 | Loss: 0.00079490
Iteration 404/1000 | Loss: 0.00090139
Iteration 405/1000 | Loss: 0.00083262
Iteration 406/1000 | Loss: 0.00117637
Iteration 407/1000 | Loss: 0.00089016
Iteration 408/1000 | Loss: 0.00103066
Iteration 409/1000 | Loss: 0.00096517
Iteration 410/1000 | Loss: 0.00121109
Iteration 411/1000 | Loss: 0.00102595
Iteration 412/1000 | Loss: 0.00108814
Iteration 413/1000 | Loss: 0.00089099
Iteration 414/1000 | Loss: 0.00093709
Iteration 415/1000 | Loss: 0.00091169
Iteration 416/1000 | Loss: 0.00089169
Iteration 417/1000 | Loss: 0.00091002
Iteration 418/1000 | Loss: 0.00084192
Iteration 419/1000 | Loss: 0.00088023
Iteration 420/1000 | Loss: 0.00120132
Iteration 421/1000 | Loss: 0.00096233
Iteration 422/1000 | Loss: 0.00100359
Iteration 423/1000 | Loss: 0.00091850
Iteration 424/1000 | Loss: 0.00094778
Iteration 425/1000 | Loss: 0.00095276
Iteration 426/1000 | Loss: 0.00102442
Iteration 427/1000 | Loss: 0.00101331
Iteration 428/1000 | Loss: 0.00103675
Iteration 429/1000 | Loss: 0.00092381
Iteration 430/1000 | Loss: 0.00088117
Iteration 431/1000 | Loss: 0.00091087
Iteration 432/1000 | Loss: 0.00105019
Iteration 433/1000 | Loss: 0.00087324
Iteration 434/1000 | Loss: 0.00119992
Iteration 435/1000 | Loss: 0.00086745
Iteration 436/1000 | Loss: 0.00112891
Iteration 437/1000 | Loss: 0.00098190
Iteration 438/1000 | Loss: 0.00103257
Iteration 439/1000 | Loss: 0.00096830
Iteration 440/1000 | Loss: 0.00092310
Iteration 441/1000 | Loss: 0.00109040
Iteration 442/1000 | Loss: 0.00088873
Iteration 443/1000 | Loss: 0.00091234
Iteration 444/1000 | Loss: 0.00086462
Iteration 445/1000 | Loss: 0.00097557
Iteration 446/1000 | Loss: 0.00084215
Iteration 447/1000 | Loss: 0.00088293
Iteration 448/1000 | Loss: 0.00088557
Iteration 449/1000 | Loss: 0.00111238
Iteration 450/1000 | Loss: 0.00092999
Iteration 451/1000 | Loss: 0.00095222
Iteration 452/1000 | Loss: 0.00088267
Iteration 453/1000 | Loss: 0.00085458
Iteration 454/1000 | Loss: 0.00093362
Iteration 455/1000 | Loss: 0.00083736
Iteration 456/1000 | Loss: 0.00091524
Iteration 457/1000 | Loss: 0.00077532
Iteration 458/1000 | Loss: 0.00087549
Iteration 459/1000 | Loss: 0.00082382
Iteration 460/1000 | Loss: 0.00086167
Iteration 461/1000 | Loss: 0.00087298
Iteration 462/1000 | Loss: 0.00092046
Iteration 463/1000 | Loss: 0.00090464
Iteration 464/1000 | Loss: 0.00093110
Iteration 465/1000 | Loss: 0.00094709
Iteration 466/1000 | Loss: 0.00087709
Iteration 467/1000 | Loss: 0.00118651
Iteration 468/1000 | Loss: 0.00092643
Iteration 469/1000 | Loss: 0.00090951
Iteration 470/1000 | Loss: 0.00092761
Iteration 471/1000 | Loss: 0.00092122
Iteration 472/1000 | Loss: 0.00094577
Iteration 473/1000 | Loss: 0.00096891
Iteration 474/1000 | Loss: 0.00094316
Iteration 475/1000 | Loss: 0.00093528
Iteration 476/1000 | Loss: 0.00093585
Iteration 477/1000 | Loss: 0.00089507
Iteration 478/1000 | Loss: 0.00112130
Iteration 479/1000 | Loss: 0.00091554
Iteration 480/1000 | Loss: 0.00075314
Iteration 481/1000 | Loss: 0.00089748
Iteration 482/1000 | Loss: 0.00102426
Iteration 483/1000 | Loss: 0.00093447
Iteration 484/1000 | Loss: 0.00103226
Iteration 485/1000 | Loss: 0.00093826
Iteration 486/1000 | Loss: 0.00096878
Iteration 487/1000 | Loss: 0.00097575
Iteration 488/1000 | Loss: 0.00097394
Iteration 489/1000 | Loss: 0.00091132
Iteration 490/1000 | Loss: 0.00084763
Iteration 491/1000 | Loss: 0.00119185
Iteration 492/1000 | Loss: 0.00092732
Iteration 493/1000 | Loss: 0.00111238
Iteration 494/1000 | Loss: 0.00124833
Iteration 495/1000 | Loss: 0.00106412
Iteration 496/1000 | Loss: 0.00092686
Iteration 497/1000 | Loss: 0.00092507
Iteration 498/1000 | Loss: 0.00082712
Iteration 499/1000 | Loss: 0.00091012
Iteration 500/1000 | Loss: 0.00083945
Iteration 501/1000 | Loss: 0.00112878
Iteration 502/1000 | Loss: 0.00097313
Iteration 503/1000 | Loss: 0.00098026
Iteration 504/1000 | Loss: 0.00093268
Iteration 505/1000 | Loss: 0.00118329
Iteration 506/1000 | Loss: 0.00091012
Iteration 507/1000 | Loss: 0.00113055
Iteration 508/1000 | Loss: 0.00093269
Iteration 509/1000 | Loss: 0.00104764
Iteration 510/1000 | Loss: 0.00102306
Iteration 511/1000 | Loss: 0.00099198
Iteration 512/1000 | Loss: 0.00099927
Iteration 513/1000 | Loss: 0.00095797
Iteration 514/1000 | Loss: 0.00093572
Iteration 515/1000 | Loss: 0.00097275
Iteration 516/1000 | Loss: 0.00091826
Iteration 517/1000 | Loss: 0.00091933
Iteration 518/1000 | Loss: 0.00087808
Iteration 519/1000 | Loss: 0.00094528
Iteration 520/1000 | Loss: 0.00090360
Iteration 521/1000 | Loss: 0.00094637
Iteration 522/1000 | Loss: 0.00091166
Iteration 523/1000 | Loss: 0.00084279
Iteration 524/1000 | Loss: 0.00089590
Iteration 525/1000 | Loss: 0.00088923
Iteration 526/1000 | Loss: 0.00082781
Iteration 527/1000 | Loss: 0.00090254
Iteration 528/1000 | Loss: 0.00082510
Iteration 529/1000 | Loss: 0.00082244
Iteration 530/1000 | Loss: 0.00083960
Iteration 531/1000 | Loss: 0.00091683
Iteration 532/1000 | Loss: 0.00081830
Iteration 533/1000 | Loss: 0.00090047
Iteration 534/1000 | Loss: 0.00081088
Iteration 535/1000 | Loss: 0.00086450
Iteration 536/1000 | Loss: 0.00084605
Iteration 537/1000 | Loss: 0.00087895
Iteration 538/1000 | Loss: 0.00084357
Iteration 539/1000 | Loss: 0.00102091
Iteration 540/1000 | Loss: 0.00089726
Iteration 541/1000 | Loss: 0.00099855
Iteration 542/1000 | Loss: 0.00111693
Iteration 543/1000 | Loss: 0.00093660
Iteration 544/1000 | Loss: 0.00121380
Iteration 545/1000 | Loss: 0.00109331
Iteration 546/1000 | Loss: 0.00101136
Iteration 547/1000 | Loss: 0.00171951
Iteration 548/1000 | Loss: 0.00137914
Iteration 549/1000 | Loss: 0.00105576
Iteration 550/1000 | Loss: 0.00158066
Iteration 551/1000 | Loss: 0.00111866
Iteration 552/1000 | Loss: 0.00142151
Iteration 553/1000 | Loss: 0.00119008
Iteration 554/1000 | Loss: 0.00111095
Iteration 555/1000 | Loss: 0.00097922
Iteration 556/1000 | Loss: 0.00136958
Iteration 557/1000 | Loss: 0.00097232
Iteration 558/1000 | Loss: 0.00141188
Iteration 559/1000 | Loss: 0.00103668
Iteration 560/1000 | Loss: 0.00107921
Iteration 561/1000 | Loss: 0.00134581
Iteration 562/1000 | Loss: 0.00109901
Iteration 563/1000 | Loss: 0.00127305
Iteration 564/1000 | Loss: 0.00104226
Iteration 565/1000 | Loss: 0.00096559
Iteration 566/1000 | Loss: 0.00121913
Iteration 567/1000 | Loss: 0.00096184
Iteration 568/1000 | Loss: 0.00125155
Iteration 569/1000 | Loss: 0.00091221
Iteration 570/1000 | Loss: 0.00113435
Iteration 571/1000 | Loss: 0.00096724
Iteration 572/1000 | Loss: 0.00095366
Iteration 573/1000 | Loss: 0.00091287
Iteration 574/1000 | Loss: 0.00089902
Iteration 575/1000 | Loss: 0.00085605
Iteration 576/1000 | Loss: 0.00094828
Iteration 577/1000 | Loss: 0.00103014
Iteration 578/1000 | Loss: 0.00085303
Iteration 579/1000 | Loss: 0.00093156
Iteration 580/1000 | Loss: 0.00089425
Iteration 581/1000 | Loss: 0.00133121
Iteration 582/1000 | Loss: 0.00091963
Iteration 583/1000 | Loss: 0.00125607
Iteration 584/1000 | Loss: 0.00100891
Iteration 585/1000 | Loss: 0.00094536
Iteration 586/1000 | Loss: 0.00092730
Iteration 587/1000 | Loss: 0.00133791
Iteration 588/1000 | Loss: 0.00093158
Iteration 589/1000 | Loss: 0.00123358
Iteration 590/1000 | Loss: 0.00111942
Iteration 591/1000 | Loss: 0.00099822
Iteration 592/1000 | Loss: 0.00104147
Iteration 593/1000 | Loss: 0.00093341
Iteration 594/1000 | Loss: 0.00093055
Iteration 595/1000 | Loss: 0.00092563
Iteration 596/1000 | Loss: 0.00088204
Iteration 597/1000 | Loss: 0.00090099
Iteration 598/1000 | Loss: 0.00088729
Iteration 599/1000 | Loss: 0.00133426
Iteration 600/1000 | Loss: 0.00100988
Iteration 601/1000 | Loss: 0.00097825
Iteration 602/1000 | Loss: 0.00103508
Iteration 603/1000 | Loss: 0.00102206
Iteration 604/1000 | Loss: 0.00119257
Iteration 605/1000 | Loss: 0.00104454
Iteration 606/1000 | Loss: 0.00120354
Iteration 607/1000 | Loss: 0.00105924
Iteration 608/1000 | Loss: 0.00107647
Iteration 609/1000 | Loss: 0.00110524
Iteration 610/1000 | Loss: 0.00106816
Iteration 611/1000 | Loss: 0.00092363
Iteration 612/1000 | Loss: 0.00098845
Iteration 613/1000 | Loss: 0.00090310
Iteration 614/1000 | Loss: 0.00111253
Iteration 615/1000 | Loss: 0.00106540
Iteration 616/1000 | Loss: 0.00096558
Iteration 617/1000 | Loss: 0.00093103
Iteration 618/1000 | Loss: 0.00096695
Iteration 619/1000 | Loss: 0.00090300
Iteration 620/1000 | Loss: 0.00094862
Iteration 621/1000 | Loss: 0.00090224
Iteration 622/1000 | Loss: 0.00127826
Iteration 623/1000 | Loss: 0.00101648
Iteration 624/1000 | Loss: 0.00097918
Iteration 625/1000 | Loss: 0.00092099
Iteration 626/1000 | Loss: 0.00101206
Iteration 627/1000 | Loss: 0.00117036
Iteration 628/1000 | Loss: 0.00097406
Iteration 629/1000 | Loss: 0.00104564
Iteration 630/1000 | Loss: 0.00100292
Iteration 631/1000 | Loss: 0.00099329
Iteration 632/1000 | Loss: 0.00096151
Iteration 633/1000 | Loss: 0.00107958
Iteration 634/1000 | Loss: 0.00097664
Iteration 635/1000 | Loss: 0.00093195
Iteration 636/1000 | Loss: 0.00097881
Iteration 637/1000 | Loss: 0.00094641
Iteration 638/1000 | Loss: 0.00095270
Iteration 639/1000 | Loss: 0.00093905
Iteration 640/1000 | Loss: 0.00095259
Iteration 641/1000 | Loss: 0.00117494
Iteration 642/1000 | Loss: 0.00097673
Iteration 643/1000 | Loss: 0.00113461
Iteration 644/1000 | Loss: 0.00096614
Iteration 645/1000 | Loss: 0.00124741
Iteration 646/1000 | Loss: 0.00097164
Iteration 647/1000 | Loss: 0.00136321
Iteration 648/1000 | Loss: 0.00116605
Iteration 649/1000 | Loss: 0.00107998
Iteration 650/1000 | Loss: 0.00096794
Iteration 651/1000 | Loss: 0.00100365
Iteration 652/1000 | Loss: 0.00094804
Iteration 653/1000 | Loss: 0.00109620
Iteration 654/1000 | Loss: 0.00095067
Iteration 655/1000 | Loss: 0.00129628
Iteration 656/1000 | Loss: 0.00098270
Iteration 657/1000 | Loss: 0.00125407
Iteration 658/1000 | Loss: 0.00103501
Iteration 659/1000 | Loss: 0.00108190
Iteration 660/1000 | Loss: 0.00096817
Iteration 661/1000 | Loss: 0.00107567
Iteration 662/1000 | Loss: 0.00098790
Iteration 663/1000 | Loss: 0.00103095
Iteration 664/1000 | Loss: 0.00094469
Iteration 665/1000 | Loss: 0.00100800
Iteration 666/1000 | Loss: 0.00114404
Iteration 667/1000 | Loss: 0.00104046
Iteration 668/1000 | Loss: 0.00109523
Iteration 669/1000 | Loss: 0.00096888
Iteration 670/1000 | Loss: 0.00101197
Iteration 671/1000 | Loss: 0.00087945
Iteration 672/1000 | Loss: 0.00092355
Iteration 673/1000 | Loss: 0.00100474
Iteration 674/1000 | Loss: 0.00104377
Iteration 675/1000 | Loss: 0.00102594
Iteration 676/1000 | Loss: 0.00098575
Iteration 677/1000 | Loss: 0.00093417
Iteration 678/1000 | Loss: 0.00096447
Iteration 679/1000 | Loss: 0.00090569
Iteration 680/1000 | Loss: 0.00089831
Iteration 681/1000 | Loss: 0.00088600
Iteration 682/1000 | Loss: 0.00109521
Iteration 683/1000 | Loss: 0.00088242
Iteration 684/1000 | Loss: 0.00087261
Iteration 685/1000 | Loss: 0.00089193
Iteration 686/1000 | Loss: 0.00092612
Iteration 687/1000 | Loss: 0.00083650
Iteration 688/1000 | Loss: 0.00093874
Iteration 689/1000 | Loss: 0.00087336
Iteration 690/1000 | Loss: 0.00093947
Iteration 691/1000 | Loss: 0.00089637
Iteration 692/1000 | Loss: 0.00090874
Iteration 693/1000 | Loss: 0.00091297
Iteration 694/1000 | Loss: 0.00088369
Iteration 695/1000 | Loss: 0.00105985
Iteration 696/1000 | Loss: 0.00091784
Iteration 697/1000 | Loss: 0.00102198
Iteration 698/1000 | Loss: 0.00087122
Iteration 699/1000 | Loss: 0.00088315
Iteration 700/1000 | Loss: 0.00091518
Iteration 701/1000 | Loss: 0.00105354
Iteration 702/1000 | Loss: 0.00087327
Iteration 703/1000 | Loss: 0.00098064
Iteration 704/1000 | Loss: 0.00088815
Iteration 705/1000 | Loss: 0.00088671
Iteration 706/1000 | Loss: 0.00095920
Iteration 707/1000 | Loss: 0.00089501
Iteration 708/1000 | Loss: 0.00108293
Iteration 709/1000 | Loss: 0.00090183
Iteration 710/1000 | Loss: 0.00114958
Iteration 711/1000 | Loss: 0.00089306
Iteration 712/1000 | Loss: 0.00118900
Iteration 713/1000 | Loss: 0.00099110
Iteration 714/1000 | Loss: 0.00099965
Iteration 715/1000 | Loss: 0.00088210
Iteration 716/1000 | Loss: 0.00097899
Iteration 717/1000 | Loss: 0.00086796
Iteration 718/1000 | Loss: 0.00111482
Iteration 719/1000 | Loss: 0.00088789
Iteration 720/1000 | Loss: 0.00095237
Iteration 721/1000 | Loss: 0.00082897
Iteration 722/1000 | Loss: 0.00088632
Iteration 723/1000 | Loss: 0.00090286
Iteration 724/1000 | Loss: 0.00100831
Iteration 725/1000 | Loss: 0.00102198
Iteration 726/1000 | Loss: 0.00093109
Iteration 727/1000 | Loss: 0.00089690
Iteration 728/1000 | Loss: 0.00090557
Iteration 729/1000 | Loss: 0.00085533
Iteration 730/1000 | Loss: 0.00098567
Iteration 731/1000 | Loss: 0.00085270
Iteration 732/1000 | Loss: 0.00098503
Iteration 733/1000 | Loss: 0.00101100
Iteration 734/1000 | Loss: 0.00094578
Iteration 735/1000 | Loss: 0.00101185
Iteration 736/1000 | Loss: 0.00090739
Iteration 737/1000 | Loss: 0.00085652
Iteration 738/1000 | Loss: 0.00094245
Iteration 739/1000 | Loss: 0.00084344
Iteration 740/1000 | Loss: 0.00098498
Iteration 741/1000 | Loss: 0.00090582
Iteration 742/1000 | Loss: 0.00091641
Iteration 743/1000 | Loss: 0.00084814
Iteration 744/1000 | Loss: 0.00088431
Iteration 745/1000 | Loss: 0.00087891
Iteration 746/1000 | Loss: 0.00089809
Iteration 747/1000 | Loss: 0.00087753
Iteration 748/1000 | Loss: 0.00112234
Iteration 749/1000 | Loss: 0.00096087
Iteration 750/1000 | Loss: 0.00084183
Iteration 751/1000 | Loss: 0.00089297
Iteration 752/1000 | Loss: 0.00096057
Iteration 753/1000 | Loss: 0.00095394
Iteration 754/1000 | Loss: 0.00086867
Iteration 755/1000 | Loss: 0.00096812
Iteration 756/1000 | Loss: 0.00087818
Iteration 757/1000 | Loss: 0.00088467
Iteration 758/1000 | Loss: 0.00087527
Iteration 759/1000 | Loss: 0.00083194
Iteration 760/1000 | Loss: 0.00094702
Iteration 761/1000 | Loss: 0.00082780
Iteration 762/1000 | Loss: 0.00087788
Iteration 763/1000 | Loss: 0.00089495
Iteration 764/1000 | Loss: 0.00100662
Iteration 765/1000 | Loss: 0.00090862
Iteration 766/1000 | Loss: 0.00098627
Iteration 767/1000 | Loss: 0.00092529
Iteration 768/1000 | Loss: 0.00098585
Iteration 769/1000 | Loss: 0.00091914
Iteration 770/1000 | Loss: 0.00093505
Iteration 771/1000 | Loss: 0.00088384
Iteration 772/1000 | Loss: 0.00090283
Iteration 773/1000 | Loss: 0.00101260
Iteration 774/1000 | Loss: 0.00098880
Iteration 775/1000 | Loss: 0.00090754
Iteration 776/1000 | Loss: 0.00085590
Iteration 777/1000 | Loss: 0.00086481
Iteration 778/1000 | Loss: 0.00097163
Iteration 779/1000 | Loss: 0.00080667
Iteration 780/1000 | Loss: 0.00093065
Iteration 781/1000 | Loss: 0.00105717
Iteration 782/1000 | Loss: 0.00092290
Iteration 783/1000 | Loss: 0.00089612
Iteration 784/1000 | Loss: 0.00087273
Iteration 785/1000 | Loss: 0.00086561
Iteration 786/1000 | Loss: 0.00095231
Iteration 787/1000 | Loss: 0.00091070
Iteration 788/1000 | Loss: 0.00091139
Iteration 789/1000 | Loss: 0.00080710
Iteration 790/1000 | Loss: 0.00089239
Iteration 791/1000 | Loss: 0.00085741
Iteration 792/1000 | Loss: 0.00095745
Iteration 793/1000 | Loss: 0.00090591
Iteration 794/1000 | Loss: 0.00088808
Iteration 795/1000 | Loss: 0.00081103
Iteration 796/1000 | Loss: 0.00086112
Iteration 797/1000 | Loss: 0.00087484
Iteration 798/1000 | Loss: 0.00093023
Iteration 799/1000 | Loss: 0.00106685
Iteration 800/1000 | Loss: 0.00094237
Iteration 801/1000 | Loss: 0.00107077
Iteration 802/1000 | Loss: 0.00105161
Iteration 803/1000 | Loss: 0.00114509
Iteration 804/1000 | Loss: 0.00101829
Iteration 805/1000 | Loss: 0.00096046
Iteration 806/1000 | Loss: 0.00099045
Iteration 807/1000 | Loss: 0.00087352
Iteration 808/1000 | Loss: 0.00095923
Iteration 809/1000 | Loss: 0.00113264
Iteration 810/1000 | Loss: 0.00100959
Iteration 811/1000 | Loss: 0.00124167
Iteration 812/1000 | Loss: 0.00115350
Iteration 813/1000 | Loss: 0.00115901
Iteration 814/1000 | Loss: 0.00093988
Iteration 815/1000 | Loss: 0.00096275
Iteration 816/1000 | Loss: 0.00091146
Iteration 817/1000 | Loss: 0.00124258
Iteration 818/1000 | Loss: 0.00100091
Iteration 819/1000 | Loss: 0.00123404
Iteration 820/1000 | Loss: 0.00100005
Iteration 821/1000 | Loss: 0.00095410
Iteration 822/1000 | Loss: 0.00093031
Iteration 823/1000 | Loss: 0.00092610
Iteration 824/1000 | Loss: 0.00101599
Iteration 825/1000 | Loss: 0.00121182
Iteration 826/1000 | Loss: 0.00095726
Iteration 827/1000 | Loss: 0.00103380
Iteration 828/1000 | Loss: 0.00093601
Iteration 829/1000 | Loss: 0.00106820
Iteration 830/1000 | Loss: 0.00107222
Iteration 831/1000 | Loss: 0.00101484
Iteration 832/1000 | Loss: 0.00095605
Iteration 833/1000 | Loss: 0.00102971
Iteration 834/1000 | Loss: 0.00094822
Iteration 835/1000 | Loss: 0.00091593
Iteration 836/1000 | Loss: 0.00096811
Iteration 837/1000 | Loss: 0.00091967
Iteration 838/1000 | Loss: 0.00097417
Iteration 839/1000 | Loss: 0.00088309
Iteration 840/1000 | Loss: 0.00101321
Iteration 841/1000 | Loss: 0.00087134
Iteration 842/1000 | Loss: 0.00092194
Iteration 843/1000 | Loss: 0.00091374
Iteration 844/1000 | Loss: 0.00105216
Iteration 845/1000 | Loss: 0.00091228
Iteration 846/1000 | Loss: 0.00112326
Iteration 847/1000 | Loss: 0.00095254
Iteration 848/1000 | Loss: 0.00116144
Iteration 849/1000 | Loss: 0.00089699
Iteration 850/1000 | Loss: 0.00070651
Iteration 851/1000 | Loss: 0.00084066
Iteration 852/1000 | Loss: 0.00072361
Iteration 853/1000 | Loss: 0.00085702
Iteration 854/1000 | Loss: 0.00091300
Iteration 855/1000 | Loss: 0.00109989
Iteration 856/1000 | Loss: 0.00095465
Iteration 857/1000 | Loss: 0.00113091
Iteration 858/1000 | Loss: 0.00233737
Iteration 859/1000 | Loss: 0.00128529
Iteration 860/1000 | Loss: 0.00101991
Iteration 861/1000 | Loss: 0.00114899
Iteration 862/1000 | Loss: 0.00091079
Iteration 863/1000 | Loss: 0.00106996
Iteration 864/1000 | Loss: 0.00093102
Iteration 865/1000 | Loss: 0.00110589
Iteration 866/1000 | Loss: 0.00090706
Iteration 867/1000 | Loss: 0.00110796
Iteration 868/1000 | Loss: 0.00089648
Iteration 869/1000 | Loss: 0.00101932
Iteration 870/1000 | Loss: 0.00086381
Iteration 871/1000 | Loss: 0.00096699
Iteration 872/1000 | Loss: 0.00107603
Iteration 873/1000 | Loss: 0.00090893
Iteration 874/1000 | Loss: 0.00100294
Iteration 875/1000 | Loss: 0.00090350
Iteration 876/1000 | Loss: 0.00121007
Iteration 877/1000 | Loss: 0.00131192
Iteration 878/1000 | Loss: 0.00110933
Iteration 879/1000 | Loss: 0.00099492
Iteration 880/1000 | Loss: 0.00110054
Iteration 881/1000 | Loss: 0.00092661
Iteration 882/1000 | Loss: 0.00100968
Iteration 883/1000 | Loss: 0.00092565
Iteration 884/1000 | Loss: 0.00098222
Iteration 885/1000 | Loss: 0.00093228
Iteration 886/1000 | Loss: 0.00126084
Iteration 887/1000 | Loss: 0.00125501
Iteration 888/1000 | Loss: 0.00124054
Iteration 889/1000 | Loss: 0.00134532
Iteration 890/1000 | Loss: 0.00110443
Iteration 891/1000 | Loss: 0.00114856
Iteration 892/1000 | Loss: 0.00107500
Iteration 893/1000 | Loss: 0.00111623
Iteration 894/1000 | Loss: 0.00102339
Iteration 895/1000 | Loss: 0.00120822
Iteration 896/1000 | Loss: 0.00104899
Iteration 897/1000 | Loss: 0.00128890
Iteration 898/1000 | Loss: 0.00110399
Iteration 899/1000 | Loss: 0.00106816
Iteration 900/1000 | Loss: 0.00099738
Iteration 901/1000 | Loss: 0.00099157
Iteration 902/1000 | Loss: 0.00097268
Iteration 903/1000 | Loss: 0.00112084
Iteration 904/1000 | Loss: 0.00095496
Iteration 905/1000 | Loss: 0.00108222
Iteration 906/1000 | Loss: 0.00111473
Iteration 907/1000 | Loss: 0.00103180
Iteration 908/1000 | Loss: 0.00101472
Iteration 909/1000 | Loss: 0.00131831
Iteration 910/1000 | Loss: 0.00213452
Iteration 911/1000 | Loss: 0.00117420
Iteration 912/1000 | Loss: 0.00108724
Iteration 913/1000 | Loss: 0.00103740
Iteration 914/1000 | Loss: 0.00107994
Iteration 915/1000 | Loss: 0.00102959
Iteration 916/1000 | Loss: 0.00102429
Iteration 917/1000 | Loss: 0.00100352
Iteration 918/1000 | Loss: 0.00099032
Iteration 919/1000 | Loss: 0.00096177
Iteration 920/1000 | Loss: 0.00096360
Iteration 921/1000 | Loss: 0.00096911
Iteration 922/1000 | Loss: 0.00093123
Iteration 923/1000 | Loss: 0.00174838
Iteration 924/1000 | Loss: 0.00110095
Iteration 925/1000 | Loss: 0.00094636
Iteration 926/1000 | Loss: 0.00197289
Iteration 927/1000 | Loss: 0.00170484
Iteration 928/1000 | Loss: 0.00096762
Iteration 929/1000 | Loss: 0.00125833
Iteration 930/1000 | Loss: 0.00150474
Iteration 931/1000 | Loss: 0.00161366
Iteration 932/1000 | Loss: 0.00176900
Iteration 933/1000 | Loss: 0.00163815
Iteration 934/1000 | Loss: 0.00217256
Iteration 935/1000 | Loss: 0.00172895
Iteration 936/1000 | Loss: 0.00096714
Iteration 937/1000 | Loss: 0.00104489
Iteration 938/1000 | Loss: 0.00188032
Iteration 939/1000 | Loss: 0.00141741
Iteration 940/1000 | Loss: 0.00103780
Iteration 941/1000 | Loss: 0.00095172
Iteration 942/1000 | Loss: 0.00131543
Iteration 943/1000 | Loss: 0.00126184
Iteration 944/1000 | Loss: 0.00162180
Iteration 945/1000 | Loss: 0.00134678
Iteration 946/1000 | Loss: 0.00131698
Iteration 947/1000 | Loss: 0.00160414
Iteration 948/1000 | Loss: 0.00115687
Iteration 949/1000 | Loss: 0.00092679
Iteration 950/1000 | Loss: 0.00104912
Iteration 951/1000 | Loss: 0.00101352
Iteration 952/1000 | Loss: 0.00199184
Iteration 953/1000 | Loss: 0.00127550
Iteration 954/1000 | Loss: 0.00173021
Iteration 955/1000 | Loss: 0.00095086
Iteration 956/1000 | Loss: 0.00107719
Iteration 957/1000 | Loss: 0.00088875
Iteration 958/1000 | Loss: 0.00119526
Iteration 959/1000 | Loss: 0.00092913
Iteration 960/1000 | Loss: 0.00162173
Iteration 961/1000 | Loss: 0.00116105
Iteration 962/1000 | Loss: 0.00095286
Iteration 963/1000 | Loss: 0.00090873
Iteration 964/1000 | Loss: 0.00088717
Iteration 965/1000 | Loss: 0.00100169
Iteration 966/1000 | Loss: 0.00097387
Iteration 967/1000 | Loss: 0.00115630
Iteration 968/1000 | Loss: 0.00125747
Iteration 969/1000 | Loss: 0.00096028
Iteration 970/1000 | Loss: 0.00088507
Iteration 971/1000 | Loss: 0.00104193
Iteration 972/1000 | Loss: 0.00092561
Iteration 973/1000 | Loss: 0.00095849
Iteration 974/1000 | Loss: 0.00090454
Iteration 975/1000 | Loss: 0.00104155
Iteration 976/1000 | Loss: 0.00090688
Iteration 977/1000 | Loss: 0.00091549
Iteration 978/1000 | Loss: 0.00101372
Iteration 979/1000 | Loss: 0.00089173
Iteration 980/1000 | Loss: 0.00097482
Iteration 981/1000 | Loss: 0.00098020
Iteration 982/1000 | Loss: 0.00140118
Iteration 983/1000 | Loss: 0.00086321
Iteration 984/1000 | Loss: 0.00177182
Iteration 985/1000 | Loss: 0.00203911
Iteration 986/1000 | Loss: 0.00124037
Iteration 987/1000 | Loss: 0.00188431
Iteration 988/1000 | Loss: 0.00121015
Iteration 989/1000 | Loss: 0.00117623
Iteration 990/1000 | Loss: 0.00104290
Iteration 991/1000 | Loss: 0.00109611
Iteration 992/1000 | Loss: 0.00120593
Iteration 993/1000 | Loss: 0.00105536
Iteration 994/1000 | Loss: 0.00094985
Iteration 995/1000 | Loss: 0.00094719
Iteration 996/1000 | Loss: 0.00132409
Iteration 997/1000 | Loss: 0.00112029
Iteration 998/1000 | Loss: 0.00095038
Iteration 999/1000 | Loss: 0.00134797
Iteration 1000/1000 | Loss: 0.00095362

Optimization complete. Final v2v error: 17.32024383544922 mm

Highest mean error: 64.1584701538086 mm for frame 137

Lowest mean error: 8.789410591125488 mm for frame 208

Saving results

Total time: 1524.891563653946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466787
Iteration 2/25 | Loss: 0.00162584
Iteration 3/25 | Loss: 0.00153085
Iteration 4/25 | Loss: 0.00151818
Iteration 5/25 | Loss: 0.00151360
Iteration 6/25 | Loss: 0.00151284
Iteration 7/25 | Loss: 0.00151284
Iteration 8/25 | Loss: 0.00151284
Iteration 9/25 | Loss: 0.00151284
Iteration 10/25 | Loss: 0.00151284
Iteration 11/25 | Loss: 0.00151284
Iteration 12/25 | Loss: 0.00151284
Iteration 13/25 | Loss: 0.00151284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015128444647416472, 0.0015128444647416472, 0.0015128444647416472, 0.0015128444647416472, 0.0015128444647416472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015128444647416472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80278432
Iteration 2/25 | Loss: 0.00201865
Iteration 3/25 | Loss: 0.00201865
Iteration 4/25 | Loss: 0.00201865
Iteration 5/25 | Loss: 0.00201865
Iteration 6/25 | Loss: 0.00201865
Iteration 7/25 | Loss: 0.00201865
Iteration 8/25 | Loss: 0.00201865
Iteration 9/25 | Loss: 0.00201865
Iteration 10/25 | Loss: 0.00201865
Iteration 11/25 | Loss: 0.00201865
Iteration 12/25 | Loss: 0.00201865
Iteration 13/25 | Loss: 0.00201865
Iteration 14/25 | Loss: 0.00201865
Iteration 15/25 | Loss: 0.00201865
Iteration 16/25 | Loss: 0.00201865
Iteration 17/25 | Loss: 0.00201865
Iteration 18/25 | Loss: 0.00201865
Iteration 19/25 | Loss: 0.00201865
Iteration 20/25 | Loss: 0.00201865
Iteration 21/25 | Loss: 0.00201865
Iteration 22/25 | Loss: 0.00201865
Iteration 23/25 | Loss: 0.00201865
Iteration 24/25 | Loss: 0.00201865
Iteration 25/25 | Loss: 0.00201865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201865
Iteration 2/1000 | Loss: 0.00005579
Iteration 3/1000 | Loss: 0.00003955
Iteration 4/1000 | Loss: 0.00003370
Iteration 5/1000 | Loss: 0.00003039
Iteration 6/1000 | Loss: 0.00002897
Iteration 7/1000 | Loss: 0.00002790
Iteration 8/1000 | Loss: 0.00002738
Iteration 9/1000 | Loss: 0.00002691
Iteration 10/1000 | Loss: 0.00002663
Iteration 11/1000 | Loss: 0.00002661
Iteration 12/1000 | Loss: 0.00002660
Iteration 13/1000 | Loss: 0.00002655
Iteration 14/1000 | Loss: 0.00002653
Iteration 15/1000 | Loss: 0.00002650
Iteration 16/1000 | Loss: 0.00002648
Iteration 17/1000 | Loss: 0.00002642
Iteration 18/1000 | Loss: 0.00002638
Iteration 19/1000 | Loss: 0.00002638
Iteration 20/1000 | Loss: 0.00002638
Iteration 21/1000 | Loss: 0.00002637
Iteration 22/1000 | Loss: 0.00002636
Iteration 23/1000 | Loss: 0.00002636
Iteration 24/1000 | Loss: 0.00002636
Iteration 25/1000 | Loss: 0.00002636
Iteration 26/1000 | Loss: 0.00002636
Iteration 27/1000 | Loss: 0.00002636
Iteration 28/1000 | Loss: 0.00002636
Iteration 29/1000 | Loss: 0.00002635
Iteration 30/1000 | Loss: 0.00002635
Iteration 31/1000 | Loss: 0.00002635
Iteration 32/1000 | Loss: 0.00002635
Iteration 33/1000 | Loss: 0.00002635
Iteration 34/1000 | Loss: 0.00002635
Iteration 35/1000 | Loss: 0.00002634
Iteration 36/1000 | Loss: 0.00002634
Iteration 37/1000 | Loss: 0.00002634
Iteration 38/1000 | Loss: 0.00002634
Iteration 39/1000 | Loss: 0.00002634
Iteration 40/1000 | Loss: 0.00002634
Iteration 41/1000 | Loss: 0.00002634
Iteration 42/1000 | Loss: 0.00002634
Iteration 43/1000 | Loss: 0.00002634
Iteration 44/1000 | Loss: 0.00002634
Iteration 45/1000 | Loss: 0.00002634
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002634
Iteration 48/1000 | Loss: 0.00002634
Iteration 49/1000 | Loss: 0.00002633
Iteration 50/1000 | Loss: 0.00002633
Iteration 51/1000 | Loss: 0.00002633
Iteration 52/1000 | Loss: 0.00002633
Iteration 53/1000 | Loss: 0.00002633
Iteration 54/1000 | Loss: 0.00002633
Iteration 55/1000 | Loss: 0.00002633
Iteration 56/1000 | Loss: 0.00002632
Iteration 57/1000 | Loss: 0.00002632
Iteration 58/1000 | Loss: 0.00002632
Iteration 59/1000 | Loss: 0.00002632
Iteration 60/1000 | Loss: 0.00002632
Iteration 61/1000 | Loss: 0.00002632
Iteration 62/1000 | Loss: 0.00002631
Iteration 63/1000 | Loss: 0.00002631
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002631
Iteration 68/1000 | Loss: 0.00002631
Iteration 69/1000 | Loss: 0.00002631
Iteration 70/1000 | Loss: 0.00002631
Iteration 71/1000 | Loss: 0.00002631
Iteration 72/1000 | Loss: 0.00002631
Iteration 73/1000 | Loss: 0.00002631
Iteration 74/1000 | Loss: 0.00002631
Iteration 75/1000 | Loss: 0.00002631
Iteration 76/1000 | Loss: 0.00002630
Iteration 77/1000 | Loss: 0.00002630
Iteration 78/1000 | Loss: 0.00002630
Iteration 79/1000 | Loss: 0.00002630
Iteration 80/1000 | Loss: 0.00002630
Iteration 81/1000 | Loss: 0.00002629
Iteration 82/1000 | Loss: 0.00002629
Iteration 83/1000 | Loss: 0.00002629
Iteration 84/1000 | Loss: 0.00002629
Iteration 85/1000 | Loss: 0.00002629
Iteration 86/1000 | Loss: 0.00002629
Iteration 87/1000 | Loss: 0.00002629
Iteration 88/1000 | Loss: 0.00002628
Iteration 89/1000 | Loss: 0.00002628
Iteration 90/1000 | Loss: 0.00002628
Iteration 91/1000 | Loss: 0.00002628
Iteration 92/1000 | Loss: 0.00002628
Iteration 93/1000 | Loss: 0.00002628
Iteration 94/1000 | Loss: 0.00002628
Iteration 95/1000 | Loss: 0.00002628
Iteration 96/1000 | Loss: 0.00002628
Iteration 97/1000 | Loss: 0.00002628
Iteration 98/1000 | Loss: 0.00002628
Iteration 99/1000 | Loss: 0.00002628
Iteration 100/1000 | Loss: 0.00002628
Iteration 101/1000 | Loss: 0.00002628
Iteration 102/1000 | Loss: 0.00002628
Iteration 103/1000 | Loss: 0.00002628
Iteration 104/1000 | Loss: 0.00002628
Iteration 105/1000 | Loss: 0.00002628
Iteration 106/1000 | Loss: 0.00002628
Iteration 107/1000 | Loss: 0.00002628
Iteration 108/1000 | Loss: 0.00002628
Iteration 109/1000 | Loss: 0.00002628
Iteration 110/1000 | Loss: 0.00002628
Iteration 111/1000 | Loss: 0.00002628
Iteration 112/1000 | Loss: 0.00002628
Iteration 113/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.6280169549863786e-05, 2.6280169549863786e-05, 2.6280169549863786e-05, 2.6280169549863786e-05, 2.6280169549863786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6280169549863786e-05

Optimization complete. Final v2v error: 4.32653284072876 mm

Highest mean error: 4.650041580200195 mm for frame 126

Lowest mean error: 3.9292285442352295 mm for frame 27

Saving results

Total time: 30.53174328804016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950181
Iteration 2/25 | Loss: 0.00207129
Iteration 3/25 | Loss: 0.00176166
Iteration 4/25 | Loss: 0.00172202
Iteration 5/25 | Loss: 0.00169426
Iteration 6/25 | Loss: 0.00165079
Iteration 7/25 | Loss: 0.00163895
Iteration 8/25 | Loss: 0.00163104
Iteration 9/25 | Loss: 0.00162704
Iteration 10/25 | Loss: 0.00162578
Iteration 11/25 | Loss: 0.00162554
Iteration 12/25 | Loss: 0.00162502
Iteration 13/25 | Loss: 0.00162663
Iteration 14/25 | Loss: 0.00162235
Iteration 15/25 | Loss: 0.00162184
Iteration 16/25 | Loss: 0.00162176
Iteration 17/25 | Loss: 0.00162163
Iteration 18/25 | Loss: 0.00162141
Iteration 19/25 | Loss: 0.00162138
Iteration 20/25 | Loss: 0.00162138
Iteration 21/25 | Loss: 0.00162138
Iteration 22/25 | Loss: 0.00162138
Iteration 23/25 | Loss: 0.00162137
Iteration 24/25 | Loss: 0.00162137
Iteration 25/25 | Loss: 0.00162137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34931803
Iteration 2/25 | Loss: 0.00187679
Iteration 3/25 | Loss: 0.00187674
Iteration 4/25 | Loss: 0.00187673
Iteration 5/25 | Loss: 0.00187673
Iteration 6/25 | Loss: 0.00187673
Iteration 7/25 | Loss: 0.00187673
Iteration 8/25 | Loss: 0.00187673
Iteration 9/25 | Loss: 0.00187673
Iteration 10/25 | Loss: 0.00187673
Iteration 11/25 | Loss: 0.00187673
Iteration 12/25 | Loss: 0.00187673
Iteration 13/25 | Loss: 0.00187673
Iteration 14/25 | Loss: 0.00187673
Iteration 15/25 | Loss: 0.00187673
Iteration 16/25 | Loss: 0.00187673
Iteration 17/25 | Loss: 0.00187673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018767317524179816, 0.0018767317524179816, 0.0018767317524179816, 0.0018767317524179816, 0.0018767317524179816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018767317524179816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187673
Iteration 2/1000 | Loss: 0.00011514
Iteration 3/1000 | Loss: 0.00006061
Iteration 4/1000 | Loss: 0.00004905
Iteration 5/1000 | Loss: 0.00004268
Iteration 6/1000 | Loss: 0.00003859
Iteration 7/1000 | Loss: 0.00003553
Iteration 8/1000 | Loss: 0.00003411
Iteration 9/1000 | Loss: 0.00003282
Iteration 10/1000 | Loss: 0.00012060
Iteration 11/1000 | Loss: 0.00025481
Iteration 12/1000 | Loss: 0.00009427
Iteration 13/1000 | Loss: 0.00004424
Iteration 14/1000 | Loss: 0.00003313
Iteration 15/1000 | Loss: 0.00003106
Iteration 16/1000 | Loss: 0.00003080
Iteration 17/1000 | Loss: 0.00024886
Iteration 18/1000 | Loss: 0.00003701
Iteration 19/1000 | Loss: 0.00003189
Iteration 20/1000 | Loss: 0.00003049
Iteration 21/1000 | Loss: 0.00002950
Iteration 22/1000 | Loss: 0.00002889
Iteration 23/1000 | Loss: 0.00002833
Iteration 24/1000 | Loss: 0.00002809
Iteration 25/1000 | Loss: 0.00002795
Iteration 26/1000 | Loss: 0.00002795
Iteration 27/1000 | Loss: 0.00002786
Iteration 28/1000 | Loss: 0.00002784
Iteration 29/1000 | Loss: 0.00002784
Iteration 30/1000 | Loss: 0.00002782
Iteration 31/1000 | Loss: 0.00002781
Iteration 32/1000 | Loss: 0.00002780
Iteration 33/1000 | Loss: 0.00002780
Iteration 34/1000 | Loss: 0.00002780
Iteration 35/1000 | Loss: 0.00002780
Iteration 36/1000 | Loss: 0.00002780
Iteration 37/1000 | Loss: 0.00002779
Iteration 38/1000 | Loss: 0.00002779
Iteration 39/1000 | Loss: 0.00002779
Iteration 40/1000 | Loss: 0.00002779
Iteration 41/1000 | Loss: 0.00002779
Iteration 42/1000 | Loss: 0.00002779
Iteration 43/1000 | Loss: 0.00002778
Iteration 44/1000 | Loss: 0.00002778
Iteration 45/1000 | Loss: 0.00002778
Iteration 46/1000 | Loss: 0.00002778
Iteration 47/1000 | Loss: 0.00002777
Iteration 48/1000 | Loss: 0.00002777
Iteration 49/1000 | Loss: 0.00002777
Iteration 50/1000 | Loss: 0.00002777
Iteration 51/1000 | Loss: 0.00002776
Iteration 52/1000 | Loss: 0.00002776
Iteration 53/1000 | Loss: 0.00002776
Iteration 54/1000 | Loss: 0.00002776
Iteration 55/1000 | Loss: 0.00002776
Iteration 56/1000 | Loss: 0.00002776
Iteration 57/1000 | Loss: 0.00002775
Iteration 58/1000 | Loss: 0.00002775
Iteration 59/1000 | Loss: 0.00002775
Iteration 60/1000 | Loss: 0.00002775
Iteration 61/1000 | Loss: 0.00002775
Iteration 62/1000 | Loss: 0.00002774
Iteration 63/1000 | Loss: 0.00002774
Iteration 64/1000 | Loss: 0.00002774
Iteration 65/1000 | Loss: 0.00002774
Iteration 66/1000 | Loss: 0.00002774
Iteration 67/1000 | Loss: 0.00002774
Iteration 68/1000 | Loss: 0.00002774
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002773
Iteration 71/1000 | Loss: 0.00002773
Iteration 72/1000 | Loss: 0.00002773
Iteration 73/1000 | Loss: 0.00002773
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002772
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002772
Iteration 79/1000 | Loss: 0.00002772
Iteration 80/1000 | Loss: 0.00002771
Iteration 81/1000 | Loss: 0.00002771
Iteration 82/1000 | Loss: 0.00002771
Iteration 83/1000 | Loss: 0.00002771
Iteration 84/1000 | Loss: 0.00002771
Iteration 85/1000 | Loss: 0.00002771
Iteration 86/1000 | Loss: 0.00002771
Iteration 87/1000 | Loss: 0.00002770
Iteration 88/1000 | Loss: 0.00002770
Iteration 89/1000 | Loss: 0.00002770
Iteration 90/1000 | Loss: 0.00002769
Iteration 91/1000 | Loss: 0.00002769
Iteration 92/1000 | Loss: 0.00002769
Iteration 93/1000 | Loss: 0.00002768
Iteration 94/1000 | Loss: 0.00002768
Iteration 95/1000 | Loss: 0.00002768
Iteration 96/1000 | Loss: 0.00002767
Iteration 97/1000 | Loss: 0.00002767
Iteration 98/1000 | Loss: 0.00002767
Iteration 99/1000 | Loss: 0.00002766
Iteration 100/1000 | Loss: 0.00002766
Iteration 101/1000 | Loss: 0.00002766
Iteration 102/1000 | Loss: 0.00002766
Iteration 103/1000 | Loss: 0.00002766
Iteration 104/1000 | Loss: 0.00002766
Iteration 105/1000 | Loss: 0.00002766
Iteration 106/1000 | Loss: 0.00002765
Iteration 107/1000 | Loss: 0.00002765
Iteration 108/1000 | Loss: 0.00002765
Iteration 109/1000 | Loss: 0.00002765
Iteration 110/1000 | Loss: 0.00002765
Iteration 111/1000 | Loss: 0.00002765
Iteration 112/1000 | Loss: 0.00002765
Iteration 113/1000 | Loss: 0.00002765
Iteration 114/1000 | Loss: 0.00002765
Iteration 115/1000 | Loss: 0.00002765
Iteration 116/1000 | Loss: 0.00002765
Iteration 117/1000 | Loss: 0.00002765
Iteration 118/1000 | Loss: 0.00002765
Iteration 119/1000 | Loss: 0.00002765
Iteration 120/1000 | Loss: 0.00002765
Iteration 121/1000 | Loss: 0.00002765
Iteration 122/1000 | Loss: 0.00002765
Iteration 123/1000 | Loss: 0.00002765
Iteration 124/1000 | Loss: 0.00002765
Iteration 125/1000 | Loss: 0.00002765
Iteration 126/1000 | Loss: 0.00002765
Iteration 127/1000 | Loss: 0.00002765
Iteration 128/1000 | Loss: 0.00002765
Iteration 129/1000 | Loss: 0.00002765
Iteration 130/1000 | Loss: 0.00002765
Iteration 131/1000 | Loss: 0.00002765
Iteration 132/1000 | Loss: 0.00002765
Iteration 133/1000 | Loss: 0.00002765
Iteration 134/1000 | Loss: 0.00002765
Iteration 135/1000 | Loss: 0.00002765
Iteration 136/1000 | Loss: 0.00002765
Iteration 137/1000 | Loss: 0.00002765
Iteration 138/1000 | Loss: 0.00002765
Iteration 139/1000 | Loss: 0.00002765
Iteration 140/1000 | Loss: 0.00002765
Iteration 141/1000 | Loss: 0.00002765
Iteration 142/1000 | Loss: 0.00002765
Iteration 143/1000 | Loss: 0.00002765
Iteration 144/1000 | Loss: 0.00002765
Iteration 145/1000 | Loss: 0.00002765
Iteration 146/1000 | Loss: 0.00002765
Iteration 147/1000 | Loss: 0.00002765
Iteration 148/1000 | Loss: 0.00002765
Iteration 149/1000 | Loss: 0.00002765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.7652386052068323e-05, 2.7652386052068323e-05, 2.7652386052068323e-05, 2.7652386052068323e-05, 2.7652386052068323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7652386052068323e-05

Optimization complete. Final v2v error: 4.448512554168701 mm

Highest mean error: 15.402485847473145 mm for frame 57

Lowest mean error: 3.8570868968963623 mm for frame 150

Saving results

Total time: 86.83597779273987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660880
Iteration 2/25 | Loss: 0.00160588
Iteration 3/25 | Loss: 0.00152769
Iteration 4/25 | Loss: 0.00151887
Iteration 5/25 | Loss: 0.00151454
Iteration 6/25 | Loss: 0.00151340
Iteration 7/25 | Loss: 0.00151340
Iteration 8/25 | Loss: 0.00151340
Iteration 9/25 | Loss: 0.00151340
Iteration 10/25 | Loss: 0.00151340
Iteration 11/25 | Loss: 0.00151340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015133979031816125, 0.0015133979031816125, 0.0015133979031816125, 0.0015133979031816125, 0.0015133979031816125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015133979031816125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.83790159
Iteration 2/25 | Loss: 0.00212439
Iteration 3/25 | Loss: 0.00212439
Iteration 4/25 | Loss: 0.00212439
Iteration 5/25 | Loss: 0.00212439
Iteration 6/25 | Loss: 0.00212439
Iteration 7/25 | Loss: 0.00212439
Iteration 8/25 | Loss: 0.00212439
Iteration 9/25 | Loss: 0.00212439
Iteration 10/25 | Loss: 0.00212439
Iteration 11/25 | Loss: 0.00212439
Iteration 12/25 | Loss: 0.00212439
Iteration 13/25 | Loss: 0.00212439
Iteration 14/25 | Loss: 0.00212439
Iteration 15/25 | Loss: 0.00212439
Iteration 16/25 | Loss: 0.00212439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002124385442584753, 0.002124385442584753, 0.002124385442584753, 0.002124385442584753, 0.002124385442584753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002124385442584753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212439
Iteration 2/1000 | Loss: 0.00005194
Iteration 3/1000 | Loss: 0.00003646
Iteration 4/1000 | Loss: 0.00003186
Iteration 5/1000 | Loss: 0.00002937
Iteration 6/1000 | Loss: 0.00002774
Iteration 7/1000 | Loss: 0.00002680
Iteration 8/1000 | Loss: 0.00002620
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002546
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002531
Iteration 13/1000 | Loss: 0.00002529
Iteration 14/1000 | Loss: 0.00002524
Iteration 15/1000 | Loss: 0.00002523
Iteration 16/1000 | Loss: 0.00002521
Iteration 17/1000 | Loss: 0.00002520
Iteration 18/1000 | Loss: 0.00002519
Iteration 19/1000 | Loss: 0.00002518
Iteration 20/1000 | Loss: 0.00002517
Iteration 21/1000 | Loss: 0.00002515
Iteration 22/1000 | Loss: 0.00002513
Iteration 23/1000 | Loss: 0.00002512
Iteration 24/1000 | Loss: 0.00002511
Iteration 25/1000 | Loss: 0.00002510
Iteration 26/1000 | Loss: 0.00002510
Iteration 27/1000 | Loss: 0.00002510
Iteration 28/1000 | Loss: 0.00002509
Iteration 29/1000 | Loss: 0.00002508
Iteration 30/1000 | Loss: 0.00002508
Iteration 31/1000 | Loss: 0.00002508
Iteration 32/1000 | Loss: 0.00002507
Iteration 33/1000 | Loss: 0.00002507
Iteration 34/1000 | Loss: 0.00002507
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002507
Iteration 37/1000 | Loss: 0.00002507
Iteration 38/1000 | Loss: 0.00002506
Iteration 39/1000 | Loss: 0.00002506
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002505
Iteration 43/1000 | Loss: 0.00002505
Iteration 44/1000 | Loss: 0.00002505
Iteration 45/1000 | Loss: 0.00002504
Iteration 46/1000 | Loss: 0.00002504
Iteration 47/1000 | Loss: 0.00002504
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002502
Iteration 53/1000 | Loss: 0.00002502
Iteration 54/1000 | Loss: 0.00002502
Iteration 55/1000 | Loss: 0.00002501
Iteration 56/1000 | Loss: 0.00002501
Iteration 57/1000 | Loss: 0.00002501
Iteration 58/1000 | Loss: 0.00002501
Iteration 59/1000 | Loss: 0.00002500
Iteration 60/1000 | Loss: 0.00002500
Iteration 61/1000 | Loss: 0.00002500
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002500
Iteration 65/1000 | Loss: 0.00002500
Iteration 66/1000 | Loss: 0.00002500
Iteration 67/1000 | Loss: 0.00002499
Iteration 68/1000 | Loss: 0.00002499
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002499
Iteration 71/1000 | Loss: 0.00002499
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00002498
Iteration 75/1000 | Loss: 0.00002498
Iteration 76/1000 | Loss: 0.00002498
Iteration 77/1000 | Loss: 0.00002498
Iteration 78/1000 | Loss: 0.00002498
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002498
Iteration 85/1000 | Loss: 0.00002498
Iteration 86/1000 | Loss: 0.00002498
Iteration 87/1000 | Loss: 0.00002497
Iteration 88/1000 | Loss: 0.00002497
Iteration 89/1000 | Loss: 0.00002497
Iteration 90/1000 | Loss: 0.00002497
Iteration 91/1000 | Loss: 0.00002497
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002497
Iteration 94/1000 | Loss: 0.00002497
Iteration 95/1000 | Loss: 0.00002497
Iteration 96/1000 | Loss: 0.00002497
Iteration 97/1000 | Loss: 0.00002496
Iteration 98/1000 | Loss: 0.00002496
Iteration 99/1000 | Loss: 0.00002496
Iteration 100/1000 | Loss: 0.00002496
Iteration 101/1000 | Loss: 0.00002496
Iteration 102/1000 | Loss: 0.00002496
Iteration 103/1000 | Loss: 0.00002496
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002495
Iteration 106/1000 | Loss: 0.00002495
Iteration 107/1000 | Loss: 0.00002495
Iteration 108/1000 | Loss: 0.00002494
Iteration 109/1000 | Loss: 0.00002494
Iteration 110/1000 | Loss: 0.00002494
Iteration 111/1000 | Loss: 0.00002494
Iteration 112/1000 | Loss: 0.00002494
Iteration 113/1000 | Loss: 0.00002494
Iteration 114/1000 | Loss: 0.00002494
Iteration 115/1000 | Loss: 0.00002494
Iteration 116/1000 | Loss: 0.00002494
Iteration 117/1000 | Loss: 0.00002494
Iteration 118/1000 | Loss: 0.00002494
Iteration 119/1000 | Loss: 0.00002494
Iteration 120/1000 | Loss: 0.00002494
Iteration 121/1000 | Loss: 0.00002494
Iteration 122/1000 | Loss: 0.00002493
Iteration 123/1000 | Loss: 0.00002493
Iteration 124/1000 | Loss: 0.00002493
Iteration 125/1000 | Loss: 0.00002493
Iteration 126/1000 | Loss: 0.00002493
Iteration 127/1000 | Loss: 0.00002493
Iteration 128/1000 | Loss: 0.00002493
Iteration 129/1000 | Loss: 0.00002493
Iteration 130/1000 | Loss: 0.00002493
Iteration 131/1000 | Loss: 0.00002493
Iteration 132/1000 | Loss: 0.00002493
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002493
Iteration 135/1000 | Loss: 0.00002493
Iteration 136/1000 | Loss: 0.00002493
Iteration 137/1000 | Loss: 0.00002493
Iteration 138/1000 | Loss: 0.00002493
Iteration 139/1000 | Loss: 0.00002493
Iteration 140/1000 | Loss: 0.00002493
Iteration 141/1000 | Loss: 0.00002493
Iteration 142/1000 | Loss: 0.00002492
Iteration 143/1000 | Loss: 0.00002492
Iteration 144/1000 | Loss: 0.00002492
Iteration 145/1000 | Loss: 0.00002492
Iteration 146/1000 | Loss: 0.00002492
Iteration 147/1000 | Loss: 0.00002492
Iteration 148/1000 | Loss: 0.00002492
Iteration 149/1000 | Loss: 0.00002492
Iteration 150/1000 | Loss: 0.00002492
Iteration 151/1000 | Loss: 0.00002492
Iteration 152/1000 | Loss: 0.00002492
Iteration 153/1000 | Loss: 0.00002492
Iteration 154/1000 | Loss: 0.00002492
Iteration 155/1000 | Loss: 0.00002492
Iteration 156/1000 | Loss: 0.00002492
Iteration 157/1000 | Loss: 0.00002491
Iteration 158/1000 | Loss: 0.00002491
Iteration 159/1000 | Loss: 0.00002491
Iteration 160/1000 | Loss: 0.00002491
Iteration 161/1000 | Loss: 0.00002491
Iteration 162/1000 | Loss: 0.00002491
Iteration 163/1000 | Loss: 0.00002491
Iteration 164/1000 | Loss: 0.00002491
Iteration 165/1000 | Loss: 0.00002491
Iteration 166/1000 | Loss: 0.00002491
Iteration 167/1000 | Loss: 0.00002491
Iteration 168/1000 | Loss: 0.00002491
Iteration 169/1000 | Loss: 0.00002491
Iteration 170/1000 | Loss: 0.00002491
Iteration 171/1000 | Loss: 0.00002491
Iteration 172/1000 | Loss: 0.00002491
Iteration 173/1000 | Loss: 0.00002491
Iteration 174/1000 | Loss: 0.00002491
Iteration 175/1000 | Loss: 0.00002491
Iteration 176/1000 | Loss: 0.00002491
Iteration 177/1000 | Loss: 0.00002491
Iteration 178/1000 | Loss: 0.00002491
Iteration 179/1000 | Loss: 0.00002491
Iteration 180/1000 | Loss: 0.00002491
Iteration 181/1000 | Loss: 0.00002491
Iteration 182/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.491392478987109e-05, 2.491392478987109e-05, 2.491392478987109e-05, 2.491392478987109e-05, 2.491392478987109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.491392478987109e-05

Optimization complete. Final v2v error: 4.310680866241455 mm

Highest mean error: 4.650528430938721 mm for frame 41

Lowest mean error: 4.001549243927002 mm for frame 1

Saving results

Total time: 35.75462865829468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433443
Iteration 2/25 | Loss: 0.00172147
Iteration 3/25 | Loss: 0.00161533
Iteration 4/25 | Loss: 0.00160036
Iteration 5/25 | Loss: 0.00159491
Iteration 6/25 | Loss: 0.00159369
Iteration 7/25 | Loss: 0.00159369
Iteration 8/25 | Loss: 0.00159369
Iteration 9/25 | Loss: 0.00159369
Iteration 10/25 | Loss: 0.00159369
Iteration 11/25 | Loss: 0.00159369
Iteration 12/25 | Loss: 0.00159369
Iteration 13/25 | Loss: 0.00159369
Iteration 14/25 | Loss: 0.00159369
Iteration 15/25 | Loss: 0.00159369
Iteration 16/25 | Loss: 0.00159369
Iteration 17/25 | Loss: 0.00159369
Iteration 18/25 | Loss: 0.00159369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015936880372464657, 0.0015936880372464657, 0.0015936880372464657, 0.0015936880372464657, 0.0015936880372464657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015936880372464657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20988333
Iteration 2/25 | Loss: 0.00226709
Iteration 3/25 | Loss: 0.00226709
Iteration 4/25 | Loss: 0.00226709
Iteration 5/25 | Loss: 0.00226709
Iteration 6/25 | Loss: 0.00226709
Iteration 7/25 | Loss: 0.00226709
Iteration 8/25 | Loss: 0.00226709
Iteration 9/25 | Loss: 0.00226709
Iteration 10/25 | Loss: 0.00226709
Iteration 11/25 | Loss: 0.00226709
Iteration 12/25 | Loss: 0.00226709
Iteration 13/25 | Loss: 0.00226709
Iteration 14/25 | Loss: 0.00226709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0022670873440802097, 0.0022670873440802097, 0.0022670873440802097, 0.0022670873440802097, 0.0022670873440802097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022670873440802097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226709
Iteration 2/1000 | Loss: 0.00009942
Iteration 3/1000 | Loss: 0.00006119
Iteration 4/1000 | Loss: 0.00004564
Iteration 5/1000 | Loss: 0.00003996
Iteration 6/1000 | Loss: 0.00003606
Iteration 7/1000 | Loss: 0.00003363
Iteration 8/1000 | Loss: 0.00003242
Iteration 9/1000 | Loss: 0.00003139
Iteration 10/1000 | Loss: 0.00003045
Iteration 11/1000 | Loss: 0.00002984
Iteration 12/1000 | Loss: 0.00002937
Iteration 13/1000 | Loss: 0.00002895
Iteration 14/1000 | Loss: 0.00002865
Iteration 15/1000 | Loss: 0.00002862
Iteration 16/1000 | Loss: 0.00002843
Iteration 17/1000 | Loss: 0.00002842
Iteration 18/1000 | Loss: 0.00002835
Iteration 19/1000 | Loss: 0.00002829
Iteration 20/1000 | Loss: 0.00002828
Iteration 21/1000 | Loss: 0.00002826
Iteration 22/1000 | Loss: 0.00002826
Iteration 23/1000 | Loss: 0.00002826
Iteration 24/1000 | Loss: 0.00002825
Iteration 25/1000 | Loss: 0.00002824
Iteration 26/1000 | Loss: 0.00002824
Iteration 27/1000 | Loss: 0.00002823
Iteration 28/1000 | Loss: 0.00002823
Iteration 29/1000 | Loss: 0.00002822
Iteration 30/1000 | Loss: 0.00002822
Iteration 31/1000 | Loss: 0.00002821
Iteration 32/1000 | Loss: 0.00002820
Iteration 33/1000 | Loss: 0.00002820
Iteration 34/1000 | Loss: 0.00002818
Iteration 35/1000 | Loss: 0.00002817
Iteration 36/1000 | Loss: 0.00002817
Iteration 37/1000 | Loss: 0.00002816
Iteration 38/1000 | Loss: 0.00002816
Iteration 39/1000 | Loss: 0.00002814
Iteration 40/1000 | Loss: 0.00002814
Iteration 41/1000 | Loss: 0.00002814
Iteration 42/1000 | Loss: 0.00002814
Iteration 43/1000 | Loss: 0.00002814
Iteration 44/1000 | Loss: 0.00002814
Iteration 45/1000 | Loss: 0.00002814
Iteration 46/1000 | Loss: 0.00002814
Iteration 47/1000 | Loss: 0.00002814
Iteration 48/1000 | Loss: 0.00002814
Iteration 49/1000 | Loss: 0.00002813
Iteration 50/1000 | Loss: 0.00002813
Iteration 51/1000 | Loss: 0.00002813
Iteration 52/1000 | Loss: 0.00002812
Iteration 53/1000 | Loss: 0.00002812
Iteration 54/1000 | Loss: 0.00002812
Iteration 55/1000 | Loss: 0.00002811
Iteration 56/1000 | Loss: 0.00002811
Iteration 57/1000 | Loss: 0.00002811
Iteration 58/1000 | Loss: 0.00002811
Iteration 59/1000 | Loss: 0.00002811
Iteration 60/1000 | Loss: 0.00002810
Iteration 61/1000 | Loss: 0.00002810
Iteration 62/1000 | Loss: 0.00002810
Iteration 63/1000 | Loss: 0.00002810
Iteration 64/1000 | Loss: 0.00002809
Iteration 65/1000 | Loss: 0.00002809
Iteration 66/1000 | Loss: 0.00002809
Iteration 67/1000 | Loss: 0.00002808
Iteration 68/1000 | Loss: 0.00002808
Iteration 69/1000 | Loss: 0.00002808
Iteration 70/1000 | Loss: 0.00002807
Iteration 71/1000 | Loss: 0.00002807
Iteration 72/1000 | Loss: 0.00002806
Iteration 73/1000 | Loss: 0.00002806
Iteration 74/1000 | Loss: 0.00002806
Iteration 75/1000 | Loss: 0.00002806
Iteration 76/1000 | Loss: 0.00002806
Iteration 77/1000 | Loss: 0.00002806
Iteration 78/1000 | Loss: 0.00002806
Iteration 79/1000 | Loss: 0.00002806
Iteration 80/1000 | Loss: 0.00002806
Iteration 81/1000 | Loss: 0.00002805
Iteration 82/1000 | Loss: 0.00002805
Iteration 83/1000 | Loss: 0.00002804
Iteration 84/1000 | Loss: 0.00002804
Iteration 85/1000 | Loss: 0.00002804
Iteration 86/1000 | Loss: 0.00002803
Iteration 87/1000 | Loss: 0.00002803
Iteration 88/1000 | Loss: 0.00002803
Iteration 89/1000 | Loss: 0.00002803
Iteration 90/1000 | Loss: 0.00002802
Iteration 91/1000 | Loss: 0.00002802
Iteration 92/1000 | Loss: 0.00002802
Iteration 93/1000 | Loss: 0.00002802
Iteration 94/1000 | Loss: 0.00002802
Iteration 95/1000 | Loss: 0.00002802
Iteration 96/1000 | Loss: 0.00002802
Iteration 97/1000 | Loss: 0.00002802
Iteration 98/1000 | Loss: 0.00002802
Iteration 99/1000 | Loss: 0.00002801
Iteration 100/1000 | Loss: 0.00002801
Iteration 101/1000 | Loss: 0.00002801
Iteration 102/1000 | Loss: 0.00002801
Iteration 103/1000 | Loss: 0.00002801
Iteration 104/1000 | Loss: 0.00002801
Iteration 105/1000 | Loss: 0.00002801
Iteration 106/1000 | Loss: 0.00002801
Iteration 107/1000 | Loss: 0.00002801
Iteration 108/1000 | Loss: 0.00002801
Iteration 109/1000 | Loss: 0.00002801
Iteration 110/1000 | Loss: 0.00002801
Iteration 111/1000 | Loss: 0.00002801
Iteration 112/1000 | Loss: 0.00002801
Iteration 113/1000 | Loss: 0.00002801
Iteration 114/1000 | Loss: 0.00002801
Iteration 115/1000 | Loss: 0.00002801
Iteration 116/1000 | Loss: 0.00002801
Iteration 117/1000 | Loss: 0.00002801
Iteration 118/1000 | Loss: 0.00002801
Iteration 119/1000 | Loss: 0.00002801
Iteration 120/1000 | Loss: 0.00002801
Iteration 121/1000 | Loss: 0.00002801
Iteration 122/1000 | Loss: 0.00002801
Iteration 123/1000 | Loss: 0.00002801
Iteration 124/1000 | Loss: 0.00002801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.800993388518691e-05, 2.800993388518691e-05, 2.800993388518691e-05, 2.800993388518691e-05, 2.800993388518691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.800993388518691e-05

Optimization complete. Final v2v error: 4.540985584259033 mm

Highest mean error: 5.258927345275879 mm for frame 107

Lowest mean error: 3.931087017059326 mm for frame 12

Saving results

Total time: 40.81966423988342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433315
Iteration 2/25 | Loss: 0.00165601
Iteration 3/25 | Loss: 0.00152754
Iteration 4/25 | Loss: 0.00151305
Iteration 5/25 | Loss: 0.00150905
Iteration 6/25 | Loss: 0.00150820
Iteration 7/25 | Loss: 0.00150820
Iteration 8/25 | Loss: 0.00150820
Iteration 9/25 | Loss: 0.00150820
Iteration 10/25 | Loss: 0.00150820
Iteration 11/25 | Loss: 0.00150820
Iteration 12/25 | Loss: 0.00150820
Iteration 13/25 | Loss: 0.00150820
Iteration 14/25 | Loss: 0.00150820
Iteration 15/25 | Loss: 0.00150820
Iteration 16/25 | Loss: 0.00150820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015081977471709251, 0.0015081977471709251, 0.0015081977471709251, 0.0015081977471709251, 0.0015081977471709251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015081977471709251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27205932
Iteration 2/25 | Loss: 0.00185478
Iteration 3/25 | Loss: 0.00185478
Iteration 4/25 | Loss: 0.00185478
Iteration 5/25 | Loss: 0.00185478
Iteration 6/25 | Loss: 0.00185478
Iteration 7/25 | Loss: 0.00185478
Iteration 8/25 | Loss: 0.00185478
Iteration 9/25 | Loss: 0.00185478
Iteration 10/25 | Loss: 0.00185478
Iteration 11/25 | Loss: 0.00185478
Iteration 12/25 | Loss: 0.00185478
Iteration 13/25 | Loss: 0.00185478
Iteration 14/25 | Loss: 0.00185478
Iteration 15/25 | Loss: 0.00185478
Iteration 16/25 | Loss: 0.00185478
Iteration 17/25 | Loss: 0.00185478
Iteration 18/25 | Loss: 0.00185478
Iteration 19/25 | Loss: 0.00185478
Iteration 20/25 | Loss: 0.00185478
Iteration 21/25 | Loss: 0.00185478
Iteration 22/25 | Loss: 0.00185478
Iteration 23/25 | Loss: 0.00185478
Iteration 24/25 | Loss: 0.00185478
Iteration 25/25 | Loss: 0.00185478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185478
Iteration 2/1000 | Loss: 0.00005563
Iteration 3/1000 | Loss: 0.00003745
Iteration 4/1000 | Loss: 0.00003005
Iteration 5/1000 | Loss: 0.00002659
Iteration 6/1000 | Loss: 0.00002498
Iteration 7/1000 | Loss: 0.00002393
Iteration 8/1000 | Loss: 0.00002326
Iteration 9/1000 | Loss: 0.00002286
Iteration 10/1000 | Loss: 0.00002248
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002202
Iteration 14/1000 | Loss: 0.00002200
Iteration 15/1000 | Loss: 0.00002199
Iteration 16/1000 | Loss: 0.00002199
Iteration 17/1000 | Loss: 0.00002199
Iteration 18/1000 | Loss: 0.00002199
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002199
Iteration 22/1000 | Loss: 0.00002199
Iteration 23/1000 | Loss: 0.00002199
Iteration 24/1000 | Loss: 0.00002199
Iteration 25/1000 | Loss: 0.00002199
Iteration 26/1000 | Loss: 0.00002197
Iteration 27/1000 | Loss: 0.00002197
Iteration 28/1000 | Loss: 0.00002196
Iteration 29/1000 | Loss: 0.00002195
Iteration 30/1000 | Loss: 0.00002195
Iteration 31/1000 | Loss: 0.00002195
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002195
Iteration 34/1000 | Loss: 0.00002195
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002193
Iteration 37/1000 | Loss: 0.00002193
Iteration 38/1000 | Loss: 0.00002192
Iteration 39/1000 | Loss: 0.00002192
Iteration 40/1000 | Loss: 0.00002192
Iteration 41/1000 | Loss: 0.00002191
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00002190
Iteration 44/1000 | Loss: 0.00002190
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002190
Iteration 47/1000 | Loss: 0.00002190
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002190
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002189
Iteration 54/1000 | Loss: 0.00002189
Iteration 55/1000 | Loss: 0.00002189
Iteration 56/1000 | Loss: 0.00002189
Iteration 57/1000 | Loss: 0.00002189
Iteration 58/1000 | Loss: 0.00002189
Iteration 59/1000 | Loss: 0.00002189
Iteration 60/1000 | Loss: 0.00002189
Iteration 61/1000 | Loss: 0.00002189
Iteration 62/1000 | Loss: 0.00002189
Iteration 63/1000 | Loss: 0.00002189
Iteration 64/1000 | Loss: 0.00002189
Iteration 65/1000 | Loss: 0.00002189
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002189
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002189
Iteration 72/1000 | Loss: 0.00002189
Iteration 73/1000 | Loss: 0.00002189
Iteration 74/1000 | Loss: 0.00002189
Iteration 75/1000 | Loss: 0.00002189
Iteration 76/1000 | Loss: 0.00002189
Iteration 77/1000 | Loss: 0.00002189
Iteration 78/1000 | Loss: 0.00002189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.1892770746489987e-05, 2.1892770746489987e-05, 2.1892770746489987e-05, 2.1892770746489987e-05, 2.1892770746489987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1892770746489987e-05

Optimization complete. Final v2v error: 3.9164018630981445 mm

Highest mean error: 4.328708171844482 mm for frame 18

Lowest mean error: 3.5357420444488525 mm for frame 3

Saving results

Total time: 29.230426788330078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892528
Iteration 2/25 | Loss: 0.00184683
Iteration 3/25 | Loss: 0.00163438
Iteration 4/25 | Loss: 0.00160066
Iteration 5/25 | Loss: 0.00159524
Iteration 6/25 | Loss: 0.00159480
Iteration 7/25 | Loss: 0.00159480
Iteration 8/25 | Loss: 0.00159480
Iteration 9/25 | Loss: 0.00159480
Iteration 10/25 | Loss: 0.00159480
Iteration 11/25 | Loss: 0.00159480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015947988722473383, 0.0015947988722473383, 0.0015947988722473383, 0.0015947988722473383, 0.0015947988722473383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015947988722473383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20776105
Iteration 2/25 | Loss: 0.00200607
Iteration 3/25 | Loss: 0.00200606
Iteration 4/25 | Loss: 0.00200606
Iteration 5/25 | Loss: 0.00200606
Iteration 6/25 | Loss: 0.00200606
Iteration 7/25 | Loss: 0.00200606
Iteration 8/25 | Loss: 0.00200606
Iteration 9/25 | Loss: 0.00200606
Iteration 10/25 | Loss: 0.00200606
Iteration 11/25 | Loss: 0.00200606
Iteration 12/25 | Loss: 0.00200606
Iteration 13/25 | Loss: 0.00200606
Iteration 14/25 | Loss: 0.00200606
Iteration 15/25 | Loss: 0.00200606
Iteration 16/25 | Loss: 0.00200606
Iteration 17/25 | Loss: 0.00200606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020060595124959946, 0.0020060595124959946, 0.0020060595124959946, 0.0020060595124959946, 0.0020060595124959946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020060595124959946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200606
Iteration 2/1000 | Loss: 0.00008772
Iteration 3/1000 | Loss: 0.00005565
Iteration 4/1000 | Loss: 0.00004074
Iteration 5/1000 | Loss: 0.00003545
Iteration 6/1000 | Loss: 0.00003227
Iteration 7/1000 | Loss: 0.00002958
Iteration 8/1000 | Loss: 0.00002864
Iteration 9/1000 | Loss: 0.00002766
Iteration 10/1000 | Loss: 0.00002689
Iteration 11/1000 | Loss: 0.00002616
Iteration 12/1000 | Loss: 0.00002574
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002521
Iteration 15/1000 | Loss: 0.00002502
Iteration 16/1000 | Loss: 0.00002499
Iteration 17/1000 | Loss: 0.00002494
Iteration 18/1000 | Loss: 0.00002488
Iteration 19/1000 | Loss: 0.00002485
Iteration 20/1000 | Loss: 0.00002483
Iteration 21/1000 | Loss: 0.00002480
Iteration 22/1000 | Loss: 0.00002477
Iteration 23/1000 | Loss: 0.00002475
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002473
Iteration 26/1000 | Loss: 0.00002473
Iteration 27/1000 | Loss: 0.00002472
Iteration 28/1000 | Loss: 0.00002472
Iteration 29/1000 | Loss: 0.00002472
Iteration 30/1000 | Loss: 0.00002471
Iteration 31/1000 | Loss: 0.00002471
Iteration 32/1000 | Loss: 0.00002470
Iteration 33/1000 | Loss: 0.00002469
Iteration 34/1000 | Loss: 0.00002469
Iteration 35/1000 | Loss: 0.00002468
Iteration 36/1000 | Loss: 0.00002468
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002467
Iteration 39/1000 | Loss: 0.00002466
Iteration 40/1000 | Loss: 0.00002466
Iteration 41/1000 | Loss: 0.00002463
Iteration 42/1000 | Loss: 0.00002463
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002460
Iteration 46/1000 | Loss: 0.00002460
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00002459
Iteration 49/1000 | Loss: 0.00002459
Iteration 50/1000 | Loss: 0.00002458
Iteration 51/1000 | Loss: 0.00002458
Iteration 52/1000 | Loss: 0.00002458
Iteration 53/1000 | Loss: 0.00002457
Iteration 54/1000 | Loss: 0.00002457
Iteration 55/1000 | Loss: 0.00002456
Iteration 56/1000 | Loss: 0.00002456
Iteration 57/1000 | Loss: 0.00002456
Iteration 58/1000 | Loss: 0.00002456
Iteration 59/1000 | Loss: 0.00002455
Iteration 60/1000 | Loss: 0.00002455
Iteration 61/1000 | Loss: 0.00002455
Iteration 62/1000 | Loss: 0.00002454
Iteration 63/1000 | Loss: 0.00002454
Iteration 64/1000 | Loss: 0.00002454
Iteration 65/1000 | Loss: 0.00002454
Iteration 66/1000 | Loss: 0.00002454
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00002453
Iteration 70/1000 | Loss: 0.00002452
Iteration 71/1000 | Loss: 0.00002452
Iteration 72/1000 | Loss: 0.00002452
Iteration 73/1000 | Loss: 0.00002452
Iteration 74/1000 | Loss: 0.00002452
Iteration 75/1000 | Loss: 0.00002451
Iteration 76/1000 | Loss: 0.00002451
Iteration 77/1000 | Loss: 0.00002451
Iteration 78/1000 | Loss: 0.00002451
Iteration 79/1000 | Loss: 0.00002451
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002451
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002450
Iteration 85/1000 | Loss: 0.00002450
Iteration 86/1000 | Loss: 0.00002450
Iteration 87/1000 | Loss: 0.00002450
Iteration 88/1000 | Loss: 0.00002450
Iteration 89/1000 | Loss: 0.00002450
Iteration 90/1000 | Loss: 0.00002450
Iteration 91/1000 | Loss: 0.00002450
Iteration 92/1000 | Loss: 0.00002450
Iteration 93/1000 | Loss: 0.00002450
Iteration 94/1000 | Loss: 0.00002450
Iteration 95/1000 | Loss: 0.00002450
Iteration 96/1000 | Loss: 0.00002450
Iteration 97/1000 | Loss: 0.00002449
Iteration 98/1000 | Loss: 0.00002449
Iteration 99/1000 | Loss: 0.00002449
Iteration 100/1000 | Loss: 0.00002449
Iteration 101/1000 | Loss: 0.00002449
Iteration 102/1000 | Loss: 0.00002449
Iteration 103/1000 | Loss: 0.00002448
Iteration 104/1000 | Loss: 0.00002448
Iteration 105/1000 | Loss: 0.00002448
Iteration 106/1000 | Loss: 0.00002448
Iteration 107/1000 | Loss: 0.00002448
Iteration 108/1000 | Loss: 0.00002447
Iteration 109/1000 | Loss: 0.00002447
Iteration 110/1000 | Loss: 0.00002447
Iteration 111/1000 | Loss: 0.00002447
Iteration 112/1000 | Loss: 0.00002447
Iteration 113/1000 | Loss: 0.00002447
Iteration 114/1000 | Loss: 0.00002447
Iteration 115/1000 | Loss: 0.00002447
Iteration 116/1000 | Loss: 0.00002447
Iteration 117/1000 | Loss: 0.00002447
Iteration 118/1000 | Loss: 0.00002447
Iteration 119/1000 | Loss: 0.00002447
Iteration 120/1000 | Loss: 0.00002447
Iteration 121/1000 | Loss: 0.00002446
Iteration 122/1000 | Loss: 0.00002446
Iteration 123/1000 | Loss: 0.00002446
Iteration 124/1000 | Loss: 0.00002446
Iteration 125/1000 | Loss: 0.00002446
Iteration 126/1000 | Loss: 0.00002446
Iteration 127/1000 | Loss: 0.00002446
Iteration 128/1000 | Loss: 0.00002446
Iteration 129/1000 | Loss: 0.00002446
Iteration 130/1000 | Loss: 0.00002446
Iteration 131/1000 | Loss: 0.00002446
Iteration 132/1000 | Loss: 0.00002446
Iteration 133/1000 | Loss: 0.00002446
Iteration 134/1000 | Loss: 0.00002446
Iteration 135/1000 | Loss: 0.00002446
Iteration 136/1000 | Loss: 0.00002446
Iteration 137/1000 | Loss: 0.00002446
Iteration 138/1000 | Loss: 0.00002446
Iteration 139/1000 | Loss: 0.00002446
Iteration 140/1000 | Loss: 0.00002446
Iteration 141/1000 | Loss: 0.00002446
Iteration 142/1000 | Loss: 0.00002446
Iteration 143/1000 | Loss: 0.00002446
Iteration 144/1000 | Loss: 0.00002446
Iteration 145/1000 | Loss: 0.00002446
Iteration 146/1000 | Loss: 0.00002446
Iteration 147/1000 | Loss: 0.00002446
Iteration 148/1000 | Loss: 0.00002446
Iteration 149/1000 | Loss: 0.00002446
Iteration 150/1000 | Loss: 0.00002446
Iteration 151/1000 | Loss: 0.00002446
Iteration 152/1000 | Loss: 0.00002446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.4457411200273782e-05, 2.4457411200273782e-05, 2.4457411200273782e-05, 2.4457411200273782e-05, 2.4457411200273782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4457411200273782e-05

Optimization complete. Final v2v error: 4.3288702964782715 mm

Highest mean error: 5.125171661376953 mm for frame 152

Lowest mean error: 3.8620874881744385 mm for frame 33

Saving results

Total time: 46.94096374511719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934380
Iteration 2/25 | Loss: 0.00244001
Iteration 3/25 | Loss: 0.00209177
Iteration 4/25 | Loss: 0.00172800
Iteration 5/25 | Loss: 0.00171935
Iteration 6/25 | Loss: 0.00173473
Iteration 7/25 | Loss: 0.00169824
Iteration 8/25 | Loss: 0.00168088
Iteration 9/25 | Loss: 0.00165477
Iteration 10/25 | Loss: 0.00163868
Iteration 11/25 | Loss: 0.00162759
Iteration 12/25 | Loss: 0.00162745
Iteration 13/25 | Loss: 0.00162637
Iteration 14/25 | Loss: 0.00162462
Iteration 15/25 | Loss: 0.00162359
Iteration 16/25 | Loss: 0.00162568
Iteration 17/25 | Loss: 0.00162161
Iteration 18/25 | Loss: 0.00162096
Iteration 19/25 | Loss: 0.00162297
Iteration 20/25 | Loss: 0.00161826
Iteration 21/25 | Loss: 0.00161415
Iteration 22/25 | Loss: 0.00162021
Iteration 23/25 | Loss: 0.00161267
Iteration 24/25 | Loss: 0.00161739
Iteration 25/25 | Loss: 0.00161722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66932869
Iteration 2/25 | Loss: 0.00379395
Iteration 3/25 | Loss: 0.00379395
Iteration 4/25 | Loss: 0.00379394
Iteration 5/25 | Loss: 0.00379394
Iteration 6/25 | Loss: 0.00379394
Iteration 7/25 | Loss: 0.00379394
Iteration 8/25 | Loss: 0.00379394
Iteration 9/25 | Loss: 0.00379394
Iteration 10/25 | Loss: 0.00379394
Iteration 11/25 | Loss: 0.00379394
Iteration 12/25 | Loss: 0.00379394
Iteration 13/25 | Loss: 0.00379394
Iteration 14/25 | Loss: 0.00379394
Iteration 15/25 | Loss: 0.00379394
Iteration 16/25 | Loss: 0.00379394
Iteration 17/25 | Loss: 0.00379394
Iteration 18/25 | Loss: 0.00379394
Iteration 19/25 | Loss: 0.00379394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003793943440541625, 0.003793943440541625, 0.003793943440541625, 0.003793943440541625, 0.003793943440541625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003793943440541625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00379394
Iteration 2/1000 | Loss: 0.00040133
Iteration 3/1000 | Loss: 0.00207837
Iteration 4/1000 | Loss: 0.00180193
Iteration 5/1000 | Loss: 0.00188965
Iteration 6/1000 | Loss: 0.00171339
Iteration 7/1000 | Loss: 0.00029015
Iteration 8/1000 | Loss: 0.00011724
Iteration 9/1000 | Loss: 0.00043473
Iteration 10/1000 | Loss: 0.00007905
Iteration 11/1000 | Loss: 0.00078793
Iteration 12/1000 | Loss: 0.00013529
Iteration 13/1000 | Loss: 0.00073458
Iteration 14/1000 | Loss: 0.00014438
Iteration 15/1000 | Loss: 0.00006782
Iteration 16/1000 | Loss: 0.00075749
Iteration 17/1000 | Loss: 0.00084215
Iteration 18/1000 | Loss: 0.00082649
Iteration 19/1000 | Loss: 0.00016313
Iteration 20/1000 | Loss: 0.00004960
Iteration 21/1000 | Loss: 0.00111470
Iteration 22/1000 | Loss: 0.00023136
Iteration 23/1000 | Loss: 0.00100051
Iteration 24/1000 | Loss: 0.00017817
Iteration 25/1000 | Loss: 0.00005789
Iteration 26/1000 | Loss: 0.00059376
Iteration 27/1000 | Loss: 0.00063506
Iteration 28/1000 | Loss: 0.00029137
Iteration 29/1000 | Loss: 0.00007770
Iteration 30/1000 | Loss: 0.00006037
Iteration 31/1000 | Loss: 0.00005661
Iteration 32/1000 | Loss: 0.00004401
Iteration 33/1000 | Loss: 0.00004725
Iteration 34/1000 | Loss: 0.00079309
Iteration 35/1000 | Loss: 0.00024510
Iteration 36/1000 | Loss: 0.00024862
Iteration 37/1000 | Loss: 0.00064880
Iteration 38/1000 | Loss: 0.00038773
Iteration 39/1000 | Loss: 0.00005307
Iteration 40/1000 | Loss: 0.00068100
Iteration 41/1000 | Loss: 0.00038371
Iteration 42/1000 | Loss: 0.00022468
Iteration 43/1000 | Loss: 0.00013913
Iteration 44/1000 | Loss: 0.00023858
Iteration 45/1000 | Loss: 0.00017127
Iteration 46/1000 | Loss: 0.00019105
Iteration 47/1000 | Loss: 0.00018190
Iteration 48/1000 | Loss: 0.00017541
Iteration 49/1000 | Loss: 0.00006341
Iteration 50/1000 | Loss: 0.00030006
Iteration 51/1000 | Loss: 0.00016600
Iteration 52/1000 | Loss: 0.00006125
Iteration 53/1000 | Loss: 0.00005297
Iteration 54/1000 | Loss: 0.00004719
Iteration 55/1000 | Loss: 0.00005643
Iteration 56/1000 | Loss: 0.00003634
Iteration 57/1000 | Loss: 0.00005161
Iteration 58/1000 | Loss: 0.00048032
Iteration 59/1000 | Loss: 0.00020779
Iteration 60/1000 | Loss: 0.00005013
Iteration 61/1000 | Loss: 0.00004219
Iteration 62/1000 | Loss: 0.00003939
Iteration 63/1000 | Loss: 0.00042072
Iteration 64/1000 | Loss: 0.00003743
Iteration 65/1000 | Loss: 0.00057578
Iteration 66/1000 | Loss: 0.00055337
Iteration 67/1000 | Loss: 0.00027819
Iteration 68/1000 | Loss: 0.00055694
Iteration 69/1000 | Loss: 0.00061747
Iteration 70/1000 | Loss: 0.00058125
Iteration 71/1000 | Loss: 0.00018868
Iteration 72/1000 | Loss: 0.00016043
Iteration 73/1000 | Loss: 0.00015161
Iteration 74/1000 | Loss: 0.00005155
Iteration 75/1000 | Loss: 0.00004333
Iteration 76/1000 | Loss: 0.00046642
Iteration 77/1000 | Loss: 0.00004356
Iteration 78/1000 | Loss: 0.00003689
Iteration 79/1000 | Loss: 0.00004856
Iteration 80/1000 | Loss: 0.00003644
Iteration 81/1000 | Loss: 0.00022518
Iteration 82/1000 | Loss: 0.00025265
Iteration 83/1000 | Loss: 0.00054110
Iteration 84/1000 | Loss: 0.00021620
Iteration 85/1000 | Loss: 0.00027159
Iteration 86/1000 | Loss: 0.00057629
Iteration 87/1000 | Loss: 0.00010295
Iteration 88/1000 | Loss: 0.00003931
Iteration 89/1000 | Loss: 0.00003353
Iteration 90/1000 | Loss: 0.00003230
Iteration 91/1000 | Loss: 0.00003169
Iteration 92/1000 | Loss: 0.00003117
Iteration 93/1000 | Loss: 0.00003074
Iteration 94/1000 | Loss: 0.00003041
Iteration 95/1000 | Loss: 0.00003036
Iteration 96/1000 | Loss: 0.00003006
Iteration 97/1000 | Loss: 0.00063678
Iteration 98/1000 | Loss: 0.00023523
Iteration 99/1000 | Loss: 0.00003130
Iteration 100/1000 | Loss: 0.00002998
Iteration 101/1000 | Loss: 0.00002979
Iteration 102/1000 | Loss: 0.00069081
Iteration 103/1000 | Loss: 0.00020325
Iteration 104/1000 | Loss: 0.00003070
Iteration 105/1000 | Loss: 0.00002974
Iteration 106/1000 | Loss: 0.00070218
Iteration 107/1000 | Loss: 0.00049154
Iteration 108/1000 | Loss: 0.00004390
Iteration 109/1000 | Loss: 0.00003824
Iteration 110/1000 | Loss: 0.00016521
Iteration 111/1000 | Loss: 0.00003577
Iteration 112/1000 | Loss: 0.00021347
Iteration 113/1000 | Loss: 0.00016234
Iteration 114/1000 | Loss: 0.00021886
Iteration 115/1000 | Loss: 0.00003736
Iteration 116/1000 | Loss: 0.00012606
Iteration 117/1000 | Loss: 0.00003146
Iteration 118/1000 | Loss: 0.00022424
Iteration 119/1000 | Loss: 0.00014361
Iteration 120/1000 | Loss: 0.00019860
Iteration 121/1000 | Loss: 0.00003506
Iteration 122/1000 | Loss: 0.00003192
Iteration 123/1000 | Loss: 0.00003034
Iteration 124/1000 | Loss: 0.00002934
Iteration 125/1000 | Loss: 0.00002876
Iteration 126/1000 | Loss: 0.00002809
Iteration 127/1000 | Loss: 0.00002781
Iteration 128/1000 | Loss: 0.00002749
Iteration 129/1000 | Loss: 0.00002737
Iteration 130/1000 | Loss: 0.00002724
Iteration 131/1000 | Loss: 0.00002689
Iteration 132/1000 | Loss: 0.00002670
Iteration 133/1000 | Loss: 0.00002662
Iteration 134/1000 | Loss: 0.00002659
Iteration 135/1000 | Loss: 0.00002656
Iteration 136/1000 | Loss: 0.00002652
Iteration 137/1000 | Loss: 0.00002652
Iteration 138/1000 | Loss: 0.00002650
Iteration 139/1000 | Loss: 0.00002650
Iteration 140/1000 | Loss: 0.00002649
Iteration 141/1000 | Loss: 0.00002649
Iteration 142/1000 | Loss: 0.00002648
Iteration 143/1000 | Loss: 0.00002648
Iteration 144/1000 | Loss: 0.00002647
Iteration 145/1000 | Loss: 0.00002647
Iteration 146/1000 | Loss: 0.00002647
Iteration 147/1000 | Loss: 0.00002646
Iteration 148/1000 | Loss: 0.00002645
Iteration 149/1000 | Loss: 0.00002645
Iteration 150/1000 | Loss: 0.00002644
Iteration 151/1000 | Loss: 0.00002644
Iteration 152/1000 | Loss: 0.00002643
Iteration 153/1000 | Loss: 0.00002642
Iteration 154/1000 | Loss: 0.00002642
Iteration 155/1000 | Loss: 0.00002641
Iteration 156/1000 | Loss: 0.00002641
Iteration 157/1000 | Loss: 0.00002640
Iteration 158/1000 | Loss: 0.00002640
Iteration 159/1000 | Loss: 0.00002638
Iteration 160/1000 | Loss: 0.00002638
Iteration 161/1000 | Loss: 0.00002638
Iteration 162/1000 | Loss: 0.00002638
Iteration 163/1000 | Loss: 0.00002638
Iteration 164/1000 | Loss: 0.00002638
Iteration 165/1000 | Loss: 0.00002638
Iteration 166/1000 | Loss: 0.00002637
Iteration 167/1000 | Loss: 0.00002637
Iteration 168/1000 | Loss: 0.00002636
Iteration 169/1000 | Loss: 0.00002636
Iteration 170/1000 | Loss: 0.00002636
Iteration 171/1000 | Loss: 0.00002635
Iteration 172/1000 | Loss: 0.00002634
Iteration 173/1000 | Loss: 0.00002634
Iteration 174/1000 | Loss: 0.00002634
Iteration 175/1000 | Loss: 0.00002634
Iteration 176/1000 | Loss: 0.00002633
Iteration 177/1000 | Loss: 0.00002633
Iteration 178/1000 | Loss: 0.00002633
Iteration 179/1000 | Loss: 0.00002633
Iteration 180/1000 | Loss: 0.00002632
Iteration 181/1000 | Loss: 0.00002632
Iteration 182/1000 | Loss: 0.00002632
Iteration 183/1000 | Loss: 0.00002632
Iteration 184/1000 | Loss: 0.00002632
Iteration 185/1000 | Loss: 0.00002631
Iteration 186/1000 | Loss: 0.00002631
Iteration 187/1000 | Loss: 0.00002631
Iteration 188/1000 | Loss: 0.00002630
Iteration 189/1000 | Loss: 0.00002630
Iteration 190/1000 | Loss: 0.00002630
Iteration 191/1000 | Loss: 0.00002630
Iteration 192/1000 | Loss: 0.00002630
Iteration 193/1000 | Loss: 0.00002630
Iteration 194/1000 | Loss: 0.00002630
Iteration 195/1000 | Loss: 0.00002630
Iteration 196/1000 | Loss: 0.00002629
Iteration 197/1000 | Loss: 0.00002629
Iteration 198/1000 | Loss: 0.00002629
Iteration 199/1000 | Loss: 0.00002629
Iteration 200/1000 | Loss: 0.00002629
Iteration 201/1000 | Loss: 0.00002629
Iteration 202/1000 | Loss: 0.00002629
Iteration 203/1000 | Loss: 0.00002629
Iteration 204/1000 | Loss: 0.00002629
Iteration 205/1000 | Loss: 0.00002628
Iteration 206/1000 | Loss: 0.00002628
Iteration 207/1000 | Loss: 0.00002628
Iteration 208/1000 | Loss: 0.00002627
Iteration 209/1000 | Loss: 0.00002627
Iteration 210/1000 | Loss: 0.00002627
Iteration 211/1000 | Loss: 0.00002626
Iteration 212/1000 | Loss: 0.00002626
Iteration 213/1000 | Loss: 0.00002626
Iteration 214/1000 | Loss: 0.00002625
Iteration 215/1000 | Loss: 0.00002625
Iteration 216/1000 | Loss: 0.00002624
Iteration 217/1000 | Loss: 0.00002624
Iteration 218/1000 | Loss: 0.00002624
Iteration 219/1000 | Loss: 0.00002623
Iteration 220/1000 | Loss: 0.00002623
Iteration 221/1000 | Loss: 0.00002623
Iteration 222/1000 | Loss: 0.00002622
Iteration 223/1000 | Loss: 0.00002622
Iteration 224/1000 | Loss: 0.00002622
Iteration 225/1000 | Loss: 0.00002622
Iteration 226/1000 | Loss: 0.00002621
Iteration 227/1000 | Loss: 0.00002621
Iteration 228/1000 | Loss: 0.00002621
Iteration 229/1000 | Loss: 0.00002620
Iteration 230/1000 | Loss: 0.00002620
Iteration 231/1000 | Loss: 0.00002620
Iteration 232/1000 | Loss: 0.00002619
Iteration 233/1000 | Loss: 0.00002619
Iteration 234/1000 | Loss: 0.00002619
Iteration 235/1000 | Loss: 0.00002619
Iteration 236/1000 | Loss: 0.00002619
Iteration 237/1000 | Loss: 0.00002619
Iteration 238/1000 | Loss: 0.00002618
Iteration 239/1000 | Loss: 0.00002618
Iteration 240/1000 | Loss: 0.00002618
Iteration 241/1000 | Loss: 0.00002618
Iteration 242/1000 | Loss: 0.00002618
Iteration 243/1000 | Loss: 0.00002618
Iteration 244/1000 | Loss: 0.00002617
Iteration 245/1000 | Loss: 0.00002617
Iteration 246/1000 | Loss: 0.00002617
Iteration 247/1000 | Loss: 0.00002617
Iteration 248/1000 | Loss: 0.00002616
Iteration 249/1000 | Loss: 0.00002616
Iteration 250/1000 | Loss: 0.00002616
Iteration 251/1000 | Loss: 0.00002616
Iteration 252/1000 | Loss: 0.00002616
Iteration 253/1000 | Loss: 0.00002616
Iteration 254/1000 | Loss: 0.00002616
Iteration 255/1000 | Loss: 0.00002616
Iteration 256/1000 | Loss: 0.00002616
Iteration 257/1000 | Loss: 0.00002616
Iteration 258/1000 | Loss: 0.00002616
Iteration 259/1000 | Loss: 0.00002615
Iteration 260/1000 | Loss: 0.00002615
Iteration 261/1000 | Loss: 0.00002615
Iteration 262/1000 | Loss: 0.00002615
Iteration 263/1000 | Loss: 0.00002615
Iteration 264/1000 | Loss: 0.00002615
Iteration 265/1000 | Loss: 0.00002615
Iteration 266/1000 | Loss: 0.00002615
Iteration 267/1000 | Loss: 0.00002615
Iteration 268/1000 | Loss: 0.00002615
Iteration 269/1000 | Loss: 0.00002615
Iteration 270/1000 | Loss: 0.00002615
Iteration 271/1000 | Loss: 0.00002614
Iteration 272/1000 | Loss: 0.00002614
Iteration 273/1000 | Loss: 0.00002614
Iteration 274/1000 | Loss: 0.00002614
Iteration 275/1000 | Loss: 0.00002614
Iteration 276/1000 | Loss: 0.00002613
Iteration 277/1000 | Loss: 0.00002613
Iteration 278/1000 | Loss: 0.00002613
Iteration 279/1000 | Loss: 0.00002612
Iteration 280/1000 | Loss: 0.00002612
Iteration 281/1000 | Loss: 0.00002612
Iteration 282/1000 | Loss: 0.00002612
Iteration 283/1000 | Loss: 0.00002612
Iteration 284/1000 | Loss: 0.00002612
Iteration 285/1000 | Loss: 0.00002612
Iteration 286/1000 | Loss: 0.00002611
Iteration 287/1000 | Loss: 0.00002611
Iteration 288/1000 | Loss: 0.00002611
Iteration 289/1000 | Loss: 0.00002611
Iteration 290/1000 | Loss: 0.00002611
Iteration 291/1000 | Loss: 0.00002611
Iteration 292/1000 | Loss: 0.00002611
Iteration 293/1000 | Loss: 0.00002611
Iteration 294/1000 | Loss: 0.00002611
Iteration 295/1000 | Loss: 0.00002610
Iteration 296/1000 | Loss: 0.00002610
Iteration 297/1000 | Loss: 0.00002610
Iteration 298/1000 | Loss: 0.00002610
Iteration 299/1000 | Loss: 0.00002609
Iteration 300/1000 | Loss: 0.00002609
Iteration 301/1000 | Loss: 0.00002609
Iteration 302/1000 | Loss: 0.00002609
Iteration 303/1000 | Loss: 0.00002609
Iteration 304/1000 | Loss: 0.00002609
Iteration 305/1000 | Loss: 0.00002609
Iteration 306/1000 | Loss: 0.00002609
Iteration 307/1000 | Loss: 0.00002608
Iteration 308/1000 | Loss: 0.00002608
Iteration 309/1000 | Loss: 0.00002608
Iteration 310/1000 | Loss: 0.00002608
Iteration 311/1000 | Loss: 0.00002608
Iteration 312/1000 | Loss: 0.00002608
Iteration 313/1000 | Loss: 0.00002608
Iteration 314/1000 | Loss: 0.00002608
Iteration 315/1000 | Loss: 0.00002608
Iteration 316/1000 | Loss: 0.00002608
Iteration 317/1000 | Loss: 0.00002608
Iteration 318/1000 | Loss: 0.00002607
Iteration 319/1000 | Loss: 0.00002607
Iteration 320/1000 | Loss: 0.00002607
Iteration 321/1000 | Loss: 0.00002607
Iteration 322/1000 | Loss: 0.00002607
Iteration 323/1000 | Loss: 0.00002607
Iteration 324/1000 | Loss: 0.00002607
Iteration 325/1000 | Loss: 0.00002607
Iteration 326/1000 | Loss: 0.00002606
Iteration 327/1000 | Loss: 0.00002606
Iteration 328/1000 | Loss: 0.00002606
Iteration 329/1000 | Loss: 0.00002606
Iteration 330/1000 | Loss: 0.00002605
Iteration 331/1000 | Loss: 0.00002605
Iteration 332/1000 | Loss: 0.00002605
Iteration 333/1000 | Loss: 0.00002604
Iteration 334/1000 | Loss: 0.00002604
Iteration 335/1000 | Loss: 0.00002604
Iteration 336/1000 | Loss: 0.00002604
Iteration 337/1000 | Loss: 0.00002603
Iteration 338/1000 | Loss: 0.00002603
Iteration 339/1000 | Loss: 0.00002603
Iteration 340/1000 | Loss: 0.00002603
Iteration 341/1000 | Loss: 0.00002603
Iteration 342/1000 | Loss: 0.00002603
Iteration 343/1000 | Loss: 0.00002602
Iteration 344/1000 | Loss: 0.00002602
Iteration 345/1000 | Loss: 0.00002601
Iteration 346/1000 | Loss: 0.00002601
Iteration 347/1000 | Loss: 0.00002601
Iteration 348/1000 | Loss: 0.00002601
Iteration 349/1000 | Loss: 0.00002600
Iteration 350/1000 | Loss: 0.00002600
Iteration 351/1000 | Loss: 0.00002600
Iteration 352/1000 | Loss: 0.00002600
Iteration 353/1000 | Loss: 0.00002600
Iteration 354/1000 | Loss: 0.00002600
Iteration 355/1000 | Loss: 0.00002600
Iteration 356/1000 | Loss: 0.00002600
Iteration 357/1000 | Loss: 0.00002599
Iteration 358/1000 | Loss: 0.00002599
Iteration 359/1000 | Loss: 0.00002599
Iteration 360/1000 | Loss: 0.00002599
Iteration 361/1000 | Loss: 0.00002599
Iteration 362/1000 | Loss: 0.00002599
Iteration 363/1000 | Loss: 0.00002598
Iteration 364/1000 | Loss: 0.00002598
Iteration 365/1000 | Loss: 0.00002598
Iteration 366/1000 | Loss: 0.00002598
Iteration 367/1000 | Loss: 0.00002598
Iteration 368/1000 | Loss: 0.00002598
Iteration 369/1000 | Loss: 0.00002598
Iteration 370/1000 | Loss: 0.00002598
Iteration 371/1000 | Loss: 0.00002598
Iteration 372/1000 | Loss: 0.00002598
Iteration 373/1000 | Loss: 0.00002598
Iteration 374/1000 | Loss: 0.00002598
Iteration 375/1000 | Loss: 0.00002598
Iteration 376/1000 | Loss: 0.00002598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 376. Stopping optimization.
Last 5 losses: [2.5979286874644458e-05, 2.5979286874644458e-05, 2.5979286874644458e-05, 2.5979286874644458e-05, 2.5979286874644458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5979286874644458e-05

Optimization complete. Final v2v error: 3.933387279510498 mm

Highest mean error: 13.421833992004395 mm for frame 102

Lowest mean error: 3.2591140270233154 mm for frame 186

Saving results

Total time: 261.1889383792877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996635
Iteration 2/25 | Loss: 0.00284020
Iteration 3/25 | Loss: 0.00214000
Iteration 4/25 | Loss: 0.00205161
Iteration 5/25 | Loss: 0.00201558
Iteration 6/25 | Loss: 0.00198285
Iteration 7/25 | Loss: 0.00197860
Iteration 8/25 | Loss: 0.00197769
Iteration 9/25 | Loss: 0.00196167
Iteration 10/25 | Loss: 0.00196105
Iteration 11/25 | Loss: 0.00195794
Iteration 12/25 | Loss: 0.00195989
Iteration 13/25 | Loss: 0.00195200
Iteration 14/25 | Loss: 0.00195080
Iteration 15/25 | Loss: 0.00195537
Iteration 16/25 | Loss: 0.00194978
Iteration 17/25 | Loss: 0.00194952
Iteration 18/25 | Loss: 0.00194937
Iteration 19/25 | Loss: 0.00194934
Iteration 20/25 | Loss: 0.00194934
Iteration 21/25 | Loss: 0.00194933
Iteration 22/25 | Loss: 0.00194933
Iteration 23/25 | Loss: 0.00194933
Iteration 24/25 | Loss: 0.00194933
Iteration 25/25 | Loss: 0.00194933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21572447
Iteration 2/25 | Loss: 0.00641750
Iteration 3/25 | Loss: 0.00641747
Iteration 4/25 | Loss: 0.00641747
Iteration 5/25 | Loss: 0.00641747
Iteration 6/25 | Loss: 0.00641747
Iteration 7/25 | Loss: 0.00641747
Iteration 8/25 | Loss: 0.00641747
Iteration 9/25 | Loss: 0.00641747
Iteration 10/25 | Loss: 0.00641747
Iteration 11/25 | Loss: 0.00641747
Iteration 12/25 | Loss: 0.00641747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.006417468190193176, 0.006417468190193176, 0.006417468190193176, 0.006417468190193176, 0.006417468190193176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006417468190193176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00641747
Iteration 2/1000 | Loss: 0.00154375
Iteration 3/1000 | Loss: 0.00064658
Iteration 4/1000 | Loss: 0.00278427
Iteration 5/1000 | Loss: 0.01214511
Iteration 6/1000 | Loss: 0.00091539
Iteration 7/1000 | Loss: 0.00385983
Iteration 8/1000 | Loss: 0.00438866
Iteration 9/1000 | Loss: 0.00077452
Iteration 10/1000 | Loss: 0.00056384
Iteration 11/1000 | Loss: 0.00079209
Iteration 12/1000 | Loss: 0.00105947
Iteration 13/1000 | Loss: 0.00201228
Iteration 14/1000 | Loss: 0.00072727
Iteration 15/1000 | Loss: 0.00020076
Iteration 16/1000 | Loss: 0.00016929
Iteration 17/1000 | Loss: 0.00028301
Iteration 18/1000 | Loss: 0.00048576
Iteration 19/1000 | Loss: 0.00027611
Iteration 20/1000 | Loss: 0.00009540
Iteration 21/1000 | Loss: 0.00013104
Iteration 22/1000 | Loss: 0.00030968
Iteration 23/1000 | Loss: 0.00009371
Iteration 24/1000 | Loss: 0.00021754
Iteration 25/1000 | Loss: 0.00028281
Iteration 26/1000 | Loss: 0.00015913
Iteration 27/1000 | Loss: 0.00005137
Iteration 28/1000 | Loss: 0.00004815
Iteration 29/1000 | Loss: 0.00016968
Iteration 30/1000 | Loss: 0.00024586
Iteration 31/1000 | Loss: 0.00004605
Iteration 32/1000 | Loss: 0.00004431
Iteration 33/1000 | Loss: 0.00019789
Iteration 34/1000 | Loss: 0.00041042
Iteration 35/1000 | Loss: 0.00050704
Iteration 36/1000 | Loss: 0.00019332
Iteration 37/1000 | Loss: 0.00019094
Iteration 38/1000 | Loss: 0.00004346
Iteration 39/1000 | Loss: 0.00019754
Iteration 40/1000 | Loss: 0.00004246
Iteration 41/1000 | Loss: 0.00060757
Iteration 42/1000 | Loss: 0.00047889
Iteration 43/1000 | Loss: 0.00015569
Iteration 44/1000 | Loss: 0.00072271
Iteration 45/1000 | Loss: 0.00051345
Iteration 46/1000 | Loss: 0.00123113
Iteration 47/1000 | Loss: 0.00062846
Iteration 48/1000 | Loss: 0.00167675
Iteration 49/1000 | Loss: 0.00086760
Iteration 50/1000 | Loss: 0.00205774
Iteration 51/1000 | Loss: 0.00092998
Iteration 52/1000 | Loss: 0.00165375
Iteration 53/1000 | Loss: 0.00114892
Iteration 54/1000 | Loss: 0.00081570
Iteration 55/1000 | Loss: 0.00077293
Iteration 56/1000 | Loss: 0.00023311
Iteration 57/1000 | Loss: 0.00020105
Iteration 58/1000 | Loss: 0.00012875
Iteration 59/1000 | Loss: 0.00043840
Iteration 60/1000 | Loss: 0.00032114
Iteration 61/1000 | Loss: 0.00040555
Iteration 62/1000 | Loss: 0.00014339
Iteration 63/1000 | Loss: 0.00058017
Iteration 64/1000 | Loss: 0.00075799
Iteration 65/1000 | Loss: 0.00064018
Iteration 66/1000 | Loss: 0.00028938
Iteration 67/1000 | Loss: 0.00017499
Iteration 68/1000 | Loss: 0.00015829
Iteration 69/1000 | Loss: 0.00021647
Iteration 70/1000 | Loss: 0.00022582
Iteration 71/1000 | Loss: 0.00021988
Iteration 72/1000 | Loss: 0.00016480
Iteration 73/1000 | Loss: 0.00003713
Iteration 74/1000 | Loss: 0.00003427
Iteration 75/1000 | Loss: 0.00029449
Iteration 76/1000 | Loss: 0.00060529
Iteration 77/1000 | Loss: 0.00005134
Iteration 78/1000 | Loss: 0.00005145
Iteration 79/1000 | Loss: 0.00003050
Iteration 80/1000 | Loss: 0.00007223
Iteration 81/1000 | Loss: 0.00002917
Iteration 82/1000 | Loss: 0.00002857
Iteration 83/1000 | Loss: 0.00017754
Iteration 84/1000 | Loss: 0.00004099
Iteration 85/1000 | Loss: 0.00005617
Iteration 86/1000 | Loss: 0.00002808
Iteration 87/1000 | Loss: 0.00002790
Iteration 88/1000 | Loss: 0.00002769
Iteration 89/1000 | Loss: 0.00002765
Iteration 90/1000 | Loss: 0.00002758
Iteration 91/1000 | Loss: 0.00002754
Iteration 92/1000 | Loss: 0.00002748
Iteration 93/1000 | Loss: 0.00002745
Iteration 94/1000 | Loss: 0.00002744
Iteration 95/1000 | Loss: 0.00002744
Iteration 96/1000 | Loss: 0.00002744
Iteration 97/1000 | Loss: 0.00002744
Iteration 98/1000 | Loss: 0.00002744
Iteration 99/1000 | Loss: 0.00002744
Iteration 100/1000 | Loss: 0.00002744
Iteration 101/1000 | Loss: 0.00002744
Iteration 102/1000 | Loss: 0.00002744
Iteration 103/1000 | Loss: 0.00002743
Iteration 104/1000 | Loss: 0.00002743
Iteration 105/1000 | Loss: 0.00002743
Iteration 106/1000 | Loss: 0.00002741
Iteration 107/1000 | Loss: 0.00002741
Iteration 108/1000 | Loss: 0.00002741
Iteration 109/1000 | Loss: 0.00002740
Iteration 110/1000 | Loss: 0.00002740
Iteration 111/1000 | Loss: 0.00002740
Iteration 112/1000 | Loss: 0.00002740
Iteration 113/1000 | Loss: 0.00002740
Iteration 114/1000 | Loss: 0.00024378
Iteration 115/1000 | Loss: 0.00034044
Iteration 116/1000 | Loss: 0.00021762
Iteration 117/1000 | Loss: 0.00003604
Iteration 118/1000 | Loss: 0.00018742
Iteration 119/1000 | Loss: 0.00023747
Iteration 120/1000 | Loss: 0.00003978
Iteration 121/1000 | Loss: 0.00021502
Iteration 122/1000 | Loss: 0.00004099
Iteration 123/1000 | Loss: 0.00003081
Iteration 124/1000 | Loss: 0.00002808
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00015466
Iteration 127/1000 | Loss: 0.00003983
Iteration 128/1000 | Loss: 0.00005150
Iteration 129/1000 | Loss: 0.00002453
Iteration 130/1000 | Loss: 0.00010506
Iteration 131/1000 | Loss: 0.00002402
Iteration 132/1000 | Loss: 0.00002372
Iteration 133/1000 | Loss: 0.00018932
Iteration 134/1000 | Loss: 0.00026529
Iteration 135/1000 | Loss: 0.00002562
Iteration 136/1000 | Loss: 0.00015634
Iteration 137/1000 | Loss: 0.00002346
Iteration 138/1000 | Loss: 0.00002336
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002335
Iteration 141/1000 | Loss: 0.00002333
Iteration 142/1000 | Loss: 0.00002333
Iteration 143/1000 | Loss: 0.00002333
Iteration 144/1000 | Loss: 0.00002333
Iteration 145/1000 | Loss: 0.00002332
Iteration 146/1000 | Loss: 0.00002332
Iteration 147/1000 | Loss: 0.00002332
Iteration 148/1000 | Loss: 0.00002332
Iteration 149/1000 | Loss: 0.00002332
Iteration 150/1000 | Loss: 0.00002332
Iteration 151/1000 | Loss: 0.00002332
Iteration 152/1000 | Loss: 0.00002332
Iteration 153/1000 | Loss: 0.00002332
Iteration 154/1000 | Loss: 0.00002332
Iteration 155/1000 | Loss: 0.00002332
Iteration 156/1000 | Loss: 0.00002331
Iteration 157/1000 | Loss: 0.00002331
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002329
Iteration 160/1000 | Loss: 0.00002329
Iteration 161/1000 | Loss: 0.00002329
Iteration 162/1000 | Loss: 0.00002328
Iteration 163/1000 | Loss: 0.00002328
Iteration 164/1000 | Loss: 0.00002328
Iteration 165/1000 | Loss: 0.00002328
Iteration 166/1000 | Loss: 0.00002327
Iteration 167/1000 | Loss: 0.00002327
Iteration 168/1000 | Loss: 0.00020082
Iteration 169/1000 | Loss: 0.00002797
Iteration 170/1000 | Loss: 0.00002520
Iteration 171/1000 | Loss: 0.00002341
Iteration 172/1000 | Loss: 0.00002330
Iteration 173/1000 | Loss: 0.00002329
Iteration 174/1000 | Loss: 0.00002328
Iteration 175/1000 | Loss: 0.00002328
Iteration 176/1000 | Loss: 0.00002328
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002328
Iteration 180/1000 | Loss: 0.00002328
Iteration 181/1000 | Loss: 0.00002327
Iteration 182/1000 | Loss: 0.00002327
Iteration 183/1000 | Loss: 0.00002327
Iteration 184/1000 | Loss: 0.00002326
Iteration 185/1000 | Loss: 0.00002326
Iteration 186/1000 | Loss: 0.00002326
Iteration 187/1000 | Loss: 0.00002325
Iteration 188/1000 | Loss: 0.00002325
Iteration 189/1000 | Loss: 0.00002325
Iteration 190/1000 | Loss: 0.00002325
Iteration 191/1000 | Loss: 0.00002325
Iteration 192/1000 | Loss: 0.00002325
Iteration 193/1000 | Loss: 0.00002325
Iteration 194/1000 | Loss: 0.00002325
Iteration 195/1000 | Loss: 0.00002325
Iteration 196/1000 | Loss: 0.00002325
Iteration 197/1000 | Loss: 0.00002325
Iteration 198/1000 | Loss: 0.00002325
Iteration 199/1000 | Loss: 0.00002325
Iteration 200/1000 | Loss: 0.00002325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.324846172996331e-05, 2.324846172996331e-05, 2.324846172996331e-05, 2.324846172996331e-05, 2.324846172996331e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.324846172996331e-05

Optimization complete. Final v2v error: 3.9976487159729004 mm

Highest mean error: 11.856836318969727 mm for frame 86

Lowest mean error: 3.608320474624634 mm for frame 106

Saving results

Total time: 231.48303651809692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01158085
Iteration 2/25 | Loss: 0.00217337
Iteration 3/25 | Loss: 0.00166651
Iteration 4/25 | Loss: 0.00148057
Iteration 5/25 | Loss: 0.00140453
Iteration 6/25 | Loss: 0.00138448
Iteration 7/25 | Loss: 0.00137579
Iteration 8/25 | Loss: 0.00137432
Iteration 9/25 | Loss: 0.00135927
Iteration 10/25 | Loss: 0.00135240
Iteration 11/25 | Loss: 0.00135125
Iteration 12/25 | Loss: 0.00135090
Iteration 13/25 | Loss: 0.00135049
Iteration 14/25 | Loss: 0.00135035
Iteration 15/25 | Loss: 0.00135034
Iteration 16/25 | Loss: 0.00135033
Iteration 17/25 | Loss: 0.00135033
Iteration 18/25 | Loss: 0.00135033
Iteration 19/25 | Loss: 0.00135033
Iteration 20/25 | Loss: 0.00135033
Iteration 21/25 | Loss: 0.00135032
Iteration 22/25 | Loss: 0.00135032
Iteration 23/25 | Loss: 0.00135032
Iteration 24/25 | Loss: 0.00135032
Iteration 25/25 | Loss: 0.00135031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32750535
Iteration 2/25 | Loss: 0.00172741
Iteration 3/25 | Loss: 0.00172741
Iteration 4/25 | Loss: 0.00172741
Iteration 5/25 | Loss: 0.00172741
Iteration 6/25 | Loss: 0.00172741
Iteration 7/25 | Loss: 0.00172741
Iteration 8/25 | Loss: 0.00172741
Iteration 9/25 | Loss: 0.00172741
Iteration 10/25 | Loss: 0.00172741
Iteration 11/25 | Loss: 0.00172741
Iteration 12/25 | Loss: 0.00172741
Iteration 13/25 | Loss: 0.00172741
Iteration 14/25 | Loss: 0.00172741
Iteration 15/25 | Loss: 0.00172741
Iteration 16/25 | Loss: 0.00172741
Iteration 17/25 | Loss: 0.00172741
Iteration 18/25 | Loss: 0.00172741
Iteration 19/25 | Loss: 0.00172741
Iteration 20/25 | Loss: 0.00172741
Iteration 21/25 | Loss: 0.00172741
Iteration 22/25 | Loss: 0.00172741
Iteration 23/25 | Loss: 0.00172741
Iteration 24/25 | Loss: 0.00172741
Iteration 25/25 | Loss: 0.00172741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172741
Iteration 2/1000 | Loss: 0.00007980
Iteration 3/1000 | Loss: 0.00005510
Iteration 4/1000 | Loss: 0.00004612
Iteration 5/1000 | Loss: 0.00014162
Iteration 6/1000 | Loss: 0.00019142
Iteration 7/1000 | Loss: 0.00010776
Iteration 8/1000 | Loss: 0.00004556
Iteration 9/1000 | Loss: 0.00012754
Iteration 10/1000 | Loss: 0.00009186
Iteration 11/1000 | Loss: 0.00007736
Iteration 12/1000 | Loss: 0.00016330
Iteration 13/1000 | Loss: 0.00020148
Iteration 14/1000 | Loss: 0.00015798
Iteration 15/1000 | Loss: 0.00010765
Iteration 16/1000 | Loss: 0.00007732
Iteration 17/1000 | Loss: 0.00004041
Iteration 18/1000 | Loss: 0.00012004
Iteration 19/1000 | Loss: 0.00008067
Iteration 20/1000 | Loss: 0.00007316
Iteration 21/1000 | Loss: 0.00008972
Iteration 22/1000 | Loss: 0.00005552
Iteration 23/1000 | Loss: 0.00003620
Iteration 24/1000 | Loss: 0.00009400
Iteration 25/1000 | Loss: 0.00007244
Iteration 26/1000 | Loss: 0.00009794
Iteration 27/1000 | Loss: 0.00006357
Iteration 28/1000 | Loss: 0.00012917
Iteration 29/1000 | Loss: 0.00008335
Iteration 30/1000 | Loss: 0.00013066
Iteration 31/1000 | Loss: 0.00010385
Iteration 32/1000 | Loss: 0.00009130
Iteration 33/1000 | Loss: 0.00019344
Iteration 34/1000 | Loss: 0.00017935
Iteration 35/1000 | Loss: 0.00004210
Iteration 36/1000 | Loss: 0.00009976
Iteration 37/1000 | Loss: 0.00006641
Iteration 38/1000 | Loss: 0.00006379
Iteration 39/1000 | Loss: 0.00004869
Iteration 40/1000 | Loss: 0.00003431
Iteration 41/1000 | Loss: 0.00008382
Iteration 42/1000 | Loss: 0.00005275
Iteration 43/1000 | Loss: 0.00005119
Iteration 44/1000 | Loss: 0.00007509
Iteration 45/1000 | Loss: 0.00005025
Iteration 46/1000 | Loss: 0.00006228
Iteration 47/1000 | Loss: 0.00004626
Iteration 48/1000 | Loss: 0.00005821
Iteration 49/1000 | Loss: 0.00004398
Iteration 50/1000 | Loss: 0.00005674
Iteration 51/1000 | Loss: 0.00008728
Iteration 52/1000 | Loss: 0.00010701
Iteration 53/1000 | Loss: 0.00007678
Iteration 54/1000 | Loss: 0.00005177
Iteration 55/1000 | Loss: 0.00003930
Iteration 56/1000 | Loss: 0.00003352
Iteration 57/1000 | Loss: 0.00005198
Iteration 58/1000 | Loss: 0.00003799
Iteration 59/1000 | Loss: 0.00007587
Iteration 60/1000 | Loss: 0.00004289
Iteration 61/1000 | Loss: 0.00008850
Iteration 62/1000 | Loss: 0.00004136
Iteration 63/1000 | Loss: 0.00003829
Iteration 64/1000 | Loss: 0.00003340
Iteration 65/1000 | Loss: 0.00008466
Iteration 66/1000 | Loss: 0.00004118
Iteration 67/1000 | Loss: 0.00003588
Iteration 68/1000 | Loss: 0.00004856
Iteration 69/1000 | Loss: 0.00003307
Iteration 70/1000 | Loss: 0.00003322
Iteration 71/1000 | Loss: 0.00003336
Iteration 72/1000 | Loss: 0.00005836
Iteration 73/1000 | Loss: 0.00004864
Iteration 74/1000 | Loss: 0.00006259
Iteration 75/1000 | Loss: 0.00005760
Iteration 76/1000 | Loss: 0.00005525
Iteration 77/1000 | Loss: 0.00003786
Iteration 78/1000 | Loss: 0.00003658
Iteration 79/1000 | Loss: 0.00003260
Iteration 80/1000 | Loss: 0.00005520
Iteration 81/1000 | Loss: 0.00003446
Iteration 82/1000 | Loss: 0.00005446
Iteration 83/1000 | Loss: 0.00003420
Iteration 84/1000 | Loss: 0.00003191
Iteration 85/1000 | Loss: 0.00007612
Iteration 86/1000 | Loss: 0.00003530
Iteration 87/1000 | Loss: 0.00004323
Iteration 88/1000 | Loss: 0.00007580
Iteration 89/1000 | Loss: 0.00003933
Iteration 90/1000 | Loss: 0.00004426
Iteration 91/1000 | Loss: 0.00003311
Iteration 92/1000 | Loss: 0.00004066
Iteration 93/1000 | Loss: 0.00003207
Iteration 94/1000 | Loss: 0.00006202
Iteration 95/1000 | Loss: 0.00003501
Iteration 96/1000 | Loss: 0.00005608
Iteration 97/1000 | Loss: 0.00003304
Iteration 98/1000 | Loss: 0.00007196
Iteration 99/1000 | Loss: 0.00005112
Iteration 100/1000 | Loss: 0.00006796
Iteration 101/1000 | Loss: 0.00003307
Iteration 102/1000 | Loss: 0.00003585
Iteration 103/1000 | Loss: 0.00004595
Iteration 104/1000 | Loss: 0.00005387
Iteration 105/1000 | Loss: 0.00004428
Iteration 106/1000 | Loss: 0.00003541
Iteration 107/1000 | Loss: 0.00005668
Iteration 108/1000 | Loss: 0.00003657
Iteration 109/1000 | Loss: 0.00004967
Iteration 110/1000 | Loss: 0.00003398
Iteration 111/1000 | Loss: 0.00005539
Iteration 112/1000 | Loss: 0.00003539
Iteration 113/1000 | Loss: 0.00003297
Iteration 114/1000 | Loss: 0.00003225
Iteration 115/1000 | Loss: 0.00003892
Iteration 116/1000 | Loss: 0.00003328
Iteration 117/1000 | Loss: 0.00003057
Iteration 118/1000 | Loss: 0.00004135
Iteration 119/1000 | Loss: 0.00003043
Iteration 120/1000 | Loss: 0.00003042
Iteration 121/1000 | Loss: 0.00003041
Iteration 122/1000 | Loss: 0.00003385
Iteration 123/1000 | Loss: 0.00003082
Iteration 124/1000 | Loss: 0.00003797
Iteration 125/1000 | Loss: 0.00003583
Iteration 126/1000 | Loss: 0.00003235
Iteration 127/1000 | Loss: 0.00003246
Iteration 128/1000 | Loss: 0.00003128
Iteration 129/1000 | Loss: 0.00004394
Iteration 130/1000 | Loss: 0.00003180
Iteration 131/1000 | Loss: 0.00003721
Iteration 132/1000 | Loss: 0.00003429
Iteration 133/1000 | Loss: 0.00003094
Iteration 134/1000 | Loss: 0.00003362
Iteration 135/1000 | Loss: 0.00003075
Iteration 136/1000 | Loss: 0.00003641
Iteration 137/1000 | Loss: 0.00003253
Iteration 138/1000 | Loss: 0.00003025
Iteration 139/1000 | Loss: 0.00003307
Iteration 140/1000 | Loss: 0.00003248
Iteration 141/1000 | Loss: 0.00003111
Iteration 142/1000 | Loss: 0.00003185
Iteration 143/1000 | Loss: 0.00003211
Iteration 144/1000 | Loss: 0.00003359
Iteration 145/1000 | Loss: 0.00003836
Iteration 146/1000 | Loss: 0.00003509
Iteration 147/1000 | Loss: 0.00003350
Iteration 148/1000 | Loss: 0.00003602
Iteration 149/1000 | Loss: 0.00003504
Iteration 150/1000 | Loss: 0.00003574
Iteration 151/1000 | Loss: 0.00003417
Iteration 152/1000 | Loss: 0.00003119
Iteration 153/1000 | Loss: 0.00003159
Iteration 154/1000 | Loss: 0.00003017
Iteration 155/1000 | Loss: 0.00003189
Iteration 156/1000 | Loss: 0.00003023
Iteration 157/1000 | Loss: 0.00003001
Iteration 158/1000 | Loss: 0.00002999
Iteration 159/1000 | Loss: 0.00002999
Iteration 160/1000 | Loss: 0.00003034
Iteration 161/1000 | Loss: 0.00003010
Iteration 162/1000 | Loss: 0.00002998
Iteration 163/1000 | Loss: 0.00002998
Iteration 164/1000 | Loss: 0.00003139
Iteration 165/1000 | Loss: 0.00003138
Iteration 166/1000 | Loss: 0.00003364
Iteration 167/1000 | Loss: 0.00003431
Iteration 168/1000 | Loss: 0.00003466
Iteration 169/1000 | Loss: 0.00003700
Iteration 170/1000 | Loss: 0.00003077
Iteration 171/1000 | Loss: 0.00003027
Iteration 172/1000 | Loss: 0.00004328
Iteration 173/1000 | Loss: 0.00003018
Iteration 174/1000 | Loss: 0.00003013
Iteration 175/1000 | Loss: 0.00003011
Iteration 176/1000 | Loss: 0.00003010
Iteration 177/1000 | Loss: 0.00003071
Iteration 178/1000 | Loss: 0.00003277
Iteration 179/1000 | Loss: 0.00003880
Iteration 180/1000 | Loss: 0.00003215
Iteration 181/1000 | Loss: 0.00003782
Iteration 182/1000 | Loss: 0.00003658
Iteration 183/1000 | Loss: 0.00003440
Iteration 184/1000 | Loss: 0.00004193
Iteration 185/1000 | Loss: 0.00003531
Iteration 186/1000 | Loss: 0.00003830
Iteration 187/1000 | Loss: 0.00004131
Iteration 188/1000 | Loss: 0.00003720
Iteration 189/1000 | Loss: 0.00004106
Iteration 190/1000 | Loss: 0.00004281
Iteration 191/1000 | Loss: 0.00003775
Iteration 192/1000 | Loss: 0.00003330
Iteration 193/1000 | Loss: 0.00003206
Iteration 194/1000 | Loss: 0.00003143
Iteration 195/1000 | Loss: 0.00003043
Iteration 196/1000 | Loss: 0.00003019
Iteration 197/1000 | Loss: 0.00003046
Iteration 198/1000 | Loss: 0.00003012
Iteration 199/1000 | Loss: 0.00003041
Iteration 200/1000 | Loss: 0.00003011
Iteration 201/1000 | Loss: 0.00003011
Iteration 202/1000 | Loss: 0.00002995
Iteration 203/1000 | Loss: 0.00003040
Iteration 204/1000 | Loss: 0.00003356
Iteration 205/1000 | Loss: 0.00003253
Iteration 206/1000 | Loss: 0.00003130
Iteration 207/1000 | Loss: 0.00004605
Iteration 208/1000 | Loss: 0.00003273
Iteration 209/1000 | Loss: 0.00003499
Iteration 210/1000 | Loss: 0.00004394
Iteration 211/1000 | Loss: 0.00003651
Iteration 212/1000 | Loss: 0.00003873
Iteration 213/1000 | Loss: 0.00003711
Iteration 214/1000 | Loss: 0.00004606
Iteration 215/1000 | Loss: 0.00003675
Iteration 216/1000 | Loss: 0.00004693
Iteration 217/1000 | Loss: 0.00003511
Iteration 218/1000 | Loss: 0.00005680
Iteration 219/1000 | Loss: 0.00003643
Iteration 220/1000 | Loss: 0.00003961
Iteration 221/1000 | Loss: 0.00003699
Iteration 222/1000 | Loss: 0.00003968
Iteration 223/1000 | Loss: 0.00003858
Iteration 224/1000 | Loss: 0.00004026
Iteration 225/1000 | Loss: 0.00003652
Iteration 226/1000 | Loss: 0.00003813
Iteration 227/1000 | Loss: 0.00003631
Iteration 228/1000 | Loss: 0.00004214
Iteration 229/1000 | Loss: 0.00004489
Iteration 230/1000 | Loss: 0.00003755
Iteration 231/1000 | Loss: 0.00004003
Iteration 232/1000 | Loss: 0.00004578
Iteration 233/1000 | Loss: 0.00003076
Iteration 234/1000 | Loss: 0.00003566
Iteration 235/1000 | Loss: 0.00003809
Iteration 236/1000 | Loss: 0.00003500
Iteration 237/1000 | Loss: 0.00003613
Iteration 238/1000 | Loss: 0.00003082
Iteration 239/1000 | Loss: 0.00003054
Iteration 240/1000 | Loss: 0.00003023
Iteration 241/1000 | Loss: 0.00003040
Iteration 242/1000 | Loss: 0.00003014
Iteration 243/1000 | Loss: 0.00003046
Iteration 244/1000 | Loss: 0.00003026
Iteration 245/1000 | Loss: 0.00003045
Iteration 246/1000 | Loss: 0.00003019
Iteration 247/1000 | Loss: 0.00003019
Iteration 248/1000 | Loss: 0.00003046
Iteration 249/1000 | Loss: 0.00003015
Iteration 250/1000 | Loss: 0.00003029
Iteration 251/1000 | Loss: 0.00003009
Iteration 252/1000 | Loss: 0.00003008
Iteration 253/1000 | Loss: 0.00002998
Iteration 254/1000 | Loss: 0.00002997
Iteration 255/1000 | Loss: 0.00003012
Iteration 256/1000 | Loss: 0.00003000
Iteration 257/1000 | Loss: 0.00003000
Iteration 258/1000 | Loss: 0.00002997
Iteration 259/1000 | Loss: 0.00002997
Iteration 260/1000 | Loss: 0.00002996
Iteration 261/1000 | Loss: 0.00002996
Iteration 262/1000 | Loss: 0.00002996
Iteration 263/1000 | Loss: 0.00002995
Iteration 264/1000 | Loss: 0.00002994
Iteration 265/1000 | Loss: 0.00002994
Iteration 266/1000 | Loss: 0.00002993
Iteration 267/1000 | Loss: 0.00003130
Iteration 268/1000 | Loss: 0.00003060
Iteration 269/1000 | Loss: 0.00003134
Iteration 270/1000 | Loss: 0.00003044
Iteration 271/1000 | Loss: 0.00003069
Iteration 272/1000 | Loss: 0.00003495
Iteration 273/1000 | Loss: 0.00003636
Iteration 274/1000 | Loss: 0.00003753
Iteration 275/1000 | Loss: 0.00003581
Iteration 276/1000 | Loss: 0.00003760
Iteration 277/1000 | Loss: 0.00003606
Iteration 278/1000 | Loss: 0.00003580
Iteration 279/1000 | Loss: 0.00003698
Iteration 280/1000 | Loss: 0.00003665
Iteration 281/1000 | Loss: 0.00003290
Iteration 282/1000 | Loss: 0.00003448
Iteration 283/1000 | Loss: 0.00003708
Iteration 284/1000 | Loss: 0.00003713
Iteration 285/1000 | Loss: 0.00003912
Iteration 286/1000 | Loss: 0.00005388
Iteration 287/1000 | Loss: 0.00003992
Iteration 288/1000 | Loss: 0.00003692
Iteration 289/1000 | Loss: 0.00003895
Iteration 290/1000 | Loss: 0.00003790
Iteration 291/1000 | Loss: 0.00003902
Iteration 292/1000 | Loss: 0.00003928
Iteration 293/1000 | Loss: 0.00003889
Iteration 294/1000 | Loss: 0.00003931
Iteration 295/1000 | Loss: 0.00004011
Iteration 296/1000 | Loss: 0.00003815
Iteration 297/1000 | Loss: 0.00003998
Iteration 298/1000 | Loss: 0.00004709
Iteration 299/1000 | Loss: 0.00003969
Iteration 300/1000 | Loss: 0.00004416
Iteration 301/1000 | Loss: 0.00003857
Iteration 302/1000 | Loss: 0.00004673
Iteration 303/1000 | Loss: 0.00003355
Iteration 304/1000 | Loss: 0.00003835
Iteration 305/1000 | Loss: 0.00003262
Iteration 306/1000 | Loss: 0.00003113
Iteration 307/1000 | Loss: 0.00003054
Iteration 308/1000 | Loss: 0.00003943
Iteration 309/1000 | Loss: 0.00003168
Iteration 310/1000 | Loss: 0.00003029
Iteration 311/1000 | Loss: 0.00002994
Iteration 312/1000 | Loss: 0.00002994
Iteration 313/1000 | Loss: 0.00002994
Iteration 314/1000 | Loss: 0.00002994
Iteration 315/1000 | Loss: 0.00002994
Iteration 316/1000 | Loss: 0.00002994
Iteration 317/1000 | Loss: 0.00002994
Iteration 318/1000 | Loss: 0.00003081
Iteration 319/1000 | Loss: 0.00003040
Iteration 320/1000 | Loss: 0.00002992
Iteration 321/1000 | Loss: 0.00002992
Iteration 322/1000 | Loss: 0.00002992
Iteration 323/1000 | Loss: 0.00002992
Iteration 324/1000 | Loss: 0.00002992
Iteration 325/1000 | Loss: 0.00002992
Iteration 326/1000 | Loss: 0.00003068
Iteration 327/1000 | Loss: 0.00003028
Iteration 328/1000 | Loss: 0.00003061
Iteration 329/1000 | Loss: 0.00003041
Iteration 330/1000 | Loss: 0.00003049
Iteration 331/1000 | Loss: 0.00003029
Iteration 332/1000 | Loss: 0.00002991
Iteration 333/1000 | Loss: 0.00003046
Iteration 334/1000 | Loss: 0.00003020
Iteration 335/1000 | Loss: 0.00003128
Iteration 336/1000 | Loss: 0.00003103
Iteration 337/1000 | Loss: 0.00003408
Iteration 338/1000 | Loss: 0.00003194
Iteration 339/1000 | Loss: 0.00003398
Iteration 340/1000 | Loss: 0.00002993
Iteration 341/1000 | Loss: 0.00002992
Iteration 342/1000 | Loss: 0.00002992
Iteration 343/1000 | Loss: 0.00002991
Iteration 344/1000 | Loss: 0.00003097
Iteration 345/1000 | Loss: 0.00003137
Iteration 346/1000 | Loss: 0.00003457
Iteration 347/1000 | Loss: 0.00003570
Iteration 348/1000 | Loss: 0.00003908
Iteration 349/1000 | Loss: 0.00003337
Iteration 350/1000 | Loss: 0.00003223
Iteration 351/1000 | Loss: 0.00003162
Iteration 352/1000 | Loss: 0.00003704
Iteration 353/1000 | Loss: 0.00003984
Iteration 354/1000 | Loss: 0.00003446
Iteration 355/1000 | Loss: 0.00003601
Iteration 356/1000 | Loss: 0.00003372
Iteration 357/1000 | Loss: 0.00003618
Iteration 358/1000 | Loss: 0.00003395
Iteration 359/1000 | Loss: 0.00003121
Iteration 360/1000 | Loss: 0.00003093
Iteration 361/1000 | Loss: 0.00003069
Iteration 362/1000 | Loss: 0.00003196
Iteration 363/1000 | Loss: 0.00003601
Iteration 364/1000 | Loss: 0.00003304
Iteration 365/1000 | Loss: 0.00003888
Iteration 366/1000 | Loss: 0.00003888
Iteration 367/1000 | Loss: 0.00003206
Iteration 368/1000 | Loss: 0.00004063
Iteration 369/1000 | Loss: 0.00004310
Iteration 370/1000 | Loss: 0.00003502
Iteration 371/1000 | Loss: 0.00003425
Iteration 372/1000 | Loss: 0.00003430
Iteration 373/1000 | Loss: 0.00003120
Iteration 374/1000 | Loss: 0.00003037
Iteration 375/1000 | Loss: 0.00002996
Iteration 376/1000 | Loss: 0.00003123
Iteration 377/1000 | Loss: 0.00003038
Iteration 378/1000 | Loss: 0.00003090
Iteration 379/1000 | Loss: 0.00003029
Iteration 380/1000 | Loss: 0.00003673
Iteration 381/1000 | Loss: 0.00003689
Iteration 382/1000 | Loss: 0.00004095
Iteration 383/1000 | Loss: 0.00003571
Iteration 384/1000 | Loss: 0.00003364
Iteration 385/1000 | Loss: 0.00003077
Iteration 386/1000 | Loss: 0.00002991
Iteration 387/1000 | Loss: 0.00003668
Iteration 388/1000 | Loss: 0.00003535
Iteration 389/1000 | Loss: 0.00003936
Iteration 390/1000 | Loss: 0.00003643
Iteration 391/1000 | Loss: 0.00003124
Iteration 392/1000 | Loss: 0.00003038
Iteration 393/1000 | Loss: 0.00005448
Iteration 394/1000 | Loss: 0.00003079
Iteration 395/1000 | Loss: 0.00003468
Iteration 396/1000 | Loss: 0.00003050
Iteration 397/1000 | Loss: 0.00003678
Iteration 398/1000 | Loss: 0.00003620
Iteration 399/1000 | Loss: 0.00003825
Iteration 400/1000 | Loss: 0.00003317
Iteration 401/1000 | Loss: 0.00004114
Iteration 402/1000 | Loss: 0.00003110
Iteration 403/1000 | Loss: 0.00004048
Iteration 404/1000 | Loss: 0.00003101
Iteration 405/1000 | Loss: 0.00003458
Iteration 406/1000 | Loss: 0.00003633
Iteration 407/1000 | Loss: 0.00003500
Iteration 408/1000 | Loss: 0.00003048
Iteration 409/1000 | Loss: 0.00004000
Iteration 410/1000 | Loss: 0.00003155
Iteration 411/1000 | Loss: 0.00003964
Iteration 412/1000 | Loss: 0.00004551
Iteration 413/1000 | Loss: 0.00003696
Iteration 414/1000 | Loss: 0.00005270
Iteration 415/1000 | Loss: 0.00003958
Iteration 416/1000 | Loss: 0.00003149
Iteration 417/1000 | Loss: 0.00003100
Iteration 418/1000 | Loss: 0.00003846
Iteration 419/1000 | Loss: 0.00003556
Iteration 420/1000 | Loss: 0.00003716
Iteration 421/1000 | Loss: 0.00003421
Iteration 422/1000 | Loss: 0.00003553
Iteration 423/1000 | Loss: 0.00003242
Iteration 424/1000 | Loss: 0.00003454
Iteration 425/1000 | Loss: 0.00003254
Iteration 426/1000 | Loss: 0.00003627
Iteration 427/1000 | Loss: 0.00003261
Iteration 428/1000 | Loss: 0.00004065
Iteration 429/1000 | Loss: 0.00003235
Iteration 430/1000 | Loss: 0.00003294
Iteration 431/1000 | Loss: 0.00003090
Iteration 432/1000 | Loss: 0.00003765
Iteration 433/1000 | Loss: 0.00003232
Iteration 434/1000 | Loss: 0.00003736
Iteration 435/1000 | Loss: 0.00003287
Iteration 436/1000 | Loss: 0.00004167
Iteration 437/1000 | Loss: 0.00003251
Iteration 438/1000 | Loss: 0.00003232
Iteration 439/1000 | Loss: 0.00003895
Iteration 440/1000 | Loss: 0.00003260
Iteration 441/1000 | Loss: 0.00003129
Iteration 442/1000 | Loss: 0.00003827
Iteration 443/1000 | Loss: 0.00003218
Iteration 444/1000 | Loss: 0.00003660
Iteration 445/1000 | Loss: 0.00003385
Iteration 446/1000 | Loss: 0.00003810
Iteration 447/1000 | Loss: 0.00003723
Iteration 448/1000 | Loss: 0.00003815
Iteration 449/1000 | Loss: 0.00003464
Iteration 450/1000 | Loss: 0.00003488
Iteration 451/1000 | Loss: 0.00005043
Iteration 452/1000 | Loss: 0.00003747
Iteration 453/1000 | Loss: 0.00003024
Iteration 454/1000 | Loss: 0.00003620
Iteration 455/1000 | Loss: 0.00003214
Iteration 456/1000 | Loss: 0.00003283
Iteration 457/1000 | Loss: 0.00003083
Iteration 458/1000 | Loss: 0.00003670
Iteration 459/1000 | Loss: 0.00003249
Iteration 460/1000 | Loss: 0.00003319
Iteration 461/1000 | Loss: 0.00003147
Iteration 462/1000 | Loss: 0.00003705
Iteration 463/1000 | Loss: 0.00003284
Iteration 464/1000 | Loss: 0.00003283
Iteration 465/1000 | Loss: 0.00003284
Iteration 466/1000 | Loss: 0.00003448
Iteration 467/1000 | Loss: 0.00003730
Iteration 468/1000 | Loss: 0.00004423
Iteration 469/1000 | Loss: 0.00003199
Iteration 470/1000 | Loss: 0.00003160
Iteration 471/1000 | Loss: 0.00003024
Iteration 472/1000 | Loss: 0.00003024
Iteration 473/1000 | Loss: 0.00003394
Iteration 474/1000 | Loss: 0.00003231
Iteration 475/1000 | Loss: 0.00003520
Iteration 476/1000 | Loss: 0.00005016
Iteration 477/1000 | Loss: 0.00003923
Iteration 478/1000 | Loss: 0.00003031
Iteration 479/1000 | Loss: 0.00003217
Iteration 480/1000 | Loss: 0.00003061
Iteration 481/1000 | Loss: 0.00003253
Iteration 482/1000 | Loss: 0.00003511
Iteration 483/1000 | Loss: 0.00003254
Iteration 484/1000 | Loss: 0.00003231
Iteration 485/1000 | Loss: 0.00003068
Iteration 486/1000 | Loss: 0.00003192
Iteration 487/1000 | Loss: 0.00003839
Iteration 488/1000 | Loss: 0.00003805
Iteration 489/1000 | Loss: 0.00003810
Iteration 490/1000 | Loss: 0.00003809
Iteration 491/1000 | Loss: 0.00003336
Iteration 492/1000 | Loss: 0.00004364
Iteration 493/1000 | Loss: 0.00003251
Iteration 494/1000 | Loss: 0.00003476
Iteration 495/1000 | Loss: 0.00003802
Iteration 496/1000 | Loss: 0.00003713
Iteration 497/1000 | Loss: 0.00003788
Iteration 498/1000 | Loss: 0.00003282
Iteration 499/1000 | Loss: 0.00003298
Iteration 500/1000 | Loss: 0.00003884
Iteration 501/1000 | Loss: 0.00003400
Iteration 502/1000 | Loss: 0.00003767
Iteration 503/1000 | Loss: 0.00003966
Iteration 504/1000 | Loss: 0.00003463
Iteration 505/1000 | Loss: 0.00003123
Iteration 506/1000 | Loss: 0.00003035
Iteration 507/1000 | Loss: 0.00003285
Iteration 508/1000 | Loss: 0.00003569
Iteration 509/1000 | Loss: 0.00003100
Iteration 510/1000 | Loss: 0.00003176
Iteration 511/1000 | Loss: 0.00003485
Iteration 512/1000 | Loss: 0.00003461
Iteration 513/1000 | Loss: 0.00003728
Iteration 514/1000 | Loss: 0.00003560
Iteration 515/1000 | Loss: 0.00003121
Iteration 516/1000 | Loss: 0.00003171
Iteration 517/1000 | Loss: 0.00003041
Iteration 518/1000 | Loss: 0.00003149
Iteration 519/1000 | Loss: 0.00003375
Iteration 520/1000 | Loss: 0.00003658
Iteration 521/1000 | Loss: 0.00003206
Iteration 522/1000 | Loss: 0.00003168
Iteration 523/1000 | Loss: 0.00003069
Iteration 524/1000 | Loss: 0.00003166
Iteration 525/1000 | Loss: 0.00003050
Iteration 526/1000 | Loss: 0.00003068
Iteration 527/1000 | Loss: 0.00003010
Iteration 528/1000 | Loss: 0.00003067
Iteration 529/1000 | Loss: 0.00003009
Iteration 530/1000 | Loss: 0.00003065
Iteration 531/1000 | Loss: 0.00003241
Iteration 532/1000 | Loss: 0.00003695
Iteration 533/1000 | Loss: 0.00003162
Iteration 534/1000 | Loss: 0.00003298
Iteration 535/1000 | Loss: 0.00003438
Iteration 536/1000 | Loss: 0.00003615
Iteration 537/1000 | Loss: 0.00005231
Iteration 538/1000 | Loss: 0.00004637
Iteration 539/1000 | Loss: 0.00003557
Iteration 540/1000 | Loss: 0.00003895
Iteration 541/1000 | Loss: 0.00003654
Iteration 542/1000 | Loss: 0.00003649
Iteration 543/1000 | Loss: 0.00003854
Iteration 544/1000 | Loss: 0.00004039
Iteration 545/1000 | Loss: 0.00003257
Iteration 546/1000 | Loss: 0.00003418
Iteration 547/1000 | Loss: 0.00004105
Iteration 548/1000 | Loss: 0.00003829
Iteration 549/1000 | Loss: 0.00004495
Iteration 550/1000 | Loss: 0.00004814
Iteration 551/1000 | Loss: 0.00004228
Iteration 552/1000 | Loss: 0.00004516
Iteration 553/1000 | Loss: 0.00003245
Iteration 554/1000 | Loss: 0.00003608
Iteration 555/1000 | Loss: 0.00003879
Iteration 556/1000 | Loss: 0.00003783
Iteration 557/1000 | Loss: 0.00003476
Iteration 558/1000 | Loss: 0.00003740
Iteration 559/1000 | Loss: 0.00003859
Iteration 560/1000 | Loss: 0.00003723
Iteration 561/1000 | Loss: 0.00003762
Iteration 562/1000 | Loss: 0.00003958
Iteration 563/1000 | Loss: 0.00004259
Iteration 564/1000 | Loss: 0.00004028
Iteration 565/1000 | Loss: 0.00004057
Iteration 566/1000 | Loss: 0.00007253
Iteration 567/1000 | Loss: 0.00005210
Iteration 568/1000 | Loss: 0.00003358
Iteration 569/1000 | Loss: 0.00003646
Iteration 570/1000 | Loss: 0.00003664
Iteration 571/1000 | Loss: 0.00003543
Iteration 572/1000 | Loss: 0.00004088
Iteration 573/1000 | Loss: 0.00003738
Iteration 574/1000 | Loss: 0.00004446
Iteration 575/1000 | Loss: 0.00003785
Iteration 576/1000 | Loss: 0.00004089
Iteration 577/1000 | Loss: 0.00003657
Iteration 578/1000 | Loss: 0.00005221
Iteration 579/1000 | Loss: 0.00003646
Iteration 580/1000 | Loss: 0.00003189
Iteration 581/1000 | Loss: 0.00003588
Iteration 582/1000 | Loss: 0.00003945
Iteration 583/1000 | Loss: 0.00003912
Iteration 584/1000 | Loss: 0.00004057
Iteration 585/1000 | Loss: 0.00003898
Iteration 586/1000 | Loss: 0.00004279
Iteration 587/1000 | Loss: 0.00003671
Iteration 588/1000 | Loss: 0.00003933
Iteration 589/1000 | Loss: 0.00003663
Iteration 590/1000 | Loss: 0.00003733
Iteration 591/1000 | Loss: 0.00003782
Iteration 592/1000 | Loss: 0.00003607
Iteration 593/1000 | Loss: 0.00003878
Iteration 594/1000 | Loss: 0.00003886
Iteration 595/1000 | Loss: 0.00003626
Iteration 596/1000 | Loss: 0.00003668
Iteration 597/1000 | Loss: 0.00004230
Iteration 598/1000 | Loss: 0.00003719
Iteration 599/1000 | Loss: 0.00003661
Iteration 600/1000 | Loss: 0.00003600
Iteration 601/1000 | Loss: 0.00003754
Iteration 602/1000 | Loss: 0.00003260
Iteration 603/1000 | Loss: 0.00003848
Iteration 604/1000 | Loss: 0.00003249
Iteration 605/1000 | Loss: 0.00003606
Iteration 606/1000 | Loss: 0.00003650
Iteration 607/1000 | Loss: 0.00003387
Iteration 608/1000 | Loss: 0.00003625
Iteration 609/1000 | Loss: 0.00003540
Iteration 610/1000 | Loss: 0.00003743
Iteration 611/1000 | Loss: 0.00003473
Iteration 612/1000 | Loss: 0.00003804
Iteration 613/1000 | Loss: 0.00003604
Iteration 614/1000 | Loss: 0.00004460
Iteration 615/1000 | Loss: 0.00003286
Iteration 616/1000 | Loss: 0.00003170
Iteration 617/1000 | Loss: 0.00003309
Iteration 618/1000 | Loss: 0.00003856
Iteration 619/1000 | Loss: 0.00003510
Iteration 620/1000 | Loss: 0.00006601
Iteration 621/1000 | Loss: 0.00003761
Iteration 622/1000 | Loss: 0.00003784
Iteration 623/1000 | Loss: 0.00004046
Iteration 624/1000 | Loss: 0.00003974
Iteration 625/1000 | Loss: 0.00003626
Iteration 626/1000 | Loss: 0.00003836
Iteration 627/1000 | Loss: 0.00003571
Iteration 628/1000 | Loss: 0.00003586
Iteration 629/1000 | Loss: 0.00003692
Iteration 630/1000 | Loss: 0.00003902
Iteration 631/1000 | Loss: 0.00003667
Iteration 632/1000 | Loss: 0.00003826
Iteration 633/1000 | Loss: 0.00003842
Iteration 634/1000 | Loss: 0.00003773
Iteration 635/1000 | Loss: 0.00004556
Iteration 636/1000 | Loss: 0.00003880
Iteration 637/1000 | Loss: 0.00003492
Iteration 638/1000 | Loss: 0.00003855
Iteration 639/1000 | Loss: 0.00003443
Iteration 640/1000 | Loss: 0.00003880
Iteration 641/1000 | Loss: 0.00003448
Iteration 642/1000 | Loss: 0.00003825
Iteration 643/1000 | Loss: 0.00003567
Iteration 644/1000 | Loss: 0.00004136
Iteration 645/1000 | Loss: 0.00003380
Iteration 646/1000 | Loss: 0.00003424
Iteration 647/1000 | Loss: 0.00003686
Iteration 648/1000 | Loss: 0.00003684
Iteration 649/1000 | Loss: 0.00003511
Iteration 650/1000 | Loss: 0.00003647
Iteration 651/1000 | Loss: 0.00005133
Iteration 652/1000 | Loss: 0.00003643
Iteration 653/1000 | Loss: 0.00004198
Iteration 654/1000 | Loss: 0.00003367
Iteration 655/1000 | Loss: 0.00003393
Iteration 656/1000 | Loss: 0.00003143
Iteration 657/1000 | Loss: 0.00003393
Iteration 658/1000 | Loss: 0.00003333
Iteration 659/1000 | Loss: 0.00003068
Iteration 660/1000 | Loss: 0.00003196
Iteration 661/1000 | Loss: 0.00003092
Iteration 662/1000 | Loss: 0.00003403
Iteration 663/1000 | Loss: 0.00003430
Iteration 664/1000 | Loss: 0.00003163
Iteration 665/1000 | Loss: 0.00003321
Iteration 666/1000 | Loss: 0.00003163
Iteration 667/1000 | Loss: 0.00004394
Iteration 668/1000 | Loss: 0.00003367
Iteration 669/1000 | Loss: 0.00003057
Iteration 670/1000 | Loss: 0.00003323
Iteration 671/1000 | Loss: 0.00003322
Iteration 672/1000 | Loss: 0.00003648
Iteration 673/1000 | Loss: 0.00003160
Iteration 674/1000 | Loss: 0.00003047
Iteration 675/1000 | Loss: 0.00003017
Iteration 676/1000 | Loss: 0.00003217
Iteration 677/1000 | Loss: 0.00003356
Iteration 678/1000 | Loss: 0.00003413
Iteration 679/1000 | Loss: 0.00003050
Iteration 680/1000 | Loss: 0.00003283
Iteration 681/1000 | Loss: 0.00003521
Iteration 682/1000 | Loss: 0.00005170
Iteration 683/1000 | Loss: 0.00003212
Iteration 684/1000 | Loss: 0.00003309
Iteration 685/1000 | Loss: 0.00004386
Iteration 686/1000 | Loss: 0.00003613
Iteration 687/1000 | Loss: 0.00003633
Iteration 688/1000 | Loss: 0.00003709
Iteration 689/1000 | Loss: 0.00003072
Iteration 690/1000 | Loss: 0.00003252
Iteration 691/1000 | Loss: 0.00003069
Iteration 692/1000 | Loss: 0.00003014
Iteration 693/1000 | Loss: 0.00003006
Iteration 694/1000 | Loss: 0.00003006
Iteration 695/1000 | Loss: 0.00003201
Iteration 696/1000 | Loss: 0.00004062
Iteration 697/1000 | Loss: 0.00003166
Iteration 698/1000 | Loss: 0.00003453
Iteration 699/1000 | Loss: 0.00003696
Iteration 700/1000 | Loss: 0.00003355
Iteration 701/1000 | Loss: 0.00003248
Iteration 702/1000 | Loss: 0.00003617
Iteration 703/1000 | Loss: 0.00004665
Iteration 704/1000 | Loss: 0.00003708
Iteration 705/1000 | Loss: 0.00004092
Iteration 706/1000 | Loss: 0.00003594
Iteration 707/1000 | Loss: 0.00004054
Iteration 708/1000 | Loss: 0.00003307
Iteration 709/1000 | Loss: 0.00003252
Iteration 710/1000 | Loss: 0.00003624
Iteration 711/1000 | Loss: 0.00004044
Iteration 712/1000 | Loss: 0.00003554
Iteration 713/1000 | Loss: 0.00003466
Iteration 714/1000 | Loss: 0.00003170
Iteration 715/1000 | Loss: 0.00003105
Iteration 716/1000 | Loss: 0.00004245
Iteration 717/1000 | Loss: 0.00003658
Iteration 718/1000 | Loss: 0.00003404
Iteration 719/1000 | Loss: 0.00003721
Iteration 720/1000 | Loss: 0.00003640
Iteration 721/1000 | Loss: 0.00003115
Iteration 722/1000 | Loss: 0.00004337
Iteration 723/1000 | Loss: 0.00003871
Iteration 724/1000 | Loss: 0.00004025
Iteration 725/1000 | Loss: 0.00003286
Iteration 726/1000 | Loss: 0.00003701
Iteration 727/1000 | Loss: 0.00003118
Iteration 728/1000 | Loss: 0.00003514
Iteration 729/1000 | Loss: 0.00003021
Iteration 730/1000 | Loss: 0.00003016
Iteration 731/1000 | Loss: 0.00003147
Iteration 732/1000 | Loss: 0.00003061
Iteration 733/1000 | Loss: 0.00003173
Iteration 734/1000 | Loss: 0.00003051
Iteration 735/1000 | Loss: 0.00003094
Iteration 736/1000 | Loss: 0.00003360
Iteration 737/1000 | Loss: 0.00003336
Iteration 738/1000 | Loss: 0.00003528
Iteration 739/1000 | Loss: 0.00003380
Iteration 740/1000 | Loss: 0.00003657
Iteration 741/1000 | Loss: 0.00003476
Iteration 742/1000 | Loss: 0.00003539
Iteration 743/1000 | Loss: 0.00003394
Iteration 744/1000 | Loss: 0.00003682
Iteration 745/1000 | Loss: 0.00003349
Iteration 746/1000 | Loss: 0.00003629
Iteration 747/1000 | Loss: 0.00003318
Iteration 748/1000 | Loss: 0.00003676
Iteration 749/1000 | Loss: 0.00003258
Iteration 750/1000 | Loss: 0.00003612
Iteration 751/1000 | Loss: 0.00003211
Iteration 752/1000 | Loss: 0.00003410
Iteration 753/1000 | Loss: 0.00004322
Iteration 754/1000 | Loss: 0.00006296
Iteration 755/1000 | Loss: 0.00003454
Iteration 756/1000 | Loss: 0.00003167
Iteration 757/1000 | Loss: 0.00003086
Iteration 758/1000 | Loss: 0.00003097
Iteration 759/1000 | Loss: 0.00003055
Iteration 760/1000 | Loss: 0.00004410
Iteration 761/1000 | Loss: 0.00025095
Iteration 762/1000 | Loss: 0.00006106
Iteration 763/1000 | Loss: 0.00013501
Iteration 764/1000 | Loss: 0.00005881
Iteration 765/1000 | Loss: 0.00003379
Iteration 766/1000 | Loss: 0.00003125
Iteration 767/1000 | Loss: 0.00003071
Iteration 768/1000 | Loss: 0.00003087
Iteration 769/1000 | Loss: 0.00003087
Iteration 770/1000 | Loss: 0.00003334
Iteration 771/1000 | Loss: 0.00004406
Iteration 772/1000 | Loss: 0.00003418
Iteration 773/1000 | Loss: 0.00003085
Iteration 774/1000 | Loss: 0.00003274
Iteration 775/1000 | Loss: 0.00003054
Iteration 776/1000 | Loss: 0.00003054
Iteration 777/1000 | Loss: 0.00003059
Iteration 778/1000 | Loss: 0.00003032
Iteration 779/1000 | Loss: 0.00003019
Iteration 780/1000 | Loss: 0.00003018
Iteration 781/1000 | Loss: 0.00003012
Iteration 782/1000 | Loss: 0.00003008
Iteration 783/1000 | Loss: 0.00003008
Iteration 784/1000 | Loss: 0.00003005
Iteration 785/1000 | Loss: 0.00003005
Iteration 786/1000 | Loss: 0.00003005
Iteration 787/1000 | Loss: 0.00003005
Iteration 788/1000 | Loss: 0.00003001
Iteration 789/1000 | Loss: 0.00003000
Iteration 790/1000 | Loss: 0.00003003
Iteration 791/1000 | Loss: 0.00003009
Iteration 792/1000 | Loss: 0.00003006
Iteration 793/1000 | Loss: 0.00003010
Iteration 794/1000 | Loss: 0.00003085
Iteration 795/1000 | Loss: 0.00003404
Iteration 796/1000 | Loss: 0.00003402
Iteration 797/1000 | Loss: 0.00003436
Iteration 798/1000 | Loss: 0.00003488
Iteration 799/1000 | Loss: 0.00003488
Iteration 800/1000 | Loss: 0.00003606
Iteration 801/1000 | Loss: 0.00003178
Iteration 802/1000 | Loss: 0.00003127
Iteration 803/1000 | Loss: 0.00003474
Iteration 804/1000 | Loss: 0.00003127
Iteration 805/1000 | Loss: 0.00003043
Iteration 806/1000 | Loss: 0.00003010
Iteration 807/1000 | Loss: 0.00003003
Iteration 808/1000 | Loss: 0.00003091
Iteration 809/1000 | Loss: 0.00003458
Iteration 810/1000 | Loss: 0.00003201
Iteration 811/1000 | Loss: 0.00006110
Iteration 812/1000 | Loss: 0.00004255
Iteration 813/1000 | Loss: 0.00003285
Iteration 814/1000 | Loss: 0.00003140
Iteration 815/1000 | Loss: 0.00003877
Iteration 816/1000 | Loss: 0.00003535
Iteration 817/1000 | Loss: 0.00004013
Iteration 818/1000 | Loss: 0.00003453
Iteration 819/1000 | Loss: 0.00004848
Iteration 820/1000 | Loss: 0.00003463
Iteration 821/1000 | Loss: 0.00003169
Iteration 822/1000 | Loss: 0.00003701
Iteration 823/1000 | Loss: 0.00003251
Iteration 824/1000 | Loss: 0.00003339
Iteration 825/1000 | Loss: 0.00003380
Iteration 826/1000 | Loss: 0.00003380
Iteration 827/1000 | Loss: 0.00003379
Iteration 828/1000 | Loss: 0.00005969
Iteration 829/1000 | Loss: 0.00003277
Iteration 830/1000 | Loss: 0.00004213
Iteration 831/1000 | Loss: 0.00003536
Iteration 832/1000 | Loss: 0.00003364
Iteration 833/1000 | Loss: 0.00003658
Iteration 834/1000 | Loss: 0.00003531
Iteration 835/1000 | Loss: 0.00003714
Iteration 836/1000 | Loss: 0.00003477
Iteration 837/1000 | Loss: 0.00003657
Iteration 838/1000 | Loss: 0.00004127
Iteration 839/1000 | Loss: 0.00003613
Iteration 840/1000 | Loss: 0.00003600
Iteration 841/1000 | Loss: 0.00003694
Iteration 842/1000 | Loss: 0.00003862
Iteration 843/1000 | Loss: 0.00003540
Iteration 844/1000 | Loss: 0.00003497
Iteration 845/1000 | Loss: 0.00003506
Iteration 846/1000 | Loss: 0.00003571
Iteration 847/1000 | Loss: 0.00003804
Iteration 848/1000 | Loss: 0.00003554
Iteration 849/1000 | Loss: 0.00003466
Iteration 850/1000 | Loss: 0.00003382
Iteration 851/1000 | Loss: 0.00003477
Iteration 852/1000 | Loss: 0.00003336
Iteration 853/1000 | Loss: 0.00003778
Iteration 854/1000 | Loss: 0.00003825
Iteration 855/1000 | Loss: 0.00003627
Iteration 856/1000 | Loss: 0.00003305
Iteration 857/1000 | Loss: 0.00003839
Iteration 858/1000 | Loss: 0.00003326
Iteration 859/1000 | Loss: 0.00003527
Iteration 860/1000 | Loss: 0.00003971
Iteration 861/1000 | Loss: 0.00003507
Iteration 862/1000 | Loss: 0.00003481
Iteration 863/1000 | Loss: 0.00003844
Iteration 864/1000 | Loss: 0.00003811
Iteration 865/1000 | Loss: 0.00003586
Iteration 866/1000 | Loss: 0.00003815
Iteration 867/1000 | Loss: 0.00003444
Iteration 868/1000 | Loss: 0.00003419
Iteration 869/1000 | Loss: 0.00003207
Iteration 870/1000 | Loss: 0.00003238
Iteration 871/1000 | Loss: 0.00003448
Iteration 872/1000 | Loss: 0.00003469
Iteration 873/1000 | Loss: 0.00003395
Iteration 874/1000 | Loss: 0.00003402
Iteration 875/1000 | Loss: 0.00003540
Iteration 876/1000 | Loss: 0.00003630
Iteration 877/1000 | Loss: 0.00003516
Iteration 878/1000 | Loss: 0.00003714
Iteration 879/1000 | Loss: 0.00003552
Iteration 880/1000 | Loss: 0.00003442
Iteration 881/1000 | Loss: 0.00003908
Iteration 882/1000 | Loss: 0.00003374
Iteration 883/1000 | Loss: 0.00003732
Iteration 884/1000 | Loss: 0.00003327
Iteration 885/1000 | Loss: 0.00003396
Iteration 886/1000 | Loss: 0.00003445
Iteration 887/1000 | Loss: 0.00003463
Iteration 888/1000 | Loss: 0.00003490
Iteration 889/1000 | Loss: 0.00003674
Iteration 890/1000 | Loss: 0.00003636
Iteration 891/1000 | Loss: 0.00003777
Iteration 892/1000 | Loss: 0.00003395
Iteration 893/1000 | Loss: 0.00003606
Iteration 894/1000 | Loss: 0.00003581
Iteration 895/1000 | Loss: 0.00003692
Iteration 896/1000 | Loss: 0.00003742
Iteration 897/1000 | Loss: 0.00003754
Iteration 898/1000 | Loss: 0.00003698
Iteration 899/1000 | Loss: 0.00003543
Iteration 900/1000 | Loss: 0.00003353
Iteration 901/1000 | Loss: 0.00003560
Iteration 902/1000 | Loss: 0.00003581
Iteration 903/1000 | Loss: 0.00003656
Iteration 904/1000 | Loss: 0.00003460
Iteration 905/1000 | Loss: 0.00003506
Iteration 906/1000 | Loss: 0.00003552
Iteration 907/1000 | Loss: 0.00003439
Iteration 908/1000 | Loss: 0.00003535
Iteration 909/1000 | Loss: 0.00003540
Iteration 910/1000 | Loss: 0.00003544
Iteration 911/1000 | Loss: 0.00003498
Iteration 912/1000 | Loss: 0.00003492
Iteration 913/1000 | Loss: 0.00003626
Iteration 914/1000 | Loss: 0.00003384
Iteration 915/1000 | Loss: 0.00003457
Iteration 916/1000 | Loss: 0.00003799
Iteration 917/1000 | Loss: 0.00003517
Iteration 918/1000 | Loss: 0.00003743
Iteration 919/1000 | Loss: 0.00003400
Iteration 920/1000 | Loss: 0.00003354
Iteration 921/1000 | Loss: 0.00003558
Iteration 922/1000 | Loss: 0.00003944
Iteration 923/1000 | Loss: 0.00004188
Iteration 924/1000 | Loss: 0.00003801
Iteration 925/1000 | Loss: 0.00003813
Iteration 926/1000 | Loss: 0.00004249
Iteration 927/1000 | Loss: 0.00003701
Iteration 928/1000 | Loss: 0.00003879
Iteration 929/1000 | Loss: 0.00003533
Iteration 930/1000 | Loss: 0.00003597
Iteration 931/1000 | Loss: 0.00003464
Iteration 932/1000 | Loss: 0.00004120
Iteration 933/1000 | Loss: 0.00004140
Iteration 934/1000 | Loss: 0.00003492
Iteration 935/1000 | Loss: 0.00003544
Iteration 936/1000 | Loss: 0.00004404
Iteration 937/1000 | Loss: 0.00003606
Iteration 938/1000 | Loss: 0.00005688
Iteration 939/1000 | Loss: 0.00003527
Iteration 940/1000 | Loss: 0.00003388
Iteration 941/1000 | Loss: 0.00003660
Iteration 942/1000 | Loss: 0.00003954
Iteration 943/1000 | Loss: 0.00003255
Iteration 944/1000 | Loss: 0.00004383
Iteration 945/1000 | Loss: 0.00003740
Iteration 946/1000 | Loss: 0.00003457
Iteration 947/1000 | Loss: 0.00003306
Iteration 948/1000 | Loss: 0.00003506
Iteration 949/1000 | Loss: 0.00003883
Iteration 950/1000 | Loss: 0.00003700
Iteration 951/1000 | Loss: 0.00003796
Iteration 952/1000 | Loss: 0.00003598
Iteration 953/1000 | Loss: 0.00003712
Iteration 954/1000 | Loss: 0.00003661
Iteration 955/1000 | Loss: 0.00003882
Iteration 956/1000 | Loss: 0.00003445
Iteration 957/1000 | Loss: 0.00003799
Iteration 958/1000 | Loss: 0.00003593
Iteration 959/1000 | Loss: 0.00003732
Iteration 960/1000 | Loss: 0.00003547
Iteration 961/1000 | Loss: 0.00003747
Iteration 962/1000 | Loss: 0.00003875
Iteration 963/1000 | Loss: 0.00003117
Iteration 964/1000 | Loss: 0.00004131
Iteration 965/1000 | Loss: 0.00003810
Iteration 966/1000 | Loss: 0.00003425
Iteration 967/1000 | Loss: 0.00003803
Iteration 968/1000 | Loss: 0.00003893
Iteration 969/1000 | Loss: 0.00003462
Iteration 970/1000 | Loss: 0.00003690
Iteration 971/1000 | Loss: 0.00003034
Iteration 972/1000 | Loss: 0.00003431
Iteration 973/1000 | Loss: 0.00003319
Iteration 974/1000 | Loss: 0.00003262
Iteration 975/1000 | Loss: 0.00003520
Iteration 976/1000 | Loss: 0.00003449
Iteration 977/1000 | Loss: 0.00003634
Iteration 978/1000 | Loss: 0.00003774
Iteration 979/1000 | Loss: 0.00003769
Iteration 980/1000 | Loss: 0.00003707
Iteration 981/1000 | Loss: 0.00003539
Iteration 982/1000 | Loss: 0.00003590
Iteration 983/1000 | Loss: 0.00003572
Iteration 984/1000 | Loss: 0.00003899
Iteration 985/1000 | Loss: 0.00003568
Iteration 986/1000 | Loss: 0.00003889
Iteration 987/1000 | Loss: 0.00003714
Iteration 988/1000 | Loss: 0.00003784
Iteration 989/1000 | Loss: 0.00003633
Iteration 990/1000 | Loss: 0.00003979
Iteration 991/1000 | Loss: 0.00003584
Iteration 992/1000 | Loss: 0.00003753
Iteration 993/1000 | Loss: 0.00003672
Iteration 994/1000 | Loss: 0.00003314
Iteration 995/1000 | Loss: 0.00003637
Iteration 996/1000 | Loss: 0.00003517
Iteration 997/1000 | Loss: 0.00003518
Iteration 998/1000 | Loss: 0.00003546
Iteration 999/1000 | Loss: 0.00003859
Iteration 1000/1000 | Loss: 0.00003440

Optimization complete. Final v2v error: 4.5716166496276855 mm

Highest mean error: 15.86467170715332 mm for frame 102

Lowest mean error: 3.8796467781066895 mm for frame 21

Saving results

Total time: 1258.1691915988922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010517
Iteration 2/25 | Loss: 0.00285422
Iteration 3/25 | Loss: 0.00205045
Iteration 4/25 | Loss: 0.00195310
Iteration 5/25 | Loss: 0.00184703
Iteration 6/25 | Loss: 0.00173642
Iteration 7/25 | Loss: 0.00170054
Iteration 8/25 | Loss: 0.00168287
Iteration 9/25 | Loss: 0.00167627
Iteration 10/25 | Loss: 0.00172332
Iteration 11/25 | Loss: 0.00168314
Iteration 12/25 | Loss: 0.00159542
Iteration 13/25 | Loss: 0.00151992
Iteration 14/25 | Loss: 0.00148903
Iteration 15/25 | Loss: 0.00147681
Iteration 16/25 | Loss: 0.00145692
Iteration 17/25 | Loss: 0.00144929
Iteration 18/25 | Loss: 0.00145281
Iteration 19/25 | Loss: 0.00144679
Iteration 20/25 | Loss: 0.00144781
Iteration 21/25 | Loss: 0.00144246
Iteration 22/25 | Loss: 0.00144362
Iteration 23/25 | Loss: 0.00144081
Iteration 24/25 | Loss: 0.00144755
Iteration 25/25 | Loss: 0.00144248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23774898
Iteration 2/25 | Loss: 0.00324646
Iteration 3/25 | Loss: 0.00324646
Iteration 4/25 | Loss: 0.00324646
Iteration 5/25 | Loss: 0.00324645
Iteration 6/25 | Loss: 0.00324645
Iteration 7/25 | Loss: 0.00324645
Iteration 8/25 | Loss: 0.00323788
Iteration 9/25 | Loss: 0.00323788
Iteration 10/25 | Loss: 0.00323788
Iteration 11/25 | Loss: 0.00323788
Iteration 12/25 | Loss: 0.00323788
Iteration 13/25 | Loss: 0.00323788
Iteration 14/25 | Loss: 0.00323788
Iteration 15/25 | Loss: 0.00323788
Iteration 16/25 | Loss: 0.00323788
Iteration 17/25 | Loss: 0.00323788
Iteration 18/25 | Loss: 0.00323788
Iteration 19/25 | Loss: 0.00323788
Iteration 20/25 | Loss: 0.00323788
Iteration 21/25 | Loss: 0.00323788
Iteration 22/25 | Loss: 0.00323788
Iteration 23/25 | Loss: 0.00323788
Iteration 24/25 | Loss: 0.00323788
Iteration 25/25 | Loss: 0.00323788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00323788
Iteration 2/1000 | Loss: 0.00044885
Iteration 3/1000 | Loss: 0.00032635
Iteration 4/1000 | Loss: 0.00022792
Iteration 5/1000 | Loss: 0.00042079
Iteration 6/1000 | Loss: 0.00019258
Iteration 7/1000 | Loss: 0.00036791
Iteration 8/1000 | Loss: 0.00014598
Iteration 9/1000 | Loss: 0.00067852
Iteration 10/1000 | Loss: 0.00015408
Iteration 11/1000 | Loss: 0.00010130
Iteration 12/1000 | Loss: 0.00019150
Iteration 13/1000 | Loss: 0.00112454
Iteration 14/1000 | Loss: 0.00066135
Iteration 15/1000 | Loss: 0.00034292
Iteration 16/1000 | Loss: 0.00010207
Iteration 17/1000 | Loss: 0.00018819
Iteration 18/1000 | Loss: 0.00021826
Iteration 19/1000 | Loss: 0.00017882
Iteration 20/1000 | Loss: 0.00010961
Iteration 21/1000 | Loss: 0.00120288
Iteration 22/1000 | Loss: 0.00047399
Iteration 23/1000 | Loss: 0.00140442
Iteration 24/1000 | Loss: 0.00121597
Iteration 25/1000 | Loss: 0.00065545
Iteration 26/1000 | Loss: 0.00042982
Iteration 27/1000 | Loss: 0.00101796
Iteration 28/1000 | Loss: 0.00022176
Iteration 29/1000 | Loss: 0.00018143
Iteration 30/1000 | Loss: 0.00019106
Iteration 31/1000 | Loss: 0.00015839
Iteration 32/1000 | Loss: 0.00064076
Iteration 33/1000 | Loss: 0.00017434
Iteration 34/1000 | Loss: 0.00008895
Iteration 35/1000 | Loss: 0.00010091
Iteration 36/1000 | Loss: 0.00015026
Iteration 37/1000 | Loss: 0.00008569
Iteration 38/1000 | Loss: 0.00023507
Iteration 39/1000 | Loss: 0.00012652
Iteration 40/1000 | Loss: 0.00010312
Iteration 41/1000 | Loss: 0.00019584
Iteration 42/1000 | Loss: 0.00017341
Iteration 43/1000 | Loss: 0.00017409
Iteration 44/1000 | Loss: 0.00018740
Iteration 45/1000 | Loss: 0.00009172
Iteration 46/1000 | Loss: 0.00007833
Iteration 47/1000 | Loss: 0.00007443
Iteration 48/1000 | Loss: 0.00007238
Iteration 49/1000 | Loss: 0.00007118
Iteration 50/1000 | Loss: 0.00097074
Iteration 51/1000 | Loss: 0.00097591
Iteration 52/1000 | Loss: 0.00059040
Iteration 53/1000 | Loss: 0.00068797
Iteration 54/1000 | Loss: 0.00009565
Iteration 55/1000 | Loss: 0.00008601
Iteration 56/1000 | Loss: 0.00007571
Iteration 57/1000 | Loss: 0.00009757
Iteration 58/1000 | Loss: 0.00016053
Iteration 59/1000 | Loss: 0.00008591
Iteration 60/1000 | Loss: 0.00016265
Iteration 61/1000 | Loss: 0.00009560
Iteration 62/1000 | Loss: 0.00013695
Iteration 63/1000 | Loss: 0.00021304
Iteration 64/1000 | Loss: 0.00019348
Iteration 65/1000 | Loss: 0.00023363
Iteration 66/1000 | Loss: 0.00022938
Iteration 67/1000 | Loss: 0.00020524
Iteration 68/1000 | Loss: 0.00022571
Iteration 69/1000 | Loss: 0.00016465
Iteration 70/1000 | Loss: 0.00014761
Iteration 71/1000 | Loss: 0.00013754
Iteration 72/1000 | Loss: 0.00015365
Iteration 73/1000 | Loss: 0.00012366
Iteration 74/1000 | Loss: 0.00013546
Iteration 75/1000 | Loss: 0.00014232
Iteration 76/1000 | Loss: 0.00014371
Iteration 77/1000 | Loss: 0.00013751
Iteration 78/1000 | Loss: 0.00015111
Iteration 79/1000 | Loss: 0.00028011
Iteration 80/1000 | Loss: 0.00020666
Iteration 81/1000 | Loss: 0.00014060
Iteration 82/1000 | Loss: 0.00025244
Iteration 83/1000 | Loss: 0.00010721
Iteration 84/1000 | Loss: 0.00025099
Iteration 85/1000 | Loss: 0.00008791
Iteration 86/1000 | Loss: 0.00010369
Iteration 87/1000 | Loss: 0.00013745
Iteration 88/1000 | Loss: 0.00009820
Iteration 89/1000 | Loss: 0.00021613
Iteration 90/1000 | Loss: 0.00019416
Iteration 91/1000 | Loss: 0.00009606
Iteration 92/1000 | Loss: 0.00009625
Iteration 93/1000 | Loss: 0.00009937
Iteration 94/1000 | Loss: 0.00008812
Iteration 95/1000 | Loss: 0.00011307
Iteration 96/1000 | Loss: 0.00008815
Iteration 97/1000 | Loss: 0.00011459
Iteration 98/1000 | Loss: 0.00006545
Iteration 99/1000 | Loss: 0.00006495
Iteration 100/1000 | Loss: 0.00006378
Iteration 101/1000 | Loss: 0.00006219
Iteration 102/1000 | Loss: 0.00006210
Iteration 103/1000 | Loss: 0.00006101
Iteration 104/1000 | Loss: 0.00007017
Iteration 105/1000 | Loss: 0.00006250
Iteration 106/1000 | Loss: 0.00006423
Iteration 107/1000 | Loss: 0.00006356
Iteration 108/1000 | Loss: 0.00006302
Iteration 109/1000 | Loss: 0.00019387
Iteration 110/1000 | Loss: 0.00016796
Iteration 111/1000 | Loss: 0.00007742
Iteration 112/1000 | Loss: 0.00012079
Iteration 113/1000 | Loss: 0.00019576
Iteration 114/1000 | Loss: 0.00011174
Iteration 115/1000 | Loss: 0.00010779
Iteration 116/1000 | Loss: 0.00014018
Iteration 117/1000 | Loss: 0.00011436
Iteration 118/1000 | Loss: 0.00012127
Iteration 119/1000 | Loss: 0.00016177
Iteration 120/1000 | Loss: 0.00011275
Iteration 121/1000 | Loss: 0.00011569
Iteration 122/1000 | Loss: 0.00010965
Iteration 123/1000 | Loss: 0.00006163
Iteration 124/1000 | Loss: 0.00006578
Iteration 125/1000 | Loss: 0.00006236
Iteration 126/1000 | Loss: 0.00006081
Iteration 127/1000 | Loss: 0.00007411
Iteration 128/1000 | Loss: 0.00006045
Iteration 129/1000 | Loss: 0.00006834
Iteration 130/1000 | Loss: 0.00006084
Iteration 131/1000 | Loss: 0.00005987
Iteration 132/1000 | Loss: 0.00005987
Iteration 133/1000 | Loss: 0.00006087
Iteration 134/1000 | Loss: 0.00006037
Iteration 135/1000 | Loss: 0.00006559
Iteration 136/1000 | Loss: 0.00006744
Iteration 137/1000 | Loss: 0.00006213
Iteration 138/1000 | Loss: 0.00006494
Iteration 139/1000 | Loss: 0.00005983
Iteration 140/1000 | Loss: 0.00005972
Iteration 141/1000 | Loss: 0.00005972
Iteration 142/1000 | Loss: 0.00005972
Iteration 143/1000 | Loss: 0.00005971
Iteration 144/1000 | Loss: 0.00005971
Iteration 145/1000 | Loss: 0.00005971
Iteration 146/1000 | Loss: 0.00005971
Iteration 147/1000 | Loss: 0.00005971
Iteration 148/1000 | Loss: 0.00005971
Iteration 149/1000 | Loss: 0.00005971
Iteration 150/1000 | Loss: 0.00005971
Iteration 151/1000 | Loss: 0.00005970
Iteration 152/1000 | Loss: 0.00005970
Iteration 153/1000 | Loss: 0.00005970
Iteration 154/1000 | Loss: 0.00005969
Iteration 155/1000 | Loss: 0.00005969
Iteration 156/1000 | Loss: 0.00005969
Iteration 157/1000 | Loss: 0.00005969
Iteration 158/1000 | Loss: 0.00005969
Iteration 159/1000 | Loss: 0.00005969
Iteration 160/1000 | Loss: 0.00006091
Iteration 161/1000 | Loss: 0.00006356
Iteration 162/1000 | Loss: 0.00006289
Iteration 163/1000 | Loss: 0.00005964
Iteration 164/1000 | Loss: 0.00006162
Iteration 165/1000 | Loss: 0.00006020
Iteration 166/1000 | Loss: 0.00006320
Iteration 167/1000 | Loss: 0.00006229
Iteration 168/1000 | Loss: 0.00006036
Iteration 169/1000 | Loss: 0.00006087
Iteration 170/1000 | Loss: 0.00006139
Iteration 171/1000 | Loss: 0.00006214
Iteration 172/1000 | Loss: 0.00006108
Iteration 173/1000 | Loss: 0.00006459
Iteration 174/1000 | Loss: 0.00006474
Iteration 175/1000 | Loss: 0.00006484
Iteration 176/1000 | Loss: 0.00006464
Iteration 177/1000 | Loss: 0.00006480
Iteration 178/1000 | Loss: 0.00006060
Iteration 179/1000 | Loss: 0.00005964
Iteration 180/1000 | Loss: 0.00005961
Iteration 181/1000 | Loss: 0.00005961
Iteration 182/1000 | Loss: 0.00005961
Iteration 183/1000 | Loss: 0.00005961
Iteration 184/1000 | Loss: 0.00005960
Iteration 185/1000 | Loss: 0.00005960
Iteration 186/1000 | Loss: 0.00005960
Iteration 187/1000 | Loss: 0.00005960
Iteration 188/1000 | Loss: 0.00005960
Iteration 189/1000 | Loss: 0.00005959
Iteration 190/1000 | Loss: 0.00005958
Iteration 191/1000 | Loss: 0.00005957
Iteration 192/1000 | Loss: 0.00005956
Iteration 193/1000 | Loss: 0.00005956
Iteration 194/1000 | Loss: 0.00005955
Iteration 195/1000 | Loss: 0.00005955
Iteration 196/1000 | Loss: 0.00005955
Iteration 197/1000 | Loss: 0.00005955
Iteration 198/1000 | Loss: 0.00005955
Iteration 199/1000 | Loss: 0.00005955
Iteration 200/1000 | Loss: 0.00005955
Iteration 201/1000 | Loss: 0.00005954
Iteration 202/1000 | Loss: 0.00005954
Iteration 203/1000 | Loss: 0.00005954
Iteration 204/1000 | Loss: 0.00006043
Iteration 205/1000 | Loss: 0.00005983
Iteration 206/1000 | Loss: 0.00005953
Iteration 207/1000 | Loss: 0.00005953
Iteration 208/1000 | Loss: 0.00005953
Iteration 209/1000 | Loss: 0.00006038
Iteration 210/1000 | Loss: 0.00006149
Iteration 211/1000 | Loss: 0.00006454
Iteration 212/1000 | Loss: 0.00006031
Iteration 213/1000 | Loss: 0.00006855
Iteration 214/1000 | Loss: 0.00006160
Iteration 215/1000 | Loss: 0.00006242
Iteration 216/1000 | Loss: 0.00006021
Iteration 217/1000 | Loss: 0.00005952
Iteration 218/1000 | Loss: 0.00005952
Iteration 219/1000 | Loss: 0.00005951
Iteration 220/1000 | Loss: 0.00006049
Iteration 221/1000 | Loss: 0.00005988
Iteration 222/1000 | Loss: 0.00005951
Iteration 223/1000 | Loss: 0.00005951
Iteration 224/1000 | Loss: 0.00005951
Iteration 225/1000 | Loss: 0.00005951
Iteration 226/1000 | Loss: 0.00005951
Iteration 227/1000 | Loss: 0.00005951
Iteration 228/1000 | Loss: 0.00005951
Iteration 229/1000 | Loss: 0.00005951
Iteration 230/1000 | Loss: 0.00005951
Iteration 231/1000 | Loss: 0.00005951
Iteration 232/1000 | Loss: 0.00005951
Iteration 233/1000 | Loss: 0.00005951
Iteration 234/1000 | Loss: 0.00005951
Iteration 235/1000 | Loss: 0.00005951
Iteration 236/1000 | Loss: 0.00005951
Iteration 237/1000 | Loss: 0.00005951
Iteration 238/1000 | Loss: 0.00005951
Iteration 239/1000 | Loss: 0.00005951
Iteration 240/1000 | Loss: 0.00005951
Iteration 241/1000 | Loss: 0.00005951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [5.950727791059762e-05, 5.950727791059762e-05, 5.950727791059762e-05, 5.950727791059762e-05, 5.950727791059762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.950727791059762e-05

Optimization complete. Final v2v error: 5.156624794006348 mm

Highest mean error: 12.458697319030762 mm for frame 46

Lowest mean error: 3.414003849029541 mm for frame 35

Saving results

Total time: 283.01805901527405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958949
Iteration 2/25 | Loss: 0.00171936
Iteration 3/25 | Loss: 0.00156661
Iteration 4/25 | Loss: 0.00153055
Iteration 5/25 | Loss: 0.00152564
Iteration 6/25 | Loss: 0.00149297
Iteration 7/25 | Loss: 0.00149105
Iteration 8/25 | Loss: 0.00148818
Iteration 9/25 | Loss: 0.00148723
Iteration 10/25 | Loss: 0.00148736
Iteration 11/25 | Loss: 0.00148699
Iteration 12/25 | Loss: 0.00148682
Iteration 13/25 | Loss: 0.00148713
Iteration 14/25 | Loss: 0.00148699
Iteration 15/25 | Loss: 0.00148719
Iteration 16/25 | Loss: 0.00148698
Iteration 17/25 | Loss: 0.00148719
Iteration 18/25 | Loss: 0.00148701
Iteration 19/25 | Loss: 0.00148724
Iteration 20/25 | Loss: 0.00148699
Iteration 21/25 | Loss: 0.00148716
Iteration 22/25 | Loss: 0.00148694
Iteration 23/25 | Loss: 0.00148711
Iteration 24/25 | Loss: 0.00148694
Iteration 25/25 | Loss: 0.00148708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.43362141
Iteration 2/25 | Loss: 0.00219122
Iteration 3/25 | Loss: 0.00219118
Iteration 4/25 | Loss: 0.00219118
Iteration 5/25 | Loss: 0.00219118
Iteration 6/25 | Loss: 0.00219118
Iteration 7/25 | Loss: 0.00219118
Iteration 8/25 | Loss: 0.00219118
Iteration 9/25 | Loss: 0.00219118
Iteration 10/25 | Loss: 0.00219118
Iteration 11/25 | Loss: 0.00219118
Iteration 12/25 | Loss: 0.00219118
Iteration 13/25 | Loss: 0.00219118
Iteration 14/25 | Loss: 0.00219118
Iteration 15/25 | Loss: 0.00219118
Iteration 16/25 | Loss: 0.00219118
Iteration 17/25 | Loss: 0.00219118
Iteration 18/25 | Loss: 0.00219118
Iteration 19/25 | Loss: 0.00219118
Iteration 20/25 | Loss: 0.00219118
Iteration 21/25 | Loss: 0.00219118
Iteration 22/25 | Loss: 0.00219118
Iteration 23/25 | Loss: 0.00219118
Iteration 24/25 | Loss: 0.00219118
Iteration 25/25 | Loss: 0.00219118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219118
Iteration 2/1000 | Loss: 0.00005510
Iteration 3/1000 | Loss: 0.00003754
Iteration 4/1000 | Loss: 0.00003023
Iteration 5/1000 | Loss: 0.00002699
Iteration 6/1000 | Loss: 0.00002496
Iteration 7/1000 | Loss: 0.00002390
Iteration 8/1000 | Loss: 0.00002320
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00002246
Iteration 11/1000 | Loss: 0.00002206
Iteration 12/1000 | Loss: 0.00002189
Iteration 13/1000 | Loss: 0.00002178
Iteration 14/1000 | Loss: 0.00002233
Iteration 15/1000 | Loss: 0.00002184
Iteration 16/1000 | Loss: 0.00002220
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00002201
Iteration 19/1000 | Loss: 0.00002168
Iteration 20/1000 | Loss: 0.00002153
Iteration 21/1000 | Loss: 0.00002153
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002152
Iteration 24/1000 | Loss: 0.00002152
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002152
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002176
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002147
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002178
Iteration 35/1000 | Loss: 0.00002149
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002137
Iteration 54/1000 | Loss: 0.00002137
Iteration 55/1000 | Loss: 0.00002137
Iteration 56/1000 | Loss: 0.00002137
Iteration 57/1000 | Loss: 0.00002137
Iteration 58/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.1366959117585793e-05, 2.1366959117585793e-05, 2.1366959117585793e-05, 2.1366959117585793e-05, 2.1366959117585793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1366959117585793e-05

Optimization complete. Final v2v error: 3.969545841217041 mm

Highest mean error: 10.439919471740723 mm for frame 84

Lowest mean error: 3.5911874771118164 mm for frame 236

Saving results

Total time: 85.46865129470825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069138
Iteration 2/25 | Loss: 0.00233067
Iteration 3/25 | Loss: 0.00171677
Iteration 4/25 | Loss: 0.00170380
Iteration 5/25 | Loss: 0.00162476
Iteration 6/25 | Loss: 0.00160804
Iteration 7/25 | Loss: 0.00160529
Iteration 8/25 | Loss: 0.00160477
Iteration 9/25 | Loss: 0.00160465
Iteration 10/25 | Loss: 0.00160465
Iteration 11/25 | Loss: 0.00160465
Iteration 12/25 | Loss: 0.00160465
Iteration 13/25 | Loss: 0.00160465
Iteration 14/25 | Loss: 0.00160465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016046520322561264, 0.0016046520322561264, 0.0016046520322561264, 0.0016046520322561264, 0.0016046520322561264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016046520322561264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18585956
Iteration 2/25 | Loss: 0.00174197
Iteration 3/25 | Loss: 0.00174197
Iteration 4/25 | Loss: 0.00174197
Iteration 5/25 | Loss: 0.00174197
Iteration 6/25 | Loss: 0.00174197
Iteration 7/25 | Loss: 0.00174197
Iteration 8/25 | Loss: 0.00174197
Iteration 9/25 | Loss: 0.00174197
Iteration 10/25 | Loss: 0.00174197
Iteration 11/25 | Loss: 0.00174197
Iteration 12/25 | Loss: 0.00174197
Iteration 13/25 | Loss: 0.00174197
Iteration 14/25 | Loss: 0.00174197
Iteration 15/25 | Loss: 0.00174197
Iteration 16/25 | Loss: 0.00174197
Iteration 17/25 | Loss: 0.00174197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017419651849195361, 0.0017419651849195361, 0.0017419651849195361, 0.0017419651849195361, 0.0017419651849195361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017419651849195361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174197
Iteration 2/1000 | Loss: 0.00008787
Iteration 3/1000 | Loss: 0.00005500
Iteration 4/1000 | Loss: 0.00003920
Iteration 5/1000 | Loss: 0.00003317
Iteration 6/1000 | Loss: 0.00002950
Iteration 7/1000 | Loss: 0.00002805
Iteration 8/1000 | Loss: 0.00002663
Iteration 9/1000 | Loss: 0.00002594
Iteration 10/1000 | Loss: 0.00002530
Iteration 11/1000 | Loss: 0.00002487
Iteration 12/1000 | Loss: 0.00002458
Iteration 13/1000 | Loss: 0.00002436
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00002425
Iteration 16/1000 | Loss: 0.00002425
Iteration 17/1000 | Loss: 0.00002424
Iteration 18/1000 | Loss: 0.00002423
Iteration 19/1000 | Loss: 0.00002423
Iteration 20/1000 | Loss: 0.00002422
Iteration 21/1000 | Loss: 0.00002418
Iteration 22/1000 | Loss: 0.00002415
Iteration 23/1000 | Loss: 0.00002415
Iteration 24/1000 | Loss: 0.00002415
Iteration 25/1000 | Loss: 0.00002415
Iteration 26/1000 | Loss: 0.00002415
Iteration 27/1000 | Loss: 0.00002415
Iteration 28/1000 | Loss: 0.00002414
Iteration 29/1000 | Loss: 0.00002414
Iteration 30/1000 | Loss: 0.00002414
Iteration 31/1000 | Loss: 0.00002414
Iteration 32/1000 | Loss: 0.00002414
Iteration 33/1000 | Loss: 0.00002414
Iteration 34/1000 | Loss: 0.00002414
Iteration 35/1000 | Loss: 0.00002414
Iteration 36/1000 | Loss: 0.00002414
Iteration 37/1000 | Loss: 0.00002414
Iteration 38/1000 | Loss: 0.00002413
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00002413
Iteration 41/1000 | Loss: 0.00002413
Iteration 42/1000 | Loss: 0.00002412
Iteration 43/1000 | Loss: 0.00002412
Iteration 44/1000 | Loss: 0.00002412
Iteration 45/1000 | Loss: 0.00002412
Iteration 46/1000 | Loss: 0.00002412
Iteration 47/1000 | Loss: 0.00002412
Iteration 48/1000 | Loss: 0.00002412
Iteration 49/1000 | Loss: 0.00002412
Iteration 50/1000 | Loss: 0.00002412
Iteration 51/1000 | Loss: 0.00002411
Iteration 52/1000 | Loss: 0.00002411
Iteration 53/1000 | Loss: 0.00002411
Iteration 54/1000 | Loss: 0.00002411
Iteration 55/1000 | Loss: 0.00002411
Iteration 56/1000 | Loss: 0.00002411
Iteration 57/1000 | Loss: 0.00002411
Iteration 58/1000 | Loss: 0.00002411
Iteration 59/1000 | Loss: 0.00002411
Iteration 60/1000 | Loss: 0.00002411
Iteration 61/1000 | Loss: 0.00002411
Iteration 62/1000 | Loss: 0.00002410
Iteration 63/1000 | Loss: 0.00002410
Iteration 64/1000 | Loss: 0.00002410
Iteration 65/1000 | Loss: 0.00002410
Iteration 66/1000 | Loss: 0.00002410
Iteration 67/1000 | Loss: 0.00002410
Iteration 68/1000 | Loss: 0.00002410
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002409
Iteration 71/1000 | Loss: 0.00002409
Iteration 72/1000 | Loss: 0.00002409
Iteration 73/1000 | Loss: 0.00002409
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002409
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.40878107433673e-05, 2.40878107433673e-05, 2.40878107433673e-05, 2.40878107433673e-05, 2.40878107433673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.40878107433673e-05

Optimization complete. Final v2v error: 4.2414231300354 mm

Highest mean error: 4.527670383453369 mm for frame 64

Lowest mean error: 3.9303536415100098 mm for frame 103

Saving results

Total time: 37.58629512786865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917005
Iteration 2/25 | Loss: 0.00168218
Iteration 3/25 | Loss: 0.00156239
Iteration 4/25 | Loss: 0.00155175
Iteration 5/25 | Loss: 0.00154815
Iteration 6/25 | Loss: 0.00154747
Iteration 7/25 | Loss: 0.00154747
Iteration 8/25 | Loss: 0.00154747
Iteration 9/25 | Loss: 0.00154747
Iteration 10/25 | Loss: 0.00154747
Iteration 11/25 | Loss: 0.00154747
Iteration 12/25 | Loss: 0.00154747
Iteration 13/25 | Loss: 0.00154747
Iteration 14/25 | Loss: 0.00154747
Iteration 15/25 | Loss: 0.00154747
Iteration 16/25 | Loss: 0.00154747
Iteration 17/25 | Loss: 0.00154747
Iteration 18/25 | Loss: 0.00154747
Iteration 19/25 | Loss: 0.00154747
Iteration 20/25 | Loss: 0.00154747
Iteration 21/25 | Loss: 0.00154747
Iteration 22/25 | Loss: 0.00154747
Iteration 23/25 | Loss: 0.00154747
Iteration 24/25 | Loss: 0.00154747
Iteration 25/25 | Loss: 0.00154747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26072931
Iteration 2/25 | Loss: 0.00219021
Iteration 3/25 | Loss: 0.00219021
Iteration 4/25 | Loss: 0.00219021
Iteration 5/25 | Loss: 0.00219021
Iteration 6/25 | Loss: 0.00219021
Iteration 7/25 | Loss: 0.00219021
Iteration 8/25 | Loss: 0.00219021
Iteration 9/25 | Loss: 0.00219021
Iteration 10/25 | Loss: 0.00219021
Iteration 11/25 | Loss: 0.00219021
Iteration 12/25 | Loss: 0.00219021
Iteration 13/25 | Loss: 0.00219021
Iteration 14/25 | Loss: 0.00219021
Iteration 15/25 | Loss: 0.00219021
Iteration 16/25 | Loss: 0.00219021
Iteration 17/25 | Loss: 0.00219021
Iteration 18/25 | Loss: 0.00219021
Iteration 19/25 | Loss: 0.00219021
Iteration 20/25 | Loss: 0.00219021
Iteration 21/25 | Loss: 0.00219021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021902075968682766, 0.0021902075968682766, 0.0021902075968682766, 0.0021902075968682766, 0.0021902075968682766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021902075968682766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219021
Iteration 2/1000 | Loss: 0.00005002
Iteration 3/1000 | Loss: 0.00003411
Iteration 4/1000 | Loss: 0.00002823
Iteration 5/1000 | Loss: 0.00002537
Iteration 6/1000 | Loss: 0.00002417
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002327
Iteration 9/1000 | Loss: 0.00002290
Iteration 10/1000 | Loss: 0.00002260
Iteration 11/1000 | Loss: 0.00002250
Iteration 12/1000 | Loss: 0.00002243
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002234
Iteration 15/1000 | Loss: 0.00002233
Iteration 16/1000 | Loss: 0.00002231
Iteration 17/1000 | Loss: 0.00002231
Iteration 18/1000 | Loss: 0.00002230
Iteration 19/1000 | Loss: 0.00002230
Iteration 20/1000 | Loss: 0.00002229
Iteration 21/1000 | Loss: 0.00002229
Iteration 22/1000 | Loss: 0.00002228
Iteration 23/1000 | Loss: 0.00002227
Iteration 24/1000 | Loss: 0.00002225
Iteration 25/1000 | Loss: 0.00002225
Iteration 26/1000 | Loss: 0.00002225
Iteration 27/1000 | Loss: 0.00002225
Iteration 28/1000 | Loss: 0.00002225
Iteration 29/1000 | Loss: 0.00002225
Iteration 30/1000 | Loss: 0.00002225
Iteration 31/1000 | Loss: 0.00002225
Iteration 32/1000 | Loss: 0.00002225
Iteration 33/1000 | Loss: 0.00002225
Iteration 34/1000 | Loss: 0.00002225
Iteration 35/1000 | Loss: 0.00002225
Iteration 36/1000 | Loss: 0.00002224
Iteration 37/1000 | Loss: 0.00002224
Iteration 38/1000 | Loss: 0.00002224
Iteration 39/1000 | Loss: 0.00002223
Iteration 40/1000 | Loss: 0.00002223
Iteration 41/1000 | Loss: 0.00002223
Iteration 42/1000 | Loss: 0.00002223
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00002222
Iteration 45/1000 | Loss: 0.00002222
Iteration 46/1000 | Loss: 0.00002222
Iteration 47/1000 | Loss: 0.00002222
Iteration 48/1000 | Loss: 0.00002222
Iteration 49/1000 | Loss: 0.00002222
Iteration 50/1000 | Loss: 0.00002222
Iteration 51/1000 | Loss: 0.00002222
Iteration 52/1000 | Loss: 0.00002221
Iteration 53/1000 | Loss: 0.00002221
Iteration 54/1000 | Loss: 0.00002221
Iteration 55/1000 | Loss: 0.00002220
Iteration 56/1000 | Loss: 0.00002220
Iteration 57/1000 | Loss: 0.00002220
Iteration 58/1000 | Loss: 0.00002220
Iteration 59/1000 | Loss: 0.00002219
Iteration 60/1000 | Loss: 0.00002219
Iteration 61/1000 | Loss: 0.00002219
Iteration 62/1000 | Loss: 0.00002219
Iteration 63/1000 | Loss: 0.00002219
Iteration 64/1000 | Loss: 0.00002219
Iteration 65/1000 | Loss: 0.00002219
Iteration 66/1000 | Loss: 0.00002219
Iteration 67/1000 | Loss: 0.00002219
Iteration 68/1000 | Loss: 0.00002219
Iteration 69/1000 | Loss: 0.00002219
Iteration 70/1000 | Loss: 0.00002219
Iteration 71/1000 | Loss: 0.00002219
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002219
Iteration 74/1000 | Loss: 0.00002218
Iteration 75/1000 | Loss: 0.00002218
Iteration 76/1000 | Loss: 0.00002218
Iteration 77/1000 | Loss: 0.00002218
Iteration 78/1000 | Loss: 0.00002218
Iteration 79/1000 | Loss: 0.00002218
Iteration 80/1000 | Loss: 0.00002218
Iteration 81/1000 | Loss: 0.00002217
Iteration 82/1000 | Loss: 0.00002217
Iteration 83/1000 | Loss: 0.00002217
Iteration 84/1000 | Loss: 0.00002217
Iteration 85/1000 | Loss: 0.00002217
Iteration 86/1000 | Loss: 0.00002216
Iteration 87/1000 | Loss: 0.00002216
Iteration 88/1000 | Loss: 0.00002216
Iteration 89/1000 | Loss: 0.00002216
Iteration 90/1000 | Loss: 0.00002216
Iteration 91/1000 | Loss: 0.00002216
Iteration 92/1000 | Loss: 0.00002216
Iteration 93/1000 | Loss: 0.00002216
Iteration 94/1000 | Loss: 0.00002215
Iteration 95/1000 | Loss: 0.00002215
Iteration 96/1000 | Loss: 0.00002215
Iteration 97/1000 | Loss: 0.00002215
Iteration 98/1000 | Loss: 0.00002215
Iteration 99/1000 | Loss: 0.00002215
Iteration 100/1000 | Loss: 0.00002215
Iteration 101/1000 | Loss: 0.00002214
Iteration 102/1000 | Loss: 0.00002214
Iteration 103/1000 | Loss: 0.00002214
Iteration 104/1000 | Loss: 0.00002214
Iteration 105/1000 | Loss: 0.00002214
Iteration 106/1000 | Loss: 0.00002214
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002213
Iteration 109/1000 | Loss: 0.00002213
Iteration 110/1000 | Loss: 0.00002213
Iteration 111/1000 | Loss: 0.00002213
Iteration 112/1000 | Loss: 0.00002213
Iteration 113/1000 | Loss: 0.00002213
Iteration 114/1000 | Loss: 0.00002213
Iteration 115/1000 | Loss: 0.00002213
Iteration 116/1000 | Loss: 0.00002213
Iteration 117/1000 | Loss: 0.00002212
Iteration 118/1000 | Loss: 0.00002212
Iteration 119/1000 | Loss: 0.00002212
Iteration 120/1000 | Loss: 0.00002212
Iteration 121/1000 | Loss: 0.00002212
Iteration 122/1000 | Loss: 0.00002212
Iteration 123/1000 | Loss: 0.00002211
Iteration 124/1000 | Loss: 0.00002211
Iteration 125/1000 | Loss: 0.00002210
Iteration 126/1000 | Loss: 0.00002210
Iteration 127/1000 | Loss: 0.00002210
Iteration 128/1000 | Loss: 0.00002209
Iteration 129/1000 | Loss: 0.00002209
Iteration 130/1000 | Loss: 0.00002209
Iteration 131/1000 | Loss: 0.00002209
Iteration 132/1000 | Loss: 0.00002209
Iteration 133/1000 | Loss: 0.00002209
Iteration 134/1000 | Loss: 0.00002209
Iteration 135/1000 | Loss: 0.00002209
Iteration 136/1000 | Loss: 0.00002208
Iteration 137/1000 | Loss: 0.00002208
Iteration 138/1000 | Loss: 0.00002208
Iteration 139/1000 | Loss: 0.00002208
Iteration 140/1000 | Loss: 0.00002208
Iteration 141/1000 | Loss: 0.00002208
Iteration 142/1000 | Loss: 0.00002208
Iteration 143/1000 | Loss: 0.00002207
Iteration 144/1000 | Loss: 0.00002207
Iteration 145/1000 | Loss: 0.00002207
Iteration 146/1000 | Loss: 0.00002207
Iteration 147/1000 | Loss: 0.00002207
Iteration 148/1000 | Loss: 0.00002207
Iteration 149/1000 | Loss: 0.00002207
Iteration 150/1000 | Loss: 0.00002207
Iteration 151/1000 | Loss: 0.00002207
Iteration 152/1000 | Loss: 0.00002207
Iteration 153/1000 | Loss: 0.00002207
Iteration 154/1000 | Loss: 0.00002207
Iteration 155/1000 | Loss: 0.00002207
Iteration 156/1000 | Loss: 0.00002207
Iteration 157/1000 | Loss: 0.00002206
Iteration 158/1000 | Loss: 0.00002206
Iteration 159/1000 | Loss: 0.00002206
Iteration 160/1000 | Loss: 0.00002206
Iteration 161/1000 | Loss: 0.00002206
Iteration 162/1000 | Loss: 0.00002206
Iteration 163/1000 | Loss: 0.00002206
Iteration 164/1000 | Loss: 0.00002206
Iteration 165/1000 | Loss: 0.00002206
Iteration 166/1000 | Loss: 0.00002206
Iteration 167/1000 | Loss: 0.00002206
Iteration 168/1000 | Loss: 0.00002206
Iteration 169/1000 | Loss: 0.00002206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.2059590264689177e-05, 2.2059590264689177e-05, 2.2059590264689177e-05, 2.2059590264689177e-05, 2.2059590264689177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2059590264689177e-05

Optimization complete. Final v2v error: 4.078311920166016 mm

Highest mean error: 4.724747180938721 mm for frame 159

Lowest mean error: 3.8622658252716064 mm for frame 0

Saving results

Total time: 35.333730697631836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091362
Iteration 2/25 | Loss: 0.00203770
Iteration 3/25 | Loss: 0.00167630
Iteration 4/25 | Loss: 0.00156685
Iteration 5/25 | Loss: 0.00155628
Iteration 6/25 | Loss: 0.00156525
Iteration 7/25 | Loss: 0.00156361
Iteration 8/25 | Loss: 0.00155695
Iteration 9/25 | Loss: 0.00154580
Iteration 10/25 | Loss: 0.00152343
Iteration 11/25 | Loss: 0.00152540
Iteration 12/25 | Loss: 0.00153040
Iteration 13/25 | Loss: 0.00152375
Iteration 14/25 | Loss: 0.00151666
Iteration 15/25 | Loss: 0.00151395
Iteration 16/25 | Loss: 0.00151347
Iteration 17/25 | Loss: 0.00151318
Iteration 18/25 | Loss: 0.00151304
Iteration 19/25 | Loss: 0.00151295
Iteration 20/25 | Loss: 0.00151993
Iteration 21/25 | Loss: 0.00152391
Iteration 22/25 | Loss: 0.00151675
Iteration 23/25 | Loss: 0.00151485
Iteration 24/25 | Loss: 0.00151317
Iteration 25/25 | Loss: 0.00151199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60202992
Iteration 2/25 | Loss: 0.00211201
Iteration 3/25 | Loss: 0.00189975
Iteration 4/25 | Loss: 0.00189974
Iteration 5/25 | Loss: 0.00189974
Iteration 6/25 | Loss: 0.00189974
Iteration 7/25 | Loss: 0.00189973
Iteration 8/25 | Loss: 0.00189973
Iteration 9/25 | Loss: 0.00189973
Iteration 10/25 | Loss: 0.00189973
Iteration 11/25 | Loss: 0.00189973
Iteration 12/25 | Loss: 0.00189973
Iteration 13/25 | Loss: 0.00189973
Iteration 14/25 | Loss: 0.00189973
Iteration 15/25 | Loss: 0.00189973
Iteration 16/25 | Loss: 0.00189973
Iteration 17/25 | Loss: 0.00189973
Iteration 18/25 | Loss: 0.00189973
Iteration 19/25 | Loss: 0.00189973
Iteration 20/25 | Loss: 0.00189973
Iteration 21/25 | Loss: 0.00189973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018997336737811565, 0.0018997336737811565, 0.0018997336737811565, 0.0018997336737811565, 0.0018997336737811565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018997336737811565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189973
Iteration 2/1000 | Loss: 0.00165100
Iteration 3/1000 | Loss: 0.00017946
Iteration 4/1000 | Loss: 0.00008142
Iteration 5/1000 | Loss: 0.00006264
Iteration 6/1000 | Loss: 0.00026478
Iteration 7/1000 | Loss: 0.00004775
Iteration 8/1000 | Loss: 0.00004242
Iteration 9/1000 | Loss: 0.00003915
Iteration 10/1000 | Loss: 0.00003642
Iteration 11/1000 | Loss: 0.00003466
Iteration 12/1000 | Loss: 0.00003334
Iteration 13/1000 | Loss: 0.00003198
Iteration 14/1000 | Loss: 0.00010211
Iteration 15/1000 | Loss: 0.00002966
Iteration 16/1000 | Loss: 0.00002874
Iteration 17/1000 | Loss: 0.00002807
Iteration 18/1000 | Loss: 0.00009339
Iteration 19/1000 | Loss: 0.00002788
Iteration 20/1000 | Loss: 0.00002742
Iteration 21/1000 | Loss: 0.00002719
Iteration 22/1000 | Loss: 0.00002701
Iteration 23/1000 | Loss: 0.00002678
Iteration 24/1000 | Loss: 0.00002669
Iteration 25/1000 | Loss: 0.00002662
Iteration 26/1000 | Loss: 0.00002660
Iteration 27/1000 | Loss: 0.00002658
Iteration 28/1000 | Loss: 0.00002657
Iteration 29/1000 | Loss: 0.00002651
Iteration 30/1000 | Loss: 0.00002650
Iteration 31/1000 | Loss: 0.00002650
Iteration 32/1000 | Loss: 0.00002649
Iteration 33/1000 | Loss: 0.00002649
Iteration 34/1000 | Loss: 0.00002648
Iteration 35/1000 | Loss: 0.00002647
Iteration 36/1000 | Loss: 0.00002646
Iteration 37/1000 | Loss: 0.00002646
Iteration 38/1000 | Loss: 0.00002645
Iteration 39/1000 | Loss: 0.00002644
Iteration 40/1000 | Loss: 0.00002644
Iteration 41/1000 | Loss: 0.00002644
Iteration 42/1000 | Loss: 0.00002643
Iteration 43/1000 | Loss: 0.00002643
Iteration 44/1000 | Loss: 0.00002643
Iteration 45/1000 | Loss: 0.00002643
Iteration 46/1000 | Loss: 0.00002643
Iteration 47/1000 | Loss: 0.00002642
Iteration 48/1000 | Loss: 0.00002641
Iteration 49/1000 | Loss: 0.00002641
Iteration 50/1000 | Loss: 0.00002640
Iteration 51/1000 | Loss: 0.00002640
Iteration 52/1000 | Loss: 0.00002638
Iteration 53/1000 | Loss: 0.00002638
Iteration 54/1000 | Loss: 0.00002638
Iteration 55/1000 | Loss: 0.00002638
Iteration 56/1000 | Loss: 0.00002638
Iteration 57/1000 | Loss: 0.00002638
Iteration 58/1000 | Loss: 0.00002637
Iteration 59/1000 | Loss: 0.00002637
Iteration 60/1000 | Loss: 0.00002637
Iteration 61/1000 | Loss: 0.00002637
Iteration 62/1000 | Loss: 0.00002637
Iteration 63/1000 | Loss: 0.00002637
Iteration 64/1000 | Loss: 0.00002637
Iteration 65/1000 | Loss: 0.00002637
Iteration 66/1000 | Loss: 0.00002636
Iteration 67/1000 | Loss: 0.00002636
Iteration 68/1000 | Loss: 0.00002636
Iteration 69/1000 | Loss: 0.00002636
Iteration 70/1000 | Loss: 0.00002635
Iteration 71/1000 | Loss: 0.00002635
Iteration 72/1000 | Loss: 0.00002635
Iteration 73/1000 | Loss: 0.00002635
Iteration 74/1000 | Loss: 0.00002635
Iteration 75/1000 | Loss: 0.00002634
Iteration 76/1000 | Loss: 0.00002634
Iteration 77/1000 | Loss: 0.00002634
Iteration 78/1000 | Loss: 0.00002634
Iteration 79/1000 | Loss: 0.00002633
Iteration 80/1000 | Loss: 0.00002633
Iteration 81/1000 | Loss: 0.00002633
Iteration 82/1000 | Loss: 0.00002632
Iteration 83/1000 | Loss: 0.00002632
Iteration 84/1000 | Loss: 0.00002631
Iteration 85/1000 | Loss: 0.00002631
Iteration 86/1000 | Loss: 0.00002631
Iteration 87/1000 | Loss: 0.00002631
Iteration 88/1000 | Loss: 0.00002631
Iteration 89/1000 | Loss: 0.00002631
Iteration 90/1000 | Loss: 0.00002631
Iteration 91/1000 | Loss: 0.00002631
Iteration 92/1000 | Loss: 0.00002630
Iteration 93/1000 | Loss: 0.00002630
Iteration 94/1000 | Loss: 0.00002630
Iteration 95/1000 | Loss: 0.00002630
Iteration 96/1000 | Loss: 0.00002630
Iteration 97/1000 | Loss: 0.00002630
Iteration 98/1000 | Loss: 0.00002630
Iteration 99/1000 | Loss: 0.00002629
Iteration 100/1000 | Loss: 0.00002629
Iteration 101/1000 | Loss: 0.00002629
Iteration 102/1000 | Loss: 0.00002629
Iteration 103/1000 | Loss: 0.00002629
Iteration 104/1000 | Loss: 0.00002629
Iteration 105/1000 | Loss: 0.00002629
Iteration 106/1000 | Loss: 0.00002629
Iteration 107/1000 | Loss: 0.00002629
Iteration 108/1000 | Loss: 0.00002628
Iteration 109/1000 | Loss: 0.00002628
Iteration 110/1000 | Loss: 0.00002628
Iteration 111/1000 | Loss: 0.00002628
Iteration 112/1000 | Loss: 0.00002628
Iteration 113/1000 | Loss: 0.00002628
Iteration 114/1000 | Loss: 0.00002628
Iteration 115/1000 | Loss: 0.00002628
Iteration 116/1000 | Loss: 0.00002628
Iteration 117/1000 | Loss: 0.00002628
Iteration 118/1000 | Loss: 0.00002628
Iteration 119/1000 | Loss: 0.00002628
Iteration 120/1000 | Loss: 0.00002628
Iteration 121/1000 | Loss: 0.00002628
Iteration 122/1000 | Loss: 0.00002628
Iteration 123/1000 | Loss: 0.00002628
Iteration 124/1000 | Loss: 0.00002628
Iteration 125/1000 | Loss: 0.00002628
Iteration 126/1000 | Loss: 0.00002628
Iteration 127/1000 | Loss: 0.00002628
Iteration 128/1000 | Loss: 0.00002628
Iteration 129/1000 | Loss: 0.00002628
Iteration 130/1000 | Loss: 0.00002628
Iteration 131/1000 | Loss: 0.00002628
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00002628
Iteration 134/1000 | Loss: 0.00002628
Iteration 135/1000 | Loss: 0.00002628
Iteration 136/1000 | Loss: 0.00002628
Iteration 137/1000 | Loss: 0.00002628
Iteration 138/1000 | Loss: 0.00002628
Iteration 139/1000 | Loss: 0.00002628
Iteration 140/1000 | Loss: 0.00002628
Iteration 141/1000 | Loss: 0.00002628
Iteration 142/1000 | Loss: 0.00002628
Iteration 143/1000 | Loss: 0.00002628
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002628
Iteration 154/1000 | Loss: 0.00002628
Iteration 155/1000 | Loss: 0.00002628
Iteration 156/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.627955109346658e-05, 2.627955109346658e-05, 2.627955109346658e-05, 2.627955109346658e-05, 2.627955109346658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.627955109346658e-05

Optimization complete. Final v2v error: 4.230832099914551 mm

Highest mean error: 10.858621597290039 mm for frame 87

Lowest mean error: 3.705583095550537 mm for frame 131

Saving results

Total time: 87.30250120162964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751982
Iteration 2/25 | Loss: 0.00203451
Iteration 3/25 | Loss: 0.00157415
Iteration 4/25 | Loss: 0.00152119
Iteration 5/25 | Loss: 0.00151818
Iteration 6/25 | Loss: 0.00153341
Iteration 7/25 | Loss: 0.00151430
Iteration 8/25 | Loss: 0.00150027
Iteration 9/25 | Loss: 0.00149905
Iteration 10/25 | Loss: 0.00149881
Iteration 11/25 | Loss: 0.00149871
Iteration 12/25 | Loss: 0.00149863
Iteration 13/25 | Loss: 0.00149863
Iteration 14/25 | Loss: 0.00149863
Iteration 15/25 | Loss: 0.00149863
Iteration 16/25 | Loss: 0.00149862
Iteration 17/25 | Loss: 0.00149862
Iteration 18/25 | Loss: 0.00149862
Iteration 19/25 | Loss: 0.00149862
Iteration 20/25 | Loss: 0.00149862
Iteration 21/25 | Loss: 0.00149862
Iteration 22/25 | Loss: 0.00149862
Iteration 23/25 | Loss: 0.00149862
Iteration 24/25 | Loss: 0.00149862
Iteration 25/25 | Loss: 0.00149862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.63629723
Iteration 2/25 | Loss: 0.00157904
Iteration 3/25 | Loss: 0.00157904
Iteration 4/25 | Loss: 0.00157904
Iteration 5/25 | Loss: 0.00157904
Iteration 6/25 | Loss: 0.00157904
Iteration 7/25 | Loss: 0.00157904
Iteration 8/25 | Loss: 0.00157904
Iteration 9/25 | Loss: 0.00157904
Iteration 10/25 | Loss: 0.00157904
Iteration 11/25 | Loss: 0.00157904
Iteration 12/25 | Loss: 0.00157904
Iteration 13/25 | Loss: 0.00157904
Iteration 14/25 | Loss: 0.00157904
Iteration 15/25 | Loss: 0.00157904
Iteration 16/25 | Loss: 0.00157904
Iteration 17/25 | Loss: 0.00157904
Iteration 18/25 | Loss: 0.00157904
Iteration 19/25 | Loss: 0.00157904
Iteration 20/25 | Loss: 0.00157904
Iteration 21/25 | Loss: 0.00157904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015790389152243733, 0.0015790389152243733, 0.0015790389152243733, 0.0015790389152243733, 0.0015790389152243733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015790389152243733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157904
Iteration 2/1000 | Loss: 0.00006334
Iteration 3/1000 | Loss: 0.00003579
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002391
Iteration 6/1000 | Loss: 0.00002226
Iteration 7/1000 | Loss: 0.00002129
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002006
Iteration 11/1000 | Loss: 0.00001980
Iteration 12/1000 | Loss: 0.00001965
Iteration 13/1000 | Loss: 0.00001955
Iteration 14/1000 | Loss: 0.00001955
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001954
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001953
Iteration 22/1000 | Loss: 0.00001952
Iteration 23/1000 | Loss: 0.00001951
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001950
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001950
Iteration 29/1000 | Loss: 0.00001950
Iteration 30/1000 | Loss: 0.00001950
Iteration 31/1000 | Loss: 0.00001949
Iteration 32/1000 | Loss: 0.00001949
Iteration 33/1000 | Loss: 0.00001949
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001948
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001946
Iteration 46/1000 | Loss: 0.00001946
Iteration 47/1000 | Loss: 0.00001946
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001946
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.9455908841337077e-05, 1.9455908841337077e-05, 1.9455908841337077e-05, 1.9455908841337077e-05, 1.9455908841337077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9455908841337077e-05

Optimization complete. Final v2v error: 3.7577130794525146 mm

Highest mean error: 4.317273139953613 mm for frame 0

Lowest mean error: 3.5346486568450928 mm for frame 120

Saving results

Total time: 40.18680477142334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136217
Iteration 2/25 | Loss: 0.01136217
Iteration 3/25 | Loss: 0.01136216
Iteration 4/25 | Loss: 0.01136216
Iteration 5/25 | Loss: 0.01136216
Iteration 6/25 | Loss: 0.01136216
Iteration 7/25 | Loss: 0.01136216
Iteration 8/25 | Loss: 0.01136216
Iteration 9/25 | Loss: 0.01136216
Iteration 10/25 | Loss: 0.01136216
Iteration 11/25 | Loss: 0.01136216
Iteration 12/25 | Loss: 0.01136215
Iteration 13/25 | Loss: 0.01136215
Iteration 14/25 | Loss: 0.01136215
Iteration 15/25 | Loss: 0.01136215
Iteration 16/25 | Loss: 0.01136215
Iteration 17/25 | Loss: 0.01136215
Iteration 18/25 | Loss: 0.01136215
Iteration 19/25 | Loss: 0.01136215
Iteration 20/25 | Loss: 0.01136214
Iteration 21/25 | Loss: 0.01136214
Iteration 22/25 | Loss: 0.01136214
Iteration 23/25 | Loss: 0.01136214
Iteration 24/25 | Loss: 0.01136214
Iteration 25/25 | Loss: 0.01136214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31406784
Iteration 2/25 | Loss: 0.12930247
Iteration 3/25 | Loss: 0.12929307
Iteration 4/25 | Loss: 0.12919767
Iteration 5/25 | Loss: 0.12919764
Iteration 6/25 | Loss: 0.12919761
Iteration 7/25 | Loss: 0.12919761
Iteration 8/25 | Loss: 0.12919758
Iteration 9/25 | Loss: 0.12919758
Iteration 10/25 | Loss: 0.12919758
Iteration 11/25 | Loss: 0.12919758
Iteration 12/25 | Loss: 0.12919758
Iteration 13/25 | Loss: 0.12919758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.12919758260250092, 0.12919758260250092, 0.12919758260250092, 0.12919758260250092, 0.12919758260250092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12919758260250092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12919758
Iteration 2/1000 | Loss: 0.00485987
Iteration 3/1000 | Loss: 0.00170290
Iteration 4/1000 | Loss: 0.00024205
Iteration 5/1000 | Loss: 0.00009096
Iteration 6/1000 | Loss: 0.00005636
Iteration 7/1000 | Loss: 0.00015959
Iteration 8/1000 | Loss: 0.00007872
Iteration 9/1000 | Loss: 0.00003703
Iteration 10/1000 | Loss: 0.00007894
Iteration 11/1000 | Loss: 0.00007092
Iteration 12/1000 | Loss: 0.00002736
Iteration 13/1000 | Loss: 0.00002504
Iteration 14/1000 | Loss: 0.00002340
Iteration 15/1000 | Loss: 0.00002234
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002301
Iteration 18/1000 | Loss: 0.00002145
Iteration 19/1000 | Loss: 0.00002096
Iteration 20/1000 | Loss: 0.00002093
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00002039
Iteration 24/1000 | Loss: 0.00002035
Iteration 25/1000 | Loss: 0.00002030
Iteration 26/1000 | Loss: 0.00002027
Iteration 27/1000 | Loss: 0.00002026
Iteration 28/1000 | Loss: 0.00002024
Iteration 29/1000 | Loss: 0.00002019
Iteration 30/1000 | Loss: 0.00002019
Iteration 31/1000 | Loss: 0.00002011
Iteration 32/1000 | Loss: 0.00002006
Iteration 33/1000 | Loss: 0.00002004
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002001
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00002000
Iteration 42/1000 | Loss: 0.00002000
Iteration 43/1000 | Loss: 0.00002052
Iteration 44/1000 | Loss: 0.00002014
Iteration 45/1000 | Loss: 0.00002031
Iteration 46/1000 | Loss: 0.00002003
Iteration 47/1000 | Loss: 0.00002000
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001999
Iteration 50/1000 | Loss: 0.00001999
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001999
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001999
Iteration 56/1000 | Loss: 0.00002019
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00001999
Iteration 63/1000 | Loss: 0.00001999
Iteration 64/1000 | Loss: 0.00001999
Iteration 65/1000 | Loss: 0.00001999
Iteration 66/1000 | Loss: 0.00001999
Iteration 67/1000 | Loss: 0.00001999
Iteration 68/1000 | Loss: 0.00001999
Iteration 69/1000 | Loss: 0.00001999
Iteration 70/1000 | Loss: 0.00001999
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001999
Iteration 73/1000 | Loss: 0.00001999
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.998771222133655e-05, 1.998771222133655e-05, 1.998771222133655e-05, 1.998771222133655e-05, 1.998771222133655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.998771222133655e-05

Optimization complete. Final v2v error: 3.8448007106781006 mm

Highest mean error: 11.335945129394531 mm for frame 219

Lowest mean error: 3.5896246433258057 mm for frame 5

Saving results

Total time: 50.85988211631775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747290
Iteration 2/25 | Loss: 0.00164014
Iteration 3/25 | Loss: 0.00155670
Iteration 4/25 | Loss: 0.00151656
Iteration 5/25 | Loss: 0.00150871
Iteration 6/25 | Loss: 0.00150654
Iteration 7/25 | Loss: 0.00150601
Iteration 8/25 | Loss: 0.00150586
Iteration 9/25 | Loss: 0.00150583
Iteration 10/25 | Loss: 0.00150583
Iteration 11/25 | Loss: 0.00150583
Iteration 12/25 | Loss: 0.00150583
Iteration 13/25 | Loss: 0.00150583
Iteration 14/25 | Loss: 0.00150583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0015058309072628617, 0.0015058309072628617, 0.0015058309072628617, 0.0015058309072628617, 0.0015058309072628617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015058309072628617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38866830
Iteration 2/25 | Loss: 0.00206671
Iteration 3/25 | Loss: 0.00206670
Iteration 4/25 | Loss: 0.00206670
Iteration 5/25 | Loss: 0.00206670
Iteration 6/25 | Loss: 0.00206670
Iteration 7/25 | Loss: 0.00206670
Iteration 8/25 | Loss: 0.00206670
Iteration 9/25 | Loss: 0.00206670
Iteration 10/25 | Loss: 0.00206670
Iteration 11/25 | Loss: 0.00206670
Iteration 12/25 | Loss: 0.00206670
Iteration 13/25 | Loss: 0.00206670
Iteration 14/25 | Loss: 0.00206670
Iteration 15/25 | Loss: 0.00206670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020667025819420815, 0.0020667025819420815, 0.0020667025819420815, 0.0020667025819420815, 0.0020667025819420815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020667025819420815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206670
Iteration 2/1000 | Loss: 0.00008328
Iteration 3/1000 | Loss: 0.00006592
Iteration 4/1000 | Loss: 0.00002976
Iteration 5/1000 | Loss: 0.00002693
Iteration 6/1000 | Loss: 0.00005510
Iteration 7/1000 | Loss: 0.00002566
Iteration 8/1000 | Loss: 0.00002504
Iteration 9/1000 | Loss: 0.00002437
Iteration 10/1000 | Loss: 0.00002402
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002383
Iteration 13/1000 | Loss: 0.00002382
Iteration 14/1000 | Loss: 0.00002382
Iteration 15/1000 | Loss: 0.00002381
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00002380
Iteration 18/1000 | Loss: 0.00002379
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002373
Iteration 22/1000 | Loss: 0.00002372
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002372
Iteration 25/1000 | Loss: 0.00002371
Iteration 26/1000 | Loss: 0.00002371
Iteration 27/1000 | Loss: 0.00002367
Iteration 28/1000 | Loss: 0.00002366
Iteration 29/1000 | Loss: 0.00002366
Iteration 30/1000 | Loss: 0.00002365
Iteration 31/1000 | Loss: 0.00002364
Iteration 32/1000 | Loss: 0.00002364
Iteration 33/1000 | Loss: 0.00002358
Iteration 34/1000 | Loss: 0.00002356
Iteration 35/1000 | Loss: 0.00002356
Iteration 36/1000 | Loss: 0.00002355
Iteration 37/1000 | Loss: 0.00002355
Iteration 38/1000 | Loss: 0.00002354
Iteration 39/1000 | Loss: 0.00002354
Iteration 40/1000 | Loss: 0.00002354
Iteration 41/1000 | Loss: 0.00002354
Iteration 42/1000 | Loss: 0.00002353
Iteration 43/1000 | Loss: 0.00002353
Iteration 44/1000 | Loss: 0.00002353
Iteration 45/1000 | Loss: 0.00002353
Iteration 46/1000 | Loss: 0.00002353
Iteration 47/1000 | Loss: 0.00002353
Iteration 48/1000 | Loss: 0.00002352
Iteration 49/1000 | Loss: 0.00002352
Iteration 50/1000 | Loss: 0.00002352
Iteration 51/1000 | Loss: 0.00002352
Iteration 52/1000 | Loss: 0.00002352
Iteration 53/1000 | Loss: 0.00002352
Iteration 54/1000 | Loss: 0.00002352
Iteration 55/1000 | Loss: 0.00002352
Iteration 56/1000 | Loss: 0.00002351
Iteration 57/1000 | Loss: 0.00002351
Iteration 58/1000 | Loss: 0.00002350
Iteration 59/1000 | Loss: 0.00002350
Iteration 60/1000 | Loss: 0.00002350
Iteration 61/1000 | Loss: 0.00002350
Iteration 62/1000 | Loss: 0.00002349
Iteration 63/1000 | Loss: 0.00002349
Iteration 64/1000 | Loss: 0.00002349
Iteration 65/1000 | Loss: 0.00002349
Iteration 66/1000 | Loss: 0.00002348
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002347
Iteration 69/1000 | Loss: 0.00002347
Iteration 70/1000 | Loss: 0.00002347
Iteration 71/1000 | Loss: 0.00002346
Iteration 72/1000 | Loss: 0.00002346
Iteration 73/1000 | Loss: 0.00002346
Iteration 74/1000 | Loss: 0.00002346
Iteration 75/1000 | Loss: 0.00002346
Iteration 76/1000 | Loss: 0.00002346
Iteration 77/1000 | Loss: 0.00002345
Iteration 78/1000 | Loss: 0.00002345
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002345
Iteration 81/1000 | Loss: 0.00002344
Iteration 82/1000 | Loss: 0.00002344
Iteration 83/1000 | Loss: 0.00002344
Iteration 84/1000 | Loss: 0.00002344
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002343
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002342
Iteration 101/1000 | Loss: 0.00002342
Iteration 102/1000 | Loss: 0.00002342
Iteration 103/1000 | Loss: 0.00002342
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002341
Iteration 112/1000 | Loss: 0.00002341
Iteration 113/1000 | Loss: 0.00002341
Iteration 114/1000 | Loss: 0.00002341
Iteration 115/1000 | Loss: 0.00002341
Iteration 116/1000 | Loss: 0.00002341
Iteration 117/1000 | Loss: 0.00002341
Iteration 118/1000 | Loss: 0.00002341
Iteration 119/1000 | Loss: 0.00002341
Iteration 120/1000 | Loss: 0.00002341
Iteration 121/1000 | Loss: 0.00002341
Iteration 122/1000 | Loss: 0.00002340
Iteration 123/1000 | Loss: 0.00002340
Iteration 124/1000 | Loss: 0.00002340
Iteration 125/1000 | Loss: 0.00002340
Iteration 126/1000 | Loss: 0.00002340
Iteration 127/1000 | Loss: 0.00002340
Iteration 128/1000 | Loss: 0.00002340
Iteration 129/1000 | Loss: 0.00002340
Iteration 130/1000 | Loss: 0.00002340
Iteration 131/1000 | Loss: 0.00002340
Iteration 132/1000 | Loss: 0.00002340
Iteration 133/1000 | Loss: 0.00002340
Iteration 134/1000 | Loss: 0.00002340
Iteration 135/1000 | Loss: 0.00002340
Iteration 136/1000 | Loss: 0.00002340
Iteration 137/1000 | Loss: 0.00002340
Iteration 138/1000 | Loss: 0.00002340
Iteration 139/1000 | Loss: 0.00002340
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002339
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002339
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002339
Iteration 152/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.3394250092678703e-05, 2.3394250092678703e-05, 2.3394250092678703e-05, 2.3394250092678703e-05, 2.3394250092678703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3394250092678703e-05

Optimization complete. Final v2v error: 4.129084587097168 mm

Highest mean error: 4.631959438323975 mm for frame 72

Lowest mean error: 3.650606155395508 mm for frame 0

Saving results

Total time: 45.55521297454834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883673
Iteration 2/25 | Loss: 0.00165136
Iteration 3/25 | Loss: 0.00155213
Iteration 4/25 | Loss: 0.00154190
Iteration 5/25 | Loss: 0.00153973
Iteration 6/25 | Loss: 0.00153973
Iteration 7/25 | Loss: 0.00153973
Iteration 8/25 | Loss: 0.00153973
Iteration 9/25 | Loss: 0.00153973
Iteration 10/25 | Loss: 0.00153973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015397326787933707, 0.0015397326787933707, 0.0015397326787933707, 0.0015397326787933707, 0.0015397326787933707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015397326787933707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36004961
Iteration 2/25 | Loss: 0.00217253
Iteration 3/25 | Loss: 0.00217253
Iteration 4/25 | Loss: 0.00217253
Iteration 5/25 | Loss: 0.00217253
Iteration 6/25 | Loss: 0.00217253
Iteration 7/25 | Loss: 0.00217253
Iteration 8/25 | Loss: 0.00217253
Iteration 9/25 | Loss: 0.00217253
Iteration 10/25 | Loss: 0.00217253
Iteration 11/25 | Loss: 0.00217253
Iteration 12/25 | Loss: 0.00217253
Iteration 13/25 | Loss: 0.00217253
Iteration 14/25 | Loss: 0.00217253
Iteration 15/25 | Loss: 0.00217253
Iteration 16/25 | Loss: 0.00217253
Iteration 17/25 | Loss: 0.00217253
Iteration 18/25 | Loss: 0.00217253
Iteration 19/25 | Loss: 0.00217253
Iteration 20/25 | Loss: 0.00217253
Iteration 21/25 | Loss: 0.00217253
Iteration 22/25 | Loss: 0.00217253
Iteration 23/25 | Loss: 0.00217253
Iteration 24/25 | Loss: 0.00217253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021725252736359835, 0.0021725252736359835, 0.0021725252736359835, 0.0021725252736359835, 0.0021725252736359835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021725252736359835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217253
Iteration 2/1000 | Loss: 0.00006253
Iteration 3/1000 | Loss: 0.00003995
Iteration 4/1000 | Loss: 0.00003305
Iteration 5/1000 | Loss: 0.00002941
Iteration 6/1000 | Loss: 0.00002695
Iteration 7/1000 | Loss: 0.00002583
Iteration 8/1000 | Loss: 0.00002502
Iteration 9/1000 | Loss: 0.00002433
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002345
Iteration 14/1000 | Loss: 0.00002334
Iteration 15/1000 | Loss: 0.00002329
Iteration 16/1000 | Loss: 0.00002329
Iteration 17/1000 | Loss: 0.00002329
Iteration 18/1000 | Loss: 0.00002329
Iteration 19/1000 | Loss: 0.00002329
Iteration 20/1000 | Loss: 0.00002328
Iteration 21/1000 | Loss: 0.00002328
Iteration 22/1000 | Loss: 0.00002327
Iteration 23/1000 | Loss: 0.00002326
Iteration 24/1000 | Loss: 0.00002326
Iteration 25/1000 | Loss: 0.00002326
Iteration 26/1000 | Loss: 0.00002326
Iteration 27/1000 | Loss: 0.00002325
Iteration 28/1000 | Loss: 0.00002325
Iteration 29/1000 | Loss: 0.00002324
Iteration 30/1000 | Loss: 0.00002324
Iteration 31/1000 | Loss: 0.00002324
Iteration 32/1000 | Loss: 0.00002324
Iteration 33/1000 | Loss: 0.00002324
Iteration 34/1000 | Loss: 0.00002324
Iteration 35/1000 | Loss: 0.00002323
Iteration 36/1000 | Loss: 0.00002323
Iteration 37/1000 | Loss: 0.00002323
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002319
Iteration 40/1000 | Loss: 0.00002319
Iteration 41/1000 | Loss: 0.00002319
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00002318
Iteration 44/1000 | Loss: 0.00002318
Iteration 45/1000 | Loss: 0.00002316
Iteration 46/1000 | Loss: 0.00002316
Iteration 47/1000 | Loss: 0.00002315
Iteration 48/1000 | Loss: 0.00002315
Iteration 49/1000 | Loss: 0.00002315
Iteration 50/1000 | Loss: 0.00002315
Iteration 51/1000 | Loss: 0.00002315
Iteration 52/1000 | Loss: 0.00002314
Iteration 53/1000 | Loss: 0.00002314
Iteration 54/1000 | Loss: 0.00002314
Iteration 55/1000 | Loss: 0.00002314
Iteration 56/1000 | Loss: 0.00002314
Iteration 57/1000 | Loss: 0.00002313
Iteration 58/1000 | Loss: 0.00002313
Iteration 59/1000 | Loss: 0.00002313
Iteration 60/1000 | Loss: 0.00002312
Iteration 61/1000 | Loss: 0.00002312
Iteration 62/1000 | Loss: 0.00002311
Iteration 63/1000 | Loss: 0.00002311
Iteration 64/1000 | Loss: 0.00002311
Iteration 65/1000 | Loss: 0.00002311
Iteration 66/1000 | Loss: 0.00002311
Iteration 67/1000 | Loss: 0.00002310
Iteration 68/1000 | Loss: 0.00002310
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002310
Iteration 71/1000 | Loss: 0.00002309
Iteration 72/1000 | Loss: 0.00002309
Iteration 73/1000 | Loss: 0.00002309
Iteration 74/1000 | Loss: 0.00002308
Iteration 75/1000 | Loss: 0.00002308
Iteration 76/1000 | Loss: 0.00002308
Iteration 77/1000 | Loss: 0.00002308
Iteration 78/1000 | Loss: 0.00002308
Iteration 79/1000 | Loss: 0.00002308
Iteration 80/1000 | Loss: 0.00002308
Iteration 81/1000 | Loss: 0.00002308
Iteration 82/1000 | Loss: 0.00002308
Iteration 83/1000 | Loss: 0.00002308
Iteration 84/1000 | Loss: 0.00002308
Iteration 85/1000 | Loss: 0.00002307
Iteration 86/1000 | Loss: 0.00002307
Iteration 87/1000 | Loss: 0.00002306
Iteration 88/1000 | Loss: 0.00002306
Iteration 89/1000 | Loss: 0.00002306
Iteration 90/1000 | Loss: 0.00002306
Iteration 91/1000 | Loss: 0.00002306
Iteration 92/1000 | Loss: 0.00002306
Iteration 93/1000 | Loss: 0.00002306
Iteration 94/1000 | Loss: 0.00002306
Iteration 95/1000 | Loss: 0.00002305
Iteration 96/1000 | Loss: 0.00002305
Iteration 97/1000 | Loss: 0.00002305
Iteration 98/1000 | Loss: 0.00002305
Iteration 99/1000 | Loss: 0.00002305
Iteration 100/1000 | Loss: 0.00002305
Iteration 101/1000 | Loss: 0.00002305
Iteration 102/1000 | Loss: 0.00002304
Iteration 103/1000 | Loss: 0.00002304
Iteration 104/1000 | Loss: 0.00002304
Iteration 105/1000 | Loss: 0.00002304
Iteration 106/1000 | Loss: 0.00002304
Iteration 107/1000 | Loss: 0.00002304
Iteration 108/1000 | Loss: 0.00002304
Iteration 109/1000 | Loss: 0.00002304
Iteration 110/1000 | Loss: 0.00002303
Iteration 111/1000 | Loss: 0.00002303
Iteration 112/1000 | Loss: 0.00002303
Iteration 113/1000 | Loss: 0.00002303
Iteration 114/1000 | Loss: 0.00002303
Iteration 115/1000 | Loss: 0.00002303
Iteration 116/1000 | Loss: 0.00002303
Iteration 117/1000 | Loss: 0.00002303
Iteration 118/1000 | Loss: 0.00002303
Iteration 119/1000 | Loss: 0.00002303
Iteration 120/1000 | Loss: 0.00002303
Iteration 121/1000 | Loss: 0.00002303
Iteration 122/1000 | Loss: 0.00002303
Iteration 123/1000 | Loss: 0.00002303
Iteration 124/1000 | Loss: 0.00002303
Iteration 125/1000 | Loss: 0.00002303
Iteration 126/1000 | Loss: 0.00002303
Iteration 127/1000 | Loss: 0.00002303
Iteration 128/1000 | Loss: 0.00002303
Iteration 129/1000 | Loss: 0.00002303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.3034228433971293e-05, 2.3034228433971293e-05, 2.3034228433971293e-05, 2.3034228433971293e-05, 2.3034228433971293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3034228433971293e-05

Optimization complete. Final v2v error: 4.0853800773620605 mm

Highest mean error: 4.9042439460754395 mm for frame 239

Lowest mean error: 3.6475143432617188 mm for frame 199

Saving results

Total time: 38.760069131851196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773523
Iteration 2/25 | Loss: 0.00223994
Iteration 3/25 | Loss: 0.00178342
Iteration 4/25 | Loss: 0.00172034
Iteration 5/25 | Loss: 0.00170805
Iteration 6/25 | Loss: 0.00169426
Iteration 7/25 | Loss: 0.00168122
Iteration 8/25 | Loss: 0.00168350
Iteration 9/25 | Loss: 0.00167630
Iteration 10/25 | Loss: 0.00166634
Iteration 11/25 | Loss: 0.00167295
Iteration 12/25 | Loss: 0.00167364
Iteration 13/25 | Loss: 0.00167060
Iteration 14/25 | Loss: 0.00166682
Iteration 15/25 | Loss: 0.00165672
Iteration 16/25 | Loss: 0.00163194
Iteration 17/25 | Loss: 0.00162450
Iteration 18/25 | Loss: 0.00162159
Iteration 19/25 | Loss: 0.00161740
Iteration 20/25 | Loss: 0.00161316
Iteration 21/25 | Loss: 0.00160616
Iteration 22/25 | Loss: 0.00160249
Iteration 23/25 | Loss: 0.00160213
Iteration 24/25 | Loss: 0.00160202
Iteration 25/25 | Loss: 0.00160200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.02929497
Iteration 2/25 | Loss: 0.00212057
Iteration 3/25 | Loss: 0.00211934
Iteration 4/25 | Loss: 0.00211934
Iteration 5/25 | Loss: 0.00211934
Iteration 6/25 | Loss: 0.00211934
Iteration 7/25 | Loss: 0.00211934
Iteration 8/25 | Loss: 0.00211934
Iteration 9/25 | Loss: 0.00211934
Iteration 10/25 | Loss: 0.00211934
Iteration 11/25 | Loss: 0.00211934
Iteration 12/25 | Loss: 0.00211934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0021193395368754864, 0.0021193395368754864, 0.0021193395368754864, 0.0021193395368754864, 0.0021193395368754864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021193395368754864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211934
Iteration 2/1000 | Loss: 0.00033953
Iteration 3/1000 | Loss: 0.00007835
Iteration 4/1000 | Loss: 0.00005664
Iteration 5/1000 | Loss: 0.00004881
Iteration 6/1000 | Loss: 0.00004482
Iteration 7/1000 | Loss: 0.00004148
Iteration 8/1000 | Loss: 0.00003909
Iteration 9/1000 | Loss: 0.00003786
Iteration 10/1000 | Loss: 0.00003685
Iteration 11/1000 | Loss: 0.00003608
Iteration 12/1000 | Loss: 0.00003540
Iteration 13/1000 | Loss: 0.00003487
Iteration 14/1000 | Loss: 0.00003430
Iteration 15/1000 | Loss: 0.00003390
Iteration 16/1000 | Loss: 0.00003361
Iteration 17/1000 | Loss: 0.00003345
Iteration 18/1000 | Loss: 0.00003331
Iteration 19/1000 | Loss: 0.00003329
Iteration 20/1000 | Loss: 0.00003327
Iteration 21/1000 | Loss: 0.00003312
Iteration 22/1000 | Loss: 0.00003312
Iteration 23/1000 | Loss: 0.00003304
Iteration 24/1000 | Loss: 0.00003298
Iteration 25/1000 | Loss: 0.00003297
Iteration 26/1000 | Loss: 0.00003282
Iteration 27/1000 | Loss: 0.00003280
Iteration 28/1000 | Loss: 0.00003278
Iteration 29/1000 | Loss: 0.00003275
Iteration 30/1000 | Loss: 0.00003269
Iteration 31/1000 | Loss: 0.00003263
Iteration 32/1000 | Loss: 0.00003259
Iteration 33/1000 | Loss: 0.00003259
Iteration 34/1000 | Loss: 0.00003258
Iteration 35/1000 | Loss: 0.00003256
Iteration 36/1000 | Loss: 0.00003256
Iteration 37/1000 | Loss: 0.00003256
Iteration 38/1000 | Loss: 0.00003256
Iteration 39/1000 | Loss: 0.00003255
Iteration 40/1000 | Loss: 0.00003255
Iteration 41/1000 | Loss: 0.00003254
Iteration 42/1000 | Loss: 0.00003254
Iteration 43/1000 | Loss: 0.00003253
Iteration 44/1000 | Loss: 0.00003253
Iteration 45/1000 | Loss: 0.00003253
Iteration 46/1000 | Loss: 0.00003252
Iteration 47/1000 | Loss: 0.00003252
Iteration 48/1000 | Loss: 0.00003252
Iteration 49/1000 | Loss: 0.00003252
Iteration 50/1000 | Loss: 0.00003251
Iteration 51/1000 | Loss: 0.00003251
Iteration 52/1000 | Loss: 0.00003251
Iteration 53/1000 | Loss: 0.00003250
Iteration 54/1000 | Loss: 0.00003250
Iteration 55/1000 | Loss: 0.00003250
Iteration 56/1000 | Loss: 0.00003250
Iteration 57/1000 | Loss: 0.00003250
Iteration 58/1000 | Loss: 0.00003249
Iteration 59/1000 | Loss: 0.00003249
Iteration 60/1000 | Loss: 0.00003249
Iteration 61/1000 | Loss: 0.00003249
Iteration 62/1000 | Loss: 0.00003249
Iteration 63/1000 | Loss: 0.00003248
Iteration 64/1000 | Loss: 0.00003248
Iteration 65/1000 | Loss: 0.00003248
Iteration 66/1000 | Loss: 0.00003248
Iteration 67/1000 | Loss: 0.00003247
Iteration 68/1000 | Loss: 0.00003247
Iteration 69/1000 | Loss: 0.00003247
Iteration 70/1000 | Loss: 0.00003247
Iteration 71/1000 | Loss: 0.00003247
Iteration 72/1000 | Loss: 0.00003247
Iteration 73/1000 | Loss: 0.00003247
Iteration 74/1000 | Loss: 0.00003246
Iteration 75/1000 | Loss: 0.00003246
Iteration 76/1000 | Loss: 0.00003246
Iteration 77/1000 | Loss: 0.00003246
Iteration 78/1000 | Loss: 0.00003246
Iteration 79/1000 | Loss: 0.00003246
Iteration 80/1000 | Loss: 0.00003245
Iteration 81/1000 | Loss: 0.00003245
Iteration 82/1000 | Loss: 0.00003245
Iteration 83/1000 | Loss: 0.00003245
Iteration 84/1000 | Loss: 0.00003244
Iteration 85/1000 | Loss: 0.00003244
Iteration 86/1000 | Loss: 0.00003244
Iteration 87/1000 | Loss: 0.00003244
Iteration 88/1000 | Loss: 0.00003244
Iteration 89/1000 | Loss: 0.00003243
Iteration 90/1000 | Loss: 0.00003243
Iteration 91/1000 | Loss: 0.00003243
Iteration 92/1000 | Loss: 0.00003243
Iteration 93/1000 | Loss: 0.00003243
Iteration 94/1000 | Loss: 0.00003243
Iteration 95/1000 | Loss: 0.00003242
Iteration 96/1000 | Loss: 0.00003242
Iteration 97/1000 | Loss: 0.00003242
Iteration 98/1000 | Loss: 0.00003242
Iteration 99/1000 | Loss: 0.00003241
Iteration 100/1000 | Loss: 0.00003241
Iteration 101/1000 | Loss: 0.00003241
Iteration 102/1000 | Loss: 0.00003241
Iteration 103/1000 | Loss: 0.00003241
Iteration 104/1000 | Loss: 0.00003240
Iteration 105/1000 | Loss: 0.00003240
Iteration 106/1000 | Loss: 0.00003240
Iteration 107/1000 | Loss: 0.00003240
Iteration 108/1000 | Loss: 0.00003240
Iteration 109/1000 | Loss: 0.00003240
Iteration 110/1000 | Loss: 0.00003240
Iteration 111/1000 | Loss: 0.00003240
Iteration 112/1000 | Loss: 0.00003239
Iteration 113/1000 | Loss: 0.00003239
Iteration 114/1000 | Loss: 0.00003239
Iteration 115/1000 | Loss: 0.00003239
Iteration 116/1000 | Loss: 0.00003239
Iteration 117/1000 | Loss: 0.00003239
Iteration 118/1000 | Loss: 0.00003239
Iteration 119/1000 | Loss: 0.00003239
Iteration 120/1000 | Loss: 0.00003239
Iteration 121/1000 | Loss: 0.00003239
Iteration 122/1000 | Loss: 0.00003239
Iteration 123/1000 | Loss: 0.00003239
Iteration 124/1000 | Loss: 0.00003239
Iteration 125/1000 | Loss: 0.00003239
Iteration 126/1000 | Loss: 0.00003239
Iteration 127/1000 | Loss: 0.00003238
Iteration 128/1000 | Loss: 0.00003238
Iteration 129/1000 | Loss: 0.00003238
Iteration 130/1000 | Loss: 0.00003238
Iteration 131/1000 | Loss: 0.00003238
Iteration 132/1000 | Loss: 0.00003237
Iteration 133/1000 | Loss: 0.00003237
Iteration 134/1000 | Loss: 0.00003237
Iteration 135/1000 | Loss: 0.00003236
Iteration 136/1000 | Loss: 0.00003236
Iteration 137/1000 | Loss: 0.00003236
Iteration 138/1000 | Loss: 0.00003236
Iteration 139/1000 | Loss: 0.00003236
Iteration 140/1000 | Loss: 0.00003236
Iteration 141/1000 | Loss: 0.00003235
Iteration 142/1000 | Loss: 0.00003235
Iteration 143/1000 | Loss: 0.00003235
Iteration 144/1000 | Loss: 0.00003235
Iteration 145/1000 | Loss: 0.00003235
Iteration 146/1000 | Loss: 0.00003235
Iteration 147/1000 | Loss: 0.00003235
Iteration 148/1000 | Loss: 0.00003234
Iteration 149/1000 | Loss: 0.00003234
Iteration 150/1000 | Loss: 0.00003234
Iteration 151/1000 | Loss: 0.00003234
Iteration 152/1000 | Loss: 0.00003234
Iteration 153/1000 | Loss: 0.00003234
Iteration 154/1000 | Loss: 0.00003234
Iteration 155/1000 | Loss: 0.00003234
Iteration 156/1000 | Loss: 0.00003234
Iteration 157/1000 | Loss: 0.00003234
Iteration 158/1000 | Loss: 0.00003234
Iteration 159/1000 | Loss: 0.00003234
Iteration 160/1000 | Loss: 0.00003234
Iteration 161/1000 | Loss: 0.00003233
Iteration 162/1000 | Loss: 0.00003233
Iteration 163/1000 | Loss: 0.00003233
Iteration 164/1000 | Loss: 0.00003233
Iteration 165/1000 | Loss: 0.00003233
Iteration 166/1000 | Loss: 0.00003233
Iteration 167/1000 | Loss: 0.00003233
Iteration 168/1000 | Loss: 0.00003233
Iteration 169/1000 | Loss: 0.00003233
Iteration 170/1000 | Loss: 0.00003232
Iteration 171/1000 | Loss: 0.00003232
Iteration 172/1000 | Loss: 0.00003232
Iteration 173/1000 | Loss: 0.00003232
Iteration 174/1000 | Loss: 0.00003232
Iteration 175/1000 | Loss: 0.00003231
Iteration 176/1000 | Loss: 0.00003231
Iteration 177/1000 | Loss: 0.00003231
Iteration 178/1000 | Loss: 0.00003231
Iteration 179/1000 | Loss: 0.00003231
Iteration 180/1000 | Loss: 0.00003231
Iteration 181/1000 | Loss: 0.00003231
Iteration 182/1000 | Loss: 0.00003231
Iteration 183/1000 | Loss: 0.00003230
Iteration 184/1000 | Loss: 0.00003230
Iteration 185/1000 | Loss: 0.00003230
Iteration 186/1000 | Loss: 0.00003230
Iteration 187/1000 | Loss: 0.00003230
Iteration 188/1000 | Loss: 0.00003230
Iteration 189/1000 | Loss: 0.00003229
Iteration 190/1000 | Loss: 0.00003229
Iteration 191/1000 | Loss: 0.00003229
Iteration 192/1000 | Loss: 0.00003229
Iteration 193/1000 | Loss: 0.00003229
Iteration 194/1000 | Loss: 0.00003229
Iteration 195/1000 | Loss: 0.00003229
Iteration 196/1000 | Loss: 0.00003229
Iteration 197/1000 | Loss: 0.00003229
Iteration 198/1000 | Loss: 0.00003229
Iteration 199/1000 | Loss: 0.00003229
Iteration 200/1000 | Loss: 0.00003229
Iteration 201/1000 | Loss: 0.00003228
Iteration 202/1000 | Loss: 0.00003228
Iteration 203/1000 | Loss: 0.00003228
Iteration 204/1000 | Loss: 0.00003228
Iteration 205/1000 | Loss: 0.00003228
Iteration 206/1000 | Loss: 0.00003228
Iteration 207/1000 | Loss: 0.00003228
Iteration 208/1000 | Loss: 0.00003228
Iteration 209/1000 | Loss: 0.00003228
Iteration 210/1000 | Loss: 0.00003228
Iteration 211/1000 | Loss: 0.00003228
Iteration 212/1000 | Loss: 0.00003228
Iteration 213/1000 | Loss: 0.00003228
Iteration 214/1000 | Loss: 0.00003227
Iteration 215/1000 | Loss: 0.00003227
Iteration 216/1000 | Loss: 0.00003227
Iteration 217/1000 | Loss: 0.00003227
Iteration 218/1000 | Loss: 0.00003227
Iteration 219/1000 | Loss: 0.00003226
Iteration 220/1000 | Loss: 0.00003226
Iteration 221/1000 | Loss: 0.00003226
Iteration 222/1000 | Loss: 0.00003226
Iteration 223/1000 | Loss: 0.00003226
Iteration 224/1000 | Loss: 0.00003226
Iteration 225/1000 | Loss: 0.00003226
Iteration 226/1000 | Loss: 0.00003226
Iteration 227/1000 | Loss: 0.00003226
Iteration 228/1000 | Loss: 0.00003226
Iteration 229/1000 | Loss: 0.00003226
Iteration 230/1000 | Loss: 0.00003226
Iteration 231/1000 | Loss: 0.00003226
Iteration 232/1000 | Loss: 0.00003226
Iteration 233/1000 | Loss: 0.00003226
Iteration 234/1000 | Loss: 0.00003225
Iteration 235/1000 | Loss: 0.00003225
Iteration 236/1000 | Loss: 0.00003225
Iteration 237/1000 | Loss: 0.00003225
Iteration 238/1000 | Loss: 0.00003225
Iteration 239/1000 | Loss: 0.00003225
Iteration 240/1000 | Loss: 0.00003225
Iteration 241/1000 | Loss: 0.00003225
Iteration 242/1000 | Loss: 0.00003225
Iteration 243/1000 | Loss: 0.00003225
Iteration 244/1000 | Loss: 0.00003225
Iteration 245/1000 | Loss: 0.00003225
Iteration 246/1000 | Loss: 0.00003224
Iteration 247/1000 | Loss: 0.00003224
Iteration 248/1000 | Loss: 0.00003224
Iteration 249/1000 | Loss: 0.00003224
Iteration 250/1000 | Loss: 0.00003224
Iteration 251/1000 | Loss: 0.00003224
Iteration 252/1000 | Loss: 0.00003224
Iteration 253/1000 | Loss: 0.00003224
Iteration 254/1000 | Loss: 0.00003224
Iteration 255/1000 | Loss: 0.00003224
Iteration 256/1000 | Loss: 0.00003223
Iteration 257/1000 | Loss: 0.00003223
Iteration 258/1000 | Loss: 0.00003223
Iteration 259/1000 | Loss: 0.00003223
Iteration 260/1000 | Loss: 0.00003222
Iteration 261/1000 | Loss: 0.00003222
Iteration 262/1000 | Loss: 0.00003222
Iteration 263/1000 | Loss: 0.00003222
Iteration 264/1000 | Loss: 0.00003222
Iteration 265/1000 | Loss: 0.00003222
Iteration 266/1000 | Loss: 0.00003222
Iteration 267/1000 | Loss: 0.00003222
Iteration 268/1000 | Loss: 0.00003221
Iteration 269/1000 | Loss: 0.00003221
Iteration 270/1000 | Loss: 0.00003221
Iteration 271/1000 | Loss: 0.00003221
Iteration 272/1000 | Loss: 0.00003221
Iteration 273/1000 | Loss: 0.00003221
Iteration 274/1000 | Loss: 0.00003221
Iteration 275/1000 | Loss: 0.00003221
Iteration 276/1000 | Loss: 0.00003221
Iteration 277/1000 | Loss: 0.00003221
Iteration 278/1000 | Loss: 0.00003221
Iteration 279/1000 | Loss: 0.00003221
Iteration 280/1000 | Loss: 0.00003221
Iteration 281/1000 | Loss: 0.00003221
Iteration 282/1000 | Loss: 0.00003221
Iteration 283/1000 | Loss: 0.00003221
Iteration 284/1000 | Loss: 0.00003221
Iteration 285/1000 | Loss: 0.00003221
Iteration 286/1000 | Loss: 0.00003221
Iteration 287/1000 | Loss: 0.00003220
Iteration 288/1000 | Loss: 0.00003220
Iteration 289/1000 | Loss: 0.00003220
Iteration 290/1000 | Loss: 0.00003220
Iteration 291/1000 | Loss: 0.00003220
Iteration 292/1000 | Loss: 0.00003220
Iteration 293/1000 | Loss: 0.00003220
Iteration 294/1000 | Loss: 0.00003220
Iteration 295/1000 | Loss: 0.00003220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [3.220271537429653e-05, 3.220271537429653e-05, 3.220271537429653e-05, 3.220271537429653e-05, 3.220271537429653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.220271537429653e-05

Optimization complete. Final v2v error: 4.784581661224365 mm

Highest mean error: 5.601415157318115 mm for frame 40

Lowest mean error: 3.922910451889038 mm for frame 0

Saving results

Total time: 95.85617995262146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877506
Iteration 2/25 | Loss: 0.00172904
Iteration 3/25 | Loss: 0.00155630
Iteration 4/25 | Loss: 0.00154559
Iteration 5/25 | Loss: 0.00154430
Iteration 6/25 | Loss: 0.00154430
Iteration 7/25 | Loss: 0.00154430
Iteration 8/25 | Loss: 0.00154430
Iteration 9/25 | Loss: 0.00154430
Iteration 10/25 | Loss: 0.00154430
Iteration 11/25 | Loss: 0.00154430
Iteration 12/25 | Loss: 0.00154430
Iteration 13/25 | Loss: 0.00154430
Iteration 14/25 | Loss: 0.00154430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001544297905638814, 0.001544297905638814, 0.001544297905638814, 0.001544297905638814, 0.001544297905638814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001544297905638814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24883008
Iteration 2/25 | Loss: 0.00132742
Iteration 3/25 | Loss: 0.00132741
Iteration 4/25 | Loss: 0.00132741
Iteration 5/25 | Loss: 0.00132741
Iteration 6/25 | Loss: 0.00132741
Iteration 7/25 | Loss: 0.00132741
Iteration 8/25 | Loss: 0.00132741
Iteration 9/25 | Loss: 0.00132741
Iteration 10/25 | Loss: 0.00132741
Iteration 11/25 | Loss: 0.00132741
Iteration 12/25 | Loss: 0.00132741
Iteration 13/25 | Loss: 0.00132741
Iteration 14/25 | Loss: 0.00132741
Iteration 15/25 | Loss: 0.00132741
Iteration 16/25 | Loss: 0.00132741
Iteration 17/25 | Loss: 0.00132741
Iteration 18/25 | Loss: 0.00132741
Iteration 19/25 | Loss: 0.00132741
Iteration 20/25 | Loss: 0.00132741
Iteration 21/25 | Loss: 0.00132741
Iteration 22/25 | Loss: 0.00132741
Iteration 23/25 | Loss: 0.00132741
Iteration 24/25 | Loss: 0.00132741
Iteration 25/25 | Loss: 0.00132741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001327408361248672, 0.001327408361248672, 0.001327408361248672, 0.001327408361248672, 0.001327408361248672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001327408361248672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132741
Iteration 2/1000 | Loss: 0.00005677
Iteration 3/1000 | Loss: 0.00003498
Iteration 4/1000 | Loss: 0.00002909
Iteration 5/1000 | Loss: 0.00002673
Iteration 6/1000 | Loss: 0.00002590
Iteration 7/1000 | Loss: 0.00002543
Iteration 8/1000 | Loss: 0.00002513
Iteration 9/1000 | Loss: 0.00002494
Iteration 10/1000 | Loss: 0.00002490
Iteration 11/1000 | Loss: 0.00002481
Iteration 12/1000 | Loss: 0.00002480
Iteration 13/1000 | Loss: 0.00002480
Iteration 14/1000 | Loss: 0.00002479
Iteration 15/1000 | Loss: 0.00002478
Iteration 16/1000 | Loss: 0.00002478
Iteration 17/1000 | Loss: 0.00002472
Iteration 18/1000 | Loss: 0.00002472
Iteration 19/1000 | Loss: 0.00002471
Iteration 20/1000 | Loss: 0.00002468
Iteration 21/1000 | Loss: 0.00002468
Iteration 22/1000 | Loss: 0.00002468
Iteration 23/1000 | Loss: 0.00002462
Iteration 24/1000 | Loss: 0.00002462
Iteration 25/1000 | Loss: 0.00002462
Iteration 26/1000 | Loss: 0.00002461
Iteration 27/1000 | Loss: 0.00002461
Iteration 28/1000 | Loss: 0.00002461
Iteration 29/1000 | Loss: 0.00002459
Iteration 30/1000 | Loss: 0.00002459
Iteration 31/1000 | Loss: 0.00002459
Iteration 32/1000 | Loss: 0.00002458
Iteration 33/1000 | Loss: 0.00002458
Iteration 34/1000 | Loss: 0.00002458
Iteration 35/1000 | Loss: 0.00002457
Iteration 36/1000 | Loss: 0.00002457
Iteration 37/1000 | Loss: 0.00002457
Iteration 38/1000 | Loss: 0.00002456
Iteration 39/1000 | Loss: 0.00002456
Iteration 40/1000 | Loss: 0.00002456
Iteration 41/1000 | Loss: 0.00002456
Iteration 42/1000 | Loss: 0.00002456
Iteration 43/1000 | Loss: 0.00002455
Iteration 44/1000 | Loss: 0.00002455
Iteration 45/1000 | Loss: 0.00002454
Iteration 46/1000 | Loss: 0.00002454
Iteration 47/1000 | Loss: 0.00002454
Iteration 48/1000 | Loss: 0.00002454
Iteration 49/1000 | Loss: 0.00002454
Iteration 50/1000 | Loss: 0.00002454
Iteration 51/1000 | Loss: 0.00002454
Iteration 52/1000 | Loss: 0.00002454
Iteration 53/1000 | Loss: 0.00002454
Iteration 54/1000 | Loss: 0.00002454
Iteration 55/1000 | Loss: 0.00002454
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00002454
Iteration 58/1000 | Loss: 0.00002453
Iteration 59/1000 | Loss: 0.00002453
Iteration 60/1000 | Loss: 0.00002453
Iteration 61/1000 | Loss: 0.00002453
Iteration 62/1000 | Loss: 0.00002453
Iteration 63/1000 | Loss: 0.00002453
Iteration 64/1000 | Loss: 0.00002453
Iteration 65/1000 | Loss: 0.00002453
Iteration 66/1000 | Loss: 0.00002453
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00002453
Iteration 70/1000 | Loss: 0.00002452
Iteration 71/1000 | Loss: 0.00002452
Iteration 72/1000 | Loss: 0.00002452
Iteration 73/1000 | Loss: 0.00002452
Iteration 74/1000 | Loss: 0.00002452
Iteration 75/1000 | Loss: 0.00002452
Iteration 76/1000 | Loss: 0.00002452
Iteration 77/1000 | Loss: 0.00002452
Iteration 78/1000 | Loss: 0.00002452
Iteration 79/1000 | Loss: 0.00002452
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002451
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002451
Iteration 85/1000 | Loss: 0.00002451
Iteration 86/1000 | Loss: 0.00002451
Iteration 87/1000 | Loss: 0.00002451
Iteration 88/1000 | Loss: 0.00002451
Iteration 89/1000 | Loss: 0.00002451
Iteration 90/1000 | Loss: 0.00002451
Iteration 91/1000 | Loss: 0.00002451
Iteration 92/1000 | Loss: 0.00002451
Iteration 93/1000 | Loss: 0.00002451
Iteration 94/1000 | Loss: 0.00002451
Iteration 95/1000 | Loss: 0.00002451
Iteration 96/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.451199179631658e-05, 2.451199179631658e-05, 2.451199179631658e-05, 2.451199179631658e-05, 2.451199179631658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.451199179631658e-05

Optimization complete. Final v2v error: 4.206467628479004 mm

Highest mean error: 4.489550590515137 mm for frame 92

Lowest mean error: 3.9211950302124023 mm for frame 162

Saving results

Total time: 30.410497903823853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454518
Iteration 2/25 | Loss: 0.00175475
Iteration 3/25 | Loss: 0.00159664
Iteration 4/25 | Loss: 0.00156953
Iteration 5/25 | Loss: 0.00156298
Iteration 6/25 | Loss: 0.00156162
Iteration 7/25 | Loss: 0.00156162
Iteration 8/25 | Loss: 0.00156162
Iteration 9/25 | Loss: 0.00156162
Iteration 10/25 | Loss: 0.00156162
Iteration 11/25 | Loss: 0.00156162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001561622484587133, 0.001561622484587133, 0.001561622484587133, 0.001561622484587133, 0.001561622484587133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001561622484587133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23008120
Iteration 2/25 | Loss: 0.00216126
Iteration 3/25 | Loss: 0.00216126
Iteration 4/25 | Loss: 0.00216126
Iteration 5/25 | Loss: 0.00216126
Iteration 6/25 | Loss: 0.00216126
Iteration 7/25 | Loss: 0.00216126
Iteration 8/25 | Loss: 0.00216126
Iteration 9/25 | Loss: 0.00216126
Iteration 10/25 | Loss: 0.00216126
Iteration 11/25 | Loss: 0.00216126
Iteration 12/25 | Loss: 0.00216126
Iteration 13/25 | Loss: 0.00216126
Iteration 14/25 | Loss: 0.00216126
Iteration 15/25 | Loss: 0.00216126
Iteration 16/25 | Loss: 0.00216126
Iteration 17/25 | Loss: 0.00216126
Iteration 18/25 | Loss: 0.00216126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021612595301121473, 0.0021612595301121473, 0.0021612595301121473, 0.0021612595301121473, 0.0021612595301121473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021612595301121473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216126
Iteration 2/1000 | Loss: 0.00007785
Iteration 3/1000 | Loss: 0.00004761
Iteration 4/1000 | Loss: 0.00003820
Iteration 5/1000 | Loss: 0.00003258
Iteration 6/1000 | Loss: 0.00002930
Iteration 7/1000 | Loss: 0.00002779
Iteration 8/1000 | Loss: 0.00002671
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002523
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002427
Iteration 13/1000 | Loss: 0.00002405
Iteration 14/1000 | Loss: 0.00002390
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002386
Iteration 17/1000 | Loss: 0.00002386
Iteration 18/1000 | Loss: 0.00002386
Iteration 19/1000 | Loss: 0.00002385
Iteration 20/1000 | Loss: 0.00002385
Iteration 21/1000 | Loss: 0.00002383
Iteration 22/1000 | Loss: 0.00002381
Iteration 23/1000 | Loss: 0.00002381
Iteration 24/1000 | Loss: 0.00002381
Iteration 25/1000 | Loss: 0.00002381
Iteration 26/1000 | Loss: 0.00002380
Iteration 27/1000 | Loss: 0.00002380
Iteration 28/1000 | Loss: 0.00002380
Iteration 29/1000 | Loss: 0.00002380
Iteration 30/1000 | Loss: 0.00002380
Iteration 31/1000 | Loss: 0.00002379
Iteration 32/1000 | Loss: 0.00002378
Iteration 33/1000 | Loss: 0.00002378
Iteration 34/1000 | Loss: 0.00002375
Iteration 35/1000 | Loss: 0.00002374
Iteration 36/1000 | Loss: 0.00002374
Iteration 37/1000 | Loss: 0.00002373
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002371
Iteration 40/1000 | Loss: 0.00002371
Iteration 41/1000 | Loss: 0.00002371
Iteration 42/1000 | Loss: 0.00002371
Iteration 43/1000 | Loss: 0.00002371
Iteration 44/1000 | Loss: 0.00002371
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002370
Iteration 48/1000 | Loss: 0.00002370
Iteration 49/1000 | Loss: 0.00002369
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002369
Iteration 52/1000 | Loss: 0.00002369
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00002368
Iteration 55/1000 | Loss: 0.00002367
Iteration 56/1000 | Loss: 0.00002367
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002366
Iteration 60/1000 | Loss: 0.00002365
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002365
Iteration 63/1000 | Loss: 0.00002364
Iteration 64/1000 | Loss: 0.00002364
Iteration 65/1000 | Loss: 0.00002364
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002363
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002362
Iteration 70/1000 | Loss: 0.00002362
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002361
Iteration 73/1000 | Loss: 0.00002361
Iteration 74/1000 | Loss: 0.00002361
Iteration 75/1000 | Loss: 0.00002361
Iteration 76/1000 | Loss: 0.00002361
Iteration 77/1000 | Loss: 0.00002361
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002361
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002360
Iteration 91/1000 | Loss: 0.00002360
Iteration 92/1000 | Loss: 0.00002360
Iteration 93/1000 | Loss: 0.00002360
Iteration 94/1000 | Loss: 0.00002360
Iteration 95/1000 | Loss: 0.00002359
Iteration 96/1000 | Loss: 0.00002359
Iteration 97/1000 | Loss: 0.00002359
Iteration 98/1000 | Loss: 0.00002358
Iteration 99/1000 | Loss: 0.00002358
Iteration 100/1000 | Loss: 0.00002358
Iteration 101/1000 | Loss: 0.00002358
Iteration 102/1000 | Loss: 0.00002357
Iteration 103/1000 | Loss: 0.00002357
Iteration 104/1000 | Loss: 0.00002357
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002356
Iteration 108/1000 | Loss: 0.00002355
Iteration 109/1000 | Loss: 0.00002355
Iteration 110/1000 | Loss: 0.00002355
Iteration 111/1000 | Loss: 0.00002355
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002355
Iteration 114/1000 | Loss: 0.00002355
Iteration 115/1000 | Loss: 0.00002355
Iteration 116/1000 | Loss: 0.00002355
Iteration 117/1000 | Loss: 0.00002355
Iteration 118/1000 | Loss: 0.00002355
Iteration 119/1000 | Loss: 0.00002355
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002354
Iteration 122/1000 | Loss: 0.00002354
Iteration 123/1000 | Loss: 0.00002354
Iteration 124/1000 | Loss: 0.00002354
Iteration 125/1000 | Loss: 0.00002354
Iteration 126/1000 | Loss: 0.00002354
Iteration 127/1000 | Loss: 0.00002354
Iteration 128/1000 | Loss: 0.00002354
Iteration 129/1000 | Loss: 0.00002354
Iteration 130/1000 | Loss: 0.00002354
Iteration 131/1000 | Loss: 0.00002354
Iteration 132/1000 | Loss: 0.00002354
Iteration 133/1000 | Loss: 0.00002354
Iteration 134/1000 | Loss: 0.00002353
Iteration 135/1000 | Loss: 0.00002353
Iteration 136/1000 | Loss: 0.00002353
Iteration 137/1000 | Loss: 0.00002353
Iteration 138/1000 | Loss: 0.00002353
Iteration 139/1000 | Loss: 0.00002352
Iteration 140/1000 | Loss: 0.00002352
Iteration 141/1000 | Loss: 0.00002352
Iteration 142/1000 | Loss: 0.00002352
Iteration 143/1000 | Loss: 0.00002352
Iteration 144/1000 | Loss: 0.00002352
Iteration 145/1000 | Loss: 0.00002352
Iteration 146/1000 | Loss: 0.00002352
Iteration 147/1000 | Loss: 0.00002352
Iteration 148/1000 | Loss: 0.00002352
Iteration 149/1000 | Loss: 0.00002352
Iteration 150/1000 | Loss: 0.00002352
Iteration 151/1000 | Loss: 0.00002352
Iteration 152/1000 | Loss: 0.00002352
Iteration 153/1000 | Loss: 0.00002352
Iteration 154/1000 | Loss: 0.00002352
Iteration 155/1000 | Loss: 0.00002352
Iteration 156/1000 | Loss: 0.00002352
Iteration 157/1000 | Loss: 0.00002352
Iteration 158/1000 | Loss: 0.00002352
Iteration 159/1000 | Loss: 0.00002352
Iteration 160/1000 | Loss: 0.00002352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.352173214603681e-05, 2.352173214603681e-05, 2.352173214603681e-05, 2.352173214603681e-05, 2.352173214603681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.352173214603681e-05

Optimization complete. Final v2v error: 4.157709121704102 mm

Highest mean error: 4.608088970184326 mm for frame 110

Lowest mean error: 3.829503059387207 mm for frame 239

Saving results

Total time: 43.49528741836548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017325
Iteration 2/25 | Loss: 0.01017325
Iteration 3/25 | Loss: 0.01017325
Iteration 4/25 | Loss: 0.01017324
Iteration 5/25 | Loss: 0.01017324
Iteration 6/25 | Loss: 0.01017324
Iteration 7/25 | Loss: 0.01017324
Iteration 8/25 | Loss: 0.01017324
Iteration 9/25 | Loss: 0.01017324
Iteration 10/25 | Loss: 0.01017324
Iteration 11/25 | Loss: 0.01017324
Iteration 12/25 | Loss: 0.01017324
Iteration 13/25 | Loss: 0.01017324
Iteration 14/25 | Loss: 0.01017323
Iteration 15/25 | Loss: 0.01017323
Iteration 16/25 | Loss: 0.01017323
Iteration 17/25 | Loss: 0.01017323
Iteration 18/25 | Loss: 0.01017323
Iteration 19/25 | Loss: 0.01017323
Iteration 20/25 | Loss: 0.01017323
Iteration 21/25 | Loss: 0.01017323
Iteration 22/25 | Loss: 0.01017323
Iteration 23/25 | Loss: 0.01017323
Iteration 24/25 | Loss: 0.01017323
Iteration 25/25 | Loss: 0.01017322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35702360
Iteration 2/25 | Loss: 0.18869010
Iteration 3/25 | Loss: 0.17663606
Iteration 4/25 | Loss: 0.17466731
Iteration 5/25 | Loss: 0.17466716
Iteration 6/25 | Loss: 0.17466716
Iteration 7/25 | Loss: 0.17466716
Iteration 8/25 | Loss: 0.17466715
Iteration 9/25 | Loss: 0.17466715
Iteration 10/25 | Loss: 0.17466715
Iteration 11/25 | Loss: 0.17466715
Iteration 12/25 | Loss: 0.17466716
Iteration 13/25 | Loss: 0.17466712
Iteration 14/25 | Loss: 0.17466712
Iteration 15/25 | Loss: 0.17466712
Iteration 16/25 | Loss: 0.17466712
Iteration 17/25 | Loss: 0.17466712
Iteration 18/25 | Loss: 0.17466712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.1746671199798584, 0.1746671199798584, 0.1746671199798584, 0.1746671199798584, 0.1746671199798584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1746671199798584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17466712
Iteration 2/1000 | Loss: 0.00573924
Iteration 3/1000 | Loss: 0.00297997
Iteration 4/1000 | Loss: 0.00294338
Iteration 5/1000 | Loss: 0.00378147
Iteration 6/1000 | Loss: 0.00156042
Iteration 7/1000 | Loss: 0.00102394
Iteration 8/1000 | Loss: 0.00075503
Iteration 9/1000 | Loss: 0.00105633
Iteration 10/1000 | Loss: 0.00042009
Iteration 11/1000 | Loss: 0.00122951
Iteration 12/1000 | Loss: 0.00015365
Iteration 13/1000 | Loss: 0.00034108
Iteration 14/1000 | Loss: 0.00019436
Iteration 15/1000 | Loss: 0.00014588
Iteration 16/1000 | Loss: 0.00088608
Iteration 17/1000 | Loss: 0.00058453
Iteration 18/1000 | Loss: 0.00008952
Iteration 19/1000 | Loss: 0.00022009
Iteration 20/1000 | Loss: 0.00040947
Iteration 21/1000 | Loss: 0.00082499
Iteration 22/1000 | Loss: 0.00005599
Iteration 23/1000 | Loss: 0.00005060
Iteration 24/1000 | Loss: 0.00014244
Iteration 25/1000 | Loss: 0.00035834
Iteration 26/1000 | Loss: 0.00016845
Iteration 27/1000 | Loss: 0.00024571
Iteration 28/1000 | Loss: 0.00033746
Iteration 29/1000 | Loss: 0.00007171
Iteration 30/1000 | Loss: 0.00019385
Iteration 31/1000 | Loss: 0.00017125
Iteration 32/1000 | Loss: 0.00005467
Iteration 33/1000 | Loss: 0.00004342
Iteration 34/1000 | Loss: 0.00004013
Iteration 35/1000 | Loss: 0.00012059
Iteration 36/1000 | Loss: 0.00016823
Iteration 37/1000 | Loss: 0.00006690
Iteration 38/1000 | Loss: 0.00004840
Iteration 39/1000 | Loss: 0.00003575
Iteration 40/1000 | Loss: 0.00003472
Iteration 41/1000 | Loss: 0.00017052
Iteration 42/1000 | Loss: 0.00009432
Iteration 43/1000 | Loss: 0.00008115
Iteration 44/1000 | Loss: 0.00022347
Iteration 45/1000 | Loss: 0.00003345
Iteration 46/1000 | Loss: 0.00003324
Iteration 47/1000 | Loss: 0.00009978
Iteration 48/1000 | Loss: 0.00006806
Iteration 49/1000 | Loss: 0.00007654
Iteration 50/1000 | Loss: 0.00004663
Iteration 51/1000 | Loss: 0.00004304
Iteration 52/1000 | Loss: 0.00008023
Iteration 53/1000 | Loss: 0.00003236
Iteration 54/1000 | Loss: 0.00009150
Iteration 55/1000 | Loss: 0.00003210
Iteration 56/1000 | Loss: 0.00003162
Iteration 57/1000 | Loss: 0.00003151
Iteration 58/1000 | Loss: 0.00015593
Iteration 59/1000 | Loss: 0.00010426
Iteration 60/1000 | Loss: 0.00003149
Iteration 61/1000 | Loss: 0.00003124
Iteration 62/1000 | Loss: 0.00003122
Iteration 63/1000 | Loss: 0.00003121
Iteration 64/1000 | Loss: 0.00003121
Iteration 65/1000 | Loss: 0.00003121
Iteration 66/1000 | Loss: 0.00003120
Iteration 67/1000 | Loss: 0.00003119
Iteration 68/1000 | Loss: 0.00003119
Iteration 69/1000 | Loss: 0.00003119
Iteration 70/1000 | Loss: 0.00003119
Iteration 71/1000 | Loss: 0.00003118
Iteration 72/1000 | Loss: 0.00003118
Iteration 73/1000 | Loss: 0.00003117
Iteration 74/1000 | Loss: 0.00003117
Iteration 75/1000 | Loss: 0.00003117
Iteration 76/1000 | Loss: 0.00003117
Iteration 77/1000 | Loss: 0.00003117
Iteration 78/1000 | Loss: 0.00003116
Iteration 79/1000 | Loss: 0.00003116
Iteration 80/1000 | Loss: 0.00003116
Iteration 81/1000 | Loss: 0.00003116
Iteration 82/1000 | Loss: 0.00003115
Iteration 83/1000 | Loss: 0.00003115
Iteration 84/1000 | Loss: 0.00003115
Iteration 85/1000 | Loss: 0.00003115
Iteration 86/1000 | Loss: 0.00003113
Iteration 87/1000 | Loss: 0.00003113
Iteration 88/1000 | Loss: 0.00003113
Iteration 89/1000 | Loss: 0.00003113
Iteration 90/1000 | Loss: 0.00003113
Iteration 91/1000 | Loss: 0.00003113
Iteration 92/1000 | Loss: 0.00003113
Iteration 93/1000 | Loss: 0.00003113
Iteration 94/1000 | Loss: 0.00003113
Iteration 95/1000 | Loss: 0.00003113
Iteration 96/1000 | Loss: 0.00003113
Iteration 97/1000 | Loss: 0.00003113
Iteration 98/1000 | Loss: 0.00003112
Iteration 99/1000 | Loss: 0.00003112
Iteration 100/1000 | Loss: 0.00003112
Iteration 101/1000 | Loss: 0.00003111
Iteration 102/1000 | Loss: 0.00003111
Iteration 103/1000 | Loss: 0.00003110
Iteration 104/1000 | Loss: 0.00003110
Iteration 105/1000 | Loss: 0.00003110
Iteration 106/1000 | Loss: 0.00003110
Iteration 107/1000 | Loss: 0.00003110
Iteration 108/1000 | Loss: 0.00003110
Iteration 109/1000 | Loss: 0.00003110
Iteration 110/1000 | Loss: 0.00003110
Iteration 111/1000 | Loss: 0.00003110
Iteration 112/1000 | Loss: 0.00003109
Iteration 113/1000 | Loss: 0.00003109
Iteration 114/1000 | Loss: 0.00003109
Iteration 115/1000 | Loss: 0.00003109
Iteration 116/1000 | Loss: 0.00003109
Iteration 117/1000 | Loss: 0.00003109
Iteration 118/1000 | Loss: 0.00003109
Iteration 119/1000 | Loss: 0.00003108
Iteration 120/1000 | Loss: 0.00003108
Iteration 121/1000 | Loss: 0.00003108
Iteration 122/1000 | Loss: 0.00003108
Iteration 123/1000 | Loss: 0.00003108
Iteration 124/1000 | Loss: 0.00003108
Iteration 125/1000 | Loss: 0.00003108
Iteration 126/1000 | Loss: 0.00003108
Iteration 127/1000 | Loss: 0.00003107
Iteration 128/1000 | Loss: 0.00003107
Iteration 129/1000 | Loss: 0.00003107
Iteration 130/1000 | Loss: 0.00003107
Iteration 131/1000 | Loss: 0.00003107
Iteration 132/1000 | Loss: 0.00003107
Iteration 133/1000 | Loss: 0.00003107
Iteration 134/1000 | Loss: 0.00003107
Iteration 135/1000 | Loss: 0.00003107
Iteration 136/1000 | Loss: 0.00003107
Iteration 137/1000 | Loss: 0.00003106
Iteration 138/1000 | Loss: 0.00003106
Iteration 139/1000 | Loss: 0.00003106
Iteration 140/1000 | Loss: 0.00003106
Iteration 141/1000 | Loss: 0.00003106
Iteration 142/1000 | Loss: 0.00003106
Iteration 143/1000 | Loss: 0.00003106
Iteration 144/1000 | Loss: 0.00003106
Iteration 145/1000 | Loss: 0.00003106
Iteration 146/1000 | Loss: 0.00003106
Iteration 147/1000 | Loss: 0.00003106
Iteration 148/1000 | Loss: 0.00003105
Iteration 149/1000 | Loss: 0.00003105
Iteration 150/1000 | Loss: 0.00003105
Iteration 151/1000 | Loss: 0.00003105
Iteration 152/1000 | Loss: 0.00003105
Iteration 153/1000 | Loss: 0.00003105
Iteration 154/1000 | Loss: 0.00003105
Iteration 155/1000 | Loss: 0.00003104
Iteration 156/1000 | Loss: 0.00003104
Iteration 157/1000 | Loss: 0.00003104
Iteration 158/1000 | Loss: 0.00003104
Iteration 159/1000 | Loss: 0.00003104
Iteration 160/1000 | Loss: 0.00003104
Iteration 161/1000 | Loss: 0.00003103
Iteration 162/1000 | Loss: 0.00003103
Iteration 163/1000 | Loss: 0.00003103
Iteration 164/1000 | Loss: 0.00003103
Iteration 165/1000 | Loss: 0.00003103
Iteration 166/1000 | Loss: 0.00003103
Iteration 167/1000 | Loss: 0.00003103
Iteration 168/1000 | Loss: 0.00003103
Iteration 169/1000 | Loss: 0.00003103
Iteration 170/1000 | Loss: 0.00003103
Iteration 171/1000 | Loss: 0.00003103
Iteration 172/1000 | Loss: 0.00003103
Iteration 173/1000 | Loss: 0.00003103
Iteration 174/1000 | Loss: 0.00003103
Iteration 175/1000 | Loss: 0.00003103
Iteration 176/1000 | Loss: 0.00003103
Iteration 177/1000 | Loss: 0.00003103
Iteration 178/1000 | Loss: 0.00003103
Iteration 179/1000 | Loss: 0.00003102
Iteration 180/1000 | Loss: 0.00003102
Iteration 181/1000 | Loss: 0.00003102
Iteration 182/1000 | Loss: 0.00003102
Iteration 183/1000 | Loss: 0.00003102
Iteration 184/1000 | Loss: 0.00003102
Iteration 185/1000 | Loss: 0.00003102
Iteration 186/1000 | Loss: 0.00003102
Iteration 187/1000 | Loss: 0.00003102
Iteration 188/1000 | Loss: 0.00003102
Iteration 189/1000 | Loss: 0.00003102
Iteration 190/1000 | Loss: 0.00003102
Iteration 191/1000 | Loss: 0.00003102
Iteration 192/1000 | Loss: 0.00003102
Iteration 193/1000 | Loss: 0.00003102
Iteration 194/1000 | Loss: 0.00003102
Iteration 195/1000 | Loss: 0.00003102
Iteration 196/1000 | Loss: 0.00003102
Iteration 197/1000 | Loss: 0.00003102
Iteration 198/1000 | Loss: 0.00003102
Iteration 199/1000 | Loss: 0.00003102
Iteration 200/1000 | Loss: 0.00003101
Iteration 201/1000 | Loss: 0.00003101
Iteration 202/1000 | Loss: 0.00003101
Iteration 203/1000 | Loss: 0.00003101
Iteration 204/1000 | Loss: 0.00003101
Iteration 205/1000 | Loss: 0.00003101
Iteration 206/1000 | Loss: 0.00003101
Iteration 207/1000 | Loss: 0.00003101
Iteration 208/1000 | Loss: 0.00003101
Iteration 209/1000 | Loss: 0.00003100
Iteration 210/1000 | Loss: 0.00003100
Iteration 211/1000 | Loss: 0.00003100
Iteration 212/1000 | Loss: 0.00003100
Iteration 213/1000 | Loss: 0.00003100
Iteration 214/1000 | Loss: 0.00003100
Iteration 215/1000 | Loss: 0.00003100
Iteration 216/1000 | Loss: 0.00003100
Iteration 217/1000 | Loss: 0.00003099
Iteration 218/1000 | Loss: 0.00003099
Iteration 219/1000 | Loss: 0.00003099
Iteration 220/1000 | Loss: 0.00003099
Iteration 221/1000 | Loss: 0.00003099
Iteration 222/1000 | Loss: 0.00003099
Iteration 223/1000 | Loss: 0.00003099
Iteration 224/1000 | Loss: 0.00003099
Iteration 225/1000 | Loss: 0.00003099
Iteration 226/1000 | Loss: 0.00003099
Iteration 227/1000 | Loss: 0.00003099
Iteration 228/1000 | Loss: 0.00003099
Iteration 229/1000 | Loss: 0.00003099
Iteration 230/1000 | Loss: 0.00003099
Iteration 231/1000 | Loss: 0.00003099
Iteration 232/1000 | Loss: 0.00003099
Iteration 233/1000 | Loss: 0.00003099
Iteration 234/1000 | Loss: 0.00003099
Iteration 235/1000 | Loss: 0.00003099
Iteration 236/1000 | Loss: 0.00003099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [3.098551314906217e-05, 3.098551314906217e-05, 3.098551314906217e-05, 3.098551314906217e-05, 3.098551314906217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.098551314906217e-05

Optimization complete. Final v2v error: 4.354733943939209 mm

Highest mean error: 11.802470207214355 mm for frame 208

Lowest mean error: 3.624465227127075 mm for frame 209

Saving results

Total time: 119.48323655128479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073091
Iteration 2/25 | Loss: 0.00317715
Iteration 3/25 | Loss: 0.00237780
Iteration 4/25 | Loss: 0.00231742
Iteration 5/25 | Loss: 0.00221735
Iteration 6/25 | Loss: 0.00210736
Iteration 7/25 | Loss: 0.00205319
Iteration 8/25 | Loss: 0.00200204
Iteration 9/25 | Loss: 0.00197932
Iteration 10/25 | Loss: 0.00195544
Iteration 11/25 | Loss: 0.00195559
Iteration 12/25 | Loss: 0.00195669
Iteration 13/25 | Loss: 0.00196350
Iteration 14/25 | Loss: 0.00195330
Iteration 15/25 | Loss: 0.00195453
Iteration 16/25 | Loss: 0.00195800
Iteration 17/25 | Loss: 0.00194475
Iteration 18/25 | Loss: 0.00194428
Iteration 19/25 | Loss: 0.00194147
Iteration 20/25 | Loss: 0.00193983
Iteration 21/25 | Loss: 0.00193159
Iteration 22/25 | Loss: 0.00193029
Iteration 23/25 | Loss: 0.00193020
Iteration 24/25 | Loss: 0.00192707
Iteration 25/25 | Loss: 0.00192726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22274852
Iteration 2/25 | Loss: 0.00794493
Iteration 3/25 | Loss: 0.00647009
Iteration 4/25 | Loss: 0.00621553
Iteration 5/25 | Loss: 0.00611663
Iteration 6/25 | Loss: 0.00611663
Iteration 7/25 | Loss: 0.00611662
Iteration 8/25 | Loss: 0.00611662
Iteration 9/25 | Loss: 0.00611662
Iteration 10/25 | Loss: 0.00611662
Iteration 11/25 | Loss: 0.00611662
Iteration 12/25 | Loss: 0.00611662
Iteration 13/25 | Loss: 0.00611662
Iteration 14/25 | Loss: 0.00611662
Iteration 15/25 | Loss: 0.00611662
Iteration 16/25 | Loss: 0.00611662
Iteration 17/25 | Loss: 0.00611662
Iteration 18/25 | Loss: 0.00611662
Iteration 19/25 | Loss: 0.00611662
Iteration 20/25 | Loss: 0.00611662
Iteration 21/25 | Loss: 0.00611662
Iteration 22/25 | Loss: 0.00611662
Iteration 23/25 | Loss: 0.00611662
Iteration 24/25 | Loss: 0.00611662
Iteration 25/25 | Loss: 0.00611662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00611662
Iteration 2/1000 | Loss: 0.00167325
Iteration 3/1000 | Loss: 0.00171291
Iteration 4/1000 | Loss: 0.00053372
Iteration 5/1000 | Loss: 0.00116287
Iteration 6/1000 | Loss: 0.00048680
Iteration 7/1000 | Loss: 0.00343394
Iteration 8/1000 | Loss: 0.00079978
Iteration 9/1000 | Loss: 0.00086530
Iteration 10/1000 | Loss: 0.00151198
Iteration 11/1000 | Loss: 0.00317811
Iteration 12/1000 | Loss: 0.00073594
Iteration 13/1000 | Loss: 0.00101211
Iteration 14/1000 | Loss: 0.00039705
Iteration 15/1000 | Loss: 0.00059210
Iteration 16/1000 | Loss: 0.00175501
Iteration 17/1000 | Loss: 0.00169471
Iteration 18/1000 | Loss: 0.00068780
Iteration 19/1000 | Loss: 0.00036227
Iteration 20/1000 | Loss: 0.00029882
Iteration 21/1000 | Loss: 0.00074650
Iteration 22/1000 | Loss: 0.00221607
Iteration 23/1000 | Loss: 0.00367019
Iteration 24/1000 | Loss: 0.00150439
Iteration 25/1000 | Loss: 0.00356614
Iteration 26/1000 | Loss: 0.01364432
Iteration 27/1000 | Loss: 0.00348810
Iteration 28/1000 | Loss: 0.00075252
Iteration 29/1000 | Loss: 0.00071361
Iteration 30/1000 | Loss: 0.00119775
Iteration 31/1000 | Loss: 0.00409185
Iteration 32/1000 | Loss: 0.00310956
Iteration 33/1000 | Loss: 0.00251221
Iteration 34/1000 | Loss: 0.00276914
Iteration 35/1000 | Loss: 0.00102562
Iteration 36/1000 | Loss: 0.00043276
Iteration 37/1000 | Loss: 0.00032565
Iteration 38/1000 | Loss: 0.00070303
Iteration 39/1000 | Loss: 0.00078571
Iteration 40/1000 | Loss: 0.00096962
Iteration 41/1000 | Loss: 0.00186471
Iteration 42/1000 | Loss: 0.00050930
Iteration 43/1000 | Loss: 0.00185517
Iteration 44/1000 | Loss: 0.00245048
Iteration 45/1000 | Loss: 0.00169042
Iteration 46/1000 | Loss: 0.00141936
Iteration 47/1000 | Loss: 0.00086415
Iteration 48/1000 | Loss: 0.00088675
Iteration 49/1000 | Loss: 0.00078732
Iteration 50/1000 | Loss: 0.00049407
Iteration 51/1000 | Loss: 0.00166167
Iteration 52/1000 | Loss: 0.00112496
Iteration 53/1000 | Loss: 0.00094097
Iteration 54/1000 | Loss: 0.00165626
Iteration 55/1000 | Loss: 0.00143216
Iteration 56/1000 | Loss: 0.00128177
Iteration 57/1000 | Loss: 0.00118877
Iteration 58/1000 | Loss: 0.00075975
Iteration 59/1000 | Loss: 0.00052908
Iteration 60/1000 | Loss: 0.00134184
Iteration 61/1000 | Loss: 0.00130779
Iteration 62/1000 | Loss: 0.00156162
Iteration 63/1000 | Loss: 0.00087993
Iteration 64/1000 | Loss: 0.00113529
Iteration 65/1000 | Loss: 0.00128212
Iteration 66/1000 | Loss: 0.00131849
Iteration 67/1000 | Loss: 0.00102655
Iteration 68/1000 | Loss: 0.00127924
Iteration 69/1000 | Loss: 0.00075159
Iteration 70/1000 | Loss: 0.00045566
Iteration 71/1000 | Loss: 0.00148444
Iteration 72/1000 | Loss: 0.00063372
Iteration 73/1000 | Loss: 0.00109466
Iteration 74/1000 | Loss: 0.00075012
Iteration 75/1000 | Loss: 0.00118310
Iteration 76/1000 | Loss: 0.00105054
Iteration 77/1000 | Loss: 0.00072636
Iteration 78/1000 | Loss: 0.00083405
Iteration 79/1000 | Loss: 0.00096191
Iteration 80/1000 | Loss: 0.00082668
Iteration 81/1000 | Loss: 0.00057731
Iteration 82/1000 | Loss: 0.00051088
Iteration 83/1000 | Loss: 0.00085203
Iteration 84/1000 | Loss: 0.00100967
Iteration 85/1000 | Loss: 0.00119553
Iteration 86/1000 | Loss: 0.00100993
Iteration 87/1000 | Loss: 0.00055293
Iteration 88/1000 | Loss: 0.00107479
Iteration 89/1000 | Loss: 0.00073332
Iteration 90/1000 | Loss: 0.00178865
Iteration 91/1000 | Loss: 0.00070188
Iteration 92/1000 | Loss: 0.00120853
Iteration 93/1000 | Loss: 0.00138739
Iteration 94/1000 | Loss: 0.00062846
Iteration 95/1000 | Loss: 0.00048919
Iteration 96/1000 | Loss: 0.00044470
Iteration 97/1000 | Loss: 0.00026387
Iteration 98/1000 | Loss: 0.00057055
Iteration 99/1000 | Loss: 0.00029226
Iteration 100/1000 | Loss: 0.00012952
Iteration 101/1000 | Loss: 0.00011822
Iteration 102/1000 | Loss: 0.00060588
Iteration 103/1000 | Loss: 0.00036084
Iteration 104/1000 | Loss: 0.00019612
Iteration 105/1000 | Loss: 0.00019392
Iteration 106/1000 | Loss: 0.00016141
Iteration 107/1000 | Loss: 0.00019845
Iteration 108/1000 | Loss: 0.00017848
Iteration 109/1000 | Loss: 0.00037824
Iteration 110/1000 | Loss: 0.00012039
Iteration 111/1000 | Loss: 0.00044016
Iteration 112/1000 | Loss: 0.00019838
Iteration 113/1000 | Loss: 0.00049724
Iteration 114/1000 | Loss: 0.00023606
Iteration 115/1000 | Loss: 0.00040918
Iteration 116/1000 | Loss: 0.00040223
Iteration 117/1000 | Loss: 0.00023637
Iteration 118/1000 | Loss: 0.00020047
Iteration 119/1000 | Loss: 0.00011791
Iteration 120/1000 | Loss: 0.00049611
Iteration 121/1000 | Loss: 0.00067444
Iteration 122/1000 | Loss: 0.00043250
Iteration 123/1000 | Loss: 0.00055982
Iteration 124/1000 | Loss: 0.00040298
Iteration 125/1000 | Loss: 0.00082662
Iteration 126/1000 | Loss: 0.00012387
Iteration 127/1000 | Loss: 0.00010769
Iteration 128/1000 | Loss: 0.00009922
Iteration 129/1000 | Loss: 0.00023101
Iteration 130/1000 | Loss: 0.00024114
Iteration 131/1000 | Loss: 0.00072643
Iteration 132/1000 | Loss: 0.00060281
Iteration 133/1000 | Loss: 0.00023651
Iteration 134/1000 | Loss: 0.00023658
Iteration 135/1000 | Loss: 0.00022459
Iteration 136/1000 | Loss: 0.00010619
Iteration 137/1000 | Loss: 0.00009211
Iteration 138/1000 | Loss: 0.00008770
Iteration 139/1000 | Loss: 0.00008471
Iteration 140/1000 | Loss: 0.00008157
Iteration 141/1000 | Loss: 0.00007925
Iteration 142/1000 | Loss: 0.00007804
Iteration 143/1000 | Loss: 0.00007723
Iteration 144/1000 | Loss: 0.00007655
Iteration 145/1000 | Loss: 0.00007604
Iteration 146/1000 | Loss: 0.00007560
Iteration 147/1000 | Loss: 0.00007529
Iteration 148/1000 | Loss: 0.00007501
Iteration 149/1000 | Loss: 0.00007480
Iteration 150/1000 | Loss: 0.00007457
Iteration 151/1000 | Loss: 0.00007430
Iteration 152/1000 | Loss: 0.00007390
Iteration 153/1000 | Loss: 0.00082260
Iteration 154/1000 | Loss: 0.00034890
Iteration 155/1000 | Loss: 0.00042457
Iteration 156/1000 | Loss: 0.00095399
Iteration 157/1000 | Loss: 0.00044198
Iteration 158/1000 | Loss: 0.00050838
Iteration 159/1000 | Loss: 0.00039908
Iteration 160/1000 | Loss: 0.00014643
Iteration 161/1000 | Loss: 0.00010758
Iteration 162/1000 | Loss: 0.00029241
Iteration 163/1000 | Loss: 0.00007571
Iteration 164/1000 | Loss: 0.00052675
Iteration 165/1000 | Loss: 0.00162102
Iteration 166/1000 | Loss: 0.00138973
Iteration 167/1000 | Loss: 0.00031903
Iteration 168/1000 | Loss: 0.00008933
Iteration 169/1000 | Loss: 0.00137476
Iteration 170/1000 | Loss: 0.00055527
Iteration 171/1000 | Loss: 0.00145610
Iteration 172/1000 | Loss: 0.00049092
Iteration 173/1000 | Loss: 0.00108116
Iteration 174/1000 | Loss: 0.00052220
Iteration 175/1000 | Loss: 0.00077760
Iteration 176/1000 | Loss: 0.00084058
Iteration 177/1000 | Loss: 0.00084862
Iteration 178/1000 | Loss: 0.00025420
Iteration 179/1000 | Loss: 0.00188405
Iteration 180/1000 | Loss: 0.00091143
Iteration 181/1000 | Loss: 0.00099780
Iteration 182/1000 | Loss: 0.00083223
Iteration 183/1000 | Loss: 0.00064439
Iteration 184/1000 | Loss: 0.00032711
Iteration 185/1000 | Loss: 0.00069815
Iteration 186/1000 | Loss: 0.00053912
Iteration 187/1000 | Loss: 0.00028366
Iteration 188/1000 | Loss: 0.00038143
Iteration 189/1000 | Loss: 0.00006117
Iteration 190/1000 | Loss: 0.00020157
Iteration 191/1000 | Loss: 0.00034050
Iteration 192/1000 | Loss: 0.00032101
Iteration 193/1000 | Loss: 0.00008601
Iteration 194/1000 | Loss: 0.00024721
Iteration 195/1000 | Loss: 0.00026768
Iteration 196/1000 | Loss: 0.00034400
Iteration 197/1000 | Loss: 0.00029080
Iteration 198/1000 | Loss: 0.00029112
Iteration 199/1000 | Loss: 0.00046560
Iteration 200/1000 | Loss: 0.00012780
Iteration 201/1000 | Loss: 0.00005413
Iteration 202/1000 | Loss: 0.00004286
Iteration 203/1000 | Loss: 0.00003757
Iteration 204/1000 | Loss: 0.00024363
Iteration 205/1000 | Loss: 0.00025896
Iteration 206/1000 | Loss: 0.00082489
Iteration 207/1000 | Loss: 0.00068250
Iteration 208/1000 | Loss: 0.00003902
Iteration 209/1000 | Loss: 0.00010237
Iteration 210/1000 | Loss: 0.00003434
Iteration 211/1000 | Loss: 0.00035949
Iteration 212/1000 | Loss: 0.00003375
Iteration 213/1000 | Loss: 0.00003295
Iteration 214/1000 | Loss: 0.00003258
Iteration 215/1000 | Loss: 0.00003226
Iteration 216/1000 | Loss: 0.00003200
Iteration 217/1000 | Loss: 0.00003195
Iteration 218/1000 | Loss: 0.00003188
Iteration 219/1000 | Loss: 0.00003186
Iteration 220/1000 | Loss: 0.00003177
Iteration 221/1000 | Loss: 0.00003170
Iteration 222/1000 | Loss: 0.00003168
Iteration 223/1000 | Loss: 0.00003168
Iteration 224/1000 | Loss: 0.00003168
Iteration 225/1000 | Loss: 0.00003167
Iteration 226/1000 | Loss: 0.00003167
Iteration 227/1000 | Loss: 0.00003167
Iteration 228/1000 | Loss: 0.00003167
Iteration 229/1000 | Loss: 0.00003167
Iteration 230/1000 | Loss: 0.00003166
Iteration 231/1000 | Loss: 0.00003166
Iteration 232/1000 | Loss: 0.00003166
Iteration 233/1000 | Loss: 0.00003164
Iteration 234/1000 | Loss: 0.00003163
Iteration 235/1000 | Loss: 0.00003162
Iteration 236/1000 | Loss: 0.00003162
Iteration 237/1000 | Loss: 0.00003162
Iteration 238/1000 | Loss: 0.00003161
Iteration 239/1000 | Loss: 0.00003161
Iteration 240/1000 | Loss: 0.00003161
Iteration 241/1000 | Loss: 0.00003160
Iteration 242/1000 | Loss: 0.00003159
Iteration 243/1000 | Loss: 0.00003159
Iteration 244/1000 | Loss: 0.00003159
Iteration 245/1000 | Loss: 0.00003158
Iteration 246/1000 | Loss: 0.00003158
Iteration 247/1000 | Loss: 0.00003158
Iteration 248/1000 | Loss: 0.00003158
Iteration 249/1000 | Loss: 0.00003157
Iteration 250/1000 | Loss: 0.00003157
Iteration 251/1000 | Loss: 0.00003156
Iteration 252/1000 | Loss: 0.00003156
Iteration 253/1000 | Loss: 0.00003156
Iteration 254/1000 | Loss: 0.00003155
Iteration 255/1000 | Loss: 0.00003155
Iteration 256/1000 | Loss: 0.00003154
Iteration 257/1000 | Loss: 0.00003154
Iteration 258/1000 | Loss: 0.00003154
Iteration 259/1000 | Loss: 0.00003154
Iteration 260/1000 | Loss: 0.00003154
Iteration 261/1000 | Loss: 0.00003154
Iteration 262/1000 | Loss: 0.00003154
Iteration 263/1000 | Loss: 0.00003153
Iteration 264/1000 | Loss: 0.00003153
Iteration 265/1000 | Loss: 0.00003153
Iteration 266/1000 | Loss: 0.00003152
Iteration 267/1000 | Loss: 0.00003152
Iteration 268/1000 | Loss: 0.00003152
Iteration 269/1000 | Loss: 0.00003152
Iteration 270/1000 | Loss: 0.00003152
Iteration 271/1000 | Loss: 0.00003152
Iteration 272/1000 | Loss: 0.00003151
Iteration 273/1000 | Loss: 0.00003151
Iteration 274/1000 | Loss: 0.00003151
Iteration 275/1000 | Loss: 0.00003151
Iteration 276/1000 | Loss: 0.00003151
Iteration 277/1000 | Loss: 0.00003150
Iteration 278/1000 | Loss: 0.00003150
Iteration 279/1000 | Loss: 0.00003150
Iteration 280/1000 | Loss: 0.00003150
Iteration 281/1000 | Loss: 0.00003150
Iteration 282/1000 | Loss: 0.00003150
Iteration 283/1000 | Loss: 0.00003149
Iteration 284/1000 | Loss: 0.00003149
Iteration 285/1000 | Loss: 0.00003149
Iteration 286/1000 | Loss: 0.00003149
Iteration 287/1000 | Loss: 0.00003148
Iteration 288/1000 | Loss: 0.00003148
Iteration 289/1000 | Loss: 0.00003148
Iteration 290/1000 | Loss: 0.00003147
Iteration 291/1000 | Loss: 0.00003147
Iteration 292/1000 | Loss: 0.00003147
Iteration 293/1000 | Loss: 0.00003146
Iteration 294/1000 | Loss: 0.00003146
Iteration 295/1000 | Loss: 0.00003146
Iteration 296/1000 | Loss: 0.00003146
Iteration 297/1000 | Loss: 0.00003145
Iteration 298/1000 | Loss: 0.00003145
Iteration 299/1000 | Loss: 0.00003145
Iteration 300/1000 | Loss: 0.00003145
Iteration 301/1000 | Loss: 0.00003144
Iteration 302/1000 | Loss: 0.00003144
Iteration 303/1000 | Loss: 0.00003144
Iteration 304/1000 | Loss: 0.00003144
Iteration 305/1000 | Loss: 0.00003144
Iteration 306/1000 | Loss: 0.00003144
Iteration 307/1000 | Loss: 0.00003143
Iteration 308/1000 | Loss: 0.00003143
Iteration 309/1000 | Loss: 0.00003143
Iteration 310/1000 | Loss: 0.00003143
Iteration 311/1000 | Loss: 0.00003143
Iteration 312/1000 | Loss: 0.00003143
Iteration 313/1000 | Loss: 0.00003143
Iteration 314/1000 | Loss: 0.00003143
Iteration 315/1000 | Loss: 0.00003143
Iteration 316/1000 | Loss: 0.00003143
Iteration 317/1000 | Loss: 0.00003143
Iteration 318/1000 | Loss: 0.00003143
Iteration 319/1000 | Loss: 0.00003143
Iteration 320/1000 | Loss: 0.00003143
Iteration 321/1000 | Loss: 0.00003142
Iteration 322/1000 | Loss: 0.00003142
Iteration 323/1000 | Loss: 0.00003142
Iteration 324/1000 | Loss: 0.00003142
Iteration 325/1000 | Loss: 0.00003142
Iteration 326/1000 | Loss: 0.00003142
Iteration 327/1000 | Loss: 0.00003142
Iteration 328/1000 | Loss: 0.00003142
Iteration 329/1000 | Loss: 0.00003142
Iteration 330/1000 | Loss: 0.00003142
Iteration 331/1000 | Loss: 0.00003142
Iteration 332/1000 | Loss: 0.00003141
Iteration 333/1000 | Loss: 0.00003141
Iteration 334/1000 | Loss: 0.00003141
Iteration 335/1000 | Loss: 0.00003141
Iteration 336/1000 | Loss: 0.00003141
Iteration 337/1000 | Loss: 0.00003141
Iteration 338/1000 | Loss: 0.00003140
Iteration 339/1000 | Loss: 0.00003140
Iteration 340/1000 | Loss: 0.00003140
Iteration 341/1000 | Loss: 0.00003140
Iteration 342/1000 | Loss: 0.00003140
Iteration 343/1000 | Loss: 0.00003140
Iteration 344/1000 | Loss: 0.00003140
Iteration 345/1000 | Loss: 0.00003140
Iteration 346/1000 | Loss: 0.00003140
Iteration 347/1000 | Loss: 0.00003140
Iteration 348/1000 | Loss: 0.00003140
Iteration 349/1000 | Loss: 0.00003140
Iteration 350/1000 | Loss: 0.00003140
Iteration 351/1000 | Loss: 0.00003140
Iteration 352/1000 | Loss: 0.00003140
Iteration 353/1000 | Loss: 0.00003140
Iteration 354/1000 | Loss: 0.00003140
Iteration 355/1000 | Loss: 0.00003140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [3.1396240956382826e-05, 3.1396240956382826e-05, 3.1396240956382826e-05, 3.1396240956382826e-05, 3.1396240956382826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1396240956382826e-05

Optimization complete. Final v2v error: 4.36972713470459 mm

Highest mean error: 14.167818069458008 mm for frame 170

Lowest mean error: 3.767728805541992 mm for frame 0

Saving results

Total time: 392.0974555015564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_43_us_2744/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_43_us_2744/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397684
Iteration 2/25 | Loss: 0.00159282
Iteration 3/25 | Loss: 0.00152209
Iteration 4/25 | Loss: 0.00151416
Iteration 5/25 | Loss: 0.00151076
Iteration 6/25 | Loss: 0.00151065
Iteration 7/25 | Loss: 0.00151065
Iteration 8/25 | Loss: 0.00151065
Iteration 9/25 | Loss: 0.00151065
Iteration 10/25 | Loss: 0.00151065
Iteration 11/25 | Loss: 0.00151065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015106512000784278, 0.0015106512000784278, 0.0015106512000784278, 0.0015106512000784278, 0.0015106512000784278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015106512000784278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51874197
Iteration 2/25 | Loss: 0.00208484
Iteration 3/25 | Loss: 0.00208484
Iteration 4/25 | Loss: 0.00208484
Iteration 5/25 | Loss: 0.00208484
Iteration 6/25 | Loss: 0.00208484
Iteration 7/25 | Loss: 0.00208484
Iteration 8/25 | Loss: 0.00208484
Iteration 9/25 | Loss: 0.00208484
Iteration 10/25 | Loss: 0.00208484
Iteration 11/25 | Loss: 0.00208484
Iteration 12/25 | Loss: 0.00208484
Iteration 13/25 | Loss: 0.00208484
Iteration 14/25 | Loss: 0.00208484
Iteration 15/25 | Loss: 0.00208484
Iteration 16/25 | Loss: 0.00208484
Iteration 17/25 | Loss: 0.00208484
Iteration 18/25 | Loss: 0.00208484
Iteration 19/25 | Loss: 0.00208484
Iteration 20/25 | Loss: 0.00208484
Iteration 21/25 | Loss: 0.00208484
Iteration 22/25 | Loss: 0.00208484
Iteration 23/25 | Loss: 0.00208484
Iteration 24/25 | Loss: 0.00208484
Iteration 25/25 | Loss: 0.00208484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208484
Iteration 2/1000 | Loss: 0.00004477
Iteration 3/1000 | Loss: 0.00003277
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002373
Iteration 7/1000 | Loss: 0.00002325
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002233
Iteration 10/1000 | Loss: 0.00002231
Iteration 11/1000 | Loss: 0.00002230
Iteration 12/1000 | Loss: 0.00002221
Iteration 13/1000 | Loss: 0.00002221
Iteration 14/1000 | Loss: 0.00002219
Iteration 15/1000 | Loss: 0.00002219
Iteration 16/1000 | Loss: 0.00002219
Iteration 17/1000 | Loss: 0.00002219
Iteration 18/1000 | Loss: 0.00002219
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00002218
Iteration 21/1000 | Loss: 0.00002218
Iteration 22/1000 | Loss: 0.00002218
Iteration 23/1000 | Loss: 0.00002217
Iteration 24/1000 | Loss: 0.00002216
Iteration 25/1000 | Loss: 0.00002215
Iteration 26/1000 | Loss: 0.00002215
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002214
Iteration 29/1000 | Loss: 0.00002214
Iteration 30/1000 | Loss: 0.00002214
Iteration 31/1000 | Loss: 0.00002214
Iteration 32/1000 | Loss: 0.00002214
Iteration 33/1000 | Loss: 0.00002213
Iteration 34/1000 | Loss: 0.00002212
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002212
Iteration 37/1000 | Loss: 0.00002212
Iteration 38/1000 | Loss: 0.00002212
Iteration 39/1000 | Loss: 0.00002212
Iteration 40/1000 | Loss: 0.00002211
Iteration 41/1000 | Loss: 0.00002211
Iteration 42/1000 | Loss: 0.00002211
Iteration 43/1000 | Loss: 0.00002210
Iteration 44/1000 | Loss: 0.00002210
Iteration 45/1000 | Loss: 0.00002210
Iteration 46/1000 | Loss: 0.00002209
Iteration 47/1000 | Loss: 0.00002209
Iteration 48/1000 | Loss: 0.00002209
Iteration 49/1000 | Loss: 0.00002208
Iteration 50/1000 | Loss: 0.00002208
Iteration 51/1000 | Loss: 0.00002208
Iteration 52/1000 | Loss: 0.00002207
Iteration 53/1000 | Loss: 0.00002207
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002205
Iteration 59/1000 | Loss: 0.00002205
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002203
Iteration 63/1000 | Loss: 0.00002203
Iteration 64/1000 | Loss: 0.00002202
Iteration 65/1000 | Loss: 0.00002202
Iteration 66/1000 | Loss: 0.00002202
Iteration 67/1000 | Loss: 0.00002201
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00002201
Iteration 70/1000 | Loss: 0.00002200
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002199
Iteration 75/1000 | Loss: 0.00002199
Iteration 76/1000 | Loss: 0.00002199
Iteration 77/1000 | Loss: 0.00002199
Iteration 78/1000 | Loss: 0.00002199
Iteration 79/1000 | Loss: 0.00002199
Iteration 80/1000 | Loss: 0.00002199
Iteration 81/1000 | Loss: 0.00002199
Iteration 82/1000 | Loss: 0.00002199
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00002199
Iteration 86/1000 | Loss: 0.00002199
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002198
Iteration 92/1000 | Loss: 0.00002198
Iteration 93/1000 | Loss: 0.00002198
Iteration 94/1000 | Loss: 0.00002198
Iteration 95/1000 | Loss: 0.00002198
Iteration 96/1000 | Loss: 0.00002198
Iteration 97/1000 | Loss: 0.00002198
Iteration 98/1000 | Loss: 0.00002198
Iteration 99/1000 | Loss: 0.00002198
Iteration 100/1000 | Loss: 0.00002197
Iteration 101/1000 | Loss: 0.00002197
Iteration 102/1000 | Loss: 0.00002197
Iteration 103/1000 | Loss: 0.00002197
Iteration 104/1000 | Loss: 0.00002197
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002197
Iteration 108/1000 | Loss: 0.00002197
Iteration 109/1000 | Loss: 0.00002197
Iteration 110/1000 | Loss: 0.00002197
Iteration 111/1000 | Loss: 0.00002197
Iteration 112/1000 | Loss: 0.00002197
Iteration 113/1000 | Loss: 0.00002197
Iteration 114/1000 | Loss: 0.00002197
Iteration 115/1000 | Loss: 0.00002197
Iteration 116/1000 | Loss: 0.00002197
Iteration 117/1000 | Loss: 0.00002197
Iteration 118/1000 | Loss: 0.00002196
Iteration 119/1000 | Loss: 0.00002196
Iteration 120/1000 | Loss: 0.00002196
Iteration 121/1000 | Loss: 0.00002196
Iteration 122/1000 | Loss: 0.00002196
Iteration 123/1000 | Loss: 0.00002196
Iteration 124/1000 | Loss: 0.00002196
Iteration 125/1000 | Loss: 0.00002195
Iteration 126/1000 | Loss: 0.00002195
Iteration 127/1000 | Loss: 0.00002195
Iteration 128/1000 | Loss: 0.00002195
Iteration 129/1000 | Loss: 0.00002195
Iteration 130/1000 | Loss: 0.00002195
Iteration 131/1000 | Loss: 0.00002195
Iteration 132/1000 | Loss: 0.00002195
Iteration 133/1000 | Loss: 0.00002195
Iteration 134/1000 | Loss: 0.00002195
Iteration 135/1000 | Loss: 0.00002195
Iteration 136/1000 | Loss: 0.00002195
Iteration 137/1000 | Loss: 0.00002195
Iteration 138/1000 | Loss: 0.00002195
Iteration 139/1000 | Loss: 0.00002195
Iteration 140/1000 | Loss: 0.00002195
Iteration 141/1000 | Loss: 0.00002195
Iteration 142/1000 | Loss: 0.00002194
Iteration 143/1000 | Loss: 0.00002194
Iteration 144/1000 | Loss: 0.00002194
Iteration 145/1000 | Loss: 0.00002194
Iteration 146/1000 | Loss: 0.00002194
Iteration 147/1000 | Loss: 0.00002194
Iteration 148/1000 | Loss: 0.00002194
Iteration 149/1000 | Loss: 0.00002194
Iteration 150/1000 | Loss: 0.00002194
Iteration 151/1000 | Loss: 0.00002194
Iteration 152/1000 | Loss: 0.00002194
Iteration 153/1000 | Loss: 0.00002194
Iteration 154/1000 | Loss: 0.00002194
Iteration 155/1000 | Loss: 0.00002194
Iteration 156/1000 | Loss: 0.00002193
Iteration 157/1000 | Loss: 0.00002193
Iteration 158/1000 | Loss: 0.00002193
Iteration 159/1000 | Loss: 0.00002193
Iteration 160/1000 | Loss: 0.00002193
Iteration 161/1000 | Loss: 0.00002193
Iteration 162/1000 | Loss: 0.00002193
Iteration 163/1000 | Loss: 0.00002193
Iteration 164/1000 | Loss: 0.00002193
Iteration 165/1000 | Loss: 0.00002193
Iteration 166/1000 | Loss: 0.00002193
Iteration 167/1000 | Loss: 0.00002193
Iteration 168/1000 | Loss: 0.00002193
Iteration 169/1000 | Loss: 0.00002193
Iteration 170/1000 | Loss: 0.00002193
Iteration 171/1000 | Loss: 0.00002193
Iteration 172/1000 | Loss: 0.00002193
Iteration 173/1000 | Loss: 0.00002193
Iteration 174/1000 | Loss: 0.00002193
Iteration 175/1000 | Loss: 0.00002193
Iteration 176/1000 | Loss: 0.00002193
Iteration 177/1000 | Loss: 0.00002193
Iteration 178/1000 | Loss: 0.00002193
Iteration 179/1000 | Loss: 0.00002193
Iteration 180/1000 | Loss: 0.00002193
Iteration 181/1000 | Loss: 0.00002193
Iteration 182/1000 | Loss: 0.00002193
Iteration 183/1000 | Loss: 0.00002193
Iteration 184/1000 | Loss: 0.00002193
Iteration 185/1000 | Loss: 0.00002193
Iteration 186/1000 | Loss: 0.00002193
Iteration 187/1000 | Loss: 0.00002193
Iteration 188/1000 | Loss: 0.00002193
Iteration 189/1000 | Loss: 0.00002193
Iteration 190/1000 | Loss: 0.00002193
Iteration 191/1000 | Loss: 0.00002193
Iteration 192/1000 | Loss: 0.00002193
Iteration 193/1000 | Loss: 0.00002193
Iteration 194/1000 | Loss: 0.00002193
Iteration 195/1000 | Loss: 0.00002193
Iteration 196/1000 | Loss: 0.00002193
Iteration 197/1000 | Loss: 0.00002193
Iteration 198/1000 | Loss: 0.00002193
Iteration 199/1000 | Loss: 0.00002193
Iteration 200/1000 | Loss: 0.00002193
Iteration 201/1000 | Loss: 0.00002193
Iteration 202/1000 | Loss: 0.00002193
Iteration 203/1000 | Loss: 0.00002193
Iteration 204/1000 | Loss: 0.00002193
Iteration 205/1000 | Loss: 0.00002193
Iteration 206/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.1926412955508567e-05, 2.1926412955508567e-05, 2.1926412955508567e-05, 2.1926412955508567e-05, 2.1926412955508567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1926412955508567e-05

Optimization complete. Final v2v error: 4.020926475524902 mm

Highest mean error: 4.413931369781494 mm for frame 131

Lowest mean error: 3.873215913772583 mm for frame 119

Saving results

Total time: 39.001203536987305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399281
Iteration 2/25 | Loss: 0.00119075
Iteration 3/25 | Loss: 0.00111910
Iteration 4/25 | Loss: 0.00110886
Iteration 5/25 | Loss: 0.00110520
Iteration 6/25 | Loss: 0.00110499
Iteration 7/25 | Loss: 0.00110499
Iteration 8/25 | Loss: 0.00110499
Iteration 9/25 | Loss: 0.00110499
Iteration 10/25 | Loss: 0.00110499
Iteration 11/25 | Loss: 0.00110499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011049918830394745, 0.0011049918830394745, 0.0011049918830394745, 0.0011049918830394745, 0.0011049918830394745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011049918830394745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78630495
Iteration 2/25 | Loss: 0.00083081
Iteration 3/25 | Loss: 0.00083081
Iteration 4/25 | Loss: 0.00083081
Iteration 5/25 | Loss: 0.00083081
Iteration 6/25 | Loss: 0.00083081
Iteration 7/25 | Loss: 0.00083081
Iteration 8/25 | Loss: 0.00083081
Iteration 9/25 | Loss: 0.00083081
Iteration 10/25 | Loss: 0.00083081
Iteration 11/25 | Loss: 0.00083081
Iteration 12/25 | Loss: 0.00083081
Iteration 13/25 | Loss: 0.00083081
Iteration 14/25 | Loss: 0.00083081
Iteration 15/25 | Loss: 0.00083081
Iteration 16/25 | Loss: 0.00083081
Iteration 17/25 | Loss: 0.00083081
Iteration 18/25 | Loss: 0.00083081
Iteration 19/25 | Loss: 0.00083081
Iteration 20/25 | Loss: 0.00083081
Iteration 21/25 | Loss: 0.00083081
Iteration 22/25 | Loss: 0.00083081
Iteration 23/25 | Loss: 0.00083081
Iteration 24/25 | Loss: 0.00083081
Iteration 25/25 | Loss: 0.00083081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083081
Iteration 2/1000 | Loss: 0.00001629
Iteration 3/1000 | Loss: 0.00001201
Iteration 4/1000 | Loss: 0.00001093
Iteration 5/1000 | Loss: 0.00001036
Iteration 6/1000 | Loss: 0.00000999
Iteration 7/1000 | Loss: 0.00000979
Iteration 8/1000 | Loss: 0.00000954
Iteration 9/1000 | Loss: 0.00000946
Iteration 10/1000 | Loss: 0.00000943
Iteration 11/1000 | Loss: 0.00000943
Iteration 12/1000 | Loss: 0.00000941
Iteration 13/1000 | Loss: 0.00000935
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000929
Iteration 16/1000 | Loss: 0.00000929
Iteration 17/1000 | Loss: 0.00000926
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000923
Iteration 21/1000 | Loss: 0.00000923
Iteration 22/1000 | Loss: 0.00000923
Iteration 23/1000 | Loss: 0.00000923
Iteration 24/1000 | Loss: 0.00000922
Iteration 25/1000 | Loss: 0.00000921
Iteration 26/1000 | Loss: 0.00000921
Iteration 27/1000 | Loss: 0.00000920
Iteration 28/1000 | Loss: 0.00000918
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000909
Iteration 31/1000 | Loss: 0.00000906
Iteration 32/1000 | Loss: 0.00000905
Iteration 33/1000 | Loss: 0.00000905
Iteration 34/1000 | Loss: 0.00000904
Iteration 35/1000 | Loss: 0.00000903
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000902
Iteration 38/1000 | Loss: 0.00000901
Iteration 39/1000 | Loss: 0.00000901
Iteration 40/1000 | Loss: 0.00000900
Iteration 41/1000 | Loss: 0.00000900
Iteration 42/1000 | Loss: 0.00000900
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000899
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000898
Iteration 47/1000 | Loss: 0.00000898
Iteration 48/1000 | Loss: 0.00000895
Iteration 49/1000 | Loss: 0.00000895
Iteration 50/1000 | Loss: 0.00000895
Iteration 51/1000 | Loss: 0.00000895
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000893
Iteration 55/1000 | Loss: 0.00000893
Iteration 56/1000 | Loss: 0.00000892
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000891
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000891
Iteration 61/1000 | Loss: 0.00000890
Iteration 62/1000 | Loss: 0.00000889
Iteration 63/1000 | Loss: 0.00000888
Iteration 64/1000 | Loss: 0.00000887
Iteration 65/1000 | Loss: 0.00000886
Iteration 66/1000 | Loss: 0.00000885
Iteration 67/1000 | Loss: 0.00000884
Iteration 68/1000 | Loss: 0.00000883
Iteration 69/1000 | Loss: 0.00000883
Iteration 70/1000 | Loss: 0.00000883
Iteration 71/1000 | Loss: 0.00000882
Iteration 72/1000 | Loss: 0.00000882
Iteration 73/1000 | Loss: 0.00000881
Iteration 74/1000 | Loss: 0.00000880
Iteration 75/1000 | Loss: 0.00000880
Iteration 76/1000 | Loss: 0.00000880
Iteration 77/1000 | Loss: 0.00000880
Iteration 78/1000 | Loss: 0.00000880
Iteration 79/1000 | Loss: 0.00000880
Iteration 80/1000 | Loss: 0.00000880
Iteration 81/1000 | Loss: 0.00000880
Iteration 82/1000 | Loss: 0.00000880
Iteration 83/1000 | Loss: 0.00000880
Iteration 84/1000 | Loss: 0.00000880
Iteration 85/1000 | Loss: 0.00000879
Iteration 86/1000 | Loss: 0.00000879
Iteration 87/1000 | Loss: 0.00000879
Iteration 88/1000 | Loss: 0.00000879
Iteration 89/1000 | Loss: 0.00000878
Iteration 90/1000 | Loss: 0.00000878
Iteration 91/1000 | Loss: 0.00000878
Iteration 92/1000 | Loss: 0.00000878
Iteration 93/1000 | Loss: 0.00000878
Iteration 94/1000 | Loss: 0.00000878
Iteration 95/1000 | Loss: 0.00000878
Iteration 96/1000 | Loss: 0.00000878
Iteration 97/1000 | Loss: 0.00000878
Iteration 98/1000 | Loss: 0.00000878
Iteration 99/1000 | Loss: 0.00000877
Iteration 100/1000 | Loss: 0.00000877
Iteration 101/1000 | Loss: 0.00000877
Iteration 102/1000 | Loss: 0.00000877
Iteration 103/1000 | Loss: 0.00000877
Iteration 104/1000 | Loss: 0.00000877
Iteration 105/1000 | Loss: 0.00000876
Iteration 106/1000 | Loss: 0.00000876
Iteration 107/1000 | Loss: 0.00000876
Iteration 108/1000 | Loss: 0.00000876
Iteration 109/1000 | Loss: 0.00000876
Iteration 110/1000 | Loss: 0.00000876
Iteration 111/1000 | Loss: 0.00000875
Iteration 112/1000 | Loss: 0.00000875
Iteration 113/1000 | Loss: 0.00000875
Iteration 114/1000 | Loss: 0.00000875
Iteration 115/1000 | Loss: 0.00000874
Iteration 116/1000 | Loss: 0.00000874
Iteration 117/1000 | Loss: 0.00000874
Iteration 118/1000 | Loss: 0.00000874
Iteration 119/1000 | Loss: 0.00000874
Iteration 120/1000 | Loss: 0.00000874
Iteration 121/1000 | Loss: 0.00000874
Iteration 122/1000 | Loss: 0.00000873
Iteration 123/1000 | Loss: 0.00000873
Iteration 124/1000 | Loss: 0.00000873
Iteration 125/1000 | Loss: 0.00000873
Iteration 126/1000 | Loss: 0.00000873
Iteration 127/1000 | Loss: 0.00000872
Iteration 128/1000 | Loss: 0.00000872
Iteration 129/1000 | Loss: 0.00000872
Iteration 130/1000 | Loss: 0.00000872
Iteration 131/1000 | Loss: 0.00000872
Iteration 132/1000 | Loss: 0.00000872
Iteration 133/1000 | Loss: 0.00000872
Iteration 134/1000 | Loss: 0.00000871
Iteration 135/1000 | Loss: 0.00000871
Iteration 136/1000 | Loss: 0.00000871
Iteration 137/1000 | Loss: 0.00000870
Iteration 138/1000 | Loss: 0.00000870
Iteration 139/1000 | Loss: 0.00000869
Iteration 140/1000 | Loss: 0.00000869
Iteration 141/1000 | Loss: 0.00000869
Iteration 142/1000 | Loss: 0.00000869
Iteration 143/1000 | Loss: 0.00000869
Iteration 144/1000 | Loss: 0.00000868
Iteration 145/1000 | Loss: 0.00000868
Iteration 146/1000 | Loss: 0.00000868
Iteration 147/1000 | Loss: 0.00000868
Iteration 148/1000 | Loss: 0.00000868
Iteration 149/1000 | Loss: 0.00000868
Iteration 150/1000 | Loss: 0.00000867
Iteration 151/1000 | Loss: 0.00000867
Iteration 152/1000 | Loss: 0.00000867
Iteration 153/1000 | Loss: 0.00000867
Iteration 154/1000 | Loss: 0.00000866
Iteration 155/1000 | Loss: 0.00000866
Iteration 156/1000 | Loss: 0.00000866
Iteration 157/1000 | Loss: 0.00000865
Iteration 158/1000 | Loss: 0.00000865
Iteration 159/1000 | Loss: 0.00000865
Iteration 160/1000 | Loss: 0.00000865
Iteration 161/1000 | Loss: 0.00000865
Iteration 162/1000 | Loss: 0.00000865
Iteration 163/1000 | Loss: 0.00000865
Iteration 164/1000 | Loss: 0.00000865
Iteration 165/1000 | Loss: 0.00000865
Iteration 166/1000 | Loss: 0.00000865
Iteration 167/1000 | Loss: 0.00000865
Iteration 168/1000 | Loss: 0.00000865
Iteration 169/1000 | Loss: 0.00000865
Iteration 170/1000 | Loss: 0.00000865
Iteration 171/1000 | Loss: 0.00000865
Iteration 172/1000 | Loss: 0.00000865
Iteration 173/1000 | Loss: 0.00000865
Iteration 174/1000 | Loss: 0.00000865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [8.645261914352886e-06, 8.645261914352886e-06, 8.645261914352886e-06, 8.645261914352886e-06, 8.645261914352886e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.645261914352886e-06

Optimization complete. Final v2v error: 2.5327224731445312 mm

Highest mean error: 2.9275617599487305 mm for frame 86

Lowest mean error: 2.431638717651367 mm for frame 208

Saving results

Total time: 40.00997495651245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867123
Iteration 2/25 | Loss: 0.00168147
Iteration 3/25 | Loss: 0.00134797
Iteration 4/25 | Loss: 0.00134964
Iteration 5/25 | Loss: 0.00129248
Iteration 6/25 | Loss: 0.00125424
Iteration 7/25 | Loss: 0.00124514
Iteration 8/25 | Loss: 0.00123614
Iteration 9/25 | Loss: 0.00123394
Iteration 10/25 | Loss: 0.00123346
Iteration 11/25 | Loss: 0.00123325
Iteration 12/25 | Loss: 0.00123305
Iteration 13/25 | Loss: 0.00123288
Iteration 14/25 | Loss: 0.00123275
Iteration 15/25 | Loss: 0.00123274
Iteration 16/25 | Loss: 0.00123274
Iteration 17/25 | Loss: 0.00123274
Iteration 18/25 | Loss: 0.00123273
Iteration 19/25 | Loss: 0.00123273
Iteration 20/25 | Loss: 0.00123273
Iteration 21/25 | Loss: 0.00123273
Iteration 22/25 | Loss: 0.00123273
Iteration 23/25 | Loss: 0.00123273
Iteration 24/25 | Loss: 0.00123273
Iteration 25/25 | Loss: 0.00123273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53840828
Iteration 2/25 | Loss: 0.00076418
Iteration 3/25 | Loss: 0.00076412
Iteration 4/25 | Loss: 0.00076412
Iteration 5/25 | Loss: 0.00076412
Iteration 6/25 | Loss: 0.00076412
Iteration 7/25 | Loss: 0.00076412
Iteration 8/25 | Loss: 0.00076412
Iteration 9/25 | Loss: 0.00076412
Iteration 10/25 | Loss: 0.00076412
Iteration 11/25 | Loss: 0.00076411
Iteration 12/25 | Loss: 0.00076411
Iteration 13/25 | Loss: 0.00076411
Iteration 14/25 | Loss: 0.00076411
Iteration 15/25 | Loss: 0.00076411
Iteration 16/25 | Loss: 0.00076411
Iteration 17/25 | Loss: 0.00076411
Iteration 18/25 | Loss: 0.00076411
Iteration 19/25 | Loss: 0.00076411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007641148404218256, 0.0007641148404218256, 0.0007641148404218256, 0.0007641148404218256, 0.0007641148404218256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007641148404218256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076411
Iteration 2/1000 | Loss: 0.00004486
Iteration 3/1000 | Loss: 0.00003375
Iteration 4/1000 | Loss: 0.00003062
Iteration 5/1000 | Loss: 0.00002925
Iteration 6/1000 | Loss: 0.00002808
Iteration 7/1000 | Loss: 0.00002722
Iteration 8/1000 | Loss: 0.00002664
Iteration 9/1000 | Loss: 0.00018790
Iteration 10/1000 | Loss: 0.00003028
Iteration 11/1000 | Loss: 0.00002650
Iteration 12/1000 | Loss: 0.00002544
Iteration 13/1000 | Loss: 0.00002479
Iteration 14/1000 | Loss: 0.00002426
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00002379
Iteration 18/1000 | Loss: 0.00002376
Iteration 19/1000 | Loss: 0.00002375
Iteration 20/1000 | Loss: 0.00002370
Iteration 21/1000 | Loss: 0.00002362
Iteration 22/1000 | Loss: 0.00002356
Iteration 23/1000 | Loss: 0.00002355
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002351
Iteration 26/1000 | Loss: 0.00002351
Iteration 27/1000 | Loss: 0.00002351
Iteration 28/1000 | Loss: 0.00002350
Iteration 29/1000 | Loss: 0.00002350
Iteration 30/1000 | Loss: 0.00002350
Iteration 31/1000 | Loss: 0.00002347
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00002347
Iteration 35/1000 | Loss: 0.00002347
Iteration 36/1000 | Loss: 0.00002346
Iteration 37/1000 | Loss: 0.00002346
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002346
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002345
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002344
Iteration 49/1000 | Loss: 0.00002344
Iteration 50/1000 | Loss: 0.00002344
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002343
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002343
Iteration 56/1000 | Loss: 0.00002343
Iteration 57/1000 | Loss: 0.00002343
Iteration 58/1000 | Loss: 0.00002343
Iteration 59/1000 | Loss: 0.00002343
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002343
Iteration 62/1000 | Loss: 0.00002343
Iteration 63/1000 | Loss: 0.00002342
Iteration 64/1000 | Loss: 0.00002342
Iteration 65/1000 | Loss: 0.00002342
Iteration 66/1000 | Loss: 0.00002341
Iteration 67/1000 | Loss: 0.00002341
Iteration 68/1000 | Loss: 0.00002341
Iteration 69/1000 | Loss: 0.00002341
Iteration 70/1000 | Loss: 0.00002340
Iteration 71/1000 | Loss: 0.00002340
Iteration 72/1000 | Loss: 0.00002340
Iteration 73/1000 | Loss: 0.00002339
Iteration 74/1000 | Loss: 0.00002339
Iteration 75/1000 | Loss: 0.00002339
Iteration 76/1000 | Loss: 0.00002339
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002338
Iteration 79/1000 | Loss: 0.00002337
Iteration 80/1000 | Loss: 0.00002337
Iteration 81/1000 | Loss: 0.00002337
Iteration 82/1000 | Loss: 0.00002337
Iteration 83/1000 | Loss: 0.00002337
Iteration 84/1000 | Loss: 0.00002337
Iteration 85/1000 | Loss: 0.00002337
Iteration 86/1000 | Loss: 0.00002337
Iteration 87/1000 | Loss: 0.00002337
Iteration 88/1000 | Loss: 0.00002336
Iteration 89/1000 | Loss: 0.00002336
Iteration 90/1000 | Loss: 0.00002336
Iteration 91/1000 | Loss: 0.00002336
Iteration 92/1000 | Loss: 0.00002336
Iteration 93/1000 | Loss: 0.00002336
Iteration 94/1000 | Loss: 0.00002336
Iteration 95/1000 | Loss: 0.00002336
Iteration 96/1000 | Loss: 0.00002336
Iteration 97/1000 | Loss: 0.00002335
Iteration 98/1000 | Loss: 0.00002334
Iteration 99/1000 | Loss: 0.00002334
Iteration 100/1000 | Loss: 0.00002334
Iteration 101/1000 | Loss: 0.00002334
Iteration 102/1000 | Loss: 0.00002334
Iteration 103/1000 | Loss: 0.00002333
Iteration 104/1000 | Loss: 0.00002333
Iteration 105/1000 | Loss: 0.00002333
Iteration 106/1000 | Loss: 0.00002333
Iteration 107/1000 | Loss: 0.00002333
Iteration 108/1000 | Loss: 0.00002333
Iteration 109/1000 | Loss: 0.00002333
Iteration 110/1000 | Loss: 0.00002333
Iteration 111/1000 | Loss: 0.00002333
Iteration 112/1000 | Loss: 0.00002332
Iteration 113/1000 | Loss: 0.00002332
Iteration 114/1000 | Loss: 0.00002332
Iteration 115/1000 | Loss: 0.00002332
Iteration 116/1000 | Loss: 0.00002332
Iteration 117/1000 | Loss: 0.00002332
Iteration 118/1000 | Loss: 0.00002332
Iteration 119/1000 | Loss: 0.00002332
Iteration 120/1000 | Loss: 0.00002332
Iteration 121/1000 | Loss: 0.00002332
Iteration 122/1000 | Loss: 0.00002332
Iteration 123/1000 | Loss: 0.00002332
Iteration 124/1000 | Loss: 0.00002332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.3322181732510217e-05, 2.3322181732510217e-05, 2.3322181732510217e-05, 2.3322181732510217e-05, 2.3322181732510217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3322181732510217e-05

Optimization complete. Final v2v error: 4.040256977081299 mm

Highest mean error: 5.312532424926758 mm for frame 53

Lowest mean error: 3.315117359161377 mm for frame 150

Saving results

Total time: 65.28586173057556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418167
Iteration 2/25 | Loss: 0.00119988
Iteration 3/25 | Loss: 0.00113584
Iteration 4/25 | Loss: 0.00112505
Iteration 5/25 | Loss: 0.00112114
Iteration 6/25 | Loss: 0.00112021
Iteration 7/25 | Loss: 0.00111986
Iteration 8/25 | Loss: 0.00111979
Iteration 9/25 | Loss: 0.00111979
Iteration 10/25 | Loss: 0.00111979
Iteration 11/25 | Loss: 0.00111979
Iteration 12/25 | Loss: 0.00111979
Iteration 13/25 | Loss: 0.00111979
Iteration 14/25 | Loss: 0.00111979
Iteration 15/25 | Loss: 0.00111979
Iteration 16/25 | Loss: 0.00111979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011197928106412292, 0.0011197928106412292, 0.0011197928106412292, 0.0011197928106412292, 0.0011197928106412292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011197928106412292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71019959
Iteration 2/25 | Loss: 0.00087220
Iteration 3/25 | Loss: 0.00087219
Iteration 4/25 | Loss: 0.00087219
Iteration 5/25 | Loss: 0.00087219
Iteration 6/25 | Loss: 0.00087219
Iteration 7/25 | Loss: 0.00087219
Iteration 8/25 | Loss: 0.00087219
Iteration 9/25 | Loss: 0.00087219
Iteration 10/25 | Loss: 0.00087219
Iteration 11/25 | Loss: 0.00087219
Iteration 12/25 | Loss: 0.00087219
Iteration 13/25 | Loss: 0.00087219
Iteration 14/25 | Loss: 0.00087219
Iteration 15/25 | Loss: 0.00087219
Iteration 16/25 | Loss: 0.00087219
Iteration 17/25 | Loss: 0.00087219
Iteration 18/25 | Loss: 0.00087219
Iteration 19/25 | Loss: 0.00087219
Iteration 20/25 | Loss: 0.00087219
Iteration 21/25 | Loss: 0.00087219
Iteration 22/25 | Loss: 0.00087219
Iteration 23/25 | Loss: 0.00087219
Iteration 24/25 | Loss: 0.00087219
Iteration 25/25 | Loss: 0.00087219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087219
Iteration 2/1000 | Loss: 0.00002959
Iteration 3/1000 | Loss: 0.00001974
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001411
Iteration 6/1000 | Loss: 0.00001330
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001192
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001159
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001153
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001135
Iteration 21/1000 | Loss: 0.00001134
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001133
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001132
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001129
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001117
Iteration 81/1000 | Loss: 0.00001117
Iteration 82/1000 | Loss: 0.00001117
Iteration 83/1000 | Loss: 0.00001117
Iteration 84/1000 | Loss: 0.00001117
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001116
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001116
Iteration 95/1000 | Loss: 0.00001115
Iteration 96/1000 | Loss: 0.00001115
Iteration 97/1000 | Loss: 0.00001115
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001114
Iteration 100/1000 | Loss: 0.00001114
Iteration 101/1000 | Loss: 0.00001114
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001112
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001106
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001106
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001104
Iteration 142/1000 | Loss: 0.00001104
Iteration 143/1000 | Loss: 0.00001104
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001103
Iteration 147/1000 | Loss: 0.00001103
Iteration 148/1000 | Loss: 0.00001103
Iteration 149/1000 | Loss: 0.00001103
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001099
Iteration 176/1000 | Loss: 0.00001099
Iteration 177/1000 | Loss: 0.00001099
Iteration 178/1000 | Loss: 0.00001099
Iteration 179/1000 | Loss: 0.00001099
Iteration 180/1000 | Loss: 0.00001099
Iteration 181/1000 | Loss: 0.00001099
Iteration 182/1000 | Loss: 0.00001099
Iteration 183/1000 | Loss: 0.00001099
Iteration 184/1000 | Loss: 0.00001099
Iteration 185/1000 | Loss: 0.00001098
Iteration 186/1000 | Loss: 0.00001098
Iteration 187/1000 | Loss: 0.00001098
Iteration 188/1000 | Loss: 0.00001097
Iteration 189/1000 | Loss: 0.00001097
Iteration 190/1000 | Loss: 0.00001097
Iteration 191/1000 | Loss: 0.00001097
Iteration 192/1000 | Loss: 0.00001097
Iteration 193/1000 | Loss: 0.00001097
Iteration 194/1000 | Loss: 0.00001097
Iteration 195/1000 | Loss: 0.00001097
Iteration 196/1000 | Loss: 0.00001097
Iteration 197/1000 | Loss: 0.00001097
Iteration 198/1000 | Loss: 0.00001096
Iteration 199/1000 | Loss: 0.00001096
Iteration 200/1000 | Loss: 0.00001096
Iteration 201/1000 | Loss: 0.00001096
Iteration 202/1000 | Loss: 0.00001096
Iteration 203/1000 | Loss: 0.00001096
Iteration 204/1000 | Loss: 0.00001096
Iteration 205/1000 | Loss: 0.00001096
Iteration 206/1000 | Loss: 0.00001096
Iteration 207/1000 | Loss: 0.00001096
Iteration 208/1000 | Loss: 0.00001096
Iteration 209/1000 | Loss: 0.00001096
Iteration 210/1000 | Loss: 0.00001096
Iteration 211/1000 | Loss: 0.00001096
Iteration 212/1000 | Loss: 0.00001095
Iteration 213/1000 | Loss: 0.00001095
Iteration 214/1000 | Loss: 0.00001095
Iteration 215/1000 | Loss: 0.00001095
Iteration 216/1000 | Loss: 0.00001095
Iteration 217/1000 | Loss: 0.00001095
Iteration 218/1000 | Loss: 0.00001095
Iteration 219/1000 | Loss: 0.00001095
Iteration 220/1000 | Loss: 0.00001095
Iteration 221/1000 | Loss: 0.00001095
Iteration 222/1000 | Loss: 0.00001095
Iteration 223/1000 | Loss: 0.00001094
Iteration 224/1000 | Loss: 0.00001094
Iteration 225/1000 | Loss: 0.00001094
Iteration 226/1000 | Loss: 0.00001094
Iteration 227/1000 | Loss: 0.00001094
Iteration 228/1000 | Loss: 0.00001094
Iteration 229/1000 | Loss: 0.00001094
Iteration 230/1000 | Loss: 0.00001094
Iteration 231/1000 | Loss: 0.00001094
Iteration 232/1000 | Loss: 0.00001094
Iteration 233/1000 | Loss: 0.00001093
Iteration 234/1000 | Loss: 0.00001093
Iteration 235/1000 | Loss: 0.00001093
Iteration 236/1000 | Loss: 0.00001093
Iteration 237/1000 | Loss: 0.00001093
Iteration 238/1000 | Loss: 0.00001093
Iteration 239/1000 | Loss: 0.00001093
Iteration 240/1000 | Loss: 0.00001093
Iteration 241/1000 | Loss: 0.00001093
Iteration 242/1000 | Loss: 0.00001093
Iteration 243/1000 | Loss: 0.00001093
Iteration 244/1000 | Loss: 0.00001093
Iteration 245/1000 | Loss: 0.00001093
Iteration 246/1000 | Loss: 0.00001093
Iteration 247/1000 | Loss: 0.00001093
Iteration 248/1000 | Loss: 0.00001093
Iteration 249/1000 | Loss: 0.00001093
Iteration 250/1000 | Loss: 0.00001093
Iteration 251/1000 | Loss: 0.00001093
Iteration 252/1000 | Loss: 0.00001093
Iteration 253/1000 | Loss: 0.00001093
Iteration 254/1000 | Loss: 0.00001093
Iteration 255/1000 | Loss: 0.00001093
Iteration 256/1000 | Loss: 0.00001093
Iteration 257/1000 | Loss: 0.00001093
Iteration 258/1000 | Loss: 0.00001093
Iteration 259/1000 | Loss: 0.00001093
Iteration 260/1000 | Loss: 0.00001093
Iteration 261/1000 | Loss: 0.00001093
Iteration 262/1000 | Loss: 0.00001093
Iteration 263/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.0931109500234015e-05, 1.0931109500234015e-05, 1.0931109500234015e-05, 1.0931109500234015e-05, 1.0931109500234015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0931109500234015e-05

Optimization complete. Final v2v error: 2.820380926132202 mm

Highest mean error: 3.752363681793213 mm for frame 75

Lowest mean error: 2.5168628692626953 mm for frame 97

Saving results

Total time: 47.28378129005432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453811
Iteration 2/25 | Loss: 0.00138926
Iteration 3/25 | Loss: 0.00122256
Iteration 4/25 | Loss: 0.00120805
Iteration 5/25 | Loss: 0.00120581
Iteration 6/25 | Loss: 0.00120554
Iteration 7/25 | Loss: 0.00120554
Iteration 8/25 | Loss: 0.00120554
Iteration 9/25 | Loss: 0.00120554
Iteration 10/25 | Loss: 0.00120554
Iteration 11/25 | Loss: 0.00120554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012055439874529839, 0.0012055439874529839, 0.0012055439874529839, 0.0012055439874529839, 0.0012055439874529839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012055439874529839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11377954
Iteration 2/25 | Loss: 0.00062014
Iteration 3/25 | Loss: 0.00062012
Iteration 4/25 | Loss: 0.00062012
Iteration 5/25 | Loss: 0.00062012
Iteration 6/25 | Loss: 0.00062012
Iteration 7/25 | Loss: 0.00062012
Iteration 8/25 | Loss: 0.00062012
Iteration 9/25 | Loss: 0.00062012
Iteration 10/25 | Loss: 0.00062012
Iteration 11/25 | Loss: 0.00062012
Iteration 12/25 | Loss: 0.00062012
Iteration 13/25 | Loss: 0.00062012
Iteration 14/25 | Loss: 0.00062012
Iteration 15/25 | Loss: 0.00062012
Iteration 16/25 | Loss: 0.00062012
Iteration 17/25 | Loss: 0.00062012
Iteration 18/25 | Loss: 0.00062012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006201154319569468, 0.0006201154319569468, 0.0006201154319569468, 0.0006201154319569468, 0.0006201154319569468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006201154319569468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062012
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00002307
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001797
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001721
Iteration 17/1000 | Loss: 0.00001720
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001719
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001683
Iteration 33/1000 | Loss: 0.00001683
Iteration 34/1000 | Loss: 0.00001683
Iteration 35/1000 | Loss: 0.00001683
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001682
Iteration 38/1000 | Loss: 0.00001682
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001681
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001678
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00001678
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001677
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001670
Iteration 71/1000 | Loss: 0.00001670
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001668
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001667
Iteration 76/1000 | Loss: 0.00001667
Iteration 77/1000 | Loss: 0.00001667
Iteration 78/1000 | Loss: 0.00001667
Iteration 79/1000 | Loss: 0.00001667
Iteration 80/1000 | Loss: 0.00001667
Iteration 81/1000 | Loss: 0.00001667
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001666
Iteration 84/1000 | Loss: 0.00001666
Iteration 85/1000 | Loss: 0.00001666
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001663
Iteration 92/1000 | Loss: 0.00001663
Iteration 93/1000 | Loss: 0.00001663
Iteration 94/1000 | Loss: 0.00001662
Iteration 95/1000 | Loss: 0.00001662
Iteration 96/1000 | Loss: 0.00001661
Iteration 97/1000 | Loss: 0.00001661
Iteration 98/1000 | Loss: 0.00001661
Iteration 99/1000 | Loss: 0.00001661
Iteration 100/1000 | Loss: 0.00001661
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001660
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001659
Iteration 115/1000 | Loss: 0.00001659
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001658
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001658
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.6573098037042655e-05, 1.6573098037042655e-05, 1.6573098037042655e-05, 1.6573098037042655e-05, 1.6573098037042655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6573098037042655e-05

Optimization complete. Final v2v error: 3.43011212348938 mm

Highest mean error: 4.008269309997559 mm for frame 62

Lowest mean error: 3.0284202098846436 mm for frame 1

Saving results

Total time: 37.35300803184509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745828
Iteration 2/25 | Loss: 0.00135167
Iteration 3/25 | Loss: 0.00125386
Iteration 4/25 | Loss: 0.00124555
Iteration 5/25 | Loss: 0.00124299
Iteration 6/25 | Loss: 0.00124279
Iteration 7/25 | Loss: 0.00124279
Iteration 8/25 | Loss: 0.00124279
Iteration 9/25 | Loss: 0.00124279
Iteration 10/25 | Loss: 0.00124279
Iteration 11/25 | Loss: 0.00124279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001242788159288466, 0.001242788159288466, 0.001242788159288466, 0.001242788159288466, 0.001242788159288466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242788159288466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42472208
Iteration 2/25 | Loss: 0.00087111
Iteration 3/25 | Loss: 0.00087111
Iteration 4/25 | Loss: 0.00087111
Iteration 5/25 | Loss: 0.00087111
Iteration 6/25 | Loss: 0.00087111
Iteration 7/25 | Loss: 0.00087111
Iteration 8/25 | Loss: 0.00087111
Iteration 9/25 | Loss: 0.00087111
Iteration 10/25 | Loss: 0.00087111
Iteration 11/25 | Loss: 0.00087111
Iteration 12/25 | Loss: 0.00087111
Iteration 13/25 | Loss: 0.00087111
Iteration 14/25 | Loss: 0.00087111
Iteration 15/25 | Loss: 0.00087111
Iteration 16/25 | Loss: 0.00087111
Iteration 17/25 | Loss: 0.00087111
Iteration 18/25 | Loss: 0.00087111
Iteration 19/25 | Loss: 0.00087111
Iteration 20/25 | Loss: 0.00087111
Iteration 21/25 | Loss: 0.00087111
Iteration 22/25 | Loss: 0.00087111
Iteration 23/25 | Loss: 0.00087111
Iteration 24/25 | Loss: 0.00087111
Iteration 25/25 | Loss: 0.00087111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008711059344932437, 0.0008711059344932437, 0.0008711059344932437, 0.0008711059344932437, 0.0008711059344932437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008711059344932437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087111
Iteration 2/1000 | Loss: 0.00003723
Iteration 3/1000 | Loss: 0.00002397
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001879
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001764
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001743
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001721
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001714
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001713
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001707
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001707
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001707
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001702
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001700
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001699
Iteration 80/1000 | Loss: 0.00001699
Iteration 81/1000 | Loss: 0.00001699
Iteration 82/1000 | Loss: 0.00001699
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001695
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001688
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001685
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001681
Iteration 178/1000 | Loss: 0.00001681
Iteration 179/1000 | Loss: 0.00001680
Iteration 180/1000 | Loss: 0.00001680
Iteration 181/1000 | Loss: 0.00001680
Iteration 182/1000 | Loss: 0.00001680
Iteration 183/1000 | Loss: 0.00001680
Iteration 184/1000 | Loss: 0.00001680
Iteration 185/1000 | Loss: 0.00001680
Iteration 186/1000 | Loss: 0.00001680
Iteration 187/1000 | Loss: 0.00001680
Iteration 188/1000 | Loss: 0.00001680
Iteration 189/1000 | Loss: 0.00001679
Iteration 190/1000 | Loss: 0.00001679
Iteration 191/1000 | Loss: 0.00001679
Iteration 192/1000 | Loss: 0.00001679
Iteration 193/1000 | Loss: 0.00001679
Iteration 194/1000 | Loss: 0.00001679
Iteration 195/1000 | Loss: 0.00001678
Iteration 196/1000 | Loss: 0.00001678
Iteration 197/1000 | Loss: 0.00001678
Iteration 198/1000 | Loss: 0.00001678
Iteration 199/1000 | Loss: 0.00001678
Iteration 200/1000 | Loss: 0.00001678
Iteration 201/1000 | Loss: 0.00001678
Iteration 202/1000 | Loss: 0.00001678
Iteration 203/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.67753933055792e-05, 1.67753933055792e-05, 1.67753933055792e-05, 1.67753933055792e-05, 1.67753933055792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.67753933055792e-05

Optimization complete. Final v2v error: 3.4150257110595703 mm

Highest mean error: 3.6619369983673096 mm for frame 143

Lowest mean error: 3.1138875484466553 mm for frame 160

Saving results

Total time: 41.076183795928955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788097
Iteration 2/25 | Loss: 0.00119636
Iteration 3/25 | Loss: 0.00111765
Iteration 4/25 | Loss: 0.00111037
Iteration 5/25 | Loss: 0.00110832
Iteration 6/25 | Loss: 0.00110832
Iteration 7/25 | Loss: 0.00110832
Iteration 8/25 | Loss: 0.00110832
Iteration 9/25 | Loss: 0.00110832
Iteration 10/25 | Loss: 0.00110832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011083182180300355, 0.0011083182180300355, 0.0011083182180300355, 0.0011083182180300355, 0.0011083182180300355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011083182180300355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36841249
Iteration 2/25 | Loss: 0.00077775
Iteration 3/25 | Loss: 0.00077775
Iteration 4/25 | Loss: 0.00077775
Iteration 5/25 | Loss: 0.00077775
Iteration 6/25 | Loss: 0.00077775
Iteration 7/25 | Loss: 0.00077775
Iteration 8/25 | Loss: 0.00077775
Iteration 9/25 | Loss: 0.00077775
Iteration 10/25 | Loss: 0.00077775
Iteration 11/25 | Loss: 0.00077775
Iteration 12/25 | Loss: 0.00077775
Iteration 13/25 | Loss: 0.00077775
Iteration 14/25 | Loss: 0.00077775
Iteration 15/25 | Loss: 0.00077775
Iteration 16/25 | Loss: 0.00077775
Iteration 17/25 | Loss: 0.00077775
Iteration 18/25 | Loss: 0.00077775
Iteration 19/25 | Loss: 0.00077775
Iteration 20/25 | Loss: 0.00077775
Iteration 21/25 | Loss: 0.00077775
Iteration 22/25 | Loss: 0.00077775
Iteration 23/25 | Loss: 0.00077775
Iteration 24/25 | Loss: 0.00077775
Iteration 25/25 | Loss: 0.00077775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077775
Iteration 2/1000 | Loss: 0.00002512
Iteration 3/1000 | Loss: 0.00001664
Iteration 4/1000 | Loss: 0.00001393
Iteration 5/1000 | Loss: 0.00001237
Iteration 6/1000 | Loss: 0.00001158
Iteration 7/1000 | Loss: 0.00001109
Iteration 8/1000 | Loss: 0.00001068
Iteration 9/1000 | Loss: 0.00001046
Iteration 10/1000 | Loss: 0.00001045
Iteration 11/1000 | Loss: 0.00001044
Iteration 12/1000 | Loss: 0.00001018
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001005
Iteration 15/1000 | Loss: 0.00001004
Iteration 16/1000 | Loss: 0.00001004
Iteration 17/1000 | Loss: 0.00001001
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00001000
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000992
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000988
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000985
Iteration 30/1000 | Loss: 0.00000984
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000982
Iteration 33/1000 | Loss: 0.00000981
Iteration 34/1000 | Loss: 0.00000980
Iteration 35/1000 | Loss: 0.00000980
Iteration 36/1000 | Loss: 0.00000979
Iteration 37/1000 | Loss: 0.00000979
Iteration 38/1000 | Loss: 0.00000978
Iteration 39/1000 | Loss: 0.00000977
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000976
Iteration 42/1000 | Loss: 0.00000976
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000975
Iteration 45/1000 | Loss: 0.00000975
Iteration 46/1000 | Loss: 0.00000974
Iteration 47/1000 | Loss: 0.00000974
Iteration 48/1000 | Loss: 0.00000974
Iteration 49/1000 | Loss: 0.00000974
Iteration 50/1000 | Loss: 0.00000974
Iteration 51/1000 | Loss: 0.00000973
Iteration 52/1000 | Loss: 0.00000973
Iteration 53/1000 | Loss: 0.00000973
Iteration 54/1000 | Loss: 0.00000972
Iteration 55/1000 | Loss: 0.00000972
Iteration 56/1000 | Loss: 0.00000971
Iteration 57/1000 | Loss: 0.00000970
Iteration 58/1000 | Loss: 0.00000970
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000970
Iteration 61/1000 | Loss: 0.00000970
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000967
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000966
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000965
Iteration 75/1000 | Loss: 0.00000965
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000964
Iteration 78/1000 | Loss: 0.00000964
Iteration 79/1000 | Loss: 0.00000964
Iteration 80/1000 | Loss: 0.00000963
Iteration 81/1000 | Loss: 0.00000963
Iteration 82/1000 | Loss: 0.00000962
Iteration 83/1000 | Loss: 0.00000962
Iteration 84/1000 | Loss: 0.00000962
Iteration 85/1000 | Loss: 0.00000961
Iteration 86/1000 | Loss: 0.00000960
Iteration 87/1000 | Loss: 0.00000960
Iteration 88/1000 | Loss: 0.00000959
Iteration 89/1000 | Loss: 0.00000959
Iteration 90/1000 | Loss: 0.00000959
Iteration 91/1000 | Loss: 0.00000958
Iteration 92/1000 | Loss: 0.00000958
Iteration 93/1000 | Loss: 0.00000958
Iteration 94/1000 | Loss: 0.00000958
Iteration 95/1000 | Loss: 0.00000958
Iteration 96/1000 | Loss: 0.00000958
Iteration 97/1000 | Loss: 0.00000957
Iteration 98/1000 | Loss: 0.00000957
Iteration 99/1000 | Loss: 0.00000957
Iteration 100/1000 | Loss: 0.00000956
Iteration 101/1000 | Loss: 0.00000956
Iteration 102/1000 | Loss: 0.00000955
Iteration 103/1000 | Loss: 0.00000955
Iteration 104/1000 | Loss: 0.00000954
Iteration 105/1000 | Loss: 0.00000954
Iteration 106/1000 | Loss: 0.00000954
Iteration 107/1000 | Loss: 0.00000953
Iteration 108/1000 | Loss: 0.00000953
Iteration 109/1000 | Loss: 0.00000953
Iteration 110/1000 | Loss: 0.00000953
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000952
Iteration 113/1000 | Loss: 0.00000951
Iteration 114/1000 | Loss: 0.00000951
Iteration 115/1000 | Loss: 0.00000951
Iteration 116/1000 | Loss: 0.00000951
Iteration 117/1000 | Loss: 0.00000951
Iteration 118/1000 | Loss: 0.00000950
Iteration 119/1000 | Loss: 0.00000950
Iteration 120/1000 | Loss: 0.00000950
Iteration 121/1000 | Loss: 0.00000950
Iteration 122/1000 | Loss: 0.00000949
Iteration 123/1000 | Loss: 0.00000949
Iteration 124/1000 | Loss: 0.00000949
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000948
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000946
Iteration 133/1000 | Loss: 0.00000946
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000944
Iteration 139/1000 | Loss: 0.00000944
Iteration 140/1000 | Loss: 0.00000944
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000943
Iteration 146/1000 | Loss: 0.00000943
Iteration 147/1000 | Loss: 0.00000943
Iteration 148/1000 | Loss: 0.00000943
Iteration 149/1000 | Loss: 0.00000942
Iteration 150/1000 | Loss: 0.00000942
Iteration 151/1000 | Loss: 0.00000942
Iteration 152/1000 | Loss: 0.00000942
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000941
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000940
Iteration 158/1000 | Loss: 0.00000940
Iteration 159/1000 | Loss: 0.00000940
Iteration 160/1000 | Loss: 0.00000940
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000940
Iteration 163/1000 | Loss: 0.00000940
Iteration 164/1000 | Loss: 0.00000940
Iteration 165/1000 | Loss: 0.00000939
Iteration 166/1000 | Loss: 0.00000939
Iteration 167/1000 | Loss: 0.00000939
Iteration 168/1000 | Loss: 0.00000939
Iteration 169/1000 | Loss: 0.00000939
Iteration 170/1000 | Loss: 0.00000939
Iteration 171/1000 | Loss: 0.00000939
Iteration 172/1000 | Loss: 0.00000938
Iteration 173/1000 | Loss: 0.00000938
Iteration 174/1000 | Loss: 0.00000938
Iteration 175/1000 | Loss: 0.00000938
Iteration 176/1000 | Loss: 0.00000938
Iteration 177/1000 | Loss: 0.00000937
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000937
Iteration 181/1000 | Loss: 0.00000937
Iteration 182/1000 | Loss: 0.00000937
Iteration 183/1000 | Loss: 0.00000937
Iteration 184/1000 | Loss: 0.00000937
Iteration 185/1000 | Loss: 0.00000937
Iteration 186/1000 | Loss: 0.00000937
Iteration 187/1000 | Loss: 0.00000937
Iteration 188/1000 | Loss: 0.00000937
Iteration 189/1000 | Loss: 0.00000937
Iteration 190/1000 | Loss: 0.00000937
Iteration 191/1000 | Loss: 0.00000936
Iteration 192/1000 | Loss: 0.00000936
Iteration 193/1000 | Loss: 0.00000936
Iteration 194/1000 | Loss: 0.00000936
Iteration 195/1000 | Loss: 0.00000936
Iteration 196/1000 | Loss: 0.00000936
Iteration 197/1000 | Loss: 0.00000936
Iteration 198/1000 | Loss: 0.00000935
Iteration 199/1000 | Loss: 0.00000935
Iteration 200/1000 | Loss: 0.00000935
Iteration 201/1000 | Loss: 0.00000935
Iteration 202/1000 | Loss: 0.00000935
Iteration 203/1000 | Loss: 0.00000935
Iteration 204/1000 | Loss: 0.00000935
Iteration 205/1000 | Loss: 0.00000935
Iteration 206/1000 | Loss: 0.00000935
Iteration 207/1000 | Loss: 0.00000935
Iteration 208/1000 | Loss: 0.00000935
Iteration 209/1000 | Loss: 0.00000935
Iteration 210/1000 | Loss: 0.00000935
Iteration 211/1000 | Loss: 0.00000935
Iteration 212/1000 | Loss: 0.00000934
Iteration 213/1000 | Loss: 0.00000934
Iteration 214/1000 | Loss: 0.00000934
Iteration 215/1000 | Loss: 0.00000934
Iteration 216/1000 | Loss: 0.00000934
Iteration 217/1000 | Loss: 0.00000934
Iteration 218/1000 | Loss: 0.00000934
Iteration 219/1000 | Loss: 0.00000934
Iteration 220/1000 | Loss: 0.00000934
Iteration 221/1000 | Loss: 0.00000934
Iteration 222/1000 | Loss: 0.00000934
Iteration 223/1000 | Loss: 0.00000934
Iteration 224/1000 | Loss: 0.00000934
Iteration 225/1000 | Loss: 0.00000934
Iteration 226/1000 | Loss: 0.00000934
Iteration 227/1000 | Loss: 0.00000934
Iteration 228/1000 | Loss: 0.00000934
Iteration 229/1000 | Loss: 0.00000934
Iteration 230/1000 | Loss: 0.00000934
Iteration 231/1000 | Loss: 0.00000933
Iteration 232/1000 | Loss: 0.00000933
Iteration 233/1000 | Loss: 0.00000933
Iteration 234/1000 | Loss: 0.00000933
Iteration 235/1000 | Loss: 0.00000933
Iteration 236/1000 | Loss: 0.00000933
Iteration 237/1000 | Loss: 0.00000933
Iteration 238/1000 | Loss: 0.00000933
Iteration 239/1000 | Loss: 0.00000933
Iteration 240/1000 | Loss: 0.00000933
Iteration 241/1000 | Loss: 0.00000933
Iteration 242/1000 | Loss: 0.00000933
Iteration 243/1000 | Loss: 0.00000933
Iteration 244/1000 | Loss: 0.00000933
Iteration 245/1000 | Loss: 0.00000933
Iteration 246/1000 | Loss: 0.00000933
Iteration 247/1000 | Loss: 0.00000933
Iteration 248/1000 | Loss: 0.00000933
Iteration 249/1000 | Loss: 0.00000933
Iteration 250/1000 | Loss: 0.00000932
Iteration 251/1000 | Loss: 0.00000932
Iteration 252/1000 | Loss: 0.00000932
Iteration 253/1000 | Loss: 0.00000932
Iteration 254/1000 | Loss: 0.00000932
Iteration 255/1000 | Loss: 0.00000932
Iteration 256/1000 | Loss: 0.00000932
Iteration 257/1000 | Loss: 0.00000932
Iteration 258/1000 | Loss: 0.00000932
Iteration 259/1000 | Loss: 0.00000932
Iteration 260/1000 | Loss: 0.00000932
Iteration 261/1000 | Loss: 0.00000932
Iteration 262/1000 | Loss: 0.00000931
Iteration 263/1000 | Loss: 0.00000931
Iteration 264/1000 | Loss: 0.00000931
Iteration 265/1000 | Loss: 0.00000931
Iteration 266/1000 | Loss: 0.00000931
Iteration 267/1000 | Loss: 0.00000931
Iteration 268/1000 | Loss: 0.00000931
Iteration 269/1000 | Loss: 0.00000931
Iteration 270/1000 | Loss: 0.00000931
Iteration 271/1000 | Loss: 0.00000930
Iteration 272/1000 | Loss: 0.00000930
Iteration 273/1000 | Loss: 0.00000930
Iteration 274/1000 | Loss: 0.00000930
Iteration 275/1000 | Loss: 0.00000930
Iteration 276/1000 | Loss: 0.00000930
Iteration 277/1000 | Loss: 0.00000930
Iteration 278/1000 | Loss: 0.00000930
Iteration 279/1000 | Loss: 0.00000930
Iteration 280/1000 | Loss: 0.00000930
Iteration 281/1000 | Loss: 0.00000930
Iteration 282/1000 | Loss: 0.00000930
Iteration 283/1000 | Loss: 0.00000930
Iteration 284/1000 | Loss: 0.00000930
Iteration 285/1000 | Loss: 0.00000930
Iteration 286/1000 | Loss: 0.00000930
Iteration 287/1000 | Loss: 0.00000930
Iteration 288/1000 | Loss: 0.00000930
Iteration 289/1000 | Loss: 0.00000930
Iteration 290/1000 | Loss: 0.00000930
Iteration 291/1000 | Loss: 0.00000930
Iteration 292/1000 | Loss: 0.00000930
Iteration 293/1000 | Loss: 0.00000930
Iteration 294/1000 | Loss: 0.00000930
Iteration 295/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [9.295677045884077e-06, 9.295677045884077e-06, 9.295677045884077e-06, 9.295677045884077e-06, 9.295677045884077e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.295677045884077e-06

Optimization complete. Final v2v error: 2.5911340713500977 mm

Highest mean error: 2.7359306812286377 mm for frame 59

Lowest mean error: 2.451657772064209 mm for frame 142

Saving results

Total time: 44.53355884552002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042954
Iteration 2/25 | Loss: 0.00273771
Iteration 3/25 | Loss: 0.00192551
Iteration 4/25 | Loss: 0.00169704
Iteration 5/25 | Loss: 0.00173875
Iteration 6/25 | Loss: 0.00170422
Iteration 7/25 | Loss: 0.00155596
Iteration 8/25 | Loss: 0.00157615
Iteration 9/25 | Loss: 0.00155856
Iteration 10/25 | Loss: 0.00149997
Iteration 11/25 | Loss: 0.00140106
Iteration 12/25 | Loss: 0.00135448
Iteration 13/25 | Loss: 0.00134925
Iteration 14/25 | Loss: 0.00133010
Iteration 15/25 | Loss: 0.00128865
Iteration 16/25 | Loss: 0.00128982
Iteration 17/25 | Loss: 0.00126563
Iteration 18/25 | Loss: 0.00125071
Iteration 19/25 | Loss: 0.00124322
Iteration 20/25 | Loss: 0.00122520
Iteration 21/25 | Loss: 0.00122658
Iteration 22/25 | Loss: 0.00122350
Iteration 23/25 | Loss: 0.00122204
Iteration 24/25 | Loss: 0.00121254
Iteration 25/25 | Loss: 0.00121347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40155578
Iteration 2/25 | Loss: 0.00205270
Iteration 3/25 | Loss: 0.00199533
Iteration 4/25 | Loss: 0.00199533
Iteration 5/25 | Loss: 0.00199533
Iteration 6/25 | Loss: 0.00199533
Iteration 7/25 | Loss: 0.00199533
Iteration 8/25 | Loss: 0.00199533
Iteration 9/25 | Loss: 0.00199533
Iteration 10/25 | Loss: 0.00199533
Iteration 11/25 | Loss: 0.00199533
Iteration 12/25 | Loss: 0.00199533
Iteration 13/25 | Loss: 0.00199533
Iteration 14/25 | Loss: 0.00199533
Iteration 15/25 | Loss: 0.00199533
Iteration 16/25 | Loss: 0.00199533
Iteration 17/25 | Loss: 0.00199533
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001995326019823551, 0.001995326019823551, 0.001995326019823551, 0.001995326019823551, 0.001995326019823551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001995326019823551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199533
Iteration 2/1000 | Loss: 0.00158186
Iteration 3/1000 | Loss: 0.00217594
Iteration 4/1000 | Loss: 0.00205711
Iteration 5/1000 | Loss: 0.00198363
Iteration 6/1000 | Loss: 0.00210474
Iteration 7/1000 | Loss: 0.00207314
Iteration 8/1000 | Loss: 0.00213018
Iteration 9/1000 | Loss: 0.00220016
Iteration 10/1000 | Loss: 0.00250380
Iteration 11/1000 | Loss: 0.00282943
Iteration 12/1000 | Loss: 0.00261936
Iteration 13/1000 | Loss: 0.00324031
Iteration 14/1000 | Loss: 0.00268224
Iteration 15/1000 | Loss: 0.00242440
Iteration 16/1000 | Loss: 0.00231774
Iteration 17/1000 | Loss: 0.00190965
Iteration 18/1000 | Loss: 0.00199183
Iteration 19/1000 | Loss: 0.00220447
Iteration 20/1000 | Loss: 0.00251444
Iteration 21/1000 | Loss: 0.00254784
Iteration 22/1000 | Loss: 0.00281523
Iteration 23/1000 | Loss: 0.00272593
Iteration 24/1000 | Loss: 0.00270570
Iteration 25/1000 | Loss: 0.00236028
Iteration 26/1000 | Loss: 0.00287517
Iteration 27/1000 | Loss: 0.00293448
Iteration 28/1000 | Loss: 0.00191005
Iteration 29/1000 | Loss: 0.00201239
Iteration 30/1000 | Loss: 0.00110464
Iteration 31/1000 | Loss: 0.00161182
Iteration 32/1000 | Loss: 0.00157027
Iteration 33/1000 | Loss: 0.00160660
Iteration 34/1000 | Loss: 0.00182190
Iteration 35/1000 | Loss: 0.00159595
Iteration 36/1000 | Loss: 0.00157850
Iteration 37/1000 | Loss: 0.00180582
Iteration 38/1000 | Loss: 0.00190748
Iteration 39/1000 | Loss: 0.00213308
Iteration 40/1000 | Loss: 0.00179876
Iteration 41/1000 | Loss: 0.00200913
Iteration 42/1000 | Loss: 0.00192492
Iteration 43/1000 | Loss: 0.00213786
Iteration 44/1000 | Loss: 0.00183817
Iteration 45/1000 | Loss: 0.00193746
Iteration 46/1000 | Loss: 0.00246442
Iteration 47/1000 | Loss: 0.00201164
Iteration 48/1000 | Loss: 0.00220845
Iteration 49/1000 | Loss: 0.00250937
Iteration 50/1000 | Loss: 0.00160934
Iteration 51/1000 | Loss: 0.00091271
Iteration 52/1000 | Loss: 0.00190973
Iteration 53/1000 | Loss: 0.00164216
Iteration 54/1000 | Loss: 0.00143767
Iteration 55/1000 | Loss: 0.00181141
Iteration 56/1000 | Loss: 0.00125962
Iteration 57/1000 | Loss: 0.00150381
Iteration 58/1000 | Loss: 0.00183951
Iteration 59/1000 | Loss: 0.00190452
Iteration 60/1000 | Loss: 0.00108811
Iteration 61/1000 | Loss: 0.00150418
Iteration 62/1000 | Loss: 0.00284846
Iteration 63/1000 | Loss: 0.00232081
Iteration 64/1000 | Loss: 0.00165144
Iteration 65/1000 | Loss: 0.00177116
Iteration 66/1000 | Loss: 0.00213639
Iteration 67/1000 | Loss: 0.00201026
Iteration 68/1000 | Loss: 0.00119578
Iteration 69/1000 | Loss: 0.00140651
Iteration 70/1000 | Loss: 0.00280603
Iteration 71/1000 | Loss: 0.00197296
Iteration 72/1000 | Loss: 0.00159919
Iteration 73/1000 | Loss: 0.00479475
Iteration 74/1000 | Loss: 0.00261040
Iteration 75/1000 | Loss: 0.00153589
Iteration 76/1000 | Loss: 0.00158159
Iteration 77/1000 | Loss: 0.00142625
Iteration 78/1000 | Loss: 0.00105953
Iteration 79/1000 | Loss: 0.00114651
Iteration 80/1000 | Loss: 0.00127290
Iteration 81/1000 | Loss: 0.00128788
Iteration 82/1000 | Loss: 0.00136269
Iteration 83/1000 | Loss: 0.00142141
Iteration 84/1000 | Loss: 0.00137142
Iteration 85/1000 | Loss: 0.00149983
Iteration 86/1000 | Loss: 0.00199876
Iteration 87/1000 | Loss: 0.00254487
Iteration 88/1000 | Loss: 0.00182473
Iteration 89/1000 | Loss: 0.00242259
Iteration 90/1000 | Loss: 0.00188531
Iteration 91/1000 | Loss: 0.00113985
Iteration 92/1000 | Loss: 0.00224309
Iteration 93/1000 | Loss: 0.00123451
Iteration 94/1000 | Loss: 0.00064602
Iteration 95/1000 | Loss: 0.00110992
Iteration 96/1000 | Loss: 0.00110426
Iteration 97/1000 | Loss: 0.00100047
Iteration 98/1000 | Loss: 0.00135039
Iteration 99/1000 | Loss: 0.00119506
Iteration 100/1000 | Loss: 0.00124851
Iteration 101/1000 | Loss: 0.00111467
Iteration 102/1000 | Loss: 0.00116588
Iteration 103/1000 | Loss: 0.00139060
Iteration 104/1000 | Loss: 0.00107935
Iteration 105/1000 | Loss: 0.00122023
Iteration 106/1000 | Loss: 0.00082223
Iteration 107/1000 | Loss: 0.00087971
Iteration 108/1000 | Loss: 0.00129102
Iteration 109/1000 | Loss: 0.00124441
Iteration 110/1000 | Loss: 0.00131455
Iteration 111/1000 | Loss: 0.00135069
Iteration 112/1000 | Loss: 0.00130443
Iteration 113/1000 | Loss: 0.00139644
Iteration 114/1000 | Loss: 0.00228753
Iteration 115/1000 | Loss: 0.00157697
Iteration 116/1000 | Loss: 0.00113587
Iteration 117/1000 | Loss: 0.00127572
Iteration 118/1000 | Loss: 0.00140970
Iteration 119/1000 | Loss: 0.00139730
Iteration 120/1000 | Loss: 0.00125191
Iteration 121/1000 | Loss: 0.00133722
Iteration 122/1000 | Loss: 0.00143135
Iteration 123/1000 | Loss: 0.00128533
Iteration 124/1000 | Loss: 0.00128986
Iteration 125/1000 | Loss: 0.00117495
Iteration 126/1000 | Loss: 0.00127868
Iteration 127/1000 | Loss: 0.00127679
Iteration 128/1000 | Loss: 0.00238446
Iteration 129/1000 | Loss: 0.00249104
Iteration 130/1000 | Loss: 0.00166499
Iteration 131/1000 | Loss: 0.00124221
Iteration 132/1000 | Loss: 0.00147494
Iteration 133/1000 | Loss: 0.00177841
Iteration 134/1000 | Loss: 0.00171856
Iteration 135/1000 | Loss: 0.00184165
Iteration 136/1000 | Loss: 0.00369932
Iteration 137/1000 | Loss: 0.00252371
Iteration 138/1000 | Loss: 0.00128974
Iteration 139/1000 | Loss: 0.00169352
Iteration 140/1000 | Loss: 0.00117745
Iteration 141/1000 | Loss: 0.00104695
Iteration 142/1000 | Loss: 0.00117759
Iteration 143/1000 | Loss: 0.00119946
Iteration 144/1000 | Loss: 0.00148607
Iteration 145/1000 | Loss: 0.00146823
Iteration 146/1000 | Loss: 0.00154386
Iteration 147/1000 | Loss: 0.00133617
Iteration 148/1000 | Loss: 0.00143866
Iteration 149/1000 | Loss: 0.00162030
Iteration 150/1000 | Loss: 0.00134524
Iteration 151/1000 | Loss: 0.00085622
Iteration 152/1000 | Loss: 0.00170086
Iteration 153/1000 | Loss: 0.00196425
Iteration 154/1000 | Loss: 0.00133988
Iteration 155/1000 | Loss: 0.00143328
Iteration 156/1000 | Loss: 0.00188281
Iteration 157/1000 | Loss: 0.00138454
Iteration 158/1000 | Loss: 0.00136800
Iteration 159/1000 | Loss: 0.00149629
Iteration 160/1000 | Loss: 0.00099772
Iteration 161/1000 | Loss: 0.00120428
Iteration 162/1000 | Loss: 0.00087997
Iteration 163/1000 | Loss: 0.00152504
Iteration 164/1000 | Loss: 0.00201637
Iteration 165/1000 | Loss: 0.00099916
Iteration 166/1000 | Loss: 0.00132924
Iteration 167/1000 | Loss: 0.00139016
Iteration 168/1000 | Loss: 0.00103370
Iteration 169/1000 | Loss: 0.00180827
Iteration 170/1000 | Loss: 0.00202618
Iteration 171/1000 | Loss: 0.00225763
Iteration 172/1000 | Loss: 0.00121122
Iteration 173/1000 | Loss: 0.00427460
Iteration 174/1000 | Loss: 0.00516492
Iteration 175/1000 | Loss: 0.00413298
Iteration 176/1000 | Loss: 0.00385169
Iteration 177/1000 | Loss: 0.00247003
Iteration 178/1000 | Loss: 0.00338898
Iteration 179/1000 | Loss: 0.00567887
Iteration 180/1000 | Loss: 0.00280918
Iteration 181/1000 | Loss: 0.00157744
Iteration 182/1000 | Loss: 0.00134718
Iteration 183/1000 | Loss: 0.00136050
Iteration 184/1000 | Loss: 0.00134602
Iteration 185/1000 | Loss: 0.00151843
Iteration 186/1000 | Loss: 0.00135666
Iteration 187/1000 | Loss: 0.00155926
Iteration 188/1000 | Loss: 0.00114419
Iteration 189/1000 | Loss: 0.00103468
Iteration 190/1000 | Loss: 0.00091772
Iteration 191/1000 | Loss: 0.00099456
Iteration 192/1000 | Loss: 0.00157984
Iteration 193/1000 | Loss: 0.00137007
Iteration 194/1000 | Loss: 0.00124693
Iteration 195/1000 | Loss: 0.00093344
Iteration 196/1000 | Loss: 0.00116277
Iteration 197/1000 | Loss: 0.00128976
Iteration 198/1000 | Loss: 0.00108658
Iteration 199/1000 | Loss: 0.00109492
Iteration 200/1000 | Loss: 0.00134125
Iteration 201/1000 | Loss: 0.00123395
Iteration 202/1000 | Loss: 0.00097959
Iteration 203/1000 | Loss: 0.00144871
Iteration 204/1000 | Loss: 0.00114275
Iteration 205/1000 | Loss: 0.00099264
Iteration 206/1000 | Loss: 0.00122942
Iteration 207/1000 | Loss: 0.00098724
Iteration 208/1000 | Loss: 0.00107061
Iteration 209/1000 | Loss: 0.00120752
Iteration 210/1000 | Loss: 0.00152529
Iteration 211/1000 | Loss: 0.00118847
Iteration 212/1000 | Loss: 0.00102976
Iteration 213/1000 | Loss: 0.00155372
Iteration 214/1000 | Loss: 0.00211986
Iteration 215/1000 | Loss: 0.00131845
Iteration 216/1000 | Loss: 0.00129606
Iteration 217/1000 | Loss: 0.00121877
Iteration 218/1000 | Loss: 0.00113589
Iteration 219/1000 | Loss: 0.00133763
Iteration 220/1000 | Loss: 0.00151385
Iteration 221/1000 | Loss: 0.00142181
Iteration 222/1000 | Loss: 0.00113608
Iteration 223/1000 | Loss: 0.00136223
Iteration 224/1000 | Loss: 0.00116992
Iteration 225/1000 | Loss: 0.00117695
Iteration 226/1000 | Loss: 0.00158084
Iteration 227/1000 | Loss: 0.00127588
Iteration 228/1000 | Loss: 0.00134916
Iteration 229/1000 | Loss: 0.00056788
Iteration 230/1000 | Loss: 0.00118660
Iteration 231/1000 | Loss: 0.00107154
Iteration 232/1000 | Loss: 0.00108499
Iteration 233/1000 | Loss: 0.00116273
Iteration 234/1000 | Loss: 0.00100277
Iteration 235/1000 | Loss: 0.00171258
Iteration 236/1000 | Loss: 0.00157583
Iteration 237/1000 | Loss: 0.00076772
Iteration 238/1000 | Loss: 0.00098933
Iteration 239/1000 | Loss: 0.00104480
Iteration 240/1000 | Loss: 0.00108950
Iteration 241/1000 | Loss: 0.00097408
Iteration 242/1000 | Loss: 0.00104553
Iteration 243/1000 | Loss: 0.00094284
Iteration 244/1000 | Loss: 0.00202789
Iteration 245/1000 | Loss: 0.00108773
Iteration 246/1000 | Loss: 0.00102790
Iteration 247/1000 | Loss: 0.00085876
Iteration 248/1000 | Loss: 0.00132476
Iteration 249/1000 | Loss: 0.00194876
Iteration 250/1000 | Loss: 0.00094937
Iteration 251/1000 | Loss: 0.00109516
Iteration 252/1000 | Loss: 0.00102287
Iteration 253/1000 | Loss: 0.00096240
Iteration 254/1000 | Loss: 0.00122603
Iteration 255/1000 | Loss: 0.00203274
Iteration 256/1000 | Loss: 0.00145311
Iteration 257/1000 | Loss: 0.00170485
Iteration 258/1000 | Loss: 0.00113342
Iteration 259/1000 | Loss: 0.00103716
Iteration 260/1000 | Loss: 0.00110218
Iteration 261/1000 | Loss: 0.00087537
Iteration 262/1000 | Loss: 0.00123727
Iteration 263/1000 | Loss: 0.00085660
Iteration 264/1000 | Loss: 0.00086327
Iteration 265/1000 | Loss: 0.00078755
Iteration 266/1000 | Loss: 0.00084209
Iteration 267/1000 | Loss: 0.00073095
Iteration 268/1000 | Loss: 0.00147119
Iteration 269/1000 | Loss: 0.00079810
Iteration 270/1000 | Loss: 0.00063972
Iteration 271/1000 | Loss: 0.00061082
Iteration 272/1000 | Loss: 0.00079862
Iteration 273/1000 | Loss: 0.00081249
Iteration 274/1000 | Loss: 0.00088002
Iteration 275/1000 | Loss: 0.00087315
Iteration 276/1000 | Loss: 0.00096823
Iteration 277/1000 | Loss: 0.00075071
Iteration 278/1000 | Loss: 0.00078265
Iteration 279/1000 | Loss: 0.00087874
Iteration 280/1000 | Loss: 0.00063453
Iteration 281/1000 | Loss: 0.00053426
Iteration 282/1000 | Loss: 0.00057801
Iteration 283/1000 | Loss: 0.00066643
Iteration 284/1000 | Loss: 0.00069854
Iteration 285/1000 | Loss: 0.00063076
Iteration 286/1000 | Loss: 0.00084655
Iteration 287/1000 | Loss: 0.00059730
Iteration 288/1000 | Loss: 0.00095363
Iteration 289/1000 | Loss: 0.00066058
Iteration 290/1000 | Loss: 0.00071358
Iteration 291/1000 | Loss: 0.00060046
Iteration 292/1000 | Loss: 0.00060096
Iteration 293/1000 | Loss: 0.00060997
Iteration 294/1000 | Loss: 0.00097450
Iteration 295/1000 | Loss: 0.00070842
Iteration 296/1000 | Loss: 0.00050552
Iteration 297/1000 | Loss: 0.00065333
Iteration 298/1000 | Loss: 0.00095493
Iteration 299/1000 | Loss: 0.00058983
Iteration 300/1000 | Loss: 0.00065499
Iteration 301/1000 | Loss: 0.00060816
Iteration 302/1000 | Loss: 0.00070518
Iteration 303/1000 | Loss: 0.00068645
Iteration 304/1000 | Loss: 0.00069222
Iteration 305/1000 | Loss: 0.00057194
Iteration 306/1000 | Loss: 0.00056500
Iteration 307/1000 | Loss: 0.00064830
Iteration 308/1000 | Loss: 0.00068182
Iteration 309/1000 | Loss: 0.00059930
Iteration 310/1000 | Loss: 0.00068383
Iteration 311/1000 | Loss: 0.00069467
Iteration 312/1000 | Loss: 0.00073404
Iteration 313/1000 | Loss: 0.00058223
Iteration 314/1000 | Loss: 0.00056177
Iteration 315/1000 | Loss: 0.00072828
Iteration 316/1000 | Loss: 0.00072486
Iteration 317/1000 | Loss: 0.00072195
Iteration 318/1000 | Loss: 0.00074368
Iteration 319/1000 | Loss: 0.00027151
Iteration 320/1000 | Loss: 0.00032219
Iteration 321/1000 | Loss: 0.00051102
Iteration 322/1000 | Loss: 0.00023652
Iteration 323/1000 | Loss: 0.00065515
Iteration 324/1000 | Loss: 0.00050181
Iteration 325/1000 | Loss: 0.00042010
Iteration 326/1000 | Loss: 0.00098774
Iteration 327/1000 | Loss: 0.00047082
Iteration 328/1000 | Loss: 0.00076930
Iteration 329/1000 | Loss: 0.00065193
Iteration 330/1000 | Loss: 0.00009362
Iteration 331/1000 | Loss: 0.00049426
Iteration 332/1000 | Loss: 0.00034071
Iteration 333/1000 | Loss: 0.00021458
Iteration 334/1000 | Loss: 0.00026213
Iteration 335/1000 | Loss: 0.00008476
Iteration 336/1000 | Loss: 0.00023946
Iteration 337/1000 | Loss: 0.00022280
Iteration 338/1000 | Loss: 0.00006411
Iteration 339/1000 | Loss: 0.00007662
Iteration 340/1000 | Loss: 0.00009527
Iteration 341/1000 | Loss: 0.00007214
Iteration 342/1000 | Loss: 0.00006942
Iteration 343/1000 | Loss: 0.00003739
Iteration 344/1000 | Loss: 0.00009035
Iteration 345/1000 | Loss: 0.00006889
Iteration 346/1000 | Loss: 0.00007256
Iteration 347/1000 | Loss: 0.00007285
Iteration 348/1000 | Loss: 0.00006357
Iteration 349/1000 | Loss: 0.00006608
Iteration 350/1000 | Loss: 0.00006615
Iteration 351/1000 | Loss: 0.00005581
Iteration 352/1000 | Loss: 0.00005879
Iteration 353/1000 | Loss: 0.00006396
Iteration 354/1000 | Loss: 0.00006256
Iteration 355/1000 | Loss: 0.00006535
Iteration 356/1000 | Loss: 0.00006217
Iteration 357/1000 | Loss: 0.00007190
Iteration 358/1000 | Loss: 0.00007397
Iteration 359/1000 | Loss: 0.00006342
Iteration 360/1000 | Loss: 0.00006270
Iteration 361/1000 | Loss: 0.00005926
Iteration 362/1000 | Loss: 0.00005948
Iteration 363/1000 | Loss: 0.00006327
Iteration 364/1000 | Loss: 0.00006231
Iteration 365/1000 | Loss: 0.00006301
Iteration 366/1000 | Loss: 0.00007137
Iteration 367/1000 | Loss: 0.00008075
Iteration 368/1000 | Loss: 0.00006236
Iteration 369/1000 | Loss: 0.00006908
Iteration 370/1000 | Loss: 0.00006347
Iteration 371/1000 | Loss: 0.00006455
Iteration 372/1000 | Loss: 0.00006298
Iteration 373/1000 | Loss: 0.00006886
Iteration 374/1000 | Loss: 0.00006328
Iteration 375/1000 | Loss: 0.00006111
Iteration 376/1000 | Loss: 0.00007026
Iteration 377/1000 | Loss: 0.00006904
Iteration 378/1000 | Loss: 0.00006098
Iteration 379/1000 | Loss: 0.00006243
Iteration 380/1000 | Loss: 0.00006175
Iteration 381/1000 | Loss: 0.00006278
Iteration 382/1000 | Loss: 0.00006495
Iteration 383/1000 | Loss: 0.00006481
Iteration 384/1000 | Loss: 0.00006198
Iteration 385/1000 | Loss: 0.00006313
Iteration 386/1000 | Loss: 0.00006368
Iteration 387/1000 | Loss: 0.00006272
Iteration 388/1000 | Loss: 0.00012917
Iteration 389/1000 | Loss: 0.00006665
Iteration 390/1000 | Loss: 0.00005703
Iteration 391/1000 | Loss: 0.00007264
Iteration 392/1000 | Loss: 0.00006137
Iteration 393/1000 | Loss: 0.00006406
Iteration 394/1000 | Loss: 0.00006459
Iteration 395/1000 | Loss: 0.00006394
Iteration 396/1000 | Loss: 0.00006142
Iteration 397/1000 | Loss: 0.00006490
Iteration 398/1000 | Loss: 0.00016440
Iteration 399/1000 | Loss: 0.00009076
Iteration 400/1000 | Loss: 0.00015298
Iteration 401/1000 | Loss: 0.00007844
Iteration 402/1000 | Loss: 0.00007347
Iteration 403/1000 | Loss: 0.00006278
Iteration 404/1000 | Loss: 0.00008164
Iteration 405/1000 | Loss: 0.00006535
Iteration 406/1000 | Loss: 0.00006324
Iteration 407/1000 | Loss: 0.00006397
Iteration 408/1000 | Loss: 0.00006618
Iteration 409/1000 | Loss: 0.00013449
Iteration 410/1000 | Loss: 0.00006810
Iteration 411/1000 | Loss: 0.00006106
Iteration 412/1000 | Loss: 0.00005882
Iteration 413/1000 | Loss: 0.00006469
Iteration 414/1000 | Loss: 0.00006230
Iteration 415/1000 | Loss: 0.00005456
Iteration 416/1000 | Loss: 0.00005881
Iteration 417/1000 | Loss: 0.00006478
Iteration 418/1000 | Loss: 0.00006004
Iteration 419/1000 | Loss: 0.00006483
Iteration 420/1000 | Loss: 0.00006026
Iteration 421/1000 | Loss: 0.00006643
Iteration 422/1000 | Loss: 0.00006289
Iteration 423/1000 | Loss: 0.00006452
Iteration 424/1000 | Loss: 0.00005498
Iteration 425/1000 | Loss: 0.00006158
Iteration 426/1000 | Loss: 0.00005996
Iteration 427/1000 | Loss: 0.00006785
Iteration 428/1000 | Loss: 0.00006275
Iteration 429/1000 | Loss: 0.00004597
Iteration 430/1000 | Loss: 0.00005209
Iteration 431/1000 | Loss: 0.00004471
Iteration 432/1000 | Loss: 0.00006033
Iteration 433/1000 | Loss: 0.00006493
Iteration 434/1000 | Loss: 0.00006363
Iteration 435/1000 | Loss: 0.00006749
Iteration 436/1000 | Loss: 0.00006318
Iteration 437/1000 | Loss: 0.00006391
Iteration 438/1000 | Loss: 0.00006329
Iteration 439/1000 | Loss: 0.00006850
Iteration 440/1000 | Loss: 0.00007069
Iteration 441/1000 | Loss: 0.00006113
Iteration 442/1000 | Loss: 0.00007200
Iteration 443/1000 | Loss: 0.00006645
Iteration 444/1000 | Loss: 0.00005319
Iteration 445/1000 | Loss: 0.00007751
Iteration 446/1000 | Loss: 0.00006059
Iteration 447/1000 | Loss: 0.00006501
Iteration 448/1000 | Loss: 0.00006053
Iteration 449/1000 | Loss: 0.00006282
Iteration 450/1000 | Loss: 0.00006740
Iteration 451/1000 | Loss: 0.00006354
Iteration 452/1000 | Loss: 0.00006326
Iteration 453/1000 | Loss: 0.00004651
Iteration 454/1000 | Loss: 0.00004782
Iteration 455/1000 | Loss: 0.00002085
Iteration 456/1000 | Loss: 0.00003083
Iteration 457/1000 | Loss: 0.00005308
Iteration 458/1000 | Loss: 0.00006460
Iteration 459/1000 | Loss: 0.00005933
Iteration 460/1000 | Loss: 0.00006160
Iteration 461/1000 | Loss: 0.00006848
Iteration 462/1000 | Loss: 0.00006041
Iteration 463/1000 | Loss: 0.00002123
Iteration 464/1000 | Loss: 0.00003049
Iteration 465/1000 | Loss: 0.00002510
Iteration 466/1000 | Loss: 0.00002328
Iteration 467/1000 | Loss: 0.00001498
Iteration 468/1000 | Loss: 0.00001490
Iteration 469/1000 | Loss: 0.00001014
Iteration 470/1000 | Loss: 0.00002651
Iteration 471/1000 | Loss: 0.00002601
Iteration 472/1000 | Loss: 0.00002208
Iteration 473/1000 | Loss: 0.00002499
Iteration 474/1000 | Loss: 0.00002053
Iteration 475/1000 | Loss: 0.00002733
Iteration 476/1000 | Loss: 0.00001778
Iteration 477/1000 | Loss: 0.00002430
Iteration 478/1000 | Loss: 0.00001577
Iteration 479/1000 | Loss: 0.00002415
Iteration 480/1000 | Loss: 0.00001634
Iteration 481/1000 | Loss: 0.00002316
Iteration 482/1000 | Loss: 0.00002038
Iteration 483/1000 | Loss: 0.00002475
Iteration 484/1000 | Loss: 0.00002009
Iteration 485/1000 | Loss: 0.00002164
Iteration 486/1000 | Loss: 0.00002104
Iteration 487/1000 | Loss: 0.00002383
Iteration 488/1000 | Loss: 0.00001891
Iteration 489/1000 | Loss: 0.00002109
Iteration 490/1000 | Loss: 0.00002022
Iteration 491/1000 | Loss: 0.00001727
Iteration 492/1000 | Loss: 0.00001995
Iteration 493/1000 | Loss: 0.00001881
Iteration 494/1000 | Loss: 0.00002176
Iteration 495/1000 | Loss: 0.00002482
Iteration 496/1000 | Loss: 0.00002165
Iteration 497/1000 | Loss: 0.00001989
Iteration 498/1000 | Loss: 0.00001181
Iteration 499/1000 | Loss: 0.00002449
Iteration 500/1000 | Loss: 0.00002025
Iteration 501/1000 | Loss: 0.00001986
Iteration 502/1000 | Loss: 0.00001961
Iteration 503/1000 | Loss: 0.00002263
Iteration 504/1000 | Loss: 0.00001823
Iteration 505/1000 | Loss: 0.00002230
Iteration 506/1000 | Loss: 0.00001817
Iteration 507/1000 | Loss: 0.00004074
Iteration 508/1000 | Loss: 0.00002129
Iteration 509/1000 | Loss: 0.00002362
Iteration 510/1000 | Loss: 0.00002155
Iteration 511/1000 | Loss: 0.00004173
Iteration 512/1000 | Loss: 0.00002759
Iteration 513/1000 | Loss: 0.00002241
Iteration 514/1000 | Loss: 0.00001486
Iteration 515/1000 | Loss: 0.00001617
Iteration 516/1000 | Loss: 0.00001747
Iteration 517/1000 | Loss: 0.00002828
Iteration 518/1000 | Loss: 0.00002242
Iteration 519/1000 | Loss: 0.00005649
Iteration 520/1000 | Loss: 0.00002682
Iteration 521/1000 | Loss: 0.00002309
Iteration 522/1000 | Loss: 0.00001937
Iteration 523/1000 | Loss: 0.00001974
Iteration 524/1000 | Loss: 0.00001678
Iteration 525/1000 | Loss: 0.00002301
Iteration 526/1000 | Loss: 0.00001943
Iteration 527/1000 | Loss: 0.00002589
Iteration 528/1000 | Loss: 0.00007644
Iteration 529/1000 | Loss: 0.00006565
Iteration 530/1000 | Loss: 0.00002273
Iteration 531/1000 | Loss: 0.00002051
Iteration 532/1000 | Loss: 0.00003943
Iteration 533/1000 | Loss: 0.00002374
Iteration 534/1000 | Loss: 0.00001125
Iteration 535/1000 | Loss: 0.00001864
Iteration 536/1000 | Loss: 0.00003606
Iteration 537/1000 | Loss: 0.00001163
Iteration 538/1000 | Loss: 0.00000885
Iteration 539/1000 | Loss: 0.00000830
Iteration 540/1000 | Loss: 0.00000801
Iteration 541/1000 | Loss: 0.00000796
Iteration 542/1000 | Loss: 0.00000793
Iteration 543/1000 | Loss: 0.00000793
Iteration 544/1000 | Loss: 0.00000789
Iteration 545/1000 | Loss: 0.00000788
Iteration 546/1000 | Loss: 0.00000788
Iteration 547/1000 | Loss: 0.00000785
Iteration 548/1000 | Loss: 0.00000783
Iteration 549/1000 | Loss: 0.00000783
Iteration 550/1000 | Loss: 0.00000783
Iteration 551/1000 | Loss: 0.00000782
Iteration 552/1000 | Loss: 0.00000781
Iteration 553/1000 | Loss: 0.00000781
Iteration 554/1000 | Loss: 0.00000780
Iteration 555/1000 | Loss: 0.00000779
Iteration 556/1000 | Loss: 0.00000778
Iteration 557/1000 | Loss: 0.00000778
Iteration 558/1000 | Loss: 0.00000777
Iteration 559/1000 | Loss: 0.00000777
Iteration 560/1000 | Loss: 0.00000776
Iteration 561/1000 | Loss: 0.00000776
Iteration 562/1000 | Loss: 0.00000776
Iteration 563/1000 | Loss: 0.00000776
Iteration 564/1000 | Loss: 0.00000775
Iteration 565/1000 | Loss: 0.00000775
Iteration 566/1000 | Loss: 0.00000775
Iteration 567/1000 | Loss: 0.00000775
Iteration 568/1000 | Loss: 0.00000775
Iteration 569/1000 | Loss: 0.00000775
Iteration 570/1000 | Loss: 0.00000775
Iteration 571/1000 | Loss: 0.00000775
Iteration 572/1000 | Loss: 0.00000775
Iteration 573/1000 | Loss: 0.00000774
Iteration 574/1000 | Loss: 0.00000774
Iteration 575/1000 | Loss: 0.00000774
Iteration 576/1000 | Loss: 0.00000774
Iteration 577/1000 | Loss: 0.00000773
Iteration 578/1000 | Loss: 0.00000773
Iteration 579/1000 | Loss: 0.00000773
Iteration 580/1000 | Loss: 0.00000773
Iteration 581/1000 | Loss: 0.00000773
Iteration 582/1000 | Loss: 0.00000773
Iteration 583/1000 | Loss: 0.00000773
Iteration 584/1000 | Loss: 0.00000773
Iteration 585/1000 | Loss: 0.00000773
Iteration 586/1000 | Loss: 0.00000772
Iteration 587/1000 | Loss: 0.00000772
Iteration 588/1000 | Loss: 0.00000772
Iteration 589/1000 | Loss: 0.00000772
Iteration 590/1000 | Loss: 0.00000772
Iteration 591/1000 | Loss: 0.00000772
Iteration 592/1000 | Loss: 0.00000772
Iteration 593/1000 | Loss: 0.00000772
Iteration 594/1000 | Loss: 0.00000772
Iteration 595/1000 | Loss: 0.00000772
Iteration 596/1000 | Loss: 0.00000772
Iteration 597/1000 | Loss: 0.00000772
Iteration 598/1000 | Loss: 0.00000772
Iteration 599/1000 | Loss: 0.00000772
Iteration 600/1000 | Loss: 0.00000772
Iteration 601/1000 | Loss: 0.00000772
Iteration 602/1000 | Loss: 0.00000771
Iteration 603/1000 | Loss: 0.00000771
Iteration 604/1000 | Loss: 0.00000771
Iteration 605/1000 | Loss: 0.00000771
Iteration 606/1000 | Loss: 0.00000771
Iteration 607/1000 | Loss: 0.00000771
Iteration 608/1000 | Loss: 0.00000770
Iteration 609/1000 | Loss: 0.00000770
Iteration 610/1000 | Loss: 0.00000770
Iteration 611/1000 | Loss: 0.00000770
Iteration 612/1000 | Loss: 0.00000770
Iteration 613/1000 | Loss: 0.00000769
Iteration 614/1000 | Loss: 0.00000769
Iteration 615/1000 | Loss: 0.00000769
Iteration 616/1000 | Loss: 0.00000769
Iteration 617/1000 | Loss: 0.00000768
Iteration 618/1000 | Loss: 0.00000768
Iteration 619/1000 | Loss: 0.00000768
Iteration 620/1000 | Loss: 0.00000768
Iteration 621/1000 | Loss: 0.00000768
Iteration 622/1000 | Loss: 0.00000767
Iteration 623/1000 | Loss: 0.00000767
Iteration 624/1000 | Loss: 0.00000767
Iteration 625/1000 | Loss: 0.00000767
Iteration 626/1000 | Loss: 0.00000766
Iteration 627/1000 | Loss: 0.00000766
Iteration 628/1000 | Loss: 0.00000766
Iteration 629/1000 | Loss: 0.00000766
Iteration 630/1000 | Loss: 0.00000766
Iteration 631/1000 | Loss: 0.00000766
Iteration 632/1000 | Loss: 0.00000766
Iteration 633/1000 | Loss: 0.00000765
Iteration 634/1000 | Loss: 0.00000765
Iteration 635/1000 | Loss: 0.00000765
Iteration 636/1000 | Loss: 0.00000765
Iteration 637/1000 | Loss: 0.00000765
Iteration 638/1000 | Loss: 0.00000764
Iteration 639/1000 | Loss: 0.00000764
Iteration 640/1000 | Loss: 0.00000764
Iteration 641/1000 | Loss: 0.00000764
Iteration 642/1000 | Loss: 0.00000764
Iteration 643/1000 | Loss: 0.00000763
Iteration 644/1000 | Loss: 0.00000763
Iteration 645/1000 | Loss: 0.00000763
Iteration 646/1000 | Loss: 0.00000763
Iteration 647/1000 | Loss: 0.00000763
Iteration 648/1000 | Loss: 0.00000763
Iteration 649/1000 | Loss: 0.00000762
Iteration 650/1000 | Loss: 0.00000762
Iteration 651/1000 | Loss: 0.00000762
Iteration 652/1000 | Loss: 0.00000762
Iteration 653/1000 | Loss: 0.00000762
Iteration 654/1000 | Loss: 0.00000762
Iteration 655/1000 | Loss: 0.00000762
Iteration 656/1000 | Loss: 0.00000762
Iteration 657/1000 | Loss: 0.00000762
Iteration 658/1000 | Loss: 0.00000762
Iteration 659/1000 | Loss: 0.00000762
Iteration 660/1000 | Loss: 0.00000762
Iteration 661/1000 | Loss: 0.00000761
Iteration 662/1000 | Loss: 0.00000761
Iteration 663/1000 | Loss: 0.00000761
Iteration 664/1000 | Loss: 0.00000761
Iteration 665/1000 | Loss: 0.00000761
Iteration 666/1000 | Loss: 0.00000761
Iteration 667/1000 | Loss: 0.00000761
Iteration 668/1000 | Loss: 0.00000761
Iteration 669/1000 | Loss: 0.00000761
Iteration 670/1000 | Loss: 0.00000760
Iteration 671/1000 | Loss: 0.00000760
Iteration 672/1000 | Loss: 0.00000760
Iteration 673/1000 | Loss: 0.00000760
Iteration 674/1000 | Loss: 0.00000760
Iteration 675/1000 | Loss: 0.00000760
Iteration 676/1000 | Loss: 0.00000760
Iteration 677/1000 | Loss: 0.00000760
Iteration 678/1000 | Loss: 0.00000759
Iteration 679/1000 | Loss: 0.00000759
Iteration 680/1000 | Loss: 0.00000759
Iteration 681/1000 | Loss: 0.00000758
Iteration 682/1000 | Loss: 0.00000758
Iteration 683/1000 | Loss: 0.00000758
Iteration 684/1000 | Loss: 0.00000758
Iteration 685/1000 | Loss: 0.00000758
Iteration 686/1000 | Loss: 0.00000758
Iteration 687/1000 | Loss: 0.00000758
Iteration 688/1000 | Loss: 0.00000757
Iteration 689/1000 | Loss: 0.00000757
Iteration 690/1000 | Loss: 0.00000757
Iteration 691/1000 | Loss: 0.00000757
Iteration 692/1000 | Loss: 0.00000757
Iteration 693/1000 | Loss: 0.00000757
Iteration 694/1000 | Loss: 0.00000757
Iteration 695/1000 | Loss: 0.00000757
Iteration 696/1000 | Loss: 0.00000757
Iteration 697/1000 | Loss: 0.00000756
Iteration 698/1000 | Loss: 0.00000756
Iteration 699/1000 | Loss: 0.00000756
Iteration 700/1000 | Loss: 0.00000756
Iteration 701/1000 | Loss: 0.00000756
Iteration 702/1000 | Loss: 0.00000756
Iteration 703/1000 | Loss: 0.00000756
Iteration 704/1000 | Loss: 0.00000756
Iteration 705/1000 | Loss: 0.00000756
Iteration 706/1000 | Loss: 0.00000756
Iteration 707/1000 | Loss: 0.00000755
Iteration 708/1000 | Loss: 0.00000755
Iteration 709/1000 | Loss: 0.00000755
Iteration 710/1000 | Loss: 0.00000755
Iteration 711/1000 | Loss: 0.00000755
Iteration 712/1000 | Loss: 0.00000754
Iteration 713/1000 | Loss: 0.00000754
Iteration 714/1000 | Loss: 0.00000754
Iteration 715/1000 | Loss: 0.00000754
Iteration 716/1000 | Loss: 0.00000754
Iteration 717/1000 | Loss: 0.00000754
Iteration 718/1000 | Loss: 0.00000754
Iteration 719/1000 | Loss: 0.00000754
Iteration 720/1000 | Loss: 0.00000753
Iteration 721/1000 | Loss: 0.00000753
Iteration 722/1000 | Loss: 0.00000753
Iteration 723/1000 | Loss: 0.00000753
Iteration 724/1000 | Loss: 0.00000753
Iteration 725/1000 | Loss: 0.00000753
Iteration 726/1000 | Loss: 0.00000753
Iteration 727/1000 | Loss: 0.00000753
Iteration 728/1000 | Loss: 0.00000753
Iteration 729/1000 | Loss: 0.00000753
Iteration 730/1000 | Loss: 0.00000753
Iteration 731/1000 | Loss: 0.00000753
Iteration 732/1000 | Loss: 0.00000753
Iteration 733/1000 | Loss: 0.00000753
Iteration 734/1000 | Loss: 0.00000753
Iteration 735/1000 | Loss: 0.00000753
Iteration 736/1000 | Loss: 0.00000753
Iteration 737/1000 | Loss: 0.00000753
Iteration 738/1000 | Loss: 0.00000753
Iteration 739/1000 | Loss: 0.00000753
Iteration 740/1000 | Loss: 0.00000753
Iteration 741/1000 | Loss: 0.00000753
Iteration 742/1000 | Loss: 0.00000753
Iteration 743/1000 | Loss: 0.00000753
Iteration 744/1000 | Loss: 0.00000753
Iteration 745/1000 | Loss: 0.00000753
Iteration 746/1000 | Loss: 0.00000753
Iteration 747/1000 | Loss: 0.00000753
Iteration 748/1000 | Loss: 0.00000753
Iteration 749/1000 | Loss: 0.00000753
Iteration 750/1000 | Loss: 0.00000753
Iteration 751/1000 | Loss: 0.00000753
Iteration 752/1000 | Loss: 0.00000753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 752. Stopping optimization.
Last 5 losses: [7.529460617661243e-06, 7.529460617661243e-06, 7.529460617661243e-06, 7.529460617661243e-06, 7.529460617661243e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.529460617661243e-06

Optimization complete. Final v2v error: 2.3896987438201904 mm

Highest mean error: 3.2588419914245605 mm for frame 109

Lowest mean error: 2.2652125358581543 mm for frame 6

Saving results

Total time: 877.0862066745758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805335
Iteration 2/25 | Loss: 0.00149463
Iteration 3/25 | Loss: 0.00121441
Iteration 4/25 | Loss: 0.00118670
Iteration 5/25 | Loss: 0.00118240
Iteration 6/25 | Loss: 0.00118190
Iteration 7/25 | Loss: 0.00118190
Iteration 8/25 | Loss: 0.00118190
Iteration 9/25 | Loss: 0.00118190
Iteration 10/25 | Loss: 0.00118190
Iteration 11/25 | Loss: 0.00118190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011819020146504045, 0.0011819020146504045, 0.0011819020146504045, 0.0011819020146504045, 0.0011819020146504045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819020146504045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29285407
Iteration 2/25 | Loss: 0.00065159
Iteration 3/25 | Loss: 0.00065158
Iteration 4/25 | Loss: 0.00065158
Iteration 5/25 | Loss: 0.00065158
Iteration 6/25 | Loss: 0.00065158
Iteration 7/25 | Loss: 0.00065158
Iteration 8/25 | Loss: 0.00065158
Iteration 9/25 | Loss: 0.00065158
Iteration 10/25 | Loss: 0.00065158
Iteration 11/25 | Loss: 0.00065158
Iteration 12/25 | Loss: 0.00065158
Iteration 13/25 | Loss: 0.00065158
Iteration 14/25 | Loss: 0.00065158
Iteration 15/25 | Loss: 0.00065158
Iteration 16/25 | Loss: 0.00065158
Iteration 17/25 | Loss: 0.00065158
Iteration 18/25 | Loss: 0.00065158
Iteration 19/25 | Loss: 0.00065158
Iteration 20/25 | Loss: 0.00065158
Iteration 21/25 | Loss: 0.00065158
Iteration 22/25 | Loss: 0.00065158
Iteration 23/25 | Loss: 0.00065158
Iteration 24/25 | Loss: 0.00065158
Iteration 25/25 | Loss: 0.00065158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065158
Iteration 2/1000 | Loss: 0.00003384
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001650
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001548
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001521
Iteration 13/1000 | Loss: 0.00001519
Iteration 14/1000 | Loss: 0.00001516
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001500
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001491
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001483
Iteration 25/1000 | Loss: 0.00001483
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001478
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001478
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001478
Iteration 41/1000 | Loss: 0.00001478
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001478
Iteration 44/1000 | Loss: 0.00001478
Iteration 45/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [1.4779906450712588e-05, 1.4779906450712588e-05, 1.4779906450712588e-05, 1.4779906450712588e-05, 1.4779906450712588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4779906450712588e-05

Optimization complete. Final v2v error: 3.273562431335449 mm

Highest mean error: 3.983877182006836 mm for frame 89

Lowest mean error: 2.8446125984191895 mm for frame 52

Saving results

Total time: 34.46441626548767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963059
Iteration 2/25 | Loss: 0.00168946
Iteration 3/25 | Loss: 0.00134099
Iteration 4/25 | Loss: 0.00131857
Iteration 5/25 | Loss: 0.00131163
Iteration 6/25 | Loss: 0.00131026
Iteration 7/25 | Loss: 0.00131026
Iteration 8/25 | Loss: 0.00131026
Iteration 9/25 | Loss: 0.00131026
Iteration 10/25 | Loss: 0.00131026
Iteration 11/25 | Loss: 0.00131026
Iteration 12/25 | Loss: 0.00131026
Iteration 13/25 | Loss: 0.00131026
Iteration 14/25 | Loss: 0.00131026
Iteration 15/25 | Loss: 0.00131026
Iteration 16/25 | Loss: 0.00131026
Iteration 17/25 | Loss: 0.00131026
Iteration 18/25 | Loss: 0.00131026
Iteration 19/25 | Loss: 0.00131026
Iteration 20/25 | Loss: 0.00131026
Iteration 21/25 | Loss: 0.00131026
Iteration 22/25 | Loss: 0.00131026
Iteration 23/25 | Loss: 0.00131026
Iteration 24/25 | Loss: 0.00131026
Iteration 25/25 | Loss: 0.00131026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77304363
Iteration 2/25 | Loss: 0.00094226
Iteration 3/25 | Loss: 0.00094226
Iteration 4/25 | Loss: 0.00094225
Iteration 5/25 | Loss: 0.00094225
Iteration 6/25 | Loss: 0.00094225
Iteration 7/25 | Loss: 0.00094225
Iteration 8/25 | Loss: 0.00094225
Iteration 9/25 | Loss: 0.00094225
Iteration 10/25 | Loss: 0.00094225
Iteration 11/25 | Loss: 0.00094225
Iteration 12/25 | Loss: 0.00094225
Iteration 13/25 | Loss: 0.00094225
Iteration 14/25 | Loss: 0.00094225
Iteration 15/25 | Loss: 0.00094225
Iteration 16/25 | Loss: 0.00094225
Iteration 17/25 | Loss: 0.00094225
Iteration 18/25 | Loss: 0.00094225
Iteration 19/25 | Loss: 0.00094225
Iteration 20/25 | Loss: 0.00094225
Iteration 21/25 | Loss: 0.00094225
Iteration 22/25 | Loss: 0.00094225
Iteration 23/25 | Loss: 0.00094225
Iteration 24/25 | Loss: 0.00094225
Iteration 25/25 | Loss: 0.00094225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094225
Iteration 2/1000 | Loss: 0.00006353
Iteration 3/1000 | Loss: 0.00004380
Iteration 4/1000 | Loss: 0.00003641
Iteration 5/1000 | Loss: 0.00003436
Iteration 6/1000 | Loss: 0.00003280
Iteration 7/1000 | Loss: 0.00003199
Iteration 8/1000 | Loss: 0.00003119
Iteration 9/1000 | Loss: 0.00003082
Iteration 10/1000 | Loss: 0.00003044
Iteration 11/1000 | Loss: 0.00003002
Iteration 12/1000 | Loss: 0.00002969
Iteration 13/1000 | Loss: 0.00002944
Iteration 14/1000 | Loss: 0.00002913
Iteration 15/1000 | Loss: 0.00002889
Iteration 16/1000 | Loss: 0.00002863
Iteration 17/1000 | Loss: 0.00002843
Iteration 18/1000 | Loss: 0.00002829
Iteration 19/1000 | Loss: 0.00002824
Iteration 20/1000 | Loss: 0.00002812
Iteration 21/1000 | Loss: 0.00002809
Iteration 22/1000 | Loss: 0.00002806
Iteration 23/1000 | Loss: 0.00002796
Iteration 24/1000 | Loss: 0.00002791
Iteration 25/1000 | Loss: 0.00002789
Iteration 26/1000 | Loss: 0.00002789
Iteration 27/1000 | Loss: 0.00002789
Iteration 28/1000 | Loss: 0.00002788
Iteration 29/1000 | Loss: 0.00002788
Iteration 30/1000 | Loss: 0.00002787
Iteration 31/1000 | Loss: 0.00002787
Iteration 32/1000 | Loss: 0.00002786
Iteration 33/1000 | Loss: 0.00002786
Iteration 34/1000 | Loss: 0.00002786
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002785
Iteration 37/1000 | Loss: 0.00002785
Iteration 38/1000 | Loss: 0.00002785
Iteration 39/1000 | Loss: 0.00002785
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002784
Iteration 42/1000 | Loss: 0.00002784
Iteration 43/1000 | Loss: 0.00002784
Iteration 44/1000 | Loss: 0.00002784
Iteration 45/1000 | Loss: 0.00002784
Iteration 46/1000 | Loss: 0.00002784
Iteration 47/1000 | Loss: 0.00002783
Iteration 48/1000 | Loss: 0.00002783
Iteration 49/1000 | Loss: 0.00002783
Iteration 50/1000 | Loss: 0.00002783
Iteration 51/1000 | Loss: 0.00002783
Iteration 52/1000 | Loss: 0.00002783
Iteration 53/1000 | Loss: 0.00002783
Iteration 54/1000 | Loss: 0.00002783
Iteration 55/1000 | Loss: 0.00002783
Iteration 56/1000 | Loss: 0.00002783
Iteration 57/1000 | Loss: 0.00002782
Iteration 58/1000 | Loss: 0.00002782
Iteration 59/1000 | Loss: 0.00002782
Iteration 60/1000 | Loss: 0.00002782
Iteration 61/1000 | Loss: 0.00002782
Iteration 62/1000 | Loss: 0.00002782
Iteration 63/1000 | Loss: 0.00002782
Iteration 64/1000 | Loss: 0.00002782
Iteration 65/1000 | Loss: 0.00002782
Iteration 66/1000 | Loss: 0.00002782
Iteration 67/1000 | Loss: 0.00002781
Iteration 68/1000 | Loss: 0.00002781
Iteration 69/1000 | Loss: 0.00002781
Iteration 70/1000 | Loss: 0.00002781
Iteration 71/1000 | Loss: 0.00002781
Iteration 72/1000 | Loss: 0.00002781
Iteration 73/1000 | Loss: 0.00002781
Iteration 74/1000 | Loss: 0.00002781
Iteration 75/1000 | Loss: 0.00002781
Iteration 76/1000 | Loss: 0.00002781
Iteration 77/1000 | Loss: 0.00002781
Iteration 78/1000 | Loss: 0.00002781
Iteration 79/1000 | Loss: 0.00002781
Iteration 80/1000 | Loss: 0.00002781
Iteration 81/1000 | Loss: 0.00002780
Iteration 82/1000 | Loss: 0.00002780
Iteration 83/1000 | Loss: 0.00002780
Iteration 84/1000 | Loss: 0.00002780
Iteration 85/1000 | Loss: 0.00002780
Iteration 86/1000 | Loss: 0.00002780
Iteration 87/1000 | Loss: 0.00002779
Iteration 88/1000 | Loss: 0.00002779
Iteration 89/1000 | Loss: 0.00002779
Iteration 90/1000 | Loss: 0.00002779
Iteration 91/1000 | Loss: 0.00002779
Iteration 92/1000 | Loss: 0.00002778
Iteration 93/1000 | Loss: 0.00002778
Iteration 94/1000 | Loss: 0.00002778
Iteration 95/1000 | Loss: 0.00002778
Iteration 96/1000 | Loss: 0.00002778
Iteration 97/1000 | Loss: 0.00002778
Iteration 98/1000 | Loss: 0.00002778
Iteration 99/1000 | Loss: 0.00002778
Iteration 100/1000 | Loss: 0.00002778
Iteration 101/1000 | Loss: 0.00002778
Iteration 102/1000 | Loss: 0.00002778
Iteration 103/1000 | Loss: 0.00002777
Iteration 104/1000 | Loss: 0.00002777
Iteration 105/1000 | Loss: 0.00002777
Iteration 106/1000 | Loss: 0.00002777
Iteration 107/1000 | Loss: 0.00002776
Iteration 108/1000 | Loss: 0.00002776
Iteration 109/1000 | Loss: 0.00002776
Iteration 110/1000 | Loss: 0.00002776
Iteration 111/1000 | Loss: 0.00002775
Iteration 112/1000 | Loss: 0.00002775
Iteration 113/1000 | Loss: 0.00002775
Iteration 114/1000 | Loss: 0.00002775
Iteration 115/1000 | Loss: 0.00002775
Iteration 116/1000 | Loss: 0.00002775
Iteration 117/1000 | Loss: 0.00002775
Iteration 118/1000 | Loss: 0.00002775
Iteration 119/1000 | Loss: 0.00002775
Iteration 120/1000 | Loss: 0.00002775
Iteration 121/1000 | Loss: 0.00002775
Iteration 122/1000 | Loss: 0.00002774
Iteration 123/1000 | Loss: 0.00002774
Iteration 124/1000 | Loss: 0.00002774
Iteration 125/1000 | Loss: 0.00002774
Iteration 126/1000 | Loss: 0.00002774
Iteration 127/1000 | Loss: 0.00002774
Iteration 128/1000 | Loss: 0.00002774
Iteration 129/1000 | Loss: 0.00002774
Iteration 130/1000 | Loss: 0.00002774
Iteration 131/1000 | Loss: 0.00002774
Iteration 132/1000 | Loss: 0.00002774
Iteration 133/1000 | Loss: 0.00002773
Iteration 134/1000 | Loss: 0.00002773
Iteration 135/1000 | Loss: 0.00002773
Iteration 136/1000 | Loss: 0.00002773
Iteration 137/1000 | Loss: 0.00002773
Iteration 138/1000 | Loss: 0.00002773
Iteration 139/1000 | Loss: 0.00002773
Iteration 140/1000 | Loss: 0.00002773
Iteration 141/1000 | Loss: 0.00002773
Iteration 142/1000 | Loss: 0.00002773
Iteration 143/1000 | Loss: 0.00002773
Iteration 144/1000 | Loss: 0.00002773
Iteration 145/1000 | Loss: 0.00002773
Iteration 146/1000 | Loss: 0.00002773
Iteration 147/1000 | Loss: 0.00002773
Iteration 148/1000 | Loss: 0.00002773
Iteration 149/1000 | Loss: 0.00002773
Iteration 150/1000 | Loss: 0.00002772
Iteration 151/1000 | Loss: 0.00002772
Iteration 152/1000 | Loss: 0.00002772
Iteration 153/1000 | Loss: 0.00002772
Iteration 154/1000 | Loss: 0.00002772
Iteration 155/1000 | Loss: 0.00002772
Iteration 156/1000 | Loss: 0.00002772
Iteration 157/1000 | Loss: 0.00002772
Iteration 158/1000 | Loss: 0.00002771
Iteration 159/1000 | Loss: 0.00002771
Iteration 160/1000 | Loss: 0.00002771
Iteration 161/1000 | Loss: 0.00002771
Iteration 162/1000 | Loss: 0.00002771
Iteration 163/1000 | Loss: 0.00002771
Iteration 164/1000 | Loss: 0.00002771
Iteration 165/1000 | Loss: 0.00002771
Iteration 166/1000 | Loss: 0.00002771
Iteration 167/1000 | Loss: 0.00002770
Iteration 168/1000 | Loss: 0.00002770
Iteration 169/1000 | Loss: 0.00002770
Iteration 170/1000 | Loss: 0.00002770
Iteration 171/1000 | Loss: 0.00002770
Iteration 172/1000 | Loss: 0.00002770
Iteration 173/1000 | Loss: 0.00002770
Iteration 174/1000 | Loss: 0.00002770
Iteration 175/1000 | Loss: 0.00002770
Iteration 176/1000 | Loss: 0.00002770
Iteration 177/1000 | Loss: 0.00002770
Iteration 178/1000 | Loss: 0.00002770
Iteration 179/1000 | Loss: 0.00002770
Iteration 180/1000 | Loss: 0.00002770
Iteration 181/1000 | Loss: 0.00002770
Iteration 182/1000 | Loss: 0.00002770
Iteration 183/1000 | Loss: 0.00002770
Iteration 184/1000 | Loss: 0.00002770
Iteration 185/1000 | Loss: 0.00002770
Iteration 186/1000 | Loss: 0.00002770
Iteration 187/1000 | Loss: 0.00002770
Iteration 188/1000 | Loss: 0.00002770
Iteration 189/1000 | Loss: 0.00002770
Iteration 190/1000 | Loss: 0.00002770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.7704261810868047e-05, 2.7704261810868047e-05, 2.7704261810868047e-05, 2.7704261810868047e-05, 2.7704261810868047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7704261810868047e-05

Optimization complete. Final v2v error: 4.455974102020264 mm

Highest mean error: 5.001727104187012 mm for frame 188

Lowest mean error: 3.922977924346924 mm for frame 35

Saving results

Total time: 52.76773977279663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799523
Iteration 2/25 | Loss: 0.00194810
Iteration 3/25 | Loss: 0.00146810
Iteration 4/25 | Loss: 0.00154480
Iteration 5/25 | Loss: 0.00136986
Iteration 6/25 | Loss: 0.00136591
Iteration 7/25 | Loss: 0.00130709
Iteration 8/25 | Loss: 0.00128703
Iteration 9/25 | Loss: 0.00127711
Iteration 10/25 | Loss: 0.00126413
Iteration 11/25 | Loss: 0.00126803
Iteration 12/25 | Loss: 0.00125833
Iteration 13/25 | Loss: 0.00126216
Iteration 14/25 | Loss: 0.00126046
Iteration 15/25 | Loss: 0.00125794
Iteration 16/25 | Loss: 0.00125733
Iteration 17/25 | Loss: 0.00125719
Iteration 18/25 | Loss: 0.00125719
Iteration 19/25 | Loss: 0.00125719
Iteration 20/25 | Loss: 0.00125718
Iteration 21/25 | Loss: 0.00125718
Iteration 22/25 | Loss: 0.00125718
Iteration 23/25 | Loss: 0.00125718
Iteration 24/25 | Loss: 0.00125717
Iteration 25/25 | Loss: 0.00125716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51722240
Iteration 2/25 | Loss: 0.00099207
Iteration 3/25 | Loss: 0.00099207
Iteration 4/25 | Loss: 0.00099206
Iteration 5/25 | Loss: 0.00099206
Iteration 6/25 | Loss: 0.00099206
Iteration 7/25 | Loss: 0.00099206
Iteration 8/25 | Loss: 0.00099206
Iteration 9/25 | Loss: 0.00099206
Iteration 10/25 | Loss: 0.00099206
Iteration 11/25 | Loss: 0.00099206
Iteration 12/25 | Loss: 0.00099206
Iteration 13/25 | Loss: 0.00099206
Iteration 14/25 | Loss: 0.00099206
Iteration 15/25 | Loss: 0.00099206
Iteration 16/25 | Loss: 0.00099206
Iteration 17/25 | Loss: 0.00099206
Iteration 18/25 | Loss: 0.00099206
Iteration 19/25 | Loss: 0.00099206
Iteration 20/25 | Loss: 0.00099206
Iteration 21/25 | Loss: 0.00099206
Iteration 22/25 | Loss: 0.00099206
Iteration 23/25 | Loss: 0.00099206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009920629672706127, 0.0009920629672706127, 0.0009920629672706127, 0.0009920629672706127, 0.0009920629672706127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009920629672706127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099206
Iteration 2/1000 | Loss: 0.00029604
Iteration 3/1000 | Loss: 0.00030233
Iteration 4/1000 | Loss: 0.00024390
Iteration 5/1000 | Loss: 0.00003902
Iteration 6/1000 | Loss: 0.00003312
Iteration 7/1000 | Loss: 0.00002940
Iteration 8/1000 | Loss: 0.00002666
Iteration 9/1000 | Loss: 0.00005464
Iteration 10/1000 | Loss: 0.00002989
Iteration 11/1000 | Loss: 0.00002631
Iteration 12/1000 | Loss: 0.00002403
Iteration 13/1000 | Loss: 0.00002342
Iteration 14/1000 | Loss: 0.00002318
Iteration 15/1000 | Loss: 0.00002280
Iteration 16/1000 | Loss: 0.00002258
Iteration 17/1000 | Loss: 0.00002234
Iteration 18/1000 | Loss: 0.00042633
Iteration 19/1000 | Loss: 0.00003191
Iteration 20/1000 | Loss: 0.00002573
Iteration 21/1000 | Loss: 0.00002456
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002329
Iteration 25/1000 | Loss: 0.00002303
Iteration 26/1000 | Loss: 0.00002283
Iteration 27/1000 | Loss: 0.00002260
Iteration 28/1000 | Loss: 0.00002188
Iteration 29/1000 | Loss: 0.00002134
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002090
Iteration 34/1000 | Loss: 0.00002089
Iteration 35/1000 | Loss: 0.00002089
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002085
Iteration 38/1000 | Loss: 0.00002085
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002083
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002079
Iteration 48/1000 | Loss: 0.00002078
Iteration 49/1000 | Loss: 0.00002078
Iteration 50/1000 | Loss: 0.00002078
Iteration 51/1000 | Loss: 0.00002078
Iteration 52/1000 | Loss: 0.00002078
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002076
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002071
Iteration 65/1000 | Loss: 0.00002071
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002071
Iteration 68/1000 | Loss: 0.00002071
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002070
Iteration 72/1000 | Loss: 0.00002070
Iteration 73/1000 | Loss: 0.00002069
Iteration 74/1000 | Loss: 0.00002069
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002069
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002068
Iteration 82/1000 | Loss: 0.00002068
Iteration 83/1000 | Loss: 0.00002068
Iteration 84/1000 | Loss: 0.00002068
Iteration 85/1000 | Loss: 0.00002068
Iteration 86/1000 | Loss: 0.00002067
Iteration 87/1000 | Loss: 0.00002067
Iteration 88/1000 | Loss: 0.00002067
Iteration 89/1000 | Loss: 0.00002067
Iteration 90/1000 | Loss: 0.00002067
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002065
Iteration 95/1000 | Loss: 0.00002065
Iteration 96/1000 | Loss: 0.00002065
Iteration 97/1000 | Loss: 0.00002065
Iteration 98/1000 | Loss: 0.00002065
Iteration 99/1000 | Loss: 0.00002064
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002064
Iteration 102/1000 | Loss: 0.00002064
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002061
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002059
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002058
Iteration 122/1000 | Loss: 0.00002058
Iteration 123/1000 | Loss: 0.00002058
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002058
Iteration 126/1000 | Loss: 0.00002057
Iteration 127/1000 | Loss: 0.00002057
Iteration 128/1000 | Loss: 0.00002057
Iteration 129/1000 | Loss: 0.00002057
Iteration 130/1000 | Loss: 0.00002057
Iteration 131/1000 | Loss: 0.00002057
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002056
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002055
Iteration 140/1000 | Loss: 0.00002055
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002055
Iteration 145/1000 | Loss: 0.00002055
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002054
Iteration 148/1000 | Loss: 0.00002054
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002053
Iteration 154/1000 | Loss: 0.00002053
Iteration 155/1000 | Loss: 0.00002053
Iteration 156/1000 | Loss: 0.00002053
Iteration 157/1000 | Loss: 0.00002052
Iteration 158/1000 | Loss: 0.00002052
Iteration 159/1000 | Loss: 0.00002052
Iteration 160/1000 | Loss: 0.00002052
Iteration 161/1000 | Loss: 0.00002052
Iteration 162/1000 | Loss: 0.00002052
Iteration 163/1000 | Loss: 0.00002052
Iteration 164/1000 | Loss: 0.00002052
Iteration 165/1000 | Loss: 0.00002052
Iteration 166/1000 | Loss: 0.00002052
Iteration 167/1000 | Loss: 0.00002051
Iteration 168/1000 | Loss: 0.00002051
Iteration 169/1000 | Loss: 0.00002051
Iteration 170/1000 | Loss: 0.00002051
Iteration 171/1000 | Loss: 0.00002051
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002050
Iteration 174/1000 | Loss: 0.00002050
Iteration 175/1000 | Loss: 0.00002050
Iteration 176/1000 | Loss: 0.00002050
Iteration 177/1000 | Loss: 0.00002049
Iteration 178/1000 | Loss: 0.00002049
Iteration 179/1000 | Loss: 0.00002049
Iteration 180/1000 | Loss: 0.00002049
Iteration 181/1000 | Loss: 0.00002049
Iteration 182/1000 | Loss: 0.00002048
Iteration 183/1000 | Loss: 0.00002048
Iteration 184/1000 | Loss: 0.00002048
Iteration 185/1000 | Loss: 0.00002048
Iteration 186/1000 | Loss: 0.00002048
Iteration 187/1000 | Loss: 0.00002048
Iteration 188/1000 | Loss: 0.00002047
Iteration 189/1000 | Loss: 0.00002047
Iteration 190/1000 | Loss: 0.00002047
Iteration 191/1000 | Loss: 0.00002047
Iteration 192/1000 | Loss: 0.00002047
Iteration 193/1000 | Loss: 0.00002047
Iteration 194/1000 | Loss: 0.00002047
Iteration 195/1000 | Loss: 0.00002047
Iteration 196/1000 | Loss: 0.00002047
Iteration 197/1000 | Loss: 0.00002047
Iteration 198/1000 | Loss: 0.00002047
Iteration 199/1000 | Loss: 0.00002046
Iteration 200/1000 | Loss: 0.00002046
Iteration 201/1000 | Loss: 0.00002046
Iteration 202/1000 | Loss: 0.00002046
Iteration 203/1000 | Loss: 0.00002046
Iteration 204/1000 | Loss: 0.00002046
Iteration 205/1000 | Loss: 0.00002046
Iteration 206/1000 | Loss: 0.00002046
Iteration 207/1000 | Loss: 0.00002046
Iteration 208/1000 | Loss: 0.00002046
Iteration 209/1000 | Loss: 0.00002046
Iteration 210/1000 | Loss: 0.00002046
Iteration 211/1000 | Loss: 0.00002046
Iteration 212/1000 | Loss: 0.00002046
Iteration 213/1000 | Loss: 0.00002046
Iteration 214/1000 | Loss: 0.00002046
Iteration 215/1000 | Loss: 0.00002046
Iteration 216/1000 | Loss: 0.00002046
Iteration 217/1000 | Loss: 0.00002046
Iteration 218/1000 | Loss: 0.00002046
Iteration 219/1000 | Loss: 0.00002046
Iteration 220/1000 | Loss: 0.00002046
Iteration 221/1000 | Loss: 0.00002046
Iteration 222/1000 | Loss: 0.00002046
Iteration 223/1000 | Loss: 0.00002046
Iteration 224/1000 | Loss: 0.00002046
Iteration 225/1000 | Loss: 0.00002046
Iteration 226/1000 | Loss: 0.00002046
Iteration 227/1000 | Loss: 0.00002046
Iteration 228/1000 | Loss: 0.00002046
Iteration 229/1000 | Loss: 0.00002046
Iteration 230/1000 | Loss: 0.00002046
Iteration 231/1000 | Loss: 0.00002046
Iteration 232/1000 | Loss: 0.00002046
Iteration 233/1000 | Loss: 0.00002046
Iteration 234/1000 | Loss: 0.00002046
Iteration 235/1000 | Loss: 0.00002046
Iteration 236/1000 | Loss: 0.00002046
Iteration 237/1000 | Loss: 0.00002046
Iteration 238/1000 | Loss: 0.00002046
Iteration 239/1000 | Loss: 0.00002046
Iteration 240/1000 | Loss: 0.00002046
Iteration 241/1000 | Loss: 0.00002046
Iteration 242/1000 | Loss: 0.00002046
Iteration 243/1000 | Loss: 0.00002046
Iteration 244/1000 | Loss: 0.00002046
Iteration 245/1000 | Loss: 0.00002046
Iteration 246/1000 | Loss: 0.00002046
Iteration 247/1000 | Loss: 0.00002046
Iteration 248/1000 | Loss: 0.00002046
Iteration 249/1000 | Loss: 0.00002046
Iteration 250/1000 | Loss: 0.00002046
Iteration 251/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.046019108092878e-05, 2.046019108092878e-05, 2.046019108092878e-05, 2.046019108092878e-05, 2.046019108092878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.046019108092878e-05

Optimization complete. Final v2v error: 3.6914608478546143 mm

Highest mean error: 4.770749092102051 mm for frame 52

Lowest mean error: 2.7626402378082275 mm for frame 3

Saving results

Total time: 88.16063499450684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929585
Iteration 2/25 | Loss: 0.00292954
Iteration 3/25 | Loss: 0.00186082
Iteration 4/25 | Loss: 0.00161239
Iteration 5/25 | Loss: 0.00161316
Iteration 6/25 | Loss: 0.00160942
Iteration 7/25 | Loss: 0.00153034
Iteration 8/25 | Loss: 0.00154994
Iteration 9/25 | Loss: 0.00153786
Iteration 10/25 | Loss: 0.00155340
Iteration 11/25 | Loss: 0.00153843
Iteration 12/25 | Loss: 0.00151812
Iteration 13/25 | Loss: 0.00149754
Iteration 14/25 | Loss: 0.00147625
Iteration 15/25 | Loss: 0.00145390
Iteration 16/25 | Loss: 0.00143090
Iteration 17/25 | Loss: 0.00141991
Iteration 18/25 | Loss: 0.00140404
Iteration 19/25 | Loss: 0.00139798
Iteration 20/25 | Loss: 0.00139077
Iteration 21/25 | Loss: 0.00139349
Iteration 22/25 | Loss: 0.00139146
Iteration 23/25 | Loss: 0.00138640
Iteration 24/25 | Loss: 0.00138956
Iteration 25/25 | Loss: 0.00139128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70797515
Iteration 2/25 | Loss: 0.00299922
Iteration 3/25 | Loss: 0.00299909
Iteration 4/25 | Loss: 0.00299915
Iteration 5/25 | Loss: 0.00299197
Iteration 6/25 | Loss: 0.00299197
Iteration 7/25 | Loss: 0.00299197
Iteration 8/25 | Loss: 0.00299197
Iteration 9/25 | Loss: 0.00299197
Iteration 10/25 | Loss: 0.00299197
Iteration 11/25 | Loss: 0.00299197
Iteration 12/25 | Loss: 0.00299197
Iteration 13/25 | Loss: 0.00299197
Iteration 14/25 | Loss: 0.00299197
Iteration 15/25 | Loss: 0.00299197
Iteration 16/25 | Loss: 0.00299197
Iteration 17/25 | Loss: 0.00299197
Iteration 18/25 | Loss: 0.00299197
Iteration 19/25 | Loss: 0.00299197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029919687658548355, 0.0029919687658548355, 0.0029919687658548355, 0.0029919687658548355, 0.0029919687658548355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029919687658548355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299197
Iteration 2/1000 | Loss: 0.00031393
Iteration 3/1000 | Loss: 0.00033748
Iteration 4/1000 | Loss: 0.00036718
Iteration 5/1000 | Loss: 0.00040483
Iteration 6/1000 | Loss: 0.00075962
Iteration 7/1000 | Loss: 0.00134451
Iteration 8/1000 | Loss: 0.00203958
Iteration 9/1000 | Loss: 0.00020612
Iteration 10/1000 | Loss: 0.00015823
Iteration 11/1000 | Loss: 0.00014710
Iteration 12/1000 | Loss: 0.00259467
Iteration 13/1000 | Loss: 0.00022913
Iteration 14/1000 | Loss: 0.00078210
Iteration 15/1000 | Loss: 0.00033813
Iteration 16/1000 | Loss: 0.00049182
Iteration 17/1000 | Loss: 0.00013046
Iteration 18/1000 | Loss: 0.00012079
Iteration 19/1000 | Loss: 0.00234303
Iteration 20/1000 | Loss: 0.00843126
Iteration 21/1000 | Loss: 0.00294054
Iteration 22/1000 | Loss: 0.00629029
Iteration 23/1000 | Loss: 0.00201375
Iteration 24/1000 | Loss: 0.00115069
Iteration 25/1000 | Loss: 0.00015721
Iteration 26/1000 | Loss: 0.00103676
Iteration 27/1000 | Loss: 0.00044835
Iteration 28/1000 | Loss: 0.00021232
Iteration 29/1000 | Loss: 0.00118509
Iteration 30/1000 | Loss: 0.00055678
Iteration 31/1000 | Loss: 0.00091281
Iteration 32/1000 | Loss: 0.00025984
Iteration 33/1000 | Loss: 0.00021064
Iteration 34/1000 | Loss: 0.00013981
Iteration 35/1000 | Loss: 0.00017503
Iteration 36/1000 | Loss: 0.00014730
Iteration 37/1000 | Loss: 0.00022363
Iteration 38/1000 | Loss: 0.00019573
Iteration 39/1000 | Loss: 0.00018553
Iteration 40/1000 | Loss: 0.00037749
Iteration 41/1000 | Loss: 0.00010023
Iteration 42/1000 | Loss: 0.00017774
Iteration 43/1000 | Loss: 0.00024272
Iteration 44/1000 | Loss: 0.00020386
Iteration 45/1000 | Loss: 0.00020508
Iteration 46/1000 | Loss: 0.00034957
Iteration 47/1000 | Loss: 0.00020505
Iteration 48/1000 | Loss: 0.00051585
Iteration 49/1000 | Loss: 0.00014691
Iteration 50/1000 | Loss: 0.00007856
Iteration 51/1000 | Loss: 0.00056259
Iteration 52/1000 | Loss: 0.00060843
Iteration 53/1000 | Loss: 0.00049245
Iteration 54/1000 | Loss: 0.00053113
Iteration 55/1000 | Loss: 0.00007349
Iteration 56/1000 | Loss: 0.00006875
Iteration 57/1000 | Loss: 0.00006673
Iteration 58/1000 | Loss: 0.00057077
Iteration 59/1000 | Loss: 0.00079149
Iteration 60/1000 | Loss: 0.00140670
Iteration 61/1000 | Loss: 0.00030803
Iteration 62/1000 | Loss: 0.00006766
Iteration 63/1000 | Loss: 0.00006284
Iteration 64/1000 | Loss: 0.00005925
Iteration 65/1000 | Loss: 0.00079757
Iteration 66/1000 | Loss: 0.00071345
Iteration 67/1000 | Loss: 0.00115061
Iteration 68/1000 | Loss: 0.00071461
Iteration 69/1000 | Loss: 0.00058406
Iteration 70/1000 | Loss: 0.00050132
Iteration 71/1000 | Loss: 0.00087312
Iteration 72/1000 | Loss: 0.00022072
Iteration 73/1000 | Loss: 0.00076117
Iteration 74/1000 | Loss: 0.00100451
Iteration 75/1000 | Loss: 0.00026816
Iteration 76/1000 | Loss: 0.00055945
Iteration 77/1000 | Loss: 0.00053243
Iteration 78/1000 | Loss: 0.00034364
Iteration 79/1000 | Loss: 0.00005889
Iteration 80/1000 | Loss: 0.00005337
Iteration 81/1000 | Loss: 0.00147009
Iteration 82/1000 | Loss: 0.00109297
Iteration 83/1000 | Loss: 0.00094852
Iteration 84/1000 | Loss: 0.00073312
Iteration 85/1000 | Loss: 0.00030012
Iteration 86/1000 | Loss: 0.00004930
Iteration 87/1000 | Loss: 0.00039278
Iteration 88/1000 | Loss: 0.00041901
Iteration 89/1000 | Loss: 0.00006080
Iteration 90/1000 | Loss: 0.00004714
Iteration 91/1000 | Loss: 0.00080702
Iteration 92/1000 | Loss: 0.00175404
Iteration 93/1000 | Loss: 0.00212496
Iteration 94/1000 | Loss: 0.00195640
Iteration 95/1000 | Loss: 0.00172127
Iteration 96/1000 | Loss: 0.00180589
Iteration 97/1000 | Loss: 0.00161922
Iteration 98/1000 | Loss: 0.00089924
Iteration 99/1000 | Loss: 0.00091873
Iteration 100/1000 | Loss: 0.00008474
Iteration 101/1000 | Loss: 0.00071384
Iteration 102/1000 | Loss: 0.00006193
Iteration 103/1000 | Loss: 0.00004908
Iteration 104/1000 | Loss: 0.00004058
Iteration 105/1000 | Loss: 0.00034244
Iteration 106/1000 | Loss: 0.00099879
Iteration 107/1000 | Loss: 0.00037308
Iteration 108/1000 | Loss: 0.00006162
Iteration 109/1000 | Loss: 0.00004331
Iteration 110/1000 | Loss: 0.00003487
Iteration 111/1000 | Loss: 0.00038180
Iteration 112/1000 | Loss: 0.00004254
Iteration 113/1000 | Loss: 0.00002806
Iteration 114/1000 | Loss: 0.00002593
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002350
Iteration 117/1000 | Loss: 0.00002276
Iteration 118/1000 | Loss: 0.00002221
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002146
Iteration 121/1000 | Loss: 0.00002112
Iteration 122/1000 | Loss: 0.00002077
Iteration 123/1000 | Loss: 0.00002042
Iteration 124/1000 | Loss: 0.00002017
Iteration 125/1000 | Loss: 0.00002012
Iteration 126/1000 | Loss: 0.00002009
Iteration 127/1000 | Loss: 0.00001987
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001982
Iteration 131/1000 | Loss: 0.00004306
Iteration 132/1000 | Loss: 0.00003620
Iteration 133/1000 | Loss: 0.00004455
Iteration 134/1000 | Loss: 0.00003234
Iteration 135/1000 | Loss: 0.00004059
Iteration 136/1000 | Loss: 0.00003159
Iteration 137/1000 | Loss: 0.00002120
Iteration 138/1000 | Loss: 0.00002028
Iteration 139/1000 | Loss: 0.00003325
Iteration 140/1000 | Loss: 0.00002453
Iteration 141/1000 | Loss: 0.00003527
Iteration 142/1000 | Loss: 0.00002434
Iteration 143/1000 | Loss: 0.00003464
Iteration 144/1000 | Loss: 0.00002352
Iteration 145/1000 | Loss: 0.00003388
Iteration 146/1000 | Loss: 0.00002178
Iteration 147/1000 | Loss: 0.00003417
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002125
Iteration 150/1000 | Loss: 0.00002016
Iteration 151/1000 | Loss: 0.00001983
Iteration 152/1000 | Loss: 0.00001966
Iteration 153/1000 | Loss: 0.00001950
Iteration 154/1000 | Loss: 0.00001946
Iteration 155/1000 | Loss: 0.00001943
Iteration 156/1000 | Loss: 0.00001935
Iteration 157/1000 | Loss: 0.00001934
Iteration 158/1000 | Loss: 0.00001934
Iteration 159/1000 | Loss: 0.00001933
Iteration 160/1000 | Loss: 0.00001933
Iteration 161/1000 | Loss: 0.00001933
Iteration 162/1000 | Loss: 0.00001932
Iteration 163/1000 | Loss: 0.00001932
Iteration 164/1000 | Loss: 0.00001931
Iteration 165/1000 | Loss: 0.00001931
Iteration 166/1000 | Loss: 0.00001930
Iteration 167/1000 | Loss: 0.00001930
Iteration 168/1000 | Loss: 0.00001930
Iteration 169/1000 | Loss: 0.00001929
Iteration 170/1000 | Loss: 0.00001929
Iteration 171/1000 | Loss: 0.00001928
Iteration 172/1000 | Loss: 0.00001928
Iteration 173/1000 | Loss: 0.00001928
Iteration 174/1000 | Loss: 0.00001928
Iteration 175/1000 | Loss: 0.00001928
Iteration 176/1000 | Loss: 0.00001928
Iteration 177/1000 | Loss: 0.00001927
Iteration 178/1000 | Loss: 0.00001927
Iteration 179/1000 | Loss: 0.00001926
Iteration 180/1000 | Loss: 0.00001926
Iteration 181/1000 | Loss: 0.00001926
Iteration 182/1000 | Loss: 0.00001926
Iteration 183/1000 | Loss: 0.00001925
Iteration 184/1000 | Loss: 0.00001925
Iteration 185/1000 | Loss: 0.00001925
Iteration 186/1000 | Loss: 0.00001925
Iteration 187/1000 | Loss: 0.00001924
Iteration 188/1000 | Loss: 0.00001924
Iteration 189/1000 | Loss: 0.00001924
Iteration 190/1000 | Loss: 0.00001924
Iteration 191/1000 | Loss: 0.00001923
Iteration 192/1000 | Loss: 0.00001923
Iteration 193/1000 | Loss: 0.00001923
Iteration 194/1000 | Loss: 0.00001923
Iteration 195/1000 | Loss: 0.00001923
Iteration 196/1000 | Loss: 0.00001923
Iteration 197/1000 | Loss: 0.00001923
Iteration 198/1000 | Loss: 0.00001923
Iteration 199/1000 | Loss: 0.00001923
Iteration 200/1000 | Loss: 0.00001923
Iteration 201/1000 | Loss: 0.00001923
Iteration 202/1000 | Loss: 0.00001923
Iteration 203/1000 | Loss: 0.00001923
Iteration 204/1000 | Loss: 0.00001923
Iteration 205/1000 | Loss: 0.00001923
Iteration 206/1000 | Loss: 0.00001923
Iteration 207/1000 | Loss: 0.00001923
Iteration 208/1000 | Loss: 0.00001923
Iteration 209/1000 | Loss: 0.00001923
Iteration 210/1000 | Loss: 0.00001923
Iteration 211/1000 | Loss: 0.00001923
Iteration 212/1000 | Loss: 0.00001923
Iteration 213/1000 | Loss: 0.00001923
Iteration 214/1000 | Loss: 0.00001923
Iteration 215/1000 | Loss: 0.00001923
Iteration 216/1000 | Loss: 0.00001923
Iteration 217/1000 | Loss: 0.00001923
Iteration 218/1000 | Loss: 0.00001923
Iteration 219/1000 | Loss: 0.00001923
Iteration 220/1000 | Loss: 0.00001923
Iteration 221/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.9228100427426398e-05, 1.9228100427426398e-05, 1.9228100427426398e-05, 1.9228100427426398e-05, 1.9228100427426398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9228100427426398e-05

Optimization complete. Final v2v error: 3.369485378265381 mm

Highest mean error: 10.919315338134766 mm for frame 46

Lowest mean error: 2.762483596801758 mm for frame 160

Saving results

Total time: 269.5425353050232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528414
Iteration 2/25 | Loss: 0.00133838
Iteration 3/25 | Loss: 0.00124432
Iteration 4/25 | Loss: 0.00122529
Iteration 5/25 | Loss: 0.00121866
Iteration 6/25 | Loss: 0.00121739
Iteration 7/25 | Loss: 0.00121715
Iteration 8/25 | Loss: 0.00121715
Iteration 9/25 | Loss: 0.00121715
Iteration 10/25 | Loss: 0.00121715
Iteration 11/25 | Loss: 0.00121715
Iteration 12/25 | Loss: 0.00121715
Iteration 13/25 | Loss: 0.00121715
Iteration 14/25 | Loss: 0.00121715
Iteration 15/25 | Loss: 0.00121715
Iteration 16/25 | Loss: 0.00121715
Iteration 17/25 | Loss: 0.00121715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012171549024060369, 0.0012171549024060369, 0.0012171549024060369, 0.0012171549024060369, 0.0012171549024060369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012171549024060369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36607873
Iteration 2/25 | Loss: 0.00108152
Iteration 3/25 | Loss: 0.00108152
Iteration 4/25 | Loss: 0.00108152
Iteration 5/25 | Loss: 0.00108152
Iteration 6/25 | Loss: 0.00108152
Iteration 7/25 | Loss: 0.00108152
Iteration 8/25 | Loss: 0.00108152
Iteration 9/25 | Loss: 0.00108152
Iteration 10/25 | Loss: 0.00108152
Iteration 11/25 | Loss: 0.00108152
Iteration 12/25 | Loss: 0.00108152
Iteration 13/25 | Loss: 0.00108152
Iteration 14/25 | Loss: 0.00108152
Iteration 15/25 | Loss: 0.00108152
Iteration 16/25 | Loss: 0.00108152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010815166169777513, 0.0010815166169777513, 0.0010815166169777513, 0.0010815166169777513, 0.0010815166169777513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010815166169777513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108152
Iteration 2/1000 | Loss: 0.00005189
Iteration 3/1000 | Loss: 0.00003621
Iteration 4/1000 | Loss: 0.00002967
Iteration 5/1000 | Loss: 0.00002752
Iteration 6/1000 | Loss: 0.00002638
Iteration 7/1000 | Loss: 0.00002553
Iteration 8/1000 | Loss: 0.00002494
Iteration 9/1000 | Loss: 0.00002453
Iteration 10/1000 | Loss: 0.00002417
Iteration 11/1000 | Loss: 0.00002389
Iteration 12/1000 | Loss: 0.00002370
Iteration 13/1000 | Loss: 0.00002350
Iteration 14/1000 | Loss: 0.00002341
Iteration 15/1000 | Loss: 0.00002333
Iteration 16/1000 | Loss: 0.00002327
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002323
Iteration 19/1000 | Loss: 0.00002322
Iteration 20/1000 | Loss: 0.00002321
Iteration 21/1000 | Loss: 0.00002320
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002316
Iteration 24/1000 | Loss: 0.00002315
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002310
Iteration 27/1000 | Loss: 0.00002309
Iteration 28/1000 | Loss: 0.00002308
Iteration 29/1000 | Loss: 0.00002305
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002304
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002296
Iteration 34/1000 | Loss: 0.00002295
Iteration 35/1000 | Loss: 0.00002294
Iteration 36/1000 | Loss: 0.00002294
Iteration 37/1000 | Loss: 0.00002294
Iteration 38/1000 | Loss: 0.00002294
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00002294
Iteration 41/1000 | Loss: 0.00002292
Iteration 42/1000 | Loss: 0.00002292
Iteration 43/1000 | Loss: 0.00002291
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002289
Iteration 46/1000 | Loss: 0.00002289
Iteration 47/1000 | Loss: 0.00002288
Iteration 48/1000 | Loss: 0.00002288
Iteration 49/1000 | Loss: 0.00002288
Iteration 50/1000 | Loss: 0.00002288
Iteration 51/1000 | Loss: 0.00002288
Iteration 52/1000 | Loss: 0.00002288
Iteration 53/1000 | Loss: 0.00002288
Iteration 54/1000 | Loss: 0.00002288
Iteration 55/1000 | Loss: 0.00002287
Iteration 56/1000 | Loss: 0.00002287
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002286
Iteration 59/1000 | Loss: 0.00002286
Iteration 60/1000 | Loss: 0.00002285
Iteration 61/1000 | Loss: 0.00002285
Iteration 62/1000 | Loss: 0.00002284
Iteration 63/1000 | Loss: 0.00002283
Iteration 64/1000 | Loss: 0.00002283
Iteration 65/1000 | Loss: 0.00002283
Iteration 66/1000 | Loss: 0.00002282
Iteration 67/1000 | Loss: 0.00002282
Iteration 68/1000 | Loss: 0.00002282
Iteration 69/1000 | Loss: 0.00002281
Iteration 70/1000 | Loss: 0.00002281
Iteration 71/1000 | Loss: 0.00002281
Iteration 72/1000 | Loss: 0.00002280
Iteration 73/1000 | Loss: 0.00002280
Iteration 74/1000 | Loss: 0.00002280
Iteration 75/1000 | Loss: 0.00002279
Iteration 76/1000 | Loss: 0.00002279
Iteration 77/1000 | Loss: 0.00002279
Iteration 78/1000 | Loss: 0.00002279
Iteration 79/1000 | Loss: 0.00002278
Iteration 80/1000 | Loss: 0.00002278
Iteration 81/1000 | Loss: 0.00002278
Iteration 82/1000 | Loss: 0.00002278
Iteration 83/1000 | Loss: 0.00002277
Iteration 84/1000 | Loss: 0.00002277
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00002277
Iteration 87/1000 | Loss: 0.00002277
Iteration 88/1000 | Loss: 0.00002276
Iteration 89/1000 | Loss: 0.00002276
Iteration 90/1000 | Loss: 0.00002276
Iteration 91/1000 | Loss: 0.00002276
Iteration 92/1000 | Loss: 0.00002276
Iteration 93/1000 | Loss: 0.00002276
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00002276
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002276
Iteration 100/1000 | Loss: 0.00002276
Iteration 101/1000 | Loss: 0.00002275
Iteration 102/1000 | Loss: 0.00002275
Iteration 103/1000 | Loss: 0.00002275
Iteration 104/1000 | Loss: 0.00002275
Iteration 105/1000 | Loss: 0.00002275
Iteration 106/1000 | Loss: 0.00002275
Iteration 107/1000 | Loss: 0.00002275
Iteration 108/1000 | Loss: 0.00002275
Iteration 109/1000 | Loss: 0.00002275
Iteration 110/1000 | Loss: 0.00002274
Iteration 111/1000 | Loss: 0.00002274
Iteration 112/1000 | Loss: 0.00002274
Iteration 113/1000 | Loss: 0.00002274
Iteration 114/1000 | Loss: 0.00002274
Iteration 115/1000 | Loss: 0.00002274
Iteration 116/1000 | Loss: 0.00002274
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002273
Iteration 120/1000 | Loss: 0.00002273
Iteration 121/1000 | Loss: 0.00002273
Iteration 122/1000 | Loss: 0.00002273
Iteration 123/1000 | Loss: 0.00002273
Iteration 124/1000 | Loss: 0.00002273
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002273
Iteration 127/1000 | Loss: 0.00002273
Iteration 128/1000 | Loss: 0.00002273
Iteration 129/1000 | Loss: 0.00002272
Iteration 130/1000 | Loss: 0.00002272
Iteration 131/1000 | Loss: 0.00002272
Iteration 132/1000 | Loss: 0.00002272
Iteration 133/1000 | Loss: 0.00002272
Iteration 134/1000 | Loss: 0.00002272
Iteration 135/1000 | Loss: 0.00002272
Iteration 136/1000 | Loss: 0.00002272
Iteration 137/1000 | Loss: 0.00002272
Iteration 138/1000 | Loss: 0.00002272
Iteration 139/1000 | Loss: 0.00002272
Iteration 140/1000 | Loss: 0.00002272
Iteration 141/1000 | Loss: 0.00002272
Iteration 142/1000 | Loss: 0.00002271
Iteration 143/1000 | Loss: 0.00002271
Iteration 144/1000 | Loss: 0.00002271
Iteration 145/1000 | Loss: 0.00002271
Iteration 146/1000 | Loss: 0.00002271
Iteration 147/1000 | Loss: 0.00002271
Iteration 148/1000 | Loss: 0.00002271
Iteration 149/1000 | Loss: 0.00002271
Iteration 150/1000 | Loss: 0.00002271
Iteration 151/1000 | Loss: 0.00002271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.271118682983797e-05, 2.271118682983797e-05, 2.271118682983797e-05, 2.271118682983797e-05, 2.271118682983797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.271118682983797e-05

Optimization complete. Final v2v error: 3.7904860973358154 mm

Highest mean error: 4.523698806762695 mm for frame 110

Lowest mean error: 2.8416566848754883 mm for frame 8

Saving results

Total time: 41.960367918014526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920277
Iteration 2/25 | Loss: 0.00169682
Iteration 3/25 | Loss: 0.00134045
Iteration 4/25 | Loss: 0.00131163
Iteration 5/25 | Loss: 0.00130210
Iteration 6/25 | Loss: 0.00130081
Iteration 7/25 | Loss: 0.00130081
Iteration 8/25 | Loss: 0.00130081
Iteration 9/25 | Loss: 0.00130081
Iteration 10/25 | Loss: 0.00130081
Iteration 11/25 | Loss: 0.00130081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013008065288886428, 0.0013008065288886428, 0.0013008065288886428, 0.0013008065288886428, 0.0013008065288886428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013008065288886428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97243673
Iteration 2/25 | Loss: 0.00100230
Iteration 3/25 | Loss: 0.00100229
Iteration 4/25 | Loss: 0.00100229
Iteration 5/25 | Loss: 0.00100229
Iteration 6/25 | Loss: 0.00100229
Iteration 7/25 | Loss: 0.00100229
Iteration 8/25 | Loss: 0.00100229
Iteration 9/25 | Loss: 0.00100229
Iteration 10/25 | Loss: 0.00100229
Iteration 11/25 | Loss: 0.00100229
Iteration 12/25 | Loss: 0.00100229
Iteration 13/25 | Loss: 0.00100229
Iteration 14/25 | Loss: 0.00100229
Iteration 15/25 | Loss: 0.00100229
Iteration 16/25 | Loss: 0.00100229
Iteration 17/25 | Loss: 0.00100229
Iteration 18/25 | Loss: 0.00100229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010022919159382582, 0.0010022919159382582, 0.0010022919159382582, 0.0010022919159382582, 0.0010022919159382582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010022919159382582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100229
Iteration 2/1000 | Loss: 0.00007183
Iteration 3/1000 | Loss: 0.00004877
Iteration 4/1000 | Loss: 0.00003919
Iteration 5/1000 | Loss: 0.00003723
Iteration 6/1000 | Loss: 0.00003588
Iteration 7/1000 | Loss: 0.00003487
Iteration 8/1000 | Loss: 0.00003408
Iteration 9/1000 | Loss: 0.00003366
Iteration 10/1000 | Loss: 0.00003320
Iteration 11/1000 | Loss: 0.00003290
Iteration 12/1000 | Loss: 0.00003260
Iteration 13/1000 | Loss: 0.00003230
Iteration 14/1000 | Loss: 0.00003204
Iteration 15/1000 | Loss: 0.00003183
Iteration 16/1000 | Loss: 0.00003159
Iteration 17/1000 | Loss: 0.00003139
Iteration 18/1000 | Loss: 0.00003125
Iteration 19/1000 | Loss: 0.00003112
Iteration 20/1000 | Loss: 0.00003103
Iteration 21/1000 | Loss: 0.00003099
Iteration 22/1000 | Loss: 0.00003098
Iteration 23/1000 | Loss: 0.00003090
Iteration 24/1000 | Loss: 0.00003090
Iteration 25/1000 | Loss: 0.00003088
Iteration 26/1000 | Loss: 0.00003084
Iteration 27/1000 | Loss: 0.00003082
Iteration 28/1000 | Loss: 0.00003081
Iteration 29/1000 | Loss: 0.00003080
Iteration 30/1000 | Loss: 0.00003080
Iteration 31/1000 | Loss: 0.00003079
Iteration 32/1000 | Loss: 0.00003079
Iteration 33/1000 | Loss: 0.00003078
Iteration 34/1000 | Loss: 0.00003078
Iteration 35/1000 | Loss: 0.00003077
Iteration 36/1000 | Loss: 0.00003077
Iteration 37/1000 | Loss: 0.00003077
Iteration 38/1000 | Loss: 0.00003076
Iteration 39/1000 | Loss: 0.00003075
Iteration 40/1000 | Loss: 0.00003075
Iteration 41/1000 | Loss: 0.00003074
Iteration 42/1000 | Loss: 0.00003074
Iteration 43/1000 | Loss: 0.00003074
Iteration 44/1000 | Loss: 0.00003073
Iteration 45/1000 | Loss: 0.00003073
Iteration 46/1000 | Loss: 0.00003072
Iteration 47/1000 | Loss: 0.00003072
Iteration 48/1000 | Loss: 0.00003071
Iteration 49/1000 | Loss: 0.00003071
Iteration 50/1000 | Loss: 0.00003071
Iteration 51/1000 | Loss: 0.00003071
Iteration 52/1000 | Loss: 0.00003070
Iteration 53/1000 | Loss: 0.00003070
Iteration 54/1000 | Loss: 0.00003070
Iteration 55/1000 | Loss: 0.00003070
Iteration 56/1000 | Loss: 0.00003070
Iteration 57/1000 | Loss: 0.00003070
Iteration 58/1000 | Loss: 0.00003070
Iteration 59/1000 | Loss: 0.00003069
Iteration 60/1000 | Loss: 0.00003069
Iteration 61/1000 | Loss: 0.00003069
Iteration 62/1000 | Loss: 0.00003069
Iteration 63/1000 | Loss: 0.00003069
Iteration 64/1000 | Loss: 0.00003068
Iteration 65/1000 | Loss: 0.00003068
Iteration 66/1000 | Loss: 0.00003068
Iteration 67/1000 | Loss: 0.00003068
Iteration 68/1000 | Loss: 0.00003068
Iteration 69/1000 | Loss: 0.00003067
Iteration 70/1000 | Loss: 0.00003067
Iteration 71/1000 | Loss: 0.00003067
Iteration 72/1000 | Loss: 0.00003067
Iteration 73/1000 | Loss: 0.00003066
Iteration 74/1000 | Loss: 0.00003066
Iteration 75/1000 | Loss: 0.00003066
Iteration 76/1000 | Loss: 0.00003066
Iteration 77/1000 | Loss: 0.00003066
Iteration 78/1000 | Loss: 0.00003066
Iteration 79/1000 | Loss: 0.00003066
Iteration 80/1000 | Loss: 0.00003066
Iteration 81/1000 | Loss: 0.00003066
Iteration 82/1000 | Loss: 0.00003066
Iteration 83/1000 | Loss: 0.00003065
Iteration 84/1000 | Loss: 0.00003065
Iteration 85/1000 | Loss: 0.00003065
Iteration 86/1000 | Loss: 0.00003065
Iteration 87/1000 | Loss: 0.00003065
Iteration 88/1000 | Loss: 0.00003065
Iteration 89/1000 | Loss: 0.00003065
Iteration 90/1000 | Loss: 0.00003065
Iteration 91/1000 | Loss: 0.00003065
Iteration 92/1000 | Loss: 0.00003065
Iteration 93/1000 | Loss: 0.00003064
Iteration 94/1000 | Loss: 0.00003064
Iteration 95/1000 | Loss: 0.00003064
Iteration 96/1000 | Loss: 0.00003064
Iteration 97/1000 | Loss: 0.00003064
Iteration 98/1000 | Loss: 0.00003064
Iteration 99/1000 | Loss: 0.00003064
Iteration 100/1000 | Loss: 0.00003064
Iteration 101/1000 | Loss: 0.00003064
Iteration 102/1000 | Loss: 0.00003064
Iteration 103/1000 | Loss: 0.00003064
Iteration 104/1000 | Loss: 0.00003063
Iteration 105/1000 | Loss: 0.00003063
Iteration 106/1000 | Loss: 0.00003063
Iteration 107/1000 | Loss: 0.00003063
Iteration 108/1000 | Loss: 0.00003063
Iteration 109/1000 | Loss: 0.00003062
Iteration 110/1000 | Loss: 0.00003062
Iteration 111/1000 | Loss: 0.00003062
Iteration 112/1000 | Loss: 0.00003062
Iteration 113/1000 | Loss: 0.00003062
Iteration 114/1000 | Loss: 0.00003062
Iteration 115/1000 | Loss: 0.00003062
Iteration 116/1000 | Loss: 0.00003062
Iteration 117/1000 | Loss: 0.00003062
Iteration 118/1000 | Loss: 0.00003061
Iteration 119/1000 | Loss: 0.00003061
Iteration 120/1000 | Loss: 0.00003061
Iteration 121/1000 | Loss: 0.00003061
Iteration 122/1000 | Loss: 0.00003061
Iteration 123/1000 | Loss: 0.00003061
Iteration 124/1000 | Loss: 0.00003061
Iteration 125/1000 | Loss: 0.00003061
Iteration 126/1000 | Loss: 0.00003061
Iteration 127/1000 | Loss: 0.00003061
Iteration 128/1000 | Loss: 0.00003061
Iteration 129/1000 | Loss: 0.00003061
Iteration 130/1000 | Loss: 0.00003061
Iteration 131/1000 | Loss: 0.00003061
Iteration 132/1000 | Loss: 0.00003061
Iteration 133/1000 | Loss: 0.00003061
Iteration 134/1000 | Loss: 0.00003061
Iteration 135/1000 | Loss: 0.00003061
Iteration 136/1000 | Loss: 0.00003061
Iteration 137/1000 | Loss: 0.00003061
Iteration 138/1000 | Loss: 0.00003061
Iteration 139/1000 | Loss: 0.00003061
Iteration 140/1000 | Loss: 0.00003061
Iteration 141/1000 | Loss: 0.00003061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.061241295654327e-05, 3.061241295654327e-05, 3.061241295654327e-05, 3.061241295654327e-05, 3.061241295654327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.061241295654327e-05

Optimization complete. Final v2v error: 4.532802104949951 mm

Highest mean error: 5.67724609375 mm for frame 118

Lowest mean error: 3.693052053451538 mm for frame 19

Saving results

Total time: 53.51111125946045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844319
Iteration 2/25 | Loss: 0.00121251
Iteration 3/25 | Loss: 0.00112896
Iteration 4/25 | Loss: 0.00111825
Iteration 5/25 | Loss: 0.00111441
Iteration 6/25 | Loss: 0.00111366
Iteration 7/25 | Loss: 0.00111366
Iteration 8/25 | Loss: 0.00111366
Iteration 9/25 | Loss: 0.00111366
Iteration 10/25 | Loss: 0.00111366
Iteration 11/25 | Loss: 0.00111366
Iteration 12/25 | Loss: 0.00111366
Iteration 13/25 | Loss: 0.00111366
Iteration 14/25 | Loss: 0.00111366
Iteration 15/25 | Loss: 0.00111366
Iteration 16/25 | Loss: 0.00111366
Iteration 17/25 | Loss: 0.00111366
Iteration 18/25 | Loss: 0.00111366
Iteration 19/25 | Loss: 0.00111366
Iteration 20/25 | Loss: 0.00111366
Iteration 21/25 | Loss: 0.00111366
Iteration 22/25 | Loss: 0.00111366
Iteration 23/25 | Loss: 0.00111366
Iteration 24/25 | Loss: 0.00111366
Iteration 25/25 | Loss: 0.00111366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69109666
Iteration 2/25 | Loss: 0.00086437
Iteration 3/25 | Loss: 0.00086437
Iteration 4/25 | Loss: 0.00086437
Iteration 5/25 | Loss: 0.00086437
Iteration 6/25 | Loss: 0.00086437
Iteration 7/25 | Loss: 0.00086437
Iteration 8/25 | Loss: 0.00086437
Iteration 9/25 | Loss: 0.00086437
Iteration 10/25 | Loss: 0.00086437
Iteration 11/25 | Loss: 0.00086437
Iteration 12/25 | Loss: 0.00086437
Iteration 13/25 | Loss: 0.00086437
Iteration 14/25 | Loss: 0.00086437
Iteration 15/25 | Loss: 0.00086437
Iteration 16/25 | Loss: 0.00086437
Iteration 17/25 | Loss: 0.00086437
Iteration 18/25 | Loss: 0.00086437
Iteration 19/25 | Loss: 0.00086437
Iteration 20/25 | Loss: 0.00086437
Iteration 21/25 | Loss: 0.00086437
Iteration 22/25 | Loss: 0.00086437
Iteration 23/25 | Loss: 0.00086437
Iteration 24/25 | Loss: 0.00086437
Iteration 25/25 | Loss: 0.00086437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086437
Iteration 2/1000 | Loss: 0.00001976
Iteration 3/1000 | Loss: 0.00001424
Iteration 4/1000 | Loss: 0.00001228
Iteration 5/1000 | Loss: 0.00001149
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001053
Iteration 8/1000 | Loss: 0.00001021
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00001006
Iteration 11/1000 | Loss: 0.00001004
Iteration 12/1000 | Loss: 0.00001000
Iteration 13/1000 | Loss: 0.00000998
Iteration 14/1000 | Loss: 0.00000993
Iteration 15/1000 | Loss: 0.00000993
Iteration 16/1000 | Loss: 0.00000986
Iteration 17/1000 | Loss: 0.00000968
Iteration 18/1000 | Loss: 0.00000963
Iteration 19/1000 | Loss: 0.00000959
Iteration 20/1000 | Loss: 0.00000959
Iteration 21/1000 | Loss: 0.00000958
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000949
Iteration 25/1000 | Loss: 0.00000948
Iteration 26/1000 | Loss: 0.00000947
Iteration 27/1000 | Loss: 0.00000947
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000946
Iteration 30/1000 | Loss: 0.00000945
Iteration 31/1000 | Loss: 0.00000944
Iteration 32/1000 | Loss: 0.00000944
Iteration 33/1000 | Loss: 0.00000944
Iteration 34/1000 | Loss: 0.00000943
Iteration 35/1000 | Loss: 0.00000943
Iteration 36/1000 | Loss: 0.00000943
Iteration 37/1000 | Loss: 0.00000943
Iteration 38/1000 | Loss: 0.00000942
Iteration 39/1000 | Loss: 0.00000942
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000937
Iteration 43/1000 | Loss: 0.00000936
Iteration 44/1000 | Loss: 0.00000934
Iteration 45/1000 | Loss: 0.00000934
Iteration 46/1000 | Loss: 0.00000934
Iteration 47/1000 | Loss: 0.00000933
Iteration 48/1000 | Loss: 0.00000932
Iteration 49/1000 | Loss: 0.00000932
Iteration 50/1000 | Loss: 0.00000931
Iteration 51/1000 | Loss: 0.00000931
Iteration 52/1000 | Loss: 0.00000931
Iteration 53/1000 | Loss: 0.00000930
Iteration 54/1000 | Loss: 0.00000929
Iteration 55/1000 | Loss: 0.00000929
Iteration 56/1000 | Loss: 0.00000929
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000927
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000927
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000927
Iteration 67/1000 | Loss: 0.00000926
Iteration 68/1000 | Loss: 0.00000926
Iteration 69/1000 | Loss: 0.00000926
Iteration 70/1000 | Loss: 0.00000926
Iteration 71/1000 | Loss: 0.00000926
Iteration 72/1000 | Loss: 0.00000926
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000926
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000925
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000923
Iteration 84/1000 | Loss: 0.00000923
Iteration 85/1000 | Loss: 0.00000923
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000921
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000921
Iteration 105/1000 | Loss: 0.00000921
Iteration 106/1000 | Loss: 0.00000921
Iteration 107/1000 | Loss: 0.00000921
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [9.20506408874644e-06, 9.20506408874644e-06, 9.20506408874644e-06, 9.20506408874644e-06, 9.20506408874644e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.20506408874644e-06

Optimization complete. Final v2v error: 2.607705593109131 mm

Highest mean error: 2.999408483505249 mm for frame 91

Lowest mean error: 2.4123315811157227 mm for frame 128

Saving results

Total time: 33.94896697998047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967662
Iteration 2/25 | Loss: 0.00195280
Iteration 3/25 | Loss: 0.00133460
Iteration 4/25 | Loss: 0.00128314
Iteration 5/25 | Loss: 0.00128768
Iteration 6/25 | Loss: 0.00125504
Iteration 7/25 | Loss: 0.00121711
Iteration 8/25 | Loss: 0.00121683
Iteration 9/25 | Loss: 0.00120451
Iteration 10/25 | Loss: 0.00120163
Iteration 11/25 | Loss: 0.00121652
Iteration 12/25 | Loss: 0.00118853
Iteration 13/25 | Loss: 0.00116923
Iteration 14/25 | Loss: 0.00116231
Iteration 15/25 | Loss: 0.00115783
Iteration 16/25 | Loss: 0.00115382
Iteration 17/25 | Loss: 0.00115223
Iteration 18/25 | Loss: 0.00115382
Iteration 19/25 | Loss: 0.00115215
Iteration 20/25 | Loss: 0.00115215
Iteration 21/25 | Loss: 0.00115215
Iteration 22/25 | Loss: 0.00115215
Iteration 23/25 | Loss: 0.00115215
Iteration 24/25 | Loss: 0.00115215
Iteration 25/25 | Loss: 0.00115215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46923482
Iteration 2/25 | Loss: 0.00112051
Iteration 3/25 | Loss: 0.00112051
Iteration 4/25 | Loss: 0.00112051
Iteration 5/25 | Loss: 0.00112051
Iteration 6/25 | Loss: 0.00112051
Iteration 7/25 | Loss: 0.00112051
Iteration 8/25 | Loss: 0.00112051
Iteration 9/25 | Loss: 0.00112051
Iteration 10/25 | Loss: 0.00112051
Iteration 11/25 | Loss: 0.00112051
Iteration 12/25 | Loss: 0.00112051
Iteration 13/25 | Loss: 0.00112051
Iteration 14/25 | Loss: 0.00112051
Iteration 15/25 | Loss: 0.00112051
Iteration 16/25 | Loss: 0.00112051
Iteration 17/25 | Loss: 0.00112051
Iteration 18/25 | Loss: 0.00112051
Iteration 19/25 | Loss: 0.00112051
Iteration 20/25 | Loss: 0.00112051
Iteration 21/25 | Loss: 0.00110432
Iteration 22/25 | Loss: 0.00110432
Iteration 23/25 | Loss: 0.00110432
Iteration 24/25 | Loss: 0.00110432
Iteration 25/25 | Loss: 0.00110432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110432
Iteration 2/1000 | Loss: 0.00004543
Iteration 3/1000 | Loss: 0.00005339
Iteration 4/1000 | Loss: 0.00002607
Iteration 5/1000 | Loss: 0.00006692
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00005815
Iteration 8/1000 | Loss: 0.00014044
Iteration 9/1000 | Loss: 0.00021171
Iteration 10/1000 | Loss: 0.00002855
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00002099
Iteration 13/1000 | Loss: 0.00011447
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002015
Iteration 17/1000 | Loss: 0.00002010
Iteration 18/1000 | Loss: 0.00002002
Iteration 19/1000 | Loss: 0.00015826
Iteration 20/1000 | Loss: 0.00007273
Iteration 21/1000 | Loss: 0.00006054
Iteration 22/1000 | Loss: 0.00023043
Iteration 23/1000 | Loss: 0.00021225
Iteration 24/1000 | Loss: 0.00009242
Iteration 25/1000 | Loss: 0.00028688
Iteration 26/1000 | Loss: 0.00032657
Iteration 27/1000 | Loss: 0.00006464
Iteration 28/1000 | Loss: 0.00002414
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00005939
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001666
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00004701
Iteration 36/1000 | Loss: 0.00004701
Iteration 37/1000 | Loss: 0.00039557
Iteration 38/1000 | Loss: 0.00005919
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00003322
Iteration 41/1000 | Loss: 0.00020681
Iteration 42/1000 | Loss: 0.00005928
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00035292
Iteration 48/1000 | Loss: 0.00006547
Iteration 49/1000 | Loss: 0.00005438
Iteration 50/1000 | Loss: 0.00001830
Iteration 51/1000 | Loss: 0.00004063
Iteration 52/1000 | Loss: 0.00001613
Iteration 53/1000 | Loss: 0.00034121
Iteration 54/1000 | Loss: 0.00004421
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00004639
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00004428
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00038458
Iteration 62/1000 | Loss: 0.00009527
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00002649
Iteration 65/1000 | Loss: 0.00038004
Iteration 66/1000 | Loss: 0.00008245
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00046966
Iteration 70/1000 | Loss: 0.00006905
Iteration 71/1000 | Loss: 0.00001848
Iteration 72/1000 | Loss: 0.00002720
Iteration 73/1000 | Loss: 0.00017447
Iteration 74/1000 | Loss: 0.00004595
Iteration 75/1000 | Loss: 0.00002391
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00038292
Iteration 78/1000 | Loss: 0.00002055
Iteration 79/1000 | Loss: 0.00001650
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00005027
Iteration 82/1000 | Loss: 0.00003427
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00006307
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00002181
Iteration 87/1000 | Loss: 0.00003903
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00008671
Iteration 96/1000 | Loss: 0.00010452
Iteration 97/1000 | Loss: 0.00001924
Iteration 98/1000 | Loss: 0.00001301
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001275
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001271
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001270
Iteration 116/1000 | Loss: 0.00001270
Iteration 117/1000 | Loss: 0.00001270
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001268
Iteration 121/1000 | Loss: 0.00001268
Iteration 122/1000 | Loss: 0.00001268
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001266
Iteration 128/1000 | Loss: 0.00001266
Iteration 129/1000 | Loss: 0.00001266
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001265
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001264
Iteration 143/1000 | Loss: 0.00001264
Iteration 144/1000 | Loss: 0.00001264
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001262
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001261
Iteration 154/1000 | Loss: 0.00001261
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001260
Iteration 157/1000 | Loss: 0.00001260
Iteration 158/1000 | Loss: 0.00001260
Iteration 159/1000 | Loss: 0.00001260
Iteration 160/1000 | Loss: 0.00001260
Iteration 161/1000 | Loss: 0.00001260
Iteration 162/1000 | Loss: 0.00001260
Iteration 163/1000 | Loss: 0.00001259
Iteration 164/1000 | Loss: 0.00001259
Iteration 165/1000 | Loss: 0.00001259
Iteration 166/1000 | Loss: 0.00001259
Iteration 167/1000 | Loss: 0.00001259
Iteration 168/1000 | Loss: 0.00001259
Iteration 169/1000 | Loss: 0.00001259
Iteration 170/1000 | Loss: 0.00001258
Iteration 171/1000 | Loss: 0.00004352
Iteration 172/1000 | Loss: 0.00001785
Iteration 173/1000 | Loss: 0.00001939
Iteration 174/1000 | Loss: 0.00001258
Iteration 175/1000 | Loss: 0.00001258
Iteration 176/1000 | Loss: 0.00001258
Iteration 177/1000 | Loss: 0.00001257
Iteration 178/1000 | Loss: 0.00001257
Iteration 179/1000 | Loss: 0.00001257
Iteration 180/1000 | Loss: 0.00001257
Iteration 181/1000 | Loss: 0.00001257
Iteration 182/1000 | Loss: 0.00001257
Iteration 183/1000 | Loss: 0.00001257
Iteration 184/1000 | Loss: 0.00001257
Iteration 185/1000 | Loss: 0.00001257
Iteration 186/1000 | Loss: 0.00001257
Iteration 187/1000 | Loss: 0.00001257
Iteration 188/1000 | Loss: 0.00001257
Iteration 189/1000 | Loss: 0.00001257
Iteration 190/1000 | Loss: 0.00001257
Iteration 191/1000 | Loss: 0.00001257
Iteration 192/1000 | Loss: 0.00001257
Iteration 193/1000 | Loss: 0.00001257
Iteration 194/1000 | Loss: 0.00001257
Iteration 195/1000 | Loss: 0.00001257
Iteration 196/1000 | Loss: 0.00001257
Iteration 197/1000 | Loss: 0.00001257
Iteration 198/1000 | Loss: 0.00001257
Iteration 199/1000 | Loss: 0.00001257
Iteration 200/1000 | Loss: 0.00001257
Iteration 201/1000 | Loss: 0.00001257
Iteration 202/1000 | Loss: 0.00001257
Iteration 203/1000 | Loss: 0.00001257
Iteration 204/1000 | Loss: 0.00001257
Iteration 205/1000 | Loss: 0.00001257
Iteration 206/1000 | Loss: 0.00001257
Iteration 207/1000 | Loss: 0.00001257
Iteration 208/1000 | Loss: 0.00001257
Iteration 209/1000 | Loss: 0.00001257
Iteration 210/1000 | Loss: 0.00001257
Iteration 211/1000 | Loss: 0.00001257
Iteration 212/1000 | Loss: 0.00001257
Iteration 213/1000 | Loss: 0.00001257
Iteration 214/1000 | Loss: 0.00001257
Iteration 215/1000 | Loss: 0.00001257
Iteration 216/1000 | Loss: 0.00001257
Iteration 217/1000 | Loss: 0.00001257
Iteration 218/1000 | Loss: 0.00001257
Iteration 219/1000 | Loss: 0.00001257
Iteration 220/1000 | Loss: 0.00001257
Iteration 221/1000 | Loss: 0.00001257
Iteration 222/1000 | Loss: 0.00001257
Iteration 223/1000 | Loss: 0.00001257
Iteration 224/1000 | Loss: 0.00001257
Iteration 225/1000 | Loss: 0.00001257
Iteration 226/1000 | Loss: 0.00001257
Iteration 227/1000 | Loss: 0.00001257
Iteration 228/1000 | Loss: 0.00001257
Iteration 229/1000 | Loss: 0.00001257
Iteration 230/1000 | Loss: 0.00001257
Iteration 231/1000 | Loss: 0.00001257
Iteration 232/1000 | Loss: 0.00001257
Iteration 233/1000 | Loss: 0.00001257
Iteration 234/1000 | Loss: 0.00001257
Iteration 235/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2565456017910037e-05, 1.2565456017910037e-05, 1.2565456017910037e-05, 1.2565456017910037e-05, 1.2565456017910037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2565456017910037e-05

Optimization complete. Final v2v error: 3.0078372955322266 mm

Highest mean error: 4.6939849853515625 mm for frame 58

Lowest mean error: 2.36830472946167 mm for frame 97

Saving results

Total time: 168.15572237968445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414698
Iteration 2/25 | Loss: 0.00119766
Iteration 3/25 | Loss: 0.00112958
Iteration 4/25 | Loss: 0.00112264
Iteration 5/25 | Loss: 0.00112067
Iteration 6/25 | Loss: 0.00112067
Iteration 7/25 | Loss: 0.00112067
Iteration 8/25 | Loss: 0.00112067
Iteration 9/25 | Loss: 0.00112067
Iteration 10/25 | Loss: 0.00112067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001120667322538793, 0.001120667322538793, 0.001120667322538793, 0.001120667322538793, 0.001120667322538793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001120667322538793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17292309
Iteration 2/25 | Loss: 0.00072973
Iteration 3/25 | Loss: 0.00072972
Iteration 4/25 | Loss: 0.00072972
Iteration 5/25 | Loss: 0.00072972
Iteration 6/25 | Loss: 0.00072972
Iteration 7/25 | Loss: 0.00072972
Iteration 8/25 | Loss: 0.00072972
Iteration 9/25 | Loss: 0.00072972
Iteration 10/25 | Loss: 0.00072972
Iteration 11/25 | Loss: 0.00072972
Iteration 12/25 | Loss: 0.00072972
Iteration 13/25 | Loss: 0.00072972
Iteration 14/25 | Loss: 0.00072972
Iteration 15/25 | Loss: 0.00072972
Iteration 16/25 | Loss: 0.00072972
Iteration 17/25 | Loss: 0.00072972
Iteration 18/25 | Loss: 0.00072972
Iteration 19/25 | Loss: 0.00072972
Iteration 20/25 | Loss: 0.00072972
Iteration 21/25 | Loss: 0.00072972
Iteration 22/25 | Loss: 0.00072972
Iteration 23/25 | Loss: 0.00072972
Iteration 24/25 | Loss: 0.00072972
Iteration 25/25 | Loss: 0.00072972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072972
Iteration 2/1000 | Loss: 0.00002013
Iteration 3/1000 | Loss: 0.00001617
Iteration 4/1000 | Loss: 0.00001513
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001271
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001237
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001211
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001209
Iteration 30/1000 | Loss: 0.00001209
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001184
Iteration 110/1000 | Loss: 0.00001184
Iteration 111/1000 | Loss: 0.00001184
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001183
Iteration 116/1000 | Loss: 0.00001183
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001182
Iteration 121/1000 | Loss: 0.00001182
Iteration 122/1000 | Loss: 0.00001182
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001182
Iteration 126/1000 | Loss: 0.00001182
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001181
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001181
Iteration 149/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1813605851784814e-05, 1.1813605851784814e-05, 1.1813605851784814e-05, 1.1813605851784814e-05, 1.1813605851784814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1813605851784814e-05

Optimization complete. Final v2v error: 2.9684505462646484 mm

Highest mean error: 3.2385239601135254 mm for frame 169

Lowest mean error: 2.7758264541625977 mm for frame 9

Saving results

Total time: 42.07166409492493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802755
Iteration 2/25 | Loss: 0.00191552
Iteration 3/25 | Loss: 0.00151665
Iteration 4/25 | Loss: 0.00138087
Iteration 5/25 | Loss: 0.00132963
Iteration 6/25 | Loss: 0.00128677
Iteration 7/25 | Loss: 0.00127939
Iteration 8/25 | Loss: 0.00127817
Iteration 9/25 | Loss: 0.00127792
Iteration 10/25 | Loss: 0.00127791
Iteration 11/25 | Loss: 0.00127791
Iteration 12/25 | Loss: 0.00127791
Iteration 13/25 | Loss: 0.00127791
Iteration 14/25 | Loss: 0.00127791
Iteration 15/25 | Loss: 0.00127791
Iteration 16/25 | Loss: 0.00127791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012779140379279852, 0.0012779140379279852, 0.0012779140379279852, 0.0012779140379279852, 0.0012779140379279852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012779140379279852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29805207
Iteration 2/25 | Loss: 0.00065135
Iteration 3/25 | Loss: 0.00065133
Iteration 4/25 | Loss: 0.00065133
Iteration 5/25 | Loss: 0.00065133
Iteration 6/25 | Loss: 0.00065133
Iteration 7/25 | Loss: 0.00065133
Iteration 8/25 | Loss: 0.00065133
Iteration 9/25 | Loss: 0.00065133
Iteration 10/25 | Loss: 0.00065133
Iteration 11/25 | Loss: 0.00065133
Iteration 12/25 | Loss: 0.00065133
Iteration 13/25 | Loss: 0.00065133
Iteration 14/25 | Loss: 0.00065133
Iteration 15/25 | Loss: 0.00065133
Iteration 16/25 | Loss: 0.00065133
Iteration 17/25 | Loss: 0.00065133
Iteration 18/25 | Loss: 0.00065133
Iteration 19/25 | Loss: 0.00065133
Iteration 20/25 | Loss: 0.00065133
Iteration 21/25 | Loss: 0.00065133
Iteration 22/25 | Loss: 0.00065133
Iteration 23/25 | Loss: 0.00065133
Iteration 24/25 | Loss: 0.00065133
Iteration 25/25 | Loss: 0.00065133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065133
Iteration 2/1000 | Loss: 0.00003794
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001818
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001734
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001729
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001712
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001711
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001709
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001704
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001692
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001691
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.6906991731957532e-05, 1.6906991731957532e-05, 1.6906991731957532e-05, 1.6906991731957532e-05, 1.6906991731957532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6906991731957532e-05

Optimization complete. Final v2v error: 3.421811103820801 mm

Highest mean error: 3.744774341583252 mm for frame 7

Lowest mean error: 3.2982542514801025 mm for frame 89

Saving results

Total time: 36.54937028884888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438332
Iteration 2/25 | Loss: 0.00121471
Iteration 3/25 | Loss: 0.00115285
Iteration 4/25 | Loss: 0.00113884
Iteration 5/25 | Loss: 0.00113458
Iteration 6/25 | Loss: 0.00113394
Iteration 7/25 | Loss: 0.00113394
Iteration 8/25 | Loss: 0.00113394
Iteration 9/25 | Loss: 0.00113394
Iteration 10/25 | Loss: 0.00113394
Iteration 11/25 | Loss: 0.00113394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011339373886585236, 0.0011339373886585236, 0.0011339373886585236, 0.0011339373886585236, 0.0011339373886585236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011339373886585236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52311695
Iteration 2/25 | Loss: 0.00080996
Iteration 3/25 | Loss: 0.00080996
Iteration 4/25 | Loss: 0.00080996
Iteration 5/25 | Loss: 0.00080996
Iteration 6/25 | Loss: 0.00080996
Iteration 7/25 | Loss: 0.00080996
Iteration 8/25 | Loss: 0.00080996
Iteration 9/25 | Loss: 0.00080996
Iteration 10/25 | Loss: 0.00080995
Iteration 11/25 | Loss: 0.00080995
Iteration 12/25 | Loss: 0.00080995
Iteration 13/25 | Loss: 0.00080995
Iteration 14/25 | Loss: 0.00080995
Iteration 15/25 | Loss: 0.00080995
Iteration 16/25 | Loss: 0.00080995
Iteration 17/25 | Loss: 0.00080995
Iteration 18/25 | Loss: 0.00080995
Iteration 19/25 | Loss: 0.00080995
Iteration 20/25 | Loss: 0.00080995
Iteration 21/25 | Loss: 0.00080995
Iteration 22/25 | Loss: 0.00080995
Iteration 23/25 | Loss: 0.00080995
Iteration 24/25 | Loss: 0.00080995
Iteration 25/25 | Loss: 0.00080995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080995
Iteration 2/1000 | Loss: 0.00002598
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001367
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001325
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001291
Iteration 30/1000 | Loss: 0.00001291
Iteration 31/1000 | Loss: 0.00001289
Iteration 32/1000 | Loss: 0.00001288
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001282
Iteration 39/1000 | Loss: 0.00001282
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001281
Iteration 46/1000 | Loss: 0.00001281
Iteration 47/1000 | Loss: 0.00001281
Iteration 48/1000 | Loss: 0.00001281
Iteration 49/1000 | Loss: 0.00001280
Iteration 50/1000 | Loss: 0.00001280
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001279
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001270
Iteration 63/1000 | Loss: 0.00001270
Iteration 64/1000 | Loss: 0.00001270
Iteration 65/1000 | Loss: 0.00001270
Iteration 66/1000 | Loss: 0.00001270
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001266
Iteration 74/1000 | Loss: 0.00001266
Iteration 75/1000 | Loss: 0.00001266
Iteration 76/1000 | Loss: 0.00001266
Iteration 77/1000 | Loss: 0.00001266
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001265
Iteration 80/1000 | Loss: 0.00001265
Iteration 81/1000 | Loss: 0.00001265
Iteration 82/1000 | Loss: 0.00001265
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001264
Iteration 85/1000 | Loss: 0.00001264
Iteration 86/1000 | Loss: 0.00001263
Iteration 87/1000 | Loss: 0.00001263
Iteration 88/1000 | Loss: 0.00001263
Iteration 89/1000 | Loss: 0.00001263
Iteration 90/1000 | Loss: 0.00001263
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001261
Iteration 102/1000 | Loss: 0.00001261
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001256
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001254
Iteration 145/1000 | Loss: 0.00001254
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001254
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001253
Iteration 159/1000 | Loss: 0.00001253
Iteration 160/1000 | Loss: 0.00001252
Iteration 161/1000 | Loss: 0.00001252
Iteration 162/1000 | Loss: 0.00001252
Iteration 163/1000 | Loss: 0.00001252
Iteration 164/1000 | Loss: 0.00001252
Iteration 165/1000 | Loss: 0.00001252
Iteration 166/1000 | Loss: 0.00001252
Iteration 167/1000 | Loss: 0.00001252
Iteration 168/1000 | Loss: 0.00001252
Iteration 169/1000 | Loss: 0.00001252
Iteration 170/1000 | Loss: 0.00001252
Iteration 171/1000 | Loss: 0.00001252
Iteration 172/1000 | Loss: 0.00001252
Iteration 173/1000 | Loss: 0.00001252
Iteration 174/1000 | Loss: 0.00001251
Iteration 175/1000 | Loss: 0.00001251
Iteration 176/1000 | Loss: 0.00001251
Iteration 177/1000 | Loss: 0.00001251
Iteration 178/1000 | Loss: 0.00001251
Iteration 179/1000 | Loss: 0.00001251
Iteration 180/1000 | Loss: 0.00001251
Iteration 181/1000 | Loss: 0.00001251
Iteration 182/1000 | Loss: 0.00001251
Iteration 183/1000 | Loss: 0.00001251
Iteration 184/1000 | Loss: 0.00001251
Iteration 185/1000 | Loss: 0.00001251
Iteration 186/1000 | Loss: 0.00001251
Iteration 187/1000 | Loss: 0.00001251
Iteration 188/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.2507935025496408e-05, 1.2507935025496408e-05, 1.2507935025496408e-05, 1.2507935025496408e-05, 1.2507935025496408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2507935025496408e-05

Optimization complete. Final v2v error: 3.0281765460968018 mm

Highest mean error: 3.4145662784576416 mm for frame 113

Lowest mean error: 2.927449941635132 mm for frame 86

Saving results

Total time: 39.98004198074341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00279411
Iteration 2/25 | Loss: 0.00127007
Iteration 3/25 | Loss: 0.00113858
Iteration 4/25 | Loss: 0.00111652
Iteration 5/25 | Loss: 0.00111017
Iteration 6/25 | Loss: 0.00110811
Iteration 7/25 | Loss: 0.00110739
Iteration 8/25 | Loss: 0.00110739
Iteration 9/25 | Loss: 0.00110739
Iteration 10/25 | Loss: 0.00110739
Iteration 11/25 | Loss: 0.00110739
Iteration 12/25 | Loss: 0.00110739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011073884088546038, 0.0011073884088546038, 0.0011073884088546038, 0.0011073884088546038, 0.0011073884088546038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011073884088546038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32275915
Iteration 2/25 | Loss: 0.00103557
Iteration 3/25 | Loss: 0.00103556
Iteration 4/25 | Loss: 0.00103556
Iteration 5/25 | Loss: 0.00103556
Iteration 6/25 | Loss: 0.00103556
Iteration 7/25 | Loss: 0.00103556
Iteration 8/25 | Loss: 0.00103556
Iteration 9/25 | Loss: 0.00103556
Iteration 10/25 | Loss: 0.00103556
Iteration 11/25 | Loss: 0.00103556
Iteration 12/25 | Loss: 0.00103556
Iteration 13/25 | Loss: 0.00103556
Iteration 14/25 | Loss: 0.00103556
Iteration 15/25 | Loss: 0.00103556
Iteration 16/25 | Loss: 0.00103556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010355622507631779, 0.0010355622507631779, 0.0010355622507631779, 0.0010355622507631779, 0.0010355622507631779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010355622507631779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103556
Iteration 2/1000 | Loss: 0.00003995
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00001846
Iteration 5/1000 | Loss: 0.00001682
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001392
Iteration 10/1000 | Loss: 0.00001359
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001286
Iteration 24/1000 | Loss: 0.00001286
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001283
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001283
Iteration 31/1000 | Loss: 0.00001281
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001280
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001277
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001277
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001275
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001264
Iteration 89/1000 | Loss: 0.00001264
Iteration 90/1000 | Loss: 0.00001263
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001261
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001260
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001255
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001254
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001253
Iteration 147/1000 | Loss: 0.00001253
Iteration 148/1000 | Loss: 0.00001253
Iteration 149/1000 | Loss: 0.00001253
Iteration 150/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.2528843399195466e-05, 1.2528843399195466e-05, 1.2528843399195466e-05, 1.2528843399195466e-05, 1.2528843399195466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2528843399195466e-05

Optimization complete. Final v2v error: 3.0575668811798096 mm

Highest mean error: 3.2265398502349854 mm for frame 195

Lowest mean error: 2.702946424484253 mm for frame 0

Saving results

Total time: 46.78114557266235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921917
Iteration 2/25 | Loss: 0.00176721
Iteration 3/25 | Loss: 0.00139743
Iteration 4/25 | Loss: 0.00133304
Iteration 5/25 | Loss: 0.00131600
Iteration 6/25 | Loss: 0.00131151
Iteration 7/25 | Loss: 0.00130923
Iteration 8/25 | Loss: 0.00131020
Iteration 9/25 | Loss: 0.00130814
Iteration 10/25 | Loss: 0.00130334
Iteration 11/25 | Loss: 0.00129819
Iteration 12/25 | Loss: 0.00129591
Iteration 13/25 | Loss: 0.00129493
Iteration 14/25 | Loss: 0.00129471
Iteration 15/25 | Loss: 0.00129462
Iteration 16/25 | Loss: 0.00129462
Iteration 17/25 | Loss: 0.00129462
Iteration 18/25 | Loss: 0.00129462
Iteration 19/25 | Loss: 0.00129462
Iteration 20/25 | Loss: 0.00129462
Iteration 21/25 | Loss: 0.00129461
Iteration 22/25 | Loss: 0.00129461
Iteration 23/25 | Loss: 0.00129461
Iteration 24/25 | Loss: 0.00129461
Iteration 25/25 | Loss: 0.00129461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32430613
Iteration 2/25 | Loss: 0.00271050
Iteration 3/25 | Loss: 0.00271047
Iteration 4/25 | Loss: 0.00271047
Iteration 5/25 | Loss: 0.00271047
Iteration 6/25 | Loss: 0.00271047
Iteration 7/25 | Loss: 0.00271047
Iteration 8/25 | Loss: 0.00271047
Iteration 9/25 | Loss: 0.00271047
Iteration 10/25 | Loss: 0.00271047
Iteration 11/25 | Loss: 0.00271047
Iteration 12/25 | Loss: 0.00271047
Iteration 13/25 | Loss: 0.00271047
Iteration 14/25 | Loss: 0.00271047
Iteration 15/25 | Loss: 0.00271047
Iteration 16/25 | Loss: 0.00271047
Iteration 17/25 | Loss: 0.00271047
Iteration 18/25 | Loss: 0.00271047
Iteration 19/25 | Loss: 0.00271047
Iteration 20/25 | Loss: 0.00271047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0027104681357741356, 0.0027104681357741356, 0.0027104681357741356, 0.0027104681357741356, 0.0027104681357741356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027104681357741356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271047
Iteration 2/1000 | Loss: 0.00024424
Iteration 3/1000 | Loss: 0.00015996
Iteration 4/1000 | Loss: 0.00013721
Iteration 5/1000 | Loss: 0.00012534
Iteration 6/1000 | Loss: 0.00011975
Iteration 7/1000 | Loss: 0.00011588
Iteration 8/1000 | Loss: 0.00011275
Iteration 9/1000 | Loss: 0.00010998
Iteration 10/1000 | Loss: 0.00010859
Iteration 11/1000 | Loss: 0.00010718
Iteration 12/1000 | Loss: 0.00010600
Iteration 13/1000 | Loss: 0.00010481
Iteration 14/1000 | Loss: 0.00010383
Iteration 15/1000 | Loss: 0.00839469
Iteration 16/1000 | Loss: 0.00761326
Iteration 17/1000 | Loss: 0.00014153
Iteration 18/1000 | Loss: 0.00008105
Iteration 19/1000 | Loss: 0.00006254
Iteration 20/1000 | Loss: 0.00005344
Iteration 21/1000 | Loss: 0.00004368
Iteration 22/1000 | Loss: 0.00003837
Iteration 23/1000 | Loss: 0.00003408
Iteration 24/1000 | Loss: 0.00003123
Iteration 25/1000 | Loss: 0.00002912
Iteration 26/1000 | Loss: 0.00002717
Iteration 27/1000 | Loss: 0.00002603
Iteration 28/1000 | Loss: 0.00002526
Iteration 29/1000 | Loss: 0.00002449
Iteration 30/1000 | Loss: 0.00002402
Iteration 31/1000 | Loss: 0.00002364
Iteration 32/1000 | Loss: 0.00002328
Iteration 33/1000 | Loss: 0.00002302
Iteration 34/1000 | Loss: 0.00002299
Iteration 35/1000 | Loss: 0.00002294
Iteration 36/1000 | Loss: 0.00002279
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002270
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002251
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002243
Iteration 43/1000 | Loss: 0.00002243
Iteration 44/1000 | Loss: 0.00002243
Iteration 45/1000 | Loss: 0.00002242
Iteration 46/1000 | Loss: 0.00002241
Iteration 47/1000 | Loss: 0.00002236
Iteration 48/1000 | Loss: 0.00002236
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002230
Iteration 51/1000 | Loss: 0.00002228
Iteration 52/1000 | Loss: 0.00002226
Iteration 53/1000 | Loss: 0.00002226
Iteration 54/1000 | Loss: 0.00002226
Iteration 55/1000 | Loss: 0.00002226
Iteration 56/1000 | Loss: 0.00002226
Iteration 57/1000 | Loss: 0.00002226
Iteration 58/1000 | Loss: 0.00002225
Iteration 59/1000 | Loss: 0.00002225
Iteration 60/1000 | Loss: 0.00002225
Iteration 61/1000 | Loss: 0.00002225
Iteration 62/1000 | Loss: 0.00002225
Iteration 63/1000 | Loss: 0.00002225
Iteration 64/1000 | Loss: 0.00002225
Iteration 65/1000 | Loss: 0.00002225
Iteration 66/1000 | Loss: 0.00002225
Iteration 67/1000 | Loss: 0.00002224
Iteration 68/1000 | Loss: 0.00002224
Iteration 69/1000 | Loss: 0.00002224
Iteration 70/1000 | Loss: 0.00002224
Iteration 71/1000 | Loss: 0.00002224
Iteration 72/1000 | Loss: 0.00002224
Iteration 73/1000 | Loss: 0.00002224
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002224
Iteration 76/1000 | Loss: 0.00002223
Iteration 77/1000 | Loss: 0.00002223
Iteration 78/1000 | Loss: 0.00002221
Iteration 79/1000 | Loss: 0.00002221
Iteration 80/1000 | Loss: 0.00002220
Iteration 81/1000 | Loss: 0.00002220
Iteration 82/1000 | Loss: 0.00002220
Iteration 83/1000 | Loss: 0.00002220
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00002220
Iteration 86/1000 | Loss: 0.00002220
Iteration 87/1000 | Loss: 0.00002220
Iteration 88/1000 | Loss: 0.00002220
Iteration 89/1000 | Loss: 0.00002219
Iteration 90/1000 | Loss: 0.00002219
Iteration 91/1000 | Loss: 0.00002218
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002218
Iteration 94/1000 | Loss: 0.00002218
Iteration 95/1000 | Loss: 0.00002218
Iteration 96/1000 | Loss: 0.00002217
Iteration 97/1000 | Loss: 0.00002217
Iteration 98/1000 | Loss: 0.00002217
Iteration 99/1000 | Loss: 0.00002217
Iteration 100/1000 | Loss: 0.00002216
Iteration 101/1000 | Loss: 0.00002216
Iteration 102/1000 | Loss: 0.00002216
Iteration 103/1000 | Loss: 0.00002215
Iteration 104/1000 | Loss: 0.00002215
Iteration 105/1000 | Loss: 0.00002215
Iteration 106/1000 | Loss: 0.00002214
Iteration 107/1000 | Loss: 0.00002214
Iteration 108/1000 | Loss: 0.00002214
Iteration 109/1000 | Loss: 0.00002214
Iteration 110/1000 | Loss: 0.00002214
Iteration 111/1000 | Loss: 0.00002214
Iteration 112/1000 | Loss: 0.00002213
Iteration 113/1000 | Loss: 0.00002213
Iteration 114/1000 | Loss: 0.00002213
Iteration 115/1000 | Loss: 0.00002213
Iteration 116/1000 | Loss: 0.00002213
Iteration 117/1000 | Loss: 0.00002212
Iteration 118/1000 | Loss: 0.00002212
Iteration 119/1000 | Loss: 0.00002212
Iteration 120/1000 | Loss: 0.00002211
Iteration 121/1000 | Loss: 0.00002211
Iteration 122/1000 | Loss: 0.00002211
Iteration 123/1000 | Loss: 0.00002211
Iteration 124/1000 | Loss: 0.00002211
Iteration 125/1000 | Loss: 0.00002211
Iteration 126/1000 | Loss: 0.00002211
Iteration 127/1000 | Loss: 0.00002211
Iteration 128/1000 | Loss: 0.00002210
Iteration 129/1000 | Loss: 0.00002210
Iteration 130/1000 | Loss: 0.00002210
Iteration 131/1000 | Loss: 0.00002210
Iteration 132/1000 | Loss: 0.00002210
Iteration 133/1000 | Loss: 0.00002209
Iteration 134/1000 | Loss: 0.00002209
Iteration 135/1000 | Loss: 0.00002209
Iteration 136/1000 | Loss: 0.00002209
Iteration 137/1000 | Loss: 0.00002209
Iteration 138/1000 | Loss: 0.00002209
Iteration 139/1000 | Loss: 0.00002209
Iteration 140/1000 | Loss: 0.00002209
Iteration 141/1000 | Loss: 0.00002209
Iteration 142/1000 | Loss: 0.00002209
Iteration 143/1000 | Loss: 0.00002208
Iteration 144/1000 | Loss: 0.00002208
Iteration 145/1000 | Loss: 0.00002207
Iteration 146/1000 | Loss: 0.00002207
Iteration 147/1000 | Loss: 0.00002206
Iteration 148/1000 | Loss: 0.00002206
Iteration 149/1000 | Loss: 0.00002206
Iteration 150/1000 | Loss: 0.00002206
Iteration 151/1000 | Loss: 0.00002206
Iteration 152/1000 | Loss: 0.00002206
Iteration 153/1000 | Loss: 0.00002205
Iteration 154/1000 | Loss: 0.00002205
Iteration 155/1000 | Loss: 0.00002205
Iteration 156/1000 | Loss: 0.00002204
Iteration 157/1000 | Loss: 0.00002204
Iteration 158/1000 | Loss: 0.00002204
Iteration 159/1000 | Loss: 0.00002204
Iteration 160/1000 | Loss: 0.00002204
Iteration 161/1000 | Loss: 0.00002204
Iteration 162/1000 | Loss: 0.00002204
Iteration 163/1000 | Loss: 0.00002204
Iteration 164/1000 | Loss: 0.00002204
Iteration 165/1000 | Loss: 0.00002203
Iteration 166/1000 | Loss: 0.00002203
Iteration 167/1000 | Loss: 0.00002203
Iteration 168/1000 | Loss: 0.00002203
Iteration 169/1000 | Loss: 0.00002203
Iteration 170/1000 | Loss: 0.00002203
Iteration 171/1000 | Loss: 0.00002203
Iteration 172/1000 | Loss: 0.00002203
Iteration 173/1000 | Loss: 0.00002203
Iteration 174/1000 | Loss: 0.00002203
Iteration 175/1000 | Loss: 0.00002203
Iteration 176/1000 | Loss: 0.00002203
Iteration 177/1000 | Loss: 0.00002202
Iteration 178/1000 | Loss: 0.00002202
Iteration 179/1000 | Loss: 0.00002202
Iteration 180/1000 | Loss: 0.00002201
Iteration 181/1000 | Loss: 0.00002201
Iteration 182/1000 | Loss: 0.00002201
Iteration 183/1000 | Loss: 0.00002201
Iteration 184/1000 | Loss: 0.00002201
Iteration 185/1000 | Loss: 0.00002201
Iteration 186/1000 | Loss: 0.00002201
Iteration 187/1000 | Loss: 0.00002201
Iteration 188/1000 | Loss: 0.00002201
Iteration 189/1000 | Loss: 0.00002200
Iteration 190/1000 | Loss: 0.00002200
Iteration 191/1000 | Loss: 0.00002200
Iteration 192/1000 | Loss: 0.00002200
Iteration 193/1000 | Loss: 0.00002200
Iteration 194/1000 | Loss: 0.00002200
Iteration 195/1000 | Loss: 0.00002200
Iteration 196/1000 | Loss: 0.00002200
Iteration 197/1000 | Loss: 0.00002200
Iteration 198/1000 | Loss: 0.00002200
Iteration 199/1000 | Loss: 0.00002200
Iteration 200/1000 | Loss: 0.00002199
Iteration 201/1000 | Loss: 0.00002199
Iteration 202/1000 | Loss: 0.00002199
Iteration 203/1000 | Loss: 0.00002199
Iteration 204/1000 | Loss: 0.00002199
Iteration 205/1000 | Loss: 0.00002199
Iteration 206/1000 | Loss: 0.00002198
Iteration 207/1000 | Loss: 0.00002198
Iteration 208/1000 | Loss: 0.00002198
Iteration 209/1000 | Loss: 0.00002198
Iteration 210/1000 | Loss: 0.00002198
Iteration 211/1000 | Loss: 0.00002198
Iteration 212/1000 | Loss: 0.00002198
Iteration 213/1000 | Loss: 0.00002198
Iteration 214/1000 | Loss: 0.00002198
Iteration 215/1000 | Loss: 0.00002198
Iteration 216/1000 | Loss: 0.00002198
Iteration 217/1000 | Loss: 0.00002198
Iteration 218/1000 | Loss: 0.00002198
Iteration 219/1000 | Loss: 0.00002198
Iteration 220/1000 | Loss: 0.00002198
Iteration 221/1000 | Loss: 0.00002198
Iteration 222/1000 | Loss: 0.00002198
Iteration 223/1000 | Loss: 0.00002198
Iteration 224/1000 | Loss: 0.00002198
Iteration 225/1000 | Loss: 0.00002198
Iteration 226/1000 | Loss: 0.00002198
Iteration 227/1000 | Loss: 0.00002198
Iteration 228/1000 | Loss: 0.00002198
Iteration 229/1000 | Loss: 0.00002198
Iteration 230/1000 | Loss: 0.00002198
Iteration 231/1000 | Loss: 0.00002198
Iteration 232/1000 | Loss: 0.00002198
Iteration 233/1000 | Loss: 0.00002198
Iteration 234/1000 | Loss: 0.00002198
Iteration 235/1000 | Loss: 0.00002198
Iteration 236/1000 | Loss: 0.00002198
Iteration 237/1000 | Loss: 0.00002198
Iteration 238/1000 | Loss: 0.00002198
Iteration 239/1000 | Loss: 0.00002198
Iteration 240/1000 | Loss: 0.00002198
Iteration 241/1000 | Loss: 0.00002198
Iteration 242/1000 | Loss: 0.00002198
Iteration 243/1000 | Loss: 0.00002198
Iteration 244/1000 | Loss: 0.00002198
Iteration 245/1000 | Loss: 0.00002198
Iteration 246/1000 | Loss: 0.00002198
Iteration 247/1000 | Loss: 0.00002198
Iteration 248/1000 | Loss: 0.00002198
Iteration 249/1000 | Loss: 0.00002198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.1978166842018254e-05, 2.1978166842018254e-05, 2.1978166842018254e-05, 2.1978166842018254e-05, 2.1978166842018254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1978166842018254e-05

Optimization complete. Final v2v error: 3.892085075378418 mm

Highest mean error: 4.275968074798584 mm for frame 8

Lowest mean error: 3.642869234085083 mm for frame 151

Saving results

Total time: 95.91211175918579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590080
Iteration 2/25 | Loss: 0.00120432
Iteration 3/25 | Loss: 0.00114006
Iteration 4/25 | Loss: 0.00113115
Iteration 5/25 | Loss: 0.00112853
Iteration 6/25 | Loss: 0.00112853
Iteration 7/25 | Loss: 0.00112853
Iteration 8/25 | Loss: 0.00112853
Iteration 9/25 | Loss: 0.00112853
Iteration 10/25 | Loss: 0.00112852
Iteration 11/25 | Loss: 0.00112852
Iteration 12/25 | Loss: 0.00112852
Iteration 13/25 | Loss: 0.00112852
Iteration 14/25 | Loss: 0.00112853
Iteration 15/25 | Loss: 0.00112852
Iteration 16/25 | Loss: 0.00112852
Iteration 17/25 | Loss: 0.00112852
Iteration 18/25 | Loss: 0.00112852
Iteration 19/25 | Loss: 0.00112852
Iteration 20/25 | Loss: 0.00112852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011285248911008239, 0.0011285248911008239, 0.0011285248911008239, 0.0011285248911008239, 0.0011285248911008239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011285248911008239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12424231
Iteration 2/25 | Loss: 0.00074802
Iteration 3/25 | Loss: 0.00074802
Iteration 4/25 | Loss: 0.00074802
Iteration 5/25 | Loss: 0.00074802
Iteration 6/25 | Loss: 0.00074802
Iteration 7/25 | Loss: 0.00074802
Iteration 8/25 | Loss: 0.00074802
Iteration 9/25 | Loss: 0.00074801
Iteration 10/25 | Loss: 0.00074801
Iteration 11/25 | Loss: 0.00074801
Iteration 12/25 | Loss: 0.00074801
Iteration 13/25 | Loss: 0.00074801
Iteration 14/25 | Loss: 0.00074801
Iteration 15/25 | Loss: 0.00074801
Iteration 16/25 | Loss: 0.00074801
Iteration 17/25 | Loss: 0.00074801
Iteration 18/25 | Loss: 0.00074801
Iteration 19/25 | Loss: 0.00074801
Iteration 20/25 | Loss: 0.00074801
Iteration 21/25 | Loss: 0.00074801
Iteration 22/25 | Loss: 0.00074801
Iteration 23/25 | Loss: 0.00074801
Iteration 24/25 | Loss: 0.00074801
Iteration 25/25 | Loss: 0.00074801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074801
Iteration 2/1000 | Loss: 0.00002220
Iteration 3/1000 | Loss: 0.00001764
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001453
Iteration 7/1000 | Loss: 0.00001411
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001264
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001251
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001246
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001242
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001230
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001229
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.220522062794771e-05, 1.220522062794771e-05, 1.220522062794771e-05, 1.220522062794771e-05, 1.220522062794771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.220522062794771e-05

Optimization complete. Final v2v error: 3.0054452419281006 mm

Highest mean error: 3.2743470668792725 mm for frame 140

Lowest mean error: 2.82951283454895 mm for frame 4

Saving results

Total time: 37.959599018096924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778498
Iteration 2/25 | Loss: 0.00131520
Iteration 3/25 | Loss: 0.00120287
Iteration 4/25 | Loss: 0.00119753
Iteration 5/25 | Loss: 0.00119625
Iteration 6/25 | Loss: 0.00119625
Iteration 7/25 | Loss: 0.00119625
Iteration 8/25 | Loss: 0.00119625
Iteration 9/25 | Loss: 0.00119625
Iteration 10/25 | Loss: 0.00119625
Iteration 11/25 | Loss: 0.00119625
Iteration 12/25 | Loss: 0.00119625
Iteration 13/25 | Loss: 0.00119625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001196252298541367, 0.001196252298541367, 0.001196252298541367, 0.001196252298541367, 0.001196252298541367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196252298541367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30581582
Iteration 2/25 | Loss: 0.00057479
Iteration 3/25 | Loss: 0.00057474
Iteration 4/25 | Loss: 0.00057474
Iteration 5/25 | Loss: 0.00057474
Iteration 6/25 | Loss: 0.00057474
Iteration 7/25 | Loss: 0.00057474
Iteration 8/25 | Loss: 0.00057474
Iteration 9/25 | Loss: 0.00057474
Iteration 10/25 | Loss: 0.00057474
Iteration 11/25 | Loss: 0.00057474
Iteration 12/25 | Loss: 0.00057474
Iteration 13/25 | Loss: 0.00057474
Iteration 14/25 | Loss: 0.00057474
Iteration 15/25 | Loss: 0.00057474
Iteration 16/25 | Loss: 0.00057474
Iteration 17/25 | Loss: 0.00057474
Iteration 18/25 | Loss: 0.00057474
Iteration 19/25 | Loss: 0.00057474
Iteration 20/25 | Loss: 0.00057474
Iteration 21/25 | Loss: 0.00057474
Iteration 22/25 | Loss: 0.00057474
Iteration 23/25 | Loss: 0.00057474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005747380782850087, 0.0005747380782850087, 0.0005747380782850087, 0.0005747380782850087, 0.0005747380782850087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005747380782850087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057474
Iteration 2/1000 | Loss: 0.00002461
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001504
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001392
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001364
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001349
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001339
Iteration 30/1000 | Loss: 0.00001338
Iteration 31/1000 | Loss: 0.00001338
Iteration 32/1000 | Loss: 0.00001337
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001335
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001327
Iteration 38/1000 | Loss: 0.00001326
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001313
Iteration 58/1000 | Loss: 0.00001313
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001310
Iteration 69/1000 | Loss: 0.00001309
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001306
Iteration 76/1000 | Loss: 0.00001306
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001305
Iteration 81/1000 | Loss: 0.00001305
Iteration 82/1000 | Loss: 0.00001305
Iteration 83/1000 | Loss: 0.00001305
Iteration 84/1000 | Loss: 0.00001305
Iteration 85/1000 | Loss: 0.00001304
Iteration 86/1000 | Loss: 0.00001304
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001303
Iteration 90/1000 | Loss: 0.00001303
Iteration 91/1000 | Loss: 0.00001303
Iteration 92/1000 | Loss: 0.00001303
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001302
Iteration 97/1000 | Loss: 0.00001302
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001302
Iteration 102/1000 | Loss: 0.00001301
Iteration 103/1000 | Loss: 0.00001301
Iteration 104/1000 | Loss: 0.00001301
Iteration 105/1000 | Loss: 0.00001301
Iteration 106/1000 | Loss: 0.00001301
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001301
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001301
Iteration 114/1000 | Loss: 0.00001301
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.3006397239223588e-05, 1.3006397239223588e-05, 1.3006397239223588e-05, 1.3006397239223588e-05, 1.3006397239223588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3006397239223588e-05

Optimization complete. Final v2v error: 3.0680572986602783 mm

Highest mean error: 3.247142791748047 mm for frame 46

Lowest mean error: 2.9862563610076904 mm for frame 100

Saving results

Total time: 37.473265647888184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810712
Iteration 2/25 | Loss: 0.00174119
Iteration 3/25 | Loss: 0.00135266
Iteration 4/25 | Loss: 0.00132760
Iteration 5/25 | Loss: 0.00132082
Iteration 6/25 | Loss: 0.00131924
Iteration 7/25 | Loss: 0.00131900
Iteration 8/25 | Loss: 0.00131900
Iteration 9/25 | Loss: 0.00131900
Iteration 10/25 | Loss: 0.00131900
Iteration 11/25 | Loss: 0.00131900
Iteration 12/25 | Loss: 0.00131900
Iteration 13/25 | Loss: 0.00131900
Iteration 14/25 | Loss: 0.00131900
Iteration 15/25 | Loss: 0.00131900
Iteration 16/25 | Loss: 0.00131900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013190028257668018, 0.0013190028257668018, 0.0013190028257668018, 0.0013190028257668018, 0.0013190028257668018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013190028257668018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.36929962
Iteration 2/25 | Loss: 0.00079778
Iteration 3/25 | Loss: 0.00079775
Iteration 4/25 | Loss: 0.00079775
Iteration 5/25 | Loss: 0.00079775
Iteration 6/25 | Loss: 0.00079775
Iteration 7/25 | Loss: 0.00079775
Iteration 8/25 | Loss: 0.00079775
Iteration 9/25 | Loss: 0.00079775
Iteration 10/25 | Loss: 0.00079775
Iteration 11/25 | Loss: 0.00079775
Iteration 12/25 | Loss: 0.00079775
Iteration 13/25 | Loss: 0.00079775
Iteration 14/25 | Loss: 0.00079775
Iteration 15/25 | Loss: 0.00079775
Iteration 16/25 | Loss: 0.00079775
Iteration 17/25 | Loss: 0.00079775
Iteration 18/25 | Loss: 0.00079775
Iteration 19/25 | Loss: 0.00079775
Iteration 20/25 | Loss: 0.00079775
Iteration 21/25 | Loss: 0.00079775
Iteration 22/25 | Loss: 0.00079775
Iteration 23/25 | Loss: 0.00079775
Iteration 24/25 | Loss: 0.00079775
Iteration 25/25 | Loss: 0.00079775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079775
Iteration 2/1000 | Loss: 0.00008807
Iteration 3/1000 | Loss: 0.00005122
Iteration 4/1000 | Loss: 0.00004326
Iteration 5/1000 | Loss: 0.00003987
Iteration 6/1000 | Loss: 0.00003817
Iteration 7/1000 | Loss: 0.00003715
Iteration 8/1000 | Loss: 0.00003592
Iteration 9/1000 | Loss: 0.00003503
Iteration 10/1000 | Loss: 0.00003443
Iteration 11/1000 | Loss: 0.00003392
Iteration 12/1000 | Loss: 0.00003353
Iteration 13/1000 | Loss: 0.00003327
Iteration 14/1000 | Loss: 0.00003298
Iteration 15/1000 | Loss: 0.00003276
Iteration 16/1000 | Loss: 0.00003259
Iteration 17/1000 | Loss: 0.00003249
Iteration 18/1000 | Loss: 0.00003236
Iteration 19/1000 | Loss: 0.00003234
Iteration 20/1000 | Loss: 0.00003232
Iteration 21/1000 | Loss: 0.00003217
Iteration 22/1000 | Loss: 0.00003215
Iteration 23/1000 | Loss: 0.00003204
Iteration 24/1000 | Loss: 0.00003201
Iteration 25/1000 | Loss: 0.00003194
Iteration 26/1000 | Loss: 0.00003194
Iteration 27/1000 | Loss: 0.00003190
Iteration 28/1000 | Loss: 0.00003189
Iteration 29/1000 | Loss: 0.00003189
Iteration 30/1000 | Loss: 0.00003188
Iteration 31/1000 | Loss: 0.00003188
Iteration 32/1000 | Loss: 0.00003188
Iteration 33/1000 | Loss: 0.00003187
Iteration 34/1000 | Loss: 0.00003187
Iteration 35/1000 | Loss: 0.00003185
Iteration 36/1000 | Loss: 0.00003185
Iteration 37/1000 | Loss: 0.00003185
Iteration 38/1000 | Loss: 0.00003184
Iteration 39/1000 | Loss: 0.00003184
Iteration 40/1000 | Loss: 0.00003183
Iteration 41/1000 | Loss: 0.00003183
Iteration 42/1000 | Loss: 0.00003183
Iteration 43/1000 | Loss: 0.00003183
Iteration 44/1000 | Loss: 0.00003182
Iteration 45/1000 | Loss: 0.00003182
Iteration 46/1000 | Loss: 0.00003182
Iteration 47/1000 | Loss: 0.00003182
Iteration 48/1000 | Loss: 0.00003181
Iteration 49/1000 | Loss: 0.00003180
Iteration 50/1000 | Loss: 0.00003180
Iteration 51/1000 | Loss: 0.00003180
Iteration 52/1000 | Loss: 0.00003179
Iteration 53/1000 | Loss: 0.00003179
Iteration 54/1000 | Loss: 0.00003179
Iteration 55/1000 | Loss: 0.00003179
Iteration 56/1000 | Loss: 0.00003178
Iteration 57/1000 | Loss: 0.00003178
Iteration 58/1000 | Loss: 0.00003178
Iteration 59/1000 | Loss: 0.00003178
Iteration 60/1000 | Loss: 0.00003178
Iteration 61/1000 | Loss: 0.00003178
Iteration 62/1000 | Loss: 0.00003178
Iteration 63/1000 | Loss: 0.00003178
Iteration 64/1000 | Loss: 0.00003178
Iteration 65/1000 | Loss: 0.00003177
Iteration 66/1000 | Loss: 0.00003177
Iteration 67/1000 | Loss: 0.00003177
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003175
Iteration 73/1000 | Loss: 0.00003175
Iteration 74/1000 | Loss: 0.00003175
Iteration 75/1000 | Loss: 0.00003174
Iteration 76/1000 | Loss: 0.00003174
Iteration 77/1000 | Loss: 0.00003173
Iteration 78/1000 | Loss: 0.00003173
Iteration 79/1000 | Loss: 0.00003173
Iteration 80/1000 | Loss: 0.00003173
Iteration 81/1000 | Loss: 0.00003173
Iteration 82/1000 | Loss: 0.00003173
Iteration 83/1000 | Loss: 0.00003172
Iteration 84/1000 | Loss: 0.00003172
Iteration 85/1000 | Loss: 0.00003172
Iteration 86/1000 | Loss: 0.00003172
Iteration 87/1000 | Loss: 0.00003172
Iteration 88/1000 | Loss: 0.00003172
Iteration 89/1000 | Loss: 0.00003172
Iteration 90/1000 | Loss: 0.00003172
Iteration 91/1000 | Loss: 0.00003172
Iteration 92/1000 | Loss: 0.00003171
Iteration 93/1000 | Loss: 0.00003171
Iteration 94/1000 | Loss: 0.00003171
Iteration 95/1000 | Loss: 0.00003171
Iteration 96/1000 | Loss: 0.00003171
Iteration 97/1000 | Loss: 0.00003171
Iteration 98/1000 | Loss: 0.00003171
Iteration 99/1000 | Loss: 0.00003171
Iteration 100/1000 | Loss: 0.00003171
Iteration 101/1000 | Loss: 0.00003171
Iteration 102/1000 | Loss: 0.00003171
Iteration 103/1000 | Loss: 0.00003171
Iteration 104/1000 | Loss: 0.00003171
Iteration 105/1000 | Loss: 0.00003171
Iteration 106/1000 | Loss: 0.00003171
Iteration 107/1000 | Loss: 0.00003171
Iteration 108/1000 | Loss: 0.00003171
Iteration 109/1000 | Loss: 0.00003171
Iteration 110/1000 | Loss: 0.00003171
Iteration 111/1000 | Loss: 0.00003171
Iteration 112/1000 | Loss: 0.00003171
Iteration 113/1000 | Loss: 0.00003171
Iteration 114/1000 | Loss: 0.00003170
Iteration 115/1000 | Loss: 0.00003170
Iteration 116/1000 | Loss: 0.00003170
Iteration 117/1000 | Loss: 0.00003170
Iteration 118/1000 | Loss: 0.00003170
Iteration 119/1000 | Loss: 0.00003170
Iteration 120/1000 | Loss: 0.00003170
Iteration 121/1000 | Loss: 0.00003170
Iteration 122/1000 | Loss: 0.00003170
Iteration 123/1000 | Loss: 0.00003170
Iteration 124/1000 | Loss: 0.00003170
Iteration 125/1000 | Loss: 0.00003170
Iteration 126/1000 | Loss: 0.00003170
Iteration 127/1000 | Loss: 0.00003170
Iteration 128/1000 | Loss: 0.00003170
Iteration 129/1000 | Loss: 0.00003170
Iteration 130/1000 | Loss: 0.00003170
Iteration 131/1000 | Loss: 0.00003170
Iteration 132/1000 | Loss: 0.00003170
Iteration 133/1000 | Loss: 0.00003170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [3.1704090361017734e-05, 3.1704090361017734e-05, 3.1704090361017734e-05, 3.1704090361017734e-05, 3.1704090361017734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1704090361017734e-05

Optimization complete. Final v2v error: 4.481845855712891 mm

Highest mean error: 6.01754093170166 mm for frame 58

Lowest mean error: 3.1445600986480713 mm for frame 98

Saving results

Total time: 50.09120559692383
