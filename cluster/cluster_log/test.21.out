Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=21, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1176-1231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065576
Iteration 2/25 | Loss: 0.00209531
Iteration 3/25 | Loss: 0.00150508
Iteration 4/25 | Loss: 0.00145926
Iteration 5/25 | Loss: 0.00133865
Iteration 6/25 | Loss: 0.00128767
Iteration 7/25 | Loss: 0.00125766
Iteration 8/25 | Loss: 0.00123494
Iteration 9/25 | Loss: 0.00122590
Iteration 10/25 | Loss: 0.00121703
Iteration 11/25 | Loss: 0.00121512
Iteration 12/25 | Loss: 0.00121226
Iteration 13/25 | Loss: 0.00121135
Iteration 14/25 | Loss: 0.00121095
Iteration 15/25 | Loss: 0.00121076
Iteration 16/25 | Loss: 0.00121067
Iteration 17/25 | Loss: 0.00121059
Iteration 18/25 | Loss: 0.00121055
Iteration 19/25 | Loss: 0.00121045
Iteration 20/25 | Loss: 0.00124174
Iteration 21/25 | Loss: 0.00120327
Iteration 22/25 | Loss: 0.00120128
Iteration 23/25 | Loss: 0.00120110
Iteration 24/25 | Loss: 0.00120108
Iteration 25/25 | Loss: 0.00120108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33337760
Iteration 2/25 | Loss: 0.00139097
Iteration 3/25 | Loss: 0.00139097
Iteration 4/25 | Loss: 0.00139097
Iteration 5/25 | Loss: 0.00139097
Iteration 6/25 | Loss: 0.00139097
Iteration 7/25 | Loss: 0.00139097
Iteration 8/25 | Loss: 0.00139096
Iteration 9/25 | Loss: 0.00139096
Iteration 10/25 | Loss: 0.00139096
Iteration 11/25 | Loss: 0.00139096
Iteration 12/25 | Loss: 0.00139096
Iteration 13/25 | Loss: 0.00139096
Iteration 14/25 | Loss: 0.00139096
Iteration 15/25 | Loss: 0.00139096
Iteration 16/25 | Loss: 0.00139096
Iteration 17/25 | Loss: 0.00139096
Iteration 18/25 | Loss: 0.00139096
Iteration 19/25 | Loss: 0.00139096
Iteration 20/25 | Loss: 0.00139096
Iteration 21/25 | Loss: 0.00139096
Iteration 22/25 | Loss: 0.00139096
Iteration 23/25 | Loss: 0.00139096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013909639092162251, 0.0013909639092162251, 0.0013909639092162251, 0.0013909639092162251, 0.0013909639092162251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013909639092162251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139096
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001971
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001614
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001560
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001521
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001488
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001472
Iteration 24/1000 | Loss: 0.00001471
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001461
Iteration 33/1000 | Loss: 0.00001461
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001456
Iteration 45/1000 | Loss: 0.00001456
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001455
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001455
Iteration 59/1000 | Loss: 0.00001455
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001455
Iteration 62/1000 | Loss: 0.00001455
Iteration 63/1000 | Loss: 0.00001455
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.454985704185674e-05, 1.454985704185674e-05, 1.454985704185674e-05, 1.454985704185674e-05, 1.454985704185674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.454985704185674e-05

Optimization complete. Final v2v error: 3.2440602779388428 mm

Highest mean error: 4.414328098297119 mm for frame 67

Lowest mean error: 2.9214742183685303 mm for frame 44

Saving results

Total time: 65.39126873016357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434805
Iteration 2/25 | Loss: 0.00134326
Iteration 3/25 | Loss: 0.00125087
Iteration 4/25 | Loss: 0.00123962
Iteration 5/25 | Loss: 0.00123742
Iteration 6/25 | Loss: 0.00123742
Iteration 7/25 | Loss: 0.00123742
Iteration 8/25 | Loss: 0.00123742
Iteration 9/25 | Loss: 0.00123742
Iteration 10/25 | Loss: 0.00123742
Iteration 11/25 | Loss: 0.00123742
Iteration 12/25 | Loss: 0.00123742
Iteration 13/25 | Loss: 0.00123742
Iteration 14/25 | Loss: 0.00123742
Iteration 15/25 | Loss: 0.00123738
Iteration 16/25 | Loss: 0.00123738
Iteration 17/25 | Loss: 0.00123738
Iteration 18/25 | Loss: 0.00123738
Iteration 19/25 | Loss: 0.00123738
Iteration 20/25 | Loss: 0.00123738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012373836943879724, 0.0012373836943879724, 0.0012373836943879724, 0.0012373836943879724, 0.0012373836943879724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012373836943879724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32123852
Iteration 2/25 | Loss: 0.00111018
Iteration 3/25 | Loss: 0.00111018
Iteration 4/25 | Loss: 0.00111018
Iteration 5/25 | Loss: 0.00111018
Iteration 6/25 | Loss: 0.00111018
Iteration 7/25 | Loss: 0.00111018
Iteration 8/25 | Loss: 0.00111018
Iteration 9/25 | Loss: 0.00111018
Iteration 10/25 | Loss: 0.00111018
Iteration 11/25 | Loss: 0.00111018
Iteration 12/25 | Loss: 0.00111018
Iteration 13/25 | Loss: 0.00111018
Iteration 14/25 | Loss: 0.00111018
Iteration 15/25 | Loss: 0.00111018
Iteration 16/25 | Loss: 0.00111018
Iteration 17/25 | Loss: 0.00111018
Iteration 18/25 | Loss: 0.00111018
Iteration 19/25 | Loss: 0.00111018
Iteration 20/25 | Loss: 0.00111018
Iteration 21/25 | Loss: 0.00111018
Iteration 22/25 | Loss: 0.00111018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011101779527962208, 0.0011101779527962208, 0.0011101779527962208, 0.0011101779527962208, 0.0011101779527962208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011101779527962208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111018
Iteration 2/1000 | Loss: 0.00002227
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001408
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001305
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001280
Iteration 15/1000 | Loss: 0.00001273
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001239
Iteration 65/1000 | Loss: 0.00001239
Iteration 66/1000 | Loss: 0.00001239
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001238
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001223
Iteration 130/1000 | Loss: 0.00001223
Iteration 131/1000 | Loss: 0.00001223
Iteration 132/1000 | Loss: 0.00001223
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001222
Iteration 136/1000 | Loss: 0.00001222
Iteration 137/1000 | Loss: 0.00001222
Iteration 138/1000 | Loss: 0.00001222
Iteration 139/1000 | Loss: 0.00001222
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Iteration 142/1000 | Loss: 0.00001222
Iteration 143/1000 | Loss: 0.00001221
Iteration 144/1000 | Loss: 0.00001221
Iteration 145/1000 | Loss: 0.00001221
Iteration 146/1000 | Loss: 0.00001221
Iteration 147/1000 | Loss: 0.00001221
Iteration 148/1000 | Loss: 0.00001221
Iteration 149/1000 | Loss: 0.00001221
Iteration 150/1000 | Loss: 0.00001221
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001220
Iteration 158/1000 | Loss: 0.00001220
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001219
Iteration 161/1000 | Loss: 0.00001219
Iteration 162/1000 | Loss: 0.00001219
Iteration 163/1000 | Loss: 0.00001219
Iteration 164/1000 | Loss: 0.00001219
Iteration 165/1000 | Loss: 0.00001219
Iteration 166/1000 | Loss: 0.00001219
Iteration 167/1000 | Loss: 0.00001219
Iteration 168/1000 | Loss: 0.00001219
Iteration 169/1000 | Loss: 0.00001219
Iteration 170/1000 | Loss: 0.00001219
Iteration 171/1000 | Loss: 0.00001219
Iteration 172/1000 | Loss: 0.00001219
Iteration 173/1000 | Loss: 0.00001219
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.2185256309749093e-05, 1.2185256309749093e-05, 1.2185256309749093e-05, 1.2185256309749093e-05, 1.2185256309749093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2185256309749093e-05

Optimization complete. Final v2v error: 2.9585201740264893 mm

Highest mean error: 3.138993263244629 mm for frame 178

Lowest mean error: 2.7848074436187744 mm for frame 238

Saving results

Total time: 44.21258997917175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901988
Iteration 2/25 | Loss: 0.00171207
Iteration 3/25 | Loss: 0.00136116
Iteration 4/25 | Loss: 0.00130632
Iteration 5/25 | Loss: 0.00129272
Iteration 6/25 | Loss: 0.00129004
Iteration 7/25 | Loss: 0.00128927
Iteration 8/25 | Loss: 0.00128927
Iteration 9/25 | Loss: 0.00128927
Iteration 10/25 | Loss: 0.00128927
Iteration 11/25 | Loss: 0.00128927
Iteration 12/25 | Loss: 0.00128927
Iteration 13/25 | Loss: 0.00128927
Iteration 14/25 | Loss: 0.00128927
Iteration 15/25 | Loss: 0.00128927
Iteration 16/25 | Loss: 0.00128927
Iteration 17/25 | Loss: 0.00128927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001289266743697226, 0.001289266743697226, 0.001289266743697226, 0.001289266743697226, 0.001289266743697226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001289266743697226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40929139
Iteration 2/25 | Loss: 0.00152459
Iteration 3/25 | Loss: 0.00152450
Iteration 4/25 | Loss: 0.00152450
Iteration 5/25 | Loss: 0.00152450
Iteration 6/25 | Loss: 0.00152450
Iteration 7/25 | Loss: 0.00152450
Iteration 8/25 | Loss: 0.00152450
Iteration 9/25 | Loss: 0.00152450
Iteration 10/25 | Loss: 0.00152450
Iteration 11/25 | Loss: 0.00152450
Iteration 12/25 | Loss: 0.00152450
Iteration 13/25 | Loss: 0.00152450
Iteration 14/25 | Loss: 0.00152450
Iteration 15/25 | Loss: 0.00152450
Iteration 16/25 | Loss: 0.00152450
Iteration 17/25 | Loss: 0.00152450
Iteration 18/25 | Loss: 0.00152450
Iteration 19/25 | Loss: 0.00152450
Iteration 20/25 | Loss: 0.00152450
Iteration 21/25 | Loss: 0.00152450
Iteration 22/25 | Loss: 0.00152450
Iteration 23/25 | Loss: 0.00152450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015245015965774655, 0.0015245015965774655, 0.0015245015965774655, 0.0015245015965774655, 0.0015245015965774655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015245015965774655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152450
Iteration 2/1000 | Loss: 0.00007518
Iteration 3/1000 | Loss: 0.00004895
Iteration 4/1000 | Loss: 0.00003732
Iteration 5/1000 | Loss: 0.00003477
Iteration 6/1000 | Loss: 0.00003317
Iteration 7/1000 | Loss: 0.00003227
Iteration 8/1000 | Loss: 0.00003151
Iteration 9/1000 | Loss: 0.00003095
Iteration 10/1000 | Loss: 0.00003056
Iteration 11/1000 | Loss: 0.00003020
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00002968
Iteration 14/1000 | Loss: 0.00002946
Iteration 15/1000 | Loss: 0.00002929
Iteration 16/1000 | Loss: 0.00002926
Iteration 17/1000 | Loss: 0.00002924
Iteration 18/1000 | Loss: 0.00002911
Iteration 19/1000 | Loss: 0.00002900
Iteration 20/1000 | Loss: 0.00002898
Iteration 21/1000 | Loss: 0.00002885
Iteration 22/1000 | Loss: 0.00002881
Iteration 23/1000 | Loss: 0.00002878
Iteration 24/1000 | Loss: 0.00002877
Iteration 25/1000 | Loss: 0.00002877
Iteration 26/1000 | Loss: 0.00002870
Iteration 27/1000 | Loss: 0.00002866
Iteration 28/1000 | Loss: 0.00002866
Iteration 29/1000 | Loss: 0.00002863
Iteration 30/1000 | Loss: 0.00002862
Iteration 31/1000 | Loss: 0.00002862
Iteration 32/1000 | Loss: 0.00002861
Iteration 33/1000 | Loss: 0.00002860
Iteration 34/1000 | Loss: 0.00002860
Iteration 35/1000 | Loss: 0.00002860
Iteration 36/1000 | Loss: 0.00002860
Iteration 37/1000 | Loss: 0.00002860
Iteration 38/1000 | Loss: 0.00002859
Iteration 39/1000 | Loss: 0.00002859
Iteration 40/1000 | Loss: 0.00002858
Iteration 41/1000 | Loss: 0.00002857
Iteration 42/1000 | Loss: 0.00002857
Iteration 43/1000 | Loss: 0.00002857
Iteration 44/1000 | Loss: 0.00002856
Iteration 45/1000 | Loss: 0.00002856
Iteration 46/1000 | Loss: 0.00002856
Iteration 47/1000 | Loss: 0.00002856
Iteration 48/1000 | Loss: 0.00002855
Iteration 49/1000 | Loss: 0.00002855
Iteration 50/1000 | Loss: 0.00002855
Iteration 51/1000 | Loss: 0.00002854
Iteration 52/1000 | Loss: 0.00002854
Iteration 53/1000 | Loss: 0.00002854
Iteration 54/1000 | Loss: 0.00002853
Iteration 55/1000 | Loss: 0.00002853
Iteration 56/1000 | Loss: 0.00002853
Iteration 57/1000 | Loss: 0.00002852
Iteration 58/1000 | Loss: 0.00002852
Iteration 59/1000 | Loss: 0.00002852
Iteration 60/1000 | Loss: 0.00002852
Iteration 61/1000 | Loss: 0.00002852
Iteration 62/1000 | Loss: 0.00002852
Iteration 63/1000 | Loss: 0.00002851
Iteration 64/1000 | Loss: 0.00002851
Iteration 65/1000 | Loss: 0.00002851
Iteration 66/1000 | Loss: 0.00002851
Iteration 67/1000 | Loss: 0.00002851
Iteration 68/1000 | Loss: 0.00002851
Iteration 69/1000 | Loss: 0.00002851
Iteration 70/1000 | Loss: 0.00002850
Iteration 71/1000 | Loss: 0.00002850
Iteration 72/1000 | Loss: 0.00002850
Iteration 73/1000 | Loss: 0.00002850
Iteration 74/1000 | Loss: 0.00002850
Iteration 75/1000 | Loss: 0.00002849
Iteration 76/1000 | Loss: 0.00002849
Iteration 77/1000 | Loss: 0.00002849
Iteration 78/1000 | Loss: 0.00002849
Iteration 79/1000 | Loss: 0.00002849
Iteration 80/1000 | Loss: 0.00002849
Iteration 81/1000 | Loss: 0.00002849
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00002848
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002847
Iteration 86/1000 | Loss: 0.00002847
Iteration 87/1000 | Loss: 0.00002847
Iteration 88/1000 | Loss: 0.00002847
Iteration 89/1000 | Loss: 0.00002847
Iteration 90/1000 | Loss: 0.00002847
Iteration 91/1000 | Loss: 0.00002847
Iteration 92/1000 | Loss: 0.00002847
Iteration 93/1000 | Loss: 0.00002847
Iteration 94/1000 | Loss: 0.00002846
Iteration 95/1000 | Loss: 0.00002846
Iteration 96/1000 | Loss: 0.00002846
Iteration 97/1000 | Loss: 0.00002846
Iteration 98/1000 | Loss: 0.00002846
Iteration 99/1000 | Loss: 0.00002846
Iteration 100/1000 | Loss: 0.00002846
Iteration 101/1000 | Loss: 0.00002846
Iteration 102/1000 | Loss: 0.00002846
Iteration 103/1000 | Loss: 0.00002846
Iteration 104/1000 | Loss: 0.00002846
Iteration 105/1000 | Loss: 0.00002846
Iteration 106/1000 | Loss: 0.00002845
Iteration 107/1000 | Loss: 0.00002845
Iteration 108/1000 | Loss: 0.00002845
Iteration 109/1000 | Loss: 0.00002845
Iteration 110/1000 | Loss: 0.00002845
Iteration 111/1000 | Loss: 0.00002845
Iteration 112/1000 | Loss: 0.00002845
Iteration 113/1000 | Loss: 0.00002845
Iteration 114/1000 | Loss: 0.00002844
Iteration 115/1000 | Loss: 0.00002844
Iteration 116/1000 | Loss: 0.00002844
Iteration 117/1000 | Loss: 0.00002844
Iteration 118/1000 | Loss: 0.00002844
Iteration 119/1000 | Loss: 0.00002844
Iteration 120/1000 | Loss: 0.00002844
Iteration 121/1000 | Loss: 0.00002844
Iteration 122/1000 | Loss: 0.00002844
Iteration 123/1000 | Loss: 0.00002843
Iteration 124/1000 | Loss: 0.00002843
Iteration 125/1000 | Loss: 0.00002843
Iteration 126/1000 | Loss: 0.00002842
Iteration 127/1000 | Loss: 0.00002842
Iteration 128/1000 | Loss: 0.00002842
Iteration 129/1000 | Loss: 0.00002842
Iteration 130/1000 | Loss: 0.00002841
Iteration 131/1000 | Loss: 0.00002841
Iteration 132/1000 | Loss: 0.00002841
Iteration 133/1000 | Loss: 0.00002841
Iteration 134/1000 | Loss: 0.00002841
Iteration 135/1000 | Loss: 0.00002841
Iteration 136/1000 | Loss: 0.00002840
Iteration 137/1000 | Loss: 0.00002840
Iteration 138/1000 | Loss: 0.00002840
Iteration 139/1000 | Loss: 0.00002840
Iteration 140/1000 | Loss: 0.00002840
Iteration 141/1000 | Loss: 0.00002840
Iteration 142/1000 | Loss: 0.00002840
Iteration 143/1000 | Loss: 0.00002840
Iteration 144/1000 | Loss: 0.00002840
Iteration 145/1000 | Loss: 0.00002840
Iteration 146/1000 | Loss: 0.00002839
Iteration 147/1000 | Loss: 0.00002839
Iteration 148/1000 | Loss: 0.00002839
Iteration 149/1000 | Loss: 0.00002839
Iteration 150/1000 | Loss: 0.00002839
Iteration 151/1000 | Loss: 0.00002839
Iteration 152/1000 | Loss: 0.00002839
Iteration 153/1000 | Loss: 0.00002839
Iteration 154/1000 | Loss: 0.00002839
Iteration 155/1000 | Loss: 0.00002839
Iteration 156/1000 | Loss: 0.00002839
Iteration 157/1000 | Loss: 0.00002839
Iteration 158/1000 | Loss: 0.00002839
Iteration 159/1000 | Loss: 0.00002839
Iteration 160/1000 | Loss: 0.00002839
Iteration 161/1000 | Loss: 0.00002839
Iteration 162/1000 | Loss: 0.00002839
Iteration 163/1000 | Loss: 0.00002839
Iteration 164/1000 | Loss: 0.00002839
Iteration 165/1000 | Loss: 0.00002839
Iteration 166/1000 | Loss: 0.00002839
Iteration 167/1000 | Loss: 0.00002839
Iteration 168/1000 | Loss: 0.00002839
Iteration 169/1000 | Loss: 0.00002839
Iteration 170/1000 | Loss: 0.00002839
Iteration 171/1000 | Loss: 0.00002839
Iteration 172/1000 | Loss: 0.00002839
Iteration 173/1000 | Loss: 0.00002839
Iteration 174/1000 | Loss: 0.00002839
Iteration 175/1000 | Loss: 0.00002839
Iteration 176/1000 | Loss: 0.00002839
Iteration 177/1000 | Loss: 0.00002839
Iteration 178/1000 | Loss: 0.00002839
Iteration 179/1000 | Loss: 0.00002839
Iteration 180/1000 | Loss: 0.00002839
Iteration 181/1000 | Loss: 0.00002839
Iteration 182/1000 | Loss: 0.00002839
Iteration 183/1000 | Loss: 0.00002839
Iteration 184/1000 | Loss: 0.00002839
Iteration 185/1000 | Loss: 0.00002839
Iteration 186/1000 | Loss: 0.00002839
Iteration 187/1000 | Loss: 0.00002839
Iteration 188/1000 | Loss: 0.00002839
Iteration 189/1000 | Loss: 0.00002839
Iteration 190/1000 | Loss: 0.00002839
Iteration 191/1000 | Loss: 0.00002839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.8385526093188673e-05, 2.8385526093188673e-05, 2.8385526093188673e-05, 2.8385526093188673e-05, 2.8385526093188673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8385526093188673e-05

Optimization complete. Final v2v error: 4.3700408935546875 mm

Highest mean error: 6.629586219787598 mm for frame 82

Lowest mean error: 3.2012948989868164 mm for frame 33

Saving results

Total time: 48.243775367736816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412565
Iteration 2/25 | Loss: 0.00141381
Iteration 3/25 | Loss: 0.00124371
Iteration 4/25 | Loss: 0.00121446
Iteration 5/25 | Loss: 0.00120808
Iteration 6/25 | Loss: 0.00120717
Iteration 7/25 | Loss: 0.00120717
Iteration 8/25 | Loss: 0.00120717
Iteration 9/25 | Loss: 0.00120717
Iteration 10/25 | Loss: 0.00120717
Iteration 11/25 | Loss: 0.00120717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012071722885593772, 0.0012071722885593772, 0.0012071722885593772, 0.0012071722885593772, 0.0012071722885593772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012071722885593772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30121732
Iteration 2/25 | Loss: 0.00112212
Iteration 3/25 | Loss: 0.00112212
Iteration 4/25 | Loss: 0.00112212
Iteration 5/25 | Loss: 0.00112212
Iteration 6/25 | Loss: 0.00112212
Iteration 7/25 | Loss: 0.00112212
Iteration 8/25 | Loss: 0.00112212
Iteration 9/25 | Loss: 0.00112212
Iteration 10/25 | Loss: 0.00112212
Iteration 11/25 | Loss: 0.00112212
Iteration 12/25 | Loss: 0.00112212
Iteration 13/25 | Loss: 0.00112212
Iteration 14/25 | Loss: 0.00112212
Iteration 15/25 | Loss: 0.00112212
Iteration 16/25 | Loss: 0.00112212
Iteration 17/25 | Loss: 0.00112212
Iteration 18/25 | Loss: 0.00112212
Iteration 19/25 | Loss: 0.00112212
Iteration 20/25 | Loss: 0.00112212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001122117624618113, 0.001122117624618113, 0.001122117624618113, 0.001122117624618113, 0.001122117624618113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001122117624618113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112212
Iteration 2/1000 | Loss: 0.00003021
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001550
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001454
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001390
Iteration 11/1000 | Loss: 0.00001355
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001350
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001313
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001291
Iteration 26/1000 | Loss: 0.00001290
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001288
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001286
Iteration 35/1000 | Loss: 0.00001286
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001282
Iteration 40/1000 | Loss: 0.00001281
Iteration 41/1000 | Loss: 0.00001281
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001280
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001274
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001272
Iteration 55/1000 | Loss: 0.00001272
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001270
Iteration 61/1000 | Loss: 0.00001270
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001267
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001267
Iteration 70/1000 | Loss: 0.00001266
Iteration 71/1000 | Loss: 0.00001266
Iteration 72/1000 | Loss: 0.00001266
Iteration 73/1000 | Loss: 0.00001266
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001265
Iteration 77/1000 | Loss: 0.00001265
Iteration 78/1000 | Loss: 0.00001265
Iteration 79/1000 | Loss: 0.00001265
Iteration 80/1000 | Loss: 0.00001265
Iteration 81/1000 | Loss: 0.00001265
Iteration 82/1000 | Loss: 0.00001265
Iteration 83/1000 | Loss: 0.00001264
Iteration 84/1000 | Loss: 0.00001264
Iteration 85/1000 | Loss: 0.00001264
Iteration 86/1000 | Loss: 0.00001263
Iteration 87/1000 | Loss: 0.00001263
Iteration 88/1000 | Loss: 0.00001263
Iteration 89/1000 | Loss: 0.00001263
Iteration 90/1000 | Loss: 0.00001262
Iteration 91/1000 | Loss: 0.00001262
Iteration 92/1000 | Loss: 0.00001262
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001261
Iteration 102/1000 | Loss: 0.00001261
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001260
Iteration 107/1000 | Loss: 0.00001260
Iteration 108/1000 | Loss: 0.00001260
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001255
Iteration 129/1000 | Loss: 0.00001255
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001252
Iteration 140/1000 | Loss: 0.00001252
Iteration 141/1000 | Loss: 0.00001252
Iteration 142/1000 | Loss: 0.00001252
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001251
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001251
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001250
Iteration 152/1000 | Loss: 0.00001250
Iteration 153/1000 | Loss: 0.00001250
Iteration 154/1000 | Loss: 0.00001249
Iteration 155/1000 | Loss: 0.00001249
Iteration 156/1000 | Loss: 0.00001249
Iteration 157/1000 | Loss: 0.00001249
Iteration 158/1000 | Loss: 0.00001249
Iteration 159/1000 | Loss: 0.00001249
Iteration 160/1000 | Loss: 0.00001249
Iteration 161/1000 | Loss: 0.00001249
Iteration 162/1000 | Loss: 0.00001249
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Iteration 165/1000 | Loss: 0.00001249
Iteration 166/1000 | Loss: 0.00001249
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001249
Iteration 174/1000 | Loss: 0.00001249
Iteration 175/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.2488130778365303e-05, 1.2488130778365303e-05, 1.2488130778365303e-05, 1.2488130778365303e-05, 1.2488130778365303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2488130778365303e-05

Optimization complete. Final v2v error: 3.0295093059539795 mm

Highest mean error: 3.672344923019409 mm for frame 4

Lowest mean error: 2.7587358951568604 mm for frame 196

Saving results

Total time: 47.50208902359009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837352
Iteration 2/25 | Loss: 0.00143650
Iteration 3/25 | Loss: 0.00130380
Iteration 4/25 | Loss: 0.00128140
Iteration 5/25 | Loss: 0.00127415
Iteration 6/25 | Loss: 0.00127243
Iteration 7/25 | Loss: 0.00127243
Iteration 8/25 | Loss: 0.00127243
Iteration 9/25 | Loss: 0.00127243
Iteration 10/25 | Loss: 0.00127243
Iteration 11/25 | Loss: 0.00127243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012724269181489944, 0.0012724269181489944, 0.0012724269181489944, 0.0012724269181489944, 0.0012724269181489944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012724269181489944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.91725874
Iteration 2/25 | Loss: 0.00159941
Iteration 3/25 | Loss: 0.00159921
Iteration 4/25 | Loss: 0.00159921
Iteration 5/25 | Loss: 0.00159921
Iteration 6/25 | Loss: 0.00159921
Iteration 7/25 | Loss: 0.00159921
Iteration 8/25 | Loss: 0.00159921
Iteration 9/25 | Loss: 0.00159921
Iteration 10/25 | Loss: 0.00159921
Iteration 11/25 | Loss: 0.00159921
Iteration 12/25 | Loss: 0.00159921
Iteration 13/25 | Loss: 0.00159921
Iteration 14/25 | Loss: 0.00159921
Iteration 15/25 | Loss: 0.00159921
Iteration 16/25 | Loss: 0.00159921
Iteration 17/25 | Loss: 0.00159921
Iteration 18/25 | Loss: 0.00159921
Iteration 19/25 | Loss: 0.00159921
Iteration 20/25 | Loss: 0.00159921
Iteration 21/25 | Loss: 0.00159921
Iteration 22/25 | Loss: 0.00159921
Iteration 23/25 | Loss: 0.00159921
Iteration 24/25 | Loss: 0.00159921
Iteration 25/25 | Loss: 0.00159921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159921
Iteration 2/1000 | Loss: 0.00006174
Iteration 3/1000 | Loss: 0.00004129
Iteration 4/1000 | Loss: 0.00003492
Iteration 5/1000 | Loss: 0.00003236
Iteration 6/1000 | Loss: 0.00003068
Iteration 7/1000 | Loss: 0.00002953
Iteration 8/1000 | Loss: 0.00002873
Iteration 9/1000 | Loss: 0.00002809
Iteration 10/1000 | Loss: 0.00002773
Iteration 11/1000 | Loss: 0.00002729
Iteration 12/1000 | Loss: 0.00002696
Iteration 13/1000 | Loss: 0.00002673
Iteration 14/1000 | Loss: 0.00002649
Iteration 15/1000 | Loss: 0.00002631
Iteration 16/1000 | Loss: 0.00002625
Iteration 17/1000 | Loss: 0.00002611
Iteration 18/1000 | Loss: 0.00002598
Iteration 19/1000 | Loss: 0.00002598
Iteration 20/1000 | Loss: 0.00002595
Iteration 21/1000 | Loss: 0.00002594
Iteration 22/1000 | Loss: 0.00002588
Iteration 23/1000 | Loss: 0.00002587
Iteration 24/1000 | Loss: 0.00002584
Iteration 25/1000 | Loss: 0.00002584
Iteration 26/1000 | Loss: 0.00002583
Iteration 27/1000 | Loss: 0.00002582
Iteration 28/1000 | Loss: 0.00002581
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00002579
Iteration 31/1000 | Loss: 0.00002579
Iteration 32/1000 | Loss: 0.00002578
Iteration 33/1000 | Loss: 0.00002578
Iteration 34/1000 | Loss: 0.00002578
Iteration 35/1000 | Loss: 0.00002576
Iteration 36/1000 | Loss: 0.00002574
Iteration 37/1000 | Loss: 0.00002573
Iteration 38/1000 | Loss: 0.00002573
Iteration 39/1000 | Loss: 0.00002573
Iteration 40/1000 | Loss: 0.00002572
Iteration 41/1000 | Loss: 0.00002572
Iteration 42/1000 | Loss: 0.00002572
Iteration 43/1000 | Loss: 0.00002571
Iteration 44/1000 | Loss: 0.00002571
Iteration 45/1000 | Loss: 0.00002570
Iteration 46/1000 | Loss: 0.00002570
Iteration 47/1000 | Loss: 0.00002564
Iteration 48/1000 | Loss: 0.00002564
Iteration 49/1000 | Loss: 0.00002564
Iteration 50/1000 | Loss: 0.00002561
Iteration 51/1000 | Loss: 0.00002561
Iteration 52/1000 | Loss: 0.00002561
Iteration 53/1000 | Loss: 0.00002561
Iteration 54/1000 | Loss: 0.00002561
Iteration 55/1000 | Loss: 0.00002561
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002560
Iteration 58/1000 | Loss: 0.00002560
Iteration 59/1000 | Loss: 0.00002560
Iteration 60/1000 | Loss: 0.00002559
Iteration 61/1000 | Loss: 0.00002559
Iteration 62/1000 | Loss: 0.00002558
Iteration 63/1000 | Loss: 0.00002558
Iteration 64/1000 | Loss: 0.00002558
Iteration 65/1000 | Loss: 0.00002558
Iteration 66/1000 | Loss: 0.00002557
Iteration 67/1000 | Loss: 0.00002557
Iteration 68/1000 | Loss: 0.00002557
Iteration 69/1000 | Loss: 0.00002557
Iteration 70/1000 | Loss: 0.00002557
Iteration 71/1000 | Loss: 0.00002557
Iteration 72/1000 | Loss: 0.00002556
Iteration 73/1000 | Loss: 0.00002556
Iteration 74/1000 | Loss: 0.00002556
Iteration 75/1000 | Loss: 0.00002556
Iteration 76/1000 | Loss: 0.00002556
Iteration 77/1000 | Loss: 0.00002555
Iteration 78/1000 | Loss: 0.00002555
Iteration 79/1000 | Loss: 0.00002555
Iteration 80/1000 | Loss: 0.00002555
Iteration 81/1000 | Loss: 0.00002554
Iteration 82/1000 | Loss: 0.00002554
Iteration 83/1000 | Loss: 0.00002554
Iteration 84/1000 | Loss: 0.00002554
Iteration 85/1000 | Loss: 0.00002554
Iteration 86/1000 | Loss: 0.00002554
Iteration 87/1000 | Loss: 0.00002553
Iteration 88/1000 | Loss: 0.00002553
Iteration 89/1000 | Loss: 0.00002553
Iteration 90/1000 | Loss: 0.00002553
Iteration 91/1000 | Loss: 0.00002553
Iteration 92/1000 | Loss: 0.00002552
Iteration 93/1000 | Loss: 0.00002552
Iteration 94/1000 | Loss: 0.00002552
Iteration 95/1000 | Loss: 0.00002552
Iteration 96/1000 | Loss: 0.00002551
Iteration 97/1000 | Loss: 0.00002551
Iteration 98/1000 | Loss: 0.00002550
Iteration 99/1000 | Loss: 0.00002550
Iteration 100/1000 | Loss: 0.00002550
Iteration 101/1000 | Loss: 0.00002549
Iteration 102/1000 | Loss: 0.00002549
Iteration 103/1000 | Loss: 0.00002549
Iteration 104/1000 | Loss: 0.00002548
Iteration 105/1000 | Loss: 0.00002548
Iteration 106/1000 | Loss: 0.00002548
Iteration 107/1000 | Loss: 0.00002548
Iteration 108/1000 | Loss: 0.00002547
Iteration 109/1000 | Loss: 0.00002547
Iteration 110/1000 | Loss: 0.00002547
Iteration 111/1000 | Loss: 0.00002547
Iteration 112/1000 | Loss: 0.00002546
Iteration 113/1000 | Loss: 0.00002546
Iteration 114/1000 | Loss: 0.00002546
Iteration 115/1000 | Loss: 0.00002546
Iteration 116/1000 | Loss: 0.00002546
Iteration 117/1000 | Loss: 0.00002546
Iteration 118/1000 | Loss: 0.00002546
Iteration 119/1000 | Loss: 0.00002545
Iteration 120/1000 | Loss: 0.00002545
Iteration 121/1000 | Loss: 0.00002545
Iteration 122/1000 | Loss: 0.00002544
Iteration 123/1000 | Loss: 0.00002544
Iteration 124/1000 | Loss: 0.00002544
Iteration 125/1000 | Loss: 0.00002544
Iteration 126/1000 | Loss: 0.00002544
Iteration 127/1000 | Loss: 0.00002543
Iteration 128/1000 | Loss: 0.00002543
Iteration 129/1000 | Loss: 0.00002543
Iteration 130/1000 | Loss: 0.00002543
Iteration 131/1000 | Loss: 0.00002542
Iteration 132/1000 | Loss: 0.00002542
Iteration 133/1000 | Loss: 0.00002542
Iteration 134/1000 | Loss: 0.00002541
Iteration 135/1000 | Loss: 0.00002541
Iteration 136/1000 | Loss: 0.00002541
Iteration 137/1000 | Loss: 0.00002541
Iteration 138/1000 | Loss: 0.00002541
Iteration 139/1000 | Loss: 0.00002541
Iteration 140/1000 | Loss: 0.00002540
Iteration 141/1000 | Loss: 0.00002540
Iteration 142/1000 | Loss: 0.00002540
Iteration 143/1000 | Loss: 0.00002540
Iteration 144/1000 | Loss: 0.00002540
Iteration 145/1000 | Loss: 0.00002540
Iteration 146/1000 | Loss: 0.00002540
Iteration 147/1000 | Loss: 0.00002540
Iteration 148/1000 | Loss: 0.00002540
Iteration 149/1000 | Loss: 0.00002539
Iteration 150/1000 | Loss: 0.00002539
Iteration 151/1000 | Loss: 0.00002539
Iteration 152/1000 | Loss: 0.00002539
Iteration 153/1000 | Loss: 0.00002539
Iteration 154/1000 | Loss: 0.00002539
Iteration 155/1000 | Loss: 0.00002538
Iteration 156/1000 | Loss: 0.00002538
Iteration 157/1000 | Loss: 0.00002538
Iteration 158/1000 | Loss: 0.00002538
Iteration 159/1000 | Loss: 0.00002538
Iteration 160/1000 | Loss: 0.00002537
Iteration 161/1000 | Loss: 0.00002537
Iteration 162/1000 | Loss: 0.00002537
Iteration 163/1000 | Loss: 0.00002537
Iteration 164/1000 | Loss: 0.00002537
Iteration 165/1000 | Loss: 0.00002537
Iteration 166/1000 | Loss: 0.00002537
Iteration 167/1000 | Loss: 0.00002537
Iteration 168/1000 | Loss: 0.00002537
Iteration 169/1000 | Loss: 0.00002536
Iteration 170/1000 | Loss: 0.00002536
Iteration 171/1000 | Loss: 0.00002536
Iteration 172/1000 | Loss: 0.00002536
Iteration 173/1000 | Loss: 0.00002535
Iteration 174/1000 | Loss: 0.00002535
Iteration 175/1000 | Loss: 0.00002535
Iteration 176/1000 | Loss: 0.00002535
Iteration 177/1000 | Loss: 0.00002535
Iteration 178/1000 | Loss: 0.00002535
Iteration 179/1000 | Loss: 0.00002535
Iteration 180/1000 | Loss: 0.00002535
Iteration 181/1000 | Loss: 0.00002535
Iteration 182/1000 | Loss: 0.00002534
Iteration 183/1000 | Loss: 0.00002534
Iteration 184/1000 | Loss: 0.00002534
Iteration 185/1000 | Loss: 0.00002534
Iteration 186/1000 | Loss: 0.00002534
Iteration 187/1000 | Loss: 0.00002534
Iteration 188/1000 | Loss: 0.00002534
Iteration 189/1000 | Loss: 0.00002534
Iteration 190/1000 | Loss: 0.00002534
Iteration 191/1000 | Loss: 0.00002534
Iteration 192/1000 | Loss: 0.00002534
Iteration 193/1000 | Loss: 0.00002534
Iteration 194/1000 | Loss: 0.00002534
Iteration 195/1000 | Loss: 0.00002534
Iteration 196/1000 | Loss: 0.00002534
Iteration 197/1000 | Loss: 0.00002534
Iteration 198/1000 | Loss: 0.00002534
Iteration 199/1000 | Loss: 0.00002534
Iteration 200/1000 | Loss: 0.00002534
Iteration 201/1000 | Loss: 0.00002534
Iteration 202/1000 | Loss: 0.00002534
Iteration 203/1000 | Loss: 0.00002534
Iteration 204/1000 | Loss: 0.00002534
Iteration 205/1000 | Loss: 0.00002534
Iteration 206/1000 | Loss: 0.00002534
Iteration 207/1000 | Loss: 0.00002534
Iteration 208/1000 | Loss: 0.00002534
Iteration 209/1000 | Loss: 0.00002534
Iteration 210/1000 | Loss: 0.00002534
Iteration 211/1000 | Loss: 0.00002534
Iteration 212/1000 | Loss: 0.00002534
Iteration 213/1000 | Loss: 0.00002534
Iteration 214/1000 | Loss: 0.00002534
Iteration 215/1000 | Loss: 0.00002534
Iteration 216/1000 | Loss: 0.00002534
Iteration 217/1000 | Loss: 0.00002534
Iteration 218/1000 | Loss: 0.00002534
Iteration 219/1000 | Loss: 0.00002534
Iteration 220/1000 | Loss: 0.00002534
Iteration 221/1000 | Loss: 0.00002534
Iteration 222/1000 | Loss: 0.00002534
Iteration 223/1000 | Loss: 0.00002534
Iteration 224/1000 | Loss: 0.00002534
Iteration 225/1000 | Loss: 0.00002534
Iteration 226/1000 | Loss: 0.00002534
Iteration 227/1000 | Loss: 0.00002534
Iteration 228/1000 | Loss: 0.00002534
Iteration 229/1000 | Loss: 0.00002534
Iteration 230/1000 | Loss: 0.00002534
Iteration 231/1000 | Loss: 0.00002534
Iteration 232/1000 | Loss: 0.00002534
Iteration 233/1000 | Loss: 0.00002534
Iteration 234/1000 | Loss: 0.00002534
Iteration 235/1000 | Loss: 0.00002534
Iteration 236/1000 | Loss: 0.00002534
Iteration 237/1000 | Loss: 0.00002534
Iteration 238/1000 | Loss: 0.00002534
Iteration 239/1000 | Loss: 0.00002534
Iteration 240/1000 | Loss: 0.00002534
Iteration 241/1000 | Loss: 0.00002534
Iteration 242/1000 | Loss: 0.00002534
Iteration 243/1000 | Loss: 0.00002534
Iteration 244/1000 | Loss: 0.00002534
Iteration 245/1000 | Loss: 0.00002534
Iteration 246/1000 | Loss: 0.00002534
Iteration 247/1000 | Loss: 0.00002534
Iteration 248/1000 | Loss: 0.00002534
Iteration 249/1000 | Loss: 0.00002534
Iteration 250/1000 | Loss: 0.00002534
Iteration 251/1000 | Loss: 0.00002534
Iteration 252/1000 | Loss: 0.00002534
Iteration 253/1000 | Loss: 0.00002534
Iteration 254/1000 | Loss: 0.00002534
Iteration 255/1000 | Loss: 0.00002534
Iteration 256/1000 | Loss: 0.00002534
Iteration 257/1000 | Loss: 0.00002534
Iteration 258/1000 | Loss: 0.00002534
Iteration 259/1000 | Loss: 0.00002534
Iteration 260/1000 | Loss: 0.00002534
Iteration 261/1000 | Loss: 0.00002534
Iteration 262/1000 | Loss: 0.00002534
Iteration 263/1000 | Loss: 0.00002534
Iteration 264/1000 | Loss: 0.00002534
Iteration 265/1000 | Loss: 0.00002534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.533687074901536e-05, 2.533687074901536e-05, 2.533687074901536e-05, 2.533687074901536e-05, 2.533687074901536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.533687074901536e-05

Optimization complete. Final v2v error: 4.152697563171387 mm

Highest mean error: 6.386931419372559 mm for frame 130

Lowest mean error: 2.9504904747009277 mm for frame 153

Saving results

Total time: 58.831031799316406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448807
Iteration 2/25 | Loss: 0.00133478
Iteration 3/25 | Loss: 0.00122267
Iteration 4/25 | Loss: 0.00120221
Iteration 5/25 | Loss: 0.00119758
Iteration 6/25 | Loss: 0.00119715
Iteration 7/25 | Loss: 0.00119715
Iteration 8/25 | Loss: 0.00119715
Iteration 9/25 | Loss: 0.00119715
Iteration 10/25 | Loss: 0.00119715
Iteration 11/25 | Loss: 0.00119715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011971504427492619, 0.0011971504427492619, 0.0011971504427492619, 0.0011971504427492619, 0.0011971504427492619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011971504427492619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.15355206
Iteration 2/25 | Loss: 0.00126704
Iteration 3/25 | Loss: 0.00126701
Iteration 4/25 | Loss: 0.00126701
Iteration 5/25 | Loss: 0.00126701
Iteration 6/25 | Loss: 0.00126701
Iteration 7/25 | Loss: 0.00126701
Iteration 8/25 | Loss: 0.00126701
Iteration 9/25 | Loss: 0.00126701
Iteration 10/25 | Loss: 0.00126701
Iteration 11/25 | Loss: 0.00126701
Iteration 12/25 | Loss: 0.00126701
Iteration 13/25 | Loss: 0.00126701
Iteration 14/25 | Loss: 0.00126701
Iteration 15/25 | Loss: 0.00126701
Iteration 16/25 | Loss: 0.00126701
Iteration 17/25 | Loss: 0.00126701
Iteration 18/25 | Loss: 0.00126701
Iteration 19/25 | Loss: 0.00126701
Iteration 20/25 | Loss: 0.00126701
Iteration 21/25 | Loss: 0.00126701
Iteration 22/25 | Loss: 0.00126701
Iteration 23/25 | Loss: 0.00126701
Iteration 24/25 | Loss: 0.00126701
Iteration 25/25 | Loss: 0.00126701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126701
Iteration 2/1000 | Loss: 0.00002453
Iteration 3/1000 | Loss: 0.00001784
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001560
Iteration 6/1000 | Loss: 0.00001485
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001367
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001313
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001264
Iteration 18/1000 | Loss: 0.00001263
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001260
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001244
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001242
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001239
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001234
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2271413652342744e-05, 1.2271413652342744e-05, 1.2271413652342744e-05, 1.2271413652342744e-05, 1.2271413652342744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2271413652342744e-05

Optimization complete. Final v2v error: 3.0055794715881348 mm

Highest mean error: 3.4098570346832275 mm for frame 68

Lowest mean error: 2.6596012115478516 mm for frame 0

Saving results

Total time: 43.39285397529602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754653
Iteration 2/25 | Loss: 0.00131452
Iteration 3/25 | Loss: 0.00119217
Iteration 4/25 | Loss: 0.00117690
Iteration 5/25 | Loss: 0.00117306
Iteration 6/25 | Loss: 0.00117233
Iteration 7/25 | Loss: 0.00117233
Iteration 8/25 | Loss: 0.00117233
Iteration 9/25 | Loss: 0.00117233
Iteration 10/25 | Loss: 0.00117233
Iteration 11/25 | Loss: 0.00117233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011723339557647705, 0.0011723339557647705, 0.0011723339557647705, 0.0011723339557647705, 0.0011723339557647705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011723339557647705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86477435
Iteration 2/25 | Loss: 0.00127922
Iteration 3/25 | Loss: 0.00127922
Iteration 4/25 | Loss: 0.00127922
Iteration 5/25 | Loss: 0.00127922
Iteration 6/25 | Loss: 0.00127922
Iteration 7/25 | Loss: 0.00127922
Iteration 8/25 | Loss: 0.00127922
Iteration 9/25 | Loss: 0.00127922
Iteration 10/25 | Loss: 0.00127922
Iteration 11/25 | Loss: 0.00127922
Iteration 12/25 | Loss: 0.00127922
Iteration 13/25 | Loss: 0.00127922
Iteration 14/25 | Loss: 0.00127922
Iteration 15/25 | Loss: 0.00127922
Iteration 16/25 | Loss: 0.00127922
Iteration 17/25 | Loss: 0.00127922
Iteration 18/25 | Loss: 0.00127922
Iteration 19/25 | Loss: 0.00127922
Iteration 20/25 | Loss: 0.00127922
Iteration 21/25 | Loss: 0.00127922
Iteration 22/25 | Loss: 0.00127922
Iteration 23/25 | Loss: 0.00127922
Iteration 24/25 | Loss: 0.00127922
Iteration 25/25 | Loss: 0.00127922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127922
Iteration 2/1000 | Loss: 0.00002226
Iteration 3/1000 | Loss: 0.00001631
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001262
Iteration 7/1000 | Loss: 0.00001194
Iteration 8/1000 | Loss: 0.00001149
Iteration 9/1000 | Loss: 0.00001122
Iteration 10/1000 | Loss: 0.00001088
Iteration 11/1000 | Loss: 0.00001069
Iteration 12/1000 | Loss: 0.00001068
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001061
Iteration 15/1000 | Loss: 0.00001050
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001043
Iteration 18/1000 | Loss: 0.00001043
Iteration 19/1000 | Loss: 0.00001041
Iteration 20/1000 | Loss: 0.00001041
Iteration 21/1000 | Loss: 0.00001040
Iteration 22/1000 | Loss: 0.00001035
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001030
Iteration 26/1000 | Loss: 0.00001028
Iteration 27/1000 | Loss: 0.00001028
Iteration 28/1000 | Loss: 0.00001027
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001021
Iteration 31/1000 | Loss: 0.00001021
Iteration 32/1000 | Loss: 0.00001020
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001019
Iteration 35/1000 | Loss: 0.00001019
Iteration 36/1000 | Loss: 0.00001014
Iteration 37/1000 | Loss: 0.00001014
Iteration 38/1000 | Loss: 0.00001013
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001002
Iteration 49/1000 | Loss: 0.00001001
Iteration 50/1000 | Loss: 0.00001000
Iteration 51/1000 | Loss: 0.00001000
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000995
Iteration 55/1000 | Loss: 0.00000995
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000994
Iteration 62/1000 | Loss: 0.00000994
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000994
Iteration 66/1000 | Loss: 0.00000993
Iteration 67/1000 | Loss: 0.00000993
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000992
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000990
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000988
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000985
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000982
Iteration 101/1000 | Loss: 0.00000982
Iteration 102/1000 | Loss: 0.00000982
Iteration 103/1000 | Loss: 0.00000982
Iteration 104/1000 | Loss: 0.00000981
Iteration 105/1000 | Loss: 0.00000981
Iteration 106/1000 | Loss: 0.00000981
Iteration 107/1000 | Loss: 0.00000981
Iteration 108/1000 | Loss: 0.00000981
Iteration 109/1000 | Loss: 0.00000981
Iteration 110/1000 | Loss: 0.00000980
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000979
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000977
Iteration 124/1000 | Loss: 0.00000977
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000976
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000975
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000974
Iteration 141/1000 | Loss: 0.00000974
Iteration 142/1000 | Loss: 0.00000974
Iteration 143/1000 | Loss: 0.00000974
Iteration 144/1000 | Loss: 0.00000974
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000973
Iteration 147/1000 | Loss: 0.00000973
Iteration 148/1000 | Loss: 0.00000973
Iteration 149/1000 | Loss: 0.00000972
Iteration 150/1000 | Loss: 0.00000972
Iteration 151/1000 | Loss: 0.00000972
Iteration 152/1000 | Loss: 0.00000972
Iteration 153/1000 | Loss: 0.00000972
Iteration 154/1000 | Loss: 0.00000972
Iteration 155/1000 | Loss: 0.00000972
Iteration 156/1000 | Loss: 0.00000972
Iteration 157/1000 | Loss: 0.00000972
Iteration 158/1000 | Loss: 0.00000972
Iteration 159/1000 | Loss: 0.00000972
Iteration 160/1000 | Loss: 0.00000972
Iteration 161/1000 | Loss: 0.00000972
Iteration 162/1000 | Loss: 0.00000972
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [9.715297892398667e-06, 9.715297892398667e-06, 9.715297892398667e-06, 9.715297892398667e-06, 9.715297892398667e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.715297892398667e-06

Optimization complete. Final v2v error: 2.726243495941162 mm

Highest mean error: 2.8758044242858887 mm for frame 66

Lowest mean error: 2.6037981510162354 mm for frame 1

Saving results

Total time: 41.261752128601074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015921
Iteration 2/25 | Loss: 0.00206390
Iteration 3/25 | Loss: 0.00155855
Iteration 4/25 | Loss: 0.00145203
Iteration 5/25 | Loss: 0.00155295
Iteration 6/25 | Loss: 0.00154423
Iteration 7/25 | Loss: 0.00145114
Iteration 8/25 | Loss: 0.00135613
Iteration 9/25 | Loss: 0.00130803
Iteration 10/25 | Loss: 0.00130122
Iteration 11/25 | Loss: 0.00129576
Iteration 12/25 | Loss: 0.00129804
Iteration 13/25 | Loss: 0.00129688
Iteration 14/25 | Loss: 0.00129291
Iteration 15/25 | Loss: 0.00128978
Iteration 16/25 | Loss: 0.00128355
Iteration 17/25 | Loss: 0.00127220
Iteration 18/25 | Loss: 0.00127427
Iteration 19/25 | Loss: 0.00126670
Iteration 20/25 | Loss: 0.00126352
Iteration 21/25 | Loss: 0.00126400
Iteration 22/25 | Loss: 0.00126699
Iteration 23/25 | Loss: 0.00126175
Iteration 24/25 | Loss: 0.00126346
Iteration 25/25 | Loss: 0.00126381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33582449
Iteration 2/25 | Loss: 0.00189185
Iteration 3/25 | Loss: 0.00185907
Iteration 4/25 | Loss: 0.00185907
Iteration 5/25 | Loss: 0.00185907
Iteration 6/25 | Loss: 0.00185907
Iteration 7/25 | Loss: 0.00185907
Iteration 8/25 | Loss: 0.00185907
Iteration 9/25 | Loss: 0.00185907
Iteration 10/25 | Loss: 0.00185907
Iteration 11/25 | Loss: 0.00185907
Iteration 12/25 | Loss: 0.00185907
Iteration 13/25 | Loss: 0.00185907
Iteration 14/25 | Loss: 0.00185907
Iteration 15/25 | Loss: 0.00185907
Iteration 16/25 | Loss: 0.00185907
Iteration 17/25 | Loss: 0.00185907
Iteration 18/25 | Loss: 0.00185907
Iteration 19/25 | Loss: 0.00185907
Iteration 20/25 | Loss: 0.00185907
Iteration 21/25 | Loss: 0.00185907
Iteration 22/25 | Loss: 0.00185907
Iteration 23/25 | Loss: 0.00185907
Iteration 24/25 | Loss: 0.00185907
Iteration 25/25 | Loss: 0.00185907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185907
Iteration 2/1000 | Loss: 0.00049804
Iteration 3/1000 | Loss: 0.00034920
Iteration 4/1000 | Loss: 0.00014847
Iteration 5/1000 | Loss: 0.00121979
Iteration 6/1000 | Loss: 0.00121400
Iteration 7/1000 | Loss: 0.00068984
Iteration 8/1000 | Loss: 0.00063496
Iteration 9/1000 | Loss: 0.00123220
Iteration 10/1000 | Loss: 0.00263911
Iteration 11/1000 | Loss: 0.00034676
Iteration 12/1000 | Loss: 0.00020858
Iteration 13/1000 | Loss: 0.00125350
Iteration 14/1000 | Loss: 0.00188082
Iteration 15/1000 | Loss: 0.00036401
Iteration 16/1000 | Loss: 0.00044870
Iteration 17/1000 | Loss: 0.00044558
Iteration 18/1000 | Loss: 0.00101497
Iteration 19/1000 | Loss: 0.00021077
Iteration 20/1000 | Loss: 0.00018107
Iteration 21/1000 | Loss: 0.00005020
Iteration 22/1000 | Loss: 0.00020549
Iteration 23/1000 | Loss: 0.00020457
Iteration 24/1000 | Loss: 0.00009175
Iteration 25/1000 | Loss: 0.00062478
Iteration 26/1000 | Loss: 0.00012924
Iteration 27/1000 | Loss: 0.00064552
Iteration 28/1000 | Loss: 0.00013272
Iteration 29/1000 | Loss: 0.00014035
Iteration 30/1000 | Loss: 0.00005077
Iteration 31/1000 | Loss: 0.00017762
Iteration 32/1000 | Loss: 0.00018831
Iteration 33/1000 | Loss: 0.00005525
Iteration 34/1000 | Loss: 0.00003465
Iteration 35/1000 | Loss: 0.00002798
Iteration 36/1000 | Loss: 0.00003465
Iteration 37/1000 | Loss: 0.00042496
Iteration 38/1000 | Loss: 0.00023638
Iteration 39/1000 | Loss: 0.00029507
Iteration 40/1000 | Loss: 0.00034955
Iteration 41/1000 | Loss: 0.00003900
Iteration 42/1000 | Loss: 0.00003576
Iteration 43/1000 | Loss: 0.00002991
Iteration 44/1000 | Loss: 0.00002905
Iteration 45/1000 | Loss: 0.00009598
Iteration 46/1000 | Loss: 0.00002977
Iteration 47/1000 | Loss: 0.00002864
Iteration 48/1000 | Loss: 0.00002923
Iteration 49/1000 | Loss: 0.00003816
Iteration 50/1000 | Loss: 0.00003522
Iteration 51/1000 | Loss: 0.00002737
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00002659
Iteration 54/1000 | Loss: 0.00002701
Iteration 55/1000 | Loss: 0.00002923
Iteration 56/1000 | Loss: 0.00002891
Iteration 57/1000 | Loss: 0.00002866
Iteration 58/1000 | Loss: 0.00003315
Iteration 59/1000 | Loss: 0.00002999
Iteration 60/1000 | Loss: 0.00003241
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00001767
Iteration 63/1000 | Loss: 0.00001651
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001569
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00016817
Iteration 69/1000 | Loss: 0.00014687
Iteration 70/1000 | Loss: 0.00015509
Iteration 71/1000 | Loss: 0.00012644
Iteration 72/1000 | Loss: 0.00015033
Iteration 73/1000 | Loss: 0.00011056
Iteration 74/1000 | Loss: 0.00014920
Iteration 75/1000 | Loss: 0.00001979
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00010351
Iteration 79/1000 | Loss: 0.00006436
Iteration 80/1000 | Loss: 0.00009271
Iteration 81/1000 | Loss: 0.00008936
Iteration 82/1000 | Loss: 0.00012471
Iteration 83/1000 | Loss: 0.00008559
Iteration 84/1000 | Loss: 0.00009062
Iteration 85/1000 | Loss: 0.00007208
Iteration 86/1000 | Loss: 0.00008552
Iteration 87/1000 | Loss: 0.00007237
Iteration 88/1000 | Loss: 0.00007769
Iteration 89/1000 | Loss: 0.00007338
Iteration 90/1000 | Loss: 0.00007615
Iteration 91/1000 | Loss: 0.00008483
Iteration 92/1000 | Loss: 0.00011976
Iteration 93/1000 | Loss: 0.00012498
Iteration 94/1000 | Loss: 0.00010556
Iteration 95/1000 | Loss: 0.00011807
Iteration 96/1000 | Loss: 0.00009651
Iteration 97/1000 | Loss: 0.00007286
Iteration 98/1000 | Loss: 0.00007876
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00008550
Iteration 101/1000 | Loss: 0.00033422
Iteration 102/1000 | Loss: 0.00003796
Iteration 103/1000 | Loss: 0.00002340
Iteration 104/1000 | Loss: 0.00015247
Iteration 105/1000 | Loss: 0.00003381
Iteration 106/1000 | Loss: 0.00002538
Iteration 107/1000 | Loss: 0.00021540
Iteration 108/1000 | Loss: 0.00010271
Iteration 109/1000 | Loss: 0.00020786
Iteration 110/1000 | Loss: 0.00015716
Iteration 111/1000 | Loss: 0.00014034
Iteration 112/1000 | Loss: 0.00001971
Iteration 113/1000 | Loss: 0.00012928
Iteration 114/1000 | Loss: 0.00004975
Iteration 115/1000 | Loss: 0.00001873
Iteration 116/1000 | Loss: 0.00004200
Iteration 117/1000 | Loss: 0.00003331
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001409
Iteration 123/1000 | Loss: 0.00002129
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001335
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001312
Iteration 131/1000 | Loss: 0.00001312
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001308
Iteration 134/1000 | Loss: 0.00001308
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001306
Iteration 138/1000 | Loss: 0.00001306
Iteration 139/1000 | Loss: 0.00001305
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001260
Iteration 142/1000 | Loss: 0.00001238
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001229
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001229
Iteration 147/1000 | Loss: 0.00001229
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001224
Iteration 169/1000 | Loss: 0.00001224
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001223
Iteration 174/1000 | Loss: 0.00001223
Iteration 175/1000 | Loss: 0.00001223
Iteration 176/1000 | Loss: 0.00001223
Iteration 177/1000 | Loss: 0.00001223
Iteration 178/1000 | Loss: 0.00001223
Iteration 179/1000 | Loss: 0.00001222
Iteration 180/1000 | Loss: 0.00001222
Iteration 181/1000 | Loss: 0.00001222
Iteration 182/1000 | Loss: 0.00001222
Iteration 183/1000 | Loss: 0.00001222
Iteration 184/1000 | Loss: 0.00001222
Iteration 185/1000 | Loss: 0.00001221
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001220
Iteration 190/1000 | Loss: 0.00001220
Iteration 191/1000 | Loss: 0.00001220
Iteration 192/1000 | Loss: 0.00001220
Iteration 193/1000 | Loss: 0.00001220
Iteration 194/1000 | Loss: 0.00001219
Iteration 195/1000 | Loss: 0.00001219
Iteration 196/1000 | Loss: 0.00001219
Iteration 197/1000 | Loss: 0.00001219
Iteration 198/1000 | Loss: 0.00001219
Iteration 199/1000 | Loss: 0.00001219
Iteration 200/1000 | Loss: 0.00001219
Iteration 201/1000 | Loss: 0.00001219
Iteration 202/1000 | Loss: 0.00001219
Iteration 203/1000 | Loss: 0.00001219
Iteration 204/1000 | Loss: 0.00001219
Iteration 205/1000 | Loss: 0.00001219
Iteration 206/1000 | Loss: 0.00001219
Iteration 207/1000 | Loss: 0.00001219
Iteration 208/1000 | Loss: 0.00001218
Iteration 209/1000 | Loss: 0.00001218
Iteration 210/1000 | Loss: 0.00001218
Iteration 211/1000 | Loss: 0.00001218
Iteration 212/1000 | Loss: 0.00001218
Iteration 213/1000 | Loss: 0.00001217
Iteration 214/1000 | Loss: 0.00001217
Iteration 215/1000 | Loss: 0.00001217
Iteration 216/1000 | Loss: 0.00001217
Iteration 217/1000 | Loss: 0.00001217
Iteration 218/1000 | Loss: 0.00001217
Iteration 219/1000 | Loss: 0.00001217
Iteration 220/1000 | Loss: 0.00001217
Iteration 221/1000 | Loss: 0.00001217
Iteration 222/1000 | Loss: 0.00001217
Iteration 223/1000 | Loss: 0.00001217
Iteration 224/1000 | Loss: 0.00001217
Iteration 225/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.216729015141027e-05, 1.216729015141027e-05, 1.216729015141027e-05, 1.216729015141027e-05, 1.216729015141027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.216729015141027e-05

Optimization complete. Final v2v error: 2.955070734024048 mm

Highest mean error: 4.986162185668945 mm for frame 226

Lowest mean error: 2.6252710819244385 mm for frame 230

Saving results

Total time: 264.6219894886017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788707
Iteration 2/25 | Loss: 0.00126402
Iteration 3/25 | Loss: 0.00119760
Iteration 4/25 | Loss: 0.00118323
Iteration 5/25 | Loss: 0.00117914
Iteration 6/25 | Loss: 0.00117914
Iteration 7/25 | Loss: 0.00117914
Iteration 8/25 | Loss: 0.00117914
Iteration 9/25 | Loss: 0.00117914
Iteration 10/25 | Loss: 0.00117914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011791414581239223, 0.0011791414581239223, 0.0011791414581239223, 0.0011791414581239223, 0.0011791414581239223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011791414581239223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34652495
Iteration 2/25 | Loss: 0.00121000
Iteration 3/25 | Loss: 0.00121000
Iteration 4/25 | Loss: 0.00120999
Iteration 5/25 | Loss: 0.00120999
Iteration 6/25 | Loss: 0.00120999
Iteration 7/25 | Loss: 0.00120999
Iteration 8/25 | Loss: 0.00120999
Iteration 9/25 | Loss: 0.00120999
Iteration 10/25 | Loss: 0.00120999
Iteration 11/25 | Loss: 0.00120999
Iteration 12/25 | Loss: 0.00120999
Iteration 13/25 | Loss: 0.00120999
Iteration 14/25 | Loss: 0.00120999
Iteration 15/25 | Loss: 0.00120999
Iteration 16/25 | Loss: 0.00120999
Iteration 17/25 | Loss: 0.00120999
Iteration 18/25 | Loss: 0.00120999
Iteration 19/25 | Loss: 0.00120999
Iteration 20/25 | Loss: 0.00120999
Iteration 21/25 | Loss: 0.00120999
Iteration 22/25 | Loss: 0.00120999
Iteration 23/25 | Loss: 0.00120999
Iteration 24/25 | Loss: 0.00120999
Iteration 25/25 | Loss: 0.00120999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120999
Iteration 2/1000 | Loss: 0.00002258
Iteration 3/1000 | Loss: 0.00001612
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001401
Iteration 6/1000 | Loss: 0.00001339
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001223
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001182
Iteration 13/1000 | Loss: 0.00001181
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001151
Iteration 18/1000 | Loss: 0.00001150
Iteration 19/1000 | Loss: 0.00001147
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001136
Iteration 36/1000 | Loss: 0.00001130
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001126
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001114
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001113
Iteration 58/1000 | Loss: 0.00001113
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001112
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001111
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001109
Iteration 72/1000 | Loss: 0.00001109
Iteration 73/1000 | Loss: 0.00001109
Iteration 74/1000 | Loss: 0.00001109
Iteration 75/1000 | Loss: 0.00001108
Iteration 76/1000 | Loss: 0.00001108
Iteration 77/1000 | Loss: 0.00001108
Iteration 78/1000 | Loss: 0.00001108
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001108
Iteration 82/1000 | Loss: 0.00001107
Iteration 83/1000 | Loss: 0.00001107
Iteration 84/1000 | Loss: 0.00001107
Iteration 85/1000 | Loss: 0.00001107
Iteration 86/1000 | Loss: 0.00001106
Iteration 87/1000 | Loss: 0.00001106
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001101
Iteration 93/1000 | Loss: 0.00001101
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001100
Iteration 97/1000 | Loss: 0.00001100
Iteration 98/1000 | Loss: 0.00001100
Iteration 99/1000 | Loss: 0.00001099
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001099
Iteration 102/1000 | Loss: 0.00001098
Iteration 103/1000 | Loss: 0.00001098
Iteration 104/1000 | Loss: 0.00001098
Iteration 105/1000 | Loss: 0.00001098
Iteration 106/1000 | Loss: 0.00001098
Iteration 107/1000 | Loss: 0.00001098
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001097
Iteration 118/1000 | Loss: 0.00001097
Iteration 119/1000 | Loss: 0.00001097
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001097
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001096
Iteration 126/1000 | Loss: 0.00001096
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001096
Iteration 134/1000 | Loss: 0.00001096
Iteration 135/1000 | Loss: 0.00001096
Iteration 136/1000 | Loss: 0.00001096
Iteration 137/1000 | Loss: 0.00001096
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001096
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001096
Iteration 142/1000 | Loss: 0.00001096
Iteration 143/1000 | Loss: 0.00001096
Iteration 144/1000 | Loss: 0.00001095
Iteration 145/1000 | Loss: 0.00001095
Iteration 146/1000 | Loss: 0.00001095
Iteration 147/1000 | Loss: 0.00001095
Iteration 148/1000 | Loss: 0.00001095
Iteration 149/1000 | Loss: 0.00001095
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001095
Iteration 152/1000 | Loss: 0.00001095
Iteration 153/1000 | Loss: 0.00001094
Iteration 154/1000 | Loss: 0.00001094
Iteration 155/1000 | Loss: 0.00001094
Iteration 156/1000 | Loss: 0.00001094
Iteration 157/1000 | Loss: 0.00001094
Iteration 158/1000 | Loss: 0.00001094
Iteration 159/1000 | Loss: 0.00001094
Iteration 160/1000 | Loss: 0.00001094
Iteration 161/1000 | Loss: 0.00001094
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001093
Iteration 164/1000 | Loss: 0.00001093
Iteration 165/1000 | Loss: 0.00001093
Iteration 166/1000 | Loss: 0.00001093
Iteration 167/1000 | Loss: 0.00001093
Iteration 168/1000 | Loss: 0.00001093
Iteration 169/1000 | Loss: 0.00001093
Iteration 170/1000 | Loss: 0.00001093
Iteration 171/1000 | Loss: 0.00001092
Iteration 172/1000 | Loss: 0.00001092
Iteration 173/1000 | Loss: 0.00001092
Iteration 174/1000 | Loss: 0.00001092
Iteration 175/1000 | Loss: 0.00001092
Iteration 176/1000 | Loss: 0.00001092
Iteration 177/1000 | Loss: 0.00001092
Iteration 178/1000 | Loss: 0.00001091
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001091
Iteration 183/1000 | Loss: 0.00001091
Iteration 184/1000 | Loss: 0.00001091
Iteration 185/1000 | Loss: 0.00001091
Iteration 186/1000 | Loss: 0.00001091
Iteration 187/1000 | Loss: 0.00001091
Iteration 188/1000 | Loss: 0.00001091
Iteration 189/1000 | Loss: 0.00001091
Iteration 190/1000 | Loss: 0.00001091
Iteration 191/1000 | Loss: 0.00001090
Iteration 192/1000 | Loss: 0.00001090
Iteration 193/1000 | Loss: 0.00001090
Iteration 194/1000 | Loss: 0.00001090
Iteration 195/1000 | Loss: 0.00001090
Iteration 196/1000 | Loss: 0.00001090
Iteration 197/1000 | Loss: 0.00001090
Iteration 198/1000 | Loss: 0.00001090
Iteration 199/1000 | Loss: 0.00001090
Iteration 200/1000 | Loss: 0.00001090
Iteration 201/1000 | Loss: 0.00001090
Iteration 202/1000 | Loss: 0.00001090
Iteration 203/1000 | Loss: 0.00001090
Iteration 204/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0903589100053068e-05, 1.0903589100053068e-05, 1.0903589100053068e-05, 1.0903589100053068e-05, 1.0903589100053068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0903589100053068e-05

Optimization complete. Final v2v error: 2.8448355197906494 mm

Highest mean error: 3.0842719078063965 mm for frame 62

Lowest mean error: 2.7314233779907227 mm for frame 143

Saving results

Total time: 41.518091917037964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492086
Iteration 2/25 | Loss: 0.00140399
Iteration 3/25 | Loss: 0.00129025
Iteration 4/25 | Loss: 0.00126400
Iteration 5/25 | Loss: 0.00125377
Iteration 6/25 | Loss: 0.00125151
Iteration 7/25 | Loss: 0.00125151
Iteration 8/25 | Loss: 0.00125151
Iteration 9/25 | Loss: 0.00125151
Iteration 10/25 | Loss: 0.00125151
Iteration 11/25 | Loss: 0.00125151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001251512672752142, 0.001251512672752142, 0.001251512672752142, 0.001251512672752142, 0.001251512672752142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001251512672752142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47479880
Iteration 2/25 | Loss: 0.00178944
Iteration 3/25 | Loss: 0.00178944
Iteration 4/25 | Loss: 0.00178944
Iteration 5/25 | Loss: 0.00178943
Iteration 6/25 | Loss: 0.00178943
Iteration 7/25 | Loss: 0.00178943
Iteration 8/25 | Loss: 0.00178943
Iteration 9/25 | Loss: 0.00178943
Iteration 10/25 | Loss: 0.00178943
Iteration 11/25 | Loss: 0.00178943
Iteration 12/25 | Loss: 0.00178943
Iteration 13/25 | Loss: 0.00178943
Iteration 14/25 | Loss: 0.00178943
Iteration 15/25 | Loss: 0.00178943
Iteration 16/25 | Loss: 0.00178943
Iteration 17/25 | Loss: 0.00178943
Iteration 18/25 | Loss: 0.00178943
Iteration 19/25 | Loss: 0.00178943
Iteration 20/25 | Loss: 0.00178943
Iteration 21/25 | Loss: 0.00178943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017894324846565723, 0.0017894324846565723, 0.0017894324846565723, 0.0017894324846565723, 0.0017894324846565723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017894324846565723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178943
Iteration 2/1000 | Loss: 0.00006070
Iteration 3/1000 | Loss: 0.00004308
Iteration 4/1000 | Loss: 0.00003505
Iteration 5/1000 | Loss: 0.00003217
Iteration 6/1000 | Loss: 0.00003027
Iteration 7/1000 | Loss: 0.00002904
Iteration 8/1000 | Loss: 0.00002827
Iteration 9/1000 | Loss: 0.00002770
Iteration 10/1000 | Loss: 0.00002726
Iteration 11/1000 | Loss: 0.00002686
Iteration 12/1000 | Loss: 0.00002655
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002594
Iteration 16/1000 | Loss: 0.00002594
Iteration 17/1000 | Loss: 0.00002593
Iteration 18/1000 | Loss: 0.00002592
Iteration 19/1000 | Loss: 0.00002591
Iteration 20/1000 | Loss: 0.00002589
Iteration 21/1000 | Loss: 0.00002587
Iteration 22/1000 | Loss: 0.00002587
Iteration 23/1000 | Loss: 0.00002586
Iteration 24/1000 | Loss: 0.00002585
Iteration 25/1000 | Loss: 0.00002584
Iteration 26/1000 | Loss: 0.00002583
Iteration 27/1000 | Loss: 0.00002581
Iteration 28/1000 | Loss: 0.00002576
Iteration 29/1000 | Loss: 0.00002573
Iteration 30/1000 | Loss: 0.00002573
Iteration 31/1000 | Loss: 0.00002571
Iteration 32/1000 | Loss: 0.00002570
Iteration 33/1000 | Loss: 0.00002561
Iteration 34/1000 | Loss: 0.00002559
Iteration 35/1000 | Loss: 0.00002559
Iteration 36/1000 | Loss: 0.00002559
Iteration 37/1000 | Loss: 0.00002558
Iteration 38/1000 | Loss: 0.00002557
Iteration 39/1000 | Loss: 0.00002557
Iteration 40/1000 | Loss: 0.00002556
Iteration 41/1000 | Loss: 0.00002556
Iteration 42/1000 | Loss: 0.00002556
Iteration 43/1000 | Loss: 0.00002555
Iteration 44/1000 | Loss: 0.00002555
Iteration 45/1000 | Loss: 0.00002554
Iteration 46/1000 | Loss: 0.00002554
Iteration 47/1000 | Loss: 0.00002554
Iteration 48/1000 | Loss: 0.00002553
Iteration 49/1000 | Loss: 0.00002553
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002553
Iteration 52/1000 | Loss: 0.00002553
Iteration 53/1000 | Loss: 0.00002553
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002553
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002552
Iteration 58/1000 | Loss: 0.00002551
Iteration 59/1000 | Loss: 0.00002551
Iteration 60/1000 | Loss: 0.00002551
Iteration 61/1000 | Loss: 0.00002550
Iteration 62/1000 | Loss: 0.00002550
Iteration 63/1000 | Loss: 0.00002550
Iteration 64/1000 | Loss: 0.00002550
Iteration 65/1000 | Loss: 0.00002549
Iteration 66/1000 | Loss: 0.00002549
Iteration 67/1000 | Loss: 0.00002549
Iteration 68/1000 | Loss: 0.00002548
Iteration 69/1000 | Loss: 0.00002548
Iteration 70/1000 | Loss: 0.00002548
Iteration 71/1000 | Loss: 0.00002547
Iteration 72/1000 | Loss: 0.00002547
Iteration 73/1000 | Loss: 0.00002547
Iteration 74/1000 | Loss: 0.00002547
Iteration 75/1000 | Loss: 0.00002547
Iteration 76/1000 | Loss: 0.00002547
Iteration 77/1000 | Loss: 0.00002546
Iteration 78/1000 | Loss: 0.00002546
Iteration 79/1000 | Loss: 0.00002546
Iteration 80/1000 | Loss: 0.00002546
Iteration 81/1000 | Loss: 0.00002546
Iteration 82/1000 | Loss: 0.00002546
Iteration 83/1000 | Loss: 0.00002545
Iteration 84/1000 | Loss: 0.00002545
Iteration 85/1000 | Loss: 0.00002545
Iteration 86/1000 | Loss: 0.00002545
Iteration 87/1000 | Loss: 0.00002544
Iteration 88/1000 | Loss: 0.00002544
Iteration 89/1000 | Loss: 0.00002544
Iteration 90/1000 | Loss: 0.00002544
Iteration 91/1000 | Loss: 0.00002544
Iteration 92/1000 | Loss: 0.00002544
Iteration 93/1000 | Loss: 0.00002544
Iteration 94/1000 | Loss: 0.00002543
Iteration 95/1000 | Loss: 0.00002543
Iteration 96/1000 | Loss: 0.00002543
Iteration 97/1000 | Loss: 0.00002543
Iteration 98/1000 | Loss: 0.00002543
Iteration 99/1000 | Loss: 0.00002543
Iteration 100/1000 | Loss: 0.00002543
Iteration 101/1000 | Loss: 0.00002543
Iteration 102/1000 | Loss: 0.00002543
Iteration 103/1000 | Loss: 0.00002543
Iteration 104/1000 | Loss: 0.00002543
Iteration 105/1000 | Loss: 0.00002542
Iteration 106/1000 | Loss: 0.00002542
Iteration 107/1000 | Loss: 0.00002542
Iteration 108/1000 | Loss: 0.00002542
Iteration 109/1000 | Loss: 0.00002542
Iteration 110/1000 | Loss: 0.00002542
Iteration 111/1000 | Loss: 0.00002542
Iteration 112/1000 | Loss: 0.00002542
Iteration 113/1000 | Loss: 0.00002542
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002542
Iteration 116/1000 | Loss: 0.00002541
Iteration 117/1000 | Loss: 0.00002541
Iteration 118/1000 | Loss: 0.00002541
Iteration 119/1000 | Loss: 0.00002541
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002541
Iteration 123/1000 | Loss: 0.00002540
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002539
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002539
Iteration 134/1000 | Loss: 0.00002539
Iteration 135/1000 | Loss: 0.00002538
Iteration 136/1000 | Loss: 0.00002538
Iteration 137/1000 | Loss: 0.00002538
Iteration 138/1000 | Loss: 0.00002538
Iteration 139/1000 | Loss: 0.00002538
Iteration 140/1000 | Loss: 0.00002537
Iteration 141/1000 | Loss: 0.00002537
Iteration 142/1000 | Loss: 0.00002537
Iteration 143/1000 | Loss: 0.00002537
Iteration 144/1000 | Loss: 0.00002537
Iteration 145/1000 | Loss: 0.00002537
Iteration 146/1000 | Loss: 0.00002537
Iteration 147/1000 | Loss: 0.00002536
Iteration 148/1000 | Loss: 0.00002536
Iteration 149/1000 | Loss: 0.00002536
Iteration 150/1000 | Loss: 0.00002536
Iteration 151/1000 | Loss: 0.00002536
Iteration 152/1000 | Loss: 0.00002536
Iteration 153/1000 | Loss: 0.00002536
Iteration 154/1000 | Loss: 0.00002536
Iteration 155/1000 | Loss: 0.00002536
Iteration 156/1000 | Loss: 0.00002536
Iteration 157/1000 | Loss: 0.00002535
Iteration 158/1000 | Loss: 0.00002535
Iteration 159/1000 | Loss: 0.00002535
Iteration 160/1000 | Loss: 0.00002535
Iteration 161/1000 | Loss: 0.00002534
Iteration 162/1000 | Loss: 0.00002534
Iteration 163/1000 | Loss: 0.00002534
Iteration 164/1000 | Loss: 0.00002534
Iteration 165/1000 | Loss: 0.00002534
Iteration 166/1000 | Loss: 0.00002533
Iteration 167/1000 | Loss: 0.00002533
Iteration 168/1000 | Loss: 0.00002533
Iteration 169/1000 | Loss: 0.00002533
Iteration 170/1000 | Loss: 0.00002532
Iteration 171/1000 | Loss: 0.00002532
Iteration 172/1000 | Loss: 0.00002532
Iteration 173/1000 | Loss: 0.00002532
Iteration 174/1000 | Loss: 0.00002532
Iteration 175/1000 | Loss: 0.00002531
Iteration 176/1000 | Loss: 0.00002531
Iteration 177/1000 | Loss: 0.00002531
Iteration 178/1000 | Loss: 0.00002531
Iteration 179/1000 | Loss: 0.00002531
Iteration 180/1000 | Loss: 0.00002531
Iteration 181/1000 | Loss: 0.00002530
Iteration 182/1000 | Loss: 0.00002530
Iteration 183/1000 | Loss: 0.00002530
Iteration 184/1000 | Loss: 0.00002530
Iteration 185/1000 | Loss: 0.00002530
Iteration 186/1000 | Loss: 0.00002530
Iteration 187/1000 | Loss: 0.00002530
Iteration 188/1000 | Loss: 0.00002530
Iteration 189/1000 | Loss: 0.00002530
Iteration 190/1000 | Loss: 0.00002530
Iteration 191/1000 | Loss: 0.00002529
Iteration 192/1000 | Loss: 0.00002529
Iteration 193/1000 | Loss: 0.00002529
Iteration 194/1000 | Loss: 0.00002529
Iteration 195/1000 | Loss: 0.00002529
Iteration 196/1000 | Loss: 0.00002529
Iteration 197/1000 | Loss: 0.00002529
Iteration 198/1000 | Loss: 0.00002529
Iteration 199/1000 | Loss: 0.00002529
Iteration 200/1000 | Loss: 0.00002529
Iteration 201/1000 | Loss: 0.00002529
Iteration 202/1000 | Loss: 0.00002529
Iteration 203/1000 | Loss: 0.00002529
Iteration 204/1000 | Loss: 0.00002529
Iteration 205/1000 | Loss: 0.00002529
Iteration 206/1000 | Loss: 0.00002529
Iteration 207/1000 | Loss: 0.00002529
Iteration 208/1000 | Loss: 0.00002529
Iteration 209/1000 | Loss: 0.00002529
Iteration 210/1000 | Loss: 0.00002529
Iteration 211/1000 | Loss: 0.00002529
Iteration 212/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.5289560653618537e-05, 2.5289560653618537e-05, 2.5289560653618537e-05, 2.5289560653618537e-05, 2.5289560653618537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5289560653618537e-05

Optimization complete. Final v2v error: 4.173316478729248 mm

Highest mean error: 5.268086910247803 mm for frame 40

Lowest mean error: 3.3309757709503174 mm for frame 174

Saving results

Total time: 53.73729395866394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359801
Iteration 2/25 | Loss: 0.00126463
Iteration 3/25 | Loss: 0.00117702
Iteration 4/25 | Loss: 0.00116451
Iteration 5/25 | Loss: 0.00116188
Iteration 6/25 | Loss: 0.00116153
Iteration 7/25 | Loss: 0.00116153
Iteration 8/25 | Loss: 0.00116153
Iteration 9/25 | Loss: 0.00116153
Iteration 10/25 | Loss: 0.00116153
Iteration 11/25 | Loss: 0.00116153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011615348048508167, 0.0011615348048508167, 0.0011615348048508167, 0.0011615348048508167, 0.0011615348048508167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011615348048508167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35542142
Iteration 2/25 | Loss: 0.00148219
Iteration 3/25 | Loss: 0.00148219
Iteration 4/25 | Loss: 0.00148219
Iteration 5/25 | Loss: 0.00148219
Iteration 6/25 | Loss: 0.00148219
Iteration 7/25 | Loss: 0.00148219
Iteration 8/25 | Loss: 0.00148219
Iteration 9/25 | Loss: 0.00148219
Iteration 10/25 | Loss: 0.00148219
Iteration 11/25 | Loss: 0.00148219
Iteration 12/25 | Loss: 0.00148219
Iteration 13/25 | Loss: 0.00148219
Iteration 14/25 | Loss: 0.00148219
Iteration 15/25 | Loss: 0.00148219
Iteration 16/25 | Loss: 0.00148219
Iteration 17/25 | Loss: 0.00148219
Iteration 18/25 | Loss: 0.00148219
Iteration 19/25 | Loss: 0.00148219
Iteration 20/25 | Loss: 0.00148219
Iteration 21/25 | Loss: 0.00148219
Iteration 22/25 | Loss: 0.00148219
Iteration 23/25 | Loss: 0.00148219
Iteration 24/25 | Loss: 0.00148219
Iteration 25/25 | Loss: 0.00148219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148219
Iteration 2/1000 | Loss: 0.00002534
Iteration 3/1000 | Loss: 0.00001396
Iteration 4/1000 | Loss: 0.00001260
Iteration 5/1000 | Loss: 0.00001183
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001085
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001073
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001038
Iteration 14/1000 | Loss: 0.00001033
Iteration 15/1000 | Loss: 0.00001023
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001015
Iteration 18/1000 | Loss: 0.00001015
Iteration 19/1000 | Loss: 0.00001014
Iteration 20/1000 | Loss: 0.00001012
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001005
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000996
Iteration 26/1000 | Loss: 0.00000994
Iteration 27/1000 | Loss: 0.00000993
Iteration 28/1000 | Loss: 0.00000993
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000990
Iteration 32/1000 | Loss: 0.00000990
Iteration 33/1000 | Loss: 0.00000987
Iteration 34/1000 | Loss: 0.00000987
Iteration 35/1000 | Loss: 0.00000986
Iteration 36/1000 | Loss: 0.00000986
Iteration 37/1000 | Loss: 0.00000986
Iteration 38/1000 | Loss: 0.00000985
Iteration 39/1000 | Loss: 0.00000985
Iteration 40/1000 | Loss: 0.00000985
Iteration 41/1000 | Loss: 0.00000984
Iteration 42/1000 | Loss: 0.00000984
Iteration 43/1000 | Loss: 0.00000984
Iteration 44/1000 | Loss: 0.00000984
Iteration 45/1000 | Loss: 0.00000984
Iteration 46/1000 | Loss: 0.00000983
Iteration 47/1000 | Loss: 0.00000983
Iteration 48/1000 | Loss: 0.00000983
Iteration 49/1000 | Loss: 0.00000982
Iteration 50/1000 | Loss: 0.00000982
Iteration 51/1000 | Loss: 0.00000982
Iteration 52/1000 | Loss: 0.00000981
Iteration 53/1000 | Loss: 0.00000981
Iteration 54/1000 | Loss: 0.00000981
Iteration 55/1000 | Loss: 0.00000980
Iteration 56/1000 | Loss: 0.00000980
Iteration 57/1000 | Loss: 0.00000980
Iteration 58/1000 | Loss: 0.00000980
Iteration 59/1000 | Loss: 0.00000979
Iteration 60/1000 | Loss: 0.00000979
Iteration 61/1000 | Loss: 0.00000978
Iteration 62/1000 | Loss: 0.00000978
Iteration 63/1000 | Loss: 0.00000978
Iteration 64/1000 | Loss: 0.00000977
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000977
Iteration 68/1000 | Loss: 0.00000977
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000975
Iteration 72/1000 | Loss: 0.00000975
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000974
Iteration 75/1000 | Loss: 0.00000974
Iteration 76/1000 | Loss: 0.00000974
Iteration 77/1000 | Loss: 0.00000974
Iteration 78/1000 | Loss: 0.00000973
Iteration 79/1000 | Loss: 0.00000973
Iteration 80/1000 | Loss: 0.00000973
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000972
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000970
Iteration 86/1000 | Loss: 0.00000970
Iteration 87/1000 | Loss: 0.00000970
Iteration 88/1000 | Loss: 0.00000970
Iteration 89/1000 | Loss: 0.00000970
Iteration 90/1000 | Loss: 0.00000970
Iteration 91/1000 | Loss: 0.00000970
Iteration 92/1000 | Loss: 0.00000970
Iteration 93/1000 | Loss: 0.00000970
Iteration 94/1000 | Loss: 0.00000969
Iteration 95/1000 | Loss: 0.00000969
Iteration 96/1000 | Loss: 0.00000969
Iteration 97/1000 | Loss: 0.00000968
Iteration 98/1000 | Loss: 0.00000968
Iteration 99/1000 | Loss: 0.00000968
Iteration 100/1000 | Loss: 0.00000968
Iteration 101/1000 | Loss: 0.00000967
Iteration 102/1000 | Loss: 0.00000967
Iteration 103/1000 | Loss: 0.00000967
Iteration 104/1000 | Loss: 0.00000967
Iteration 105/1000 | Loss: 0.00000967
Iteration 106/1000 | Loss: 0.00000966
Iteration 107/1000 | Loss: 0.00000966
Iteration 108/1000 | Loss: 0.00000966
Iteration 109/1000 | Loss: 0.00000966
Iteration 110/1000 | Loss: 0.00000966
Iteration 111/1000 | Loss: 0.00000966
Iteration 112/1000 | Loss: 0.00000966
Iteration 113/1000 | Loss: 0.00000966
Iteration 114/1000 | Loss: 0.00000966
Iteration 115/1000 | Loss: 0.00000966
Iteration 116/1000 | Loss: 0.00000966
Iteration 117/1000 | Loss: 0.00000966
Iteration 118/1000 | Loss: 0.00000966
Iteration 119/1000 | Loss: 0.00000966
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000965
Iteration 122/1000 | Loss: 0.00000965
Iteration 123/1000 | Loss: 0.00000965
Iteration 124/1000 | Loss: 0.00000965
Iteration 125/1000 | Loss: 0.00000965
Iteration 126/1000 | Loss: 0.00000965
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000964
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000964
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000964
Iteration 136/1000 | Loss: 0.00000964
Iteration 137/1000 | Loss: 0.00000964
Iteration 138/1000 | Loss: 0.00000964
Iteration 139/1000 | Loss: 0.00000964
Iteration 140/1000 | Loss: 0.00000964
Iteration 141/1000 | Loss: 0.00000964
Iteration 142/1000 | Loss: 0.00000963
Iteration 143/1000 | Loss: 0.00000963
Iteration 144/1000 | Loss: 0.00000963
Iteration 145/1000 | Loss: 0.00000963
Iteration 146/1000 | Loss: 0.00000963
Iteration 147/1000 | Loss: 0.00000963
Iteration 148/1000 | Loss: 0.00000963
Iteration 149/1000 | Loss: 0.00000963
Iteration 150/1000 | Loss: 0.00000963
Iteration 151/1000 | Loss: 0.00000963
Iteration 152/1000 | Loss: 0.00000963
Iteration 153/1000 | Loss: 0.00000963
Iteration 154/1000 | Loss: 0.00000963
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000962
Iteration 160/1000 | Loss: 0.00000962
Iteration 161/1000 | Loss: 0.00000962
Iteration 162/1000 | Loss: 0.00000962
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000962
Iteration 170/1000 | Loss: 0.00000962
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000962
Iteration 173/1000 | Loss: 0.00000962
Iteration 174/1000 | Loss: 0.00000962
Iteration 175/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [9.617822797736153e-06, 9.617822797736153e-06, 9.617822797736153e-06, 9.617822797736153e-06, 9.617822797736153e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.617822797736153e-06

Optimization complete. Final v2v error: 2.698800563812256 mm

Highest mean error: 2.955258369445801 mm for frame 34

Lowest mean error: 2.4638006687164307 mm for frame 133

Saving results

Total time: 38.65105175971985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578512
Iteration 2/25 | Loss: 0.00125563
Iteration 3/25 | Loss: 0.00119037
Iteration 4/25 | Loss: 0.00118046
Iteration 5/25 | Loss: 0.00117713
Iteration 6/25 | Loss: 0.00117648
Iteration 7/25 | Loss: 0.00117648
Iteration 8/25 | Loss: 0.00117648
Iteration 9/25 | Loss: 0.00117648
Iteration 10/25 | Loss: 0.00117648
Iteration 11/25 | Loss: 0.00117648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011764842784032226, 0.0011764842784032226, 0.0011764842784032226, 0.0011764842784032226, 0.0011764842784032226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011764842784032226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98272276
Iteration 2/25 | Loss: 0.00127985
Iteration 3/25 | Loss: 0.00127985
Iteration 4/25 | Loss: 0.00127984
Iteration 5/25 | Loss: 0.00127984
Iteration 6/25 | Loss: 0.00127984
Iteration 7/25 | Loss: 0.00127984
Iteration 8/25 | Loss: 0.00127984
Iteration 9/25 | Loss: 0.00127984
Iteration 10/25 | Loss: 0.00127984
Iteration 11/25 | Loss: 0.00127984
Iteration 12/25 | Loss: 0.00127984
Iteration 13/25 | Loss: 0.00127984
Iteration 14/25 | Loss: 0.00127984
Iteration 15/25 | Loss: 0.00127984
Iteration 16/25 | Loss: 0.00127984
Iteration 17/25 | Loss: 0.00127984
Iteration 18/25 | Loss: 0.00127984
Iteration 19/25 | Loss: 0.00127984
Iteration 20/25 | Loss: 0.00127984
Iteration 21/25 | Loss: 0.00127984
Iteration 22/25 | Loss: 0.00127984
Iteration 23/25 | Loss: 0.00127984
Iteration 24/25 | Loss: 0.00127984
Iteration 25/25 | Loss: 0.00127984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012798417592421174, 0.0012798417592421174, 0.0012798417592421174, 0.0012798417592421174, 0.0012798417592421174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012798417592421174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127984
Iteration 2/1000 | Loss: 0.00001863
Iteration 3/1000 | Loss: 0.00001417
Iteration 4/1000 | Loss: 0.00001278
Iteration 5/1000 | Loss: 0.00001216
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001116
Iteration 8/1000 | Loss: 0.00001094
Iteration 9/1000 | Loss: 0.00001072
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001035
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001028
Iteration 15/1000 | Loss: 0.00001028
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001026
Iteration 18/1000 | Loss: 0.00001025
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001006
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00001002
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001001
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000996
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000992
Iteration 39/1000 | Loss: 0.00000991
Iteration 40/1000 | Loss: 0.00000989
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000988
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000983
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000982
Iteration 57/1000 | Loss: 0.00000982
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000979
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000976
Iteration 67/1000 | Loss: 0.00000976
Iteration 68/1000 | Loss: 0.00000975
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000973
Iteration 72/1000 | Loss: 0.00000973
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000972
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000971
Iteration 77/1000 | Loss: 0.00000971
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000970
Iteration 80/1000 | Loss: 0.00000970
Iteration 81/1000 | Loss: 0.00000969
Iteration 82/1000 | Loss: 0.00000969
Iteration 83/1000 | Loss: 0.00000969
Iteration 84/1000 | Loss: 0.00000969
Iteration 85/1000 | Loss: 0.00000968
Iteration 86/1000 | Loss: 0.00000968
Iteration 87/1000 | Loss: 0.00000968
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000967
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000967
Iteration 92/1000 | Loss: 0.00000967
Iteration 93/1000 | Loss: 0.00000967
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000966
Iteration 96/1000 | Loss: 0.00000966
Iteration 97/1000 | Loss: 0.00000966
Iteration 98/1000 | Loss: 0.00000966
Iteration 99/1000 | Loss: 0.00000966
Iteration 100/1000 | Loss: 0.00000966
Iteration 101/1000 | Loss: 0.00000966
Iteration 102/1000 | Loss: 0.00000966
Iteration 103/1000 | Loss: 0.00000966
Iteration 104/1000 | Loss: 0.00000966
Iteration 105/1000 | Loss: 0.00000966
Iteration 106/1000 | Loss: 0.00000966
Iteration 107/1000 | Loss: 0.00000966
Iteration 108/1000 | Loss: 0.00000965
Iteration 109/1000 | Loss: 0.00000965
Iteration 110/1000 | Loss: 0.00000965
Iteration 111/1000 | Loss: 0.00000965
Iteration 112/1000 | Loss: 0.00000965
Iteration 113/1000 | Loss: 0.00000965
Iteration 114/1000 | Loss: 0.00000965
Iteration 115/1000 | Loss: 0.00000965
Iteration 116/1000 | Loss: 0.00000964
Iteration 117/1000 | Loss: 0.00000964
Iteration 118/1000 | Loss: 0.00000964
Iteration 119/1000 | Loss: 0.00000964
Iteration 120/1000 | Loss: 0.00000964
Iteration 121/1000 | Loss: 0.00000964
Iteration 122/1000 | Loss: 0.00000964
Iteration 123/1000 | Loss: 0.00000964
Iteration 124/1000 | Loss: 0.00000964
Iteration 125/1000 | Loss: 0.00000964
Iteration 126/1000 | Loss: 0.00000964
Iteration 127/1000 | Loss: 0.00000964
Iteration 128/1000 | Loss: 0.00000964
Iteration 129/1000 | Loss: 0.00000964
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000964
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000964
Iteration 136/1000 | Loss: 0.00000964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [9.639542440709192e-06, 9.639542440709192e-06, 9.639542440709192e-06, 9.639542440709192e-06, 9.639542440709192e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.639542440709192e-06

Optimization complete. Final v2v error: 2.6947691440582275 mm

Highest mean error: 2.9801273345947266 mm for frame 96

Lowest mean error: 2.524301052093506 mm for frame 28

Saving results

Total time: 35.83111596107483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882056
Iteration 2/25 | Loss: 0.00210623
Iteration 3/25 | Loss: 0.00140461
Iteration 4/25 | Loss: 0.00134120
Iteration 5/25 | Loss: 0.00131880
Iteration 6/25 | Loss: 0.00128059
Iteration 7/25 | Loss: 0.00125460
Iteration 8/25 | Loss: 0.00123377
Iteration 9/25 | Loss: 0.00121825
Iteration 10/25 | Loss: 0.00119376
Iteration 11/25 | Loss: 0.00119685
Iteration 12/25 | Loss: 0.00118474
Iteration 13/25 | Loss: 0.00117728
Iteration 14/25 | Loss: 0.00117614
Iteration 15/25 | Loss: 0.00117515
Iteration 16/25 | Loss: 0.00117471
Iteration 17/25 | Loss: 0.00117450
Iteration 18/25 | Loss: 0.00117447
Iteration 19/25 | Loss: 0.00117444
Iteration 20/25 | Loss: 0.00117444
Iteration 21/25 | Loss: 0.00117444
Iteration 22/25 | Loss: 0.00117443
Iteration 23/25 | Loss: 0.00117443
Iteration 24/25 | Loss: 0.00117443
Iteration 25/25 | Loss: 0.00117442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92559409
Iteration 2/25 | Loss: 0.00132078
Iteration 3/25 | Loss: 0.00132078
Iteration 4/25 | Loss: 0.00127160
Iteration 5/25 | Loss: 0.00124138
Iteration 6/25 | Loss: 0.00124138
Iteration 7/25 | Loss: 0.00124138
Iteration 8/25 | Loss: 0.00124138
Iteration 9/25 | Loss: 0.00124138
Iteration 10/25 | Loss: 0.00124138
Iteration 11/25 | Loss: 0.00124138
Iteration 12/25 | Loss: 0.00124138
Iteration 13/25 | Loss: 0.00124138
Iteration 14/25 | Loss: 0.00124138
Iteration 15/25 | Loss: 0.00124138
Iteration 16/25 | Loss: 0.00124138
Iteration 17/25 | Loss: 0.00124138
Iteration 18/25 | Loss: 0.00124138
Iteration 19/25 | Loss: 0.00124138
Iteration 20/25 | Loss: 0.00124138
Iteration 21/25 | Loss: 0.00124138
Iteration 22/25 | Loss: 0.00124138
Iteration 23/25 | Loss: 0.00124138
Iteration 24/25 | Loss: 0.00124138
Iteration 25/25 | Loss: 0.00124138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124138
Iteration 2/1000 | Loss: 0.00011622
Iteration 3/1000 | Loss: 0.00005818
Iteration 4/1000 | Loss: 0.00008458
Iteration 5/1000 | Loss: 0.00016264
Iteration 6/1000 | Loss: 0.00001455
Iteration 7/1000 | Loss: 0.00003399
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00008123
Iteration 10/1000 | Loss: 0.00005379
Iteration 11/1000 | Loss: 0.00007557
Iteration 12/1000 | Loss: 0.00008008
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001220
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001176
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00004717
Iteration 19/1000 | Loss: 0.00001624
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001132
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001131
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00004732
Iteration 36/1000 | Loss: 0.00001126
Iteration 37/1000 | Loss: 0.00004059
Iteration 38/1000 | Loss: 0.00003158
Iteration 39/1000 | Loss: 0.00003993
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001106
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001096
Iteration 67/1000 | Loss: 0.00001096
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001095
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001095
Iteration 81/1000 | Loss: 0.00001095
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001094
Iteration 89/1000 | Loss: 0.00001094
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.0942755579890218e-05, 1.0942755579890218e-05, 1.0942755579890218e-05, 1.0942755579890218e-05, 1.0942755579890218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0942755579890218e-05

Optimization complete. Final v2v error: 2.8527777194976807 mm

Highest mean error: 3.2444815635681152 mm for frame 128

Lowest mean error: 2.5184576511383057 mm for frame 234

Saving results

Total time: 84.51029181480408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992444
Iteration 2/25 | Loss: 0.00300967
Iteration 3/25 | Loss: 0.00154429
Iteration 4/25 | Loss: 0.00146014
Iteration 5/25 | Loss: 0.00144406
Iteration 6/25 | Loss: 0.00145982
Iteration 7/25 | Loss: 0.00138585
Iteration 8/25 | Loss: 0.00134742
Iteration 9/25 | Loss: 0.00133698
Iteration 10/25 | Loss: 0.00131617
Iteration 11/25 | Loss: 0.00130083
Iteration 12/25 | Loss: 0.00129693
Iteration 13/25 | Loss: 0.00129559
Iteration 14/25 | Loss: 0.00127940
Iteration 15/25 | Loss: 0.00128156
Iteration 16/25 | Loss: 0.00128336
Iteration 17/25 | Loss: 0.00127759
Iteration 18/25 | Loss: 0.00127150
Iteration 19/25 | Loss: 0.00126248
Iteration 20/25 | Loss: 0.00125381
Iteration 21/25 | Loss: 0.00124534
Iteration 22/25 | Loss: 0.00124237
Iteration 23/25 | Loss: 0.00124130
Iteration 24/25 | Loss: 0.00124083
Iteration 25/25 | Loss: 0.00123777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36699879
Iteration 2/25 | Loss: 0.00245391
Iteration 3/25 | Loss: 0.00226000
Iteration 4/25 | Loss: 0.00226000
Iteration 5/25 | Loss: 0.00226000
Iteration 6/25 | Loss: 0.00226000
Iteration 7/25 | Loss: 0.00226000
Iteration 8/25 | Loss: 0.00226000
Iteration 9/25 | Loss: 0.00226000
Iteration 10/25 | Loss: 0.00226000
Iteration 11/25 | Loss: 0.00226000
Iteration 12/25 | Loss: 0.00226000
Iteration 13/25 | Loss: 0.00226000
Iteration 14/25 | Loss: 0.00225999
Iteration 15/25 | Loss: 0.00225999
Iteration 16/25 | Loss: 0.00225999
Iteration 17/25 | Loss: 0.00225999
Iteration 18/25 | Loss: 0.00225999
Iteration 19/25 | Loss: 0.00225999
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002259994624182582, 0.002259994624182582, 0.002259994624182582, 0.002259994624182582, 0.002259994624182582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002259994624182582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225999
Iteration 2/1000 | Loss: 0.00028511
Iteration 3/1000 | Loss: 0.00076268
Iteration 4/1000 | Loss: 0.00039167
Iteration 5/1000 | Loss: 0.00011430
Iteration 6/1000 | Loss: 0.00004665
Iteration 7/1000 | Loss: 0.00020181
Iteration 8/1000 | Loss: 0.00011879
Iteration 9/1000 | Loss: 0.00003163
Iteration 10/1000 | Loss: 0.00012102
Iteration 11/1000 | Loss: 0.00006948
Iteration 12/1000 | Loss: 0.00003188
Iteration 13/1000 | Loss: 0.00008246
Iteration 14/1000 | Loss: 0.00002620
Iteration 15/1000 | Loss: 0.00008841
Iteration 16/1000 | Loss: 0.00004314
Iteration 17/1000 | Loss: 0.00011597
Iteration 18/1000 | Loss: 0.00031680
Iteration 19/1000 | Loss: 0.00026104
Iteration 20/1000 | Loss: 0.00020923
Iteration 21/1000 | Loss: 0.00015124
Iteration 22/1000 | Loss: 0.00033263
Iteration 23/1000 | Loss: 0.00027540
Iteration 24/1000 | Loss: 0.00019183
Iteration 25/1000 | Loss: 0.00012966
Iteration 26/1000 | Loss: 0.00008992
Iteration 27/1000 | Loss: 0.00012828
Iteration 28/1000 | Loss: 0.00007536
Iteration 29/1000 | Loss: 0.00021274
Iteration 30/1000 | Loss: 0.00010492
Iteration 31/1000 | Loss: 0.00006868
Iteration 32/1000 | Loss: 0.00013477
Iteration 33/1000 | Loss: 0.00030850
Iteration 34/1000 | Loss: 0.00011546
Iteration 35/1000 | Loss: 0.00003063
Iteration 36/1000 | Loss: 0.00041312
Iteration 37/1000 | Loss: 0.00009437
Iteration 38/1000 | Loss: 0.00008821
Iteration 39/1000 | Loss: 0.00040814
Iteration 40/1000 | Loss: 0.00028780
Iteration 41/1000 | Loss: 0.00013922
Iteration 42/1000 | Loss: 0.00002554
Iteration 43/1000 | Loss: 0.00026235
Iteration 44/1000 | Loss: 0.00007901
Iteration 45/1000 | Loss: 0.00021008
Iteration 46/1000 | Loss: 0.00007627
Iteration 47/1000 | Loss: 0.00006896
Iteration 48/1000 | Loss: 0.00006249
Iteration 49/1000 | Loss: 0.00002338
Iteration 50/1000 | Loss: 0.00002211
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00002578
Iteration 53/1000 | Loss: 0.00003278
Iteration 54/1000 | Loss: 0.00003074
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001854
Iteration 57/1000 | Loss: 0.00012343
Iteration 58/1000 | Loss: 0.00004570
Iteration 59/1000 | Loss: 0.00002390
Iteration 60/1000 | Loss: 0.00011447
Iteration 61/1000 | Loss: 0.00006657
Iteration 62/1000 | Loss: 0.00001746
Iteration 63/1000 | Loss: 0.00012528
Iteration 64/1000 | Loss: 0.00005370
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00002003
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00002690
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00002281
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001288
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001277
Iteration 116/1000 | Loss: 0.00001277
Iteration 117/1000 | Loss: 0.00001277
Iteration 118/1000 | Loss: 0.00001277
Iteration 119/1000 | Loss: 0.00001276
Iteration 120/1000 | Loss: 0.00001276
Iteration 121/1000 | Loss: 0.00001276
Iteration 122/1000 | Loss: 0.00001276
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001275
Iteration 125/1000 | Loss: 0.00001275
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001274
Iteration 136/1000 | Loss: 0.00001274
Iteration 137/1000 | Loss: 0.00001274
Iteration 138/1000 | Loss: 0.00001274
Iteration 139/1000 | Loss: 0.00001274
Iteration 140/1000 | Loss: 0.00001274
Iteration 141/1000 | Loss: 0.00001274
Iteration 142/1000 | Loss: 0.00001274
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001274
Iteration 145/1000 | Loss: 0.00001274
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001272
Iteration 155/1000 | Loss: 0.00001272
Iteration 156/1000 | Loss: 0.00001272
Iteration 157/1000 | Loss: 0.00001272
Iteration 158/1000 | Loss: 0.00001272
Iteration 159/1000 | Loss: 0.00001272
Iteration 160/1000 | Loss: 0.00001272
Iteration 161/1000 | Loss: 0.00001272
Iteration 162/1000 | Loss: 0.00001271
Iteration 163/1000 | Loss: 0.00001271
Iteration 164/1000 | Loss: 0.00001271
Iteration 165/1000 | Loss: 0.00001271
Iteration 166/1000 | Loss: 0.00001271
Iteration 167/1000 | Loss: 0.00001271
Iteration 168/1000 | Loss: 0.00001271
Iteration 169/1000 | Loss: 0.00001270
Iteration 170/1000 | Loss: 0.00001270
Iteration 171/1000 | Loss: 0.00001270
Iteration 172/1000 | Loss: 0.00001270
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001269
Iteration 176/1000 | Loss: 0.00001268
Iteration 177/1000 | Loss: 0.00001267
Iteration 178/1000 | Loss: 0.00001267
Iteration 179/1000 | Loss: 0.00001267
Iteration 180/1000 | Loss: 0.00001267
Iteration 181/1000 | Loss: 0.00001267
Iteration 182/1000 | Loss: 0.00001267
Iteration 183/1000 | Loss: 0.00001266
Iteration 184/1000 | Loss: 0.00001266
Iteration 185/1000 | Loss: 0.00001266
Iteration 186/1000 | Loss: 0.00001266
Iteration 187/1000 | Loss: 0.00001265
Iteration 188/1000 | Loss: 0.00001265
Iteration 189/1000 | Loss: 0.00001265
Iteration 190/1000 | Loss: 0.00001265
Iteration 191/1000 | Loss: 0.00001264
Iteration 192/1000 | Loss: 0.00001264
Iteration 193/1000 | Loss: 0.00001264
Iteration 194/1000 | Loss: 0.00001264
Iteration 195/1000 | Loss: 0.00001264
Iteration 196/1000 | Loss: 0.00001264
Iteration 197/1000 | Loss: 0.00001263
Iteration 198/1000 | Loss: 0.00001263
Iteration 199/1000 | Loss: 0.00001263
Iteration 200/1000 | Loss: 0.00001263
Iteration 201/1000 | Loss: 0.00001263
Iteration 202/1000 | Loss: 0.00001263
Iteration 203/1000 | Loss: 0.00001263
Iteration 204/1000 | Loss: 0.00001263
Iteration 205/1000 | Loss: 0.00001263
Iteration 206/1000 | Loss: 0.00001263
Iteration 207/1000 | Loss: 0.00001263
Iteration 208/1000 | Loss: 0.00001263
Iteration 209/1000 | Loss: 0.00001263
Iteration 210/1000 | Loss: 0.00001262
Iteration 211/1000 | Loss: 0.00001262
Iteration 212/1000 | Loss: 0.00001262
Iteration 213/1000 | Loss: 0.00001262
Iteration 214/1000 | Loss: 0.00001262
Iteration 215/1000 | Loss: 0.00001262
Iteration 216/1000 | Loss: 0.00001262
Iteration 217/1000 | Loss: 0.00001261
Iteration 218/1000 | Loss: 0.00001261
Iteration 219/1000 | Loss: 0.00001261
Iteration 220/1000 | Loss: 0.00001261
Iteration 221/1000 | Loss: 0.00001261
Iteration 222/1000 | Loss: 0.00001261
Iteration 223/1000 | Loss: 0.00001261
Iteration 224/1000 | Loss: 0.00001261
Iteration 225/1000 | Loss: 0.00001261
Iteration 226/1000 | Loss: 0.00001261
Iteration 227/1000 | Loss: 0.00001261
Iteration 228/1000 | Loss: 0.00001261
Iteration 229/1000 | Loss: 0.00001261
Iteration 230/1000 | Loss: 0.00001261
Iteration 231/1000 | Loss: 0.00001261
Iteration 232/1000 | Loss: 0.00001261
Iteration 233/1000 | Loss: 0.00001261
Iteration 234/1000 | Loss: 0.00001261
Iteration 235/1000 | Loss: 0.00001261
Iteration 236/1000 | Loss: 0.00001261
Iteration 237/1000 | Loss: 0.00001261
Iteration 238/1000 | Loss: 0.00001261
Iteration 239/1000 | Loss: 0.00001261
Iteration 240/1000 | Loss: 0.00001261
Iteration 241/1000 | Loss: 0.00001261
Iteration 242/1000 | Loss: 0.00001261
Iteration 243/1000 | Loss: 0.00001261
Iteration 244/1000 | Loss: 0.00001261
Iteration 245/1000 | Loss: 0.00001261
Iteration 246/1000 | Loss: 0.00001261
Iteration 247/1000 | Loss: 0.00001261
Iteration 248/1000 | Loss: 0.00001261
Iteration 249/1000 | Loss: 0.00001261
Iteration 250/1000 | Loss: 0.00001261
Iteration 251/1000 | Loss: 0.00001261
Iteration 252/1000 | Loss: 0.00001261
Iteration 253/1000 | Loss: 0.00001261
Iteration 254/1000 | Loss: 0.00001261
Iteration 255/1000 | Loss: 0.00001261
Iteration 256/1000 | Loss: 0.00001261
Iteration 257/1000 | Loss: 0.00001261
Iteration 258/1000 | Loss: 0.00001261
Iteration 259/1000 | Loss: 0.00001261
Iteration 260/1000 | Loss: 0.00001261
Iteration 261/1000 | Loss: 0.00001261
Iteration 262/1000 | Loss: 0.00001261
Iteration 263/1000 | Loss: 0.00001261
Iteration 264/1000 | Loss: 0.00001261
Iteration 265/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.260773842659546e-05, 1.260773842659546e-05, 1.260773842659546e-05, 1.260773842659546e-05, 1.260773842659546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.260773842659546e-05

Optimization complete. Final v2v error: 3.039072036743164 mm

Highest mean error: 4.2935590744018555 mm for frame 99

Lowest mean error: 2.463975429534912 mm for frame 201

Saving results

Total time: 189.83638739585876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604580
Iteration 2/25 | Loss: 0.00141787
Iteration 3/25 | Loss: 0.00124885
Iteration 4/25 | Loss: 0.00123409
Iteration 5/25 | Loss: 0.00123143
Iteration 6/25 | Loss: 0.00123115
Iteration 7/25 | Loss: 0.00123115
Iteration 8/25 | Loss: 0.00123115
Iteration 9/25 | Loss: 0.00123115
Iteration 10/25 | Loss: 0.00123115
Iteration 11/25 | Loss: 0.00123115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00123115093447268, 0.00123115093447268, 0.00123115093447268, 0.00123115093447268, 0.00123115093447268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00123115093447268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.04236460
Iteration 2/25 | Loss: 0.00130208
Iteration 3/25 | Loss: 0.00130201
Iteration 4/25 | Loss: 0.00130201
Iteration 5/25 | Loss: 0.00130201
Iteration 6/25 | Loss: 0.00130201
Iteration 7/25 | Loss: 0.00130201
Iteration 8/25 | Loss: 0.00130201
Iteration 9/25 | Loss: 0.00130201
Iteration 10/25 | Loss: 0.00130201
Iteration 11/25 | Loss: 0.00130201
Iteration 12/25 | Loss: 0.00130201
Iteration 13/25 | Loss: 0.00130201
Iteration 14/25 | Loss: 0.00130201
Iteration 15/25 | Loss: 0.00130201
Iteration 16/25 | Loss: 0.00130201
Iteration 17/25 | Loss: 0.00130201
Iteration 18/25 | Loss: 0.00130201
Iteration 19/25 | Loss: 0.00130201
Iteration 20/25 | Loss: 0.00130200
Iteration 21/25 | Loss: 0.00130200
Iteration 22/25 | Loss: 0.00130200
Iteration 23/25 | Loss: 0.00130200
Iteration 24/25 | Loss: 0.00130200
Iteration 25/25 | Loss: 0.00130200

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130200
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00001751
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001387
Iteration 8/1000 | Loss: 0.00001362
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001289
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001276
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001275
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001273
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001271
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001270
Iteration 39/1000 | Loss: 0.00001270
Iteration 40/1000 | Loss: 0.00001270
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001263
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001262
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001261
Iteration 75/1000 | Loss: 0.00001260
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001260
Iteration 80/1000 | Loss: 0.00001260
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001259
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001259
Iteration 87/1000 | Loss: 0.00001259
Iteration 88/1000 | Loss: 0.00001259
Iteration 89/1000 | Loss: 0.00001259
Iteration 90/1000 | Loss: 0.00001259
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001258
Iteration 96/1000 | Loss: 0.00001258
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001258
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001252
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001250
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001249
Iteration 141/1000 | Loss: 0.00001249
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001247
Iteration 147/1000 | Loss: 0.00001247
Iteration 148/1000 | Loss: 0.00001247
Iteration 149/1000 | Loss: 0.00001247
Iteration 150/1000 | Loss: 0.00001247
Iteration 151/1000 | Loss: 0.00001247
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001247
Iteration 154/1000 | Loss: 0.00001247
Iteration 155/1000 | Loss: 0.00001246
Iteration 156/1000 | Loss: 0.00001246
Iteration 157/1000 | Loss: 0.00001246
Iteration 158/1000 | Loss: 0.00001246
Iteration 159/1000 | Loss: 0.00001246
Iteration 160/1000 | Loss: 0.00001246
Iteration 161/1000 | Loss: 0.00001246
Iteration 162/1000 | Loss: 0.00001245
Iteration 163/1000 | Loss: 0.00001245
Iteration 164/1000 | Loss: 0.00001245
Iteration 165/1000 | Loss: 0.00001245
Iteration 166/1000 | Loss: 0.00001244
Iteration 167/1000 | Loss: 0.00001244
Iteration 168/1000 | Loss: 0.00001244
Iteration 169/1000 | Loss: 0.00001244
Iteration 170/1000 | Loss: 0.00001244
Iteration 171/1000 | Loss: 0.00001244
Iteration 172/1000 | Loss: 0.00001244
Iteration 173/1000 | Loss: 0.00001244
Iteration 174/1000 | Loss: 0.00001244
Iteration 175/1000 | Loss: 0.00001244
Iteration 176/1000 | Loss: 0.00001244
Iteration 177/1000 | Loss: 0.00001244
Iteration 178/1000 | Loss: 0.00001244
Iteration 179/1000 | Loss: 0.00001244
Iteration 180/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.2438033081707545e-05, 1.2438033081707545e-05, 1.2438033081707545e-05, 1.2438033081707545e-05, 1.2438033081707545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2438033081707545e-05

Optimization complete. Final v2v error: 2.9807238578796387 mm

Highest mean error: 4.005718231201172 mm for frame 33

Lowest mean error: 2.5914571285247803 mm for frame 154

Saving results

Total time: 37.88136339187622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531507
Iteration 2/25 | Loss: 0.00124905
Iteration 3/25 | Loss: 0.00119995
Iteration 4/25 | Loss: 0.00119414
Iteration 5/25 | Loss: 0.00119200
Iteration 6/25 | Loss: 0.00119170
Iteration 7/25 | Loss: 0.00119170
Iteration 8/25 | Loss: 0.00119170
Iteration 9/25 | Loss: 0.00119170
Iteration 10/25 | Loss: 0.00119170
Iteration 11/25 | Loss: 0.00119170
Iteration 12/25 | Loss: 0.00119170
Iteration 13/25 | Loss: 0.00119170
Iteration 14/25 | Loss: 0.00119170
Iteration 15/25 | Loss: 0.00119170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011917016236111522, 0.0011917016236111522, 0.0011917016236111522, 0.0011917016236111522, 0.0011917016236111522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011917016236111522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.08098412
Iteration 2/25 | Loss: 0.00120858
Iteration 3/25 | Loss: 0.00120858
Iteration 4/25 | Loss: 0.00120858
Iteration 5/25 | Loss: 0.00120858
Iteration 6/25 | Loss: 0.00120857
Iteration 7/25 | Loss: 0.00120857
Iteration 8/25 | Loss: 0.00120857
Iteration 9/25 | Loss: 0.00120857
Iteration 10/25 | Loss: 0.00120857
Iteration 11/25 | Loss: 0.00120857
Iteration 12/25 | Loss: 0.00120857
Iteration 13/25 | Loss: 0.00120857
Iteration 14/25 | Loss: 0.00120857
Iteration 15/25 | Loss: 0.00120857
Iteration 16/25 | Loss: 0.00120857
Iteration 17/25 | Loss: 0.00120857
Iteration 18/25 | Loss: 0.00120857
Iteration 19/25 | Loss: 0.00120857
Iteration 20/25 | Loss: 0.00120857
Iteration 21/25 | Loss: 0.00120857
Iteration 22/25 | Loss: 0.00120857
Iteration 23/25 | Loss: 0.00120857
Iteration 24/25 | Loss: 0.00120857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012085726484656334, 0.0012085726484656334, 0.0012085726484656334, 0.0012085726484656334, 0.0012085726484656334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012085726484656334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120857
Iteration 2/1000 | Loss: 0.00002186
Iteration 3/1000 | Loss: 0.00001606
Iteration 4/1000 | Loss: 0.00001481
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00001183
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001177
Iteration 19/1000 | Loss: 0.00001177
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001176
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001160
Iteration 33/1000 | Loss: 0.00001160
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001149
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001148
Iteration 49/1000 | Loss: 0.00001148
Iteration 50/1000 | Loss: 0.00001148
Iteration 51/1000 | Loss: 0.00001148
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001144
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001143
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001140
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001136
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001134
Iteration 80/1000 | Loss: 0.00001134
Iteration 81/1000 | Loss: 0.00001134
Iteration 82/1000 | Loss: 0.00001133
Iteration 83/1000 | Loss: 0.00001133
Iteration 84/1000 | Loss: 0.00001133
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001131
Iteration 89/1000 | Loss: 0.00001131
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001130
Iteration 94/1000 | Loss: 0.00001130
Iteration 95/1000 | Loss: 0.00001130
Iteration 96/1000 | Loss: 0.00001130
Iteration 97/1000 | Loss: 0.00001130
Iteration 98/1000 | Loss: 0.00001130
Iteration 99/1000 | Loss: 0.00001130
Iteration 100/1000 | Loss: 0.00001130
Iteration 101/1000 | Loss: 0.00001130
Iteration 102/1000 | Loss: 0.00001130
Iteration 103/1000 | Loss: 0.00001129
Iteration 104/1000 | Loss: 0.00001129
Iteration 105/1000 | Loss: 0.00001129
Iteration 106/1000 | Loss: 0.00001129
Iteration 107/1000 | Loss: 0.00001129
Iteration 108/1000 | Loss: 0.00001128
Iteration 109/1000 | Loss: 0.00001128
Iteration 110/1000 | Loss: 0.00001128
Iteration 111/1000 | Loss: 0.00001128
Iteration 112/1000 | Loss: 0.00001128
Iteration 113/1000 | Loss: 0.00001128
Iteration 114/1000 | Loss: 0.00001128
Iteration 115/1000 | Loss: 0.00001128
Iteration 116/1000 | Loss: 0.00001128
Iteration 117/1000 | Loss: 0.00001128
Iteration 118/1000 | Loss: 0.00001127
Iteration 119/1000 | Loss: 0.00001127
Iteration 120/1000 | Loss: 0.00001127
Iteration 121/1000 | Loss: 0.00001127
Iteration 122/1000 | Loss: 0.00001127
Iteration 123/1000 | Loss: 0.00001127
Iteration 124/1000 | Loss: 0.00001127
Iteration 125/1000 | Loss: 0.00001127
Iteration 126/1000 | Loss: 0.00001127
Iteration 127/1000 | Loss: 0.00001127
Iteration 128/1000 | Loss: 0.00001127
Iteration 129/1000 | Loss: 0.00001127
Iteration 130/1000 | Loss: 0.00001127
Iteration 131/1000 | Loss: 0.00001127
Iteration 132/1000 | Loss: 0.00001126
Iteration 133/1000 | Loss: 0.00001126
Iteration 134/1000 | Loss: 0.00001126
Iteration 135/1000 | Loss: 0.00001126
Iteration 136/1000 | Loss: 0.00001126
Iteration 137/1000 | Loss: 0.00001126
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001126
Iteration 142/1000 | Loss: 0.00001126
Iteration 143/1000 | Loss: 0.00001126
Iteration 144/1000 | Loss: 0.00001125
Iteration 145/1000 | Loss: 0.00001125
Iteration 146/1000 | Loss: 0.00001125
Iteration 147/1000 | Loss: 0.00001125
Iteration 148/1000 | Loss: 0.00001125
Iteration 149/1000 | Loss: 0.00001125
Iteration 150/1000 | Loss: 0.00001125
Iteration 151/1000 | Loss: 0.00001125
Iteration 152/1000 | Loss: 0.00001125
Iteration 153/1000 | Loss: 0.00001124
Iteration 154/1000 | Loss: 0.00001124
Iteration 155/1000 | Loss: 0.00001124
Iteration 156/1000 | Loss: 0.00001124
Iteration 157/1000 | Loss: 0.00001124
Iteration 158/1000 | Loss: 0.00001124
Iteration 159/1000 | Loss: 0.00001124
Iteration 160/1000 | Loss: 0.00001124
Iteration 161/1000 | Loss: 0.00001124
Iteration 162/1000 | Loss: 0.00001124
Iteration 163/1000 | Loss: 0.00001124
Iteration 164/1000 | Loss: 0.00001124
Iteration 165/1000 | Loss: 0.00001124
Iteration 166/1000 | Loss: 0.00001124
Iteration 167/1000 | Loss: 0.00001124
Iteration 168/1000 | Loss: 0.00001124
Iteration 169/1000 | Loss: 0.00001124
Iteration 170/1000 | Loss: 0.00001123
Iteration 171/1000 | Loss: 0.00001123
Iteration 172/1000 | Loss: 0.00001123
Iteration 173/1000 | Loss: 0.00001123
Iteration 174/1000 | Loss: 0.00001123
Iteration 175/1000 | Loss: 0.00001123
Iteration 176/1000 | Loss: 0.00001123
Iteration 177/1000 | Loss: 0.00001122
Iteration 178/1000 | Loss: 0.00001122
Iteration 179/1000 | Loss: 0.00001122
Iteration 180/1000 | Loss: 0.00001122
Iteration 181/1000 | Loss: 0.00001122
Iteration 182/1000 | Loss: 0.00001122
Iteration 183/1000 | Loss: 0.00001122
Iteration 184/1000 | Loss: 0.00001122
Iteration 185/1000 | Loss: 0.00001122
Iteration 186/1000 | Loss: 0.00001122
Iteration 187/1000 | Loss: 0.00001122
Iteration 188/1000 | Loss: 0.00001122
Iteration 189/1000 | Loss: 0.00001122
Iteration 190/1000 | Loss: 0.00001122
Iteration 191/1000 | Loss: 0.00001122
Iteration 192/1000 | Loss: 0.00001122
Iteration 193/1000 | Loss: 0.00001122
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001121
Iteration 196/1000 | Loss: 0.00001121
Iteration 197/1000 | Loss: 0.00001121
Iteration 198/1000 | Loss: 0.00001121
Iteration 199/1000 | Loss: 0.00001121
Iteration 200/1000 | Loss: 0.00001121
Iteration 201/1000 | Loss: 0.00001121
Iteration 202/1000 | Loss: 0.00001121
Iteration 203/1000 | Loss: 0.00001121
Iteration 204/1000 | Loss: 0.00001121
Iteration 205/1000 | Loss: 0.00001121
Iteration 206/1000 | Loss: 0.00001121
Iteration 207/1000 | Loss: 0.00001121
Iteration 208/1000 | Loss: 0.00001121
Iteration 209/1000 | Loss: 0.00001121
Iteration 210/1000 | Loss: 0.00001121
Iteration 211/1000 | Loss: 0.00001121
Iteration 212/1000 | Loss: 0.00001121
Iteration 213/1000 | Loss: 0.00001121
Iteration 214/1000 | Loss: 0.00001121
Iteration 215/1000 | Loss: 0.00001121
Iteration 216/1000 | Loss: 0.00001121
Iteration 217/1000 | Loss: 0.00001121
Iteration 218/1000 | Loss: 0.00001121
Iteration 219/1000 | Loss: 0.00001121
Iteration 220/1000 | Loss: 0.00001121
Iteration 221/1000 | Loss: 0.00001121
Iteration 222/1000 | Loss: 0.00001121
Iteration 223/1000 | Loss: 0.00001121
Iteration 224/1000 | Loss: 0.00001121
Iteration 225/1000 | Loss: 0.00001121
Iteration 226/1000 | Loss: 0.00001121
Iteration 227/1000 | Loss: 0.00001121
Iteration 228/1000 | Loss: 0.00001121
Iteration 229/1000 | Loss: 0.00001121
Iteration 230/1000 | Loss: 0.00001121
Iteration 231/1000 | Loss: 0.00001121
Iteration 232/1000 | Loss: 0.00001121
Iteration 233/1000 | Loss: 0.00001121
Iteration 234/1000 | Loss: 0.00001121
Iteration 235/1000 | Loss: 0.00001121
Iteration 236/1000 | Loss: 0.00001121
Iteration 237/1000 | Loss: 0.00001121
Iteration 238/1000 | Loss: 0.00001121
Iteration 239/1000 | Loss: 0.00001121
Iteration 240/1000 | Loss: 0.00001121
Iteration 241/1000 | Loss: 0.00001121
Iteration 242/1000 | Loss: 0.00001121
Iteration 243/1000 | Loss: 0.00001121
Iteration 244/1000 | Loss: 0.00001121
Iteration 245/1000 | Loss: 0.00001121
Iteration 246/1000 | Loss: 0.00001121
Iteration 247/1000 | Loss: 0.00001121
Iteration 248/1000 | Loss: 0.00001121
Iteration 249/1000 | Loss: 0.00001121
Iteration 250/1000 | Loss: 0.00001121
Iteration 251/1000 | Loss: 0.00001121
Iteration 252/1000 | Loss: 0.00001121
Iteration 253/1000 | Loss: 0.00001121
Iteration 254/1000 | Loss: 0.00001121
Iteration 255/1000 | Loss: 0.00001121
Iteration 256/1000 | Loss: 0.00001121
Iteration 257/1000 | Loss: 0.00001121
Iteration 258/1000 | Loss: 0.00001121
Iteration 259/1000 | Loss: 0.00001121
Iteration 260/1000 | Loss: 0.00001121
Iteration 261/1000 | Loss: 0.00001121
Iteration 262/1000 | Loss: 0.00001121
Iteration 263/1000 | Loss: 0.00001121
Iteration 264/1000 | Loss: 0.00001121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.1212618119316176e-05, 1.1212618119316176e-05, 1.1212618119316176e-05, 1.1212618119316176e-05, 1.1212618119316176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1212618119316176e-05

Optimization complete. Final v2v error: 2.8690145015716553 mm

Highest mean error: 3.4442505836486816 mm for frame 59

Lowest mean error: 2.563964366912842 mm for frame 38

Saving results

Total time: 42.08186626434326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526472
Iteration 2/25 | Loss: 0.00133565
Iteration 3/25 | Loss: 0.00126109
Iteration 4/25 | Loss: 0.00125357
Iteration 5/25 | Loss: 0.00125157
Iteration 6/25 | Loss: 0.00125157
Iteration 7/25 | Loss: 0.00125157
Iteration 8/25 | Loss: 0.00125157
Iteration 9/25 | Loss: 0.00125157
Iteration 10/25 | Loss: 0.00125157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001251567737199366, 0.001251567737199366, 0.001251567737199366, 0.001251567737199366, 0.001251567737199366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001251567737199366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31738997
Iteration 2/25 | Loss: 0.00132432
Iteration 3/25 | Loss: 0.00132429
Iteration 4/25 | Loss: 0.00132429
Iteration 5/25 | Loss: 0.00132429
Iteration 6/25 | Loss: 0.00132429
Iteration 7/25 | Loss: 0.00132428
Iteration 8/25 | Loss: 0.00132428
Iteration 9/25 | Loss: 0.00132428
Iteration 10/25 | Loss: 0.00132428
Iteration 11/25 | Loss: 0.00132428
Iteration 12/25 | Loss: 0.00132428
Iteration 13/25 | Loss: 0.00132428
Iteration 14/25 | Loss: 0.00132428
Iteration 15/25 | Loss: 0.00132428
Iteration 16/25 | Loss: 0.00132428
Iteration 17/25 | Loss: 0.00132428
Iteration 18/25 | Loss: 0.00132428
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013242833083495498, 0.0013242833083495498, 0.0013242833083495498, 0.0013242833083495498, 0.0013242833083495498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013242833083495498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132428
Iteration 2/1000 | Loss: 0.00002977
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001610
Iteration 11/1000 | Loss: 0.00001591
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001559
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00001531
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001526
Iteration 28/1000 | Loss: 0.00001521
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001518
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001502
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001497
Iteration 72/1000 | Loss: 0.00001496
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001492
Iteration 78/1000 | Loss: 0.00001492
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001485
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001480
Iteration 124/1000 | Loss: 0.00001480
Iteration 125/1000 | Loss: 0.00001480
Iteration 126/1000 | Loss: 0.00001479
Iteration 127/1000 | Loss: 0.00001479
Iteration 128/1000 | Loss: 0.00001479
Iteration 129/1000 | Loss: 0.00001479
Iteration 130/1000 | Loss: 0.00001478
Iteration 131/1000 | Loss: 0.00001478
Iteration 132/1000 | Loss: 0.00001478
Iteration 133/1000 | Loss: 0.00001478
Iteration 134/1000 | Loss: 0.00001478
Iteration 135/1000 | Loss: 0.00001477
Iteration 136/1000 | Loss: 0.00001477
Iteration 137/1000 | Loss: 0.00001477
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001477
Iteration 141/1000 | Loss: 0.00001477
Iteration 142/1000 | Loss: 0.00001477
Iteration 143/1000 | Loss: 0.00001477
Iteration 144/1000 | Loss: 0.00001476
Iteration 145/1000 | Loss: 0.00001476
Iteration 146/1000 | Loss: 0.00001476
Iteration 147/1000 | Loss: 0.00001476
Iteration 148/1000 | Loss: 0.00001476
Iteration 149/1000 | Loss: 0.00001476
Iteration 150/1000 | Loss: 0.00001476
Iteration 151/1000 | Loss: 0.00001475
Iteration 152/1000 | Loss: 0.00001475
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Iteration 156/1000 | Loss: 0.00001474
Iteration 157/1000 | Loss: 0.00001473
Iteration 158/1000 | Loss: 0.00001473
Iteration 159/1000 | Loss: 0.00001473
Iteration 160/1000 | Loss: 0.00001473
Iteration 161/1000 | Loss: 0.00001473
Iteration 162/1000 | Loss: 0.00001473
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Iteration 165/1000 | Loss: 0.00001472
Iteration 166/1000 | Loss: 0.00001472
Iteration 167/1000 | Loss: 0.00001472
Iteration 168/1000 | Loss: 0.00001472
Iteration 169/1000 | Loss: 0.00001471
Iteration 170/1000 | Loss: 0.00001471
Iteration 171/1000 | Loss: 0.00001471
Iteration 172/1000 | Loss: 0.00001471
Iteration 173/1000 | Loss: 0.00001471
Iteration 174/1000 | Loss: 0.00001471
Iteration 175/1000 | Loss: 0.00001471
Iteration 176/1000 | Loss: 0.00001471
Iteration 177/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.4711106814502273e-05, 1.4711106814502273e-05, 1.4711106814502273e-05, 1.4711106814502273e-05, 1.4711106814502273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4711106814502273e-05

Optimization complete. Final v2v error: 3.2150442600250244 mm

Highest mean error: 3.7626936435699463 mm for frame 143

Lowest mean error: 2.609750747680664 mm for frame 36

Saving results

Total time: 48.28114175796509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992639
Iteration 2/25 | Loss: 0.00187903
Iteration 3/25 | Loss: 0.00156368
Iteration 4/25 | Loss: 0.00152675
Iteration 5/25 | Loss: 0.00151976
Iteration 6/25 | Loss: 0.00150258
Iteration 7/25 | Loss: 0.00146657
Iteration 8/25 | Loss: 0.00144430
Iteration 9/25 | Loss: 0.00142870
Iteration 10/25 | Loss: 0.00142554
Iteration 11/25 | Loss: 0.00143554
Iteration 12/25 | Loss: 0.00145667
Iteration 13/25 | Loss: 0.00146457
Iteration 14/25 | Loss: 0.00145756
Iteration 15/25 | Loss: 0.00141735
Iteration 16/25 | Loss: 0.00139959
Iteration 17/25 | Loss: 0.00139764
Iteration 18/25 | Loss: 0.00137795
Iteration 19/25 | Loss: 0.00136920
Iteration 20/25 | Loss: 0.00135217
Iteration 21/25 | Loss: 0.00135101
Iteration 22/25 | Loss: 0.00135422
Iteration 23/25 | Loss: 0.00135882
Iteration 24/25 | Loss: 0.00135622
Iteration 25/25 | Loss: 0.00134709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91244447
Iteration 2/25 | Loss: 0.00862348
Iteration 3/25 | Loss: 0.00243551
Iteration 4/25 | Loss: 0.00243551
Iteration 5/25 | Loss: 0.00243551
Iteration 6/25 | Loss: 0.00243551
Iteration 7/25 | Loss: 0.00243551
Iteration 8/25 | Loss: 0.00243551
Iteration 9/25 | Loss: 0.00243551
Iteration 10/25 | Loss: 0.00243551
Iteration 11/25 | Loss: 0.00243551
Iteration 12/25 | Loss: 0.00243551
Iteration 13/25 | Loss: 0.00243551
Iteration 14/25 | Loss: 0.00243551
Iteration 15/25 | Loss: 0.00243551
Iteration 16/25 | Loss: 0.00243551
Iteration 17/25 | Loss: 0.00243551
Iteration 18/25 | Loss: 0.00243551
Iteration 19/25 | Loss: 0.00243551
Iteration 20/25 | Loss: 0.00243551
Iteration 21/25 | Loss: 0.00243551
Iteration 22/25 | Loss: 0.00243551
Iteration 23/25 | Loss: 0.00243551
Iteration 24/25 | Loss: 0.00243551
Iteration 25/25 | Loss: 0.00243551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243551
Iteration 2/1000 | Loss: 0.00378149
Iteration 3/1000 | Loss: 0.00089360
Iteration 4/1000 | Loss: 0.00054108
Iteration 5/1000 | Loss: 0.00046721
Iteration 6/1000 | Loss: 0.00031063
Iteration 7/1000 | Loss: 0.00040429
Iteration 8/1000 | Loss: 0.00049286
Iteration 9/1000 | Loss: 0.00145756
Iteration 10/1000 | Loss: 0.00037499
Iteration 11/1000 | Loss: 0.00021040
Iteration 12/1000 | Loss: 0.00027217
Iteration 13/1000 | Loss: 0.00019706
Iteration 14/1000 | Loss: 0.00035041
Iteration 15/1000 | Loss: 0.00044421
Iteration 16/1000 | Loss: 0.00061714
Iteration 17/1000 | Loss: 0.00066892
Iteration 18/1000 | Loss: 0.00059602
Iteration 19/1000 | Loss: 0.00060738
Iteration 20/1000 | Loss: 0.00057319
Iteration 21/1000 | Loss: 0.00069729
Iteration 22/1000 | Loss: 0.00058862
Iteration 23/1000 | Loss: 0.00037221
Iteration 24/1000 | Loss: 0.00037132
Iteration 25/1000 | Loss: 0.00049008
Iteration 26/1000 | Loss: 0.00049458
Iteration 27/1000 | Loss: 0.00052757
Iteration 28/1000 | Loss: 0.00051647
Iteration 29/1000 | Loss: 0.00050591
Iteration 30/1000 | Loss: 0.00088931
Iteration 31/1000 | Loss: 0.00042504
Iteration 32/1000 | Loss: 0.00038280
Iteration 33/1000 | Loss: 0.00022261
Iteration 34/1000 | Loss: 0.00024142
Iteration 35/1000 | Loss: 0.00082316
Iteration 36/1000 | Loss: 0.00040931
Iteration 37/1000 | Loss: 0.00025473
Iteration 38/1000 | Loss: 0.00040080
Iteration 39/1000 | Loss: 0.00108163
Iteration 40/1000 | Loss: 0.00032148
Iteration 41/1000 | Loss: 0.00031364
Iteration 42/1000 | Loss: 0.00034743
Iteration 43/1000 | Loss: 0.00043590
Iteration 44/1000 | Loss: 0.00038448
Iteration 45/1000 | Loss: 0.00029603
Iteration 46/1000 | Loss: 0.00023460
Iteration 47/1000 | Loss: 0.00023491
Iteration 48/1000 | Loss: 0.00009530
Iteration 49/1000 | Loss: 0.00025626
Iteration 50/1000 | Loss: 0.00017223
Iteration 51/1000 | Loss: 0.00024773
Iteration 52/1000 | Loss: 0.00022129
Iteration 53/1000 | Loss: 0.00068696
Iteration 54/1000 | Loss: 0.00049588
Iteration 55/1000 | Loss: 0.00032702
Iteration 56/1000 | Loss: 0.00020344
Iteration 57/1000 | Loss: 0.00025666
Iteration 58/1000 | Loss: 0.00035525
Iteration 59/1000 | Loss: 0.00140259
Iteration 60/1000 | Loss: 0.00051847
Iteration 61/1000 | Loss: 0.00062411
Iteration 62/1000 | Loss: 0.00017211
Iteration 63/1000 | Loss: 0.00032951
Iteration 64/1000 | Loss: 0.00027328
Iteration 65/1000 | Loss: 0.00025697
Iteration 66/1000 | Loss: 0.00028046
Iteration 67/1000 | Loss: 0.00020909
Iteration 68/1000 | Loss: 0.00035737
Iteration 69/1000 | Loss: 0.00041169
Iteration 70/1000 | Loss: 0.00043308
Iteration 71/1000 | Loss: 0.00039427
Iteration 72/1000 | Loss: 0.00027745
Iteration 73/1000 | Loss: 0.00034985
Iteration 74/1000 | Loss: 0.00132177
Iteration 75/1000 | Loss: 0.00043857
Iteration 76/1000 | Loss: 0.00040674
Iteration 77/1000 | Loss: 0.00032968
Iteration 78/1000 | Loss: 0.00030142
Iteration 79/1000 | Loss: 0.00025713
Iteration 80/1000 | Loss: 0.00029010
Iteration 81/1000 | Loss: 0.00029666
Iteration 82/1000 | Loss: 0.00169405
Iteration 83/1000 | Loss: 0.00056845
Iteration 84/1000 | Loss: 0.00037298
Iteration 85/1000 | Loss: 0.00035852
Iteration 86/1000 | Loss: 0.00038014
Iteration 87/1000 | Loss: 0.00051352
Iteration 88/1000 | Loss: 0.00047522
Iteration 89/1000 | Loss: 0.00056500
Iteration 90/1000 | Loss: 0.00057647
Iteration 91/1000 | Loss: 0.00042772
Iteration 92/1000 | Loss: 0.00060488
Iteration 93/1000 | Loss: 0.00040083
Iteration 94/1000 | Loss: 0.00075067
Iteration 95/1000 | Loss: 0.00132261
Iteration 96/1000 | Loss: 0.00069116
Iteration 97/1000 | Loss: 0.00059530
Iteration 98/1000 | Loss: 0.00033892
Iteration 99/1000 | Loss: 0.00044791
Iteration 100/1000 | Loss: 0.00037334
Iteration 101/1000 | Loss: 0.00038603
Iteration 102/1000 | Loss: 0.00040195
Iteration 103/1000 | Loss: 0.00042365
Iteration 104/1000 | Loss: 0.00102313
Iteration 105/1000 | Loss: 0.00016365
Iteration 106/1000 | Loss: 0.00013210
Iteration 107/1000 | Loss: 0.00035101
Iteration 108/1000 | Loss: 0.00109836
Iteration 109/1000 | Loss: 0.00042437
Iteration 110/1000 | Loss: 0.00037808
Iteration 111/1000 | Loss: 0.00058853
Iteration 112/1000 | Loss: 0.00053290
Iteration 113/1000 | Loss: 0.00061354
Iteration 114/1000 | Loss: 0.00059754
Iteration 115/1000 | Loss: 0.00054131
Iteration 116/1000 | Loss: 0.00049430
Iteration 117/1000 | Loss: 0.00033890
Iteration 118/1000 | Loss: 0.00043202
Iteration 119/1000 | Loss: 0.00046182
Iteration 120/1000 | Loss: 0.00048659
Iteration 121/1000 | Loss: 0.00042867
Iteration 122/1000 | Loss: 0.00045297
Iteration 123/1000 | Loss: 0.00037841
Iteration 124/1000 | Loss: 0.00027118
Iteration 125/1000 | Loss: 0.00031743
Iteration 126/1000 | Loss: 0.00038116
Iteration 127/1000 | Loss: 0.00037146
Iteration 128/1000 | Loss: 0.00035140
Iteration 129/1000 | Loss: 0.00041128
Iteration 130/1000 | Loss: 0.00032265
Iteration 131/1000 | Loss: 0.00033971
Iteration 132/1000 | Loss: 0.00017453
Iteration 133/1000 | Loss: 0.00015833
Iteration 134/1000 | Loss: 0.00033080
Iteration 135/1000 | Loss: 0.00025498
Iteration 136/1000 | Loss: 0.00022012
Iteration 137/1000 | Loss: 0.00022399
Iteration 138/1000 | Loss: 0.00032008
Iteration 139/1000 | Loss: 0.00029490
Iteration 140/1000 | Loss: 0.00032247
Iteration 141/1000 | Loss: 0.00207642
Iteration 142/1000 | Loss: 0.00110282
Iteration 143/1000 | Loss: 0.00019770
Iteration 144/1000 | Loss: 0.00016205
Iteration 145/1000 | Loss: 0.00011527
Iteration 146/1000 | Loss: 0.00025728
Iteration 147/1000 | Loss: 0.00030322
Iteration 148/1000 | Loss: 0.00037819
Iteration 149/1000 | Loss: 0.00066100
Iteration 150/1000 | Loss: 0.00041457
Iteration 151/1000 | Loss: 0.00044133
Iteration 152/1000 | Loss: 0.00067812
Iteration 153/1000 | Loss: 0.00033732
Iteration 154/1000 | Loss: 0.00042482
Iteration 155/1000 | Loss: 0.00030568
Iteration 156/1000 | Loss: 0.00061426
Iteration 157/1000 | Loss: 0.00046515
Iteration 158/1000 | Loss: 0.00057767
Iteration 159/1000 | Loss: 0.00023101
Iteration 160/1000 | Loss: 0.00012453
Iteration 161/1000 | Loss: 0.00017373
Iteration 162/1000 | Loss: 0.00020947
Iteration 163/1000 | Loss: 0.00019701
Iteration 164/1000 | Loss: 0.00020506
Iteration 165/1000 | Loss: 0.00010836
Iteration 166/1000 | Loss: 0.00075956
Iteration 167/1000 | Loss: 0.00050664
Iteration 168/1000 | Loss: 0.00019719
Iteration 169/1000 | Loss: 0.00015770
Iteration 170/1000 | Loss: 0.00078708
Iteration 171/1000 | Loss: 0.00019391
Iteration 172/1000 | Loss: 0.00013674
Iteration 173/1000 | Loss: 0.00018465
Iteration 174/1000 | Loss: 0.00019783
Iteration 175/1000 | Loss: 0.00023917
Iteration 176/1000 | Loss: 0.00022015
Iteration 177/1000 | Loss: 0.00024879
Iteration 178/1000 | Loss: 0.00025961
Iteration 179/1000 | Loss: 0.00023873
Iteration 180/1000 | Loss: 0.00026048
Iteration 181/1000 | Loss: 0.00020896
Iteration 182/1000 | Loss: 0.00021007
Iteration 183/1000 | Loss: 0.00015260
Iteration 184/1000 | Loss: 0.00017361
Iteration 185/1000 | Loss: 0.00014541
Iteration 186/1000 | Loss: 0.00004830
Iteration 187/1000 | Loss: 0.00018454
Iteration 188/1000 | Loss: 0.00025710
Iteration 189/1000 | Loss: 0.00012191
Iteration 190/1000 | Loss: 0.00010975
Iteration 191/1000 | Loss: 0.00018349
Iteration 192/1000 | Loss: 0.00031869
Iteration 193/1000 | Loss: 0.00061026
Iteration 194/1000 | Loss: 0.00179357
Iteration 195/1000 | Loss: 0.00047282
Iteration 196/1000 | Loss: 0.00072011
Iteration 197/1000 | Loss: 0.00041117
Iteration 198/1000 | Loss: 0.00016140
Iteration 199/1000 | Loss: 0.00062754
Iteration 200/1000 | Loss: 0.00056816
Iteration 201/1000 | Loss: 0.00065425
Iteration 202/1000 | Loss: 0.00024643
Iteration 203/1000 | Loss: 0.00025915
Iteration 204/1000 | Loss: 0.00009159
Iteration 205/1000 | Loss: 0.00032013
Iteration 206/1000 | Loss: 0.00029991
Iteration 207/1000 | Loss: 0.00066543
Iteration 208/1000 | Loss: 0.00029888
Iteration 209/1000 | Loss: 0.00041805
Iteration 210/1000 | Loss: 0.00022413
Iteration 211/1000 | Loss: 0.00047876
Iteration 212/1000 | Loss: 0.00051132
Iteration 213/1000 | Loss: 0.00043740
Iteration 214/1000 | Loss: 0.00055152
Iteration 215/1000 | Loss: 0.00054204
Iteration 216/1000 | Loss: 0.00106751
Iteration 217/1000 | Loss: 0.00020151
Iteration 218/1000 | Loss: 0.00019633
Iteration 219/1000 | Loss: 0.00050906
Iteration 220/1000 | Loss: 0.00053545
Iteration 221/1000 | Loss: 0.00024503
Iteration 222/1000 | Loss: 0.00053792
Iteration 223/1000 | Loss: 0.00038192
Iteration 224/1000 | Loss: 0.00129156
Iteration 225/1000 | Loss: 0.00035379
Iteration 226/1000 | Loss: 0.00027628
Iteration 227/1000 | Loss: 0.00051091
Iteration 228/1000 | Loss: 0.00055685
Iteration 229/1000 | Loss: 0.00071940
Iteration 230/1000 | Loss: 0.00036817
Iteration 231/1000 | Loss: 0.00030302
Iteration 232/1000 | Loss: 0.00028533
Iteration 233/1000 | Loss: 0.00067249
Iteration 234/1000 | Loss: 0.00052392
Iteration 235/1000 | Loss: 0.00063584
Iteration 236/1000 | Loss: 0.00030227
Iteration 237/1000 | Loss: 0.00031632
Iteration 238/1000 | Loss: 0.00050977
Iteration 239/1000 | Loss: 0.00046312
Iteration 240/1000 | Loss: 0.00013942
Iteration 241/1000 | Loss: 0.00011782
Iteration 242/1000 | Loss: 0.00013254
Iteration 243/1000 | Loss: 0.00023889
Iteration 244/1000 | Loss: 0.00071208
Iteration 245/1000 | Loss: 0.00025772
Iteration 246/1000 | Loss: 0.00026765
Iteration 247/1000 | Loss: 0.00026728
Iteration 248/1000 | Loss: 0.00020364
Iteration 249/1000 | Loss: 0.00012299
Iteration 250/1000 | Loss: 0.00023922
Iteration 251/1000 | Loss: 0.00019443
Iteration 252/1000 | Loss: 0.00013090
Iteration 253/1000 | Loss: 0.00022742
Iteration 254/1000 | Loss: 0.00023374
Iteration 255/1000 | Loss: 0.00010470
Iteration 256/1000 | Loss: 0.00017113
Iteration 257/1000 | Loss: 0.00024127
Iteration 258/1000 | Loss: 0.00022541
Iteration 259/1000 | Loss: 0.00011643
Iteration 260/1000 | Loss: 0.00016487
Iteration 261/1000 | Loss: 0.00024666
Iteration 262/1000 | Loss: 0.00020551
Iteration 263/1000 | Loss: 0.00019771
Iteration 264/1000 | Loss: 0.00016244
Iteration 265/1000 | Loss: 0.00015447
Iteration 266/1000 | Loss: 0.00019018
Iteration 267/1000 | Loss: 0.00011225
Iteration 268/1000 | Loss: 0.00014629
Iteration 269/1000 | Loss: 0.00017991
Iteration 270/1000 | Loss: 0.00006417
Iteration 271/1000 | Loss: 0.00020924
Iteration 272/1000 | Loss: 0.00009959
Iteration 273/1000 | Loss: 0.00017327
Iteration 274/1000 | Loss: 0.00017836
Iteration 275/1000 | Loss: 0.00016201
Iteration 276/1000 | Loss: 0.00018339
Iteration 277/1000 | Loss: 0.00027386
Iteration 278/1000 | Loss: 0.00013343
Iteration 279/1000 | Loss: 0.00011099
Iteration 280/1000 | Loss: 0.00024103
Iteration 281/1000 | Loss: 0.00024999
Iteration 282/1000 | Loss: 0.00017928
Iteration 283/1000 | Loss: 0.00020631
Iteration 284/1000 | Loss: 0.00021637
Iteration 285/1000 | Loss: 0.00019385
Iteration 286/1000 | Loss: 0.00021647
Iteration 287/1000 | Loss: 0.00034282
Iteration 288/1000 | Loss: 0.00017922
Iteration 289/1000 | Loss: 0.00004578
Iteration 290/1000 | Loss: 0.00013385
Iteration 291/1000 | Loss: 0.00017172
Iteration 292/1000 | Loss: 0.00019039
Iteration 293/1000 | Loss: 0.00032521
Iteration 294/1000 | Loss: 0.00022355
Iteration 295/1000 | Loss: 0.00008724
Iteration 296/1000 | Loss: 0.00024608
Iteration 297/1000 | Loss: 0.00023297
Iteration 298/1000 | Loss: 0.00015435
Iteration 299/1000 | Loss: 0.00012215
Iteration 300/1000 | Loss: 0.00040994
Iteration 301/1000 | Loss: 0.00013476
Iteration 302/1000 | Loss: 0.00037791
Iteration 303/1000 | Loss: 0.00033218
Iteration 304/1000 | Loss: 0.00024769
Iteration 305/1000 | Loss: 0.00041112
Iteration 306/1000 | Loss: 0.00024779
Iteration 307/1000 | Loss: 0.00018481
Iteration 308/1000 | Loss: 0.00020253
Iteration 309/1000 | Loss: 0.00009510
Iteration 310/1000 | Loss: 0.00020147
Iteration 311/1000 | Loss: 0.00008648
Iteration 312/1000 | Loss: 0.00019718
Iteration 313/1000 | Loss: 0.00171503
Iteration 314/1000 | Loss: 0.00026279
Iteration 315/1000 | Loss: 0.00036168
Iteration 316/1000 | Loss: 0.00018108
Iteration 317/1000 | Loss: 0.00066508
Iteration 318/1000 | Loss: 0.00018547
Iteration 319/1000 | Loss: 0.00020913
Iteration 320/1000 | Loss: 0.00015020
Iteration 321/1000 | Loss: 0.00011085
Iteration 322/1000 | Loss: 0.00026686
Iteration 323/1000 | Loss: 0.00014534
Iteration 324/1000 | Loss: 0.00018391
Iteration 325/1000 | Loss: 0.00022689
Iteration 326/1000 | Loss: 0.00013426
Iteration 327/1000 | Loss: 0.00008093
Iteration 328/1000 | Loss: 0.00017178
Iteration 329/1000 | Loss: 0.00006649
Iteration 330/1000 | Loss: 0.00013047
Iteration 331/1000 | Loss: 0.00013902
Iteration 332/1000 | Loss: 0.00025895
Iteration 333/1000 | Loss: 0.00062103
Iteration 334/1000 | Loss: 0.00043617
Iteration 335/1000 | Loss: 0.00012471
Iteration 336/1000 | Loss: 0.00012105
Iteration 337/1000 | Loss: 0.00014529
Iteration 338/1000 | Loss: 0.00040610
Iteration 339/1000 | Loss: 0.00010033
Iteration 340/1000 | Loss: 0.00004348
Iteration 341/1000 | Loss: 0.00023354
Iteration 342/1000 | Loss: 0.00075155
Iteration 343/1000 | Loss: 0.00072218
Iteration 344/1000 | Loss: 0.00028363
Iteration 345/1000 | Loss: 0.00021955
Iteration 346/1000 | Loss: 0.00033693
Iteration 347/1000 | Loss: 0.00027652
Iteration 348/1000 | Loss: 0.00043854
Iteration 349/1000 | Loss: 0.00076911
Iteration 350/1000 | Loss: 0.00172843
Iteration 351/1000 | Loss: 0.00028752
Iteration 352/1000 | Loss: 0.00035516
Iteration 353/1000 | Loss: 0.00026212
Iteration 354/1000 | Loss: 0.00025946
Iteration 355/1000 | Loss: 0.00062464
Iteration 356/1000 | Loss: 0.00029466
Iteration 357/1000 | Loss: 0.00040776
Iteration 358/1000 | Loss: 0.00041004
Iteration 359/1000 | Loss: 0.00018012
Iteration 360/1000 | Loss: 0.00013599
Iteration 361/1000 | Loss: 0.00013812
Iteration 362/1000 | Loss: 0.00023150
Iteration 363/1000 | Loss: 0.00018133
Iteration 364/1000 | Loss: 0.00024681
Iteration 365/1000 | Loss: 0.00013647
Iteration 366/1000 | Loss: 0.00011739
Iteration 367/1000 | Loss: 0.00029133
Iteration 368/1000 | Loss: 0.00028499
Iteration 369/1000 | Loss: 0.00015981
Iteration 370/1000 | Loss: 0.00016113
Iteration 371/1000 | Loss: 0.00016887
Iteration 372/1000 | Loss: 0.00020281
Iteration 373/1000 | Loss: 0.00013512
Iteration 374/1000 | Loss: 0.00020099
Iteration 375/1000 | Loss: 0.00019266
Iteration 376/1000 | Loss: 0.00026456
Iteration 377/1000 | Loss: 0.00025755
Iteration 378/1000 | Loss: 0.00034616
Iteration 379/1000 | Loss: 0.00024726
Iteration 380/1000 | Loss: 0.00015647
Iteration 381/1000 | Loss: 0.00025752
Iteration 382/1000 | Loss: 0.00025611
Iteration 383/1000 | Loss: 0.00024968
Iteration 384/1000 | Loss: 0.00013705
Iteration 385/1000 | Loss: 0.00012446
Iteration 386/1000 | Loss: 0.00011527
Iteration 387/1000 | Loss: 0.00011370
Iteration 388/1000 | Loss: 0.00012334
Iteration 389/1000 | Loss: 0.00017039
Iteration 390/1000 | Loss: 0.00017186
Iteration 391/1000 | Loss: 0.00017450
Iteration 392/1000 | Loss: 0.00017423
Iteration 393/1000 | Loss: 0.00009384
Iteration 394/1000 | Loss: 0.00012357
Iteration 395/1000 | Loss: 0.00011023
Iteration 396/1000 | Loss: 0.00015944
Iteration 397/1000 | Loss: 0.00017025
Iteration 398/1000 | Loss: 0.00017591
Iteration 399/1000 | Loss: 0.00031471
Iteration 400/1000 | Loss: 0.00009225
Iteration 401/1000 | Loss: 0.00008637
Iteration 402/1000 | Loss: 0.00009119
Iteration 403/1000 | Loss: 0.00010011
Iteration 404/1000 | Loss: 0.00012296
Iteration 405/1000 | Loss: 0.00012265
Iteration 406/1000 | Loss: 0.00017862
Iteration 407/1000 | Loss: 0.00013005
Iteration 408/1000 | Loss: 0.00015670
Iteration 409/1000 | Loss: 0.00020549
Iteration 410/1000 | Loss: 0.00020283
Iteration 411/1000 | Loss: 0.00025711
Iteration 412/1000 | Loss: 0.00019442
Iteration 413/1000 | Loss: 0.00023834
Iteration 414/1000 | Loss: 0.00019404
Iteration 415/1000 | Loss: 0.00004854
Iteration 416/1000 | Loss: 0.00016987
Iteration 417/1000 | Loss: 0.00021843
Iteration 418/1000 | Loss: 0.00024875
Iteration 419/1000 | Loss: 0.00015016
Iteration 420/1000 | Loss: 0.00059640
Iteration 421/1000 | Loss: 0.00064439
Iteration 422/1000 | Loss: 0.00009409
Iteration 423/1000 | Loss: 0.00016924
Iteration 424/1000 | Loss: 0.00009892
Iteration 425/1000 | Loss: 0.00011345
Iteration 426/1000 | Loss: 0.00017210
Iteration 427/1000 | Loss: 0.00014403
Iteration 428/1000 | Loss: 0.00022891
Iteration 429/1000 | Loss: 0.00009709
Iteration 430/1000 | Loss: 0.00005635
Iteration 431/1000 | Loss: 0.00013461
Iteration 432/1000 | Loss: 0.00007852
Iteration 433/1000 | Loss: 0.00005988
Iteration 434/1000 | Loss: 0.00004753
Iteration 435/1000 | Loss: 0.00007916
Iteration 436/1000 | Loss: 0.00006644
Iteration 437/1000 | Loss: 0.00004500
Iteration 438/1000 | Loss: 0.00004214
Iteration 439/1000 | Loss: 0.00002752
Iteration 440/1000 | Loss: 0.00008449
Iteration 441/1000 | Loss: 0.00010731
Iteration 442/1000 | Loss: 0.00010423
Iteration 443/1000 | Loss: 0.00004781
Iteration 444/1000 | Loss: 0.00010042
Iteration 445/1000 | Loss: 0.00013569
Iteration 446/1000 | Loss: 0.00010184
Iteration 447/1000 | Loss: 0.00003181
Iteration 448/1000 | Loss: 0.00016222
Iteration 449/1000 | Loss: 0.00019341
Iteration 450/1000 | Loss: 0.00015188
Iteration 451/1000 | Loss: 0.00018709
Iteration 452/1000 | Loss: 0.00040412
Iteration 453/1000 | Loss: 0.00065265
Iteration 454/1000 | Loss: 0.00031658
Iteration 455/1000 | Loss: 0.00031243
Iteration 456/1000 | Loss: 0.00003237
Iteration 457/1000 | Loss: 0.00002762
Iteration 458/1000 | Loss: 0.00002614
Iteration 459/1000 | Loss: 0.00002522
Iteration 460/1000 | Loss: 0.00002438
Iteration 461/1000 | Loss: 0.00002411
Iteration 462/1000 | Loss: 0.00002382
Iteration 463/1000 | Loss: 0.00016615
Iteration 464/1000 | Loss: 0.00003078
Iteration 465/1000 | Loss: 0.00002711
Iteration 466/1000 | Loss: 0.00002534
Iteration 467/1000 | Loss: 0.00002423
Iteration 468/1000 | Loss: 0.00002356
Iteration 469/1000 | Loss: 0.00002306
Iteration 470/1000 | Loss: 0.00002283
Iteration 471/1000 | Loss: 0.00002278
Iteration 472/1000 | Loss: 0.00002274
Iteration 473/1000 | Loss: 0.00002269
Iteration 474/1000 | Loss: 0.00002267
Iteration 475/1000 | Loss: 0.00002265
Iteration 476/1000 | Loss: 0.00002265
Iteration 477/1000 | Loss: 0.00002263
Iteration 478/1000 | Loss: 0.00002263
Iteration 479/1000 | Loss: 0.00002262
Iteration 480/1000 | Loss: 0.00002261
Iteration 481/1000 | Loss: 0.00002261
Iteration 482/1000 | Loss: 0.00002259
Iteration 483/1000 | Loss: 0.00002258
Iteration 484/1000 | Loss: 0.00002258
Iteration 485/1000 | Loss: 0.00002253
Iteration 486/1000 | Loss: 0.00002247
Iteration 487/1000 | Loss: 0.00002247
Iteration 488/1000 | Loss: 0.00002246
Iteration 489/1000 | Loss: 0.00002246
Iteration 490/1000 | Loss: 0.00002245
Iteration 491/1000 | Loss: 0.00002245
Iteration 492/1000 | Loss: 0.00002245
Iteration 493/1000 | Loss: 0.00002244
Iteration 494/1000 | Loss: 0.00002242
Iteration 495/1000 | Loss: 0.00002242
Iteration 496/1000 | Loss: 0.00002241
Iteration 497/1000 | Loss: 0.00002240
Iteration 498/1000 | Loss: 0.00002240
Iteration 499/1000 | Loss: 0.00002239
Iteration 500/1000 | Loss: 0.00002238
Iteration 501/1000 | Loss: 0.00002237
Iteration 502/1000 | Loss: 0.00002237
Iteration 503/1000 | Loss: 0.00002236
Iteration 504/1000 | Loss: 0.00002236
Iteration 505/1000 | Loss: 0.00002236
Iteration 506/1000 | Loss: 0.00002235
Iteration 507/1000 | Loss: 0.00002235
Iteration 508/1000 | Loss: 0.00002233
Iteration 509/1000 | Loss: 0.00002232
Iteration 510/1000 | Loss: 0.00002232
Iteration 511/1000 | Loss: 0.00002232
Iteration 512/1000 | Loss: 0.00002231
Iteration 513/1000 | Loss: 0.00002231
Iteration 514/1000 | Loss: 0.00002231
Iteration 515/1000 | Loss: 0.00002230
Iteration 516/1000 | Loss: 0.00002230
Iteration 517/1000 | Loss: 0.00002230
Iteration 518/1000 | Loss: 0.00002230
Iteration 519/1000 | Loss: 0.00002230
Iteration 520/1000 | Loss: 0.00002229
Iteration 521/1000 | Loss: 0.00002229
Iteration 522/1000 | Loss: 0.00002228
Iteration 523/1000 | Loss: 0.00002228
Iteration 524/1000 | Loss: 0.00002228
Iteration 525/1000 | Loss: 0.00002228
Iteration 526/1000 | Loss: 0.00002228
Iteration 527/1000 | Loss: 0.00002227
Iteration 528/1000 | Loss: 0.00002227
Iteration 529/1000 | Loss: 0.00002227
Iteration 530/1000 | Loss: 0.00002227
Iteration 531/1000 | Loss: 0.00002227
Iteration 532/1000 | Loss: 0.00002227
Iteration 533/1000 | Loss: 0.00002226
Iteration 534/1000 | Loss: 0.00002226
Iteration 535/1000 | Loss: 0.00002226
Iteration 536/1000 | Loss: 0.00002224
Iteration 537/1000 | Loss: 0.00002224
Iteration 538/1000 | Loss: 0.00002222
Iteration 539/1000 | Loss: 0.00002221
Iteration 540/1000 | Loss: 0.00002217
Iteration 541/1000 | Loss: 0.00002217
Iteration 542/1000 | Loss: 0.00002216
Iteration 543/1000 | Loss: 0.00002216
Iteration 544/1000 | Loss: 0.00002215
Iteration 545/1000 | Loss: 0.00002215
Iteration 546/1000 | Loss: 0.00002215
Iteration 547/1000 | Loss: 0.00002215
Iteration 548/1000 | Loss: 0.00002215
Iteration 549/1000 | Loss: 0.00002215
Iteration 550/1000 | Loss: 0.00002215
Iteration 551/1000 | Loss: 0.00002215
Iteration 552/1000 | Loss: 0.00002215
Iteration 553/1000 | Loss: 0.00002215
Iteration 554/1000 | Loss: 0.00002215
Iteration 555/1000 | Loss: 0.00002214
Iteration 556/1000 | Loss: 0.00002214
Iteration 557/1000 | Loss: 0.00002214
Iteration 558/1000 | Loss: 0.00002214
Iteration 559/1000 | Loss: 0.00002214
Iteration 560/1000 | Loss: 0.00002214
Iteration 561/1000 | Loss: 0.00002214
Iteration 562/1000 | Loss: 0.00002214
Iteration 563/1000 | Loss: 0.00002214
Iteration 564/1000 | Loss: 0.00002214
Iteration 565/1000 | Loss: 0.00002214
Iteration 566/1000 | Loss: 0.00002214
Iteration 567/1000 | Loss: 0.00002214
Iteration 568/1000 | Loss: 0.00002214
Iteration 569/1000 | Loss: 0.00002214
Iteration 570/1000 | Loss: 0.00002214
Iteration 571/1000 | Loss: 0.00002214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 571. Stopping optimization.
Last 5 losses: [2.2141277440823615e-05, 2.2141277440823615e-05, 2.2141277440823615e-05, 2.2141277440823615e-05, 2.2141277440823615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2141277440823615e-05

Optimization complete. Final v2v error: 3.865598201751709 mm

Highest mean error: 5.446578025817871 mm for frame 179

Lowest mean error: 3.3092105388641357 mm for frame 0

Saving results

Total time: 804.5729115009308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597281
Iteration 2/25 | Loss: 0.00125417
Iteration 3/25 | Loss: 0.00119002
Iteration 4/25 | Loss: 0.00117997
Iteration 5/25 | Loss: 0.00117587
Iteration 6/25 | Loss: 0.00117487
Iteration 7/25 | Loss: 0.00117487
Iteration 8/25 | Loss: 0.00117487
Iteration 9/25 | Loss: 0.00117487
Iteration 10/25 | Loss: 0.00117487
Iteration 11/25 | Loss: 0.00117487
Iteration 12/25 | Loss: 0.00117487
Iteration 13/25 | Loss: 0.00117487
Iteration 14/25 | Loss: 0.00117487
Iteration 15/25 | Loss: 0.00117487
Iteration 16/25 | Loss: 0.00117487
Iteration 17/25 | Loss: 0.00117487
Iteration 18/25 | Loss: 0.00117487
Iteration 19/25 | Loss: 0.00117487
Iteration 20/25 | Loss: 0.00117487
Iteration 21/25 | Loss: 0.00117487
Iteration 22/25 | Loss: 0.00117487
Iteration 23/25 | Loss: 0.00117487
Iteration 24/25 | Loss: 0.00117487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011748677352443337, 0.0011748677352443337, 0.0011748677352443337, 0.0011748677352443337, 0.0011748677352443337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011748677352443337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.85306883
Iteration 2/25 | Loss: 0.00128607
Iteration 3/25 | Loss: 0.00128607
Iteration 4/25 | Loss: 0.00128607
Iteration 5/25 | Loss: 0.00128607
Iteration 6/25 | Loss: 0.00128607
Iteration 7/25 | Loss: 0.00128607
Iteration 8/25 | Loss: 0.00128607
Iteration 9/25 | Loss: 0.00128607
Iteration 10/25 | Loss: 0.00128607
Iteration 11/25 | Loss: 0.00128607
Iteration 12/25 | Loss: 0.00128607
Iteration 13/25 | Loss: 0.00128607
Iteration 14/25 | Loss: 0.00128607
Iteration 15/25 | Loss: 0.00128607
Iteration 16/25 | Loss: 0.00128607
Iteration 17/25 | Loss: 0.00128607
Iteration 18/25 | Loss: 0.00128607
Iteration 19/25 | Loss: 0.00128607
Iteration 20/25 | Loss: 0.00128607
Iteration 21/25 | Loss: 0.00128607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012860675342381, 0.0012860675342381, 0.0012860675342381, 0.0012860675342381, 0.0012860675342381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012860675342381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128607
Iteration 2/1000 | Loss: 0.00001997
Iteration 3/1000 | Loss: 0.00001463
Iteration 4/1000 | Loss: 0.00001300
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001192
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001121
Iteration 9/1000 | Loss: 0.00001116
Iteration 10/1000 | Loss: 0.00001087
Iteration 11/1000 | Loss: 0.00001063
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001049
Iteration 14/1000 | Loss: 0.00001045
Iteration 15/1000 | Loss: 0.00001045
Iteration 16/1000 | Loss: 0.00001039
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001019
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001014
Iteration 24/1000 | Loss: 0.00001012
Iteration 25/1000 | Loss: 0.00001011
Iteration 26/1000 | Loss: 0.00001010
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001007
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001003
Iteration 37/1000 | Loss: 0.00001003
Iteration 38/1000 | Loss: 0.00001003
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001003
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00001000
Iteration 43/1000 | Loss: 0.00000996
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000995
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000994
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000993
Iteration 51/1000 | Loss: 0.00000990
Iteration 52/1000 | Loss: 0.00000989
Iteration 53/1000 | Loss: 0.00000989
Iteration 54/1000 | Loss: 0.00000988
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000988
Iteration 57/1000 | Loss: 0.00000987
Iteration 58/1000 | Loss: 0.00000987
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000986
Iteration 61/1000 | Loss: 0.00000986
Iteration 62/1000 | Loss: 0.00000986
Iteration 63/1000 | Loss: 0.00000985
Iteration 64/1000 | Loss: 0.00000985
Iteration 65/1000 | Loss: 0.00000985
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00000985
Iteration 69/1000 | Loss: 0.00000985
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000985
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000983
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000982
Iteration 80/1000 | Loss: 0.00000982
Iteration 81/1000 | Loss: 0.00000982
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000981
Iteration 84/1000 | Loss: 0.00000981
Iteration 85/1000 | Loss: 0.00000981
Iteration 86/1000 | Loss: 0.00000981
Iteration 87/1000 | Loss: 0.00000981
Iteration 88/1000 | Loss: 0.00000981
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000980
Iteration 92/1000 | Loss: 0.00000980
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000979
Iteration 112/1000 | Loss: 0.00000979
Iteration 113/1000 | Loss: 0.00000979
Iteration 114/1000 | Loss: 0.00000979
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000979
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000979
Iteration 119/1000 | Loss: 0.00000979
Iteration 120/1000 | Loss: 0.00000979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [9.788843271962833e-06, 9.788843271962833e-06, 9.788843271962833e-06, 9.788843271962833e-06, 9.788843271962833e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.788843271962833e-06

Optimization complete. Final v2v error: 2.7091498374938965 mm

Highest mean error: 2.8938937187194824 mm for frame 99

Lowest mean error: 2.565178871154785 mm for frame 122

Saving results

Total time: 35.62591791152954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422921
Iteration 2/25 | Loss: 0.00128070
Iteration 3/25 | Loss: 0.00121072
Iteration 4/25 | Loss: 0.00119347
Iteration 5/25 | Loss: 0.00118784
Iteration 6/25 | Loss: 0.00118656
Iteration 7/25 | Loss: 0.00118656
Iteration 8/25 | Loss: 0.00118656
Iteration 9/25 | Loss: 0.00118656
Iteration 10/25 | Loss: 0.00118656
Iteration 11/25 | Loss: 0.00118656
Iteration 12/25 | Loss: 0.00118656
Iteration 13/25 | Loss: 0.00118656
Iteration 14/25 | Loss: 0.00118656
Iteration 15/25 | Loss: 0.00118656
Iteration 16/25 | Loss: 0.00118656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011865601409226656, 0.0011865601409226656, 0.0011865601409226656, 0.0011865601409226656, 0.0011865601409226656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011865601409226656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31788683
Iteration 2/25 | Loss: 0.00118994
Iteration 3/25 | Loss: 0.00118994
Iteration 4/25 | Loss: 0.00118994
Iteration 5/25 | Loss: 0.00118994
Iteration 6/25 | Loss: 0.00118994
Iteration 7/25 | Loss: 0.00118994
Iteration 8/25 | Loss: 0.00118994
Iteration 9/25 | Loss: 0.00118994
Iteration 10/25 | Loss: 0.00118994
Iteration 11/25 | Loss: 0.00118994
Iteration 12/25 | Loss: 0.00118994
Iteration 13/25 | Loss: 0.00118994
Iteration 14/25 | Loss: 0.00118994
Iteration 15/25 | Loss: 0.00118994
Iteration 16/25 | Loss: 0.00118994
Iteration 17/25 | Loss: 0.00118994
Iteration 18/25 | Loss: 0.00118994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011899401433765888, 0.0011899401433765888, 0.0011899401433765888, 0.0011899401433765888, 0.0011899401433765888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011899401433765888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118994
Iteration 2/1000 | Loss: 0.00002581
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001379
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001221
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001214
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001205
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001184
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001183
Iteration 116/1000 | Loss: 0.00001183
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001183
Iteration 121/1000 | Loss: 0.00001183
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001182
Iteration 126/1000 | Loss: 0.00001182
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001181
Iteration 149/1000 | Loss: 0.00001181
Iteration 150/1000 | Loss: 0.00001181
Iteration 151/1000 | Loss: 0.00001181
Iteration 152/1000 | Loss: 0.00001181
Iteration 153/1000 | Loss: 0.00001181
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001181
Iteration 156/1000 | Loss: 0.00001181
Iteration 157/1000 | Loss: 0.00001181
Iteration 158/1000 | Loss: 0.00001181
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001181
Iteration 162/1000 | Loss: 0.00001181
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.18070838652784e-05, 1.18070838652784e-05, 1.18070838652784e-05, 1.18070838652784e-05, 1.18070838652784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.18070838652784e-05

Optimization complete. Final v2v error: 2.9859604835510254 mm

Highest mean error: 3.224538803100586 mm for frame 104

Lowest mean error: 2.8221969604492188 mm for frame 157

Saving results

Total time: 39.20698618888855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034571
Iteration 2/25 | Loss: 0.01034571
Iteration 3/25 | Loss: 0.01034571
Iteration 4/25 | Loss: 0.01034571
Iteration 5/25 | Loss: 0.01034571
Iteration 6/25 | Loss: 0.01034571
Iteration 7/25 | Loss: 0.01034571
Iteration 8/25 | Loss: 0.01034570
Iteration 9/25 | Loss: 0.01034570
Iteration 10/25 | Loss: 0.01034570
Iteration 11/25 | Loss: 0.01034570
Iteration 12/25 | Loss: 0.01034570
Iteration 13/25 | Loss: 0.01034570
Iteration 14/25 | Loss: 0.01034570
Iteration 15/25 | Loss: 0.01034570
Iteration 16/25 | Loss: 0.01034569
Iteration 17/25 | Loss: 0.01034569
Iteration 18/25 | Loss: 0.01034569
Iteration 19/25 | Loss: 0.01034569
Iteration 20/25 | Loss: 0.01034569
Iteration 21/25 | Loss: 0.01034569
Iteration 22/25 | Loss: 0.01034569
Iteration 23/25 | Loss: 0.01034569
Iteration 24/25 | Loss: 0.01034569
Iteration 25/25 | Loss: 0.01034569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65444815
Iteration 2/25 | Loss: 0.08661458
Iteration 3/25 | Loss: 0.08653940
Iteration 4/25 | Loss: 0.08653940
Iteration 5/25 | Loss: 0.08653939
Iteration 6/25 | Loss: 0.08653939
Iteration 7/25 | Loss: 0.08653938
Iteration 8/25 | Loss: 0.08653937
Iteration 9/25 | Loss: 0.08653937
Iteration 10/25 | Loss: 0.08653938
Iteration 11/25 | Loss: 0.08653939
Iteration 12/25 | Loss: 0.08653939
Iteration 13/25 | Loss: 0.08653939
Iteration 14/25 | Loss: 0.08653939
Iteration 15/25 | Loss: 0.08653938
Iteration 16/25 | Loss: 0.08653938
Iteration 17/25 | Loss: 0.08653937
Iteration 18/25 | Loss: 0.08653937
Iteration 19/25 | Loss: 0.08653937
Iteration 20/25 | Loss: 0.08653937
Iteration 21/25 | Loss: 0.08653937
Iteration 22/25 | Loss: 0.08653937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0865393653512001, 0.0865393653512001, 0.0865393653512001, 0.0865393653512001, 0.0865393653512001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0865393653512001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08653937
Iteration 2/1000 | Loss: 0.00659541
Iteration 3/1000 | Loss: 0.00328741
Iteration 4/1000 | Loss: 0.00109040
Iteration 5/1000 | Loss: 0.00212550
Iteration 6/1000 | Loss: 0.00145789
Iteration 7/1000 | Loss: 0.00039927
Iteration 8/1000 | Loss: 0.00060870
Iteration 9/1000 | Loss: 0.00055759
Iteration 10/1000 | Loss: 0.00042143
Iteration 11/1000 | Loss: 0.00019153
Iteration 12/1000 | Loss: 0.00040968
Iteration 13/1000 | Loss: 0.00030845
Iteration 14/1000 | Loss: 0.00066225
Iteration 15/1000 | Loss: 0.00141456
Iteration 16/1000 | Loss: 0.00063548
Iteration 17/1000 | Loss: 0.00432111
Iteration 18/1000 | Loss: 0.00234184
Iteration 19/1000 | Loss: 0.00031077
Iteration 20/1000 | Loss: 0.00065297
Iteration 21/1000 | Loss: 0.00027142
Iteration 22/1000 | Loss: 0.00149846
Iteration 23/1000 | Loss: 0.00010664
Iteration 24/1000 | Loss: 0.00005803
Iteration 25/1000 | Loss: 0.00004785
Iteration 26/1000 | Loss: 0.00004490
Iteration 27/1000 | Loss: 0.00094968
Iteration 28/1000 | Loss: 0.00029865
Iteration 29/1000 | Loss: 0.00006929
Iteration 30/1000 | Loss: 0.00034917
Iteration 31/1000 | Loss: 0.00007256
Iteration 32/1000 | Loss: 0.00009330
Iteration 33/1000 | Loss: 0.00006765
Iteration 34/1000 | Loss: 0.00020602
Iteration 35/1000 | Loss: 0.00003885
Iteration 36/1000 | Loss: 0.00025738
Iteration 37/1000 | Loss: 0.00003705
Iteration 38/1000 | Loss: 0.00023030
Iteration 39/1000 | Loss: 0.00017448
Iteration 40/1000 | Loss: 0.00004963
Iteration 41/1000 | Loss: 0.00004489
Iteration 42/1000 | Loss: 0.00005399
Iteration 43/1000 | Loss: 0.00003288
Iteration 44/1000 | Loss: 0.00057265
Iteration 45/1000 | Loss: 0.00029232
Iteration 46/1000 | Loss: 0.00013862
Iteration 47/1000 | Loss: 0.00003400
Iteration 48/1000 | Loss: 0.00028605
Iteration 49/1000 | Loss: 0.00003220
Iteration 50/1000 | Loss: 0.00003783
Iteration 51/1000 | Loss: 0.00003425
Iteration 52/1000 | Loss: 0.00025266
Iteration 53/1000 | Loss: 0.00007412
Iteration 54/1000 | Loss: 0.00004897
Iteration 55/1000 | Loss: 0.00003795
Iteration 56/1000 | Loss: 0.00004095
Iteration 57/1000 | Loss: 0.00004296
Iteration 58/1000 | Loss: 0.00011991
Iteration 59/1000 | Loss: 0.00032254
Iteration 60/1000 | Loss: 0.00005911
Iteration 61/1000 | Loss: 0.00004153
Iteration 62/1000 | Loss: 0.00004042
Iteration 63/1000 | Loss: 0.00003884
Iteration 64/1000 | Loss: 0.00016431
Iteration 65/1000 | Loss: 0.00004355
Iteration 66/1000 | Loss: 0.00003378
Iteration 67/1000 | Loss: 0.00035293
Iteration 68/1000 | Loss: 0.00006122
Iteration 69/1000 | Loss: 0.00005105
Iteration 70/1000 | Loss: 0.00003685
Iteration 71/1000 | Loss: 0.00003348
Iteration 72/1000 | Loss: 0.00007698
Iteration 73/1000 | Loss: 0.00004597
Iteration 74/1000 | Loss: 0.00003416
Iteration 75/1000 | Loss: 0.00003203
Iteration 76/1000 | Loss: 0.00003417
Iteration 77/1000 | Loss: 0.00003207
Iteration 78/1000 | Loss: 0.00003119
Iteration 79/1000 | Loss: 0.00003082
Iteration 80/1000 | Loss: 0.00003044
Iteration 81/1000 | Loss: 0.00003022
Iteration 82/1000 | Loss: 0.00003018
Iteration 83/1000 | Loss: 0.00003010
Iteration 84/1000 | Loss: 0.00002989
Iteration 85/1000 | Loss: 0.00010472
Iteration 86/1000 | Loss: 0.00003835
Iteration 87/1000 | Loss: 0.00002967
Iteration 88/1000 | Loss: 0.00003662
Iteration 89/1000 | Loss: 0.00002958
Iteration 90/1000 | Loss: 0.00004165
Iteration 91/1000 | Loss: 0.00002947
Iteration 92/1000 | Loss: 0.00002928
Iteration 93/1000 | Loss: 0.00002908
Iteration 94/1000 | Loss: 0.00002893
Iteration 95/1000 | Loss: 0.00002889
Iteration 96/1000 | Loss: 0.00002885
Iteration 97/1000 | Loss: 0.00002884
Iteration 98/1000 | Loss: 0.00002884
Iteration 99/1000 | Loss: 0.00002879
Iteration 100/1000 | Loss: 0.00002879
Iteration 101/1000 | Loss: 0.00013985
Iteration 102/1000 | Loss: 0.00002974
Iteration 103/1000 | Loss: 0.00002875
Iteration 104/1000 | Loss: 0.00002867
Iteration 105/1000 | Loss: 0.00002867
Iteration 106/1000 | Loss: 0.00002867
Iteration 107/1000 | Loss: 0.00002867
Iteration 108/1000 | Loss: 0.00002866
Iteration 109/1000 | Loss: 0.00002866
Iteration 110/1000 | Loss: 0.00002866
Iteration 111/1000 | Loss: 0.00002866
Iteration 112/1000 | Loss: 0.00002866
Iteration 113/1000 | Loss: 0.00002866
Iteration 114/1000 | Loss: 0.00002866
Iteration 115/1000 | Loss: 0.00002866
Iteration 116/1000 | Loss: 0.00002866
Iteration 117/1000 | Loss: 0.00002866
Iteration 118/1000 | Loss: 0.00002866
Iteration 119/1000 | Loss: 0.00002866
Iteration 120/1000 | Loss: 0.00002866
Iteration 121/1000 | Loss: 0.00002865
Iteration 122/1000 | Loss: 0.00002865
Iteration 123/1000 | Loss: 0.00002865
Iteration 124/1000 | Loss: 0.00002865
Iteration 125/1000 | Loss: 0.00002864
Iteration 126/1000 | Loss: 0.00002864
Iteration 127/1000 | Loss: 0.00002863
Iteration 128/1000 | Loss: 0.00002863
Iteration 129/1000 | Loss: 0.00002863
Iteration 130/1000 | Loss: 0.00002863
Iteration 131/1000 | Loss: 0.00002863
Iteration 132/1000 | Loss: 0.00002863
Iteration 133/1000 | Loss: 0.00002863
Iteration 134/1000 | Loss: 0.00002862
Iteration 135/1000 | Loss: 0.00002862
Iteration 136/1000 | Loss: 0.00002862
Iteration 137/1000 | Loss: 0.00002862
Iteration 138/1000 | Loss: 0.00002862
Iteration 139/1000 | Loss: 0.00002862
Iteration 140/1000 | Loss: 0.00002861
Iteration 141/1000 | Loss: 0.00002861
Iteration 142/1000 | Loss: 0.00002861
Iteration 143/1000 | Loss: 0.00002861
Iteration 144/1000 | Loss: 0.00002860
Iteration 145/1000 | Loss: 0.00002860
Iteration 146/1000 | Loss: 0.00002860
Iteration 147/1000 | Loss: 0.00002860
Iteration 148/1000 | Loss: 0.00002859
Iteration 149/1000 | Loss: 0.00002859
Iteration 150/1000 | Loss: 0.00002859
Iteration 151/1000 | Loss: 0.00002859
Iteration 152/1000 | Loss: 0.00002859
Iteration 153/1000 | Loss: 0.00002859
Iteration 154/1000 | Loss: 0.00002859
Iteration 155/1000 | Loss: 0.00002858
Iteration 156/1000 | Loss: 0.00002858
Iteration 157/1000 | Loss: 0.00002858
Iteration 158/1000 | Loss: 0.00002858
Iteration 159/1000 | Loss: 0.00002858
Iteration 160/1000 | Loss: 0.00002858
Iteration 161/1000 | Loss: 0.00002858
Iteration 162/1000 | Loss: 0.00002858
Iteration 163/1000 | Loss: 0.00002858
Iteration 164/1000 | Loss: 0.00002858
Iteration 165/1000 | Loss: 0.00002858
Iteration 166/1000 | Loss: 0.00002858
Iteration 167/1000 | Loss: 0.00002858
Iteration 168/1000 | Loss: 0.00002858
Iteration 169/1000 | Loss: 0.00002858
Iteration 170/1000 | Loss: 0.00002858
Iteration 171/1000 | Loss: 0.00002857
Iteration 172/1000 | Loss: 0.00002857
Iteration 173/1000 | Loss: 0.00002857
Iteration 174/1000 | Loss: 0.00002857
Iteration 175/1000 | Loss: 0.00002856
Iteration 176/1000 | Loss: 0.00002856
Iteration 177/1000 | Loss: 0.00002856
Iteration 178/1000 | Loss: 0.00002856
Iteration 179/1000 | Loss: 0.00002856
Iteration 180/1000 | Loss: 0.00002856
Iteration 181/1000 | Loss: 0.00002856
Iteration 182/1000 | Loss: 0.00002856
Iteration 183/1000 | Loss: 0.00002856
Iteration 184/1000 | Loss: 0.00002856
Iteration 185/1000 | Loss: 0.00002855
Iteration 186/1000 | Loss: 0.00002855
Iteration 187/1000 | Loss: 0.00011807
Iteration 188/1000 | Loss: 0.00002941
Iteration 189/1000 | Loss: 0.00006760
Iteration 190/1000 | Loss: 0.00002867
Iteration 191/1000 | Loss: 0.00002857
Iteration 192/1000 | Loss: 0.00002853
Iteration 193/1000 | Loss: 0.00002853
Iteration 194/1000 | Loss: 0.00002853
Iteration 195/1000 | Loss: 0.00002853
Iteration 196/1000 | Loss: 0.00002853
Iteration 197/1000 | Loss: 0.00002853
Iteration 198/1000 | Loss: 0.00002853
Iteration 199/1000 | Loss: 0.00002853
Iteration 200/1000 | Loss: 0.00002853
Iteration 201/1000 | Loss: 0.00002852
Iteration 202/1000 | Loss: 0.00002852
Iteration 203/1000 | Loss: 0.00002851
Iteration 204/1000 | Loss: 0.00002851
Iteration 205/1000 | Loss: 0.00002851
Iteration 206/1000 | Loss: 0.00002851
Iteration 207/1000 | Loss: 0.00002850
Iteration 208/1000 | Loss: 0.00002850
Iteration 209/1000 | Loss: 0.00002850
Iteration 210/1000 | Loss: 0.00002850
Iteration 211/1000 | Loss: 0.00002850
Iteration 212/1000 | Loss: 0.00002850
Iteration 213/1000 | Loss: 0.00002849
Iteration 214/1000 | Loss: 0.00002849
Iteration 215/1000 | Loss: 0.00002849
Iteration 216/1000 | Loss: 0.00002849
Iteration 217/1000 | Loss: 0.00002849
Iteration 218/1000 | Loss: 0.00002848
Iteration 219/1000 | Loss: 0.00002848
Iteration 220/1000 | Loss: 0.00002848
Iteration 221/1000 | Loss: 0.00002848
Iteration 222/1000 | Loss: 0.00002848
Iteration 223/1000 | Loss: 0.00002847
Iteration 224/1000 | Loss: 0.00002847
Iteration 225/1000 | Loss: 0.00002847
Iteration 226/1000 | Loss: 0.00002846
Iteration 227/1000 | Loss: 0.00002846
Iteration 228/1000 | Loss: 0.00002846
Iteration 229/1000 | Loss: 0.00002846
Iteration 230/1000 | Loss: 0.00002846
Iteration 231/1000 | Loss: 0.00002846
Iteration 232/1000 | Loss: 0.00002845
Iteration 233/1000 | Loss: 0.00002845
Iteration 234/1000 | Loss: 0.00002845
Iteration 235/1000 | Loss: 0.00002845
Iteration 236/1000 | Loss: 0.00002845
Iteration 237/1000 | Loss: 0.00002845
Iteration 238/1000 | Loss: 0.00002845
Iteration 239/1000 | Loss: 0.00002845
Iteration 240/1000 | Loss: 0.00002845
Iteration 241/1000 | Loss: 0.00002845
Iteration 242/1000 | Loss: 0.00002845
Iteration 243/1000 | Loss: 0.00002845
Iteration 244/1000 | Loss: 0.00002845
Iteration 245/1000 | Loss: 0.00002845
Iteration 246/1000 | Loss: 0.00002845
Iteration 247/1000 | Loss: 0.00002845
Iteration 248/1000 | Loss: 0.00002845
Iteration 249/1000 | Loss: 0.00002845
Iteration 250/1000 | Loss: 0.00002845
Iteration 251/1000 | Loss: 0.00002845
Iteration 252/1000 | Loss: 0.00002845
Iteration 253/1000 | Loss: 0.00002845
Iteration 254/1000 | Loss: 0.00002845
Iteration 255/1000 | Loss: 0.00002845
Iteration 256/1000 | Loss: 0.00002845
Iteration 257/1000 | Loss: 0.00002845
Iteration 258/1000 | Loss: 0.00002845
Iteration 259/1000 | Loss: 0.00002845
Iteration 260/1000 | Loss: 0.00002845
Iteration 261/1000 | Loss: 0.00002845
Iteration 262/1000 | Loss: 0.00002845
Iteration 263/1000 | Loss: 0.00002845
Iteration 264/1000 | Loss: 0.00002845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.8445243515307084e-05, 2.8445243515307084e-05, 2.8445243515307084e-05, 2.8445243515307084e-05, 2.8445243515307084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8445243515307084e-05

Optimization complete. Final v2v error: 4.012607097625732 mm

Highest mean error: 6.553600311279297 mm for frame 39

Lowest mean error: 3.0714774131774902 mm for frame 90

Saving results

Total time: 175.20708465576172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768266
Iteration 2/25 | Loss: 0.00178674
Iteration 3/25 | Loss: 0.00139271
Iteration 4/25 | Loss: 0.00135590
Iteration 5/25 | Loss: 0.00135341
Iteration 6/25 | Loss: 0.00135330
Iteration 7/25 | Loss: 0.00135330
Iteration 8/25 | Loss: 0.00135330
Iteration 9/25 | Loss: 0.00135330
Iteration 10/25 | Loss: 0.00135330
Iteration 11/25 | Loss: 0.00135330
Iteration 12/25 | Loss: 0.00135330
Iteration 13/25 | Loss: 0.00135330
Iteration 14/25 | Loss: 0.00135330
Iteration 15/25 | Loss: 0.00135330
Iteration 16/25 | Loss: 0.00135330
Iteration 17/25 | Loss: 0.00135330
Iteration 18/25 | Loss: 0.00135330
Iteration 19/25 | Loss: 0.00135330
Iteration 20/25 | Loss: 0.00135330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001353297382593155, 0.001353297382593155, 0.001353297382593155, 0.001353297382593155, 0.001353297382593155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001353297382593155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30124366
Iteration 2/25 | Loss: 0.00102890
Iteration 3/25 | Loss: 0.00102888
Iteration 4/25 | Loss: 0.00102888
Iteration 5/25 | Loss: 0.00102888
Iteration 6/25 | Loss: 0.00102888
Iteration 7/25 | Loss: 0.00102888
Iteration 8/25 | Loss: 0.00102888
Iteration 9/25 | Loss: 0.00102888
Iteration 10/25 | Loss: 0.00102888
Iteration 11/25 | Loss: 0.00102888
Iteration 12/25 | Loss: 0.00102888
Iteration 13/25 | Loss: 0.00102888
Iteration 14/25 | Loss: 0.00102888
Iteration 15/25 | Loss: 0.00102888
Iteration 16/25 | Loss: 0.00102888
Iteration 17/25 | Loss: 0.00102888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010288760531693697, 0.0010288760531693697, 0.0010288760531693697, 0.0010288760531693697, 0.0010288760531693697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010288760531693697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102888
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002949
Iteration 4/1000 | Loss: 0.00002676
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002491
Iteration 7/1000 | Loss: 0.00002449
Iteration 8/1000 | Loss: 0.00002416
Iteration 9/1000 | Loss: 0.00002386
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002337
Iteration 12/1000 | Loss: 0.00002318
Iteration 13/1000 | Loss: 0.00002315
Iteration 14/1000 | Loss: 0.00002314
Iteration 15/1000 | Loss: 0.00002312
Iteration 16/1000 | Loss: 0.00002310
Iteration 17/1000 | Loss: 0.00002308
Iteration 18/1000 | Loss: 0.00002303
Iteration 19/1000 | Loss: 0.00002289
Iteration 20/1000 | Loss: 0.00002288
Iteration 21/1000 | Loss: 0.00002284
Iteration 22/1000 | Loss: 0.00002284
Iteration 23/1000 | Loss: 0.00002283
Iteration 24/1000 | Loss: 0.00002282
Iteration 25/1000 | Loss: 0.00002282
Iteration 26/1000 | Loss: 0.00002281
Iteration 27/1000 | Loss: 0.00002277
Iteration 28/1000 | Loss: 0.00002277
Iteration 29/1000 | Loss: 0.00002277
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002277
Iteration 32/1000 | Loss: 0.00002276
Iteration 33/1000 | Loss: 0.00002276
Iteration 34/1000 | Loss: 0.00002276
Iteration 35/1000 | Loss: 0.00002276
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00002276
Iteration 39/1000 | Loss: 0.00002275
Iteration 40/1000 | Loss: 0.00002275
Iteration 41/1000 | Loss: 0.00002275
Iteration 42/1000 | Loss: 0.00002275
Iteration 43/1000 | Loss: 0.00002275
Iteration 44/1000 | Loss: 0.00002274
Iteration 45/1000 | Loss: 0.00002274
Iteration 46/1000 | Loss: 0.00002274
Iteration 47/1000 | Loss: 0.00002274
Iteration 48/1000 | Loss: 0.00002274
Iteration 49/1000 | Loss: 0.00002274
Iteration 50/1000 | Loss: 0.00002274
Iteration 51/1000 | Loss: 0.00002274
Iteration 52/1000 | Loss: 0.00002274
Iteration 53/1000 | Loss: 0.00002274
Iteration 54/1000 | Loss: 0.00002274
Iteration 55/1000 | Loss: 0.00002274
Iteration 56/1000 | Loss: 0.00002274
Iteration 57/1000 | Loss: 0.00002274
Iteration 58/1000 | Loss: 0.00002274
Iteration 59/1000 | Loss: 0.00002274
Iteration 60/1000 | Loss: 0.00002274
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002273
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002273
Iteration 65/1000 | Loss: 0.00002273
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002273
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002272
Iteration 71/1000 | Loss: 0.00002271
Iteration 72/1000 | Loss: 0.00002271
Iteration 73/1000 | Loss: 0.00002271
Iteration 74/1000 | Loss: 0.00002271
Iteration 75/1000 | Loss: 0.00002271
Iteration 76/1000 | Loss: 0.00002271
Iteration 77/1000 | Loss: 0.00002271
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002271
Iteration 80/1000 | Loss: 0.00002271
Iteration 81/1000 | Loss: 0.00002270
Iteration 82/1000 | Loss: 0.00002270
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002270
Iteration 85/1000 | Loss: 0.00002270
Iteration 86/1000 | Loss: 0.00002270
Iteration 87/1000 | Loss: 0.00002270
Iteration 88/1000 | Loss: 0.00002270
Iteration 89/1000 | Loss: 0.00002270
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002269
Iteration 94/1000 | Loss: 0.00002269
Iteration 95/1000 | Loss: 0.00002269
Iteration 96/1000 | Loss: 0.00002269
Iteration 97/1000 | Loss: 0.00002269
Iteration 98/1000 | Loss: 0.00002269
Iteration 99/1000 | Loss: 0.00002269
Iteration 100/1000 | Loss: 0.00002269
Iteration 101/1000 | Loss: 0.00002269
Iteration 102/1000 | Loss: 0.00002269
Iteration 103/1000 | Loss: 0.00002268
Iteration 104/1000 | Loss: 0.00002268
Iteration 105/1000 | Loss: 0.00002268
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00002268
Iteration 108/1000 | Loss: 0.00002268
Iteration 109/1000 | Loss: 0.00002268
Iteration 110/1000 | Loss: 0.00002268
Iteration 111/1000 | Loss: 0.00002267
Iteration 112/1000 | Loss: 0.00002267
Iteration 113/1000 | Loss: 0.00002267
Iteration 114/1000 | Loss: 0.00002267
Iteration 115/1000 | Loss: 0.00002267
Iteration 116/1000 | Loss: 0.00002266
Iteration 117/1000 | Loss: 0.00002266
Iteration 118/1000 | Loss: 0.00002266
Iteration 119/1000 | Loss: 0.00002266
Iteration 120/1000 | Loss: 0.00002266
Iteration 121/1000 | Loss: 0.00002266
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002266
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002265
Iteration 129/1000 | Loss: 0.00002265
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002265
Iteration 132/1000 | Loss: 0.00002265
Iteration 133/1000 | Loss: 0.00002265
Iteration 134/1000 | Loss: 0.00002265
Iteration 135/1000 | Loss: 0.00002265
Iteration 136/1000 | Loss: 0.00002265
Iteration 137/1000 | Loss: 0.00002265
Iteration 138/1000 | Loss: 0.00002265
Iteration 139/1000 | Loss: 0.00002265
Iteration 140/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.264821159769781e-05, 2.264821159769781e-05, 2.264821159769781e-05, 2.264821159769781e-05, 2.264821159769781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.264821159769781e-05

Optimization complete. Final v2v error: 4.019017696380615 mm

Highest mean error: 4.198024272918701 mm for frame 54

Lowest mean error: 3.857414484024048 mm for frame 26

Saving results

Total time: 34.126911640167236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897997
Iteration 2/25 | Loss: 0.00143825
Iteration 3/25 | Loss: 0.00127589
Iteration 4/25 | Loss: 0.00125491
Iteration 5/25 | Loss: 0.00124796
Iteration 6/25 | Loss: 0.00124614
Iteration 7/25 | Loss: 0.00124604
Iteration 8/25 | Loss: 0.00124603
Iteration 9/25 | Loss: 0.00124604
Iteration 10/25 | Loss: 0.00124604
Iteration 11/25 | Loss: 0.00124604
Iteration 12/25 | Loss: 0.00124604
Iteration 13/25 | Loss: 0.00124604
Iteration 14/25 | Loss: 0.00124604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001246035099029541, 0.001246035099029541, 0.001246035099029541, 0.001246035099029541, 0.001246035099029541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001246035099029541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32873976
Iteration 2/25 | Loss: 0.00115213
Iteration 3/25 | Loss: 0.00115211
Iteration 4/25 | Loss: 0.00115211
Iteration 5/25 | Loss: 0.00115211
Iteration 6/25 | Loss: 0.00115211
Iteration 7/25 | Loss: 0.00115211
Iteration 8/25 | Loss: 0.00115211
Iteration 9/25 | Loss: 0.00115211
Iteration 10/25 | Loss: 0.00115211
Iteration 11/25 | Loss: 0.00115211
Iteration 12/25 | Loss: 0.00115211
Iteration 13/25 | Loss: 0.00115211
Iteration 14/25 | Loss: 0.00115211
Iteration 15/25 | Loss: 0.00115211
Iteration 16/25 | Loss: 0.00115211
Iteration 17/25 | Loss: 0.00115211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011521079577505589, 0.0011521079577505589, 0.0011521079577505589, 0.0011521079577505589, 0.0011521079577505589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011521079577505589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115211
Iteration 2/1000 | Loss: 0.00004725
Iteration 3/1000 | Loss: 0.00003258
Iteration 4/1000 | Loss: 0.00002637
Iteration 5/1000 | Loss: 0.00002467
Iteration 6/1000 | Loss: 0.00002381
Iteration 7/1000 | Loss: 0.00002288
Iteration 8/1000 | Loss: 0.00002233
Iteration 9/1000 | Loss: 0.00002169
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002058
Iteration 14/1000 | Loss: 0.00002055
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00002032
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002025
Iteration 23/1000 | Loss: 0.00002024
Iteration 24/1000 | Loss: 0.00002018
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002012
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002007
Iteration 32/1000 | Loss: 0.00002006
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00002006
Iteration 37/1000 | Loss: 0.00002004
Iteration 38/1000 | Loss: 0.00002003
Iteration 39/1000 | Loss: 0.00002003
Iteration 40/1000 | Loss: 0.00002002
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00002000
Iteration 43/1000 | Loss: 0.00002000
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001996
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001994
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001993
Iteration 55/1000 | Loss: 0.00001993
Iteration 56/1000 | Loss: 0.00001992
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001992
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001989
Iteration 66/1000 | Loss: 0.00001989
Iteration 67/1000 | Loss: 0.00001989
Iteration 68/1000 | Loss: 0.00001989
Iteration 69/1000 | Loss: 0.00001989
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001986
Iteration 76/1000 | Loss: 0.00001986
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001985
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001985
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001980
Iteration 108/1000 | Loss: 0.00001979
Iteration 109/1000 | Loss: 0.00001978
Iteration 110/1000 | Loss: 0.00001978
Iteration 111/1000 | Loss: 0.00001978
Iteration 112/1000 | Loss: 0.00001978
Iteration 113/1000 | Loss: 0.00001978
Iteration 114/1000 | Loss: 0.00001978
Iteration 115/1000 | Loss: 0.00001977
Iteration 116/1000 | Loss: 0.00001977
Iteration 117/1000 | Loss: 0.00001977
Iteration 118/1000 | Loss: 0.00001977
Iteration 119/1000 | Loss: 0.00001977
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001976
Iteration 124/1000 | Loss: 0.00001976
Iteration 125/1000 | Loss: 0.00001975
Iteration 126/1000 | Loss: 0.00001975
Iteration 127/1000 | Loss: 0.00001975
Iteration 128/1000 | Loss: 0.00001975
Iteration 129/1000 | Loss: 0.00001975
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001974
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00001974
Iteration 135/1000 | Loss: 0.00001974
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001973
Iteration 138/1000 | Loss: 0.00001973
Iteration 139/1000 | Loss: 0.00001973
Iteration 140/1000 | Loss: 0.00001972
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001971
Iteration 144/1000 | Loss: 0.00001971
Iteration 145/1000 | Loss: 0.00001971
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001970
Iteration 154/1000 | Loss: 0.00001970
Iteration 155/1000 | Loss: 0.00001970
Iteration 156/1000 | Loss: 0.00001970
Iteration 157/1000 | Loss: 0.00001970
Iteration 158/1000 | Loss: 0.00001970
Iteration 159/1000 | Loss: 0.00001970
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001969
Iteration 162/1000 | Loss: 0.00001969
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Iteration 167/1000 | Loss: 0.00001968
Iteration 168/1000 | Loss: 0.00001968
Iteration 169/1000 | Loss: 0.00001968
Iteration 170/1000 | Loss: 0.00001968
Iteration 171/1000 | Loss: 0.00001968
Iteration 172/1000 | Loss: 0.00001968
Iteration 173/1000 | Loss: 0.00001968
Iteration 174/1000 | Loss: 0.00001968
Iteration 175/1000 | Loss: 0.00001968
Iteration 176/1000 | Loss: 0.00001968
Iteration 177/1000 | Loss: 0.00001968
Iteration 178/1000 | Loss: 0.00001968
Iteration 179/1000 | Loss: 0.00001968
Iteration 180/1000 | Loss: 0.00001968
Iteration 181/1000 | Loss: 0.00001968
Iteration 182/1000 | Loss: 0.00001968
Iteration 183/1000 | Loss: 0.00001968
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001967
Iteration 186/1000 | Loss: 0.00001967
Iteration 187/1000 | Loss: 0.00001967
Iteration 188/1000 | Loss: 0.00001967
Iteration 189/1000 | Loss: 0.00001967
Iteration 190/1000 | Loss: 0.00001967
Iteration 191/1000 | Loss: 0.00001967
Iteration 192/1000 | Loss: 0.00001967
Iteration 193/1000 | Loss: 0.00001967
Iteration 194/1000 | Loss: 0.00001967
Iteration 195/1000 | Loss: 0.00001967
Iteration 196/1000 | Loss: 0.00001967
Iteration 197/1000 | Loss: 0.00001967
Iteration 198/1000 | Loss: 0.00001967
Iteration 199/1000 | Loss: 0.00001966
Iteration 200/1000 | Loss: 0.00001966
Iteration 201/1000 | Loss: 0.00001966
Iteration 202/1000 | Loss: 0.00001966
Iteration 203/1000 | Loss: 0.00001966
Iteration 204/1000 | Loss: 0.00001966
Iteration 205/1000 | Loss: 0.00001966
Iteration 206/1000 | Loss: 0.00001966
Iteration 207/1000 | Loss: 0.00001966
Iteration 208/1000 | Loss: 0.00001966
Iteration 209/1000 | Loss: 0.00001966
Iteration 210/1000 | Loss: 0.00001966
Iteration 211/1000 | Loss: 0.00001966
Iteration 212/1000 | Loss: 0.00001966
Iteration 213/1000 | Loss: 0.00001966
Iteration 214/1000 | Loss: 0.00001966
Iteration 215/1000 | Loss: 0.00001966
Iteration 216/1000 | Loss: 0.00001966
Iteration 217/1000 | Loss: 0.00001966
Iteration 218/1000 | Loss: 0.00001966
Iteration 219/1000 | Loss: 0.00001966
Iteration 220/1000 | Loss: 0.00001966
Iteration 221/1000 | Loss: 0.00001966
Iteration 222/1000 | Loss: 0.00001966
Iteration 223/1000 | Loss: 0.00001966
Iteration 224/1000 | Loss: 0.00001966
Iteration 225/1000 | Loss: 0.00001966
Iteration 226/1000 | Loss: 0.00001966
Iteration 227/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.966294803423807e-05, 1.966294803423807e-05, 1.966294803423807e-05, 1.966294803423807e-05, 1.966294803423807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.966294803423807e-05

Optimization complete. Final v2v error: 3.7312769889831543 mm

Highest mean error: 5.2305097579956055 mm for frame 68

Lowest mean error: 3.145190477371216 mm for frame 44

Saving results

Total time: 45.400161266326904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394436
Iteration 2/25 | Loss: 0.00125439
Iteration 3/25 | Loss: 0.00118054
Iteration 4/25 | Loss: 0.00117003
Iteration 5/25 | Loss: 0.00116646
Iteration 6/25 | Loss: 0.00116556
Iteration 7/25 | Loss: 0.00116556
Iteration 8/25 | Loss: 0.00116556
Iteration 9/25 | Loss: 0.00116556
Iteration 10/25 | Loss: 0.00116556
Iteration 11/25 | Loss: 0.00116556
Iteration 12/25 | Loss: 0.00116556
Iteration 13/25 | Loss: 0.00116556
Iteration 14/25 | Loss: 0.00116556
Iteration 15/25 | Loss: 0.00116556
Iteration 16/25 | Loss: 0.00116556
Iteration 17/25 | Loss: 0.00116556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011655560228973627, 0.0011655560228973627, 0.0011655560228973627, 0.0011655560228973627, 0.0011655560228973627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011655560228973627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54598665
Iteration 2/25 | Loss: 0.00130261
Iteration 3/25 | Loss: 0.00130260
Iteration 4/25 | Loss: 0.00130260
Iteration 5/25 | Loss: 0.00130260
Iteration 6/25 | Loss: 0.00130260
Iteration 7/25 | Loss: 0.00130260
Iteration 8/25 | Loss: 0.00130260
Iteration 9/25 | Loss: 0.00130260
Iteration 10/25 | Loss: 0.00130260
Iteration 11/25 | Loss: 0.00130260
Iteration 12/25 | Loss: 0.00130260
Iteration 13/25 | Loss: 0.00130260
Iteration 14/25 | Loss: 0.00130260
Iteration 15/25 | Loss: 0.00130260
Iteration 16/25 | Loss: 0.00130260
Iteration 17/25 | Loss: 0.00130260
Iteration 18/25 | Loss: 0.00130260
Iteration 19/25 | Loss: 0.00130260
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013026015367358923, 0.0013026015367358923, 0.0013026015367358923, 0.0013026015367358923, 0.0013026015367358923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013026015367358923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130260
Iteration 2/1000 | Loss: 0.00002316
Iteration 3/1000 | Loss: 0.00001578
Iteration 4/1000 | Loss: 0.00001255
Iteration 5/1000 | Loss: 0.00001175
Iteration 6/1000 | Loss: 0.00001124
Iteration 7/1000 | Loss: 0.00001087
Iteration 8/1000 | Loss: 0.00001046
Iteration 9/1000 | Loss: 0.00001023
Iteration 10/1000 | Loss: 0.00001000
Iteration 11/1000 | Loss: 0.00000975
Iteration 12/1000 | Loss: 0.00000974
Iteration 13/1000 | Loss: 0.00000972
Iteration 14/1000 | Loss: 0.00000964
Iteration 15/1000 | Loss: 0.00000961
Iteration 16/1000 | Loss: 0.00000951
Iteration 17/1000 | Loss: 0.00000943
Iteration 18/1000 | Loss: 0.00000937
Iteration 19/1000 | Loss: 0.00000937
Iteration 20/1000 | Loss: 0.00000937
Iteration 21/1000 | Loss: 0.00000932
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000926
Iteration 25/1000 | Loss: 0.00000926
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000925
Iteration 28/1000 | Loss: 0.00000921
Iteration 29/1000 | Loss: 0.00000921
Iteration 30/1000 | Loss: 0.00000920
Iteration 31/1000 | Loss: 0.00000920
Iteration 32/1000 | Loss: 0.00000919
Iteration 33/1000 | Loss: 0.00000918
Iteration 34/1000 | Loss: 0.00000918
Iteration 35/1000 | Loss: 0.00000918
Iteration 36/1000 | Loss: 0.00000918
Iteration 37/1000 | Loss: 0.00000918
Iteration 38/1000 | Loss: 0.00000918
Iteration 39/1000 | Loss: 0.00000918
Iteration 40/1000 | Loss: 0.00000917
Iteration 41/1000 | Loss: 0.00000917
Iteration 42/1000 | Loss: 0.00000917
Iteration 43/1000 | Loss: 0.00000917
Iteration 44/1000 | Loss: 0.00000917
Iteration 45/1000 | Loss: 0.00000916
Iteration 46/1000 | Loss: 0.00000916
Iteration 47/1000 | Loss: 0.00000916
Iteration 48/1000 | Loss: 0.00000916
Iteration 49/1000 | Loss: 0.00000916
Iteration 50/1000 | Loss: 0.00000916
Iteration 51/1000 | Loss: 0.00000916
Iteration 52/1000 | Loss: 0.00000916
Iteration 53/1000 | Loss: 0.00000916
Iteration 54/1000 | Loss: 0.00000915
Iteration 55/1000 | Loss: 0.00000915
Iteration 56/1000 | Loss: 0.00000914
Iteration 57/1000 | Loss: 0.00000914
Iteration 58/1000 | Loss: 0.00000914
Iteration 59/1000 | Loss: 0.00000914
Iteration 60/1000 | Loss: 0.00000913
Iteration 61/1000 | Loss: 0.00000913
Iteration 62/1000 | Loss: 0.00000913
Iteration 63/1000 | Loss: 0.00000913
Iteration 64/1000 | Loss: 0.00000913
Iteration 65/1000 | Loss: 0.00000913
Iteration 66/1000 | Loss: 0.00000913
Iteration 67/1000 | Loss: 0.00000913
Iteration 68/1000 | Loss: 0.00000913
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000911
Iteration 71/1000 | Loss: 0.00000909
Iteration 72/1000 | Loss: 0.00000909
Iteration 73/1000 | Loss: 0.00000909
Iteration 74/1000 | Loss: 0.00000909
Iteration 75/1000 | Loss: 0.00000909
Iteration 76/1000 | Loss: 0.00000909
Iteration 77/1000 | Loss: 0.00000909
Iteration 78/1000 | Loss: 0.00000908
Iteration 79/1000 | Loss: 0.00000908
Iteration 80/1000 | Loss: 0.00000908
Iteration 81/1000 | Loss: 0.00000908
Iteration 82/1000 | Loss: 0.00000908
Iteration 83/1000 | Loss: 0.00000908
Iteration 84/1000 | Loss: 0.00000905
Iteration 85/1000 | Loss: 0.00000905
Iteration 86/1000 | Loss: 0.00000904
Iteration 87/1000 | Loss: 0.00000904
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000903
Iteration 94/1000 | Loss: 0.00000903
Iteration 95/1000 | Loss: 0.00000903
Iteration 96/1000 | Loss: 0.00000903
Iteration 97/1000 | Loss: 0.00000903
Iteration 98/1000 | Loss: 0.00000903
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000902
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000901
Iteration 108/1000 | Loss: 0.00000900
Iteration 109/1000 | Loss: 0.00000900
Iteration 110/1000 | Loss: 0.00000900
Iteration 111/1000 | Loss: 0.00000900
Iteration 112/1000 | Loss: 0.00000900
Iteration 113/1000 | Loss: 0.00000900
Iteration 114/1000 | Loss: 0.00000900
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000899
Iteration 119/1000 | Loss: 0.00000899
Iteration 120/1000 | Loss: 0.00000899
Iteration 121/1000 | Loss: 0.00000899
Iteration 122/1000 | Loss: 0.00000899
Iteration 123/1000 | Loss: 0.00000899
Iteration 124/1000 | Loss: 0.00000899
Iteration 125/1000 | Loss: 0.00000899
Iteration 126/1000 | Loss: 0.00000899
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000898
Iteration 132/1000 | Loss: 0.00000898
Iteration 133/1000 | Loss: 0.00000897
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000897
Iteration 136/1000 | Loss: 0.00000897
Iteration 137/1000 | Loss: 0.00000897
Iteration 138/1000 | Loss: 0.00000897
Iteration 139/1000 | Loss: 0.00000897
Iteration 140/1000 | Loss: 0.00000897
Iteration 141/1000 | Loss: 0.00000897
Iteration 142/1000 | Loss: 0.00000897
Iteration 143/1000 | Loss: 0.00000897
Iteration 144/1000 | Loss: 0.00000897
Iteration 145/1000 | Loss: 0.00000897
Iteration 146/1000 | Loss: 0.00000897
Iteration 147/1000 | Loss: 0.00000896
Iteration 148/1000 | Loss: 0.00000896
Iteration 149/1000 | Loss: 0.00000896
Iteration 150/1000 | Loss: 0.00000896
Iteration 151/1000 | Loss: 0.00000896
Iteration 152/1000 | Loss: 0.00000896
Iteration 153/1000 | Loss: 0.00000896
Iteration 154/1000 | Loss: 0.00000896
Iteration 155/1000 | Loss: 0.00000896
Iteration 156/1000 | Loss: 0.00000896
Iteration 157/1000 | Loss: 0.00000896
Iteration 158/1000 | Loss: 0.00000896
Iteration 159/1000 | Loss: 0.00000895
Iteration 160/1000 | Loss: 0.00000895
Iteration 161/1000 | Loss: 0.00000895
Iteration 162/1000 | Loss: 0.00000895
Iteration 163/1000 | Loss: 0.00000895
Iteration 164/1000 | Loss: 0.00000894
Iteration 165/1000 | Loss: 0.00000894
Iteration 166/1000 | Loss: 0.00000894
Iteration 167/1000 | Loss: 0.00000894
Iteration 168/1000 | Loss: 0.00000894
Iteration 169/1000 | Loss: 0.00000894
Iteration 170/1000 | Loss: 0.00000894
Iteration 171/1000 | Loss: 0.00000894
Iteration 172/1000 | Loss: 0.00000894
Iteration 173/1000 | Loss: 0.00000894
Iteration 174/1000 | Loss: 0.00000894
Iteration 175/1000 | Loss: 0.00000894
Iteration 176/1000 | Loss: 0.00000893
Iteration 177/1000 | Loss: 0.00000893
Iteration 178/1000 | Loss: 0.00000893
Iteration 179/1000 | Loss: 0.00000893
Iteration 180/1000 | Loss: 0.00000893
Iteration 181/1000 | Loss: 0.00000893
Iteration 182/1000 | Loss: 0.00000892
Iteration 183/1000 | Loss: 0.00000892
Iteration 184/1000 | Loss: 0.00000892
Iteration 185/1000 | Loss: 0.00000892
Iteration 186/1000 | Loss: 0.00000892
Iteration 187/1000 | Loss: 0.00000892
Iteration 188/1000 | Loss: 0.00000892
Iteration 189/1000 | Loss: 0.00000892
Iteration 190/1000 | Loss: 0.00000892
Iteration 191/1000 | Loss: 0.00000892
Iteration 192/1000 | Loss: 0.00000892
Iteration 193/1000 | Loss: 0.00000892
Iteration 194/1000 | Loss: 0.00000892
Iteration 195/1000 | Loss: 0.00000892
Iteration 196/1000 | Loss: 0.00000891
Iteration 197/1000 | Loss: 0.00000891
Iteration 198/1000 | Loss: 0.00000891
Iteration 199/1000 | Loss: 0.00000891
Iteration 200/1000 | Loss: 0.00000891
Iteration 201/1000 | Loss: 0.00000891
Iteration 202/1000 | Loss: 0.00000891
Iteration 203/1000 | Loss: 0.00000891
Iteration 204/1000 | Loss: 0.00000891
Iteration 205/1000 | Loss: 0.00000891
Iteration 206/1000 | Loss: 0.00000891
Iteration 207/1000 | Loss: 0.00000891
Iteration 208/1000 | Loss: 0.00000891
Iteration 209/1000 | Loss: 0.00000890
Iteration 210/1000 | Loss: 0.00000890
Iteration 211/1000 | Loss: 0.00000890
Iteration 212/1000 | Loss: 0.00000890
Iteration 213/1000 | Loss: 0.00000890
Iteration 214/1000 | Loss: 0.00000890
Iteration 215/1000 | Loss: 0.00000890
Iteration 216/1000 | Loss: 0.00000890
Iteration 217/1000 | Loss: 0.00000890
Iteration 218/1000 | Loss: 0.00000890
Iteration 219/1000 | Loss: 0.00000890
Iteration 220/1000 | Loss: 0.00000890
Iteration 221/1000 | Loss: 0.00000890
Iteration 222/1000 | Loss: 0.00000890
Iteration 223/1000 | Loss: 0.00000890
Iteration 224/1000 | Loss: 0.00000890
Iteration 225/1000 | Loss: 0.00000890
Iteration 226/1000 | Loss: 0.00000890
Iteration 227/1000 | Loss: 0.00000890
Iteration 228/1000 | Loss: 0.00000890
Iteration 229/1000 | Loss: 0.00000889
Iteration 230/1000 | Loss: 0.00000889
Iteration 231/1000 | Loss: 0.00000889
Iteration 232/1000 | Loss: 0.00000889
Iteration 233/1000 | Loss: 0.00000889
Iteration 234/1000 | Loss: 0.00000889
Iteration 235/1000 | Loss: 0.00000889
Iteration 236/1000 | Loss: 0.00000889
Iteration 237/1000 | Loss: 0.00000889
Iteration 238/1000 | Loss: 0.00000889
Iteration 239/1000 | Loss: 0.00000889
Iteration 240/1000 | Loss: 0.00000889
Iteration 241/1000 | Loss: 0.00000889
Iteration 242/1000 | Loss: 0.00000889
Iteration 243/1000 | Loss: 0.00000889
Iteration 244/1000 | Loss: 0.00000889
Iteration 245/1000 | Loss: 0.00000889
Iteration 246/1000 | Loss: 0.00000889
Iteration 247/1000 | Loss: 0.00000889
Iteration 248/1000 | Loss: 0.00000889
Iteration 249/1000 | Loss: 0.00000889
Iteration 250/1000 | Loss: 0.00000889
Iteration 251/1000 | Loss: 0.00000889
Iteration 252/1000 | Loss: 0.00000889
Iteration 253/1000 | Loss: 0.00000889
Iteration 254/1000 | Loss: 0.00000889
Iteration 255/1000 | Loss: 0.00000889
Iteration 256/1000 | Loss: 0.00000889
Iteration 257/1000 | Loss: 0.00000889
Iteration 258/1000 | Loss: 0.00000889
Iteration 259/1000 | Loss: 0.00000889
Iteration 260/1000 | Loss: 0.00000889
Iteration 261/1000 | Loss: 0.00000889
Iteration 262/1000 | Loss: 0.00000889
Iteration 263/1000 | Loss: 0.00000889
Iteration 264/1000 | Loss: 0.00000889
Iteration 265/1000 | Loss: 0.00000889
Iteration 266/1000 | Loss: 0.00000889
Iteration 267/1000 | Loss: 0.00000889
Iteration 268/1000 | Loss: 0.00000889
Iteration 269/1000 | Loss: 0.00000889
Iteration 270/1000 | Loss: 0.00000889
Iteration 271/1000 | Loss: 0.00000889
Iteration 272/1000 | Loss: 0.00000889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [8.885280294634867e-06, 8.885280294634867e-06, 8.885280294634867e-06, 8.885280294634867e-06, 8.885280294634867e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.885280294634867e-06

Optimization complete. Final v2v error: 2.5916225910186768 mm

Highest mean error: 2.8547463417053223 mm for frame 98

Lowest mean error: 2.38023042678833 mm for frame 13

Saving results

Total time: 43.719910860061646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642128
Iteration 2/25 | Loss: 0.00174333
Iteration 3/25 | Loss: 0.00142451
Iteration 4/25 | Loss: 0.00139784
Iteration 5/25 | Loss: 0.00139400
Iteration 6/25 | Loss: 0.00139395
Iteration 7/25 | Loss: 0.00139395
Iteration 8/25 | Loss: 0.00139395
Iteration 9/25 | Loss: 0.00139395
Iteration 10/25 | Loss: 0.00139395
Iteration 11/25 | Loss: 0.00139395
Iteration 12/25 | Loss: 0.00139395
Iteration 13/25 | Loss: 0.00139395
Iteration 14/25 | Loss: 0.00139395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001393947284668684, 0.001393947284668684, 0.001393947284668684, 0.001393947284668684, 0.001393947284668684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001393947284668684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36174321
Iteration 2/25 | Loss: 0.00121171
Iteration 3/25 | Loss: 0.00121169
Iteration 4/25 | Loss: 0.00121169
Iteration 5/25 | Loss: 0.00121169
Iteration 6/25 | Loss: 0.00121169
Iteration 7/25 | Loss: 0.00121168
Iteration 8/25 | Loss: 0.00121168
Iteration 9/25 | Loss: 0.00121168
Iteration 10/25 | Loss: 0.00121168
Iteration 11/25 | Loss: 0.00121168
Iteration 12/25 | Loss: 0.00121168
Iteration 13/25 | Loss: 0.00121168
Iteration 14/25 | Loss: 0.00121168
Iteration 15/25 | Loss: 0.00121168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012116843136027455, 0.0012116843136027455, 0.0012116843136027455, 0.0012116843136027455, 0.0012116843136027455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012116843136027455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121168
Iteration 2/1000 | Loss: 0.00005301
Iteration 3/1000 | Loss: 0.00003361
Iteration 4/1000 | Loss: 0.00002973
Iteration 5/1000 | Loss: 0.00002858
Iteration 6/1000 | Loss: 0.00002789
Iteration 7/1000 | Loss: 0.00002731
Iteration 8/1000 | Loss: 0.00002694
Iteration 9/1000 | Loss: 0.00002655
Iteration 10/1000 | Loss: 0.00002621
Iteration 11/1000 | Loss: 0.00002597
Iteration 12/1000 | Loss: 0.00002577
Iteration 13/1000 | Loss: 0.00002574
Iteration 14/1000 | Loss: 0.00002568
Iteration 15/1000 | Loss: 0.00002566
Iteration 16/1000 | Loss: 0.00002564
Iteration 17/1000 | Loss: 0.00002563
Iteration 18/1000 | Loss: 0.00002562
Iteration 19/1000 | Loss: 0.00002562
Iteration 20/1000 | Loss: 0.00002557
Iteration 21/1000 | Loss: 0.00002557
Iteration 22/1000 | Loss: 0.00002554
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002552
Iteration 25/1000 | Loss: 0.00002552
Iteration 26/1000 | Loss: 0.00002552
Iteration 27/1000 | Loss: 0.00002552
Iteration 28/1000 | Loss: 0.00002552
Iteration 29/1000 | Loss: 0.00002552
Iteration 30/1000 | Loss: 0.00002552
Iteration 31/1000 | Loss: 0.00002552
Iteration 32/1000 | Loss: 0.00002551
Iteration 33/1000 | Loss: 0.00002551
Iteration 34/1000 | Loss: 0.00002551
Iteration 35/1000 | Loss: 0.00002551
Iteration 36/1000 | Loss: 0.00002551
Iteration 37/1000 | Loss: 0.00002551
Iteration 38/1000 | Loss: 0.00002549
Iteration 39/1000 | Loss: 0.00002549
Iteration 40/1000 | Loss: 0.00002547
Iteration 41/1000 | Loss: 0.00002547
Iteration 42/1000 | Loss: 0.00002547
Iteration 43/1000 | Loss: 0.00002544
Iteration 44/1000 | Loss: 0.00002544
Iteration 45/1000 | Loss: 0.00002544
Iteration 46/1000 | Loss: 0.00002544
Iteration 47/1000 | Loss: 0.00002544
Iteration 48/1000 | Loss: 0.00002544
Iteration 49/1000 | Loss: 0.00002544
Iteration 50/1000 | Loss: 0.00002544
Iteration 51/1000 | Loss: 0.00002543
Iteration 52/1000 | Loss: 0.00002543
Iteration 53/1000 | Loss: 0.00002543
Iteration 54/1000 | Loss: 0.00002542
Iteration 55/1000 | Loss: 0.00002541
Iteration 56/1000 | Loss: 0.00002540
Iteration 57/1000 | Loss: 0.00002540
Iteration 58/1000 | Loss: 0.00002540
Iteration 59/1000 | Loss: 0.00002540
Iteration 60/1000 | Loss: 0.00002540
Iteration 61/1000 | Loss: 0.00002539
Iteration 62/1000 | Loss: 0.00002539
Iteration 63/1000 | Loss: 0.00002539
Iteration 64/1000 | Loss: 0.00002539
Iteration 65/1000 | Loss: 0.00002538
Iteration 66/1000 | Loss: 0.00002538
Iteration 67/1000 | Loss: 0.00002538
Iteration 68/1000 | Loss: 0.00002538
Iteration 69/1000 | Loss: 0.00002538
Iteration 70/1000 | Loss: 0.00002538
Iteration 71/1000 | Loss: 0.00002538
Iteration 72/1000 | Loss: 0.00002538
Iteration 73/1000 | Loss: 0.00002538
Iteration 74/1000 | Loss: 0.00002538
Iteration 75/1000 | Loss: 0.00002538
Iteration 76/1000 | Loss: 0.00002537
Iteration 77/1000 | Loss: 0.00002537
Iteration 78/1000 | Loss: 0.00002537
Iteration 79/1000 | Loss: 0.00002537
Iteration 80/1000 | Loss: 0.00002537
Iteration 81/1000 | Loss: 0.00002537
Iteration 82/1000 | Loss: 0.00002537
Iteration 83/1000 | Loss: 0.00002537
Iteration 84/1000 | Loss: 0.00002536
Iteration 85/1000 | Loss: 0.00002536
Iteration 86/1000 | Loss: 0.00002536
Iteration 87/1000 | Loss: 0.00002536
Iteration 88/1000 | Loss: 0.00002536
Iteration 89/1000 | Loss: 0.00002536
Iteration 90/1000 | Loss: 0.00002536
Iteration 91/1000 | Loss: 0.00002536
Iteration 92/1000 | Loss: 0.00002536
Iteration 93/1000 | Loss: 0.00002536
Iteration 94/1000 | Loss: 0.00002536
Iteration 95/1000 | Loss: 0.00002536
Iteration 96/1000 | Loss: 0.00002536
Iteration 97/1000 | Loss: 0.00002536
Iteration 98/1000 | Loss: 0.00002536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.5361154257552698e-05, 2.5361154257552698e-05, 2.5361154257552698e-05, 2.5361154257552698e-05, 2.5361154257552698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5361154257552698e-05

Optimization complete. Final v2v error: 4.191259860992432 mm

Highest mean error: 4.631991863250732 mm for frame 63

Lowest mean error: 3.6961567401885986 mm for frame 175

Saving results

Total time: 32.84533715248108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806358
Iteration 2/25 | Loss: 0.00176309
Iteration 3/25 | Loss: 0.00143856
Iteration 4/25 | Loss: 0.00136597
Iteration 5/25 | Loss: 0.00135025
Iteration 6/25 | Loss: 0.00133810
Iteration 7/25 | Loss: 0.00136771
Iteration 8/25 | Loss: 0.00133382
Iteration 9/25 | Loss: 0.00134491
Iteration 10/25 | Loss: 0.00131903
Iteration 11/25 | Loss: 0.00131744
Iteration 12/25 | Loss: 0.00131707
Iteration 13/25 | Loss: 0.00131588
Iteration 14/25 | Loss: 0.00131525
Iteration 15/25 | Loss: 0.00131995
Iteration 16/25 | Loss: 0.00131973
Iteration 17/25 | Loss: 0.00132162
Iteration 18/25 | Loss: 0.00132012
Iteration 19/25 | Loss: 0.00131858
Iteration 20/25 | Loss: 0.00132111
Iteration 21/25 | Loss: 0.00131952
Iteration 22/25 | Loss: 0.00132048
Iteration 23/25 | Loss: 0.00132005
Iteration 24/25 | Loss: 0.00132045
Iteration 25/25 | Loss: 0.00132052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.42501545
Iteration 2/25 | Loss: 0.00209594
Iteration 3/25 | Loss: 0.00209566
Iteration 4/25 | Loss: 0.00209566
Iteration 5/25 | Loss: 0.00209566
Iteration 6/25 | Loss: 0.00209566
Iteration 7/25 | Loss: 0.00209566
Iteration 8/25 | Loss: 0.00209566
Iteration 9/25 | Loss: 0.00209566
Iteration 10/25 | Loss: 0.00209565
Iteration 11/25 | Loss: 0.00209565
Iteration 12/25 | Loss: 0.00209565
Iteration 13/25 | Loss: 0.00209565
Iteration 14/25 | Loss: 0.00209565
Iteration 15/25 | Loss: 0.00209565
Iteration 16/25 | Loss: 0.00209565
Iteration 17/25 | Loss: 0.00209565
Iteration 18/25 | Loss: 0.00209565
Iteration 19/25 | Loss: 0.00209565
Iteration 20/25 | Loss: 0.00209565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002095653908327222, 0.002095653908327222, 0.002095653908327222, 0.002095653908327222, 0.002095653908327222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002095653908327222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209565
Iteration 2/1000 | Loss: 0.00019189
Iteration 3/1000 | Loss: 0.00023645
Iteration 4/1000 | Loss: 0.00022804
Iteration 5/1000 | Loss: 0.00009522
Iteration 6/1000 | Loss: 0.00008484
Iteration 7/1000 | Loss: 0.00019515
Iteration 8/1000 | Loss: 0.00020593
Iteration 9/1000 | Loss: 0.00017025
Iteration 10/1000 | Loss: 0.00025084
Iteration 11/1000 | Loss: 0.00007598
Iteration 12/1000 | Loss: 0.00016751
Iteration 13/1000 | Loss: 0.00017319
Iteration 14/1000 | Loss: 0.00017509
Iteration 15/1000 | Loss: 0.00016112
Iteration 16/1000 | Loss: 0.00018199
Iteration 17/1000 | Loss: 0.00013272
Iteration 18/1000 | Loss: 0.00017389
Iteration 19/1000 | Loss: 0.00007857
Iteration 20/1000 | Loss: 0.00016703
Iteration 21/1000 | Loss: 0.00018237
Iteration 22/1000 | Loss: 0.00015872
Iteration 23/1000 | Loss: 0.00019746
Iteration 24/1000 | Loss: 0.00019429
Iteration 25/1000 | Loss: 0.00014536
Iteration 26/1000 | Loss: 0.00019221
Iteration 27/1000 | Loss: 0.00016322
Iteration 28/1000 | Loss: 0.00019182
Iteration 29/1000 | Loss: 0.00017867
Iteration 30/1000 | Loss: 0.00014401
Iteration 31/1000 | Loss: 0.00016746
Iteration 32/1000 | Loss: 0.00012861
Iteration 33/1000 | Loss: 0.00014204
Iteration 34/1000 | Loss: 0.00018866
Iteration 35/1000 | Loss: 0.00017727
Iteration 36/1000 | Loss: 0.00011988
Iteration 37/1000 | Loss: 0.00012535
Iteration 38/1000 | Loss: 0.00077735
Iteration 39/1000 | Loss: 0.00018502
Iteration 40/1000 | Loss: 0.00009816
Iteration 41/1000 | Loss: 0.00010532
Iteration 42/1000 | Loss: 0.00009385
Iteration 43/1000 | Loss: 0.00011412
Iteration 44/1000 | Loss: 0.00009990
Iteration 45/1000 | Loss: 0.00011676
Iteration 46/1000 | Loss: 0.00027903
Iteration 47/1000 | Loss: 0.00017400
Iteration 48/1000 | Loss: 0.00094856
Iteration 49/1000 | Loss: 0.00084632
Iteration 50/1000 | Loss: 0.00014827
Iteration 51/1000 | Loss: 0.00029141
Iteration 52/1000 | Loss: 0.00023994
Iteration 53/1000 | Loss: 0.00092950
Iteration 54/1000 | Loss: 0.00021668
Iteration 55/1000 | Loss: 0.00041325
Iteration 56/1000 | Loss: 0.00077925
Iteration 57/1000 | Loss: 0.00032217
Iteration 58/1000 | Loss: 0.00011482
Iteration 59/1000 | Loss: 0.00008648
Iteration 60/1000 | Loss: 0.00010473
Iteration 61/1000 | Loss: 0.00006608
Iteration 62/1000 | Loss: 0.00005998
Iteration 63/1000 | Loss: 0.00005858
Iteration 64/1000 | Loss: 0.00005740
Iteration 65/1000 | Loss: 0.00005654
Iteration 66/1000 | Loss: 0.00005581
Iteration 67/1000 | Loss: 0.00066983
Iteration 68/1000 | Loss: 0.00059646
Iteration 69/1000 | Loss: 0.00061095
Iteration 70/1000 | Loss: 0.00069781
Iteration 71/1000 | Loss: 0.00043079
Iteration 72/1000 | Loss: 0.00062843
Iteration 73/1000 | Loss: 0.00031298
Iteration 74/1000 | Loss: 0.00035849
Iteration 75/1000 | Loss: 0.00005071
Iteration 76/1000 | Loss: 0.00004825
Iteration 77/1000 | Loss: 0.00004623
Iteration 78/1000 | Loss: 0.00004524
Iteration 79/1000 | Loss: 0.00004430
Iteration 80/1000 | Loss: 0.00004360
Iteration 81/1000 | Loss: 0.00004310
Iteration 82/1000 | Loss: 0.00004268
Iteration 83/1000 | Loss: 0.00004240
Iteration 84/1000 | Loss: 0.00004226
Iteration 85/1000 | Loss: 0.00004219
Iteration 86/1000 | Loss: 0.00004211
Iteration 87/1000 | Loss: 0.00004201
Iteration 88/1000 | Loss: 0.00004196
Iteration 89/1000 | Loss: 0.00004193
Iteration 90/1000 | Loss: 0.00004193
Iteration 91/1000 | Loss: 0.00004183
Iteration 92/1000 | Loss: 0.00004178
Iteration 93/1000 | Loss: 0.00004171
Iteration 94/1000 | Loss: 0.00004168
Iteration 95/1000 | Loss: 0.00004168
Iteration 96/1000 | Loss: 0.00004167
Iteration 97/1000 | Loss: 0.00004167
Iteration 98/1000 | Loss: 0.00004167
Iteration 99/1000 | Loss: 0.00004167
Iteration 100/1000 | Loss: 0.00004166
Iteration 101/1000 | Loss: 0.00004166
Iteration 102/1000 | Loss: 0.00004166
Iteration 103/1000 | Loss: 0.00004165
Iteration 104/1000 | Loss: 0.00004165
Iteration 105/1000 | Loss: 0.00004163
Iteration 106/1000 | Loss: 0.00004161
Iteration 107/1000 | Loss: 0.00004160
Iteration 108/1000 | Loss: 0.00004158
Iteration 109/1000 | Loss: 0.00004158
Iteration 110/1000 | Loss: 0.00004158
Iteration 111/1000 | Loss: 0.00004157
Iteration 112/1000 | Loss: 0.00004157
Iteration 113/1000 | Loss: 0.00004155
Iteration 114/1000 | Loss: 0.00004154
Iteration 115/1000 | Loss: 0.00004154
Iteration 116/1000 | Loss: 0.00004153
Iteration 117/1000 | Loss: 0.00004153
Iteration 118/1000 | Loss: 0.00004153
Iteration 119/1000 | Loss: 0.00004153
Iteration 120/1000 | Loss: 0.00004153
Iteration 121/1000 | Loss: 0.00004153
Iteration 122/1000 | Loss: 0.00004152
Iteration 123/1000 | Loss: 0.00004152
Iteration 124/1000 | Loss: 0.00004152
Iteration 125/1000 | Loss: 0.00004152
Iteration 126/1000 | Loss: 0.00004151
Iteration 127/1000 | Loss: 0.00004151
Iteration 128/1000 | Loss: 0.00004151
Iteration 129/1000 | Loss: 0.00004149
Iteration 130/1000 | Loss: 0.00004148
Iteration 131/1000 | Loss: 0.00004148
Iteration 132/1000 | Loss: 0.00004148
Iteration 133/1000 | Loss: 0.00004145
Iteration 134/1000 | Loss: 0.00004144
Iteration 135/1000 | Loss: 0.00004144
Iteration 136/1000 | Loss: 0.00004144
Iteration 137/1000 | Loss: 0.00004143
Iteration 138/1000 | Loss: 0.00004143
Iteration 139/1000 | Loss: 0.00004142
Iteration 140/1000 | Loss: 0.00004141
Iteration 141/1000 | Loss: 0.00004141
Iteration 142/1000 | Loss: 0.00004141
Iteration 143/1000 | Loss: 0.00004141
Iteration 144/1000 | Loss: 0.00004141
Iteration 145/1000 | Loss: 0.00004140
Iteration 146/1000 | Loss: 0.00004140
Iteration 147/1000 | Loss: 0.00004140
Iteration 148/1000 | Loss: 0.00004140
Iteration 149/1000 | Loss: 0.00004140
Iteration 150/1000 | Loss: 0.00004139
Iteration 151/1000 | Loss: 0.00004139
Iteration 152/1000 | Loss: 0.00004138
Iteration 153/1000 | Loss: 0.00004138
Iteration 154/1000 | Loss: 0.00004138
Iteration 155/1000 | Loss: 0.00004138
Iteration 156/1000 | Loss: 0.00004137
Iteration 157/1000 | Loss: 0.00004137
Iteration 158/1000 | Loss: 0.00004137
Iteration 159/1000 | Loss: 0.00004136
Iteration 160/1000 | Loss: 0.00004135
Iteration 161/1000 | Loss: 0.00004134
Iteration 162/1000 | Loss: 0.00004134
Iteration 163/1000 | Loss: 0.00004134
Iteration 164/1000 | Loss: 0.00004134
Iteration 165/1000 | Loss: 0.00004133
Iteration 166/1000 | Loss: 0.00004133
Iteration 167/1000 | Loss: 0.00004133
Iteration 168/1000 | Loss: 0.00004132
Iteration 169/1000 | Loss: 0.00004132
Iteration 170/1000 | Loss: 0.00004132
Iteration 171/1000 | Loss: 0.00004131
Iteration 172/1000 | Loss: 0.00004131
Iteration 173/1000 | Loss: 0.00004131
Iteration 174/1000 | Loss: 0.00004131
Iteration 175/1000 | Loss: 0.00004131
Iteration 176/1000 | Loss: 0.00004131
Iteration 177/1000 | Loss: 0.00004130
Iteration 178/1000 | Loss: 0.00004130
Iteration 179/1000 | Loss: 0.00004130
Iteration 180/1000 | Loss: 0.00004129
Iteration 181/1000 | Loss: 0.00004129
Iteration 182/1000 | Loss: 0.00004128
Iteration 183/1000 | Loss: 0.00004127
Iteration 184/1000 | Loss: 0.00004127
Iteration 185/1000 | Loss: 0.00004126
Iteration 186/1000 | Loss: 0.00004126
Iteration 187/1000 | Loss: 0.00004125
Iteration 188/1000 | Loss: 0.00004125
Iteration 189/1000 | Loss: 0.00004124
Iteration 190/1000 | Loss: 0.00004124
Iteration 191/1000 | Loss: 0.00004124
Iteration 192/1000 | Loss: 0.00004124
Iteration 193/1000 | Loss: 0.00004123
Iteration 194/1000 | Loss: 0.00004123
Iteration 195/1000 | Loss: 0.00004123
Iteration 196/1000 | Loss: 0.00004123
Iteration 197/1000 | Loss: 0.00004123
Iteration 198/1000 | Loss: 0.00004123
Iteration 199/1000 | Loss: 0.00004123
Iteration 200/1000 | Loss: 0.00004123
Iteration 201/1000 | Loss: 0.00004122
Iteration 202/1000 | Loss: 0.00004122
Iteration 203/1000 | Loss: 0.00004120
Iteration 204/1000 | Loss: 0.00004120
Iteration 205/1000 | Loss: 0.00004119
Iteration 206/1000 | Loss: 0.00004119
Iteration 207/1000 | Loss: 0.00004118
Iteration 208/1000 | Loss: 0.00004118
Iteration 209/1000 | Loss: 0.00004117
Iteration 210/1000 | Loss: 0.00004115
Iteration 211/1000 | Loss: 0.00004115
Iteration 212/1000 | Loss: 0.00004115
Iteration 213/1000 | Loss: 0.00004115
Iteration 214/1000 | Loss: 0.00004115
Iteration 215/1000 | Loss: 0.00004115
Iteration 216/1000 | Loss: 0.00004115
Iteration 217/1000 | Loss: 0.00004114
Iteration 218/1000 | Loss: 0.00004113
Iteration 219/1000 | Loss: 0.00004112
Iteration 220/1000 | Loss: 0.00004111
Iteration 221/1000 | Loss: 0.00004111
Iteration 222/1000 | Loss: 0.00004111
Iteration 223/1000 | Loss: 0.00004110
Iteration 224/1000 | Loss: 0.00004109
Iteration 225/1000 | Loss: 0.00004109
Iteration 226/1000 | Loss: 0.00004109
Iteration 227/1000 | Loss: 0.00004108
Iteration 228/1000 | Loss: 0.00004108
Iteration 229/1000 | Loss: 0.00004108
Iteration 230/1000 | Loss: 0.00004107
Iteration 231/1000 | Loss: 0.00004107
Iteration 232/1000 | Loss: 0.00004106
Iteration 233/1000 | Loss: 0.00004106
Iteration 234/1000 | Loss: 0.00004105
Iteration 235/1000 | Loss: 0.00004103
Iteration 236/1000 | Loss: 0.00004103
Iteration 237/1000 | Loss: 0.00004103
Iteration 238/1000 | Loss: 0.00004103
Iteration 239/1000 | Loss: 0.00004103
Iteration 240/1000 | Loss: 0.00004102
Iteration 241/1000 | Loss: 0.00004102
Iteration 242/1000 | Loss: 0.00004102
Iteration 243/1000 | Loss: 0.00004100
Iteration 244/1000 | Loss: 0.00004100
Iteration 245/1000 | Loss: 0.00004099
Iteration 246/1000 | Loss: 0.00004099
Iteration 247/1000 | Loss: 0.00004099
Iteration 248/1000 | Loss: 0.00004099
Iteration 249/1000 | Loss: 0.00004098
Iteration 250/1000 | Loss: 0.00004098
Iteration 251/1000 | Loss: 0.00004098
Iteration 252/1000 | Loss: 0.00004097
Iteration 253/1000 | Loss: 0.00004097
Iteration 254/1000 | Loss: 0.00004096
Iteration 255/1000 | Loss: 0.00004095
Iteration 256/1000 | Loss: 0.00004095
Iteration 257/1000 | Loss: 0.00004094
Iteration 258/1000 | Loss: 0.00004094
Iteration 259/1000 | Loss: 0.00004093
Iteration 260/1000 | Loss: 0.00004093
Iteration 261/1000 | Loss: 0.00004091
Iteration 262/1000 | Loss: 0.00004088
Iteration 263/1000 | Loss: 0.00011768
Iteration 264/1000 | Loss: 0.00010947
Iteration 265/1000 | Loss: 0.00003812
Iteration 266/1000 | Loss: 0.00003711
Iteration 267/1000 | Loss: 0.00003615
Iteration 268/1000 | Loss: 0.00003556
Iteration 269/1000 | Loss: 0.00003520
Iteration 270/1000 | Loss: 0.00003491
Iteration 271/1000 | Loss: 0.00003465
Iteration 272/1000 | Loss: 0.00003449
Iteration 273/1000 | Loss: 0.00003447
Iteration 274/1000 | Loss: 0.00003441
Iteration 275/1000 | Loss: 0.00003441
Iteration 276/1000 | Loss: 0.00003440
Iteration 277/1000 | Loss: 0.00003439
Iteration 278/1000 | Loss: 0.00003439
Iteration 279/1000 | Loss: 0.00003438
Iteration 280/1000 | Loss: 0.00003436
Iteration 281/1000 | Loss: 0.00003434
Iteration 282/1000 | Loss: 0.00003433
Iteration 283/1000 | Loss: 0.00003432
Iteration 284/1000 | Loss: 0.00003432
Iteration 285/1000 | Loss: 0.00003427
Iteration 286/1000 | Loss: 0.00003426
Iteration 287/1000 | Loss: 0.00003424
Iteration 288/1000 | Loss: 0.00003424
Iteration 289/1000 | Loss: 0.00003424
Iteration 290/1000 | Loss: 0.00003423
Iteration 291/1000 | Loss: 0.00003423
Iteration 292/1000 | Loss: 0.00003423
Iteration 293/1000 | Loss: 0.00003423
Iteration 294/1000 | Loss: 0.00003423
Iteration 295/1000 | Loss: 0.00003423
Iteration 296/1000 | Loss: 0.00003423
Iteration 297/1000 | Loss: 0.00003423
Iteration 298/1000 | Loss: 0.00003422
Iteration 299/1000 | Loss: 0.00003422
Iteration 300/1000 | Loss: 0.00003420
Iteration 301/1000 | Loss: 0.00003419
Iteration 302/1000 | Loss: 0.00003419
Iteration 303/1000 | Loss: 0.00003418
Iteration 304/1000 | Loss: 0.00003418
Iteration 305/1000 | Loss: 0.00003417
Iteration 306/1000 | Loss: 0.00003417
Iteration 307/1000 | Loss: 0.00003417
Iteration 308/1000 | Loss: 0.00003417
Iteration 309/1000 | Loss: 0.00003416
Iteration 310/1000 | Loss: 0.00003416
Iteration 311/1000 | Loss: 0.00003416
Iteration 312/1000 | Loss: 0.00003414
Iteration 313/1000 | Loss: 0.00003414
Iteration 314/1000 | Loss: 0.00003414
Iteration 315/1000 | Loss: 0.00003413
Iteration 316/1000 | Loss: 0.00003411
Iteration 317/1000 | Loss: 0.00003411
Iteration 318/1000 | Loss: 0.00003411
Iteration 319/1000 | Loss: 0.00003410
Iteration 320/1000 | Loss: 0.00003410
Iteration 321/1000 | Loss: 0.00003408
Iteration 322/1000 | Loss: 0.00003408
Iteration 323/1000 | Loss: 0.00003408
Iteration 324/1000 | Loss: 0.00003408
Iteration 325/1000 | Loss: 0.00003408
Iteration 326/1000 | Loss: 0.00003407
Iteration 327/1000 | Loss: 0.00003407
Iteration 328/1000 | Loss: 0.00003407
Iteration 329/1000 | Loss: 0.00003406
Iteration 330/1000 | Loss: 0.00003406
Iteration 331/1000 | Loss: 0.00003405
Iteration 332/1000 | Loss: 0.00003405
Iteration 333/1000 | Loss: 0.00003405
Iteration 334/1000 | Loss: 0.00003405
Iteration 335/1000 | Loss: 0.00003404
Iteration 336/1000 | Loss: 0.00003404
Iteration 337/1000 | Loss: 0.00003404
Iteration 338/1000 | Loss: 0.00003404
Iteration 339/1000 | Loss: 0.00003403
Iteration 340/1000 | Loss: 0.00003403
Iteration 341/1000 | Loss: 0.00003403
Iteration 342/1000 | Loss: 0.00003402
Iteration 343/1000 | Loss: 0.00003402
Iteration 344/1000 | Loss: 0.00003402
Iteration 345/1000 | Loss: 0.00003402
Iteration 346/1000 | Loss: 0.00003402
Iteration 347/1000 | Loss: 0.00003402
Iteration 348/1000 | Loss: 0.00003401
Iteration 349/1000 | Loss: 0.00003401
Iteration 350/1000 | Loss: 0.00003401
Iteration 351/1000 | Loss: 0.00003401
Iteration 352/1000 | Loss: 0.00003401
Iteration 353/1000 | Loss: 0.00003400
Iteration 354/1000 | Loss: 0.00003400
Iteration 355/1000 | Loss: 0.00003400
Iteration 356/1000 | Loss: 0.00003399
Iteration 357/1000 | Loss: 0.00003399
Iteration 358/1000 | Loss: 0.00003399
Iteration 359/1000 | Loss: 0.00003399
Iteration 360/1000 | Loss: 0.00003399
Iteration 361/1000 | Loss: 0.00003398
Iteration 362/1000 | Loss: 0.00003398
Iteration 363/1000 | Loss: 0.00003398
Iteration 364/1000 | Loss: 0.00003398
Iteration 365/1000 | Loss: 0.00003398
Iteration 366/1000 | Loss: 0.00003398
Iteration 367/1000 | Loss: 0.00003397
Iteration 368/1000 | Loss: 0.00003397
Iteration 369/1000 | Loss: 0.00003397
Iteration 370/1000 | Loss: 0.00003397
Iteration 371/1000 | Loss: 0.00003397
Iteration 372/1000 | Loss: 0.00003397
Iteration 373/1000 | Loss: 0.00003397
Iteration 374/1000 | Loss: 0.00003396
Iteration 375/1000 | Loss: 0.00003396
Iteration 376/1000 | Loss: 0.00003396
Iteration 377/1000 | Loss: 0.00003396
Iteration 378/1000 | Loss: 0.00003396
Iteration 379/1000 | Loss: 0.00003396
Iteration 380/1000 | Loss: 0.00003396
Iteration 381/1000 | Loss: 0.00003396
Iteration 382/1000 | Loss: 0.00003396
Iteration 383/1000 | Loss: 0.00003396
Iteration 384/1000 | Loss: 0.00003396
Iteration 385/1000 | Loss: 0.00003395
Iteration 386/1000 | Loss: 0.00003395
Iteration 387/1000 | Loss: 0.00003395
Iteration 388/1000 | Loss: 0.00003395
Iteration 389/1000 | Loss: 0.00003395
Iteration 390/1000 | Loss: 0.00003394
Iteration 391/1000 | Loss: 0.00003394
Iteration 392/1000 | Loss: 0.00003394
Iteration 393/1000 | Loss: 0.00003394
Iteration 394/1000 | Loss: 0.00003393
Iteration 395/1000 | Loss: 0.00003393
Iteration 396/1000 | Loss: 0.00003393
Iteration 397/1000 | Loss: 0.00003393
Iteration 398/1000 | Loss: 0.00003393
Iteration 399/1000 | Loss: 0.00003393
Iteration 400/1000 | Loss: 0.00003392
Iteration 401/1000 | Loss: 0.00003392
Iteration 402/1000 | Loss: 0.00003392
Iteration 403/1000 | Loss: 0.00003392
Iteration 404/1000 | Loss: 0.00003392
Iteration 405/1000 | Loss: 0.00003392
Iteration 406/1000 | Loss: 0.00003392
Iteration 407/1000 | Loss: 0.00003392
Iteration 408/1000 | Loss: 0.00003391
Iteration 409/1000 | Loss: 0.00003391
Iteration 410/1000 | Loss: 0.00003391
Iteration 411/1000 | Loss: 0.00003391
Iteration 412/1000 | Loss: 0.00003391
Iteration 413/1000 | Loss: 0.00003391
Iteration 414/1000 | Loss: 0.00003391
Iteration 415/1000 | Loss: 0.00003391
Iteration 416/1000 | Loss: 0.00003391
Iteration 417/1000 | Loss: 0.00003390
Iteration 418/1000 | Loss: 0.00003390
Iteration 419/1000 | Loss: 0.00003390
Iteration 420/1000 | Loss: 0.00003390
Iteration 421/1000 | Loss: 0.00003390
Iteration 422/1000 | Loss: 0.00003390
Iteration 423/1000 | Loss: 0.00003390
Iteration 424/1000 | Loss: 0.00003390
Iteration 425/1000 | Loss: 0.00003390
Iteration 426/1000 | Loss: 0.00003390
Iteration 427/1000 | Loss: 0.00003390
Iteration 428/1000 | Loss: 0.00003390
Iteration 429/1000 | Loss: 0.00003390
Iteration 430/1000 | Loss: 0.00003390
Iteration 431/1000 | Loss: 0.00003390
Iteration 432/1000 | Loss: 0.00003390
Iteration 433/1000 | Loss: 0.00003390
Iteration 434/1000 | Loss: 0.00003390
Iteration 435/1000 | Loss: 0.00003390
Iteration 436/1000 | Loss: 0.00003390
Iteration 437/1000 | Loss: 0.00003390
Iteration 438/1000 | Loss: 0.00003390
Iteration 439/1000 | Loss: 0.00003390
Iteration 440/1000 | Loss: 0.00003390
Iteration 441/1000 | Loss: 0.00003390
Iteration 442/1000 | Loss: 0.00003390
Iteration 443/1000 | Loss: 0.00003390
Iteration 444/1000 | Loss: 0.00003390
Iteration 445/1000 | Loss: 0.00003390
Iteration 446/1000 | Loss: 0.00003390
Iteration 447/1000 | Loss: 0.00003390
Iteration 448/1000 | Loss: 0.00003390
Iteration 449/1000 | Loss: 0.00003390
Iteration 450/1000 | Loss: 0.00003390
Iteration 451/1000 | Loss: 0.00003390
Iteration 452/1000 | Loss: 0.00003390
Iteration 453/1000 | Loss: 0.00003390
Iteration 454/1000 | Loss: 0.00003390
Iteration 455/1000 | Loss: 0.00003390
Iteration 456/1000 | Loss: 0.00003390
Iteration 457/1000 | Loss: 0.00003390
Iteration 458/1000 | Loss: 0.00003390
Iteration 459/1000 | Loss: 0.00003390
Iteration 460/1000 | Loss: 0.00003390
Iteration 461/1000 | Loss: 0.00003390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 461. Stopping optimization.
Last 5 losses: [3.389743869774975e-05, 3.389743869774975e-05, 3.389743869774975e-05, 3.389743869774975e-05, 3.389743869774975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.389743869774975e-05

Optimization complete. Final v2v error: 4.2637619972229 mm

Highest mean error: 13.053969383239746 mm for frame 64

Lowest mean error: 2.774899482727051 mm for frame 103

Saving results

Total time: 209.09571766853333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994623
Iteration 2/25 | Loss: 0.00994623
Iteration 3/25 | Loss: 0.00994623
Iteration 4/25 | Loss: 0.00264019
Iteration 5/25 | Loss: 0.00190281
Iteration 6/25 | Loss: 0.00180040
Iteration 7/25 | Loss: 0.00177604
Iteration 8/25 | Loss: 0.00180887
Iteration 9/25 | Loss: 0.00171228
Iteration 10/25 | Loss: 0.00154367
Iteration 11/25 | Loss: 0.00146868
Iteration 12/25 | Loss: 0.00146450
Iteration 13/25 | Loss: 0.00151058
Iteration 14/25 | Loss: 0.00147118
Iteration 15/25 | Loss: 0.00139853
Iteration 16/25 | Loss: 0.00137600
Iteration 17/25 | Loss: 0.00136377
Iteration 18/25 | Loss: 0.00136202
Iteration 19/25 | Loss: 0.00136920
Iteration 20/25 | Loss: 0.00136734
Iteration 21/25 | Loss: 0.00135597
Iteration 22/25 | Loss: 0.00135313
Iteration 23/25 | Loss: 0.00134990
Iteration 24/25 | Loss: 0.00135159
Iteration 25/25 | Loss: 0.00134645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29464853
Iteration 2/25 | Loss: 0.00222203
Iteration 3/25 | Loss: 0.00217989
Iteration 4/25 | Loss: 0.00217989
Iteration 5/25 | Loss: 0.00217989
Iteration 6/25 | Loss: 0.00217989
Iteration 7/25 | Loss: 0.00217989
Iteration 8/25 | Loss: 0.00217988
Iteration 9/25 | Loss: 0.00217988
Iteration 10/25 | Loss: 0.00217988
Iteration 11/25 | Loss: 0.00217988
Iteration 12/25 | Loss: 0.00217988
Iteration 13/25 | Loss: 0.00217988
Iteration 14/25 | Loss: 0.00217988
Iteration 15/25 | Loss: 0.00217988
Iteration 16/25 | Loss: 0.00217988
Iteration 17/25 | Loss: 0.00217988
Iteration 18/25 | Loss: 0.00217988
Iteration 19/25 | Loss: 0.00217988
Iteration 20/25 | Loss: 0.00217988
Iteration 21/25 | Loss: 0.00217988
Iteration 22/25 | Loss: 0.00217988
Iteration 23/25 | Loss: 0.00217988
Iteration 24/25 | Loss: 0.00217988
Iteration 25/25 | Loss: 0.00217988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217988
Iteration 2/1000 | Loss: 0.00075301
Iteration 3/1000 | Loss: 0.00067071
Iteration 4/1000 | Loss: 0.00207975
Iteration 5/1000 | Loss: 0.00020488
Iteration 6/1000 | Loss: 0.00033053
Iteration 7/1000 | Loss: 0.00013488
Iteration 8/1000 | Loss: 0.00022076
Iteration 9/1000 | Loss: 0.00022628
Iteration 10/1000 | Loss: 0.00009050
Iteration 11/1000 | Loss: 0.00043883
Iteration 12/1000 | Loss: 0.00014101
Iteration 13/1000 | Loss: 0.00009275
Iteration 14/1000 | Loss: 0.00030114
Iteration 15/1000 | Loss: 0.00023279
Iteration 16/1000 | Loss: 0.00026196
Iteration 17/1000 | Loss: 0.00008449
Iteration 18/1000 | Loss: 0.00152221
Iteration 19/1000 | Loss: 0.00170786
Iteration 20/1000 | Loss: 0.00049255
Iteration 21/1000 | Loss: 0.00012300
Iteration 22/1000 | Loss: 0.00015036
Iteration 23/1000 | Loss: 0.00055721
Iteration 24/1000 | Loss: 0.00040068
Iteration 25/1000 | Loss: 0.00015814
Iteration 26/1000 | Loss: 0.00026244
Iteration 27/1000 | Loss: 0.00069675
Iteration 28/1000 | Loss: 0.00025853
Iteration 29/1000 | Loss: 0.00043301
Iteration 30/1000 | Loss: 0.00057427
Iteration 31/1000 | Loss: 0.00017938
Iteration 32/1000 | Loss: 0.00013276
Iteration 33/1000 | Loss: 0.00013219
Iteration 34/1000 | Loss: 0.00022397
Iteration 35/1000 | Loss: 0.00006098
Iteration 36/1000 | Loss: 0.00004982
Iteration 37/1000 | Loss: 0.00005168
Iteration 38/1000 | Loss: 0.00016144
Iteration 39/1000 | Loss: 0.00009082
Iteration 40/1000 | Loss: 0.00056752
Iteration 41/1000 | Loss: 0.00019442
Iteration 42/1000 | Loss: 0.00005139
Iteration 43/1000 | Loss: 0.00005043
Iteration 44/1000 | Loss: 0.00003383
Iteration 45/1000 | Loss: 0.00030705
Iteration 46/1000 | Loss: 0.00003796
Iteration 47/1000 | Loss: 0.00003270
Iteration 48/1000 | Loss: 0.00003108
Iteration 49/1000 | Loss: 0.00005101
Iteration 50/1000 | Loss: 0.00062684
Iteration 51/1000 | Loss: 0.00035846
Iteration 52/1000 | Loss: 0.00012237
Iteration 53/1000 | Loss: 0.00070863
Iteration 54/1000 | Loss: 0.00024876
Iteration 55/1000 | Loss: 0.00003161
Iteration 56/1000 | Loss: 0.00002992
Iteration 57/1000 | Loss: 0.00040784
Iteration 58/1000 | Loss: 0.00017491
Iteration 59/1000 | Loss: 0.00014212
Iteration 60/1000 | Loss: 0.00002861
Iteration 61/1000 | Loss: 0.00002643
Iteration 62/1000 | Loss: 0.00002529
Iteration 63/1000 | Loss: 0.00004139
Iteration 64/1000 | Loss: 0.00014703
Iteration 65/1000 | Loss: 0.00002409
Iteration 66/1000 | Loss: 0.00002380
Iteration 67/1000 | Loss: 0.00002357
Iteration 68/1000 | Loss: 0.00003449
Iteration 69/1000 | Loss: 0.00026543
Iteration 70/1000 | Loss: 0.00026109
Iteration 71/1000 | Loss: 0.00003472
Iteration 72/1000 | Loss: 0.00038594
Iteration 73/1000 | Loss: 0.00014662
Iteration 74/1000 | Loss: 0.00003056
Iteration 75/1000 | Loss: 0.00002878
Iteration 76/1000 | Loss: 0.00002593
Iteration 77/1000 | Loss: 0.00025407
Iteration 78/1000 | Loss: 0.00002886
Iteration 79/1000 | Loss: 0.00002625
Iteration 80/1000 | Loss: 0.00002551
Iteration 81/1000 | Loss: 0.00002471
Iteration 82/1000 | Loss: 0.00002532
Iteration 83/1000 | Loss: 0.00003223
Iteration 84/1000 | Loss: 0.00002484
Iteration 85/1000 | Loss: 0.00002315
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00004194
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002251
Iteration 90/1000 | Loss: 0.00002251
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002248
Iteration 94/1000 | Loss: 0.00002248
Iteration 95/1000 | Loss: 0.00002245
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002232
Iteration 98/1000 | Loss: 0.00002231
Iteration 99/1000 | Loss: 0.00002230
Iteration 100/1000 | Loss: 0.00002229
Iteration 101/1000 | Loss: 0.00002229
Iteration 102/1000 | Loss: 0.00002229
Iteration 103/1000 | Loss: 0.00002229
Iteration 104/1000 | Loss: 0.00002229
Iteration 105/1000 | Loss: 0.00002228
Iteration 106/1000 | Loss: 0.00002228
Iteration 107/1000 | Loss: 0.00002227
Iteration 108/1000 | Loss: 0.00002227
Iteration 109/1000 | Loss: 0.00002227
Iteration 110/1000 | Loss: 0.00002226
Iteration 111/1000 | Loss: 0.00002226
Iteration 112/1000 | Loss: 0.00002226
Iteration 113/1000 | Loss: 0.00002226
Iteration 114/1000 | Loss: 0.00002226
Iteration 115/1000 | Loss: 0.00002225
Iteration 116/1000 | Loss: 0.00002225
Iteration 117/1000 | Loss: 0.00002225
Iteration 118/1000 | Loss: 0.00002225
Iteration 119/1000 | Loss: 0.00002224
Iteration 120/1000 | Loss: 0.00002223
Iteration 121/1000 | Loss: 0.00002223
Iteration 122/1000 | Loss: 0.00002222
Iteration 123/1000 | Loss: 0.00002221
Iteration 124/1000 | Loss: 0.00002221
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Iteration 127/1000 | Loss: 0.00002220
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002219
Iteration 131/1000 | Loss: 0.00002219
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002219
Iteration 135/1000 | Loss: 0.00002219
Iteration 136/1000 | Loss: 0.00002218
Iteration 137/1000 | Loss: 0.00002218
Iteration 138/1000 | Loss: 0.00002217
Iteration 139/1000 | Loss: 0.00002217
Iteration 140/1000 | Loss: 0.00002217
Iteration 141/1000 | Loss: 0.00002217
Iteration 142/1000 | Loss: 0.00002217
Iteration 143/1000 | Loss: 0.00002217
Iteration 144/1000 | Loss: 0.00002217
Iteration 145/1000 | Loss: 0.00002217
Iteration 146/1000 | Loss: 0.00002217
Iteration 147/1000 | Loss: 0.00002217
Iteration 148/1000 | Loss: 0.00002216
Iteration 149/1000 | Loss: 0.00002216
Iteration 150/1000 | Loss: 0.00002216
Iteration 151/1000 | Loss: 0.00002215
Iteration 152/1000 | Loss: 0.00002215
Iteration 153/1000 | Loss: 0.00002215
Iteration 154/1000 | Loss: 0.00002215
Iteration 155/1000 | Loss: 0.00002215
Iteration 156/1000 | Loss: 0.00002215
Iteration 157/1000 | Loss: 0.00002215
Iteration 158/1000 | Loss: 0.00002215
Iteration 159/1000 | Loss: 0.00002214
Iteration 160/1000 | Loss: 0.00002214
Iteration 161/1000 | Loss: 0.00002214
Iteration 162/1000 | Loss: 0.00002214
Iteration 163/1000 | Loss: 0.00002214
Iteration 164/1000 | Loss: 0.00002214
Iteration 165/1000 | Loss: 0.00002214
Iteration 166/1000 | Loss: 0.00002214
Iteration 167/1000 | Loss: 0.00002214
Iteration 168/1000 | Loss: 0.00002214
Iteration 169/1000 | Loss: 0.00002214
Iteration 170/1000 | Loss: 0.00002213
Iteration 171/1000 | Loss: 0.00002213
Iteration 172/1000 | Loss: 0.00002213
Iteration 173/1000 | Loss: 0.00002213
Iteration 174/1000 | Loss: 0.00002213
Iteration 175/1000 | Loss: 0.00002213
Iteration 176/1000 | Loss: 0.00002213
Iteration 177/1000 | Loss: 0.00002212
Iteration 178/1000 | Loss: 0.00002212
Iteration 179/1000 | Loss: 0.00002212
Iteration 180/1000 | Loss: 0.00002212
Iteration 181/1000 | Loss: 0.00002212
Iteration 182/1000 | Loss: 0.00002212
Iteration 183/1000 | Loss: 0.00002212
Iteration 184/1000 | Loss: 0.00002212
Iteration 185/1000 | Loss: 0.00002212
Iteration 186/1000 | Loss: 0.00002212
Iteration 187/1000 | Loss: 0.00002212
Iteration 188/1000 | Loss: 0.00002212
Iteration 189/1000 | Loss: 0.00002212
Iteration 190/1000 | Loss: 0.00002212
Iteration 191/1000 | Loss: 0.00002212
Iteration 192/1000 | Loss: 0.00002212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.2118752895039506e-05, 2.2118752895039506e-05, 2.2118752895039506e-05, 2.2118752895039506e-05, 2.2118752895039506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2118752895039506e-05

Optimization complete. Final v2v error: 3.2759406566619873 mm

Highest mean error: 10.969167709350586 mm for frame 4

Lowest mean error: 2.85184383392334 mm for frame 173

Saving results

Total time: 196.6295108795166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007961
Iteration 2/25 | Loss: 0.00203053
Iteration 3/25 | Loss: 0.00165130
Iteration 4/25 | Loss: 0.00156414
Iteration 5/25 | Loss: 0.00153779
Iteration 6/25 | Loss: 0.00133680
Iteration 7/25 | Loss: 0.00128230
Iteration 8/25 | Loss: 0.00127661
Iteration 9/25 | Loss: 0.00129035
Iteration 10/25 | Loss: 0.00127638
Iteration 11/25 | Loss: 0.00125574
Iteration 12/25 | Loss: 0.00125225
Iteration 13/25 | Loss: 0.00125265
Iteration 14/25 | Loss: 0.00124562
Iteration 15/25 | Loss: 0.00124353
Iteration 16/25 | Loss: 0.00123692
Iteration 17/25 | Loss: 0.00123443
Iteration 18/25 | Loss: 0.00123334
Iteration 19/25 | Loss: 0.00123248
Iteration 20/25 | Loss: 0.00123738
Iteration 21/25 | Loss: 0.00123699
Iteration 22/25 | Loss: 0.00123635
Iteration 23/25 | Loss: 0.00123523
Iteration 24/25 | Loss: 0.00123243
Iteration 25/25 | Loss: 0.00123335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37346530
Iteration 2/25 | Loss: 0.00127783
Iteration 3/25 | Loss: 0.00127782
Iteration 4/25 | Loss: 0.00127782
Iteration 5/25 | Loss: 0.00127782
Iteration 6/25 | Loss: 0.00127782
Iteration 7/25 | Loss: 0.00127782
Iteration 8/25 | Loss: 0.00127782
Iteration 9/25 | Loss: 0.00127782
Iteration 10/25 | Loss: 0.00127782
Iteration 11/25 | Loss: 0.00127782
Iteration 12/25 | Loss: 0.00127782
Iteration 13/25 | Loss: 0.00127782
Iteration 14/25 | Loss: 0.00127782
Iteration 15/25 | Loss: 0.00127782
Iteration 16/25 | Loss: 0.00127782
Iteration 17/25 | Loss: 0.00127782
Iteration 18/25 | Loss: 0.00127782
Iteration 19/25 | Loss: 0.00127782
Iteration 20/25 | Loss: 0.00127782
Iteration 21/25 | Loss: 0.00127782
Iteration 22/25 | Loss: 0.00127782
Iteration 23/25 | Loss: 0.00127782
Iteration 24/25 | Loss: 0.00127782
Iteration 25/25 | Loss: 0.00127782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127782
Iteration 2/1000 | Loss: 0.00003227
Iteration 3/1000 | Loss: 0.00054846
Iteration 4/1000 | Loss: 0.00003883
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00001810
Iteration 7/1000 | Loss: 0.00001666
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00015553
Iteration 10/1000 | Loss: 0.00003710
Iteration 11/1000 | Loss: 0.00002011
Iteration 12/1000 | Loss: 0.00001594
Iteration 13/1000 | Loss: 0.00001479
Iteration 14/1000 | Loss: 0.00012449
Iteration 15/1000 | Loss: 0.00002011
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001419
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00002329
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001346
Iteration 24/1000 | Loss: 0.00017298
Iteration 25/1000 | Loss: 0.00002119
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00009337
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001259
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001253
Iteration 50/1000 | Loss: 0.00001253
Iteration 51/1000 | Loss: 0.00001253
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001244
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001240
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001237
Iteration 107/1000 | Loss: 0.00018084
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001229
Iteration 123/1000 | Loss: 0.00001229
Iteration 124/1000 | Loss: 0.00001229
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001228
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Iteration 135/1000 | Loss: 0.00001228
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001228
Iteration 138/1000 | Loss: 0.00001228
Iteration 139/1000 | Loss: 0.00001228
Iteration 140/1000 | Loss: 0.00001228
Iteration 141/1000 | Loss: 0.00001228
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2272939784452319e-05, 1.2272939784452319e-05, 1.2272939784452319e-05, 1.2272939784452319e-05, 1.2272939784452319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2272939784452319e-05

Optimization complete. Final v2v error: 2.9233200550079346 mm

Highest mean error: 5.645165920257568 mm for frame 31

Lowest mean error: 2.626082420349121 mm for frame 26

Saving results

Total time: 97.32195496559143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000942
Iteration 2/25 | Loss: 0.00223925
Iteration 3/25 | Loss: 0.00171384
Iteration 4/25 | Loss: 0.00185199
Iteration 5/25 | Loss: 0.00142764
Iteration 6/25 | Loss: 0.00136817
Iteration 7/25 | Loss: 0.00134585
Iteration 8/25 | Loss: 0.00132923
Iteration 9/25 | Loss: 0.00132545
Iteration 10/25 | Loss: 0.00133195
Iteration 11/25 | Loss: 0.00132324
Iteration 12/25 | Loss: 0.00131470
Iteration 13/25 | Loss: 0.00130945
Iteration 14/25 | Loss: 0.00130868
Iteration 15/25 | Loss: 0.00131287
Iteration 16/25 | Loss: 0.00130821
Iteration 17/25 | Loss: 0.00130733
Iteration 18/25 | Loss: 0.00130708
Iteration 19/25 | Loss: 0.00130707
Iteration 20/25 | Loss: 0.00130704
Iteration 21/25 | Loss: 0.00130704
Iteration 22/25 | Loss: 0.00130704
Iteration 23/25 | Loss: 0.00130704
Iteration 24/25 | Loss: 0.00130704
Iteration 25/25 | Loss: 0.00130704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27630973
Iteration 2/25 | Loss: 0.00117934
Iteration 3/25 | Loss: 0.00116182
Iteration 4/25 | Loss: 0.00116182
Iteration 5/25 | Loss: 0.00116182
Iteration 6/25 | Loss: 0.00116182
Iteration 7/25 | Loss: 0.00116182
Iteration 8/25 | Loss: 0.00116182
Iteration 9/25 | Loss: 0.00116182
Iteration 10/25 | Loss: 0.00116182
Iteration 11/25 | Loss: 0.00116182
Iteration 12/25 | Loss: 0.00116182
Iteration 13/25 | Loss: 0.00116182
Iteration 14/25 | Loss: 0.00116182
Iteration 15/25 | Loss: 0.00116182
Iteration 16/25 | Loss: 0.00116182
Iteration 17/25 | Loss: 0.00116182
Iteration 18/25 | Loss: 0.00116182
Iteration 19/25 | Loss: 0.00116182
Iteration 20/25 | Loss: 0.00116182
Iteration 21/25 | Loss: 0.00116182
Iteration 22/25 | Loss: 0.00116182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011618175776675344, 0.0011618175776675344, 0.0011618175776675344, 0.0011618175776675344, 0.0011618175776675344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011618175776675344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116182
Iteration 2/1000 | Loss: 0.00026120
Iteration 3/1000 | Loss: 0.00021583
Iteration 4/1000 | Loss: 0.00006836
Iteration 5/1000 | Loss: 0.00005066
Iteration 6/1000 | Loss: 0.00024340
Iteration 7/1000 | Loss: 0.00004546
Iteration 8/1000 | Loss: 0.00004026
Iteration 9/1000 | Loss: 0.00003735
Iteration 10/1000 | Loss: 0.00004131
Iteration 11/1000 | Loss: 0.00006552
Iteration 12/1000 | Loss: 0.00021610
Iteration 13/1000 | Loss: 0.00017922
Iteration 14/1000 | Loss: 0.00003772
Iteration 15/1000 | Loss: 0.00010925
Iteration 16/1000 | Loss: 0.00004433
Iteration 17/1000 | Loss: 0.00045230
Iteration 18/1000 | Loss: 0.00003218
Iteration 19/1000 | Loss: 0.00005508
Iteration 20/1000 | Loss: 0.00003731
Iteration 21/1000 | Loss: 0.00003022
Iteration 22/1000 | Loss: 0.00002934
Iteration 23/1000 | Loss: 0.00002932
Iteration 24/1000 | Loss: 0.00017945
Iteration 25/1000 | Loss: 0.00003615
Iteration 26/1000 | Loss: 0.00003063
Iteration 27/1000 | Loss: 0.00002929
Iteration 28/1000 | Loss: 0.00007702
Iteration 29/1000 | Loss: 0.00018849
Iteration 30/1000 | Loss: 0.00018885
Iteration 31/1000 | Loss: 0.00021420
Iteration 32/1000 | Loss: 0.00004119
Iteration 33/1000 | Loss: 0.00006044
Iteration 34/1000 | Loss: 0.00002841
Iteration 35/1000 | Loss: 0.00004192
Iteration 36/1000 | Loss: 0.00003297
Iteration 37/1000 | Loss: 0.00002619
Iteration 38/1000 | Loss: 0.00002567
Iteration 39/1000 | Loss: 0.00002514
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002419
Iteration 42/1000 | Loss: 0.00002408
Iteration 43/1000 | Loss: 0.00002375
Iteration 44/1000 | Loss: 0.00002369
Iteration 45/1000 | Loss: 0.00002359
Iteration 46/1000 | Loss: 0.00004559
Iteration 47/1000 | Loss: 0.00002349
Iteration 48/1000 | Loss: 0.00002348
Iteration 49/1000 | Loss: 0.00003121
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002340
Iteration 54/1000 | Loss: 0.00002339
Iteration 55/1000 | Loss: 0.00002338
Iteration 56/1000 | Loss: 0.00002336
Iteration 57/1000 | Loss: 0.00002336
Iteration 58/1000 | Loss: 0.00002336
Iteration 59/1000 | Loss: 0.00002335
Iteration 60/1000 | Loss: 0.00002335
Iteration 61/1000 | Loss: 0.00002335
Iteration 62/1000 | Loss: 0.00002334
Iteration 63/1000 | Loss: 0.00002334
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00002332
Iteration 66/1000 | Loss: 0.00002332
Iteration 67/1000 | Loss: 0.00002332
Iteration 68/1000 | Loss: 0.00002332
Iteration 69/1000 | Loss: 0.00002332
Iteration 70/1000 | Loss: 0.00002332
Iteration 71/1000 | Loss: 0.00002332
Iteration 72/1000 | Loss: 0.00002331
Iteration 73/1000 | Loss: 0.00002331
Iteration 74/1000 | Loss: 0.00002331
Iteration 75/1000 | Loss: 0.00002331
Iteration 76/1000 | Loss: 0.00002331
Iteration 77/1000 | Loss: 0.00002331
Iteration 78/1000 | Loss: 0.00002331
Iteration 79/1000 | Loss: 0.00002330
Iteration 80/1000 | Loss: 0.00002330
Iteration 81/1000 | Loss: 0.00002330
Iteration 82/1000 | Loss: 0.00002330
Iteration 83/1000 | Loss: 0.00002330
Iteration 84/1000 | Loss: 0.00002330
Iteration 85/1000 | Loss: 0.00002330
Iteration 86/1000 | Loss: 0.00002329
Iteration 87/1000 | Loss: 0.00002329
Iteration 88/1000 | Loss: 0.00002329
Iteration 89/1000 | Loss: 0.00002329
Iteration 90/1000 | Loss: 0.00002329
Iteration 91/1000 | Loss: 0.00002329
Iteration 92/1000 | Loss: 0.00002328
Iteration 93/1000 | Loss: 0.00002328
Iteration 94/1000 | Loss: 0.00002328
Iteration 95/1000 | Loss: 0.00002327
Iteration 96/1000 | Loss: 0.00002327
Iteration 97/1000 | Loss: 0.00002327
Iteration 98/1000 | Loss: 0.00002326
Iteration 99/1000 | Loss: 0.00002326
Iteration 100/1000 | Loss: 0.00002326
Iteration 101/1000 | Loss: 0.00002326
Iteration 102/1000 | Loss: 0.00002326
Iteration 103/1000 | Loss: 0.00002326
Iteration 104/1000 | Loss: 0.00002326
Iteration 105/1000 | Loss: 0.00002325
Iteration 106/1000 | Loss: 0.00002325
Iteration 107/1000 | Loss: 0.00002325
Iteration 108/1000 | Loss: 0.00002325
Iteration 109/1000 | Loss: 0.00002325
Iteration 110/1000 | Loss: 0.00002325
Iteration 111/1000 | Loss: 0.00002325
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002325
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002324
Iteration 120/1000 | Loss: 0.00002324
Iteration 121/1000 | Loss: 0.00002324
Iteration 122/1000 | Loss: 0.00002324
Iteration 123/1000 | Loss: 0.00002324
Iteration 124/1000 | Loss: 0.00002324
Iteration 125/1000 | Loss: 0.00002324
Iteration 126/1000 | Loss: 0.00002324
Iteration 127/1000 | Loss: 0.00002324
Iteration 128/1000 | Loss: 0.00002324
Iteration 129/1000 | Loss: 0.00002324
Iteration 130/1000 | Loss: 0.00002324
Iteration 131/1000 | Loss: 0.00002323
Iteration 132/1000 | Loss: 0.00002323
Iteration 133/1000 | Loss: 0.00002323
Iteration 134/1000 | Loss: 0.00002323
Iteration 135/1000 | Loss: 0.00002323
Iteration 136/1000 | Loss: 0.00002323
Iteration 137/1000 | Loss: 0.00002323
Iteration 138/1000 | Loss: 0.00002323
Iteration 139/1000 | Loss: 0.00002323
Iteration 140/1000 | Loss: 0.00002323
Iteration 141/1000 | Loss: 0.00002323
Iteration 142/1000 | Loss: 0.00002323
Iteration 143/1000 | Loss: 0.00002323
Iteration 144/1000 | Loss: 0.00002323
Iteration 145/1000 | Loss: 0.00002322
Iteration 146/1000 | Loss: 0.00002322
Iteration 147/1000 | Loss: 0.00002322
Iteration 148/1000 | Loss: 0.00002322
Iteration 149/1000 | Loss: 0.00002322
Iteration 150/1000 | Loss: 0.00002322
Iteration 151/1000 | Loss: 0.00002322
Iteration 152/1000 | Loss: 0.00002322
Iteration 153/1000 | Loss: 0.00002322
Iteration 154/1000 | Loss: 0.00002322
Iteration 155/1000 | Loss: 0.00002322
Iteration 156/1000 | Loss: 0.00002322
Iteration 157/1000 | Loss: 0.00002322
Iteration 158/1000 | Loss: 0.00002322
Iteration 159/1000 | Loss: 0.00002322
Iteration 160/1000 | Loss: 0.00002322
Iteration 161/1000 | Loss: 0.00002322
Iteration 162/1000 | Loss: 0.00002322
Iteration 163/1000 | Loss: 0.00002322
Iteration 164/1000 | Loss: 0.00002322
Iteration 165/1000 | Loss: 0.00002321
Iteration 166/1000 | Loss: 0.00002321
Iteration 167/1000 | Loss: 0.00002321
Iteration 168/1000 | Loss: 0.00002321
Iteration 169/1000 | Loss: 0.00002321
Iteration 170/1000 | Loss: 0.00002321
Iteration 171/1000 | Loss: 0.00002321
Iteration 172/1000 | Loss: 0.00002321
Iteration 173/1000 | Loss: 0.00002321
Iteration 174/1000 | Loss: 0.00002321
Iteration 175/1000 | Loss: 0.00002321
Iteration 176/1000 | Loss: 0.00002321
Iteration 177/1000 | Loss: 0.00002321
Iteration 178/1000 | Loss: 0.00002321
Iteration 179/1000 | Loss: 0.00002321
Iteration 180/1000 | Loss: 0.00002321
Iteration 181/1000 | Loss: 0.00002321
Iteration 182/1000 | Loss: 0.00002321
Iteration 183/1000 | Loss: 0.00002321
Iteration 184/1000 | Loss: 0.00002321
Iteration 185/1000 | Loss: 0.00002321
Iteration 186/1000 | Loss: 0.00002321
Iteration 187/1000 | Loss: 0.00002321
Iteration 188/1000 | Loss: 0.00002321
Iteration 189/1000 | Loss: 0.00002321
Iteration 190/1000 | Loss: 0.00002321
Iteration 191/1000 | Loss: 0.00002321
Iteration 192/1000 | Loss: 0.00002321
Iteration 193/1000 | Loss: 0.00002321
Iteration 194/1000 | Loss: 0.00002321
Iteration 195/1000 | Loss: 0.00002321
Iteration 196/1000 | Loss: 0.00002321
Iteration 197/1000 | Loss: 0.00002321
Iteration 198/1000 | Loss: 0.00002321
Iteration 199/1000 | Loss: 0.00002321
Iteration 200/1000 | Loss: 0.00002321
Iteration 201/1000 | Loss: 0.00002321
Iteration 202/1000 | Loss: 0.00002321
Iteration 203/1000 | Loss: 0.00002321
Iteration 204/1000 | Loss: 0.00002321
Iteration 205/1000 | Loss: 0.00002321
Iteration 206/1000 | Loss: 0.00002321
Iteration 207/1000 | Loss: 0.00002321
Iteration 208/1000 | Loss: 0.00002321
Iteration 209/1000 | Loss: 0.00002321
Iteration 210/1000 | Loss: 0.00002321
Iteration 211/1000 | Loss: 0.00002321
Iteration 212/1000 | Loss: 0.00002321
Iteration 213/1000 | Loss: 0.00002321
Iteration 214/1000 | Loss: 0.00002321
Iteration 215/1000 | Loss: 0.00002321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.3213668100652285e-05, 2.3213668100652285e-05, 2.3213668100652285e-05, 2.3213668100652285e-05, 2.3213668100652285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3213668100652285e-05

Optimization complete. Final v2v error: 4.072606563568115 mm

Highest mean error: 4.427964687347412 mm for frame 24

Lowest mean error: 3.504028081893921 mm for frame 0

Saving results

Total time: 103.89320158958435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587767
Iteration 2/25 | Loss: 0.00140934
Iteration 3/25 | Loss: 0.00134747
Iteration 4/25 | Loss: 0.00122157
Iteration 5/25 | Loss: 0.00121759
Iteration 6/25 | Loss: 0.00120834
Iteration 7/25 | Loss: 0.00120428
Iteration 8/25 | Loss: 0.00120224
Iteration 9/25 | Loss: 0.00120079
Iteration 10/25 | Loss: 0.00120019
Iteration 11/25 | Loss: 0.00120234
Iteration 12/25 | Loss: 0.00119877
Iteration 13/25 | Loss: 0.00119748
Iteration 14/25 | Loss: 0.00119708
Iteration 15/25 | Loss: 0.00119703
Iteration 16/25 | Loss: 0.00119701
Iteration 17/25 | Loss: 0.00119701
Iteration 18/25 | Loss: 0.00119701
Iteration 19/25 | Loss: 0.00119701
Iteration 20/25 | Loss: 0.00119701
Iteration 21/25 | Loss: 0.00119700
Iteration 22/25 | Loss: 0.00119700
Iteration 23/25 | Loss: 0.00119700
Iteration 24/25 | Loss: 0.00119700
Iteration 25/25 | Loss: 0.00119700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15059280
Iteration 2/25 | Loss: 0.00123264
Iteration 3/25 | Loss: 0.00123264
Iteration 4/25 | Loss: 0.00123263
Iteration 5/25 | Loss: 0.00123263
Iteration 6/25 | Loss: 0.00123263
Iteration 7/25 | Loss: 0.00123263
Iteration 8/25 | Loss: 0.00123263
Iteration 9/25 | Loss: 0.00123263
Iteration 10/25 | Loss: 0.00123263
Iteration 11/25 | Loss: 0.00123263
Iteration 12/25 | Loss: 0.00123263
Iteration 13/25 | Loss: 0.00123263
Iteration 14/25 | Loss: 0.00123263
Iteration 15/25 | Loss: 0.00123263
Iteration 16/25 | Loss: 0.00123263
Iteration 17/25 | Loss: 0.00123263
Iteration 18/25 | Loss: 0.00123263
Iteration 19/25 | Loss: 0.00123263
Iteration 20/25 | Loss: 0.00123263
Iteration 21/25 | Loss: 0.00123263
Iteration 22/25 | Loss: 0.00123263
Iteration 23/25 | Loss: 0.00123263
Iteration 24/25 | Loss: 0.00123263
Iteration 25/25 | Loss: 0.00123263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123263
Iteration 2/1000 | Loss: 0.00002277
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001274
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001153
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001151
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001150
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001143
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001137
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001128
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001123
Iteration 72/1000 | Loss: 0.00001123
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001122
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001120
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001117
Iteration 92/1000 | Loss: 0.00001117
Iteration 93/1000 | Loss: 0.00001117
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001115
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001114
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001114
Iteration 122/1000 | Loss: 0.00001114
Iteration 123/1000 | Loss: 0.00001114
Iteration 124/1000 | Loss: 0.00001114
Iteration 125/1000 | Loss: 0.00001114
Iteration 126/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.114186852646526e-05, 1.114186852646526e-05, 1.114186852646526e-05, 1.114186852646526e-05, 1.114186852646526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.114186852646526e-05

Optimization complete. Final v2v error: 2.847843647003174 mm

Highest mean error: 3.220346689224243 mm for frame 66

Lowest mean error: 2.565664052963257 mm for frame 120

Saving results

Total time: 56.79553198814392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038416
Iteration 2/25 | Loss: 0.00187553
Iteration 3/25 | Loss: 0.00135672
Iteration 4/25 | Loss: 0.00128782
Iteration 5/25 | Loss: 0.00126730
Iteration 6/25 | Loss: 0.00119990
Iteration 7/25 | Loss: 0.00118276
Iteration 8/25 | Loss: 0.00116865
Iteration 9/25 | Loss: 0.00115750
Iteration 10/25 | Loss: 0.00115472
Iteration 11/25 | Loss: 0.00115227
Iteration 12/25 | Loss: 0.00115181
Iteration 13/25 | Loss: 0.00115169
Iteration 14/25 | Loss: 0.00115171
Iteration 15/25 | Loss: 0.00115171
Iteration 16/25 | Loss: 0.00115247
Iteration 17/25 | Loss: 0.00115165
Iteration 18/25 | Loss: 0.00115215
Iteration 19/25 | Loss: 0.00115159
Iteration 20/25 | Loss: 0.00115159
Iteration 21/25 | Loss: 0.00115159
Iteration 22/25 | Loss: 0.00115159
Iteration 23/25 | Loss: 0.00115159
Iteration 24/25 | Loss: 0.00115159
Iteration 25/25 | Loss: 0.00115159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37720811
Iteration 2/25 | Loss: 0.00165227
Iteration 3/25 | Loss: 0.00161097
Iteration 4/25 | Loss: 0.00161097
Iteration 5/25 | Loss: 0.00161097
Iteration 6/25 | Loss: 0.00161097
Iteration 7/25 | Loss: 0.00161097
Iteration 8/25 | Loss: 0.00161097
Iteration 9/25 | Loss: 0.00161097
Iteration 10/25 | Loss: 0.00161097
Iteration 11/25 | Loss: 0.00161097
Iteration 12/25 | Loss: 0.00161097
Iteration 13/25 | Loss: 0.00161097
Iteration 14/25 | Loss: 0.00161097
Iteration 15/25 | Loss: 0.00161097
Iteration 16/25 | Loss: 0.00161097
Iteration 17/25 | Loss: 0.00161097
Iteration 18/25 | Loss: 0.00161097
Iteration 19/25 | Loss: 0.00161097
Iteration 20/25 | Loss: 0.00161097
Iteration 21/25 | Loss: 0.00161097
Iteration 22/25 | Loss: 0.00161097
Iteration 23/25 | Loss: 0.00161097
Iteration 24/25 | Loss: 0.00161097
Iteration 25/25 | Loss: 0.00161097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161097
Iteration 2/1000 | Loss: 0.00005178
Iteration 3/1000 | Loss: 0.00004691
Iteration 4/1000 | Loss: 0.00007966
Iteration 5/1000 | Loss: 0.00015823
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00002488
Iteration 8/1000 | Loss: 0.00002495
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00009597
Iteration 11/1000 | Loss: 0.00003180
Iteration 12/1000 | Loss: 0.00001294
Iteration 13/1000 | Loss: 0.00000927
Iteration 14/1000 | Loss: 0.00000921
Iteration 15/1000 | Loss: 0.00000920
Iteration 16/1000 | Loss: 0.00001319
Iteration 17/1000 | Loss: 0.00000913
Iteration 18/1000 | Loss: 0.00002978
Iteration 19/1000 | Loss: 0.00003743
Iteration 20/1000 | Loss: 0.00000940
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00000971
Iteration 26/1000 | Loss: 0.00000951
Iteration 27/1000 | Loss: 0.00001363
Iteration 28/1000 | Loss: 0.00003256
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001004
Iteration 31/1000 | Loss: 0.00000858
Iteration 32/1000 | Loss: 0.00000857
Iteration 33/1000 | Loss: 0.00000857
Iteration 34/1000 | Loss: 0.00000857
Iteration 35/1000 | Loss: 0.00000857
Iteration 36/1000 | Loss: 0.00000857
Iteration 37/1000 | Loss: 0.00000857
Iteration 38/1000 | Loss: 0.00000857
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00000893
Iteration 42/1000 | Loss: 0.00000867
Iteration 43/1000 | Loss: 0.00000869
Iteration 44/1000 | Loss: 0.00000853
Iteration 45/1000 | Loss: 0.00000853
Iteration 46/1000 | Loss: 0.00000853
Iteration 47/1000 | Loss: 0.00000852
Iteration 48/1000 | Loss: 0.00000850
Iteration 49/1000 | Loss: 0.00000850
Iteration 50/1000 | Loss: 0.00000850
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000850
Iteration 53/1000 | Loss: 0.00000850
Iteration 54/1000 | Loss: 0.00000850
Iteration 55/1000 | Loss: 0.00000850
Iteration 56/1000 | Loss: 0.00000850
Iteration 57/1000 | Loss: 0.00000850
Iteration 58/1000 | Loss: 0.00000850
Iteration 59/1000 | Loss: 0.00000850
Iteration 60/1000 | Loss: 0.00000850
Iteration 61/1000 | Loss: 0.00000850
Iteration 62/1000 | Loss: 0.00000850
Iteration 63/1000 | Loss: 0.00000850
Iteration 64/1000 | Loss: 0.00000850
Iteration 65/1000 | Loss: 0.00000850
Iteration 66/1000 | Loss: 0.00000850
Iteration 67/1000 | Loss: 0.00000850
Iteration 68/1000 | Loss: 0.00000850
Iteration 69/1000 | Loss: 0.00000850
Iteration 70/1000 | Loss: 0.00000850
Iteration 71/1000 | Loss: 0.00000850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [8.496141163050197e-06, 8.496141163050197e-06, 8.496141163050197e-06, 8.496141163050197e-06, 8.496141163050197e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.496141163050197e-06

Optimization complete. Final v2v error: 2.559619188308716 mm

Highest mean error: 2.7372186183929443 mm for frame 39

Lowest mean error: 2.392258405685425 mm for frame 46

Saving results

Total time: 66.45900201797485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395690
Iteration 2/25 | Loss: 0.00125894
Iteration 3/25 | Loss: 0.00118805
Iteration 4/25 | Loss: 0.00117455
Iteration 5/25 | Loss: 0.00116973
Iteration 6/25 | Loss: 0.00116877
Iteration 7/25 | Loss: 0.00116877
Iteration 8/25 | Loss: 0.00116877
Iteration 9/25 | Loss: 0.00116877
Iteration 10/25 | Loss: 0.00116877
Iteration 11/25 | Loss: 0.00116877
Iteration 12/25 | Loss: 0.00116877
Iteration 13/25 | Loss: 0.00116877
Iteration 14/25 | Loss: 0.00116877
Iteration 15/25 | Loss: 0.00116877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011687737423926592, 0.0011687737423926592, 0.0011687737423926592, 0.0011687737423926592, 0.0011687737423926592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011687737423926592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30796850
Iteration 2/25 | Loss: 0.00186907
Iteration 3/25 | Loss: 0.00186907
Iteration 4/25 | Loss: 0.00186907
Iteration 5/25 | Loss: 0.00186907
Iteration 6/25 | Loss: 0.00186907
Iteration 7/25 | Loss: 0.00186907
Iteration 8/25 | Loss: 0.00186907
Iteration 9/25 | Loss: 0.00186907
Iteration 10/25 | Loss: 0.00186907
Iteration 11/25 | Loss: 0.00186907
Iteration 12/25 | Loss: 0.00186907
Iteration 13/25 | Loss: 0.00186907
Iteration 14/25 | Loss: 0.00186907
Iteration 15/25 | Loss: 0.00186907
Iteration 16/25 | Loss: 0.00186907
Iteration 17/25 | Loss: 0.00186907
Iteration 18/25 | Loss: 0.00186907
Iteration 19/25 | Loss: 0.00186907
Iteration 20/25 | Loss: 0.00186907
Iteration 21/25 | Loss: 0.00186907
Iteration 22/25 | Loss: 0.00186907
Iteration 23/25 | Loss: 0.00186907
Iteration 24/25 | Loss: 0.00186907
Iteration 25/25 | Loss: 0.00186907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186907
Iteration 2/1000 | Loss: 0.00003623
Iteration 3/1000 | Loss: 0.00002290
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001503
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001468
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001449
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001448
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001445
Iteration 65/1000 | Loss: 0.00001445
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001438
Iteration 95/1000 | Loss: 0.00001438
Iteration 96/1000 | Loss: 0.00001438
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001438
Iteration 101/1000 | Loss: 0.00001437
Iteration 102/1000 | Loss: 0.00001437
Iteration 103/1000 | Loss: 0.00001437
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001436
Iteration 106/1000 | Loss: 0.00001436
Iteration 107/1000 | Loss: 0.00001436
Iteration 108/1000 | Loss: 0.00001436
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001435
Iteration 114/1000 | Loss: 0.00001435
Iteration 115/1000 | Loss: 0.00001435
Iteration 116/1000 | Loss: 0.00001435
Iteration 117/1000 | Loss: 0.00001435
Iteration 118/1000 | Loss: 0.00001435
Iteration 119/1000 | Loss: 0.00001435
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001434
Iteration 123/1000 | Loss: 0.00001434
Iteration 124/1000 | Loss: 0.00001434
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001434
Iteration 127/1000 | Loss: 0.00001434
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001432
Iteration 138/1000 | Loss: 0.00001432
Iteration 139/1000 | Loss: 0.00001432
Iteration 140/1000 | Loss: 0.00001432
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001432
Iteration 143/1000 | Loss: 0.00001432
Iteration 144/1000 | Loss: 0.00001432
Iteration 145/1000 | Loss: 0.00001431
Iteration 146/1000 | Loss: 0.00001431
Iteration 147/1000 | Loss: 0.00001431
Iteration 148/1000 | Loss: 0.00001431
Iteration 149/1000 | Loss: 0.00001431
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001429
Iteration 157/1000 | Loss: 0.00001429
Iteration 158/1000 | Loss: 0.00001429
Iteration 159/1000 | Loss: 0.00001429
Iteration 160/1000 | Loss: 0.00001429
Iteration 161/1000 | Loss: 0.00001429
Iteration 162/1000 | Loss: 0.00001429
Iteration 163/1000 | Loss: 0.00001429
Iteration 164/1000 | Loss: 0.00001429
Iteration 165/1000 | Loss: 0.00001428
Iteration 166/1000 | Loss: 0.00001428
Iteration 167/1000 | Loss: 0.00001428
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001425
Iteration 180/1000 | Loss: 0.00001425
Iteration 181/1000 | Loss: 0.00001425
Iteration 182/1000 | Loss: 0.00001424
Iteration 183/1000 | Loss: 0.00001424
Iteration 184/1000 | Loss: 0.00001423
Iteration 185/1000 | Loss: 0.00001423
Iteration 186/1000 | Loss: 0.00001423
Iteration 187/1000 | Loss: 0.00001423
Iteration 188/1000 | Loss: 0.00001423
Iteration 189/1000 | Loss: 0.00001423
Iteration 190/1000 | Loss: 0.00001423
Iteration 191/1000 | Loss: 0.00001423
Iteration 192/1000 | Loss: 0.00001422
Iteration 193/1000 | Loss: 0.00001422
Iteration 194/1000 | Loss: 0.00001422
Iteration 195/1000 | Loss: 0.00001422
Iteration 196/1000 | Loss: 0.00001422
Iteration 197/1000 | Loss: 0.00001422
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001420
Iteration 207/1000 | Loss: 0.00001420
Iteration 208/1000 | Loss: 0.00001420
Iteration 209/1000 | Loss: 0.00001420
Iteration 210/1000 | Loss: 0.00001419
Iteration 211/1000 | Loss: 0.00001419
Iteration 212/1000 | Loss: 0.00001419
Iteration 213/1000 | Loss: 0.00001419
Iteration 214/1000 | Loss: 0.00001419
Iteration 215/1000 | Loss: 0.00001419
Iteration 216/1000 | Loss: 0.00001419
Iteration 217/1000 | Loss: 0.00001419
Iteration 218/1000 | Loss: 0.00001419
Iteration 219/1000 | Loss: 0.00001419
Iteration 220/1000 | Loss: 0.00001419
Iteration 221/1000 | Loss: 0.00001419
Iteration 222/1000 | Loss: 0.00001418
Iteration 223/1000 | Loss: 0.00001418
Iteration 224/1000 | Loss: 0.00001418
Iteration 225/1000 | Loss: 0.00001418
Iteration 226/1000 | Loss: 0.00001417
Iteration 227/1000 | Loss: 0.00001417
Iteration 228/1000 | Loss: 0.00001417
Iteration 229/1000 | Loss: 0.00001417
Iteration 230/1000 | Loss: 0.00001417
Iteration 231/1000 | Loss: 0.00001417
Iteration 232/1000 | Loss: 0.00001416
Iteration 233/1000 | Loss: 0.00001416
Iteration 234/1000 | Loss: 0.00001416
Iteration 235/1000 | Loss: 0.00001416
Iteration 236/1000 | Loss: 0.00001416
Iteration 237/1000 | Loss: 0.00001416
Iteration 238/1000 | Loss: 0.00001416
Iteration 239/1000 | Loss: 0.00001416
Iteration 240/1000 | Loss: 0.00001416
Iteration 241/1000 | Loss: 0.00001416
Iteration 242/1000 | Loss: 0.00001416
Iteration 243/1000 | Loss: 0.00001416
Iteration 244/1000 | Loss: 0.00001416
Iteration 245/1000 | Loss: 0.00001416
Iteration 246/1000 | Loss: 0.00001416
Iteration 247/1000 | Loss: 0.00001416
Iteration 248/1000 | Loss: 0.00001416
Iteration 249/1000 | Loss: 0.00001416
Iteration 250/1000 | Loss: 0.00001416
Iteration 251/1000 | Loss: 0.00001416
Iteration 252/1000 | Loss: 0.00001416
Iteration 253/1000 | Loss: 0.00001416
Iteration 254/1000 | Loss: 0.00001416
Iteration 255/1000 | Loss: 0.00001416
Iteration 256/1000 | Loss: 0.00001416
Iteration 257/1000 | Loss: 0.00001416
Iteration 258/1000 | Loss: 0.00001416
Iteration 259/1000 | Loss: 0.00001416
Iteration 260/1000 | Loss: 0.00001416
Iteration 261/1000 | Loss: 0.00001416
Iteration 262/1000 | Loss: 0.00001416
Iteration 263/1000 | Loss: 0.00001416
Iteration 264/1000 | Loss: 0.00001416
Iteration 265/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.4164582353259902e-05, 1.4164582353259902e-05, 1.4164582353259902e-05, 1.4164582353259902e-05, 1.4164582353259902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4164582353259902e-05

Optimization complete. Final v2v error: 3.1052327156066895 mm

Highest mean error: 3.9403908252716064 mm for frame 120

Lowest mean error: 2.5312862396240234 mm for frame 27

Saving results

Total time: 51.75078558921814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556775
Iteration 2/25 | Loss: 0.00138190
Iteration 3/25 | Loss: 0.00130321
Iteration 4/25 | Loss: 0.00129638
Iteration 5/25 | Loss: 0.00129384
Iteration 6/25 | Loss: 0.00129384
Iteration 7/25 | Loss: 0.00129384
Iteration 8/25 | Loss: 0.00129384
Iteration 9/25 | Loss: 0.00129384
Iteration 10/25 | Loss: 0.00129384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012938369764015079, 0.0012938369764015079, 0.0012938369764015079, 0.0012938369764015079, 0.0012938369764015079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012938369764015079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77263308
Iteration 2/25 | Loss: 0.00106036
Iteration 3/25 | Loss: 0.00106026
Iteration 4/25 | Loss: 0.00106026
Iteration 5/25 | Loss: 0.00106026
Iteration 6/25 | Loss: 0.00106026
Iteration 7/25 | Loss: 0.00106026
Iteration 8/25 | Loss: 0.00106026
Iteration 9/25 | Loss: 0.00106026
Iteration 10/25 | Loss: 0.00106026
Iteration 11/25 | Loss: 0.00106026
Iteration 12/25 | Loss: 0.00106026
Iteration 13/25 | Loss: 0.00106026
Iteration 14/25 | Loss: 0.00106026
Iteration 15/25 | Loss: 0.00106026
Iteration 16/25 | Loss: 0.00106026
Iteration 17/25 | Loss: 0.00106026
Iteration 18/25 | Loss: 0.00106026
Iteration 19/25 | Loss: 0.00106026
Iteration 20/25 | Loss: 0.00106026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010602573165670037, 0.0010602573165670037, 0.0010602573165670037, 0.0010602573165670037, 0.0010602573165670037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010602573165670037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106026
Iteration 2/1000 | Loss: 0.00002835
Iteration 3/1000 | Loss: 0.00002201
Iteration 4/1000 | Loss: 0.00002039
Iteration 5/1000 | Loss: 0.00001954
Iteration 6/1000 | Loss: 0.00001904
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001730
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001689
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001675
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001658
Iteration 21/1000 | Loss: 0.00001657
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001635
Iteration 46/1000 | Loss: 0.00001635
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001630
Iteration 84/1000 | Loss: 0.00001630
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001629
Iteration 91/1000 | Loss: 0.00001629
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001629
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001627
Iteration 110/1000 | Loss: 0.00001627
Iteration 111/1000 | Loss: 0.00001627
Iteration 112/1000 | Loss: 0.00001627
Iteration 113/1000 | Loss: 0.00001627
Iteration 114/1000 | Loss: 0.00001627
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001627
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001626
Iteration 122/1000 | Loss: 0.00001626
Iteration 123/1000 | Loss: 0.00001626
Iteration 124/1000 | Loss: 0.00001626
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001626
Iteration 130/1000 | Loss: 0.00001625
Iteration 131/1000 | Loss: 0.00001625
Iteration 132/1000 | Loss: 0.00001625
Iteration 133/1000 | Loss: 0.00001625
Iteration 134/1000 | Loss: 0.00001625
Iteration 135/1000 | Loss: 0.00001625
Iteration 136/1000 | Loss: 0.00001625
Iteration 137/1000 | Loss: 0.00001625
Iteration 138/1000 | Loss: 0.00001625
Iteration 139/1000 | Loss: 0.00001625
Iteration 140/1000 | Loss: 0.00001625
Iteration 141/1000 | Loss: 0.00001625
Iteration 142/1000 | Loss: 0.00001625
Iteration 143/1000 | Loss: 0.00001625
Iteration 144/1000 | Loss: 0.00001625
Iteration 145/1000 | Loss: 0.00001625
Iteration 146/1000 | Loss: 0.00001625
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001625
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001625
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.624531068955548e-05, 1.624531068955548e-05, 1.624531068955548e-05, 1.624531068955548e-05, 1.624531068955548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.624531068955548e-05

Optimization complete. Final v2v error: 3.3566462993621826 mm

Highest mean error: 3.594151735305786 mm for frame 137

Lowest mean error: 3.183582305908203 mm for frame 21

Saving results

Total time: 48.184037923812866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037346
Iteration 2/25 | Loss: 0.01037346
Iteration 3/25 | Loss: 0.00274847
Iteration 4/25 | Loss: 0.00200538
Iteration 5/25 | Loss: 0.00203114
Iteration 6/25 | Loss: 0.00172698
Iteration 7/25 | Loss: 0.00147960
Iteration 8/25 | Loss: 0.00135218
Iteration 9/25 | Loss: 0.00132995
Iteration 10/25 | Loss: 0.00131519
Iteration 11/25 | Loss: 0.00131630
Iteration 12/25 | Loss: 0.00133945
Iteration 13/25 | Loss: 0.00131397
Iteration 14/25 | Loss: 0.00131593
Iteration 15/25 | Loss: 0.00131653
Iteration 16/25 | Loss: 0.00130227
Iteration 17/25 | Loss: 0.00129391
Iteration 18/25 | Loss: 0.00130222
Iteration 19/25 | Loss: 0.00128636
Iteration 20/25 | Loss: 0.00128681
Iteration 21/25 | Loss: 0.00127047
Iteration 22/25 | Loss: 0.00127285
Iteration 23/25 | Loss: 0.00126786
Iteration 24/25 | Loss: 0.00126264
Iteration 25/25 | Loss: 0.00126076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34652221
Iteration 2/25 | Loss: 0.00246178
Iteration 3/25 | Loss: 0.00237465
Iteration 4/25 | Loss: 0.00237465
Iteration 5/25 | Loss: 0.00237465
Iteration 6/25 | Loss: 0.00237465
Iteration 7/25 | Loss: 0.00237465
Iteration 8/25 | Loss: 0.00237465
Iteration 9/25 | Loss: 0.00237465
Iteration 10/25 | Loss: 0.00237465
Iteration 11/25 | Loss: 0.00237465
Iteration 12/25 | Loss: 0.00237465
Iteration 13/25 | Loss: 0.00237465
Iteration 14/25 | Loss: 0.00237465
Iteration 15/25 | Loss: 0.00237465
Iteration 16/25 | Loss: 0.00237465
Iteration 17/25 | Loss: 0.00237465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002374649979174137, 0.002374649979174137, 0.002374649979174137, 0.002374649979174137, 0.002374649979174137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002374649979174137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237465
Iteration 2/1000 | Loss: 0.00140223
Iteration 3/1000 | Loss: 0.00310424
Iteration 4/1000 | Loss: 0.00272379
Iteration 5/1000 | Loss: 0.00195774
Iteration 6/1000 | Loss: 0.00256058
Iteration 7/1000 | Loss: 0.00274215
Iteration 8/1000 | Loss: 0.00312410
Iteration 9/1000 | Loss: 0.00254481
Iteration 10/1000 | Loss: 0.00291792
Iteration 11/1000 | Loss: 0.00279549
Iteration 12/1000 | Loss: 0.00180785
Iteration 13/1000 | Loss: 0.00229382
Iteration 14/1000 | Loss: 0.00226153
Iteration 15/1000 | Loss: 0.00270745
Iteration 16/1000 | Loss: 0.00290676
Iteration 17/1000 | Loss: 0.00257784
Iteration 18/1000 | Loss: 0.00167828
Iteration 19/1000 | Loss: 0.00195158
Iteration 20/1000 | Loss: 0.00207362
Iteration 21/1000 | Loss: 0.00223098
Iteration 22/1000 | Loss: 0.00239047
Iteration 23/1000 | Loss: 0.00193338
Iteration 24/1000 | Loss: 0.00151888
Iteration 25/1000 | Loss: 0.00151868
Iteration 26/1000 | Loss: 0.00182860
Iteration 27/1000 | Loss: 0.00175987
Iteration 28/1000 | Loss: 0.00134205
Iteration 29/1000 | Loss: 0.00123574
Iteration 30/1000 | Loss: 0.00177572
Iteration 31/1000 | Loss: 0.00168750
Iteration 32/1000 | Loss: 0.00138110
Iteration 33/1000 | Loss: 0.00140902
Iteration 34/1000 | Loss: 0.00123685
Iteration 35/1000 | Loss: 0.00142426
Iteration 36/1000 | Loss: 0.00162657
Iteration 37/1000 | Loss: 0.00177700
Iteration 38/1000 | Loss: 0.00154413
Iteration 39/1000 | Loss: 0.00187812
Iteration 40/1000 | Loss: 0.00195288
Iteration 41/1000 | Loss: 0.00297730
Iteration 42/1000 | Loss: 0.00119116
Iteration 43/1000 | Loss: 0.00117973
Iteration 44/1000 | Loss: 0.00150882
Iteration 45/1000 | Loss: 0.00136821
Iteration 46/1000 | Loss: 0.00144994
Iteration 47/1000 | Loss: 0.00156586
Iteration 48/1000 | Loss: 0.00174212
Iteration 49/1000 | Loss: 0.00176141
Iteration 50/1000 | Loss: 0.00167869
Iteration 51/1000 | Loss: 0.00164426
Iteration 52/1000 | Loss: 0.00167096
Iteration 53/1000 | Loss: 0.00178739
Iteration 54/1000 | Loss: 0.00236537
Iteration 55/1000 | Loss: 0.00224119
Iteration 56/1000 | Loss: 0.00188237
Iteration 57/1000 | Loss: 0.00193416
Iteration 58/1000 | Loss: 0.00171539
Iteration 59/1000 | Loss: 0.00173302
Iteration 60/1000 | Loss: 0.00226498
Iteration 61/1000 | Loss: 0.00182749
Iteration 62/1000 | Loss: 0.00140535
Iteration 63/1000 | Loss: 0.00160589
Iteration 64/1000 | Loss: 0.00161735
Iteration 65/1000 | Loss: 0.00175849
Iteration 66/1000 | Loss: 0.00158250
Iteration 67/1000 | Loss: 0.00164049
Iteration 68/1000 | Loss: 0.00150016
Iteration 69/1000 | Loss: 0.00166017
Iteration 70/1000 | Loss: 0.00153010
Iteration 71/1000 | Loss: 0.00211037
Iteration 72/1000 | Loss: 0.00147907
Iteration 73/1000 | Loss: 0.00206885
Iteration 74/1000 | Loss: 0.00220051
Iteration 75/1000 | Loss: 0.00137285
Iteration 76/1000 | Loss: 0.00141360
Iteration 77/1000 | Loss: 0.00062439
Iteration 78/1000 | Loss: 0.00103619
Iteration 79/1000 | Loss: 0.00099932
Iteration 80/1000 | Loss: 0.00144694
Iteration 81/1000 | Loss: 0.00177964
Iteration 82/1000 | Loss: 0.00096926
Iteration 83/1000 | Loss: 0.00126695
Iteration 84/1000 | Loss: 0.00128022
Iteration 85/1000 | Loss: 0.00122202
Iteration 86/1000 | Loss: 0.00123284
Iteration 87/1000 | Loss: 0.00114325
Iteration 88/1000 | Loss: 0.00171362
Iteration 89/1000 | Loss: 0.00147612
Iteration 90/1000 | Loss: 0.00166612
Iteration 91/1000 | Loss: 0.00150405
Iteration 92/1000 | Loss: 0.00148341
Iteration 93/1000 | Loss: 0.00121657
Iteration 94/1000 | Loss: 0.00109310
Iteration 95/1000 | Loss: 0.00118090
Iteration 96/1000 | Loss: 0.00111202
Iteration 97/1000 | Loss: 0.00122251
Iteration 98/1000 | Loss: 0.00180272
Iteration 99/1000 | Loss: 0.00144205
Iteration 100/1000 | Loss: 0.00107915
Iteration 101/1000 | Loss: 0.00099040
Iteration 102/1000 | Loss: 0.00090100
Iteration 103/1000 | Loss: 0.00110464
Iteration 104/1000 | Loss: 0.00116989
Iteration 105/1000 | Loss: 0.00103882
Iteration 106/1000 | Loss: 0.00077467
Iteration 107/1000 | Loss: 0.00087319
Iteration 108/1000 | Loss: 0.00111765
Iteration 109/1000 | Loss: 0.00114338
Iteration 110/1000 | Loss: 0.00098085
Iteration 111/1000 | Loss: 0.00114143
Iteration 112/1000 | Loss: 0.00098261
Iteration 113/1000 | Loss: 0.00140647
Iteration 114/1000 | Loss: 0.00082502
Iteration 115/1000 | Loss: 0.00100182
Iteration 116/1000 | Loss: 0.00110042
Iteration 117/1000 | Loss: 0.00100872
Iteration 118/1000 | Loss: 0.00137874
Iteration 119/1000 | Loss: 0.00118899
Iteration 120/1000 | Loss: 0.00116661
Iteration 121/1000 | Loss: 0.00121543
Iteration 122/1000 | Loss: 0.00091842
Iteration 123/1000 | Loss: 0.00105982
Iteration 124/1000 | Loss: 0.00081889
Iteration 125/1000 | Loss: 0.00114739
Iteration 126/1000 | Loss: 0.00112407
Iteration 127/1000 | Loss: 0.00107903
Iteration 128/1000 | Loss: 0.00117010
Iteration 129/1000 | Loss: 0.00115365
Iteration 130/1000 | Loss: 0.00142884
Iteration 131/1000 | Loss: 0.00162147
Iteration 132/1000 | Loss: 0.00126367
Iteration 133/1000 | Loss: 0.00153419
Iteration 134/1000 | Loss: 0.00168626
Iteration 135/1000 | Loss: 0.00086916
Iteration 136/1000 | Loss: 0.00156277
Iteration 137/1000 | Loss: 0.00138319
Iteration 138/1000 | Loss: 0.00152262
Iteration 139/1000 | Loss: 0.00132778
Iteration 140/1000 | Loss: 0.00153589
Iteration 141/1000 | Loss: 0.00092625
Iteration 142/1000 | Loss: 0.00104627
Iteration 143/1000 | Loss: 0.00104663
Iteration 144/1000 | Loss: 0.00109194
Iteration 145/1000 | Loss: 0.00110183
Iteration 146/1000 | Loss: 0.00078127
Iteration 147/1000 | Loss: 0.00096446
Iteration 148/1000 | Loss: 0.00141941
Iteration 149/1000 | Loss: 0.00125429
Iteration 150/1000 | Loss: 0.00100186
Iteration 151/1000 | Loss: 0.00106248
Iteration 152/1000 | Loss: 0.00101982
Iteration 153/1000 | Loss: 0.00087298
Iteration 154/1000 | Loss: 0.00085494
Iteration 155/1000 | Loss: 0.00099742
Iteration 156/1000 | Loss: 0.00109712
Iteration 157/1000 | Loss: 0.00090332
Iteration 158/1000 | Loss: 0.00099333
Iteration 159/1000 | Loss: 0.00093936
Iteration 160/1000 | Loss: 0.00108165
Iteration 161/1000 | Loss: 0.00095168
Iteration 162/1000 | Loss: 0.00109569
Iteration 163/1000 | Loss: 0.00096161
Iteration 164/1000 | Loss: 0.00104918
Iteration 165/1000 | Loss: 0.00095646
Iteration 166/1000 | Loss: 0.00113323
Iteration 167/1000 | Loss: 0.00140746
Iteration 168/1000 | Loss: 0.00133436
Iteration 169/1000 | Loss: 0.00105581
Iteration 170/1000 | Loss: 0.00152635
Iteration 171/1000 | Loss: 0.00092213
Iteration 172/1000 | Loss: 0.00131195
Iteration 173/1000 | Loss: 0.00099442
Iteration 174/1000 | Loss: 0.00115726
Iteration 175/1000 | Loss: 0.00111258
Iteration 176/1000 | Loss: 0.00120644
Iteration 177/1000 | Loss: 0.00096614
Iteration 178/1000 | Loss: 0.00129780
Iteration 179/1000 | Loss: 0.00080382
Iteration 180/1000 | Loss: 0.00071141
Iteration 181/1000 | Loss: 0.00079241
Iteration 182/1000 | Loss: 0.00086131
Iteration 183/1000 | Loss: 0.00141115
Iteration 184/1000 | Loss: 0.00095228
Iteration 185/1000 | Loss: 0.00121665
Iteration 186/1000 | Loss: 0.00131487
Iteration 187/1000 | Loss: 0.00110820
Iteration 188/1000 | Loss: 0.00116485
Iteration 189/1000 | Loss: 0.00076869
Iteration 190/1000 | Loss: 0.00067751
Iteration 191/1000 | Loss: 0.00066626
Iteration 192/1000 | Loss: 0.00081076
Iteration 193/1000 | Loss: 0.00092669
Iteration 194/1000 | Loss: 0.00079651
Iteration 195/1000 | Loss: 0.00096972
Iteration 196/1000 | Loss: 0.00162764
Iteration 197/1000 | Loss: 0.00118627
Iteration 198/1000 | Loss: 0.00106255
Iteration 199/1000 | Loss: 0.00096839
Iteration 200/1000 | Loss: 0.00075415
Iteration 201/1000 | Loss: 0.00048962
Iteration 202/1000 | Loss: 0.00110404
Iteration 203/1000 | Loss: 0.00091494
Iteration 204/1000 | Loss: 0.00080247
Iteration 205/1000 | Loss: 0.00081176
Iteration 206/1000 | Loss: 0.00090544
Iteration 207/1000 | Loss: 0.00074790
Iteration 208/1000 | Loss: 0.00081427
Iteration 209/1000 | Loss: 0.00102428
Iteration 210/1000 | Loss: 0.00133483
Iteration 211/1000 | Loss: 0.00119439
Iteration 212/1000 | Loss: 0.00082151
Iteration 213/1000 | Loss: 0.00094564
Iteration 214/1000 | Loss: 0.00080997
Iteration 215/1000 | Loss: 0.00099574
Iteration 216/1000 | Loss: 0.00075591
Iteration 217/1000 | Loss: 0.00112167
Iteration 218/1000 | Loss: 0.00074927
Iteration 219/1000 | Loss: 0.00088817
Iteration 220/1000 | Loss: 0.00102044
Iteration 221/1000 | Loss: 0.00060302
Iteration 222/1000 | Loss: 0.00069087
Iteration 223/1000 | Loss: 0.00101075
Iteration 224/1000 | Loss: 0.00091507
Iteration 225/1000 | Loss: 0.00066917
Iteration 226/1000 | Loss: 0.00065447
Iteration 227/1000 | Loss: 0.00055231
Iteration 228/1000 | Loss: 0.00081514
Iteration 229/1000 | Loss: 0.00029076
Iteration 230/1000 | Loss: 0.00054401
Iteration 231/1000 | Loss: 0.00056872
Iteration 232/1000 | Loss: 0.00074506
Iteration 233/1000 | Loss: 0.00081205
Iteration 234/1000 | Loss: 0.00135046
Iteration 235/1000 | Loss: 0.00134050
Iteration 236/1000 | Loss: 0.00071553
Iteration 237/1000 | Loss: 0.00040780
Iteration 238/1000 | Loss: 0.00073396
Iteration 239/1000 | Loss: 0.00081073
Iteration 240/1000 | Loss: 0.00088283
Iteration 241/1000 | Loss: 0.00040061
Iteration 242/1000 | Loss: 0.00050318
Iteration 243/1000 | Loss: 0.00053419
Iteration 244/1000 | Loss: 0.00046777
Iteration 245/1000 | Loss: 0.00075520
Iteration 246/1000 | Loss: 0.00053164
Iteration 247/1000 | Loss: 0.00049202
Iteration 248/1000 | Loss: 0.00038206
Iteration 249/1000 | Loss: 0.00072423
Iteration 250/1000 | Loss: 0.00089503
Iteration 251/1000 | Loss: 0.00074988
Iteration 252/1000 | Loss: 0.00063618
Iteration 253/1000 | Loss: 0.00070066
Iteration 254/1000 | Loss: 0.00058699
Iteration 255/1000 | Loss: 0.00055372
Iteration 256/1000 | Loss: 0.00073815
Iteration 257/1000 | Loss: 0.00073902
Iteration 258/1000 | Loss: 0.00070164
Iteration 259/1000 | Loss: 0.00074883
Iteration 260/1000 | Loss: 0.00074899
Iteration 261/1000 | Loss: 0.00063155
Iteration 262/1000 | Loss: 0.00071485
Iteration 263/1000 | Loss: 0.00065918
Iteration 264/1000 | Loss: 0.00071775
Iteration 265/1000 | Loss: 0.00073724
Iteration 266/1000 | Loss: 0.00069518
Iteration 267/1000 | Loss: 0.00061713
Iteration 268/1000 | Loss: 0.00067991
Iteration 269/1000 | Loss: 0.00084629
Iteration 270/1000 | Loss: 0.00073838
Iteration 271/1000 | Loss: 0.00079749
Iteration 272/1000 | Loss: 0.00078346
Iteration 273/1000 | Loss: 0.00111139
Iteration 274/1000 | Loss: 0.00086040
Iteration 275/1000 | Loss: 0.00053556
Iteration 276/1000 | Loss: 0.00071487
Iteration 277/1000 | Loss: 0.00063851
Iteration 278/1000 | Loss: 0.00064236
Iteration 279/1000 | Loss: 0.00044755
Iteration 280/1000 | Loss: 0.00083191
Iteration 281/1000 | Loss: 0.00059476
Iteration 282/1000 | Loss: 0.00055981
Iteration 283/1000 | Loss: 0.00053178
Iteration 284/1000 | Loss: 0.00063704
Iteration 285/1000 | Loss: 0.00075886
Iteration 286/1000 | Loss: 0.00139833
Iteration 287/1000 | Loss: 0.00234603
Iteration 288/1000 | Loss: 0.00118104
Iteration 289/1000 | Loss: 0.00071181
Iteration 290/1000 | Loss: 0.00048651
Iteration 291/1000 | Loss: 0.00041609
Iteration 292/1000 | Loss: 0.00069221
Iteration 293/1000 | Loss: 0.00077942
Iteration 294/1000 | Loss: 0.00088428
Iteration 295/1000 | Loss: 0.00077432
Iteration 296/1000 | Loss: 0.00061089
Iteration 297/1000 | Loss: 0.00074265
Iteration 298/1000 | Loss: 0.00076299
Iteration 299/1000 | Loss: 0.00084861
Iteration 300/1000 | Loss: 0.00100054
Iteration 301/1000 | Loss: 0.00100463
Iteration 302/1000 | Loss: 0.00090574
Iteration 303/1000 | Loss: 0.00084740
Iteration 304/1000 | Loss: 0.00083482
Iteration 305/1000 | Loss: 0.00072196
Iteration 306/1000 | Loss: 0.00083248
Iteration 307/1000 | Loss: 0.00075684
Iteration 308/1000 | Loss: 0.00084997
Iteration 309/1000 | Loss: 0.00082032
Iteration 310/1000 | Loss: 0.00075945
Iteration 311/1000 | Loss: 0.00077151
Iteration 312/1000 | Loss: 0.00079224
Iteration 313/1000 | Loss: 0.00076976
Iteration 314/1000 | Loss: 0.00077259
Iteration 315/1000 | Loss: 0.00103716
Iteration 316/1000 | Loss: 0.00084209
Iteration 317/1000 | Loss: 0.00074815
Iteration 318/1000 | Loss: 0.00088085
Iteration 319/1000 | Loss: 0.00074067
Iteration 320/1000 | Loss: 0.00079236
Iteration 321/1000 | Loss: 0.00067775
Iteration 322/1000 | Loss: 0.00084660
Iteration 323/1000 | Loss: 0.00073984
Iteration 324/1000 | Loss: 0.00092221
Iteration 325/1000 | Loss: 0.00092421
Iteration 326/1000 | Loss: 0.00106246
Iteration 327/1000 | Loss: 0.00089098
Iteration 328/1000 | Loss: 0.00062137
Iteration 329/1000 | Loss: 0.00047982
Iteration 330/1000 | Loss: 0.00067734
Iteration 331/1000 | Loss: 0.00084425
Iteration 332/1000 | Loss: 0.00103102
Iteration 333/1000 | Loss: 0.00083945
Iteration 334/1000 | Loss: 0.00084085
Iteration 335/1000 | Loss: 0.00057517
Iteration 336/1000 | Loss: 0.00067749
Iteration 337/1000 | Loss: 0.00044046
Iteration 338/1000 | Loss: 0.00052546
Iteration 339/1000 | Loss: 0.00075944
Iteration 340/1000 | Loss: 0.00055919
Iteration 341/1000 | Loss: 0.00083782
Iteration 342/1000 | Loss: 0.00060809
Iteration 343/1000 | Loss: 0.00075557
Iteration 344/1000 | Loss: 0.00068470
Iteration 345/1000 | Loss: 0.00077735
Iteration 346/1000 | Loss: 0.00060952
Iteration 347/1000 | Loss: 0.00084469
Iteration 348/1000 | Loss: 0.00095902
Iteration 349/1000 | Loss: 0.00053832
Iteration 350/1000 | Loss: 0.00059015
Iteration 351/1000 | Loss: 0.00062000
Iteration 352/1000 | Loss: 0.00075250
Iteration 353/1000 | Loss: 0.00084216
Iteration 354/1000 | Loss: 0.00082336
Iteration 355/1000 | Loss: 0.00090163
Iteration 356/1000 | Loss: 0.00059079
Iteration 357/1000 | Loss: 0.00072316
Iteration 358/1000 | Loss: 0.00098644
Iteration 359/1000 | Loss: 0.00065790
Iteration 360/1000 | Loss: 0.00113883
Iteration 361/1000 | Loss: 0.00085558
Iteration 362/1000 | Loss: 0.00059708
Iteration 363/1000 | Loss: 0.00076330
Iteration 364/1000 | Loss: 0.00097389
Iteration 365/1000 | Loss: 0.00070212
Iteration 366/1000 | Loss: 0.00083693
Iteration 367/1000 | Loss: 0.00086471
Iteration 368/1000 | Loss: 0.00099521
Iteration 369/1000 | Loss: 0.00054601
Iteration 370/1000 | Loss: 0.00077440
Iteration 371/1000 | Loss: 0.00065119
Iteration 372/1000 | Loss: 0.00072961
Iteration 373/1000 | Loss: 0.00066598
Iteration 374/1000 | Loss: 0.00062953
Iteration 375/1000 | Loss: 0.00069216
Iteration 376/1000 | Loss: 0.00067942
Iteration 377/1000 | Loss: 0.00068551
Iteration 378/1000 | Loss: 0.00068636
Iteration 379/1000 | Loss: 0.00064141
Iteration 380/1000 | Loss: 0.00075294
Iteration 381/1000 | Loss: 0.00065649
Iteration 382/1000 | Loss: 0.00073117
Iteration 383/1000 | Loss: 0.00122531
Iteration 384/1000 | Loss: 0.00043445
Iteration 385/1000 | Loss: 0.00052709
Iteration 386/1000 | Loss: 0.00061577
Iteration 387/1000 | Loss: 0.00033011
Iteration 388/1000 | Loss: 0.00051941
Iteration 389/1000 | Loss: 0.00054048
Iteration 390/1000 | Loss: 0.00084363
Iteration 391/1000 | Loss: 0.00034515
Iteration 392/1000 | Loss: 0.00031424
Iteration 393/1000 | Loss: 0.00036152
Iteration 394/1000 | Loss: 0.00036141
Iteration 395/1000 | Loss: 0.00040096
Iteration 396/1000 | Loss: 0.00039667
Iteration 397/1000 | Loss: 0.00040420
Iteration 398/1000 | Loss: 0.00035226
Iteration 399/1000 | Loss: 0.00034082
Iteration 400/1000 | Loss: 0.00072098
Iteration 401/1000 | Loss: 0.00046533
Iteration 402/1000 | Loss: 0.00048157
Iteration 403/1000 | Loss: 0.00034427
Iteration 404/1000 | Loss: 0.00041585
Iteration 405/1000 | Loss: 0.00051718
Iteration 406/1000 | Loss: 0.00049344
Iteration 407/1000 | Loss: 0.00058818
Iteration 408/1000 | Loss: 0.00085870
Iteration 409/1000 | Loss: 0.00043737
Iteration 410/1000 | Loss: 0.00057575
Iteration 411/1000 | Loss: 0.00059100
Iteration 412/1000 | Loss: 0.00051967
Iteration 413/1000 | Loss: 0.00050592
Iteration 414/1000 | Loss: 0.00036530
Iteration 415/1000 | Loss: 0.00030314
Iteration 416/1000 | Loss: 0.00051011
Iteration 417/1000 | Loss: 0.00050366
Iteration 418/1000 | Loss: 0.00051347
Iteration 419/1000 | Loss: 0.00045880
Iteration 420/1000 | Loss: 0.00098133
Iteration 421/1000 | Loss: 0.00082110
Iteration 422/1000 | Loss: 0.00095061
Iteration 423/1000 | Loss: 0.00086732
Iteration 424/1000 | Loss: 0.00051251
Iteration 425/1000 | Loss: 0.00045911
Iteration 426/1000 | Loss: 0.00115724
Iteration 427/1000 | Loss: 0.00067033
Iteration 428/1000 | Loss: 0.00032247
Iteration 429/1000 | Loss: 0.00030185
Iteration 430/1000 | Loss: 0.00017701
Iteration 431/1000 | Loss: 0.00041569
Iteration 432/1000 | Loss: 0.00033739
Iteration 433/1000 | Loss: 0.00051806
Iteration 434/1000 | Loss: 0.00087289
Iteration 435/1000 | Loss: 0.00060471
Iteration 436/1000 | Loss: 0.00085416
Iteration 437/1000 | Loss: 0.00057615
Iteration 438/1000 | Loss: 0.00069746
Iteration 439/1000 | Loss: 0.00056876
Iteration 440/1000 | Loss: 0.00083895
Iteration 441/1000 | Loss: 0.00051741
Iteration 442/1000 | Loss: 0.00037322
Iteration 443/1000 | Loss: 0.00033047
Iteration 444/1000 | Loss: 0.00036395
Iteration 445/1000 | Loss: 0.00036455
Iteration 446/1000 | Loss: 0.00060254
Iteration 447/1000 | Loss: 0.00041022
Iteration 448/1000 | Loss: 0.00032195
Iteration 449/1000 | Loss: 0.00043794
Iteration 450/1000 | Loss: 0.00032857
Iteration 451/1000 | Loss: 0.00033240
Iteration 452/1000 | Loss: 0.00044373
Iteration 453/1000 | Loss: 0.00036213
Iteration 454/1000 | Loss: 0.00061509
Iteration 455/1000 | Loss: 0.00063652
Iteration 456/1000 | Loss: 0.00095458
Iteration 457/1000 | Loss: 0.00071871
Iteration 458/1000 | Loss: 0.00032550
Iteration 459/1000 | Loss: 0.00034424
Iteration 460/1000 | Loss: 0.00035828
Iteration 461/1000 | Loss: 0.00031637
Iteration 462/1000 | Loss: 0.00034012
Iteration 463/1000 | Loss: 0.00026738
Iteration 464/1000 | Loss: 0.00018785
Iteration 465/1000 | Loss: 0.00029373
Iteration 466/1000 | Loss: 0.00033118
Iteration 467/1000 | Loss: 0.00033875
Iteration 468/1000 | Loss: 0.00045292
Iteration 469/1000 | Loss: 0.00038375
Iteration 470/1000 | Loss: 0.00044710
Iteration 471/1000 | Loss: 0.00041289
Iteration 472/1000 | Loss: 0.00091437
Iteration 473/1000 | Loss: 0.00124132
Iteration 474/1000 | Loss: 0.00081838
Iteration 475/1000 | Loss: 0.00041484
Iteration 476/1000 | Loss: 0.00044424
Iteration 477/1000 | Loss: 0.00019728
Iteration 478/1000 | Loss: 0.00034971
Iteration 479/1000 | Loss: 0.00027276
Iteration 480/1000 | Loss: 0.00016767
Iteration 481/1000 | Loss: 0.00036935
Iteration 482/1000 | Loss: 0.00028876
Iteration 483/1000 | Loss: 0.00017529
Iteration 484/1000 | Loss: 0.00054571
Iteration 485/1000 | Loss: 0.00028158
Iteration 486/1000 | Loss: 0.00017839
Iteration 487/1000 | Loss: 0.00034714
Iteration 488/1000 | Loss: 0.00017867
Iteration 489/1000 | Loss: 0.00023122
Iteration 490/1000 | Loss: 0.00024361
Iteration 491/1000 | Loss: 0.00028310
Iteration 492/1000 | Loss: 0.00042661
Iteration 493/1000 | Loss: 0.00020875
Iteration 494/1000 | Loss: 0.00029763
Iteration 495/1000 | Loss: 0.00031819
Iteration 496/1000 | Loss: 0.00029711
Iteration 497/1000 | Loss: 0.00030886
Iteration 498/1000 | Loss: 0.00028703
Iteration 499/1000 | Loss: 0.00027407
Iteration 500/1000 | Loss: 0.00027816
Iteration 501/1000 | Loss: 0.00028241
Iteration 502/1000 | Loss: 0.00073672
Iteration 503/1000 | Loss: 0.00063880
Iteration 504/1000 | Loss: 0.00030855
Iteration 505/1000 | Loss: 0.00024744
Iteration 506/1000 | Loss: 0.00013321
Iteration 507/1000 | Loss: 0.00009453
Iteration 508/1000 | Loss: 0.00010559
Iteration 509/1000 | Loss: 0.00008621
Iteration 510/1000 | Loss: 0.00010716
Iteration 511/1000 | Loss: 0.00018463
Iteration 512/1000 | Loss: 0.00011385
Iteration 513/1000 | Loss: 0.00011603
Iteration 514/1000 | Loss: 0.00009042
Iteration 515/1000 | Loss: 0.00011783
Iteration 516/1000 | Loss: 0.00005571
Iteration 517/1000 | Loss: 0.00010683
Iteration 518/1000 | Loss: 0.00009485
Iteration 519/1000 | Loss: 0.00011553
Iteration 520/1000 | Loss: 0.00011802
Iteration 521/1000 | Loss: 0.00021257
Iteration 522/1000 | Loss: 0.00012229
Iteration 523/1000 | Loss: 0.00012756
Iteration 524/1000 | Loss: 0.00011446
Iteration 525/1000 | Loss: 0.00012508
Iteration 526/1000 | Loss: 0.00013565
Iteration 527/1000 | Loss: 0.00028024
Iteration 528/1000 | Loss: 0.00005681
Iteration 529/1000 | Loss: 0.00009146
Iteration 530/1000 | Loss: 0.00007893
Iteration 531/1000 | Loss: 0.00014462
Iteration 532/1000 | Loss: 0.00008462
Iteration 533/1000 | Loss: 0.00007170
Iteration 534/1000 | Loss: 0.00020679
Iteration 535/1000 | Loss: 0.00008016
Iteration 536/1000 | Loss: 0.00010003
Iteration 537/1000 | Loss: 0.00014177
Iteration 538/1000 | Loss: 0.00001818
Iteration 539/1000 | Loss: 0.00001601
Iteration 540/1000 | Loss: 0.00001427
Iteration 541/1000 | Loss: 0.00053709
Iteration 542/1000 | Loss: 0.00034804
Iteration 543/1000 | Loss: 0.00036455
Iteration 544/1000 | Loss: 0.00007417
Iteration 545/1000 | Loss: 0.00001883
Iteration 546/1000 | Loss: 0.00001321
Iteration 547/1000 | Loss: 0.00001294
Iteration 548/1000 | Loss: 0.00004074
Iteration 549/1000 | Loss: 0.00000989
Iteration 550/1000 | Loss: 0.00000938
Iteration 551/1000 | Loss: 0.00000885
Iteration 552/1000 | Loss: 0.00004319
Iteration 553/1000 | Loss: 0.00001503
Iteration 554/1000 | Loss: 0.00000809
Iteration 555/1000 | Loss: 0.00000804
Iteration 556/1000 | Loss: 0.00000803
Iteration 557/1000 | Loss: 0.00000802
Iteration 558/1000 | Loss: 0.00000801
Iteration 559/1000 | Loss: 0.00000800
Iteration 560/1000 | Loss: 0.00000797
Iteration 561/1000 | Loss: 0.00000790
Iteration 562/1000 | Loss: 0.00000770
Iteration 563/1000 | Loss: 0.00000769
Iteration 564/1000 | Loss: 0.00000767
Iteration 565/1000 | Loss: 0.00000766
Iteration 566/1000 | Loss: 0.00000765
Iteration 567/1000 | Loss: 0.00000760
Iteration 568/1000 | Loss: 0.00000760
Iteration 569/1000 | Loss: 0.00000758
Iteration 570/1000 | Loss: 0.00000758
Iteration 571/1000 | Loss: 0.00000758
Iteration 572/1000 | Loss: 0.00000757
Iteration 573/1000 | Loss: 0.00000757
Iteration 574/1000 | Loss: 0.00000757
Iteration 575/1000 | Loss: 0.00000756
Iteration 576/1000 | Loss: 0.00000756
Iteration 577/1000 | Loss: 0.00000755
Iteration 578/1000 | Loss: 0.00000754
Iteration 579/1000 | Loss: 0.00000754
Iteration 580/1000 | Loss: 0.00000754
Iteration 581/1000 | Loss: 0.00000754
Iteration 582/1000 | Loss: 0.00000753
Iteration 583/1000 | Loss: 0.00000753
Iteration 584/1000 | Loss: 0.00000753
Iteration 585/1000 | Loss: 0.00000752
Iteration 586/1000 | Loss: 0.00000752
Iteration 587/1000 | Loss: 0.00000752
Iteration 588/1000 | Loss: 0.00000752
Iteration 589/1000 | Loss: 0.00000752
Iteration 590/1000 | Loss: 0.00000752
Iteration 591/1000 | Loss: 0.00000752
Iteration 592/1000 | Loss: 0.00000751
Iteration 593/1000 | Loss: 0.00000751
Iteration 594/1000 | Loss: 0.00000751
Iteration 595/1000 | Loss: 0.00000751
Iteration 596/1000 | Loss: 0.00000750
Iteration 597/1000 | Loss: 0.00000750
Iteration 598/1000 | Loss: 0.00000750
Iteration 599/1000 | Loss: 0.00000749
Iteration 600/1000 | Loss: 0.00000749
Iteration 601/1000 | Loss: 0.00000749
Iteration 602/1000 | Loss: 0.00000749
Iteration 603/1000 | Loss: 0.00000749
Iteration 604/1000 | Loss: 0.00000749
Iteration 605/1000 | Loss: 0.00000749
Iteration 606/1000 | Loss: 0.00000749
Iteration 607/1000 | Loss: 0.00000749
Iteration 608/1000 | Loss: 0.00000748
Iteration 609/1000 | Loss: 0.00000748
Iteration 610/1000 | Loss: 0.00000748
Iteration 611/1000 | Loss: 0.00000748
Iteration 612/1000 | Loss: 0.00000748
Iteration 613/1000 | Loss: 0.00000748
Iteration 614/1000 | Loss: 0.00000748
Iteration 615/1000 | Loss: 0.00000748
Iteration 616/1000 | Loss: 0.00000748
Iteration 617/1000 | Loss: 0.00000748
Iteration 618/1000 | Loss: 0.00000747
Iteration 619/1000 | Loss: 0.00000747
Iteration 620/1000 | Loss: 0.00000747
Iteration 621/1000 | Loss: 0.00000747
Iteration 622/1000 | Loss: 0.00000747
Iteration 623/1000 | Loss: 0.00000747
Iteration 624/1000 | Loss: 0.00000747
Iteration 625/1000 | Loss: 0.00000747
Iteration 626/1000 | Loss: 0.00000747
Iteration 627/1000 | Loss: 0.00000747
Iteration 628/1000 | Loss: 0.00000747
Iteration 629/1000 | Loss: 0.00000747
Iteration 630/1000 | Loss: 0.00000747
Iteration 631/1000 | Loss: 0.00000747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 631. Stopping optimization.
Last 5 losses: [7.471116077795159e-06, 7.471116077795159e-06, 7.471116077795159e-06, 7.471116077795159e-06, 7.471116077795159e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.471116077795159e-06

Optimization complete. Final v2v error: 2.3855700492858887 mm

Highest mean error: 3.3612513542175293 mm for frame 66

Lowest mean error: 2.2788851261138916 mm for frame 147

Saving results

Total time: 867.4762446880341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447683
Iteration 2/25 | Loss: 0.00130108
Iteration 3/25 | Loss: 0.00121135
Iteration 4/25 | Loss: 0.00119762
Iteration 5/25 | Loss: 0.00119411
Iteration 6/25 | Loss: 0.00119367
Iteration 7/25 | Loss: 0.00119367
Iteration 8/25 | Loss: 0.00119367
Iteration 9/25 | Loss: 0.00119367
Iteration 10/25 | Loss: 0.00119367
Iteration 11/25 | Loss: 0.00119367
Iteration 12/25 | Loss: 0.00119367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011936709051951766, 0.0011936709051951766, 0.0011936709051951766, 0.0011936709051951766, 0.0011936709051951766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011936709051951766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37760639
Iteration 2/25 | Loss: 0.00122216
Iteration 3/25 | Loss: 0.00122216
Iteration 4/25 | Loss: 0.00122216
Iteration 5/25 | Loss: 0.00122216
Iteration 6/25 | Loss: 0.00122216
Iteration 7/25 | Loss: 0.00122216
Iteration 8/25 | Loss: 0.00122216
Iteration 9/25 | Loss: 0.00122216
Iteration 10/25 | Loss: 0.00122216
Iteration 11/25 | Loss: 0.00122216
Iteration 12/25 | Loss: 0.00122216
Iteration 13/25 | Loss: 0.00122216
Iteration 14/25 | Loss: 0.00122216
Iteration 15/25 | Loss: 0.00122216
Iteration 16/25 | Loss: 0.00122216
Iteration 17/25 | Loss: 0.00122216
Iteration 18/25 | Loss: 0.00122216
Iteration 19/25 | Loss: 0.00122216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001222156803123653, 0.001222156803123653, 0.001222156803123653, 0.001222156803123653, 0.001222156803123653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001222156803123653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122216
Iteration 2/1000 | Loss: 0.00002565
Iteration 3/1000 | Loss: 0.00001928
Iteration 4/1000 | Loss: 0.00001752
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001594
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001390
Iteration 22/1000 | Loss: 0.00001390
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001389
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001376
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001356
Iteration 42/1000 | Loss: 0.00001356
Iteration 43/1000 | Loss: 0.00001356
Iteration 44/1000 | Loss: 0.00001356
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001351
Iteration 50/1000 | Loss: 0.00001351
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001349
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001347
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001337
Iteration 83/1000 | Loss: 0.00001337
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001334
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001330
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001330
Iteration 141/1000 | Loss: 0.00001330
Iteration 142/1000 | Loss: 0.00001330
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001330
Iteration 146/1000 | Loss: 0.00001330
Iteration 147/1000 | Loss: 0.00001330
Iteration 148/1000 | Loss: 0.00001330
Iteration 149/1000 | Loss: 0.00001330
Iteration 150/1000 | Loss: 0.00001330
Iteration 151/1000 | Loss: 0.00001330
Iteration 152/1000 | Loss: 0.00001330
Iteration 153/1000 | Loss: 0.00001330
Iteration 154/1000 | Loss: 0.00001330
Iteration 155/1000 | Loss: 0.00001330
Iteration 156/1000 | Loss: 0.00001330
Iteration 157/1000 | Loss: 0.00001330
Iteration 158/1000 | Loss: 0.00001330
Iteration 159/1000 | Loss: 0.00001330
Iteration 160/1000 | Loss: 0.00001330
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001330
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001330
Iteration 167/1000 | Loss: 0.00001330
Iteration 168/1000 | Loss: 0.00001330
Iteration 169/1000 | Loss: 0.00001330
Iteration 170/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.3296743418322876e-05, 1.3296743418322876e-05, 1.3296743418322876e-05, 1.3296743418322876e-05, 1.3296743418322876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3296743418322876e-05

Optimization complete. Final v2v error: 3.091449022293091 mm

Highest mean error: 3.69651198387146 mm for frame 94

Lowest mean error: 2.728637218475342 mm for frame 30

Saving results

Total time: 40.40010643005371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911649
Iteration 2/25 | Loss: 0.00142641
Iteration 3/25 | Loss: 0.00127511
Iteration 4/25 | Loss: 0.00125121
Iteration 5/25 | Loss: 0.00124430
Iteration 6/25 | Loss: 0.00124264
Iteration 7/25 | Loss: 0.00124264
Iteration 8/25 | Loss: 0.00124264
Iteration 9/25 | Loss: 0.00124264
Iteration 10/25 | Loss: 0.00124264
Iteration 11/25 | Loss: 0.00124264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012426385655999184, 0.0012426385655999184, 0.0012426385655999184, 0.0012426385655999184, 0.0012426385655999184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012426385655999184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42176640
Iteration 2/25 | Loss: 0.00158497
Iteration 3/25 | Loss: 0.00158497
Iteration 4/25 | Loss: 0.00158497
Iteration 5/25 | Loss: 0.00158497
Iteration 6/25 | Loss: 0.00158497
Iteration 7/25 | Loss: 0.00158497
Iteration 8/25 | Loss: 0.00158497
Iteration 9/25 | Loss: 0.00158497
Iteration 10/25 | Loss: 0.00158497
Iteration 11/25 | Loss: 0.00158497
Iteration 12/25 | Loss: 0.00158497
Iteration 13/25 | Loss: 0.00158497
Iteration 14/25 | Loss: 0.00158497
Iteration 15/25 | Loss: 0.00158497
Iteration 16/25 | Loss: 0.00158497
Iteration 17/25 | Loss: 0.00158497
Iteration 18/25 | Loss: 0.00158497
Iteration 19/25 | Loss: 0.00158497
Iteration 20/25 | Loss: 0.00158497
Iteration 21/25 | Loss: 0.00158497
Iteration 22/25 | Loss: 0.00158497
Iteration 23/25 | Loss: 0.00158497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015849691117182374, 0.0015849691117182374, 0.0015849691117182374, 0.0015849691117182374, 0.0015849691117182374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015849691117182374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158497
Iteration 2/1000 | Loss: 0.00004888
Iteration 3/1000 | Loss: 0.00002860
Iteration 4/1000 | Loss: 0.00002291
Iteration 5/1000 | Loss: 0.00002117
Iteration 6/1000 | Loss: 0.00002023
Iteration 7/1000 | Loss: 0.00001963
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001884
Iteration 11/1000 | Loss: 0.00001863
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001838
Iteration 14/1000 | Loss: 0.00001824
Iteration 15/1000 | Loss: 0.00001822
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001800
Iteration 20/1000 | Loss: 0.00001799
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001798
Iteration 23/1000 | Loss: 0.00001797
Iteration 24/1000 | Loss: 0.00001787
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001777
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001774
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001773
Iteration 41/1000 | Loss: 0.00001773
Iteration 42/1000 | Loss: 0.00001772
Iteration 43/1000 | Loss: 0.00001768
Iteration 44/1000 | Loss: 0.00001767
Iteration 45/1000 | Loss: 0.00001767
Iteration 46/1000 | Loss: 0.00001767
Iteration 47/1000 | Loss: 0.00001767
Iteration 48/1000 | Loss: 0.00001767
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001766
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001751
Iteration 77/1000 | Loss: 0.00001751
Iteration 78/1000 | Loss: 0.00001750
Iteration 79/1000 | Loss: 0.00001750
Iteration 80/1000 | Loss: 0.00001750
Iteration 81/1000 | Loss: 0.00001749
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001749
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001745
Iteration 91/1000 | Loss: 0.00001745
Iteration 92/1000 | Loss: 0.00001745
Iteration 93/1000 | Loss: 0.00001745
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001744
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001741
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001739
Iteration 121/1000 | Loss: 0.00001739
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001738
Iteration 125/1000 | Loss: 0.00001738
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001735
Iteration 132/1000 | Loss: 0.00001735
Iteration 133/1000 | Loss: 0.00001735
Iteration 134/1000 | Loss: 0.00001735
Iteration 135/1000 | Loss: 0.00001734
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001733
Iteration 141/1000 | Loss: 0.00001733
Iteration 142/1000 | Loss: 0.00001733
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Iteration 145/1000 | Loss: 0.00001731
Iteration 146/1000 | Loss: 0.00001730
Iteration 147/1000 | Loss: 0.00001729
Iteration 148/1000 | Loss: 0.00001729
Iteration 149/1000 | Loss: 0.00001729
Iteration 150/1000 | Loss: 0.00001729
Iteration 151/1000 | Loss: 0.00001729
Iteration 152/1000 | Loss: 0.00001729
Iteration 153/1000 | Loss: 0.00001729
Iteration 154/1000 | Loss: 0.00001729
Iteration 155/1000 | Loss: 0.00001729
Iteration 156/1000 | Loss: 0.00001729
Iteration 157/1000 | Loss: 0.00001728
Iteration 158/1000 | Loss: 0.00001728
Iteration 159/1000 | Loss: 0.00001728
Iteration 160/1000 | Loss: 0.00001728
Iteration 161/1000 | Loss: 0.00001727
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001726
Iteration 165/1000 | Loss: 0.00001726
Iteration 166/1000 | Loss: 0.00001725
Iteration 167/1000 | Loss: 0.00001725
Iteration 168/1000 | Loss: 0.00001724
Iteration 169/1000 | Loss: 0.00001724
Iteration 170/1000 | Loss: 0.00001724
Iteration 171/1000 | Loss: 0.00001723
Iteration 172/1000 | Loss: 0.00001723
Iteration 173/1000 | Loss: 0.00001723
Iteration 174/1000 | Loss: 0.00001723
Iteration 175/1000 | Loss: 0.00001723
Iteration 176/1000 | Loss: 0.00001722
Iteration 177/1000 | Loss: 0.00001722
Iteration 178/1000 | Loss: 0.00001722
Iteration 179/1000 | Loss: 0.00001722
Iteration 180/1000 | Loss: 0.00001722
Iteration 181/1000 | Loss: 0.00001721
Iteration 182/1000 | Loss: 0.00001721
Iteration 183/1000 | Loss: 0.00001721
Iteration 184/1000 | Loss: 0.00001721
Iteration 185/1000 | Loss: 0.00001721
Iteration 186/1000 | Loss: 0.00001721
Iteration 187/1000 | Loss: 0.00001721
Iteration 188/1000 | Loss: 0.00001721
Iteration 189/1000 | Loss: 0.00001721
Iteration 190/1000 | Loss: 0.00001721
Iteration 191/1000 | Loss: 0.00001721
Iteration 192/1000 | Loss: 0.00001721
Iteration 193/1000 | Loss: 0.00001721
Iteration 194/1000 | Loss: 0.00001721
Iteration 195/1000 | Loss: 0.00001721
Iteration 196/1000 | Loss: 0.00001720
Iteration 197/1000 | Loss: 0.00001720
Iteration 198/1000 | Loss: 0.00001720
Iteration 199/1000 | Loss: 0.00001720
Iteration 200/1000 | Loss: 0.00001720
Iteration 201/1000 | Loss: 0.00001720
Iteration 202/1000 | Loss: 0.00001720
Iteration 203/1000 | Loss: 0.00001719
Iteration 204/1000 | Loss: 0.00001719
Iteration 205/1000 | Loss: 0.00001719
Iteration 206/1000 | Loss: 0.00001719
Iteration 207/1000 | Loss: 0.00001719
Iteration 208/1000 | Loss: 0.00001719
Iteration 209/1000 | Loss: 0.00001719
Iteration 210/1000 | Loss: 0.00001718
Iteration 211/1000 | Loss: 0.00001718
Iteration 212/1000 | Loss: 0.00001718
Iteration 213/1000 | Loss: 0.00001718
Iteration 214/1000 | Loss: 0.00001718
Iteration 215/1000 | Loss: 0.00001718
Iteration 216/1000 | Loss: 0.00001718
Iteration 217/1000 | Loss: 0.00001718
Iteration 218/1000 | Loss: 0.00001718
Iteration 219/1000 | Loss: 0.00001718
Iteration 220/1000 | Loss: 0.00001718
Iteration 221/1000 | Loss: 0.00001718
Iteration 222/1000 | Loss: 0.00001718
Iteration 223/1000 | Loss: 0.00001718
Iteration 224/1000 | Loss: 0.00001718
Iteration 225/1000 | Loss: 0.00001718
Iteration 226/1000 | Loss: 0.00001718
Iteration 227/1000 | Loss: 0.00001718
Iteration 228/1000 | Loss: 0.00001718
Iteration 229/1000 | Loss: 0.00001718
Iteration 230/1000 | Loss: 0.00001718
Iteration 231/1000 | Loss: 0.00001717
Iteration 232/1000 | Loss: 0.00001717
Iteration 233/1000 | Loss: 0.00001717
Iteration 234/1000 | Loss: 0.00001717
Iteration 235/1000 | Loss: 0.00001717
Iteration 236/1000 | Loss: 0.00001717
Iteration 237/1000 | Loss: 0.00001717
Iteration 238/1000 | Loss: 0.00001717
Iteration 239/1000 | Loss: 0.00001717
Iteration 240/1000 | Loss: 0.00001717
Iteration 241/1000 | Loss: 0.00001717
Iteration 242/1000 | Loss: 0.00001716
Iteration 243/1000 | Loss: 0.00001716
Iteration 244/1000 | Loss: 0.00001716
Iteration 245/1000 | Loss: 0.00001716
Iteration 246/1000 | Loss: 0.00001716
Iteration 247/1000 | Loss: 0.00001716
Iteration 248/1000 | Loss: 0.00001716
Iteration 249/1000 | Loss: 0.00001716
Iteration 250/1000 | Loss: 0.00001716
Iteration 251/1000 | Loss: 0.00001716
Iteration 252/1000 | Loss: 0.00001716
Iteration 253/1000 | Loss: 0.00001716
Iteration 254/1000 | Loss: 0.00001716
Iteration 255/1000 | Loss: 0.00001716
Iteration 256/1000 | Loss: 0.00001716
Iteration 257/1000 | Loss: 0.00001716
Iteration 258/1000 | Loss: 0.00001716
Iteration 259/1000 | Loss: 0.00001716
Iteration 260/1000 | Loss: 0.00001716
Iteration 261/1000 | Loss: 0.00001716
Iteration 262/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.716182305244729e-05, 1.716182305244729e-05, 1.716182305244729e-05, 1.716182305244729e-05, 1.716182305244729e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.716182305244729e-05

Optimization complete. Final v2v error: 3.495012044906616 mm

Highest mean error: 3.827392816543579 mm for frame 168

Lowest mean error: 3.091475009918213 mm for frame 0

Saving results

Total time: 48.98262023925781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443864
Iteration 2/25 | Loss: 0.00134041
Iteration 3/25 | Loss: 0.00124094
Iteration 4/25 | Loss: 0.00122680
Iteration 5/25 | Loss: 0.00122308
Iteration 6/25 | Loss: 0.00122308
Iteration 7/25 | Loss: 0.00122308
Iteration 8/25 | Loss: 0.00122308
Iteration 9/25 | Loss: 0.00122308
Iteration 10/25 | Loss: 0.00122308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012230770662426949, 0.0012230770662426949, 0.0012230770662426949, 0.0012230770662426949, 0.0012230770662426949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012230770662426949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32523763
Iteration 2/25 | Loss: 0.00129711
Iteration 3/25 | Loss: 0.00129710
Iteration 4/25 | Loss: 0.00129710
Iteration 5/25 | Loss: 0.00129710
Iteration 6/25 | Loss: 0.00129710
Iteration 7/25 | Loss: 0.00129710
Iteration 8/25 | Loss: 0.00129710
Iteration 9/25 | Loss: 0.00129710
Iteration 10/25 | Loss: 0.00129710
Iteration 11/25 | Loss: 0.00129710
Iteration 12/25 | Loss: 0.00129710
Iteration 13/25 | Loss: 0.00129710
Iteration 14/25 | Loss: 0.00129710
Iteration 15/25 | Loss: 0.00129710
Iteration 16/25 | Loss: 0.00129710
Iteration 17/25 | Loss: 0.00129710
Iteration 18/25 | Loss: 0.00129710
Iteration 19/25 | Loss: 0.00129710
Iteration 20/25 | Loss: 0.00129710
Iteration 21/25 | Loss: 0.00129710
Iteration 22/25 | Loss: 0.00129710
Iteration 23/25 | Loss: 0.00129710
Iteration 24/25 | Loss: 0.00129710
Iteration 25/25 | Loss: 0.00129710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129710
Iteration 2/1000 | Loss: 0.00002999
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00002234
Iteration 5/1000 | Loss: 0.00002148
Iteration 6/1000 | Loss: 0.00002060
Iteration 7/1000 | Loss: 0.00002004
Iteration 8/1000 | Loss: 0.00001970
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001893
Iteration 11/1000 | Loss: 0.00001874
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001834
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001823
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001821
Iteration 23/1000 | Loss: 0.00001820
Iteration 24/1000 | Loss: 0.00001820
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001818
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001814
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001806
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001800
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001796
Iteration 44/1000 | Loss: 0.00001796
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001794
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001791
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001789
Iteration 56/1000 | Loss: 0.00001788
Iteration 57/1000 | Loss: 0.00001788
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001785
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00001785
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001784
Iteration 80/1000 | Loss: 0.00001784
Iteration 81/1000 | Loss: 0.00001784
Iteration 82/1000 | Loss: 0.00001784
Iteration 83/1000 | Loss: 0.00001784
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001783
Iteration 87/1000 | Loss: 0.00001783
Iteration 88/1000 | Loss: 0.00001783
Iteration 89/1000 | Loss: 0.00001783
Iteration 90/1000 | Loss: 0.00001783
Iteration 91/1000 | Loss: 0.00001783
Iteration 92/1000 | Loss: 0.00001783
Iteration 93/1000 | Loss: 0.00001783
Iteration 94/1000 | Loss: 0.00001783
Iteration 95/1000 | Loss: 0.00001783
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00001783
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001783
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.783222978701815e-05, 1.783222978701815e-05, 1.783222978701815e-05, 1.783222978701815e-05, 1.783222978701815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.783222978701815e-05

Optimization complete. Final v2v error: 3.601485252380371 mm

Highest mean error: 4.152281284332275 mm for frame 143

Lowest mean error: 3.168830633163452 mm for frame 8

Saving results

Total time: 40.629093408584595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806747
Iteration 2/25 | Loss: 0.00146600
Iteration 3/25 | Loss: 0.00131437
Iteration 4/25 | Loss: 0.00129913
Iteration 5/25 | Loss: 0.00129461
Iteration 6/25 | Loss: 0.00129328
Iteration 7/25 | Loss: 0.00129328
Iteration 8/25 | Loss: 0.00129328
Iteration 9/25 | Loss: 0.00129328
Iteration 10/25 | Loss: 0.00129328
Iteration 11/25 | Loss: 0.00129328
Iteration 12/25 | Loss: 0.00129328
Iteration 13/25 | Loss: 0.00129328
Iteration 14/25 | Loss: 0.00129328
Iteration 15/25 | Loss: 0.00129328
Iteration 16/25 | Loss: 0.00129328
Iteration 17/25 | Loss: 0.00129328
Iteration 18/25 | Loss: 0.00129328
Iteration 19/25 | Loss: 0.00129328
Iteration 20/25 | Loss: 0.00129328
Iteration 21/25 | Loss: 0.00129328
Iteration 22/25 | Loss: 0.00129328
Iteration 23/25 | Loss: 0.00129328
Iteration 24/25 | Loss: 0.00129328
Iteration 25/25 | Loss: 0.00129328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19466043
Iteration 2/25 | Loss: 0.00230401
Iteration 3/25 | Loss: 0.00230401
Iteration 4/25 | Loss: 0.00230400
Iteration 5/25 | Loss: 0.00230400
Iteration 6/25 | Loss: 0.00230400
Iteration 7/25 | Loss: 0.00230400
Iteration 8/25 | Loss: 0.00230400
Iteration 9/25 | Loss: 0.00230400
Iteration 10/25 | Loss: 0.00230400
Iteration 11/25 | Loss: 0.00230400
Iteration 12/25 | Loss: 0.00230400
Iteration 13/25 | Loss: 0.00230400
Iteration 14/25 | Loss: 0.00230400
Iteration 15/25 | Loss: 0.00230400
Iteration 16/25 | Loss: 0.00230400
Iteration 17/25 | Loss: 0.00230400
Iteration 18/25 | Loss: 0.00230400
Iteration 19/25 | Loss: 0.00230400
Iteration 20/25 | Loss: 0.00230400
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002304001711308956, 0.002304001711308956, 0.002304001711308956, 0.002304001711308956, 0.002304001711308956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002304001711308956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230400
Iteration 2/1000 | Loss: 0.00006995
Iteration 3/1000 | Loss: 0.00004375
Iteration 4/1000 | Loss: 0.00003895
Iteration 5/1000 | Loss: 0.00003711
Iteration 6/1000 | Loss: 0.00003616
Iteration 7/1000 | Loss: 0.00003525
Iteration 8/1000 | Loss: 0.00003412
Iteration 9/1000 | Loss: 0.00003341
Iteration 10/1000 | Loss: 0.00003296
Iteration 11/1000 | Loss: 0.00003256
Iteration 12/1000 | Loss: 0.00003226
Iteration 13/1000 | Loss: 0.00003204
Iteration 14/1000 | Loss: 0.00003187
Iteration 15/1000 | Loss: 0.00003186
Iteration 16/1000 | Loss: 0.00003172
Iteration 17/1000 | Loss: 0.00003157
Iteration 18/1000 | Loss: 0.00003156
Iteration 19/1000 | Loss: 0.00003155
Iteration 20/1000 | Loss: 0.00003152
Iteration 21/1000 | Loss: 0.00003152
Iteration 22/1000 | Loss: 0.00003151
Iteration 23/1000 | Loss: 0.00003151
Iteration 24/1000 | Loss: 0.00003149
Iteration 25/1000 | Loss: 0.00003148
Iteration 26/1000 | Loss: 0.00003148
Iteration 27/1000 | Loss: 0.00003147
Iteration 28/1000 | Loss: 0.00003147
Iteration 29/1000 | Loss: 0.00003145
Iteration 30/1000 | Loss: 0.00003145
Iteration 31/1000 | Loss: 0.00003145
Iteration 32/1000 | Loss: 0.00003144
Iteration 33/1000 | Loss: 0.00003144
Iteration 34/1000 | Loss: 0.00003144
Iteration 35/1000 | Loss: 0.00003144
Iteration 36/1000 | Loss: 0.00003144
Iteration 37/1000 | Loss: 0.00003144
Iteration 38/1000 | Loss: 0.00003143
Iteration 39/1000 | Loss: 0.00003143
Iteration 40/1000 | Loss: 0.00003143
Iteration 41/1000 | Loss: 0.00003143
Iteration 42/1000 | Loss: 0.00003143
Iteration 43/1000 | Loss: 0.00003143
Iteration 44/1000 | Loss: 0.00003143
Iteration 45/1000 | Loss: 0.00003143
Iteration 46/1000 | Loss: 0.00003142
Iteration 47/1000 | Loss: 0.00003142
Iteration 48/1000 | Loss: 0.00003141
Iteration 49/1000 | Loss: 0.00003140
Iteration 50/1000 | Loss: 0.00003139
Iteration 51/1000 | Loss: 0.00003139
Iteration 52/1000 | Loss: 0.00003139
Iteration 53/1000 | Loss: 0.00003139
Iteration 54/1000 | Loss: 0.00003138
Iteration 55/1000 | Loss: 0.00003136
Iteration 56/1000 | Loss: 0.00003136
Iteration 57/1000 | Loss: 0.00003133
Iteration 58/1000 | Loss: 0.00003133
Iteration 59/1000 | Loss: 0.00003133
Iteration 60/1000 | Loss: 0.00003133
Iteration 61/1000 | Loss: 0.00003133
Iteration 62/1000 | Loss: 0.00003132
Iteration 63/1000 | Loss: 0.00003132
Iteration 64/1000 | Loss: 0.00003132
Iteration 65/1000 | Loss: 0.00003132
Iteration 66/1000 | Loss: 0.00003132
Iteration 67/1000 | Loss: 0.00003132
Iteration 68/1000 | Loss: 0.00003132
Iteration 69/1000 | Loss: 0.00003131
Iteration 70/1000 | Loss: 0.00003131
Iteration 71/1000 | Loss: 0.00003131
Iteration 72/1000 | Loss: 0.00003131
Iteration 73/1000 | Loss: 0.00003131
Iteration 74/1000 | Loss: 0.00003131
Iteration 75/1000 | Loss: 0.00003131
Iteration 76/1000 | Loss: 0.00003131
Iteration 77/1000 | Loss: 0.00003130
Iteration 78/1000 | Loss: 0.00003130
Iteration 79/1000 | Loss: 0.00003130
Iteration 80/1000 | Loss: 0.00003130
Iteration 81/1000 | Loss: 0.00003130
Iteration 82/1000 | Loss: 0.00003130
Iteration 83/1000 | Loss: 0.00003130
Iteration 84/1000 | Loss: 0.00003130
Iteration 85/1000 | Loss: 0.00003130
Iteration 86/1000 | Loss: 0.00003130
Iteration 87/1000 | Loss: 0.00003130
Iteration 88/1000 | Loss: 0.00003130
Iteration 89/1000 | Loss: 0.00003130
Iteration 90/1000 | Loss: 0.00003130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.1302748539019376e-05, 3.1302748539019376e-05, 3.1302748539019376e-05, 3.1302748539019376e-05, 3.1302748539019376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1302748539019376e-05

Optimization complete. Final v2v error: 4.507780075073242 mm

Highest mean error: 4.574223041534424 mm for frame 36

Lowest mean error: 4.372437000274658 mm for frame 86

Saving results

Total time: 36.208800315856934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832151
Iteration 2/25 | Loss: 0.00159480
Iteration 3/25 | Loss: 0.00132975
Iteration 4/25 | Loss: 0.00130802
Iteration 5/25 | Loss: 0.00130468
Iteration 6/25 | Loss: 0.00130462
Iteration 7/25 | Loss: 0.00130462
Iteration 8/25 | Loss: 0.00130462
Iteration 9/25 | Loss: 0.00130462
Iteration 10/25 | Loss: 0.00130462
Iteration 11/25 | Loss: 0.00130462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013046162202954292, 0.0013046162202954292, 0.0013046162202954292, 0.0013046162202954292, 0.0013046162202954292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013046162202954292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09255278
Iteration 2/25 | Loss: 0.00105140
Iteration 3/25 | Loss: 0.00105138
Iteration 4/25 | Loss: 0.00105138
Iteration 5/25 | Loss: 0.00105138
Iteration 6/25 | Loss: 0.00105138
Iteration 7/25 | Loss: 0.00105138
Iteration 8/25 | Loss: 0.00105138
Iteration 9/25 | Loss: 0.00105138
Iteration 10/25 | Loss: 0.00105138
Iteration 11/25 | Loss: 0.00105138
Iteration 12/25 | Loss: 0.00105138
Iteration 13/25 | Loss: 0.00105138
Iteration 14/25 | Loss: 0.00105138
Iteration 15/25 | Loss: 0.00105138
Iteration 16/25 | Loss: 0.00105138
Iteration 17/25 | Loss: 0.00105138
Iteration 18/25 | Loss: 0.00105138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010513782035559416, 0.0010513782035559416, 0.0010513782035559416, 0.0010513782035559416, 0.0010513782035559416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010513782035559416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105138
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00002602
Iteration 4/1000 | Loss: 0.00002326
Iteration 5/1000 | Loss: 0.00002158
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00001963
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001828
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001758
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001746
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001742
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001742
Iteration 28/1000 | Loss: 0.00001742
Iteration 29/1000 | Loss: 0.00001742
Iteration 30/1000 | Loss: 0.00001742
Iteration 31/1000 | Loss: 0.00001742
Iteration 32/1000 | Loss: 0.00001741
Iteration 33/1000 | Loss: 0.00001741
Iteration 34/1000 | Loss: 0.00001741
Iteration 35/1000 | Loss: 0.00001740
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001739
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001737
Iteration 46/1000 | Loss: 0.00001737
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001734
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001732
Iteration 94/1000 | Loss: 0.00001732
Iteration 95/1000 | Loss: 0.00001732
Iteration 96/1000 | Loss: 0.00001732
Iteration 97/1000 | Loss: 0.00001732
Iteration 98/1000 | Loss: 0.00001732
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001732
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.731748488964513e-05, 1.731748488964513e-05, 1.731748488964513e-05, 1.731748488964513e-05, 1.731748488964513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.731748488964513e-05

Optimization complete. Final v2v error: 3.496063709259033 mm

Highest mean error: 3.9161429405212402 mm for frame 0

Lowest mean error: 3.0722923278808594 mm for frame 78

Saving results

Total time: 36.05977964401245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881789
Iteration 2/25 | Loss: 0.00134640
Iteration 3/25 | Loss: 0.00124947
Iteration 4/25 | Loss: 0.00123124
Iteration 5/25 | Loss: 0.00122537
Iteration 6/25 | Loss: 0.00122379
Iteration 7/25 | Loss: 0.00122379
Iteration 8/25 | Loss: 0.00122379
Iteration 9/25 | Loss: 0.00122379
Iteration 10/25 | Loss: 0.00122379
Iteration 11/25 | Loss: 0.00122379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012237938353791833, 0.0012237938353791833, 0.0012237938353791833, 0.0012237938353791833, 0.0012237938353791833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012237938353791833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33044457
Iteration 2/25 | Loss: 0.00122115
Iteration 3/25 | Loss: 0.00122113
Iteration 4/25 | Loss: 0.00122113
Iteration 5/25 | Loss: 0.00122113
Iteration 6/25 | Loss: 0.00122113
Iteration 7/25 | Loss: 0.00122113
Iteration 8/25 | Loss: 0.00122113
Iteration 9/25 | Loss: 0.00122113
Iteration 10/25 | Loss: 0.00122113
Iteration 11/25 | Loss: 0.00122113
Iteration 12/25 | Loss: 0.00122113
Iteration 13/25 | Loss: 0.00122113
Iteration 14/25 | Loss: 0.00122113
Iteration 15/25 | Loss: 0.00122113
Iteration 16/25 | Loss: 0.00122113
Iteration 17/25 | Loss: 0.00122113
Iteration 18/25 | Loss: 0.00122113
Iteration 19/25 | Loss: 0.00122113
Iteration 20/25 | Loss: 0.00122113
Iteration 21/25 | Loss: 0.00122113
Iteration 22/25 | Loss: 0.00122113
Iteration 23/25 | Loss: 0.00122113
Iteration 24/25 | Loss: 0.00122113
Iteration 25/25 | Loss: 0.00122113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122113
Iteration 2/1000 | Loss: 0.00004231
Iteration 3/1000 | Loss: 0.00002952
Iteration 4/1000 | Loss: 0.00002410
Iteration 5/1000 | Loss: 0.00002237
Iteration 6/1000 | Loss: 0.00002138
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00002005
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001871
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001863
Iteration 15/1000 | Loss: 0.00001862
Iteration 16/1000 | Loss: 0.00001853
Iteration 17/1000 | Loss: 0.00001848
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001833
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001819
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001810
Iteration 25/1000 | Loss: 0.00001808
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001803
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001793
Iteration 33/1000 | Loss: 0.00001793
Iteration 34/1000 | Loss: 0.00001793
Iteration 35/1000 | Loss: 0.00001792
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001791
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001783
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001782
Iteration 66/1000 | Loss: 0.00001782
Iteration 67/1000 | Loss: 0.00001782
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001782
Iteration 70/1000 | Loss: 0.00001782
Iteration 71/1000 | Loss: 0.00001782
Iteration 72/1000 | Loss: 0.00001781
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001780
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001779
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001775
Iteration 91/1000 | Loss: 0.00001775
Iteration 92/1000 | Loss: 0.00001775
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001765
Iteration 131/1000 | Loss: 0.00001765
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001764
Iteration 134/1000 | Loss: 0.00001764
Iteration 135/1000 | Loss: 0.00001764
Iteration 136/1000 | Loss: 0.00001764
Iteration 137/1000 | Loss: 0.00001764
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001764
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001763
Iteration 144/1000 | Loss: 0.00001763
Iteration 145/1000 | Loss: 0.00001763
Iteration 146/1000 | Loss: 0.00001763
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001762
Iteration 149/1000 | Loss: 0.00001762
Iteration 150/1000 | Loss: 0.00001762
Iteration 151/1000 | Loss: 0.00001762
Iteration 152/1000 | Loss: 0.00001762
Iteration 153/1000 | Loss: 0.00001762
Iteration 154/1000 | Loss: 0.00001762
Iteration 155/1000 | Loss: 0.00001762
Iteration 156/1000 | Loss: 0.00001761
Iteration 157/1000 | Loss: 0.00001761
Iteration 158/1000 | Loss: 0.00001761
Iteration 159/1000 | Loss: 0.00001761
Iteration 160/1000 | Loss: 0.00001761
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001760
Iteration 167/1000 | Loss: 0.00001760
Iteration 168/1000 | Loss: 0.00001759
Iteration 169/1000 | Loss: 0.00001759
Iteration 170/1000 | Loss: 0.00001759
Iteration 171/1000 | Loss: 0.00001759
Iteration 172/1000 | Loss: 0.00001759
Iteration 173/1000 | Loss: 0.00001759
Iteration 174/1000 | Loss: 0.00001759
Iteration 175/1000 | Loss: 0.00001759
Iteration 176/1000 | Loss: 0.00001759
Iteration 177/1000 | Loss: 0.00001759
Iteration 178/1000 | Loss: 0.00001759
Iteration 179/1000 | Loss: 0.00001759
Iteration 180/1000 | Loss: 0.00001759
Iteration 181/1000 | Loss: 0.00001758
Iteration 182/1000 | Loss: 0.00001758
Iteration 183/1000 | Loss: 0.00001758
Iteration 184/1000 | Loss: 0.00001758
Iteration 185/1000 | Loss: 0.00001758
Iteration 186/1000 | Loss: 0.00001758
Iteration 187/1000 | Loss: 0.00001758
Iteration 188/1000 | Loss: 0.00001758
Iteration 189/1000 | Loss: 0.00001758
Iteration 190/1000 | Loss: 0.00001758
Iteration 191/1000 | Loss: 0.00001758
Iteration 192/1000 | Loss: 0.00001758
Iteration 193/1000 | Loss: 0.00001758
Iteration 194/1000 | Loss: 0.00001758
Iteration 195/1000 | Loss: 0.00001758
Iteration 196/1000 | Loss: 0.00001758
Iteration 197/1000 | Loss: 0.00001758
Iteration 198/1000 | Loss: 0.00001757
Iteration 199/1000 | Loss: 0.00001757
Iteration 200/1000 | Loss: 0.00001757
Iteration 201/1000 | Loss: 0.00001757
Iteration 202/1000 | Loss: 0.00001757
Iteration 203/1000 | Loss: 0.00001757
Iteration 204/1000 | Loss: 0.00001757
Iteration 205/1000 | Loss: 0.00001757
Iteration 206/1000 | Loss: 0.00001757
Iteration 207/1000 | Loss: 0.00001757
Iteration 208/1000 | Loss: 0.00001757
Iteration 209/1000 | Loss: 0.00001757
Iteration 210/1000 | Loss: 0.00001757
Iteration 211/1000 | Loss: 0.00001757
Iteration 212/1000 | Loss: 0.00001757
Iteration 213/1000 | Loss: 0.00001757
Iteration 214/1000 | Loss: 0.00001757
Iteration 215/1000 | Loss: 0.00001757
Iteration 216/1000 | Loss: 0.00001757
Iteration 217/1000 | Loss: 0.00001757
Iteration 218/1000 | Loss: 0.00001757
Iteration 219/1000 | Loss: 0.00001757
Iteration 220/1000 | Loss: 0.00001757
Iteration 221/1000 | Loss: 0.00001757
Iteration 222/1000 | Loss: 0.00001757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.7573305740370415e-05, 1.7573305740370415e-05, 1.7573305740370415e-05, 1.7573305740370415e-05, 1.7573305740370415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7573305740370415e-05

Optimization complete. Final v2v error: 3.5527451038360596 mm

Highest mean error: 5.195676326751709 mm for frame 70

Lowest mean error: 3.078859567642212 mm for frame 45

Saving results

Total time: 45.818031787872314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801514
Iteration 2/25 | Loss: 0.00163954
Iteration 3/25 | Loss: 0.00134380
Iteration 4/25 | Loss: 0.00131307
Iteration 5/25 | Loss: 0.00130421
Iteration 6/25 | Loss: 0.00130272
Iteration 7/25 | Loss: 0.00130272
Iteration 8/25 | Loss: 0.00130272
Iteration 9/25 | Loss: 0.00130272
Iteration 10/25 | Loss: 0.00130272
Iteration 11/25 | Loss: 0.00130272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001302723423577845, 0.001302723423577845, 0.001302723423577845, 0.001302723423577845, 0.001302723423577845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302723423577845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00483811
Iteration 2/25 | Loss: 0.00125605
Iteration 3/25 | Loss: 0.00125605
Iteration 4/25 | Loss: 0.00125605
Iteration 5/25 | Loss: 0.00125605
Iteration 6/25 | Loss: 0.00125605
Iteration 7/25 | Loss: 0.00125605
Iteration 8/25 | Loss: 0.00125604
Iteration 9/25 | Loss: 0.00125604
Iteration 10/25 | Loss: 0.00125604
Iteration 11/25 | Loss: 0.00125604
Iteration 12/25 | Loss: 0.00125604
Iteration 13/25 | Loss: 0.00125604
Iteration 14/25 | Loss: 0.00125604
Iteration 15/25 | Loss: 0.00125604
Iteration 16/25 | Loss: 0.00125604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012560441391542554, 0.0012560441391542554, 0.0012560441391542554, 0.0012560441391542554, 0.0012560441391542554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012560441391542554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125604
Iteration 2/1000 | Loss: 0.00008801
Iteration 3/1000 | Loss: 0.00005205
Iteration 4/1000 | Loss: 0.00003979
Iteration 5/1000 | Loss: 0.00003736
Iteration 6/1000 | Loss: 0.00003550
Iteration 7/1000 | Loss: 0.00003442
Iteration 8/1000 | Loss: 0.00003366
Iteration 9/1000 | Loss: 0.00003300
Iteration 10/1000 | Loss: 0.00003262
Iteration 11/1000 | Loss: 0.00003213
Iteration 12/1000 | Loss: 0.00003180
Iteration 13/1000 | Loss: 0.00003156
Iteration 14/1000 | Loss: 0.00003132
Iteration 15/1000 | Loss: 0.00003110
Iteration 16/1000 | Loss: 0.00003091
Iteration 17/1000 | Loss: 0.00003071
Iteration 18/1000 | Loss: 0.00003056
Iteration 19/1000 | Loss: 0.00003051
Iteration 20/1000 | Loss: 0.00003051
Iteration 21/1000 | Loss: 0.00003048
Iteration 22/1000 | Loss: 0.00003045
Iteration 23/1000 | Loss: 0.00003045
Iteration 24/1000 | Loss: 0.00003044
Iteration 25/1000 | Loss: 0.00003044
Iteration 26/1000 | Loss: 0.00003039
Iteration 27/1000 | Loss: 0.00003039
Iteration 28/1000 | Loss: 0.00003038
Iteration 29/1000 | Loss: 0.00003038
Iteration 30/1000 | Loss: 0.00003038
Iteration 31/1000 | Loss: 0.00003037
Iteration 32/1000 | Loss: 0.00003037
Iteration 33/1000 | Loss: 0.00003032
Iteration 34/1000 | Loss: 0.00003032
Iteration 35/1000 | Loss: 0.00003032
Iteration 36/1000 | Loss: 0.00003032
Iteration 37/1000 | Loss: 0.00003032
Iteration 38/1000 | Loss: 0.00003028
Iteration 39/1000 | Loss: 0.00003028
Iteration 40/1000 | Loss: 0.00003027
Iteration 41/1000 | Loss: 0.00003026
Iteration 42/1000 | Loss: 0.00003026
Iteration 43/1000 | Loss: 0.00003025
Iteration 44/1000 | Loss: 0.00003025
Iteration 45/1000 | Loss: 0.00003022
Iteration 46/1000 | Loss: 0.00003022
Iteration 47/1000 | Loss: 0.00003021
Iteration 48/1000 | Loss: 0.00003021
Iteration 49/1000 | Loss: 0.00003020
Iteration 50/1000 | Loss: 0.00003019
Iteration 51/1000 | Loss: 0.00003019
Iteration 52/1000 | Loss: 0.00003019
Iteration 53/1000 | Loss: 0.00003018
Iteration 54/1000 | Loss: 0.00003018
Iteration 55/1000 | Loss: 0.00003016
Iteration 56/1000 | Loss: 0.00003016
Iteration 57/1000 | Loss: 0.00003014
Iteration 58/1000 | Loss: 0.00003014
Iteration 59/1000 | Loss: 0.00003013
Iteration 60/1000 | Loss: 0.00003013
Iteration 61/1000 | Loss: 0.00003011
Iteration 62/1000 | Loss: 0.00003011
Iteration 63/1000 | Loss: 0.00003011
Iteration 64/1000 | Loss: 0.00003011
Iteration 65/1000 | Loss: 0.00003010
Iteration 66/1000 | Loss: 0.00003010
Iteration 67/1000 | Loss: 0.00003010
Iteration 68/1000 | Loss: 0.00003010
Iteration 69/1000 | Loss: 0.00003010
Iteration 70/1000 | Loss: 0.00003009
Iteration 71/1000 | Loss: 0.00003009
Iteration 72/1000 | Loss: 0.00003009
Iteration 73/1000 | Loss: 0.00003009
Iteration 74/1000 | Loss: 0.00003009
Iteration 75/1000 | Loss: 0.00003009
Iteration 76/1000 | Loss: 0.00003009
Iteration 77/1000 | Loss: 0.00003009
Iteration 78/1000 | Loss: 0.00003009
Iteration 79/1000 | Loss: 0.00003009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.0085853722994216e-05, 3.0085853722994216e-05, 3.0085853722994216e-05, 3.0085853722994216e-05, 3.0085853722994216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0085853722994216e-05

Optimization complete. Final v2v error: 4.389313220977783 mm

Highest mean error: 5.813196659088135 mm for frame 154

Lowest mean error: 3.740013837814331 mm for frame 126

Saving results

Total time: 47.670427083969116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953398
Iteration 2/25 | Loss: 0.00376184
Iteration 3/25 | Loss: 0.00223124
Iteration 4/25 | Loss: 0.00197081
Iteration 5/25 | Loss: 0.00185888
Iteration 6/25 | Loss: 0.00181261
Iteration 7/25 | Loss: 0.00192148
Iteration 8/25 | Loss: 0.00185969
Iteration 9/25 | Loss: 0.00168174
Iteration 10/25 | Loss: 0.00160586
Iteration 11/25 | Loss: 0.00153446
Iteration 12/25 | Loss: 0.00152151
Iteration 13/25 | Loss: 0.00148958
Iteration 14/25 | Loss: 0.00147918
Iteration 15/25 | Loss: 0.00145790
Iteration 16/25 | Loss: 0.00145553
Iteration 17/25 | Loss: 0.00145083
Iteration 18/25 | Loss: 0.00144646
Iteration 19/25 | Loss: 0.00144311
Iteration 20/25 | Loss: 0.00144283
Iteration 21/25 | Loss: 0.00144207
Iteration 22/25 | Loss: 0.00144197
Iteration 23/25 | Loss: 0.00144240
Iteration 24/25 | Loss: 0.00144185
Iteration 25/25 | Loss: 0.00144185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29627180
Iteration 2/25 | Loss: 0.00469560
Iteration 3/25 | Loss: 0.00469559
Iteration 4/25 | Loss: 0.00426097
Iteration 5/25 | Loss: 0.00426054
Iteration 6/25 | Loss: 0.00426054
Iteration 7/25 | Loss: 0.00426025
Iteration 8/25 | Loss: 0.00426025
Iteration 9/25 | Loss: 0.00426025
Iteration 10/25 | Loss: 0.00426025
Iteration 11/25 | Loss: 0.00426025
Iteration 12/25 | Loss: 0.00426025
Iteration 13/25 | Loss: 0.00426025
Iteration 14/25 | Loss: 0.00426025
Iteration 15/25 | Loss: 0.00426025
Iteration 16/25 | Loss: 0.00426025
Iteration 17/25 | Loss: 0.00426025
Iteration 18/25 | Loss: 0.00426025
Iteration 19/25 | Loss: 0.00426025
Iteration 20/25 | Loss: 0.00426025
Iteration 21/25 | Loss: 0.00426025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004260247107595205, 0.004260247107595205, 0.004260247107595205, 0.004260247107595205, 0.004260247107595205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004260247107595205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00426025
Iteration 2/1000 | Loss: 0.00141668
Iteration 3/1000 | Loss: 0.00051895
Iteration 4/1000 | Loss: 0.00113529
Iteration 5/1000 | Loss: 0.00035311
Iteration 6/1000 | Loss: 0.00087436
Iteration 7/1000 | Loss: 0.00019864
Iteration 8/1000 | Loss: 0.00020779
Iteration 9/1000 | Loss: 0.00009601
Iteration 10/1000 | Loss: 0.00021676
Iteration 11/1000 | Loss: 0.00003986
Iteration 12/1000 | Loss: 0.00004449
Iteration 13/1000 | Loss: 0.00003763
Iteration 14/1000 | Loss: 0.00007473
Iteration 15/1000 | Loss: 0.00004610
Iteration 16/1000 | Loss: 0.00009716
Iteration 17/1000 | Loss: 0.00002589
Iteration 18/1000 | Loss: 0.00002103
Iteration 19/1000 | Loss: 0.00005604
Iteration 20/1000 | Loss: 0.00003383
Iteration 21/1000 | Loss: 0.00002229
Iteration 22/1000 | Loss: 0.00004041
Iteration 23/1000 | Loss: 0.00001916
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00002675
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00002632
Iteration 28/1000 | Loss: 0.00003698
Iteration 29/1000 | Loss: 0.00002014
Iteration 30/1000 | Loss: 0.00001831
Iteration 31/1000 | Loss: 0.00001830
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001822
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001822
Iteration 40/1000 | Loss: 0.00001822
Iteration 41/1000 | Loss: 0.00001822
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001821
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001820
Iteration 46/1000 | Loss: 0.00001820
Iteration 47/1000 | Loss: 0.00001818
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00002413
Iteration 50/1000 | Loss: 0.00003080
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001935
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001795
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001807
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00001791
Iteration 64/1000 | Loss: 0.00001791
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001790
Iteration 73/1000 | Loss: 0.00001790
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001790
Iteration 78/1000 | Loss: 0.00001790
Iteration 79/1000 | Loss: 0.00001790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.790295027603861e-05, 1.790295027603861e-05, 1.790295027603861e-05, 1.790295027603861e-05, 1.790295027603861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.790295027603861e-05

Optimization complete. Final v2v error: 3.1516737937927246 mm

Highest mean error: 11.307526588439941 mm for frame 9

Lowest mean error: 2.674739360809326 mm for frame 95

Saving results

Total time: 109.72324323654175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391714
Iteration 2/25 | Loss: 0.00121685
Iteration 3/25 | Loss: 0.00116113
Iteration 4/25 | Loss: 0.00115506
Iteration 5/25 | Loss: 0.00115462
Iteration 6/25 | Loss: 0.00115462
Iteration 7/25 | Loss: 0.00115462
Iteration 8/25 | Loss: 0.00115462
Iteration 9/25 | Loss: 0.00115462
Iteration 10/25 | Loss: 0.00115462
Iteration 11/25 | Loss: 0.00115462
Iteration 12/25 | Loss: 0.00115462
Iteration 13/25 | Loss: 0.00115462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001154620898887515, 0.001154620898887515, 0.001154620898887515, 0.001154620898887515, 0.001154620898887515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001154620898887515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36446810
Iteration 2/25 | Loss: 0.00121712
Iteration 3/25 | Loss: 0.00121711
Iteration 4/25 | Loss: 0.00121711
Iteration 5/25 | Loss: 0.00121711
Iteration 6/25 | Loss: 0.00121711
Iteration 7/25 | Loss: 0.00121711
Iteration 8/25 | Loss: 0.00121711
Iteration 9/25 | Loss: 0.00121711
Iteration 10/25 | Loss: 0.00121711
Iteration 11/25 | Loss: 0.00121711
Iteration 12/25 | Loss: 0.00121711
Iteration 13/25 | Loss: 0.00121711
Iteration 14/25 | Loss: 0.00121711
Iteration 15/25 | Loss: 0.00121711
Iteration 16/25 | Loss: 0.00121711
Iteration 17/25 | Loss: 0.00121711
Iteration 18/25 | Loss: 0.00121711
Iteration 19/25 | Loss: 0.00121711
Iteration 20/25 | Loss: 0.00121711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012171122943982482, 0.0012171122943982482, 0.0012171122943982482, 0.0012171122943982482, 0.0012171122943982482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012171122943982482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121711
Iteration 2/1000 | Loss: 0.00001743
Iteration 3/1000 | Loss: 0.00001372
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001178
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001117
Iteration 8/1000 | Loss: 0.00001074
Iteration 9/1000 | Loss: 0.00001042
Iteration 10/1000 | Loss: 0.00001027
Iteration 11/1000 | Loss: 0.00001015
Iteration 12/1000 | Loss: 0.00001006
Iteration 13/1000 | Loss: 0.00001002
Iteration 14/1000 | Loss: 0.00000995
Iteration 15/1000 | Loss: 0.00000985
Iteration 16/1000 | Loss: 0.00000981
Iteration 17/1000 | Loss: 0.00000979
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000974
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000961
Iteration 22/1000 | Loss: 0.00000956
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000952
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000951
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000945
Iteration 29/1000 | Loss: 0.00000944
Iteration 30/1000 | Loss: 0.00000944
Iteration 31/1000 | Loss: 0.00000943
Iteration 32/1000 | Loss: 0.00000943
Iteration 33/1000 | Loss: 0.00000942
Iteration 34/1000 | Loss: 0.00000941
Iteration 35/1000 | Loss: 0.00000940
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000935
Iteration 43/1000 | Loss: 0.00000934
Iteration 44/1000 | Loss: 0.00000933
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000932
Iteration 47/1000 | Loss: 0.00000931
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000930
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000928
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000927
Iteration 56/1000 | Loss: 0.00000926
Iteration 57/1000 | Loss: 0.00000925
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000923
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000922
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000921
Iteration 64/1000 | Loss: 0.00000921
Iteration 65/1000 | Loss: 0.00000921
Iteration 66/1000 | Loss: 0.00000920
Iteration 67/1000 | Loss: 0.00000920
Iteration 68/1000 | Loss: 0.00000920
Iteration 69/1000 | Loss: 0.00000918
Iteration 70/1000 | Loss: 0.00000917
Iteration 71/1000 | Loss: 0.00000917
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000916
Iteration 74/1000 | Loss: 0.00000916
Iteration 75/1000 | Loss: 0.00000916
Iteration 76/1000 | Loss: 0.00000916
Iteration 77/1000 | Loss: 0.00000915
Iteration 78/1000 | Loss: 0.00000915
Iteration 79/1000 | Loss: 0.00000915
Iteration 80/1000 | Loss: 0.00000914
Iteration 81/1000 | Loss: 0.00000914
Iteration 82/1000 | Loss: 0.00000913
Iteration 83/1000 | Loss: 0.00000913
Iteration 84/1000 | Loss: 0.00000913
Iteration 85/1000 | Loss: 0.00000912
Iteration 86/1000 | Loss: 0.00000911
Iteration 87/1000 | Loss: 0.00000911
Iteration 88/1000 | Loss: 0.00000910
Iteration 89/1000 | Loss: 0.00000910
Iteration 90/1000 | Loss: 0.00000910
Iteration 91/1000 | Loss: 0.00000910
Iteration 92/1000 | Loss: 0.00000910
Iteration 93/1000 | Loss: 0.00000910
Iteration 94/1000 | Loss: 0.00000910
Iteration 95/1000 | Loss: 0.00000910
Iteration 96/1000 | Loss: 0.00000910
Iteration 97/1000 | Loss: 0.00000910
Iteration 98/1000 | Loss: 0.00000910
Iteration 99/1000 | Loss: 0.00000909
Iteration 100/1000 | Loss: 0.00000909
Iteration 101/1000 | Loss: 0.00000909
Iteration 102/1000 | Loss: 0.00000909
Iteration 103/1000 | Loss: 0.00000908
Iteration 104/1000 | Loss: 0.00000908
Iteration 105/1000 | Loss: 0.00000907
Iteration 106/1000 | Loss: 0.00000907
Iteration 107/1000 | Loss: 0.00000907
Iteration 108/1000 | Loss: 0.00000907
Iteration 109/1000 | Loss: 0.00000907
Iteration 110/1000 | Loss: 0.00000907
Iteration 111/1000 | Loss: 0.00000907
Iteration 112/1000 | Loss: 0.00000907
Iteration 113/1000 | Loss: 0.00000907
Iteration 114/1000 | Loss: 0.00000907
Iteration 115/1000 | Loss: 0.00000907
Iteration 116/1000 | Loss: 0.00000907
Iteration 117/1000 | Loss: 0.00000907
Iteration 118/1000 | Loss: 0.00000907
Iteration 119/1000 | Loss: 0.00000907
Iteration 120/1000 | Loss: 0.00000907
Iteration 121/1000 | Loss: 0.00000907
Iteration 122/1000 | Loss: 0.00000907
Iteration 123/1000 | Loss: 0.00000907
Iteration 124/1000 | Loss: 0.00000907
Iteration 125/1000 | Loss: 0.00000907
Iteration 126/1000 | Loss: 0.00000907
Iteration 127/1000 | Loss: 0.00000907
Iteration 128/1000 | Loss: 0.00000907
Iteration 129/1000 | Loss: 0.00000907
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000907
Iteration 134/1000 | Loss: 0.00000907
Iteration 135/1000 | Loss: 0.00000907
Iteration 136/1000 | Loss: 0.00000907
Iteration 137/1000 | Loss: 0.00000907
Iteration 138/1000 | Loss: 0.00000907
Iteration 139/1000 | Loss: 0.00000907
Iteration 140/1000 | Loss: 0.00000907
Iteration 141/1000 | Loss: 0.00000907
Iteration 142/1000 | Loss: 0.00000907
Iteration 143/1000 | Loss: 0.00000907
Iteration 144/1000 | Loss: 0.00000907
Iteration 145/1000 | Loss: 0.00000907
Iteration 146/1000 | Loss: 0.00000907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [9.06775767361978e-06, 9.06775767361978e-06, 9.06775767361978e-06, 9.06775767361978e-06, 9.06775767361978e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.06775767361978e-06

Optimization complete. Final v2v error: 2.634518623352051 mm

Highest mean error: 2.816408157348633 mm for frame 128

Lowest mean error: 2.533724546432495 mm for frame 1

Saving results

Total time: 40.72654414176941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038062
Iteration 2/25 | Loss: 0.01038062
Iteration 3/25 | Loss: 0.01038062
Iteration 4/25 | Loss: 0.01038062
Iteration 5/25 | Loss: 0.01038062
Iteration 6/25 | Loss: 0.01038062
Iteration 7/25 | Loss: 0.01038062
Iteration 8/25 | Loss: 0.01038062
Iteration 9/25 | Loss: 0.01038061
Iteration 10/25 | Loss: 0.01038061
Iteration 11/25 | Loss: 0.01038061
Iteration 12/25 | Loss: 0.01038061
Iteration 13/25 | Loss: 0.01038061
Iteration 14/25 | Loss: 0.01038061
Iteration 15/25 | Loss: 0.01038061
Iteration 16/25 | Loss: 0.01038061
Iteration 17/25 | Loss: 0.01038061
Iteration 18/25 | Loss: 0.01038060
Iteration 19/25 | Loss: 0.01038060
Iteration 20/25 | Loss: 0.01038060
Iteration 21/25 | Loss: 0.01038060
Iteration 22/25 | Loss: 0.01038060
Iteration 23/25 | Loss: 0.01038060
Iteration 24/25 | Loss: 0.01038060
Iteration 25/25 | Loss: 0.01038060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27885509
Iteration 2/25 | Loss: 0.17763776
Iteration 3/25 | Loss: 0.17639092
Iteration 4/25 | Loss: 0.17101336
Iteration 5/25 | Loss: 0.17101333
Iteration 6/25 | Loss: 0.17101331
Iteration 7/25 | Loss: 0.17101331
Iteration 8/25 | Loss: 0.17101330
Iteration 9/25 | Loss: 0.17101330
Iteration 10/25 | Loss: 0.17101330
Iteration 11/25 | Loss: 0.17101330
Iteration 12/25 | Loss: 0.17101330
Iteration 13/25 | Loss: 0.17101330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17101329565048218, 0.17101329565048218, 0.17101329565048218, 0.17101329565048218, 0.17101329565048218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17101329565048218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17101330
Iteration 2/1000 | Loss: 0.00264720
Iteration 3/1000 | Loss: 0.00097631
Iteration 4/1000 | Loss: 0.00062931
Iteration 5/1000 | Loss: 0.00027198
Iteration 6/1000 | Loss: 0.00044277
Iteration 7/1000 | Loss: 0.00061016
Iteration 8/1000 | Loss: 0.00027947
Iteration 9/1000 | Loss: 0.00016008
Iteration 10/1000 | Loss: 0.00008147
Iteration 11/1000 | Loss: 0.00006459
Iteration 12/1000 | Loss: 0.00003610
Iteration 13/1000 | Loss: 0.00011554
Iteration 14/1000 | Loss: 0.00003458
Iteration 15/1000 | Loss: 0.00002605
Iteration 16/1000 | Loss: 0.00002177
Iteration 17/1000 | Loss: 0.00001962
Iteration 18/1000 | Loss: 0.00001841
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00012165
Iteration 22/1000 | Loss: 0.00062109
Iteration 23/1000 | Loss: 0.00061688
Iteration 24/1000 | Loss: 0.00011882
Iteration 25/1000 | Loss: 0.00009303
Iteration 26/1000 | Loss: 0.00001595
Iteration 27/1000 | Loss: 0.00013524
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00008074
Iteration 30/1000 | Loss: 0.00027290
Iteration 31/1000 | Loss: 0.00091525
Iteration 32/1000 | Loss: 0.00013962
Iteration 33/1000 | Loss: 0.00047232
Iteration 34/1000 | Loss: 0.00049064
Iteration 35/1000 | Loss: 0.00010021
Iteration 36/1000 | Loss: 0.00004968
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00011707
Iteration 40/1000 | Loss: 0.00022560
Iteration 41/1000 | Loss: 0.00038370
Iteration 42/1000 | Loss: 0.00104941
Iteration 43/1000 | Loss: 0.00028109
Iteration 44/1000 | Loss: 0.00002283
Iteration 45/1000 | Loss: 0.00007396
Iteration 46/1000 | Loss: 0.00001531
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00021078
Iteration 49/1000 | Loss: 0.00011460
Iteration 50/1000 | Loss: 0.00007991
Iteration 51/1000 | Loss: 0.00047788
Iteration 52/1000 | Loss: 0.00013235
Iteration 53/1000 | Loss: 0.00006228
Iteration 54/1000 | Loss: 0.00003713
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00003606
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00011502
Iteration 59/1000 | Loss: 0.00006828
Iteration 60/1000 | Loss: 0.00012901
Iteration 61/1000 | Loss: 0.00004911
Iteration 62/1000 | Loss: 0.00002147
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00018545
Iteration 65/1000 | Loss: 0.00018665
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00004430
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00005383
Iteration 73/1000 | Loss: 0.00001365
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001300
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00016790
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00011015
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001270
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001260
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001260
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001252
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001252
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001252
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001252
Iteration 140/1000 | Loss: 0.00001251
Iteration 141/1000 | Loss: 0.00001251
Iteration 142/1000 | Loss: 0.00001251
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001250
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001250
Iteration 152/1000 | Loss: 0.00001250
Iteration 153/1000 | Loss: 0.00001250
Iteration 154/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2502449862950016e-05, 1.2502449862950016e-05, 1.2502449862950016e-05, 1.2502449862950016e-05, 1.2502449862950016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2502449862950016e-05

Optimization complete. Final v2v error: 2.9750218391418457 mm

Highest mean error: 3.6413426399230957 mm for frame 15

Lowest mean error: 2.6558587551116943 mm for frame 3

Saving results

Total time: 137.55670762062073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796159
Iteration 2/25 | Loss: 0.00155193
Iteration 3/25 | Loss: 0.00136464
Iteration 4/25 | Loss: 0.00135204
Iteration 5/25 | Loss: 0.00134989
Iteration 6/25 | Loss: 0.00134989
Iteration 7/25 | Loss: 0.00134989
Iteration 8/25 | Loss: 0.00134989
Iteration 9/25 | Loss: 0.00134989
Iteration 10/25 | Loss: 0.00134989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013498935149982572, 0.0013498935149982572, 0.0013498935149982572, 0.0013498935149982572, 0.0013498935149982572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013498935149982572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18895829
Iteration 2/25 | Loss: 0.00092421
Iteration 3/25 | Loss: 0.00092421
Iteration 4/25 | Loss: 0.00092421
Iteration 5/25 | Loss: 0.00092421
Iteration 6/25 | Loss: 0.00092421
Iteration 7/25 | Loss: 0.00092421
Iteration 8/25 | Loss: 0.00092421
Iteration 9/25 | Loss: 0.00092421
Iteration 10/25 | Loss: 0.00092421
Iteration 11/25 | Loss: 0.00092421
Iteration 12/25 | Loss: 0.00092421
Iteration 13/25 | Loss: 0.00092421
Iteration 14/25 | Loss: 0.00092421
Iteration 15/25 | Loss: 0.00092421
Iteration 16/25 | Loss: 0.00092421
Iteration 17/25 | Loss: 0.00092421
Iteration 18/25 | Loss: 0.00092421
Iteration 19/25 | Loss: 0.00092421
Iteration 20/25 | Loss: 0.00092421
Iteration 21/25 | Loss: 0.00092421
Iteration 22/25 | Loss: 0.00092421
Iteration 23/25 | Loss: 0.00092421
Iteration 24/25 | Loss: 0.00092421
Iteration 25/25 | Loss: 0.00092421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009242073283530772, 0.0009242073283530772, 0.0009242073283530772, 0.0009242073283530772, 0.0009242073283530772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009242073283530772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092421
Iteration 2/1000 | Loss: 0.00003284
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002307
Iteration 5/1000 | Loss: 0.00002188
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00001994
Iteration 10/1000 | Loss: 0.00001976
Iteration 11/1000 | Loss: 0.00001975
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001944
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001943
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001942
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001939
Iteration 38/1000 | Loss: 0.00001938
Iteration 39/1000 | Loss: 0.00001938
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001937
Iteration 43/1000 | Loss: 0.00001937
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001937
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001937
Iteration 63/1000 | Loss: 0.00001937
Iteration 64/1000 | Loss: 0.00001937
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001937
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.9368950233911164e-05, 1.9368950233911164e-05, 1.9368950233911164e-05, 1.9368950233911164e-05, 1.9368950233911164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9368950233911164e-05

Optimization complete. Final v2v error: 3.6978845596313477 mm

Highest mean error: 4.0279459953308105 mm for frame 10

Lowest mean error: 3.460446357727051 mm for frame 214

Saving results

Total time: 30.120317697525024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804466
Iteration 2/25 | Loss: 0.00144124
Iteration 3/25 | Loss: 0.00126976
Iteration 4/25 | Loss: 0.00126031
Iteration 5/25 | Loss: 0.00125966
Iteration 6/25 | Loss: 0.00125966
Iteration 7/25 | Loss: 0.00125966
Iteration 8/25 | Loss: 0.00125966
Iteration 9/25 | Loss: 0.00125966
Iteration 10/25 | Loss: 0.00125966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012596598826348782, 0.0012596598826348782, 0.0012596598826348782, 0.0012596598826348782, 0.0012596598826348782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012596598826348782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21764731
Iteration 2/25 | Loss: 0.00098744
Iteration 3/25 | Loss: 0.00098743
Iteration 4/25 | Loss: 0.00098743
Iteration 5/25 | Loss: 0.00098743
Iteration 6/25 | Loss: 0.00098743
Iteration 7/25 | Loss: 0.00098743
Iteration 8/25 | Loss: 0.00098743
Iteration 9/25 | Loss: 0.00098743
Iteration 10/25 | Loss: 0.00098743
Iteration 11/25 | Loss: 0.00098743
Iteration 12/25 | Loss: 0.00098743
Iteration 13/25 | Loss: 0.00098743
Iteration 14/25 | Loss: 0.00098743
Iteration 15/25 | Loss: 0.00098743
Iteration 16/25 | Loss: 0.00098743
Iteration 17/25 | Loss: 0.00098743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009874280076473951, 0.0009874280076473951, 0.0009874280076473951, 0.0009874280076473951, 0.0009874280076473951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009874280076473951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098743
Iteration 2/1000 | Loss: 0.00002817
Iteration 3/1000 | Loss: 0.00002175
Iteration 4/1000 | Loss: 0.00002039
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001832
Iteration 8/1000 | Loss: 0.00001777
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001649
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001630
Iteration 40/1000 | Loss: 0.00001630
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001630
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001625
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001616
Iteration 57/1000 | Loss: 0.00001616
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001616
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001615
Iteration 62/1000 | Loss: 0.00001615
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001614
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001613
Iteration 67/1000 | Loss: 0.00001613
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001611
Iteration 71/1000 | Loss: 0.00001611
Iteration 72/1000 | Loss: 0.00001611
Iteration 73/1000 | Loss: 0.00001611
Iteration 74/1000 | Loss: 0.00001611
Iteration 75/1000 | Loss: 0.00001611
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001610
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001609
Iteration 85/1000 | Loss: 0.00001609
Iteration 86/1000 | Loss: 0.00001609
Iteration 87/1000 | Loss: 0.00001609
Iteration 88/1000 | Loss: 0.00001609
Iteration 89/1000 | Loss: 0.00001609
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001608
Iteration 92/1000 | Loss: 0.00001608
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001608
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.6075666280812584e-05, 1.6075666280812584e-05, 1.6075666280812584e-05, 1.6075666280812584e-05, 1.6075666280812584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6075666280812584e-05

Optimization complete. Final v2v error: 3.409287452697754 mm

Highest mean error: 3.5666897296905518 mm for frame 212

Lowest mean error: 3.2746469974517822 mm for frame 62

Saving results

Total time: 38.39302396774292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067963
Iteration 2/25 | Loss: 0.00207931
Iteration 3/25 | Loss: 0.00151540
Iteration 4/25 | Loss: 0.00146218
Iteration 5/25 | Loss: 0.00144357
Iteration 6/25 | Loss: 0.00143470
Iteration 7/25 | Loss: 0.00139835
Iteration 8/25 | Loss: 0.00137695
Iteration 9/25 | Loss: 0.00137712
Iteration 10/25 | Loss: 0.00137876
Iteration 11/25 | Loss: 0.00137252
Iteration 12/25 | Loss: 0.00136553
Iteration 13/25 | Loss: 0.00135878
Iteration 14/25 | Loss: 0.00135935
Iteration 15/25 | Loss: 0.00135659
Iteration 16/25 | Loss: 0.00135471
Iteration 17/25 | Loss: 0.00135452
Iteration 18/25 | Loss: 0.00135534
Iteration 19/25 | Loss: 0.00135178
Iteration 20/25 | Loss: 0.00135136
Iteration 21/25 | Loss: 0.00135126
Iteration 22/25 | Loss: 0.00135121
Iteration 23/25 | Loss: 0.00135120
Iteration 24/25 | Loss: 0.00135120
Iteration 25/25 | Loss: 0.00135120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66215199
Iteration 2/25 | Loss: 0.00113186
Iteration 3/25 | Loss: 0.00113183
Iteration 4/25 | Loss: 0.00113183
Iteration 5/25 | Loss: 0.00113183
Iteration 6/25 | Loss: 0.00113183
Iteration 7/25 | Loss: 0.00113183
Iteration 8/25 | Loss: 0.00113183
Iteration 9/25 | Loss: 0.00113183
Iteration 10/25 | Loss: 0.00113183
Iteration 11/25 | Loss: 0.00113183
Iteration 12/25 | Loss: 0.00113183
Iteration 13/25 | Loss: 0.00113183
Iteration 14/25 | Loss: 0.00113183
Iteration 15/25 | Loss: 0.00113183
Iteration 16/25 | Loss: 0.00113183
Iteration 17/25 | Loss: 0.00113183
Iteration 18/25 | Loss: 0.00113183
Iteration 19/25 | Loss: 0.00113183
Iteration 20/25 | Loss: 0.00113183
Iteration 21/25 | Loss: 0.00113183
Iteration 22/25 | Loss: 0.00113183
Iteration 23/25 | Loss: 0.00113183
Iteration 24/25 | Loss: 0.00113183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011318274773657322, 0.0011318274773657322, 0.0011318274773657322, 0.0011318274773657322, 0.0011318274773657322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011318274773657322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113183
Iteration 2/1000 | Loss: 0.00006578
Iteration 3/1000 | Loss: 0.00004612
Iteration 4/1000 | Loss: 0.00003969
Iteration 5/1000 | Loss: 0.00003769
Iteration 6/1000 | Loss: 0.00003600
Iteration 7/1000 | Loss: 0.00003482
Iteration 8/1000 | Loss: 0.00003391
Iteration 9/1000 | Loss: 0.00003332
Iteration 10/1000 | Loss: 0.00003287
Iteration 11/1000 | Loss: 0.00003245
Iteration 12/1000 | Loss: 0.00003204
Iteration 13/1000 | Loss: 0.00003168
Iteration 14/1000 | Loss: 0.00003142
Iteration 15/1000 | Loss: 0.00003123
Iteration 16/1000 | Loss: 0.00003108
Iteration 17/1000 | Loss: 0.00003105
Iteration 18/1000 | Loss: 0.00003093
Iteration 19/1000 | Loss: 0.00003085
Iteration 20/1000 | Loss: 0.00003084
Iteration 21/1000 | Loss: 0.00003082
Iteration 22/1000 | Loss: 0.00003081
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003080
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003070
Iteration 27/1000 | Loss: 0.00003065
Iteration 28/1000 | Loss: 0.00003064
Iteration 29/1000 | Loss: 0.00003060
Iteration 30/1000 | Loss: 0.00003060
Iteration 31/1000 | Loss: 0.00003058
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003056
Iteration 34/1000 | Loss: 0.00003055
Iteration 35/1000 | Loss: 0.00003055
Iteration 36/1000 | Loss: 0.00003053
Iteration 37/1000 | Loss: 0.00003052
Iteration 38/1000 | Loss: 0.00003052
Iteration 39/1000 | Loss: 0.00003051
Iteration 40/1000 | Loss: 0.00003050
Iteration 41/1000 | Loss: 0.00003050
Iteration 42/1000 | Loss: 0.00003050
Iteration 43/1000 | Loss: 0.00003050
Iteration 44/1000 | Loss: 0.00003050
Iteration 45/1000 | Loss: 0.00003050
Iteration 46/1000 | Loss: 0.00003050
Iteration 47/1000 | Loss: 0.00003050
Iteration 48/1000 | Loss: 0.00003050
Iteration 49/1000 | Loss: 0.00003050
Iteration 50/1000 | Loss: 0.00003049
Iteration 51/1000 | Loss: 0.00003049
Iteration 52/1000 | Loss: 0.00003049
Iteration 53/1000 | Loss: 0.00003049
Iteration 54/1000 | Loss: 0.00003049
Iteration 55/1000 | Loss: 0.00003049
Iteration 56/1000 | Loss: 0.00003049
Iteration 57/1000 | Loss: 0.00003049
Iteration 58/1000 | Loss: 0.00003049
Iteration 59/1000 | Loss: 0.00003048
Iteration 60/1000 | Loss: 0.00003048
Iteration 61/1000 | Loss: 0.00003048
Iteration 62/1000 | Loss: 0.00003047
Iteration 63/1000 | Loss: 0.00003047
Iteration 64/1000 | Loss: 0.00003047
Iteration 65/1000 | Loss: 0.00003047
Iteration 66/1000 | Loss: 0.00003047
Iteration 67/1000 | Loss: 0.00003046
Iteration 68/1000 | Loss: 0.00003044
Iteration 69/1000 | Loss: 0.00003044
Iteration 70/1000 | Loss: 0.00003044
Iteration 71/1000 | Loss: 0.00003044
Iteration 72/1000 | Loss: 0.00003044
Iteration 73/1000 | Loss: 0.00003044
Iteration 74/1000 | Loss: 0.00003043
Iteration 75/1000 | Loss: 0.00003043
Iteration 76/1000 | Loss: 0.00003042
Iteration 77/1000 | Loss: 0.00003042
Iteration 78/1000 | Loss: 0.00003042
Iteration 79/1000 | Loss: 0.00003042
Iteration 80/1000 | Loss: 0.00003042
Iteration 81/1000 | Loss: 0.00003042
Iteration 82/1000 | Loss: 0.00003042
Iteration 83/1000 | Loss: 0.00003042
Iteration 84/1000 | Loss: 0.00003042
Iteration 85/1000 | Loss: 0.00003041
Iteration 86/1000 | Loss: 0.00003041
Iteration 87/1000 | Loss: 0.00003040
Iteration 88/1000 | Loss: 0.00003040
Iteration 89/1000 | Loss: 0.00003040
Iteration 90/1000 | Loss: 0.00003039
Iteration 91/1000 | Loss: 0.00003039
Iteration 92/1000 | Loss: 0.00003039
Iteration 93/1000 | Loss: 0.00003038
Iteration 94/1000 | Loss: 0.00003038
Iteration 95/1000 | Loss: 0.00003038
Iteration 96/1000 | Loss: 0.00003038
Iteration 97/1000 | Loss: 0.00003037
Iteration 98/1000 | Loss: 0.00003037
Iteration 99/1000 | Loss: 0.00003037
Iteration 100/1000 | Loss: 0.00003037
Iteration 101/1000 | Loss: 0.00003037
Iteration 102/1000 | Loss: 0.00003037
Iteration 103/1000 | Loss: 0.00003037
Iteration 104/1000 | Loss: 0.00003036
Iteration 105/1000 | Loss: 0.00003036
Iteration 106/1000 | Loss: 0.00003036
Iteration 107/1000 | Loss: 0.00003036
Iteration 108/1000 | Loss: 0.00003036
Iteration 109/1000 | Loss: 0.00003036
Iteration 110/1000 | Loss: 0.00003036
Iteration 111/1000 | Loss: 0.00003035
Iteration 112/1000 | Loss: 0.00003035
Iteration 113/1000 | Loss: 0.00003035
Iteration 114/1000 | Loss: 0.00003035
Iteration 115/1000 | Loss: 0.00003035
Iteration 116/1000 | Loss: 0.00003035
Iteration 117/1000 | Loss: 0.00003035
Iteration 118/1000 | Loss: 0.00003035
Iteration 119/1000 | Loss: 0.00003035
Iteration 120/1000 | Loss: 0.00003035
Iteration 121/1000 | Loss: 0.00003035
Iteration 122/1000 | Loss: 0.00003035
Iteration 123/1000 | Loss: 0.00003034
Iteration 124/1000 | Loss: 0.00003034
Iteration 125/1000 | Loss: 0.00003034
Iteration 126/1000 | Loss: 0.00003034
Iteration 127/1000 | Loss: 0.00003033
Iteration 128/1000 | Loss: 0.00003033
Iteration 129/1000 | Loss: 0.00003033
Iteration 130/1000 | Loss: 0.00003033
Iteration 131/1000 | Loss: 0.00003032
Iteration 132/1000 | Loss: 0.00003032
Iteration 133/1000 | Loss: 0.00003032
Iteration 134/1000 | Loss: 0.00003032
Iteration 135/1000 | Loss: 0.00003032
Iteration 136/1000 | Loss: 0.00003032
Iteration 137/1000 | Loss: 0.00003032
Iteration 138/1000 | Loss: 0.00003032
Iteration 139/1000 | Loss: 0.00003032
Iteration 140/1000 | Loss: 0.00003031
Iteration 141/1000 | Loss: 0.00003031
Iteration 142/1000 | Loss: 0.00003031
Iteration 143/1000 | Loss: 0.00003031
Iteration 144/1000 | Loss: 0.00003031
Iteration 145/1000 | Loss: 0.00003031
Iteration 146/1000 | Loss: 0.00003030
Iteration 147/1000 | Loss: 0.00003030
Iteration 148/1000 | Loss: 0.00003030
Iteration 149/1000 | Loss: 0.00003030
Iteration 150/1000 | Loss: 0.00003030
Iteration 151/1000 | Loss: 0.00003030
Iteration 152/1000 | Loss: 0.00003030
Iteration 153/1000 | Loss: 0.00003029
Iteration 154/1000 | Loss: 0.00003029
Iteration 155/1000 | Loss: 0.00003029
Iteration 156/1000 | Loss: 0.00003029
Iteration 157/1000 | Loss: 0.00003029
Iteration 158/1000 | Loss: 0.00003029
Iteration 159/1000 | Loss: 0.00003029
Iteration 160/1000 | Loss: 0.00003029
Iteration 161/1000 | Loss: 0.00003029
Iteration 162/1000 | Loss: 0.00003028
Iteration 163/1000 | Loss: 0.00003028
Iteration 164/1000 | Loss: 0.00003028
Iteration 165/1000 | Loss: 0.00003028
Iteration 166/1000 | Loss: 0.00003028
Iteration 167/1000 | Loss: 0.00003028
Iteration 168/1000 | Loss: 0.00003028
Iteration 169/1000 | Loss: 0.00003028
Iteration 170/1000 | Loss: 0.00003028
Iteration 171/1000 | Loss: 0.00003028
Iteration 172/1000 | Loss: 0.00003028
Iteration 173/1000 | Loss: 0.00003028
Iteration 174/1000 | Loss: 0.00003028
Iteration 175/1000 | Loss: 0.00003028
Iteration 176/1000 | Loss: 0.00003028
Iteration 177/1000 | Loss: 0.00003028
Iteration 178/1000 | Loss: 0.00003028
Iteration 179/1000 | Loss: 0.00003028
Iteration 180/1000 | Loss: 0.00003028
Iteration 181/1000 | Loss: 0.00003028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [3.0281975341495126e-05, 3.0281975341495126e-05, 3.0281975341495126e-05, 3.0281975341495126e-05, 3.0281975341495126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0281975341495126e-05

Optimization complete. Final v2v error: 4.519752502441406 mm

Highest mean error: 5.520506858825684 mm for frame 171

Lowest mean error: 3.476738691329956 mm for frame 206

Saving results

Total time: 87.73686957359314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429808
Iteration 2/25 | Loss: 0.00131609
Iteration 3/25 | Loss: 0.00123326
Iteration 4/25 | Loss: 0.00122038
Iteration 5/25 | Loss: 0.00121613
Iteration 6/25 | Loss: 0.00121545
Iteration 7/25 | Loss: 0.00121545
Iteration 8/25 | Loss: 0.00121545
Iteration 9/25 | Loss: 0.00121545
Iteration 10/25 | Loss: 0.00121545
Iteration 11/25 | Loss: 0.00121545
Iteration 12/25 | Loss: 0.00121545
Iteration 13/25 | Loss: 0.00121545
Iteration 14/25 | Loss: 0.00121545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012154472060501575, 0.0012154472060501575, 0.0012154472060501575, 0.0012154472060501575, 0.0012154472060501575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012154472060501575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73474741
Iteration 2/25 | Loss: 0.00128164
Iteration 3/25 | Loss: 0.00128164
Iteration 4/25 | Loss: 0.00128164
Iteration 5/25 | Loss: 0.00128164
Iteration 6/25 | Loss: 0.00128164
Iteration 7/25 | Loss: 0.00128164
Iteration 8/25 | Loss: 0.00128164
Iteration 9/25 | Loss: 0.00128164
Iteration 10/25 | Loss: 0.00128164
Iteration 11/25 | Loss: 0.00128164
Iteration 12/25 | Loss: 0.00128164
Iteration 13/25 | Loss: 0.00128164
Iteration 14/25 | Loss: 0.00128164
Iteration 15/25 | Loss: 0.00128164
Iteration 16/25 | Loss: 0.00128164
Iteration 17/25 | Loss: 0.00128164
Iteration 18/25 | Loss: 0.00128164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012816357193514705, 0.0012816357193514705, 0.0012816357193514705, 0.0012816357193514705, 0.0012816357193514705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012816357193514705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128164
Iteration 2/1000 | Loss: 0.00003153
Iteration 3/1000 | Loss: 0.00002385
Iteration 4/1000 | Loss: 0.00002083
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00001893
Iteration 7/1000 | Loss: 0.00001849
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001760
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001646
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001638
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001625
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001623
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001623
Iteration 38/1000 | Loss: 0.00001623
Iteration 39/1000 | Loss: 0.00001623
Iteration 40/1000 | Loss: 0.00001622
Iteration 41/1000 | Loss: 0.00001622
Iteration 42/1000 | Loss: 0.00001621
Iteration 43/1000 | Loss: 0.00001615
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001614
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001607
Iteration 61/1000 | Loss: 0.00001606
Iteration 62/1000 | Loss: 0.00001606
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001602
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001602
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001601
Iteration 76/1000 | Loss: 0.00001601
Iteration 77/1000 | Loss: 0.00001601
Iteration 78/1000 | Loss: 0.00001601
Iteration 79/1000 | Loss: 0.00001601
Iteration 80/1000 | Loss: 0.00001601
Iteration 81/1000 | Loss: 0.00001600
Iteration 82/1000 | Loss: 0.00001599
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001597
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001596
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001596
Iteration 97/1000 | Loss: 0.00001596
Iteration 98/1000 | Loss: 0.00001596
Iteration 99/1000 | Loss: 0.00001596
Iteration 100/1000 | Loss: 0.00001596
Iteration 101/1000 | Loss: 0.00001596
Iteration 102/1000 | Loss: 0.00001596
Iteration 103/1000 | Loss: 0.00001596
Iteration 104/1000 | Loss: 0.00001596
Iteration 105/1000 | Loss: 0.00001595
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001595
Iteration 108/1000 | Loss: 0.00001595
Iteration 109/1000 | Loss: 0.00001595
Iteration 110/1000 | Loss: 0.00001595
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001594
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001592
Iteration 131/1000 | Loss: 0.00001592
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001592
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001591
Iteration 139/1000 | Loss: 0.00001591
Iteration 140/1000 | Loss: 0.00001590
Iteration 141/1000 | Loss: 0.00001590
Iteration 142/1000 | Loss: 0.00001590
Iteration 143/1000 | Loss: 0.00001590
Iteration 144/1000 | Loss: 0.00001590
Iteration 145/1000 | Loss: 0.00001590
Iteration 146/1000 | Loss: 0.00001590
Iteration 147/1000 | Loss: 0.00001590
Iteration 148/1000 | Loss: 0.00001590
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001590
Iteration 151/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.5896663171588443e-05, 1.5896663171588443e-05, 1.5896663171588443e-05, 1.5896663171588443e-05, 1.5896663171588443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5896663171588443e-05

Optimization complete. Final v2v error: 3.4135916233062744 mm

Highest mean error: 3.6701133251190186 mm for frame 31

Lowest mean error: 3.150974750518799 mm for frame 21

Saving results

Total time: 40.22186303138733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426450
Iteration 2/25 | Loss: 0.00125293
Iteration 3/25 | Loss: 0.00120126
Iteration 4/25 | Loss: 0.00119281
Iteration 5/25 | Loss: 0.00119051
Iteration 6/25 | Loss: 0.00119026
Iteration 7/25 | Loss: 0.00119026
Iteration 8/25 | Loss: 0.00119026
Iteration 9/25 | Loss: 0.00119026
Iteration 10/25 | Loss: 0.00119026
Iteration 11/25 | Loss: 0.00119026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001190255512483418, 0.001190255512483418, 0.001190255512483418, 0.001190255512483418, 0.001190255512483418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190255512483418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31616020
Iteration 2/25 | Loss: 0.00109344
Iteration 3/25 | Loss: 0.00109344
Iteration 4/25 | Loss: 0.00109344
Iteration 5/25 | Loss: 0.00109344
Iteration 6/25 | Loss: 0.00109343
Iteration 7/25 | Loss: 0.00109343
Iteration 8/25 | Loss: 0.00109343
Iteration 9/25 | Loss: 0.00109343
Iteration 10/25 | Loss: 0.00109343
Iteration 11/25 | Loss: 0.00109343
Iteration 12/25 | Loss: 0.00109343
Iteration 13/25 | Loss: 0.00109343
Iteration 14/25 | Loss: 0.00109343
Iteration 15/25 | Loss: 0.00109343
Iteration 16/25 | Loss: 0.00109343
Iteration 17/25 | Loss: 0.00109343
Iteration 18/25 | Loss: 0.00109343
Iteration 19/25 | Loss: 0.00109343
Iteration 20/25 | Loss: 0.00109343
Iteration 21/25 | Loss: 0.00109343
Iteration 22/25 | Loss: 0.00109343
Iteration 23/25 | Loss: 0.00109343
Iteration 24/25 | Loss: 0.00109343
Iteration 25/25 | Loss: 0.00109343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109343
Iteration 2/1000 | Loss: 0.00002441
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001417
Iteration 5/1000 | Loss: 0.00001323
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001220
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001133
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001085
Iteration 21/1000 | Loss: 0.00001085
Iteration 22/1000 | Loss: 0.00001082
Iteration 23/1000 | Loss: 0.00001081
Iteration 24/1000 | Loss: 0.00001081
Iteration 25/1000 | Loss: 0.00001080
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001077
Iteration 28/1000 | Loss: 0.00001077
Iteration 29/1000 | Loss: 0.00001076
Iteration 30/1000 | Loss: 0.00001076
Iteration 31/1000 | Loss: 0.00001075
Iteration 32/1000 | Loss: 0.00001074
Iteration 33/1000 | Loss: 0.00001073
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001071
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001070
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001065
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001064
Iteration 61/1000 | Loss: 0.00001064
Iteration 62/1000 | Loss: 0.00001064
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001061
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001061
Iteration 89/1000 | Loss: 0.00001061
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.0611183824948967e-05, 1.0611183824948967e-05, 1.0611183824948967e-05, 1.0611183824948967e-05, 1.0611183824948967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0611183824948967e-05

Optimization complete. Final v2v error: 2.7923202514648438 mm

Highest mean error: 2.94461989402771 mm for frame 182

Lowest mean error: 2.630364418029785 mm for frame 6

Saving results

Total time: 34.38434171676636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046718
Iteration 2/25 | Loss: 0.01046718
Iteration 3/25 | Loss: 0.01046718
Iteration 4/25 | Loss: 0.01046718
Iteration 5/25 | Loss: 0.01046718
Iteration 6/25 | Loss: 0.01046718
Iteration 7/25 | Loss: 0.01046718
Iteration 8/25 | Loss: 0.01046718
Iteration 9/25 | Loss: 0.01046718
Iteration 10/25 | Loss: 0.01046718
Iteration 11/25 | Loss: 0.01046718
Iteration 12/25 | Loss: 0.01046718
Iteration 13/25 | Loss: 0.01046718
Iteration 14/25 | Loss: 0.01046718
Iteration 15/25 | Loss: 0.01046718
Iteration 16/25 | Loss: 0.01046718
Iteration 17/25 | Loss: 0.01046717
Iteration 18/25 | Loss: 0.01046717
Iteration 19/25 | Loss: 0.01046717
Iteration 20/25 | Loss: 0.01046717
Iteration 21/25 | Loss: 0.01046717
Iteration 22/25 | Loss: 0.01046717
Iteration 23/25 | Loss: 0.01046717
Iteration 24/25 | Loss: 0.01046717
Iteration 25/25 | Loss: 0.01046717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46589565
Iteration 2/25 | Loss: 0.09511802
Iteration 3/25 | Loss: 0.09260488
Iteration 4/25 | Loss: 0.09255884
Iteration 5/25 | Loss: 0.09255774
Iteration 6/25 | Loss: 0.09255230
Iteration 7/25 | Loss: 0.09255230
Iteration 8/25 | Loss: 0.09255230
Iteration 9/25 | Loss: 0.09255230
Iteration 10/25 | Loss: 0.09255230
Iteration 11/25 | Loss: 0.09255230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0925522968173027, 0.0925522968173027, 0.0925522968173027, 0.0925522968173027, 0.0925522968173027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0925522968173027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09255230
Iteration 2/1000 | Loss: 0.00144972
Iteration 3/1000 | Loss: 0.00285503
Iteration 4/1000 | Loss: 0.00069724
Iteration 5/1000 | Loss: 0.00022362
Iteration 6/1000 | Loss: 0.00032271
Iteration 7/1000 | Loss: 0.00021228
Iteration 8/1000 | Loss: 0.00020443
Iteration 9/1000 | Loss: 0.00141016
Iteration 10/1000 | Loss: 0.00060845
Iteration 11/1000 | Loss: 0.00013742
Iteration 12/1000 | Loss: 0.00022191
Iteration 13/1000 | Loss: 0.00003019
Iteration 14/1000 | Loss: 0.00007116
Iteration 15/1000 | Loss: 0.00008867
Iteration 16/1000 | Loss: 0.00009335
Iteration 17/1000 | Loss: 0.00002650
Iteration 18/1000 | Loss: 0.00011487
Iteration 19/1000 | Loss: 0.00003874
Iteration 20/1000 | Loss: 0.00053541
Iteration 21/1000 | Loss: 0.00012390
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00004001
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00032473
Iteration 26/1000 | Loss: 0.00023107
Iteration 27/1000 | Loss: 0.00012862
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00016190
Iteration 31/1000 | Loss: 0.00005031
Iteration 32/1000 | Loss: 0.00007584
Iteration 33/1000 | Loss: 0.00002228
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00012491
Iteration 36/1000 | Loss: 0.00039854
Iteration 37/1000 | Loss: 0.00002703
Iteration 38/1000 | Loss: 0.00004052
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00006324
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00004051
Iteration 43/1000 | Loss: 0.00008820
Iteration 44/1000 | Loss: 0.00002226
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00007698
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00005420
Iteration 49/1000 | Loss: 0.00005272
Iteration 50/1000 | Loss: 0.00005886
Iteration 51/1000 | Loss: 0.00001271
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00002978
Iteration 56/1000 | Loss: 0.00001250
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00005925
Iteration 60/1000 | Loss: 0.00004549
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00005530
Iteration 66/1000 | Loss: 0.00002656
Iteration 67/1000 | Loss: 0.00002665
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00005149
Iteration 70/1000 | Loss: 0.00078972
Iteration 71/1000 | Loss: 0.00003035
Iteration 72/1000 | Loss: 0.00003646
Iteration 73/1000 | Loss: 0.00001982
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001790
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00003162
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001165
Iteration 106/1000 | Loss: 0.00001165
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001164
Iteration 109/1000 | Loss: 0.00001164
Iteration 110/1000 | Loss: 0.00001164
Iteration 111/1000 | Loss: 0.00001164
Iteration 112/1000 | Loss: 0.00001164
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001163
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001162
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001162
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001161
Iteration 146/1000 | Loss: 0.00001161
Iteration 147/1000 | Loss: 0.00001161
Iteration 148/1000 | Loss: 0.00001161
Iteration 149/1000 | Loss: 0.00001161
Iteration 150/1000 | Loss: 0.00001161
Iteration 151/1000 | Loss: 0.00001161
Iteration 152/1000 | Loss: 0.00001161
Iteration 153/1000 | Loss: 0.00001160
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001159
Iteration 162/1000 | Loss: 0.00001159
Iteration 163/1000 | Loss: 0.00001159
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001159
Iteration 172/1000 | Loss: 0.00001159
Iteration 173/1000 | Loss: 0.00001159
Iteration 174/1000 | Loss: 0.00001159
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001158
Iteration 177/1000 | Loss: 0.00001158
Iteration 178/1000 | Loss: 0.00001158
Iteration 179/1000 | Loss: 0.00001158
Iteration 180/1000 | Loss: 0.00001158
Iteration 181/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1584849744394887e-05, 1.1584849744394887e-05, 1.1584849744394887e-05, 1.1584849744394887e-05, 1.1584849744394887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1584849744394887e-05

Optimization complete. Final v2v error: 2.890866994857788 mm

Highest mean error: 3.2869713306427 mm for frame 146

Lowest mean error: 2.618992805480957 mm for frame 266

Saving results

Total time: 125.84877872467041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043323
Iteration 2/25 | Loss: 0.01043323
Iteration 3/25 | Loss: 0.01043323
Iteration 4/25 | Loss: 0.01043323
Iteration 5/25 | Loss: 0.01043322
Iteration 6/25 | Loss: 0.01043322
Iteration 7/25 | Loss: 0.01043322
Iteration 8/25 | Loss: 0.01043322
Iteration 9/25 | Loss: 0.01043321
Iteration 10/25 | Loss: 0.01043321
Iteration 11/25 | Loss: 0.01043321
Iteration 12/25 | Loss: 0.01043321
Iteration 13/25 | Loss: 0.01043320
Iteration 14/25 | Loss: 0.01043320
Iteration 15/25 | Loss: 0.01043320
Iteration 16/25 | Loss: 0.01043320
Iteration 17/25 | Loss: 0.01043319
Iteration 18/25 | Loss: 0.01043319
Iteration 19/25 | Loss: 0.01043319
Iteration 20/25 | Loss: 0.01043319
Iteration 21/25 | Loss: 0.01043319
Iteration 22/25 | Loss: 0.01043318
Iteration 23/25 | Loss: 0.01043318
Iteration 24/25 | Loss: 0.01043318
Iteration 25/25 | Loss: 0.01043317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94958365
Iteration 2/25 | Loss: 0.09084987
Iteration 3/25 | Loss: 0.09022509
Iteration 4/25 | Loss: 0.08892204
Iteration 5/25 | Loss: 0.08884555
Iteration 6/25 | Loss: 0.08884552
Iteration 7/25 | Loss: 0.08884551
Iteration 8/25 | Loss: 0.08884551
Iteration 9/25 | Loss: 0.08884551
Iteration 10/25 | Loss: 0.08884551
Iteration 11/25 | Loss: 0.08884551
Iteration 12/25 | Loss: 0.08884551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08884550631046295, 0.08884550631046295, 0.08884550631046295, 0.08884550631046295, 0.08884550631046295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08884550631046295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08884551
Iteration 2/1000 | Loss: 0.00158416
Iteration 3/1000 | Loss: 0.00189018
Iteration 4/1000 | Loss: 0.00793874
Iteration 5/1000 | Loss: 0.00024318
Iteration 6/1000 | Loss: 0.00203862
Iteration 7/1000 | Loss: 0.00397796
Iteration 8/1000 | Loss: 0.00082379
Iteration 9/1000 | Loss: 0.00013091
Iteration 10/1000 | Loss: 0.00197523
Iteration 11/1000 | Loss: 0.00536720
Iteration 12/1000 | Loss: 0.00043553
Iteration 13/1000 | Loss: 0.00337343
Iteration 14/1000 | Loss: 0.00115413
Iteration 15/1000 | Loss: 0.00036853
Iteration 16/1000 | Loss: 0.00008772
Iteration 17/1000 | Loss: 0.00087520
Iteration 18/1000 | Loss: 0.00174192
Iteration 19/1000 | Loss: 0.00006484
Iteration 20/1000 | Loss: 0.00005896
Iteration 21/1000 | Loss: 0.00015653
Iteration 22/1000 | Loss: 0.00007272
Iteration 23/1000 | Loss: 0.00008683
Iteration 24/1000 | Loss: 0.00007795
Iteration 25/1000 | Loss: 0.00008390
Iteration 26/1000 | Loss: 0.00140742
Iteration 27/1000 | Loss: 0.00010466
Iteration 28/1000 | Loss: 0.00010860
Iteration 29/1000 | Loss: 0.00004190
Iteration 30/1000 | Loss: 0.00003373
Iteration 31/1000 | Loss: 0.00003808
Iteration 32/1000 | Loss: 0.00049987
Iteration 33/1000 | Loss: 0.00009013
Iteration 34/1000 | Loss: 0.00041595
Iteration 35/1000 | Loss: 0.00012193
Iteration 36/1000 | Loss: 0.00008370
Iteration 37/1000 | Loss: 0.00042270
Iteration 38/1000 | Loss: 0.00003448
Iteration 39/1000 | Loss: 0.00004157
Iteration 40/1000 | Loss: 0.00008537
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00004420
Iteration 43/1000 | Loss: 0.00002963
Iteration 44/1000 | Loss: 0.00004322
Iteration 45/1000 | Loss: 0.00005245
Iteration 46/1000 | Loss: 0.00013111
Iteration 47/1000 | Loss: 0.00002549
Iteration 48/1000 | Loss: 0.00004854
Iteration 49/1000 | Loss: 0.00003968
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00004708
Iteration 52/1000 | Loss: 0.00007672
Iteration 53/1000 | Loss: 0.00002511
Iteration 54/1000 | Loss: 0.00007781
Iteration 55/1000 | Loss: 0.00002397
Iteration 56/1000 | Loss: 0.00005130
Iteration 57/1000 | Loss: 0.00002030
Iteration 58/1000 | Loss: 0.00002311
Iteration 59/1000 | Loss: 0.00009785
Iteration 60/1000 | Loss: 0.00008045
Iteration 61/1000 | Loss: 0.00002305
Iteration 62/1000 | Loss: 0.00003859
Iteration 63/1000 | Loss: 0.00034451
Iteration 64/1000 | Loss: 0.00024704
Iteration 65/1000 | Loss: 0.00003218
Iteration 66/1000 | Loss: 0.00020667
Iteration 67/1000 | Loss: 0.00003077
Iteration 68/1000 | Loss: 0.00002755
Iteration 69/1000 | Loss: 0.00001920
Iteration 70/1000 | Loss: 0.00002826
Iteration 71/1000 | Loss: 0.00002220
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00005290
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001869
Iteration 83/1000 | Loss: 0.00001869
Iteration 84/1000 | Loss: 0.00001869
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00011600
Iteration 89/1000 | Loss: 0.00011600
Iteration 90/1000 | Loss: 0.00028400
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00006742
Iteration 93/1000 | Loss: 0.00047247
Iteration 94/1000 | Loss: 0.00003600
Iteration 95/1000 | Loss: 0.00012568
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.8255093891639262e-05, 1.8255093891639262e-05, 1.8255093891639262e-05, 1.8255093891639262e-05, 1.8255093891639262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8255093891639262e-05

Optimization complete. Final v2v error: 3.6505253314971924 mm

Highest mean error: 5.04790735244751 mm for frame 53

Lowest mean error: 2.801816463470459 mm for frame 134

Saving results

Total time: 147.93266892433167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839726
Iteration 2/25 | Loss: 0.00164039
Iteration 3/25 | Loss: 0.00133633
Iteration 4/25 | Loss: 0.00130420
Iteration 5/25 | Loss: 0.00129884
Iteration 6/25 | Loss: 0.00129870
Iteration 7/25 | Loss: 0.00129870
Iteration 8/25 | Loss: 0.00129870
Iteration 9/25 | Loss: 0.00129870
Iteration 10/25 | Loss: 0.00129870
Iteration 11/25 | Loss: 0.00129870
Iteration 12/25 | Loss: 0.00129870
Iteration 13/25 | Loss: 0.00129870
Iteration 14/25 | Loss: 0.00129870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012986961519345641, 0.0012986961519345641, 0.0012986961519345641, 0.0012986961519345641, 0.0012986961519345641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012986961519345641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.14925781
Iteration 2/25 | Loss: 0.00104190
Iteration 3/25 | Loss: 0.00104188
Iteration 4/25 | Loss: 0.00104188
Iteration 5/25 | Loss: 0.00104188
Iteration 6/25 | Loss: 0.00104188
Iteration 7/25 | Loss: 0.00104188
Iteration 8/25 | Loss: 0.00104188
Iteration 9/25 | Loss: 0.00104188
Iteration 10/25 | Loss: 0.00104188
Iteration 11/25 | Loss: 0.00104188
Iteration 12/25 | Loss: 0.00104188
Iteration 13/25 | Loss: 0.00104188
Iteration 14/25 | Loss: 0.00104188
Iteration 15/25 | Loss: 0.00104188
Iteration 16/25 | Loss: 0.00104188
Iteration 17/25 | Loss: 0.00104188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010418799938634038, 0.0010418799938634038, 0.0010418799938634038, 0.0010418799938634038, 0.0010418799938634038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010418799938634038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104188
Iteration 2/1000 | Loss: 0.00007243
Iteration 3/1000 | Loss: 0.00003354
Iteration 4/1000 | Loss: 0.00002749
Iteration 5/1000 | Loss: 0.00002565
Iteration 6/1000 | Loss: 0.00002456
Iteration 7/1000 | Loss: 0.00002381
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002300
Iteration 10/1000 | Loss: 0.00002270
Iteration 11/1000 | Loss: 0.00002243
Iteration 12/1000 | Loss: 0.00002221
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002193
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002191
Iteration 22/1000 | Loss: 0.00002191
Iteration 23/1000 | Loss: 0.00002191
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002191
Iteration 26/1000 | Loss: 0.00002191
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002190
Iteration 29/1000 | Loss: 0.00002190
Iteration 30/1000 | Loss: 0.00002190
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00002190
Iteration 36/1000 | Loss: 0.00002189
Iteration 37/1000 | Loss: 0.00002189
Iteration 38/1000 | Loss: 0.00002189
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002188
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002180
Iteration 46/1000 | Loss: 0.00002180
Iteration 47/1000 | Loss: 0.00002180
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002178
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002174
Iteration 54/1000 | Loss: 0.00002174
Iteration 55/1000 | Loss: 0.00002173
Iteration 56/1000 | Loss: 0.00002173
Iteration 57/1000 | Loss: 0.00002172
Iteration 58/1000 | Loss: 0.00002172
Iteration 59/1000 | Loss: 0.00002172
Iteration 60/1000 | Loss: 0.00002172
Iteration 61/1000 | Loss: 0.00002172
Iteration 62/1000 | Loss: 0.00002171
Iteration 63/1000 | Loss: 0.00002171
Iteration 64/1000 | Loss: 0.00002171
Iteration 65/1000 | Loss: 0.00002171
Iteration 66/1000 | Loss: 0.00002170
Iteration 67/1000 | Loss: 0.00002170
Iteration 68/1000 | Loss: 0.00002170
Iteration 69/1000 | Loss: 0.00002170
Iteration 70/1000 | Loss: 0.00002170
Iteration 71/1000 | Loss: 0.00002170
Iteration 72/1000 | Loss: 0.00002169
Iteration 73/1000 | Loss: 0.00002169
Iteration 74/1000 | Loss: 0.00002169
Iteration 75/1000 | Loss: 0.00002169
Iteration 76/1000 | Loss: 0.00002167
Iteration 77/1000 | Loss: 0.00002165
Iteration 78/1000 | Loss: 0.00002165
Iteration 79/1000 | Loss: 0.00002165
Iteration 80/1000 | Loss: 0.00002165
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002164
Iteration 86/1000 | Loss: 0.00002164
Iteration 87/1000 | Loss: 0.00002164
Iteration 88/1000 | Loss: 0.00002164
Iteration 89/1000 | Loss: 0.00002164
Iteration 90/1000 | Loss: 0.00002164
Iteration 91/1000 | Loss: 0.00002164
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002164
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002164
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00002163
Iteration 100/1000 | Loss: 0.00002163
Iteration 101/1000 | Loss: 0.00002163
Iteration 102/1000 | Loss: 0.00002163
Iteration 103/1000 | Loss: 0.00002163
Iteration 104/1000 | Loss: 0.00002163
Iteration 105/1000 | Loss: 0.00002163
Iteration 106/1000 | Loss: 0.00002162
Iteration 107/1000 | Loss: 0.00002162
Iteration 108/1000 | Loss: 0.00002162
Iteration 109/1000 | Loss: 0.00002162
Iteration 110/1000 | Loss: 0.00002162
Iteration 111/1000 | Loss: 0.00002162
Iteration 112/1000 | Loss: 0.00002162
Iteration 113/1000 | Loss: 0.00002162
Iteration 114/1000 | Loss: 0.00002162
Iteration 115/1000 | Loss: 0.00002162
Iteration 116/1000 | Loss: 0.00002162
Iteration 117/1000 | Loss: 0.00002161
Iteration 118/1000 | Loss: 0.00002161
Iteration 119/1000 | Loss: 0.00002161
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002161
Iteration 123/1000 | Loss: 0.00002161
Iteration 124/1000 | Loss: 0.00002161
Iteration 125/1000 | Loss: 0.00002161
Iteration 126/1000 | Loss: 0.00002160
Iteration 127/1000 | Loss: 0.00002160
Iteration 128/1000 | Loss: 0.00002160
Iteration 129/1000 | Loss: 0.00002160
Iteration 130/1000 | Loss: 0.00002160
Iteration 131/1000 | Loss: 0.00002160
Iteration 132/1000 | Loss: 0.00002160
Iteration 133/1000 | Loss: 0.00002160
Iteration 134/1000 | Loss: 0.00002160
Iteration 135/1000 | Loss: 0.00002160
Iteration 136/1000 | Loss: 0.00002160
Iteration 137/1000 | Loss: 0.00002160
Iteration 138/1000 | Loss: 0.00002160
Iteration 139/1000 | Loss: 0.00002160
Iteration 140/1000 | Loss: 0.00002160
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00002160
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00002160
Iteration 153/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.160094845748972e-05, 2.160094845748972e-05, 2.160094845748972e-05, 2.160094845748972e-05, 2.160094845748972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.160094845748972e-05

Optimization complete. Final v2v error: 3.967784881591797 mm

Highest mean error: 4.133423805236816 mm for frame 160

Lowest mean error: 3.550309658050537 mm for frame 102

Saving results

Total time: 41.45053458213806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524299
Iteration 2/25 | Loss: 0.00130567
Iteration 3/25 | Loss: 0.00123781
Iteration 4/25 | Loss: 0.00122958
Iteration 5/25 | Loss: 0.00122724
Iteration 6/25 | Loss: 0.00122724
Iteration 7/25 | Loss: 0.00122724
Iteration 8/25 | Loss: 0.00122724
Iteration 9/25 | Loss: 0.00122724
Iteration 10/25 | Loss: 0.00122724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012272399617359042, 0.0012272399617359042, 0.0012272399617359042, 0.0012272399617359042, 0.0012272399617359042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012272399617359042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.20745659
Iteration 2/25 | Loss: 0.00115212
Iteration 3/25 | Loss: 0.00115211
Iteration 4/25 | Loss: 0.00115211
Iteration 5/25 | Loss: 0.00115211
Iteration 6/25 | Loss: 0.00115211
Iteration 7/25 | Loss: 0.00115211
Iteration 8/25 | Loss: 0.00115211
Iteration 9/25 | Loss: 0.00115211
Iteration 10/25 | Loss: 0.00115211
Iteration 11/25 | Loss: 0.00115211
Iteration 12/25 | Loss: 0.00115211
Iteration 13/25 | Loss: 0.00115211
Iteration 14/25 | Loss: 0.00115211
Iteration 15/25 | Loss: 0.00115211
Iteration 16/25 | Loss: 0.00115211
Iteration 17/25 | Loss: 0.00115211
Iteration 18/25 | Loss: 0.00115211
Iteration 19/25 | Loss: 0.00115211
Iteration 20/25 | Loss: 0.00115211
Iteration 21/25 | Loss: 0.00115211
Iteration 22/25 | Loss: 0.00115211
Iteration 23/25 | Loss: 0.00115211
Iteration 24/25 | Loss: 0.00115211
Iteration 25/25 | Loss: 0.00115211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115211
Iteration 2/1000 | Loss: 0.00002258
Iteration 3/1000 | Loss: 0.00001617
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001240
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001200
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001199
Iteration 69/1000 | Loss: 0.00001199
Iteration 70/1000 | Loss: 0.00001199
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001198
Iteration 75/1000 | Loss: 0.00001198
Iteration 76/1000 | Loss: 0.00001198
Iteration 77/1000 | Loss: 0.00001198
Iteration 78/1000 | Loss: 0.00001198
Iteration 79/1000 | Loss: 0.00001198
Iteration 80/1000 | Loss: 0.00001198
Iteration 81/1000 | Loss: 0.00001198
Iteration 82/1000 | Loss: 0.00001198
Iteration 83/1000 | Loss: 0.00001197
Iteration 84/1000 | Loss: 0.00001197
Iteration 85/1000 | Loss: 0.00001197
Iteration 86/1000 | Loss: 0.00001197
Iteration 87/1000 | Loss: 0.00001197
Iteration 88/1000 | Loss: 0.00001197
Iteration 89/1000 | Loss: 0.00001197
Iteration 90/1000 | Loss: 0.00001197
Iteration 91/1000 | Loss: 0.00001197
Iteration 92/1000 | Loss: 0.00001197
Iteration 93/1000 | Loss: 0.00001197
Iteration 94/1000 | Loss: 0.00001196
Iteration 95/1000 | Loss: 0.00001196
Iteration 96/1000 | Loss: 0.00001196
Iteration 97/1000 | Loss: 0.00001196
Iteration 98/1000 | Loss: 0.00001196
Iteration 99/1000 | Loss: 0.00001196
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1959496987401508e-05, 1.1959496987401508e-05, 1.1959496987401508e-05, 1.1959496987401508e-05, 1.1959496987401508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1959496987401508e-05

Optimization complete. Final v2v error: 2.948561668395996 mm

Highest mean error: 3.1267123222351074 mm for frame 41

Lowest mean error: 2.747248888015747 mm for frame 32

Saving results

Total time: 43.086493492126465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360759
Iteration 2/25 | Loss: 0.00128751
Iteration 3/25 | Loss: 0.00120269
Iteration 4/25 | Loss: 0.00118289
Iteration 5/25 | Loss: 0.00117574
Iteration 6/25 | Loss: 0.00117378
Iteration 7/25 | Loss: 0.00117369
Iteration 8/25 | Loss: 0.00117369
Iteration 9/25 | Loss: 0.00117369
Iteration 10/25 | Loss: 0.00117369
Iteration 11/25 | Loss: 0.00117369
Iteration 12/25 | Loss: 0.00117369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011736871674656868, 0.0011736871674656868, 0.0011736871674656868, 0.0011736871674656868, 0.0011736871674656868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011736871674656868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31575155
Iteration 2/25 | Loss: 0.00169331
Iteration 3/25 | Loss: 0.00169330
Iteration 4/25 | Loss: 0.00169330
Iteration 5/25 | Loss: 0.00169330
Iteration 6/25 | Loss: 0.00169330
Iteration 7/25 | Loss: 0.00169330
Iteration 8/25 | Loss: 0.00169330
Iteration 9/25 | Loss: 0.00169330
Iteration 10/25 | Loss: 0.00169330
Iteration 11/25 | Loss: 0.00169330
Iteration 12/25 | Loss: 0.00169330
Iteration 13/25 | Loss: 0.00169330
Iteration 14/25 | Loss: 0.00169330
Iteration 15/25 | Loss: 0.00169330
Iteration 16/25 | Loss: 0.00169330
Iteration 17/25 | Loss: 0.00169330
Iteration 18/25 | Loss: 0.00169330
Iteration 19/25 | Loss: 0.00169330
Iteration 20/25 | Loss: 0.00169330
Iteration 21/25 | Loss: 0.00169330
Iteration 22/25 | Loss: 0.00169330
Iteration 23/25 | Loss: 0.00169330
Iteration 24/25 | Loss: 0.00169330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016933014849200845, 0.0016933014849200845, 0.0016933014849200845, 0.0016933014849200845, 0.0016933014849200845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016933014849200845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169330
Iteration 2/1000 | Loss: 0.00004937
Iteration 3/1000 | Loss: 0.00003266
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002074
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001799
Iteration 8/1000 | Loss: 0.00001725
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001558
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001530
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00001521
Iteration 28/1000 | Loss: 0.00001521
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001521
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001520
Iteration 36/1000 | Loss: 0.00001520
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001520
Iteration 39/1000 | Loss: 0.00001520
Iteration 40/1000 | Loss: 0.00001520
Iteration 41/1000 | Loss: 0.00001518
Iteration 42/1000 | Loss: 0.00001518
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001512
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001509
Iteration 72/1000 | Loss: 0.00001509
Iteration 73/1000 | Loss: 0.00001509
Iteration 74/1000 | Loss: 0.00001508
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001508
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001507
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001506
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001505
Iteration 84/1000 | Loss: 0.00001505
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001504
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001504
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001501
Iteration 100/1000 | Loss: 0.00001501
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001501
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001500
Iteration 106/1000 | Loss: 0.00001500
Iteration 107/1000 | Loss: 0.00001500
Iteration 108/1000 | Loss: 0.00001500
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001499
Iteration 111/1000 | Loss: 0.00001499
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Iteration 131/1000 | Loss: 0.00001497
Iteration 132/1000 | Loss: 0.00001497
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001496
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001495
Iteration 144/1000 | Loss: 0.00001495
Iteration 145/1000 | Loss: 0.00001495
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001494
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001494
Iteration 153/1000 | Loss: 0.00001494
Iteration 154/1000 | Loss: 0.00001494
Iteration 155/1000 | Loss: 0.00001494
Iteration 156/1000 | Loss: 0.00001494
Iteration 157/1000 | Loss: 0.00001494
Iteration 158/1000 | Loss: 0.00001494
Iteration 159/1000 | Loss: 0.00001494
Iteration 160/1000 | Loss: 0.00001494
Iteration 161/1000 | Loss: 0.00001494
Iteration 162/1000 | Loss: 0.00001494
Iteration 163/1000 | Loss: 0.00001493
Iteration 164/1000 | Loss: 0.00001493
Iteration 165/1000 | Loss: 0.00001493
Iteration 166/1000 | Loss: 0.00001493
Iteration 167/1000 | Loss: 0.00001493
Iteration 168/1000 | Loss: 0.00001493
Iteration 169/1000 | Loss: 0.00001492
Iteration 170/1000 | Loss: 0.00001492
Iteration 171/1000 | Loss: 0.00001492
Iteration 172/1000 | Loss: 0.00001492
Iteration 173/1000 | Loss: 0.00001492
Iteration 174/1000 | Loss: 0.00001492
Iteration 175/1000 | Loss: 0.00001492
Iteration 176/1000 | Loss: 0.00001492
Iteration 177/1000 | Loss: 0.00001491
Iteration 178/1000 | Loss: 0.00001491
Iteration 179/1000 | Loss: 0.00001491
Iteration 180/1000 | Loss: 0.00001491
Iteration 181/1000 | Loss: 0.00001491
Iteration 182/1000 | Loss: 0.00001491
Iteration 183/1000 | Loss: 0.00001491
Iteration 184/1000 | Loss: 0.00001491
Iteration 185/1000 | Loss: 0.00001491
Iteration 186/1000 | Loss: 0.00001491
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001490
Iteration 189/1000 | Loss: 0.00001490
Iteration 190/1000 | Loss: 0.00001490
Iteration 191/1000 | Loss: 0.00001490
Iteration 192/1000 | Loss: 0.00001490
Iteration 193/1000 | Loss: 0.00001490
Iteration 194/1000 | Loss: 0.00001490
Iteration 195/1000 | Loss: 0.00001490
Iteration 196/1000 | Loss: 0.00001490
Iteration 197/1000 | Loss: 0.00001490
Iteration 198/1000 | Loss: 0.00001490
Iteration 199/1000 | Loss: 0.00001490
Iteration 200/1000 | Loss: 0.00001489
Iteration 201/1000 | Loss: 0.00001489
Iteration 202/1000 | Loss: 0.00001489
Iteration 203/1000 | Loss: 0.00001489
Iteration 204/1000 | Loss: 0.00001489
Iteration 205/1000 | Loss: 0.00001489
Iteration 206/1000 | Loss: 0.00001489
Iteration 207/1000 | Loss: 0.00001489
Iteration 208/1000 | Loss: 0.00001488
Iteration 209/1000 | Loss: 0.00001488
Iteration 210/1000 | Loss: 0.00001488
Iteration 211/1000 | Loss: 0.00001488
Iteration 212/1000 | Loss: 0.00001488
Iteration 213/1000 | Loss: 0.00001488
Iteration 214/1000 | Loss: 0.00001488
Iteration 215/1000 | Loss: 0.00001488
Iteration 216/1000 | Loss: 0.00001488
Iteration 217/1000 | Loss: 0.00001487
Iteration 218/1000 | Loss: 0.00001487
Iteration 219/1000 | Loss: 0.00001487
Iteration 220/1000 | Loss: 0.00001487
Iteration 221/1000 | Loss: 0.00001487
Iteration 222/1000 | Loss: 0.00001487
Iteration 223/1000 | Loss: 0.00001486
Iteration 224/1000 | Loss: 0.00001486
Iteration 225/1000 | Loss: 0.00001486
Iteration 226/1000 | Loss: 0.00001486
Iteration 227/1000 | Loss: 0.00001486
Iteration 228/1000 | Loss: 0.00001486
Iteration 229/1000 | Loss: 0.00001486
Iteration 230/1000 | Loss: 0.00001486
Iteration 231/1000 | Loss: 0.00001486
Iteration 232/1000 | Loss: 0.00001486
Iteration 233/1000 | Loss: 0.00001486
Iteration 234/1000 | Loss: 0.00001486
Iteration 235/1000 | Loss: 0.00001486
Iteration 236/1000 | Loss: 0.00001486
Iteration 237/1000 | Loss: 0.00001486
Iteration 238/1000 | Loss: 0.00001486
Iteration 239/1000 | Loss: 0.00001485
Iteration 240/1000 | Loss: 0.00001485
Iteration 241/1000 | Loss: 0.00001485
Iteration 242/1000 | Loss: 0.00001485
Iteration 243/1000 | Loss: 0.00001485
Iteration 244/1000 | Loss: 0.00001485
Iteration 245/1000 | Loss: 0.00001485
Iteration 246/1000 | Loss: 0.00001485
Iteration 247/1000 | Loss: 0.00001485
Iteration 248/1000 | Loss: 0.00001485
Iteration 249/1000 | Loss: 0.00001485
Iteration 250/1000 | Loss: 0.00001485
Iteration 251/1000 | Loss: 0.00001485
Iteration 252/1000 | Loss: 0.00001485
Iteration 253/1000 | Loss: 0.00001485
Iteration 254/1000 | Loss: 0.00001485
Iteration 255/1000 | Loss: 0.00001484
Iteration 256/1000 | Loss: 0.00001484
Iteration 257/1000 | Loss: 0.00001484
Iteration 258/1000 | Loss: 0.00001484
Iteration 259/1000 | Loss: 0.00001484
Iteration 260/1000 | Loss: 0.00001484
Iteration 261/1000 | Loss: 0.00001484
Iteration 262/1000 | Loss: 0.00001484
Iteration 263/1000 | Loss: 0.00001484
Iteration 264/1000 | Loss: 0.00001484
Iteration 265/1000 | Loss: 0.00001484
Iteration 266/1000 | Loss: 0.00001484
Iteration 267/1000 | Loss: 0.00001484
Iteration 268/1000 | Loss: 0.00001484
Iteration 269/1000 | Loss: 0.00001484
Iteration 270/1000 | Loss: 0.00001484
Iteration 271/1000 | Loss: 0.00001484
Iteration 272/1000 | Loss: 0.00001484
Iteration 273/1000 | Loss: 0.00001484
Iteration 274/1000 | Loss: 0.00001484
Iteration 275/1000 | Loss: 0.00001484
Iteration 276/1000 | Loss: 0.00001484
Iteration 277/1000 | Loss: 0.00001484
Iteration 278/1000 | Loss: 0.00001484
Iteration 279/1000 | Loss: 0.00001484
Iteration 280/1000 | Loss: 0.00001484
Iteration 281/1000 | Loss: 0.00001484
Iteration 282/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.4844573342998046e-05, 1.4844573342998046e-05, 1.4844573342998046e-05, 1.4844573342998046e-05, 1.4844573342998046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4844573342998046e-05

Optimization complete. Final v2v error: 3.2383534908294678 mm

Highest mean error: 4.772055149078369 mm for frame 151

Lowest mean error: 2.3374075889587402 mm for frame 165

Saving results

Total time: 48.57892990112305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009027
Iteration 2/25 | Loss: 0.00185308
Iteration 3/25 | Loss: 0.00167789
Iteration 4/25 | Loss: 0.00146253
Iteration 5/25 | Loss: 0.00134024
Iteration 6/25 | Loss: 0.00136403
Iteration 7/25 | Loss: 0.00136234
Iteration 8/25 | Loss: 0.00134953
Iteration 9/25 | Loss: 0.00129236
Iteration 10/25 | Loss: 0.00125207
Iteration 11/25 | Loss: 0.00123624
Iteration 12/25 | Loss: 0.00123808
Iteration 13/25 | Loss: 0.00122047
Iteration 14/25 | Loss: 0.00121487
Iteration 15/25 | Loss: 0.00120840
Iteration 16/25 | Loss: 0.00121339
Iteration 17/25 | Loss: 0.00121052
Iteration 18/25 | Loss: 0.00120906
Iteration 19/25 | Loss: 0.00120493
Iteration 20/25 | Loss: 0.00120801
Iteration 21/25 | Loss: 0.00120688
Iteration 22/25 | Loss: 0.00120604
Iteration 23/25 | Loss: 0.00120451
Iteration 24/25 | Loss: 0.00120496
Iteration 25/25 | Loss: 0.00120627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38032496
Iteration 2/25 | Loss: 0.00156173
Iteration 3/25 | Loss: 0.00147217
Iteration 4/25 | Loss: 0.00147217
Iteration 5/25 | Loss: 0.00147217
Iteration 6/25 | Loss: 0.00147217
Iteration 7/25 | Loss: 0.00147217
Iteration 8/25 | Loss: 0.00147217
Iteration 9/25 | Loss: 0.00147217
Iteration 10/25 | Loss: 0.00147217
Iteration 11/25 | Loss: 0.00147217
Iteration 12/25 | Loss: 0.00147217
Iteration 13/25 | Loss: 0.00147217
Iteration 14/25 | Loss: 0.00147217
Iteration 15/25 | Loss: 0.00147217
Iteration 16/25 | Loss: 0.00147217
Iteration 17/25 | Loss: 0.00147217
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014721701154485345, 0.0014721701154485345, 0.0014721701154485345, 0.0014721701154485345, 0.0014721701154485345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014721701154485345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147217
Iteration 2/1000 | Loss: 0.00007234
Iteration 3/1000 | Loss: 0.00067593
Iteration 4/1000 | Loss: 0.00019292
Iteration 5/1000 | Loss: 0.00016014
Iteration 6/1000 | Loss: 0.00016165
Iteration 7/1000 | Loss: 0.00019145
Iteration 8/1000 | Loss: 0.00020241
Iteration 9/1000 | Loss: 0.00017473
Iteration 10/1000 | Loss: 0.00016373
Iteration 11/1000 | Loss: 0.00025597
Iteration 12/1000 | Loss: 0.00021082
Iteration 13/1000 | Loss: 0.00025917
Iteration 14/1000 | Loss: 0.00035260
Iteration 15/1000 | Loss: 0.00005585
Iteration 16/1000 | Loss: 0.00006020
Iteration 17/1000 | Loss: 0.00005583
Iteration 18/1000 | Loss: 0.00007439
Iteration 19/1000 | Loss: 0.00013741
Iteration 20/1000 | Loss: 0.00005933
Iteration 21/1000 | Loss: 0.00005244
Iteration 22/1000 | Loss: 0.00004786
Iteration 23/1000 | Loss: 0.00007856
Iteration 24/1000 | Loss: 0.00008214
Iteration 25/1000 | Loss: 0.00004407
Iteration 26/1000 | Loss: 0.00005309
Iteration 27/1000 | Loss: 0.00005341
Iteration 28/1000 | Loss: 0.00008065
Iteration 29/1000 | Loss: 0.00007770
Iteration 30/1000 | Loss: 0.00008974
Iteration 31/1000 | Loss: 0.00009242
Iteration 32/1000 | Loss: 0.00014471
Iteration 33/1000 | Loss: 0.00012540
Iteration 34/1000 | Loss: 0.00011927
Iteration 35/1000 | Loss: 0.00007405
Iteration 36/1000 | Loss: 0.00004136
Iteration 37/1000 | Loss: 0.00014212
Iteration 38/1000 | Loss: 0.00002170
Iteration 39/1000 | Loss: 0.00010057
Iteration 40/1000 | Loss: 0.00006538
Iteration 41/1000 | Loss: 0.00002527
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00004998
Iteration 44/1000 | Loss: 0.00004443
Iteration 45/1000 | Loss: 0.00002860
Iteration 46/1000 | Loss: 0.00017291
Iteration 47/1000 | Loss: 0.00004998
Iteration 48/1000 | Loss: 0.00003259
Iteration 49/1000 | Loss: 0.00003253
Iteration 50/1000 | Loss: 0.00002899
Iteration 51/1000 | Loss: 0.00002784
Iteration 52/1000 | Loss: 0.00004674
Iteration 53/1000 | Loss: 0.00009606
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00011035
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00014049
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001062
Iteration 66/1000 | Loss: 0.00012171
Iteration 67/1000 | Loss: 0.00001802
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001056
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001055
Iteration 76/1000 | Loss: 0.00001051
Iteration 77/1000 | Loss: 0.00001050
Iteration 78/1000 | Loss: 0.00001050
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001044
Iteration 83/1000 | Loss: 0.00001044
Iteration 84/1000 | Loss: 0.00001044
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001042
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001041
Iteration 94/1000 | Loss: 0.00001040
Iteration 95/1000 | Loss: 0.00001040
Iteration 96/1000 | Loss: 0.00001040
Iteration 97/1000 | Loss: 0.00001040
Iteration 98/1000 | Loss: 0.00001040
Iteration 99/1000 | Loss: 0.00001040
Iteration 100/1000 | Loss: 0.00001040
Iteration 101/1000 | Loss: 0.00001040
Iteration 102/1000 | Loss: 0.00001040
Iteration 103/1000 | Loss: 0.00001040
Iteration 104/1000 | Loss: 0.00001039
Iteration 105/1000 | Loss: 0.00001039
Iteration 106/1000 | Loss: 0.00001038
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001035
Iteration 113/1000 | Loss: 0.00001035
Iteration 114/1000 | Loss: 0.00001035
Iteration 115/1000 | Loss: 0.00001034
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001032
Iteration 118/1000 | Loss: 0.00001032
Iteration 119/1000 | Loss: 0.00001032
Iteration 120/1000 | Loss: 0.00001032
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001032
Iteration 124/1000 | Loss: 0.00001032
Iteration 125/1000 | Loss: 0.00001032
Iteration 126/1000 | Loss: 0.00001032
Iteration 127/1000 | Loss: 0.00001032
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001031
Iteration 130/1000 | Loss: 0.00001031
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001029
Iteration 133/1000 | Loss: 0.00001029
Iteration 134/1000 | Loss: 0.00001029
Iteration 135/1000 | Loss: 0.00001029
Iteration 136/1000 | Loss: 0.00001028
Iteration 137/1000 | Loss: 0.00001028
Iteration 138/1000 | Loss: 0.00001027
Iteration 139/1000 | Loss: 0.00001026
Iteration 140/1000 | Loss: 0.00001023
Iteration 141/1000 | Loss: 0.00001023
Iteration 142/1000 | Loss: 0.00001023
Iteration 143/1000 | Loss: 0.00001023
Iteration 144/1000 | Loss: 0.00001022
Iteration 145/1000 | Loss: 0.00001022
Iteration 146/1000 | Loss: 0.00001022
Iteration 147/1000 | Loss: 0.00001021
Iteration 148/1000 | Loss: 0.00001021
Iteration 149/1000 | Loss: 0.00001021
Iteration 150/1000 | Loss: 0.00001020
Iteration 151/1000 | Loss: 0.00001020
Iteration 152/1000 | Loss: 0.00001020
Iteration 153/1000 | Loss: 0.00001019
Iteration 154/1000 | Loss: 0.00001019
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001018
Iteration 157/1000 | Loss: 0.00001018
Iteration 158/1000 | Loss: 0.00001018
Iteration 159/1000 | Loss: 0.00001018
Iteration 160/1000 | Loss: 0.00001018
Iteration 161/1000 | Loss: 0.00001017
Iteration 162/1000 | Loss: 0.00001017
Iteration 163/1000 | Loss: 0.00001017
Iteration 164/1000 | Loss: 0.00001017
Iteration 165/1000 | Loss: 0.00001017
Iteration 166/1000 | Loss: 0.00001017
Iteration 167/1000 | Loss: 0.00001016
Iteration 168/1000 | Loss: 0.00001016
Iteration 169/1000 | Loss: 0.00001016
Iteration 170/1000 | Loss: 0.00001016
Iteration 171/1000 | Loss: 0.00001016
Iteration 172/1000 | Loss: 0.00001016
Iteration 173/1000 | Loss: 0.00001016
Iteration 174/1000 | Loss: 0.00001015
Iteration 175/1000 | Loss: 0.00001015
Iteration 176/1000 | Loss: 0.00001015
Iteration 177/1000 | Loss: 0.00001014
Iteration 178/1000 | Loss: 0.00001014
Iteration 179/1000 | Loss: 0.00001013
Iteration 180/1000 | Loss: 0.00001013
Iteration 181/1000 | Loss: 0.00001013
Iteration 182/1000 | Loss: 0.00001013
Iteration 183/1000 | Loss: 0.00001013
Iteration 184/1000 | Loss: 0.00001013
Iteration 185/1000 | Loss: 0.00001013
Iteration 186/1000 | Loss: 0.00001013
Iteration 187/1000 | Loss: 0.00001013
Iteration 188/1000 | Loss: 0.00001013
Iteration 189/1000 | Loss: 0.00001013
Iteration 190/1000 | Loss: 0.00001013
Iteration 191/1000 | Loss: 0.00001012
Iteration 192/1000 | Loss: 0.00001012
Iteration 193/1000 | Loss: 0.00001012
Iteration 194/1000 | Loss: 0.00001012
Iteration 195/1000 | Loss: 0.00001012
Iteration 196/1000 | Loss: 0.00001012
Iteration 197/1000 | Loss: 0.00001012
Iteration 198/1000 | Loss: 0.00001012
Iteration 199/1000 | Loss: 0.00001012
Iteration 200/1000 | Loss: 0.00001012
Iteration 201/1000 | Loss: 0.00001012
Iteration 202/1000 | Loss: 0.00001012
Iteration 203/1000 | Loss: 0.00001012
Iteration 204/1000 | Loss: 0.00001012
Iteration 205/1000 | Loss: 0.00001012
Iteration 206/1000 | Loss: 0.00001012
Iteration 207/1000 | Loss: 0.00001011
Iteration 208/1000 | Loss: 0.00001011
Iteration 209/1000 | Loss: 0.00001011
Iteration 210/1000 | Loss: 0.00001011
Iteration 211/1000 | Loss: 0.00001011
Iteration 212/1000 | Loss: 0.00001011
Iteration 213/1000 | Loss: 0.00001011
Iteration 214/1000 | Loss: 0.00001011
Iteration 215/1000 | Loss: 0.00001011
Iteration 216/1000 | Loss: 0.00001011
Iteration 217/1000 | Loss: 0.00001011
Iteration 218/1000 | Loss: 0.00001011
Iteration 219/1000 | Loss: 0.00001010
Iteration 220/1000 | Loss: 0.00001010
Iteration 221/1000 | Loss: 0.00001010
Iteration 222/1000 | Loss: 0.00001010
Iteration 223/1000 | Loss: 0.00001010
Iteration 224/1000 | Loss: 0.00001010
Iteration 225/1000 | Loss: 0.00001010
Iteration 226/1000 | Loss: 0.00001010
Iteration 227/1000 | Loss: 0.00001010
Iteration 228/1000 | Loss: 0.00001010
Iteration 229/1000 | Loss: 0.00001010
Iteration 230/1000 | Loss: 0.00001010
Iteration 231/1000 | Loss: 0.00001010
Iteration 232/1000 | Loss: 0.00001010
Iteration 233/1000 | Loss: 0.00001010
Iteration 234/1000 | Loss: 0.00001010
Iteration 235/1000 | Loss: 0.00001010
Iteration 236/1000 | Loss: 0.00001010
Iteration 237/1000 | Loss: 0.00001010
Iteration 238/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.010160667647142e-05, 1.010160667647142e-05, 1.010160667647142e-05, 1.010160667647142e-05, 1.010160667647142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.010160667647142e-05

Optimization complete. Final v2v error: 2.7332310676574707 mm

Highest mean error: 3.4226646423339844 mm for frame 90

Lowest mean error: 2.5847814083099365 mm for frame 9

Saving results

Total time: 149.48496890068054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464587
Iteration 2/25 | Loss: 0.00131842
Iteration 3/25 | Loss: 0.00123689
Iteration 4/25 | Loss: 0.00122474
Iteration 5/25 | Loss: 0.00122290
Iteration 6/25 | Loss: 0.00122290
Iteration 7/25 | Loss: 0.00122290
Iteration 8/25 | Loss: 0.00122290
Iteration 9/25 | Loss: 0.00122290
Iteration 10/25 | Loss: 0.00122290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012228959240019321, 0.0012228959240019321, 0.0012228959240019321, 0.0012228959240019321, 0.0012228959240019321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012228959240019321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32711518
Iteration 2/25 | Loss: 0.00128457
Iteration 3/25 | Loss: 0.00128457
Iteration 4/25 | Loss: 0.00128457
Iteration 5/25 | Loss: 0.00128456
Iteration 6/25 | Loss: 0.00128456
Iteration 7/25 | Loss: 0.00128456
Iteration 8/25 | Loss: 0.00128456
Iteration 9/25 | Loss: 0.00128456
Iteration 10/25 | Loss: 0.00128456
Iteration 11/25 | Loss: 0.00128456
Iteration 12/25 | Loss: 0.00128456
Iteration 13/25 | Loss: 0.00128456
Iteration 14/25 | Loss: 0.00128456
Iteration 15/25 | Loss: 0.00128456
Iteration 16/25 | Loss: 0.00128456
Iteration 17/25 | Loss: 0.00128456
Iteration 18/25 | Loss: 0.00128456
Iteration 19/25 | Loss: 0.00128456
Iteration 20/25 | Loss: 0.00128456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012845629826188087, 0.0012845629826188087, 0.0012845629826188087, 0.0012845629826188087, 0.0012845629826188087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012845629826188087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128456
Iteration 2/1000 | Loss: 0.00002901
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00002052
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00001892
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00001708
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001694
Iteration 16/1000 | Loss: 0.00001690
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001666
Iteration 20/1000 | Loss: 0.00001664
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001660
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001655
Iteration 32/1000 | Loss: 0.00001655
Iteration 33/1000 | Loss: 0.00001654
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001649
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001647
Iteration 40/1000 | Loss: 0.00001647
Iteration 41/1000 | Loss: 0.00001644
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001638
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001635
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001634
Iteration 49/1000 | Loss: 0.00001633
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001626
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001624
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001621
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001620
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001619
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001619
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00001619
Iteration 78/1000 | Loss: 0.00001619
Iteration 79/1000 | Loss: 0.00001619
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001617
Iteration 90/1000 | Loss: 0.00001617
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001616
Iteration 96/1000 | Loss: 0.00001616
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001614
Iteration 119/1000 | Loss: 0.00001614
Iteration 120/1000 | Loss: 0.00001614
Iteration 121/1000 | Loss: 0.00001614
Iteration 122/1000 | Loss: 0.00001614
Iteration 123/1000 | Loss: 0.00001614
Iteration 124/1000 | Loss: 0.00001614
Iteration 125/1000 | Loss: 0.00001614
Iteration 126/1000 | Loss: 0.00001613
Iteration 127/1000 | Loss: 0.00001613
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Iteration 131/1000 | Loss: 0.00001613
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6132345990627073e-05, 1.6132345990627073e-05, 1.6132345990627073e-05, 1.6132345990627073e-05, 1.6132345990627073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6132345990627073e-05

Optimization complete. Final v2v error: 3.383981227874756 mm

Highest mean error: 3.816244125366211 mm for frame 193

Lowest mean error: 2.793952226638794 mm for frame 66

Saving results

Total time: 43.47940230369568
