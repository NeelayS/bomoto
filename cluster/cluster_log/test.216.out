Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=216, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12096-12151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611001
Iteration 2/25 | Loss: 0.00151094
Iteration 3/25 | Loss: 0.00127744
Iteration 4/25 | Loss: 0.00125097
Iteration 5/25 | Loss: 0.00124409
Iteration 6/25 | Loss: 0.00124397
Iteration 7/25 | Loss: 0.00124397
Iteration 8/25 | Loss: 0.00124397
Iteration 9/25 | Loss: 0.00124397
Iteration 10/25 | Loss: 0.00124397
Iteration 11/25 | Loss: 0.00124397
Iteration 12/25 | Loss: 0.00124397
Iteration 13/25 | Loss: 0.00124397
Iteration 14/25 | Loss: 0.00124397
Iteration 15/25 | Loss: 0.00124397
Iteration 16/25 | Loss: 0.00124397
Iteration 17/25 | Loss: 0.00124397
Iteration 18/25 | Loss: 0.00124397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012439702404662967, 0.0012439702404662967, 0.0012439702404662967, 0.0012439702404662967, 0.0012439702404662967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012439702404662967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30856860
Iteration 2/25 | Loss: 0.00100522
Iteration 3/25 | Loss: 0.00100521
Iteration 4/25 | Loss: 0.00100521
Iteration 5/25 | Loss: 0.00100521
Iteration 6/25 | Loss: 0.00100521
Iteration 7/25 | Loss: 0.00100521
Iteration 8/25 | Loss: 0.00100521
Iteration 9/25 | Loss: 0.00100521
Iteration 10/25 | Loss: 0.00100521
Iteration 11/25 | Loss: 0.00100521
Iteration 12/25 | Loss: 0.00100521
Iteration 13/25 | Loss: 0.00100521
Iteration 14/25 | Loss: 0.00100521
Iteration 15/25 | Loss: 0.00100521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010052069555968046, 0.0010052069555968046, 0.0010052069555968046, 0.0010052069555968046, 0.0010052069555968046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010052069555968046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100521
Iteration 2/1000 | Loss: 0.00004519
Iteration 3/1000 | Loss: 0.00003426
Iteration 4/1000 | Loss: 0.00003179
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00003006
Iteration 7/1000 | Loss: 0.00002939
Iteration 8/1000 | Loss: 0.00002910
Iteration 9/1000 | Loss: 0.00002879
Iteration 10/1000 | Loss: 0.00002860
Iteration 11/1000 | Loss: 0.00002850
Iteration 12/1000 | Loss: 0.00002843
Iteration 13/1000 | Loss: 0.00002832
Iteration 14/1000 | Loss: 0.00002824
Iteration 15/1000 | Loss: 0.00002818
Iteration 16/1000 | Loss: 0.00002816
Iteration 17/1000 | Loss: 0.00002816
Iteration 18/1000 | Loss: 0.00002815
Iteration 19/1000 | Loss: 0.00002814
Iteration 20/1000 | Loss: 0.00002814
Iteration 21/1000 | Loss: 0.00002810
Iteration 22/1000 | Loss: 0.00002810
Iteration 23/1000 | Loss: 0.00002809
Iteration 24/1000 | Loss: 0.00002807
Iteration 25/1000 | Loss: 0.00002807
Iteration 26/1000 | Loss: 0.00002806
Iteration 27/1000 | Loss: 0.00002806
Iteration 28/1000 | Loss: 0.00002806
Iteration 29/1000 | Loss: 0.00002806
Iteration 30/1000 | Loss: 0.00002805
Iteration 31/1000 | Loss: 0.00002805
Iteration 32/1000 | Loss: 0.00002804
Iteration 33/1000 | Loss: 0.00002804
Iteration 34/1000 | Loss: 0.00002804
Iteration 35/1000 | Loss: 0.00002803
Iteration 36/1000 | Loss: 0.00002803
Iteration 37/1000 | Loss: 0.00002803
Iteration 38/1000 | Loss: 0.00002803
Iteration 39/1000 | Loss: 0.00002802
Iteration 40/1000 | Loss: 0.00002802
Iteration 41/1000 | Loss: 0.00002802
Iteration 42/1000 | Loss: 0.00002802
Iteration 43/1000 | Loss: 0.00002802
Iteration 44/1000 | Loss: 0.00002801
Iteration 45/1000 | Loss: 0.00002801
Iteration 46/1000 | Loss: 0.00002801
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002801
Iteration 49/1000 | Loss: 0.00002801
Iteration 50/1000 | Loss: 0.00002801
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002800
Iteration 53/1000 | Loss: 0.00002800
Iteration 54/1000 | Loss: 0.00002800
Iteration 55/1000 | Loss: 0.00002800
Iteration 56/1000 | Loss: 0.00002800
Iteration 57/1000 | Loss: 0.00002800
Iteration 58/1000 | Loss: 0.00002800
Iteration 59/1000 | Loss: 0.00002800
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002800
Iteration 62/1000 | Loss: 0.00002800
Iteration 63/1000 | Loss: 0.00002800
Iteration 64/1000 | Loss: 0.00002800
Iteration 65/1000 | Loss: 0.00002800
Iteration 66/1000 | Loss: 0.00002800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.8001131795463152e-05, 2.8001131795463152e-05, 2.8001131795463152e-05, 2.8001131795463152e-05, 2.8001131795463152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8001131795463152e-05

Optimization complete. Final v2v error: 4.375552654266357 mm

Highest mean error: 4.8091535568237305 mm for frame 38

Lowest mean error: 3.9973464012145996 mm for frame 0

Saving results

Total time: 36.077948570251465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453777
Iteration 2/25 | Loss: 0.00143215
Iteration 3/25 | Loss: 0.00121063
Iteration 4/25 | Loss: 0.00118597
Iteration 5/25 | Loss: 0.00118370
Iteration 6/25 | Loss: 0.00118370
Iteration 7/25 | Loss: 0.00118370
Iteration 8/25 | Loss: 0.00118370
Iteration 9/25 | Loss: 0.00118370
Iteration 10/25 | Loss: 0.00118370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011837044730782509, 0.0011837044730782509, 0.0011837044730782509, 0.0011837044730782509, 0.0011837044730782509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011837044730782509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36195529
Iteration 2/25 | Loss: 0.00060095
Iteration 3/25 | Loss: 0.00060095
Iteration 4/25 | Loss: 0.00060095
Iteration 5/25 | Loss: 0.00060095
Iteration 6/25 | Loss: 0.00060095
Iteration 7/25 | Loss: 0.00060095
Iteration 8/25 | Loss: 0.00060095
Iteration 9/25 | Loss: 0.00060095
Iteration 10/25 | Loss: 0.00060095
Iteration 11/25 | Loss: 0.00060095
Iteration 12/25 | Loss: 0.00060095
Iteration 13/25 | Loss: 0.00060095
Iteration 14/25 | Loss: 0.00060095
Iteration 15/25 | Loss: 0.00060095
Iteration 16/25 | Loss: 0.00060095
Iteration 17/25 | Loss: 0.00060095
Iteration 18/25 | Loss: 0.00060095
Iteration 19/25 | Loss: 0.00060095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006009450880810618, 0.0006009450880810618, 0.0006009450880810618, 0.0006009450880810618, 0.0006009450880810618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006009450880810618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060095
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002474
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00002005
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001960
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001925
Iteration 15/1000 | Loss: 0.00001923
Iteration 16/1000 | Loss: 0.00001923
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001921
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001915
Iteration 23/1000 | Loss: 0.00001915
Iteration 24/1000 | Loss: 0.00001914
Iteration 25/1000 | Loss: 0.00001914
Iteration 26/1000 | Loss: 0.00001913
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001907
Iteration 33/1000 | Loss: 0.00001907
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001906
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001906
Iteration 41/1000 | Loss: 0.00001906
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001905
Iteration 46/1000 | Loss: 0.00001905
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001904
Iteration 52/1000 | Loss: 0.00001904
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001903
Iteration 65/1000 | Loss: 0.00001903
Iteration 66/1000 | Loss: 0.00001903
Iteration 67/1000 | Loss: 0.00001902
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001902
Iteration 71/1000 | Loss: 0.00001902
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001900
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001900
Iteration 92/1000 | Loss: 0.00001900
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001899
Iteration 97/1000 | Loss: 0.00001899
Iteration 98/1000 | Loss: 0.00001899
Iteration 99/1000 | Loss: 0.00001899
Iteration 100/1000 | Loss: 0.00001899
Iteration 101/1000 | Loss: 0.00001899
Iteration 102/1000 | Loss: 0.00001899
Iteration 103/1000 | Loss: 0.00001899
Iteration 104/1000 | Loss: 0.00001899
Iteration 105/1000 | Loss: 0.00001898
Iteration 106/1000 | Loss: 0.00001898
Iteration 107/1000 | Loss: 0.00001898
Iteration 108/1000 | Loss: 0.00001898
Iteration 109/1000 | Loss: 0.00001898
Iteration 110/1000 | Loss: 0.00001898
Iteration 111/1000 | Loss: 0.00001898
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001898
Iteration 114/1000 | Loss: 0.00001897
Iteration 115/1000 | Loss: 0.00001897
Iteration 116/1000 | Loss: 0.00001897
Iteration 117/1000 | Loss: 0.00001897
Iteration 118/1000 | Loss: 0.00001897
Iteration 119/1000 | Loss: 0.00001897
Iteration 120/1000 | Loss: 0.00001897
Iteration 121/1000 | Loss: 0.00001897
Iteration 122/1000 | Loss: 0.00001897
Iteration 123/1000 | Loss: 0.00001897
Iteration 124/1000 | Loss: 0.00001896
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00001896
Iteration 127/1000 | Loss: 0.00001896
Iteration 128/1000 | Loss: 0.00001896
Iteration 129/1000 | Loss: 0.00001896
Iteration 130/1000 | Loss: 0.00001895
Iteration 131/1000 | Loss: 0.00001895
Iteration 132/1000 | Loss: 0.00001895
Iteration 133/1000 | Loss: 0.00001895
Iteration 134/1000 | Loss: 0.00001894
Iteration 135/1000 | Loss: 0.00001894
Iteration 136/1000 | Loss: 0.00001894
Iteration 137/1000 | Loss: 0.00001894
Iteration 138/1000 | Loss: 0.00001894
Iteration 139/1000 | Loss: 0.00001894
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001893
Iteration 143/1000 | Loss: 0.00001893
Iteration 144/1000 | Loss: 0.00001893
Iteration 145/1000 | Loss: 0.00001893
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001892
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Iteration 158/1000 | Loss: 0.00001892
Iteration 159/1000 | Loss: 0.00001892
Iteration 160/1000 | Loss: 0.00001892
Iteration 161/1000 | Loss: 0.00001892
Iteration 162/1000 | Loss: 0.00001892
Iteration 163/1000 | Loss: 0.00001892
Iteration 164/1000 | Loss: 0.00001892
Iteration 165/1000 | Loss: 0.00001892
Iteration 166/1000 | Loss: 0.00001892
Iteration 167/1000 | Loss: 0.00001892
Iteration 168/1000 | Loss: 0.00001892
Iteration 169/1000 | Loss: 0.00001892
Iteration 170/1000 | Loss: 0.00001892
Iteration 171/1000 | Loss: 0.00001892
Iteration 172/1000 | Loss: 0.00001892
Iteration 173/1000 | Loss: 0.00001892
Iteration 174/1000 | Loss: 0.00001892
Iteration 175/1000 | Loss: 0.00001892
Iteration 176/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.8919821741292253e-05, 1.8919821741292253e-05, 1.8919821741292253e-05, 1.8919821741292253e-05, 1.8919821741292253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8919821741292253e-05

Optimization complete. Final v2v error: 3.639444589614868 mm

Highest mean error: 3.8561482429504395 mm for frame 46

Lowest mean error: 3.3691470623016357 mm for frame 9

Saving results

Total time: 41.38137674331665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527781
Iteration 2/25 | Loss: 0.00117972
Iteration 3/25 | Loss: 0.00110206
Iteration 4/25 | Loss: 0.00109175
Iteration 5/25 | Loss: 0.00108856
Iteration 6/25 | Loss: 0.00108784
Iteration 7/25 | Loss: 0.00108784
Iteration 8/25 | Loss: 0.00108784
Iteration 9/25 | Loss: 0.00108784
Iteration 10/25 | Loss: 0.00108784
Iteration 11/25 | Loss: 0.00108784
Iteration 12/25 | Loss: 0.00108784
Iteration 13/25 | Loss: 0.00108784
Iteration 14/25 | Loss: 0.00108784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010878407629206777, 0.0010878407629206777, 0.0010878407629206777, 0.0010878407629206777, 0.0010878407629206777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010878407629206777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.71117020
Iteration 2/25 | Loss: 0.00078580
Iteration 3/25 | Loss: 0.00078578
Iteration 4/25 | Loss: 0.00078578
Iteration 5/25 | Loss: 0.00078578
Iteration 6/25 | Loss: 0.00078578
Iteration 7/25 | Loss: 0.00078578
Iteration 8/25 | Loss: 0.00078578
Iteration 9/25 | Loss: 0.00078578
Iteration 10/25 | Loss: 0.00078578
Iteration 11/25 | Loss: 0.00078578
Iteration 12/25 | Loss: 0.00078578
Iteration 13/25 | Loss: 0.00078578
Iteration 14/25 | Loss: 0.00078578
Iteration 15/25 | Loss: 0.00078578
Iteration 16/25 | Loss: 0.00078578
Iteration 17/25 | Loss: 0.00078578
Iteration 18/25 | Loss: 0.00078578
Iteration 19/25 | Loss: 0.00078578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007857807795517147, 0.0007857807795517147, 0.0007857807795517147, 0.0007857807795517147, 0.0007857807795517147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007857807795517147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078578
Iteration 2/1000 | Loss: 0.00002111
Iteration 3/1000 | Loss: 0.00001574
Iteration 4/1000 | Loss: 0.00001392
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001222
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001106
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001099
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001096
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001091
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001087
Iteration 36/1000 | Loss: 0.00001087
Iteration 37/1000 | Loss: 0.00001087
Iteration 38/1000 | Loss: 0.00001086
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001085
Iteration 42/1000 | Loss: 0.00001084
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001084
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001083
Iteration 48/1000 | Loss: 0.00001083
Iteration 49/1000 | Loss: 0.00001082
Iteration 50/1000 | Loss: 0.00001082
Iteration 51/1000 | Loss: 0.00001082
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001075
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001071
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001066
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001064
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001062
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001060
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001057
Iteration 100/1000 | Loss: 0.00001057
Iteration 101/1000 | Loss: 0.00001057
Iteration 102/1000 | Loss: 0.00001057
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001056
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001055
Iteration 108/1000 | Loss: 0.00001055
Iteration 109/1000 | Loss: 0.00001055
Iteration 110/1000 | Loss: 0.00001054
Iteration 111/1000 | Loss: 0.00001054
Iteration 112/1000 | Loss: 0.00001054
Iteration 113/1000 | Loss: 0.00001054
Iteration 114/1000 | Loss: 0.00001054
Iteration 115/1000 | Loss: 0.00001053
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001053
Iteration 118/1000 | Loss: 0.00001053
Iteration 119/1000 | Loss: 0.00001053
Iteration 120/1000 | Loss: 0.00001053
Iteration 121/1000 | Loss: 0.00001053
Iteration 122/1000 | Loss: 0.00001053
Iteration 123/1000 | Loss: 0.00001052
Iteration 124/1000 | Loss: 0.00001052
Iteration 125/1000 | Loss: 0.00001051
Iteration 126/1000 | Loss: 0.00001051
Iteration 127/1000 | Loss: 0.00001051
Iteration 128/1000 | Loss: 0.00001051
Iteration 129/1000 | Loss: 0.00001050
Iteration 130/1000 | Loss: 0.00001050
Iteration 131/1000 | Loss: 0.00001050
Iteration 132/1000 | Loss: 0.00001050
Iteration 133/1000 | Loss: 0.00001050
Iteration 134/1000 | Loss: 0.00001050
Iteration 135/1000 | Loss: 0.00001050
Iteration 136/1000 | Loss: 0.00001050
Iteration 137/1000 | Loss: 0.00001050
Iteration 138/1000 | Loss: 0.00001050
Iteration 139/1000 | Loss: 0.00001050
Iteration 140/1000 | Loss: 0.00001050
Iteration 141/1000 | Loss: 0.00001050
Iteration 142/1000 | Loss: 0.00001050
Iteration 143/1000 | Loss: 0.00001049
Iteration 144/1000 | Loss: 0.00001049
Iteration 145/1000 | Loss: 0.00001049
Iteration 146/1000 | Loss: 0.00001049
Iteration 147/1000 | Loss: 0.00001049
Iteration 148/1000 | Loss: 0.00001049
Iteration 149/1000 | Loss: 0.00001049
Iteration 150/1000 | Loss: 0.00001049
Iteration 151/1000 | Loss: 0.00001049
Iteration 152/1000 | Loss: 0.00001049
Iteration 153/1000 | Loss: 0.00001049
Iteration 154/1000 | Loss: 0.00001049
Iteration 155/1000 | Loss: 0.00001049
Iteration 156/1000 | Loss: 0.00001049
Iteration 157/1000 | Loss: 0.00001048
Iteration 158/1000 | Loss: 0.00001048
Iteration 159/1000 | Loss: 0.00001048
Iteration 160/1000 | Loss: 0.00001048
Iteration 161/1000 | Loss: 0.00001048
Iteration 162/1000 | Loss: 0.00001048
Iteration 163/1000 | Loss: 0.00001048
Iteration 164/1000 | Loss: 0.00001048
Iteration 165/1000 | Loss: 0.00001048
Iteration 166/1000 | Loss: 0.00001048
Iteration 167/1000 | Loss: 0.00001048
Iteration 168/1000 | Loss: 0.00001048
Iteration 169/1000 | Loss: 0.00001048
Iteration 170/1000 | Loss: 0.00001048
Iteration 171/1000 | Loss: 0.00001047
Iteration 172/1000 | Loss: 0.00001047
Iteration 173/1000 | Loss: 0.00001047
Iteration 174/1000 | Loss: 0.00001047
Iteration 175/1000 | Loss: 0.00001047
Iteration 176/1000 | Loss: 0.00001047
Iteration 177/1000 | Loss: 0.00001047
Iteration 178/1000 | Loss: 0.00001047
Iteration 179/1000 | Loss: 0.00001047
Iteration 180/1000 | Loss: 0.00001047
Iteration 181/1000 | Loss: 0.00001047
Iteration 182/1000 | Loss: 0.00001047
Iteration 183/1000 | Loss: 0.00001047
Iteration 184/1000 | Loss: 0.00001047
Iteration 185/1000 | Loss: 0.00001047
Iteration 186/1000 | Loss: 0.00001046
Iteration 187/1000 | Loss: 0.00001046
Iteration 188/1000 | Loss: 0.00001046
Iteration 189/1000 | Loss: 0.00001046
Iteration 190/1000 | Loss: 0.00001046
Iteration 191/1000 | Loss: 0.00001046
Iteration 192/1000 | Loss: 0.00001046
Iteration 193/1000 | Loss: 0.00001046
Iteration 194/1000 | Loss: 0.00001046
Iteration 195/1000 | Loss: 0.00001045
Iteration 196/1000 | Loss: 0.00001045
Iteration 197/1000 | Loss: 0.00001045
Iteration 198/1000 | Loss: 0.00001045
Iteration 199/1000 | Loss: 0.00001045
Iteration 200/1000 | Loss: 0.00001045
Iteration 201/1000 | Loss: 0.00001045
Iteration 202/1000 | Loss: 0.00001045
Iteration 203/1000 | Loss: 0.00001045
Iteration 204/1000 | Loss: 0.00001045
Iteration 205/1000 | Loss: 0.00001045
Iteration 206/1000 | Loss: 0.00001045
Iteration 207/1000 | Loss: 0.00001045
Iteration 208/1000 | Loss: 0.00001045
Iteration 209/1000 | Loss: 0.00001045
Iteration 210/1000 | Loss: 0.00001045
Iteration 211/1000 | Loss: 0.00001045
Iteration 212/1000 | Loss: 0.00001045
Iteration 213/1000 | Loss: 0.00001044
Iteration 214/1000 | Loss: 0.00001044
Iteration 215/1000 | Loss: 0.00001044
Iteration 216/1000 | Loss: 0.00001044
Iteration 217/1000 | Loss: 0.00001044
Iteration 218/1000 | Loss: 0.00001044
Iteration 219/1000 | Loss: 0.00001044
Iteration 220/1000 | Loss: 0.00001044
Iteration 221/1000 | Loss: 0.00001044
Iteration 222/1000 | Loss: 0.00001044
Iteration 223/1000 | Loss: 0.00001044
Iteration 224/1000 | Loss: 0.00001044
Iteration 225/1000 | Loss: 0.00001044
Iteration 226/1000 | Loss: 0.00001044
Iteration 227/1000 | Loss: 0.00001044
Iteration 228/1000 | Loss: 0.00001044
Iteration 229/1000 | Loss: 0.00001044
Iteration 230/1000 | Loss: 0.00001044
Iteration 231/1000 | Loss: 0.00001044
Iteration 232/1000 | Loss: 0.00001044
Iteration 233/1000 | Loss: 0.00001044
Iteration 234/1000 | Loss: 0.00001044
Iteration 235/1000 | Loss: 0.00001044
Iteration 236/1000 | Loss: 0.00001044
Iteration 237/1000 | Loss: 0.00001044
Iteration 238/1000 | Loss: 0.00001044
Iteration 239/1000 | Loss: 0.00001044
Iteration 240/1000 | Loss: 0.00001044
Iteration 241/1000 | Loss: 0.00001044
Iteration 242/1000 | Loss: 0.00001044
Iteration 243/1000 | Loss: 0.00001044
Iteration 244/1000 | Loss: 0.00001044
Iteration 245/1000 | Loss: 0.00001044
Iteration 246/1000 | Loss: 0.00001044
Iteration 247/1000 | Loss: 0.00001044
Iteration 248/1000 | Loss: 0.00001044
Iteration 249/1000 | Loss: 0.00001044
Iteration 250/1000 | Loss: 0.00001044
Iteration 251/1000 | Loss: 0.00001044
Iteration 252/1000 | Loss: 0.00001044
Iteration 253/1000 | Loss: 0.00001044
Iteration 254/1000 | Loss: 0.00001044
Iteration 255/1000 | Loss: 0.00001044
Iteration 256/1000 | Loss: 0.00001044
Iteration 257/1000 | Loss: 0.00001044
Iteration 258/1000 | Loss: 0.00001044
Iteration 259/1000 | Loss: 0.00001044
Iteration 260/1000 | Loss: 0.00001044
Iteration 261/1000 | Loss: 0.00001044
Iteration 262/1000 | Loss: 0.00001044
Iteration 263/1000 | Loss: 0.00001044
Iteration 264/1000 | Loss: 0.00001044
Iteration 265/1000 | Loss: 0.00001044
Iteration 266/1000 | Loss: 0.00001044
Iteration 267/1000 | Loss: 0.00001044
Iteration 268/1000 | Loss: 0.00001044
Iteration 269/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.0443807695992291e-05, 1.0443807695992291e-05, 1.0443807695992291e-05, 1.0443807695992291e-05, 1.0443807695992291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0443807695992291e-05

Optimization complete. Final v2v error: 2.7519896030426025 mm

Highest mean error: 3.5340583324432373 mm for frame 70

Lowest mean error: 2.4198665618896484 mm for frame 28

Saving results

Total time: 45.60256838798523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084307
Iteration 2/25 | Loss: 0.00176387
Iteration 3/25 | Loss: 0.00126607
Iteration 4/25 | Loss: 0.00124990
Iteration 5/25 | Loss: 0.00124730
Iteration 6/25 | Loss: 0.00122630
Iteration 7/25 | Loss: 0.00120885
Iteration 8/25 | Loss: 0.00120727
Iteration 9/25 | Loss: 0.00120078
Iteration 10/25 | Loss: 0.00119564
Iteration 11/25 | Loss: 0.00120909
Iteration 12/25 | Loss: 0.00120678
Iteration 13/25 | Loss: 0.00120586
Iteration 14/25 | Loss: 0.00120183
Iteration 15/25 | Loss: 0.00119505
Iteration 16/25 | Loss: 0.00119117
Iteration 17/25 | Loss: 0.00119509
Iteration 18/25 | Loss: 0.00119681
Iteration 19/25 | Loss: 0.00119477
Iteration 20/25 | Loss: 0.00119477
Iteration 21/25 | Loss: 0.00119358
Iteration 22/25 | Loss: 0.00119391
Iteration 23/25 | Loss: 0.00119500
Iteration 24/25 | Loss: 0.00118874
Iteration 25/25 | Loss: 0.00118736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05795193
Iteration 2/25 | Loss: 0.00074381
Iteration 3/25 | Loss: 0.00074381
Iteration 4/25 | Loss: 0.00074381
Iteration 5/25 | Loss: 0.00074381
Iteration 6/25 | Loss: 0.00074381
Iteration 7/25 | Loss: 0.00074380
Iteration 8/25 | Loss: 0.00074380
Iteration 9/25 | Loss: 0.00074380
Iteration 10/25 | Loss: 0.00074380
Iteration 11/25 | Loss: 0.00074380
Iteration 12/25 | Loss: 0.00074380
Iteration 13/25 | Loss: 0.00074380
Iteration 14/25 | Loss: 0.00074380
Iteration 15/25 | Loss: 0.00074380
Iteration 16/25 | Loss: 0.00074380
Iteration 17/25 | Loss: 0.00074380
Iteration 18/25 | Loss: 0.00074380
Iteration 19/25 | Loss: 0.00074380
Iteration 20/25 | Loss: 0.00074380
Iteration 21/25 | Loss: 0.00074380
Iteration 22/25 | Loss: 0.00074380
Iteration 23/25 | Loss: 0.00074380
Iteration 24/25 | Loss: 0.00074380
Iteration 25/25 | Loss: 0.00074380

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074380
Iteration 2/1000 | Loss: 0.00003753
Iteration 3/1000 | Loss: 0.00002491
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002095
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001990
Iteration 8/1000 | Loss: 0.00001942
Iteration 9/1000 | Loss: 0.00001906
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00002839
Iteration 14/1000 | Loss: 0.00001820
Iteration 15/1000 | Loss: 0.00001802
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00002828
Iteration 18/1000 | Loss: 0.00001780
Iteration 19/1000 | Loss: 0.00001780
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001779
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002437
Iteration 33/1000 | Loss: 0.00001774
Iteration 34/1000 | Loss: 0.00001774
Iteration 35/1000 | Loss: 0.00001774
Iteration 36/1000 | Loss: 0.00001774
Iteration 37/1000 | Loss: 0.00001773
Iteration 38/1000 | Loss: 0.00001773
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001773
Iteration 41/1000 | Loss: 0.00001773
Iteration 42/1000 | Loss: 0.00001773
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001773
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001773
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.772823634382803e-05, 1.772823634382803e-05, 1.772823634382803e-05, 1.772823634382803e-05, 1.772823634382803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.772823634382803e-05

Optimization complete. Final v2v error: 3.5098166465759277 mm

Highest mean error: 3.7501542568206787 mm for frame 131

Lowest mean error: 3.313584566116333 mm for frame 7

Saving results

Total time: 75.1599223613739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871803
Iteration 2/25 | Loss: 0.00236425
Iteration 3/25 | Loss: 0.00167680
Iteration 4/25 | Loss: 0.00147340
Iteration 5/25 | Loss: 0.00158905
Iteration 6/25 | Loss: 0.00157145
Iteration 7/25 | Loss: 0.00138427
Iteration 8/25 | Loss: 0.00130533
Iteration 9/25 | Loss: 0.00133816
Iteration 10/25 | Loss: 0.00126891
Iteration 11/25 | Loss: 0.00126895
Iteration 12/25 | Loss: 0.00126219
Iteration 13/25 | Loss: 0.00125150
Iteration 14/25 | Loss: 0.00125181
Iteration 15/25 | Loss: 0.00123604
Iteration 16/25 | Loss: 0.00123135
Iteration 17/25 | Loss: 0.00122607
Iteration 18/25 | Loss: 0.00122464
Iteration 19/25 | Loss: 0.00123905
Iteration 20/25 | Loss: 0.00123465
Iteration 21/25 | Loss: 0.00121794
Iteration 22/25 | Loss: 0.00121858
Iteration 23/25 | Loss: 0.00120881
Iteration 24/25 | Loss: 0.00121096
Iteration 25/25 | Loss: 0.00120906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45830369
Iteration 2/25 | Loss: 0.00084449
Iteration 3/25 | Loss: 0.00084430
Iteration 4/25 | Loss: 0.00084430
Iteration 5/25 | Loss: 0.00084430
Iteration 6/25 | Loss: 0.00084430
Iteration 7/25 | Loss: 0.00084430
Iteration 8/25 | Loss: 0.00084429
Iteration 9/25 | Loss: 0.00084429
Iteration 10/25 | Loss: 0.00084429
Iteration 11/25 | Loss: 0.00084429
Iteration 12/25 | Loss: 0.00084429
Iteration 13/25 | Loss: 0.00084429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000844294554553926, 0.000844294554553926, 0.000844294554553926, 0.000844294554553926, 0.000844294554553926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000844294554553926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084429
Iteration 2/1000 | Loss: 0.00007429
Iteration 3/1000 | Loss: 0.00018194
Iteration 4/1000 | Loss: 0.00005254
Iteration 5/1000 | Loss: 0.00004191
Iteration 6/1000 | Loss: 0.00003888
Iteration 7/1000 | Loss: 0.00003675
Iteration 8/1000 | Loss: 0.00003562
Iteration 9/1000 | Loss: 0.00008209
Iteration 10/1000 | Loss: 0.00004038
Iteration 11/1000 | Loss: 0.00012454
Iteration 12/1000 | Loss: 0.00003398
Iteration 13/1000 | Loss: 0.00003357
Iteration 14/1000 | Loss: 0.00003322
Iteration 15/1000 | Loss: 0.00003292
Iteration 16/1000 | Loss: 0.00003285
Iteration 17/1000 | Loss: 0.00003268
Iteration 18/1000 | Loss: 0.00009594
Iteration 19/1000 | Loss: 0.00003261
Iteration 20/1000 | Loss: 0.00007799
Iteration 21/1000 | Loss: 0.00003254
Iteration 22/1000 | Loss: 0.00003240
Iteration 23/1000 | Loss: 0.00003240
Iteration 24/1000 | Loss: 0.00003240
Iteration 25/1000 | Loss: 0.00003239
Iteration 26/1000 | Loss: 0.00003239
Iteration 27/1000 | Loss: 0.00003239
Iteration 28/1000 | Loss: 0.00003239
Iteration 29/1000 | Loss: 0.00003239
Iteration 30/1000 | Loss: 0.00003238
Iteration 31/1000 | Loss: 0.00003238
Iteration 32/1000 | Loss: 0.00003238
Iteration 33/1000 | Loss: 0.00003238
Iteration 34/1000 | Loss: 0.00003238
Iteration 35/1000 | Loss: 0.00003238
Iteration 36/1000 | Loss: 0.00003238
Iteration 37/1000 | Loss: 0.00003238
Iteration 38/1000 | Loss: 0.00003238
Iteration 39/1000 | Loss: 0.00003237
Iteration 40/1000 | Loss: 0.00003237
Iteration 41/1000 | Loss: 0.00003237
Iteration 42/1000 | Loss: 0.00003237
Iteration 43/1000 | Loss: 0.00003236
Iteration 44/1000 | Loss: 0.00003236
Iteration 45/1000 | Loss: 0.00003235
Iteration 46/1000 | Loss: 0.00003235
Iteration 47/1000 | Loss: 0.00003234
Iteration 48/1000 | Loss: 0.00003234
Iteration 49/1000 | Loss: 0.00003234
Iteration 50/1000 | Loss: 0.00003234
Iteration 51/1000 | Loss: 0.00003234
Iteration 52/1000 | Loss: 0.00003233
Iteration 53/1000 | Loss: 0.00003233
Iteration 54/1000 | Loss: 0.00003233
Iteration 55/1000 | Loss: 0.00003232
Iteration 56/1000 | Loss: 0.00003232
Iteration 57/1000 | Loss: 0.00003232
Iteration 58/1000 | Loss: 0.00003232
Iteration 59/1000 | Loss: 0.00003231
Iteration 60/1000 | Loss: 0.00003231
Iteration 61/1000 | Loss: 0.00003230
Iteration 62/1000 | Loss: 0.00003230
Iteration 63/1000 | Loss: 0.00003230
Iteration 64/1000 | Loss: 0.00003229
Iteration 65/1000 | Loss: 0.00003229
Iteration 66/1000 | Loss: 0.00003228
Iteration 67/1000 | Loss: 0.00003228
Iteration 68/1000 | Loss: 0.00003227
Iteration 69/1000 | Loss: 0.00003227
Iteration 70/1000 | Loss: 0.00003226
Iteration 71/1000 | Loss: 0.00003226
Iteration 72/1000 | Loss: 0.00003226
Iteration 73/1000 | Loss: 0.00003226
Iteration 74/1000 | Loss: 0.00003226
Iteration 75/1000 | Loss: 0.00003225
Iteration 76/1000 | Loss: 0.00003225
Iteration 77/1000 | Loss: 0.00003225
Iteration 78/1000 | Loss: 0.00003224
Iteration 79/1000 | Loss: 0.00003224
Iteration 80/1000 | Loss: 0.00003224
Iteration 81/1000 | Loss: 0.00003224
Iteration 82/1000 | Loss: 0.00003223
Iteration 83/1000 | Loss: 0.00003223
Iteration 84/1000 | Loss: 0.00003223
Iteration 85/1000 | Loss: 0.00003222
Iteration 86/1000 | Loss: 0.00003222
Iteration 87/1000 | Loss: 0.00003222
Iteration 88/1000 | Loss: 0.00003222
Iteration 89/1000 | Loss: 0.00003222
Iteration 90/1000 | Loss: 0.00003222
Iteration 91/1000 | Loss: 0.00003221
Iteration 92/1000 | Loss: 0.00003221
Iteration 93/1000 | Loss: 0.00003221
Iteration 94/1000 | Loss: 0.00003221
Iteration 95/1000 | Loss: 0.00003221
Iteration 96/1000 | Loss: 0.00003221
Iteration 97/1000 | Loss: 0.00003221
Iteration 98/1000 | Loss: 0.00003221
Iteration 99/1000 | Loss: 0.00003221
Iteration 100/1000 | Loss: 0.00003221
Iteration 101/1000 | Loss: 0.00003221
Iteration 102/1000 | Loss: 0.00003221
Iteration 103/1000 | Loss: 0.00003221
Iteration 104/1000 | Loss: 0.00003221
Iteration 105/1000 | Loss: 0.00003221
Iteration 106/1000 | Loss: 0.00003221
Iteration 107/1000 | Loss: 0.00003221
Iteration 108/1000 | Loss: 0.00003221
Iteration 109/1000 | Loss: 0.00003221
Iteration 110/1000 | Loss: 0.00003221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.221274528186768e-05, 3.221274528186768e-05, 3.221274528186768e-05, 3.221274528186768e-05, 3.221274528186768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.221274528186768e-05

Optimization complete. Final v2v error: 4.165031433105469 mm

Highest mean error: 11.859190940856934 mm for frame 33

Lowest mean error: 3.3839306831359863 mm for frame 59

Saving results

Total time: 93.86739110946655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818426
Iteration 2/25 | Loss: 0.00118233
Iteration 3/25 | Loss: 0.00109881
Iteration 4/25 | Loss: 0.00107917
Iteration 5/25 | Loss: 0.00107525
Iteration 6/25 | Loss: 0.00107469
Iteration 7/25 | Loss: 0.00107469
Iteration 8/25 | Loss: 0.00107469
Iteration 9/25 | Loss: 0.00107469
Iteration 10/25 | Loss: 0.00107469
Iteration 11/25 | Loss: 0.00107469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001074685249477625, 0.001074685249477625, 0.001074685249477625, 0.001074685249477625, 0.001074685249477625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001074685249477625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37316930
Iteration 2/25 | Loss: 0.00076815
Iteration 3/25 | Loss: 0.00076815
Iteration 4/25 | Loss: 0.00076814
Iteration 5/25 | Loss: 0.00076814
Iteration 6/25 | Loss: 0.00076814
Iteration 7/25 | Loss: 0.00076814
Iteration 8/25 | Loss: 0.00076814
Iteration 9/25 | Loss: 0.00076814
Iteration 10/25 | Loss: 0.00076814
Iteration 11/25 | Loss: 0.00076814
Iteration 12/25 | Loss: 0.00076814
Iteration 13/25 | Loss: 0.00076814
Iteration 14/25 | Loss: 0.00076814
Iteration 15/25 | Loss: 0.00076814
Iteration 16/25 | Loss: 0.00076814
Iteration 17/25 | Loss: 0.00076814
Iteration 18/25 | Loss: 0.00076814
Iteration 19/25 | Loss: 0.00076814
Iteration 20/25 | Loss: 0.00076814
Iteration 21/25 | Loss: 0.00076814
Iteration 22/25 | Loss: 0.00076814
Iteration 23/25 | Loss: 0.00076814
Iteration 24/25 | Loss: 0.00076814
Iteration 25/25 | Loss: 0.00076814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076814
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00001698
Iteration 4/1000 | Loss: 0.00001419
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001296
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001143
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001142
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001135
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001131
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001123
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001121
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001117
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001115
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001109
Iteration 75/1000 | Loss: 0.00001109
Iteration 76/1000 | Loss: 0.00001109
Iteration 77/1000 | Loss: 0.00001108
Iteration 78/1000 | Loss: 0.00001108
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001107
Iteration 82/1000 | Loss: 0.00001107
Iteration 83/1000 | Loss: 0.00001106
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001100
Iteration 101/1000 | Loss: 0.00001100
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001099
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001099
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001097
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001096
Iteration 118/1000 | Loss: 0.00001096
Iteration 119/1000 | Loss: 0.00001096
Iteration 120/1000 | Loss: 0.00001096
Iteration 121/1000 | Loss: 0.00001096
Iteration 122/1000 | Loss: 0.00001096
Iteration 123/1000 | Loss: 0.00001096
Iteration 124/1000 | Loss: 0.00001096
Iteration 125/1000 | Loss: 0.00001096
Iteration 126/1000 | Loss: 0.00001096
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001095
Iteration 129/1000 | Loss: 0.00001095
Iteration 130/1000 | Loss: 0.00001095
Iteration 131/1000 | Loss: 0.00001095
Iteration 132/1000 | Loss: 0.00001095
Iteration 133/1000 | Loss: 0.00001095
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001094
Iteration 136/1000 | Loss: 0.00001094
Iteration 137/1000 | Loss: 0.00001094
Iteration 138/1000 | Loss: 0.00001094
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001094
Iteration 141/1000 | Loss: 0.00001094
Iteration 142/1000 | Loss: 0.00001094
Iteration 143/1000 | Loss: 0.00001093
Iteration 144/1000 | Loss: 0.00001093
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001093
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001092
Iteration 159/1000 | Loss: 0.00001092
Iteration 160/1000 | Loss: 0.00001092
Iteration 161/1000 | Loss: 0.00001092
Iteration 162/1000 | Loss: 0.00001092
Iteration 163/1000 | Loss: 0.00001092
Iteration 164/1000 | Loss: 0.00001092
Iteration 165/1000 | Loss: 0.00001092
Iteration 166/1000 | Loss: 0.00001092
Iteration 167/1000 | Loss: 0.00001092
Iteration 168/1000 | Loss: 0.00001092
Iteration 169/1000 | Loss: 0.00001092
Iteration 170/1000 | Loss: 0.00001092
Iteration 171/1000 | Loss: 0.00001092
Iteration 172/1000 | Loss: 0.00001092
Iteration 173/1000 | Loss: 0.00001092
Iteration 174/1000 | Loss: 0.00001092
Iteration 175/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.0920053682639264e-05, 1.0920053682639264e-05, 1.0920053682639264e-05, 1.0920053682639264e-05, 1.0920053682639264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0920053682639264e-05

Optimization complete. Final v2v error: 2.8675272464752197 mm

Highest mean error: 2.9196019172668457 mm for frame 65

Lowest mean error: 2.8045430183410645 mm for frame 52

Saving results

Total time: 39.302833795547485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709295
Iteration 2/25 | Loss: 0.00123174
Iteration 3/25 | Loss: 0.00113469
Iteration 4/25 | Loss: 0.00111639
Iteration 5/25 | Loss: 0.00110956
Iteration 6/25 | Loss: 0.00110776
Iteration 7/25 | Loss: 0.00110767
Iteration 8/25 | Loss: 0.00110767
Iteration 9/25 | Loss: 0.00110767
Iteration 10/25 | Loss: 0.00110767
Iteration 11/25 | Loss: 0.00110767
Iteration 12/25 | Loss: 0.00110767
Iteration 13/25 | Loss: 0.00110767
Iteration 14/25 | Loss: 0.00110767
Iteration 15/25 | Loss: 0.00110767
Iteration 16/25 | Loss: 0.00110767
Iteration 17/25 | Loss: 0.00110767
Iteration 18/25 | Loss: 0.00110767
Iteration 19/25 | Loss: 0.00110767
Iteration 20/25 | Loss: 0.00110767
Iteration 21/25 | Loss: 0.00110767
Iteration 22/25 | Loss: 0.00110767
Iteration 23/25 | Loss: 0.00110767
Iteration 24/25 | Loss: 0.00110767
Iteration 25/25 | Loss: 0.00110767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.20400286
Iteration 2/25 | Loss: 0.00083669
Iteration 3/25 | Loss: 0.00083669
Iteration 4/25 | Loss: 0.00083668
Iteration 5/25 | Loss: 0.00083668
Iteration 6/25 | Loss: 0.00083668
Iteration 7/25 | Loss: 0.00083668
Iteration 8/25 | Loss: 0.00083668
Iteration 9/25 | Loss: 0.00083668
Iteration 10/25 | Loss: 0.00083668
Iteration 11/25 | Loss: 0.00083668
Iteration 12/25 | Loss: 0.00083668
Iteration 13/25 | Loss: 0.00083668
Iteration 14/25 | Loss: 0.00083668
Iteration 15/25 | Loss: 0.00083668
Iteration 16/25 | Loss: 0.00083668
Iteration 17/25 | Loss: 0.00083668
Iteration 18/25 | Loss: 0.00083668
Iteration 19/25 | Loss: 0.00083668
Iteration 20/25 | Loss: 0.00083668
Iteration 21/25 | Loss: 0.00083668
Iteration 22/25 | Loss: 0.00083668
Iteration 23/25 | Loss: 0.00083668
Iteration 24/25 | Loss: 0.00083668
Iteration 25/25 | Loss: 0.00083668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083668
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00001801
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001347
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001286
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001286
Iteration 35/1000 | Loss: 0.00001286
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001281
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001280
Iteration 46/1000 | Loss: 0.00001279
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001279
Iteration 54/1000 | Loss: 0.00001279
Iteration 55/1000 | Loss: 0.00001279
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001278
Iteration 59/1000 | Loss: 0.00001278
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001271
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001267
Iteration 96/1000 | Loss: 0.00001267
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001264
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001261
Iteration 143/1000 | Loss: 0.00001261
Iteration 144/1000 | Loss: 0.00001261
Iteration 145/1000 | Loss: 0.00001261
Iteration 146/1000 | Loss: 0.00001261
Iteration 147/1000 | Loss: 0.00001261
Iteration 148/1000 | Loss: 0.00001261
Iteration 149/1000 | Loss: 0.00001261
Iteration 150/1000 | Loss: 0.00001261
Iteration 151/1000 | Loss: 0.00001261
Iteration 152/1000 | Loss: 0.00001261
Iteration 153/1000 | Loss: 0.00001261
Iteration 154/1000 | Loss: 0.00001261
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001261
Iteration 157/1000 | Loss: 0.00001261
Iteration 158/1000 | Loss: 0.00001261
Iteration 159/1000 | Loss: 0.00001261
Iteration 160/1000 | Loss: 0.00001261
Iteration 161/1000 | Loss: 0.00001261
Iteration 162/1000 | Loss: 0.00001261
Iteration 163/1000 | Loss: 0.00001261
Iteration 164/1000 | Loss: 0.00001261
Iteration 165/1000 | Loss: 0.00001261
Iteration 166/1000 | Loss: 0.00001261
Iteration 167/1000 | Loss: 0.00001261
Iteration 168/1000 | Loss: 0.00001261
Iteration 169/1000 | Loss: 0.00001261
Iteration 170/1000 | Loss: 0.00001261
Iteration 171/1000 | Loss: 0.00001261
Iteration 172/1000 | Loss: 0.00001261
Iteration 173/1000 | Loss: 0.00001261
Iteration 174/1000 | Loss: 0.00001261
Iteration 175/1000 | Loss: 0.00001261
Iteration 176/1000 | Loss: 0.00001261
Iteration 177/1000 | Loss: 0.00001261
Iteration 178/1000 | Loss: 0.00001261
Iteration 179/1000 | Loss: 0.00001261
Iteration 180/1000 | Loss: 0.00001261
Iteration 181/1000 | Loss: 0.00001261
Iteration 182/1000 | Loss: 0.00001261
Iteration 183/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.2611755664693192e-05, 1.2611755664693192e-05, 1.2611755664693192e-05, 1.2611755664693192e-05, 1.2611755664693192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2611755664693192e-05

Optimization complete. Final v2v error: 3.0725114345550537 mm

Highest mean error: 3.3222219944000244 mm for frame 98

Lowest mean error: 2.819815158843994 mm for frame 3

Saving results

Total time: 39.935988426208496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044400
Iteration 2/25 | Loss: 0.00286013
Iteration 3/25 | Loss: 0.00227281
Iteration 4/25 | Loss: 0.00182039
Iteration 5/25 | Loss: 0.00175143
Iteration 6/25 | Loss: 0.00147441
Iteration 7/25 | Loss: 0.00141908
Iteration 8/25 | Loss: 0.00133617
Iteration 9/25 | Loss: 0.00127656
Iteration 10/25 | Loss: 0.00123907
Iteration 11/25 | Loss: 0.00120890
Iteration 12/25 | Loss: 0.00119115
Iteration 13/25 | Loss: 0.00120615
Iteration 14/25 | Loss: 0.00117739
Iteration 15/25 | Loss: 0.00116220
Iteration 16/25 | Loss: 0.00116359
Iteration 17/25 | Loss: 0.00115971
Iteration 18/25 | Loss: 0.00115313
Iteration 19/25 | Loss: 0.00115257
Iteration 20/25 | Loss: 0.00114393
Iteration 21/25 | Loss: 0.00114010
Iteration 22/25 | Loss: 0.00113843
Iteration 23/25 | Loss: 0.00113801
Iteration 24/25 | Loss: 0.00113588
Iteration 25/25 | Loss: 0.00113318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30959404
Iteration 2/25 | Loss: 0.00205441
Iteration 3/25 | Loss: 0.00115279
Iteration 4/25 | Loss: 0.00115279
Iteration 5/25 | Loss: 0.00115279
Iteration 6/25 | Loss: 0.00115279
Iteration 7/25 | Loss: 0.00115279
Iteration 8/25 | Loss: 0.00115279
Iteration 9/25 | Loss: 0.00115279
Iteration 10/25 | Loss: 0.00115279
Iteration 11/25 | Loss: 0.00115279
Iteration 12/25 | Loss: 0.00115279
Iteration 13/25 | Loss: 0.00115279
Iteration 14/25 | Loss: 0.00115279
Iteration 15/25 | Loss: 0.00115278
Iteration 16/25 | Loss: 0.00115278
Iteration 17/25 | Loss: 0.00115278
Iteration 18/25 | Loss: 0.00115278
Iteration 19/25 | Loss: 0.00115278
Iteration 20/25 | Loss: 0.00115278
Iteration 21/25 | Loss: 0.00115278
Iteration 22/25 | Loss: 0.00115278
Iteration 23/25 | Loss: 0.00115278
Iteration 24/25 | Loss: 0.00115278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011527846800163388, 0.0011527846800163388, 0.0011527846800163388, 0.0011527846800163388, 0.0011527846800163388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011527846800163388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115278
Iteration 2/1000 | Loss: 0.00213668
Iteration 3/1000 | Loss: 0.00004855
Iteration 4/1000 | Loss: 0.00003759
Iteration 5/1000 | Loss: 0.00003207
Iteration 6/1000 | Loss: 0.00002959
Iteration 7/1000 | Loss: 0.00002839
Iteration 8/1000 | Loss: 0.00002741
Iteration 9/1000 | Loss: 0.00002648
Iteration 10/1000 | Loss: 0.00099618
Iteration 11/1000 | Loss: 0.00415743
Iteration 12/1000 | Loss: 0.00235254
Iteration 13/1000 | Loss: 0.00111111
Iteration 14/1000 | Loss: 0.00007343
Iteration 15/1000 | Loss: 0.00003940
Iteration 16/1000 | Loss: 0.00002679
Iteration 17/1000 | Loss: 0.00002393
Iteration 18/1000 | Loss: 0.00002152
Iteration 19/1000 | Loss: 0.00002036
Iteration 20/1000 | Loss: 0.00001949
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001821
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001679
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001676
Iteration 32/1000 | Loss: 0.00001676
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001674
Iteration 36/1000 | Loss: 0.00001666
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001643
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001642
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001641
Iteration 52/1000 | Loss: 0.00001641
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001624
Iteration 118/1000 | Loss: 0.00001624
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001623
Iteration 122/1000 | Loss: 0.00001623
Iteration 123/1000 | Loss: 0.00001623
Iteration 124/1000 | Loss: 0.00001623
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001622
Iteration 129/1000 | Loss: 0.00001622
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001622
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001622
Iteration 142/1000 | Loss: 0.00001622
Iteration 143/1000 | Loss: 0.00001622
Iteration 144/1000 | Loss: 0.00001622
Iteration 145/1000 | Loss: 0.00001622
Iteration 146/1000 | Loss: 0.00001622
Iteration 147/1000 | Loss: 0.00001622
Iteration 148/1000 | Loss: 0.00001622
Iteration 149/1000 | Loss: 0.00001622
Iteration 150/1000 | Loss: 0.00001622
Iteration 151/1000 | Loss: 0.00001622
Iteration 152/1000 | Loss: 0.00001622
Iteration 153/1000 | Loss: 0.00001622
Iteration 154/1000 | Loss: 0.00001622
Iteration 155/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.6222909835050814e-05, 1.6222909835050814e-05, 1.6222909835050814e-05, 1.6222909835050814e-05, 1.6222909835050814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6222909835050814e-05

Optimization complete. Final v2v error: 3.4191713333129883 mm

Highest mean error: 4.51342248916626 mm for frame 56

Lowest mean error: 2.9768879413604736 mm for frame 167

Saving results

Total time: 102.81173372268677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008763
Iteration 2/25 | Loss: 0.00226515
Iteration 3/25 | Loss: 0.00166233
Iteration 4/25 | Loss: 0.00190637
Iteration 5/25 | Loss: 0.00129392
Iteration 6/25 | Loss: 0.00122278
Iteration 7/25 | Loss: 0.00121934
Iteration 8/25 | Loss: 0.00121601
Iteration 9/25 | Loss: 0.00120817
Iteration 10/25 | Loss: 0.00119818
Iteration 11/25 | Loss: 0.00119611
Iteration 12/25 | Loss: 0.00119554
Iteration 13/25 | Loss: 0.00119545
Iteration 14/25 | Loss: 0.00119544
Iteration 15/25 | Loss: 0.00119544
Iteration 16/25 | Loss: 0.00119544
Iteration 17/25 | Loss: 0.00119544
Iteration 18/25 | Loss: 0.00119543
Iteration 19/25 | Loss: 0.00119543
Iteration 20/25 | Loss: 0.00119543
Iteration 21/25 | Loss: 0.00119543
Iteration 22/25 | Loss: 0.00119543
Iteration 23/25 | Loss: 0.00119542
Iteration 24/25 | Loss: 0.00119542
Iteration 25/25 | Loss: 0.00119542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33042657
Iteration 2/25 | Loss: 0.00082965
Iteration 3/25 | Loss: 0.00082965
Iteration 4/25 | Loss: 0.00082965
Iteration 5/25 | Loss: 0.00082965
Iteration 6/25 | Loss: 0.00082965
Iteration 7/25 | Loss: 0.00082965
Iteration 8/25 | Loss: 0.00082965
Iteration 9/25 | Loss: 0.00082965
Iteration 10/25 | Loss: 0.00082965
Iteration 11/25 | Loss: 0.00082965
Iteration 12/25 | Loss: 0.00082965
Iteration 13/25 | Loss: 0.00082965
Iteration 14/25 | Loss: 0.00082965
Iteration 15/25 | Loss: 0.00082965
Iteration 16/25 | Loss: 0.00082965
Iteration 17/25 | Loss: 0.00082965
Iteration 18/25 | Loss: 0.00082965
Iteration 19/25 | Loss: 0.00082965
Iteration 20/25 | Loss: 0.00082965
Iteration 21/25 | Loss: 0.00082965
Iteration 22/25 | Loss: 0.00082965
Iteration 23/25 | Loss: 0.00082965
Iteration 24/25 | Loss: 0.00082965
Iteration 25/25 | Loss: 0.00082965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082965
Iteration 2/1000 | Loss: 0.00015632
Iteration 3/1000 | Loss: 0.00003850
Iteration 4/1000 | Loss: 0.00002731
Iteration 5/1000 | Loss: 0.00002299
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001881
Iteration 10/1000 | Loss: 0.00001839
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001794
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001757
Iteration 18/1000 | Loss: 0.00001757
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001748
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00001747
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001745
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001742
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001741
Iteration 51/1000 | Loss: 0.00001741
Iteration 52/1000 | Loss: 0.00001741
Iteration 53/1000 | Loss: 0.00001741
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.7406224287697114e-05, 1.7406224287697114e-05, 1.7406224287697114e-05, 1.7406224287697114e-05, 1.7406224287697114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7406224287697114e-05

Optimization complete. Final v2v error: 3.4770636558532715 mm

Highest mean error: 3.9048330783843994 mm for frame 3

Lowest mean error: 3.392221450805664 mm for frame 10

Saving results

Total time: 47.4975049495697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008517
Iteration 2/25 | Loss: 0.00261788
Iteration 3/25 | Loss: 0.00208321
Iteration 4/25 | Loss: 0.00181892
Iteration 5/25 | Loss: 0.00195898
Iteration 6/25 | Loss: 0.00166894
Iteration 7/25 | Loss: 0.00135337
Iteration 8/25 | Loss: 0.00122010
Iteration 9/25 | Loss: 0.00121036
Iteration 10/25 | Loss: 0.00116577
Iteration 11/25 | Loss: 0.00119887
Iteration 12/25 | Loss: 0.00117934
Iteration 13/25 | Loss: 0.00115310
Iteration 14/25 | Loss: 0.00114887
Iteration 15/25 | Loss: 0.00114972
Iteration 16/25 | Loss: 0.00114666
Iteration 17/25 | Loss: 0.00114277
Iteration 18/25 | Loss: 0.00114761
Iteration 19/25 | Loss: 0.00114869
Iteration 20/25 | Loss: 0.00113965
Iteration 21/25 | Loss: 0.00113254
Iteration 22/25 | Loss: 0.00113399
Iteration 23/25 | Loss: 0.00113124
Iteration 24/25 | Loss: 0.00112988
Iteration 25/25 | Loss: 0.00112812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36931586
Iteration 2/25 | Loss: 0.00139337
Iteration 3/25 | Loss: 0.00088060
Iteration 4/25 | Loss: 0.00088059
Iteration 5/25 | Loss: 0.00088059
Iteration 6/25 | Loss: 0.00088059
Iteration 7/25 | Loss: 0.00088059
Iteration 8/25 | Loss: 0.00088059
Iteration 9/25 | Loss: 0.00088059
Iteration 10/25 | Loss: 0.00088059
Iteration 11/25 | Loss: 0.00088059
Iteration 12/25 | Loss: 0.00088059
Iteration 13/25 | Loss: 0.00088059
Iteration 14/25 | Loss: 0.00088059
Iteration 15/25 | Loss: 0.00088059
Iteration 16/25 | Loss: 0.00088059
Iteration 17/25 | Loss: 0.00088059
Iteration 18/25 | Loss: 0.00088059
Iteration 19/25 | Loss: 0.00088059
Iteration 20/25 | Loss: 0.00088059
Iteration 21/25 | Loss: 0.00088059
Iteration 22/25 | Loss: 0.00088059
Iteration 23/25 | Loss: 0.00088059
Iteration 24/25 | Loss: 0.00088059
Iteration 25/25 | Loss: 0.00088059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088059
Iteration 2/1000 | Loss: 0.00048315
Iteration 3/1000 | Loss: 0.00003945
Iteration 4/1000 | Loss: 0.00003333
Iteration 5/1000 | Loss: 0.00003098
Iteration 6/1000 | Loss: 0.00002921
Iteration 7/1000 | Loss: 0.00002801
Iteration 8/1000 | Loss: 0.00002698
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00002591
Iteration 11/1000 | Loss: 0.00252014
Iteration 12/1000 | Loss: 0.00002493
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001540
Iteration 16/1000 | Loss: 0.00001432
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001196
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001112
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001107
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001106
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001102
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001100
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001098
Iteration 57/1000 | Loss: 0.00001098
Iteration 58/1000 | Loss: 0.00001098
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001097
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001094
Iteration 83/1000 | Loss: 0.00001094
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001094
Iteration 87/1000 | Loss: 0.00001094
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001093
Iteration 91/1000 | Loss: 0.00001093
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001093
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001090
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.0903243492066395e-05, 1.0903243492066395e-05, 1.0903243492066395e-05, 1.0903243492066395e-05, 1.0903243492066395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0903243492066395e-05

Optimization complete. Final v2v error: 2.8172216415405273 mm

Highest mean error: 3.4873108863830566 mm for frame 73

Lowest mean error: 2.5461134910583496 mm for frame 124

Saving results

Total time: 89.29433012008667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786749
Iteration 2/25 | Loss: 0.00212143
Iteration 3/25 | Loss: 0.00166850
Iteration 4/25 | Loss: 0.00157532
Iteration 5/25 | Loss: 0.00147141
Iteration 6/25 | Loss: 0.00143946
Iteration 7/25 | Loss: 0.00137396
Iteration 8/25 | Loss: 0.00134416
Iteration 9/25 | Loss: 0.00136967
Iteration 10/25 | Loss: 0.00140967
Iteration 11/25 | Loss: 0.00133981
Iteration 12/25 | Loss: 0.00133278
Iteration 13/25 | Loss: 0.00129854
Iteration 14/25 | Loss: 0.00128641
Iteration 15/25 | Loss: 0.00128924
Iteration 16/25 | Loss: 0.00128557
Iteration 17/25 | Loss: 0.00128202
Iteration 18/25 | Loss: 0.00128135
Iteration 19/25 | Loss: 0.00128112
Iteration 20/25 | Loss: 0.00128103
Iteration 21/25 | Loss: 0.00128101
Iteration 22/25 | Loss: 0.00128101
Iteration 23/25 | Loss: 0.00128100
Iteration 24/25 | Loss: 0.00128100
Iteration 25/25 | Loss: 0.00128100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34321642
Iteration 2/25 | Loss: 0.00064608
Iteration 3/25 | Loss: 0.00064607
Iteration 4/25 | Loss: 0.00064606
Iteration 5/25 | Loss: 0.00064606
Iteration 6/25 | Loss: 0.00064606
Iteration 7/25 | Loss: 0.00064606
Iteration 8/25 | Loss: 0.00064606
Iteration 9/25 | Loss: 0.00064606
Iteration 10/25 | Loss: 0.00064606
Iteration 11/25 | Loss: 0.00064606
Iteration 12/25 | Loss: 0.00064606
Iteration 13/25 | Loss: 0.00064606
Iteration 14/25 | Loss: 0.00064606
Iteration 15/25 | Loss: 0.00064606
Iteration 16/25 | Loss: 0.00064606
Iteration 17/25 | Loss: 0.00064606
Iteration 18/25 | Loss: 0.00064606
Iteration 19/25 | Loss: 0.00064606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006460627191700041, 0.0006460627191700041, 0.0006460627191700041, 0.0006460627191700041, 0.0006460627191700041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006460627191700041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064606
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002724
Iteration 5/1000 | Loss: 0.00002629
Iteration 6/1000 | Loss: 0.00002568
Iteration 7/1000 | Loss: 0.00002537
Iteration 8/1000 | Loss: 0.00002510
Iteration 9/1000 | Loss: 0.00002510
Iteration 10/1000 | Loss: 0.00002509
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002481
Iteration 13/1000 | Loss: 0.00002465
Iteration 14/1000 | Loss: 0.00002464
Iteration 15/1000 | Loss: 0.00002464
Iteration 16/1000 | Loss: 0.00002464
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002459
Iteration 19/1000 | Loss: 0.00002458
Iteration 20/1000 | Loss: 0.00002456
Iteration 21/1000 | Loss: 0.00002455
Iteration 22/1000 | Loss: 0.00002454
Iteration 23/1000 | Loss: 0.00002454
Iteration 24/1000 | Loss: 0.00002453
Iteration 25/1000 | Loss: 0.00002453
Iteration 26/1000 | Loss: 0.00002452
Iteration 27/1000 | Loss: 0.00002452
Iteration 28/1000 | Loss: 0.00002451
Iteration 29/1000 | Loss: 0.00002451
Iteration 30/1000 | Loss: 0.00002443
Iteration 31/1000 | Loss: 0.00002440
Iteration 32/1000 | Loss: 0.00002440
Iteration 33/1000 | Loss: 0.00002440
Iteration 34/1000 | Loss: 0.00002439
Iteration 35/1000 | Loss: 0.00002439
Iteration 36/1000 | Loss: 0.00002439
Iteration 37/1000 | Loss: 0.00002439
Iteration 38/1000 | Loss: 0.00002439
Iteration 39/1000 | Loss: 0.00002439
Iteration 40/1000 | Loss: 0.00002439
Iteration 41/1000 | Loss: 0.00002439
Iteration 42/1000 | Loss: 0.00002439
Iteration 43/1000 | Loss: 0.00002439
Iteration 44/1000 | Loss: 0.00002439
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002438
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002437
Iteration 50/1000 | Loss: 0.00002437
Iteration 51/1000 | Loss: 0.00002437
Iteration 52/1000 | Loss: 0.00002437
Iteration 53/1000 | Loss: 0.00002436
Iteration 54/1000 | Loss: 0.00002436
Iteration 55/1000 | Loss: 0.00002436
Iteration 56/1000 | Loss: 0.00002436
Iteration 57/1000 | Loss: 0.00002436
Iteration 58/1000 | Loss: 0.00002436
Iteration 59/1000 | Loss: 0.00002436
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002436
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002435
Iteration 64/1000 | Loss: 0.00002435
Iteration 65/1000 | Loss: 0.00002435
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002433
Iteration 69/1000 | Loss: 0.00002433
Iteration 70/1000 | Loss: 0.00002433
Iteration 71/1000 | Loss: 0.00002433
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002433
Iteration 75/1000 | Loss: 0.00002433
Iteration 76/1000 | Loss: 0.00002433
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002432
Iteration 79/1000 | Loss: 0.00002432
Iteration 80/1000 | Loss: 0.00002432
Iteration 81/1000 | Loss: 0.00002432
Iteration 82/1000 | Loss: 0.00002432
Iteration 83/1000 | Loss: 0.00002432
Iteration 84/1000 | Loss: 0.00002431
Iteration 85/1000 | Loss: 0.00002431
Iteration 86/1000 | Loss: 0.00002431
Iteration 87/1000 | Loss: 0.00002431
Iteration 88/1000 | Loss: 0.00002431
Iteration 89/1000 | Loss: 0.00002430
Iteration 90/1000 | Loss: 0.00002430
Iteration 91/1000 | Loss: 0.00002430
Iteration 92/1000 | Loss: 0.00002430
Iteration 93/1000 | Loss: 0.00002430
Iteration 94/1000 | Loss: 0.00002430
Iteration 95/1000 | Loss: 0.00002430
Iteration 96/1000 | Loss: 0.00002429
Iteration 97/1000 | Loss: 0.00002429
Iteration 98/1000 | Loss: 0.00002429
Iteration 99/1000 | Loss: 0.00002428
Iteration 100/1000 | Loss: 0.00002428
Iteration 101/1000 | Loss: 0.00002428
Iteration 102/1000 | Loss: 0.00002428
Iteration 103/1000 | Loss: 0.00002428
Iteration 104/1000 | Loss: 0.00002428
Iteration 105/1000 | Loss: 0.00002428
Iteration 106/1000 | Loss: 0.00002428
Iteration 107/1000 | Loss: 0.00002428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.4284548999276012e-05, 2.4284548999276012e-05, 2.4284548999276012e-05, 2.4284548999276012e-05, 2.4284548999276012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4284548999276012e-05

Optimization complete. Final v2v error: 4.122283935546875 mm

Highest mean error: 4.377063751220703 mm for frame 7

Lowest mean error: 3.8204450607299805 mm for frame 154

Saving results

Total time: 59.33458733558655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812752
Iteration 2/25 | Loss: 0.00119872
Iteration 3/25 | Loss: 0.00109969
Iteration 4/25 | Loss: 0.00108925
Iteration 5/25 | Loss: 0.00108664
Iteration 6/25 | Loss: 0.00108656
Iteration 7/25 | Loss: 0.00108656
Iteration 8/25 | Loss: 0.00108656
Iteration 9/25 | Loss: 0.00108656
Iteration 10/25 | Loss: 0.00108656
Iteration 11/25 | Loss: 0.00108656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010865571675822139, 0.0010865571675822139, 0.0010865571675822139, 0.0010865571675822139, 0.0010865571675822139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010865571675822139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34904027
Iteration 2/25 | Loss: 0.00091233
Iteration 3/25 | Loss: 0.00091233
Iteration 4/25 | Loss: 0.00091233
Iteration 5/25 | Loss: 0.00091232
Iteration 6/25 | Loss: 0.00091232
Iteration 7/25 | Loss: 0.00091232
Iteration 8/25 | Loss: 0.00091232
Iteration 9/25 | Loss: 0.00091232
Iteration 10/25 | Loss: 0.00091232
Iteration 11/25 | Loss: 0.00091232
Iteration 12/25 | Loss: 0.00091232
Iteration 13/25 | Loss: 0.00091232
Iteration 14/25 | Loss: 0.00091232
Iteration 15/25 | Loss: 0.00091232
Iteration 16/25 | Loss: 0.00091232
Iteration 17/25 | Loss: 0.00091232
Iteration 18/25 | Loss: 0.00091232
Iteration 19/25 | Loss: 0.00091232
Iteration 20/25 | Loss: 0.00091232
Iteration 21/25 | Loss: 0.00091232
Iteration 22/25 | Loss: 0.00091232
Iteration 23/25 | Loss: 0.00091232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009123222553171217, 0.0009123222553171217, 0.0009123222553171217, 0.0009123222553171217, 0.0009123222553171217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009123222553171217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091232
Iteration 2/1000 | Loss: 0.00002722
Iteration 3/1000 | Loss: 0.00001665
Iteration 4/1000 | Loss: 0.00001405
Iteration 5/1000 | Loss: 0.00001269
Iteration 6/1000 | Loss: 0.00001176
Iteration 7/1000 | Loss: 0.00001124
Iteration 8/1000 | Loss: 0.00001079
Iteration 9/1000 | Loss: 0.00001059
Iteration 10/1000 | Loss: 0.00001048
Iteration 11/1000 | Loss: 0.00001018
Iteration 12/1000 | Loss: 0.00001012
Iteration 13/1000 | Loss: 0.00001005
Iteration 14/1000 | Loss: 0.00001002
Iteration 15/1000 | Loss: 0.00001001
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00000997
Iteration 18/1000 | Loss: 0.00000997
Iteration 19/1000 | Loss: 0.00000996
Iteration 20/1000 | Loss: 0.00000994
Iteration 21/1000 | Loss: 0.00000982
Iteration 22/1000 | Loss: 0.00000980
Iteration 23/1000 | Loss: 0.00000980
Iteration 24/1000 | Loss: 0.00000980
Iteration 25/1000 | Loss: 0.00000979
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000979
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000978
Iteration 30/1000 | Loss: 0.00000977
Iteration 31/1000 | Loss: 0.00000976
Iteration 32/1000 | Loss: 0.00000976
Iteration 33/1000 | Loss: 0.00000975
Iteration 34/1000 | Loss: 0.00000973
Iteration 35/1000 | Loss: 0.00000973
Iteration 36/1000 | Loss: 0.00000972
Iteration 37/1000 | Loss: 0.00000971
Iteration 38/1000 | Loss: 0.00000970
Iteration 39/1000 | Loss: 0.00000970
Iteration 40/1000 | Loss: 0.00000970
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000968
Iteration 44/1000 | Loss: 0.00000967
Iteration 45/1000 | Loss: 0.00000967
Iteration 46/1000 | Loss: 0.00000966
Iteration 47/1000 | Loss: 0.00000966
Iteration 48/1000 | Loss: 0.00000965
Iteration 49/1000 | Loss: 0.00000965
Iteration 50/1000 | Loss: 0.00000965
Iteration 51/1000 | Loss: 0.00000965
Iteration 52/1000 | Loss: 0.00000964
Iteration 53/1000 | Loss: 0.00000964
Iteration 54/1000 | Loss: 0.00000963
Iteration 55/1000 | Loss: 0.00000963
Iteration 56/1000 | Loss: 0.00000963
Iteration 57/1000 | Loss: 0.00000963
Iteration 58/1000 | Loss: 0.00000962
Iteration 59/1000 | Loss: 0.00000962
Iteration 60/1000 | Loss: 0.00000962
Iteration 61/1000 | Loss: 0.00000962
Iteration 62/1000 | Loss: 0.00000961
Iteration 63/1000 | Loss: 0.00000961
Iteration 64/1000 | Loss: 0.00000961
Iteration 65/1000 | Loss: 0.00000961
Iteration 66/1000 | Loss: 0.00000960
Iteration 67/1000 | Loss: 0.00000960
Iteration 68/1000 | Loss: 0.00000959
Iteration 69/1000 | Loss: 0.00000959
Iteration 70/1000 | Loss: 0.00000959
Iteration 71/1000 | Loss: 0.00000958
Iteration 72/1000 | Loss: 0.00000958
Iteration 73/1000 | Loss: 0.00000958
Iteration 74/1000 | Loss: 0.00000958
Iteration 75/1000 | Loss: 0.00000958
Iteration 76/1000 | Loss: 0.00000957
Iteration 77/1000 | Loss: 0.00000957
Iteration 78/1000 | Loss: 0.00000957
Iteration 79/1000 | Loss: 0.00000957
Iteration 80/1000 | Loss: 0.00000957
Iteration 81/1000 | Loss: 0.00000956
Iteration 82/1000 | Loss: 0.00000956
Iteration 83/1000 | Loss: 0.00000956
Iteration 84/1000 | Loss: 0.00000956
Iteration 85/1000 | Loss: 0.00000955
Iteration 86/1000 | Loss: 0.00000955
Iteration 87/1000 | Loss: 0.00000955
Iteration 88/1000 | Loss: 0.00000955
Iteration 89/1000 | Loss: 0.00000955
Iteration 90/1000 | Loss: 0.00000954
Iteration 91/1000 | Loss: 0.00000954
Iteration 92/1000 | Loss: 0.00000954
Iteration 93/1000 | Loss: 0.00000954
Iteration 94/1000 | Loss: 0.00000954
Iteration 95/1000 | Loss: 0.00000954
Iteration 96/1000 | Loss: 0.00000954
Iteration 97/1000 | Loss: 0.00000953
Iteration 98/1000 | Loss: 0.00000953
Iteration 99/1000 | Loss: 0.00000953
Iteration 100/1000 | Loss: 0.00000952
Iteration 101/1000 | Loss: 0.00000952
Iteration 102/1000 | Loss: 0.00000952
Iteration 103/1000 | Loss: 0.00000952
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000951
Iteration 106/1000 | Loss: 0.00000951
Iteration 107/1000 | Loss: 0.00000951
Iteration 108/1000 | Loss: 0.00000951
Iteration 109/1000 | Loss: 0.00000950
Iteration 110/1000 | Loss: 0.00000950
Iteration 111/1000 | Loss: 0.00000950
Iteration 112/1000 | Loss: 0.00000950
Iteration 113/1000 | Loss: 0.00000950
Iteration 114/1000 | Loss: 0.00000950
Iteration 115/1000 | Loss: 0.00000949
Iteration 116/1000 | Loss: 0.00000949
Iteration 117/1000 | Loss: 0.00000949
Iteration 118/1000 | Loss: 0.00000949
Iteration 119/1000 | Loss: 0.00000948
Iteration 120/1000 | Loss: 0.00000948
Iteration 121/1000 | Loss: 0.00000948
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000947
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000946
Iteration 133/1000 | Loss: 0.00000946
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000945
Iteration 139/1000 | Loss: 0.00000945
Iteration 140/1000 | Loss: 0.00000945
Iteration 141/1000 | Loss: 0.00000945
Iteration 142/1000 | Loss: 0.00000945
Iteration 143/1000 | Loss: 0.00000945
Iteration 144/1000 | Loss: 0.00000944
Iteration 145/1000 | Loss: 0.00000944
Iteration 146/1000 | Loss: 0.00000944
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000943
Iteration 150/1000 | Loss: 0.00000943
Iteration 151/1000 | Loss: 0.00000943
Iteration 152/1000 | Loss: 0.00000943
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000943
Iteration 155/1000 | Loss: 0.00000943
Iteration 156/1000 | Loss: 0.00000943
Iteration 157/1000 | Loss: 0.00000942
Iteration 158/1000 | Loss: 0.00000942
Iteration 159/1000 | Loss: 0.00000942
Iteration 160/1000 | Loss: 0.00000942
Iteration 161/1000 | Loss: 0.00000942
Iteration 162/1000 | Loss: 0.00000942
Iteration 163/1000 | Loss: 0.00000942
Iteration 164/1000 | Loss: 0.00000941
Iteration 165/1000 | Loss: 0.00000941
Iteration 166/1000 | Loss: 0.00000941
Iteration 167/1000 | Loss: 0.00000941
Iteration 168/1000 | Loss: 0.00000940
Iteration 169/1000 | Loss: 0.00000940
Iteration 170/1000 | Loss: 0.00000940
Iteration 171/1000 | Loss: 0.00000940
Iteration 172/1000 | Loss: 0.00000940
Iteration 173/1000 | Loss: 0.00000940
Iteration 174/1000 | Loss: 0.00000940
Iteration 175/1000 | Loss: 0.00000940
Iteration 176/1000 | Loss: 0.00000940
Iteration 177/1000 | Loss: 0.00000940
Iteration 178/1000 | Loss: 0.00000940
Iteration 179/1000 | Loss: 0.00000940
Iteration 180/1000 | Loss: 0.00000939
Iteration 181/1000 | Loss: 0.00000939
Iteration 182/1000 | Loss: 0.00000939
Iteration 183/1000 | Loss: 0.00000939
Iteration 184/1000 | Loss: 0.00000939
Iteration 185/1000 | Loss: 0.00000939
Iteration 186/1000 | Loss: 0.00000939
Iteration 187/1000 | Loss: 0.00000939
Iteration 188/1000 | Loss: 0.00000939
Iteration 189/1000 | Loss: 0.00000939
Iteration 190/1000 | Loss: 0.00000939
Iteration 191/1000 | Loss: 0.00000939
Iteration 192/1000 | Loss: 0.00000939
Iteration 193/1000 | Loss: 0.00000939
Iteration 194/1000 | Loss: 0.00000939
Iteration 195/1000 | Loss: 0.00000939
Iteration 196/1000 | Loss: 0.00000939
Iteration 197/1000 | Loss: 0.00000939
Iteration 198/1000 | Loss: 0.00000939
Iteration 199/1000 | Loss: 0.00000939
Iteration 200/1000 | Loss: 0.00000939
Iteration 201/1000 | Loss: 0.00000938
Iteration 202/1000 | Loss: 0.00000938
Iteration 203/1000 | Loss: 0.00000938
Iteration 204/1000 | Loss: 0.00000938
Iteration 205/1000 | Loss: 0.00000938
Iteration 206/1000 | Loss: 0.00000938
Iteration 207/1000 | Loss: 0.00000938
Iteration 208/1000 | Loss: 0.00000938
Iteration 209/1000 | Loss: 0.00000938
Iteration 210/1000 | Loss: 0.00000938
Iteration 211/1000 | Loss: 0.00000938
Iteration 212/1000 | Loss: 0.00000938
Iteration 213/1000 | Loss: 0.00000938
Iteration 214/1000 | Loss: 0.00000938
Iteration 215/1000 | Loss: 0.00000938
Iteration 216/1000 | Loss: 0.00000938
Iteration 217/1000 | Loss: 0.00000938
Iteration 218/1000 | Loss: 0.00000938
Iteration 219/1000 | Loss: 0.00000938
Iteration 220/1000 | Loss: 0.00000938
Iteration 221/1000 | Loss: 0.00000938
Iteration 222/1000 | Loss: 0.00000938
Iteration 223/1000 | Loss: 0.00000938
Iteration 224/1000 | Loss: 0.00000937
Iteration 225/1000 | Loss: 0.00000937
Iteration 226/1000 | Loss: 0.00000937
Iteration 227/1000 | Loss: 0.00000937
Iteration 228/1000 | Loss: 0.00000937
Iteration 229/1000 | Loss: 0.00000937
Iteration 230/1000 | Loss: 0.00000937
Iteration 231/1000 | Loss: 0.00000937
Iteration 232/1000 | Loss: 0.00000937
Iteration 233/1000 | Loss: 0.00000937
Iteration 234/1000 | Loss: 0.00000937
Iteration 235/1000 | Loss: 0.00000937
Iteration 236/1000 | Loss: 0.00000937
Iteration 237/1000 | Loss: 0.00000937
Iteration 238/1000 | Loss: 0.00000937
Iteration 239/1000 | Loss: 0.00000937
Iteration 240/1000 | Loss: 0.00000937
Iteration 241/1000 | Loss: 0.00000937
Iteration 242/1000 | Loss: 0.00000937
Iteration 243/1000 | Loss: 0.00000937
Iteration 244/1000 | Loss: 0.00000937
Iteration 245/1000 | Loss: 0.00000937
Iteration 246/1000 | Loss: 0.00000937
Iteration 247/1000 | Loss: 0.00000936
Iteration 248/1000 | Loss: 0.00000936
Iteration 249/1000 | Loss: 0.00000936
Iteration 250/1000 | Loss: 0.00000936
Iteration 251/1000 | Loss: 0.00000936
Iteration 252/1000 | Loss: 0.00000936
Iteration 253/1000 | Loss: 0.00000936
Iteration 254/1000 | Loss: 0.00000936
Iteration 255/1000 | Loss: 0.00000936
Iteration 256/1000 | Loss: 0.00000936
Iteration 257/1000 | Loss: 0.00000936
Iteration 258/1000 | Loss: 0.00000936
Iteration 259/1000 | Loss: 0.00000936
Iteration 260/1000 | Loss: 0.00000936
Iteration 261/1000 | Loss: 0.00000936
Iteration 262/1000 | Loss: 0.00000936
Iteration 263/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [9.364338438899722e-06, 9.364338438899722e-06, 9.364338438899722e-06, 9.364338438899722e-06, 9.364338438899722e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.364338438899722e-06

Optimization complete. Final v2v error: 2.5751190185546875 mm

Highest mean error: 3.634253740310669 mm for frame 67

Lowest mean error: 2.2317094802856445 mm for frame 143

Saving results

Total time: 45.272810220718384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387213
Iteration 2/25 | Loss: 0.00115199
Iteration 3/25 | Loss: 0.00109413
Iteration 4/25 | Loss: 0.00108735
Iteration 5/25 | Loss: 0.00108505
Iteration 6/25 | Loss: 0.00108467
Iteration 7/25 | Loss: 0.00108467
Iteration 8/25 | Loss: 0.00108467
Iteration 9/25 | Loss: 0.00108467
Iteration 10/25 | Loss: 0.00108467
Iteration 11/25 | Loss: 0.00108467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010846741497516632, 0.0010846741497516632, 0.0010846741497516632, 0.0010846741497516632, 0.0010846741497516632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010846741497516632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37722826
Iteration 2/25 | Loss: 0.00084865
Iteration 3/25 | Loss: 0.00084865
Iteration 4/25 | Loss: 0.00084865
Iteration 5/25 | Loss: 0.00084865
Iteration 6/25 | Loss: 0.00084865
Iteration 7/25 | Loss: 0.00084865
Iteration 8/25 | Loss: 0.00084864
Iteration 9/25 | Loss: 0.00084864
Iteration 10/25 | Loss: 0.00084864
Iteration 11/25 | Loss: 0.00084864
Iteration 12/25 | Loss: 0.00084864
Iteration 13/25 | Loss: 0.00084864
Iteration 14/25 | Loss: 0.00084864
Iteration 15/25 | Loss: 0.00084864
Iteration 16/25 | Loss: 0.00084864
Iteration 17/25 | Loss: 0.00084864
Iteration 18/25 | Loss: 0.00084864
Iteration 19/25 | Loss: 0.00084864
Iteration 20/25 | Loss: 0.00084864
Iteration 21/25 | Loss: 0.00084864
Iteration 22/25 | Loss: 0.00084864
Iteration 23/25 | Loss: 0.00084864
Iteration 24/25 | Loss: 0.00084864
Iteration 25/25 | Loss: 0.00084864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084864
Iteration 2/1000 | Loss: 0.00002024
Iteration 3/1000 | Loss: 0.00001285
Iteration 4/1000 | Loss: 0.00001155
Iteration 5/1000 | Loss: 0.00001103
Iteration 6/1000 | Loss: 0.00001082
Iteration 7/1000 | Loss: 0.00001057
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001023
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001020
Iteration 13/1000 | Loss: 0.00001014
Iteration 14/1000 | Loss: 0.00001013
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001006
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001001
Iteration 20/1000 | Loss: 0.00001000
Iteration 21/1000 | Loss: 0.00000998
Iteration 22/1000 | Loss: 0.00000996
Iteration 23/1000 | Loss: 0.00000995
Iteration 24/1000 | Loss: 0.00000995
Iteration 25/1000 | Loss: 0.00000993
Iteration 26/1000 | Loss: 0.00000993
Iteration 27/1000 | Loss: 0.00000993
Iteration 28/1000 | Loss: 0.00000993
Iteration 29/1000 | Loss: 0.00000993
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000992
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000990
Iteration 37/1000 | Loss: 0.00000989
Iteration 38/1000 | Loss: 0.00000989
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000983
Iteration 43/1000 | Loss: 0.00000983
Iteration 44/1000 | Loss: 0.00000983
Iteration 45/1000 | Loss: 0.00000983
Iteration 46/1000 | Loss: 0.00000983
Iteration 47/1000 | Loss: 0.00000983
Iteration 48/1000 | Loss: 0.00000981
Iteration 49/1000 | Loss: 0.00000980
Iteration 50/1000 | Loss: 0.00000980
Iteration 51/1000 | Loss: 0.00000980
Iteration 52/1000 | Loss: 0.00000980
Iteration 53/1000 | Loss: 0.00000979
Iteration 54/1000 | Loss: 0.00000979
Iteration 55/1000 | Loss: 0.00000979
Iteration 56/1000 | Loss: 0.00000979
Iteration 57/1000 | Loss: 0.00000979
Iteration 58/1000 | Loss: 0.00000979
Iteration 59/1000 | Loss: 0.00000979
Iteration 60/1000 | Loss: 0.00000978
Iteration 61/1000 | Loss: 0.00000978
Iteration 62/1000 | Loss: 0.00000977
Iteration 63/1000 | Loss: 0.00000977
Iteration 64/1000 | Loss: 0.00000977
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000976
Iteration 68/1000 | Loss: 0.00000976
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000975
Iteration 72/1000 | Loss: 0.00000975
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000973
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000972
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000972
Iteration 83/1000 | Loss: 0.00000972
Iteration 84/1000 | Loss: 0.00000972
Iteration 85/1000 | Loss: 0.00000972
Iteration 86/1000 | Loss: 0.00000971
Iteration 87/1000 | Loss: 0.00000971
Iteration 88/1000 | Loss: 0.00000971
Iteration 89/1000 | Loss: 0.00000971
Iteration 90/1000 | Loss: 0.00000971
Iteration 91/1000 | Loss: 0.00000971
Iteration 92/1000 | Loss: 0.00000971
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000971
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000968
Iteration 106/1000 | Loss: 0.00000968
Iteration 107/1000 | Loss: 0.00000968
Iteration 108/1000 | Loss: 0.00000968
Iteration 109/1000 | Loss: 0.00000968
Iteration 110/1000 | Loss: 0.00000968
Iteration 111/1000 | Loss: 0.00000968
Iteration 112/1000 | Loss: 0.00000968
Iteration 113/1000 | Loss: 0.00000968
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000967
Iteration 121/1000 | Loss: 0.00000967
Iteration 122/1000 | Loss: 0.00000967
Iteration 123/1000 | Loss: 0.00000967
Iteration 124/1000 | Loss: 0.00000967
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000965
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000964
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000964
Iteration 136/1000 | Loss: 0.00000963
Iteration 137/1000 | Loss: 0.00000963
Iteration 138/1000 | Loss: 0.00000963
Iteration 139/1000 | Loss: 0.00000963
Iteration 140/1000 | Loss: 0.00000963
Iteration 141/1000 | Loss: 0.00000963
Iteration 142/1000 | Loss: 0.00000963
Iteration 143/1000 | Loss: 0.00000963
Iteration 144/1000 | Loss: 0.00000963
Iteration 145/1000 | Loss: 0.00000963
Iteration 146/1000 | Loss: 0.00000963
Iteration 147/1000 | Loss: 0.00000962
Iteration 148/1000 | Loss: 0.00000962
Iteration 149/1000 | Loss: 0.00000962
Iteration 150/1000 | Loss: 0.00000962
Iteration 151/1000 | Loss: 0.00000962
Iteration 152/1000 | Loss: 0.00000962
Iteration 153/1000 | Loss: 0.00000962
Iteration 154/1000 | Loss: 0.00000962
Iteration 155/1000 | Loss: 0.00000962
Iteration 156/1000 | Loss: 0.00000961
Iteration 157/1000 | Loss: 0.00000961
Iteration 158/1000 | Loss: 0.00000961
Iteration 159/1000 | Loss: 0.00000961
Iteration 160/1000 | Loss: 0.00000961
Iteration 161/1000 | Loss: 0.00000961
Iteration 162/1000 | Loss: 0.00000961
Iteration 163/1000 | Loss: 0.00000961
Iteration 164/1000 | Loss: 0.00000961
Iteration 165/1000 | Loss: 0.00000961
Iteration 166/1000 | Loss: 0.00000961
Iteration 167/1000 | Loss: 0.00000961
Iteration 168/1000 | Loss: 0.00000961
Iteration 169/1000 | Loss: 0.00000961
Iteration 170/1000 | Loss: 0.00000961
Iteration 171/1000 | Loss: 0.00000961
Iteration 172/1000 | Loss: 0.00000961
Iteration 173/1000 | Loss: 0.00000961
Iteration 174/1000 | Loss: 0.00000961
Iteration 175/1000 | Loss: 0.00000961
Iteration 176/1000 | Loss: 0.00000961
Iteration 177/1000 | Loss: 0.00000961
Iteration 178/1000 | Loss: 0.00000961
Iteration 179/1000 | Loss: 0.00000961
Iteration 180/1000 | Loss: 0.00000961
Iteration 181/1000 | Loss: 0.00000961
Iteration 182/1000 | Loss: 0.00000961
Iteration 183/1000 | Loss: 0.00000961
Iteration 184/1000 | Loss: 0.00000961
Iteration 185/1000 | Loss: 0.00000961
Iteration 186/1000 | Loss: 0.00000961
Iteration 187/1000 | Loss: 0.00000961
Iteration 188/1000 | Loss: 0.00000961
Iteration 189/1000 | Loss: 0.00000961
Iteration 190/1000 | Loss: 0.00000961
Iteration 191/1000 | Loss: 0.00000961
Iteration 192/1000 | Loss: 0.00000961
Iteration 193/1000 | Loss: 0.00000961
Iteration 194/1000 | Loss: 0.00000961
Iteration 195/1000 | Loss: 0.00000961
Iteration 196/1000 | Loss: 0.00000961
Iteration 197/1000 | Loss: 0.00000961
Iteration 198/1000 | Loss: 0.00000961
Iteration 199/1000 | Loss: 0.00000961
Iteration 200/1000 | Loss: 0.00000961
Iteration 201/1000 | Loss: 0.00000961
Iteration 202/1000 | Loss: 0.00000961
Iteration 203/1000 | Loss: 0.00000961
Iteration 204/1000 | Loss: 0.00000961
Iteration 205/1000 | Loss: 0.00000961
Iteration 206/1000 | Loss: 0.00000961
Iteration 207/1000 | Loss: 0.00000961
Iteration 208/1000 | Loss: 0.00000961
Iteration 209/1000 | Loss: 0.00000961
Iteration 210/1000 | Loss: 0.00000961
Iteration 211/1000 | Loss: 0.00000961
Iteration 212/1000 | Loss: 0.00000961
Iteration 213/1000 | Loss: 0.00000961
Iteration 214/1000 | Loss: 0.00000961
Iteration 215/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [9.606756975699682e-06, 9.606756975699682e-06, 9.606756975699682e-06, 9.606756975699682e-06, 9.606756975699682e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.606756975699682e-06

Optimization complete. Final v2v error: 2.66800594329834 mm

Highest mean error: 2.877962589263916 mm for frame 105

Lowest mean error: 2.5740129947662354 mm for frame 89

Saving results

Total time: 35.24667406082153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460308
Iteration 2/25 | Loss: 0.00144465
Iteration 3/25 | Loss: 0.00119491
Iteration 4/25 | Loss: 0.00116300
Iteration 5/25 | Loss: 0.00115352
Iteration 6/25 | Loss: 0.00115140
Iteration 7/25 | Loss: 0.00115068
Iteration 8/25 | Loss: 0.00115068
Iteration 9/25 | Loss: 0.00115068
Iteration 10/25 | Loss: 0.00115068
Iteration 11/25 | Loss: 0.00115068
Iteration 12/25 | Loss: 0.00115068
Iteration 13/25 | Loss: 0.00115068
Iteration 14/25 | Loss: 0.00115068
Iteration 15/25 | Loss: 0.00115068
Iteration 16/25 | Loss: 0.00115068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011506830342113972, 0.0011506830342113972, 0.0011506830342113972, 0.0011506830342113972, 0.0011506830342113972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011506830342113972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18233931
Iteration 2/25 | Loss: 0.00100663
Iteration 3/25 | Loss: 0.00100661
Iteration 4/25 | Loss: 0.00100661
Iteration 5/25 | Loss: 0.00100661
Iteration 6/25 | Loss: 0.00100661
Iteration 7/25 | Loss: 0.00100661
Iteration 8/25 | Loss: 0.00100660
Iteration 9/25 | Loss: 0.00100660
Iteration 10/25 | Loss: 0.00100660
Iteration 11/25 | Loss: 0.00100660
Iteration 12/25 | Loss: 0.00100660
Iteration 13/25 | Loss: 0.00100660
Iteration 14/25 | Loss: 0.00100660
Iteration 15/25 | Loss: 0.00100660
Iteration 16/25 | Loss: 0.00100660
Iteration 17/25 | Loss: 0.00100660
Iteration 18/25 | Loss: 0.00100660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010066041722893715, 0.0010066041722893715, 0.0010066041722893715, 0.0010066041722893715, 0.0010066041722893715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010066041722893715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100660
Iteration 2/1000 | Loss: 0.00004973
Iteration 3/1000 | Loss: 0.00003478
Iteration 4/1000 | Loss: 0.00002888
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00002601
Iteration 7/1000 | Loss: 0.00002528
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002419
Iteration 10/1000 | Loss: 0.00002376
Iteration 11/1000 | Loss: 0.00002356
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002326
Iteration 14/1000 | Loss: 0.00002315
Iteration 15/1000 | Loss: 0.00002314
Iteration 16/1000 | Loss: 0.00002301
Iteration 17/1000 | Loss: 0.00002296
Iteration 18/1000 | Loss: 0.00002294
Iteration 19/1000 | Loss: 0.00002289
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002274
Iteration 22/1000 | Loss: 0.00002273
Iteration 23/1000 | Loss: 0.00002272
Iteration 24/1000 | Loss: 0.00002271
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002269
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002263
Iteration 32/1000 | Loss: 0.00002263
Iteration 33/1000 | Loss: 0.00002263
Iteration 34/1000 | Loss: 0.00002262
Iteration 35/1000 | Loss: 0.00002262
Iteration 36/1000 | Loss: 0.00002262
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002261
Iteration 39/1000 | Loss: 0.00002261
Iteration 40/1000 | Loss: 0.00002260
Iteration 41/1000 | Loss: 0.00002260
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00002259
Iteration 44/1000 | Loss: 0.00002257
Iteration 45/1000 | Loss: 0.00002257
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002251
Iteration 55/1000 | Loss: 0.00002249
Iteration 56/1000 | Loss: 0.00002249
Iteration 57/1000 | Loss: 0.00002248
Iteration 58/1000 | Loss: 0.00002248
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002247
Iteration 61/1000 | Loss: 0.00002247
Iteration 62/1000 | Loss: 0.00002246
Iteration 63/1000 | Loss: 0.00002246
Iteration 64/1000 | Loss: 0.00002246
Iteration 65/1000 | Loss: 0.00002245
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002245
Iteration 68/1000 | Loss: 0.00002245
Iteration 69/1000 | Loss: 0.00002245
Iteration 70/1000 | Loss: 0.00002244
Iteration 71/1000 | Loss: 0.00002244
Iteration 72/1000 | Loss: 0.00002243
Iteration 73/1000 | Loss: 0.00002243
Iteration 74/1000 | Loss: 0.00002243
Iteration 75/1000 | Loss: 0.00002242
Iteration 76/1000 | Loss: 0.00002242
Iteration 77/1000 | Loss: 0.00002242
Iteration 78/1000 | Loss: 0.00002242
Iteration 79/1000 | Loss: 0.00002241
Iteration 80/1000 | Loss: 0.00002241
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00002241
Iteration 84/1000 | Loss: 0.00002241
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002241
Iteration 87/1000 | Loss: 0.00002240
Iteration 88/1000 | Loss: 0.00002240
Iteration 89/1000 | Loss: 0.00002240
Iteration 90/1000 | Loss: 0.00002240
Iteration 91/1000 | Loss: 0.00002240
Iteration 92/1000 | Loss: 0.00002239
Iteration 93/1000 | Loss: 0.00002239
Iteration 94/1000 | Loss: 0.00002239
Iteration 95/1000 | Loss: 0.00002238
Iteration 96/1000 | Loss: 0.00002238
Iteration 97/1000 | Loss: 0.00002238
Iteration 98/1000 | Loss: 0.00002238
Iteration 99/1000 | Loss: 0.00002238
Iteration 100/1000 | Loss: 0.00002237
Iteration 101/1000 | Loss: 0.00002237
Iteration 102/1000 | Loss: 0.00002237
Iteration 103/1000 | Loss: 0.00002237
Iteration 104/1000 | Loss: 0.00002237
Iteration 105/1000 | Loss: 0.00002237
Iteration 106/1000 | Loss: 0.00002236
Iteration 107/1000 | Loss: 0.00002236
Iteration 108/1000 | Loss: 0.00002236
Iteration 109/1000 | Loss: 0.00002236
Iteration 110/1000 | Loss: 0.00002236
Iteration 111/1000 | Loss: 0.00002236
Iteration 112/1000 | Loss: 0.00002236
Iteration 113/1000 | Loss: 0.00002236
Iteration 114/1000 | Loss: 0.00002236
Iteration 115/1000 | Loss: 0.00002236
Iteration 116/1000 | Loss: 0.00002236
Iteration 117/1000 | Loss: 0.00002236
Iteration 118/1000 | Loss: 0.00002236
Iteration 119/1000 | Loss: 0.00002236
Iteration 120/1000 | Loss: 0.00002236
Iteration 121/1000 | Loss: 0.00002235
Iteration 122/1000 | Loss: 0.00002235
Iteration 123/1000 | Loss: 0.00002235
Iteration 124/1000 | Loss: 0.00002235
Iteration 125/1000 | Loss: 0.00002235
Iteration 126/1000 | Loss: 0.00002234
Iteration 127/1000 | Loss: 0.00002234
Iteration 128/1000 | Loss: 0.00002234
Iteration 129/1000 | Loss: 0.00002234
Iteration 130/1000 | Loss: 0.00002234
Iteration 131/1000 | Loss: 0.00002234
Iteration 132/1000 | Loss: 0.00002234
Iteration 133/1000 | Loss: 0.00002234
Iteration 134/1000 | Loss: 0.00002234
Iteration 135/1000 | Loss: 0.00002234
Iteration 136/1000 | Loss: 0.00002234
Iteration 137/1000 | Loss: 0.00002234
Iteration 138/1000 | Loss: 0.00002234
Iteration 139/1000 | Loss: 0.00002233
Iteration 140/1000 | Loss: 0.00002233
Iteration 141/1000 | Loss: 0.00002233
Iteration 142/1000 | Loss: 0.00002233
Iteration 143/1000 | Loss: 0.00002233
Iteration 144/1000 | Loss: 0.00002233
Iteration 145/1000 | Loss: 0.00002233
Iteration 146/1000 | Loss: 0.00002233
Iteration 147/1000 | Loss: 0.00002233
Iteration 148/1000 | Loss: 0.00002233
Iteration 149/1000 | Loss: 0.00002232
Iteration 150/1000 | Loss: 0.00002232
Iteration 151/1000 | Loss: 0.00002232
Iteration 152/1000 | Loss: 0.00002232
Iteration 153/1000 | Loss: 0.00002232
Iteration 154/1000 | Loss: 0.00002232
Iteration 155/1000 | Loss: 0.00002232
Iteration 156/1000 | Loss: 0.00002232
Iteration 157/1000 | Loss: 0.00002232
Iteration 158/1000 | Loss: 0.00002232
Iteration 159/1000 | Loss: 0.00002232
Iteration 160/1000 | Loss: 0.00002232
Iteration 161/1000 | Loss: 0.00002232
Iteration 162/1000 | Loss: 0.00002232
Iteration 163/1000 | Loss: 0.00002232
Iteration 164/1000 | Loss: 0.00002231
Iteration 165/1000 | Loss: 0.00002231
Iteration 166/1000 | Loss: 0.00002231
Iteration 167/1000 | Loss: 0.00002231
Iteration 168/1000 | Loss: 0.00002231
Iteration 169/1000 | Loss: 0.00002231
Iteration 170/1000 | Loss: 0.00002231
Iteration 171/1000 | Loss: 0.00002231
Iteration 172/1000 | Loss: 0.00002231
Iteration 173/1000 | Loss: 0.00002231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.2313652152661234e-05, 2.2313652152661234e-05, 2.2313652152661234e-05, 2.2313652152661234e-05, 2.2313652152661234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2313652152661234e-05

Optimization complete. Final v2v error: 3.773057222366333 mm

Highest mean error: 5.450153827667236 mm for frame 76

Lowest mean error: 2.9150471687316895 mm for frame 36

Saving results

Total time: 47.13428521156311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041258
Iteration 2/25 | Loss: 0.00386046
Iteration 3/25 | Loss: 0.00255136
Iteration 4/25 | Loss: 0.00226286
Iteration 5/25 | Loss: 0.00188798
Iteration 6/25 | Loss: 0.00187663
Iteration 7/25 | Loss: 0.00165775
Iteration 8/25 | Loss: 0.00158825
Iteration 9/25 | Loss: 0.00152389
Iteration 10/25 | Loss: 0.00152080
Iteration 11/25 | Loss: 0.00149266
Iteration 12/25 | Loss: 0.00147639
Iteration 13/25 | Loss: 0.00147341
Iteration 14/25 | Loss: 0.00146452
Iteration 15/25 | Loss: 0.00146193
Iteration 16/25 | Loss: 0.00146390
Iteration 17/25 | Loss: 0.00147109
Iteration 18/25 | Loss: 0.00146083
Iteration 19/25 | Loss: 0.00146290
Iteration 20/25 | Loss: 0.00145818
Iteration 21/25 | Loss: 0.00145564
Iteration 22/25 | Loss: 0.00145648
Iteration 23/25 | Loss: 0.00145453
Iteration 24/25 | Loss: 0.00145469
Iteration 25/25 | Loss: 0.00145273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24279416
Iteration 2/25 | Loss: 0.00405741
Iteration 3/25 | Loss: 0.00334899
Iteration 4/25 | Loss: 0.00334899
Iteration 5/25 | Loss: 0.00334899
Iteration 6/25 | Loss: 0.00334899
Iteration 7/25 | Loss: 0.00334899
Iteration 8/25 | Loss: 0.00334899
Iteration 9/25 | Loss: 0.00334899
Iteration 10/25 | Loss: 0.00334899
Iteration 11/25 | Loss: 0.00334899
Iteration 12/25 | Loss: 0.00334899
Iteration 13/25 | Loss: 0.00334899
Iteration 14/25 | Loss: 0.00334899
Iteration 15/25 | Loss: 0.00334899
Iteration 16/25 | Loss: 0.00334899
Iteration 17/25 | Loss: 0.00334899
Iteration 18/25 | Loss: 0.00334899
Iteration 19/25 | Loss: 0.00334899
Iteration 20/25 | Loss: 0.00334899
Iteration 21/25 | Loss: 0.00334899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0033489884808659554, 0.0033489884808659554, 0.0033489884808659554, 0.0033489884808659554, 0.0033489884808659554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033489884808659554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00334899
Iteration 2/1000 | Loss: 0.01152322
Iteration 3/1000 | Loss: 0.01300453
Iteration 4/1000 | Loss: 0.00066710
Iteration 5/1000 | Loss: 0.00161443
Iteration 6/1000 | Loss: 0.00094628
Iteration 7/1000 | Loss: 0.00035149
Iteration 8/1000 | Loss: 0.00136773
Iteration 9/1000 | Loss: 0.00043596
Iteration 10/1000 | Loss: 0.00073396
Iteration 11/1000 | Loss: 0.00100031
Iteration 12/1000 | Loss: 0.00018732
Iteration 13/1000 | Loss: 0.00018339
Iteration 14/1000 | Loss: 0.00020199
Iteration 15/1000 | Loss: 0.00013702
Iteration 16/1000 | Loss: 0.00029503
Iteration 17/1000 | Loss: 0.00043418
Iteration 18/1000 | Loss: 0.00011463
Iteration 19/1000 | Loss: 0.00024530
Iteration 20/1000 | Loss: 0.00019154
Iteration 21/1000 | Loss: 0.00040702
Iteration 22/1000 | Loss: 0.00383729
Iteration 23/1000 | Loss: 0.00431465
Iteration 24/1000 | Loss: 0.00057650
Iteration 25/1000 | Loss: 0.00022452
Iteration 26/1000 | Loss: 0.00034701
Iteration 27/1000 | Loss: 0.00040805
Iteration 28/1000 | Loss: 0.00008100
Iteration 29/1000 | Loss: 0.00018251
Iteration 30/1000 | Loss: 0.00036682
Iteration 31/1000 | Loss: 0.00015193
Iteration 32/1000 | Loss: 0.00024565
Iteration 33/1000 | Loss: 0.00007019
Iteration 34/1000 | Loss: 0.00019339
Iteration 35/1000 | Loss: 0.00054650
Iteration 36/1000 | Loss: 0.00008373
Iteration 37/1000 | Loss: 0.00029911
Iteration 38/1000 | Loss: 0.00004598
Iteration 39/1000 | Loss: 0.00019140
Iteration 40/1000 | Loss: 0.00027713
Iteration 41/1000 | Loss: 0.00008627
Iteration 42/1000 | Loss: 0.00008578
Iteration 43/1000 | Loss: 0.00007273
Iteration 44/1000 | Loss: 0.00005296
Iteration 45/1000 | Loss: 0.00006724
Iteration 46/1000 | Loss: 0.00015144
Iteration 47/1000 | Loss: 0.00005202
Iteration 48/1000 | Loss: 0.00014311
Iteration 49/1000 | Loss: 0.00006392
Iteration 50/1000 | Loss: 0.00020039
Iteration 51/1000 | Loss: 0.00003853
Iteration 52/1000 | Loss: 0.00004481
Iteration 53/1000 | Loss: 0.00006705
Iteration 54/1000 | Loss: 0.00005946
Iteration 55/1000 | Loss: 0.00005102
Iteration 56/1000 | Loss: 0.00004226
Iteration 57/1000 | Loss: 0.00005179
Iteration 58/1000 | Loss: 0.00005577
Iteration 59/1000 | Loss: 0.00005110
Iteration 60/1000 | Loss: 0.00005459
Iteration 61/1000 | Loss: 0.00005021
Iteration 62/1000 | Loss: 0.00010883
Iteration 63/1000 | Loss: 0.00009292
Iteration 64/1000 | Loss: 0.00007856
Iteration 65/1000 | Loss: 0.00006345
Iteration 66/1000 | Loss: 0.00008001
Iteration 67/1000 | Loss: 0.00006135
Iteration 68/1000 | Loss: 0.00006758
Iteration 69/1000 | Loss: 0.00011533
Iteration 70/1000 | Loss: 0.00006693
Iteration 71/1000 | Loss: 0.00005895
Iteration 72/1000 | Loss: 0.00007556
Iteration 73/1000 | Loss: 0.00003422
Iteration 74/1000 | Loss: 0.00003507
Iteration 75/1000 | Loss: 0.00005530
Iteration 76/1000 | Loss: 0.00005285
Iteration 77/1000 | Loss: 0.00005660
Iteration 78/1000 | Loss: 0.00005554
Iteration 79/1000 | Loss: 0.00005627
Iteration 80/1000 | Loss: 0.00005699
Iteration 81/1000 | Loss: 0.00013404
Iteration 82/1000 | Loss: 0.00011335
Iteration 83/1000 | Loss: 0.00007254
Iteration 84/1000 | Loss: 0.00006879
Iteration 85/1000 | Loss: 0.00010434
Iteration 86/1000 | Loss: 0.00006601
Iteration 87/1000 | Loss: 0.00007505
Iteration 88/1000 | Loss: 0.00006937
Iteration 89/1000 | Loss: 0.00005737
Iteration 90/1000 | Loss: 0.00005872
Iteration 91/1000 | Loss: 0.00005455
Iteration 92/1000 | Loss: 0.00007761
Iteration 93/1000 | Loss: 0.00005715
Iteration 94/1000 | Loss: 0.00006735
Iteration 95/1000 | Loss: 0.00005551
Iteration 96/1000 | Loss: 0.00003545
Iteration 97/1000 | Loss: 0.00004750
Iteration 98/1000 | Loss: 0.00011349
Iteration 99/1000 | Loss: 0.00004694
Iteration 100/1000 | Loss: 0.00004718
Iteration 101/1000 | Loss: 0.00003914
Iteration 102/1000 | Loss: 0.00002844
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00003071
Iteration 105/1000 | Loss: 0.00003225
Iteration 106/1000 | Loss: 0.00002709
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002442
Iteration 109/1000 | Loss: 0.00007355
Iteration 110/1000 | Loss: 0.00011132
Iteration 111/1000 | Loss: 0.00002490
Iteration 112/1000 | Loss: 0.00003701
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00003829
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00001872
Iteration 118/1000 | Loss: 0.00001610
Iteration 119/1000 | Loss: 0.00001570
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001534
Iteration 123/1000 | Loss: 0.00001532
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00004778
Iteration 127/1000 | Loss: 0.00003790
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00002702
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001496
Iteration 132/1000 | Loss: 0.00001495
Iteration 133/1000 | Loss: 0.00001495
Iteration 134/1000 | Loss: 0.00001495
Iteration 135/1000 | Loss: 0.00001495
Iteration 136/1000 | Loss: 0.00001495
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001494
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001494
Iteration 153/1000 | Loss: 0.00001494
Iteration 154/1000 | Loss: 0.00001494
Iteration 155/1000 | Loss: 0.00001494
Iteration 156/1000 | Loss: 0.00001494
Iteration 157/1000 | Loss: 0.00001494
Iteration 158/1000 | Loss: 0.00001494
Iteration 159/1000 | Loss: 0.00001494
Iteration 160/1000 | Loss: 0.00001494
Iteration 161/1000 | Loss: 0.00001494
Iteration 162/1000 | Loss: 0.00001494
Iteration 163/1000 | Loss: 0.00001494
Iteration 164/1000 | Loss: 0.00001494
Iteration 165/1000 | Loss: 0.00001494
Iteration 166/1000 | Loss: 0.00001494
Iteration 167/1000 | Loss: 0.00001494
Iteration 168/1000 | Loss: 0.00001494
Iteration 169/1000 | Loss: 0.00001494
Iteration 170/1000 | Loss: 0.00001494
Iteration 171/1000 | Loss: 0.00001494
Iteration 172/1000 | Loss: 0.00001494
Iteration 173/1000 | Loss: 0.00001494
Iteration 174/1000 | Loss: 0.00001494
Iteration 175/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.4940716027922463e-05, 1.4940716027922463e-05, 1.4940716027922463e-05, 1.4940716027922463e-05, 1.4940716027922463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4940716027922463e-05

Optimization complete. Final v2v error: 3.1399855613708496 mm

Highest mean error: 4.9619669914245605 mm for frame 61

Lowest mean error: 2.517636299133301 mm for frame 92

Saving results

Total time: 241.02658104896545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876535
Iteration 2/25 | Loss: 0.00119401
Iteration 3/25 | Loss: 0.00110098
Iteration 4/25 | Loss: 0.00109174
Iteration 5/25 | Loss: 0.00108923
Iteration 6/25 | Loss: 0.00108860
Iteration 7/25 | Loss: 0.00108860
Iteration 8/25 | Loss: 0.00108860
Iteration 9/25 | Loss: 0.00108860
Iteration 10/25 | Loss: 0.00108860
Iteration 11/25 | Loss: 0.00108860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010886023519560695, 0.0010886023519560695, 0.0010886023519560695, 0.0010886023519560695, 0.0010886023519560695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010886023519560695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11566830
Iteration 2/25 | Loss: 0.00086818
Iteration 3/25 | Loss: 0.00086818
Iteration 4/25 | Loss: 0.00086818
Iteration 5/25 | Loss: 0.00086818
Iteration 6/25 | Loss: 0.00086817
Iteration 7/25 | Loss: 0.00086817
Iteration 8/25 | Loss: 0.00086817
Iteration 9/25 | Loss: 0.00086817
Iteration 10/25 | Loss: 0.00086817
Iteration 11/25 | Loss: 0.00086817
Iteration 12/25 | Loss: 0.00086817
Iteration 13/25 | Loss: 0.00086817
Iteration 14/25 | Loss: 0.00086817
Iteration 15/25 | Loss: 0.00086817
Iteration 16/25 | Loss: 0.00086817
Iteration 17/25 | Loss: 0.00086817
Iteration 18/25 | Loss: 0.00086817
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008681737235747278, 0.0008681737235747278, 0.0008681737235747278, 0.0008681737235747278, 0.0008681737235747278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008681737235747278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086817
Iteration 2/1000 | Loss: 0.00002111
Iteration 3/1000 | Loss: 0.00001386
Iteration 4/1000 | Loss: 0.00001231
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001107
Iteration 7/1000 | Loss: 0.00001072
Iteration 8/1000 | Loss: 0.00001044
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001028
Iteration 12/1000 | Loss: 0.00001027
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001007
Iteration 15/1000 | Loss: 0.00001002
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000998
Iteration 19/1000 | Loss: 0.00000997
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000985
Iteration 27/1000 | Loss: 0.00000981
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000978
Iteration 30/1000 | Loss: 0.00000977
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000967
Iteration 33/1000 | Loss: 0.00000965
Iteration 34/1000 | Loss: 0.00000965
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000964
Iteration 42/1000 | Loss: 0.00000964
Iteration 43/1000 | Loss: 0.00000964
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000961
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000960
Iteration 49/1000 | Loss: 0.00000960
Iteration 50/1000 | Loss: 0.00000960
Iteration 51/1000 | Loss: 0.00000959
Iteration 52/1000 | Loss: 0.00000959
Iteration 53/1000 | Loss: 0.00000958
Iteration 54/1000 | Loss: 0.00000958
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000956
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00000956
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000956
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000956
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000955
Iteration 67/1000 | Loss: 0.00000955
Iteration 68/1000 | Loss: 0.00000955
Iteration 69/1000 | Loss: 0.00000954
Iteration 70/1000 | Loss: 0.00000954
Iteration 71/1000 | Loss: 0.00000954
Iteration 72/1000 | Loss: 0.00000954
Iteration 73/1000 | Loss: 0.00000954
Iteration 74/1000 | Loss: 0.00000953
Iteration 75/1000 | Loss: 0.00000953
Iteration 76/1000 | Loss: 0.00000952
Iteration 77/1000 | Loss: 0.00000952
Iteration 78/1000 | Loss: 0.00000952
Iteration 79/1000 | Loss: 0.00000952
Iteration 80/1000 | Loss: 0.00000951
Iteration 81/1000 | Loss: 0.00000951
Iteration 82/1000 | Loss: 0.00000951
Iteration 83/1000 | Loss: 0.00000951
Iteration 84/1000 | Loss: 0.00000951
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000951
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000950
Iteration 92/1000 | Loss: 0.00000950
Iteration 93/1000 | Loss: 0.00000949
Iteration 94/1000 | Loss: 0.00000949
Iteration 95/1000 | Loss: 0.00000949
Iteration 96/1000 | Loss: 0.00000948
Iteration 97/1000 | Loss: 0.00000948
Iteration 98/1000 | Loss: 0.00000948
Iteration 99/1000 | Loss: 0.00000948
Iteration 100/1000 | Loss: 0.00000948
Iteration 101/1000 | Loss: 0.00000947
Iteration 102/1000 | Loss: 0.00000947
Iteration 103/1000 | Loss: 0.00000947
Iteration 104/1000 | Loss: 0.00000947
Iteration 105/1000 | Loss: 0.00000947
Iteration 106/1000 | Loss: 0.00000947
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000946
Iteration 109/1000 | Loss: 0.00000946
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000945
Iteration 112/1000 | Loss: 0.00000945
Iteration 113/1000 | Loss: 0.00000945
Iteration 114/1000 | Loss: 0.00000945
Iteration 115/1000 | Loss: 0.00000945
Iteration 116/1000 | Loss: 0.00000945
Iteration 117/1000 | Loss: 0.00000945
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000943
Iteration 121/1000 | Loss: 0.00000943
Iteration 122/1000 | Loss: 0.00000943
Iteration 123/1000 | Loss: 0.00000943
Iteration 124/1000 | Loss: 0.00000943
Iteration 125/1000 | Loss: 0.00000943
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000943
Iteration 129/1000 | Loss: 0.00000943
Iteration 130/1000 | Loss: 0.00000943
Iteration 131/1000 | Loss: 0.00000943
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000941
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000940
Iteration 139/1000 | Loss: 0.00000940
Iteration 140/1000 | Loss: 0.00000940
Iteration 141/1000 | Loss: 0.00000940
Iteration 142/1000 | Loss: 0.00000940
Iteration 143/1000 | Loss: 0.00000940
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000939
Iteration 147/1000 | Loss: 0.00000939
Iteration 148/1000 | Loss: 0.00000939
Iteration 149/1000 | Loss: 0.00000939
Iteration 150/1000 | Loss: 0.00000939
Iteration 151/1000 | Loss: 0.00000939
Iteration 152/1000 | Loss: 0.00000939
Iteration 153/1000 | Loss: 0.00000938
Iteration 154/1000 | Loss: 0.00000938
Iteration 155/1000 | Loss: 0.00000938
Iteration 156/1000 | Loss: 0.00000937
Iteration 157/1000 | Loss: 0.00000937
Iteration 158/1000 | Loss: 0.00000937
Iteration 159/1000 | Loss: 0.00000937
Iteration 160/1000 | Loss: 0.00000937
Iteration 161/1000 | Loss: 0.00000936
Iteration 162/1000 | Loss: 0.00000936
Iteration 163/1000 | Loss: 0.00000936
Iteration 164/1000 | Loss: 0.00000936
Iteration 165/1000 | Loss: 0.00000936
Iteration 166/1000 | Loss: 0.00000936
Iteration 167/1000 | Loss: 0.00000935
Iteration 168/1000 | Loss: 0.00000935
Iteration 169/1000 | Loss: 0.00000935
Iteration 170/1000 | Loss: 0.00000935
Iteration 171/1000 | Loss: 0.00000935
Iteration 172/1000 | Loss: 0.00000935
Iteration 173/1000 | Loss: 0.00000935
Iteration 174/1000 | Loss: 0.00000934
Iteration 175/1000 | Loss: 0.00000934
Iteration 176/1000 | Loss: 0.00000934
Iteration 177/1000 | Loss: 0.00000934
Iteration 178/1000 | Loss: 0.00000934
Iteration 179/1000 | Loss: 0.00000934
Iteration 180/1000 | Loss: 0.00000934
Iteration 181/1000 | Loss: 0.00000933
Iteration 182/1000 | Loss: 0.00000933
Iteration 183/1000 | Loss: 0.00000933
Iteration 184/1000 | Loss: 0.00000933
Iteration 185/1000 | Loss: 0.00000933
Iteration 186/1000 | Loss: 0.00000933
Iteration 187/1000 | Loss: 0.00000933
Iteration 188/1000 | Loss: 0.00000933
Iteration 189/1000 | Loss: 0.00000933
Iteration 190/1000 | Loss: 0.00000933
Iteration 191/1000 | Loss: 0.00000933
Iteration 192/1000 | Loss: 0.00000933
Iteration 193/1000 | Loss: 0.00000933
Iteration 194/1000 | Loss: 0.00000933
Iteration 195/1000 | Loss: 0.00000932
Iteration 196/1000 | Loss: 0.00000932
Iteration 197/1000 | Loss: 0.00000932
Iteration 198/1000 | Loss: 0.00000932
Iteration 199/1000 | Loss: 0.00000932
Iteration 200/1000 | Loss: 0.00000932
Iteration 201/1000 | Loss: 0.00000932
Iteration 202/1000 | Loss: 0.00000932
Iteration 203/1000 | Loss: 0.00000932
Iteration 204/1000 | Loss: 0.00000932
Iteration 205/1000 | Loss: 0.00000932
Iteration 206/1000 | Loss: 0.00000932
Iteration 207/1000 | Loss: 0.00000932
Iteration 208/1000 | Loss: 0.00000932
Iteration 209/1000 | Loss: 0.00000932
Iteration 210/1000 | Loss: 0.00000932
Iteration 211/1000 | Loss: 0.00000932
Iteration 212/1000 | Loss: 0.00000932
Iteration 213/1000 | Loss: 0.00000932
Iteration 214/1000 | Loss: 0.00000932
Iteration 215/1000 | Loss: 0.00000932
Iteration 216/1000 | Loss: 0.00000932
Iteration 217/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.317294825450517e-06, 9.317294825450517e-06, 9.317294825450517e-06, 9.317294825450517e-06, 9.317294825450517e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.317294825450517e-06

Optimization complete. Final v2v error: 2.62568736076355 mm

Highest mean error: 2.9292235374450684 mm for frame 45

Lowest mean error: 2.360291004180908 mm for frame 129

Saving results

Total time: 40.43616843223572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506278
Iteration 2/25 | Loss: 0.00124534
Iteration 3/25 | Loss: 0.00113791
Iteration 4/25 | Loss: 0.00111771
Iteration 5/25 | Loss: 0.00111285
Iteration 6/25 | Loss: 0.00111213
Iteration 7/25 | Loss: 0.00111213
Iteration 8/25 | Loss: 0.00111213
Iteration 9/25 | Loss: 0.00111213
Iteration 10/25 | Loss: 0.00111213
Iteration 11/25 | Loss: 0.00111213
Iteration 12/25 | Loss: 0.00111213
Iteration 13/25 | Loss: 0.00111213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001112129888497293, 0.001112129888497293, 0.001112129888497293, 0.001112129888497293, 0.001112129888497293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001112129888497293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17709827
Iteration 2/25 | Loss: 0.00085922
Iteration 3/25 | Loss: 0.00085919
Iteration 4/25 | Loss: 0.00085919
Iteration 5/25 | Loss: 0.00085919
Iteration 6/25 | Loss: 0.00085919
Iteration 7/25 | Loss: 0.00085919
Iteration 8/25 | Loss: 0.00085919
Iteration 9/25 | Loss: 0.00085919
Iteration 10/25 | Loss: 0.00085919
Iteration 11/25 | Loss: 0.00085919
Iteration 12/25 | Loss: 0.00085919
Iteration 13/25 | Loss: 0.00085919
Iteration 14/25 | Loss: 0.00085919
Iteration 15/25 | Loss: 0.00085919
Iteration 16/25 | Loss: 0.00085919
Iteration 17/25 | Loss: 0.00085919
Iteration 18/25 | Loss: 0.00085919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008591872756369412, 0.0008591872756369412, 0.0008591872756369412, 0.0008591872756369412, 0.0008591872756369412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008591872756369412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085919
Iteration 2/1000 | Loss: 0.00002684
Iteration 3/1000 | Loss: 0.00001740
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001247
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001224
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001217
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001216
Iteration 86/1000 | Loss: 0.00001216
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001213
Iteration 110/1000 | Loss: 0.00001213
Iteration 111/1000 | Loss: 0.00001213
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001213
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001212
Iteration 117/1000 | Loss: 0.00001212
Iteration 118/1000 | Loss: 0.00001212
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001211
Iteration 122/1000 | Loss: 0.00001211
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001210
Iteration 125/1000 | Loss: 0.00001210
Iteration 126/1000 | Loss: 0.00001210
Iteration 127/1000 | Loss: 0.00001210
Iteration 128/1000 | Loss: 0.00001210
Iteration 129/1000 | Loss: 0.00001210
Iteration 130/1000 | Loss: 0.00001210
Iteration 131/1000 | Loss: 0.00001210
Iteration 132/1000 | Loss: 0.00001210
Iteration 133/1000 | Loss: 0.00001210
Iteration 134/1000 | Loss: 0.00001209
Iteration 135/1000 | Loss: 0.00001209
Iteration 136/1000 | Loss: 0.00001209
Iteration 137/1000 | Loss: 0.00001209
Iteration 138/1000 | Loss: 0.00001209
Iteration 139/1000 | Loss: 0.00001209
Iteration 140/1000 | Loss: 0.00001209
Iteration 141/1000 | Loss: 0.00001209
Iteration 142/1000 | Loss: 0.00001209
Iteration 143/1000 | Loss: 0.00001208
Iteration 144/1000 | Loss: 0.00001208
Iteration 145/1000 | Loss: 0.00001208
Iteration 146/1000 | Loss: 0.00001208
Iteration 147/1000 | Loss: 0.00001208
Iteration 148/1000 | Loss: 0.00001208
Iteration 149/1000 | Loss: 0.00001208
Iteration 150/1000 | Loss: 0.00001208
Iteration 151/1000 | Loss: 0.00001208
Iteration 152/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.2084497939213179e-05, 1.2084497939213179e-05, 1.2084497939213179e-05, 1.2084497939213179e-05, 1.2084497939213179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2084497939213179e-05

Optimization complete. Final v2v error: 2.9635181427001953 mm

Highest mean error: 3.338562250137329 mm for frame 155

Lowest mean error: 2.444652795791626 mm for frame 0

Saving results

Total time: 39.34097099304199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979692
Iteration 2/25 | Loss: 0.00232243
Iteration 3/25 | Loss: 0.00179179
Iteration 4/25 | Loss: 0.00163836
Iteration 5/25 | Loss: 0.00165510
Iteration 6/25 | Loss: 0.00164520
Iteration 7/25 | Loss: 0.00172078
Iteration 8/25 | Loss: 0.00156308
Iteration 9/25 | Loss: 0.00150985
Iteration 10/25 | Loss: 0.00144581
Iteration 11/25 | Loss: 0.00139361
Iteration 12/25 | Loss: 0.00134609
Iteration 13/25 | Loss: 0.00133445
Iteration 14/25 | Loss: 0.00133563
Iteration 15/25 | Loss: 0.00132205
Iteration 16/25 | Loss: 0.00132506
Iteration 17/25 | Loss: 0.00131650
Iteration 18/25 | Loss: 0.00130339
Iteration 19/25 | Loss: 0.00129442
Iteration 20/25 | Loss: 0.00128767
Iteration 21/25 | Loss: 0.00128672
Iteration 22/25 | Loss: 0.00129327
Iteration 23/25 | Loss: 0.00129129
Iteration 24/25 | Loss: 0.00128703
Iteration 25/25 | Loss: 0.00127469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39325237
Iteration 2/25 | Loss: 0.00584196
Iteration 3/25 | Loss: 0.00288874
Iteration 4/25 | Loss: 0.00288872
Iteration 5/25 | Loss: 0.00288871
Iteration 6/25 | Loss: 0.00288871
Iteration 7/25 | Loss: 0.00288871
Iteration 8/25 | Loss: 0.00288871
Iteration 9/25 | Loss: 0.00288871
Iteration 10/25 | Loss: 0.00288871
Iteration 11/25 | Loss: 0.00288871
Iteration 12/25 | Loss: 0.00288871
Iteration 13/25 | Loss: 0.00288871
Iteration 14/25 | Loss: 0.00288871
Iteration 15/25 | Loss: 0.00288871
Iteration 16/25 | Loss: 0.00288871
Iteration 17/25 | Loss: 0.00288871
Iteration 18/25 | Loss: 0.00288871
Iteration 19/25 | Loss: 0.00288871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002888710703700781, 0.002888710703700781, 0.002888710703700781, 0.002888710703700781, 0.002888710703700781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002888710703700781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288871
Iteration 2/1000 | Loss: 0.00287370
Iteration 3/1000 | Loss: 0.00057138
Iteration 4/1000 | Loss: 0.00157080
Iteration 5/1000 | Loss: 0.00207170
Iteration 6/1000 | Loss: 0.00129924
Iteration 7/1000 | Loss: 0.00142762
Iteration 8/1000 | Loss: 0.00036779
Iteration 9/1000 | Loss: 0.00023868
Iteration 10/1000 | Loss: 0.00157380
Iteration 11/1000 | Loss: 0.00059590
Iteration 12/1000 | Loss: 0.00067214
Iteration 13/1000 | Loss: 0.00063356
Iteration 14/1000 | Loss: 0.00075803
Iteration 15/1000 | Loss: 0.00047618
Iteration 16/1000 | Loss: 0.00087869
Iteration 17/1000 | Loss: 0.00053970
Iteration 18/1000 | Loss: 0.00057020
Iteration 19/1000 | Loss: 0.00043604
Iteration 20/1000 | Loss: 0.00038810
Iteration 21/1000 | Loss: 0.00057436
Iteration 22/1000 | Loss: 0.00117871
Iteration 23/1000 | Loss: 0.00146562
Iteration 24/1000 | Loss: 0.00202923
Iteration 25/1000 | Loss: 0.00165142
Iteration 26/1000 | Loss: 0.00184842
Iteration 27/1000 | Loss: 0.00150182
Iteration 28/1000 | Loss: 0.00110131
Iteration 29/1000 | Loss: 0.00094934
Iteration 30/1000 | Loss: 0.00161499
Iteration 31/1000 | Loss: 0.00079884
Iteration 32/1000 | Loss: 0.00104130
Iteration 33/1000 | Loss: 0.00043384
Iteration 34/1000 | Loss: 0.00019923
Iteration 35/1000 | Loss: 0.00018480
Iteration 36/1000 | Loss: 0.00075398
Iteration 37/1000 | Loss: 0.00029991
Iteration 38/1000 | Loss: 0.00018173
Iteration 39/1000 | Loss: 0.00022042
Iteration 40/1000 | Loss: 0.00018730
Iteration 41/1000 | Loss: 0.00038960
Iteration 42/1000 | Loss: 0.00042416
Iteration 43/1000 | Loss: 0.00043350
Iteration 44/1000 | Loss: 0.00128404
Iteration 45/1000 | Loss: 0.00076584
Iteration 46/1000 | Loss: 0.00074439
Iteration 47/1000 | Loss: 0.00033931
Iteration 48/1000 | Loss: 0.00031834
Iteration 49/1000 | Loss: 0.00019776
Iteration 50/1000 | Loss: 0.00032346
Iteration 51/1000 | Loss: 0.00045044
Iteration 52/1000 | Loss: 0.00014767
Iteration 53/1000 | Loss: 0.00037176
Iteration 54/1000 | Loss: 0.00193315
Iteration 55/1000 | Loss: 0.00079129
Iteration 56/1000 | Loss: 0.00022081
Iteration 57/1000 | Loss: 0.00038431
Iteration 58/1000 | Loss: 0.00047933
Iteration 59/1000 | Loss: 0.00039670
Iteration 60/1000 | Loss: 0.00097678
Iteration 61/1000 | Loss: 0.00143881
Iteration 62/1000 | Loss: 0.00053595
Iteration 63/1000 | Loss: 0.00051090
Iteration 64/1000 | Loss: 0.00065322
Iteration 65/1000 | Loss: 0.00061574
Iteration 66/1000 | Loss: 0.00105163
Iteration 67/1000 | Loss: 0.00065411
Iteration 68/1000 | Loss: 0.00040814
Iteration 69/1000 | Loss: 0.00014367
Iteration 70/1000 | Loss: 0.00009680
Iteration 71/1000 | Loss: 0.00174644
Iteration 72/1000 | Loss: 0.00264080
Iteration 73/1000 | Loss: 0.00134292
Iteration 74/1000 | Loss: 0.00069874
Iteration 75/1000 | Loss: 0.00144102
Iteration 76/1000 | Loss: 0.00087108
Iteration 77/1000 | Loss: 0.00141845
Iteration 78/1000 | Loss: 0.00122663
Iteration 79/1000 | Loss: 0.00067966
Iteration 80/1000 | Loss: 0.00121550
Iteration 81/1000 | Loss: 0.00135649
Iteration 82/1000 | Loss: 0.00080561
Iteration 83/1000 | Loss: 0.00077407
Iteration 84/1000 | Loss: 0.00058825
Iteration 85/1000 | Loss: 0.00084226
Iteration 86/1000 | Loss: 0.00039244
Iteration 87/1000 | Loss: 0.00063288
Iteration 88/1000 | Loss: 0.00009977
Iteration 89/1000 | Loss: 0.00009013
Iteration 90/1000 | Loss: 0.00007252
Iteration 91/1000 | Loss: 0.00079404
Iteration 92/1000 | Loss: 0.00023966
Iteration 93/1000 | Loss: 0.00048334
Iteration 94/1000 | Loss: 0.00014875
Iteration 95/1000 | Loss: 0.00006177
Iteration 96/1000 | Loss: 0.00036135
Iteration 97/1000 | Loss: 0.00183108
Iteration 98/1000 | Loss: 0.00274568
Iteration 99/1000 | Loss: 0.00080431
Iteration 100/1000 | Loss: 0.00180521
Iteration 101/1000 | Loss: 0.00126072
Iteration 102/1000 | Loss: 0.00037588
Iteration 103/1000 | Loss: 0.00009252
Iteration 104/1000 | Loss: 0.00013154
Iteration 105/1000 | Loss: 0.00006256
Iteration 106/1000 | Loss: 0.00005069
Iteration 107/1000 | Loss: 0.00020351
Iteration 108/1000 | Loss: 0.00022201
Iteration 109/1000 | Loss: 0.00079546
Iteration 110/1000 | Loss: 0.00048036
Iteration 111/1000 | Loss: 0.00056337
Iteration 112/1000 | Loss: 0.00059306
Iteration 113/1000 | Loss: 0.00053035
Iteration 114/1000 | Loss: 0.00032139
Iteration 115/1000 | Loss: 0.00017398
Iteration 116/1000 | Loss: 0.00004970
Iteration 117/1000 | Loss: 0.00008303
Iteration 118/1000 | Loss: 0.00028386
Iteration 119/1000 | Loss: 0.00007032
Iteration 120/1000 | Loss: 0.00066948
Iteration 121/1000 | Loss: 0.00033373
Iteration 122/1000 | Loss: 0.00009073
Iteration 123/1000 | Loss: 0.00005529
Iteration 124/1000 | Loss: 0.00004493
Iteration 125/1000 | Loss: 0.00021966
Iteration 126/1000 | Loss: 0.00004265
Iteration 127/1000 | Loss: 0.00165781
Iteration 128/1000 | Loss: 0.00055900
Iteration 129/1000 | Loss: 0.00055308
Iteration 130/1000 | Loss: 0.00054406
Iteration 131/1000 | Loss: 0.00005865
Iteration 132/1000 | Loss: 0.00034530
Iteration 133/1000 | Loss: 0.00031724
Iteration 134/1000 | Loss: 0.00009048
Iteration 135/1000 | Loss: 0.00008801
Iteration 136/1000 | Loss: 0.00079160
Iteration 137/1000 | Loss: 0.00130945
Iteration 138/1000 | Loss: 0.00090113
Iteration 139/1000 | Loss: 0.00082357
Iteration 140/1000 | Loss: 0.00258298
Iteration 141/1000 | Loss: 0.00041804
Iteration 142/1000 | Loss: 0.00171100
Iteration 143/1000 | Loss: 0.00063608
Iteration 144/1000 | Loss: 0.00094019
Iteration 145/1000 | Loss: 0.00038215
Iteration 146/1000 | Loss: 0.00012988
Iteration 147/1000 | Loss: 0.00039180
Iteration 148/1000 | Loss: 0.00003828
Iteration 149/1000 | Loss: 0.00079926
Iteration 150/1000 | Loss: 0.00198802
Iteration 151/1000 | Loss: 0.00081850
Iteration 152/1000 | Loss: 0.00104008
Iteration 153/1000 | Loss: 0.00085782
Iteration 154/1000 | Loss: 0.00030238
Iteration 155/1000 | Loss: 0.00006963
Iteration 156/1000 | Loss: 0.00005988
Iteration 157/1000 | Loss: 0.00029421
Iteration 158/1000 | Loss: 0.00005395
Iteration 159/1000 | Loss: 0.00019988
Iteration 160/1000 | Loss: 0.00019660
Iteration 161/1000 | Loss: 0.00036375
Iteration 162/1000 | Loss: 0.00002788
Iteration 163/1000 | Loss: 0.00031245
Iteration 164/1000 | Loss: 0.00044364
Iteration 165/1000 | Loss: 0.00004899
Iteration 166/1000 | Loss: 0.00014270
Iteration 167/1000 | Loss: 0.00004048
Iteration 168/1000 | Loss: 0.00002193
Iteration 169/1000 | Loss: 0.00003570
Iteration 170/1000 | Loss: 0.00002577
Iteration 171/1000 | Loss: 0.00032736
Iteration 172/1000 | Loss: 0.00030841
Iteration 173/1000 | Loss: 0.00038173
Iteration 174/1000 | Loss: 0.00006978
Iteration 175/1000 | Loss: 0.00010338
Iteration 176/1000 | Loss: 0.00013427
Iteration 177/1000 | Loss: 0.00026145
Iteration 178/1000 | Loss: 0.00004661
Iteration 179/1000 | Loss: 0.00014210
Iteration 180/1000 | Loss: 0.00019865
Iteration 181/1000 | Loss: 0.00019035
Iteration 182/1000 | Loss: 0.00013469
Iteration 183/1000 | Loss: 0.00014848
Iteration 184/1000 | Loss: 0.00018803
Iteration 185/1000 | Loss: 0.00014617
Iteration 186/1000 | Loss: 0.00013999
Iteration 187/1000 | Loss: 0.00003613
Iteration 188/1000 | Loss: 0.00015117
Iteration 189/1000 | Loss: 0.00024001
Iteration 190/1000 | Loss: 0.00015481
Iteration 191/1000 | Loss: 0.00003562
Iteration 192/1000 | Loss: 0.00003588
Iteration 193/1000 | Loss: 0.00004101
Iteration 194/1000 | Loss: 0.00002811
Iteration 195/1000 | Loss: 0.00002999
Iteration 196/1000 | Loss: 0.00103627
Iteration 197/1000 | Loss: 0.00075099
Iteration 198/1000 | Loss: 0.00085762
Iteration 199/1000 | Loss: 0.00084622
Iteration 200/1000 | Loss: 0.00009861
Iteration 201/1000 | Loss: 0.00002334
Iteration 202/1000 | Loss: 0.00023931
Iteration 203/1000 | Loss: 0.00002714
Iteration 204/1000 | Loss: 0.00002720
Iteration 205/1000 | Loss: 0.00002622
Iteration 206/1000 | Loss: 0.00023000
Iteration 207/1000 | Loss: 0.00020113
Iteration 208/1000 | Loss: 0.00032954
Iteration 209/1000 | Loss: 0.00002624
Iteration 210/1000 | Loss: 0.00002864
Iteration 211/1000 | Loss: 0.00002235
Iteration 212/1000 | Loss: 0.00002365
Iteration 213/1000 | Loss: 0.00002093
Iteration 214/1000 | Loss: 0.00017679
Iteration 215/1000 | Loss: 0.00038699
Iteration 216/1000 | Loss: 0.00037242
Iteration 217/1000 | Loss: 0.00035594
Iteration 218/1000 | Loss: 0.00033784
Iteration 219/1000 | Loss: 0.00015964
Iteration 220/1000 | Loss: 0.00013248
Iteration 221/1000 | Loss: 0.00017640
Iteration 222/1000 | Loss: 0.00012449
Iteration 223/1000 | Loss: 0.00002917
Iteration 224/1000 | Loss: 0.00002364
Iteration 225/1000 | Loss: 0.00013987
Iteration 226/1000 | Loss: 0.00002432
Iteration 227/1000 | Loss: 0.00002522
Iteration 228/1000 | Loss: 0.00002013
Iteration 229/1000 | Loss: 0.00003474
Iteration 230/1000 | Loss: 0.00003784
Iteration 231/1000 | Loss: 0.00003629
Iteration 232/1000 | Loss: 0.00004319
Iteration 233/1000 | Loss: 0.00007409
Iteration 234/1000 | Loss: 0.00002517
Iteration 235/1000 | Loss: 0.00002384
Iteration 236/1000 | Loss: 0.00002501
Iteration 237/1000 | Loss: 0.00002450
Iteration 238/1000 | Loss: 0.00019580
Iteration 239/1000 | Loss: 0.00028151
Iteration 240/1000 | Loss: 0.00020505
Iteration 241/1000 | Loss: 0.00003200
Iteration 242/1000 | Loss: 0.00003114
Iteration 243/1000 | Loss: 0.00010528
Iteration 244/1000 | Loss: 0.00003982
Iteration 245/1000 | Loss: 0.00002685
Iteration 246/1000 | Loss: 0.00002406
Iteration 247/1000 | Loss: 0.00002247
Iteration 248/1000 | Loss: 0.00002401
Iteration 249/1000 | Loss: 0.00002413
Iteration 250/1000 | Loss: 0.00002364
Iteration 251/1000 | Loss: 0.00002374
Iteration 252/1000 | Loss: 0.00002845
Iteration 253/1000 | Loss: 0.00002395
Iteration 254/1000 | Loss: 0.00002786
Iteration 255/1000 | Loss: 0.00001379
Iteration 256/1000 | Loss: 0.00002025
Iteration 257/1000 | Loss: 0.00002474
Iteration 258/1000 | Loss: 0.00001951
Iteration 259/1000 | Loss: 0.00001951
Iteration 260/1000 | Loss: 0.00013643
Iteration 261/1000 | Loss: 0.00010016
Iteration 262/1000 | Loss: 0.00003733
Iteration 263/1000 | Loss: 0.00006658
Iteration 264/1000 | Loss: 0.00002338
Iteration 265/1000 | Loss: 0.00002591
Iteration 266/1000 | Loss: 0.00001432
Iteration 267/1000 | Loss: 0.00007010
Iteration 268/1000 | Loss: 0.00002795
Iteration 269/1000 | Loss: 0.00002648
Iteration 270/1000 | Loss: 0.00002324
Iteration 271/1000 | Loss: 0.00005346
Iteration 272/1000 | Loss: 0.00003254
Iteration 273/1000 | Loss: 0.00007782
Iteration 274/1000 | Loss: 0.00002641
Iteration 275/1000 | Loss: 0.00002174
Iteration 276/1000 | Loss: 0.00002676
Iteration 277/1000 | Loss: 0.00002525
Iteration 278/1000 | Loss: 0.00002579
Iteration 279/1000 | Loss: 0.00002355
Iteration 280/1000 | Loss: 0.00002594
Iteration 281/1000 | Loss: 0.00001969
Iteration 282/1000 | Loss: 0.00002379
Iteration 283/1000 | Loss: 0.00002049
Iteration 284/1000 | Loss: 0.00002362
Iteration 285/1000 | Loss: 0.00013924
Iteration 286/1000 | Loss: 0.00003436
Iteration 287/1000 | Loss: 0.00003381
Iteration 288/1000 | Loss: 0.00002178
Iteration 289/1000 | Loss: 0.00006965
Iteration 290/1000 | Loss: 0.00006636
Iteration 291/1000 | Loss: 0.00002663
Iteration 292/1000 | Loss: 0.00008653
Iteration 293/1000 | Loss: 0.00002892
Iteration 294/1000 | Loss: 0.00002452
Iteration 295/1000 | Loss: 0.00017308
Iteration 296/1000 | Loss: 0.00002439
Iteration 297/1000 | Loss: 0.00002616
Iteration 298/1000 | Loss: 0.00002326
Iteration 299/1000 | Loss: 0.00002561
Iteration 300/1000 | Loss: 0.00023583
Iteration 301/1000 | Loss: 0.00003508
Iteration 302/1000 | Loss: 0.00002307
Iteration 303/1000 | Loss: 0.00007707
Iteration 304/1000 | Loss: 0.00003826
Iteration 305/1000 | Loss: 0.00002790
Iteration 306/1000 | Loss: 0.00002488
Iteration 307/1000 | Loss: 0.00002851
Iteration 308/1000 | Loss: 0.00013150
Iteration 309/1000 | Loss: 0.00008269
Iteration 310/1000 | Loss: 0.00002107
Iteration 311/1000 | Loss: 0.00002820
Iteration 312/1000 | Loss: 0.00008736
Iteration 313/1000 | Loss: 0.00014988
Iteration 314/1000 | Loss: 0.00006357
Iteration 315/1000 | Loss: 0.00004657
Iteration 316/1000 | Loss: 0.00002342
Iteration 317/1000 | Loss: 0.00002486
Iteration 318/1000 | Loss: 0.00001997
Iteration 319/1000 | Loss: 0.00002999
Iteration 320/1000 | Loss: 0.00006851
Iteration 321/1000 | Loss: 0.00003178
Iteration 322/1000 | Loss: 0.00006624
Iteration 323/1000 | Loss: 0.00005239
Iteration 324/1000 | Loss: 0.00002913
Iteration 325/1000 | Loss: 0.00012583
Iteration 326/1000 | Loss: 0.00011080
Iteration 327/1000 | Loss: 0.00010067
Iteration 328/1000 | Loss: 0.00007107
Iteration 329/1000 | Loss: 0.00003646
Iteration 330/1000 | Loss: 0.00001957
Iteration 331/1000 | Loss: 0.00003107
Iteration 332/1000 | Loss: 0.00002747
Iteration 333/1000 | Loss: 0.00005821
Iteration 334/1000 | Loss: 0.00002454
Iteration 335/1000 | Loss: 0.00002554
Iteration 336/1000 | Loss: 0.00002358
Iteration 337/1000 | Loss: 0.00003828
Iteration 338/1000 | Loss: 0.00002442
Iteration 339/1000 | Loss: 0.00003557
Iteration 340/1000 | Loss: 0.00003228
Iteration 341/1000 | Loss: 0.00002731
Iteration 342/1000 | Loss: 0.00002599
Iteration 343/1000 | Loss: 0.00002642
Iteration 344/1000 | Loss: 0.00001635
Iteration 345/1000 | Loss: 0.00005241
Iteration 346/1000 | Loss: 0.00002790
Iteration 347/1000 | Loss: 0.00002371
Iteration 348/1000 | Loss: 0.00002648
Iteration 349/1000 | Loss: 0.00002517
Iteration 350/1000 | Loss: 0.00002640
Iteration 351/1000 | Loss: 0.00001917
Iteration 352/1000 | Loss: 0.00002527
Iteration 353/1000 | Loss: 0.00002574
Iteration 354/1000 | Loss: 0.00006425
Iteration 355/1000 | Loss: 0.00002597
Iteration 356/1000 | Loss: 0.00003999
Iteration 357/1000 | Loss: 0.00002530
Iteration 358/1000 | Loss: 0.00002373
Iteration 359/1000 | Loss: 0.00002785
Iteration 360/1000 | Loss: 0.00002379
Iteration 361/1000 | Loss: 0.00002717
Iteration 362/1000 | Loss: 0.00002327
Iteration 363/1000 | Loss: 0.00003010
Iteration 364/1000 | Loss: 0.00006233
Iteration 365/1000 | Loss: 0.00002726
Iteration 366/1000 | Loss: 0.00002783
Iteration 367/1000 | Loss: 0.00002441
Iteration 368/1000 | Loss: 0.00002662
Iteration 369/1000 | Loss: 0.00002375
Iteration 370/1000 | Loss: 0.00018288
Iteration 371/1000 | Loss: 0.00002610
Iteration 372/1000 | Loss: 0.00002546
Iteration 373/1000 | Loss: 0.00002348
Iteration 374/1000 | Loss: 0.00002465
Iteration 375/1000 | Loss: 0.00019992
Iteration 376/1000 | Loss: 0.00005616
Iteration 377/1000 | Loss: 0.00004993
Iteration 378/1000 | Loss: 0.00004130
Iteration 379/1000 | Loss: 0.00003612
Iteration 380/1000 | Loss: 0.00002363
Iteration 381/1000 | Loss: 0.00002793
Iteration 382/1000 | Loss: 0.00002349
Iteration 383/1000 | Loss: 0.00002978
Iteration 384/1000 | Loss: 0.00002717
Iteration 385/1000 | Loss: 0.00008874
Iteration 386/1000 | Loss: 0.00002647
Iteration 387/1000 | Loss: 0.00002500
Iteration 388/1000 | Loss: 0.00002407
Iteration 389/1000 | Loss: 0.00002619
Iteration 390/1000 | Loss: 0.00002362
Iteration 391/1000 | Loss: 0.00002967
Iteration 392/1000 | Loss: 0.00029665
Iteration 393/1000 | Loss: 0.00004835
Iteration 394/1000 | Loss: 0.00002433
Iteration 395/1000 | Loss: 0.00002646
Iteration 396/1000 | Loss: 0.00002383
Iteration 397/1000 | Loss: 0.00002567
Iteration 398/1000 | Loss: 0.00002413
Iteration 399/1000 | Loss: 0.00002558
Iteration 400/1000 | Loss: 0.00002368
Iteration 401/1000 | Loss: 0.00002517
Iteration 402/1000 | Loss: 0.00002473
Iteration 403/1000 | Loss: 0.00002984
Iteration 404/1000 | Loss: 0.00018851
Iteration 405/1000 | Loss: 0.00005976
Iteration 406/1000 | Loss: 0.00004888
Iteration 407/1000 | Loss: 0.00003599
Iteration 408/1000 | Loss: 0.00002407
Iteration 409/1000 | Loss: 0.00004809
Iteration 410/1000 | Loss: 0.00007670
Iteration 411/1000 | Loss: 0.00002782
Iteration 412/1000 | Loss: 0.00002357
Iteration 413/1000 | Loss: 0.00002356
Iteration 414/1000 | Loss: 0.00013830
Iteration 415/1000 | Loss: 0.00006857
Iteration 416/1000 | Loss: 0.00001618
Iteration 417/1000 | Loss: 0.00001314
Iteration 418/1000 | Loss: 0.00001243
Iteration 419/1000 | Loss: 0.00001213
Iteration 420/1000 | Loss: 0.00001185
Iteration 421/1000 | Loss: 0.00001165
Iteration 422/1000 | Loss: 0.00001157
Iteration 423/1000 | Loss: 0.00001150
Iteration 424/1000 | Loss: 0.00001148
Iteration 425/1000 | Loss: 0.00001147
Iteration 426/1000 | Loss: 0.00001142
Iteration 427/1000 | Loss: 0.00001142
Iteration 428/1000 | Loss: 0.00001141
Iteration 429/1000 | Loss: 0.00001139
Iteration 430/1000 | Loss: 0.00001136
Iteration 431/1000 | Loss: 0.00001134
Iteration 432/1000 | Loss: 0.00001134
Iteration 433/1000 | Loss: 0.00001133
Iteration 434/1000 | Loss: 0.00001132
Iteration 435/1000 | Loss: 0.00001132
Iteration 436/1000 | Loss: 0.00001132
Iteration 437/1000 | Loss: 0.00001132
Iteration 438/1000 | Loss: 0.00001131
Iteration 439/1000 | Loss: 0.00001131
Iteration 440/1000 | Loss: 0.00001131
Iteration 441/1000 | Loss: 0.00001130
Iteration 442/1000 | Loss: 0.00001129
Iteration 443/1000 | Loss: 0.00001129
Iteration 444/1000 | Loss: 0.00001129
Iteration 445/1000 | Loss: 0.00001129
Iteration 446/1000 | Loss: 0.00001129
Iteration 447/1000 | Loss: 0.00001129
Iteration 448/1000 | Loss: 0.00001129
Iteration 449/1000 | Loss: 0.00001129
Iteration 450/1000 | Loss: 0.00001127
Iteration 451/1000 | Loss: 0.00001127
Iteration 452/1000 | Loss: 0.00001127
Iteration 453/1000 | Loss: 0.00001127
Iteration 454/1000 | Loss: 0.00001126
Iteration 455/1000 | Loss: 0.00001126
Iteration 456/1000 | Loss: 0.00001125
Iteration 457/1000 | Loss: 0.00001125
Iteration 458/1000 | Loss: 0.00001123
Iteration 459/1000 | Loss: 0.00001123
Iteration 460/1000 | Loss: 0.00001123
Iteration 461/1000 | Loss: 0.00001123
Iteration 462/1000 | Loss: 0.00001122
Iteration 463/1000 | Loss: 0.00001122
Iteration 464/1000 | Loss: 0.00001122
Iteration 465/1000 | Loss: 0.00001122
Iteration 466/1000 | Loss: 0.00001122
Iteration 467/1000 | Loss: 0.00001122
Iteration 468/1000 | Loss: 0.00001121
Iteration 469/1000 | Loss: 0.00001120
Iteration 470/1000 | Loss: 0.00001120
Iteration 471/1000 | Loss: 0.00001120
Iteration 472/1000 | Loss: 0.00001119
Iteration 473/1000 | Loss: 0.00001119
Iteration 474/1000 | Loss: 0.00001119
Iteration 475/1000 | Loss: 0.00001119
Iteration 476/1000 | Loss: 0.00001118
Iteration 477/1000 | Loss: 0.00001118
Iteration 478/1000 | Loss: 0.00001118
Iteration 479/1000 | Loss: 0.00001117
Iteration 480/1000 | Loss: 0.00001117
Iteration 481/1000 | Loss: 0.00001116
Iteration 482/1000 | Loss: 0.00001116
Iteration 483/1000 | Loss: 0.00001115
Iteration 484/1000 | Loss: 0.00001115
Iteration 485/1000 | Loss: 0.00001115
Iteration 486/1000 | Loss: 0.00001115
Iteration 487/1000 | Loss: 0.00001114
Iteration 488/1000 | Loss: 0.00001114
Iteration 489/1000 | Loss: 0.00001114
Iteration 490/1000 | Loss: 0.00001114
Iteration 491/1000 | Loss: 0.00001114
Iteration 492/1000 | Loss: 0.00001114
Iteration 493/1000 | Loss: 0.00001114
Iteration 494/1000 | Loss: 0.00001114
Iteration 495/1000 | Loss: 0.00001114
Iteration 496/1000 | Loss: 0.00001113
Iteration 497/1000 | Loss: 0.00001113
Iteration 498/1000 | Loss: 0.00001113
Iteration 499/1000 | Loss: 0.00001113
Iteration 500/1000 | Loss: 0.00001113
Iteration 501/1000 | Loss: 0.00001113
Iteration 502/1000 | Loss: 0.00001113
Iteration 503/1000 | Loss: 0.00001112
Iteration 504/1000 | Loss: 0.00001112
Iteration 505/1000 | Loss: 0.00001112
Iteration 506/1000 | Loss: 0.00001112
Iteration 507/1000 | Loss: 0.00001112
Iteration 508/1000 | Loss: 0.00001112
Iteration 509/1000 | Loss: 0.00001112
Iteration 510/1000 | Loss: 0.00001112
Iteration 511/1000 | Loss: 0.00001111
Iteration 512/1000 | Loss: 0.00001111
Iteration 513/1000 | Loss: 0.00001111
Iteration 514/1000 | Loss: 0.00001111
Iteration 515/1000 | Loss: 0.00001111
Iteration 516/1000 | Loss: 0.00001111
Iteration 517/1000 | Loss: 0.00001110
Iteration 518/1000 | Loss: 0.00001110
Iteration 519/1000 | Loss: 0.00001110
Iteration 520/1000 | Loss: 0.00001110
Iteration 521/1000 | Loss: 0.00001109
Iteration 522/1000 | Loss: 0.00001109
Iteration 523/1000 | Loss: 0.00001109
Iteration 524/1000 | Loss: 0.00001109
Iteration 525/1000 | Loss: 0.00001109
Iteration 526/1000 | Loss: 0.00001109
Iteration 527/1000 | Loss: 0.00001109
Iteration 528/1000 | Loss: 0.00001109
Iteration 529/1000 | Loss: 0.00001108
Iteration 530/1000 | Loss: 0.00001108
Iteration 531/1000 | Loss: 0.00001108
Iteration 532/1000 | Loss: 0.00001108
Iteration 533/1000 | Loss: 0.00001108
Iteration 534/1000 | Loss: 0.00001108
Iteration 535/1000 | Loss: 0.00001108
Iteration 536/1000 | Loss: 0.00001108
Iteration 537/1000 | Loss: 0.00001108
Iteration 538/1000 | Loss: 0.00001108
Iteration 539/1000 | Loss: 0.00001108
Iteration 540/1000 | Loss: 0.00001108
Iteration 541/1000 | Loss: 0.00001108
Iteration 542/1000 | Loss: 0.00001108
Iteration 543/1000 | Loss: 0.00001108
Iteration 544/1000 | Loss: 0.00001108
Iteration 545/1000 | Loss: 0.00001108
Iteration 546/1000 | Loss: 0.00001108
Iteration 547/1000 | Loss: 0.00001108
Iteration 548/1000 | Loss: 0.00001108
Iteration 549/1000 | Loss: 0.00001108
Iteration 550/1000 | Loss: 0.00001108
Iteration 551/1000 | Loss: 0.00001108
Iteration 552/1000 | Loss: 0.00001108
Iteration 553/1000 | Loss: 0.00001108
Iteration 554/1000 | Loss: 0.00001107
Iteration 555/1000 | Loss: 0.00001107
Iteration 556/1000 | Loss: 0.00001107
Iteration 557/1000 | Loss: 0.00001107
Iteration 558/1000 | Loss: 0.00001107
Iteration 559/1000 | Loss: 0.00001107
Iteration 560/1000 | Loss: 0.00001107
Iteration 561/1000 | Loss: 0.00001107
Iteration 562/1000 | Loss: 0.00001107
Iteration 563/1000 | Loss: 0.00001107
Iteration 564/1000 | Loss: 0.00001107
Iteration 565/1000 | Loss: 0.00001107
Iteration 566/1000 | Loss: 0.00001107
Iteration 567/1000 | Loss: 0.00001107
Iteration 568/1000 | Loss: 0.00001107
Iteration 569/1000 | Loss: 0.00001107
Iteration 570/1000 | Loss: 0.00001107
Iteration 571/1000 | Loss: 0.00001107
Iteration 572/1000 | Loss: 0.00001106
Iteration 573/1000 | Loss: 0.00001106
Iteration 574/1000 | Loss: 0.00001106
Iteration 575/1000 | Loss: 0.00001106
Iteration 576/1000 | Loss: 0.00001106
Iteration 577/1000 | Loss: 0.00001106
Iteration 578/1000 | Loss: 0.00001106
Iteration 579/1000 | Loss: 0.00001106
Iteration 580/1000 | Loss: 0.00001106
Iteration 581/1000 | Loss: 0.00001106
Iteration 582/1000 | Loss: 0.00001106
Iteration 583/1000 | Loss: 0.00001106
Iteration 584/1000 | Loss: 0.00001106
Iteration 585/1000 | Loss: 0.00001106
Iteration 586/1000 | Loss: 0.00001106
Iteration 587/1000 | Loss: 0.00001106
Iteration 588/1000 | Loss: 0.00001106
Iteration 589/1000 | Loss: 0.00001106
Iteration 590/1000 | Loss: 0.00001106
Iteration 591/1000 | Loss: 0.00001106
Iteration 592/1000 | Loss: 0.00001106
Iteration 593/1000 | Loss: 0.00001106
Iteration 594/1000 | Loss: 0.00001106
Iteration 595/1000 | Loss: 0.00001106
Iteration 596/1000 | Loss: 0.00001106
Iteration 597/1000 | Loss: 0.00001106
Iteration 598/1000 | Loss: 0.00001106
Iteration 599/1000 | Loss: 0.00001106
Iteration 600/1000 | Loss: 0.00001106
Iteration 601/1000 | Loss: 0.00001106
Iteration 602/1000 | Loss: 0.00001106
Iteration 603/1000 | Loss: 0.00001106
Iteration 604/1000 | Loss: 0.00001106
Iteration 605/1000 | Loss: 0.00001106
Iteration 606/1000 | Loss: 0.00001106
Iteration 607/1000 | Loss: 0.00001106
Iteration 608/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 608. Stopping optimization.
Last 5 losses: [1.1057626579713542e-05, 1.1057626579713542e-05, 1.1057626579713542e-05, 1.1057626579713542e-05, 1.1057626579713542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1057626579713542e-05

Optimization complete. Final v2v error: 2.8047971725463867 mm

Highest mean error: 4.75470495223999 mm for frame 59

Lowest mean error: 2.3841440677642822 mm for frame 0

Saving results

Total time: 648.0676734447479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712413
Iteration 2/25 | Loss: 0.00124818
Iteration 3/25 | Loss: 0.00111766
Iteration 4/25 | Loss: 0.00108266
Iteration 5/25 | Loss: 0.00107477
Iteration 6/25 | Loss: 0.00107421
Iteration 7/25 | Loss: 0.00107224
Iteration 8/25 | Loss: 0.00107123
Iteration 9/25 | Loss: 0.00107070
Iteration 10/25 | Loss: 0.00107037
Iteration 11/25 | Loss: 0.00107016
Iteration 12/25 | Loss: 0.00107006
Iteration 13/25 | Loss: 0.00107003
Iteration 14/25 | Loss: 0.00107003
Iteration 15/25 | Loss: 0.00107003
Iteration 16/25 | Loss: 0.00107003
Iteration 17/25 | Loss: 0.00107003
Iteration 18/25 | Loss: 0.00107002
Iteration 19/25 | Loss: 0.00107002
Iteration 20/25 | Loss: 0.00107002
Iteration 21/25 | Loss: 0.00107001
Iteration 22/25 | Loss: 0.00107001
Iteration 23/25 | Loss: 0.00107001
Iteration 24/25 | Loss: 0.00107000
Iteration 25/25 | Loss: 0.00107000

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81573987
Iteration 2/25 | Loss: 0.00086653
Iteration 3/25 | Loss: 0.00086653
Iteration 4/25 | Loss: 0.00086653
Iteration 5/25 | Loss: 0.00086653
Iteration 6/25 | Loss: 0.00086653
Iteration 7/25 | Loss: 0.00086652
Iteration 8/25 | Loss: 0.00086652
Iteration 9/25 | Loss: 0.00086652
Iteration 10/25 | Loss: 0.00086652
Iteration 11/25 | Loss: 0.00086652
Iteration 12/25 | Loss: 0.00086652
Iteration 13/25 | Loss: 0.00086652
Iteration 14/25 | Loss: 0.00086652
Iteration 15/25 | Loss: 0.00086652
Iteration 16/25 | Loss: 0.00086652
Iteration 17/25 | Loss: 0.00086652
Iteration 18/25 | Loss: 0.00086652
Iteration 19/25 | Loss: 0.00086652
Iteration 20/25 | Loss: 0.00086652
Iteration 21/25 | Loss: 0.00086652
Iteration 22/25 | Loss: 0.00086652
Iteration 23/25 | Loss: 0.00086652
Iteration 24/25 | Loss: 0.00086652
Iteration 25/25 | Loss: 0.00086652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086652
Iteration 2/1000 | Loss: 0.00001549
Iteration 3/1000 | Loss: 0.00001191
Iteration 4/1000 | Loss: 0.00001115
Iteration 5/1000 | Loss: 0.00001053
Iteration 6/1000 | Loss: 0.00005686
Iteration 7/1000 | Loss: 0.00001017
Iteration 8/1000 | Loss: 0.00000992
Iteration 9/1000 | Loss: 0.00000967
Iteration 10/1000 | Loss: 0.00000952
Iteration 11/1000 | Loss: 0.00000939
Iteration 12/1000 | Loss: 0.00000935
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000922
Iteration 15/1000 | Loss: 0.00006368
Iteration 16/1000 | Loss: 0.00039147
Iteration 17/1000 | Loss: 0.00000927
Iteration 18/1000 | Loss: 0.00000913
Iteration 19/1000 | Loss: 0.00000911
Iteration 20/1000 | Loss: 0.00000910
Iteration 21/1000 | Loss: 0.00000910
Iteration 22/1000 | Loss: 0.00000910
Iteration 23/1000 | Loss: 0.00000910
Iteration 24/1000 | Loss: 0.00000909
Iteration 25/1000 | Loss: 0.00000909
Iteration 26/1000 | Loss: 0.00000908
Iteration 27/1000 | Loss: 0.00000907
Iteration 28/1000 | Loss: 0.00000906
Iteration 29/1000 | Loss: 0.00000905
Iteration 30/1000 | Loss: 0.00000905
Iteration 31/1000 | Loss: 0.00000905
Iteration 32/1000 | Loss: 0.00000905
Iteration 33/1000 | Loss: 0.00000905
Iteration 34/1000 | Loss: 0.00000905
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000902
Iteration 38/1000 | Loss: 0.00000902
Iteration 39/1000 | Loss: 0.00000902
Iteration 40/1000 | Loss: 0.00005651
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000899
Iteration 43/1000 | Loss: 0.00000899
Iteration 44/1000 | Loss: 0.00000898
Iteration 45/1000 | Loss: 0.00000898
Iteration 46/1000 | Loss: 0.00000898
Iteration 47/1000 | Loss: 0.00002171
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000898
Iteration 50/1000 | Loss: 0.00000898
Iteration 51/1000 | Loss: 0.00000898
Iteration 52/1000 | Loss: 0.00000898
Iteration 53/1000 | Loss: 0.00000897
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000897
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000897
Iteration 58/1000 | Loss: 0.00000897
Iteration 59/1000 | Loss: 0.00000897
Iteration 60/1000 | Loss: 0.00000897
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000896
Iteration 63/1000 | Loss: 0.00000896
Iteration 64/1000 | Loss: 0.00000893
Iteration 65/1000 | Loss: 0.00000893
Iteration 66/1000 | Loss: 0.00000893
Iteration 67/1000 | Loss: 0.00000893
Iteration 68/1000 | Loss: 0.00000893
Iteration 69/1000 | Loss: 0.00000892
Iteration 70/1000 | Loss: 0.00000892
Iteration 71/1000 | Loss: 0.00000892
Iteration 72/1000 | Loss: 0.00002301
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00000892
Iteration 75/1000 | Loss: 0.00000891
Iteration 76/1000 | Loss: 0.00000891
Iteration 77/1000 | Loss: 0.00000891
Iteration 78/1000 | Loss: 0.00000891
Iteration 79/1000 | Loss: 0.00000891
Iteration 80/1000 | Loss: 0.00000891
Iteration 81/1000 | Loss: 0.00000891
Iteration 82/1000 | Loss: 0.00000891
Iteration 83/1000 | Loss: 0.00000891
Iteration 84/1000 | Loss: 0.00000891
Iteration 85/1000 | Loss: 0.00000890
Iteration 86/1000 | Loss: 0.00000889
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000887
Iteration 91/1000 | Loss: 0.00000887
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000887
Iteration 95/1000 | Loss: 0.00000887
Iteration 96/1000 | Loss: 0.00000887
Iteration 97/1000 | Loss: 0.00000887
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000887
Iteration 101/1000 | Loss: 0.00000887
Iteration 102/1000 | Loss: 0.00000887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [8.867889846442267e-06, 8.867889846442267e-06, 8.867889846442267e-06, 8.867889846442267e-06, 8.867889846442267e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.867889846442267e-06

Optimization complete. Final v2v error: 2.572618007659912 mm

Highest mean error: 3.6579513549804688 mm for frame 145

Lowest mean error: 2.3855855464935303 mm for frame 28

Saving results

Total time: 64.69484400749207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848440
Iteration 2/25 | Loss: 0.00143287
Iteration 3/25 | Loss: 0.00118089
Iteration 4/25 | Loss: 0.00115029
Iteration 5/25 | Loss: 0.00114568
Iteration 6/25 | Loss: 0.00114512
Iteration 7/25 | Loss: 0.00114512
Iteration 8/25 | Loss: 0.00114512
Iteration 9/25 | Loss: 0.00114512
Iteration 10/25 | Loss: 0.00114512
Iteration 11/25 | Loss: 0.00114512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001145119545981288, 0.001145119545981288, 0.001145119545981288, 0.001145119545981288, 0.001145119545981288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001145119545981288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91605109
Iteration 2/25 | Loss: 0.00047986
Iteration 3/25 | Loss: 0.00047986
Iteration 4/25 | Loss: 0.00047985
Iteration 5/25 | Loss: 0.00047985
Iteration 6/25 | Loss: 0.00047985
Iteration 7/25 | Loss: 0.00047985
Iteration 8/25 | Loss: 0.00047985
Iteration 9/25 | Loss: 0.00047985
Iteration 10/25 | Loss: 0.00047985
Iteration 11/25 | Loss: 0.00047985
Iteration 12/25 | Loss: 0.00047985
Iteration 13/25 | Loss: 0.00047985
Iteration 14/25 | Loss: 0.00047985
Iteration 15/25 | Loss: 0.00047985
Iteration 16/25 | Loss: 0.00047985
Iteration 17/25 | Loss: 0.00047985
Iteration 18/25 | Loss: 0.00047985
Iteration 19/25 | Loss: 0.00047985
Iteration 20/25 | Loss: 0.00047985
Iteration 21/25 | Loss: 0.00047985
Iteration 22/25 | Loss: 0.00047985
Iteration 23/25 | Loss: 0.00047985
Iteration 24/25 | Loss: 0.00047985
Iteration 25/25 | Loss: 0.00047985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047985
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00002006
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001843
Iteration 11/1000 | Loss: 0.00001824
Iteration 12/1000 | Loss: 0.00001818
Iteration 13/1000 | Loss: 0.00001805
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001805
Iteration 16/1000 | Loss: 0.00001805
Iteration 17/1000 | Loss: 0.00001805
Iteration 18/1000 | Loss: 0.00001805
Iteration 19/1000 | Loss: 0.00001805
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001804
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001804
Iteration 26/1000 | Loss: 0.00001804
Iteration 27/1000 | Loss: 0.00001804
Iteration 28/1000 | Loss: 0.00001803
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001800
Iteration 31/1000 | Loss: 0.00001800
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001793
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001790
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001788
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001778
Iteration 89/1000 | Loss: 0.00001778
Iteration 90/1000 | Loss: 0.00001778
Iteration 91/1000 | Loss: 0.00001778
Iteration 92/1000 | Loss: 0.00001778
Iteration 93/1000 | Loss: 0.00001778
Iteration 94/1000 | Loss: 0.00001778
Iteration 95/1000 | Loss: 0.00001778
Iteration 96/1000 | Loss: 0.00001778
Iteration 97/1000 | Loss: 0.00001778
Iteration 98/1000 | Loss: 0.00001777
Iteration 99/1000 | Loss: 0.00001777
Iteration 100/1000 | Loss: 0.00001777
Iteration 101/1000 | Loss: 0.00001777
Iteration 102/1000 | Loss: 0.00001777
Iteration 103/1000 | Loss: 0.00001777
Iteration 104/1000 | Loss: 0.00001777
Iteration 105/1000 | Loss: 0.00001777
Iteration 106/1000 | Loss: 0.00001777
Iteration 107/1000 | Loss: 0.00001777
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00001776
Iteration 110/1000 | Loss: 0.00001776
Iteration 111/1000 | Loss: 0.00001776
Iteration 112/1000 | Loss: 0.00001776
Iteration 113/1000 | Loss: 0.00001776
Iteration 114/1000 | Loss: 0.00001776
Iteration 115/1000 | Loss: 0.00001776
Iteration 116/1000 | Loss: 0.00001776
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001776
Iteration 119/1000 | Loss: 0.00001775
Iteration 120/1000 | Loss: 0.00001775
Iteration 121/1000 | Loss: 0.00001775
Iteration 122/1000 | Loss: 0.00001775
Iteration 123/1000 | Loss: 0.00001775
Iteration 124/1000 | Loss: 0.00001775
Iteration 125/1000 | Loss: 0.00001775
Iteration 126/1000 | Loss: 0.00001775
Iteration 127/1000 | Loss: 0.00001775
Iteration 128/1000 | Loss: 0.00001775
Iteration 129/1000 | Loss: 0.00001775
Iteration 130/1000 | Loss: 0.00001775
Iteration 131/1000 | Loss: 0.00001774
Iteration 132/1000 | Loss: 0.00001774
Iteration 133/1000 | Loss: 0.00001774
Iteration 134/1000 | Loss: 0.00001774
Iteration 135/1000 | Loss: 0.00001774
Iteration 136/1000 | Loss: 0.00001774
Iteration 137/1000 | Loss: 0.00001774
Iteration 138/1000 | Loss: 0.00001774
Iteration 139/1000 | Loss: 0.00001774
Iteration 140/1000 | Loss: 0.00001773
Iteration 141/1000 | Loss: 0.00001773
Iteration 142/1000 | Loss: 0.00001773
Iteration 143/1000 | Loss: 0.00001773
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001772
Iteration 146/1000 | Loss: 0.00001772
Iteration 147/1000 | Loss: 0.00001772
Iteration 148/1000 | Loss: 0.00001772
Iteration 149/1000 | Loss: 0.00001772
Iteration 150/1000 | Loss: 0.00001772
Iteration 151/1000 | Loss: 0.00001772
Iteration 152/1000 | Loss: 0.00001772
Iteration 153/1000 | Loss: 0.00001772
Iteration 154/1000 | Loss: 0.00001772
Iteration 155/1000 | Loss: 0.00001772
Iteration 156/1000 | Loss: 0.00001772
Iteration 157/1000 | Loss: 0.00001772
Iteration 158/1000 | Loss: 0.00001772
Iteration 159/1000 | Loss: 0.00001772
Iteration 160/1000 | Loss: 0.00001771
Iteration 161/1000 | Loss: 0.00001771
Iteration 162/1000 | Loss: 0.00001771
Iteration 163/1000 | Loss: 0.00001771
Iteration 164/1000 | Loss: 0.00001771
Iteration 165/1000 | Loss: 0.00001771
Iteration 166/1000 | Loss: 0.00001771
Iteration 167/1000 | Loss: 0.00001771
Iteration 168/1000 | Loss: 0.00001771
Iteration 169/1000 | Loss: 0.00001771
Iteration 170/1000 | Loss: 0.00001771
Iteration 171/1000 | Loss: 0.00001770
Iteration 172/1000 | Loss: 0.00001770
Iteration 173/1000 | Loss: 0.00001770
Iteration 174/1000 | Loss: 0.00001770
Iteration 175/1000 | Loss: 0.00001770
Iteration 176/1000 | Loss: 0.00001770
Iteration 177/1000 | Loss: 0.00001769
Iteration 178/1000 | Loss: 0.00001769
Iteration 179/1000 | Loss: 0.00001769
Iteration 180/1000 | Loss: 0.00001769
Iteration 181/1000 | Loss: 0.00001769
Iteration 182/1000 | Loss: 0.00001769
Iteration 183/1000 | Loss: 0.00001769
Iteration 184/1000 | Loss: 0.00001769
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001769
Iteration 187/1000 | Loss: 0.00001769
Iteration 188/1000 | Loss: 0.00001769
Iteration 189/1000 | Loss: 0.00001769
Iteration 190/1000 | Loss: 0.00001768
Iteration 191/1000 | Loss: 0.00001768
Iteration 192/1000 | Loss: 0.00001768
Iteration 193/1000 | Loss: 0.00001768
Iteration 194/1000 | Loss: 0.00001768
Iteration 195/1000 | Loss: 0.00001768
Iteration 196/1000 | Loss: 0.00001768
Iteration 197/1000 | Loss: 0.00001768
Iteration 198/1000 | Loss: 0.00001768
Iteration 199/1000 | Loss: 0.00001768
Iteration 200/1000 | Loss: 0.00001768
Iteration 201/1000 | Loss: 0.00001768
Iteration 202/1000 | Loss: 0.00001768
Iteration 203/1000 | Loss: 0.00001768
Iteration 204/1000 | Loss: 0.00001768
Iteration 205/1000 | Loss: 0.00001768
Iteration 206/1000 | Loss: 0.00001768
Iteration 207/1000 | Loss: 0.00001768
Iteration 208/1000 | Loss: 0.00001768
Iteration 209/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.7684234990156256e-05, 1.7684234990156256e-05, 1.7684234990156256e-05, 1.7684234990156256e-05, 1.7684234990156256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7684234990156256e-05

Optimization complete. Final v2v error: 3.5229270458221436 mm

Highest mean error: 3.6675329208374023 mm for frame 127

Lowest mean error: 3.4403789043426514 mm for frame 13

Saving results

Total time: 43.33956456184387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371226
Iteration 2/25 | Loss: 0.00116860
Iteration 3/25 | Loss: 0.00108050
Iteration 4/25 | Loss: 0.00107044
Iteration 5/25 | Loss: 0.00106777
Iteration 6/25 | Loss: 0.00106706
Iteration 7/25 | Loss: 0.00106706
Iteration 8/25 | Loss: 0.00106706
Iteration 9/25 | Loss: 0.00106706
Iteration 10/25 | Loss: 0.00106706
Iteration 11/25 | Loss: 0.00106706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010670567862689495, 0.0010670567862689495, 0.0010670567862689495, 0.0010670567862689495, 0.0010670567862689495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670567862689495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87024820
Iteration 2/25 | Loss: 0.00090778
Iteration 3/25 | Loss: 0.00090777
Iteration 4/25 | Loss: 0.00090777
Iteration 5/25 | Loss: 0.00090777
Iteration 6/25 | Loss: 0.00090777
Iteration 7/25 | Loss: 0.00090777
Iteration 8/25 | Loss: 0.00090777
Iteration 9/25 | Loss: 0.00090777
Iteration 10/25 | Loss: 0.00090777
Iteration 11/25 | Loss: 0.00090777
Iteration 12/25 | Loss: 0.00090777
Iteration 13/25 | Loss: 0.00090777
Iteration 14/25 | Loss: 0.00090777
Iteration 15/25 | Loss: 0.00090777
Iteration 16/25 | Loss: 0.00090777
Iteration 17/25 | Loss: 0.00090777
Iteration 18/25 | Loss: 0.00090777
Iteration 19/25 | Loss: 0.00090777
Iteration 20/25 | Loss: 0.00090777
Iteration 21/25 | Loss: 0.00090777
Iteration 22/25 | Loss: 0.00090777
Iteration 23/25 | Loss: 0.00090777
Iteration 24/25 | Loss: 0.00090777
Iteration 25/25 | Loss: 0.00090777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090777
Iteration 2/1000 | Loss: 0.00001849
Iteration 3/1000 | Loss: 0.00001194
Iteration 4/1000 | Loss: 0.00001061
Iteration 5/1000 | Loss: 0.00000989
Iteration 6/1000 | Loss: 0.00000960
Iteration 7/1000 | Loss: 0.00000917
Iteration 8/1000 | Loss: 0.00000894
Iteration 9/1000 | Loss: 0.00000892
Iteration 10/1000 | Loss: 0.00000874
Iteration 11/1000 | Loss: 0.00000853
Iteration 12/1000 | Loss: 0.00000844
Iteration 13/1000 | Loss: 0.00000840
Iteration 14/1000 | Loss: 0.00000839
Iteration 15/1000 | Loss: 0.00000836
Iteration 16/1000 | Loss: 0.00000835
Iteration 17/1000 | Loss: 0.00000831
Iteration 18/1000 | Loss: 0.00000828
Iteration 19/1000 | Loss: 0.00000827
Iteration 20/1000 | Loss: 0.00000827
Iteration 21/1000 | Loss: 0.00000827
Iteration 22/1000 | Loss: 0.00000826
Iteration 23/1000 | Loss: 0.00000826
Iteration 24/1000 | Loss: 0.00000826
Iteration 25/1000 | Loss: 0.00000826
Iteration 26/1000 | Loss: 0.00000826
Iteration 27/1000 | Loss: 0.00000826
Iteration 28/1000 | Loss: 0.00000826
Iteration 29/1000 | Loss: 0.00000826
Iteration 30/1000 | Loss: 0.00000826
Iteration 31/1000 | Loss: 0.00000825
Iteration 32/1000 | Loss: 0.00000825
Iteration 33/1000 | Loss: 0.00000825
Iteration 34/1000 | Loss: 0.00000824
Iteration 35/1000 | Loss: 0.00000824
Iteration 36/1000 | Loss: 0.00000824
Iteration 37/1000 | Loss: 0.00000823
Iteration 38/1000 | Loss: 0.00000823
Iteration 39/1000 | Loss: 0.00000822
Iteration 40/1000 | Loss: 0.00000822
Iteration 41/1000 | Loss: 0.00000821
Iteration 42/1000 | Loss: 0.00000820
Iteration 43/1000 | Loss: 0.00000820
Iteration 44/1000 | Loss: 0.00000819
Iteration 45/1000 | Loss: 0.00000819
Iteration 46/1000 | Loss: 0.00000818
Iteration 47/1000 | Loss: 0.00000818
Iteration 48/1000 | Loss: 0.00000818
Iteration 49/1000 | Loss: 0.00000818
Iteration 50/1000 | Loss: 0.00000818
Iteration 51/1000 | Loss: 0.00000817
Iteration 52/1000 | Loss: 0.00000817
Iteration 53/1000 | Loss: 0.00000817
Iteration 54/1000 | Loss: 0.00000817
Iteration 55/1000 | Loss: 0.00000816
Iteration 56/1000 | Loss: 0.00000816
Iteration 57/1000 | Loss: 0.00000815
Iteration 58/1000 | Loss: 0.00000815
Iteration 59/1000 | Loss: 0.00000815
Iteration 60/1000 | Loss: 0.00000814
Iteration 61/1000 | Loss: 0.00000814
Iteration 62/1000 | Loss: 0.00000814
Iteration 63/1000 | Loss: 0.00000813
Iteration 64/1000 | Loss: 0.00000813
Iteration 65/1000 | Loss: 0.00000812
Iteration 66/1000 | Loss: 0.00000812
Iteration 67/1000 | Loss: 0.00000811
Iteration 68/1000 | Loss: 0.00000811
Iteration 69/1000 | Loss: 0.00000811
Iteration 70/1000 | Loss: 0.00000811
Iteration 71/1000 | Loss: 0.00000811
Iteration 72/1000 | Loss: 0.00000811
Iteration 73/1000 | Loss: 0.00000810
Iteration 74/1000 | Loss: 0.00000810
Iteration 75/1000 | Loss: 0.00000810
Iteration 76/1000 | Loss: 0.00000810
Iteration 77/1000 | Loss: 0.00000809
Iteration 78/1000 | Loss: 0.00000809
Iteration 79/1000 | Loss: 0.00000809
Iteration 80/1000 | Loss: 0.00000809
Iteration 81/1000 | Loss: 0.00000809
Iteration 82/1000 | Loss: 0.00000808
Iteration 83/1000 | Loss: 0.00000808
Iteration 84/1000 | Loss: 0.00000807
Iteration 85/1000 | Loss: 0.00000807
Iteration 86/1000 | Loss: 0.00000807
Iteration 87/1000 | Loss: 0.00000807
Iteration 88/1000 | Loss: 0.00000807
Iteration 89/1000 | Loss: 0.00000807
Iteration 90/1000 | Loss: 0.00000807
Iteration 91/1000 | Loss: 0.00000806
Iteration 92/1000 | Loss: 0.00000806
Iteration 93/1000 | Loss: 0.00000806
Iteration 94/1000 | Loss: 0.00000805
Iteration 95/1000 | Loss: 0.00000805
Iteration 96/1000 | Loss: 0.00000805
Iteration 97/1000 | Loss: 0.00000804
Iteration 98/1000 | Loss: 0.00000804
Iteration 99/1000 | Loss: 0.00000804
Iteration 100/1000 | Loss: 0.00000804
Iteration 101/1000 | Loss: 0.00000803
Iteration 102/1000 | Loss: 0.00000803
Iteration 103/1000 | Loss: 0.00000803
Iteration 104/1000 | Loss: 0.00000803
Iteration 105/1000 | Loss: 0.00000803
Iteration 106/1000 | Loss: 0.00000803
Iteration 107/1000 | Loss: 0.00000803
Iteration 108/1000 | Loss: 0.00000803
Iteration 109/1000 | Loss: 0.00000802
Iteration 110/1000 | Loss: 0.00000802
Iteration 111/1000 | Loss: 0.00000802
Iteration 112/1000 | Loss: 0.00000801
Iteration 113/1000 | Loss: 0.00000801
Iteration 114/1000 | Loss: 0.00000801
Iteration 115/1000 | Loss: 0.00000801
Iteration 116/1000 | Loss: 0.00000801
Iteration 117/1000 | Loss: 0.00000801
Iteration 118/1000 | Loss: 0.00000801
Iteration 119/1000 | Loss: 0.00000801
Iteration 120/1000 | Loss: 0.00000801
Iteration 121/1000 | Loss: 0.00000801
Iteration 122/1000 | Loss: 0.00000801
Iteration 123/1000 | Loss: 0.00000800
Iteration 124/1000 | Loss: 0.00000800
Iteration 125/1000 | Loss: 0.00000800
Iteration 126/1000 | Loss: 0.00000800
Iteration 127/1000 | Loss: 0.00000800
Iteration 128/1000 | Loss: 0.00000800
Iteration 129/1000 | Loss: 0.00000800
Iteration 130/1000 | Loss: 0.00000800
Iteration 131/1000 | Loss: 0.00000800
Iteration 132/1000 | Loss: 0.00000800
Iteration 133/1000 | Loss: 0.00000800
Iteration 134/1000 | Loss: 0.00000800
Iteration 135/1000 | Loss: 0.00000800
Iteration 136/1000 | Loss: 0.00000800
Iteration 137/1000 | Loss: 0.00000800
Iteration 138/1000 | Loss: 0.00000800
Iteration 139/1000 | Loss: 0.00000800
Iteration 140/1000 | Loss: 0.00000799
Iteration 141/1000 | Loss: 0.00000799
Iteration 142/1000 | Loss: 0.00000799
Iteration 143/1000 | Loss: 0.00000799
Iteration 144/1000 | Loss: 0.00000798
Iteration 145/1000 | Loss: 0.00000798
Iteration 146/1000 | Loss: 0.00000798
Iteration 147/1000 | Loss: 0.00000798
Iteration 148/1000 | Loss: 0.00000798
Iteration 149/1000 | Loss: 0.00000798
Iteration 150/1000 | Loss: 0.00000798
Iteration 151/1000 | Loss: 0.00000798
Iteration 152/1000 | Loss: 0.00000798
Iteration 153/1000 | Loss: 0.00000798
Iteration 154/1000 | Loss: 0.00000798
Iteration 155/1000 | Loss: 0.00000797
Iteration 156/1000 | Loss: 0.00000797
Iteration 157/1000 | Loss: 0.00000797
Iteration 158/1000 | Loss: 0.00000797
Iteration 159/1000 | Loss: 0.00000797
Iteration 160/1000 | Loss: 0.00000797
Iteration 161/1000 | Loss: 0.00000797
Iteration 162/1000 | Loss: 0.00000797
Iteration 163/1000 | Loss: 0.00000797
Iteration 164/1000 | Loss: 0.00000797
Iteration 165/1000 | Loss: 0.00000797
Iteration 166/1000 | Loss: 0.00000797
Iteration 167/1000 | Loss: 0.00000796
Iteration 168/1000 | Loss: 0.00000796
Iteration 169/1000 | Loss: 0.00000796
Iteration 170/1000 | Loss: 0.00000796
Iteration 171/1000 | Loss: 0.00000796
Iteration 172/1000 | Loss: 0.00000796
Iteration 173/1000 | Loss: 0.00000796
Iteration 174/1000 | Loss: 0.00000795
Iteration 175/1000 | Loss: 0.00000795
Iteration 176/1000 | Loss: 0.00000795
Iteration 177/1000 | Loss: 0.00000795
Iteration 178/1000 | Loss: 0.00000795
Iteration 179/1000 | Loss: 0.00000795
Iteration 180/1000 | Loss: 0.00000795
Iteration 181/1000 | Loss: 0.00000795
Iteration 182/1000 | Loss: 0.00000795
Iteration 183/1000 | Loss: 0.00000795
Iteration 184/1000 | Loss: 0.00000795
Iteration 185/1000 | Loss: 0.00000795
Iteration 186/1000 | Loss: 0.00000795
Iteration 187/1000 | Loss: 0.00000795
Iteration 188/1000 | Loss: 0.00000795
Iteration 189/1000 | Loss: 0.00000795
Iteration 190/1000 | Loss: 0.00000794
Iteration 191/1000 | Loss: 0.00000794
Iteration 192/1000 | Loss: 0.00000794
Iteration 193/1000 | Loss: 0.00000794
Iteration 194/1000 | Loss: 0.00000794
Iteration 195/1000 | Loss: 0.00000794
Iteration 196/1000 | Loss: 0.00000794
Iteration 197/1000 | Loss: 0.00000794
Iteration 198/1000 | Loss: 0.00000794
Iteration 199/1000 | Loss: 0.00000794
Iteration 200/1000 | Loss: 0.00000794
Iteration 201/1000 | Loss: 0.00000793
Iteration 202/1000 | Loss: 0.00000793
Iteration 203/1000 | Loss: 0.00000793
Iteration 204/1000 | Loss: 0.00000793
Iteration 205/1000 | Loss: 0.00000793
Iteration 206/1000 | Loss: 0.00000793
Iteration 207/1000 | Loss: 0.00000793
Iteration 208/1000 | Loss: 0.00000793
Iteration 209/1000 | Loss: 0.00000793
Iteration 210/1000 | Loss: 0.00000793
Iteration 211/1000 | Loss: 0.00000793
Iteration 212/1000 | Loss: 0.00000793
Iteration 213/1000 | Loss: 0.00000793
Iteration 214/1000 | Loss: 0.00000792
Iteration 215/1000 | Loss: 0.00000792
Iteration 216/1000 | Loss: 0.00000792
Iteration 217/1000 | Loss: 0.00000792
Iteration 218/1000 | Loss: 0.00000792
Iteration 219/1000 | Loss: 0.00000792
Iteration 220/1000 | Loss: 0.00000792
Iteration 221/1000 | Loss: 0.00000792
Iteration 222/1000 | Loss: 0.00000792
Iteration 223/1000 | Loss: 0.00000792
Iteration 224/1000 | Loss: 0.00000792
Iteration 225/1000 | Loss: 0.00000792
Iteration 226/1000 | Loss: 0.00000792
Iteration 227/1000 | Loss: 0.00000792
Iteration 228/1000 | Loss: 0.00000792
Iteration 229/1000 | Loss: 0.00000792
Iteration 230/1000 | Loss: 0.00000792
Iteration 231/1000 | Loss: 0.00000792
Iteration 232/1000 | Loss: 0.00000791
Iteration 233/1000 | Loss: 0.00000791
Iteration 234/1000 | Loss: 0.00000791
Iteration 235/1000 | Loss: 0.00000791
Iteration 236/1000 | Loss: 0.00000791
Iteration 237/1000 | Loss: 0.00000791
Iteration 238/1000 | Loss: 0.00000791
Iteration 239/1000 | Loss: 0.00000791
Iteration 240/1000 | Loss: 0.00000791
Iteration 241/1000 | Loss: 0.00000791
Iteration 242/1000 | Loss: 0.00000791
Iteration 243/1000 | Loss: 0.00000791
Iteration 244/1000 | Loss: 0.00000791
Iteration 245/1000 | Loss: 0.00000791
Iteration 246/1000 | Loss: 0.00000791
Iteration 247/1000 | Loss: 0.00000791
Iteration 248/1000 | Loss: 0.00000790
Iteration 249/1000 | Loss: 0.00000790
Iteration 250/1000 | Loss: 0.00000790
Iteration 251/1000 | Loss: 0.00000790
Iteration 252/1000 | Loss: 0.00000790
Iteration 253/1000 | Loss: 0.00000790
Iteration 254/1000 | Loss: 0.00000790
Iteration 255/1000 | Loss: 0.00000790
Iteration 256/1000 | Loss: 0.00000790
Iteration 257/1000 | Loss: 0.00000790
Iteration 258/1000 | Loss: 0.00000790
Iteration 259/1000 | Loss: 0.00000790
Iteration 260/1000 | Loss: 0.00000790
Iteration 261/1000 | Loss: 0.00000790
Iteration 262/1000 | Loss: 0.00000790
Iteration 263/1000 | Loss: 0.00000789
Iteration 264/1000 | Loss: 0.00000789
Iteration 265/1000 | Loss: 0.00000789
Iteration 266/1000 | Loss: 0.00000789
Iteration 267/1000 | Loss: 0.00000789
Iteration 268/1000 | Loss: 0.00000789
Iteration 269/1000 | Loss: 0.00000789
Iteration 270/1000 | Loss: 0.00000789
Iteration 271/1000 | Loss: 0.00000789
Iteration 272/1000 | Loss: 0.00000789
Iteration 273/1000 | Loss: 0.00000789
Iteration 274/1000 | Loss: 0.00000789
Iteration 275/1000 | Loss: 0.00000789
Iteration 276/1000 | Loss: 0.00000789
Iteration 277/1000 | Loss: 0.00000789
Iteration 278/1000 | Loss: 0.00000789
Iteration 279/1000 | Loss: 0.00000789
Iteration 280/1000 | Loss: 0.00000789
Iteration 281/1000 | Loss: 0.00000789
Iteration 282/1000 | Loss: 0.00000789
Iteration 283/1000 | Loss: 0.00000789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [7.889841981523205e-06, 7.889841981523205e-06, 7.889841981523205e-06, 7.889841981523205e-06, 7.889841981523205e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.889841981523205e-06

Optimization complete. Final v2v error: 2.3982322216033936 mm

Highest mean error: 3.020101547241211 mm for frame 77

Lowest mean error: 2.2143285274505615 mm for frame 152

Saving results

Total time: 44.023754596710205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976465
Iteration 2/25 | Loss: 0.00187751
Iteration 3/25 | Loss: 0.00148773
Iteration 4/25 | Loss: 0.00140840
Iteration 5/25 | Loss: 0.00139397
Iteration 6/25 | Loss: 0.00139024
Iteration 7/25 | Loss: 0.00139024
Iteration 8/25 | Loss: 0.00139024
Iteration 9/25 | Loss: 0.00139024
Iteration 10/25 | Loss: 0.00139024
Iteration 11/25 | Loss: 0.00139024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013902424834668636, 0.0013902424834668636, 0.0013902424834668636, 0.0013902424834668636, 0.0013902424834668636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013902424834668636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32450879
Iteration 2/25 | Loss: 0.00067289
Iteration 3/25 | Loss: 0.00067289
Iteration 4/25 | Loss: 0.00067289
Iteration 5/25 | Loss: 0.00067289
Iteration 6/25 | Loss: 0.00067289
Iteration 7/25 | Loss: 0.00067289
Iteration 8/25 | Loss: 0.00067289
Iteration 9/25 | Loss: 0.00067289
Iteration 10/25 | Loss: 0.00067289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0006728873704560101, 0.0006728873704560101, 0.0006728873704560101, 0.0006728873704560101, 0.0006728873704560101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006728873704560101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067289
Iteration 2/1000 | Loss: 0.00009950
Iteration 3/1000 | Loss: 0.00090933
Iteration 4/1000 | Loss: 0.00064864
Iteration 5/1000 | Loss: 0.00010457
Iteration 6/1000 | Loss: 0.00009462
Iteration 7/1000 | Loss: 0.00008445
Iteration 8/1000 | Loss: 0.00035868
Iteration 9/1000 | Loss: 0.00008637
Iteration 10/1000 | Loss: 0.00008292
Iteration 11/1000 | Loss: 0.00035549
Iteration 12/1000 | Loss: 0.00008444
Iteration 13/1000 | Loss: 0.00008163
Iteration 14/1000 | Loss: 0.00007942
Iteration 15/1000 | Loss: 0.00007725
Iteration 16/1000 | Loss: 0.00035007
Iteration 17/1000 | Loss: 0.00014485
Iteration 18/1000 | Loss: 0.00013207
Iteration 19/1000 | Loss: 0.00011041
Iteration 20/1000 | Loss: 0.00008136
Iteration 21/1000 | Loss: 0.00007845
Iteration 22/1000 | Loss: 0.00007654
Iteration 23/1000 | Loss: 0.00007539
Iteration 24/1000 | Loss: 0.00007675
Iteration 25/1000 | Loss: 0.00063401
Iteration 26/1000 | Loss: 0.00011355
Iteration 27/1000 | Loss: 0.00008462
Iteration 28/1000 | Loss: 0.00410453
Iteration 29/1000 | Loss: 0.00383989
Iteration 30/1000 | Loss: 0.00241743
Iteration 31/1000 | Loss: 0.00069315
Iteration 32/1000 | Loss: 0.00052972
Iteration 33/1000 | Loss: 0.00009147
Iteration 34/1000 | Loss: 0.00063662
Iteration 35/1000 | Loss: 0.00027022
Iteration 36/1000 | Loss: 0.00011078
Iteration 37/1000 | Loss: 0.00020433
Iteration 38/1000 | Loss: 0.00012565
Iteration 39/1000 | Loss: 0.00030664
Iteration 40/1000 | Loss: 0.00016662
Iteration 41/1000 | Loss: 0.00029089
Iteration 42/1000 | Loss: 0.00018499
Iteration 43/1000 | Loss: 0.00008749
Iteration 44/1000 | Loss: 0.00008199
Iteration 45/1000 | Loss: 0.00007907
Iteration 46/1000 | Loss: 0.00007659
Iteration 47/1000 | Loss: 0.00007499
Iteration 48/1000 | Loss: 0.00007332
Iteration 49/1000 | Loss: 0.00035312
Iteration 50/1000 | Loss: 0.00034797
Iteration 51/1000 | Loss: 0.00227205
Iteration 52/1000 | Loss: 0.00084544
Iteration 53/1000 | Loss: 0.00056092
Iteration 54/1000 | Loss: 0.00026525
Iteration 55/1000 | Loss: 0.00032164
Iteration 56/1000 | Loss: 0.00012412
Iteration 57/1000 | Loss: 0.00010831
Iteration 58/1000 | Loss: 0.00414705
Iteration 59/1000 | Loss: 0.00340172
Iteration 60/1000 | Loss: 0.00178207
Iteration 61/1000 | Loss: 0.00052672
Iteration 62/1000 | Loss: 0.00074928
Iteration 63/1000 | Loss: 0.00029887
Iteration 64/1000 | Loss: 0.00075086
Iteration 65/1000 | Loss: 0.00047956
Iteration 66/1000 | Loss: 0.00054248
Iteration 67/1000 | Loss: 0.00014240
Iteration 68/1000 | Loss: 0.00011396
Iteration 69/1000 | Loss: 0.00009981
Iteration 70/1000 | Loss: 0.00036320
Iteration 71/1000 | Loss: 0.00163394
Iteration 72/1000 | Loss: 0.00102119
Iteration 73/1000 | Loss: 0.00046540
Iteration 74/1000 | Loss: 0.00038697
Iteration 75/1000 | Loss: 0.00031138
Iteration 76/1000 | Loss: 0.00034381
Iteration 77/1000 | Loss: 0.00024351
Iteration 78/1000 | Loss: 0.00011412
Iteration 79/1000 | Loss: 0.00010045
Iteration 80/1000 | Loss: 0.00009092
Iteration 81/1000 | Loss: 0.00008691
Iteration 82/1000 | Loss: 0.00146445
Iteration 83/1000 | Loss: 0.00093656
Iteration 84/1000 | Loss: 0.00066831
Iteration 85/1000 | Loss: 0.00035903
Iteration 86/1000 | Loss: 0.00012106
Iteration 87/1000 | Loss: 0.00039865
Iteration 88/1000 | Loss: 0.00019529
Iteration 89/1000 | Loss: 0.00012650
Iteration 90/1000 | Loss: 0.00009360
Iteration 91/1000 | Loss: 0.00069423
Iteration 92/1000 | Loss: 0.00036876
Iteration 93/1000 | Loss: 0.00016037
Iteration 94/1000 | Loss: 0.00037079
Iteration 95/1000 | Loss: 0.00045265
Iteration 96/1000 | Loss: 0.00013707
Iteration 97/1000 | Loss: 0.00020318
Iteration 98/1000 | Loss: 0.00015163
Iteration 99/1000 | Loss: 0.00023456
Iteration 100/1000 | Loss: 0.00009189
Iteration 101/1000 | Loss: 0.00048873
Iteration 102/1000 | Loss: 0.00062891
Iteration 103/1000 | Loss: 0.00024771
Iteration 104/1000 | Loss: 0.00031393
Iteration 105/1000 | Loss: 0.00012754
Iteration 106/1000 | Loss: 0.00024969
Iteration 107/1000 | Loss: 0.00016126
Iteration 108/1000 | Loss: 0.00017212
Iteration 109/1000 | Loss: 0.00007668
Iteration 110/1000 | Loss: 0.00007370
Iteration 111/1000 | Loss: 0.00010601
Iteration 112/1000 | Loss: 0.00007179
Iteration 113/1000 | Loss: 0.00007378
Iteration 114/1000 | Loss: 0.00006996
Iteration 115/1000 | Loss: 0.00035228
Iteration 116/1000 | Loss: 0.00007859
Iteration 117/1000 | Loss: 0.00007370
Iteration 118/1000 | Loss: 0.00006701
Iteration 119/1000 | Loss: 0.00006370
Iteration 120/1000 | Loss: 0.00060072
Iteration 121/1000 | Loss: 0.00060065
Iteration 122/1000 | Loss: 0.00009650
Iteration 123/1000 | Loss: 0.00033945
Iteration 124/1000 | Loss: 0.00062546
Iteration 125/1000 | Loss: 0.00011511
Iteration 126/1000 | Loss: 0.00008186
Iteration 127/1000 | Loss: 0.00007475
Iteration 128/1000 | Loss: 0.00007029
Iteration 129/1000 | Loss: 0.00006687
Iteration 130/1000 | Loss: 0.00033445
Iteration 131/1000 | Loss: 0.00406130
Iteration 132/1000 | Loss: 0.00325470
Iteration 133/1000 | Loss: 0.00253751
Iteration 134/1000 | Loss: 0.00114215
Iteration 135/1000 | Loss: 0.00077933
Iteration 136/1000 | Loss: 0.00056301
Iteration 137/1000 | Loss: 0.00043386
Iteration 138/1000 | Loss: 0.00047896
Iteration 139/1000 | Loss: 0.00067420
Iteration 140/1000 | Loss: 0.00014692
Iteration 141/1000 | Loss: 0.00011781
Iteration 142/1000 | Loss: 0.00010269
Iteration 143/1000 | Loss: 0.00065945
Iteration 144/1000 | Loss: 0.00077406
Iteration 145/1000 | Loss: 0.00015105
Iteration 146/1000 | Loss: 0.00020587
Iteration 147/1000 | Loss: 0.00025725
Iteration 148/1000 | Loss: 0.00016003
Iteration 149/1000 | Loss: 0.00034331
Iteration 150/1000 | Loss: 0.00015421
Iteration 151/1000 | Loss: 0.00009892
Iteration 152/1000 | Loss: 0.00009458
Iteration 153/1000 | Loss: 0.00008261
Iteration 154/1000 | Loss: 0.00023074
Iteration 155/1000 | Loss: 0.00024549
Iteration 156/1000 | Loss: 0.00008851
Iteration 157/1000 | Loss: 0.00007714
Iteration 158/1000 | Loss: 0.00042527
Iteration 159/1000 | Loss: 0.00052646
Iteration 160/1000 | Loss: 0.00067154
Iteration 161/1000 | Loss: 0.00040395
Iteration 162/1000 | Loss: 0.00034529
Iteration 163/1000 | Loss: 0.00022841
Iteration 164/1000 | Loss: 0.00016819
Iteration 165/1000 | Loss: 0.00009117
Iteration 166/1000 | Loss: 0.00007157
Iteration 167/1000 | Loss: 0.00007819
Iteration 168/1000 | Loss: 0.00007104
Iteration 169/1000 | Loss: 0.00006447
Iteration 170/1000 | Loss: 0.00005786
Iteration 171/1000 | Loss: 0.00005537
Iteration 172/1000 | Loss: 0.00005321
Iteration 173/1000 | Loss: 0.00005158
Iteration 174/1000 | Loss: 0.00004990
Iteration 175/1000 | Loss: 0.00031104
Iteration 176/1000 | Loss: 0.00005814
Iteration 177/1000 | Loss: 0.00004840
Iteration 178/1000 | Loss: 0.00004650
Iteration 179/1000 | Loss: 0.00004533
Iteration 180/1000 | Loss: 0.00004476
Iteration 181/1000 | Loss: 0.00004417
Iteration 182/1000 | Loss: 0.00004371
Iteration 183/1000 | Loss: 0.00031279
Iteration 184/1000 | Loss: 0.00004929
Iteration 185/1000 | Loss: 0.00032201
Iteration 186/1000 | Loss: 0.00032817
Iteration 187/1000 | Loss: 0.00258761
Iteration 188/1000 | Loss: 0.01069416
Iteration 189/1000 | Loss: 0.00073592
Iteration 190/1000 | Loss: 0.00171978
Iteration 191/1000 | Loss: 0.00068946
Iteration 192/1000 | Loss: 0.00029221
Iteration 193/1000 | Loss: 0.00017425
Iteration 194/1000 | Loss: 0.00013887
Iteration 195/1000 | Loss: 0.00029858
Iteration 196/1000 | Loss: 0.00010083
Iteration 197/1000 | Loss: 0.00023700
Iteration 198/1000 | Loss: 0.00008998
Iteration 199/1000 | Loss: 0.00007689
Iteration 200/1000 | Loss: 0.00007191
Iteration 201/1000 | Loss: 0.00006803
Iteration 202/1000 | Loss: 0.00029186
Iteration 203/1000 | Loss: 0.00023542
Iteration 204/1000 | Loss: 0.00029837
Iteration 205/1000 | Loss: 0.00020948
Iteration 206/1000 | Loss: 0.00006188
Iteration 207/1000 | Loss: 0.00005937
Iteration 208/1000 | Loss: 0.00025066
Iteration 209/1000 | Loss: 0.00009930
Iteration 210/1000 | Loss: 0.00020944
Iteration 211/1000 | Loss: 0.00019419
Iteration 212/1000 | Loss: 0.00030381
Iteration 213/1000 | Loss: 0.00005889
Iteration 214/1000 | Loss: 0.00004602
Iteration 215/1000 | Loss: 0.00004096
Iteration 216/1000 | Loss: 0.00003785
Iteration 217/1000 | Loss: 0.00003579
Iteration 218/1000 | Loss: 0.00003311
Iteration 219/1000 | Loss: 0.00003056
Iteration 220/1000 | Loss: 0.00002960
Iteration 221/1000 | Loss: 0.00002885
Iteration 222/1000 | Loss: 0.00002829
Iteration 223/1000 | Loss: 0.00002784
Iteration 224/1000 | Loss: 0.00002740
Iteration 225/1000 | Loss: 0.00002707
Iteration 226/1000 | Loss: 0.00002692
Iteration 227/1000 | Loss: 0.00002673
Iteration 228/1000 | Loss: 0.00002656
Iteration 229/1000 | Loss: 0.00002652
Iteration 230/1000 | Loss: 0.00002651
Iteration 231/1000 | Loss: 0.00002651
Iteration 232/1000 | Loss: 0.00002648
Iteration 233/1000 | Loss: 0.00002639
Iteration 234/1000 | Loss: 0.00002636
Iteration 235/1000 | Loss: 0.00002634
Iteration 236/1000 | Loss: 0.00002632
Iteration 237/1000 | Loss: 0.00002631
Iteration 238/1000 | Loss: 0.00002630
Iteration 239/1000 | Loss: 0.00002630
Iteration 240/1000 | Loss: 0.00002630
Iteration 241/1000 | Loss: 0.00002630
Iteration 242/1000 | Loss: 0.00002629
Iteration 243/1000 | Loss: 0.00002629
Iteration 244/1000 | Loss: 0.00002629
Iteration 245/1000 | Loss: 0.00002628
Iteration 246/1000 | Loss: 0.00002627
Iteration 247/1000 | Loss: 0.00002627
Iteration 248/1000 | Loss: 0.00002624
Iteration 249/1000 | Loss: 0.00002624
Iteration 250/1000 | Loss: 0.00002619
Iteration 251/1000 | Loss: 0.00002619
Iteration 252/1000 | Loss: 0.00002619
Iteration 253/1000 | Loss: 0.00002617
Iteration 254/1000 | Loss: 0.00002616
Iteration 255/1000 | Loss: 0.00002615
Iteration 256/1000 | Loss: 0.00002615
Iteration 257/1000 | Loss: 0.00002614
Iteration 258/1000 | Loss: 0.00002614
Iteration 259/1000 | Loss: 0.00002613
Iteration 260/1000 | Loss: 0.00002613
Iteration 261/1000 | Loss: 0.00002613
Iteration 262/1000 | Loss: 0.00002613
Iteration 263/1000 | Loss: 0.00002613
Iteration 264/1000 | Loss: 0.00002613
Iteration 265/1000 | Loss: 0.00002612
Iteration 266/1000 | Loss: 0.00002612
Iteration 267/1000 | Loss: 0.00002612
Iteration 268/1000 | Loss: 0.00002612
Iteration 269/1000 | Loss: 0.00002612
Iteration 270/1000 | Loss: 0.00002611
Iteration 271/1000 | Loss: 0.00002611
Iteration 272/1000 | Loss: 0.00002611
Iteration 273/1000 | Loss: 0.00002610
Iteration 274/1000 | Loss: 0.00002610
Iteration 275/1000 | Loss: 0.00002610
Iteration 276/1000 | Loss: 0.00002610
Iteration 277/1000 | Loss: 0.00002610
Iteration 278/1000 | Loss: 0.00002610
Iteration 279/1000 | Loss: 0.00002610
Iteration 280/1000 | Loss: 0.00002610
Iteration 281/1000 | Loss: 0.00002610
Iteration 282/1000 | Loss: 0.00002610
Iteration 283/1000 | Loss: 0.00002610
Iteration 284/1000 | Loss: 0.00002610
Iteration 285/1000 | Loss: 0.00002610
Iteration 286/1000 | Loss: 0.00002610
Iteration 287/1000 | Loss: 0.00002609
Iteration 288/1000 | Loss: 0.00002609
Iteration 289/1000 | Loss: 0.00002608
Iteration 290/1000 | Loss: 0.00002608
Iteration 291/1000 | Loss: 0.00002608
Iteration 292/1000 | Loss: 0.00002608
Iteration 293/1000 | Loss: 0.00002608
Iteration 294/1000 | Loss: 0.00002608
Iteration 295/1000 | Loss: 0.00002608
Iteration 296/1000 | Loss: 0.00002608
Iteration 297/1000 | Loss: 0.00002607
Iteration 298/1000 | Loss: 0.00002607
Iteration 299/1000 | Loss: 0.00002606
Iteration 300/1000 | Loss: 0.00002606
Iteration 301/1000 | Loss: 0.00002606
Iteration 302/1000 | Loss: 0.00002606
Iteration 303/1000 | Loss: 0.00002605
Iteration 304/1000 | Loss: 0.00002605
Iteration 305/1000 | Loss: 0.00002605
Iteration 306/1000 | Loss: 0.00002605
Iteration 307/1000 | Loss: 0.00002604
Iteration 308/1000 | Loss: 0.00002604
Iteration 309/1000 | Loss: 0.00002603
Iteration 310/1000 | Loss: 0.00002603
Iteration 311/1000 | Loss: 0.00002603
Iteration 312/1000 | Loss: 0.00002603
Iteration 313/1000 | Loss: 0.00002603
Iteration 314/1000 | Loss: 0.00002603
Iteration 315/1000 | Loss: 0.00002603
Iteration 316/1000 | Loss: 0.00002603
Iteration 317/1000 | Loss: 0.00002602
Iteration 318/1000 | Loss: 0.00002602
Iteration 319/1000 | Loss: 0.00002602
Iteration 320/1000 | Loss: 0.00002602
Iteration 321/1000 | Loss: 0.00002602
Iteration 322/1000 | Loss: 0.00002602
Iteration 323/1000 | Loss: 0.00002602
Iteration 324/1000 | Loss: 0.00002602
Iteration 325/1000 | Loss: 0.00002602
Iteration 326/1000 | Loss: 0.00002602
Iteration 327/1000 | Loss: 0.00002602
Iteration 328/1000 | Loss: 0.00002602
Iteration 329/1000 | Loss: 0.00002602
Iteration 330/1000 | Loss: 0.00002602
Iteration 331/1000 | Loss: 0.00002602
Iteration 332/1000 | Loss: 0.00002601
Iteration 333/1000 | Loss: 0.00002601
Iteration 334/1000 | Loss: 0.00002601
Iteration 335/1000 | Loss: 0.00002601
Iteration 336/1000 | Loss: 0.00002601
Iteration 337/1000 | Loss: 0.00002601
Iteration 338/1000 | Loss: 0.00002601
Iteration 339/1000 | Loss: 0.00002601
Iteration 340/1000 | Loss: 0.00002601
Iteration 341/1000 | Loss: 0.00002601
Iteration 342/1000 | Loss: 0.00002600
Iteration 343/1000 | Loss: 0.00002600
Iteration 344/1000 | Loss: 0.00002600
Iteration 345/1000 | Loss: 0.00002600
Iteration 346/1000 | Loss: 0.00002600
Iteration 347/1000 | Loss: 0.00002600
Iteration 348/1000 | Loss: 0.00002599
Iteration 349/1000 | Loss: 0.00002599
Iteration 350/1000 | Loss: 0.00002599
Iteration 351/1000 | Loss: 0.00002599
Iteration 352/1000 | Loss: 0.00002599
Iteration 353/1000 | Loss: 0.00002598
Iteration 354/1000 | Loss: 0.00002598
Iteration 355/1000 | Loss: 0.00002598
Iteration 356/1000 | Loss: 0.00002598
Iteration 357/1000 | Loss: 0.00002598
Iteration 358/1000 | Loss: 0.00002597
Iteration 359/1000 | Loss: 0.00002597
Iteration 360/1000 | Loss: 0.00002597
Iteration 361/1000 | Loss: 0.00002597
Iteration 362/1000 | Loss: 0.00002597
Iteration 363/1000 | Loss: 0.00002597
Iteration 364/1000 | Loss: 0.00002597
Iteration 365/1000 | Loss: 0.00002597
Iteration 366/1000 | Loss: 0.00002597
Iteration 367/1000 | Loss: 0.00002597
Iteration 368/1000 | Loss: 0.00002597
Iteration 369/1000 | Loss: 0.00002597
Iteration 370/1000 | Loss: 0.00002597
Iteration 371/1000 | Loss: 0.00002597
Iteration 372/1000 | Loss: 0.00002597
Iteration 373/1000 | Loss: 0.00002597
Iteration 374/1000 | Loss: 0.00002597
Iteration 375/1000 | Loss: 0.00002597
Iteration 376/1000 | Loss: 0.00002597
Iteration 377/1000 | Loss: 0.00002597
Iteration 378/1000 | Loss: 0.00002597
Iteration 379/1000 | Loss: 0.00002597
Iteration 380/1000 | Loss: 0.00002597
Iteration 381/1000 | Loss: 0.00002597
Iteration 382/1000 | Loss: 0.00002597
Iteration 383/1000 | Loss: 0.00002597
Iteration 384/1000 | Loss: 0.00002597
Iteration 385/1000 | Loss: 0.00002597
Iteration 386/1000 | Loss: 0.00002597
Iteration 387/1000 | Loss: 0.00002597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 387. Stopping optimization.
Last 5 losses: [2.5969917260226794e-05, 2.5969917260226794e-05, 2.5969917260226794e-05, 2.5969917260226794e-05, 2.5969917260226794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5969917260226794e-05

Optimization complete. Final v2v error: 3.4449462890625 mm

Highest mean error: 5.252495765686035 mm for frame 30

Lowest mean error: 2.7918362617492676 mm for frame 47

Saving results

Total time: 355.21918845176697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934239
Iteration 2/25 | Loss: 0.00934239
Iteration 3/25 | Loss: 0.00934239
Iteration 4/25 | Loss: 0.00934239
Iteration 5/25 | Loss: 0.00323930
Iteration 6/25 | Loss: 0.00224122
Iteration 7/25 | Loss: 0.00191008
Iteration 8/25 | Loss: 0.00164315
Iteration 9/25 | Loss: 0.00149453
Iteration 10/25 | Loss: 0.00141171
Iteration 11/25 | Loss: 0.00135632
Iteration 12/25 | Loss: 0.00131816
Iteration 13/25 | Loss: 0.00129496
Iteration 14/25 | Loss: 0.00128117
Iteration 15/25 | Loss: 0.00127054
Iteration 16/25 | Loss: 0.00126996
Iteration 17/25 | Loss: 0.00125013
Iteration 18/25 | Loss: 0.00124380
Iteration 19/25 | Loss: 0.00124175
Iteration 20/25 | Loss: 0.00123659
Iteration 21/25 | Loss: 0.00123445
Iteration 22/25 | Loss: 0.00123184
Iteration 23/25 | Loss: 0.00123158
Iteration 24/25 | Loss: 0.00123016
Iteration 25/25 | Loss: 0.00122724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34180903
Iteration 2/25 | Loss: 0.00120174
Iteration 3/25 | Loss: 0.00120174
Iteration 4/25 | Loss: 0.00112860
Iteration 5/25 | Loss: 0.00112702
Iteration 6/25 | Loss: 0.00112702
Iteration 7/25 | Loss: 0.00112702
Iteration 8/25 | Loss: 0.00112702
Iteration 9/25 | Loss: 0.00112702
Iteration 10/25 | Loss: 0.00112702
Iteration 11/25 | Loss: 0.00112702
Iteration 12/25 | Loss: 0.00112702
Iteration 13/25 | Loss: 0.00112702
Iteration 14/25 | Loss: 0.00112702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011270155664533377, 0.0011270155664533377, 0.0011270155664533377, 0.0011270155664533377, 0.0011270155664533377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011270155664533377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112702
Iteration 2/1000 | Loss: 0.00053190
Iteration 3/1000 | Loss: 0.00029307
Iteration 4/1000 | Loss: 0.00013020
Iteration 5/1000 | Loss: 0.00008061
Iteration 6/1000 | Loss: 0.00007701
Iteration 7/1000 | Loss: 0.00006334
Iteration 8/1000 | Loss: 0.00028103
Iteration 9/1000 | Loss: 0.00060844
Iteration 10/1000 | Loss: 0.00028518
Iteration 11/1000 | Loss: 0.00005486
Iteration 12/1000 | Loss: 0.00008815
Iteration 13/1000 | Loss: 0.00004387
Iteration 14/1000 | Loss: 0.00021810
Iteration 15/1000 | Loss: 0.00089109
Iteration 16/1000 | Loss: 0.00015168
Iteration 17/1000 | Loss: 0.00010240
Iteration 18/1000 | Loss: 0.00012618
Iteration 19/1000 | Loss: 0.00007499
Iteration 20/1000 | Loss: 0.00005829
Iteration 21/1000 | Loss: 0.00002900
Iteration 22/1000 | Loss: 0.00002443
Iteration 23/1000 | Loss: 0.00022294
Iteration 24/1000 | Loss: 0.00003107
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00003105
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001761
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001642
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001479
Iteration 38/1000 | Loss: 0.00001479
Iteration 39/1000 | Loss: 0.00001477
Iteration 40/1000 | Loss: 0.00001476
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001473
Iteration 47/1000 | Loss: 0.00001473
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001472
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001469
Iteration 73/1000 | Loss: 0.00001469
Iteration 74/1000 | Loss: 0.00001469
Iteration 75/1000 | Loss: 0.00001469
Iteration 76/1000 | Loss: 0.00001469
Iteration 77/1000 | Loss: 0.00001469
Iteration 78/1000 | Loss: 0.00001469
Iteration 79/1000 | Loss: 0.00001468
Iteration 80/1000 | Loss: 0.00001468
Iteration 81/1000 | Loss: 0.00001468
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001467
Iteration 103/1000 | Loss: 0.00001467
Iteration 104/1000 | Loss: 0.00001467
Iteration 105/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.4668096810055431e-05, 1.4668096810055431e-05, 1.4668096810055431e-05, 1.4668096810055431e-05, 1.4668096810055431e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4668096810055431e-05

Optimization complete. Final v2v error: 3.2404322624206543 mm

Highest mean error: 3.7770943641662598 mm for frame 78

Lowest mean error: 2.8858556747436523 mm for frame 25

Saving results

Total time: 110.83227705955505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397829
Iteration 2/25 | Loss: 0.00112843
Iteration 3/25 | Loss: 0.00106881
Iteration 4/25 | Loss: 0.00105988
Iteration 5/25 | Loss: 0.00105691
Iteration 6/25 | Loss: 0.00105641
Iteration 7/25 | Loss: 0.00105641
Iteration 8/25 | Loss: 0.00105641
Iteration 9/25 | Loss: 0.00105641
Iteration 10/25 | Loss: 0.00105641
Iteration 11/25 | Loss: 0.00105641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010564111871644855, 0.0010564111871644855, 0.0010564111871644855, 0.0010564111871644855, 0.0010564111871644855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010564111871644855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.86741400
Iteration 2/25 | Loss: 0.00075415
Iteration 3/25 | Loss: 0.00075414
Iteration 4/25 | Loss: 0.00075414
Iteration 5/25 | Loss: 0.00075414
Iteration 6/25 | Loss: 0.00075414
Iteration 7/25 | Loss: 0.00075414
Iteration 8/25 | Loss: 0.00075414
Iteration 9/25 | Loss: 0.00075414
Iteration 10/25 | Loss: 0.00075414
Iteration 11/25 | Loss: 0.00075414
Iteration 12/25 | Loss: 0.00075414
Iteration 13/25 | Loss: 0.00075414
Iteration 14/25 | Loss: 0.00075414
Iteration 15/25 | Loss: 0.00075414
Iteration 16/25 | Loss: 0.00075414
Iteration 17/25 | Loss: 0.00075414
Iteration 18/25 | Loss: 0.00075414
Iteration 19/25 | Loss: 0.00075414
Iteration 20/25 | Loss: 0.00075414
Iteration 21/25 | Loss: 0.00075414
Iteration 22/25 | Loss: 0.00075414
Iteration 23/25 | Loss: 0.00075414
Iteration 24/25 | Loss: 0.00075414
Iteration 25/25 | Loss: 0.00075414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075414
Iteration 2/1000 | Loss: 0.00002071
Iteration 3/1000 | Loss: 0.00001415
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001155
Iteration 6/1000 | Loss: 0.00001109
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001024
Iteration 10/1000 | Loss: 0.00001003
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000974
Iteration 13/1000 | Loss: 0.00000973
Iteration 14/1000 | Loss: 0.00000972
Iteration 15/1000 | Loss: 0.00000972
Iteration 16/1000 | Loss: 0.00000971
Iteration 17/1000 | Loss: 0.00000969
Iteration 18/1000 | Loss: 0.00000968
Iteration 19/1000 | Loss: 0.00000967
Iteration 20/1000 | Loss: 0.00000966
Iteration 21/1000 | Loss: 0.00000966
Iteration 22/1000 | Loss: 0.00000966
Iteration 23/1000 | Loss: 0.00000965
Iteration 24/1000 | Loss: 0.00000964
Iteration 25/1000 | Loss: 0.00000964
Iteration 26/1000 | Loss: 0.00000960
Iteration 27/1000 | Loss: 0.00000958
Iteration 28/1000 | Loss: 0.00000958
Iteration 29/1000 | Loss: 0.00000958
Iteration 30/1000 | Loss: 0.00000954
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000945
Iteration 33/1000 | Loss: 0.00000945
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000943
Iteration 37/1000 | Loss: 0.00000942
Iteration 38/1000 | Loss: 0.00000942
Iteration 39/1000 | Loss: 0.00000941
Iteration 40/1000 | Loss: 0.00000941
Iteration 41/1000 | Loss: 0.00000941
Iteration 42/1000 | Loss: 0.00000941
Iteration 43/1000 | Loss: 0.00000940
Iteration 44/1000 | Loss: 0.00000940
Iteration 45/1000 | Loss: 0.00000940
Iteration 46/1000 | Loss: 0.00000939
Iteration 47/1000 | Loss: 0.00000939
Iteration 48/1000 | Loss: 0.00000939
Iteration 49/1000 | Loss: 0.00000938
Iteration 50/1000 | Loss: 0.00000938
Iteration 51/1000 | Loss: 0.00000938
Iteration 52/1000 | Loss: 0.00000938
Iteration 53/1000 | Loss: 0.00000938
Iteration 54/1000 | Loss: 0.00000938
Iteration 55/1000 | Loss: 0.00000937
Iteration 56/1000 | Loss: 0.00000936
Iteration 57/1000 | Loss: 0.00000936
Iteration 58/1000 | Loss: 0.00000936
Iteration 59/1000 | Loss: 0.00000936
Iteration 60/1000 | Loss: 0.00000936
Iteration 61/1000 | Loss: 0.00000936
Iteration 62/1000 | Loss: 0.00000935
Iteration 63/1000 | Loss: 0.00000935
Iteration 64/1000 | Loss: 0.00000935
Iteration 65/1000 | Loss: 0.00000934
Iteration 66/1000 | Loss: 0.00000934
Iteration 67/1000 | Loss: 0.00000934
Iteration 68/1000 | Loss: 0.00000934
Iteration 69/1000 | Loss: 0.00000933
Iteration 70/1000 | Loss: 0.00000933
Iteration 71/1000 | Loss: 0.00000933
Iteration 72/1000 | Loss: 0.00000933
Iteration 73/1000 | Loss: 0.00000932
Iteration 74/1000 | Loss: 0.00000932
Iteration 75/1000 | Loss: 0.00000931
Iteration 76/1000 | Loss: 0.00000931
Iteration 77/1000 | Loss: 0.00000931
Iteration 78/1000 | Loss: 0.00000931
Iteration 79/1000 | Loss: 0.00000931
Iteration 80/1000 | Loss: 0.00000930
Iteration 81/1000 | Loss: 0.00000930
Iteration 82/1000 | Loss: 0.00000930
Iteration 83/1000 | Loss: 0.00000929
Iteration 84/1000 | Loss: 0.00000928
Iteration 85/1000 | Loss: 0.00000928
Iteration 86/1000 | Loss: 0.00000927
Iteration 87/1000 | Loss: 0.00000927
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000926
Iteration 91/1000 | Loss: 0.00000925
Iteration 92/1000 | Loss: 0.00000925
Iteration 93/1000 | Loss: 0.00000925
Iteration 94/1000 | Loss: 0.00000925
Iteration 95/1000 | Loss: 0.00000924
Iteration 96/1000 | Loss: 0.00000924
Iteration 97/1000 | Loss: 0.00000924
Iteration 98/1000 | Loss: 0.00000924
Iteration 99/1000 | Loss: 0.00000924
Iteration 100/1000 | Loss: 0.00000924
Iteration 101/1000 | Loss: 0.00000924
Iteration 102/1000 | Loss: 0.00000924
Iteration 103/1000 | Loss: 0.00000924
Iteration 104/1000 | Loss: 0.00000923
Iteration 105/1000 | Loss: 0.00000923
Iteration 106/1000 | Loss: 0.00000923
Iteration 107/1000 | Loss: 0.00000923
Iteration 108/1000 | Loss: 0.00000923
Iteration 109/1000 | Loss: 0.00000922
Iteration 110/1000 | Loss: 0.00000922
Iteration 111/1000 | Loss: 0.00000922
Iteration 112/1000 | Loss: 0.00000922
Iteration 113/1000 | Loss: 0.00000922
Iteration 114/1000 | Loss: 0.00000921
Iteration 115/1000 | Loss: 0.00000921
Iteration 116/1000 | Loss: 0.00000921
Iteration 117/1000 | Loss: 0.00000921
Iteration 118/1000 | Loss: 0.00000921
Iteration 119/1000 | Loss: 0.00000921
Iteration 120/1000 | Loss: 0.00000921
Iteration 121/1000 | Loss: 0.00000921
Iteration 122/1000 | Loss: 0.00000921
Iteration 123/1000 | Loss: 0.00000921
Iteration 124/1000 | Loss: 0.00000921
Iteration 125/1000 | Loss: 0.00000921
Iteration 126/1000 | Loss: 0.00000921
Iteration 127/1000 | Loss: 0.00000921
Iteration 128/1000 | Loss: 0.00000921
Iteration 129/1000 | Loss: 0.00000920
Iteration 130/1000 | Loss: 0.00000920
Iteration 131/1000 | Loss: 0.00000920
Iteration 132/1000 | Loss: 0.00000920
Iteration 133/1000 | Loss: 0.00000920
Iteration 134/1000 | Loss: 0.00000920
Iteration 135/1000 | Loss: 0.00000920
Iteration 136/1000 | Loss: 0.00000920
Iteration 137/1000 | Loss: 0.00000920
Iteration 138/1000 | Loss: 0.00000920
Iteration 139/1000 | Loss: 0.00000920
Iteration 140/1000 | Loss: 0.00000919
Iteration 141/1000 | Loss: 0.00000919
Iteration 142/1000 | Loss: 0.00000919
Iteration 143/1000 | Loss: 0.00000919
Iteration 144/1000 | Loss: 0.00000919
Iteration 145/1000 | Loss: 0.00000918
Iteration 146/1000 | Loss: 0.00000918
Iteration 147/1000 | Loss: 0.00000918
Iteration 148/1000 | Loss: 0.00000918
Iteration 149/1000 | Loss: 0.00000918
Iteration 150/1000 | Loss: 0.00000918
Iteration 151/1000 | Loss: 0.00000918
Iteration 152/1000 | Loss: 0.00000918
Iteration 153/1000 | Loss: 0.00000918
Iteration 154/1000 | Loss: 0.00000918
Iteration 155/1000 | Loss: 0.00000917
Iteration 156/1000 | Loss: 0.00000917
Iteration 157/1000 | Loss: 0.00000917
Iteration 158/1000 | Loss: 0.00000917
Iteration 159/1000 | Loss: 0.00000916
Iteration 160/1000 | Loss: 0.00000916
Iteration 161/1000 | Loss: 0.00000916
Iteration 162/1000 | Loss: 0.00000916
Iteration 163/1000 | Loss: 0.00000916
Iteration 164/1000 | Loss: 0.00000916
Iteration 165/1000 | Loss: 0.00000916
Iteration 166/1000 | Loss: 0.00000916
Iteration 167/1000 | Loss: 0.00000916
Iteration 168/1000 | Loss: 0.00000915
Iteration 169/1000 | Loss: 0.00000915
Iteration 170/1000 | Loss: 0.00000915
Iteration 171/1000 | Loss: 0.00000914
Iteration 172/1000 | Loss: 0.00000914
Iteration 173/1000 | Loss: 0.00000914
Iteration 174/1000 | Loss: 0.00000914
Iteration 175/1000 | Loss: 0.00000913
Iteration 176/1000 | Loss: 0.00000913
Iteration 177/1000 | Loss: 0.00000913
Iteration 178/1000 | Loss: 0.00000913
Iteration 179/1000 | Loss: 0.00000913
Iteration 180/1000 | Loss: 0.00000913
Iteration 181/1000 | Loss: 0.00000913
Iteration 182/1000 | Loss: 0.00000913
Iteration 183/1000 | Loss: 0.00000913
Iteration 184/1000 | Loss: 0.00000913
Iteration 185/1000 | Loss: 0.00000912
Iteration 186/1000 | Loss: 0.00000912
Iteration 187/1000 | Loss: 0.00000912
Iteration 188/1000 | Loss: 0.00000912
Iteration 189/1000 | Loss: 0.00000912
Iteration 190/1000 | Loss: 0.00000912
Iteration 191/1000 | Loss: 0.00000911
Iteration 192/1000 | Loss: 0.00000911
Iteration 193/1000 | Loss: 0.00000911
Iteration 194/1000 | Loss: 0.00000911
Iteration 195/1000 | Loss: 0.00000911
Iteration 196/1000 | Loss: 0.00000911
Iteration 197/1000 | Loss: 0.00000911
Iteration 198/1000 | Loss: 0.00000911
Iteration 199/1000 | Loss: 0.00000911
Iteration 200/1000 | Loss: 0.00000911
Iteration 201/1000 | Loss: 0.00000911
Iteration 202/1000 | Loss: 0.00000911
Iteration 203/1000 | Loss: 0.00000911
Iteration 204/1000 | Loss: 0.00000911
Iteration 205/1000 | Loss: 0.00000911
Iteration 206/1000 | Loss: 0.00000911
Iteration 207/1000 | Loss: 0.00000911
Iteration 208/1000 | Loss: 0.00000911
Iteration 209/1000 | Loss: 0.00000910
Iteration 210/1000 | Loss: 0.00000910
Iteration 211/1000 | Loss: 0.00000910
Iteration 212/1000 | Loss: 0.00000910
Iteration 213/1000 | Loss: 0.00000910
Iteration 214/1000 | Loss: 0.00000910
Iteration 215/1000 | Loss: 0.00000910
Iteration 216/1000 | Loss: 0.00000910
Iteration 217/1000 | Loss: 0.00000910
Iteration 218/1000 | Loss: 0.00000910
Iteration 219/1000 | Loss: 0.00000910
Iteration 220/1000 | Loss: 0.00000910
Iteration 221/1000 | Loss: 0.00000909
Iteration 222/1000 | Loss: 0.00000909
Iteration 223/1000 | Loss: 0.00000909
Iteration 224/1000 | Loss: 0.00000909
Iteration 225/1000 | Loss: 0.00000909
Iteration 226/1000 | Loss: 0.00000909
Iteration 227/1000 | Loss: 0.00000909
Iteration 228/1000 | Loss: 0.00000909
Iteration 229/1000 | Loss: 0.00000909
Iteration 230/1000 | Loss: 0.00000909
Iteration 231/1000 | Loss: 0.00000909
Iteration 232/1000 | Loss: 0.00000908
Iteration 233/1000 | Loss: 0.00000908
Iteration 234/1000 | Loss: 0.00000908
Iteration 235/1000 | Loss: 0.00000908
Iteration 236/1000 | Loss: 0.00000908
Iteration 237/1000 | Loss: 0.00000908
Iteration 238/1000 | Loss: 0.00000908
Iteration 239/1000 | Loss: 0.00000908
Iteration 240/1000 | Loss: 0.00000908
Iteration 241/1000 | Loss: 0.00000908
Iteration 242/1000 | Loss: 0.00000908
Iteration 243/1000 | Loss: 0.00000908
Iteration 244/1000 | Loss: 0.00000908
Iteration 245/1000 | Loss: 0.00000908
Iteration 246/1000 | Loss: 0.00000908
Iteration 247/1000 | Loss: 0.00000908
Iteration 248/1000 | Loss: 0.00000907
Iteration 249/1000 | Loss: 0.00000907
Iteration 250/1000 | Loss: 0.00000907
Iteration 251/1000 | Loss: 0.00000907
Iteration 252/1000 | Loss: 0.00000907
Iteration 253/1000 | Loss: 0.00000907
Iteration 254/1000 | Loss: 0.00000907
Iteration 255/1000 | Loss: 0.00000907
Iteration 256/1000 | Loss: 0.00000907
Iteration 257/1000 | Loss: 0.00000907
Iteration 258/1000 | Loss: 0.00000907
Iteration 259/1000 | Loss: 0.00000907
Iteration 260/1000 | Loss: 0.00000907
Iteration 261/1000 | Loss: 0.00000907
Iteration 262/1000 | Loss: 0.00000907
Iteration 263/1000 | Loss: 0.00000907
Iteration 264/1000 | Loss: 0.00000907
Iteration 265/1000 | Loss: 0.00000907
Iteration 266/1000 | Loss: 0.00000907
Iteration 267/1000 | Loss: 0.00000907
Iteration 268/1000 | Loss: 0.00000907
Iteration 269/1000 | Loss: 0.00000907
Iteration 270/1000 | Loss: 0.00000907
Iteration 271/1000 | Loss: 0.00000907
Iteration 272/1000 | Loss: 0.00000907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [9.065077392733656e-06, 9.065077392733656e-06, 9.065077392733656e-06, 9.065077392733656e-06, 9.065077392733656e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.065077392733656e-06

Optimization complete. Final v2v error: 2.598179578781128 mm

Highest mean error: 2.827927827835083 mm for frame 119

Lowest mean error: 2.5036582946777344 mm for frame 182

Saving results

Total time: 45.259960651397705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428011
Iteration 2/25 | Loss: 0.00137648
Iteration 3/25 | Loss: 0.00116124
Iteration 4/25 | Loss: 0.00114537
Iteration 5/25 | Loss: 0.00114331
Iteration 6/25 | Loss: 0.00114275
Iteration 7/25 | Loss: 0.00114275
Iteration 8/25 | Loss: 0.00114275
Iteration 9/25 | Loss: 0.00114275
Iteration 10/25 | Loss: 0.00114275
Iteration 11/25 | Loss: 0.00114275
Iteration 12/25 | Loss: 0.00114275
Iteration 13/25 | Loss: 0.00114275
Iteration 14/25 | Loss: 0.00114275
Iteration 15/25 | Loss: 0.00114275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011427499121055007, 0.0011427499121055007, 0.0011427499121055007, 0.0011427499121055007, 0.0011427499121055007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011427499121055007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81963444
Iteration 2/25 | Loss: 0.00074223
Iteration 3/25 | Loss: 0.00074218
Iteration 4/25 | Loss: 0.00074218
Iteration 5/25 | Loss: 0.00074218
Iteration 6/25 | Loss: 0.00074218
Iteration 7/25 | Loss: 0.00074218
Iteration 8/25 | Loss: 0.00074218
Iteration 9/25 | Loss: 0.00074218
Iteration 10/25 | Loss: 0.00074218
Iteration 11/25 | Loss: 0.00074218
Iteration 12/25 | Loss: 0.00074218
Iteration 13/25 | Loss: 0.00074218
Iteration 14/25 | Loss: 0.00074218
Iteration 15/25 | Loss: 0.00074218
Iteration 16/25 | Loss: 0.00074218
Iteration 17/25 | Loss: 0.00074218
Iteration 18/25 | Loss: 0.00074218
Iteration 19/25 | Loss: 0.00074218
Iteration 20/25 | Loss: 0.00074218
Iteration 21/25 | Loss: 0.00074218
Iteration 22/25 | Loss: 0.00074218
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007421810296364129, 0.0007421810296364129, 0.0007421810296364129, 0.0007421810296364129, 0.0007421810296364129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007421810296364129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074218
Iteration 2/1000 | Loss: 0.00002773
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001505
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001320
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00001254
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001252
Iteration 78/1000 | Loss: 0.00001252
Iteration 79/1000 | Loss: 0.00001252
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001251
Iteration 83/1000 | Loss: 0.00001251
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001249
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001248
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001245
Iteration 120/1000 | Loss: 0.00001245
Iteration 121/1000 | Loss: 0.00001245
Iteration 122/1000 | Loss: 0.00001245
Iteration 123/1000 | Loss: 0.00001245
Iteration 124/1000 | Loss: 0.00001245
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001243
Iteration 133/1000 | Loss: 0.00001243
Iteration 134/1000 | Loss: 0.00001243
Iteration 135/1000 | Loss: 0.00001243
Iteration 136/1000 | Loss: 0.00001243
Iteration 137/1000 | Loss: 0.00001243
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001242
Iteration 140/1000 | Loss: 0.00001242
Iteration 141/1000 | Loss: 0.00001242
Iteration 142/1000 | Loss: 0.00001242
Iteration 143/1000 | Loss: 0.00001242
Iteration 144/1000 | Loss: 0.00001242
Iteration 145/1000 | Loss: 0.00001242
Iteration 146/1000 | Loss: 0.00001242
Iteration 147/1000 | Loss: 0.00001242
Iteration 148/1000 | Loss: 0.00001242
Iteration 149/1000 | Loss: 0.00001242
Iteration 150/1000 | Loss: 0.00001242
Iteration 151/1000 | Loss: 0.00001241
Iteration 152/1000 | Loss: 0.00001241
Iteration 153/1000 | Loss: 0.00001241
Iteration 154/1000 | Loss: 0.00001241
Iteration 155/1000 | Loss: 0.00001241
Iteration 156/1000 | Loss: 0.00001241
Iteration 157/1000 | Loss: 0.00001241
Iteration 158/1000 | Loss: 0.00001241
Iteration 159/1000 | Loss: 0.00001241
Iteration 160/1000 | Loss: 0.00001241
Iteration 161/1000 | Loss: 0.00001241
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001240
Iteration 165/1000 | Loss: 0.00001240
Iteration 166/1000 | Loss: 0.00001240
Iteration 167/1000 | Loss: 0.00001240
Iteration 168/1000 | Loss: 0.00001240
Iteration 169/1000 | Loss: 0.00001240
Iteration 170/1000 | Loss: 0.00001240
Iteration 171/1000 | Loss: 0.00001240
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001239
Iteration 180/1000 | Loss: 0.00001239
Iteration 181/1000 | Loss: 0.00001239
Iteration 182/1000 | Loss: 0.00001239
Iteration 183/1000 | Loss: 0.00001239
Iteration 184/1000 | Loss: 0.00001239
Iteration 185/1000 | Loss: 0.00001238
Iteration 186/1000 | Loss: 0.00001238
Iteration 187/1000 | Loss: 0.00001238
Iteration 188/1000 | Loss: 0.00001238
Iteration 189/1000 | Loss: 0.00001238
Iteration 190/1000 | Loss: 0.00001238
Iteration 191/1000 | Loss: 0.00001238
Iteration 192/1000 | Loss: 0.00001238
Iteration 193/1000 | Loss: 0.00001238
Iteration 194/1000 | Loss: 0.00001238
Iteration 195/1000 | Loss: 0.00001238
Iteration 196/1000 | Loss: 0.00001238
Iteration 197/1000 | Loss: 0.00001238
Iteration 198/1000 | Loss: 0.00001238
Iteration 199/1000 | Loss: 0.00001238
Iteration 200/1000 | Loss: 0.00001238
Iteration 201/1000 | Loss: 0.00001238
Iteration 202/1000 | Loss: 0.00001238
Iteration 203/1000 | Loss: 0.00001238
Iteration 204/1000 | Loss: 0.00001238
Iteration 205/1000 | Loss: 0.00001238
Iteration 206/1000 | Loss: 0.00001238
Iteration 207/1000 | Loss: 0.00001238
Iteration 208/1000 | Loss: 0.00001238
Iteration 209/1000 | Loss: 0.00001238
Iteration 210/1000 | Loss: 0.00001238
Iteration 211/1000 | Loss: 0.00001238
Iteration 212/1000 | Loss: 0.00001238
Iteration 213/1000 | Loss: 0.00001238
Iteration 214/1000 | Loss: 0.00001238
Iteration 215/1000 | Loss: 0.00001238
Iteration 216/1000 | Loss: 0.00001238
Iteration 217/1000 | Loss: 0.00001238
Iteration 218/1000 | Loss: 0.00001238
Iteration 219/1000 | Loss: 0.00001238
Iteration 220/1000 | Loss: 0.00001238
Iteration 221/1000 | Loss: 0.00001238
Iteration 222/1000 | Loss: 0.00001238
Iteration 223/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.2375335245451424e-05, 1.2375335245451424e-05, 1.2375335245451424e-05, 1.2375335245451424e-05, 1.2375335245451424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2375335245451424e-05

Optimization complete. Final v2v error: 2.9770753383636475 mm

Highest mean error: 3.5712907314300537 mm for frame 86

Lowest mean error: 2.647268056869507 mm for frame 17

Saving results

Total time: 40.85663294792175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691154
Iteration 2/25 | Loss: 0.00137447
Iteration 3/25 | Loss: 0.00120233
Iteration 4/25 | Loss: 0.00115885
Iteration 5/25 | Loss: 0.00114317
Iteration 6/25 | Loss: 0.00113794
Iteration 7/25 | Loss: 0.00113618
Iteration 8/25 | Loss: 0.00113623
Iteration 9/25 | Loss: 0.00113484
Iteration 10/25 | Loss: 0.00113431
Iteration 11/25 | Loss: 0.00113575
Iteration 12/25 | Loss: 0.00113219
Iteration 13/25 | Loss: 0.00112717
Iteration 14/25 | Loss: 0.00112658
Iteration 15/25 | Loss: 0.00113320
Iteration 16/25 | Loss: 0.00111416
Iteration 17/25 | Loss: 0.00110987
Iteration 18/25 | Loss: 0.00110932
Iteration 19/25 | Loss: 0.00110920
Iteration 20/25 | Loss: 0.00110919
Iteration 21/25 | Loss: 0.00110919
Iteration 22/25 | Loss: 0.00110919
Iteration 23/25 | Loss: 0.00110919
Iteration 24/25 | Loss: 0.00110919
Iteration 25/25 | Loss: 0.00110916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33877337
Iteration 2/25 | Loss: 0.00146167
Iteration 3/25 | Loss: 0.00146166
Iteration 4/25 | Loss: 0.00146166
Iteration 5/25 | Loss: 0.00146166
Iteration 6/25 | Loss: 0.00146166
Iteration 7/25 | Loss: 0.00146166
Iteration 8/25 | Loss: 0.00146166
Iteration 9/25 | Loss: 0.00146166
Iteration 10/25 | Loss: 0.00146166
Iteration 11/25 | Loss: 0.00146166
Iteration 12/25 | Loss: 0.00146166
Iteration 13/25 | Loss: 0.00146166
Iteration 14/25 | Loss: 0.00146166
Iteration 15/25 | Loss: 0.00146166
Iteration 16/25 | Loss: 0.00146166
Iteration 17/25 | Loss: 0.00146166
Iteration 18/25 | Loss: 0.00146166
Iteration 19/25 | Loss: 0.00146166
Iteration 20/25 | Loss: 0.00146166
Iteration 21/25 | Loss: 0.00146166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014616625849157572, 0.0014616625849157572, 0.0014616625849157572, 0.0014616625849157572, 0.0014616625849157572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014616625849157572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146166
Iteration 2/1000 | Loss: 0.00104875
Iteration 3/1000 | Loss: 0.00048884
Iteration 4/1000 | Loss: 0.00006025
Iteration 5/1000 | Loss: 0.00051398
Iteration 6/1000 | Loss: 0.00003956
Iteration 7/1000 | Loss: 0.00039051
Iteration 8/1000 | Loss: 0.00027971
Iteration 9/1000 | Loss: 0.00003014
Iteration 10/1000 | Loss: 0.00038327
Iteration 11/1000 | Loss: 0.00034328
Iteration 12/1000 | Loss: 0.00037551
Iteration 13/1000 | Loss: 0.00036463
Iteration 14/1000 | Loss: 0.00025519
Iteration 15/1000 | Loss: 0.00025390
Iteration 16/1000 | Loss: 0.00025906
Iteration 17/1000 | Loss: 0.00025912
Iteration 18/1000 | Loss: 0.00033120
Iteration 19/1000 | Loss: 0.00022932
Iteration 20/1000 | Loss: 0.00020426
Iteration 21/1000 | Loss: 0.00011850
Iteration 22/1000 | Loss: 0.00053143
Iteration 23/1000 | Loss: 0.00026916
Iteration 24/1000 | Loss: 0.00002898
Iteration 25/1000 | Loss: 0.00002605
Iteration 26/1000 | Loss: 0.00002390
Iteration 27/1000 | Loss: 0.00002305
Iteration 28/1000 | Loss: 0.00002229
Iteration 29/1000 | Loss: 0.00002159
Iteration 30/1000 | Loss: 0.00002121
Iteration 31/1000 | Loss: 0.00002082
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001986
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00057072
Iteration 39/1000 | Loss: 0.00012255
Iteration 40/1000 | Loss: 0.00002404
Iteration 41/1000 | Loss: 0.00060621
Iteration 42/1000 | Loss: 0.00040009
Iteration 43/1000 | Loss: 0.00002141
Iteration 44/1000 | Loss: 0.00002052
Iteration 45/1000 | Loss: 0.00001997
Iteration 46/1000 | Loss: 0.00001971
Iteration 47/1000 | Loss: 0.00001953
Iteration 48/1000 | Loss: 0.00042528
Iteration 49/1000 | Loss: 0.00007190
Iteration 50/1000 | Loss: 0.00002017
Iteration 51/1000 | Loss: 0.00001964
Iteration 52/1000 | Loss: 0.00001945
Iteration 53/1000 | Loss: 0.00001945
Iteration 54/1000 | Loss: 0.00001945
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001942
Iteration 57/1000 | Loss: 0.00040790
Iteration 58/1000 | Loss: 0.00016290
Iteration 59/1000 | Loss: 0.00024844
Iteration 60/1000 | Loss: 0.00002560
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002024
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00001935
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001917
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001916
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001913
Iteration 73/1000 | Loss: 0.00001913
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001911
Iteration 77/1000 | Loss: 0.00001911
Iteration 78/1000 | Loss: 0.00001911
Iteration 79/1000 | Loss: 0.00001911
Iteration 80/1000 | Loss: 0.00001911
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001910
Iteration 84/1000 | Loss: 0.00001910
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001909
Iteration 87/1000 | Loss: 0.00001909
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001904
Iteration 112/1000 | Loss: 0.00001904
Iteration 113/1000 | Loss: 0.00001904
Iteration 114/1000 | Loss: 0.00001904
Iteration 115/1000 | Loss: 0.00001904
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001903
Iteration 118/1000 | Loss: 0.00001903
Iteration 119/1000 | Loss: 0.00001903
Iteration 120/1000 | Loss: 0.00001903
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001903
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001902
Iteration 130/1000 | Loss: 0.00001902
Iteration 131/1000 | Loss: 0.00001902
Iteration 132/1000 | Loss: 0.00001902
Iteration 133/1000 | Loss: 0.00001901
Iteration 134/1000 | Loss: 0.00001901
Iteration 135/1000 | Loss: 0.00001901
Iteration 136/1000 | Loss: 0.00029530
Iteration 137/1000 | Loss: 0.00005499
Iteration 138/1000 | Loss: 0.00027601
Iteration 139/1000 | Loss: 0.00025804
Iteration 140/1000 | Loss: 0.00028395
Iteration 141/1000 | Loss: 0.00040327
Iteration 142/1000 | Loss: 0.00027351
Iteration 143/1000 | Loss: 0.00038945
Iteration 144/1000 | Loss: 0.00005793
Iteration 145/1000 | Loss: 0.00040513
Iteration 146/1000 | Loss: 0.00009523
Iteration 147/1000 | Loss: 0.00056214
Iteration 148/1000 | Loss: 0.00010146
Iteration 149/1000 | Loss: 0.00045709
Iteration 150/1000 | Loss: 0.00003761
Iteration 151/1000 | Loss: 0.00002587
Iteration 152/1000 | Loss: 0.00002263
Iteration 153/1000 | Loss: 0.00002028
Iteration 154/1000 | Loss: 0.00001870
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001708
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001678
Iteration 159/1000 | Loss: 0.00001673
Iteration 160/1000 | Loss: 0.00001668
Iteration 161/1000 | Loss: 0.00001667
Iteration 162/1000 | Loss: 0.00001665
Iteration 163/1000 | Loss: 0.00001661
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001657
Iteration 166/1000 | Loss: 0.00001657
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001657
Iteration 169/1000 | Loss: 0.00001657
Iteration 170/1000 | Loss: 0.00001657
Iteration 171/1000 | Loss: 0.00001656
Iteration 172/1000 | Loss: 0.00001656
Iteration 173/1000 | Loss: 0.00001656
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001654
Iteration 178/1000 | Loss: 0.00001654
Iteration 179/1000 | Loss: 0.00001654
Iteration 180/1000 | Loss: 0.00001653
Iteration 181/1000 | Loss: 0.00001653
Iteration 182/1000 | Loss: 0.00001653
Iteration 183/1000 | Loss: 0.00001653
Iteration 184/1000 | Loss: 0.00001652
Iteration 185/1000 | Loss: 0.00001652
Iteration 186/1000 | Loss: 0.00001652
Iteration 187/1000 | Loss: 0.00001652
Iteration 188/1000 | Loss: 0.00001652
Iteration 189/1000 | Loss: 0.00001652
Iteration 190/1000 | Loss: 0.00001652
Iteration 191/1000 | Loss: 0.00001652
Iteration 192/1000 | Loss: 0.00001652
Iteration 193/1000 | Loss: 0.00001652
Iteration 194/1000 | Loss: 0.00001652
Iteration 195/1000 | Loss: 0.00001652
Iteration 196/1000 | Loss: 0.00001652
Iteration 197/1000 | Loss: 0.00001652
Iteration 198/1000 | Loss: 0.00001652
Iteration 199/1000 | Loss: 0.00001652
Iteration 200/1000 | Loss: 0.00001652
Iteration 201/1000 | Loss: 0.00001652
Iteration 202/1000 | Loss: 0.00001652
Iteration 203/1000 | Loss: 0.00001652
Iteration 204/1000 | Loss: 0.00001652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.6520803910680115e-05, 1.6520803910680115e-05, 1.6520803910680115e-05, 1.6520803910680115e-05, 1.6520803910680115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6520803910680115e-05

Optimization complete. Final v2v error: 3.4118525981903076 mm

Highest mean error: 5.712385654449463 mm for frame 137

Lowest mean error: 2.863494873046875 mm for frame 199

Saving results

Total time: 180.2677345275879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966305
Iteration 2/25 | Loss: 0.00151039
Iteration 3/25 | Loss: 0.00124733
Iteration 4/25 | Loss: 0.00118362
Iteration 5/25 | Loss: 0.00115690
Iteration 6/25 | Loss: 0.00115691
Iteration 7/25 | Loss: 0.00113678
Iteration 8/25 | Loss: 0.00112688
Iteration 9/25 | Loss: 0.00112229
Iteration 10/25 | Loss: 0.00112363
Iteration 11/25 | Loss: 0.00112039
Iteration 12/25 | Loss: 0.00111725
Iteration 13/25 | Loss: 0.00111751
Iteration 14/25 | Loss: 0.00111640
Iteration 15/25 | Loss: 0.00111549
Iteration 16/25 | Loss: 0.00111521
Iteration 17/25 | Loss: 0.00111515
Iteration 18/25 | Loss: 0.00111514
Iteration 19/25 | Loss: 0.00111514
Iteration 20/25 | Loss: 0.00111514
Iteration 21/25 | Loss: 0.00111514
Iteration 22/25 | Loss: 0.00111514
Iteration 23/25 | Loss: 0.00111514
Iteration 24/25 | Loss: 0.00111513
Iteration 25/25 | Loss: 0.00111513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21583438
Iteration 2/25 | Loss: 0.00078124
Iteration 3/25 | Loss: 0.00078123
Iteration 4/25 | Loss: 0.00078123
Iteration 5/25 | Loss: 0.00078123
Iteration 6/25 | Loss: 0.00078123
Iteration 7/25 | Loss: 0.00078123
Iteration 8/25 | Loss: 0.00078123
Iteration 9/25 | Loss: 0.00078123
Iteration 10/25 | Loss: 0.00078123
Iteration 11/25 | Loss: 0.00078122
Iteration 12/25 | Loss: 0.00078122
Iteration 13/25 | Loss: 0.00078122
Iteration 14/25 | Loss: 0.00078122
Iteration 15/25 | Loss: 0.00078122
Iteration 16/25 | Loss: 0.00078122
Iteration 17/25 | Loss: 0.00078122
Iteration 18/25 | Loss: 0.00078122
Iteration 19/25 | Loss: 0.00078122
Iteration 20/25 | Loss: 0.00078122
Iteration 21/25 | Loss: 0.00078122
Iteration 22/25 | Loss: 0.00078122
Iteration 23/25 | Loss: 0.00078122
Iteration 24/25 | Loss: 0.00078122
Iteration 25/25 | Loss: 0.00078122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078122
Iteration 2/1000 | Loss: 0.00003203
Iteration 3/1000 | Loss: 0.00002086
Iteration 4/1000 | Loss: 0.00001841
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001636
Iteration 8/1000 | Loss: 0.00001606
Iteration 9/1000 | Loss: 0.00001581
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001524
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001492
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001490
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001489
Iteration 41/1000 | Loss: 0.00001489
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001487
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001486
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001485
Iteration 55/1000 | Loss: 0.00001484
Iteration 56/1000 | Loss: 0.00001484
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001483
Iteration 59/1000 | Loss: 0.00001483
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.4787800864723977e-05, 1.4787800864723977e-05, 1.4787800864723977e-05, 1.4787800864723977e-05, 1.4787800864723977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4787800864723977e-05

Optimization complete. Final v2v error: 3.2970359325408936 mm

Highest mean error: 3.8904595375061035 mm for frame 0

Lowest mean error: 2.683938503265381 mm for frame 61

Saving results

Total time: 62.83902406692505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453430
Iteration 2/25 | Loss: 0.00127289
Iteration 3/25 | Loss: 0.00115527
Iteration 4/25 | Loss: 0.00113993
Iteration 5/25 | Loss: 0.00113590
Iteration 6/25 | Loss: 0.00113557
Iteration 7/25 | Loss: 0.00113557
Iteration 8/25 | Loss: 0.00113557
Iteration 9/25 | Loss: 0.00113557
Iteration 10/25 | Loss: 0.00113557
Iteration 11/25 | Loss: 0.00113557
Iteration 12/25 | Loss: 0.00113557
Iteration 13/25 | Loss: 0.00113557
Iteration 14/25 | Loss: 0.00113557
Iteration 15/25 | Loss: 0.00113557
Iteration 16/25 | Loss: 0.00113557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011355747701600194, 0.0011355747701600194, 0.0011355747701600194, 0.0011355747701600194, 0.0011355747701600194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011355747701600194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83346748
Iteration 2/25 | Loss: 0.00075862
Iteration 3/25 | Loss: 0.00075862
Iteration 4/25 | Loss: 0.00075862
Iteration 5/25 | Loss: 0.00075862
Iteration 6/25 | Loss: 0.00075862
Iteration 7/25 | Loss: 0.00075862
Iteration 8/25 | Loss: 0.00075862
Iteration 9/25 | Loss: 0.00075862
Iteration 10/25 | Loss: 0.00075862
Iteration 11/25 | Loss: 0.00075862
Iteration 12/25 | Loss: 0.00075862
Iteration 13/25 | Loss: 0.00075862
Iteration 14/25 | Loss: 0.00075862
Iteration 15/25 | Loss: 0.00075862
Iteration 16/25 | Loss: 0.00075862
Iteration 17/25 | Loss: 0.00075862
Iteration 18/25 | Loss: 0.00075862
Iteration 19/25 | Loss: 0.00075862
Iteration 20/25 | Loss: 0.00075862
Iteration 21/25 | Loss: 0.00075862
Iteration 22/25 | Loss: 0.00075862
Iteration 23/25 | Loss: 0.00075862
Iteration 24/25 | Loss: 0.00075862
Iteration 25/25 | Loss: 0.00075862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075862
Iteration 2/1000 | Loss: 0.00002693
Iteration 3/1000 | Loss: 0.00001937
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001628
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001485
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001356
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001350
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001336
Iteration 101/1000 | Loss: 0.00001336
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001331
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001330
Iteration 126/1000 | Loss: 0.00001330
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001330
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001330
Iteration 141/1000 | Loss: 0.00001330
Iteration 142/1000 | Loss: 0.00001330
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.329611950495746e-05, 1.329611950495746e-05, 1.329611950495746e-05, 1.329611950495746e-05, 1.329611950495746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.329611950495746e-05

Optimization complete. Final v2v error: 3.0923678874969482 mm

Highest mean error: 3.424316167831421 mm for frame 243

Lowest mean error: 2.9477789402008057 mm for frame 122

Saving results

Total time: 44.82802414894104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405457
Iteration 2/25 | Loss: 0.00122242
Iteration 3/25 | Loss: 0.00110473
Iteration 4/25 | Loss: 0.00109867
Iteration 5/25 | Loss: 0.00109656
Iteration 6/25 | Loss: 0.00109656
Iteration 7/25 | Loss: 0.00109656
Iteration 8/25 | Loss: 0.00109656
Iteration 9/25 | Loss: 0.00109656
Iteration 10/25 | Loss: 0.00109656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010965595720335841, 0.0010965595720335841, 0.0010965595720335841, 0.0010965595720335841, 0.0010965595720335841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010965595720335841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62897050
Iteration 2/25 | Loss: 0.00070129
Iteration 3/25 | Loss: 0.00070129
Iteration 4/25 | Loss: 0.00070129
Iteration 5/25 | Loss: 0.00070129
Iteration 6/25 | Loss: 0.00070129
Iteration 7/25 | Loss: 0.00070129
Iteration 8/25 | Loss: 0.00070129
Iteration 9/25 | Loss: 0.00070129
Iteration 10/25 | Loss: 0.00070129
Iteration 11/25 | Loss: 0.00070129
Iteration 12/25 | Loss: 0.00070129
Iteration 13/25 | Loss: 0.00070129
Iteration 14/25 | Loss: 0.00070129
Iteration 15/25 | Loss: 0.00070129
Iteration 16/25 | Loss: 0.00070129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007012862479314208, 0.0007012862479314208, 0.0007012862479314208, 0.0007012862479314208, 0.0007012862479314208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007012862479314208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070129
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00001590
Iteration 4/1000 | Loss: 0.00001252
Iteration 5/1000 | Loss: 0.00001144
Iteration 6/1000 | Loss: 0.00001090
Iteration 7/1000 | Loss: 0.00001051
Iteration 8/1000 | Loss: 0.00001028
Iteration 9/1000 | Loss: 0.00000996
Iteration 10/1000 | Loss: 0.00000968
Iteration 11/1000 | Loss: 0.00000956
Iteration 12/1000 | Loss: 0.00000950
Iteration 13/1000 | Loss: 0.00000949
Iteration 14/1000 | Loss: 0.00000946
Iteration 15/1000 | Loss: 0.00000945
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000934
Iteration 18/1000 | Loss: 0.00000930
Iteration 19/1000 | Loss: 0.00000929
Iteration 20/1000 | Loss: 0.00000920
Iteration 21/1000 | Loss: 0.00000920
Iteration 22/1000 | Loss: 0.00000919
Iteration 23/1000 | Loss: 0.00000917
Iteration 24/1000 | Loss: 0.00000916
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000915
Iteration 27/1000 | Loss: 0.00000915
Iteration 28/1000 | Loss: 0.00000914
Iteration 29/1000 | Loss: 0.00000914
Iteration 30/1000 | Loss: 0.00000914
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000913
Iteration 33/1000 | Loss: 0.00000913
Iteration 34/1000 | Loss: 0.00000912
Iteration 35/1000 | Loss: 0.00000912
Iteration 36/1000 | Loss: 0.00000911
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000911
Iteration 39/1000 | Loss: 0.00000910
Iteration 40/1000 | Loss: 0.00000910
Iteration 41/1000 | Loss: 0.00000910
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000908
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000906
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000905
Iteration 48/1000 | Loss: 0.00000905
Iteration 49/1000 | Loss: 0.00000905
Iteration 50/1000 | Loss: 0.00000905
Iteration 51/1000 | Loss: 0.00000904
Iteration 52/1000 | Loss: 0.00000904
Iteration 53/1000 | Loss: 0.00000904
Iteration 54/1000 | Loss: 0.00000904
Iteration 55/1000 | Loss: 0.00000904
Iteration 56/1000 | Loss: 0.00000904
Iteration 57/1000 | Loss: 0.00000903
Iteration 58/1000 | Loss: 0.00000903
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000903
Iteration 61/1000 | Loss: 0.00000903
Iteration 62/1000 | Loss: 0.00000903
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000903
Iteration 66/1000 | Loss: 0.00000902
Iteration 67/1000 | Loss: 0.00000902
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000902
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000901
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000901
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000900
Iteration 78/1000 | Loss: 0.00000900
Iteration 79/1000 | Loss: 0.00000900
Iteration 80/1000 | Loss: 0.00000900
Iteration 81/1000 | Loss: 0.00000900
Iteration 82/1000 | Loss: 0.00000900
Iteration 83/1000 | Loss: 0.00000899
Iteration 84/1000 | Loss: 0.00000899
Iteration 85/1000 | Loss: 0.00000899
Iteration 86/1000 | Loss: 0.00000899
Iteration 87/1000 | Loss: 0.00000899
Iteration 88/1000 | Loss: 0.00000899
Iteration 89/1000 | Loss: 0.00000899
Iteration 90/1000 | Loss: 0.00000899
Iteration 91/1000 | Loss: 0.00000899
Iteration 92/1000 | Loss: 0.00000899
Iteration 93/1000 | Loss: 0.00000898
Iteration 94/1000 | Loss: 0.00000898
Iteration 95/1000 | Loss: 0.00000897
Iteration 96/1000 | Loss: 0.00000897
Iteration 97/1000 | Loss: 0.00000897
Iteration 98/1000 | Loss: 0.00000897
Iteration 99/1000 | Loss: 0.00000896
Iteration 100/1000 | Loss: 0.00000896
Iteration 101/1000 | Loss: 0.00000896
Iteration 102/1000 | Loss: 0.00000896
Iteration 103/1000 | Loss: 0.00000896
Iteration 104/1000 | Loss: 0.00000896
Iteration 105/1000 | Loss: 0.00000896
Iteration 106/1000 | Loss: 0.00000896
Iteration 107/1000 | Loss: 0.00000896
Iteration 108/1000 | Loss: 0.00000896
Iteration 109/1000 | Loss: 0.00000896
Iteration 110/1000 | Loss: 0.00000896
Iteration 111/1000 | Loss: 0.00000896
Iteration 112/1000 | Loss: 0.00000895
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000894
Iteration 117/1000 | Loss: 0.00000894
Iteration 118/1000 | Loss: 0.00000894
Iteration 119/1000 | Loss: 0.00000894
Iteration 120/1000 | Loss: 0.00000894
Iteration 121/1000 | Loss: 0.00000894
Iteration 122/1000 | Loss: 0.00000894
Iteration 123/1000 | Loss: 0.00000894
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000893
Iteration 126/1000 | Loss: 0.00000893
Iteration 127/1000 | Loss: 0.00000893
Iteration 128/1000 | Loss: 0.00000893
Iteration 129/1000 | Loss: 0.00000893
Iteration 130/1000 | Loss: 0.00000892
Iteration 131/1000 | Loss: 0.00000892
Iteration 132/1000 | Loss: 0.00000892
Iteration 133/1000 | Loss: 0.00000892
Iteration 134/1000 | Loss: 0.00000892
Iteration 135/1000 | Loss: 0.00000892
Iteration 136/1000 | Loss: 0.00000892
Iteration 137/1000 | Loss: 0.00000892
Iteration 138/1000 | Loss: 0.00000891
Iteration 139/1000 | Loss: 0.00000891
Iteration 140/1000 | Loss: 0.00000891
Iteration 141/1000 | Loss: 0.00000891
Iteration 142/1000 | Loss: 0.00000891
Iteration 143/1000 | Loss: 0.00000891
Iteration 144/1000 | Loss: 0.00000891
Iteration 145/1000 | Loss: 0.00000891
Iteration 146/1000 | Loss: 0.00000890
Iteration 147/1000 | Loss: 0.00000890
Iteration 148/1000 | Loss: 0.00000890
Iteration 149/1000 | Loss: 0.00000890
Iteration 150/1000 | Loss: 0.00000890
Iteration 151/1000 | Loss: 0.00000890
Iteration 152/1000 | Loss: 0.00000890
Iteration 153/1000 | Loss: 0.00000890
Iteration 154/1000 | Loss: 0.00000890
Iteration 155/1000 | Loss: 0.00000890
Iteration 156/1000 | Loss: 0.00000890
Iteration 157/1000 | Loss: 0.00000890
Iteration 158/1000 | Loss: 0.00000890
Iteration 159/1000 | Loss: 0.00000890
Iteration 160/1000 | Loss: 0.00000889
Iteration 161/1000 | Loss: 0.00000889
Iteration 162/1000 | Loss: 0.00000889
Iteration 163/1000 | Loss: 0.00000889
Iteration 164/1000 | Loss: 0.00000889
Iteration 165/1000 | Loss: 0.00000889
Iteration 166/1000 | Loss: 0.00000889
Iteration 167/1000 | Loss: 0.00000889
Iteration 168/1000 | Loss: 0.00000888
Iteration 169/1000 | Loss: 0.00000888
Iteration 170/1000 | Loss: 0.00000888
Iteration 171/1000 | Loss: 0.00000888
Iteration 172/1000 | Loss: 0.00000888
Iteration 173/1000 | Loss: 0.00000888
Iteration 174/1000 | Loss: 0.00000888
Iteration 175/1000 | Loss: 0.00000888
Iteration 176/1000 | Loss: 0.00000888
Iteration 177/1000 | Loss: 0.00000888
Iteration 178/1000 | Loss: 0.00000888
Iteration 179/1000 | Loss: 0.00000888
Iteration 180/1000 | Loss: 0.00000888
Iteration 181/1000 | Loss: 0.00000888
Iteration 182/1000 | Loss: 0.00000888
Iteration 183/1000 | Loss: 0.00000888
Iteration 184/1000 | Loss: 0.00000888
Iteration 185/1000 | Loss: 0.00000887
Iteration 186/1000 | Loss: 0.00000887
Iteration 187/1000 | Loss: 0.00000887
Iteration 188/1000 | Loss: 0.00000887
Iteration 189/1000 | Loss: 0.00000887
Iteration 190/1000 | Loss: 0.00000887
Iteration 191/1000 | Loss: 0.00000887
Iteration 192/1000 | Loss: 0.00000887
Iteration 193/1000 | Loss: 0.00000887
Iteration 194/1000 | Loss: 0.00000887
Iteration 195/1000 | Loss: 0.00000887
Iteration 196/1000 | Loss: 0.00000887
Iteration 197/1000 | Loss: 0.00000887
Iteration 198/1000 | Loss: 0.00000887
Iteration 199/1000 | Loss: 0.00000887
Iteration 200/1000 | Loss: 0.00000887
Iteration 201/1000 | Loss: 0.00000887
Iteration 202/1000 | Loss: 0.00000887
Iteration 203/1000 | Loss: 0.00000887
Iteration 204/1000 | Loss: 0.00000887
Iteration 205/1000 | Loss: 0.00000887
Iteration 206/1000 | Loss: 0.00000887
Iteration 207/1000 | Loss: 0.00000887
Iteration 208/1000 | Loss: 0.00000887
Iteration 209/1000 | Loss: 0.00000887
Iteration 210/1000 | Loss: 0.00000887
Iteration 211/1000 | Loss: 0.00000887
Iteration 212/1000 | Loss: 0.00000887
Iteration 213/1000 | Loss: 0.00000887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [8.874730156094301e-06, 8.874730156094301e-06, 8.874730156094301e-06, 8.874730156094301e-06, 8.874730156094301e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.874730156094301e-06

Optimization complete. Final v2v error: 2.581940174102783 mm

Highest mean error: 2.7830684185028076 mm for frame 89

Lowest mean error: 2.4377880096435547 mm for frame 76

Saving results

Total time: 45.30937075614929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01208136
Iteration 2/25 | Loss: 0.00360599
Iteration 3/25 | Loss: 0.00283161
Iteration 4/25 | Loss: 0.00238210
Iteration 5/25 | Loss: 0.00215399
Iteration 6/25 | Loss: 0.00188197
Iteration 7/25 | Loss: 0.00179297
Iteration 8/25 | Loss: 0.00176981
Iteration 9/25 | Loss: 0.00174353
Iteration 10/25 | Loss: 0.00175969
Iteration 11/25 | Loss: 0.00169734
Iteration 12/25 | Loss: 0.00169501
Iteration 13/25 | Loss: 0.00169736
Iteration 14/25 | Loss: 0.00168613
Iteration 15/25 | Loss: 0.00167812
Iteration 16/25 | Loss: 0.00169352
Iteration 17/25 | Loss: 0.00168578
Iteration 18/25 | Loss: 0.00168530
Iteration 19/25 | Loss: 0.00167240
Iteration 20/25 | Loss: 0.00166923
Iteration 21/25 | Loss: 0.00167041
Iteration 22/25 | Loss: 0.00167283
Iteration 23/25 | Loss: 0.00166505
Iteration 24/25 | Loss: 0.00166173
Iteration 25/25 | Loss: 0.00166115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58704060
Iteration 2/25 | Loss: 0.00385468
Iteration 3/25 | Loss: 0.00385468
Iteration 4/25 | Loss: 0.00385468
Iteration 5/25 | Loss: 0.00385468
Iteration 6/25 | Loss: 0.00385468
Iteration 7/25 | Loss: 0.00385468
Iteration 8/25 | Loss: 0.00385468
Iteration 9/25 | Loss: 0.00385468
Iteration 10/25 | Loss: 0.00385468
Iteration 11/25 | Loss: 0.00385468
Iteration 12/25 | Loss: 0.00385468
Iteration 13/25 | Loss: 0.00385468
Iteration 14/25 | Loss: 0.00385468
Iteration 15/25 | Loss: 0.00385468
Iteration 16/25 | Loss: 0.00385468
Iteration 17/25 | Loss: 0.00385468
Iteration 18/25 | Loss: 0.00385468
Iteration 19/25 | Loss: 0.00385468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003854677313938737, 0.003854677313938737, 0.003854677313938737, 0.003854677313938737, 0.003854677313938737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003854677313938737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385468
Iteration 2/1000 | Loss: 0.00180850
Iteration 3/1000 | Loss: 0.00079370
Iteration 4/1000 | Loss: 0.00234794
Iteration 5/1000 | Loss: 0.00101321
Iteration 6/1000 | Loss: 0.00085113
Iteration 7/1000 | Loss: 0.00047569
Iteration 8/1000 | Loss: 0.00181276
Iteration 9/1000 | Loss: 0.00078835
Iteration 10/1000 | Loss: 0.00101416
Iteration 11/1000 | Loss: 0.00125738
Iteration 12/1000 | Loss: 0.00177943
Iteration 13/1000 | Loss: 0.00080279
Iteration 14/1000 | Loss: 0.00251583
Iteration 15/1000 | Loss: 0.00176058
Iteration 16/1000 | Loss: 0.00079118
Iteration 17/1000 | Loss: 0.00102976
Iteration 18/1000 | Loss: 0.00063631
Iteration 19/1000 | Loss: 0.00093103
Iteration 20/1000 | Loss: 0.00105425
Iteration 21/1000 | Loss: 0.00125340
Iteration 22/1000 | Loss: 0.00061148
Iteration 23/1000 | Loss: 0.00036165
Iteration 24/1000 | Loss: 0.00105234
Iteration 25/1000 | Loss: 0.00059109
Iteration 26/1000 | Loss: 0.00129632
Iteration 27/1000 | Loss: 0.00050286
Iteration 28/1000 | Loss: 0.00050431
Iteration 29/1000 | Loss: 0.00036400
Iteration 30/1000 | Loss: 0.00217340
Iteration 31/1000 | Loss: 0.00105252
Iteration 32/1000 | Loss: 0.00131808
Iteration 33/1000 | Loss: 0.00068512
Iteration 34/1000 | Loss: 0.00054776
Iteration 35/1000 | Loss: 0.00065066
Iteration 36/1000 | Loss: 0.00085651
Iteration 37/1000 | Loss: 0.00112900
Iteration 38/1000 | Loss: 0.00121656
Iteration 39/1000 | Loss: 0.00081752
Iteration 40/1000 | Loss: 0.00091519
Iteration 41/1000 | Loss: 0.00069177
Iteration 42/1000 | Loss: 0.00088561
Iteration 43/1000 | Loss: 0.00084856
Iteration 44/1000 | Loss: 0.00075101
Iteration 45/1000 | Loss: 0.00105884
Iteration 46/1000 | Loss: 0.00217970
Iteration 47/1000 | Loss: 0.00055704
Iteration 48/1000 | Loss: 0.00072142
Iteration 49/1000 | Loss: 0.00220673
Iteration 50/1000 | Loss: 0.00093991
Iteration 51/1000 | Loss: 0.00043874
Iteration 52/1000 | Loss: 0.00059237
Iteration 53/1000 | Loss: 0.00037715
Iteration 54/1000 | Loss: 0.00123063
Iteration 55/1000 | Loss: 0.00047177
Iteration 56/1000 | Loss: 0.00066266
Iteration 57/1000 | Loss: 0.00168478
Iteration 58/1000 | Loss: 0.00052823
Iteration 59/1000 | Loss: 0.00028376
Iteration 60/1000 | Loss: 0.00038177
Iteration 61/1000 | Loss: 0.00046270
Iteration 62/1000 | Loss: 0.00046441
Iteration 63/1000 | Loss: 0.00070720
Iteration 64/1000 | Loss: 0.00020748
Iteration 65/1000 | Loss: 0.00041758
Iteration 66/1000 | Loss: 0.00143120
Iteration 67/1000 | Loss: 0.00027121
Iteration 68/1000 | Loss: 0.00052516
Iteration 69/1000 | Loss: 0.00054849
Iteration 70/1000 | Loss: 0.00056569
Iteration 71/1000 | Loss: 0.00036411
Iteration 72/1000 | Loss: 0.00066034
Iteration 73/1000 | Loss: 0.00037352
Iteration 74/1000 | Loss: 0.00035701
Iteration 75/1000 | Loss: 0.00048321
Iteration 76/1000 | Loss: 0.00031226
Iteration 77/1000 | Loss: 0.00053249
Iteration 78/1000 | Loss: 0.00037449
Iteration 79/1000 | Loss: 0.00019050
Iteration 80/1000 | Loss: 0.00037901
Iteration 81/1000 | Loss: 0.00031927
Iteration 82/1000 | Loss: 0.00032543
Iteration 83/1000 | Loss: 0.00033152
Iteration 84/1000 | Loss: 0.00032582
Iteration 85/1000 | Loss: 0.00020565
Iteration 86/1000 | Loss: 0.00031248
Iteration 87/1000 | Loss: 0.00034532
Iteration 88/1000 | Loss: 0.00036688
Iteration 89/1000 | Loss: 0.00036399
Iteration 90/1000 | Loss: 0.00036207
Iteration 91/1000 | Loss: 0.00092235
Iteration 92/1000 | Loss: 0.00041233
Iteration 93/1000 | Loss: 0.00034836
Iteration 94/1000 | Loss: 0.00030573
Iteration 95/1000 | Loss: 0.00028730
Iteration 96/1000 | Loss: 0.00026402
Iteration 97/1000 | Loss: 0.00030787
Iteration 98/1000 | Loss: 0.00031642
Iteration 99/1000 | Loss: 0.00029759
Iteration 100/1000 | Loss: 0.00035418
Iteration 101/1000 | Loss: 0.00018715
Iteration 102/1000 | Loss: 0.00037986
Iteration 103/1000 | Loss: 0.00036714
Iteration 104/1000 | Loss: 0.00031421
Iteration 105/1000 | Loss: 0.00033892
Iteration 106/1000 | Loss: 0.00032921
Iteration 107/1000 | Loss: 0.00042321
Iteration 108/1000 | Loss: 0.00038372
Iteration 109/1000 | Loss: 0.00049023
Iteration 110/1000 | Loss: 0.00044885
Iteration 111/1000 | Loss: 0.00034285
Iteration 112/1000 | Loss: 0.00035635
Iteration 113/1000 | Loss: 0.00033319
Iteration 114/1000 | Loss: 0.00028867
Iteration 115/1000 | Loss: 0.00054719
Iteration 116/1000 | Loss: 0.00036778
Iteration 117/1000 | Loss: 0.00048027
Iteration 118/1000 | Loss: 0.00044088
Iteration 119/1000 | Loss: 0.00046913
Iteration 120/1000 | Loss: 0.00024910
Iteration 121/1000 | Loss: 0.00045926
Iteration 122/1000 | Loss: 0.00045356
Iteration 123/1000 | Loss: 0.00043615
Iteration 124/1000 | Loss: 0.00049853
Iteration 125/1000 | Loss: 0.00043652
Iteration 126/1000 | Loss: 0.00042764
Iteration 127/1000 | Loss: 0.00044757
Iteration 128/1000 | Loss: 0.00050484
Iteration 129/1000 | Loss: 0.00035878
Iteration 130/1000 | Loss: 0.00052815
Iteration 131/1000 | Loss: 0.00042968
Iteration 132/1000 | Loss: 0.00035761
Iteration 133/1000 | Loss: 0.00036925
Iteration 134/1000 | Loss: 0.00040396
Iteration 135/1000 | Loss: 0.00052580
Iteration 136/1000 | Loss: 0.00134593
Iteration 137/1000 | Loss: 0.00037046
Iteration 138/1000 | Loss: 0.00056799
Iteration 139/1000 | Loss: 0.00026254
Iteration 140/1000 | Loss: 0.00027824
Iteration 141/1000 | Loss: 0.00024796
Iteration 142/1000 | Loss: 0.00024440
Iteration 143/1000 | Loss: 0.00024106
Iteration 144/1000 | Loss: 0.00031169
Iteration 145/1000 | Loss: 0.00028343
Iteration 146/1000 | Loss: 0.00030242
Iteration 147/1000 | Loss: 0.00026422
Iteration 148/1000 | Loss: 0.00020350
Iteration 149/1000 | Loss: 0.00017092
Iteration 150/1000 | Loss: 0.00015574
Iteration 151/1000 | Loss: 0.00016157
Iteration 152/1000 | Loss: 0.00015290
Iteration 153/1000 | Loss: 0.00013774
Iteration 154/1000 | Loss: 0.00015595
Iteration 155/1000 | Loss: 0.00015255
Iteration 156/1000 | Loss: 0.00014440
Iteration 157/1000 | Loss: 0.00015244
Iteration 158/1000 | Loss: 0.00014295
Iteration 159/1000 | Loss: 0.00016159
Iteration 160/1000 | Loss: 0.00015369
Iteration 161/1000 | Loss: 0.00012988
Iteration 162/1000 | Loss: 0.00012567
Iteration 163/1000 | Loss: 0.00015177
Iteration 164/1000 | Loss: 0.00014597
Iteration 165/1000 | Loss: 0.00014388
Iteration 166/1000 | Loss: 0.00015358
Iteration 167/1000 | Loss: 0.00012848
Iteration 168/1000 | Loss: 0.00012568
Iteration 169/1000 | Loss: 0.00015075
Iteration 170/1000 | Loss: 0.00010749
Iteration 171/1000 | Loss: 0.00011265
Iteration 172/1000 | Loss: 0.00011357
Iteration 173/1000 | Loss: 0.00010379
Iteration 174/1000 | Loss: 0.00010776
Iteration 175/1000 | Loss: 0.00104977
Iteration 176/1000 | Loss: 0.00012965
Iteration 177/1000 | Loss: 0.00014419
Iteration 178/1000 | Loss: 0.00012323
Iteration 179/1000 | Loss: 0.00011745
Iteration 180/1000 | Loss: 0.00012364
Iteration 181/1000 | Loss: 0.00012637
Iteration 182/1000 | Loss: 0.00012155
Iteration 183/1000 | Loss: 0.00012164
Iteration 184/1000 | Loss: 0.00011685
Iteration 185/1000 | Loss: 0.00012546
Iteration 186/1000 | Loss: 0.00011905
Iteration 187/1000 | Loss: 0.00115607
Iteration 188/1000 | Loss: 0.00020658
Iteration 189/1000 | Loss: 0.00024392
Iteration 190/1000 | Loss: 0.00013694
Iteration 191/1000 | Loss: 0.00017017
Iteration 192/1000 | Loss: 0.00027846
Iteration 193/1000 | Loss: 0.00020899
Iteration 194/1000 | Loss: 0.00021796
Iteration 195/1000 | Loss: 0.00021891
Iteration 196/1000 | Loss: 0.00023031
Iteration 197/1000 | Loss: 0.00022393
Iteration 198/1000 | Loss: 0.00017279
Iteration 199/1000 | Loss: 0.00020171
Iteration 200/1000 | Loss: 0.00024003
Iteration 201/1000 | Loss: 0.00026422
Iteration 202/1000 | Loss: 0.00023369
Iteration 203/1000 | Loss: 0.00016850
Iteration 204/1000 | Loss: 0.00011612
Iteration 205/1000 | Loss: 0.00014520
Iteration 206/1000 | Loss: 0.00023670
Iteration 207/1000 | Loss: 0.00011302
Iteration 208/1000 | Loss: 0.00012914
Iteration 209/1000 | Loss: 0.00013074
Iteration 210/1000 | Loss: 0.00011498
Iteration 211/1000 | Loss: 0.00011747
Iteration 212/1000 | Loss: 0.00010945
Iteration 213/1000 | Loss: 0.00011591
Iteration 214/1000 | Loss: 0.00015095
Iteration 215/1000 | Loss: 0.00024432
Iteration 216/1000 | Loss: 0.00018418
Iteration 217/1000 | Loss: 0.00013578
Iteration 218/1000 | Loss: 0.00013095
Iteration 219/1000 | Loss: 0.00013430
Iteration 220/1000 | Loss: 0.00109569
Iteration 221/1000 | Loss: 0.00032350
Iteration 222/1000 | Loss: 0.00011615
Iteration 223/1000 | Loss: 0.00028181
Iteration 224/1000 | Loss: 0.00012007
Iteration 225/1000 | Loss: 0.00012782
Iteration 226/1000 | Loss: 0.00010768
Iteration 227/1000 | Loss: 0.00010329
Iteration 228/1000 | Loss: 0.00012298
Iteration 229/1000 | Loss: 0.00011933
Iteration 230/1000 | Loss: 0.00011578
Iteration 231/1000 | Loss: 0.00010284
Iteration 232/1000 | Loss: 0.00010487
Iteration 233/1000 | Loss: 0.00011024
Iteration 234/1000 | Loss: 0.00011183
Iteration 235/1000 | Loss: 0.00010893
Iteration 236/1000 | Loss: 0.00011067
Iteration 237/1000 | Loss: 0.00011917
Iteration 238/1000 | Loss: 0.00024317
Iteration 239/1000 | Loss: 0.00024729
Iteration 240/1000 | Loss: 0.00011033
Iteration 241/1000 | Loss: 0.00011396
Iteration 242/1000 | Loss: 0.00012245
Iteration 243/1000 | Loss: 0.00011904
Iteration 244/1000 | Loss: 0.00012442
Iteration 245/1000 | Loss: 0.00010385
Iteration 246/1000 | Loss: 0.00010899
Iteration 247/1000 | Loss: 0.00022180
Iteration 248/1000 | Loss: 0.00010136
Iteration 249/1000 | Loss: 0.00102446
Iteration 250/1000 | Loss: 0.00054267
Iteration 251/1000 | Loss: 0.00096660
Iteration 252/1000 | Loss: 0.00024486
Iteration 253/1000 | Loss: 0.00015796
Iteration 254/1000 | Loss: 0.00011677
Iteration 255/1000 | Loss: 0.00009605
Iteration 256/1000 | Loss: 0.00008607
Iteration 257/1000 | Loss: 0.00009157
Iteration 258/1000 | Loss: 0.00008308
Iteration 259/1000 | Loss: 0.00008920
Iteration 260/1000 | Loss: 0.00008631
Iteration 261/1000 | Loss: 0.00008651
Iteration 262/1000 | Loss: 0.00008171
Iteration 263/1000 | Loss: 0.00008267
Iteration 264/1000 | Loss: 0.00008846
Iteration 265/1000 | Loss: 0.00008010
Iteration 266/1000 | Loss: 0.00008476
Iteration 267/1000 | Loss: 0.00008560
Iteration 268/1000 | Loss: 0.00007581
Iteration 269/1000 | Loss: 0.00011279
Iteration 270/1000 | Loss: 0.00021189
Iteration 271/1000 | Loss: 0.00017223
Iteration 272/1000 | Loss: 0.00018669
Iteration 273/1000 | Loss: 0.00024185
Iteration 274/1000 | Loss: 0.00007789
Iteration 275/1000 | Loss: 0.00008915
Iteration 276/1000 | Loss: 0.00008480
Iteration 277/1000 | Loss: 0.00008409
Iteration 278/1000 | Loss: 0.00009393
Iteration 279/1000 | Loss: 0.00009447
Iteration 280/1000 | Loss: 0.00008686
Iteration 281/1000 | Loss: 0.00008179
Iteration 282/1000 | Loss: 0.00009182
Iteration 283/1000 | Loss: 0.00008694
Iteration 284/1000 | Loss: 0.00009000
Iteration 285/1000 | Loss: 0.00007852
Iteration 286/1000 | Loss: 0.00095729
Iteration 287/1000 | Loss: 0.00059188
Iteration 288/1000 | Loss: 0.00007424
Iteration 289/1000 | Loss: 0.00092444
Iteration 290/1000 | Loss: 0.00035748
Iteration 291/1000 | Loss: 0.00017960
Iteration 292/1000 | Loss: 0.00011650
Iteration 293/1000 | Loss: 0.00014191
Iteration 294/1000 | Loss: 0.00012414
Iteration 295/1000 | Loss: 0.00010309
Iteration 296/1000 | Loss: 0.00008949
Iteration 297/1000 | Loss: 0.00008780
Iteration 298/1000 | Loss: 0.00009522
Iteration 299/1000 | Loss: 0.00009070
Iteration 300/1000 | Loss: 0.00010092
Iteration 301/1000 | Loss: 0.00008647
Iteration 302/1000 | Loss: 0.00007867
Iteration 303/1000 | Loss: 0.00008104
Iteration 304/1000 | Loss: 0.00007558
Iteration 305/1000 | Loss: 0.00010101
Iteration 306/1000 | Loss: 0.00011796
Iteration 307/1000 | Loss: 0.00011164
Iteration 308/1000 | Loss: 0.00011196
Iteration 309/1000 | Loss: 0.00010551
Iteration 310/1000 | Loss: 0.00009521
Iteration 311/1000 | Loss: 0.00010281
Iteration 312/1000 | Loss: 0.00009001
Iteration 313/1000 | Loss: 0.00010141
Iteration 314/1000 | Loss: 0.00009987
Iteration 315/1000 | Loss: 0.00010318
Iteration 316/1000 | Loss: 0.00010984
Iteration 317/1000 | Loss: 0.00010492
Iteration 318/1000 | Loss: 0.00009532
Iteration 319/1000 | Loss: 0.00010110
Iteration 320/1000 | Loss: 0.00010914
Iteration 321/1000 | Loss: 0.00010000
Iteration 322/1000 | Loss: 0.00009854
Iteration 323/1000 | Loss: 0.00010110
Iteration 324/1000 | Loss: 0.00007655
Iteration 325/1000 | Loss: 0.00007693
Iteration 326/1000 | Loss: 0.00007202
Iteration 327/1000 | Loss: 0.00007433
Iteration 328/1000 | Loss: 0.00008220
Iteration 329/1000 | Loss: 0.00008006
Iteration 330/1000 | Loss: 0.00008241
Iteration 331/1000 | Loss: 0.00094127
Iteration 332/1000 | Loss: 0.00060793
Iteration 333/1000 | Loss: 0.00008474
Iteration 334/1000 | Loss: 0.00008397
Iteration 335/1000 | Loss: 0.00007885
Iteration 336/1000 | Loss: 0.00007740
Iteration 337/1000 | Loss: 0.00080092
Iteration 338/1000 | Loss: 0.00008668
Iteration 339/1000 | Loss: 0.00007750
Iteration 340/1000 | Loss: 0.00008057
Iteration 341/1000 | Loss: 0.00006841
Iteration 342/1000 | Loss: 0.00024902
Iteration 343/1000 | Loss: 0.00006724
Iteration 344/1000 | Loss: 0.00007951
Iteration 345/1000 | Loss: 0.00007407
Iteration 346/1000 | Loss: 0.00006987
Iteration 347/1000 | Loss: 0.00007775
Iteration 348/1000 | Loss: 0.00006942
Iteration 349/1000 | Loss: 0.00006631
Iteration 350/1000 | Loss: 0.00006759
Iteration 351/1000 | Loss: 0.00006424
Iteration 352/1000 | Loss: 0.00006436
Iteration 353/1000 | Loss: 0.00007084
Iteration 354/1000 | Loss: 0.00007220
Iteration 355/1000 | Loss: 0.00007301
Iteration 356/1000 | Loss: 0.00006682
Iteration 357/1000 | Loss: 0.00006582
Iteration 358/1000 | Loss: 0.00006562
Iteration 359/1000 | Loss: 0.00006386
Iteration 360/1000 | Loss: 0.00006994
Iteration 361/1000 | Loss: 0.00007119
Iteration 362/1000 | Loss: 0.00006152
Iteration 363/1000 | Loss: 0.00006686
Iteration 364/1000 | Loss: 0.00006965
Iteration 365/1000 | Loss: 0.00006868
Iteration 366/1000 | Loss: 0.00006646
Iteration 367/1000 | Loss: 0.00007126
Iteration 368/1000 | Loss: 0.00006441
Iteration 369/1000 | Loss: 0.00006408
Iteration 370/1000 | Loss: 0.00006545
Iteration 371/1000 | Loss: 0.00007356
Iteration 372/1000 | Loss: 0.00007686
Iteration 373/1000 | Loss: 0.00006596
Iteration 374/1000 | Loss: 0.00006388
Iteration 375/1000 | Loss: 0.00006291
Iteration 376/1000 | Loss: 0.00006246
Iteration 377/1000 | Loss: 0.00006188
Iteration 378/1000 | Loss: 0.00006148
Iteration 379/1000 | Loss: 0.00006121
Iteration 380/1000 | Loss: 0.00006099
Iteration 381/1000 | Loss: 0.00006090
Iteration 382/1000 | Loss: 0.00006082
Iteration 383/1000 | Loss: 0.00006077
Iteration 384/1000 | Loss: 0.00006062
Iteration 385/1000 | Loss: 0.00006053
Iteration 386/1000 | Loss: 0.00006039
Iteration 387/1000 | Loss: 0.00006039
Iteration 388/1000 | Loss: 0.00006038
Iteration 389/1000 | Loss: 0.00006038
Iteration 390/1000 | Loss: 0.00006038
Iteration 391/1000 | Loss: 0.00006038
Iteration 392/1000 | Loss: 0.00006037
Iteration 393/1000 | Loss: 0.00006037
Iteration 394/1000 | Loss: 0.00006037
Iteration 395/1000 | Loss: 0.00006037
Iteration 396/1000 | Loss: 0.00006037
Iteration 397/1000 | Loss: 0.00006036
Iteration 398/1000 | Loss: 0.00006036
Iteration 399/1000 | Loss: 0.00006036
Iteration 400/1000 | Loss: 0.00006036
Iteration 401/1000 | Loss: 0.00006036
Iteration 402/1000 | Loss: 0.00006036
Iteration 403/1000 | Loss: 0.00006036
Iteration 404/1000 | Loss: 0.00006036
Iteration 405/1000 | Loss: 0.00006036
Iteration 406/1000 | Loss: 0.00006036
Iteration 407/1000 | Loss: 0.00006035
Iteration 408/1000 | Loss: 0.00006035
Iteration 409/1000 | Loss: 0.00006032
Iteration 410/1000 | Loss: 0.00006032
Iteration 411/1000 | Loss: 0.00006032
Iteration 412/1000 | Loss: 0.00006032
Iteration 413/1000 | Loss: 0.00006032
Iteration 414/1000 | Loss: 0.00006032
Iteration 415/1000 | Loss: 0.00006032
Iteration 416/1000 | Loss: 0.00006031
Iteration 417/1000 | Loss: 0.00006031
Iteration 418/1000 | Loss: 0.00006031
Iteration 419/1000 | Loss: 0.00006031
Iteration 420/1000 | Loss: 0.00006031
Iteration 421/1000 | Loss: 0.00006031
Iteration 422/1000 | Loss: 0.00006030
Iteration 423/1000 | Loss: 0.00006030
Iteration 424/1000 | Loss: 0.00006030
Iteration 425/1000 | Loss: 0.00006030
Iteration 426/1000 | Loss: 0.00006030
Iteration 427/1000 | Loss: 0.00006030
Iteration 428/1000 | Loss: 0.00006199
Iteration 429/1000 | Loss: 0.00006199
Iteration 430/1000 | Loss: 0.00006105
Iteration 431/1000 | Loss: 0.00006034
Iteration 432/1000 | Loss: 0.00006017
Iteration 433/1000 | Loss: 0.00006012
Iteration 434/1000 | Loss: 0.00006006
Iteration 435/1000 | Loss: 0.00006006
Iteration 436/1000 | Loss: 0.00006006
Iteration 437/1000 | Loss: 0.00006006
Iteration 438/1000 | Loss: 0.00006005
Iteration 439/1000 | Loss: 0.00006004
Iteration 440/1000 | Loss: 0.00006003
Iteration 441/1000 | Loss: 0.00006003
Iteration 442/1000 | Loss: 0.00006003
Iteration 443/1000 | Loss: 0.00006002
Iteration 444/1000 | Loss: 0.00006002
Iteration 445/1000 | Loss: 0.00006002
Iteration 446/1000 | Loss: 0.00006002
Iteration 447/1000 | Loss: 0.00006002
Iteration 448/1000 | Loss: 0.00006002
Iteration 449/1000 | Loss: 0.00006002
Iteration 450/1000 | Loss: 0.00006002
Iteration 451/1000 | Loss: 0.00006002
Iteration 452/1000 | Loss: 0.00006002
Iteration 453/1000 | Loss: 0.00005996
Iteration 454/1000 | Loss: 0.00005991
Iteration 455/1000 | Loss: 0.00005989
Iteration 456/1000 | Loss: 0.00005988
Iteration 457/1000 | Loss: 0.00005988
Iteration 458/1000 | Loss: 0.00005987
Iteration 459/1000 | Loss: 0.00005987
Iteration 460/1000 | Loss: 0.00005983
Iteration 461/1000 | Loss: 0.00005983
Iteration 462/1000 | Loss: 0.00005982
Iteration 463/1000 | Loss: 0.00005981
Iteration 464/1000 | Loss: 0.00005981
Iteration 465/1000 | Loss: 0.00005981
Iteration 466/1000 | Loss: 0.00005981
Iteration 467/1000 | Loss: 0.00005981
Iteration 468/1000 | Loss: 0.00005981
Iteration 469/1000 | Loss: 0.00005981
Iteration 470/1000 | Loss: 0.00005981
Iteration 471/1000 | Loss: 0.00005981
Iteration 472/1000 | Loss: 0.00005981
Iteration 473/1000 | Loss: 0.00005981
Iteration 474/1000 | Loss: 0.00005981
Iteration 475/1000 | Loss: 0.00005981
Iteration 476/1000 | Loss: 0.00005981
Iteration 477/1000 | Loss: 0.00005981
Iteration 478/1000 | Loss: 0.00005981
Iteration 479/1000 | Loss: 0.00005981
Iteration 480/1000 | Loss: 0.00005981
Iteration 481/1000 | Loss: 0.00005981
Iteration 482/1000 | Loss: 0.00005981
Iteration 483/1000 | Loss: 0.00005981
Iteration 484/1000 | Loss: 0.00005981
Iteration 485/1000 | Loss: 0.00005981
Iteration 486/1000 | Loss: 0.00005981
Iteration 487/1000 | Loss: 0.00005981
Iteration 488/1000 | Loss: 0.00005981
Iteration 489/1000 | Loss: 0.00005981
Iteration 490/1000 | Loss: 0.00005981
Iteration 491/1000 | Loss: 0.00005981
Iteration 492/1000 | Loss: 0.00005981
Iteration 493/1000 | Loss: 0.00005981
Iteration 494/1000 | Loss: 0.00005981
Iteration 495/1000 | Loss: 0.00005981
Iteration 496/1000 | Loss: 0.00005981
Iteration 497/1000 | Loss: 0.00005981
Iteration 498/1000 | Loss: 0.00005981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 498. Stopping optimization.
Last 5 losses: [5.9805450291605666e-05, 5.9805450291605666e-05, 5.9805450291605666e-05, 5.9805450291605666e-05, 5.9805450291605666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.9805450291605666e-05

Optimization complete. Final v2v error: 5.009780406951904 mm

Highest mean error: 11.849270820617676 mm for frame 75

Lowest mean error: 3.9577889442443848 mm for frame 115

Saving results

Total time: 615.0903115272522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491396
Iteration 2/25 | Loss: 0.00125951
Iteration 3/25 | Loss: 0.00114895
Iteration 4/25 | Loss: 0.00113178
Iteration 5/25 | Loss: 0.00112404
Iteration 6/25 | Loss: 0.00112284
Iteration 7/25 | Loss: 0.00112284
Iteration 8/25 | Loss: 0.00112284
Iteration 9/25 | Loss: 0.00112284
Iteration 10/25 | Loss: 0.00112284
Iteration 11/25 | Loss: 0.00112284
Iteration 12/25 | Loss: 0.00112284
Iteration 13/25 | Loss: 0.00112284
Iteration 14/25 | Loss: 0.00112284
Iteration 15/25 | Loss: 0.00112284
Iteration 16/25 | Loss: 0.00112284
Iteration 17/25 | Loss: 0.00112284
Iteration 18/25 | Loss: 0.00112284
Iteration 19/25 | Loss: 0.00112284
Iteration 20/25 | Loss: 0.00112284
Iteration 21/25 | Loss: 0.00112284
Iteration 22/25 | Loss: 0.00112284
Iteration 23/25 | Loss: 0.00112284
Iteration 24/25 | Loss: 0.00112284
Iteration 25/25 | Loss: 0.00112284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011228378862142563, 0.0011228378862142563, 0.0011228378862142563, 0.0011228378862142563, 0.0011228378862142563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011228378862142563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75652099
Iteration 2/25 | Loss: 0.00076497
Iteration 3/25 | Loss: 0.00076497
Iteration 4/25 | Loss: 0.00076497
Iteration 5/25 | Loss: 0.00076497
Iteration 6/25 | Loss: 0.00076497
Iteration 7/25 | Loss: 0.00076497
Iteration 8/25 | Loss: 0.00076497
Iteration 9/25 | Loss: 0.00076497
Iteration 10/25 | Loss: 0.00076497
Iteration 11/25 | Loss: 0.00076497
Iteration 12/25 | Loss: 0.00076497
Iteration 13/25 | Loss: 0.00076497
Iteration 14/25 | Loss: 0.00076497
Iteration 15/25 | Loss: 0.00076497
Iteration 16/25 | Loss: 0.00076497
Iteration 17/25 | Loss: 0.00076497
Iteration 18/25 | Loss: 0.00076497
Iteration 19/25 | Loss: 0.00076497
Iteration 20/25 | Loss: 0.00076497
Iteration 21/25 | Loss: 0.00076497
Iteration 22/25 | Loss: 0.00076497
Iteration 23/25 | Loss: 0.00076497
Iteration 24/25 | Loss: 0.00076497
Iteration 25/25 | Loss: 0.00076497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076497
Iteration 2/1000 | Loss: 0.00003388
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00002316
Iteration 5/1000 | Loss: 0.00002228
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002096
Iteration 8/1000 | Loss: 0.00002056
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001994
Iteration 11/1000 | Loss: 0.00001974
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001918
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001918
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001907
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001902
Iteration 23/1000 | Loss: 0.00001902
Iteration 24/1000 | Loss: 0.00001901
Iteration 25/1000 | Loss: 0.00001901
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001900
Iteration 28/1000 | Loss: 0.00001900
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001889
Iteration 43/1000 | Loss: 0.00001887
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001886
Iteration 46/1000 | Loss: 0.00001884
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001879
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001866
Iteration 65/1000 | Loss: 0.00001866
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001865
Iteration 68/1000 | Loss: 0.00001865
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001863
Iteration 80/1000 | Loss: 0.00001863
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001857
Iteration 93/1000 | Loss: 0.00001857
Iteration 94/1000 | Loss: 0.00001857
Iteration 95/1000 | Loss: 0.00001857
Iteration 96/1000 | Loss: 0.00001857
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Iteration 100/1000 | Loss: 0.00001856
Iteration 101/1000 | Loss: 0.00001856
Iteration 102/1000 | Loss: 0.00001856
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001855
Iteration 106/1000 | Loss: 0.00001855
Iteration 107/1000 | Loss: 0.00001855
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001854
Iteration 112/1000 | Loss: 0.00001854
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001851
Iteration 118/1000 | Loss: 0.00001851
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001849
Iteration 123/1000 | Loss: 0.00001849
Iteration 124/1000 | Loss: 0.00001849
Iteration 125/1000 | Loss: 0.00001848
Iteration 126/1000 | Loss: 0.00001848
Iteration 127/1000 | Loss: 0.00001847
Iteration 128/1000 | Loss: 0.00001847
Iteration 129/1000 | Loss: 0.00001846
Iteration 130/1000 | Loss: 0.00001846
Iteration 131/1000 | Loss: 0.00001846
Iteration 132/1000 | Loss: 0.00001844
Iteration 133/1000 | Loss: 0.00001844
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001842
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001841
Iteration 145/1000 | Loss: 0.00001841
Iteration 146/1000 | Loss: 0.00001840
Iteration 147/1000 | Loss: 0.00001840
Iteration 148/1000 | Loss: 0.00001840
Iteration 149/1000 | Loss: 0.00001839
Iteration 150/1000 | Loss: 0.00001839
Iteration 151/1000 | Loss: 0.00001839
Iteration 152/1000 | Loss: 0.00001839
Iteration 153/1000 | Loss: 0.00001839
Iteration 154/1000 | Loss: 0.00001839
Iteration 155/1000 | Loss: 0.00001839
Iteration 156/1000 | Loss: 0.00001838
Iteration 157/1000 | Loss: 0.00001838
Iteration 158/1000 | Loss: 0.00001838
Iteration 159/1000 | Loss: 0.00001838
Iteration 160/1000 | Loss: 0.00001838
Iteration 161/1000 | Loss: 0.00001838
Iteration 162/1000 | Loss: 0.00001838
Iteration 163/1000 | Loss: 0.00001837
Iteration 164/1000 | Loss: 0.00001837
Iteration 165/1000 | Loss: 0.00001837
Iteration 166/1000 | Loss: 0.00001837
Iteration 167/1000 | Loss: 0.00001837
Iteration 168/1000 | Loss: 0.00001837
Iteration 169/1000 | Loss: 0.00001837
Iteration 170/1000 | Loss: 0.00001837
Iteration 171/1000 | Loss: 0.00001837
Iteration 172/1000 | Loss: 0.00001837
Iteration 173/1000 | Loss: 0.00001837
Iteration 174/1000 | Loss: 0.00001836
Iteration 175/1000 | Loss: 0.00001836
Iteration 176/1000 | Loss: 0.00001836
Iteration 177/1000 | Loss: 0.00001836
Iteration 178/1000 | Loss: 0.00001836
Iteration 179/1000 | Loss: 0.00001836
Iteration 180/1000 | Loss: 0.00001836
Iteration 181/1000 | Loss: 0.00001836
Iteration 182/1000 | Loss: 0.00001836
Iteration 183/1000 | Loss: 0.00001836
Iteration 184/1000 | Loss: 0.00001836
Iteration 185/1000 | Loss: 0.00001835
Iteration 186/1000 | Loss: 0.00001835
Iteration 187/1000 | Loss: 0.00001835
Iteration 188/1000 | Loss: 0.00001835
Iteration 189/1000 | Loss: 0.00001835
Iteration 190/1000 | Loss: 0.00001835
Iteration 191/1000 | Loss: 0.00001835
Iteration 192/1000 | Loss: 0.00001835
Iteration 193/1000 | Loss: 0.00001835
Iteration 194/1000 | Loss: 0.00001835
Iteration 195/1000 | Loss: 0.00001834
Iteration 196/1000 | Loss: 0.00001834
Iteration 197/1000 | Loss: 0.00001834
Iteration 198/1000 | Loss: 0.00001834
Iteration 199/1000 | Loss: 0.00001834
Iteration 200/1000 | Loss: 0.00001833
Iteration 201/1000 | Loss: 0.00001833
Iteration 202/1000 | Loss: 0.00001833
Iteration 203/1000 | Loss: 0.00001833
Iteration 204/1000 | Loss: 0.00001833
Iteration 205/1000 | Loss: 0.00001833
Iteration 206/1000 | Loss: 0.00001832
Iteration 207/1000 | Loss: 0.00001832
Iteration 208/1000 | Loss: 0.00001832
Iteration 209/1000 | Loss: 0.00001832
Iteration 210/1000 | Loss: 0.00001832
Iteration 211/1000 | Loss: 0.00001831
Iteration 212/1000 | Loss: 0.00001831
Iteration 213/1000 | Loss: 0.00001831
Iteration 214/1000 | Loss: 0.00001831
Iteration 215/1000 | Loss: 0.00001831
Iteration 216/1000 | Loss: 0.00001831
Iteration 217/1000 | Loss: 0.00001831
Iteration 218/1000 | Loss: 0.00001831
Iteration 219/1000 | Loss: 0.00001831
Iteration 220/1000 | Loss: 0.00001830
Iteration 221/1000 | Loss: 0.00001830
Iteration 222/1000 | Loss: 0.00001830
Iteration 223/1000 | Loss: 0.00001830
Iteration 224/1000 | Loss: 0.00001830
Iteration 225/1000 | Loss: 0.00001830
Iteration 226/1000 | Loss: 0.00001830
Iteration 227/1000 | Loss: 0.00001830
Iteration 228/1000 | Loss: 0.00001830
Iteration 229/1000 | Loss: 0.00001830
Iteration 230/1000 | Loss: 0.00001830
Iteration 231/1000 | Loss: 0.00001830
Iteration 232/1000 | Loss: 0.00001830
Iteration 233/1000 | Loss: 0.00001830
Iteration 234/1000 | Loss: 0.00001830
Iteration 235/1000 | Loss: 0.00001830
Iteration 236/1000 | Loss: 0.00001830
Iteration 237/1000 | Loss: 0.00001830
Iteration 238/1000 | Loss: 0.00001829
Iteration 239/1000 | Loss: 0.00001829
Iteration 240/1000 | Loss: 0.00001829
Iteration 241/1000 | Loss: 0.00001829
Iteration 242/1000 | Loss: 0.00001829
Iteration 243/1000 | Loss: 0.00001829
Iteration 244/1000 | Loss: 0.00001829
Iteration 245/1000 | Loss: 0.00001829
Iteration 246/1000 | Loss: 0.00001829
Iteration 247/1000 | Loss: 0.00001829
Iteration 248/1000 | Loss: 0.00001829
Iteration 249/1000 | Loss: 0.00001829
Iteration 250/1000 | Loss: 0.00001829
Iteration 251/1000 | Loss: 0.00001829
Iteration 252/1000 | Loss: 0.00001829
Iteration 253/1000 | Loss: 0.00001829
Iteration 254/1000 | Loss: 0.00001829
Iteration 255/1000 | Loss: 0.00001829
Iteration 256/1000 | Loss: 0.00001828
Iteration 257/1000 | Loss: 0.00001828
Iteration 258/1000 | Loss: 0.00001828
Iteration 259/1000 | Loss: 0.00001828
Iteration 260/1000 | Loss: 0.00001828
Iteration 261/1000 | Loss: 0.00001828
Iteration 262/1000 | Loss: 0.00001828
Iteration 263/1000 | Loss: 0.00001828
Iteration 264/1000 | Loss: 0.00001828
Iteration 265/1000 | Loss: 0.00001828
Iteration 266/1000 | Loss: 0.00001828
Iteration 267/1000 | Loss: 0.00001828
Iteration 268/1000 | Loss: 0.00001828
Iteration 269/1000 | Loss: 0.00001828
Iteration 270/1000 | Loss: 0.00001828
Iteration 271/1000 | Loss: 0.00001828
Iteration 272/1000 | Loss: 0.00001828
Iteration 273/1000 | Loss: 0.00001828
Iteration 274/1000 | Loss: 0.00001828
Iteration 275/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.8284668840351515e-05, 1.8284668840351515e-05, 1.8284668840351515e-05, 1.8284668840351515e-05, 1.8284668840351515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8284668840351515e-05

Optimization complete. Final v2v error: 3.659944534301758 mm

Highest mean error: 4.217913627624512 mm for frame 245

Lowest mean error: 3.5187201499938965 mm for frame 139

Saving results

Total time: 60.76003932952881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970195
Iteration 2/25 | Loss: 0.00293180
Iteration 3/25 | Loss: 0.00196553
Iteration 4/25 | Loss: 0.00182104
Iteration 5/25 | Loss: 0.00161994
Iteration 6/25 | Loss: 0.00155682
Iteration 7/25 | Loss: 0.00154799
Iteration 8/25 | Loss: 0.00146488
Iteration 9/25 | Loss: 0.00140607
Iteration 10/25 | Loss: 0.00133624
Iteration 11/25 | Loss: 0.00132229
Iteration 12/25 | Loss: 0.00130624
Iteration 13/25 | Loss: 0.00130468
Iteration 14/25 | Loss: 0.00129845
Iteration 15/25 | Loss: 0.00129663
Iteration 16/25 | Loss: 0.00129971
Iteration 17/25 | Loss: 0.00130141
Iteration 18/25 | Loss: 0.00129642
Iteration 19/25 | Loss: 0.00129399
Iteration 20/25 | Loss: 0.00129430
Iteration 21/25 | Loss: 0.00129013
Iteration 22/25 | Loss: 0.00128722
Iteration 23/25 | Loss: 0.00128399
Iteration 24/25 | Loss: 0.00129184
Iteration 25/25 | Loss: 0.00129689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31312644
Iteration 2/25 | Loss: 0.00231035
Iteration 3/25 | Loss: 0.00220735
Iteration 4/25 | Loss: 0.00220735
Iteration 5/25 | Loss: 0.00220735
Iteration 6/25 | Loss: 0.00220735
Iteration 7/25 | Loss: 0.00220734
Iteration 8/25 | Loss: 0.00220734
Iteration 9/25 | Loss: 0.00220734
Iteration 10/25 | Loss: 0.00220734
Iteration 11/25 | Loss: 0.00220734
Iteration 12/25 | Loss: 0.00220734
Iteration 13/25 | Loss: 0.00220734
Iteration 14/25 | Loss: 0.00220734
Iteration 15/25 | Loss: 0.00220734
Iteration 16/25 | Loss: 0.00220734
Iteration 17/25 | Loss: 0.00220734
Iteration 18/25 | Loss: 0.00220734
Iteration 19/25 | Loss: 0.00220734
Iteration 20/25 | Loss: 0.00220734
Iteration 21/25 | Loss: 0.00220734
Iteration 22/25 | Loss: 0.00220734
Iteration 23/25 | Loss: 0.00220734
Iteration 24/25 | Loss: 0.00220734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002207342768087983, 0.002207342768087983, 0.002207342768087983, 0.002207342768087983, 0.002207342768087983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002207342768087983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220734
Iteration 2/1000 | Loss: 0.00064011
Iteration 3/1000 | Loss: 0.00076285
Iteration 4/1000 | Loss: 0.00026357
Iteration 5/1000 | Loss: 0.00013283
Iteration 6/1000 | Loss: 0.00034539
Iteration 7/1000 | Loss: 0.00012016
Iteration 8/1000 | Loss: 0.00010823
Iteration 9/1000 | Loss: 0.00031541
Iteration 10/1000 | Loss: 0.00011663
Iteration 11/1000 | Loss: 0.00035036
Iteration 12/1000 | Loss: 0.00028440
Iteration 13/1000 | Loss: 0.00030923
Iteration 14/1000 | Loss: 0.00038564
Iteration 15/1000 | Loss: 0.00012608
Iteration 16/1000 | Loss: 0.00035429
Iteration 17/1000 | Loss: 0.00013425
Iteration 18/1000 | Loss: 0.00015676
Iteration 19/1000 | Loss: 0.00016020
Iteration 20/1000 | Loss: 0.00023615
Iteration 21/1000 | Loss: 0.00031195
Iteration 22/1000 | Loss: 0.00027523
Iteration 23/1000 | Loss: 0.00035243
Iteration 24/1000 | Loss: 0.00041965
Iteration 25/1000 | Loss: 0.00027122
Iteration 26/1000 | Loss: 0.00012098
Iteration 27/1000 | Loss: 0.00032841
Iteration 28/1000 | Loss: 0.00026824
Iteration 29/1000 | Loss: 0.00012913
Iteration 30/1000 | Loss: 0.00009933
Iteration 31/1000 | Loss: 0.00020651
Iteration 32/1000 | Loss: 0.00052549
Iteration 33/1000 | Loss: 0.00065505
Iteration 34/1000 | Loss: 0.00072678
Iteration 35/1000 | Loss: 0.00090520
Iteration 36/1000 | Loss: 0.00019429
Iteration 37/1000 | Loss: 0.00011962
Iteration 38/1000 | Loss: 0.00036160
Iteration 39/1000 | Loss: 0.00032059
Iteration 40/1000 | Loss: 0.00011652
Iteration 41/1000 | Loss: 0.00032837
Iteration 42/1000 | Loss: 0.00031000
Iteration 43/1000 | Loss: 0.00011334
Iteration 44/1000 | Loss: 0.00016741
Iteration 45/1000 | Loss: 0.00008244
Iteration 46/1000 | Loss: 0.00008006
Iteration 47/1000 | Loss: 0.00007797
Iteration 48/1000 | Loss: 0.00007594
Iteration 49/1000 | Loss: 0.00007451
Iteration 50/1000 | Loss: 0.00007362
Iteration 51/1000 | Loss: 0.00007281
Iteration 52/1000 | Loss: 0.00007208
Iteration 53/1000 | Loss: 0.00164473
Iteration 54/1000 | Loss: 0.00110827
Iteration 55/1000 | Loss: 0.00043531
Iteration 56/1000 | Loss: 0.00246912
Iteration 57/1000 | Loss: 0.00078249
Iteration 58/1000 | Loss: 0.00011802
Iteration 59/1000 | Loss: 0.00008172
Iteration 60/1000 | Loss: 0.00007323
Iteration 61/1000 | Loss: 0.00006914
Iteration 62/1000 | Loss: 0.00006622
Iteration 63/1000 | Loss: 0.00006377
Iteration 64/1000 | Loss: 0.00006215
Iteration 65/1000 | Loss: 0.00006123
Iteration 66/1000 | Loss: 0.00006054
Iteration 67/1000 | Loss: 0.00006009
Iteration 68/1000 | Loss: 0.00005963
Iteration 69/1000 | Loss: 0.00005929
Iteration 70/1000 | Loss: 0.00005902
Iteration 71/1000 | Loss: 0.00005880
Iteration 72/1000 | Loss: 0.00005875
Iteration 73/1000 | Loss: 0.00066690
Iteration 74/1000 | Loss: 0.00007107
Iteration 75/1000 | Loss: 0.00005966
Iteration 76/1000 | Loss: 0.00005848
Iteration 77/1000 | Loss: 0.00006391
Iteration 78/1000 | Loss: 0.00005728
Iteration 79/1000 | Loss: 0.00005640
Iteration 80/1000 | Loss: 0.00005631
Iteration 81/1000 | Loss: 0.00005589
Iteration 82/1000 | Loss: 0.00005571
Iteration 83/1000 | Loss: 0.00005570
Iteration 84/1000 | Loss: 0.00005570
Iteration 85/1000 | Loss: 0.00005569
Iteration 86/1000 | Loss: 0.00005568
Iteration 87/1000 | Loss: 0.00005560
Iteration 88/1000 | Loss: 0.00005551
Iteration 89/1000 | Loss: 0.00005549
Iteration 90/1000 | Loss: 0.00005548
Iteration 91/1000 | Loss: 0.00005543
Iteration 92/1000 | Loss: 0.00005542
Iteration 93/1000 | Loss: 0.00005542
Iteration 94/1000 | Loss: 0.00005541
Iteration 95/1000 | Loss: 0.00005541
Iteration 96/1000 | Loss: 0.00005540
Iteration 97/1000 | Loss: 0.00005539
Iteration 98/1000 | Loss: 0.00005539
Iteration 99/1000 | Loss: 0.00005539
Iteration 100/1000 | Loss: 0.00005538
Iteration 101/1000 | Loss: 0.00005538
Iteration 102/1000 | Loss: 0.00005538
Iteration 103/1000 | Loss: 0.00005538
Iteration 104/1000 | Loss: 0.00005538
Iteration 105/1000 | Loss: 0.00005538
Iteration 106/1000 | Loss: 0.00005537
Iteration 107/1000 | Loss: 0.00005537
Iteration 108/1000 | Loss: 0.00005537
Iteration 109/1000 | Loss: 0.00005537
Iteration 110/1000 | Loss: 0.00005537
Iteration 111/1000 | Loss: 0.00005537
Iteration 112/1000 | Loss: 0.00005536
Iteration 113/1000 | Loss: 0.00005536
Iteration 114/1000 | Loss: 0.00005536
Iteration 115/1000 | Loss: 0.00005535
Iteration 116/1000 | Loss: 0.00005535
Iteration 117/1000 | Loss: 0.00005535
Iteration 118/1000 | Loss: 0.00005531
Iteration 119/1000 | Loss: 0.00005529
Iteration 120/1000 | Loss: 0.00005529
Iteration 121/1000 | Loss: 0.00005528
Iteration 122/1000 | Loss: 0.00005528
Iteration 123/1000 | Loss: 0.00005528
Iteration 124/1000 | Loss: 0.00005527
Iteration 125/1000 | Loss: 0.00005526
Iteration 126/1000 | Loss: 0.00005526
Iteration 127/1000 | Loss: 0.00005525
Iteration 128/1000 | Loss: 0.00005525
Iteration 129/1000 | Loss: 0.00005525
Iteration 130/1000 | Loss: 0.00005524
Iteration 131/1000 | Loss: 0.00005524
Iteration 132/1000 | Loss: 0.00005523
Iteration 133/1000 | Loss: 0.00005523
Iteration 134/1000 | Loss: 0.00005523
Iteration 135/1000 | Loss: 0.00005523
Iteration 136/1000 | Loss: 0.00005522
Iteration 137/1000 | Loss: 0.00005522
Iteration 138/1000 | Loss: 0.00005522
Iteration 139/1000 | Loss: 0.00005522
Iteration 140/1000 | Loss: 0.00005521
Iteration 141/1000 | Loss: 0.00005521
Iteration 142/1000 | Loss: 0.00005521
Iteration 143/1000 | Loss: 0.00005521
Iteration 144/1000 | Loss: 0.00005521
Iteration 145/1000 | Loss: 0.00005520
Iteration 146/1000 | Loss: 0.00005520
Iteration 147/1000 | Loss: 0.00005520
Iteration 148/1000 | Loss: 0.00005520
Iteration 149/1000 | Loss: 0.00005520
Iteration 150/1000 | Loss: 0.00005520
Iteration 151/1000 | Loss: 0.00005520
Iteration 152/1000 | Loss: 0.00005520
Iteration 153/1000 | Loss: 0.00005520
Iteration 154/1000 | Loss: 0.00005519
Iteration 155/1000 | Loss: 0.00005519
Iteration 156/1000 | Loss: 0.00005519
Iteration 157/1000 | Loss: 0.00005519
Iteration 158/1000 | Loss: 0.00005519
Iteration 159/1000 | Loss: 0.00005519
Iteration 160/1000 | Loss: 0.00005518
Iteration 161/1000 | Loss: 0.00005518
Iteration 162/1000 | Loss: 0.00005518
Iteration 163/1000 | Loss: 0.00005518
Iteration 164/1000 | Loss: 0.00005518
Iteration 165/1000 | Loss: 0.00005518
Iteration 166/1000 | Loss: 0.00005518
Iteration 167/1000 | Loss: 0.00005518
Iteration 168/1000 | Loss: 0.00005518
Iteration 169/1000 | Loss: 0.00005518
Iteration 170/1000 | Loss: 0.00005517
Iteration 171/1000 | Loss: 0.00006286
Iteration 172/1000 | Loss: 0.00005559
Iteration 173/1000 | Loss: 0.00005517
Iteration 174/1000 | Loss: 0.00005517
Iteration 175/1000 | Loss: 0.00005516
Iteration 176/1000 | Loss: 0.00005516
Iteration 177/1000 | Loss: 0.00005516
Iteration 178/1000 | Loss: 0.00005516
Iteration 179/1000 | Loss: 0.00005516
Iteration 180/1000 | Loss: 0.00005516
Iteration 181/1000 | Loss: 0.00005516
Iteration 182/1000 | Loss: 0.00005516
Iteration 183/1000 | Loss: 0.00005516
Iteration 184/1000 | Loss: 0.00005516
Iteration 185/1000 | Loss: 0.00005516
Iteration 186/1000 | Loss: 0.00005516
Iteration 187/1000 | Loss: 0.00005516
Iteration 188/1000 | Loss: 0.00005516
Iteration 189/1000 | Loss: 0.00005515
Iteration 190/1000 | Loss: 0.00005515
Iteration 191/1000 | Loss: 0.00005515
Iteration 192/1000 | Loss: 0.00005515
Iteration 193/1000 | Loss: 0.00005515
Iteration 194/1000 | Loss: 0.00005515
Iteration 195/1000 | Loss: 0.00005515
Iteration 196/1000 | Loss: 0.00005515
Iteration 197/1000 | Loss: 0.00005515
Iteration 198/1000 | Loss: 0.00005515
Iteration 199/1000 | Loss: 0.00005515
Iteration 200/1000 | Loss: 0.00005515
Iteration 201/1000 | Loss: 0.00005515
Iteration 202/1000 | Loss: 0.00005515
Iteration 203/1000 | Loss: 0.00005515
Iteration 204/1000 | Loss: 0.00005515
Iteration 205/1000 | Loss: 0.00005515
Iteration 206/1000 | Loss: 0.00005515
Iteration 207/1000 | Loss: 0.00005515
Iteration 208/1000 | Loss: 0.00005515
Iteration 209/1000 | Loss: 0.00005515
Iteration 210/1000 | Loss: 0.00005515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [5.515183147508651e-05, 5.515183147508651e-05, 5.515183147508651e-05, 5.515183147508651e-05, 5.515183147508651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.515183147508651e-05

Optimization complete. Final v2v error: 4.381783962249756 mm

Highest mean error: 10.662952423095703 mm for frame 46

Lowest mean error: 2.478280782699585 mm for frame 34

Saving results

Total time: 173.47582483291626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444225
Iteration 2/25 | Loss: 0.00119059
Iteration 3/25 | Loss: 0.00109027
Iteration 4/25 | Loss: 0.00107140
Iteration 5/25 | Loss: 0.00106515
Iteration 6/25 | Loss: 0.00106332
Iteration 7/25 | Loss: 0.00106281
Iteration 8/25 | Loss: 0.00106281
Iteration 9/25 | Loss: 0.00106281
Iteration 10/25 | Loss: 0.00106281
Iteration 11/25 | Loss: 0.00106281
Iteration 12/25 | Loss: 0.00106281
Iteration 13/25 | Loss: 0.00106281
Iteration 14/25 | Loss: 0.00106281
Iteration 15/25 | Loss: 0.00106281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010628110030665994, 0.0010628110030665994, 0.0010628110030665994, 0.0010628110030665994, 0.0010628110030665994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010628110030665994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23221624
Iteration 2/25 | Loss: 0.00082409
Iteration 3/25 | Loss: 0.00082405
Iteration 4/25 | Loss: 0.00082405
Iteration 5/25 | Loss: 0.00082405
Iteration 6/25 | Loss: 0.00082405
Iteration 7/25 | Loss: 0.00082405
Iteration 8/25 | Loss: 0.00082405
Iteration 9/25 | Loss: 0.00082405
Iteration 10/25 | Loss: 0.00082405
Iteration 11/25 | Loss: 0.00082405
Iteration 12/25 | Loss: 0.00082405
Iteration 13/25 | Loss: 0.00082405
Iteration 14/25 | Loss: 0.00082405
Iteration 15/25 | Loss: 0.00082405
Iteration 16/25 | Loss: 0.00082405
Iteration 17/25 | Loss: 0.00082405
Iteration 18/25 | Loss: 0.00082405
Iteration 19/25 | Loss: 0.00082405
Iteration 20/25 | Loss: 0.00082405
Iteration 21/25 | Loss: 0.00082405
Iteration 22/25 | Loss: 0.00082405
Iteration 23/25 | Loss: 0.00082405
Iteration 24/25 | Loss: 0.00082405
Iteration 25/25 | Loss: 0.00082405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082405
Iteration 2/1000 | Loss: 0.00003711
Iteration 3/1000 | Loss: 0.00002199
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001111
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001108
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001104
Iteration 35/1000 | Loss: 0.00001104
Iteration 36/1000 | Loss: 0.00001104
Iteration 37/1000 | Loss: 0.00001104
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001089
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001089
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001083
Iteration 53/1000 | Loss: 0.00001083
Iteration 54/1000 | Loss: 0.00001083
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001081
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001080
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001080
Iteration 64/1000 | Loss: 0.00001080
Iteration 65/1000 | Loss: 0.00001080
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001079
Iteration 69/1000 | Loss: 0.00001079
Iteration 70/1000 | Loss: 0.00001079
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001078
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001075
Iteration 111/1000 | Loss: 0.00001075
Iteration 112/1000 | Loss: 0.00001075
Iteration 113/1000 | Loss: 0.00001075
Iteration 114/1000 | Loss: 0.00001075
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001074
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001073
Iteration 135/1000 | Loss: 0.00001073
Iteration 136/1000 | Loss: 0.00001073
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001071
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001071
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001070
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001068
Iteration 165/1000 | Loss: 0.00001068
Iteration 166/1000 | Loss: 0.00001068
Iteration 167/1000 | Loss: 0.00001068
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001068
Iteration 171/1000 | Loss: 0.00001068
Iteration 172/1000 | Loss: 0.00001068
Iteration 173/1000 | Loss: 0.00001068
Iteration 174/1000 | Loss: 0.00001068
Iteration 175/1000 | Loss: 0.00001068
Iteration 176/1000 | Loss: 0.00001068
Iteration 177/1000 | Loss: 0.00001068
Iteration 178/1000 | Loss: 0.00001068
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001067
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001066
Iteration 190/1000 | Loss: 0.00001066
Iteration 191/1000 | Loss: 0.00001066
Iteration 192/1000 | Loss: 0.00001066
Iteration 193/1000 | Loss: 0.00001066
Iteration 194/1000 | Loss: 0.00001066
Iteration 195/1000 | Loss: 0.00001066
Iteration 196/1000 | Loss: 0.00001066
Iteration 197/1000 | Loss: 0.00001066
Iteration 198/1000 | Loss: 0.00001066
Iteration 199/1000 | Loss: 0.00001066
Iteration 200/1000 | Loss: 0.00001066
Iteration 201/1000 | Loss: 0.00001066
Iteration 202/1000 | Loss: 0.00001066
Iteration 203/1000 | Loss: 0.00001066
Iteration 204/1000 | Loss: 0.00001066
Iteration 205/1000 | Loss: 0.00001066
Iteration 206/1000 | Loss: 0.00001066
Iteration 207/1000 | Loss: 0.00001066
Iteration 208/1000 | Loss: 0.00001066
Iteration 209/1000 | Loss: 0.00001066
Iteration 210/1000 | Loss: 0.00001066
Iteration 211/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.0661720807547681e-05, 1.0661720807547681e-05, 1.0661720807547681e-05, 1.0661720807547681e-05, 1.0661720807547681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0661720807547681e-05

Optimization complete. Final v2v error: 2.8149220943450928 mm

Highest mean error: 3.317305564880371 mm for frame 57

Lowest mean error: 2.510037899017334 mm for frame 16

Saving results

Total time: 43.72431230545044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773122
Iteration 2/25 | Loss: 0.00156648
Iteration 3/25 | Loss: 0.00135472
Iteration 4/25 | Loss: 0.00124620
Iteration 5/25 | Loss: 0.00122757
Iteration 6/25 | Loss: 0.00122304
Iteration 7/25 | Loss: 0.00122139
Iteration 8/25 | Loss: 0.00122061
Iteration 9/25 | Loss: 0.00122042
Iteration 10/25 | Loss: 0.00122041
Iteration 11/25 | Loss: 0.00122041
Iteration 12/25 | Loss: 0.00122041
Iteration 13/25 | Loss: 0.00122041
Iteration 14/25 | Loss: 0.00122041
Iteration 15/25 | Loss: 0.00122041
Iteration 16/25 | Loss: 0.00122040
Iteration 17/25 | Loss: 0.00122040
Iteration 18/25 | Loss: 0.00122040
Iteration 19/25 | Loss: 0.00122040
Iteration 20/25 | Loss: 0.00122039
Iteration 21/25 | Loss: 0.00122039
Iteration 22/25 | Loss: 0.00122039
Iteration 23/25 | Loss: 0.00122039
Iteration 24/25 | Loss: 0.00122039
Iteration 25/25 | Loss: 0.00122039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06720901
Iteration 2/25 | Loss: 0.00110387
Iteration 3/25 | Loss: 0.00110386
Iteration 4/25 | Loss: 0.00110386
Iteration 5/25 | Loss: 0.00110386
Iteration 6/25 | Loss: 0.00110386
Iteration 7/25 | Loss: 0.00110386
Iteration 8/25 | Loss: 0.00110386
Iteration 9/25 | Loss: 0.00110385
Iteration 10/25 | Loss: 0.00110385
Iteration 11/25 | Loss: 0.00110385
Iteration 12/25 | Loss: 0.00110385
Iteration 13/25 | Loss: 0.00110385
Iteration 14/25 | Loss: 0.00110385
Iteration 15/25 | Loss: 0.00110385
Iteration 16/25 | Loss: 0.00110385
Iteration 17/25 | Loss: 0.00110385
Iteration 18/25 | Loss: 0.00110385
Iteration 19/25 | Loss: 0.00110385
Iteration 20/25 | Loss: 0.00110385
Iteration 21/25 | Loss: 0.00110385
Iteration 22/25 | Loss: 0.00110385
Iteration 23/25 | Loss: 0.00110385
Iteration 24/25 | Loss: 0.00110385
Iteration 25/25 | Loss: 0.00110385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110385
Iteration 2/1000 | Loss: 0.00005493
Iteration 3/1000 | Loss: 0.00002676
Iteration 4/1000 | Loss: 0.00002521
Iteration 5/1000 | Loss: 0.00002444
Iteration 6/1000 | Loss: 0.00002381
Iteration 7/1000 | Loss: 0.00004660
Iteration 8/1000 | Loss: 0.00002805
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002251
Iteration 11/1000 | Loss: 0.00002217
Iteration 12/1000 | Loss: 0.00006227
Iteration 13/1000 | Loss: 0.00002180
Iteration 14/1000 | Loss: 0.00003760
Iteration 15/1000 | Loss: 0.00002269
Iteration 16/1000 | Loss: 0.00002187
Iteration 17/1000 | Loss: 0.00002151
Iteration 18/1000 | Loss: 0.00003887
Iteration 19/1000 | Loss: 0.00003887
Iteration 20/1000 | Loss: 0.00002165
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002126
Iteration 25/1000 | Loss: 0.00002126
Iteration 26/1000 | Loss: 0.00002126
Iteration 27/1000 | Loss: 0.00002125
Iteration 28/1000 | Loss: 0.00002125
Iteration 29/1000 | Loss: 0.00002124
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002123
Iteration 32/1000 | Loss: 0.00002122
Iteration 33/1000 | Loss: 0.00002121
Iteration 34/1000 | Loss: 0.00002121
Iteration 35/1000 | Loss: 0.00002121
Iteration 36/1000 | Loss: 0.00002121
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002120
Iteration 41/1000 | Loss: 0.00002120
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002119
Iteration 46/1000 | Loss: 0.00002119
Iteration 47/1000 | Loss: 0.00002119
Iteration 48/1000 | Loss: 0.00002118
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002116
Iteration 53/1000 | Loss: 0.00002115
Iteration 54/1000 | Loss: 0.00002115
Iteration 55/1000 | Loss: 0.00002115
Iteration 56/1000 | Loss: 0.00002115
Iteration 57/1000 | Loss: 0.00002115
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002114
Iteration 68/1000 | Loss: 0.00002114
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00002113
Iteration 73/1000 | Loss: 0.00002113
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002111
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002111
Iteration 83/1000 | Loss: 0.00002111
Iteration 84/1000 | Loss: 0.00002111
Iteration 85/1000 | Loss: 0.00002110
Iteration 86/1000 | Loss: 0.00002110
Iteration 87/1000 | Loss: 0.00002110
Iteration 88/1000 | Loss: 0.00002110
Iteration 89/1000 | Loss: 0.00002110
Iteration 90/1000 | Loss: 0.00002110
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002110
Iteration 93/1000 | Loss: 0.00002110
Iteration 94/1000 | Loss: 0.00002110
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002109
Iteration 99/1000 | Loss: 0.00002109
Iteration 100/1000 | Loss: 0.00002109
Iteration 101/1000 | Loss: 0.00002109
Iteration 102/1000 | Loss: 0.00002109
Iteration 103/1000 | Loss: 0.00002109
Iteration 104/1000 | Loss: 0.00002109
Iteration 105/1000 | Loss: 0.00002109
Iteration 106/1000 | Loss: 0.00002109
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002108
Iteration 109/1000 | Loss: 0.00002108
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002108
Iteration 114/1000 | Loss: 0.00002108
Iteration 115/1000 | Loss: 0.00002108
Iteration 116/1000 | Loss: 0.00002108
Iteration 117/1000 | Loss: 0.00002108
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002106
Iteration 124/1000 | Loss: 0.00002106
Iteration 125/1000 | Loss: 0.00002106
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002106
Iteration 128/1000 | Loss: 0.00002106
Iteration 129/1000 | Loss: 0.00002106
Iteration 130/1000 | Loss: 0.00002106
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002105
Iteration 133/1000 | Loss: 0.00002105
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00002104
Iteration 136/1000 | Loss: 0.00002104
Iteration 137/1000 | Loss: 0.00002104
Iteration 138/1000 | Loss: 0.00002104
Iteration 139/1000 | Loss: 0.00002104
Iteration 140/1000 | Loss: 0.00002104
Iteration 141/1000 | Loss: 0.00002104
Iteration 142/1000 | Loss: 0.00002104
Iteration 143/1000 | Loss: 0.00002104
Iteration 144/1000 | Loss: 0.00002104
Iteration 145/1000 | Loss: 0.00002104
Iteration 146/1000 | Loss: 0.00002104
Iteration 147/1000 | Loss: 0.00002104
Iteration 148/1000 | Loss: 0.00002104
Iteration 149/1000 | Loss: 0.00002104
Iteration 150/1000 | Loss: 0.00002104
Iteration 151/1000 | Loss: 0.00002104
Iteration 152/1000 | Loss: 0.00002104
Iteration 153/1000 | Loss: 0.00002104
Iteration 154/1000 | Loss: 0.00002104
Iteration 155/1000 | Loss: 0.00002104
Iteration 156/1000 | Loss: 0.00002104
Iteration 157/1000 | Loss: 0.00002104
Iteration 158/1000 | Loss: 0.00002104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.104062878061086e-05, 2.104062878061086e-05, 2.104062878061086e-05, 2.104062878061086e-05, 2.104062878061086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.104062878061086e-05

Optimization complete. Final v2v error: 3.819819688796997 mm

Highest mean error: 4.419909954071045 mm for frame 118

Lowest mean error: 2.9571499824523926 mm for frame 193

Saving results

Total time: 61.35549354553223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454975
Iteration 2/25 | Loss: 0.00132170
Iteration 3/25 | Loss: 0.00116014
Iteration 4/25 | Loss: 0.00114425
Iteration 5/25 | Loss: 0.00114085
Iteration 6/25 | Loss: 0.00113997
Iteration 7/25 | Loss: 0.00113983
Iteration 8/25 | Loss: 0.00113983
Iteration 9/25 | Loss: 0.00113983
Iteration 10/25 | Loss: 0.00113983
Iteration 11/25 | Loss: 0.00113983
Iteration 12/25 | Loss: 0.00113983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001139827654697001, 0.001139827654697001, 0.001139827654697001, 0.001139827654697001, 0.001139827654697001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001139827654697001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46227384
Iteration 2/25 | Loss: 0.00069791
Iteration 3/25 | Loss: 0.00069790
Iteration 4/25 | Loss: 0.00069790
Iteration 5/25 | Loss: 0.00069790
Iteration 6/25 | Loss: 0.00069790
Iteration 7/25 | Loss: 0.00069790
Iteration 8/25 | Loss: 0.00069789
Iteration 9/25 | Loss: 0.00069789
Iteration 10/25 | Loss: 0.00069789
Iteration 11/25 | Loss: 0.00069789
Iteration 12/25 | Loss: 0.00069789
Iteration 13/25 | Loss: 0.00069789
Iteration 14/25 | Loss: 0.00069789
Iteration 15/25 | Loss: 0.00069789
Iteration 16/25 | Loss: 0.00069789
Iteration 17/25 | Loss: 0.00069789
Iteration 18/25 | Loss: 0.00069789
Iteration 19/25 | Loss: 0.00069789
Iteration 20/25 | Loss: 0.00069789
Iteration 21/25 | Loss: 0.00069789
Iteration 22/25 | Loss: 0.00069789
Iteration 23/25 | Loss: 0.00069789
Iteration 24/25 | Loss: 0.00069789
Iteration 25/25 | Loss: 0.00069789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069789
Iteration 2/1000 | Loss: 0.00004139
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00001804
Iteration 5/1000 | Loss: 0.00001683
Iteration 6/1000 | Loss: 0.00001577
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001384
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001370
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001365
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001343
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001339
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001336
Iteration 71/1000 | Loss: 0.00001336
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001324
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001322
Iteration 138/1000 | Loss: 0.00001322
Iteration 139/1000 | Loss: 0.00001322
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001319
Iteration 151/1000 | Loss: 0.00001319
Iteration 152/1000 | Loss: 0.00001319
Iteration 153/1000 | Loss: 0.00001319
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001318
Iteration 158/1000 | Loss: 0.00001318
Iteration 159/1000 | Loss: 0.00001318
Iteration 160/1000 | Loss: 0.00001318
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001318
Iteration 167/1000 | Loss: 0.00001318
Iteration 168/1000 | Loss: 0.00001318
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001318
Iteration 171/1000 | Loss: 0.00001318
Iteration 172/1000 | Loss: 0.00001318
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001317
Iteration 175/1000 | Loss: 0.00001317
Iteration 176/1000 | Loss: 0.00001317
Iteration 177/1000 | Loss: 0.00001317
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001316
Iteration 183/1000 | Loss: 0.00001316
Iteration 184/1000 | Loss: 0.00001316
Iteration 185/1000 | Loss: 0.00001316
Iteration 186/1000 | Loss: 0.00001316
Iteration 187/1000 | Loss: 0.00001316
Iteration 188/1000 | Loss: 0.00001316
Iteration 189/1000 | Loss: 0.00001316
Iteration 190/1000 | Loss: 0.00001316
Iteration 191/1000 | Loss: 0.00001316
Iteration 192/1000 | Loss: 0.00001316
Iteration 193/1000 | Loss: 0.00001316
Iteration 194/1000 | Loss: 0.00001316
Iteration 195/1000 | Loss: 0.00001315
Iteration 196/1000 | Loss: 0.00001315
Iteration 197/1000 | Loss: 0.00001315
Iteration 198/1000 | Loss: 0.00001315
Iteration 199/1000 | Loss: 0.00001315
Iteration 200/1000 | Loss: 0.00001315
Iteration 201/1000 | Loss: 0.00001315
Iteration 202/1000 | Loss: 0.00001315
Iteration 203/1000 | Loss: 0.00001315
Iteration 204/1000 | Loss: 0.00001315
Iteration 205/1000 | Loss: 0.00001315
Iteration 206/1000 | Loss: 0.00001315
Iteration 207/1000 | Loss: 0.00001315
Iteration 208/1000 | Loss: 0.00001315
Iteration 209/1000 | Loss: 0.00001315
Iteration 210/1000 | Loss: 0.00001315
Iteration 211/1000 | Loss: 0.00001315
Iteration 212/1000 | Loss: 0.00001315
Iteration 213/1000 | Loss: 0.00001315
Iteration 214/1000 | Loss: 0.00001315
Iteration 215/1000 | Loss: 0.00001315
Iteration 216/1000 | Loss: 0.00001315
Iteration 217/1000 | Loss: 0.00001315
Iteration 218/1000 | Loss: 0.00001315
Iteration 219/1000 | Loss: 0.00001315
Iteration 220/1000 | Loss: 0.00001315
Iteration 221/1000 | Loss: 0.00001315
Iteration 222/1000 | Loss: 0.00001315
Iteration 223/1000 | Loss: 0.00001315
Iteration 224/1000 | Loss: 0.00001315
Iteration 225/1000 | Loss: 0.00001315
Iteration 226/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3152789506420959e-05, 1.3152789506420959e-05, 1.3152789506420959e-05, 1.3152789506420959e-05, 1.3152789506420959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3152789506420959e-05

Optimization complete. Final v2v error: 3.024007797241211 mm

Highest mean error: 4.328302383422852 mm for frame 67

Lowest mean error: 2.5084621906280518 mm for frame 8

Saving results

Total time: 43.899434328079224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773351
Iteration 2/25 | Loss: 0.00139130
Iteration 3/25 | Loss: 0.00112960
Iteration 4/25 | Loss: 0.00111017
Iteration 5/25 | Loss: 0.00110721
Iteration 6/25 | Loss: 0.00110703
Iteration 7/25 | Loss: 0.00110703
Iteration 8/25 | Loss: 0.00110703
Iteration 9/25 | Loss: 0.00110703
Iteration 10/25 | Loss: 0.00110703
Iteration 11/25 | Loss: 0.00110703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011070257751271129, 0.0011070257751271129, 0.0011070257751271129, 0.0011070257751271129, 0.0011070257751271129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011070257751271129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32552338
Iteration 2/25 | Loss: 0.00060580
Iteration 3/25 | Loss: 0.00060579
Iteration 4/25 | Loss: 0.00060579
Iteration 5/25 | Loss: 0.00060579
Iteration 6/25 | Loss: 0.00060579
Iteration 7/25 | Loss: 0.00060579
Iteration 8/25 | Loss: 0.00060579
Iteration 9/25 | Loss: 0.00060579
Iteration 10/25 | Loss: 0.00060579
Iteration 11/25 | Loss: 0.00060579
Iteration 12/25 | Loss: 0.00060579
Iteration 13/25 | Loss: 0.00060579
Iteration 14/25 | Loss: 0.00060579
Iteration 15/25 | Loss: 0.00060579
Iteration 16/25 | Loss: 0.00060579
Iteration 17/25 | Loss: 0.00060579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006057858699932694, 0.0006057858699932694, 0.0006057858699932694, 0.0006057858699932694, 0.0006057858699932694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006057858699932694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060579
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001786
Iteration 4/1000 | Loss: 0.00001652
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001264
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001222
Iteration 93/1000 | Loss: 0.00001222
Iteration 94/1000 | Loss: 0.00001222
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001220
Iteration 113/1000 | Loss: 0.00001220
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001218
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001217
Iteration 127/1000 | Loss: 0.00001217
Iteration 128/1000 | Loss: 0.00001217
Iteration 129/1000 | Loss: 0.00001217
Iteration 130/1000 | Loss: 0.00001217
Iteration 131/1000 | Loss: 0.00001217
Iteration 132/1000 | Loss: 0.00001217
Iteration 133/1000 | Loss: 0.00001217
Iteration 134/1000 | Loss: 0.00001217
Iteration 135/1000 | Loss: 0.00001217
Iteration 136/1000 | Loss: 0.00001217
Iteration 137/1000 | Loss: 0.00001217
Iteration 138/1000 | Loss: 0.00001217
Iteration 139/1000 | Loss: 0.00001217
Iteration 140/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.2171280104666948e-05, 1.2171280104666948e-05, 1.2171280104666948e-05, 1.2171280104666948e-05, 1.2171280104666948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2171280104666948e-05

Optimization complete. Final v2v error: 2.962719202041626 mm

Highest mean error: 3.3091042041778564 mm for frame 9

Lowest mean error: 2.838369131088257 mm for frame 63

Saving results

Total time: 43.29243063926697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826150
Iteration 2/25 | Loss: 0.00120406
Iteration 3/25 | Loss: 0.00108166
Iteration 4/25 | Loss: 0.00106547
Iteration 5/25 | Loss: 0.00106224
Iteration 6/25 | Loss: 0.00106224
Iteration 7/25 | Loss: 0.00106224
Iteration 8/25 | Loss: 0.00106224
Iteration 9/25 | Loss: 0.00106224
Iteration 10/25 | Loss: 0.00106224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010622377740219235, 0.0010622377740219235, 0.0010622377740219235, 0.0010622377740219235, 0.0010622377740219235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010622377740219235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34774220
Iteration 2/25 | Loss: 0.00077306
Iteration 3/25 | Loss: 0.00077306
Iteration 4/25 | Loss: 0.00077306
Iteration 5/25 | Loss: 0.00077306
Iteration 6/25 | Loss: 0.00077306
Iteration 7/25 | Loss: 0.00077305
Iteration 8/25 | Loss: 0.00077305
Iteration 9/25 | Loss: 0.00077305
Iteration 10/25 | Loss: 0.00077305
Iteration 11/25 | Loss: 0.00077305
Iteration 12/25 | Loss: 0.00077305
Iteration 13/25 | Loss: 0.00077305
Iteration 14/25 | Loss: 0.00077305
Iteration 15/25 | Loss: 0.00077305
Iteration 16/25 | Loss: 0.00077305
Iteration 17/25 | Loss: 0.00077305
Iteration 18/25 | Loss: 0.00077305
Iteration 19/25 | Loss: 0.00077305
Iteration 20/25 | Loss: 0.00077305
Iteration 21/25 | Loss: 0.00077305
Iteration 22/25 | Loss: 0.00077305
Iteration 23/25 | Loss: 0.00077305
Iteration 24/25 | Loss: 0.00077305
Iteration 25/25 | Loss: 0.00077305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077305
Iteration 2/1000 | Loss: 0.00001739
Iteration 3/1000 | Loss: 0.00001239
Iteration 4/1000 | Loss: 0.00001125
Iteration 5/1000 | Loss: 0.00001044
Iteration 6/1000 | Loss: 0.00001001
Iteration 7/1000 | Loss: 0.00000973
Iteration 8/1000 | Loss: 0.00000948
Iteration 9/1000 | Loss: 0.00000931
Iteration 10/1000 | Loss: 0.00000923
Iteration 11/1000 | Loss: 0.00000922
Iteration 12/1000 | Loss: 0.00000921
Iteration 13/1000 | Loss: 0.00000920
Iteration 14/1000 | Loss: 0.00000920
Iteration 15/1000 | Loss: 0.00000915
Iteration 16/1000 | Loss: 0.00000914
Iteration 17/1000 | Loss: 0.00000911
Iteration 18/1000 | Loss: 0.00000911
Iteration 19/1000 | Loss: 0.00000910
Iteration 20/1000 | Loss: 0.00000910
Iteration 21/1000 | Loss: 0.00000910
Iteration 22/1000 | Loss: 0.00000909
Iteration 23/1000 | Loss: 0.00000907
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000907
Iteration 26/1000 | Loss: 0.00000907
Iteration 27/1000 | Loss: 0.00000907
Iteration 28/1000 | Loss: 0.00000906
Iteration 29/1000 | Loss: 0.00000906
Iteration 30/1000 | Loss: 0.00000906
Iteration 31/1000 | Loss: 0.00000906
Iteration 32/1000 | Loss: 0.00000906
Iteration 33/1000 | Loss: 0.00000906
Iteration 34/1000 | Loss: 0.00000906
Iteration 35/1000 | Loss: 0.00000906
Iteration 36/1000 | Loss: 0.00000906
Iteration 37/1000 | Loss: 0.00000905
Iteration 38/1000 | Loss: 0.00000905
Iteration 39/1000 | Loss: 0.00000905
Iteration 40/1000 | Loss: 0.00000904
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000901
Iteration 45/1000 | Loss: 0.00000901
Iteration 46/1000 | Loss: 0.00000901
Iteration 47/1000 | Loss: 0.00000901
Iteration 48/1000 | Loss: 0.00000901
Iteration 49/1000 | Loss: 0.00000900
Iteration 50/1000 | Loss: 0.00000898
Iteration 51/1000 | Loss: 0.00000897
Iteration 52/1000 | Loss: 0.00000897
Iteration 53/1000 | Loss: 0.00000896
Iteration 54/1000 | Loss: 0.00000896
Iteration 55/1000 | Loss: 0.00000896
Iteration 56/1000 | Loss: 0.00000892
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000891
Iteration 61/1000 | Loss: 0.00000891
Iteration 62/1000 | Loss: 0.00000888
Iteration 63/1000 | Loss: 0.00000887
Iteration 64/1000 | Loss: 0.00000887
Iteration 65/1000 | Loss: 0.00000886
Iteration 66/1000 | Loss: 0.00000885
Iteration 67/1000 | Loss: 0.00000885
Iteration 68/1000 | Loss: 0.00000885
Iteration 69/1000 | Loss: 0.00000883
Iteration 70/1000 | Loss: 0.00000881
Iteration 71/1000 | Loss: 0.00000881
Iteration 72/1000 | Loss: 0.00000881
Iteration 73/1000 | Loss: 0.00000880
Iteration 74/1000 | Loss: 0.00000878
Iteration 75/1000 | Loss: 0.00000878
Iteration 76/1000 | Loss: 0.00000877
Iteration 77/1000 | Loss: 0.00000877
Iteration 78/1000 | Loss: 0.00000877
Iteration 79/1000 | Loss: 0.00000877
Iteration 80/1000 | Loss: 0.00000877
Iteration 81/1000 | Loss: 0.00000876
Iteration 82/1000 | Loss: 0.00000875
Iteration 83/1000 | Loss: 0.00000875
Iteration 84/1000 | Loss: 0.00000874
Iteration 85/1000 | Loss: 0.00000874
Iteration 86/1000 | Loss: 0.00000874
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000873
Iteration 90/1000 | Loss: 0.00000873
Iteration 91/1000 | Loss: 0.00000873
Iteration 92/1000 | Loss: 0.00000873
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000872
Iteration 95/1000 | Loss: 0.00000872
Iteration 96/1000 | Loss: 0.00000872
Iteration 97/1000 | Loss: 0.00000872
Iteration 98/1000 | Loss: 0.00000871
Iteration 99/1000 | Loss: 0.00000870
Iteration 100/1000 | Loss: 0.00000870
Iteration 101/1000 | Loss: 0.00000870
Iteration 102/1000 | Loss: 0.00000870
Iteration 103/1000 | Loss: 0.00000870
Iteration 104/1000 | Loss: 0.00000869
Iteration 105/1000 | Loss: 0.00000869
Iteration 106/1000 | Loss: 0.00000869
Iteration 107/1000 | Loss: 0.00000869
Iteration 108/1000 | Loss: 0.00000868
Iteration 109/1000 | Loss: 0.00000868
Iteration 110/1000 | Loss: 0.00000868
Iteration 111/1000 | Loss: 0.00000867
Iteration 112/1000 | Loss: 0.00000867
Iteration 113/1000 | Loss: 0.00000867
Iteration 114/1000 | Loss: 0.00000867
Iteration 115/1000 | Loss: 0.00000867
Iteration 116/1000 | Loss: 0.00000867
Iteration 117/1000 | Loss: 0.00000866
Iteration 118/1000 | Loss: 0.00000866
Iteration 119/1000 | Loss: 0.00000866
Iteration 120/1000 | Loss: 0.00000866
Iteration 121/1000 | Loss: 0.00000866
Iteration 122/1000 | Loss: 0.00000866
Iteration 123/1000 | Loss: 0.00000866
Iteration 124/1000 | Loss: 0.00000866
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000865
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000865
Iteration 130/1000 | Loss: 0.00000865
Iteration 131/1000 | Loss: 0.00000865
Iteration 132/1000 | Loss: 0.00000865
Iteration 133/1000 | Loss: 0.00000865
Iteration 134/1000 | Loss: 0.00000865
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000864
Iteration 138/1000 | Loss: 0.00000864
Iteration 139/1000 | Loss: 0.00000864
Iteration 140/1000 | Loss: 0.00000863
Iteration 141/1000 | Loss: 0.00000863
Iteration 142/1000 | Loss: 0.00000863
Iteration 143/1000 | Loss: 0.00000863
Iteration 144/1000 | Loss: 0.00000862
Iteration 145/1000 | Loss: 0.00000862
Iteration 146/1000 | Loss: 0.00000862
Iteration 147/1000 | Loss: 0.00000862
Iteration 148/1000 | Loss: 0.00000861
Iteration 149/1000 | Loss: 0.00000861
Iteration 150/1000 | Loss: 0.00000861
Iteration 151/1000 | Loss: 0.00000860
Iteration 152/1000 | Loss: 0.00000860
Iteration 153/1000 | Loss: 0.00000860
Iteration 154/1000 | Loss: 0.00000860
Iteration 155/1000 | Loss: 0.00000859
Iteration 156/1000 | Loss: 0.00000859
Iteration 157/1000 | Loss: 0.00000859
Iteration 158/1000 | Loss: 0.00000859
Iteration 159/1000 | Loss: 0.00000859
Iteration 160/1000 | Loss: 0.00000858
Iteration 161/1000 | Loss: 0.00000858
Iteration 162/1000 | Loss: 0.00000858
Iteration 163/1000 | Loss: 0.00000858
Iteration 164/1000 | Loss: 0.00000857
Iteration 165/1000 | Loss: 0.00000857
Iteration 166/1000 | Loss: 0.00000857
Iteration 167/1000 | Loss: 0.00000857
Iteration 168/1000 | Loss: 0.00000856
Iteration 169/1000 | Loss: 0.00000856
Iteration 170/1000 | Loss: 0.00000856
Iteration 171/1000 | Loss: 0.00000856
Iteration 172/1000 | Loss: 0.00000856
Iteration 173/1000 | Loss: 0.00000856
Iteration 174/1000 | Loss: 0.00000855
Iteration 175/1000 | Loss: 0.00000855
Iteration 176/1000 | Loss: 0.00000855
Iteration 177/1000 | Loss: 0.00000855
Iteration 178/1000 | Loss: 0.00000855
Iteration 179/1000 | Loss: 0.00000855
Iteration 180/1000 | Loss: 0.00000855
Iteration 181/1000 | Loss: 0.00000855
Iteration 182/1000 | Loss: 0.00000854
Iteration 183/1000 | Loss: 0.00000854
Iteration 184/1000 | Loss: 0.00000854
Iteration 185/1000 | Loss: 0.00000853
Iteration 186/1000 | Loss: 0.00000853
Iteration 187/1000 | Loss: 0.00000853
Iteration 188/1000 | Loss: 0.00000853
Iteration 189/1000 | Loss: 0.00000853
Iteration 190/1000 | Loss: 0.00000852
Iteration 191/1000 | Loss: 0.00000852
Iteration 192/1000 | Loss: 0.00000852
Iteration 193/1000 | Loss: 0.00000852
Iteration 194/1000 | Loss: 0.00000852
Iteration 195/1000 | Loss: 0.00000852
Iteration 196/1000 | Loss: 0.00000852
Iteration 197/1000 | Loss: 0.00000852
Iteration 198/1000 | Loss: 0.00000852
Iteration 199/1000 | Loss: 0.00000852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [8.516408342984505e-06, 8.516408342984505e-06, 8.516408342984505e-06, 8.516408342984505e-06, 8.516408342984505e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.516408342984505e-06

Optimization complete. Final v2v error: 2.5028727054595947 mm

Highest mean error: 2.6352334022521973 mm for frame 146

Lowest mean error: 2.3241937160491943 mm for frame 254

Saving results

Total time: 45.9715793132782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011336
Iteration 2/25 | Loss: 0.01011336
Iteration 3/25 | Loss: 0.01011336
Iteration 4/25 | Loss: 0.01011336
Iteration 5/25 | Loss: 0.01011336
Iteration 6/25 | Loss: 0.01011335
Iteration 7/25 | Loss: 0.01011335
Iteration 8/25 | Loss: 0.01011335
Iteration 9/25 | Loss: 0.00237929
Iteration 10/25 | Loss: 0.00158782
Iteration 11/25 | Loss: 0.00139865
Iteration 12/25 | Loss: 0.00131422
Iteration 13/25 | Loss: 0.00130863
Iteration 14/25 | Loss: 0.00128758
Iteration 15/25 | Loss: 0.00123933
Iteration 16/25 | Loss: 0.00123491
Iteration 17/25 | Loss: 0.00119466
Iteration 18/25 | Loss: 0.00117873
Iteration 19/25 | Loss: 0.00117237
Iteration 20/25 | Loss: 0.00115986
Iteration 21/25 | Loss: 0.00115657
Iteration 22/25 | Loss: 0.00115784
Iteration 23/25 | Loss: 0.00115486
Iteration 24/25 | Loss: 0.00115245
Iteration 25/25 | Loss: 0.00115606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35609794
Iteration 2/25 | Loss: 0.00149330
Iteration 3/25 | Loss: 0.00109202
Iteration 4/25 | Loss: 0.00109202
Iteration 5/25 | Loss: 0.00109202
Iteration 6/25 | Loss: 0.00109202
Iteration 7/25 | Loss: 0.00109202
Iteration 8/25 | Loss: 0.00109202
Iteration 9/25 | Loss: 0.00109202
Iteration 10/25 | Loss: 0.00109202
Iteration 11/25 | Loss: 0.00109202
Iteration 12/25 | Loss: 0.00109202
Iteration 13/25 | Loss: 0.00109202
Iteration 14/25 | Loss: 0.00109202
Iteration 15/25 | Loss: 0.00109202
Iteration 16/25 | Loss: 0.00109202
Iteration 17/25 | Loss: 0.00109202
Iteration 18/25 | Loss: 0.00109202
Iteration 19/25 | Loss: 0.00109202
Iteration 20/25 | Loss: 0.00109202
Iteration 21/25 | Loss: 0.00109202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010920161148533225, 0.0010920161148533225, 0.0010920161148533225, 0.0010920161148533225, 0.0010920161148533225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010920161148533225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109202
Iteration 2/1000 | Loss: 0.00004973
Iteration 3/1000 | Loss: 0.00049747
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00007641
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00002206
Iteration 8/1000 | Loss: 0.00003105
Iteration 9/1000 | Loss: 0.00002578
Iteration 10/1000 | Loss: 0.00007790
Iteration 11/1000 | Loss: 0.00002324
Iteration 12/1000 | Loss: 0.00009773
Iteration 13/1000 | Loss: 0.00002132
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00010878
Iteration 16/1000 | Loss: 0.00002879
Iteration 17/1000 | Loss: 0.00001921
Iteration 18/1000 | Loss: 0.00003852
Iteration 19/1000 | Loss: 0.00001852
Iteration 20/1000 | Loss: 0.00005552
Iteration 21/1000 | Loss: 0.00003451
Iteration 22/1000 | Loss: 0.00002256
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001796
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001793
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00005378
Iteration 57/1000 | Loss: 0.00002195
Iteration 58/1000 | Loss: 0.00001783
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00002485
Iteration 61/1000 | Loss: 0.00001772
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001768
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001767
Iteration 72/1000 | Loss: 0.00001767
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001766
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001764
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001763
Iteration 87/1000 | Loss: 0.00001762
Iteration 88/1000 | Loss: 0.00001762
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001831
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001849
Iteration 94/1000 | Loss: 0.00001849
Iteration 95/1000 | Loss: 0.00005645
Iteration 96/1000 | Loss: 0.00001803
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00002270
Iteration 102/1000 | Loss: 0.00001753
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001752
Iteration 107/1000 | Loss: 0.00001752
Iteration 108/1000 | Loss: 0.00001752
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001752
Iteration 112/1000 | Loss: 0.00001751
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001751
Iteration 115/1000 | Loss: 0.00001751
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001751
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001750
Iteration 120/1000 | Loss: 0.00001750
Iteration 121/1000 | Loss: 0.00001750
Iteration 122/1000 | Loss: 0.00001750
Iteration 123/1000 | Loss: 0.00001750
Iteration 124/1000 | Loss: 0.00001750
Iteration 125/1000 | Loss: 0.00001750
Iteration 126/1000 | Loss: 0.00001750
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001748
Iteration 147/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.7483518604421988e-05, 1.7483518604421988e-05, 1.7483518604421988e-05, 1.7483518604421988e-05, 1.7483518604421988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7483518604421988e-05

Optimization complete. Final v2v error: 3.550849199295044 mm

Highest mean error: 5.504622459411621 mm for frame 105

Lowest mean error: 3.2329866886138916 mm for frame 111

Saving results

Total time: 108.15406966209412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801751
Iteration 2/25 | Loss: 0.00131645
Iteration 3/25 | Loss: 0.00116682
Iteration 4/25 | Loss: 0.00114677
Iteration 5/25 | Loss: 0.00114241
Iteration 6/25 | Loss: 0.00114197
Iteration 7/25 | Loss: 0.00114197
Iteration 8/25 | Loss: 0.00114197
Iteration 9/25 | Loss: 0.00114197
Iteration 10/25 | Loss: 0.00114197
Iteration 11/25 | Loss: 0.00114197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011419733054935932, 0.0011419733054935932, 0.0011419733054935932, 0.0011419733054935932, 0.0011419733054935932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011419733054935932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31568182
Iteration 2/25 | Loss: 0.00074206
Iteration 3/25 | Loss: 0.00074202
Iteration 4/25 | Loss: 0.00074202
Iteration 5/25 | Loss: 0.00074202
Iteration 6/25 | Loss: 0.00074202
Iteration 7/25 | Loss: 0.00074202
Iteration 8/25 | Loss: 0.00074202
Iteration 9/25 | Loss: 0.00074202
Iteration 10/25 | Loss: 0.00074202
Iteration 11/25 | Loss: 0.00074202
Iteration 12/25 | Loss: 0.00074202
Iteration 13/25 | Loss: 0.00074202
Iteration 14/25 | Loss: 0.00074202
Iteration 15/25 | Loss: 0.00074202
Iteration 16/25 | Loss: 0.00074202
Iteration 17/25 | Loss: 0.00074202
Iteration 18/25 | Loss: 0.00074202
Iteration 19/25 | Loss: 0.00074202
Iteration 20/25 | Loss: 0.00074202
Iteration 21/25 | Loss: 0.00074202
Iteration 22/25 | Loss: 0.00074202
Iteration 23/25 | Loss: 0.00074202
Iteration 24/25 | Loss: 0.00074202
Iteration 25/25 | Loss: 0.00074202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007420162437483668, 0.0007420162437483668, 0.0007420162437483668, 0.0007420162437483668, 0.0007420162437483668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007420162437483668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074202
Iteration 2/1000 | Loss: 0.00003132
Iteration 3/1000 | Loss: 0.00002175
Iteration 4/1000 | Loss: 0.00001871
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001612
Iteration 8/1000 | Loss: 0.00001569
Iteration 9/1000 | Loss: 0.00001543
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001467
Iteration 14/1000 | Loss: 0.00001466
Iteration 15/1000 | Loss: 0.00001466
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001443
Iteration 32/1000 | Loss: 0.00001442
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001428
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001419
Iteration 78/1000 | Loss: 0.00001419
Iteration 79/1000 | Loss: 0.00001419
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001417
Iteration 83/1000 | Loss: 0.00001417
Iteration 84/1000 | Loss: 0.00001417
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001416
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001414
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001414
Iteration 99/1000 | Loss: 0.00001414
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001413
Iteration 102/1000 | Loss: 0.00001413
Iteration 103/1000 | Loss: 0.00001413
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001412
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001412
Iteration 115/1000 | Loss: 0.00001412
Iteration 116/1000 | Loss: 0.00001412
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001410
Iteration 130/1000 | Loss: 0.00001410
Iteration 131/1000 | Loss: 0.00001410
Iteration 132/1000 | Loss: 0.00001410
Iteration 133/1000 | Loss: 0.00001410
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001409
Iteration 137/1000 | Loss: 0.00001409
Iteration 138/1000 | Loss: 0.00001409
Iteration 139/1000 | Loss: 0.00001409
Iteration 140/1000 | Loss: 0.00001409
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001409
Iteration 146/1000 | Loss: 0.00001408
Iteration 147/1000 | Loss: 0.00001408
Iteration 148/1000 | Loss: 0.00001408
Iteration 149/1000 | Loss: 0.00001408
Iteration 150/1000 | Loss: 0.00001408
Iteration 151/1000 | Loss: 0.00001408
Iteration 152/1000 | Loss: 0.00001408
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001407
Iteration 156/1000 | Loss: 0.00001407
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Iteration 168/1000 | Loss: 0.00001405
Iteration 169/1000 | Loss: 0.00001405
Iteration 170/1000 | Loss: 0.00001405
Iteration 171/1000 | Loss: 0.00001405
Iteration 172/1000 | Loss: 0.00001405
Iteration 173/1000 | Loss: 0.00001405
Iteration 174/1000 | Loss: 0.00001405
Iteration 175/1000 | Loss: 0.00001405
Iteration 176/1000 | Loss: 0.00001405
Iteration 177/1000 | Loss: 0.00001405
Iteration 178/1000 | Loss: 0.00001405
Iteration 179/1000 | Loss: 0.00001404
Iteration 180/1000 | Loss: 0.00001404
Iteration 181/1000 | Loss: 0.00001404
Iteration 182/1000 | Loss: 0.00001404
Iteration 183/1000 | Loss: 0.00001404
Iteration 184/1000 | Loss: 0.00001404
Iteration 185/1000 | Loss: 0.00001404
Iteration 186/1000 | Loss: 0.00001404
Iteration 187/1000 | Loss: 0.00001404
Iteration 188/1000 | Loss: 0.00001404
Iteration 189/1000 | Loss: 0.00001404
Iteration 190/1000 | Loss: 0.00001404
Iteration 191/1000 | Loss: 0.00001403
Iteration 192/1000 | Loss: 0.00001403
Iteration 193/1000 | Loss: 0.00001403
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001402
Iteration 199/1000 | Loss: 0.00001402
Iteration 200/1000 | Loss: 0.00001402
Iteration 201/1000 | Loss: 0.00001402
Iteration 202/1000 | Loss: 0.00001402
Iteration 203/1000 | Loss: 0.00001402
Iteration 204/1000 | Loss: 0.00001402
Iteration 205/1000 | Loss: 0.00001402
Iteration 206/1000 | Loss: 0.00001402
Iteration 207/1000 | Loss: 0.00001401
Iteration 208/1000 | Loss: 0.00001401
Iteration 209/1000 | Loss: 0.00001401
Iteration 210/1000 | Loss: 0.00001401
Iteration 211/1000 | Loss: 0.00001401
Iteration 212/1000 | Loss: 0.00001401
Iteration 213/1000 | Loss: 0.00001400
Iteration 214/1000 | Loss: 0.00001400
Iteration 215/1000 | Loss: 0.00001400
Iteration 216/1000 | Loss: 0.00001400
Iteration 217/1000 | Loss: 0.00001400
Iteration 218/1000 | Loss: 0.00001400
Iteration 219/1000 | Loss: 0.00001400
Iteration 220/1000 | Loss: 0.00001400
Iteration 221/1000 | Loss: 0.00001399
Iteration 222/1000 | Loss: 0.00001399
Iteration 223/1000 | Loss: 0.00001399
Iteration 224/1000 | Loss: 0.00001399
Iteration 225/1000 | Loss: 0.00001399
Iteration 226/1000 | Loss: 0.00001399
Iteration 227/1000 | Loss: 0.00001399
Iteration 228/1000 | Loss: 0.00001399
Iteration 229/1000 | Loss: 0.00001399
Iteration 230/1000 | Loss: 0.00001399
Iteration 231/1000 | Loss: 0.00001399
Iteration 232/1000 | Loss: 0.00001399
Iteration 233/1000 | Loss: 0.00001399
Iteration 234/1000 | Loss: 0.00001399
Iteration 235/1000 | Loss: 0.00001398
Iteration 236/1000 | Loss: 0.00001398
Iteration 237/1000 | Loss: 0.00001398
Iteration 238/1000 | Loss: 0.00001398
Iteration 239/1000 | Loss: 0.00001398
Iteration 240/1000 | Loss: 0.00001398
Iteration 241/1000 | Loss: 0.00001398
Iteration 242/1000 | Loss: 0.00001398
Iteration 243/1000 | Loss: 0.00001398
Iteration 244/1000 | Loss: 0.00001398
Iteration 245/1000 | Loss: 0.00001397
Iteration 246/1000 | Loss: 0.00001397
Iteration 247/1000 | Loss: 0.00001397
Iteration 248/1000 | Loss: 0.00001397
Iteration 249/1000 | Loss: 0.00001397
Iteration 250/1000 | Loss: 0.00001397
Iteration 251/1000 | Loss: 0.00001397
Iteration 252/1000 | Loss: 0.00001397
Iteration 253/1000 | Loss: 0.00001397
Iteration 254/1000 | Loss: 0.00001397
Iteration 255/1000 | Loss: 0.00001397
Iteration 256/1000 | Loss: 0.00001397
Iteration 257/1000 | Loss: 0.00001396
Iteration 258/1000 | Loss: 0.00001396
Iteration 259/1000 | Loss: 0.00001396
Iteration 260/1000 | Loss: 0.00001396
Iteration 261/1000 | Loss: 0.00001396
Iteration 262/1000 | Loss: 0.00001396
Iteration 263/1000 | Loss: 0.00001396
Iteration 264/1000 | Loss: 0.00001396
Iteration 265/1000 | Loss: 0.00001396
Iteration 266/1000 | Loss: 0.00001396
Iteration 267/1000 | Loss: 0.00001396
Iteration 268/1000 | Loss: 0.00001395
Iteration 269/1000 | Loss: 0.00001395
Iteration 270/1000 | Loss: 0.00001395
Iteration 271/1000 | Loss: 0.00001395
Iteration 272/1000 | Loss: 0.00001395
Iteration 273/1000 | Loss: 0.00001395
Iteration 274/1000 | Loss: 0.00001395
Iteration 275/1000 | Loss: 0.00001395
Iteration 276/1000 | Loss: 0.00001395
Iteration 277/1000 | Loss: 0.00001395
Iteration 278/1000 | Loss: 0.00001395
Iteration 279/1000 | Loss: 0.00001395
Iteration 280/1000 | Loss: 0.00001395
Iteration 281/1000 | Loss: 0.00001395
Iteration 282/1000 | Loss: 0.00001395
Iteration 283/1000 | Loss: 0.00001395
Iteration 284/1000 | Loss: 0.00001395
Iteration 285/1000 | Loss: 0.00001395
Iteration 286/1000 | Loss: 0.00001395
Iteration 287/1000 | Loss: 0.00001395
Iteration 288/1000 | Loss: 0.00001395
Iteration 289/1000 | Loss: 0.00001395
Iteration 290/1000 | Loss: 0.00001394
Iteration 291/1000 | Loss: 0.00001394
Iteration 292/1000 | Loss: 0.00001394
Iteration 293/1000 | Loss: 0.00001394
Iteration 294/1000 | Loss: 0.00001394
Iteration 295/1000 | Loss: 0.00001394
Iteration 296/1000 | Loss: 0.00001394
Iteration 297/1000 | Loss: 0.00001394
Iteration 298/1000 | Loss: 0.00001394
Iteration 299/1000 | Loss: 0.00001394
Iteration 300/1000 | Loss: 0.00001394
Iteration 301/1000 | Loss: 0.00001394
Iteration 302/1000 | Loss: 0.00001394
Iteration 303/1000 | Loss: 0.00001394
Iteration 304/1000 | Loss: 0.00001394
Iteration 305/1000 | Loss: 0.00001394
Iteration 306/1000 | Loss: 0.00001394
Iteration 307/1000 | Loss: 0.00001394
Iteration 308/1000 | Loss: 0.00001394
Iteration 309/1000 | Loss: 0.00001394
Iteration 310/1000 | Loss: 0.00001394
Iteration 311/1000 | Loss: 0.00001394
Iteration 312/1000 | Loss: 0.00001394
Iteration 313/1000 | Loss: 0.00001394
Iteration 314/1000 | Loss: 0.00001394
Iteration 315/1000 | Loss: 0.00001394
Iteration 316/1000 | Loss: 0.00001394
Iteration 317/1000 | Loss: 0.00001394
Iteration 318/1000 | Loss: 0.00001394
Iteration 319/1000 | Loss: 0.00001394
Iteration 320/1000 | Loss: 0.00001394
Iteration 321/1000 | Loss: 0.00001394
Iteration 322/1000 | Loss: 0.00001394
Iteration 323/1000 | Loss: 0.00001394
Iteration 324/1000 | Loss: 0.00001394
Iteration 325/1000 | Loss: 0.00001394
Iteration 326/1000 | Loss: 0.00001394
Iteration 327/1000 | Loss: 0.00001394
Iteration 328/1000 | Loss: 0.00001394
Iteration 329/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [1.3935146853327751e-05, 1.3935146853327751e-05, 1.3935146853327751e-05, 1.3935146853327751e-05, 1.3935146853327751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3935146853327751e-05

Optimization complete. Final v2v error: 3.0968496799468994 mm

Highest mean error: 4.464896202087402 mm for frame 6

Lowest mean error: 2.5889174938201904 mm for frame 188

Saving results

Total time: 55.17706537246704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445168
Iteration 2/25 | Loss: 0.00116029
Iteration 3/25 | Loss: 0.00106602
Iteration 4/25 | Loss: 0.00105730
Iteration 5/25 | Loss: 0.00105564
Iteration 6/25 | Loss: 0.00105516
Iteration 7/25 | Loss: 0.00105516
Iteration 8/25 | Loss: 0.00105516
Iteration 9/25 | Loss: 0.00105516
Iteration 10/25 | Loss: 0.00105516
Iteration 11/25 | Loss: 0.00105516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010551577433943748, 0.0010551577433943748, 0.0010551577433943748, 0.0010551577433943748, 0.0010551577433943748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010551577433943748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29675710
Iteration 2/25 | Loss: 0.00066956
Iteration 3/25 | Loss: 0.00066955
Iteration 4/25 | Loss: 0.00066955
Iteration 5/25 | Loss: 0.00066955
Iteration 6/25 | Loss: 0.00066955
Iteration 7/25 | Loss: 0.00066955
Iteration 8/25 | Loss: 0.00066955
Iteration 9/25 | Loss: 0.00066955
Iteration 10/25 | Loss: 0.00066955
Iteration 11/25 | Loss: 0.00066955
Iteration 12/25 | Loss: 0.00066955
Iteration 13/25 | Loss: 0.00066955
Iteration 14/25 | Loss: 0.00066955
Iteration 15/25 | Loss: 0.00066955
Iteration 16/25 | Loss: 0.00066955
Iteration 17/25 | Loss: 0.00066955
Iteration 18/25 | Loss: 0.00066955
Iteration 19/25 | Loss: 0.00066955
Iteration 20/25 | Loss: 0.00066955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000669550325255841, 0.000669550325255841, 0.000669550325255841, 0.000669550325255841, 0.000669550325255841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000669550325255841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066955
Iteration 2/1000 | Loss: 0.00002034
Iteration 3/1000 | Loss: 0.00001435
Iteration 4/1000 | Loss: 0.00001233
Iteration 5/1000 | Loss: 0.00001159
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001053
Iteration 8/1000 | Loss: 0.00001028
Iteration 9/1000 | Loss: 0.00001021
Iteration 10/1000 | Loss: 0.00001020
Iteration 11/1000 | Loss: 0.00001019
Iteration 12/1000 | Loss: 0.00001016
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001006
Iteration 15/1000 | Loss: 0.00000999
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000991
Iteration 18/1000 | Loss: 0.00000979
Iteration 19/1000 | Loss: 0.00000976
Iteration 20/1000 | Loss: 0.00000971
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000970
Iteration 23/1000 | Loss: 0.00000970
Iteration 24/1000 | Loss: 0.00000969
Iteration 25/1000 | Loss: 0.00000969
Iteration 26/1000 | Loss: 0.00000968
Iteration 27/1000 | Loss: 0.00000967
Iteration 28/1000 | Loss: 0.00000966
Iteration 29/1000 | Loss: 0.00000966
Iteration 30/1000 | Loss: 0.00000965
Iteration 31/1000 | Loss: 0.00000964
Iteration 32/1000 | Loss: 0.00000963
Iteration 33/1000 | Loss: 0.00000962
Iteration 34/1000 | Loss: 0.00000962
Iteration 35/1000 | Loss: 0.00000962
Iteration 36/1000 | Loss: 0.00000961
Iteration 37/1000 | Loss: 0.00000960
Iteration 38/1000 | Loss: 0.00000960
Iteration 39/1000 | Loss: 0.00000959
Iteration 40/1000 | Loss: 0.00000958
Iteration 41/1000 | Loss: 0.00000958
Iteration 42/1000 | Loss: 0.00000957
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000953
Iteration 45/1000 | Loss: 0.00000951
Iteration 46/1000 | Loss: 0.00000950
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000949
Iteration 49/1000 | Loss: 0.00000948
Iteration 50/1000 | Loss: 0.00000946
Iteration 51/1000 | Loss: 0.00000941
Iteration 52/1000 | Loss: 0.00000941
Iteration 53/1000 | Loss: 0.00000940
Iteration 54/1000 | Loss: 0.00000938
Iteration 55/1000 | Loss: 0.00000936
Iteration 56/1000 | Loss: 0.00000935
Iteration 57/1000 | Loss: 0.00000934
Iteration 58/1000 | Loss: 0.00000934
Iteration 59/1000 | Loss: 0.00000933
Iteration 60/1000 | Loss: 0.00000933
Iteration 61/1000 | Loss: 0.00000933
Iteration 62/1000 | Loss: 0.00000933
Iteration 63/1000 | Loss: 0.00000932
Iteration 64/1000 | Loss: 0.00000932
Iteration 65/1000 | Loss: 0.00000932
Iteration 66/1000 | Loss: 0.00000932
Iteration 67/1000 | Loss: 0.00000931
Iteration 68/1000 | Loss: 0.00000931
Iteration 69/1000 | Loss: 0.00000931
Iteration 70/1000 | Loss: 0.00000930
Iteration 71/1000 | Loss: 0.00000930
Iteration 72/1000 | Loss: 0.00000930
Iteration 73/1000 | Loss: 0.00000930
Iteration 74/1000 | Loss: 0.00000930
Iteration 75/1000 | Loss: 0.00000929
Iteration 76/1000 | Loss: 0.00000929
Iteration 77/1000 | Loss: 0.00000929
Iteration 78/1000 | Loss: 0.00000929
Iteration 79/1000 | Loss: 0.00000928
Iteration 80/1000 | Loss: 0.00000928
Iteration 81/1000 | Loss: 0.00000928
Iteration 82/1000 | Loss: 0.00000928
Iteration 83/1000 | Loss: 0.00000928
Iteration 84/1000 | Loss: 0.00000927
Iteration 85/1000 | Loss: 0.00000927
Iteration 86/1000 | Loss: 0.00000927
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000925
Iteration 90/1000 | Loss: 0.00000925
Iteration 91/1000 | Loss: 0.00000925
Iteration 92/1000 | Loss: 0.00000924
Iteration 93/1000 | Loss: 0.00000924
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000922
Iteration 96/1000 | Loss: 0.00000922
Iteration 97/1000 | Loss: 0.00000922
Iteration 98/1000 | Loss: 0.00000922
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000919
Iteration 102/1000 | Loss: 0.00000918
Iteration 103/1000 | Loss: 0.00000918
Iteration 104/1000 | Loss: 0.00000917
Iteration 105/1000 | Loss: 0.00000917
Iteration 106/1000 | Loss: 0.00000916
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000915
Iteration 109/1000 | Loss: 0.00000915
Iteration 110/1000 | Loss: 0.00000915
Iteration 111/1000 | Loss: 0.00000914
Iteration 112/1000 | Loss: 0.00000914
Iteration 113/1000 | Loss: 0.00000913
Iteration 114/1000 | Loss: 0.00000913
Iteration 115/1000 | Loss: 0.00000913
Iteration 116/1000 | Loss: 0.00000913
Iteration 117/1000 | Loss: 0.00000913
Iteration 118/1000 | Loss: 0.00000913
Iteration 119/1000 | Loss: 0.00000913
Iteration 120/1000 | Loss: 0.00000913
Iteration 121/1000 | Loss: 0.00000913
Iteration 122/1000 | Loss: 0.00000913
Iteration 123/1000 | Loss: 0.00000913
Iteration 124/1000 | Loss: 0.00000913
Iteration 125/1000 | Loss: 0.00000912
Iteration 126/1000 | Loss: 0.00000912
Iteration 127/1000 | Loss: 0.00000912
Iteration 128/1000 | Loss: 0.00000912
Iteration 129/1000 | Loss: 0.00000912
Iteration 130/1000 | Loss: 0.00000912
Iteration 131/1000 | Loss: 0.00000911
Iteration 132/1000 | Loss: 0.00000911
Iteration 133/1000 | Loss: 0.00000911
Iteration 134/1000 | Loss: 0.00000911
Iteration 135/1000 | Loss: 0.00000911
Iteration 136/1000 | Loss: 0.00000911
Iteration 137/1000 | Loss: 0.00000911
Iteration 138/1000 | Loss: 0.00000911
Iteration 139/1000 | Loss: 0.00000911
Iteration 140/1000 | Loss: 0.00000911
Iteration 141/1000 | Loss: 0.00000911
Iteration 142/1000 | Loss: 0.00000910
Iteration 143/1000 | Loss: 0.00000910
Iteration 144/1000 | Loss: 0.00000910
Iteration 145/1000 | Loss: 0.00000910
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000910
Iteration 154/1000 | Loss: 0.00000909
Iteration 155/1000 | Loss: 0.00000909
Iteration 156/1000 | Loss: 0.00000909
Iteration 157/1000 | Loss: 0.00000909
Iteration 158/1000 | Loss: 0.00000909
Iteration 159/1000 | Loss: 0.00000909
Iteration 160/1000 | Loss: 0.00000909
Iteration 161/1000 | Loss: 0.00000909
Iteration 162/1000 | Loss: 0.00000909
Iteration 163/1000 | Loss: 0.00000909
Iteration 164/1000 | Loss: 0.00000909
Iteration 165/1000 | Loss: 0.00000909
Iteration 166/1000 | Loss: 0.00000909
Iteration 167/1000 | Loss: 0.00000908
Iteration 168/1000 | Loss: 0.00000908
Iteration 169/1000 | Loss: 0.00000908
Iteration 170/1000 | Loss: 0.00000908
Iteration 171/1000 | Loss: 0.00000908
Iteration 172/1000 | Loss: 0.00000908
Iteration 173/1000 | Loss: 0.00000908
Iteration 174/1000 | Loss: 0.00000908
Iteration 175/1000 | Loss: 0.00000908
Iteration 176/1000 | Loss: 0.00000908
Iteration 177/1000 | Loss: 0.00000907
Iteration 178/1000 | Loss: 0.00000907
Iteration 179/1000 | Loss: 0.00000907
Iteration 180/1000 | Loss: 0.00000907
Iteration 181/1000 | Loss: 0.00000907
Iteration 182/1000 | Loss: 0.00000907
Iteration 183/1000 | Loss: 0.00000907
Iteration 184/1000 | Loss: 0.00000907
Iteration 185/1000 | Loss: 0.00000907
Iteration 186/1000 | Loss: 0.00000907
Iteration 187/1000 | Loss: 0.00000907
Iteration 188/1000 | Loss: 0.00000906
Iteration 189/1000 | Loss: 0.00000906
Iteration 190/1000 | Loss: 0.00000906
Iteration 191/1000 | Loss: 0.00000906
Iteration 192/1000 | Loss: 0.00000906
Iteration 193/1000 | Loss: 0.00000906
Iteration 194/1000 | Loss: 0.00000906
Iteration 195/1000 | Loss: 0.00000906
Iteration 196/1000 | Loss: 0.00000906
Iteration 197/1000 | Loss: 0.00000905
Iteration 198/1000 | Loss: 0.00000905
Iteration 199/1000 | Loss: 0.00000905
Iteration 200/1000 | Loss: 0.00000905
Iteration 201/1000 | Loss: 0.00000905
Iteration 202/1000 | Loss: 0.00000905
Iteration 203/1000 | Loss: 0.00000905
Iteration 204/1000 | Loss: 0.00000905
Iteration 205/1000 | Loss: 0.00000905
Iteration 206/1000 | Loss: 0.00000905
Iteration 207/1000 | Loss: 0.00000905
Iteration 208/1000 | Loss: 0.00000905
Iteration 209/1000 | Loss: 0.00000905
Iteration 210/1000 | Loss: 0.00000904
Iteration 211/1000 | Loss: 0.00000904
Iteration 212/1000 | Loss: 0.00000904
Iteration 213/1000 | Loss: 0.00000904
Iteration 214/1000 | Loss: 0.00000904
Iteration 215/1000 | Loss: 0.00000904
Iteration 216/1000 | Loss: 0.00000903
Iteration 217/1000 | Loss: 0.00000903
Iteration 218/1000 | Loss: 0.00000903
Iteration 219/1000 | Loss: 0.00000902
Iteration 220/1000 | Loss: 0.00000902
Iteration 221/1000 | Loss: 0.00000902
Iteration 222/1000 | Loss: 0.00000902
Iteration 223/1000 | Loss: 0.00000901
Iteration 224/1000 | Loss: 0.00000901
Iteration 225/1000 | Loss: 0.00000901
Iteration 226/1000 | Loss: 0.00000900
Iteration 227/1000 | Loss: 0.00000900
Iteration 228/1000 | Loss: 0.00000899
Iteration 229/1000 | Loss: 0.00000899
Iteration 230/1000 | Loss: 0.00000899
Iteration 231/1000 | Loss: 0.00000899
Iteration 232/1000 | Loss: 0.00000899
Iteration 233/1000 | Loss: 0.00000899
Iteration 234/1000 | Loss: 0.00000898
Iteration 235/1000 | Loss: 0.00000898
Iteration 236/1000 | Loss: 0.00000898
Iteration 237/1000 | Loss: 0.00000897
Iteration 238/1000 | Loss: 0.00000897
Iteration 239/1000 | Loss: 0.00000897
Iteration 240/1000 | Loss: 0.00000897
Iteration 241/1000 | Loss: 0.00000897
Iteration 242/1000 | Loss: 0.00000897
Iteration 243/1000 | Loss: 0.00000897
Iteration 244/1000 | Loss: 0.00000896
Iteration 245/1000 | Loss: 0.00000896
Iteration 246/1000 | Loss: 0.00000896
Iteration 247/1000 | Loss: 0.00000896
Iteration 248/1000 | Loss: 0.00000896
Iteration 249/1000 | Loss: 0.00000896
Iteration 250/1000 | Loss: 0.00000896
Iteration 251/1000 | Loss: 0.00000896
Iteration 252/1000 | Loss: 0.00000895
Iteration 253/1000 | Loss: 0.00000895
Iteration 254/1000 | Loss: 0.00000895
Iteration 255/1000 | Loss: 0.00000895
Iteration 256/1000 | Loss: 0.00000895
Iteration 257/1000 | Loss: 0.00000895
Iteration 258/1000 | Loss: 0.00000895
Iteration 259/1000 | Loss: 0.00000895
Iteration 260/1000 | Loss: 0.00000895
Iteration 261/1000 | Loss: 0.00000895
Iteration 262/1000 | Loss: 0.00000895
Iteration 263/1000 | Loss: 0.00000894
Iteration 264/1000 | Loss: 0.00000894
Iteration 265/1000 | Loss: 0.00000894
Iteration 266/1000 | Loss: 0.00000894
Iteration 267/1000 | Loss: 0.00000894
Iteration 268/1000 | Loss: 0.00000894
Iteration 269/1000 | Loss: 0.00000894
Iteration 270/1000 | Loss: 0.00000894
Iteration 271/1000 | Loss: 0.00000894
Iteration 272/1000 | Loss: 0.00000894
Iteration 273/1000 | Loss: 0.00000894
Iteration 274/1000 | Loss: 0.00000894
Iteration 275/1000 | Loss: 0.00000894
Iteration 276/1000 | Loss: 0.00000894
Iteration 277/1000 | Loss: 0.00000894
Iteration 278/1000 | Loss: 0.00000894
Iteration 279/1000 | Loss: 0.00000894
Iteration 280/1000 | Loss: 0.00000894
Iteration 281/1000 | Loss: 0.00000894
Iteration 282/1000 | Loss: 0.00000894
Iteration 283/1000 | Loss: 0.00000894
Iteration 284/1000 | Loss: 0.00000894
Iteration 285/1000 | Loss: 0.00000894
Iteration 286/1000 | Loss: 0.00000894
Iteration 287/1000 | Loss: 0.00000894
Iteration 288/1000 | Loss: 0.00000894
Iteration 289/1000 | Loss: 0.00000894
Iteration 290/1000 | Loss: 0.00000894
Iteration 291/1000 | Loss: 0.00000894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [8.93727792572463e-06, 8.93727792572463e-06, 8.93727792572463e-06, 8.93727792572463e-06, 8.93727792572463e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.93727792572463e-06

Optimization complete. Final v2v error: 2.568880558013916 mm

Highest mean error: 4.707638740539551 mm for frame 220

Lowest mean error: 2.336280584335327 mm for frame 53

Saving results

Total time: 54.88642144203186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011607
Iteration 2/25 | Loss: 0.00284902
Iteration 3/25 | Loss: 0.00176458
Iteration 4/25 | Loss: 0.00161647
Iteration 5/25 | Loss: 0.00148996
Iteration 6/25 | Loss: 0.00118016
Iteration 7/25 | Loss: 0.00116333
Iteration 8/25 | Loss: 0.00115860
Iteration 9/25 | Loss: 0.00115536
Iteration 10/25 | Loss: 0.00115430
Iteration 11/25 | Loss: 0.00115397
Iteration 12/25 | Loss: 0.00115392
Iteration 13/25 | Loss: 0.00115392
Iteration 14/25 | Loss: 0.00115391
Iteration 15/25 | Loss: 0.00115391
Iteration 16/25 | Loss: 0.00115391
Iteration 17/25 | Loss: 0.00115391
Iteration 18/25 | Loss: 0.00115391
Iteration 19/25 | Loss: 0.00115391
Iteration 20/25 | Loss: 0.00115391
Iteration 21/25 | Loss: 0.00115391
Iteration 22/25 | Loss: 0.00115391
Iteration 23/25 | Loss: 0.00115390
Iteration 24/25 | Loss: 0.00115390
Iteration 25/25 | Loss: 0.00115390

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31054151
Iteration 2/25 | Loss: 0.00069373
Iteration 3/25 | Loss: 0.00069373
Iteration 4/25 | Loss: 0.00069373
Iteration 5/25 | Loss: 0.00069373
Iteration 6/25 | Loss: 0.00069373
Iteration 7/25 | Loss: 0.00069373
Iteration 8/25 | Loss: 0.00069373
Iteration 9/25 | Loss: 0.00069373
Iteration 10/25 | Loss: 0.00069373
Iteration 11/25 | Loss: 0.00069373
Iteration 12/25 | Loss: 0.00069372
Iteration 13/25 | Loss: 0.00069372
Iteration 14/25 | Loss: 0.00069372
Iteration 15/25 | Loss: 0.00069372
Iteration 16/25 | Loss: 0.00069372
Iteration 17/25 | Loss: 0.00069372
Iteration 18/25 | Loss: 0.00069372
Iteration 19/25 | Loss: 0.00069372
Iteration 20/25 | Loss: 0.00069372
Iteration 21/25 | Loss: 0.00069372
Iteration 22/25 | Loss: 0.00069372
Iteration 23/25 | Loss: 0.00069372
Iteration 24/25 | Loss: 0.00069372
Iteration 25/25 | Loss: 0.00069372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069372
Iteration 2/1000 | Loss: 0.00003342
Iteration 3/1000 | Loss: 0.00002094
Iteration 4/1000 | Loss: 0.00001875
Iteration 5/1000 | Loss: 0.00001815
Iteration 6/1000 | Loss: 0.00001776
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001726
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00001684
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001662
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001656
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001656
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001653
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001648
Iteration 56/1000 | Loss: 0.00001648
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001647
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001646
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001644
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001643
Iteration 96/1000 | Loss: 0.00001643
Iteration 97/1000 | Loss: 0.00001643
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001643
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001643
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001642
Iteration 109/1000 | Loss: 0.00001642
Iteration 110/1000 | Loss: 0.00001642
Iteration 111/1000 | Loss: 0.00001642
Iteration 112/1000 | Loss: 0.00001642
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001642
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001642
Iteration 118/1000 | Loss: 0.00001642
Iteration 119/1000 | Loss: 0.00001642
Iteration 120/1000 | Loss: 0.00001642
Iteration 121/1000 | Loss: 0.00001642
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.6417370716226287e-05, 1.6417370716226287e-05, 1.6417370716226287e-05, 1.6417370716226287e-05, 1.6417370716226287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6417370716226287e-05

Optimization complete. Final v2v error: 3.373584508895874 mm

Highest mean error: 3.5341084003448486 mm for frame 124

Lowest mean error: 3.280880928039551 mm for frame 0

Saving results

Total time: 43.815483808517456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993505
Iteration 2/25 | Loss: 0.00184646
Iteration 3/25 | Loss: 0.00144435
Iteration 4/25 | Loss: 0.00138608
Iteration 5/25 | Loss: 0.00132960
Iteration 6/25 | Loss: 0.00129268
Iteration 7/25 | Loss: 0.00127768
Iteration 8/25 | Loss: 0.00124145
Iteration 9/25 | Loss: 0.00121985
Iteration 10/25 | Loss: 0.00120625
Iteration 11/25 | Loss: 0.00117751
Iteration 12/25 | Loss: 0.00118373
Iteration 13/25 | Loss: 0.00115771
Iteration 14/25 | Loss: 0.00116200
Iteration 15/25 | Loss: 0.00115161
Iteration 16/25 | Loss: 0.00113976
Iteration 17/25 | Loss: 0.00114192
Iteration 18/25 | Loss: 0.00113855
Iteration 19/25 | Loss: 0.00113875
Iteration 20/25 | Loss: 0.00113317
Iteration 21/25 | Loss: 0.00112828
Iteration 22/25 | Loss: 0.00112894
Iteration 23/25 | Loss: 0.00112795
Iteration 24/25 | Loss: 0.00112557
Iteration 25/25 | Loss: 0.00112399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34042549
Iteration 2/25 | Loss: 0.00110587
Iteration 3/25 | Loss: 0.00077171
Iteration 4/25 | Loss: 0.00077171
Iteration 5/25 | Loss: 0.00077171
Iteration 6/25 | Loss: 0.00077171
Iteration 7/25 | Loss: 0.00077171
Iteration 8/25 | Loss: 0.00077171
Iteration 9/25 | Loss: 0.00077171
Iteration 10/25 | Loss: 0.00077171
Iteration 11/25 | Loss: 0.00077171
Iteration 12/25 | Loss: 0.00077171
Iteration 13/25 | Loss: 0.00077171
Iteration 14/25 | Loss: 0.00077171
Iteration 15/25 | Loss: 0.00077171
Iteration 16/25 | Loss: 0.00077171
Iteration 17/25 | Loss: 0.00077171
Iteration 18/25 | Loss: 0.00077171
Iteration 19/25 | Loss: 0.00077171
Iteration 20/25 | Loss: 0.00077171
Iteration 21/25 | Loss: 0.00077171
Iteration 22/25 | Loss: 0.00077171
Iteration 23/25 | Loss: 0.00077171
Iteration 24/25 | Loss: 0.00077171
Iteration 25/25 | Loss: 0.00077171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077171
Iteration 2/1000 | Loss: 0.00036608
Iteration 3/1000 | Loss: 0.00051217
Iteration 4/1000 | Loss: 0.00036514
Iteration 5/1000 | Loss: 0.00012116
Iteration 6/1000 | Loss: 0.00036190
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001609
Iteration 9/1000 | Loss: 0.00004265
Iteration 10/1000 | Loss: 0.00004885
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00007020
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00002467
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00005727
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00008221
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00005263
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00002633
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001224
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001223
Iteration 73/1000 | Loss: 0.00001223
Iteration 74/1000 | Loss: 0.00001223
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001221
Iteration 87/1000 | Loss: 0.00001221
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001220
Iteration 94/1000 | Loss: 0.00001220
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001219
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001219
Iteration 107/1000 | Loss: 0.00001219
Iteration 108/1000 | Loss: 0.00001219
Iteration 109/1000 | Loss: 0.00001219
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001219
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001219
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.2187867469037883e-05, 1.2187867469037883e-05, 1.2187867469037883e-05, 1.2187867469037883e-05, 1.2187867469037883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2187867469037883e-05

Optimization complete. Final v2v error: 2.946187734603882 mm

Highest mean error: 4.096505641937256 mm for frame 68

Lowest mean error: 2.5245866775512695 mm for frame 30

Saving results

Total time: 90.2902979850769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710183
Iteration 2/25 | Loss: 0.00157320
Iteration 3/25 | Loss: 0.00131241
Iteration 4/25 | Loss: 0.00128276
Iteration 5/25 | Loss: 0.00127708
Iteration 6/25 | Loss: 0.00127686
Iteration 7/25 | Loss: 0.00127686
Iteration 8/25 | Loss: 0.00127686
Iteration 9/25 | Loss: 0.00127686
Iteration 10/25 | Loss: 0.00127686
Iteration 11/25 | Loss: 0.00127686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012768624583259225, 0.0012768624583259225, 0.0012768624583259225, 0.0012768624583259225, 0.0012768624583259225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012768624583259225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44049525
Iteration 2/25 | Loss: 0.00104524
Iteration 3/25 | Loss: 0.00104524
Iteration 4/25 | Loss: 0.00104524
Iteration 5/25 | Loss: 0.00104524
Iteration 6/25 | Loss: 0.00104524
Iteration 7/25 | Loss: 0.00104524
Iteration 8/25 | Loss: 0.00104524
Iteration 9/25 | Loss: 0.00104524
Iteration 10/25 | Loss: 0.00104524
Iteration 11/25 | Loss: 0.00104524
Iteration 12/25 | Loss: 0.00104524
Iteration 13/25 | Loss: 0.00104524
Iteration 14/25 | Loss: 0.00104524
Iteration 15/25 | Loss: 0.00104524
Iteration 16/25 | Loss: 0.00104524
Iteration 17/25 | Loss: 0.00104524
Iteration 18/25 | Loss: 0.00104524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00104523915797472, 0.00104523915797472, 0.00104523915797472, 0.00104523915797472, 0.00104523915797472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00104523915797472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104524
Iteration 2/1000 | Loss: 0.00006103
Iteration 3/1000 | Loss: 0.00003444
Iteration 4/1000 | Loss: 0.00002800
Iteration 5/1000 | Loss: 0.00002634
Iteration 6/1000 | Loss: 0.00002514
Iteration 7/1000 | Loss: 0.00002437
Iteration 8/1000 | Loss: 0.00002385
Iteration 9/1000 | Loss: 0.00002353
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002306
Iteration 12/1000 | Loss: 0.00002284
Iteration 13/1000 | Loss: 0.00002275
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00002260
Iteration 17/1000 | Loss: 0.00002260
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002251
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00002249
Iteration 24/1000 | Loss: 0.00002249
Iteration 25/1000 | Loss: 0.00002248
Iteration 26/1000 | Loss: 0.00002248
Iteration 27/1000 | Loss: 0.00002248
Iteration 28/1000 | Loss: 0.00002248
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002248
Iteration 31/1000 | Loss: 0.00002248
Iteration 32/1000 | Loss: 0.00002247
Iteration 33/1000 | Loss: 0.00002247
Iteration 34/1000 | Loss: 0.00002247
Iteration 35/1000 | Loss: 0.00002247
Iteration 36/1000 | Loss: 0.00002247
Iteration 37/1000 | Loss: 0.00002246
Iteration 38/1000 | Loss: 0.00002246
Iteration 39/1000 | Loss: 0.00002246
Iteration 40/1000 | Loss: 0.00002245
Iteration 41/1000 | Loss: 0.00002245
Iteration 42/1000 | Loss: 0.00002245
Iteration 43/1000 | Loss: 0.00002244
Iteration 44/1000 | Loss: 0.00002244
Iteration 45/1000 | Loss: 0.00002244
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00002243
Iteration 51/1000 | Loss: 0.00002243
Iteration 52/1000 | Loss: 0.00002243
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00002243
Iteration 55/1000 | Loss: 0.00002243
Iteration 56/1000 | Loss: 0.00002242
Iteration 57/1000 | Loss: 0.00002242
Iteration 58/1000 | Loss: 0.00002242
Iteration 59/1000 | Loss: 0.00002242
Iteration 60/1000 | Loss: 0.00002242
Iteration 61/1000 | Loss: 0.00002242
Iteration 62/1000 | Loss: 0.00002242
Iteration 63/1000 | Loss: 0.00002242
Iteration 64/1000 | Loss: 0.00002242
Iteration 65/1000 | Loss: 0.00002242
Iteration 66/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.2420919776777737e-05, 2.2420919776777737e-05, 2.2420919776777737e-05, 2.2420919776777737e-05, 2.2420919776777737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2420919776777737e-05

Optimization complete. Final v2v error: 3.9666075706481934 mm

Highest mean error: 4.582332134246826 mm for frame 51

Lowest mean error: 3.507248640060425 mm for frame 0

Saving results

Total time: 35.98035192489624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843429
Iteration 2/25 | Loss: 0.00134500
Iteration 3/25 | Loss: 0.00116618
Iteration 4/25 | Loss: 0.00114680
Iteration 5/25 | Loss: 0.00114102
Iteration 6/25 | Loss: 0.00114048
Iteration 7/25 | Loss: 0.00114048
Iteration 8/25 | Loss: 0.00114048
Iteration 9/25 | Loss: 0.00114048
Iteration 10/25 | Loss: 0.00114048
Iteration 11/25 | Loss: 0.00114048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011404847027733922, 0.0011404847027733922, 0.0011404847027733922, 0.0011404847027733922, 0.0011404847027733922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011404847027733922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90085477
Iteration 2/25 | Loss: 0.00046248
Iteration 3/25 | Loss: 0.00046248
Iteration 4/25 | Loss: 0.00046248
Iteration 5/25 | Loss: 0.00046248
Iteration 6/25 | Loss: 0.00046248
Iteration 7/25 | Loss: 0.00046248
Iteration 8/25 | Loss: 0.00046248
Iteration 9/25 | Loss: 0.00046248
Iteration 10/25 | Loss: 0.00046248
Iteration 11/25 | Loss: 0.00046248
Iteration 12/25 | Loss: 0.00046248
Iteration 13/25 | Loss: 0.00046248
Iteration 14/25 | Loss: 0.00046248
Iteration 15/25 | Loss: 0.00046248
Iteration 16/25 | Loss: 0.00046248
Iteration 17/25 | Loss: 0.00046248
Iteration 18/25 | Loss: 0.00046248
Iteration 19/25 | Loss: 0.00046248
Iteration 20/25 | Loss: 0.00046248
Iteration 21/25 | Loss: 0.00046248
Iteration 22/25 | Loss: 0.00046248
Iteration 23/25 | Loss: 0.00046248
Iteration 24/25 | Loss: 0.00046248
Iteration 25/25 | Loss: 0.00046248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046248
Iteration 2/1000 | Loss: 0.00003706
Iteration 3/1000 | Loss: 0.00002952
Iteration 4/1000 | Loss: 0.00002712
Iteration 5/1000 | Loss: 0.00002584
Iteration 6/1000 | Loss: 0.00002500
Iteration 7/1000 | Loss: 0.00002462
Iteration 8/1000 | Loss: 0.00002443
Iteration 9/1000 | Loss: 0.00002425
Iteration 10/1000 | Loss: 0.00002422
Iteration 11/1000 | Loss: 0.00002407
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002407
Iteration 14/1000 | Loss: 0.00002407
Iteration 15/1000 | Loss: 0.00002407
Iteration 16/1000 | Loss: 0.00002406
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002404
Iteration 19/1000 | Loss: 0.00002392
Iteration 20/1000 | Loss: 0.00002392
Iteration 21/1000 | Loss: 0.00002392
Iteration 22/1000 | Loss: 0.00002391
Iteration 23/1000 | Loss: 0.00002391
Iteration 24/1000 | Loss: 0.00002391
Iteration 25/1000 | Loss: 0.00002391
Iteration 26/1000 | Loss: 0.00002391
Iteration 27/1000 | Loss: 0.00002391
Iteration 28/1000 | Loss: 0.00002391
Iteration 29/1000 | Loss: 0.00002391
Iteration 30/1000 | Loss: 0.00002390
Iteration 31/1000 | Loss: 0.00002390
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002390
Iteration 34/1000 | Loss: 0.00002390
Iteration 35/1000 | Loss: 0.00002390
Iteration 36/1000 | Loss: 0.00002390
Iteration 37/1000 | Loss: 0.00002389
Iteration 38/1000 | Loss: 0.00002389
Iteration 39/1000 | Loss: 0.00002389
Iteration 40/1000 | Loss: 0.00002389
Iteration 41/1000 | Loss: 0.00002389
Iteration 42/1000 | Loss: 0.00002389
Iteration 43/1000 | Loss: 0.00002389
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002389
Iteration 46/1000 | Loss: 0.00002389
Iteration 47/1000 | Loss: 0.00002389
Iteration 48/1000 | Loss: 0.00002389
Iteration 49/1000 | Loss: 0.00002389
Iteration 50/1000 | Loss: 0.00002389
Iteration 51/1000 | Loss: 0.00002389
Iteration 52/1000 | Loss: 0.00002389
Iteration 53/1000 | Loss: 0.00002389
Iteration 54/1000 | Loss: 0.00002389
Iteration 55/1000 | Loss: 0.00002389
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00002389
Iteration 58/1000 | Loss: 0.00002389
Iteration 59/1000 | Loss: 0.00002389
Iteration 60/1000 | Loss: 0.00002389
Iteration 61/1000 | Loss: 0.00002389
Iteration 62/1000 | Loss: 0.00002389
Iteration 63/1000 | Loss: 0.00002389
Iteration 64/1000 | Loss: 0.00002389
Iteration 65/1000 | Loss: 0.00002389
Iteration 66/1000 | Loss: 0.00002389
Iteration 67/1000 | Loss: 0.00002389
Iteration 68/1000 | Loss: 0.00002389
Iteration 69/1000 | Loss: 0.00002389
Iteration 70/1000 | Loss: 0.00002389
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002389
Iteration 73/1000 | Loss: 0.00002389
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00002389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.3888091163826175e-05, 2.3888091163826175e-05, 2.3888091163826175e-05, 2.3888091163826175e-05, 2.3888091163826175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3888091163826175e-05

Optimization complete. Final v2v error: 4.122016429901123 mm

Highest mean error: 4.245085716247559 mm for frame 0

Lowest mean error: 4.027598857879639 mm for frame 111

Saving results

Total time: 26.606080532073975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826196
Iteration 2/25 | Loss: 0.00123012
Iteration 3/25 | Loss: 0.00111874
Iteration 4/25 | Loss: 0.00110869
Iteration 5/25 | Loss: 0.00110674
Iteration 6/25 | Loss: 0.00110674
Iteration 7/25 | Loss: 0.00110674
Iteration 8/25 | Loss: 0.00110674
Iteration 9/25 | Loss: 0.00110674
Iteration 10/25 | Loss: 0.00110674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011067435843870044, 0.0011067435843870044, 0.0011067435843870044, 0.0011067435843870044, 0.0011067435843870044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011067435843870044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72830915
Iteration 2/25 | Loss: 0.00088233
Iteration 3/25 | Loss: 0.00088233
Iteration 4/25 | Loss: 0.00088233
Iteration 5/25 | Loss: 0.00088233
Iteration 6/25 | Loss: 0.00088233
Iteration 7/25 | Loss: 0.00088233
Iteration 8/25 | Loss: 0.00088233
Iteration 9/25 | Loss: 0.00088233
Iteration 10/25 | Loss: 0.00088233
Iteration 11/25 | Loss: 0.00088232
Iteration 12/25 | Loss: 0.00088232
Iteration 13/25 | Loss: 0.00088232
Iteration 14/25 | Loss: 0.00088232
Iteration 15/25 | Loss: 0.00088232
Iteration 16/25 | Loss: 0.00088232
Iteration 17/25 | Loss: 0.00088232
Iteration 18/25 | Loss: 0.00088232
Iteration 19/25 | Loss: 0.00088232
Iteration 20/25 | Loss: 0.00088232
Iteration 21/25 | Loss: 0.00088232
Iteration 22/25 | Loss: 0.00088232
Iteration 23/25 | Loss: 0.00088232
Iteration 24/25 | Loss: 0.00088232
Iteration 25/25 | Loss: 0.00088232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088232
Iteration 2/1000 | Loss: 0.00002182
Iteration 3/1000 | Loss: 0.00001520
Iteration 4/1000 | Loss: 0.00001359
Iteration 5/1000 | Loss: 0.00001300
Iteration 6/1000 | Loss: 0.00001247
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001095
Iteration 18/1000 | Loss: 0.00001094
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001089
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001076
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001073
Iteration 33/1000 | Loss: 0.00001070
Iteration 34/1000 | Loss: 0.00001070
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001068
Iteration 48/1000 | Loss: 0.00001068
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001066
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001062
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001060
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001060
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001059
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001055
Iteration 120/1000 | Loss: 0.00001055
Iteration 121/1000 | Loss: 0.00001055
Iteration 122/1000 | Loss: 0.00001055
Iteration 123/1000 | Loss: 0.00001055
Iteration 124/1000 | Loss: 0.00001055
Iteration 125/1000 | Loss: 0.00001055
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001054
Iteration 132/1000 | Loss: 0.00001054
Iteration 133/1000 | Loss: 0.00001054
Iteration 134/1000 | Loss: 0.00001054
Iteration 135/1000 | Loss: 0.00001054
Iteration 136/1000 | Loss: 0.00001054
Iteration 137/1000 | Loss: 0.00001054
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001053
Iteration 142/1000 | Loss: 0.00001053
Iteration 143/1000 | Loss: 0.00001053
Iteration 144/1000 | Loss: 0.00001053
Iteration 145/1000 | Loss: 0.00001053
Iteration 146/1000 | Loss: 0.00001053
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00001053
Iteration 149/1000 | Loss: 0.00001053
Iteration 150/1000 | Loss: 0.00001053
Iteration 151/1000 | Loss: 0.00001053
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001052
Iteration 157/1000 | Loss: 0.00001052
Iteration 158/1000 | Loss: 0.00001052
Iteration 159/1000 | Loss: 0.00001052
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001052
Iteration 171/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.0518331691855565e-05, 1.0518331691855565e-05, 1.0518331691855565e-05, 1.0518331691855565e-05, 1.0518331691855565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0518331691855565e-05

Optimization complete. Final v2v error: 2.7742581367492676 mm

Highest mean error: 3.4807281494140625 mm for frame 115

Lowest mean error: 2.548610210418701 mm for frame 71

Saving results

Total time: 39.65602111816406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794969
Iteration 2/25 | Loss: 0.00116287
Iteration 3/25 | Loss: 0.00107701
Iteration 4/25 | Loss: 0.00106674
Iteration 5/25 | Loss: 0.00106480
Iteration 6/25 | Loss: 0.00106480
Iteration 7/25 | Loss: 0.00106480
Iteration 8/25 | Loss: 0.00106480
Iteration 9/25 | Loss: 0.00106480
Iteration 10/25 | Loss: 0.00106480
Iteration 11/25 | Loss: 0.00106480
Iteration 12/25 | Loss: 0.00106480
Iteration 13/25 | Loss: 0.00106480
Iteration 14/25 | Loss: 0.00106480
Iteration 15/25 | Loss: 0.00106480
Iteration 16/25 | Loss: 0.00106480
Iteration 17/25 | Loss: 0.00106480
Iteration 18/25 | Loss: 0.00106480
Iteration 19/25 | Loss: 0.00106480
Iteration 20/25 | Loss: 0.00106480
Iteration 21/25 | Loss: 0.00106480
Iteration 22/25 | Loss: 0.00106480
Iteration 23/25 | Loss: 0.00106480
Iteration 24/25 | Loss: 0.00106480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001064796932041645, 0.001064796932041645, 0.001064796932041645, 0.001064796932041645, 0.001064796932041645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001064796932041645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35265183
Iteration 2/25 | Loss: 0.00079399
Iteration 3/25 | Loss: 0.00079399
Iteration 4/25 | Loss: 0.00079399
Iteration 5/25 | Loss: 0.00079399
Iteration 6/25 | Loss: 0.00079399
Iteration 7/25 | Loss: 0.00079399
Iteration 8/25 | Loss: 0.00079399
Iteration 9/25 | Loss: 0.00079398
Iteration 10/25 | Loss: 0.00079398
Iteration 11/25 | Loss: 0.00079398
Iteration 12/25 | Loss: 0.00079398
Iteration 13/25 | Loss: 0.00079398
Iteration 14/25 | Loss: 0.00079398
Iteration 15/25 | Loss: 0.00079398
Iteration 16/25 | Loss: 0.00079398
Iteration 17/25 | Loss: 0.00079398
Iteration 18/25 | Loss: 0.00079398
Iteration 19/25 | Loss: 0.00079398
Iteration 20/25 | Loss: 0.00079398
Iteration 21/25 | Loss: 0.00079398
Iteration 22/25 | Loss: 0.00079398
Iteration 23/25 | Loss: 0.00079398
Iteration 24/25 | Loss: 0.00079398
Iteration 25/25 | Loss: 0.00079398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079398
Iteration 2/1000 | Loss: 0.00001901
Iteration 3/1000 | Loss: 0.00001261
Iteration 4/1000 | Loss: 0.00001119
Iteration 5/1000 | Loss: 0.00001049
Iteration 6/1000 | Loss: 0.00001006
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000951
Iteration 9/1000 | Loss: 0.00000926
Iteration 10/1000 | Loss: 0.00000913
Iteration 11/1000 | Loss: 0.00000902
Iteration 12/1000 | Loss: 0.00000899
Iteration 13/1000 | Loss: 0.00000898
Iteration 14/1000 | Loss: 0.00000897
Iteration 15/1000 | Loss: 0.00000897
Iteration 16/1000 | Loss: 0.00000896
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000895
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000893
Iteration 21/1000 | Loss: 0.00000892
Iteration 22/1000 | Loss: 0.00000891
Iteration 23/1000 | Loss: 0.00000890
Iteration 24/1000 | Loss: 0.00000889
Iteration 25/1000 | Loss: 0.00000888
Iteration 26/1000 | Loss: 0.00000886
Iteration 27/1000 | Loss: 0.00000885
Iteration 28/1000 | Loss: 0.00000885
Iteration 29/1000 | Loss: 0.00000884
Iteration 30/1000 | Loss: 0.00000884
Iteration 31/1000 | Loss: 0.00000884
Iteration 32/1000 | Loss: 0.00000884
Iteration 33/1000 | Loss: 0.00000883
Iteration 34/1000 | Loss: 0.00000883
Iteration 35/1000 | Loss: 0.00000882
Iteration 36/1000 | Loss: 0.00000882
Iteration 37/1000 | Loss: 0.00000881
Iteration 38/1000 | Loss: 0.00000881
Iteration 39/1000 | Loss: 0.00000881
Iteration 40/1000 | Loss: 0.00000881
Iteration 41/1000 | Loss: 0.00000881
Iteration 42/1000 | Loss: 0.00000880
Iteration 43/1000 | Loss: 0.00000880
Iteration 44/1000 | Loss: 0.00000880
Iteration 45/1000 | Loss: 0.00000880
Iteration 46/1000 | Loss: 0.00000880
Iteration 47/1000 | Loss: 0.00000880
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000879
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000877
Iteration 54/1000 | Loss: 0.00000877
Iteration 55/1000 | Loss: 0.00000877
Iteration 56/1000 | Loss: 0.00000876
Iteration 57/1000 | Loss: 0.00000876
Iteration 58/1000 | Loss: 0.00000876
Iteration 59/1000 | Loss: 0.00000876
Iteration 60/1000 | Loss: 0.00000876
Iteration 61/1000 | Loss: 0.00000876
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000876
Iteration 65/1000 | Loss: 0.00000876
Iteration 66/1000 | Loss: 0.00000875
Iteration 67/1000 | Loss: 0.00000875
Iteration 68/1000 | Loss: 0.00000874
Iteration 69/1000 | Loss: 0.00000874
Iteration 70/1000 | Loss: 0.00000874
Iteration 71/1000 | Loss: 0.00000873
Iteration 72/1000 | Loss: 0.00000873
Iteration 73/1000 | Loss: 0.00000873
Iteration 74/1000 | Loss: 0.00000872
Iteration 75/1000 | Loss: 0.00000872
Iteration 76/1000 | Loss: 0.00000872
Iteration 77/1000 | Loss: 0.00000868
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000868
Iteration 80/1000 | Loss: 0.00000868
Iteration 81/1000 | Loss: 0.00000866
Iteration 82/1000 | Loss: 0.00000866
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000866
Iteration 86/1000 | Loss: 0.00000866
Iteration 87/1000 | Loss: 0.00000866
Iteration 88/1000 | Loss: 0.00000866
Iteration 89/1000 | Loss: 0.00000866
Iteration 90/1000 | Loss: 0.00000866
Iteration 91/1000 | Loss: 0.00000865
Iteration 92/1000 | Loss: 0.00000865
Iteration 93/1000 | Loss: 0.00000865
Iteration 94/1000 | Loss: 0.00000865
Iteration 95/1000 | Loss: 0.00000865
Iteration 96/1000 | Loss: 0.00000865
Iteration 97/1000 | Loss: 0.00000864
Iteration 98/1000 | Loss: 0.00000864
Iteration 99/1000 | Loss: 0.00000864
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000863
Iteration 102/1000 | Loss: 0.00000863
Iteration 103/1000 | Loss: 0.00000863
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000862
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000862
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000861
Iteration 110/1000 | Loss: 0.00000861
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000861
Iteration 113/1000 | Loss: 0.00000861
Iteration 114/1000 | Loss: 0.00000861
Iteration 115/1000 | Loss: 0.00000861
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000860
Iteration 119/1000 | Loss: 0.00000859
Iteration 120/1000 | Loss: 0.00000859
Iteration 121/1000 | Loss: 0.00000859
Iteration 122/1000 | Loss: 0.00000859
Iteration 123/1000 | Loss: 0.00000859
Iteration 124/1000 | Loss: 0.00000859
Iteration 125/1000 | Loss: 0.00000859
Iteration 126/1000 | Loss: 0.00000859
Iteration 127/1000 | Loss: 0.00000859
Iteration 128/1000 | Loss: 0.00000858
Iteration 129/1000 | Loss: 0.00000858
Iteration 130/1000 | Loss: 0.00000858
Iteration 131/1000 | Loss: 0.00000858
Iteration 132/1000 | Loss: 0.00000858
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000857
Iteration 136/1000 | Loss: 0.00000857
Iteration 137/1000 | Loss: 0.00000857
Iteration 138/1000 | Loss: 0.00000856
Iteration 139/1000 | Loss: 0.00000856
Iteration 140/1000 | Loss: 0.00000855
Iteration 141/1000 | Loss: 0.00000855
Iteration 142/1000 | Loss: 0.00000855
Iteration 143/1000 | Loss: 0.00000855
Iteration 144/1000 | Loss: 0.00000855
Iteration 145/1000 | Loss: 0.00000855
Iteration 146/1000 | Loss: 0.00000855
Iteration 147/1000 | Loss: 0.00000855
Iteration 148/1000 | Loss: 0.00000855
Iteration 149/1000 | Loss: 0.00000854
Iteration 150/1000 | Loss: 0.00000854
Iteration 151/1000 | Loss: 0.00000854
Iteration 152/1000 | Loss: 0.00000854
Iteration 153/1000 | Loss: 0.00000854
Iteration 154/1000 | Loss: 0.00000854
Iteration 155/1000 | Loss: 0.00000854
Iteration 156/1000 | Loss: 0.00000854
Iteration 157/1000 | Loss: 0.00000854
Iteration 158/1000 | Loss: 0.00000854
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000853
Iteration 161/1000 | Loss: 0.00000853
Iteration 162/1000 | Loss: 0.00000852
Iteration 163/1000 | Loss: 0.00000852
Iteration 164/1000 | Loss: 0.00000852
Iteration 165/1000 | Loss: 0.00000852
Iteration 166/1000 | Loss: 0.00000852
Iteration 167/1000 | Loss: 0.00000851
Iteration 168/1000 | Loss: 0.00000851
Iteration 169/1000 | Loss: 0.00000851
Iteration 170/1000 | Loss: 0.00000850
Iteration 171/1000 | Loss: 0.00000850
Iteration 172/1000 | Loss: 0.00000850
Iteration 173/1000 | Loss: 0.00000850
Iteration 174/1000 | Loss: 0.00000850
Iteration 175/1000 | Loss: 0.00000849
Iteration 176/1000 | Loss: 0.00000849
Iteration 177/1000 | Loss: 0.00000849
Iteration 178/1000 | Loss: 0.00000849
Iteration 179/1000 | Loss: 0.00000849
Iteration 180/1000 | Loss: 0.00000849
Iteration 181/1000 | Loss: 0.00000849
Iteration 182/1000 | Loss: 0.00000849
Iteration 183/1000 | Loss: 0.00000849
Iteration 184/1000 | Loss: 0.00000849
Iteration 185/1000 | Loss: 0.00000849
Iteration 186/1000 | Loss: 0.00000849
Iteration 187/1000 | Loss: 0.00000849
Iteration 188/1000 | Loss: 0.00000849
Iteration 189/1000 | Loss: 0.00000849
Iteration 190/1000 | Loss: 0.00000849
Iteration 191/1000 | Loss: 0.00000849
Iteration 192/1000 | Loss: 0.00000849
Iteration 193/1000 | Loss: 0.00000849
Iteration 194/1000 | Loss: 0.00000849
Iteration 195/1000 | Loss: 0.00000849
Iteration 196/1000 | Loss: 0.00000849
Iteration 197/1000 | Loss: 0.00000849
Iteration 198/1000 | Loss: 0.00000849
Iteration 199/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [8.493183486280032e-06, 8.493183486280032e-06, 8.493183486280032e-06, 8.493183486280032e-06, 8.493183486280032e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.493183486280032e-06

Optimization complete. Final v2v error: 2.4915266036987305 mm

Highest mean error: 2.681300163269043 mm for frame 42

Lowest mean error: 2.337226629257202 mm for frame 174

Saving results

Total time: 38.83253335952759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584879
Iteration 2/25 | Loss: 0.00115545
Iteration 3/25 | Loss: 0.00108338
Iteration 4/25 | Loss: 0.00107170
Iteration 5/25 | Loss: 0.00106794
Iteration 6/25 | Loss: 0.00106722
Iteration 7/25 | Loss: 0.00106722
Iteration 8/25 | Loss: 0.00106722
Iteration 9/25 | Loss: 0.00106722
Iteration 10/25 | Loss: 0.00106722
Iteration 11/25 | Loss: 0.00106722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001067221281118691, 0.001067221281118691, 0.001067221281118691, 0.001067221281118691, 0.001067221281118691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067221281118691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70981598
Iteration 2/25 | Loss: 0.00082456
Iteration 3/25 | Loss: 0.00082456
Iteration 4/25 | Loss: 0.00082456
Iteration 5/25 | Loss: 0.00082456
Iteration 6/25 | Loss: 0.00082456
Iteration 7/25 | Loss: 0.00082456
Iteration 8/25 | Loss: 0.00082456
Iteration 9/25 | Loss: 0.00082456
Iteration 10/25 | Loss: 0.00082456
Iteration 11/25 | Loss: 0.00082456
Iteration 12/25 | Loss: 0.00082456
Iteration 13/25 | Loss: 0.00082456
Iteration 14/25 | Loss: 0.00082456
Iteration 15/25 | Loss: 0.00082456
Iteration 16/25 | Loss: 0.00082456
Iteration 17/25 | Loss: 0.00082456
Iteration 18/25 | Loss: 0.00082456
Iteration 19/25 | Loss: 0.00082456
Iteration 20/25 | Loss: 0.00082456
Iteration 21/25 | Loss: 0.00082456
Iteration 22/25 | Loss: 0.00082456
Iteration 23/25 | Loss: 0.00082456
Iteration 24/25 | Loss: 0.00082456
Iteration 25/25 | Loss: 0.00082456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082456
Iteration 2/1000 | Loss: 0.00001975
Iteration 3/1000 | Loss: 0.00001322
Iteration 4/1000 | Loss: 0.00001204
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001081
Iteration 8/1000 | Loss: 0.00001080
Iteration 9/1000 | Loss: 0.00001076
Iteration 10/1000 | Loss: 0.00001065
Iteration 11/1000 | Loss: 0.00001042
Iteration 12/1000 | Loss: 0.00001037
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001015
Iteration 16/1000 | Loss: 0.00001014
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001010
Iteration 19/1000 | Loss: 0.00001010
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001002
Iteration 30/1000 | Loss: 0.00001001
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000998
Iteration 35/1000 | Loss: 0.00000997
Iteration 36/1000 | Loss: 0.00000997
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000996
Iteration 39/1000 | Loss: 0.00000996
Iteration 40/1000 | Loss: 0.00000995
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000994
Iteration 43/1000 | Loss: 0.00000994
Iteration 44/1000 | Loss: 0.00000993
Iteration 45/1000 | Loss: 0.00000993
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000992
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000992
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000991
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000990
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000989
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000988
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000984
Iteration 73/1000 | Loss: 0.00000984
Iteration 74/1000 | Loss: 0.00000983
Iteration 75/1000 | Loss: 0.00000983
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000981
Iteration 82/1000 | Loss: 0.00000981
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000980
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000978
Iteration 89/1000 | Loss: 0.00000978
Iteration 90/1000 | Loss: 0.00000978
Iteration 91/1000 | Loss: 0.00000978
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000977
Iteration 99/1000 | Loss: 0.00000977
Iteration 100/1000 | Loss: 0.00000977
Iteration 101/1000 | Loss: 0.00000977
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000976
Iteration 106/1000 | Loss: 0.00000976
Iteration 107/1000 | Loss: 0.00000976
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000975
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000975
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000974
Iteration 119/1000 | Loss: 0.00000974
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000973
Iteration 131/1000 | Loss: 0.00000973
Iteration 132/1000 | Loss: 0.00000973
Iteration 133/1000 | Loss: 0.00000972
Iteration 134/1000 | Loss: 0.00000972
Iteration 135/1000 | Loss: 0.00000972
Iteration 136/1000 | Loss: 0.00000972
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000971
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000971
Iteration 144/1000 | Loss: 0.00000971
Iteration 145/1000 | Loss: 0.00000971
Iteration 146/1000 | Loss: 0.00000970
Iteration 147/1000 | Loss: 0.00000970
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000970
Iteration 156/1000 | Loss: 0.00000969
Iteration 157/1000 | Loss: 0.00000969
Iteration 158/1000 | Loss: 0.00000969
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000968
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000968
Iteration 164/1000 | Loss: 0.00000967
Iteration 165/1000 | Loss: 0.00000967
Iteration 166/1000 | Loss: 0.00000967
Iteration 167/1000 | Loss: 0.00000967
Iteration 168/1000 | Loss: 0.00000967
Iteration 169/1000 | Loss: 0.00000967
Iteration 170/1000 | Loss: 0.00000966
Iteration 171/1000 | Loss: 0.00000966
Iteration 172/1000 | Loss: 0.00000966
Iteration 173/1000 | Loss: 0.00000966
Iteration 174/1000 | Loss: 0.00000966
Iteration 175/1000 | Loss: 0.00000966
Iteration 176/1000 | Loss: 0.00000966
Iteration 177/1000 | Loss: 0.00000965
Iteration 178/1000 | Loss: 0.00000965
Iteration 179/1000 | Loss: 0.00000965
Iteration 180/1000 | Loss: 0.00000965
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000965
Iteration 189/1000 | Loss: 0.00000965
Iteration 190/1000 | Loss: 0.00000964
Iteration 191/1000 | Loss: 0.00000964
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000964
Iteration 199/1000 | Loss: 0.00000964
Iteration 200/1000 | Loss: 0.00000964
Iteration 201/1000 | Loss: 0.00000964
Iteration 202/1000 | Loss: 0.00000964
Iteration 203/1000 | Loss: 0.00000964
Iteration 204/1000 | Loss: 0.00000964
Iteration 205/1000 | Loss: 0.00000964
Iteration 206/1000 | Loss: 0.00000963
Iteration 207/1000 | Loss: 0.00000963
Iteration 208/1000 | Loss: 0.00000963
Iteration 209/1000 | Loss: 0.00000963
Iteration 210/1000 | Loss: 0.00000963
Iteration 211/1000 | Loss: 0.00000963
Iteration 212/1000 | Loss: 0.00000963
Iteration 213/1000 | Loss: 0.00000963
Iteration 214/1000 | Loss: 0.00000963
Iteration 215/1000 | Loss: 0.00000963
Iteration 216/1000 | Loss: 0.00000963
Iteration 217/1000 | Loss: 0.00000963
Iteration 218/1000 | Loss: 0.00000963
Iteration 219/1000 | Loss: 0.00000963
Iteration 220/1000 | Loss: 0.00000963
Iteration 221/1000 | Loss: 0.00000963
Iteration 222/1000 | Loss: 0.00000963
Iteration 223/1000 | Loss: 0.00000963
Iteration 224/1000 | Loss: 0.00000963
Iteration 225/1000 | Loss: 0.00000963
Iteration 226/1000 | Loss: 0.00000963
Iteration 227/1000 | Loss: 0.00000963
Iteration 228/1000 | Loss: 0.00000962
Iteration 229/1000 | Loss: 0.00000962
Iteration 230/1000 | Loss: 0.00000962
Iteration 231/1000 | Loss: 0.00000962
Iteration 232/1000 | Loss: 0.00000962
Iteration 233/1000 | Loss: 0.00000962
Iteration 234/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [9.624195627111476e-06, 9.624195627111476e-06, 9.624195627111476e-06, 9.624195627111476e-06, 9.624195627111476e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.624195627111476e-06

Optimization complete. Final v2v error: 2.6630964279174805 mm

Highest mean error: 3.111927032470703 mm for frame 117

Lowest mean error: 2.4206676483154297 mm for frame 165

Saving results

Total time: 41.38325214385986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007202
Iteration 2/25 | Loss: 0.01007202
Iteration 3/25 | Loss: 0.00231409
Iteration 4/25 | Loss: 0.00192971
Iteration 5/25 | Loss: 0.00183037
Iteration 6/25 | Loss: 0.00177518
Iteration 7/25 | Loss: 0.00170972
Iteration 8/25 | Loss: 0.00165295
Iteration 9/25 | Loss: 0.00161488
Iteration 10/25 | Loss: 0.00159751
Iteration 11/25 | Loss: 0.00158741
Iteration 12/25 | Loss: 0.00155668
Iteration 13/25 | Loss: 0.00152954
Iteration 14/25 | Loss: 0.00153256
Iteration 15/25 | Loss: 0.00151482
Iteration 16/25 | Loss: 0.00149894
Iteration 17/25 | Loss: 0.00149642
Iteration 18/25 | Loss: 0.00149574
Iteration 19/25 | Loss: 0.00149550
Iteration 20/25 | Loss: 0.00149537
Iteration 21/25 | Loss: 0.00149530
Iteration 22/25 | Loss: 0.00149530
Iteration 23/25 | Loss: 0.00149530
Iteration 24/25 | Loss: 0.00149530
Iteration 25/25 | Loss: 0.00149530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32304382
Iteration 2/25 | Loss: 0.00340651
Iteration 3/25 | Loss: 0.00340651
Iteration 4/25 | Loss: 0.00340651
Iteration 5/25 | Loss: 0.00340651
Iteration 6/25 | Loss: 0.00340651
Iteration 7/25 | Loss: 0.00340651
Iteration 8/25 | Loss: 0.00340651
Iteration 9/25 | Loss: 0.00340651
Iteration 10/25 | Loss: 0.00340651
Iteration 11/25 | Loss: 0.00340651
Iteration 12/25 | Loss: 0.00340651
Iteration 13/25 | Loss: 0.00340651
Iteration 14/25 | Loss: 0.00340651
Iteration 15/25 | Loss: 0.00340651
Iteration 16/25 | Loss: 0.00340651
Iteration 17/25 | Loss: 0.00340651
Iteration 18/25 | Loss: 0.00340651
Iteration 19/25 | Loss: 0.00340651
Iteration 20/25 | Loss: 0.00340651
Iteration 21/25 | Loss: 0.00340651
Iteration 22/25 | Loss: 0.00340651
Iteration 23/25 | Loss: 0.00340651
Iteration 24/25 | Loss: 0.00340651
Iteration 25/25 | Loss: 0.00340651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00340651
Iteration 2/1000 | Loss: 0.00045247
Iteration 3/1000 | Loss: 0.00039003
Iteration 4/1000 | Loss: 0.00031442
Iteration 5/1000 | Loss: 0.00038905
Iteration 6/1000 | Loss: 0.00019967
Iteration 7/1000 | Loss: 0.00017955
Iteration 8/1000 | Loss: 0.00016128
Iteration 9/1000 | Loss: 0.00014844
Iteration 10/1000 | Loss: 0.00013828
Iteration 11/1000 | Loss: 0.00013311
Iteration 12/1000 | Loss: 0.00012842
Iteration 13/1000 | Loss: 0.00012574
Iteration 14/1000 | Loss: 0.00012357
Iteration 15/1000 | Loss: 0.00012191
Iteration 16/1000 | Loss: 0.00012024
Iteration 17/1000 | Loss: 0.00011925
Iteration 18/1000 | Loss: 0.00011803
Iteration 19/1000 | Loss: 0.00011702
Iteration 20/1000 | Loss: 0.00011621
Iteration 21/1000 | Loss: 0.00011552
Iteration 22/1000 | Loss: 0.00011497
Iteration 23/1000 | Loss: 0.00024243
Iteration 24/1000 | Loss: 0.00106267
Iteration 25/1000 | Loss: 0.00239013
Iteration 26/1000 | Loss: 0.00168051
Iteration 27/1000 | Loss: 0.00057760
Iteration 28/1000 | Loss: 0.00175556
Iteration 29/1000 | Loss: 0.00140858
Iteration 30/1000 | Loss: 0.00144759
Iteration 31/1000 | Loss: 0.00140073
Iteration 32/1000 | Loss: 0.00108081
Iteration 33/1000 | Loss: 0.00061610
Iteration 34/1000 | Loss: 0.00063196
Iteration 35/1000 | Loss: 0.00010622
Iteration 36/1000 | Loss: 0.00007345
Iteration 37/1000 | Loss: 0.00009739
Iteration 38/1000 | Loss: 0.00005050
Iteration 39/1000 | Loss: 0.00004347
Iteration 40/1000 | Loss: 0.00003815
Iteration 41/1000 | Loss: 0.00003251
Iteration 42/1000 | Loss: 0.00002984
Iteration 43/1000 | Loss: 0.00020363
Iteration 44/1000 | Loss: 0.00004369
Iteration 45/1000 | Loss: 0.00003099
Iteration 46/1000 | Loss: 0.00002727
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00018177
Iteration 49/1000 | Loss: 0.00003463
Iteration 50/1000 | Loss: 0.00002790
Iteration 51/1000 | Loss: 0.00002338
Iteration 52/1000 | Loss: 0.00002191
Iteration 53/1000 | Loss: 0.00002145
Iteration 54/1000 | Loss: 0.00002107
Iteration 55/1000 | Loss: 0.00002096
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002060
Iteration 58/1000 | Loss: 0.00002057
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002018
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002012
Iteration 64/1000 | Loss: 0.00002011
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00002006
Iteration 72/1000 | Loss: 0.00002004
Iteration 73/1000 | Loss: 0.00002004
Iteration 74/1000 | Loss: 0.00002003
Iteration 75/1000 | Loss: 0.00002003
Iteration 76/1000 | Loss: 0.00002003
Iteration 77/1000 | Loss: 0.00002003
Iteration 78/1000 | Loss: 0.00002002
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002001
Iteration 81/1000 | Loss: 0.00002000
Iteration 82/1000 | Loss: 0.00002000
Iteration 83/1000 | Loss: 0.00001998
Iteration 84/1000 | Loss: 0.00001997
Iteration 85/1000 | Loss: 0.00001997
Iteration 86/1000 | Loss: 0.00001997
Iteration 87/1000 | Loss: 0.00001997
Iteration 88/1000 | Loss: 0.00001996
Iteration 89/1000 | Loss: 0.00001996
Iteration 90/1000 | Loss: 0.00001996
Iteration 91/1000 | Loss: 0.00001995
Iteration 92/1000 | Loss: 0.00001995
Iteration 93/1000 | Loss: 0.00001995
Iteration 94/1000 | Loss: 0.00001994
Iteration 95/1000 | Loss: 0.00001994
Iteration 96/1000 | Loss: 0.00001994
Iteration 97/1000 | Loss: 0.00001994
Iteration 98/1000 | Loss: 0.00001993
Iteration 99/1000 | Loss: 0.00001993
Iteration 100/1000 | Loss: 0.00001993
Iteration 101/1000 | Loss: 0.00001993
Iteration 102/1000 | Loss: 0.00001992
Iteration 103/1000 | Loss: 0.00001992
Iteration 104/1000 | Loss: 0.00001992
Iteration 105/1000 | Loss: 0.00001992
Iteration 106/1000 | Loss: 0.00001992
Iteration 107/1000 | Loss: 0.00001992
Iteration 108/1000 | Loss: 0.00001992
Iteration 109/1000 | Loss: 0.00001992
Iteration 110/1000 | Loss: 0.00001992
Iteration 111/1000 | Loss: 0.00001991
Iteration 112/1000 | Loss: 0.00001991
Iteration 113/1000 | Loss: 0.00001991
Iteration 114/1000 | Loss: 0.00001991
Iteration 115/1000 | Loss: 0.00001991
Iteration 116/1000 | Loss: 0.00001991
Iteration 117/1000 | Loss: 0.00001991
Iteration 118/1000 | Loss: 0.00001991
Iteration 119/1000 | Loss: 0.00001991
Iteration 120/1000 | Loss: 0.00001990
Iteration 121/1000 | Loss: 0.00001990
Iteration 122/1000 | Loss: 0.00001990
Iteration 123/1000 | Loss: 0.00001990
Iteration 124/1000 | Loss: 0.00001990
Iteration 125/1000 | Loss: 0.00001990
Iteration 126/1000 | Loss: 0.00001990
Iteration 127/1000 | Loss: 0.00001990
Iteration 128/1000 | Loss: 0.00001990
Iteration 129/1000 | Loss: 0.00001990
Iteration 130/1000 | Loss: 0.00001990
Iteration 131/1000 | Loss: 0.00001990
Iteration 132/1000 | Loss: 0.00001989
Iteration 133/1000 | Loss: 0.00001989
Iteration 134/1000 | Loss: 0.00001989
Iteration 135/1000 | Loss: 0.00001989
Iteration 136/1000 | Loss: 0.00001989
Iteration 137/1000 | Loss: 0.00001989
Iteration 138/1000 | Loss: 0.00001989
Iteration 139/1000 | Loss: 0.00001989
Iteration 140/1000 | Loss: 0.00001989
Iteration 141/1000 | Loss: 0.00001989
Iteration 142/1000 | Loss: 0.00001989
Iteration 143/1000 | Loss: 0.00001989
Iteration 144/1000 | Loss: 0.00001989
Iteration 145/1000 | Loss: 0.00001988
Iteration 146/1000 | Loss: 0.00001988
Iteration 147/1000 | Loss: 0.00001988
Iteration 148/1000 | Loss: 0.00001988
Iteration 149/1000 | Loss: 0.00001988
Iteration 150/1000 | Loss: 0.00001988
Iteration 151/1000 | Loss: 0.00001988
Iteration 152/1000 | Loss: 0.00001988
Iteration 153/1000 | Loss: 0.00001988
Iteration 154/1000 | Loss: 0.00001988
Iteration 155/1000 | Loss: 0.00001988
Iteration 156/1000 | Loss: 0.00001987
Iteration 157/1000 | Loss: 0.00001987
Iteration 158/1000 | Loss: 0.00001987
Iteration 159/1000 | Loss: 0.00001987
Iteration 160/1000 | Loss: 0.00001987
Iteration 161/1000 | Loss: 0.00001987
Iteration 162/1000 | Loss: 0.00001987
Iteration 163/1000 | Loss: 0.00001987
Iteration 164/1000 | Loss: 0.00001987
Iteration 165/1000 | Loss: 0.00001987
Iteration 166/1000 | Loss: 0.00001986
Iteration 167/1000 | Loss: 0.00001986
Iteration 168/1000 | Loss: 0.00001986
Iteration 169/1000 | Loss: 0.00001986
Iteration 170/1000 | Loss: 0.00001986
Iteration 171/1000 | Loss: 0.00001986
Iteration 172/1000 | Loss: 0.00001986
Iteration 173/1000 | Loss: 0.00001986
Iteration 174/1000 | Loss: 0.00001986
Iteration 175/1000 | Loss: 0.00001986
Iteration 176/1000 | Loss: 0.00001986
Iteration 177/1000 | Loss: 0.00001986
Iteration 178/1000 | Loss: 0.00001986
Iteration 179/1000 | Loss: 0.00001986
Iteration 180/1000 | Loss: 0.00001985
Iteration 181/1000 | Loss: 0.00001985
Iteration 182/1000 | Loss: 0.00001985
Iteration 183/1000 | Loss: 0.00001985
Iteration 184/1000 | Loss: 0.00001985
Iteration 185/1000 | Loss: 0.00001985
Iteration 186/1000 | Loss: 0.00001985
Iteration 187/1000 | Loss: 0.00001985
Iteration 188/1000 | Loss: 0.00001985
Iteration 189/1000 | Loss: 0.00001984
Iteration 190/1000 | Loss: 0.00001984
Iteration 191/1000 | Loss: 0.00001984
Iteration 192/1000 | Loss: 0.00001984
Iteration 193/1000 | Loss: 0.00001984
Iteration 194/1000 | Loss: 0.00001984
Iteration 195/1000 | Loss: 0.00001984
Iteration 196/1000 | Loss: 0.00001984
Iteration 197/1000 | Loss: 0.00001984
Iteration 198/1000 | Loss: 0.00001984
Iteration 199/1000 | Loss: 0.00001984
Iteration 200/1000 | Loss: 0.00001984
Iteration 201/1000 | Loss: 0.00001984
Iteration 202/1000 | Loss: 0.00001984
Iteration 203/1000 | Loss: 0.00001984
Iteration 204/1000 | Loss: 0.00001984
Iteration 205/1000 | Loss: 0.00001984
Iteration 206/1000 | Loss: 0.00001984
Iteration 207/1000 | Loss: 0.00001984
Iteration 208/1000 | Loss: 0.00001984
Iteration 209/1000 | Loss: 0.00001984
Iteration 210/1000 | Loss: 0.00001984
Iteration 211/1000 | Loss: 0.00001984
Iteration 212/1000 | Loss: 0.00001984
Iteration 213/1000 | Loss: 0.00001984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.983505535463337e-05, 1.983505535463337e-05, 1.983505535463337e-05, 1.983505535463337e-05, 1.983505535463337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.983505535463337e-05

Optimization complete. Final v2v error: 3.1559760570526123 mm

Highest mean error: 10.900507926940918 mm for frame 197

Lowest mean error: 2.7764785289764404 mm for frame 5

Saving results

Total time: 146.16913199424744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339556
Iteration 2/25 | Loss: 0.00144021
Iteration 3/25 | Loss: 0.00119706
Iteration 4/25 | Loss: 0.00111582
Iteration 5/25 | Loss: 0.00109978
Iteration 6/25 | Loss: 0.00109410
Iteration 7/25 | Loss: 0.00109262
Iteration 8/25 | Loss: 0.00109486
Iteration 9/25 | Loss: 0.00109200
Iteration 10/25 | Loss: 0.00109159
Iteration 11/25 | Loss: 0.00109070
Iteration 12/25 | Loss: 0.00108922
Iteration 13/25 | Loss: 0.00108896
Iteration 14/25 | Loss: 0.00108894
Iteration 15/25 | Loss: 0.00108894
Iteration 16/25 | Loss: 0.00108893
Iteration 17/25 | Loss: 0.00108893
Iteration 18/25 | Loss: 0.00108893
Iteration 19/25 | Loss: 0.00108893
Iteration 20/25 | Loss: 0.00108893
Iteration 21/25 | Loss: 0.00108893
Iteration 22/25 | Loss: 0.00108893
Iteration 23/25 | Loss: 0.00108892
Iteration 24/25 | Loss: 0.00108892
Iteration 25/25 | Loss: 0.00108892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35234737
Iteration 2/25 | Loss: 0.00103171
Iteration 3/25 | Loss: 0.00103171
Iteration 4/25 | Loss: 0.00103171
Iteration 5/25 | Loss: 0.00103171
Iteration 6/25 | Loss: 0.00103171
Iteration 7/25 | Loss: 0.00103171
Iteration 8/25 | Loss: 0.00103171
Iteration 9/25 | Loss: 0.00103171
Iteration 10/25 | Loss: 0.00103171
Iteration 11/25 | Loss: 0.00103171
Iteration 12/25 | Loss: 0.00103171
Iteration 13/25 | Loss: 0.00103171
Iteration 14/25 | Loss: 0.00103171
Iteration 15/25 | Loss: 0.00103171
Iteration 16/25 | Loss: 0.00103171
Iteration 17/25 | Loss: 0.00103171
Iteration 18/25 | Loss: 0.00103171
Iteration 19/25 | Loss: 0.00103171
Iteration 20/25 | Loss: 0.00103171
Iteration 21/25 | Loss: 0.00103171
Iteration 22/25 | Loss: 0.00103171
Iteration 23/25 | Loss: 0.00103171
Iteration 24/25 | Loss: 0.00103171
Iteration 25/25 | Loss: 0.00103171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103171
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00002532
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001642
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001468
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001408
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001360
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001353
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001346
Iteration 61/1000 | Loss: 0.00001346
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001345
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001336
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001330
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001329
Iteration 152/1000 | Loss: 0.00001329
Iteration 153/1000 | Loss: 0.00001329
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001328
Iteration 157/1000 | Loss: 0.00001328
Iteration 158/1000 | Loss: 0.00001328
Iteration 159/1000 | Loss: 0.00001328
Iteration 160/1000 | Loss: 0.00001328
Iteration 161/1000 | Loss: 0.00001328
Iteration 162/1000 | Loss: 0.00001328
Iteration 163/1000 | Loss: 0.00001328
Iteration 164/1000 | Loss: 0.00001328
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001326
Iteration 179/1000 | Loss: 0.00001326
Iteration 180/1000 | Loss: 0.00001326
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001326
Iteration 183/1000 | Loss: 0.00001326
Iteration 184/1000 | Loss: 0.00001326
Iteration 185/1000 | Loss: 0.00001326
Iteration 186/1000 | Loss: 0.00001325
Iteration 187/1000 | Loss: 0.00001325
Iteration 188/1000 | Loss: 0.00001325
Iteration 189/1000 | Loss: 0.00001325
Iteration 190/1000 | Loss: 0.00001325
Iteration 191/1000 | Loss: 0.00001325
Iteration 192/1000 | Loss: 0.00001325
Iteration 193/1000 | Loss: 0.00001325
Iteration 194/1000 | Loss: 0.00001325
Iteration 195/1000 | Loss: 0.00001325
Iteration 196/1000 | Loss: 0.00001325
Iteration 197/1000 | Loss: 0.00001325
Iteration 198/1000 | Loss: 0.00001325
Iteration 199/1000 | Loss: 0.00001325
Iteration 200/1000 | Loss: 0.00001324
Iteration 201/1000 | Loss: 0.00001324
Iteration 202/1000 | Loss: 0.00001324
Iteration 203/1000 | Loss: 0.00001324
Iteration 204/1000 | Loss: 0.00001324
Iteration 205/1000 | Loss: 0.00001324
Iteration 206/1000 | Loss: 0.00001324
Iteration 207/1000 | Loss: 0.00001324
Iteration 208/1000 | Loss: 0.00001324
Iteration 209/1000 | Loss: 0.00001323
Iteration 210/1000 | Loss: 0.00001323
Iteration 211/1000 | Loss: 0.00001323
Iteration 212/1000 | Loss: 0.00001323
Iteration 213/1000 | Loss: 0.00001323
Iteration 214/1000 | Loss: 0.00001323
Iteration 215/1000 | Loss: 0.00001323
Iteration 216/1000 | Loss: 0.00001323
Iteration 217/1000 | Loss: 0.00001323
Iteration 218/1000 | Loss: 0.00001323
Iteration 219/1000 | Loss: 0.00001323
Iteration 220/1000 | Loss: 0.00001323
Iteration 221/1000 | Loss: 0.00001323
Iteration 222/1000 | Loss: 0.00001322
Iteration 223/1000 | Loss: 0.00001322
Iteration 224/1000 | Loss: 0.00001322
Iteration 225/1000 | Loss: 0.00001322
Iteration 226/1000 | Loss: 0.00001322
Iteration 227/1000 | Loss: 0.00001322
Iteration 228/1000 | Loss: 0.00001322
Iteration 229/1000 | Loss: 0.00001322
Iteration 230/1000 | Loss: 0.00001322
Iteration 231/1000 | Loss: 0.00001322
Iteration 232/1000 | Loss: 0.00001322
Iteration 233/1000 | Loss: 0.00001322
Iteration 234/1000 | Loss: 0.00001322
Iteration 235/1000 | Loss: 0.00001322
Iteration 236/1000 | Loss: 0.00001322
Iteration 237/1000 | Loss: 0.00001322
Iteration 238/1000 | Loss: 0.00001322
Iteration 239/1000 | Loss: 0.00001322
Iteration 240/1000 | Loss: 0.00001321
Iteration 241/1000 | Loss: 0.00001321
Iteration 242/1000 | Loss: 0.00001321
Iteration 243/1000 | Loss: 0.00001321
Iteration 244/1000 | Loss: 0.00001321
Iteration 245/1000 | Loss: 0.00001321
Iteration 246/1000 | Loss: 0.00001321
Iteration 247/1000 | Loss: 0.00001320
Iteration 248/1000 | Loss: 0.00001320
Iteration 249/1000 | Loss: 0.00001320
Iteration 250/1000 | Loss: 0.00001320
Iteration 251/1000 | Loss: 0.00001320
Iteration 252/1000 | Loss: 0.00001320
Iteration 253/1000 | Loss: 0.00001320
Iteration 254/1000 | Loss: 0.00001320
Iteration 255/1000 | Loss: 0.00001320
Iteration 256/1000 | Loss: 0.00001320
Iteration 257/1000 | Loss: 0.00001320
Iteration 258/1000 | Loss: 0.00001320
Iteration 259/1000 | Loss: 0.00001320
Iteration 260/1000 | Loss: 0.00001320
Iteration 261/1000 | Loss: 0.00001320
Iteration 262/1000 | Loss: 0.00001320
Iteration 263/1000 | Loss: 0.00001320
Iteration 264/1000 | Loss: 0.00001320
Iteration 265/1000 | Loss: 0.00001320
Iteration 266/1000 | Loss: 0.00001320
Iteration 267/1000 | Loss: 0.00001320
Iteration 268/1000 | Loss: 0.00001320
Iteration 269/1000 | Loss: 0.00001320
Iteration 270/1000 | Loss: 0.00001320
Iteration 271/1000 | Loss: 0.00001320
Iteration 272/1000 | Loss: 0.00001320
Iteration 273/1000 | Loss: 0.00001320
Iteration 274/1000 | Loss: 0.00001320
Iteration 275/1000 | Loss: 0.00001320
Iteration 276/1000 | Loss: 0.00001320
Iteration 277/1000 | Loss: 0.00001320
Iteration 278/1000 | Loss: 0.00001320
Iteration 279/1000 | Loss: 0.00001320
Iteration 280/1000 | Loss: 0.00001320
Iteration 281/1000 | Loss: 0.00001320
Iteration 282/1000 | Loss: 0.00001320
Iteration 283/1000 | Loss: 0.00001320
Iteration 284/1000 | Loss: 0.00001320
Iteration 285/1000 | Loss: 0.00001320
Iteration 286/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [1.3200407920521684e-05, 1.3200407920521684e-05, 1.3200407920521684e-05, 1.3200407920521684e-05, 1.3200407920521684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3200407920521684e-05

Optimization complete. Final v2v error: 3.071913957595825 mm

Highest mean error: 3.867978811264038 mm for frame 70

Lowest mean error: 2.4981300830841064 mm for frame 0

Saving results

Total time: 59.53797936439514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804634
Iteration 2/25 | Loss: 0.00132272
Iteration 3/25 | Loss: 0.00114560
Iteration 4/25 | Loss: 0.00112695
Iteration 5/25 | Loss: 0.00112131
Iteration 6/25 | Loss: 0.00111999
Iteration 7/25 | Loss: 0.00111999
Iteration 8/25 | Loss: 0.00111999
Iteration 9/25 | Loss: 0.00111999
Iteration 10/25 | Loss: 0.00111999
Iteration 11/25 | Loss: 0.00111999
Iteration 12/25 | Loss: 0.00111999
Iteration 13/25 | Loss: 0.00111999
Iteration 14/25 | Loss: 0.00111999
Iteration 15/25 | Loss: 0.00111999
Iteration 16/25 | Loss: 0.00111999
Iteration 17/25 | Loss: 0.00111999
Iteration 18/25 | Loss: 0.00111999
Iteration 19/25 | Loss: 0.00111999
Iteration 20/25 | Loss: 0.00111999
Iteration 21/25 | Loss: 0.00111999
Iteration 22/25 | Loss: 0.00111999
Iteration 23/25 | Loss: 0.00111999
Iteration 24/25 | Loss: 0.00111999
Iteration 25/25 | Loss: 0.00111999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23296201
Iteration 2/25 | Loss: 0.00072800
Iteration 3/25 | Loss: 0.00072798
Iteration 4/25 | Loss: 0.00072798
Iteration 5/25 | Loss: 0.00072798
Iteration 6/25 | Loss: 0.00072798
Iteration 7/25 | Loss: 0.00072798
Iteration 8/25 | Loss: 0.00072798
Iteration 9/25 | Loss: 0.00072798
Iteration 10/25 | Loss: 0.00072798
Iteration 11/25 | Loss: 0.00072798
Iteration 12/25 | Loss: 0.00072798
Iteration 13/25 | Loss: 0.00072798
Iteration 14/25 | Loss: 0.00072798
Iteration 15/25 | Loss: 0.00072798
Iteration 16/25 | Loss: 0.00072798
Iteration 17/25 | Loss: 0.00072798
Iteration 18/25 | Loss: 0.00072798
Iteration 19/25 | Loss: 0.00072798
Iteration 20/25 | Loss: 0.00072798
Iteration 21/25 | Loss: 0.00072798
Iteration 22/25 | Loss: 0.00072798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000727975566405803, 0.000727975566405803, 0.000727975566405803, 0.000727975566405803, 0.000727975566405803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000727975566405803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072798
Iteration 2/1000 | Loss: 0.00003547
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001854
Iteration 6/1000 | Loss: 0.00001756
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001644
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001530
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001528
Iteration 19/1000 | Loss: 0.00001527
Iteration 20/1000 | Loss: 0.00001524
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001506
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001498
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001492
Iteration 39/1000 | Loss: 0.00001492
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001490
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001489
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001484
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001480
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001471
Iteration 105/1000 | Loss: 0.00001471
Iteration 106/1000 | Loss: 0.00001471
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001469
Iteration 114/1000 | Loss: 0.00001469
Iteration 115/1000 | Loss: 0.00001469
Iteration 116/1000 | Loss: 0.00001469
Iteration 117/1000 | Loss: 0.00001469
Iteration 118/1000 | Loss: 0.00001469
Iteration 119/1000 | Loss: 0.00001469
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001469
Iteration 122/1000 | Loss: 0.00001469
Iteration 123/1000 | Loss: 0.00001469
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00001469
Iteration 126/1000 | Loss: 0.00001469
Iteration 127/1000 | Loss: 0.00001468
Iteration 128/1000 | Loss: 0.00001468
Iteration 129/1000 | Loss: 0.00001468
Iteration 130/1000 | Loss: 0.00001468
Iteration 131/1000 | Loss: 0.00001468
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001467
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Iteration 155/1000 | Loss: 0.00001466
Iteration 156/1000 | Loss: 0.00001466
Iteration 157/1000 | Loss: 0.00001466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.466362846258562e-05, 1.466362846258562e-05, 1.466362846258562e-05, 1.466362846258562e-05, 1.466362846258562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.466362846258562e-05

Optimization complete. Final v2v error: 3.180513858795166 mm

Highest mean error: 3.9658379554748535 mm for frame 61

Lowest mean error: 2.8353521823883057 mm for frame 198

Saving results

Total time: 45.94101333618164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436709
Iteration 2/25 | Loss: 0.00130950
Iteration 3/25 | Loss: 0.00115762
Iteration 4/25 | Loss: 0.00113776
Iteration 5/25 | Loss: 0.00113325
Iteration 6/25 | Loss: 0.00113262
Iteration 7/25 | Loss: 0.00113262
Iteration 8/25 | Loss: 0.00113262
Iteration 9/25 | Loss: 0.00113262
Iteration 10/25 | Loss: 0.00113262
Iteration 11/25 | Loss: 0.00113262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011326181702315807, 0.0011326181702315807, 0.0011326181702315807, 0.0011326181702315807, 0.0011326181702315807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011326181702315807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35094523
Iteration 2/25 | Loss: 0.00068884
Iteration 3/25 | Loss: 0.00068884
Iteration 4/25 | Loss: 0.00068884
Iteration 5/25 | Loss: 0.00068884
Iteration 6/25 | Loss: 0.00068884
Iteration 7/25 | Loss: 0.00068884
Iteration 8/25 | Loss: 0.00068884
Iteration 9/25 | Loss: 0.00068884
Iteration 10/25 | Loss: 0.00068884
Iteration 11/25 | Loss: 0.00068884
Iteration 12/25 | Loss: 0.00068884
Iteration 13/25 | Loss: 0.00068884
Iteration 14/25 | Loss: 0.00068884
Iteration 15/25 | Loss: 0.00068884
Iteration 16/25 | Loss: 0.00068884
Iteration 17/25 | Loss: 0.00068884
Iteration 18/25 | Loss: 0.00068884
Iteration 19/25 | Loss: 0.00068884
Iteration 20/25 | Loss: 0.00068884
Iteration 21/25 | Loss: 0.00068884
Iteration 22/25 | Loss: 0.00068884
Iteration 23/25 | Loss: 0.00068884
Iteration 24/25 | Loss: 0.00068884
Iteration 25/25 | Loss: 0.00068884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068884
Iteration 2/1000 | Loss: 0.00002617
Iteration 3/1000 | Loss: 0.00002004
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001791
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001650
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001590
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001555
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001550
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001542
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001542
Iteration 39/1000 | Loss: 0.00001542
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001538
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001537
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001536
Iteration 63/1000 | Loss: 0.00001536
Iteration 64/1000 | Loss: 0.00001535
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001534
Iteration 70/1000 | Loss: 0.00001534
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001533
Iteration 76/1000 | Loss: 0.00001533
Iteration 77/1000 | Loss: 0.00001533
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001533
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001532
Iteration 82/1000 | Loss: 0.00001532
Iteration 83/1000 | Loss: 0.00001532
Iteration 84/1000 | Loss: 0.00001532
Iteration 85/1000 | Loss: 0.00001532
Iteration 86/1000 | Loss: 0.00001532
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001531
Iteration 90/1000 | Loss: 0.00001531
Iteration 91/1000 | Loss: 0.00001531
Iteration 92/1000 | Loss: 0.00001531
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001530
Iteration 95/1000 | Loss: 0.00001530
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001530
Iteration 98/1000 | Loss: 0.00001530
Iteration 99/1000 | Loss: 0.00001530
Iteration 100/1000 | Loss: 0.00001530
Iteration 101/1000 | Loss: 0.00001530
Iteration 102/1000 | Loss: 0.00001530
Iteration 103/1000 | Loss: 0.00001530
Iteration 104/1000 | Loss: 0.00001530
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001529
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001528
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001528
Iteration 119/1000 | Loss: 0.00001528
Iteration 120/1000 | Loss: 0.00001528
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001528
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001528
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001528
Iteration 141/1000 | Loss: 0.00001528
Iteration 142/1000 | Loss: 0.00001528
Iteration 143/1000 | Loss: 0.00001528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.528352549939882e-05, 1.528352549939882e-05, 1.528352549939882e-05, 1.528352549939882e-05, 1.528352549939882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.528352549939882e-05

Optimization complete. Final v2v error: 3.288254499435425 mm

Highest mean error: 4.352060317993164 mm for frame 228

Lowest mean error: 2.74430251121521 mm for frame 89

Saving results

Total time: 39.95675349235535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982632
Iteration 2/25 | Loss: 0.00177930
Iteration 3/25 | Loss: 0.00135387
Iteration 4/25 | Loss: 0.00131002
Iteration 5/25 | Loss: 0.00130338
Iteration 6/25 | Loss: 0.00130241
Iteration 7/25 | Loss: 0.00130241
Iteration 8/25 | Loss: 0.00130241
Iteration 9/25 | Loss: 0.00130241
Iteration 10/25 | Loss: 0.00130241
Iteration 11/25 | Loss: 0.00130241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001302407355979085, 0.001302407355979085, 0.001302407355979085, 0.001302407355979085, 0.001302407355979085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302407355979085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.73695374
Iteration 2/25 | Loss: 0.00077645
Iteration 3/25 | Loss: 0.00077643
Iteration 4/25 | Loss: 0.00077643
Iteration 5/25 | Loss: 0.00077643
Iteration 6/25 | Loss: 0.00077643
Iteration 7/25 | Loss: 0.00077643
Iteration 8/25 | Loss: 0.00077643
Iteration 9/25 | Loss: 0.00077643
Iteration 10/25 | Loss: 0.00077643
Iteration 11/25 | Loss: 0.00077643
Iteration 12/25 | Loss: 0.00077642
Iteration 13/25 | Loss: 0.00077642
Iteration 14/25 | Loss: 0.00077642
Iteration 15/25 | Loss: 0.00077642
Iteration 16/25 | Loss: 0.00077642
Iteration 17/25 | Loss: 0.00077642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007764248875901103, 0.0007764248875901103, 0.0007764248875901103, 0.0007764248875901103, 0.0007764248875901103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007764248875901103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077642
Iteration 2/1000 | Loss: 0.00006622
Iteration 3/1000 | Loss: 0.00004449
Iteration 4/1000 | Loss: 0.00003614
Iteration 5/1000 | Loss: 0.00003429
Iteration 6/1000 | Loss: 0.00003345
Iteration 7/1000 | Loss: 0.00003257
Iteration 8/1000 | Loss: 0.00003208
Iteration 9/1000 | Loss: 0.00003166
Iteration 10/1000 | Loss: 0.00003134
Iteration 11/1000 | Loss: 0.00003129
Iteration 12/1000 | Loss: 0.00003108
Iteration 13/1000 | Loss: 0.00003088
Iteration 14/1000 | Loss: 0.00003072
Iteration 15/1000 | Loss: 0.00003057
Iteration 16/1000 | Loss: 0.00003051
Iteration 17/1000 | Loss: 0.00003049
Iteration 18/1000 | Loss: 0.00003046
Iteration 19/1000 | Loss: 0.00003045
Iteration 20/1000 | Loss: 0.00003043
Iteration 21/1000 | Loss: 0.00003043
Iteration 22/1000 | Loss: 0.00003036
Iteration 23/1000 | Loss: 0.00003031
Iteration 24/1000 | Loss: 0.00003026
Iteration 25/1000 | Loss: 0.00003023
Iteration 26/1000 | Loss: 0.00003019
Iteration 27/1000 | Loss: 0.00003019
Iteration 28/1000 | Loss: 0.00003018
Iteration 29/1000 | Loss: 0.00003014
Iteration 30/1000 | Loss: 0.00003014
Iteration 31/1000 | Loss: 0.00003014
Iteration 32/1000 | Loss: 0.00003014
Iteration 33/1000 | Loss: 0.00003013
Iteration 34/1000 | Loss: 0.00003010
Iteration 35/1000 | Loss: 0.00003006
Iteration 36/1000 | Loss: 0.00003004
Iteration 37/1000 | Loss: 0.00003003
Iteration 38/1000 | Loss: 0.00003003
Iteration 39/1000 | Loss: 0.00002997
Iteration 40/1000 | Loss: 0.00002997
Iteration 41/1000 | Loss: 0.00002996
Iteration 42/1000 | Loss: 0.00002996
Iteration 43/1000 | Loss: 0.00002995
Iteration 44/1000 | Loss: 0.00002995
Iteration 45/1000 | Loss: 0.00002994
Iteration 46/1000 | Loss: 0.00002994
Iteration 47/1000 | Loss: 0.00002994
Iteration 48/1000 | Loss: 0.00002993
Iteration 49/1000 | Loss: 0.00002992
Iteration 50/1000 | Loss: 0.00002992
Iteration 51/1000 | Loss: 0.00002992
Iteration 52/1000 | Loss: 0.00002992
Iteration 53/1000 | Loss: 0.00002992
Iteration 54/1000 | Loss: 0.00002992
Iteration 55/1000 | Loss: 0.00002992
Iteration 56/1000 | Loss: 0.00002992
Iteration 57/1000 | Loss: 0.00002992
Iteration 58/1000 | Loss: 0.00002992
Iteration 59/1000 | Loss: 0.00002991
Iteration 60/1000 | Loss: 0.00002991
Iteration 61/1000 | Loss: 0.00002991
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002990
Iteration 66/1000 | Loss: 0.00002990
Iteration 67/1000 | Loss: 0.00002990
Iteration 68/1000 | Loss: 0.00002990
Iteration 69/1000 | Loss: 0.00002990
Iteration 70/1000 | Loss: 0.00002990
Iteration 71/1000 | Loss: 0.00002989
Iteration 72/1000 | Loss: 0.00002989
Iteration 73/1000 | Loss: 0.00002989
Iteration 74/1000 | Loss: 0.00002989
Iteration 75/1000 | Loss: 0.00002989
Iteration 76/1000 | Loss: 0.00002989
Iteration 77/1000 | Loss: 0.00002989
Iteration 78/1000 | Loss: 0.00002989
Iteration 79/1000 | Loss: 0.00002989
Iteration 80/1000 | Loss: 0.00002988
Iteration 81/1000 | Loss: 0.00002988
Iteration 82/1000 | Loss: 0.00002988
Iteration 83/1000 | Loss: 0.00002988
Iteration 84/1000 | Loss: 0.00002988
Iteration 85/1000 | Loss: 0.00002988
Iteration 86/1000 | Loss: 0.00002988
Iteration 87/1000 | Loss: 0.00002988
Iteration 88/1000 | Loss: 0.00002988
Iteration 89/1000 | Loss: 0.00002988
Iteration 90/1000 | Loss: 0.00002988
Iteration 91/1000 | Loss: 0.00002988
Iteration 92/1000 | Loss: 0.00002988
Iteration 93/1000 | Loss: 0.00002988
Iteration 94/1000 | Loss: 0.00002988
Iteration 95/1000 | Loss: 0.00002988
Iteration 96/1000 | Loss: 0.00002988
Iteration 97/1000 | Loss: 0.00002988
Iteration 98/1000 | Loss: 0.00002988
Iteration 99/1000 | Loss: 0.00002988
Iteration 100/1000 | Loss: 0.00002988
Iteration 101/1000 | Loss: 0.00002988
Iteration 102/1000 | Loss: 0.00002988
Iteration 103/1000 | Loss: 0.00002988
Iteration 104/1000 | Loss: 0.00002988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.987898187711835e-05, 2.987898187711835e-05, 2.987898187711835e-05, 2.987898187711835e-05, 2.987898187711835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.987898187711835e-05

Optimization complete. Final v2v error: 4.445091247558594 mm

Highest mean error: 5.0351409912109375 mm for frame 81

Lowest mean error: 3.7767364978790283 mm for frame 157

Saving results

Total time: 45.68834161758423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00285779
Iteration 2/25 | Loss: 0.00127715
Iteration 3/25 | Loss: 0.00108016
Iteration 4/25 | Loss: 0.00105741
Iteration 5/25 | Loss: 0.00105157
Iteration 6/25 | Loss: 0.00104955
Iteration 7/25 | Loss: 0.00104934
Iteration 8/25 | Loss: 0.00104934
Iteration 9/25 | Loss: 0.00104934
Iteration 10/25 | Loss: 0.00104934
Iteration 11/25 | Loss: 0.00104934
Iteration 12/25 | Loss: 0.00104934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010493445442989469, 0.0010493445442989469, 0.0010493445442989469, 0.0010493445442989469, 0.0010493445442989469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010493445442989469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31734836
Iteration 2/25 | Loss: 0.00091630
Iteration 3/25 | Loss: 0.00091630
Iteration 4/25 | Loss: 0.00091630
Iteration 5/25 | Loss: 0.00091630
Iteration 6/25 | Loss: 0.00091630
Iteration 7/25 | Loss: 0.00091630
Iteration 8/25 | Loss: 0.00091630
Iteration 9/25 | Loss: 0.00091630
Iteration 10/25 | Loss: 0.00091630
Iteration 11/25 | Loss: 0.00091630
Iteration 12/25 | Loss: 0.00091630
Iteration 13/25 | Loss: 0.00091630
Iteration 14/25 | Loss: 0.00091630
Iteration 15/25 | Loss: 0.00091630
Iteration 16/25 | Loss: 0.00091630
Iteration 17/25 | Loss: 0.00091630
Iteration 18/25 | Loss: 0.00091630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009163016802631319, 0.0009163016802631319, 0.0009163016802631319, 0.0009163016802631319, 0.0009163016802631319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009163016802631319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091630
Iteration 2/1000 | Loss: 0.00003809
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00001676
Iteration 5/1000 | Loss: 0.00001540
Iteration 6/1000 | Loss: 0.00001449
Iteration 7/1000 | Loss: 0.00001372
Iteration 8/1000 | Loss: 0.00001314
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001197
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001177
Iteration 18/1000 | Loss: 0.00001176
Iteration 19/1000 | Loss: 0.00001176
Iteration 20/1000 | Loss: 0.00001175
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001167
Iteration 27/1000 | Loss: 0.00001167
Iteration 28/1000 | Loss: 0.00001166
Iteration 29/1000 | Loss: 0.00001166
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001165
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001150
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001146
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001146
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001145
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001144
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001143
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001143
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001142
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.1400402399885934e-05, 1.1400402399885934e-05, 1.1400402399885934e-05, 1.1400402399885934e-05, 1.1400402399885934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1400402399885934e-05

Optimization complete. Final v2v error: 2.886382579803467 mm

Highest mean error: 3.074608325958252 mm for frame 135

Lowest mean error: 2.6534762382507324 mm for frame 41

Saving results

Total time: 42.61655831336975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810032
Iteration 2/25 | Loss: 0.00130101
Iteration 3/25 | Loss: 0.00118155
Iteration 4/25 | Loss: 0.00114872
Iteration 5/25 | Loss: 0.00113709
Iteration 6/25 | Loss: 0.00113430
Iteration 7/25 | Loss: 0.00113339
Iteration 8/25 | Loss: 0.00113339
Iteration 9/25 | Loss: 0.00113339
Iteration 10/25 | Loss: 0.00113339
Iteration 11/25 | Loss: 0.00113339
Iteration 12/25 | Loss: 0.00113339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011333946604281664, 0.0011333946604281664, 0.0011333946604281664, 0.0011333946604281664, 0.0011333946604281664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011333946604281664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40336871
Iteration 2/25 | Loss: 0.00134500
Iteration 3/25 | Loss: 0.00134499
Iteration 4/25 | Loss: 0.00134499
Iteration 5/25 | Loss: 0.00134499
Iteration 6/25 | Loss: 0.00134499
Iteration 7/25 | Loss: 0.00134499
Iteration 8/25 | Loss: 0.00134499
Iteration 9/25 | Loss: 0.00134499
Iteration 10/25 | Loss: 0.00134499
Iteration 11/25 | Loss: 0.00134499
Iteration 12/25 | Loss: 0.00134499
Iteration 13/25 | Loss: 0.00134499
Iteration 14/25 | Loss: 0.00134499
Iteration 15/25 | Loss: 0.00134499
Iteration 16/25 | Loss: 0.00134499
Iteration 17/25 | Loss: 0.00134499
Iteration 18/25 | Loss: 0.00134499
Iteration 19/25 | Loss: 0.00134499
Iteration 20/25 | Loss: 0.00134499
Iteration 21/25 | Loss: 0.00134499
Iteration 22/25 | Loss: 0.00134499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013449875405058265, 0.0013449875405058265, 0.0013449875405058265, 0.0013449875405058265, 0.0013449875405058265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013449875405058265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134499
Iteration 2/1000 | Loss: 0.00004540
Iteration 3/1000 | Loss: 0.00003133
Iteration 4/1000 | Loss: 0.00002673
Iteration 5/1000 | Loss: 0.00002572
Iteration 6/1000 | Loss: 0.00002477
Iteration 7/1000 | Loss: 0.00002424
Iteration 8/1000 | Loss: 0.00002380
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002306
Iteration 11/1000 | Loss: 0.00002280
Iteration 12/1000 | Loss: 0.00002258
Iteration 13/1000 | Loss: 0.00002239
Iteration 14/1000 | Loss: 0.00002235
Iteration 15/1000 | Loss: 0.00002222
Iteration 16/1000 | Loss: 0.00002221
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002202
Iteration 22/1000 | Loss: 0.00002202
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00002201
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002200
Iteration 27/1000 | Loss: 0.00002200
Iteration 28/1000 | Loss: 0.00002199
Iteration 29/1000 | Loss: 0.00002199
Iteration 30/1000 | Loss: 0.00002198
Iteration 31/1000 | Loss: 0.00002198
Iteration 32/1000 | Loss: 0.00002198
Iteration 33/1000 | Loss: 0.00002197
Iteration 34/1000 | Loss: 0.00002197
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002196
Iteration 37/1000 | Loss: 0.00002196
Iteration 38/1000 | Loss: 0.00002196
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002195
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002195
Iteration 43/1000 | Loss: 0.00002194
Iteration 44/1000 | Loss: 0.00002194
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002193
Iteration 48/1000 | Loss: 0.00002193
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002192
Iteration 51/1000 | Loss: 0.00002192
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002192
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002192
Iteration 56/1000 | Loss: 0.00002192
Iteration 57/1000 | Loss: 0.00002191
Iteration 58/1000 | Loss: 0.00002191
Iteration 59/1000 | Loss: 0.00002191
Iteration 60/1000 | Loss: 0.00002190
Iteration 61/1000 | Loss: 0.00002190
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002189
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002188
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002188
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002185
Iteration 84/1000 | Loss: 0.00002185
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002185
Iteration 87/1000 | Loss: 0.00002185
Iteration 88/1000 | Loss: 0.00002185
Iteration 89/1000 | Loss: 0.00002185
Iteration 90/1000 | Loss: 0.00002185
Iteration 91/1000 | Loss: 0.00002185
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002184
Iteration 96/1000 | Loss: 0.00002184
Iteration 97/1000 | Loss: 0.00002184
Iteration 98/1000 | Loss: 0.00002184
Iteration 99/1000 | Loss: 0.00002184
Iteration 100/1000 | Loss: 0.00002184
Iteration 101/1000 | Loss: 0.00002184
Iteration 102/1000 | Loss: 0.00002183
Iteration 103/1000 | Loss: 0.00002183
Iteration 104/1000 | Loss: 0.00002183
Iteration 105/1000 | Loss: 0.00002183
Iteration 106/1000 | Loss: 0.00002183
Iteration 107/1000 | Loss: 0.00002183
Iteration 108/1000 | Loss: 0.00002183
Iteration 109/1000 | Loss: 0.00002183
Iteration 110/1000 | Loss: 0.00002183
Iteration 111/1000 | Loss: 0.00002182
Iteration 112/1000 | Loss: 0.00002182
Iteration 113/1000 | Loss: 0.00002182
Iteration 114/1000 | Loss: 0.00002182
Iteration 115/1000 | Loss: 0.00002182
Iteration 116/1000 | Loss: 0.00002182
Iteration 117/1000 | Loss: 0.00002181
Iteration 118/1000 | Loss: 0.00002181
Iteration 119/1000 | Loss: 0.00002181
Iteration 120/1000 | Loss: 0.00002181
Iteration 121/1000 | Loss: 0.00002181
Iteration 122/1000 | Loss: 0.00002181
Iteration 123/1000 | Loss: 0.00002181
Iteration 124/1000 | Loss: 0.00002181
Iteration 125/1000 | Loss: 0.00002181
Iteration 126/1000 | Loss: 0.00002181
Iteration 127/1000 | Loss: 0.00002181
Iteration 128/1000 | Loss: 0.00002181
Iteration 129/1000 | Loss: 0.00002181
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002181
Iteration 134/1000 | Loss: 0.00002181
Iteration 135/1000 | Loss: 0.00002181
Iteration 136/1000 | Loss: 0.00002180
Iteration 137/1000 | Loss: 0.00002180
Iteration 138/1000 | Loss: 0.00002179
Iteration 139/1000 | Loss: 0.00002179
Iteration 140/1000 | Loss: 0.00002179
Iteration 141/1000 | Loss: 0.00002179
Iteration 142/1000 | Loss: 0.00002179
Iteration 143/1000 | Loss: 0.00002179
Iteration 144/1000 | Loss: 0.00002179
Iteration 145/1000 | Loss: 0.00002179
Iteration 146/1000 | Loss: 0.00002179
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002178
Iteration 149/1000 | Loss: 0.00002178
Iteration 150/1000 | Loss: 0.00002178
Iteration 151/1000 | Loss: 0.00002178
Iteration 152/1000 | Loss: 0.00002178
Iteration 153/1000 | Loss: 0.00002178
Iteration 154/1000 | Loss: 0.00002178
Iteration 155/1000 | Loss: 0.00002178
Iteration 156/1000 | Loss: 0.00002177
Iteration 157/1000 | Loss: 0.00002177
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002177
Iteration 160/1000 | Loss: 0.00002176
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002176
Iteration 167/1000 | Loss: 0.00002176
Iteration 168/1000 | Loss: 0.00002176
Iteration 169/1000 | Loss: 0.00002176
Iteration 170/1000 | Loss: 0.00002176
Iteration 171/1000 | Loss: 0.00002175
Iteration 172/1000 | Loss: 0.00002175
Iteration 173/1000 | Loss: 0.00002175
Iteration 174/1000 | Loss: 0.00002175
Iteration 175/1000 | Loss: 0.00002175
Iteration 176/1000 | Loss: 0.00002175
Iteration 177/1000 | Loss: 0.00002175
Iteration 178/1000 | Loss: 0.00002175
Iteration 179/1000 | Loss: 0.00002175
Iteration 180/1000 | Loss: 0.00002175
Iteration 181/1000 | Loss: 0.00002175
Iteration 182/1000 | Loss: 0.00002175
Iteration 183/1000 | Loss: 0.00002175
Iteration 184/1000 | Loss: 0.00002174
Iteration 185/1000 | Loss: 0.00002174
Iteration 186/1000 | Loss: 0.00002174
Iteration 187/1000 | Loss: 0.00002174
Iteration 188/1000 | Loss: 0.00002174
Iteration 189/1000 | Loss: 0.00002174
Iteration 190/1000 | Loss: 0.00002174
Iteration 191/1000 | Loss: 0.00002174
Iteration 192/1000 | Loss: 0.00002174
Iteration 193/1000 | Loss: 0.00002174
Iteration 194/1000 | Loss: 0.00002174
Iteration 195/1000 | Loss: 0.00002174
Iteration 196/1000 | Loss: 0.00002174
Iteration 197/1000 | Loss: 0.00002174
Iteration 198/1000 | Loss: 0.00002174
Iteration 199/1000 | Loss: 0.00002174
Iteration 200/1000 | Loss: 0.00002174
Iteration 201/1000 | Loss: 0.00002174
Iteration 202/1000 | Loss: 0.00002174
Iteration 203/1000 | Loss: 0.00002174
Iteration 204/1000 | Loss: 0.00002174
Iteration 205/1000 | Loss: 0.00002174
Iteration 206/1000 | Loss: 0.00002173
Iteration 207/1000 | Loss: 0.00002173
Iteration 208/1000 | Loss: 0.00002173
Iteration 209/1000 | Loss: 0.00002173
Iteration 210/1000 | Loss: 0.00002173
Iteration 211/1000 | Loss: 0.00002173
Iteration 212/1000 | Loss: 0.00002173
Iteration 213/1000 | Loss: 0.00002173
Iteration 214/1000 | Loss: 0.00002173
Iteration 215/1000 | Loss: 0.00002173
Iteration 216/1000 | Loss: 0.00002173
Iteration 217/1000 | Loss: 0.00002173
Iteration 218/1000 | Loss: 0.00002173
Iteration 219/1000 | Loss: 0.00002173
Iteration 220/1000 | Loss: 0.00002173
Iteration 221/1000 | Loss: 0.00002173
Iteration 222/1000 | Loss: 0.00002173
Iteration 223/1000 | Loss: 0.00002173
Iteration 224/1000 | Loss: 0.00002173
Iteration 225/1000 | Loss: 0.00002172
Iteration 226/1000 | Loss: 0.00002172
Iteration 227/1000 | Loss: 0.00002172
Iteration 228/1000 | Loss: 0.00002172
Iteration 229/1000 | Loss: 0.00002172
Iteration 230/1000 | Loss: 0.00002172
Iteration 231/1000 | Loss: 0.00002172
Iteration 232/1000 | Loss: 0.00002172
Iteration 233/1000 | Loss: 0.00002172
Iteration 234/1000 | Loss: 0.00002172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.1724297766922973e-05, 2.1724297766922973e-05, 2.1724297766922973e-05, 2.1724297766922973e-05, 2.1724297766922973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1724297766922973e-05

Optimization complete. Final v2v error: 3.874358892440796 mm

Highest mean error: 4.451647758483887 mm for frame 77

Lowest mean error: 2.9579129219055176 mm for frame 0

Saving results

Total time: 46.459760904312134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621212
Iteration 2/25 | Loss: 0.00146358
Iteration 3/25 | Loss: 0.00120058
Iteration 4/25 | Loss: 0.00118323
Iteration 5/25 | Loss: 0.00118088
Iteration 6/25 | Loss: 0.00118055
Iteration 7/25 | Loss: 0.00118055
Iteration 8/25 | Loss: 0.00118055
Iteration 9/25 | Loss: 0.00118055
Iteration 10/25 | Loss: 0.00118055
Iteration 11/25 | Loss: 0.00118055
Iteration 12/25 | Loss: 0.00118055
Iteration 13/25 | Loss: 0.00118055
Iteration 14/25 | Loss: 0.00118055
Iteration 15/25 | Loss: 0.00118055
Iteration 16/25 | Loss: 0.00118055
Iteration 17/25 | Loss: 0.00118055
Iteration 18/25 | Loss: 0.00118055
Iteration 19/25 | Loss: 0.00118055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011805471731349826, 0.0011805471731349826, 0.0011805471731349826, 0.0011805471731349826, 0.0011805471731349826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011805471731349826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16329348
Iteration 2/25 | Loss: 0.00060971
Iteration 3/25 | Loss: 0.00060969
Iteration 4/25 | Loss: 0.00060969
Iteration 5/25 | Loss: 0.00060969
Iteration 6/25 | Loss: 0.00060968
Iteration 7/25 | Loss: 0.00060968
Iteration 8/25 | Loss: 0.00060968
Iteration 9/25 | Loss: 0.00060968
Iteration 10/25 | Loss: 0.00060968
Iteration 11/25 | Loss: 0.00060968
Iteration 12/25 | Loss: 0.00060968
Iteration 13/25 | Loss: 0.00060968
Iteration 14/25 | Loss: 0.00060968
Iteration 15/25 | Loss: 0.00060968
Iteration 16/25 | Loss: 0.00060968
Iteration 17/25 | Loss: 0.00060968
Iteration 18/25 | Loss: 0.00060968
Iteration 19/25 | Loss: 0.00060968
Iteration 20/25 | Loss: 0.00060968
Iteration 21/25 | Loss: 0.00060968
Iteration 22/25 | Loss: 0.00060968
Iteration 23/25 | Loss: 0.00060968
Iteration 24/25 | Loss: 0.00060968
Iteration 25/25 | Loss: 0.00060968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060968
Iteration 2/1000 | Loss: 0.00003787
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001587
Iteration 9/1000 | Loss: 0.00001560
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001522
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001502
Iteration 26/1000 | Loss: 0.00001502
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001500
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001499
Iteration 31/1000 | Loss: 0.00001498
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001498
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001497
Iteration 40/1000 | Loss: 0.00001494
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001494
Iteration 45/1000 | Loss: 0.00001493
Iteration 46/1000 | Loss: 0.00001493
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001474
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001469
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001468
Iteration 88/1000 | Loss: 0.00001468
Iteration 89/1000 | Loss: 0.00001468
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001466
Iteration 106/1000 | Loss: 0.00001465
Iteration 107/1000 | Loss: 0.00001465
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001465
Iteration 115/1000 | Loss: 0.00001465
Iteration 116/1000 | Loss: 0.00001465
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001464
Iteration 124/1000 | Loss: 0.00001464
Iteration 125/1000 | Loss: 0.00001464
Iteration 126/1000 | Loss: 0.00001464
Iteration 127/1000 | Loss: 0.00001464
Iteration 128/1000 | Loss: 0.00001464
Iteration 129/1000 | Loss: 0.00001464
Iteration 130/1000 | Loss: 0.00001464
Iteration 131/1000 | Loss: 0.00001464
Iteration 132/1000 | Loss: 0.00001464
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001463
Iteration 135/1000 | Loss: 0.00001463
Iteration 136/1000 | Loss: 0.00001463
Iteration 137/1000 | Loss: 0.00001463
Iteration 138/1000 | Loss: 0.00001463
Iteration 139/1000 | Loss: 0.00001463
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Iteration 159/1000 | Loss: 0.00001462
Iteration 160/1000 | Loss: 0.00001462
Iteration 161/1000 | Loss: 0.00001462
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001459
Iteration 177/1000 | Loss: 0.00001459
Iteration 178/1000 | Loss: 0.00001459
Iteration 179/1000 | Loss: 0.00001459
Iteration 180/1000 | Loss: 0.00001459
Iteration 181/1000 | Loss: 0.00001459
Iteration 182/1000 | Loss: 0.00001459
Iteration 183/1000 | Loss: 0.00001459
Iteration 184/1000 | Loss: 0.00001458
Iteration 185/1000 | Loss: 0.00001458
Iteration 186/1000 | Loss: 0.00001458
Iteration 187/1000 | Loss: 0.00001458
Iteration 188/1000 | Loss: 0.00001458
Iteration 189/1000 | Loss: 0.00001458
Iteration 190/1000 | Loss: 0.00001458
Iteration 191/1000 | Loss: 0.00001458
Iteration 192/1000 | Loss: 0.00001458
Iteration 193/1000 | Loss: 0.00001458
Iteration 194/1000 | Loss: 0.00001458
Iteration 195/1000 | Loss: 0.00001458
Iteration 196/1000 | Loss: 0.00001458
Iteration 197/1000 | Loss: 0.00001458
Iteration 198/1000 | Loss: 0.00001458
Iteration 199/1000 | Loss: 0.00001457
Iteration 200/1000 | Loss: 0.00001457
Iteration 201/1000 | Loss: 0.00001457
Iteration 202/1000 | Loss: 0.00001457
Iteration 203/1000 | Loss: 0.00001457
Iteration 204/1000 | Loss: 0.00001457
Iteration 205/1000 | Loss: 0.00001457
Iteration 206/1000 | Loss: 0.00001457
Iteration 207/1000 | Loss: 0.00001457
Iteration 208/1000 | Loss: 0.00001457
Iteration 209/1000 | Loss: 0.00001457
Iteration 210/1000 | Loss: 0.00001457
Iteration 211/1000 | Loss: 0.00001457
Iteration 212/1000 | Loss: 0.00001457
Iteration 213/1000 | Loss: 0.00001457
Iteration 214/1000 | Loss: 0.00001456
Iteration 215/1000 | Loss: 0.00001456
Iteration 216/1000 | Loss: 0.00001456
Iteration 217/1000 | Loss: 0.00001456
Iteration 218/1000 | Loss: 0.00001456
Iteration 219/1000 | Loss: 0.00001456
Iteration 220/1000 | Loss: 0.00001456
Iteration 221/1000 | Loss: 0.00001456
Iteration 222/1000 | Loss: 0.00001456
Iteration 223/1000 | Loss: 0.00001456
Iteration 224/1000 | Loss: 0.00001456
Iteration 225/1000 | Loss: 0.00001456
Iteration 226/1000 | Loss: 0.00001456
Iteration 227/1000 | Loss: 0.00001456
Iteration 228/1000 | Loss: 0.00001456
Iteration 229/1000 | Loss: 0.00001456
Iteration 230/1000 | Loss: 0.00001456
Iteration 231/1000 | Loss: 0.00001456
Iteration 232/1000 | Loss: 0.00001456
Iteration 233/1000 | Loss: 0.00001456
Iteration 234/1000 | Loss: 0.00001456
Iteration 235/1000 | Loss: 0.00001456
Iteration 236/1000 | Loss: 0.00001456
Iteration 237/1000 | Loss: 0.00001456
Iteration 238/1000 | Loss: 0.00001456
Iteration 239/1000 | Loss: 0.00001456
Iteration 240/1000 | Loss: 0.00001456
Iteration 241/1000 | Loss: 0.00001456
Iteration 242/1000 | Loss: 0.00001456
Iteration 243/1000 | Loss: 0.00001456
Iteration 244/1000 | Loss: 0.00001456
Iteration 245/1000 | Loss: 0.00001456
Iteration 246/1000 | Loss: 0.00001456
Iteration 247/1000 | Loss: 0.00001456
Iteration 248/1000 | Loss: 0.00001456
Iteration 249/1000 | Loss: 0.00001456
Iteration 250/1000 | Loss: 0.00001456
Iteration 251/1000 | Loss: 0.00001456
Iteration 252/1000 | Loss: 0.00001456
Iteration 253/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.4555410416505765e-05, 1.4555410416505765e-05, 1.4555410416505765e-05, 1.4555410416505765e-05, 1.4555410416505765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4555410416505765e-05

Optimization complete. Final v2v error: 3.2138190269470215 mm

Highest mean error: 3.8740367889404297 mm for frame 54

Lowest mean error: 3.037048578262329 mm for frame 2

Saving results

Total time: 42.69939684867859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779375
Iteration 2/25 | Loss: 0.00162556
Iteration 3/25 | Loss: 0.00132741
Iteration 4/25 | Loss: 0.00129135
Iteration 5/25 | Loss: 0.00126223
Iteration 6/25 | Loss: 0.00127582
Iteration 7/25 | Loss: 0.00127455
Iteration 8/25 | Loss: 0.00127856
Iteration 9/25 | Loss: 0.00126421
Iteration 10/25 | Loss: 0.00124888
Iteration 11/25 | Loss: 0.00124728
Iteration 12/25 | Loss: 0.00124755
Iteration 13/25 | Loss: 0.00124628
Iteration 14/25 | Loss: 0.00124352
Iteration 15/25 | Loss: 0.00123541
Iteration 16/25 | Loss: 0.00122839
Iteration 17/25 | Loss: 0.00122893
Iteration 18/25 | Loss: 0.00122501
Iteration 19/25 | Loss: 0.00122271
Iteration 20/25 | Loss: 0.00122653
Iteration 21/25 | Loss: 0.00122530
Iteration 22/25 | Loss: 0.00122167
Iteration 23/25 | Loss: 0.00122476
Iteration 24/25 | Loss: 0.00122159
Iteration 25/25 | Loss: 0.00122479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31046927
Iteration 2/25 | Loss: 0.00078432
Iteration 3/25 | Loss: 0.00078431
Iteration 4/25 | Loss: 0.00078431
Iteration 5/25 | Loss: 0.00078431
Iteration 6/25 | Loss: 0.00078431
Iteration 7/25 | Loss: 0.00078431
Iteration 8/25 | Loss: 0.00078431
Iteration 9/25 | Loss: 0.00078431
Iteration 10/25 | Loss: 0.00078431
Iteration 11/25 | Loss: 0.00078431
Iteration 12/25 | Loss: 0.00078431
Iteration 13/25 | Loss: 0.00078431
Iteration 14/25 | Loss: 0.00078431
Iteration 15/25 | Loss: 0.00078431
Iteration 16/25 | Loss: 0.00078431
Iteration 17/25 | Loss: 0.00078431
Iteration 18/25 | Loss: 0.00078431
Iteration 19/25 | Loss: 0.00078431
Iteration 20/25 | Loss: 0.00078431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007843090570531785, 0.0007843090570531785, 0.0007843090570531785, 0.0007843090570531785, 0.0007843090570531785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007843090570531785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078431
Iteration 2/1000 | Loss: 0.00006251
Iteration 3/1000 | Loss: 0.00003659
Iteration 4/1000 | Loss: 0.00003116
Iteration 5/1000 | Loss: 0.00003864
Iteration 6/1000 | Loss: 0.00002820
Iteration 7/1000 | Loss: 0.00003365
Iteration 8/1000 | Loss: 0.00005228
Iteration 9/1000 | Loss: 0.00003124
Iteration 10/1000 | Loss: 0.00003767
Iteration 11/1000 | Loss: 0.00003339
Iteration 12/1000 | Loss: 0.00002724
Iteration 13/1000 | Loss: 0.00002536
Iteration 14/1000 | Loss: 0.00002598
Iteration 15/1000 | Loss: 0.00003118
Iteration 16/1000 | Loss: 0.00003887
Iteration 17/1000 | Loss: 0.00003868
Iteration 18/1000 | Loss: 0.00003312
Iteration 19/1000 | Loss: 0.00002654
Iteration 20/1000 | Loss: 0.00003317
Iteration 21/1000 | Loss: 0.00003244
Iteration 22/1000 | Loss: 0.00003350
Iteration 23/1000 | Loss: 0.00003262
Iteration 24/1000 | Loss: 0.00003517
Iteration 25/1000 | Loss: 0.00003285
Iteration 26/1000 | Loss: 0.00003804
Iteration 27/1000 | Loss: 0.00003691
Iteration 28/1000 | Loss: 0.00004147
Iteration 29/1000 | Loss: 0.00003740
Iteration 30/1000 | Loss: 0.00002495
Iteration 31/1000 | Loss: 0.00004041
Iteration 32/1000 | Loss: 0.00002641
Iteration 33/1000 | Loss: 0.00004066
Iteration 34/1000 | Loss: 0.00004097
Iteration 35/1000 | Loss: 0.00002380
Iteration 36/1000 | Loss: 0.00002285
Iteration 37/1000 | Loss: 0.00002233
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002187
Iteration 43/1000 | Loss: 0.00002187
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002183
Iteration 46/1000 | Loss: 0.00002183
Iteration 47/1000 | Loss: 0.00002183
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002182
Iteration 52/1000 | Loss: 0.00002182
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002176
Iteration 71/1000 | Loss: 0.00002176
Iteration 72/1000 | Loss: 0.00002176
Iteration 73/1000 | Loss: 0.00002176
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002175
Iteration 79/1000 | Loss: 0.00002175
Iteration 80/1000 | Loss: 0.00002175
Iteration 81/1000 | Loss: 0.00002175
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002171
Iteration 97/1000 | Loss: 0.00002171
Iteration 98/1000 | Loss: 0.00002171
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002170
Iteration 101/1000 | Loss: 0.00002170
Iteration 102/1000 | Loss: 0.00002170
Iteration 103/1000 | Loss: 0.00002170
Iteration 104/1000 | Loss: 0.00002169
Iteration 105/1000 | Loss: 0.00002169
Iteration 106/1000 | Loss: 0.00002169
Iteration 107/1000 | Loss: 0.00002169
Iteration 108/1000 | Loss: 0.00002169
Iteration 109/1000 | Loss: 0.00002169
Iteration 110/1000 | Loss: 0.00002169
Iteration 111/1000 | Loss: 0.00002168
Iteration 112/1000 | Loss: 0.00002168
Iteration 113/1000 | Loss: 0.00002168
Iteration 114/1000 | Loss: 0.00002168
Iteration 115/1000 | Loss: 0.00002168
Iteration 116/1000 | Loss: 0.00002167
Iteration 117/1000 | Loss: 0.00002167
Iteration 118/1000 | Loss: 0.00002167
Iteration 119/1000 | Loss: 0.00002167
Iteration 120/1000 | Loss: 0.00002166
Iteration 121/1000 | Loss: 0.00002166
Iteration 122/1000 | Loss: 0.00002166
Iteration 123/1000 | Loss: 0.00002166
Iteration 124/1000 | Loss: 0.00002165
Iteration 125/1000 | Loss: 0.00002165
Iteration 126/1000 | Loss: 0.00002165
Iteration 127/1000 | Loss: 0.00002165
Iteration 128/1000 | Loss: 0.00002165
Iteration 129/1000 | Loss: 0.00002165
Iteration 130/1000 | Loss: 0.00002165
Iteration 131/1000 | Loss: 0.00002165
Iteration 132/1000 | Loss: 0.00002164
Iteration 133/1000 | Loss: 0.00002164
Iteration 134/1000 | Loss: 0.00002164
Iteration 135/1000 | Loss: 0.00002164
Iteration 136/1000 | Loss: 0.00002164
Iteration 137/1000 | Loss: 0.00002164
Iteration 138/1000 | Loss: 0.00002164
Iteration 139/1000 | Loss: 0.00002164
Iteration 140/1000 | Loss: 0.00002164
Iteration 141/1000 | Loss: 0.00002164
Iteration 142/1000 | Loss: 0.00002163
Iteration 143/1000 | Loss: 0.00002163
Iteration 144/1000 | Loss: 0.00002163
Iteration 145/1000 | Loss: 0.00002162
Iteration 146/1000 | Loss: 0.00002162
Iteration 147/1000 | Loss: 0.00002161
Iteration 148/1000 | Loss: 0.00002161
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00002159
Iteration 153/1000 | Loss: 0.00002159
Iteration 154/1000 | Loss: 0.00002158
Iteration 155/1000 | Loss: 0.00002158
Iteration 156/1000 | Loss: 0.00002158
Iteration 157/1000 | Loss: 0.00002157
Iteration 158/1000 | Loss: 0.00002157
Iteration 159/1000 | Loss: 0.00002157
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002156
Iteration 163/1000 | Loss: 0.00002156
Iteration 164/1000 | Loss: 0.00002155
Iteration 165/1000 | Loss: 0.00002154
Iteration 166/1000 | Loss: 0.00002154
Iteration 167/1000 | Loss: 0.00002153
Iteration 168/1000 | Loss: 0.00002153
Iteration 169/1000 | Loss: 0.00002152
Iteration 170/1000 | Loss: 0.00002151
Iteration 171/1000 | Loss: 0.00002151
Iteration 172/1000 | Loss: 0.00002151
Iteration 173/1000 | Loss: 0.00002151
Iteration 174/1000 | Loss: 0.00002150
Iteration 175/1000 | Loss: 0.00002150
Iteration 176/1000 | Loss: 0.00002150
Iteration 177/1000 | Loss: 0.00002149
Iteration 178/1000 | Loss: 0.00002149
Iteration 179/1000 | Loss: 0.00002149
Iteration 180/1000 | Loss: 0.00002149
Iteration 181/1000 | Loss: 0.00002149
Iteration 182/1000 | Loss: 0.00002149
Iteration 183/1000 | Loss: 0.00002148
Iteration 184/1000 | Loss: 0.00002148
Iteration 185/1000 | Loss: 0.00002148
Iteration 186/1000 | Loss: 0.00002148
Iteration 187/1000 | Loss: 0.00002147
Iteration 188/1000 | Loss: 0.00002147
Iteration 189/1000 | Loss: 0.00002147
Iteration 190/1000 | Loss: 0.00002147
Iteration 191/1000 | Loss: 0.00002147
Iteration 192/1000 | Loss: 0.00002147
Iteration 193/1000 | Loss: 0.00002147
Iteration 194/1000 | Loss: 0.00002147
Iteration 195/1000 | Loss: 0.00002146
Iteration 196/1000 | Loss: 0.00002146
Iteration 197/1000 | Loss: 0.00002146
Iteration 198/1000 | Loss: 0.00002146
Iteration 199/1000 | Loss: 0.00002146
Iteration 200/1000 | Loss: 0.00002146
Iteration 201/1000 | Loss: 0.00002146
Iteration 202/1000 | Loss: 0.00002146
Iteration 203/1000 | Loss: 0.00002145
Iteration 204/1000 | Loss: 0.00002145
Iteration 205/1000 | Loss: 0.00002145
Iteration 206/1000 | Loss: 0.00002145
Iteration 207/1000 | Loss: 0.00002145
Iteration 208/1000 | Loss: 0.00002145
Iteration 209/1000 | Loss: 0.00002145
Iteration 210/1000 | Loss: 0.00002145
Iteration 211/1000 | Loss: 0.00002145
Iteration 212/1000 | Loss: 0.00002145
Iteration 213/1000 | Loss: 0.00002145
Iteration 214/1000 | Loss: 0.00002145
Iteration 215/1000 | Loss: 0.00002144
Iteration 216/1000 | Loss: 0.00002144
Iteration 217/1000 | Loss: 0.00002144
Iteration 218/1000 | Loss: 0.00002144
Iteration 219/1000 | Loss: 0.00002144
Iteration 220/1000 | Loss: 0.00002144
Iteration 221/1000 | Loss: 0.00002144
Iteration 222/1000 | Loss: 0.00002144
Iteration 223/1000 | Loss: 0.00002144
Iteration 224/1000 | Loss: 0.00002144
Iteration 225/1000 | Loss: 0.00002144
Iteration 226/1000 | Loss: 0.00002144
Iteration 227/1000 | Loss: 0.00002144
Iteration 228/1000 | Loss: 0.00002144
Iteration 229/1000 | Loss: 0.00002144
Iteration 230/1000 | Loss: 0.00002144
Iteration 231/1000 | Loss: 0.00002143
Iteration 232/1000 | Loss: 0.00002143
Iteration 233/1000 | Loss: 0.00002143
Iteration 234/1000 | Loss: 0.00002143
Iteration 235/1000 | Loss: 0.00002143
Iteration 236/1000 | Loss: 0.00002143
Iteration 237/1000 | Loss: 0.00002143
Iteration 238/1000 | Loss: 0.00002143
Iteration 239/1000 | Loss: 0.00002143
Iteration 240/1000 | Loss: 0.00002143
Iteration 241/1000 | Loss: 0.00002143
Iteration 242/1000 | Loss: 0.00002143
Iteration 243/1000 | Loss: 0.00002143
Iteration 244/1000 | Loss: 0.00002143
Iteration 245/1000 | Loss: 0.00002143
Iteration 246/1000 | Loss: 0.00002143
Iteration 247/1000 | Loss: 0.00002143
Iteration 248/1000 | Loss: 0.00002142
Iteration 249/1000 | Loss: 0.00002142
Iteration 250/1000 | Loss: 0.00002142
Iteration 251/1000 | Loss: 0.00002142
Iteration 252/1000 | Loss: 0.00002142
Iteration 253/1000 | Loss: 0.00002142
Iteration 254/1000 | Loss: 0.00002142
Iteration 255/1000 | Loss: 0.00002142
Iteration 256/1000 | Loss: 0.00002142
Iteration 257/1000 | Loss: 0.00002142
Iteration 258/1000 | Loss: 0.00002142
Iteration 259/1000 | Loss: 0.00002142
Iteration 260/1000 | Loss: 0.00002142
Iteration 261/1000 | Loss: 0.00002142
Iteration 262/1000 | Loss: 0.00002142
Iteration 263/1000 | Loss: 0.00002142
Iteration 264/1000 | Loss: 0.00002142
Iteration 265/1000 | Loss: 0.00002142
Iteration 266/1000 | Loss: 0.00002142
Iteration 267/1000 | Loss: 0.00002142
Iteration 268/1000 | Loss: 0.00002142
Iteration 269/1000 | Loss: 0.00002142
Iteration 270/1000 | Loss: 0.00002142
Iteration 271/1000 | Loss: 0.00002142
Iteration 272/1000 | Loss: 0.00002142
Iteration 273/1000 | Loss: 0.00002142
Iteration 274/1000 | Loss: 0.00002142
Iteration 275/1000 | Loss: 0.00002142
Iteration 276/1000 | Loss: 0.00002142
Iteration 277/1000 | Loss: 0.00002142
Iteration 278/1000 | Loss: 0.00002142
Iteration 279/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [2.142015000572428e-05, 2.142015000572428e-05, 2.142015000572428e-05, 2.142015000572428e-05, 2.142015000572428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.142015000572428e-05

Optimization complete. Final v2v error: 3.7827260494232178 mm

Highest mean error: 4.586635112762451 mm for frame 125

Lowest mean error: 3.135303497314453 mm for frame 209

Saving results

Total time: 135.01500821113586
