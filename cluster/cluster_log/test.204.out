Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=204, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11424-11479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837768
Iteration 2/25 | Loss: 0.00120778
Iteration 3/25 | Loss: 0.00075667
Iteration 4/25 | Loss: 0.00067629
Iteration 5/25 | Loss: 0.00065561
Iteration 6/25 | Loss: 0.00065121
Iteration 7/25 | Loss: 0.00065054
Iteration 8/25 | Loss: 0.00065054
Iteration 9/25 | Loss: 0.00065054
Iteration 10/25 | Loss: 0.00065054
Iteration 11/25 | Loss: 0.00065054
Iteration 12/25 | Loss: 0.00065054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000650538771878928, 0.000650538771878928, 0.000650538771878928, 0.000650538771878928, 0.000650538771878928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000650538771878928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39692855
Iteration 2/25 | Loss: 0.00033485
Iteration 3/25 | Loss: 0.00033484
Iteration 4/25 | Loss: 0.00033484
Iteration 5/25 | Loss: 0.00033484
Iteration 6/25 | Loss: 0.00033484
Iteration 7/25 | Loss: 0.00033484
Iteration 8/25 | Loss: 0.00033484
Iteration 9/25 | Loss: 0.00033484
Iteration 10/25 | Loss: 0.00033484
Iteration 11/25 | Loss: 0.00033484
Iteration 12/25 | Loss: 0.00033484
Iteration 13/25 | Loss: 0.00033484
Iteration 14/25 | Loss: 0.00033484
Iteration 15/25 | Loss: 0.00033484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00033483916195109487, 0.00033483916195109487, 0.00033483916195109487, 0.00033483916195109487, 0.00033483916195109487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033483916195109487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033484
Iteration 2/1000 | Loss: 0.00002890
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001866
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001560
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001548
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001541
Iteration 14/1000 | Loss: 0.00001540
Iteration 15/1000 | Loss: 0.00001535
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001533
Iteration 18/1000 | Loss: 0.00001532
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001521
Iteration 24/1000 | Loss: 0.00001518
Iteration 25/1000 | Loss: 0.00001517
Iteration 26/1000 | Loss: 0.00001515
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001514
Iteration 29/1000 | Loss: 0.00001514
Iteration 30/1000 | Loss: 0.00001513
Iteration 31/1000 | Loss: 0.00001513
Iteration 32/1000 | Loss: 0.00001512
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001512
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001511
Iteration 37/1000 | Loss: 0.00001511
Iteration 38/1000 | Loss: 0.00001511
Iteration 39/1000 | Loss: 0.00001511
Iteration 40/1000 | Loss: 0.00001511
Iteration 41/1000 | Loss: 0.00001511
Iteration 42/1000 | Loss: 0.00001511
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.5108399566088337e-05, 1.5108399566088337e-05, 1.5108399566088337e-05, 1.5108399566088337e-05, 1.5108399566088337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5108399566088337e-05

Optimization complete. Final v2v error: 3.302823305130005 mm

Highest mean error: 4.121291160583496 mm for frame 87

Lowest mean error: 2.968053102493286 mm for frame 154

Saving results

Total time: 35.40344977378845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915941
Iteration 2/25 | Loss: 0.00133201
Iteration 3/25 | Loss: 0.00091312
Iteration 4/25 | Loss: 0.00084468
Iteration 5/25 | Loss: 0.00082911
Iteration 6/25 | Loss: 0.00082213
Iteration 7/25 | Loss: 0.00081979
Iteration 8/25 | Loss: 0.00081098
Iteration 9/25 | Loss: 0.00080239
Iteration 10/25 | Loss: 0.00078688
Iteration 11/25 | Loss: 0.00077861
Iteration 12/25 | Loss: 0.00077065
Iteration 13/25 | Loss: 0.00076670
Iteration 14/25 | Loss: 0.00076516
Iteration 15/25 | Loss: 0.00076477
Iteration 16/25 | Loss: 0.00076401
Iteration 17/25 | Loss: 0.00077736
Iteration 18/25 | Loss: 0.00077064
Iteration 19/25 | Loss: 0.00076891
Iteration 20/25 | Loss: 0.00076447
Iteration 21/25 | Loss: 0.00076449
Iteration 22/25 | Loss: 0.00076089
Iteration 23/25 | Loss: 0.00075829
Iteration 24/25 | Loss: 0.00075607
Iteration 25/25 | Loss: 0.00075428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.33362484
Iteration 2/25 | Loss: 0.00069357
Iteration 3/25 | Loss: 0.00069357
Iteration 4/25 | Loss: 0.00069357
Iteration 5/25 | Loss: 0.00069357
Iteration 6/25 | Loss: 0.00069357
Iteration 7/25 | Loss: 0.00069357
Iteration 8/25 | Loss: 0.00069357
Iteration 9/25 | Loss: 0.00069357
Iteration 10/25 | Loss: 0.00069357
Iteration 11/25 | Loss: 0.00069357
Iteration 12/25 | Loss: 0.00069357
Iteration 13/25 | Loss: 0.00069357
Iteration 14/25 | Loss: 0.00069357
Iteration 15/25 | Loss: 0.00069357
Iteration 16/25 | Loss: 0.00069357
Iteration 17/25 | Loss: 0.00069357
Iteration 18/25 | Loss: 0.00069357
Iteration 19/25 | Loss: 0.00069357
Iteration 20/25 | Loss: 0.00069357
Iteration 21/25 | Loss: 0.00069357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000693567912094295, 0.000693567912094295, 0.000693567912094295, 0.000693567912094295, 0.000693567912094295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000693567912094295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069357
Iteration 2/1000 | Loss: 0.00049288
Iteration 3/1000 | Loss: 0.00007201
Iteration 4/1000 | Loss: 0.00005081
Iteration 5/1000 | Loss: 0.00003823
Iteration 6/1000 | Loss: 0.00003406
Iteration 7/1000 | Loss: 0.00003258
Iteration 8/1000 | Loss: 0.00011000
Iteration 9/1000 | Loss: 0.00004544
Iteration 10/1000 | Loss: 0.00003763
Iteration 11/1000 | Loss: 0.00003403
Iteration 12/1000 | Loss: 0.00016549
Iteration 13/1000 | Loss: 0.00003589
Iteration 14/1000 | Loss: 0.00003289
Iteration 15/1000 | Loss: 0.00003193
Iteration 16/1000 | Loss: 0.00015787
Iteration 17/1000 | Loss: 0.00003595
Iteration 18/1000 | Loss: 0.00003229
Iteration 19/1000 | Loss: 0.00003170
Iteration 20/1000 | Loss: 0.00010456
Iteration 21/1000 | Loss: 0.00015477
Iteration 22/1000 | Loss: 0.00009614
Iteration 23/1000 | Loss: 0.00005587
Iteration 24/1000 | Loss: 0.00004918
Iteration 25/1000 | Loss: 0.00003656
Iteration 26/1000 | Loss: 0.00005024
Iteration 27/1000 | Loss: 0.00005185
Iteration 28/1000 | Loss: 0.00003395
Iteration 29/1000 | Loss: 0.00004726
Iteration 30/1000 | Loss: 0.00003596
Iteration 31/1000 | Loss: 0.00003336
Iteration 32/1000 | Loss: 0.00003214
Iteration 33/1000 | Loss: 0.00003348
Iteration 34/1000 | Loss: 0.00003014
Iteration 35/1000 | Loss: 0.00002923
Iteration 36/1000 | Loss: 0.00004249
Iteration 37/1000 | Loss: 0.00003822
Iteration 38/1000 | Loss: 0.00004098
Iteration 39/1000 | Loss: 0.00004170
Iteration 40/1000 | Loss: 0.00004281
Iteration 41/1000 | Loss: 0.00004102
Iteration 42/1000 | Loss: 0.00004280
Iteration 43/1000 | Loss: 0.00002944
Iteration 44/1000 | Loss: 0.00002864
Iteration 45/1000 | Loss: 0.00002833
Iteration 46/1000 | Loss: 0.00002779
Iteration 47/1000 | Loss: 0.00002750
Iteration 48/1000 | Loss: 0.00002733
Iteration 49/1000 | Loss: 0.00004625
Iteration 50/1000 | Loss: 0.00005342
Iteration 51/1000 | Loss: 0.00003288
Iteration 52/1000 | Loss: 0.00002922
Iteration 53/1000 | Loss: 0.00002757
Iteration 54/1000 | Loss: 0.00002692
Iteration 55/1000 | Loss: 0.00002675
Iteration 56/1000 | Loss: 0.00002655
Iteration 57/1000 | Loss: 0.00002652
Iteration 58/1000 | Loss: 0.00002642
Iteration 59/1000 | Loss: 0.00002619
Iteration 60/1000 | Loss: 0.00002617
Iteration 61/1000 | Loss: 0.00002613
Iteration 62/1000 | Loss: 0.00002612
Iteration 63/1000 | Loss: 0.00002611
Iteration 64/1000 | Loss: 0.00002611
Iteration 65/1000 | Loss: 0.00002610
Iteration 66/1000 | Loss: 0.00002606
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002603
Iteration 69/1000 | Loss: 0.00002603
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002602
Iteration 72/1000 | Loss: 0.00002602
Iteration 73/1000 | Loss: 0.00002602
Iteration 74/1000 | Loss: 0.00002602
Iteration 75/1000 | Loss: 0.00002602
Iteration 76/1000 | Loss: 0.00002602
Iteration 77/1000 | Loss: 0.00002601
Iteration 78/1000 | Loss: 0.00002601
Iteration 79/1000 | Loss: 0.00002601
Iteration 80/1000 | Loss: 0.00002601
Iteration 81/1000 | Loss: 0.00002601
Iteration 82/1000 | Loss: 0.00002601
Iteration 83/1000 | Loss: 0.00002601
Iteration 84/1000 | Loss: 0.00002600
Iteration 85/1000 | Loss: 0.00002600
Iteration 86/1000 | Loss: 0.00002600
Iteration 87/1000 | Loss: 0.00002600
Iteration 88/1000 | Loss: 0.00002599
Iteration 89/1000 | Loss: 0.00002599
Iteration 90/1000 | Loss: 0.00002599
Iteration 91/1000 | Loss: 0.00002599
Iteration 92/1000 | Loss: 0.00002598
Iteration 93/1000 | Loss: 0.00002598
Iteration 94/1000 | Loss: 0.00002598
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002597
Iteration 97/1000 | Loss: 0.00002597
Iteration 98/1000 | Loss: 0.00002597
Iteration 99/1000 | Loss: 0.00002597
Iteration 100/1000 | Loss: 0.00002597
Iteration 101/1000 | Loss: 0.00002597
Iteration 102/1000 | Loss: 0.00002597
Iteration 103/1000 | Loss: 0.00002596
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002595
Iteration 107/1000 | Loss: 0.00002595
Iteration 108/1000 | Loss: 0.00002595
Iteration 109/1000 | Loss: 0.00002594
Iteration 110/1000 | Loss: 0.00002594
Iteration 111/1000 | Loss: 0.00002594
Iteration 112/1000 | Loss: 0.00002594
Iteration 113/1000 | Loss: 0.00002593
Iteration 114/1000 | Loss: 0.00002593
Iteration 115/1000 | Loss: 0.00002593
Iteration 116/1000 | Loss: 0.00002593
Iteration 117/1000 | Loss: 0.00002592
Iteration 118/1000 | Loss: 0.00002592
Iteration 119/1000 | Loss: 0.00002592
Iteration 120/1000 | Loss: 0.00002592
Iteration 121/1000 | Loss: 0.00002592
Iteration 122/1000 | Loss: 0.00002592
Iteration 123/1000 | Loss: 0.00002592
Iteration 124/1000 | Loss: 0.00002592
Iteration 125/1000 | Loss: 0.00002592
Iteration 126/1000 | Loss: 0.00002592
Iteration 127/1000 | Loss: 0.00002592
Iteration 128/1000 | Loss: 0.00002592
Iteration 129/1000 | Loss: 0.00002591
Iteration 130/1000 | Loss: 0.00002591
Iteration 131/1000 | Loss: 0.00002591
Iteration 132/1000 | Loss: 0.00002591
Iteration 133/1000 | Loss: 0.00002591
Iteration 134/1000 | Loss: 0.00002591
Iteration 135/1000 | Loss: 0.00002591
Iteration 136/1000 | Loss: 0.00002591
Iteration 137/1000 | Loss: 0.00002591
Iteration 138/1000 | Loss: 0.00002591
Iteration 139/1000 | Loss: 0.00002591
Iteration 140/1000 | Loss: 0.00002591
Iteration 141/1000 | Loss: 0.00002591
Iteration 142/1000 | Loss: 0.00002591
Iteration 143/1000 | Loss: 0.00002591
Iteration 144/1000 | Loss: 0.00002591
Iteration 145/1000 | Loss: 0.00002591
Iteration 146/1000 | Loss: 0.00002591
Iteration 147/1000 | Loss: 0.00002590
Iteration 148/1000 | Loss: 0.00002590
Iteration 149/1000 | Loss: 0.00002590
Iteration 150/1000 | Loss: 0.00002590
Iteration 151/1000 | Loss: 0.00002590
Iteration 152/1000 | Loss: 0.00002590
Iteration 153/1000 | Loss: 0.00002590
Iteration 154/1000 | Loss: 0.00002590
Iteration 155/1000 | Loss: 0.00002590
Iteration 156/1000 | Loss: 0.00002589
Iteration 157/1000 | Loss: 0.00002589
Iteration 158/1000 | Loss: 0.00002589
Iteration 159/1000 | Loss: 0.00002589
Iteration 160/1000 | Loss: 0.00002589
Iteration 161/1000 | Loss: 0.00002589
Iteration 162/1000 | Loss: 0.00002589
Iteration 163/1000 | Loss: 0.00002589
Iteration 164/1000 | Loss: 0.00002589
Iteration 165/1000 | Loss: 0.00002589
Iteration 166/1000 | Loss: 0.00002589
Iteration 167/1000 | Loss: 0.00002589
Iteration 168/1000 | Loss: 0.00002589
Iteration 169/1000 | Loss: 0.00002589
Iteration 170/1000 | Loss: 0.00002589
Iteration 171/1000 | Loss: 0.00002589
Iteration 172/1000 | Loss: 0.00002589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.5892131816362962e-05, 2.5892131816362962e-05, 2.5892131816362962e-05, 2.5892131816362962e-05, 2.5892131816362962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5892131816362962e-05

Optimization complete. Final v2v error: 4.200927257537842 mm

Highest mean error: 6.421928882598877 mm for frame 98

Lowest mean error: 3.4875335693359375 mm for frame 134

Saving results

Total time: 148.26113104820251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954797
Iteration 2/25 | Loss: 0.00210314
Iteration 3/25 | Loss: 0.00119790
Iteration 4/25 | Loss: 0.00107114
Iteration 5/25 | Loss: 0.00099894
Iteration 6/25 | Loss: 0.00101353
Iteration 7/25 | Loss: 0.00099216
Iteration 8/25 | Loss: 0.00096905
Iteration 9/25 | Loss: 0.00095544
Iteration 10/25 | Loss: 0.00093693
Iteration 11/25 | Loss: 0.00093987
Iteration 12/25 | Loss: 0.00093468
Iteration 13/25 | Loss: 0.00092417
Iteration 14/25 | Loss: 0.00091679
Iteration 15/25 | Loss: 0.00091693
Iteration 16/25 | Loss: 0.00090886
Iteration 17/25 | Loss: 0.00092015
Iteration 18/25 | Loss: 0.00092026
Iteration 19/25 | Loss: 0.00091758
Iteration 20/25 | Loss: 0.00092009
Iteration 21/25 | Loss: 0.00091665
Iteration 22/25 | Loss: 0.00091349
Iteration 23/25 | Loss: 0.00091550
Iteration 24/25 | Loss: 0.00091180
Iteration 25/25 | Loss: 0.00091454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.46604347
Iteration 2/25 | Loss: 0.00237450
Iteration 3/25 | Loss: 0.00237450
Iteration 4/25 | Loss: 0.00237450
Iteration 5/25 | Loss: 0.00237450
Iteration 6/25 | Loss: 0.00237450
Iteration 7/25 | Loss: 0.00237450
Iteration 8/25 | Loss: 0.00237450
Iteration 9/25 | Loss: 0.00237450
Iteration 10/25 | Loss: 0.00237450
Iteration 11/25 | Loss: 0.00237450
Iteration 12/25 | Loss: 0.00237450
Iteration 13/25 | Loss: 0.00237450
Iteration 14/25 | Loss: 0.00237450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002374500734731555, 0.002374500734731555, 0.002374500734731555, 0.002374500734731555, 0.002374500734731555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002374500734731555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237450
Iteration 2/1000 | Loss: 0.00076909
Iteration 3/1000 | Loss: 0.00285401
Iteration 4/1000 | Loss: 0.00167575
Iteration 5/1000 | Loss: 0.00446717
Iteration 6/1000 | Loss: 0.00258725
Iteration 7/1000 | Loss: 0.00319391
Iteration 8/1000 | Loss: 0.00076603
Iteration 9/1000 | Loss: 0.00164022
Iteration 10/1000 | Loss: 0.00071919
Iteration 11/1000 | Loss: 0.00028699
Iteration 12/1000 | Loss: 0.00054867
Iteration 13/1000 | Loss: 0.00046448
Iteration 14/1000 | Loss: 0.00038925
Iteration 15/1000 | Loss: 0.00044169
Iteration 16/1000 | Loss: 0.00041161
Iteration 17/1000 | Loss: 0.00192711
Iteration 18/1000 | Loss: 0.00090830
Iteration 19/1000 | Loss: 0.00067876
Iteration 20/1000 | Loss: 0.00031068
Iteration 21/1000 | Loss: 0.00037120
Iteration 22/1000 | Loss: 0.00037239
Iteration 23/1000 | Loss: 0.00037039
Iteration 24/1000 | Loss: 0.00061761
Iteration 25/1000 | Loss: 0.00056988
Iteration 26/1000 | Loss: 0.00121591
Iteration 27/1000 | Loss: 0.00041082
Iteration 28/1000 | Loss: 0.00040583
Iteration 29/1000 | Loss: 0.00051109
Iteration 30/1000 | Loss: 0.00037862
Iteration 31/1000 | Loss: 0.00049241
Iteration 32/1000 | Loss: 0.00037647
Iteration 33/1000 | Loss: 0.00043308
Iteration 34/1000 | Loss: 0.00033323
Iteration 35/1000 | Loss: 0.00125083
Iteration 36/1000 | Loss: 0.00068955
Iteration 37/1000 | Loss: 0.00109631
Iteration 38/1000 | Loss: 0.00051678
Iteration 39/1000 | Loss: 0.00056377
Iteration 40/1000 | Loss: 0.00033862
Iteration 41/1000 | Loss: 0.00050696
Iteration 42/1000 | Loss: 0.00039414
Iteration 43/1000 | Loss: 0.00084859
Iteration 44/1000 | Loss: 0.00067414
Iteration 45/1000 | Loss: 0.00102815
Iteration 46/1000 | Loss: 0.00203603
Iteration 47/1000 | Loss: 0.00018115
Iteration 48/1000 | Loss: 0.00026310
Iteration 49/1000 | Loss: 0.00029589
Iteration 50/1000 | Loss: 0.00024446
Iteration 51/1000 | Loss: 0.00024789
Iteration 52/1000 | Loss: 0.00005730
Iteration 53/1000 | Loss: 0.00014236
Iteration 54/1000 | Loss: 0.00005231
Iteration 55/1000 | Loss: 0.00048593
Iteration 56/1000 | Loss: 0.00046025
Iteration 57/1000 | Loss: 0.00005681
Iteration 58/1000 | Loss: 0.00037357
Iteration 59/1000 | Loss: 0.00008605
Iteration 60/1000 | Loss: 0.00011347
Iteration 61/1000 | Loss: 0.00022695
Iteration 62/1000 | Loss: 0.00006061
Iteration 63/1000 | Loss: 0.00012179
Iteration 64/1000 | Loss: 0.00010356
Iteration 65/1000 | Loss: 0.00013364
Iteration 66/1000 | Loss: 0.00009207
Iteration 67/1000 | Loss: 0.00017599
Iteration 68/1000 | Loss: 0.00020367
Iteration 69/1000 | Loss: 0.00017840
Iteration 70/1000 | Loss: 0.00020069
Iteration 71/1000 | Loss: 0.00006336
Iteration 72/1000 | Loss: 0.00016781
Iteration 73/1000 | Loss: 0.00013701
Iteration 74/1000 | Loss: 0.00012410
Iteration 75/1000 | Loss: 0.00009593
Iteration 76/1000 | Loss: 0.00005211
Iteration 77/1000 | Loss: 0.00003986
Iteration 78/1000 | Loss: 0.00003794
Iteration 79/1000 | Loss: 0.00040512
Iteration 80/1000 | Loss: 0.00007209
Iteration 81/1000 | Loss: 0.00018538
Iteration 82/1000 | Loss: 0.00020409
Iteration 83/1000 | Loss: 0.00005081
Iteration 84/1000 | Loss: 0.00011996
Iteration 85/1000 | Loss: 0.00012560
Iteration 86/1000 | Loss: 0.00010007
Iteration 87/1000 | Loss: 0.00015665
Iteration 88/1000 | Loss: 0.00020150
Iteration 89/1000 | Loss: 0.00015021
Iteration 90/1000 | Loss: 0.00013272
Iteration 91/1000 | Loss: 0.00019704
Iteration 92/1000 | Loss: 0.00013467
Iteration 93/1000 | Loss: 0.00011213
Iteration 94/1000 | Loss: 0.00018662
Iteration 95/1000 | Loss: 0.00006319
Iteration 96/1000 | Loss: 0.00008093
Iteration 97/1000 | Loss: 0.00007120
Iteration 98/1000 | Loss: 0.00013135
Iteration 99/1000 | Loss: 0.00012644
Iteration 100/1000 | Loss: 0.00016405
Iteration 101/1000 | Loss: 0.00017645
Iteration 102/1000 | Loss: 0.00021007
Iteration 103/1000 | Loss: 0.00017192
Iteration 104/1000 | Loss: 0.00017808
Iteration 105/1000 | Loss: 0.00020413
Iteration 106/1000 | Loss: 0.00015335
Iteration 107/1000 | Loss: 0.00017391
Iteration 108/1000 | Loss: 0.00019088
Iteration 109/1000 | Loss: 0.00019995
Iteration 110/1000 | Loss: 0.00013863
Iteration 111/1000 | Loss: 0.00003437
Iteration 112/1000 | Loss: 0.00009967
Iteration 113/1000 | Loss: 0.00019478
Iteration 114/1000 | Loss: 0.00003752
Iteration 115/1000 | Loss: 0.00017918
Iteration 116/1000 | Loss: 0.00003766
Iteration 117/1000 | Loss: 0.00003186
Iteration 118/1000 | Loss: 0.00003003
Iteration 119/1000 | Loss: 0.00002887
Iteration 120/1000 | Loss: 0.00002840
Iteration 121/1000 | Loss: 0.00068131
Iteration 122/1000 | Loss: 0.00018323
Iteration 123/1000 | Loss: 0.00112907
Iteration 124/1000 | Loss: 0.00082420
Iteration 125/1000 | Loss: 0.00032009
Iteration 126/1000 | Loss: 0.00007460
Iteration 127/1000 | Loss: 0.00004094
Iteration 128/1000 | Loss: 0.00003496
Iteration 129/1000 | Loss: 0.00091288
Iteration 130/1000 | Loss: 0.00097650
Iteration 131/1000 | Loss: 0.00013935
Iteration 132/1000 | Loss: 0.00003142
Iteration 133/1000 | Loss: 0.00002824
Iteration 134/1000 | Loss: 0.00028788
Iteration 135/1000 | Loss: 0.00014970
Iteration 136/1000 | Loss: 0.00002701
Iteration 137/1000 | Loss: 0.00017384
Iteration 138/1000 | Loss: 0.00013415
Iteration 139/1000 | Loss: 0.00072674
Iteration 140/1000 | Loss: 0.00041736
Iteration 141/1000 | Loss: 0.00003756
Iteration 142/1000 | Loss: 0.00024880
Iteration 143/1000 | Loss: 0.00003666
Iteration 144/1000 | Loss: 0.00016255
Iteration 145/1000 | Loss: 0.00036569
Iteration 146/1000 | Loss: 0.00004467
Iteration 147/1000 | Loss: 0.00008256
Iteration 148/1000 | Loss: 0.00007154
Iteration 149/1000 | Loss: 0.00003380
Iteration 150/1000 | Loss: 0.00003057
Iteration 151/1000 | Loss: 0.00002955
Iteration 152/1000 | Loss: 0.00002856
Iteration 153/1000 | Loss: 0.00002757
Iteration 154/1000 | Loss: 0.00002685
Iteration 155/1000 | Loss: 0.00002655
Iteration 156/1000 | Loss: 0.00002548
Iteration 157/1000 | Loss: 0.00047064
Iteration 158/1000 | Loss: 0.00030922
Iteration 159/1000 | Loss: 0.00007855
Iteration 160/1000 | Loss: 0.00002857
Iteration 161/1000 | Loss: 0.00002593
Iteration 162/1000 | Loss: 0.00002487
Iteration 163/1000 | Loss: 0.00041823
Iteration 164/1000 | Loss: 0.00028278
Iteration 165/1000 | Loss: 0.00002436
Iteration 166/1000 | Loss: 0.00036999
Iteration 167/1000 | Loss: 0.00006828
Iteration 168/1000 | Loss: 0.00046918
Iteration 169/1000 | Loss: 0.00003791
Iteration 170/1000 | Loss: 0.00002406
Iteration 171/1000 | Loss: 0.00002362
Iteration 172/1000 | Loss: 0.00002334
Iteration 173/1000 | Loss: 0.00002181
Iteration 174/1000 | Loss: 0.00002079
Iteration 175/1000 | Loss: 0.00001982
Iteration 176/1000 | Loss: 0.00002142
Iteration 177/1000 | Loss: 0.00001962
Iteration 178/1000 | Loss: 0.00001959
Iteration 179/1000 | Loss: 0.00001914
Iteration 180/1000 | Loss: 0.00001897
Iteration 181/1000 | Loss: 0.00001897
Iteration 182/1000 | Loss: 0.00001895
Iteration 183/1000 | Loss: 0.00001894
Iteration 184/1000 | Loss: 0.00001893
Iteration 185/1000 | Loss: 0.00001892
Iteration 186/1000 | Loss: 0.00001892
Iteration 187/1000 | Loss: 0.00001892
Iteration 188/1000 | Loss: 0.00001891
Iteration 189/1000 | Loss: 0.00001888
Iteration 190/1000 | Loss: 0.00001887
Iteration 191/1000 | Loss: 0.00001886
Iteration 192/1000 | Loss: 0.00001885
Iteration 193/1000 | Loss: 0.00001885
Iteration 194/1000 | Loss: 0.00001884
Iteration 195/1000 | Loss: 0.00001884
Iteration 196/1000 | Loss: 0.00001884
Iteration 197/1000 | Loss: 0.00001884
Iteration 198/1000 | Loss: 0.00001883
Iteration 199/1000 | Loss: 0.00001882
Iteration 200/1000 | Loss: 0.00001882
Iteration 201/1000 | Loss: 0.00001881
Iteration 202/1000 | Loss: 0.00001880
Iteration 203/1000 | Loss: 0.00001880
Iteration 204/1000 | Loss: 0.00001879
Iteration 205/1000 | Loss: 0.00001879
Iteration 206/1000 | Loss: 0.00001878
Iteration 207/1000 | Loss: 0.00001878
Iteration 208/1000 | Loss: 0.00001877
Iteration 209/1000 | Loss: 0.00001877
Iteration 210/1000 | Loss: 0.00001876
Iteration 211/1000 | Loss: 0.00001875
Iteration 212/1000 | Loss: 0.00001875
Iteration 213/1000 | Loss: 0.00001875
Iteration 214/1000 | Loss: 0.00001875
Iteration 215/1000 | Loss: 0.00001875
Iteration 216/1000 | Loss: 0.00001874
Iteration 217/1000 | Loss: 0.00001874
Iteration 218/1000 | Loss: 0.00001874
Iteration 219/1000 | Loss: 0.00001874
Iteration 220/1000 | Loss: 0.00001873
Iteration 221/1000 | Loss: 0.00001873
Iteration 222/1000 | Loss: 0.00001873
Iteration 223/1000 | Loss: 0.00001873
Iteration 224/1000 | Loss: 0.00001872
Iteration 225/1000 | Loss: 0.00001872
Iteration 226/1000 | Loss: 0.00001872
Iteration 227/1000 | Loss: 0.00001871
Iteration 228/1000 | Loss: 0.00001871
Iteration 229/1000 | Loss: 0.00001871
Iteration 230/1000 | Loss: 0.00001871
Iteration 231/1000 | Loss: 0.00001871
Iteration 232/1000 | Loss: 0.00001870
Iteration 233/1000 | Loss: 0.00001870
Iteration 234/1000 | Loss: 0.00001870
Iteration 235/1000 | Loss: 0.00001870
Iteration 236/1000 | Loss: 0.00001870
Iteration 237/1000 | Loss: 0.00001870
Iteration 238/1000 | Loss: 0.00001870
Iteration 239/1000 | Loss: 0.00001870
Iteration 240/1000 | Loss: 0.00001870
Iteration 241/1000 | Loss: 0.00001870
Iteration 242/1000 | Loss: 0.00001870
Iteration 243/1000 | Loss: 0.00001870
Iteration 244/1000 | Loss: 0.00001870
Iteration 245/1000 | Loss: 0.00001870
Iteration 246/1000 | Loss: 0.00001869
Iteration 247/1000 | Loss: 0.00001869
Iteration 248/1000 | Loss: 0.00001869
Iteration 249/1000 | Loss: 0.00001869
Iteration 250/1000 | Loss: 0.00001869
Iteration 251/1000 | Loss: 0.00001869
Iteration 252/1000 | Loss: 0.00001869
Iteration 253/1000 | Loss: 0.00001869
Iteration 254/1000 | Loss: 0.00001869
Iteration 255/1000 | Loss: 0.00001869
Iteration 256/1000 | Loss: 0.00001869
Iteration 257/1000 | Loss: 0.00001878
Iteration 258/1000 | Loss: 0.00001878
Iteration 259/1000 | Loss: 0.00001852
Iteration 260/1000 | Loss: 0.00001843
Iteration 261/1000 | Loss: 0.00001840
Iteration 262/1000 | Loss: 0.00001840
Iteration 263/1000 | Loss: 0.00001839
Iteration 264/1000 | Loss: 0.00001839
Iteration 265/1000 | Loss: 0.00001838
Iteration 266/1000 | Loss: 0.00001838
Iteration 267/1000 | Loss: 0.00001837
Iteration 268/1000 | Loss: 0.00001837
Iteration 269/1000 | Loss: 0.00001837
Iteration 270/1000 | Loss: 0.00001837
Iteration 271/1000 | Loss: 0.00001836
Iteration 272/1000 | Loss: 0.00001836
Iteration 273/1000 | Loss: 0.00001835
Iteration 274/1000 | Loss: 0.00001835
Iteration 275/1000 | Loss: 0.00001835
Iteration 276/1000 | Loss: 0.00001835
Iteration 277/1000 | Loss: 0.00001835
Iteration 278/1000 | Loss: 0.00001835
Iteration 279/1000 | Loss: 0.00001835
Iteration 280/1000 | Loss: 0.00001834
Iteration 281/1000 | Loss: 0.00001834
Iteration 282/1000 | Loss: 0.00001834
Iteration 283/1000 | Loss: 0.00001834
Iteration 284/1000 | Loss: 0.00001834
Iteration 285/1000 | Loss: 0.00001834
Iteration 286/1000 | Loss: 0.00001834
Iteration 287/1000 | Loss: 0.00001834
Iteration 288/1000 | Loss: 0.00001834
Iteration 289/1000 | Loss: 0.00001834
Iteration 290/1000 | Loss: 0.00001834
Iteration 291/1000 | Loss: 0.00001833
Iteration 292/1000 | Loss: 0.00001833
Iteration 293/1000 | Loss: 0.00001833
Iteration 294/1000 | Loss: 0.00001833
Iteration 295/1000 | Loss: 0.00001833
Iteration 296/1000 | Loss: 0.00001833
Iteration 297/1000 | Loss: 0.00001833
Iteration 298/1000 | Loss: 0.00001833
Iteration 299/1000 | Loss: 0.00001833
Iteration 300/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.8333930711378343e-05, 1.8333930711378343e-05, 1.8333930711378343e-05, 1.8333930711378343e-05, 1.8333930711378343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8333930711378343e-05

Optimization complete. Final v2v error: 3.467708110809326 mm

Highest mean error: 9.04207992553711 mm for frame 106

Lowest mean error: 2.7636778354644775 mm for frame 119

Saving results

Total time: 321.0679852962494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498740
Iteration 2/25 | Loss: 0.00103052
Iteration 3/25 | Loss: 0.00069872
Iteration 4/25 | Loss: 0.00065498
Iteration 5/25 | Loss: 0.00064506
Iteration 6/25 | Loss: 0.00064267
Iteration 7/25 | Loss: 0.00064199
Iteration 8/25 | Loss: 0.00064176
Iteration 9/25 | Loss: 0.00064173
Iteration 10/25 | Loss: 0.00064173
Iteration 11/25 | Loss: 0.00064173
Iteration 12/25 | Loss: 0.00064173
Iteration 13/25 | Loss: 0.00064173
Iteration 14/25 | Loss: 0.00064173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006417256081476808, 0.0006417256081476808, 0.0006417256081476808, 0.0006417256081476808, 0.0006417256081476808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006417256081476808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47951710
Iteration 2/25 | Loss: 0.00027125
Iteration 3/25 | Loss: 0.00027125
Iteration 4/25 | Loss: 0.00027125
Iteration 5/25 | Loss: 0.00027125
Iteration 6/25 | Loss: 0.00027125
Iteration 7/25 | Loss: 0.00027125
Iteration 8/25 | Loss: 0.00027125
Iteration 9/25 | Loss: 0.00027125
Iteration 10/25 | Loss: 0.00027125
Iteration 11/25 | Loss: 0.00027125
Iteration 12/25 | Loss: 0.00027125
Iteration 13/25 | Loss: 0.00027125
Iteration 14/25 | Loss: 0.00027125
Iteration 15/25 | Loss: 0.00027125
Iteration 16/25 | Loss: 0.00027125
Iteration 17/25 | Loss: 0.00027125
Iteration 18/25 | Loss: 0.00027125
Iteration 19/25 | Loss: 0.00027125
Iteration 20/25 | Loss: 0.00027125
Iteration 21/25 | Loss: 0.00027125
Iteration 22/25 | Loss: 0.00027125
Iteration 23/25 | Loss: 0.00027125
Iteration 24/25 | Loss: 0.00027125
Iteration 25/25 | Loss: 0.00027125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027125
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00001946
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001683
Iteration 6/1000 | Loss: 0.00001627
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001556
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001498
Iteration 49/1000 | Loss: 0.00001498
Iteration 50/1000 | Loss: 0.00001497
Iteration 51/1000 | Loss: 0.00001497
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001496
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001496
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001495
Iteration 65/1000 | Loss: 0.00001495
Iteration 66/1000 | Loss: 0.00001495
Iteration 67/1000 | Loss: 0.00001495
Iteration 68/1000 | Loss: 0.00001495
Iteration 69/1000 | Loss: 0.00001495
Iteration 70/1000 | Loss: 0.00001495
Iteration 71/1000 | Loss: 0.00001494
Iteration 72/1000 | Loss: 0.00001494
Iteration 73/1000 | Loss: 0.00001494
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001494
Iteration 77/1000 | Loss: 0.00001494
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001493
Iteration 80/1000 | Loss: 0.00001493
Iteration 81/1000 | Loss: 0.00001493
Iteration 82/1000 | Loss: 0.00001493
Iteration 83/1000 | Loss: 0.00001493
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001492
Iteration 90/1000 | Loss: 0.00001492
Iteration 91/1000 | Loss: 0.00001492
Iteration 92/1000 | Loss: 0.00001492
Iteration 93/1000 | Loss: 0.00001492
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001492
Iteration 98/1000 | Loss: 0.00001492
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00001491
Iteration 102/1000 | Loss: 0.00001491
Iteration 103/1000 | Loss: 0.00001491
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001490
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001487
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001487
Iteration 136/1000 | Loss: 0.00001486
Iteration 137/1000 | Loss: 0.00001486
Iteration 138/1000 | Loss: 0.00001486
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001484
Iteration 146/1000 | Loss: 0.00001484
Iteration 147/1000 | Loss: 0.00001484
Iteration 148/1000 | Loss: 0.00001484
Iteration 149/1000 | Loss: 0.00001484
Iteration 150/1000 | Loss: 0.00001484
Iteration 151/1000 | Loss: 0.00001484
Iteration 152/1000 | Loss: 0.00001484
Iteration 153/1000 | Loss: 0.00001484
Iteration 154/1000 | Loss: 0.00001483
Iteration 155/1000 | Loss: 0.00001483
Iteration 156/1000 | Loss: 0.00001483
Iteration 157/1000 | Loss: 0.00001483
Iteration 158/1000 | Loss: 0.00001483
Iteration 159/1000 | Loss: 0.00001482
Iteration 160/1000 | Loss: 0.00001482
Iteration 161/1000 | Loss: 0.00001482
Iteration 162/1000 | Loss: 0.00001482
Iteration 163/1000 | Loss: 0.00001482
Iteration 164/1000 | Loss: 0.00001482
Iteration 165/1000 | Loss: 0.00001482
Iteration 166/1000 | Loss: 0.00001482
Iteration 167/1000 | Loss: 0.00001482
Iteration 168/1000 | Loss: 0.00001482
Iteration 169/1000 | Loss: 0.00001482
Iteration 170/1000 | Loss: 0.00001482
Iteration 171/1000 | Loss: 0.00001482
Iteration 172/1000 | Loss: 0.00001482
Iteration 173/1000 | Loss: 0.00001482
Iteration 174/1000 | Loss: 0.00001482
Iteration 175/1000 | Loss: 0.00001481
Iteration 176/1000 | Loss: 0.00001481
Iteration 177/1000 | Loss: 0.00001481
Iteration 178/1000 | Loss: 0.00001481
Iteration 179/1000 | Loss: 0.00001481
Iteration 180/1000 | Loss: 0.00001481
Iteration 181/1000 | Loss: 0.00001481
Iteration 182/1000 | Loss: 0.00001481
Iteration 183/1000 | Loss: 0.00001481
Iteration 184/1000 | Loss: 0.00001481
Iteration 185/1000 | Loss: 0.00001481
Iteration 186/1000 | Loss: 0.00001481
Iteration 187/1000 | Loss: 0.00001481
Iteration 188/1000 | Loss: 0.00001481
Iteration 189/1000 | Loss: 0.00001481
Iteration 190/1000 | Loss: 0.00001481
Iteration 191/1000 | Loss: 0.00001481
Iteration 192/1000 | Loss: 0.00001481
Iteration 193/1000 | Loss: 0.00001481
Iteration 194/1000 | Loss: 0.00001481
Iteration 195/1000 | Loss: 0.00001481
Iteration 196/1000 | Loss: 0.00001481
Iteration 197/1000 | Loss: 0.00001481
Iteration 198/1000 | Loss: 0.00001481
Iteration 199/1000 | Loss: 0.00001481
Iteration 200/1000 | Loss: 0.00001481
Iteration 201/1000 | Loss: 0.00001481
Iteration 202/1000 | Loss: 0.00001481
Iteration 203/1000 | Loss: 0.00001481
Iteration 204/1000 | Loss: 0.00001481
Iteration 205/1000 | Loss: 0.00001481
Iteration 206/1000 | Loss: 0.00001481
Iteration 207/1000 | Loss: 0.00001481
Iteration 208/1000 | Loss: 0.00001481
Iteration 209/1000 | Loss: 0.00001481
Iteration 210/1000 | Loss: 0.00001481
Iteration 211/1000 | Loss: 0.00001481
Iteration 212/1000 | Loss: 0.00001481
Iteration 213/1000 | Loss: 0.00001481
Iteration 214/1000 | Loss: 0.00001481
Iteration 215/1000 | Loss: 0.00001481
Iteration 216/1000 | Loss: 0.00001481
Iteration 217/1000 | Loss: 0.00001481
Iteration 218/1000 | Loss: 0.00001481
Iteration 219/1000 | Loss: 0.00001481
Iteration 220/1000 | Loss: 0.00001481
Iteration 221/1000 | Loss: 0.00001481
Iteration 222/1000 | Loss: 0.00001481
Iteration 223/1000 | Loss: 0.00001481
Iteration 224/1000 | Loss: 0.00001481
Iteration 225/1000 | Loss: 0.00001481
Iteration 226/1000 | Loss: 0.00001481
Iteration 227/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.480737319070613e-05, 1.480737319070613e-05, 1.480737319070613e-05, 1.480737319070613e-05, 1.480737319070613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.480737319070613e-05

Optimization complete. Final v2v error: 3.0283215045928955 mm

Highest mean error: 4.661098003387451 mm for frame 68

Lowest mean error: 2.535083532333374 mm for frame 120

Saving results

Total time: 42.898067235946655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550174
Iteration 2/25 | Loss: 0.00076201
Iteration 3/25 | Loss: 0.00064664
Iteration 4/25 | Loss: 0.00062517
Iteration 5/25 | Loss: 0.00062048
Iteration 6/25 | Loss: 0.00061964
Iteration 7/25 | Loss: 0.00061964
Iteration 8/25 | Loss: 0.00061964
Iteration 9/25 | Loss: 0.00061964
Iteration 10/25 | Loss: 0.00061964
Iteration 11/25 | Loss: 0.00061964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000619639118667692, 0.000619639118667692, 0.000619639118667692, 0.000619639118667692, 0.000619639118667692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000619639118667692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27125740
Iteration 2/25 | Loss: 0.00025536
Iteration 3/25 | Loss: 0.00025536
Iteration 4/25 | Loss: 0.00025536
Iteration 5/25 | Loss: 0.00025536
Iteration 6/25 | Loss: 0.00025536
Iteration 7/25 | Loss: 0.00025535
Iteration 8/25 | Loss: 0.00025535
Iteration 9/25 | Loss: 0.00025535
Iteration 10/25 | Loss: 0.00025535
Iteration 11/25 | Loss: 0.00025535
Iteration 12/25 | Loss: 0.00025535
Iteration 13/25 | Loss: 0.00025535
Iteration 14/25 | Loss: 0.00025535
Iteration 15/25 | Loss: 0.00025535
Iteration 16/25 | Loss: 0.00025535
Iteration 17/25 | Loss: 0.00025535
Iteration 18/25 | Loss: 0.00025535
Iteration 19/25 | Loss: 0.00025535
Iteration 20/25 | Loss: 0.00025535
Iteration 21/25 | Loss: 0.00025535
Iteration 22/25 | Loss: 0.00025535
Iteration 23/25 | Loss: 0.00025535
Iteration 24/25 | Loss: 0.00025535
Iteration 25/25 | Loss: 0.00025535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025535
Iteration 2/1000 | Loss: 0.00002622
Iteration 3/1000 | Loss: 0.00001899
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001660
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001516
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001491
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00001486
Iteration 21/1000 | Loss: 0.00001485
Iteration 22/1000 | Loss: 0.00001485
Iteration 23/1000 | Loss: 0.00001485
Iteration 24/1000 | Loss: 0.00001485
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001482
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001480
Iteration 39/1000 | Loss: 0.00001479
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001478
Iteration 44/1000 | Loss: 0.00001478
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001478
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001468
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001467
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001466
Iteration 68/1000 | Loss: 0.00001466
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001463
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001461
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001461
Iteration 109/1000 | Loss: 0.00001461
Iteration 110/1000 | Loss: 0.00001461
Iteration 111/1000 | Loss: 0.00001461
Iteration 112/1000 | Loss: 0.00001461
Iteration 113/1000 | Loss: 0.00001461
Iteration 114/1000 | Loss: 0.00001461
Iteration 115/1000 | Loss: 0.00001461
Iteration 116/1000 | Loss: 0.00001461
Iteration 117/1000 | Loss: 0.00001461
Iteration 118/1000 | Loss: 0.00001461
Iteration 119/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.4605561773350928e-05, 1.4605561773350928e-05, 1.4605561773350928e-05, 1.4605561773350928e-05, 1.4605561773350928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4605561773350928e-05

Optimization complete. Final v2v error: 3.258173704147339 mm

Highest mean error: 3.47857403755188 mm for frame 156

Lowest mean error: 3.0477354526519775 mm for frame 17

Saving results

Total time: 34.072065353393555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014827
Iteration 2/25 | Loss: 0.00241586
Iteration 3/25 | Loss: 0.00179920
Iteration 4/25 | Loss: 0.00173081
Iteration 5/25 | Loss: 0.00170752
Iteration 6/25 | Loss: 0.00166117
Iteration 7/25 | Loss: 0.00160831
Iteration 8/25 | Loss: 0.00158691
Iteration 9/25 | Loss: 0.00156688
Iteration 10/25 | Loss: 0.00156283
Iteration 11/25 | Loss: 0.00154748
Iteration 12/25 | Loss: 0.00154063
Iteration 13/25 | Loss: 0.00153731
Iteration 14/25 | Loss: 0.00153558
Iteration 15/25 | Loss: 0.00154522
Iteration 16/25 | Loss: 0.00153499
Iteration 17/25 | Loss: 0.00153150
Iteration 18/25 | Loss: 0.00153073
Iteration 19/25 | Loss: 0.00153050
Iteration 20/25 | Loss: 0.00153040
Iteration 21/25 | Loss: 0.00153038
Iteration 22/25 | Loss: 0.00153038
Iteration 23/25 | Loss: 0.00153038
Iteration 24/25 | Loss: 0.00153038
Iteration 25/25 | Loss: 0.00153038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47044194
Iteration 2/25 | Loss: 0.00504074
Iteration 3/25 | Loss: 0.00504074
Iteration 4/25 | Loss: 0.00504074
Iteration 5/25 | Loss: 0.00504074
Iteration 6/25 | Loss: 0.00504074
Iteration 7/25 | Loss: 0.00504074
Iteration 8/25 | Loss: 0.00504074
Iteration 9/25 | Loss: 0.00504074
Iteration 10/25 | Loss: 0.00504074
Iteration 11/25 | Loss: 0.00504074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.005040737334638834, 0.005040737334638834, 0.005040737334638834, 0.005040737334638834, 0.005040737334638834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005040737334638834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00504074
Iteration 2/1000 | Loss: 0.00109696
Iteration 3/1000 | Loss: 0.00427410
Iteration 4/1000 | Loss: 0.01020656
Iteration 5/1000 | Loss: 0.00534419
Iteration 6/1000 | Loss: 0.00098593
Iteration 7/1000 | Loss: 0.00064769
Iteration 8/1000 | Loss: 0.00044939
Iteration 9/1000 | Loss: 0.00030134
Iteration 10/1000 | Loss: 0.00021426
Iteration 11/1000 | Loss: 0.00016359
Iteration 12/1000 | Loss: 0.00013994
Iteration 13/1000 | Loss: 0.00012509
Iteration 14/1000 | Loss: 0.00011251
Iteration 15/1000 | Loss: 0.00010655
Iteration 16/1000 | Loss: 0.00010200
Iteration 17/1000 | Loss: 0.00009784
Iteration 18/1000 | Loss: 0.00009488
Iteration 19/1000 | Loss: 0.00009187
Iteration 20/1000 | Loss: 0.00008987
Iteration 21/1000 | Loss: 0.00008833
Iteration 22/1000 | Loss: 0.00008743
Iteration 23/1000 | Loss: 0.00008655
Iteration 24/1000 | Loss: 0.00168323
Iteration 25/1000 | Loss: 0.00441482
Iteration 26/1000 | Loss: 0.00037137
Iteration 27/1000 | Loss: 0.00019361
Iteration 28/1000 | Loss: 0.00014462
Iteration 29/1000 | Loss: 0.00010307
Iteration 30/1000 | Loss: 0.00006547
Iteration 31/1000 | Loss: 0.00004628
Iteration 32/1000 | Loss: 0.00003630
Iteration 33/1000 | Loss: 0.00003125
Iteration 34/1000 | Loss: 0.00002783
Iteration 35/1000 | Loss: 0.00002435
Iteration 36/1000 | Loss: 0.00002178
Iteration 37/1000 | Loss: 0.00002041
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001813
Iteration 41/1000 | Loss: 0.00001775
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001730
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001725
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001722
Iteration 71/1000 | Loss: 0.00001722
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001721
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001716
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001715
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001714
Iteration 99/1000 | Loss: 0.00001714
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001711
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001709
Iteration 122/1000 | Loss: 0.00001709
Iteration 123/1000 | Loss: 0.00001709
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001708
Iteration 126/1000 | Loss: 0.00001708
Iteration 127/1000 | Loss: 0.00001708
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001707
Iteration 133/1000 | Loss: 0.00001707
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001706
Iteration 138/1000 | Loss: 0.00001706
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Iteration 155/1000 | Loss: 0.00001704
Iteration 156/1000 | Loss: 0.00001704
Iteration 157/1000 | Loss: 0.00001704
Iteration 158/1000 | Loss: 0.00001704
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Iteration 165/1000 | Loss: 0.00001703
Iteration 166/1000 | Loss: 0.00001703
Iteration 167/1000 | Loss: 0.00001703
Iteration 168/1000 | Loss: 0.00001703
Iteration 169/1000 | Loss: 0.00001703
Iteration 170/1000 | Loss: 0.00001703
Iteration 171/1000 | Loss: 0.00001703
Iteration 172/1000 | Loss: 0.00001703
Iteration 173/1000 | Loss: 0.00001703
Iteration 174/1000 | Loss: 0.00001703
Iteration 175/1000 | Loss: 0.00001703
Iteration 176/1000 | Loss: 0.00001703
Iteration 177/1000 | Loss: 0.00001703
Iteration 178/1000 | Loss: 0.00001703
Iteration 179/1000 | Loss: 0.00001703
Iteration 180/1000 | Loss: 0.00001703
Iteration 181/1000 | Loss: 0.00001703
Iteration 182/1000 | Loss: 0.00001703
Iteration 183/1000 | Loss: 0.00001703
Iteration 184/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.702913868939504e-05, 1.702913868939504e-05, 1.702913868939504e-05, 1.702913868939504e-05, 1.702913868939504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.702913868939504e-05

Optimization complete. Final v2v error: 3.458580493927002 mm

Highest mean error: 3.540175199508667 mm for frame 88

Lowest mean error: 3.326547384262085 mm for frame 12

Saving results

Total time: 106.16019988059998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463147
Iteration 2/25 | Loss: 0.00103134
Iteration 3/25 | Loss: 0.00074523
Iteration 4/25 | Loss: 0.00069886
Iteration 5/25 | Loss: 0.00069052
Iteration 6/25 | Loss: 0.00068849
Iteration 7/25 | Loss: 0.00068849
Iteration 8/25 | Loss: 0.00068849
Iteration 9/25 | Loss: 0.00068849
Iteration 10/25 | Loss: 0.00068849
Iteration 11/25 | Loss: 0.00068849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000688494648784399, 0.000688494648784399, 0.000688494648784399, 0.000688494648784399, 0.000688494648784399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000688494648784399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47144353
Iteration 2/25 | Loss: 0.00030228
Iteration 3/25 | Loss: 0.00030226
Iteration 4/25 | Loss: 0.00030226
Iteration 5/25 | Loss: 0.00030226
Iteration 6/25 | Loss: 0.00030226
Iteration 7/25 | Loss: 0.00030226
Iteration 8/25 | Loss: 0.00030226
Iteration 9/25 | Loss: 0.00030226
Iteration 10/25 | Loss: 0.00030226
Iteration 11/25 | Loss: 0.00030226
Iteration 12/25 | Loss: 0.00030226
Iteration 13/25 | Loss: 0.00030226
Iteration 14/25 | Loss: 0.00030226
Iteration 15/25 | Loss: 0.00030226
Iteration 16/25 | Loss: 0.00030226
Iteration 17/25 | Loss: 0.00030226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003022612654604018, 0.0003022612654604018, 0.0003022612654604018, 0.0003022612654604018, 0.0003022612654604018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003022612654604018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030226
Iteration 2/1000 | Loss: 0.00002858
Iteration 3/1000 | Loss: 0.00002060
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001804
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001590
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001565
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001564
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001558
Iteration 53/1000 | Loss: 0.00001558
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001558
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001555
Iteration 66/1000 | Loss: 0.00001555
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001541
Iteration 125/1000 | Loss: 0.00001541
Iteration 126/1000 | Loss: 0.00001541
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001541
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001541
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001541
Iteration 143/1000 | Loss: 0.00001541
Iteration 144/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5408395483973436e-05, 1.5408395483973436e-05, 1.5408395483973436e-05, 1.5408395483973436e-05, 1.5408395483973436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5408395483973436e-05

Optimization complete. Final v2v error: 3.2724497318267822 mm

Highest mean error: 3.5141570568084717 mm for frame 166

Lowest mean error: 2.9296324253082275 mm for frame 150

Saving results

Total time: 42.08498668670654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101994
Iteration 2/25 | Loss: 0.00140009
Iteration 3/25 | Loss: 0.00094731
Iteration 4/25 | Loss: 0.00089470
Iteration 5/25 | Loss: 0.00087261
Iteration 6/25 | Loss: 0.00086427
Iteration 7/25 | Loss: 0.00086261
Iteration 8/25 | Loss: 0.00086258
Iteration 9/25 | Loss: 0.00086258
Iteration 10/25 | Loss: 0.00086258
Iteration 11/25 | Loss: 0.00086258
Iteration 12/25 | Loss: 0.00086258
Iteration 13/25 | Loss: 0.00086258
Iteration 14/25 | Loss: 0.00086258
Iteration 15/25 | Loss: 0.00086258
Iteration 16/25 | Loss: 0.00086258
Iteration 17/25 | Loss: 0.00086258
Iteration 18/25 | Loss: 0.00086258
Iteration 19/25 | Loss: 0.00086258
Iteration 20/25 | Loss: 0.00086258
Iteration 21/25 | Loss: 0.00086258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008625827031210065, 0.0008625827031210065, 0.0008625827031210065, 0.0008625827031210065, 0.0008625827031210065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008625827031210065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32827711
Iteration 2/25 | Loss: 0.00028093
Iteration 3/25 | Loss: 0.00028093
Iteration 4/25 | Loss: 0.00028093
Iteration 5/25 | Loss: 0.00028093
Iteration 6/25 | Loss: 0.00028093
Iteration 7/25 | Loss: 0.00028093
Iteration 8/25 | Loss: 0.00028093
Iteration 9/25 | Loss: 0.00028093
Iteration 10/25 | Loss: 0.00028093
Iteration 11/25 | Loss: 0.00028093
Iteration 12/25 | Loss: 0.00028093
Iteration 13/25 | Loss: 0.00028093
Iteration 14/25 | Loss: 0.00028093
Iteration 15/25 | Loss: 0.00028093
Iteration 16/25 | Loss: 0.00028093
Iteration 17/25 | Loss: 0.00028093
Iteration 18/25 | Loss: 0.00028093
Iteration 19/25 | Loss: 0.00028093
Iteration 20/25 | Loss: 0.00028093
Iteration 21/25 | Loss: 0.00028093
Iteration 22/25 | Loss: 0.00028093
Iteration 23/25 | Loss: 0.00028093
Iteration 24/25 | Loss: 0.00028093
Iteration 25/25 | Loss: 0.00028093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028093
Iteration 2/1000 | Loss: 0.00004026
Iteration 3/1000 | Loss: 0.00003115
Iteration 4/1000 | Loss: 0.00002932
Iteration 5/1000 | Loss: 0.00002815
Iteration 6/1000 | Loss: 0.00002751
Iteration 7/1000 | Loss: 0.00002718
Iteration 8/1000 | Loss: 0.00002694
Iteration 9/1000 | Loss: 0.00002668
Iteration 10/1000 | Loss: 0.00002663
Iteration 11/1000 | Loss: 0.00002659
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002656
Iteration 14/1000 | Loss: 0.00002655
Iteration 15/1000 | Loss: 0.00002655
Iteration 16/1000 | Loss: 0.00002654
Iteration 17/1000 | Loss: 0.00002654
Iteration 18/1000 | Loss: 0.00002654
Iteration 19/1000 | Loss: 0.00002653
Iteration 20/1000 | Loss: 0.00002653
Iteration 21/1000 | Loss: 0.00002653
Iteration 22/1000 | Loss: 0.00002652
Iteration 23/1000 | Loss: 0.00002652
Iteration 24/1000 | Loss: 0.00002651
Iteration 25/1000 | Loss: 0.00002651
Iteration 26/1000 | Loss: 0.00002650
Iteration 27/1000 | Loss: 0.00002650
Iteration 28/1000 | Loss: 0.00002649
Iteration 29/1000 | Loss: 0.00002649
Iteration 30/1000 | Loss: 0.00002649
Iteration 31/1000 | Loss: 0.00002648
Iteration 32/1000 | Loss: 0.00002648
Iteration 33/1000 | Loss: 0.00002648
Iteration 34/1000 | Loss: 0.00002647
Iteration 35/1000 | Loss: 0.00002647
Iteration 36/1000 | Loss: 0.00002647
Iteration 37/1000 | Loss: 0.00002647
Iteration 38/1000 | Loss: 0.00002646
Iteration 39/1000 | Loss: 0.00002646
Iteration 40/1000 | Loss: 0.00002646
Iteration 41/1000 | Loss: 0.00002646
Iteration 42/1000 | Loss: 0.00002645
Iteration 43/1000 | Loss: 0.00002645
Iteration 44/1000 | Loss: 0.00002645
Iteration 45/1000 | Loss: 0.00002645
Iteration 46/1000 | Loss: 0.00002645
Iteration 47/1000 | Loss: 0.00002645
Iteration 48/1000 | Loss: 0.00002644
Iteration 49/1000 | Loss: 0.00002644
Iteration 50/1000 | Loss: 0.00002644
Iteration 51/1000 | Loss: 0.00002644
Iteration 52/1000 | Loss: 0.00002644
Iteration 53/1000 | Loss: 0.00002644
Iteration 54/1000 | Loss: 0.00002644
Iteration 55/1000 | Loss: 0.00002644
Iteration 56/1000 | Loss: 0.00002644
Iteration 57/1000 | Loss: 0.00002644
Iteration 58/1000 | Loss: 0.00002644
Iteration 59/1000 | Loss: 0.00002644
Iteration 60/1000 | Loss: 0.00002643
Iteration 61/1000 | Loss: 0.00002643
Iteration 62/1000 | Loss: 0.00002643
Iteration 63/1000 | Loss: 0.00002643
Iteration 64/1000 | Loss: 0.00002643
Iteration 65/1000 | Loss: 0.00002643
Iteration 66/1000 | Loss: 0.00002643
Iteration 67/1000 | Loss: 0.00002643
Iteration 68/1000 | Loss: 0.00002643
Iteration 69/1000 | Loss: 0.00002643
Iteration 70/1000 | Loss: 0.00002643
Iteration 71/1000 | Loss: 0.00002642
Iteration 72/1000 | Loss: 0.00002642
Iteration 73/1000 | Loss: 0.00002642
Iteration 74/1000 | Loss: 0.00002642
Iteration 75/1000 | Loss: 0.00002642
Iteration 76/1000 | Loss: 0.00002642
Iteration 77/1000 | Loss: 0.00002642
Iteration 78/1000 | Loss: 0.00002642
Iteration 79/1000 | Loss: 0.00002642
Iteration 80/1000 | Loss: 0.00002642
Iteration 81/1000 | Loss: 0.00002642
Iteration 82/1000 | Loss: 0.00002642
Iteration 83/1000 | Loss: 0.00002642
Iteration 84/1000 | Loss: 0.00002642
Iteration 85/1000 | Loss: 0.00002641
Iteration 86/1000 | Loss: 0.00002641
Iteration 87/1000 | Loss: 0.00002640
Iteration 88/1000 | Loss: 0.00002640
Iteration 89/1000 | Loss: 0.00002640
Iteration 90/1000 | Loss: 0.00002640
Iteration 91/1000 | Loss: 0.00002640
Iteration 92/1000 | Loss: 0.00002640
Iteration 93/1000 | Loss: 0.00002640
Iteration 94/1000 | Loss: 0.00002640
Iteration 95/1000 | Loss: 0.00002639
Iteration 96/1000 | Loss: 0.00002639
Iteration 97/1000 | Loss: 0.00002639
Iteration 98/1000 | Loss: 0.00002638
Iteration 99/1000 | Loss: 0.00002638
Iteration 100/1000 | Loss: 0.00002638
Iteration 101/1000 | Loss: 0.00002638
Iteration 102/1000 | Loss: 0.00002638
Iteration 103/1000 | Loss: 0.00002638
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002638
Iteration 108/1000 | Loss: 0.00002638
Iteration 109/1000 | Loss: 0.00002638
Iteration 110/1000 | Loss: 0.00002637
Iteration 111/1000 | Loss: 0.00002637
Iteration 112/1000 | Loss: 0.00002637
Iteration 113/1000 | Loss: 0.00002637
Iteration 114/1000 | Loss: 0.00002637
Iteration 115/1000 | Loss: 0.00002637
Iteration 116/1000 | Loss: 0.00002637
Iteration 117/1000 | Loss: 0.00002637
Iteration 118/1000 | Loss: 0.00002637
Iteration 119/1000 | Loss: 0.00002637
Iteration 120/1000 | Loss: 0.00002637
Iteration 121/1000 | Loss: 0.00002637
Iteration 122/1000 | Loss: 0.00002636
Iteration 123/1000 | Loss: 0.00002636
Iteration 124/1000 | Loss: 0.00002636
Iteration 125/1000 | Loss: 0.00002636
Iteration 126/1000 | Loss: 0.00002636
Iteration 127/1000 | Loss: 0.00002636
Iteration 128/1000 | Loss: 0.00002636
Iteration 129/1000 | Loss: 0.00002636
Iteration 130/1000 | Loss: 0.00002636
Iteration 131/1000 | Loss: 0.00002636
Iteration 132/1000 | Loss: 0.00002636
Iteration 133/1000 | Loss: 0.00002636
Iteration 134/1000 | Loss: 0.00002636
Iteration 135/1000 | Loss: 0.00002636
Iteration 136/1000 | Loss: 0.00002636
Iteration 137/1000 | Loss: 0.00002636
Iteration 138/1000 | Loss: 0.00002636
Iteration 139/1000 | Loss: 0.00002636
Iteration 140/1000 | Loss: 0.00002636
Iteration 141/1000 | Loss: 0.00002636
Iteration 142/1000 | Loss: 0.00002636
Iteration 143/1000 | Loss: 0.00002636
Iteration 144/1000 | Loss: 0.00002636
Iteration 145/1000 | Loss: 0.00002636
Iteration 146/1000 | Loss: 0.00002636
Iteration 147/1000 | Loss: 0.00002636
Iteration 148/1000 | Loss: 0.00002636
Iteration 149/1000 | Loss: 0.00002636
Iteration 150/1000 | Loss: 0.00002636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.6357112801633775e-05, 2.6357112801633775e-05, 2.6357112801633775e-05, 2.6357112801633775e-05, 2.6357112801633775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6357112801633775e-05

Optimization complete. Final v2v error: 4.037286758422852 mm

Highest mean error: 4.863269805908203 mm for frame 151

Lowest mean error: 3.3946197032928467 mm for frame 11

Saving results

Total time: 38.53585958480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409426
Iteration 2/25 | Loss: 0.00112788
Iteration 3/25 | Loss: 0.00075724
Iteration 4/25 | Loss: 0.00066090
Iteration 5/25 | Loss: 0.00064711
Iteration 6/25 | Loss: 0.00064508
Iteration 7/25 | Loss: 0.00064451
Iteration 8/25 | Loss: 0.00064438
Iteration 9/25 | Loss: 0.00064438
Iteration 10/25 | Loss: 0.00064438
Iteration 11/25 | Loss: 0.00064438
Iteration 12/25 | Loss: 0.00064438
Iteration 13/25 | Loss: 0.00064438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006443829042837024, 0.0006443829042837024, 0.0006443829042837024, 0.0006443829042837024, 0.0006443829042837024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006443829042837024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44612467
Iteration 2/25 | Loss: 0.00030207
Iteration 3/25 | Loss: 0.00030206
Iteration 4/25 | Loss: 0.00030206
Iteration 5/25 | Loss: 0.00030206
Iteration 6/25 | Loss: 0.00030206
Iteration 7/25 | Loss: 0.00030206
Iteration 8/25 | Loss: 0.00030206
Iteration 9/25 | Loss: 0.00030206
Iteration 10/25 | Loss: 0.00030206
Iteration 11/25 | Loss: 0.00030206
Iteration 12/25 | Loss: 0.00030206
Iteration 13/25 | Loss: 0.00030206
Iteration 14/25 | Loss: 0.00030206
Iteration 15/25 | Loss: 0.00030206
Iteration 16/25 | Loss: 0.00030206
Iteration 17/25 | Loss: 0.00030206
Iteration 18/25 | Loss: 0.00030206
Iteration 19/25 | Loss: 0.00030206
Iteration 20/25 | Loss: 0.00030206
Iteration 21/25 | Loss: 0.00030206
Iteration 22/25 | Loss: 0.00030206
Iteration 23/25 | Loss: 0.00030206
Iteration 24/25 | Loss: 0.00030206
Iteration 25/25 | Loss: 0.00030206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030206
Iteration 2/1000 | Loss: 0.00003341
Iteration 3/1000 | Loss: 0.00002220
Iteration 4/1000 | Loss: 0.00001807
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001608
Iteration 7/1000 | Loss: 0.00001555
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001464
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001440
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001414
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001410
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001406
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001398
Iteration 39/1000 | Loss: 0.00001398
Iteration 40/1000 | Loss: 0.00001397
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001397
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00001397
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001394
Iteration 57/1000 | Loss: 0.00001394
Iteration 58/1000 | Loss: 0.00001394
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001391
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001386
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001386
Iteration 118/1000 | Loss: 0.00001386
Iteration 119/1000 | Loss: 0.00001386
Iteration 120/1000 | Loss: 0.00001385
Iteration 121/1000 | Loss: 0.00001385
Iteration 122/1000 | Loss: 0.00001385
Iteration 123/1000 | Loss: 0.00001385
Iteration 124/1000 | Loss: 0.00001385
Iteration 125/1000 | Loss: 0.00001384
Iteration 126/1000 | Loss: 0.00001384
Iteration 127/1000 | Loss: 0.00001384
Iteration 128/1000 | Loss: 0.00001384
Iteration 129/1000 | Loss: 0.00001384
Iteration 130/1000 | Loss: 0.00001384
Iteration 131/1000 | Loss: 0.00001384
Iteration 132/1000 | Loss: 0.00001384
Iteration 133/1000 | Loss: 0.00001384
Iteration 134/1000 | Loss: 0.00001384
Iteration 135/1000 | Loss: 0.00001384
Iteration 136/1000 | Loss: 0.00001384
Iteration 137/1000 | Loss: 0.00001383
Iteration 138/1000 | Loss: 0.00001383
Iteration 139/1000 | Loss: 0.00001383
Iteration 140/1000 | Loss: 0.00001383
Iteration 141/1000 | Loss: 0.00001383
Iteration 142/1000 | Loss: 0.00001383
Iteration 143/1000 | Loss: 0.00001383
Iteration 144/1000 | Loss: 0.00001383
Iteration 145/1000 | Loss: 0.00001383
Iteration 146/1000 | Loss: 0.00001383
Iteration 147/1000 | Loss: 0.00001383
Iteration 148/1000 | Loss: 0.00001383
Iteration 149/1000 | Loss: 0.00001383
Iteration 150/1000 | Loss: 0.00001383
Iteration 151/1000 | Loss: 0.00001383
Iteration 152/1000 | Loss: 0.00001383
Iteration 153/1000 | Loss: 0.00001383
Iteration 154/1000 | Loss: 0.00001383
Iteration 155/1000 | Loss: 0.00001382
Iteration 156/1000 | Loss: 0.00001382
Iteration 157/1000 | Loss: 0.00001382
Iteration 158/1000 | Loss: 0.00001382
Iteration 159/1000 | Loss: 0.00001382
Iteration 160/1000 | Loss: 0.00001382
Iteration 161/1000 | Loss: 0.00001382
Iteration 162/1000 | Loss: 0.00001382
Iteration 163/1000 | Loss: 0.00001382
Iteration 164/1000 | Loss: 0.00001382
Iteration 165/1000 | Loss: 0.00001382
Iteration 166/1000 | Loss: 0.00001381
Iteration 167/1000 | Loss: 0.00001381
Iteration 168/1000 | Loss: 0.00001381
Iteration 169/1000 | Loss: 0.00001381
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001381
Iteration 174/1000 | Loss: 0.00001381
Iteration 175/1000 | Loss: 0.00001381
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001381
Iteration 179/1000 | Loss: 0.00001381
Iteration 180/1000 | Loss: 0.00001381
Iteration 181/1000 | Loss: 0.00001381
Iteration 182/1000 | Loss: 0.00001381
Iteration 183/1000 | Loss: 0.00001381
Iteration 184/1000 | Loss: 0.00001381
Iteration 185/1000 | Loss: 0.00001381
Iteration 186/1000 | Loss: 0.00001381
Iteration 187/1000 | Loss: 0.00001381
Iteration 188/1000 | Loss: 0.00001381
Iteration 189/1000 | Loss: 0.00001381
Iteration 190/1000 | Loss: 0.00001381
Iteration 191/1000 | Loss: 0.00001381
Iteration 192/1000 | Loss: 0.00001381
Iteration 193/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.3806907190883067e-05, 1.3806907190883067e-05, 1.3806907190883067e-05, 1.3806907190883067e-05, 1.3806907190883067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3806907190883067e-05

Optimization complete. Final v2v error: 3.1746973991394043 mm

Highest mean error: 3.9003243446350098 mm for frame 30

Lowest mean error: 2.819916009902954 mm for frame 104

Saving results

Total time: 41.79351186752319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491611
Iteration 2/25 | Loss: 0.00086680
Iteration 3/25 | Loss: 0.00073918
Iteration 4/25 | Loss: 0.00071141
Iteration 5/25 | Loss: 0.00070202
Iteration 6/25 | Loss: 0.00070075
Iteration 7/25 | Loss: 0.00070067
Iteration 8/25 | Loss: 0.00070067
Iteration 9/25 | Loss: 0.00070067
Iteration 10/25 | Loss: 0.00070067
Iteration 11/25 | Loss: 0.00070067
Iteration 12/25 | Loss: 0.00070067
Iteration 13/25 | Loss: 0.00070067
Iteration 14/25 | Loss: 0.00070067
Iteration 15/25 | Loss: 0.00070067
Iteration 16/25 | Loss: 0.00070067
Iteration 17/25 | Loss: 0.00070067
Iteration 18/25 | Loss: 0.00070067
Iteration 19/25 | Loss: 0.00070067
Iteration 20/25 | Loss: 0.00070067
Iteration 21/25 | Loss: 0.00070067
Iteration 22/25 | Loss: 0.00070067
Iteration 23/25 | Loss: 0.00070067
Iteration 24/25 | Loss: 0.00070067
Iteration 25/25 | Loss: 0.00070067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007006687228567898, 0.0007006687228567898, 0.0007006687228567898, 0.0007006687228567898, 0.0007006687228567898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007006687228567898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41661584
Iteration 2/25 | Loss: 0.00023456
Iteration 3/25 | Loss: 0.00023453
Iteration 4/25 | Loss: 0.00023453
Iteration 5/25 | Loss: 0.00023453
Iteration 6/25 | Loss: 0.00023453
Iteration 7/25 | Loss: 0.00023453
Iteration 8/25 | Loss: 0.00023453
Iteration 9/25 | Loss: 0.00023453
Iteration 10/25 | Loss: 0.00023453
Iteration 11/25 | Loss: 0.00023453
Iteration 12/25 | Loss: 0.00023453
Iteration 13/25 | Loss: 0.00023453
Iteration 14/25 | Loss: 0.00023453
Iteration 15/25 | Loss: 0.00023453
Iteration 16/25 | Loss: 0.00023453
Iteration 17/25 | Loss: 0.00023453
Iteration 18/25 | Loss: 0.00023453
Iteration 19/25 | Loss: 0.00023453
Iteration 20/25 | Loss: 0.00023453
Iteration 21/25 | Loss: 0.00023453
Iteration 22/25 | Loss: 0.00023453
Iteration 23/25 | Loss: 0.00023453
Iteration 24/25 | Loss: 0.00023453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002345312386751175, 0.0002345312386751175, 0.0002345312386751175, 0.0002345312386751175, 0.0002345312386751175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002345312386751175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023453
Iteration 2/1000 | Loss: 0.00004473
Iteration 3/1000 | Loss: 0.00003116
Iteration 4/1000 | Loss: 0.00002794
Iteration 5/1000 | Loss: 0.00002610
Iteration 6/1000 | Loss: 0.00002485
Iteration 7/1000 | Loss: 0.00002408
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002307
Iteration 10/1000 | Loss: 0.00002302
Iteration 11/1000 | Loss: 0.00002278
Iteration 12/1000 | Loss: 0.00002257
Iteration 13/1000 | Loss: 0.00002240
Iteration 14/1000 | Loss: 0.00002237
Iteration 15/1000 | Loss: 0.00002236
Iteration 16/1000 | Loss: 0.00002231
Iteration 17/1000 | Loss: 0.00002230
Iteration 18/1000 | Loss: 0.00002228
Iteration 19/1000 | Loss: 0.00002228
Iteration 20/1000 | Loss: 0.00002228
Iteration 21/1000 | Loss: 0.00002226
Iteration 22/1000 | Loss: 0.00002226
Iteration 23/1000 | Loss: 0.00002224
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00002224
Iteration 26/1000 | Loss: 0.00002224
Iteration 27/1000 | Loss: 0.00002223
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002222
Iteration 30/1000 | Loss: 0.00002222
Iteration 31/1000 | Loss: 0.00002221
Iteration 32/1000 | Loss: 0.00002221
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002220
Iteration 35/1000 | Loss: 0.00002219
Iteration 36/1000 | Loss: 0.00002218
Iteration 37/1000 | Loss: 0.00002218
Iteration 38/1000 | Loss: 0.00002218
Iteration 39/1000 | Loss: 0.00002217
Iteration 40/1000 | Loss: 0.00002217
Iteration 41/1000 | Loss: 0.00002217
Iteration 42/1000 | Loss: 0.00002216
Iteration 43/1000 | Loss: 0.00002216
Iteration 44/1000 | Loss: 0.00002215
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002213
Iteration 49/1000 | Loss: 0.00002213
Iteration 50/1000 | Loss: 0.00002212
Iteration 51/1000 | Loss: 0.00002211
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002209
Iteration 55/1000 | Loss: 0.00002209
Iteration 56/1000 | Loss: 0.00002209
Iteration 57/1000 | Loss: 0.00002209
Iteration 58/1000 | Loss: 0.00002209
Iteration 59/1000 | Loss: 0.00002209
Iteration 60/1000 | Loss: 0.00002209
Iteration 61/1000 | Loss: 0.00002209
Iteration 62/1000 | Loss: 0.00002208
Iteration 63/1000 | Loss: 0.00002208
Iteration 64/1000 | Loss: 0.00002207
Iteration 65/1000 | Loss: 0.00002207
Iteration 66/1000 | Loss: 0.00002207
Iteration 67/1000 | Loss: 0.00002207
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002207
Iteration 70/1000 | Loss: 0.00002207
Iteration 71/1000 | Loss: 0.00002207
Iteration 72/1000 | Loss: 0.00002207
Iteration 73/1000 | Loss: 0.00002206
Iteration 74/1000 | Loss: 0.00002206
Iteration 75/1000 | Loss: 0.00002205
Iteration 76/1000 | Loss: 0.00002205
Iteration 77/1000 | Loss: 0.00002205
Iteration 78/1000 | Loss: 0.00002204
Iteration 79/1000 | Loss: 0.00002204
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002203
Iteration 83/1000 | Loss: 0.00002203
Iteration 84/1000 | Loss: 0.00002203
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002202
Iteration 87/1000 | Loss: 0.00002202
Iteration 88/1000 | Loss: 0.00002202
Iteration 89/1000 | Loss: 0.00002202
Iteration 90/1000 | Loss: 0.00002202
Iteration 91/1000 | Loss: 0.00002202
Iteration 92/1000 | Loss: 0.00002202
Iteration 93/1000 | Loss: 0.00002202
Iteration 94/1000 | Loss: 0.00002202
Iteration 95/1000 | Loss: 0.00002202
Iteration 96/1000 | Loss: 0.00002201
Iteration 97/1000 | Loss: 0.00002201
Iteration 98/1000 | Loss: 0.00002201
Iteration 99/1000 | Loss: 0.00002201
Iteration 100/1000 | Loss: 0.00002201
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002200
Iteration 103/1000 | Loss: 0.00002200
Iteration 104/1000 | Loss: 0.00002200
Iteration 105/1000 | Loss: 0.00002199
Iteration 106/1000 | Loss: 0.00002199
Iteration 107/1000 | Loss: 0.00002199
Iteration 108/1000 | Loss: 0.00002199
Iteration 109/1000 | Loss: 0.00002199
Iteration 110/1000 | Loss: 0.00002197
Iteration 111/1000 | Loss: 0.00002197
Iteration 112/1000 | Loss: 0.00002197
Iteration 113/1000 | Loss: 0.00002197
Iteration 114/1000 | Loss: 0.00002197
Iteration 115/1000 | Loss: 0.00002197
Iteration 116/1000 | Loss: 0.00002197
Iteration 117/1000 | Loss: 0.00002197
Iteration 118/1000 | Loss: 0.00002197
Iteration 119/1000 | Loss: 0.00002197
Iteration 120/1000 | Loss: 0.00002197
Iteration 121/1000 | Loss: 0.00002197
Iteration 122/1000 | Loss: 0.00002196
Iteration 123/1000 | Loss: 0.00002196
Iteration 124/1000 | Loss: 0.00002196
Iteration 125/1000 | Loss: 0.00002196
Iteration 126/1000 | Loss: 0.00002196
Iteration 127/1000 | Loss: 0.00002196
Iteration 128/1000 | Loss: 0.00002196
Iteration 129/1000 | Loss: 0.00002196
Iteration 130/1000 | Loss: 0.00002195
Iteration 131/1000 | Loss: 0.00002195
Iteration 132/1000 | Loss: 0.00002194
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002194
Iteration 135/1000 | Loss: 0.00002194
Iteration 136/1000 | Loss: 0.00002194
Iteration 137/1000 | Loss: 0.00002194
Iteration 138/1000 | Loss: 0.00002194
Iteration 139/1000 | Loss: 0.00002194
Iteration 140/1000 | Loss: 0.00002194
Iteration 141/1000 | Loss: 0.00002194
Iteration 142/1000 | Loss: 0.00002194
Iteration 143/1000 | Loss: 0.00002194
Iteration 144/1000 | Loss: 0.00002194
Iteration 145/1000 | Loss: 0.00002193
Iteration 146/1000 | Loss: 0.00002193
Iteration 147/1000 | Loss: 0.00002193
Iteration 148/1000 | Loss: 0.00002193
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00002192
Iteration 151/1000 | Loss: 0.00002192
Iteration 152/1000 | Loss: 0.00002192
Iteration 153/1000 | Loss: 0.00002192
Iteration 154/1000 | Loss: 0.00002192
Iteration 155/1000 | Loss: 0.00002192
Iteration 156/1000 | Loss: 0.00002192
Iteration 157/1000 | Loss: 0.00002192
Iteration 158/1000 | Loss: 0.00002191
Iteration 159/1000 | Loss: 0.00002191
Iteration 160/1000 | Loss: 0.00002191
Iteration 161/1000 | Loss: 0.00002191
Iteration 162/1000 | Loss: 0.00002191
Iteration 163/1000 | Loss: 0.00002191
Iteration 164/1000 | Loss: 0.00002191
Iteration 165/1000 | Loss: 0.00002191
Iteration 166/1000 | Loss: 0.00002191
Iteration 167/1000 | Loss: 0.00002191
Iteration 168/1000 | Loss: 0.00002191
Iteration 169/1000 | Loss: 0.00002191
Iteration 170/1000 | Loss: 0.00002191
Iteration 171/1000 | Loss: 0.00002191
Iteration 172/1000 | Loss: 0.00002191
Iteration 173/1000 | Loss: 0.00002191
Iteration 174/1000 | Loss: 0.00002191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.1912503143539652e-05, 2.1912503143539652e-05, 2.1912503143539652e-05, 2.1912503143539652e-05, 2.1912503143539652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1912503143539652e-05

Optimization complete. Final v2v error: 3.882539987564087 mm

Highest mean error: 4.307895660400391 mm for frame 101

Lowest mean error: 3.2871406078338623 mm for frame 20

Saving results

Total time: 40.28380084037781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019215
Iteration 2/25 | Loss: 0.00366337
Iteration 3/25 | Loss: 0.00223106
Iteration 4/25 | Loss: 0.00197021
Iteration 5/25 | Loss: 0.00162412
Iteration 6/25 | Loss: 0.00142334
Iteration 7/25 | Loss: 0.00136298
Iteration 8/25 | Loss: 0.00132759
Iteration 9/25 | Loss: 0.00125450
Iteration 10/25 | Loss: 0.00125832
Iteration 11/25 | Loss: 0.00123570
Iteration 12/25 | Loss: 0.00119686
Iteration 13/25 | Loss: 0.00120128
Iteration 14/25 | Loss: 0.00115240
Iteration 15/25 | Loss: 0.00111820
Iteration 16/25 | Loss: 0.00110547
Iteration 17/25 | Loss: 0.00109659
Iteration 18/25 | Loss: 0.00110285
Iteration 19/25 | Loss: 0.00108506
Iteration 20/25 | Loss: 0.00108549
Iteration 21/25 | Loss: 0.00108122
Iteration 22/25 | Loss: 0.00107526
Iteration 23/25 | Loss: 0.00105595
Iteration 24/25 | Loss: 0.00107527
Iteration 25/25 | Loss: 0.00108548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48594749
Iteration 2/25 | Loss: 0.00579751
Iteration 3/25 | Loss: 0.00374926
Iteration 4/25 | Loss: 0.00374900
Iteration 5/25 | Loss: 0.00374900
Iteration 6/25 | Loss: 0.00374900
Iteration 7/25 | Loss: 0.00374900
Iteration 8/25 | Loss: 0.00374900
Iteration 9/25 | Loss: 0.00374900
Iteration 10/25 | Loss: 0.00374900
Iteration 11/25 | Loss: 0.00374900
Iteration 12/25 | Loss: 0.00374900
Iteration 13/25 | Loss: 0.00374900
Iteration 14/25 | Loss: 0.00374900
Iteration 15/25 | Loss: 0.00374900
Iteration 16/25 | Loss: 0.00374900
Iteration 17/25 | Loss: 0.00374900
Iteration 18/25 | Loss: 0.00374900
Iteration 19/25 | Loss: 0.00374900
Iteration 20/25 | Loss: 0.00374900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0037490001413971186, 0.0037490001413971186, 0.0037490001413971186, 0.0037490001413971186, 0.0037490001413971186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037490001413971186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374900
Iteration 2/1000 | Loss: 0.00919346
Iteration 3/1000 | Loss: 0.00747077
Iteration 4/1000 | Loss: 0.00236035
Iteration 5/1000 | Loss: 0.00260539
Iteration 6/1000 | Loss: 0.00257593
Iteration 7/1000 | Loss: 0.00146821
Iteration 8/1000 | Loss: 0.00276685
Iteration 9/1000 | Loss: 0.00289842
Iteration 10/1000 | Loss: 0.00347868
Iteration 11/1000 | Loss: 0.00151778
Iteration 12/1000 | Loss: 0.00120200
Iteration 13/1000 | Loss: 0.00189899
Iteration 14/1000 | Loss: 0.00082071
Iteration 15/1000 | Loss: 0.00142031
Iteration 16/1000 | Loss: 0.00173929
Iteration 17/1000 | Loss: 0.00255902
Iteration 18/1000 | Loss: 0.00190493
Iteration 19/1000 | Loss: 0.00138687
Iteration 20/1000 | Loss: 0.00228418
Iteration 21/1000 | Loss: 0.00181680
Iteration 22/1000 | Loss: 0.00065430
Iteration 23/1000 | Loss: 0.00094734
Iteration 24/1000 | Loss: 0.00159926
Iteration 25/1000 | Loss: 0.00069846
Iteration 26/1000 | Loss: 0.00176594
Iteration 27/1000 | Loss: 0.00039529
Iteration 28/1000 | Loss: 0.00059852
Iteration 29/1000 | Loss: 0.00059383
Iteration 30/1000 | Loss: 0.00070530
Iteration 31/1000 | Loss: 0.00054989
Iteration 32/1000 | Loss: 0.00047488
Iteration 33/1000 | Loss: 0.00047868
Iteration 34/1000 | Loss: 0.00032799
Iteration 35/1000 | Loss: 0.00037430
Iteration 36/1000 | Loss: 0.00064837
Iteration 37/1000 | Loss: 0.00106803
Iteration 38/1000 | Loss: 0.00084064
Iteration 39/1000 | Loss: 0.00066931
Iteration 40/1000 | Loss: 0.00043633
Iteration 41/1000 | Loss: 0.00111482
Iteration 42/1000 | Loss: 0.00135933
Iteration 43/1000 | Loss: 0.00118321
Iteration 44/1000 | Loss: 0.00187006
Iteration 45/1000 | Loss: 0.00179376
Iteration 46/1000 | Loss: 0.00096676
Iteration 47/1000 | Loss: 0.00236466
Iteration 48/1000 | Loss: 0.00047216
Iteration 49/1000 | Loss: 0.00055500
Iteration 50/1000 | Loss: 0.00033628
Iteration 51/1000 | Loss: 0.00047031
Iteration 52/1000 | Loss: 0.00035662
Iteration 53/1000 | Loss: 0.00070516
Iteration 54/1000 | Loss: 0.00035182
Iteration 55/1000 | Loss: 0.00030626
Iteration 56/1000 | Loss: 0.00016692
Iteration 57/1000 | Loss: 0.00040460
Iteration 58/1000 | Loss: 0.00039051
Iteration 59/1000 | Loss: 0.00009507
Iteration 60/1000 | Loss: 0.00049152
Iteration 61/1000 | Loss: 0.00217136
Iteration 62/1000 | Loss: 0.00031914
Iteration 63/1000 | Loss: 0.00022775
Iteration 64/1000 | Loss: 0.00032640
Iteration 65/1000 | Loss: 0.00021260
Iteration 66/1000 | Loss: 0.00011567
Iteration 67/1000 | Loss: 0.00031841
Iteration 68/1000 | Loss: 0.00103508
Iteration 69/1000 | Loss: 0.00042856
Iteration 70/1000 | Loss: 0.00025169
Iteration 71/1000 | Loss: 0.00012743
Iteration 72/1000 | Loss: 0.00007207
Iteration 73/1000 | Loss: 0.00059927
Iteration 74/1000 | Loss: 0.00149677
Iteration 75/1000 | Loss: 0.00037039
Iteration 76/1000 | Loss: 0.00036438
Iteration 77/1000 | Loss: 0.00006748
Iteration 78/1000 | Loss: 0.00020357
Iteration 79/1000 | Loss: 0.00009081
Iteration 80/1000 | Loss: 0.00012078
Iteration 81/1000 | Loss: 0.00011943
Iteration 82/1000 | Loss: 0.00047295
Iteration 83/1000 | Loss: 0.00026930
Iteration 84/1000 | Loss: 0.00015671
Iteration 85/1000 | Loss: 0.00113676
Iteration 86/1000 | Loss: 0.00063597
Iteration 87/1000 | Loss: 0.00014105
Iteration 88/1000 | Loss: 0.00021756
Iteration 89/1000 | Loss: 0.00007162
Iteration 90/1000 | Loss: 0.00031956
Iteration 91/1000 | Loss: 0.00012593
Iteration 92/1000 | Loss: 0.00028480
Iteration 93/1000 | Loss: 0.00028623
Iteration 94/1000 | Loss: 0.00028256
Iteration 95/1000 | Loss: 0.00037998
Iteration 96/1000 | Loss: 0.00024777
Iteration 97/1000 | Loss: 0.00010432
Iteration 98/1000 | Loss: 0.00008444
Iteration 99/1000 | Loss: 0.00006500
Iteration 100/1000 | Loss: 0.00007029
Iteration 101/1000 | Loss: 0.00009580
Iteration 102/1000 | Loss: 0.00035153
Iteration 103/1000 | Loss: 0.00034758
Iteration 104/1000 | Loss: 0.00031811
Iteration 105/1000 | Loss: 0.00106637
Iteration 106/1000 | Loss: 0.00017587
Iteration 107/1000 | Loss: 0.00023463
Iteration 108/1000 | Loss: 0.00015973
Iteration 109/1000 | Loss: 0.00009930
Iteration 110/1000 | Loss: 0.00007469
Iteration 111/1000 | Loss: 0.00005978
Iteration 112/1000 | Loss: 0.00005213
Iteration 113/1000 | Loss: 0.00017024
Iteration 114/1000 | Loss: 0.00005118
Iteration 115/1000 | Loss: 0.00027105
Iteration 116/1000 | Loss: 0.00009495
Iteration 117/1000 | Loss: 0.00035837
Iteration 118/1000 | Loss: 0.00034063
Iteration 119/1000 | Loss: 0.00016420
Iteration 120/1000 | Loss: 0.00018615
Iteration 121/1000 | Loss: 0.00014439
Iteration 122/1000 | Loss: 0.00015469
Iteration 123/1000 | Loss: 0.00006584
Iteration 124/1000 | Loss: 0.00021700
Iteration 125/1000 | Loss: 0.00038536
Iteration 126/1000 | Loss: 0.00030764
Iteration 127/1000 | Loss: 0.00041267
Iteration 128/1000 | Loss: 0.00014091
Iteration 129/1000 | Loss: 0.00019026
Iteration 130/1000 | Loss: 0.00058752
Iteration 131/1000 | Loss: 0.00029556
Iteration 132/1000 | Loss: 0.00022626
Iteration 133/1000 | Loss: 0.00010795
Iteration 134/1000 | Loss: 0.00016998
Iteration 135/1000 | Loss: 0.00018013
Iteration 136/1000 | Loss: 0.00022504
Iteration 137/1000 | Loss: 0.00018535
Iteration 138/1000 | Loss: 0.00023104
Iteration 139/1000 | Loss: 0.00018921
Iteration 140/1000 | Loss: 0.00016688
Iteration 141/1000 | Loss: 0.00017243
Iteration 142/1000 | Loss: 0.00046189
Iteration 143/1000 | Loss: 0.00027445
Iteration 144/1000 | Loss: 0.00022503
Iteration 145/1000 | Loss: 0.00013788
Iteration 146/1000 | Loss: 0.00020702
Iteration 147/1000 | Loss: 0.00023381
Iteration 148/1000 | Loss: 0.00046561
Iteration 149/1000 | Loss: 0.00019021
Iteration 150/1000 | Loss: 0.00012170
Iteration 151/1000 | Loss: 0.00020802
Iteration 152/1000 | Loss: 0.00006293
Iteration 153/1000 | Loss: 0.00006889
Iteration 154/1000 | Loss: 0.00033277
Iteration 155/1000 | Loss: 0.00021381
Iteration 156/1000 | Loss: 0.00015957
Iteration 157/1000 | Loss: 0.00009144
Iteration 158/1000 | Loss: 0.00013574
Iteration 159/1000 | Loss: 0.00011370
Iteration 160/1000 | Loss: 0.00011247
Iteration 161/1000 | Loss: 0.00005476
Iteration 162/1000 | Loss: 0.00009773
Iteration 163/1000 | Loss: 0.00011223
Iteration 164/1000 | Loss: 0.00026002
Iteration 165/1000 | Loss: 0.00007168
Iteration 166/1000 | Loss: 0.00014440
Iteration 167/1000 | Loss: 0.00017636
Iteration 168/1000 | Loss: 0.00006238
Iteration 169/1000 | Loss: 0.00020900
Iteration 170/1000 | Loss: 0.00017884
Iteration 171/1000 | Loss: 0.00019881
Iteration 172/1000 | Loss: 0.00004690
Iteration 173/1000 | Loss: 0.00004754
Iteration 174/1000 | Loss: 0.00004581
Iteration 175/1000 | Loss: 0.00004538
Iteration 176/1000 | Loss: 0.00004503
Iteration 177/1000 | Loss: 0.00009377
Iteration 178/1000 | Loss: 0.00005804
Iteration 179/1000 | Loss: 0.00004440
Iteration 180/1000 | Loss: 0.00019506
Iteration 181/1000 | Loss: 0.00004878
Iteration 182/1000 | Loss: 0.00004686
Iteration 183/1000 | Loss: 0.00004634
Iteration 184/1000 | Loss: 0.00012740
Iteration 185/1000 | Loss: 0.00028021
Iteration 186/1000 | Loss: 0.00009195
Iteration 187/1000 | Loss: 0.00005056
Iteration 188/1000 | Loss: 0.00004505
Iteration 189/1000 | Loss: 0.00009220
Iteration 190/1000 | Loss: 0.00004311
Iteration 191/1000 | Loss: 0.00004260
Iteration 192/1000 | Loss: 0.00004221
Iteration 193/1000 | Loss: 0.00006552
Iteration 194/1000 | Loss: 0.00008706
Iteration 195/1000 | Loss: 0.00004726
Iteration 196/1000 | Loss: 0.00004190
Iteration 197/1000 | Loss: 0.00004177
Iteration 198/1000 | Loss: 0.00004175
Iteration 199/1000 | Loss: 0.00004167
Iteration 200/1000 | Loss: 0.00004167
Iteration 201/1000 | Loss: 0.00004166
Iteration 202/1000 | Loss: 0.00011707
Iteration 203/1000 | Loss: 0.00004382
Iteration 204/1000 | Loss: 0.00004138
Iteration 205/1000 | Loss: 0.00004121
Iteration 206/1000 | Loss: 0.00004110
Iteration 207/1000 | Loss: 0.00004110
Iteration 208/1000 | Loss: 0.00004109
Iteration 209/1000 | Loss: 0.00004104
Iteration 210/1000 | Loss: 0.00004104
Iteration 211/1000 | Loss: 0.00004104
Iteration 212/1000 | Loss: 0.00004104
Iteration 213/1000 | Loss: 0.00004103
Iteration 214/1000 | Loss: 0.00004103
Iteration 215/1000 | Loss: 0.00004103
Iteration 216/1000 | Loss: 0.00004099
Iteration 217/1000 | Loss: 0.00004098
Iteration 218/1000 | Loss: 0.00004098
Iteration 219/1000 | Loss: 0.00004095
Iteration 220/1000 | Loss: 0.00004095
Iteration 221/1000 | Loss: 0.00004094
Iteration 222/1000 | Loss: 0.00020509
Iteration 223/1000 | Loss: 0.00004599
Iteration 224/1000 | Loss: 0.00012949
Iteration 225/1000 | Loss: 0.00004281
Iteration 226/1000 | Loss: 0.00004228
Iteration 227/1000 | Loss: 0.00008705
Iteration 228/1000 | Loss: 0.00004160
Iteration 229/1000 | Loss: 0.00004133
Iteration 230/1000 | Loss: 0.00008534
Iteration 231/1000 | Loss: 0.00004549
Iteration 232/1000 | Loss: 0.00004334
Iteration 233/1000 | Loss: 0.00004119
Iteration 234/1000 | Loss: 0.00004086
Iteration 235/1000 | Loss: 0.00004071
Iteration 236/1000 | Loss: 0.00004069
Iteration 237/1000 | Loss: 0.00004059
Iteration 238/1000 | Loss: 0.00004055
Iteration 239/1000 | Loss: 0.00007564
Iteration 240/1000 | Loss: 0.00004055
Iteration 241/1000 | Loss: 0.00004051
Iteration 242/1000 | Loss: 0.00004048
Iteration 243/1000 | Loss: 0.00004046
Iteration 244/1000 | Loss: 0.00004046
Iteration 245/1000 | Loss: 0.00004046
Iteration 246/1000 | Loss: 0.00004045
Iteration 247/1000 | Loss: 0.00004045
Iteration 248/1000 | Loss: 0.00004045
Iteration 249/1000 | Loss: 0.00004045
Iteration 250/1000 | Loss: 0.00004045
Iteration 251/1000 | Loss: 0.00004045
Iteration 252/1000 | Loss: 0.00004044
Iteration 253/1000 | Loss: 0.00004044
Iteration 254/1000 | Loss: 0.00004044
Iteration 255/1000 | Loss: 0.00004043
Iteration 256/1000 | Loss: 0.00004043
Iteration 257/1000 | Loss: 0.00004043
Iteration 258/1000 | Loss: 0.00004043
Iteration 259/1000 | Loss: 0.00004042
Iteration 260/1000 | Loss: 0.00004042
Iteration 261/1000 | Loss: 0.00004042
Iteration 262/1000 | Loss: 0.00004042
Iteration 263/1000 | Loss: 0.00004042
Iteration 264/1000 | Loss: 0.00004042
Iteration 265/1000 | Loss: 0.00004041
Iteration 266/1000 | Loss: 0.00004041
Iteration 267/1000 | Loss: 0.00004041
Iteration 268/1000 | Loss: 0.00004041
Iteration 269/1000 | Loss: 0.00004041
Iteration 270/1000 | Loss: 0.00004041
Iteration 271/1000 | Loss: 0.00004040
Iteration 272/1000 | Loss: 0.00004040
Iteration 273/1000 | Loss: 0.00006321
Iteration 274/1000 | Loss: 0.00005075
Iteration 275/1000 | Loss: 0.00007455
Iteration 276/1000 | Loss: 0.00004951
Iteration 277/1000 | Loss: 0.00004042
Iteration 278/1000 | Loss: 0.00004040
Iteration 279/1000 | Loss: 0.00004040
Iteration 280/1000 | Loss: 0.00004039
Iteration 281/1000 | Loss: 0.00004039
Iteration 282/1000 | Loss: 0.00004038
Iteration 283/1000 | Loss: 0.00004038
Iteration 284/1000 | Loss: 0.00004038
Iteration 285/1000 | Loss: 0.00004038
Iteration 286/1000 | Loss: 0.00004038
Iteration 287/1000 | Loss: 0.00004038
Iteration 288/1000 | Loss: 0.00004038
Iteration 289/1000 | Loss: 0.00004038
Iteration 290/1000 | Loss: 0.00004038
Iteration 291/1000 | Loss: 0.00004038
Iteration 292/1000 | Loss: 0.00004038
Iteration 293/1000 | Loss: 0.00004241
Iteration 294/1000 | Loss: 0.00004039
Iteration 295/1000 | Loss: 0.00004039
Iteration 296/1000 | Loss: 0.00004039
Iteration 297/1000 | Loss: 0.00004039
Iteration 298/1000 | Loss: 0.00004038
Iteration 299/1000 | Loss: 0.00004038
Iteration 300/1000 | Loss: 0.00004038
Iteration 301/1000 | Loss: 0.00004038
Iteration 302/1000 | Loss: 0.00004038
Iteration 303/1000 | Loss: 0.00004038
Iteration 304/1000 | Loss: 0.00004037
Iteration 305/1000 | Loss: 0.00004037
Iteration 306/1000 | Loss: 0.00004037
Iteration 307/1000 | Loss: 0.00004037
Iteration 308/1000 | Loss: 0.00004036
Iteration 309/1000 | Loss: 0.00004036
Iteration 310/1000 | Loss: 0.00004036
Iteration 311/1000 | Loss: 0.00004036
Iteration 312/1000 | Loss: 0.00004035
Iteration 313/1000 | Loss: 0.00004035
Iteration 314/1000 | Loss: 0.00004035
Iteration 315/1000 | Loss: 0.00004035
Iteration 316/1000 | Loss: 0.00004034
Iteration 317/1000 | Loss: 0.00004034
Iteration 318/1000 | Loss: 0.00004034
Iteration 319/1000 | Loss: 0.00004034
Iteration 320/1000 | Loss: 0.00004034
Iteration 321/1000 | Loss: 0.00004033
Iteration 322/1000 | Loss: 0.00004033
Iteration 323/1000 | Loss: 0.00004033
Iteration 324/1000 | Loss: 0.00004033
Iteration 325/1000 | Loss: 0.00004032
Iteration 326/1000 | Loss: 0.00004032
Iteration 327/1000 | Loss: 0.00004032
Iteration 328/1000 | Loss: 0.00004032
Iteration 329/1000 | Loss: 0.00004032
Iteration 330/1000 | Loss: 0.00004032
Iteration 331/1000 | Loss: 0.00004032
Iteration 332/1000 | Loss: 0.00004032
Iteration 333/1000 | Loss: 0.00004031
Iteration 334/1000 | Loss: 0.00004031
Iteration 335/1000 | Loss: 0.00004030
Iteration 336/1000 | Loss: 0.00004030
Iteration 337/1000 | Loss: 0.00004030
Iteration 338/1000 | Loss: 0.00004030
Iteration 339/1000 | Loss: 0.00004030
Iteration 340/1000 | Loss: 0.00004029
Iteration 341/1000 | Loss: 0.00004029
Iteration 342/1000 | Loss: 0.00004029
Iteration 343/1000 | Loss: 0.00004029
Iteration 344/1000 | Loss: 0.00004029
Iteration 345/1000 | Loss: 0.00004029
Iteration 346/1000 | Loss: 0.00004029
Iteration 347/1000 | Loss: 0.00005197
Iteration 348/1000 | Loss: 0.00004029
Iteration 349/1000 | Loss: 0.00004028
Iteration 350/1000 | Loss: 0.00004028
Iteration 351/1000 | Loss: 0.00004028
Iteration 352/1000 | Loss: 0.00004028
Iteration 353/1000 | Loss: 0.00004028
Iteration 354/1000 | Loss: 0.00004028
Iteration 355/1000 | Loss: 0.00004028
Iteration 356/1000 | Loss: 0.00004028
Iteration 357/1000 | Loss: 0.00004028
Iteration 358/1000 | Loss: 0.00004028
Iteration 359/1000 | Loss: 0.00004028
Iteration 360/1000 | Loss: 0.00004027
Iteration 361/1000 | Loss: 0.00004027
Iteration 362/1000 | Loss: 0.00004027
Iteration 363/1000 | Loss: 0.00004026
Iteration 364/1000 | Loss: 0.00004026
Iteration 365/1000 | Loss: 0.00004026
Iteration 366/1000 | Loss: 0.00004026
Iteration 367/1000 | Loss: 0.00004026
Iteration 368/1000 | Loss: 0.00004026
Iteration 369/1000 | Loss: 0.00004026
Iteration 370/1000 | Loss: 0.00004026
Iteration 371/1000 | Loss: 0.00004026
Iteration 372/1000 | Loss: 0.00004025
Iteration 373/1000 | Loss: 0.00004025
Iteration 374/1000 | Loss: 0.00004025
Iteration 375/1000 | Loss: 0.00004025
Iteration 376/1000 | Loss: 0.00004025
Iteration 377/1000 | Loss: 0.00004025
Iteration 378/1000 | Loss: 0.00004025
Iteration 379/1000 | Loss: 0.00004025
Iteration 380/1000 | Loss: 0.00004025
Iteration 381/1000 | Loss: 0.00004025
Iteration 382/1000 | Loss: 0.00004024
Iteration 383/1000 | Loss: 0.00004024
Iteration 384/1000 | Loss: 0.00004024
Iteration 385/1000 | Loss: 0.00004024
Iteration 386/1000 | Loss: 0.00004023
Iteration 387/1000 | Loss: 0.00004023
Iteration 388/1000 | Loss: 0.00004023
Iteration 389/1000 | Loss: 0.00004023
Iteration 390/1000 | Loss: 0.00004023
Iteration 391/1000 | Loss: 0.00004023
Iteration 392/1000 | Loss: 0.00004022
Iteration 393/1000 | Loss: 0.00004022
Iteration 394/1000 | Loss: 0.00004022
Iteration 395/1000 | Loss: 0.00004022
Iteration 396/1000 | Loss: 0.00004022
Iteration 397/1000 | Loss: 0.00004022
Iteration 398/1000 | Loss: 0.00004022
Iteration 399/1000 | Loss: 0.00004022
Iteration 400/1000 | Loss: 0.00004022
Iteration 401/1000 | Loss: 0.00080556
Iteration 402/1000 | Loss: 0.00061081
Iteration 403/1000 | Loss: 0.00078610
Iteration 404/1000 | Loss: 0.00005851
Iteration 405/1000 | Loss: 0.00004705
Iteration 406/1000 | Loss: 0.00004375
Iteration 407/1000 | Loss: 0.00004161
Iteration 408/1000 | Loss: 0.00005325
Iteration 409/1000 | Loss: 0.00004108
Iteration 410/1000 | Loss: 0.00017439
Iteration 411/1000 | Loss: 0.00004944
Iteration 412/1000 | Loss: 0.00003850
Iteration 413/1000 | Loss: 0.00013993
Iteration 414/1000 | Loss: 0.00003823
Iteration 415/1000 | Loss: 0.00003806
Iteration 416/1000 | Loss: 0.00003797
Iteration 417/1000 | Loss: 0.00003788
Iteration 418/1000 | Loss: 0.00003784
Iteration 419/1000 | Loss: 0.00003783
Iteration 420/1000 | Loss: 0.00003783
Iteration 421/1000 | Loss: 0.00003781
Iteration 422/1000 | Loss: 0.00003778
Iteration 423/1000 | Loss: 0.00003777
Iteration 424/1000 | Loss: 0.00003777
Iteration 425/1000 | Loss: 0.00003777
Iteration 426/1000 | Loss: 0.00003777
Iteration 427/1000 | Loss: 0.00003776
Iteration 428/1000 | Loss: 0.00003776
Iteration 429/1000 | Loss: 0.00003776
Iteration 430/1000 | Loss: 0.00003775
Iteration 431/1000 | Loss: 0.00003775
Iteration 432/1000 | Loss: 0.00003774
Iteration 433/1000 | Loss: 0.00003774
Iteration 434/1000 | Loss: 0.00003774
Iteration 435/1000 | Loss: 0.00003774
Iteration 436/1000 | Loss: 0.00003774
Iteration 437/1000 | Loss: 0.00003773
Iteration 438/1000 | Loss: 0.00003773
Iteration 439/1000 | Loss: 0.00003773
Iteration 440/1000 | Loss: 0.00003773
Iteration 441/1000 | Loss: 0.00003773
Iteration 442/1000 | Loss: 0.00003773
Iteration 443/1000 | Loss: 0.00003773
Iteration 444/1000 | Loss: 0.00003773
Iteration 445/1000 | Loss: 0.00003773
Iteration 446/1000 | Loss: 0.00003773
Iteration 447/1000 | Loss: 0.00003773
Iteration 448/1000 | Loss: 0.00003773
Iteration 449/1000 | Loss: 0.00003773
Iteration 450/1000 | Loss: 0.00003773
Iteration 451/1000 | Loss: 0.00003773
Iteration 452/1000 | Loss: 0.00003773
Iteration 453/1000 | Loss: 0.00003772
Iteration 454/1000 | Loss: 0.00003772
Iteration 455/1000 | Loss: 0.00003772
Iteration 456/1000 | Loss: 0.00003772
Iteration 457/1000 | Loss: 0.00003772
Iteration 458/1000 | Loss: 0.00003772
Iteration 459/1000 | Loss: 0.00003771
Iteration 460/1000 | Loss: 0.00003771
Iteration 461/1000 | Loss: 0.00003769
Iteration 462/1000 | Loss: 0.00003769
Iteration 463/1000 | Loss: 0.00003769
Iteration 464/1000 | Loss: 0.00003769
Iteration 465/1000 | Loss: 0.00003768
Iteration 466/1000 | Loss: 0.00003768
Iteration 467/1000 | Loss: 0.00003768
Iteration 468/1000 | Loss: 0.00003768
Iteration 469/1000 | Loss: 0.00003767
Iteration 470/1000 | Loss: 0.00003767
Iteration 471/1000 | Loss: 0.00003767
Iteration 472/1000 | Loss: 0.00003767
Iteration 473/1000 | Loss: 0.00003767
Iteration 474/1000 | Loss: 0.00003767
Iteration 475/1000 | Loss: 0.00003767
Iteration 476/1000 | Loss: 0.00003767
Iteration 477/1000 | Loss: 0.00003767
Iteration 478/1000 | Loss: 0.00003767
Iteration 479/1000 | Loss: 0.00003767
Iteration 480/1000 | Loss: 0.00003767
Iteration 481/1000 | Loss: 0.00003766
Iteration 482/1000 | Loss: 0.00003766
Iteration 483/1000 | Loss: 0.00003766
Iteration 484/1000 | Loss: 0.00003766
Iteration 485/1000 | Loss: 0.00003766
Iteration 486/1000 | Loss: 0.00003766
Iteration 487/1000 | Loss: 0.00003766
Iteration 488/1000 | Loss: 0.00003766
Iteration 489/1000 | Loss: 0.00003766
Iteration 490/1000 | Loss: 0.00003766
Iteration 491/1000 | Loss: 0.00003766
Iteration 492/1000 | Loss: 0.00003766
Iteration 493/1000 | Loss: 0.00003766
Iteration 494/1000 | Loss: 0.00003766
Iteration 495/1000 | Loss: 0.00003765
Iteration 496/1000 | Loss: 0.00003765
Iteration 497/1000 | Loss: 0.00003765
Iteration 498/1000 | Loss: 0.00003765
Iteration 499/1000 | Loss: 0.00003765
Iteration 500/1000 | Loss: 0.00003765
Iteration 501/1000 | Loss: 0.00003765
Iteration 502/1000 | Loss: 0.00003765
Iteration 503/1000 | Loss: 0.00003765
Iteration 504/1000 | Loss: 0.00003765
Iteration 505/1000 | Loss: 0.00003765
Iteration 506/1000 | Loss: 0.00003765
Iteration 507/1000 | Loss: 0.00003765
Iteration 508/1000 | Loss: 0.00003765
Iteration 509/1000 | Loss: 0.00003765
Iteration 510/1000 | Loss: 0.00003764
Iteration 511/1000 | Loss: 0.00003764
Iteration 512/1000 | Loss: 0.00003764
Iteration 513/1000 | Loss: 0.00003764
Iteration 514/1000 | Loss: 0.00003764
Iteration 515/1000 | Loss: 0.00003764
Iteration 516/1000 | Loss: 0.00003764
Iteration 517/1000 | Loss: 0.00003764
Iteration 518/1000 | Loss: 0.00003764
Iteration 519/1000 | Loss: 0.00003763
Iteration 520/1000 | Loss: 0.00003763
Iteration 521/1000 | Loss: 0.00003763
Iteration 522/1000 | Loss: 0.00003762
Iteration 523/1000 | Loss: 0.00003762
Iteration 524/1000 | Loss: 0.00003762
Iteration 525/1000 | Loss: 0.00003762
Iteration 526/1000 | Loss: 0.00003761
Iteration 527/1000 | Loss: 0.00003761
Iteration 528/1000 | Loss: 0.00003761
Iteration 529/1000 | Loss: 0.00003761
Iteration 530/1000 | Loss: 0.00003761
Iteration 531/1000 | Loss: 0.00003761
Iteration 532/1000 | Loss: 0.00003760
Iteration 533/1000 | Loss: 0.00003760
Iteration 534/1000 | Loss: 0.00003760
Iteration 535/1000 | Loss: 0.00003760
Iteration 536/1000 | Loss: 0.00003760
Iteration 537/1000 | Loss: 0.00003760
Iteration 538/1000 | Loss: 0.00003760
Iteration 539/1000 | Loss: 0.00003760
Iteration 540/1000 | Loss: 0.00003760
Iteration 541/1000 | Loss: 0.00003760
Iteration 542/1000 | Loss: 0.00003760
Iteration 543/1000 | Loss: 0.00003760
Iteration 544/1000 | Loss: 0.00003760
Iteration 545/1000 | Loss: 0.00003760
Iteration 546/1000 | Loss: 0.00003760
Iteration 547/1000 | Loss: 0.00003759
Iteration 548/1000 | Loss: 0.00003759
Iteration 549/1000 | Loss: 0.00003759
Iteration 550/1000 | Loss: 0.00003759
Iteration 551/1000 | Loss: 0.00003759
Iteration 552/1000 | Loss: 0.00003759
Iteration 553/1000 | Loss: 0.00003759
Iteration 554/1000 | Loss: 0.00003759
Iteration 555/1000 | Loss: 0.00003759
Iteration 556/1000 | Loss: 0.00003759
Iteration 557/1000 | Loss: 0.00003759
Iteration 558/1000 | Loss: 0.00003759
Iteration 559/1000 | Loss: 0.00003759
Iteration 560/1000 | Loss: 0.00003759
Iteration 561/1000 | Loss: 0.00003759
Iteration 562/1000 | Loss: 0.00003759
Iteration 563/1000 | Loss: 0.00003759
Iteration 564/1000 | Loss: 0.00003759
Iteration 565/1000 | Loss: 0.00003759
Iteration 566/1000 | Loss: 0.00003759
Iteration 567/1000 | Loss: 0.00003759
Iteration 568/1000 | Loss: 0.00003758
Iteration 569/1000 | Loss: 0.00003758
Iteration 570/1000 | Loss: 0.00003758
Iteration 571/1000 | Loss: 0.00003758
Iteration 572/1000 | Loss: 0.00003758
Iteration 573/1000 | Loss: 0.00003758
Iteration 574/1000 | Loss: 0.00003758
Iteration 575/1000 | Loss: 0.00003758
Iteration 576/1000 | Loss: 0.00003758
Iteration 577/1000 | Loss: 0.00003758
Iteration 578/1000 | Loss: 0.00003758
Iteration 579/1000 | Loss: 0.00003758
Iteration 580/1000 | Loss: 0.00003758
Iteration 581/1000 | Loss: 0.00003758
Iteration 582/1000 | Loss: 0.00003758
Iteration 583/1000 | Loss: 0.00003758
Iteration 584/1000 | Loss: 0.00003758
Iteration 585/1000 | Loss: 0.00003758
Iteration 586/1000 | Loss: 0.00003758
Iteration 587/1000 | Loss: 0.00003758
Iteration 588/1000 | Loss: 0.00003758
Iteration 589/1000 | Loss: 0.00003758
Iteration 590/1000 | Loss: 0.00003757
Iteration 591/1000 | Loss: 0.00003757
Iteration 592/1000 | Loss: 0.00003757
Iteration 593/1000 | Loss: 0.00003757
Iteration 594/1000 | Loss: 0.00003757
Iteration 595/1000 | Loss: 0.00003757
Iteration 596/1000 | Loss: 0.00003757
Iteration 597/1000 | Loss: 0.00003757
Iteration 598/1000 | Loss: 0.00003757
Iteration 599/1000 | Loss: 0.00003757
Iteration 600/1000 | Loss: 0.00003757
Iteration 601/1000 | Loss: 0.00003757
Iteration 602/1000 | Loss: 0.00003757
Iteration 603/1000 | Loss: 0.00003757
Iteration 604/1000 | Loss: 0.00003757
Iteration 605/1000 | Loss: 0.00003757
Iteration 606/1000 | Loss: 0.00003757
Iteration 607/1000 | Loss: 0.00003757
Iteration 608/1000 | Loss: 0.00003757
Iteration 609/1000 | Loss: 0.00003756
Iteration 610/1000 | Loss: 0.00003756
Iteration 611/1000 | Loss: 0.00003756
Iteration 612/1000 | Loss: 0.00003756
Iteration 613/1000 | Loss: 0.00003756
Iteration 614/1000 | Loss: 0.00003756
Iteration 615/1000 | Loss: 0.00003756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 615. Stopping optimization.
Last 5 losses: [3.756469232030213e-05, 3.756469232030213e-05, 3.756469232030213e-05, 3.756469232030213e-05, 3.756469232030213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.756469232030213e-05

Optimization complete. Final v2v error: 4.433852195739746 mm

Highest mean error: 13.248054504394531 mm for frame 80

Lowest mean error: 3.5677733421325684 mm for frame 218

Saving results

Total time: 471.41360688209534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866319
Iteration 2/25 | Loss: 0.00144588
Iteration 3/25 | Loss: 0.00085723
Iteration 4/25 | Loss: 0.00077045
Iteration 5/25 | Loss: 0.00074576
Iteration 6/25 | Loss: 0.00074836
Iteration 7/25 | Loss: 0.00074133
Iteration 8/25 | Loss: 0.00072848
Iteration 9/25 | Loss: 0.00072487
Iteration 10/25 | Loss: 0.00071709
Iteration 11/25 | Loss: 0.00071432
Iteration 12/25 | Loss: 0.00071403
Iteration 13/25 | Loss: 0.00071393
Iteration 14/25 | Loss: 0.00071393
Iteration 15/25 | Loss: 0.00071393
Iteration 16/25 | Loss: 0.00071393
Iteration 17/25 | Loss: 0.00071393
Iteration 18/25 | Loss: 0.00071393
Iteration 19/25 | Loss: 0.00071393
Iteration 20/25 | Loss: 0.00071393
Iteration 21/25 | Loss: 0.00071392
Iteration 22/25 | Loss: 0.00071392
Iteration 23/25 | Loss: 0.00071392
Iteration 24/25 | Loss: 0.00071392
Iteration 25/25 | Loss: 0.00071392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56211233
Iteration 2/25 | Loss: 0.00068097
Iteration 3/25 | Loss: 0.00058610
Iteration 4/25 | Loss: 0.00058610
Iteration 5/25 | Loss: 0.00058610
Iteration 6/25 | Loss: 0.00058610
Iteration 7/25 | Loss: 0.00058610
Iteration 8/25 | Loss: 0.00058610
Iteration 9/25 | Loss: 0.00058610
Iteration 10/25 | Loss: 0.00058610
Iteration 11/25 | Loss: 0.00058610
Iteration 12/25 | Loss: 0.00058610
Iteration 13/25 | Loss: 0.00058610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005860995734110475, 0.0005860995734110475, 0.0005860995734110475, 0.0005860995734110475, 0.0005860995734110475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005860995734110475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058610
Iteration 2/1000 | Loss: 0.00029567
Iteration 3/1000 | Loss: 0.00003834
Iteration 4/1000 | Loss: 0.00002736
Iteration 5/1000 | Loss: 0.00002336
Iteration 6/1000 | Loss: 0.00002217
Iteration 7/1000 | Loss: 0.00007079
Iteration 8/1000 | Loss: 0.00002121
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001848
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001708
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001706
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001703
Iteration 36/1000 | Loss: 0.00001703
Iteration 37/1000 | Loss: 0.00001703
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001698
Iteration 52/1000 | Loss: 0.00001698
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001697
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001695
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001695
Iteration 73/1000 | Loss: 0.00001695
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001694
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001694
Iteration 84/1000 | Loss: 0.00001694
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001693
Iteration 94/1000 | Loss: 0.00001693
Iteration 95/1000 | Loss: 0.00001693
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.6928639524849132e-05, 1.6928639524849132e-05, 1.6928639524849132e-05, 1.6928639524849132e-05, 1.6928639524849132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6928639524849132e-05

Optimization complete. Final v2v error: 3.497271776199341 mm

Highest mean error: 4.001353740692139 mm for frame 185

Lowest mean error: 3.0265791416168213 mm for frame 203

Saving results

Total time: 57.70865511894226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068932
Iteration 2/25 | Loss: 0.00233363
Iteration 3/25 | Loss: 0.00159299
Iteration 4/25 | Loss: 0.00143085
Iteration 5/25 | Loss: 0.00130717
Iteration 6/25 | Loss: 0.00114747
Iteration 7/25 | Loss: 0.00108486
Iteration 8/25 | Loss: 0.00107252
Iteration 9/25 | Loss: 0.00104430
Iteration 10/25 | Loss: 0.00102507
Iteration 11/25 | Loss: 0.00100390
Iteration 12/25 | Loss: 0.00099502
Iteration 13/25 | Loss: 0.00097849
Iteration 14/25 | Loss: 0.00096719
Iteration 15/25 | Loss: 0.00096726
Iteration 16/25 | Loss: 0.00096940
Iteration 17/25 | Loss: 0.00096036
Iteration 18/25 | Loss: 0.00095236
Iteration 19/25 | Loss: 0.00095626
Iteration 20/25 | Loss: 0.00095157
Iteration 21/25 | Loss: 0.00094627
Iteration 22/25 | Loss: 0.00094414
Iteration 23/25 | Loss: 0.00094332
Iteration 24/25 | Loss: 0.00094293
Iteration 25/25 | Loss: 0.00095126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89880991
Iteration 2/25 | Loss: 0.00227253
Iteration 3/25 | Loss: 0.00227253
Iteration 4/25 | Loss: 0.00227253
Iteration 5/25 | Loss: 0.00227253
Iteration 6/25 | Loss: 0.00227253
Iteration 7/25 | Loss: 0.00227253
Iteration 8/25 | Loss: 0.00227253
Iteration 9/25 | Loss: 0.00227253
Iteration 10/25 | Loss: 0.00227253
Iteration 11/25 | Loss: 0.00227253
Iteration 12/25 | Loss: 0.00227253
Iteration 13/25 | Loss: 0.00227253
Iteration 14/25 | Loss: 0.00227253
Iteration 15/25 | Loss: 0.00227253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022725299932062626, 0.0022725299932062626, 0.0022725299932062626, 0.0022725299932062626, 0.0022725299932062626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022725299932062626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227253
Iteration 2/1000 | Loss: 0.00095306
Iteration 3/1000 | Loss: 0.00084917
Iteration 4/1000 | Loss: 0.00140428
Iteration 5/1000 | Loss: 0.00023376
Iteration 6/1000 | Loss: 0.00020280
Iteration 7/1000 | Loss: 0.00016866
Iteration 8/1000 | Loss: 0.00014728
Iteration 9/1000 | Loss: 0.00013532
Iteration 10/1000 | Loss: 0.00012586
Iteration 11/1000 | Loss: 0.00020131
Iteration 12/1000 | Loss: 0.00012473
Iteration 13/1000 | Loss: 0.00011809
Iteration 14/1000 | Loss: 0.00011471
Iteration 15/1000 | Loss: 0.00011046
Iteration 16/1000 | Loss: 0.00073779
Iteration 17/1000 | Loss: 0.00018897
Iteration 18/1000 | Loss: 0.00085888
Iteration 19/1000 | Loss: 0.00013280
Iteration 20/1000 | Loss: 0.00011465
Iteration 21/1000 | Loss: 0.00063766
Iteration 22/1000 | Loss: 0.00079482
Iteration 23/1000 | Loss: 0.00124437
Iteration 24/1000 | Loss: 0.00075113
Iteration 25/1000 | Loss: 0.00093138
Iteration 26/1000 | Loss: 0.00052657
Iteration 27/1000 | Loss: 0.00069783
Iteration 28/1000 | Loss: 0.00052093
Iteration 29/1000 | Loss: 0.00059284
Iteration 30/1000 | Loss: 0.00057319
Iteration 31/1000 | Loss: 0.00087997
Iteration 32/1000 | Loss: 0.00056593
Iteration 33/1000 | Loss: 0.00057161
Iteration 34/1000 | Loss: 0.00027014
Iteration 35/1000 | Loss: 0.00023838
Iteration 36/1000 | Loss: 0.00040372
Iteration 37/1000 | Loss: 0.00025749
Iteration 38/1000 | Loss: 0.00036231
Iteration 39/1000 | Loss: 0.00026366
Iteration 40/1000 | Loss: 0.00040802
Iteration 41/1000 | Loss: 0.00030155
Iteration 42/1000 | Loss: 0.00045440
Iteration 43/1000 | Loss: 0.00036535
Iteration 44/1000 | Loss: 0.00036404
Iteration 45/1000 | Loss: 0.00043085
Iteration 46/1000 | Loss: 0.00023838
Iteration 47/1000 | Loss: 0.00023861
Iteration 48/1000 | Loss: 0.00022798
Iteration 49/1000 | Loss: 0.00011960
Iteration 50/1000 | Loss: 0.00010895
Iteration 51/1000 | Loss: 0.00010053
Iteration 52/1000 | Loss: 0.00009345
Iteration 53/1000 | Loss: 0.00008906
Iteration 54/1000 | Loss: 0.00008656
Iteration 55/1000 | Loss: 0.00008523
Iteration 56/1000 | Loss: 0.00008436
Iteration 57/1000 | Loss: 0.00008379
Iteration 58/1000 | Loss: 0.00008318
Iteration 59/1000 | Loss: 0.00008266
Iteration 60/1000 | Loss: 0.00008208
Iteration 61/1000 | Loss: 0.00008149
Iteration 62/1000 | Loss: 0.00008106
Iteration 63/1000 | Loss: 0.00008080
Iteration 64/1000 | Loss: 0.00008043
Iteration 65/1000 | Loss: 0.00008033
Iteration 66/1000 | Loss: 0.00008010
Iteration 67/1000 | Loss: 0.00007986
Iteration 68/1000 | Loss: 0.00007964
Iteration 69/1000 | Loss: 0.00007950
Iteration 70/1000 | Loss: 0.00007934
Iteration 71/1000 | Loss: 0.00007926
Iteration 72/1000 | Loss: 0.00007925
Iteration 73/1000 | Loss: 0.00007923
Iteration 74/1000 | Loss: 0.00007913
Iteration 75/1000 | Loss: 0.00007912
Iteration 76/1000 | Loss: 0.00007911
Iteration 77/1000 | Loss: 0.00007910
Iteration 78/1000 | Loss: 0.00007908
Iteration 79/1000 | Loss: 0.00007906
Iteration 80/1000 | Loss: 0.00007905
Iteration 81/1000 | Loss: 0.00007905
Iteration 82/1000 | Loss: 0.00007905
Iteration 83/1000 | Loss: 0.00007905
Iteration 84/1000 | Loss: 0.00007905
Iteration 85/1000 | Loss: 0.00007905
Iteration 86/1000 | Loss: 0.00007905
Iteration 87/1000 | Loss: 0.00007905
Iteration 88/1000 | Loss: 0.00007905
Iteration 89/1000 | Loss: 0.00007904
Iteration 90/1000 | Loss: 0.00007904
Iteration 91/1000 | Loss: 0.00007904
Iteration 92/1000 | Loss: 0.00007903
Iteration 93/1000 | Loss: 0.00007903
Iteration 94/1000 | Loss: 0.00007903
Iteration 95/1000 | Loss: 0.00007903
Iteration 96/1000 | Loss: 0.00007903
Iteration 97/1000 | Loss: 0.00007903
Iteration 98/1000 | Loss: 0.00007903
Iteration 99/1000 | Loss: 0.00007902
Iteration 100/1000 | Loss: 0.00007902
Iteration 101/1000 | Loss: 0.00007902
Iteration 102/1000 | Loss: 0.00007902
Iteration 103/1000 | Loss: 0.00007902
Iteration 104/1000 | Loss: 0.00007902
Iteration 105/1000 | Loss: 0.00007902
Iteration 106/1000 | Loss: 0.00007901
Iteration 107/1000 | Loss: 0.00007901
Iteration 108/1000 | Loss: 0.00007901
Iteration 109/1000 | Loss: 0.00007901
Iteration 110/1000 | Loss: 0.00007899
Iteration 111/1000 | Loss: 0.00007899
Iteration 112/1000 | Loss: 0.00007899
Iteration 113/1000 | Loss: 0.00007899
Iteration 114/1000 | Loss: 0.00007899
Iteration 115/1000 | Loss: 0.00007899
Iteration 116/1000 | Loss: 0.00007898
Iteration 117/1000 | Loss: 0.00007898
Iteration 118/1000 | Loss: 0.00007898
Iteration 119/1000 | Loss: 0.00007898
Iteration 120/1000 | Loss: 0.00007898
Iteration 121/1000 | Loss: 0.00007898
Iteration 122/1000 | Loss: 0.00007898
Iteration 123/1000 | Loss: 0.00007898
Iteration 124/1000 | Loss: 0.00007898
Iteration 125/1000 | Loss: 0.00007898
Iteration 126/1000 | Loss: 0.00007897
Iteration 127/1000 | Loss: 0.00007897
Iteration 128/1000 | Loss: 0.00007896
Iteration 129/1000 | Loss: 0.00007895
Iteration 130/1000 | Loss: 0.00007895
Iteration 131/1000 | Loss: 0.00007895
Iteration 132/1000 | Loss: 0.00007894
Iteration 133/1000 | Loss: 0.00007894
Iteration 134/1000 | Loss: 0.00007894
Iteration 135/1000 | Loss: 0.00007894
Iteration 136/1000 | Loss: 0.00007894
Iteration 137/1000 | Loss: 0.00007894
Iteration 138/1000 | Loss: 0.00007893
Iteration 139/1000 | Loss: 0.00007893
Iteration 140/1000 | Loss: 0.00007892
Iteration 141/1000 | Loss: 0.00007892
Iteration 142/1000 | Loss: 0.00007891
Iteration 143/1000 | Loss: 0.00007891
Iteration 144/1000 | Loss: 0.00007891
Iteration 145/1000 | Loss: 0.00007891
Iteration 146/1000 | Loss: 0.00007891
Iteration 147/1000 | Loss: 0.00007891
Iteration 148/1000 | Loss: 0.00007890
Iteration 149/1000 | Loss: 0.00007890
Iteration 150/1000 | Loss: 0.00007890
Iteration 151/1000 | Loss: 0.00007890
Iteration 152/1000 | Loss: 0.00007890
Iteration 153/1000 | Loss: 0.00007889
Iteration 154/1000 | Loss: 0.00007889
Iteration 155/1000 | Loss: 0.00007889
Iteration 156/1000 | Loss: 0.00007888
Iteration 157/1000 | Loss: 0.00007888
Iteration 158/1000 | Loss: 0.00007888
Iteration 159/1000 | Loss: 0.00007888
Iteration 160/1000 | Loss: 0.00007888
Iteration 161/1000 | Loss: 0.00007887
Iteration 162/1000 | Loss: 0.00007887
Iteration 163/1000 | Loss: 0.00007887
Iteration 164/1000 | Loss: 0.00007887
Iteration 165/1000 | Loss: 0.00007887
Iteration 166/1000 | Loss: 0.00007887
Iteration 167/1000 | Loss: 0.00007886
Iteration 168/1000 | Loss: 0.00007886
Iteration 169/1000 | Loss: 0.00007886
Iteration 170/1000 | Loss: 0.00007886
Iteration 171/1000 | Loss: 0.00007886
Iteration 172/1000 | Loss: 0.00007885
Iteration 173/1000 | Loss: 0.00007885
Iteration 174/1000 | Loss: 0.00007885
Iteration 175/1000 | Loss: 0.00007885
Iteration 176/1000 | Loss: 0.00007885
Iteration 177/1000 | Loss: 0.00007885
Iteration 178/1000 | Loss: 0.00007885
Iteration 179/1000 | Loss: 0.00007885
Iteration 180/1000 | Loss: 0.00007885
Iteration 181/1000 | Loss: 0.00007884
Iteration 182/1000 | Loss: 0.00007884
Iteration 183/1000 | Loss: 0.00007884
Iteration 184/1000 | Loss: 0.00007884
Iteration 185/1000 | Loss: 0.00007884
Iteration 186/1000 | Loss: 0.00007883
Iteration 187/1000 | Loss: 0.00007883
Iteration 188/1000 | Loss: 0.00007883
Iteration 189/1000 | Loss: 0.00007882
Iteration 190/1000 | Loss: 0.00007882
Iteration 191/1000 | Loss: 0.00007882
Iteration 192/1000 | Loss: 0.00007882
Iteration 193/1000 | Loss: 0.00007882
Iteration 194/1000 | Loss: 0.00007882
Iteration 195/1000 | Loss: 0.00007882
Iteration 196/1000 | Loss: 0.00007882
Iteration 197/1000 | Loss: 0.00007882
Iteration 198/1000 | Loss: 0.00007882
Iteration 199/1000 | Loss: 0.00007882
Iteration 200/1000 | Loss: 0.00007882
Iteration 201/1000 | Loss: 0.00007882
Iteration 202/1000 | Loss: 0.00007882
Iteration 203/1000 | Loss: 0.00007881
Iteration 204/1000 | Loss: 0.00007881
Iteration 205/1000 | Loss: 0.00007881
Iteration 206/1000 | Loss: 0.00007881
Iteration 207/1000 | Loss: 0.00007881
Iteration 208/1000 | Loss: 0.00007881
Iteration 209/1000 | Loss: 0.00007880
Iteration 210/1000 | Loss: 0.00007880
Iteration 211/1000 | Loss: 0.00007880
Iteration 212/1000 | Loss: 0.00007880
Iteration 213/1000 | Loss: 0.00007880
Iteration 214/1000 | Loss: 0.00007880
Iteration 215/1000 | Loss: 0.00007880
Iteration 216/1000 | Loss: 0.00007880
Iteration 217/1000 | Loss: 0.00007880
Iteration 218/1000 | Loss: 0.00007880
Iteration 219/1000 | Loss: 0.00007880
Iteration 220/1000 | Loss: 0.00007880
Iteration 221/1000 | Loss: 0.00007880
Iteration 222/1000 | Loss: 0.00007880
Iteration 223/1000 | Loss: 0.00007880
Iteration 224/1000 | Loss: 0.00007880
Iteration 225/1000 | Loss: 0.00007880
Iteration 226/1000 | Loss: 0.00007879
Iteration 227/1000 | Loss: 0.00007879
Iteration 228/1000 | Loss: 0.00007879
Iteration 229/1000 | Loss: 0.00007879
Iteration 230/1000 | Loss: 0.00007879
Iteration 231/1000 | Loss: 0.00007879
Iteration 232/1000 | Loss: 0.00007879
Iteration 233/1000 | Loss: 0.00007879
Iteration 234/1000 | Loss: 0.00007879
Iteration 235/1000 | Loss: 0.00007879
Iteration 236/1000 | Loss: 0.00007879
Iteration 237/1000 | Loss: 0.00007879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [7.879457552917302e-05, 7.879457552917302e-05, 7.879457552917302e-05, 7.879457552917302e-05, 7.879457552917302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.879457552917302e-05

Optimization complete. Final v2v error: 4.866465091705322 mm

Highest mean error: 11.918614387512207 mm for frame 52

Lowest mean error: 3.099067449569702 mm for frame 1

Saving results

Total time: 157.05809235572815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817409
Iteration 2/25 | Loss: 0.00102021
Iteration 3/25 | Loss: 0.00074947
Iteration 4/25 | Loss: 0.00070176
Iteration 5/25 | Loss: 0.00069104
Iteration 6/25 | Loss: 0.00068344
Iteration 7/25 | Loss: 0.00068099
Iteration 8/25 | Loss: 0.00068014
Iteration 9/25 | Loss: 0.00068178
Iteration 10/25 | Loss: 0.00068010
Iteration 11/25 | Loss: 0.00067739
Iteration 12/25 | Loss: 0.00067594
Iteration 13/25 | Loss: 0.00067499
Iteration 14/25 | Loss: 0.00067454
Iteration 15/25 | Loss: 0.00067449
Iteration 16/25 | Loss: 0.00067448
Iteration 17/25 | Loss: 0.00067448
Iteration 18/25 | Loss: 0.00067448
Iteration 19/25 | Loss: 0.00067448
Iteration 20/25 | Loss: 0.00067448
Iteration 21/25 | Loss: 0.00067448
Iteration 22/25 | Loss: 0.00067448
Iteration 23/25 | Loss: 0.00067448
Iteration 24/25 | Loss: 0.00067448
Iteration 25/25 | Loss: 0.00067448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68536711
Iteration 2/25 | Loss: 0.00034694
Iteration 3/25 | Loss: 0.00034693
Iteration 4/25 | Loss: 0.00034693
Iteration 5/25 | Loss: 0.00034693
Iteration 6/25 | Loss: 0.00034693
Iteration 7/25 | Loss: 0.00034693
Iteration 8/25 | Loss: 0.00034693
Iteration 9/25 | Loss: 0.00034693
Iteration 10/25 | Loss: 0.00034693
Iteration 11/25 | Loss: 0.00034693
Iteration 12/25 | Loss: 0.00034693
Iteration 13/25 | Loss: 0.00034693
Iteration 14/25 | Loss: 0.00034693
Iteration 15/25 | Loss: 0.00034693
Iteration 16/25 | Loss: 0.00034693
Iteration 17/25 | Loss: 0.00034693
Iteration 18/25 | Loss: 0.00034693
Iteration 19/25 | Loss: 0.00034693
Iteration 20/25 | Loss: 0.00034693
Iteration 21/25 | Loss: 0.00034693
Iteration 22/25 | Loss: 0.00034693
Iteration 23/25 | Loss: 0.00034693
Iteration 24/25 | Loss: 0.00034693
Iteration 25/25 | Loss: 0.00034693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034693
Iteration 2/1000 | Loss: 0.00002539
Iteration 3/1000 | Loss: 0.00001768
Iteration 4/1000 | Loss: 0.00001667
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001532
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001491
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001485
Iteration 13/1000 | Loss: 0.00001484
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001483
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001468
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001466
Iteration 38/1000 | Loss: 0.00001466
Iteration 39/1000 | Loss: 0.00001465
Iteration 40/1000 | Loss: 0.00001465
Iteration 41/1000 | Loss: 0.00001465
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001462
Iteration 50/1000 | Loss: 0.00001462
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001461
Iteration 53/1000 | Loss: 0.00001460
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001460
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001457
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001453
Iteration 86/1000 | Loss: 0.00001453
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001446
Iteration 112/1000 | Loss: 0.00001445
Iteration 113/1000 | Loss: 0.00001445
Iteration 114/1000 | Loss: 0.00001445
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001440
Iteration 130/1000 | Loss: 0.00001440
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001440
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001439
Iteration 139/1000 | Loss: 0.00001439
Iteration 140/1000 | Loss: 0.00001439
Iteration 141/1000 | Loss: 0.00001439
Iteration 142/1000 | Loss: 0.00001439
Iteration 143/1000 | Loss: 0.00001439
Iteration 144/1000 | Loss: 0.00001439
Iteration 145/1000 | Loss: 0.00001439
Iteration 146/1000 | Loss: 0.00001439
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001438
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001438
Iteration 153/1000 | Loss: 0.00001438
Iteration 154/1000 | Loss: 0.00001438
Iteration 155/1000 | Loss: 0.00001438
Iteration 156/1000 | Loss: 0.00001437
Iteration 157/1000 | Loss: 0.00001437
Iteration 158/1000 | Loss: 0.00001437
Iteration 159/1000 | Loss: 0.00001437
Iteration 160/1000 | Loss: 0.00001437
Iteration 161/1000 | Loss: 0.00001437
Iteration 162/1000 | Loss: 0.00001437
Iteration 163/1000 | Loss: 0.00001437
Iteration 164/1000 | Loss: 0.00001437
Iteration 165/1000 | Loss: 0.00001437
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001437
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Iteration 170/1000 | Loss: 0.00001437
Iteration 171/1000 | Loss: 0.00001437
Iteration 172/1000 | Loss: 0.00001437
Iteration 173/1000 | Loss: 0.00001437
Iteration 174/1000 | Loss: 0.00001437
Iteration 175/1000 | Loss: 0.00001437
Iteration 176/1000 | Loss: 0.00001437
Iteration 177/1000 | Loss: 0.00001437
Iteration 178/1000 | Loss: 0.00001437
Iteration 179/1000 | Loss: 0.00001437
Iteration 180/1000 | Loss: 0.00001437
Iteration 181/1000 | Loss: 0.00001437
Iteration 182/1000 | Loss: 0.00001437
Iteration 183/1000 | Loss: 0.00001437
Iteration 184/1000 | Loss: 0.00001437
Iteration 185/1000 | Loss: 0.00001437
Iteration 186/1000 | Loss: 0.00001437
Iteration 187/1000 | Loss: 0.00001437
Iteration 188/1000 | Loss: 0.00001437
Iteration 189/1000 | Loss: 0.00001437
Iteration 190/1000 | Loss: 0.00001437
Iteration 191/1000 | Loss: 0.00001437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.4373925296240486e-05, 1.4373925296240486e-05, 1.4373925296240486e-05, 1.4373925296240486e-05, 1.4373925296240486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4373925296240486e-05

Optimization complete. Final v2v error: 3.2317421436309814 mm

Highest mean error: 3.9637629985809326 mm for frame 172

Lowest mean error: 2.912789821624756 mm for frame 14

Saving results

Total time: 57.920066595077515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428062
Iteration 2/25 | Loss: 0.00083731
Iteration 3/25 | Loss: 0.00072556
Iteration 4/25 | Loss: 0.00070165
Iteration 5/25 | Loss: 0.00069657
Iteration 6/25 | Loss: 0.00069483
Iteration 7/25 | Loss: 0.00069427
Iteration 8/25 | Loss: 0.00069427
Iteration 9/25 | Loss: 0.00069427
Iteration 10/25 | Loss: 0.00069427
Iteration 11/25 | Loss: 0.00069427
Iteration 12/25 | Loss: 0.00069427
Iteration 13/25 | Loss: 0.00069427
Iteration 14/25 | Loss: 0.00069427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006942734471522272, 0.0006942734471522272, 0.0006942734471522272, 0.0006942734471522272, 0.0006942734471522272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006942734471522272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85627627
Iteration 2/25 | Loss: 0.00031703
Iteration 3/25 | Loss: 0.00031703
Iteration 4/25 | Loss: 0.00031702
Iteration 5/25 | Loss: 0.00031702
Iteration 6/25 | Loss: 0.00031702
Iteration 7/25 | Loss: 0.00031702
Iteration 8/25 | Loss: 0.00031702
Iteration 9/25 | Loss: 0.00031702
Iteration 10/25 | Loss: 0.00031702
Iteration 11/25 | Loss: 0.00031702
Iteration 12/25 | Loss: 0.00031702
Iteration 13/25 | Loss: 0.00031702
Iteration 14/25 | Loss: 0.00031702
Iteration 15/25 | Loss: 0.00031702
Iteration 16/25 | Loss: 0.00031702
Iteration 17/25 | Loss: 0.00031702
Iteration 18/25 | Loss: 0.00031702
Iteration 19/25 | Loss: 0.00031702
Iteration 20/25 | Loss: 0.00031702
Iteration 21/25 | Loss: 0.00031702
Iteration 22/25 | Loss: 0.00031702
Iteration 23/25 | Loss: 0.00031702
Iteration 24/25 | Loss: 0.00031702
Iteration 25/25 | Loss: 0.00031702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031702
Iteration 2/1000 | Loss: 0.00002723
Iteration 3/1000 | Loss: 0.00002147
Iteration 4/1000 | Loss: 0.00002050
Iteration 5/1000 | Loss: 0.00001966
Iteration 6/1000 | Loss: 0.00001934
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001884
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001852
Iteration 11/1000 | Loss: 0.00001849
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001845
Iteration 14/1000 | Loss: 0.00001845
Iteration 15/1000 | Loss: 0.00001845
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001843
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001833
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001827
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001826
Iteration 28/1000 | Loss: 0.00001826
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001825
Iteration 31/1000 | Loss: 0.00001825
Iteration 32/1000 | Loss: 0.00001825
Iteration 33/1000 | Loss: 0.00001825
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001824
Iteration 37/1000 | Loss: 0.00001824
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001823
Iteration 40/1000 | Loss: 0.00001823
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00001814
Iteration 65/1000 | Loss: 0.00001814
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001809
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001804
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001804
Iteration 102/1000 | Loss: 0.00001804
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001803
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001802
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001800
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001800
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001799
Iteration 147/1000 | Loss: 0.00001799
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001799
Iteration 152/1000 | Loss: 0.00001799
Iteration 153/1000 | Loss: 0.00001799
Iteration 154/1000 | Loss: 0.00001799
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001798
Iteration 157/1000 | Loss: 0.00001798
Iteration 158/1000 | Loss: 0.00001798
Iteration 159/1000 | Loss: 0.00001798
Iteration 160/1000 | Loss: 0.00001798
Iteration 161/1000 | Loss: 0.00001798
Iteration 162/1000 | Loss: 0.00001798
Iteration 163/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7984641090151854e-05, 1.7984641090151854e-05, 1.7984641090151854e-05, 1.7984641090151854e-05, 1.7984641090151854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7984641090151854e-05

Optimization complete. Final v2v error: 3.59924054145813 mm

Highest mean error: 3.898887872695923 mm for frame 158

Lowest mean error: 3.386833667755127 mm for frame 191

Saving results

Total time: 42.48299956321716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904142
Iteration 2/25 | Loss: 0.00158186
Iteration 3/25 | Loss: 0.00088136
Iteration 4/25 | Loss: 0.00082658
Iteration 5/25 | Loss: 0.00081320
Iteration 6/25 | Loss: 0.00080870
Iteration 7/25 | Loss: 0.00080769
Iteration 8/25 | Loss: 0.00080757
Iteration 9/25 | Loss: 0.00080757
Iteration 10/25 | Loss: 0.00080757
Iteration 11/25 | Loss: 0.00080757
Iteration 12/25 | Loss: 0.00080757
Iteration 13/25 | Loss: 0.00080757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008075744844973087, 0.0008075744844973087, 0.0008075744844973087, 0.0008075744844973087, 0.0008075744844973087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008075744844973087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86130178
Iteration 2/25 | Loss: 0.00020407
Iteration 3/25 | Loss: 0.00020406
Iteration 4/25 | Loss: 0.00020406
Iteration 5/25 | Loss: 0.00020406
Iteration 6/25 | Loss: 0.00020406
Iteration 7/25 | Loss: 0.00020406
Iteration 8/25 | Loss: 0.00020406
Iteration 9/25 | Loss: 0.00020406
Iteration 10/25 | Loss: 0.00020406
Iteration 11/25 | Loss: 0.00020406
Iteration 12/25 | Loss: 0.00020406
Iteration 13/25 | Loss: 0.00020406
Iteration 14/25 | Loss: 0.00020406
Iteration 15/25 | Loss: 0.00020406
Iteration 16/25 | Loss: 0.00020406
Iteration 17/25 | Loss: 0.00020406
Iteration 18/25 | Loss: 0.00020406
Iteration 19/25 | Loss: 0.00020406
Iteration 20/25 | Loss: 0.00020406
Iteration 21/25 | Loss: 0.00020406
Iteration 22/25 | Loss: 0.00020406
Iteration 23/25 | Loss: 0.00020406
Iteration 24/25 | Loss: 0.00020406
Iteration 25/25 | Loss: 0.00020406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020406
Iteration 2/1000 | Loss: 0.00004781
Iteration 3/1000 | Loss: 0.00003422
Iteration 4/1000 | Loss: 0.00002995
Iteration 5/1000 | Loss: 0.00002851
Iteration 6/1000 | Loss: 0.00002752
Iteration 7/1000 | Loss: 0.00002687
Iteration 8/1000 | Loss: 0.00002592
Iteration 9/1000 | Loss: 0.00002545
Iteration 10/1000 | Loss: 0.00002508
Iteration 11/1000 | Loss: 0.00002481
Iteration 12/1000 | Loss: 0.00002456
Iteration 13/1000 | Loss: 0.00002440
Iteration 14/1000 | Loss: 0.00002433
Iteration 15/1000 | Loss: 0.00002428
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002426
Iteration 18/1000 | Loss: 0.00002425
Iteration 19/1000 | Loss: 0.00002425
Iteration 20/1000 | Loss: 0.00002423
Iteration 21/1000 | Loss: 0.00002423
Iteration 22/1000 | Loss: 0.00002422
Iteration 23/1000 | Loss: 0.00002422
Iteration 24/1000 | Loss: 0.00002422
Iteration 25/1000 | Loss: 0.00002421
Iteration 26/1000 | Loss: 0.00002421
Iteration 27/1000 | Loss: 0.00002421
Iteration 28/1000 | Loss: 0.00002421
Iteration 29/1000 | Loss: 0.00002420
Iteration 30/1000 | Loss: 0.00002420
Iteration 31/1000 | Loss: 0.00002419
Iteration 32/1000 | Loss: 0.00002419
Iteration 33/1000 | Loss: 0.00002419
Iteration 34/1000 | Loss: 0.00002419
Iteration 35/1000 | Loss: 0.00002419
Iteration 36/1000 | Loss: 0.00002419
Iteration 37/1000 | Loss: 0.00002419
Iteration 38/1000 | Loss: 0.00002419
Iteration 39/1000 | Loss: 0.00002419
Iteration 40/1000 | Loss: 0.00002419
Iteration 41/1000 | Loss: 0.00002419
Iteration 42/1000 | Loss: 0.00002417
Iteration 43/1000 | Loss: 0.00002417
Iteration 44/1000 | Loss: 0.00002417
Iteration 45/1000 | Loss: 0.00002417
Iteration 46/1000 | Loss: 0.00002416
Iteration 47/1000 | Loss: 0.00002416
Iteration 48/1000 | Loss: 0.00002416
Iteration 49/1000 | Loss: 0.00002416
Iteration 50/1000 | Loss: 0.00002415
Iteration 51/1000 | Loss: 0.00002415
Iteration 52/1000 | Loss: 0.00002415
Iteration 53/1000 | Loss: 0.00002415
Iteration 54/1000 | Loss: 0.00002414
Iteration 55/1000 | Loss: 0.00002414
Iteration 56/1000 | Loss: 0.00002414
Iteration 57/1000 | Loss: 0.00002413
Iteration 58/1000 | Loss: 0.00002413
Iteration 59/1000 | Loss: 0.00002413
Iteration 60/1000 | Loss: 0.00002412
Iteration 61/1000 | Loss: 0.00002412
Iteration 62/1000 | Loss: 0.00002412
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002411
Iteration 65/1000 | Loss: 0.00002410
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002407
Iteration 68/1000 | Loss: 0.00002407
Iteration 69/1000 | Loss: 0.00002407
Iteration 70/1000 | Loss: 0.00002407
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002405
Iteration 73/1000 | Loss: 0.00002404
Iteration 74/1000 | Loss: 0.00002404
Iteration 75/1000 | Loss: 0.00002403
Iteration 76/1000 | Loss: 0.00002403
Iteration 77/1000 | Loss: 0.00002403
Iteration 78/1000 | Loss: 0.00002403
Iteration 79/1000 | Loss: 0.00002403
Iteration 80/1000 | Loss: 0.00002402
Iteration 81/1000 | Loss: 0.00002402
Iteration 82/1000 | Loss: 0.00002402
Iteration 83/1000 | Loss: 0.00002402
Iteration 84/1000 | Loss: 0.00002401
Iteration 85/1000 | Loss: 0.00002401
Iteration 86/1000 | Loss: 0.00002401
Iteration 87/1000 | Loss: 0.00002401
Iteration 88/1000 | Loss: 0.00002401
Iteration 89/1000 | Loss: 0.00002400
Iteration 90/1000 | Loss: 0.00002400
Iteration 91/1000 | Loss: 0.00002400
Iteration 92/1000 | Loss: 0.00002400
Iteration 93/1000 | Loss: 0.00002400
Iteration 94/1000 | Loss: 0.00002399
Iteration 95/1000 | Loss: 0.00002399
Iteration 96/1000 | Loss: 0.00002399
Iteration 97/1000 | Loss: 0.00002399
Iteration 98/1000 | Loss: 0.00002398
Iteration 99/1000 | Loss: 0.00002398
Iteration 100/1000 | Loss: 0.00002398
Iteration 101/1000 | Loss: 0.00002397
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002397
Iteration 104/1000 | Loss: 0.00002397
Iteration 105/1000 | Loss: 0.00002396
Iteration 106/1000 | Loss: 0.00002396
Iteration 107/1000 | Loss: 0.00002396
Iteration 108/1000 | Loss: 0.00002396
Iteration 109/1000 | Loss: 0.00002396
Iteration 110/1000 | Loss: 0.00002396
Iteration 111/1000 | Loss: 0.00002396
Iteration 112/1000 | Loss: 0.00002396
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002396
Iteration 116/1000 | Loss: 0.00002395
Iteration 117/1000 | Loss: 0.00002395
Iteration 118/1000 | Loss: 0.00002395
Iteration 119/1000 | Loss: 0.00002395
Iteration 120/1000 | Loss: 0.00002395
Iteration 121/1000 | Loss: 0.00002395
Iteration 122/1000 | Loss: 0.00002395
Iteration 123/1000 | Loss: 0.00002395
Iteration 124/1000 | Loss: 0.00002395
Iteration 125/1000 | Loss: 0.00002395
Iteration 126/1000 | Loss: 0.00002395
Iteration 127/1000 | Loss: 0.00002395
Iteration 128/1000 | Loss: 0.00002395
Iteration 129/1000 | Loss: 0.00002395
Iteration 130/1000 | Loss: 0.00002395
Iteration 131/1000 | Loss: 0.00002394
Iteration 132/1000 | Loss: 0.00002394
Iteration 133/1000 | Loss: 0.00002393
Iteration 134/1000 | Loss: 0.00002393
Iteration 135/1000 | Loss: 0.00002393
Iteration 136/1000 | Loss: 0.00002393
Iteration 137/1000 | Loss: 0.00002393
Iteration 138/1000 | Loss: 0.00002393
Iteration 139/1000 | Loss: 0.00002393
Iteration 140/1000 | Loss: 0.00002392
Iteration 141/1000 | Loss: 0.00002392
Iteration 142/1000 | Loss: 0.00002392
Iteration 143/1000 | Loss: 0.00002392
Iteration 144/1000 | Loss: 0.00002391
Iteration 145/1000 | Loss: 0.00002391
Iteration 146/1000 | Loss: 0.00002391
Iteration 147/1000 | Loss: 0.00002391
Iteration 148/1000 | Loss: 0.00002391
Iteration 149/1000 | Loss: 0.00002391
Iteration 150/1000 | Loss: 0.00002391
Iteration 151/1000 | Loss: 0.00002391
Iteration 152/1000 | Loss: 0.00002391
Iteration 153/1000 | Loss: 0.00002391
Iteration 154/1000 | Loss: 0.00002391
Iteration 155/1000 | Loss: 0.00002390
Iteration 156/1000 | Loss: 0.00002390
Iteration 157/1000 | Loss: 0.00002390
Iteration 158/1000 | Loss: 0.00002390
Iteration 159/1000 | Loss: 0.00002390
Iteration 160/1000 | Loss: 0.00002389
Iteration 161/1000 | Loss: 0.00002389
Iteration 162/1000 | Loss: 0.00002389
Iteration 163/1000 | Loss: 0.00002389
Iteration 164/1000 | Loss: 0.00002389
Iteration 165/1000 | Loss: 0.00002389
Iteration 166/1000 | Loss: 0.00002389
Iteration 167/1000 | Loss: 0.00002388
Iteration 168/1000 | Loss: 0.00002388
Iteration 169/1000 | Loss: 0.00002388
Iteration 170/1000 | Loss: 0.00002388
Iteration 171/1000 | Loss: 0.00002388
Iteration 172/1000 | Loss: 0.00002387
Iteration 173/1000 | Loss: 0.00002387
Iteration 174/1000 | Loss: 0.00002387
Iteration 175/1000 | Loss: 0.00002387
Iteration 176/1000 | Loss: 0.00002386
Iteration 177/1000 | Loss: 0.00002386
Iteration 178/1000 | Loss: 0.00002386
Iteration 179/1000 | Loss: 0.00002386
Iteration 180/1000 | Loss: 0.00002386
Iteration 181/1000 | Loss: 0.00002385
Iteration 182/1000 | Loss: 0.00002385
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002385
Iteration 185/1000 | Loss: 0.00002385
Iteration 186/1000 | Loss: 0.00002385
Iteration 187/1000 | Loss: 0.00002384
Iteration 188/1000 | Loss: 0.00002384
Iteration 189/1000 | Loss: 0.00002384
Iteration 190/1000 | Loss: 0.00002384
Iteration 191/1000 | Loss: 0.00002384
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002384
Iteration 194/1000 | Loss: 0.00002384
Iteration 195/1000 | Loss: 0.00002384
Iteration 196/1000 | Loss: 0.00002384
Iteration 197/1000 | Loss: 0.00002384
Iteration 198/1000 | Loss: 0.00002384
Iteration 199/1000 | Loss: 0.00002384
Iteration 200/1000 | Loss: 0.00002384
Iteration 201/1000 | Loss: 0.00002384
Iteration 202/1000 | Loss: 0.00002384
Iteration 203/1000 | Loss: 0.00002383
Iteration 204/1000 | Loss: 0.00002383
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002383
Iteration 208/1000 | Loss: 0.00002383
Iteration 209/1000 | Loss: 0.00002383
Iteration 210/1000 | Loss: 0.00002383
Iteration 211/1000 | Loss: 0.00002382
Iteration 212/1000 | Loss: 0.00002382
Iteration 213/1000 | Loss: 0.00002382
Iteration 214/1000 | Loss: 0.00002382
Iteration 215/1000 | Loss: 0.00002382
Iteration 216/1000 | Loss: 0.00002382
Iteration 217/1000 | Loss: 0.00002382
Iteration 218/1000 | Loss: 0.00002382
Iteration 219/1000 | Loss: 0.00002382
Iteration 220/1000 | Loss: 0.00002382
Iteration 221/1000 | Loss: 0.00002382
Iteration 222/1000 | Loss: 0.00002381
Iteration 223/1000 | Loss: 0.00002381
Iteration 224/1000 | Loss: 0.00002381
Iteration 225/1000 | Loss: 0.00002381
Iteration 226/1000 | Loss: 0.00002381
Iteration 227/1000 | Loss: 0.00002381
Iteration 228/1000 | Loss: 0.00002381
Iteration 229/1000 | Loss: 0.00002381
Iteration 230/1000 | Loss: 0.00002381
Iteration 231/1000 | Loss: 0.00002381
Iteration 232/1000 | Loss: 0.00002381
Iteration 233/1000 | Loss: 0.00002381
Iteration 234/1000 | Loss: 0.00002381
Iteration 235/1000 | Loss: 0.00002381
Iteration 236/1000 | Loss: 0.00002381
Iteration 237/1000 | Loss: 0.00002380
Iteration 238/1000 | Loss: 0.00002380
Iteration 239/1000 | Loss: 0.00002380
Iteration 240/1000 | Loss: 0.00002380
Iteration 241/1000 | Loss: 0.00002380
Iteration 242/1000 | Loss: 0.00002380
Iteration 243/1000 | Loss: 0.00002380
Iteration 244/1000 | Loss: 0.00002380
Iteration 245/1000 | Loss: 0.00002379
Iteration 246/1000 | Loss: 0.00002379
Iteration 247/1000 | Loss: 0.00002379
Iteration 248/1000 | Loss: 0.00002379
Iteration 249/1000 | Loss: 0.00002379
Iteration 250/1000 | Loss: 0.00002379
Iteration 251/1000 | Loss: 0.00002379
Iteration 252/1000 | Loss: 0.00002379
Iteration 253/1000 | Loss: 0.00002379
Iteration 254/1000 | Loss: 0.00002379
Iteration 255/1000 | Loss: 0.00002378
Iteration 256/1000 | Loss: 0.00002378
Iteration 257/1000 | Loss: 0.00002378
Iteration 258/1000 | Loss: 0.00002378
Iteration 259/1000 | Loss: 0.00002378
Iteration 260/1000 | Loss: 0.00002378
Iteration 261/1000 | Loss: 0.00002378
Iteration 262/1000 | Loss: 0.00002378
Iteration 263/1000 | Loss: 0.00002378
Iteration 264/1000 | Loss: 0.00002378
Iteration 265/1000 | Loss: 0.00002378
Iteration 266/1000 | Loss: 0.00002378
Iteration 267/1000 | Loss: 0.00002378
Iteration 268/1000 | Loss: 0.00002378
Iteration 269/1000 | Loss: 0.00002377
Iteration 270/1000 | Loss: 0.00002377
Iteration 271/1000 | Loss: 0.00002377
Iteration 272/1000 | Loss: 0.00002377
Iteration 273/1000 | Loss: 0.00002377
Iteration 274/1000 | Loss: 0.00002377
Iteration 275/1000 | Loss: 0.00002377
Iteration 276/1000 | Loss: 0.00002377
Iteration 277/1000 | Loss: 0.00002377
Iteration 278/1000 | Loss: 0.00002377
Iteration 279/1000 | Loss: 0.00002377
Iteration 280/1000 | Loss: 0.00002377
Iteration 281/1000 | Loss: 0.00002377
Iteration 282/1000 | Loss: 0.00002377
Iteration 283/1000 | Loss: 0.00002377
Iteration 284/1000 | Loss: 0.00002377
Iteration 285/1000 | Loss: 0.00002377
Iteration 286/1000 | Loss: 0.00002377
Iteration 287/1000 | Loss: 0.00002377
Iteration 288/1000 | Loss: 0.00002376
Iteration 289/1000 | Loss: 0.00002376
Iteration 290/1000 | Loss: 0.00002376
Iteration 291/1000 | Loss: 0.00002376
Iteration 292/1000 | Loss: 0.00002376
Iteration 293/1000 | Loss: 0.00002376
Iteration 294/1000 | Loss: 0.00002376
Iteration 295/1000 | Loss: 0.00002376
Iteration 296/1000 | Loss: 0.00002376
Iteration 297/1000 | Loss: 0.00002376
Iteration 298/1000 | Loss: 0.00002376
Iteration 299/1000 | Loss: 0.00002376
Iteration 300/1000 | Loss: 0.00002376
Iteration 301/1000 | Loss: 0.00002376
Iteration 302/1000 | Loss: 0.00002376
Iteration 303/1000 | Loss: 0.00002375
Iteration 304/1000 | Loss: 0.00002375
Iteration 305/1000 | Loss: 0.00002375
Iteration 306/1000 | Loss: 0.00002375
Iteration 307/1000 | Loss: 0.00002375
Iteration 308/1000 | Loss: 0.00002375
Iteration 309/1000 | Loss: 0.00002375
Iteration 310/1000 | Loss: 0.00002375
Iteration 311/1000 | Loss: 0.00002375
Iteration 312/1000 | Loss: 0.00002375
Iteration 313/1000 | Loss: 0.00002375
Iteration 314/1000 | Loss: 0.00002375
Iteration 315/1000 | Loss: 0.00002375
Iteration 316/1000 | Loss: 0.00002374
Iteration 317/1000 | Loss: 0.00002374
Iteration 318/1000 | Loss: 0.00002374
Iteration 319/1000 | Loss: 0.00002374
Iteration 320/1000 | Loss: 0.00002374
Iteration 321/1000 | Loss: 0.00002374
Iteration 322/1000 | Loss: 0.00002374
Iteration 323/1000 | Loss: 0.00002374
Iteration 324/1000 | Loss: 0.00002374
Iteration 325/1000 | Loss: 0.00002374
Iteration 326/1000 | Loss: 0.00002374
Iteration 327/1000 | Loss: 0.00002374
Iteration 328/1000 | Loss: 0.00002374
Iteration 329/1000 | Loss: 0.00002374
Iteration 330/1000 | Loss: 0.00002374
Iteration 331/1000 | Loss: 0.00002374
Iteration 332/1000 | Loss: 0.00002374
Iteration 333/1000 | Loss: 0.00002374
Iteration 334/1000 | Loss: 0.00002374
Iteration 335/1000 | Loss: 0.00002374
Iteration 336/1000 | Loss: 0.00002374
Iteration 337/1000 | Loss: 0.00002373
Iteration 338/1000 | Loss: 0.00002373
Iteration 339/1000 | Loss: 0.00002373
Iteration 340/1000 | Loss: 0.00002373
Iteration 341/1000 | Loss: 0.00002373
Iteration 342/1000 | Loss: 0.00002373
Iteration 343/1000 | Loss: 0.00002373
Iteration 344/1000 | Loss: 0.00002373
Iteration 345/1000 | Loss: 0.00002373
Iteration 346/1000 | Loss: 0.00002373
Iteration 347/1000 | Loss: 0.00002373
Iteration 348/1000 | Loss: 0.00002373
Iteration 349/1000 | Loss: 0.00002373
Iteration 350/1000 | Loss: 0.00002373
Iteration 351/1000 | Loss: 0.00002373
Iteration 352/1000 | Loss: 0.00002373
Iteration 353/1000 | Loss: 0.00002373
Iteration 354/1000 | Loss: 0.00002373
Iteration 355/1000 | Loss: 0.00002373
Iteration 356/1000 | Loss: 0.00002373
Iteration 357/1000 | Loss: 0.00002373
Iteration 358/1000 | Loss: 0.00002373
Iteration 359/1000 | Loss: 0.00002373
Iteration 360/1000 | Loss: 0.00002373
Iteration 361/1000 | Loss: 0.00002373
Iteration 362/1000 | Loss: 0.00002373
Iteration 363/1000 | Loss: 0.00002373
Iteration 364/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 364. Stopping optimization.
Last 5 losses: [2.3727587176836096e-05, 2.3727587176836096e-05, 2.3727587176836096e-05, 2.3727587176836096e-05, 2.3727587176836096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3727587176836096e-05

Optimization complete. Final v2v error: 3.974597454071045 mm

Highest mean error: 4.822454929351807 mm for frame 24

Lowest mean error: 3.144550323486328 mm for frame 129

Saving results

Total time: 54.75484800338745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384273
Iteration 2/25 | Loss: 0.00085487
Iteration 3/25 | Loss: 0.00069269
Iteration 4/25 | Loss: 0.00065111
Iteration 5/25 | Loss: 0.00064298
Iteration 6/25 | Loss: 0.00064107
Iteration 7/25 | Loss: 0.00064045
Iteration 8/25 | Loss: 0.00064025
Iteration 9/25 | Loss: 0.00064025
Iteration 10/25 | Loss: 0.00064025
Iteration 11/25 | Loss: 0.00064025
Iteration 12/25 | Loss: 0.00064025
Iteration 13/25 | Loss: 0.00064025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000640248297713697, 0.000640248297713697, 0.000640248297713697, 0.000640248297713697, 0.000640248297713697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000640248297713697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88231969
Iteration 2/25 | Loss: 0.00039362
Iteration 3/25 | Loss: 0.00039357
Iteration 4/25 | Loss: 0.00039357
Iteration 5/25 | Loss: 0.00039357
Iteration 6/25 | Loss: 0.00039357
Iteration 7/25 | Loss: 0.00039357
Iteration 8/25 | Loss: 0.00039357
Iteration 9/25 | Loss: 0.00039357
Iteration 10/25 | Loss: 0.00039357
Iteration 11/25 | Loss: 0.00039356
Iteration 12/25 | Loss: 0.00039356
Iteration 13/25 | Loss: 0.00039356
Iteration 14/25 | Loss: 0.00039356
Iteration 15/25 | Loss: 0.00039356
Iteration 16/25 | Loss: 0.00039356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003935649583581835, 0.0003935649583581835, 0.0003935649583581835, 0.0003935649583581835, 0.0003935649583581835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003935649583581835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039356
Iteration 2/1000 | Loss: 0.00002958
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001669
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001295
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001285
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001275
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001273
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001270
Iteration 51/1000 | Loss: 0.00001270
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001269
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001262
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001254
Iteration 94/1000 | Loss: 0.00001254
Iteration 95/1000 | Loss: 0.00001254
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001253
Iteration 98/1000 | Loss: 0.00001253
Iteration 99/1000 | Loss: 0.00001253
Iteration 100/1000 | Loss: 0.00001253
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001252
Iteration 105/1000 | Loss: 0.00001252
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001250
Iteration 119/1000 | Loss: 0.00001250
Iteration 120/1000 | Loss: 0.00001250
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001250
Iteration 123/1000 | Loss: 0.00001250
Iteration 124/1000 | Loss: 0.00001250
Iteration 125/1000 | Loss: 0.00001249
Iteration 126/1000 | Loss: 0.00001249
Iteration 127/1000 | Loss: 0.00001249
Iteration 128/1000 | Loss: 0.00001249
Iteration 129/1000 | Loss: 0.00001249
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Iteration 138/1000 | Loss: 0.00001248
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001248
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001248
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2482390047807712e-05, 1.2482390047807712e-05, 1.2482390047807712e-05, 1.2482390047807712e-05, 1.2482390047807712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2482390047807712e-05

Optimization complete. Final v2v error: 3.0172903537750244 mm

Highest mean error: 3.5567705631256104 mm for frame 8

Lowest mean error: 2.4991848468780518 mm for frame 102

Saving results

Total time: 41.658864974975586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073377
Iteration 2/25 | Loss: 0.01073377
Iteration 3/25 | Loss: 0.00293130
Iteration 4/25 | Loss: 0.00200217
Iteration 5/25 | Loss: 0.00141352
Iteration 6/25 | Loss: 0.00131351
Iteration 7/25 | Loss: 0.00127963
Iteration 8/25 | Loss: 0.00121400
Iteration 9/25 | Loss: 0.00111065
Iteration 10/25 | Loss: 0.00107276
Iteration 11/25 | Loss: 0.00106690
Iteration 12/25 | Loss: 0.00104661
Iteration 13/25 | Loss: 0.00100657
Iteration 14/25 | Loss: 0.00099443
Iteration 15/25 | Loss: 0.00097431
Iteration 16/25 | Loss: 0.00097267
Iteration 17/25 | Loss: 0.00097177
Iteration 18/25 | Loss: 0.00096658
Iteration 19/25 | Loss: 0.00097875
Iteration 20/25 | Loss: 0.00102789
Iteration 21/25 | Loss: 0.00102066
Iteration 22/25 | Loss: 0.00100438
Iteration 23/25 | Loss: 0.00101507
Iteration 24/25 | Loss: 0.00100435
Iteration 25/25 | Loss: 0.00096075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42111230
Iteration 2/25 | Loss: 0.00815448
Iteration 3/25 | Loss: 0.00815448
Iteration 4/25 | Loss: 0.00753109
Iteration 5/25 | Loss: 0.00407978
Iteration 6/25 | Loss: 0.00371107
Iteration 7/25 | Loss: 0.00361186
Iteration 8/25 | Loss: 0.00361186
Iteration 9/25 | Loss: 0.00361186
Iteration 10/25 | Loss: 0.00361186
Iteration 11/25 | Loss: 0.00361186
Iteration 12/25 | Loss: 0.00361186
Iteration 13/25 | Loss: 0.00361186
Iteration 14/25 | Loss: 0.00361186
Iteration 15/25 | Loss: 0.00361186
Iteration 16/25 | Loss: 0.00361186
Iteration 17/25 | Loss: 0.00361186
Iteration 18/25 | Loss: 0.00361186
Iteration 19/25 | Loss: 0.00361186
Iteration 20/25 | Loss: 0.00361186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0036118561401963234, 0.0036118561401963234, 0.0036118561401963234, 0.0036118561401963234, 0.0036118561401963234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036118561401963234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361186
Iteration 2/1000 | Loss: 0.00152093
Iteration 3/1000 | Loss: 0.00328035
Iteration 4/1000 | Loss: 0.00439135
Iteration 5/1000 | Loss: 0.00245924
Iteration 6/1000 | Loss: 0.00133311
Iteration 7/1000 | Loss: 0.00372513
Iteration 8/1000 | Loss: 0.00426045
Iteration 9/1000 | Loss: 0.00193670
Iteration 10/1000 | Loss: 0.00136843
Iteration 11/1000 | Loss: 0.00357182
Iteration 12/1000 | Loss: 0.00307470
Iteration 13/1000 | Loss: 0.00154652
Iteration 14/1000 | Loss: 0.00156914
Iteration 15/1000 | Loss: 0.00168857
Iteration 16/1000 | Loss: 0.00121711
Iteration 17/1000 | Loss: 0.00088844
Iteration 18/1000 | Loss: 0.00148285
Iteration 19/1000 | Loss: 0.00236726
Iteration 20/1000 | Loss: 0.00099065
Iteration 21/1000 | Loss: 0.00090333
Iteration 22/1000 | Loss: 0.00069201
Iteration 23/1000 | Loss: 0.00076880
Iteration 24/1000 | Loss: 0.00370343
Iteration 25/1000 | Loss: 0.00165512
Iteration 26/1000 | Loss: 0.00156492
Iteration 27/1000 | Loss: 0.00327483
Iteration 28/1000 | Loss: 0.00224076
Iteration 29/1000 | Loss: 0.00038445
Iteration 30/1000 | Loss: 0.00032481
Iteration 31/1000 | Loss: 0.00088432
Iteration 32/1000 | Loss: 0.00137427
Iteration 33/1000 | Loss: 0.00065218
Iteration 34/1000 | Loss: 0.00076628
Iteration 35/1000 | Loss: 0.00023495
Iteration 36/1000 | Loss: 0.00126412
Iteration 37/1000 | Loss: 0.00037675
Iteration 38/1000 | Loss: 0.00060695
Iteration 39/1000 | Loss: 0.00030377
Iteration 40/1000 | Loss: 0.00108889
Iteration 41/1000 | Loss: 0.00168189
Iteration 42/1000 | Loss: 0.00088973
Iteration 43/1000 | Loss: 0.00042268
Iteration 44/1000 | Loss: 0.00039932
Iteration 45/1000 | Loss: 0.00060876
Iteration 46/1000 | Loss: 0.00086924
Iteration 47/1000 | Loss: 0.00022743
Iteration 48/1000 | Loss: 0.00032344
Iteration 49/1000 | Loss: 0.00151026
Iteration 50/1000 | Loss: 0.00687947
Iteration 51/1000 | Loss: 0.00493695
Iteration 52/1000 | Loss: 0.00380490
Iteration 53/1000 | Loss: 0.00245360
Iteration 54/1000 | Loss: 0.00491206
Iteration 55/1000 | Loss: 0.00183355
Iteration 56/1000 | Loss: 0.00201722
Iteration 57/1000 | Loss: 0.00123552
Iteration 58/1000 | Loss: 0.00116869
Iteration 59/1000 | Loss: 0.00266023
Iteration 60/1000 | Loss: 0.00035742
Iteration 61/1000 | Loss: 0.00060297
Iteration 62/1000 | Loss: 0.00216413
Iteration 63/1000 | Loss: 0.00091142
Iteration 64/1000 | Loss: 0.00288576
Iteration 65/1000 | Loss: 0.00125269
Iteration 66/1000 | Loss: 0.00168234
Iteration 67/1000 | Loss: 0.00095616
Iteration 68/1000 | Loss: 0.00129766
Iteration 69/1000 | Loss: 0.00145423
Iteration 70/1000 | Loss: 0.00130033
Iteration 71/1000 | Loss: 0.00093768
Iteration 72/1000 | Loss: 0.00139274
Iteration 73/1000 | Loss: 0.00105210
Iteration 74/1000 | Loss: 0.00113714
Iteration 75/1000 | Loss: 0.00027282
Iteration 76/1000 | Loss: 0.00012248
Iteration 77/1000 | Loss: 0.00035694
Iteration 78/1000 | Loss: 0.00032003
Iteration 79/1000 | Loss: 0.00028601
Iteration 80/1000 | Loss: 0.00011289
Iteration 81/1000 | Loss: 0.00008694
Iteration 82/1000 | Loss: 0.00009519
Iteration 83/1000 | Loss: 0.00007640
Iteration 84/1000 | Loss: 0.00141846
Iteration 85/1000 | Loss: 0.00076608
Iteration 86/1000 | Loss: 0.00056445
Iteration 87/1000 | Loss: 0.00023095
Iteration 88/1000 | Loss: 0.00075243
Iteration 89/1000 | Loss: 0.00035387
Iteration 90/1000 | Loss: 0.00033720
Iteration 91/1000 | Loss: 0.00033265
Iteration 92/1000 | Loss: 0.00007798
Iteration 93/1000 | Loss: 0.00032754
Iteration 94/1000 | Loss: 0.00030975
Iteration 95/1000 | Loss: 0.00010330
Iteration 96/1000 | Loss: 0.00107428
Iteration 97/1000 | Loss: 0.00193203
Iteration 98/1000 | Loss: 0.00044038
Iteration 99/1000 | Loss: 0.00107432
Iteration 100/1000 | Loss: 0.00177558
Iteration 101/1000 | Loss: 0.00088235
Iteration 102/1000 | Loss: 0.00045899
Iteration 103/1000 | Loss: 0.00016059
Iteration 104/1000 | Loss: 0.00007534
Iteration 105/1000 | Loss: 0.00006301
Iteration 106/1000 | Loss: 0.00007560
Iteration 107/1000 | Loss: 0.00007319
Iteration 108/1000 | Loss: 0.00006565
Iteration 109/1000 | Loss: 0.00039972
Iteration 110/1000 | Loss: 0.00005370
Iteration 111/1000 | Loss: 0.00006544
Iteration 112/1000 | Loss: 0.00011592
Iteration 113/1000 | Loss: 0.00005408
Iteration 114/1000 | Loss: 0.00039847
Iteration 115/1000 | Loss: 0.00005698
Iteration 116/1000 | Loss: 0.00103254
Iteration 117/1000 | Loss: 0.00079351
Iteration 118/1000 | Loss: 0.00055839
Iteration 119/1000 | Loss: 0.00050185
Iteration 120/1000 | Loss: 0.00005319
Iteration 121/1000 | Loss: 0.00004600
Iteration 122/1000 | Loss: 0.00005066
Iteration 123/1000 | Loss: 0.00004220
Iteration 124/1000 | Loss: 0.00004076
Iteration 125/1000 | Loss: 0.00060356
Iteration 126/1000 | Loss: 0.00030428
Iteration 127/1000 | Loss: 0.00048975
Iteration 128/1000 | Loss: 0.00004250
Iteration 129/1000 | Loss: 0.00003914
Iteration 130/1000 | Loss: 0.00031621
Iteration 131/1000 | Loss: 0.00020625
Iteration 132/1000 | Loss: 0.00140822
Iteration 133/1000 | Loss: 0.00024613
Iteration 134/1000 | Loss: 0.00026601
Iteration 135/1000 | Loss: 0.00018436
Iteration 136/1000 | Loss: 0.00003999
Iteration 137/1000 | Loss: 0.00003600
Iteration 138/1000 | Loss: 0.00026786
Iteration 139/1000 | Loss: 0.00021891
Iteration 140/1000 | Loss: 0.00022029
Iteration 141/1000 | Loss: 0.00005895
Iteration 142/1000 | Loss: 0.00004599
Iteration 143/1000 | Loss: 0.00004151
Iteration 144/1000 | Loss: 0.00003722
Iteration 145/1000 | Loss: 0.00003377
Iteration 146/1000 | Loss: 0.00003256
Iteration 147/1000 | Loss: 0.00009947
Iteration 148/1000 | Loss: 0.00003114
Iteration 149/1000 | Loss: 0.00003033
Iteration 150/1000 | Loss: 0.00002974
Iteration 151/1000 | Loss: 0.00005704
Iteration 152/1000 | Loss: 0.00002891
Iteration 153/1000 | Loss: 0.00003690
Iteration 154/1000 | Loss: 0.00099361
Iteration 155/1000 | Loss: 0.00003320
Iteration 156/1000 | Loss: 0.00002726
Iteration 157/1000 | Loss: 0.00002603
Iteration 158/1000 | Loss: 0.00003527
Iteration 159/1000 | Loss: 0.00002812
Iteration 160/1000 | Loss: 0.00002534
Iteration 161/1000 | Loss: 0.00002532
Iteration 162/1000 | Loss: 0.00002756
Iteration 163/1000 | Loss: 0.00002629
Iteration 164/1000 | Loss: 0.00002550
Iteration 165/1000 | Loss: 0.00002538
Iteration 166/1000 | Loss: 0.00002533
Iteration 167/1000 | Loss: 0.00002520
Iteration 168/1000 | Loss: 0.00002520
Iteration 169/1000 | Loss: 0.00002520
Iteration 170/1000 | Loss: 0.00002520
Iteration 171/1000 | Loss: 0.00002520
Iteration 172/1000 | Loss: 0.00002520
Iteration 173/1000 | Loss: 0.00002520
Iteration 174/1000 | Loss: 0.00002520
Iteration 175/1000 | Loss: 0.00002520
Iteration 176/1000 | Loss: 0.00002520
Iteration 177/1000 | Loss: 0.00002520
Iteration 178/1000 | Loss: 0.00002520
Iteration 179/1000 | Loss: 0.00002520
Iteration 180/1000 | Loss: 0.00002520
Iteration 181/1000 | Loss: 0.00002520
Iteration 182/1000 | Loss: 0.00002520
Iteration 183/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.5197103241225705e-05, 2.5197103241225705e-05, 2.5197103241225705e-05, 2.5197103241225705e-05, 2.5197103241225705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5197103241225705e-05

Optimization complete. Final v2v error: 3.792731523513794 mm

Highest mean error: 17.20189666748047 mm for frame 84

Lowest mean error: 3.268479585647583 mm for frame 173

Saving results

Total time: 322.7917912006378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042060
Iteration 2/25 | Loss: 0.01042060
Iteration 3/25 | Loss: 0.01042060
Iteration 4/25 | Loss: 0.01042059
Iteration 5/25 | Loss: 0.00257508
Iteration 6/25 | Loss: 0.00213077
Iteration 7/25 | Loss: 0.00211723
Iteration 8/25 | Loss: 0.00177657
Iteration 9/25 | Loss: 0.00180836
Iteration 10/25 | Loss: 0.00167599
Iteration 11/25 | Loss: 0.00159831
Iteration 12/25 | Loss: 0.00153535
Iteration 13/25 | Loss: 0.00150400
Iteration 14/25 | Loss: 0.00150388
Iteration 15/25 | Loss: 0.00139925
Iteration 16/25 | Loss: 0.00139412
Iteration 17/25 | Loss: 0.00135818
Iteration 18/25 | Loss: 0.00133780
Iteration 19/25 | Loss: 0.00131383
Iteration 20/25 | Loss: 0.00134386
Iteration 21/25 | Loss: 0.00136227
Iteration 22/25 | Loss: 0.00125389
Iteration 23/25 | Loss: 0.00120474
Iteration 24/25 | Loss: 0.00117879
Iteration 25/25 | Loss: 0.00116552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43892658
Iteration 2/25 | Loss: 0.01454361
Iteration 3/25 | Loss: 0.00339444
Iteration 4/25 | Loss: 0.00339438
Iteration 5/25 | Loss: 0.00339438
Iteration 6/25 | Loss: 0.00339438
Iteration 7/25 | Loss: 0.00339438
Iteration 8/25 | Loss: 0.00339438
Iteration 9/25 | Loss: 0.00339438
Iteration 10/25 | Loss: 0.00339438
Iteration 11/25 | Loss: 0.00339438
Iteration 12/25 | Loss: 0.00339438
Iteration 13/25 | Loss: 0.00339438
Iteration 14/25 | Loss: 0.00339438
Iteration 15/25 | Loss: 0.00339438
Iteration 16/25 | Loss: 0.00339438
Iteration 17/25 | Loss: 0.00339438
Iteration 18/25 | Loss: 0.00339438
Iteration 19/25 | Loss: 0.00339438
Iteration 20/25 | Loss: 0.00339438
Iteration 21/25 | Loss: 0.00339438
Iteration 22/25 | Loss: 0.00339438
Iteration 23/25 | Loss: 0.00339438
Iteration 24/25 | Loss: 0.00339438
Iteration 25/25 | Loss: 0.00339438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003394379746168852, 0.003394379746168852, 0.003394379746168852, 0.003394379746168852, 0.003394379746168852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003394379746168852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00339438
Iteration 2/1000 | Loss: 0.00759039
Iteration 3/1000 | Loss: 0.02374777
Iteration 4/1000 | Loss: 0.02884446
Iteration 5/1000 | Loss: 0.01354531
Iteration 6/1000 | Loss: 0.00992981
Iteration 7/1000 | Loss: 0.00738140
Iteration 8/1000 | Loss: 0.00968400
Iteration 9/1000 | Loss: 0.00262867
Iteration 10/1000 | Loss: 0.00434467
Iteration 11/1000 | Loss: 0.00447174
Iteration 12/1000 | Loss: 0.00645507
Iteration 13/1000 | Loss: 0.00866462
Iteration 14/1000 | Loss: 0.00173395
Iteration 15/1000 | Loss: 0.00506047
Iteration 16/1000 | Loss: 0.00222506
Iteration 17/1000 | Loss: 0.00327180
Iteration 18/1000 | Loss: 0.00253684
Iteration 19/1000 | Loss: 0.00221765
Iteration 20/1000 | Loss: 0.00157060
Iteration 21/1000 | Loss: 0.00073511
Iteration 22/1000 | Loss: 0.00194802
Iteration 23/1000 | Loss: 0.00302803
Iteration 24/1000 | Loss: 0.00128583
Iteration 25/1000 | Loss: 0.00110545
Iteration 26/1000 | Loss: 0.00136900
Iteration 27/1000 | Loss: 0.00211241
Iteration 28/1000 | Loss: 0.00151360
Iteration 29/1000 | Loss: 0.00089399
Iteration 30/1000 | Loss: 0.00108824
Iteration 31/1000 | Loss: 0.00136956
Iteration 32/1000 | Loss: 0.00048962
Iteration 33/1000 | Loss: 0.00097810
Iteration 34/1000 | Loss: 0.00081293
Iteration 35/1000 | Loss: 0.00048782
Iteration 36/1000 | Loss: 0.00072848
Iteration 37/1000 | Loss: 0.00055366
Iteration 38/1000 | Loss: 0.00039651
Iteration 39/1000 | Loss: 0.00018435
Iteration 40/1000 | Loss: 0.00036395
Iteration 41/1000 | Loss: 0.00075115
Iteration 42/1000 | Loss: 0.00032824
Iteration 43/1000 | Loss: 0.00072287
Iteration 44/1000 | Loss: 0.00008734
Iteration 45/1000 | Loss: 0.00041838
Iteration 46/1000 | Loss: 0.00058198
Iteration 47/1000 | Loss: 0.00007747
Iteration 48/1000 | Loss: 0.00082137
Iteration 49/1000 | Loss: 0.00137369
Iteration 50/1000 | Loss: 0.00127129
Iteration 51/1000 | Loss: 0.00290942
Iteration 52/1000 | Loss: 0.00096346
Iteration 53/1000 | Loss: 0.00232079
Iteration 54/1000 | Loss: 0.00403828
Iteration 55/1000 | Loss: 0.00198318
Iteration 56/1000 | Loss: 0.00289857
Iteration 57/1000 | Loss: 0.00025847
Iteration 58/1000 | Loss: 0.00131980
Iteration 59/1000 | Loss: 0.00142475
Iteration 60/1000 | Loss: 0.00011456
Iteration 61/1000 | Loss: 0.00091524
Iteration 62/1000 | Loss: 0.00133506
Iteration 63/1000 | Loss: 0.00064682
Iteration 64/1000 | Loss: 0.00009793
Iteration 65/1000 | Loss: 0.00066960
Iteration 66/1000 | Loss: 0.00087789
Iteration 67/1000 | Loss: 0.00013790
Iteration 68/1000 | Loss: 0.00039225
Iteration 69/1000 | Loss: 0.00005713
Iteration 70/1000 | Loss: 0.00004870
Iteration 71/1000 | Loss: 0.00004344
Iteration 72/1000 | Loss: 0.00034014
Iteration 73/1000 | Loss: 0.00017895
Iteration 74/1000 | Loss: 0.00004899
Iteration 75/1000 | Loss: 0.00003993
Iteration 76/1000 | Loss: 0.00028342
Iteration 77/1000 | Loss: 0.00003646
Iteration 78/1000 | Loss: 0.00040517
Iteration 79/1000 | Loss: 0.00013134
Iteration 80/1000 | Loss: 0.00017008
Iteration 81/1000 | Loss: 0.00018371
Iteration 82/1000 | Loss: 0.00003308
Iteration 83/1000 | Loss: 0.00025374
Iteration 84/1000 | Loss: 0.00004197
Iteration 85/1000 | Loss: 0.00005897
Iteration 86/1000 | Loss: 0.00003297
Iteration 87/1000 | Loss: 0.00003914
Iteration 88/1000 | Loss: 0.00002676
Iteration 89/1000 | Loss: 0.00012193
Iteration 90/1000 | Loss: 0.00009531
Iteration 91/1000 | Loss: 0.00002621
Iteration 92/1000 | Loss: 0.00002516
Iteration 93/1000 | Loss: 0.00004393
Iteration 94/1000 | Loss: 0.00022822
Iteration 95/1000 | Loss: 0.00006769
Iteration 96/1000 | Loss: 0.00024364
Iteration 97/1000 | Loss: 0.00009766
Iteration 98/1000 | Loss: 0.00002460
Iteration 99/1000 | Loss: 0.00002386
Iteration 100/1000 | Loss: 0.00002341
Iteration 101/1000 | Loss: 0.00002303
Iteration 102/1000 | Loss: 0.00021772
Iteration 103/1000 | Loss: 0.00003218
Iteration 104/1000 | Loss: 0.00002581
Iteration 105/1000 | Loss: 0.00012817
Iteration 106/1000 | Loss: 0.00002448
Iteration 107/1000 | Loss: 0.00008400
Iteration 108/1000 | Loss: 0.00002962
Iteration 109/1000 | Loss: 0.00003387
Iteration 110/1000 | Loss: 0.00023449
Iteration 111/1000 | Loss: 0.00013595
Iteration 112/1000 | Loss: 0.00003304
Iteration 113/1000 | Loss: 0.00002481
Iteration 114/1000 | Loss: 0.00011666
Iteration 115/1000 | Loss: 0.00067374
Iteration 116/1000 | Loss: 0.00037470
Iteration 117/1000 | Loss: 0.00058455
Iteration 118/1000 | Loss: 0.00159156
Iteration 119/1000 | Loss: 0.00006058
Iteration 120/1000 | Loss: 0.00004183
Iteration 121/1000 | Loss: 0.00002817
Iteration 122/1000 | Loss: 0.00002785
Iteration 123/1000 | Loss: 0.00003032
Iteration 124/1000 | Loss: 0.00002733
Iteration 125/1000 | Loss: 0.00002779
Iteration 126/1000 | Loss: 0.00002503
Iteration 127/1000 | Loss: 0.00018146
Iteration 128/1000 | Loss: 0.00002530
Iteration 129/1000 | Loss: 0.00018244
Iteration 130/1000 | Loss: 0.00047859
Iteration 131/1000 | Loss: 0.00003434
Iteration 132/1000 | Loss: 0.00002299
Iteration 133/1000 | Loss: 0.00002249
Iteration 134/1000 | Loss: 0.00012786
Iteration 135/1000 | Loss: 0.00002999
Iteration 136/1000 | Loss: 0.00002580
Iteration 137/1000 | Loss: 0.00002226
Iteration 138/1000 | Loss: 0.00002212
Iteration 139/1000 | Loss: 0.00002205
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002200
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002197
Iteration 144/1000 | Loss: 0.00002196
Iteration 145/1000 | Loss: 0.00002195
Iteration 146/1000 | Loss: 0.00002194
Iteration 147/1000 | Loss: 0.00002194
Iteration 148/1000 | Loss: 0.00002193
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00002192
Iteration 151/1000 | Loss: 0.00002191
Iteration 152/1000 | Loss: 0.00002191
Iteration 153/1000 | Loss: 0.00010780
Iteration 154/1000 | Loss: 0.00004082
Iteration 155/1000 | Loss: 0.00002189
Iteration 156/1000 | Loss: 0.00002188
Iteration 157/1000 | Loss: 0.00002188
Iteration 158/1000 | Loss: 0.00002188
Iteration 159/1000 | Loss: 0.00002188
Iteration 160/1000 | Loss: 0.00002187
Iteration 161/1000 | Loss: 0.00002187
Iteration 162/1000 | Loss: 0.00002187
Iteration 163/1000 | Loss: 0.00002187
Iteration 164/1000 | Loss: 0.00002187
Iteration 165/1000 | Loss: 0.00002187
Iteration 166/1000 | Loss: 0.00002186
Iteration 167/1000 | Loss: 0.00002186
Iteration 168/1000 | Loss: 0.00002186
Iteration 169/1000 | Loss: 0.00002185
Iteration 170/1000 | Loss: 0.00002185
Iteration 171/1000 | Loss: 0.00002185
Iteration 172/1000 | Loss: 0.00002184
Iteration 173/1000 | Loss: 0.00002184
Iteration 174/1000 | Loss: 0.00002184
Iteration 175/1000 | Loss: 0.00002184
Iteration 176/1000 | Loss: 0.00002184
Iteration 177/1000 | Loss: 0.00002184
Iteration 178/1000 | Loss: 0.00002183
Iteration 179/1000 | Loss: 0.00002183
Iteration 180/1000 | Loss: 0.00002183
Iteration 181/1000 | Loss: 0.00002183
Iteration 182/1000 | Loss: 0.00002183
Iteration 183/1000 | Loss: 0.00002183
Iteration 184/1000 | Loss: 0.00002183
Iteration 185/1000 | Loss: 0.00002183
Iteration 186/1000 | Loss: 0.00002183
Iteration 187/1000 | Loss: 0.00002183
Iteration 188/1000 | Loss: 0.00002183
Iteration 189/1000 | Loss: 0.00002182
Iteration 190/1000 | Loss: 0.00002182
Iteration 191/1000 | Loss: 0.00002182
Iteration 192/1000 | Loss: 0.00002182
Iteration 193/1000 | Loss: 0.00002182
Iteration 194/1000 | Loss: 0.00002182
Iteration 195/1000 | Loss: 0.00002182
Iteration 196/1000 | Loss: 0.00002182
Iteration 197/1000 | Loss: 0.00002182
Iteration 198/1000 | Loss: 0.00002182
Iteration 199/1000 | Loss: 0.00002182
Iteration 200/1000 | Loss: 0.00002182
Iteration 201/1000 | Loss: 0.00002182
Iteration 202/1000 | Loss: 0.00002182
Iteration 203/1000 | Loss: 0.00002181
Iteration 204/1000 | Loss: 0.00002181
Iteration 205/1000 | Loss: 0.00002181
Iteration 206/1000 | Loss: 0.00002181
Iteration 207/1000 | Loss: 0.00002181
Iteration 208/1000 | Loss: 0.00002181
Iteration 209/1000 | Loss: 0.00002181
Iteration 210/1000 | Loss: 0.00002181
Iteration 211/1000 | Loss: 0.00002180
Iteration 212/1000 | Loss: 0.00002180
Iteration 213/1000 | Loss: 0.00002180
Iteration 214/1000 | Loss: 0.00002180
Iteration 215/1000 | Loss: 0.00002180
Iteration 216/1000 | Loss: 0.00002180
Iteration 217/1000 | Loss: 0.00002180
Iteration 218/1000 | Loss: 0.00002180
Iteration 219/1000 | Loss: 0.00002180
Iteration 220/1000 | Loss: 0.00002180
Iteration 221/1000 | Loss: 0.00002180
Iteration 222/1000 | Loss: 0.00002180
Iteration 223/1000 | Loss: 0.00002180
Iteration 224/1000 | Loss: 0.00002180
Iteration 225/1000 | Loss: 0.00002180
Iteration 226/1000 | Loss: 0.00002179
Iteration 227/1000 | Loss: 0.00002179
Iteration 228/1000 | Loss: 0.00002179
Iteration 229/1000 | Loss: 0.00002179
Iteration 230/1000 | Loss: 0.00002179
Iteration 231/1000 | Loss: 0.00002179
Iteration 232/1000 | Loss: 0.00002179
Iteration 233/1000 | Loss: 0.00002179
Iteration 234/1000 | Loss: 0.00002179
Iteration 235/1000 | Loss: 0.00002179
Iteration 236/1000 | Loss: 0.00002179
Iteration 237/1000 | Loss: 0.00002179
Iteration 238/1000 | Loss: 0.00002178
Iteration 239/1000 | Loss: 0.00002178
Iteration 240/1000 | Loss: 0.00002178
Iteration 241/1000 | Loss: 0.00002178
Iteration 242/1000 | Loss: 0.00002178
Iteration 243/1000 | Loss: 0.00002177
Iteration 244/1000 | Loss: 0.00002177
Iteration 245/1000 | Loss: 0.00002177
Iteration 246/1000 | Loss: 0.00002177
Iteration 247/1000 | Loss: 0.00002177
Iteration 248/1000 | Loss: 0.00002177
Iteration 249/1000 | Loss: 0.00002177
Iteration 250/1000 | Loss: 0.00002177
Iteration 251/1000 | Loss: 0.00002177
Iteration 252/1000 | Loss: 0.00002177
Iteration 253/1000 | Loss: 0.00002177
Iteration 254/1000 | Loss: 0.00002177
Iteration 255/1000 | Loss: 0.00002177
Iteration 256/1000 | Loss: 0.00002177
Iteration 257/1000 | Loss: 0.00002177
Iteration 258/1000 | Loss: 0.00002177
Iteration 259/1000 | Loss: 0.00002177
Iteration 260/1000 | Loss: 0.00002177
Iteration 261/1000 | Loss: 0.00002177
Iteration 262/1000 | Loss: 0.00002177
Iteration 263/1000 | Loss: 0.00002176
Iteration 264/1000 | Loss: 0.00002176
Iteration 265/1000 | Loss: 0.00002176
Iteration 266/1000 | Loss: 0.00002176
Iteration 267/1000 | Loss: 0.00002176
Iteration 268/1000 | Loss: 0.00002176
Iteration 269/1000 | Loss: 0.00002176
Iteration 270/1000 | Loss: 0.00002176
Iteration 271/1000 | Loss: 0.00002175
Iteration 272/1000 | Loss: 0.00002175
Iteration 273/1000 | Loss: 0.00002175
Iteration 274/1000 | Loss: 0.00002175
Iteration 275/1000 | Loss: 0.00002175
Iteration 276/1000 | Loss: 0.00002175
Iteration 277/1000 | Loss: 0.00002175
Iteration 278/1000 | Loss: 0.00002175
Iteration 279/1000 | Loss: 0.00002175
Iteration 280/1000 | Loss: 0.00002175
Iteration 281/1000 | Loss: 0.00002175
Iteration 282/1000 | Loss: 0.00002174
Iteration 283/1000 | Loss: 0.00002174
Iteration 284/1000 | Loss: 0.00002174
Iteration 285/1000 | Loss: 0.00002174
Iteration 286/1000 | Loss: 0.00002174
Iteration 287/1000 | Loss: 0.00002174
Iteration 288/1000 | Loss: 0.00002174
Iteration 289/1000 | Loss: 0.00002174
Iteration 290/1000 | Loss: 0.00002174
Iteration 291/1000 | Loss: 0.00002174
Iteration 292/1000 | Loss: 0.00002173
Iteration 293/1000 | Loss: 0.00002173
Iteration 294/1000 | Loss: 0.00002173
Iteration 295/1000 | Loss: 0.00002173
Iteration 296/1000 | Loss: 0.00002173
Iteration 297/1000 | Loss: 0.00002173
Iteration 298/1000 | Loss: 0.00002173
Iteration 299/1000 | Loss: 0.00002173
Iteration 300/1000 | Loss: 0.00002173
Iteration 301/1000 | Loss: 0.00002173
Iteration 302/1000 | Loss: 0.00002172
Iteration 303/1000 | Loss: 0.00002172
Iteration 304/1000 | Loss: 0.00002172
Iteration 305/1000 | Loss: 0.00002172
Iteration 306/1000 | Loss: 0.00002172
Iteration 307/1000 | Loss: 0.00002172
Iteration 308/1000 | Loss: 0.00002171
Iteration 309/1000 | Loss: 0.00002171
Iteration 310/1000 | Loss: 0.00002171
Iteration 311/1000 | Loss: 0.00002170
Iteration 312/1000 | Loss: 0.00002170
Iteration 313/1000 | Loss: 0.00002170
Iteration 314/1000 | Loss: 0.00002170
Iteration 315/1000 | Loss: 0.00002170
Iteration 316/1000 | Loss: 0.00002170
Iteration 317/1000 | Loss: 0.00002170
Iteration 318/1000 | Loss: 0.00002170
Iteration 319/1000 | Loss: 0.00002170
Iteration 320/1000 | Loss: 0.00002170
Iteration 321/1000 | Loss: 0.00002169
Iteration 322/1000 | Loss: 0.00002169
Iteration 323/1000 | Loss: 0.00002169
Iteration 324/1000 | Loss: 0.00002169
Iteration 325/1000 | Loss: 0.00002169
Iteration 326/1000 | Loss: 0.00002168
Iteration 327/1000 | Loss: 0.00002168
Iteration 328/1000 | Loss: 0.00002168
Iteration 329/1000 | Loss: 0.00002168
Iteration 330/1000 | Loss: 0.00002168
Iteration 331/1000 | Loss: 0.00002168
Iteration 332/1000 | Loss: 0.00002168
Iteration 333/1000 | Loss: 0.00002168
Iteration 334/1000 | Loss: 0.00002168
Iteration 335/1000 | Loss: 0.00002168
Iteration 336/1000 | Loss: 0.00002168
Iteration 337/1000 | Loss: 0.00002168
Iteration 338/1000 | Loss: 0.00002168
Iteration 339/1000 | Loss: 0.00002168
Iteration 340/1000 | Loss: 0.00002167
Iteration 341/1000 | Loss: 0.00002167
Iteration 342/1000 | Loss: 0.00002167
Iteration 343/1000 | Loss: 0.00002166
Iteration 344/1000 | Loss: 0.00002165
Iteration 345/1000 | Loss: 0.00002165
Iteration 346/1000 | Loss: 0.00002165
Iteration 347/1000 | Loss: 0.00002164
Iteration 348/1000 | Loss: 0.00002164
Iteration 349/1000 | Loss: 0.00012774
Iteration 350/1000 | Loss: 0.00004293
Iteration 351/1000 | Loss: 0.00004376
Iteration 352/1000 | Loss: 0.00002160
Iteration 353/1000 | Loss: 0.00002159
Iteration 354/1000 | Loss: 0.00002159
Iteration 355/1000 | Loss: 0.00002159
Iteration 356/1000 | Loss: 0.00002159
Iteration 357/1000 | Loss: 0.00002159
Iteration 358/1000 | Loss: 0.00002159
Iteration 359/1000 | Loss: 0.00002159
Iteration 360/1000 | Loss: 0.00002159
Iteration 361/1000 | Loss: 0.00002159
Iteration 362/1000 | Loss: 0.00002159
Iteration 363/1000 | Loss: 0.00002159
Iteration 364/1000 | Loss: 0.00002159
Iteration 365/1000 | Loss: 0.00002158
Iteration 366/1000 | Loss: 0.00002158
Iteration 367/1000 | Loss: 0.00002158
Iteration 368/1000 | Loss: 0.00010347
Iteration 369/1000 | Loss: 0.00019023
Iteration 370/1000 | Loss: 0.00010624
Iteration 371/1000 | Loss: 0.00002859
Iteration 372/1000 | Loss: 0.00002484
Iteration 373/1000 | Loss: 0.00004813
Iteration 374/1000 | Loss: 0.00002169
Iteration 375/1000 | Loss: 0.00002156
Iteration 376/1000 | Loss: 0.00002153
Iteration 377/1000 | Loss: 0.00002152
Iteration 378/1000 | Loss: 0.00002151
Iteration 379/1000 | Loss: 0.00002150
Iteration 380/1000 | Loss: 0.00002150
Iteration 381/1000 | Loss: 0.00002149
Iteration 382/1000 | Loss: 0.00002147
Iteration 383/1000 | Loss: 0.00002147
Iteration 384/1000 | Loss: 0.00002146
Iteration 385/1000 | Loss: 0.00002146
Iteration 386/1000 | Loss: 0.00002145
Iteration 387/1000 | Loss: 0.00002141
Iteration 388/1000 | Loss: 0.00002140
Iteration 389/1000 | Loss: 0.00002138
Iteration 390/1000 | Loss: 0.00002136
Iteration 391/1000 | Loss: 0.00002135
Iteration 392/1000 | Loss: 0.00002135
Iteration 393/1000 | Loss: 0.00002134
Iteration 394/1000 | Loss: 0.00002134
Iteration 395/1000 | Loss: 0.00002134
Iteration 396/1000 | Loss: 0.00002133
Iteration 397/1000 | Loss: 0.00002133
Iteration 398/1000 | Loss: 0.00002132
Iteration 399/1000 | Loss: 0.00002131
Iteration 400/1000 | Loss: 0.00002131
Iteration 401/1000 | Loss: 0.00002131
Iteration 402/1000 | Loss: 0.00002131
Iteration 403/1000 | Loss: 0.00002131
Iteration 404/1000 | Loss: 0.00002131
Iteration 405/1000 | Loss: 0.00002131
Iteration 406/1000 | Loss: 0.00002131
Iteration 407/1000 | Loss: 0.00002131
Iteration 408/1000 | Loss: 0.00002131
Iteration 409/1000 | Loss: 0.00002130
Iteration 410/1000 | Loss: 0.00002130
Iteration 411/1000 | Loss: 0.00002130
Iteration 412/1000 | Loss: 0.00002130
Iteration 413/1000 | Loss: 0.00002130
Iteration 414/1000 | Loss: 0.00002130
Iteration 415/1000 | Loss: 0.00002130
Iteration 416/1000 | Loss: 0.00002130
Iteration 417/1000 | Loss: 0.00002129
Iteration 418/1000 | Loss: 0.00002129
Iteration 419/1000 | Loss: 0.00002129
Iteration 420/1000 | Loss: 0.00002129
Iteration 421/1000 | Loss: 0.00002129
Iteration 422/1000 | Loss: 0.00002129
Iteration 423/1000 | Loss: 0.00002129
Iteration 424/1000 | Loss: 0.00002128
Iteration 425/1000 | Loss: 0.00002128
Iteration 426/1000 | Loss: 0.00002128
Iteration 427/1000 | Loss: 0.00002128
Iteration 428/1000 | Loss: 0.00002128
Iteration 429/1000 | Loss: 0.00002128
Iteration 430/1000 | Loss: 0.00002128
Iteration 431/1000 | Loss: 0.00002128
Iteration 432/1000 | Loss: 0.00002128
Iteration 433/1000 | Loss: 0.00002128
Iteration 434/1000 | Loss: 0.00002128
Iteration 435/1000 | Loss: 0.00002128
Iteration 436/1000 | Loss: 0.00002128
Iteration 437/1000 | Loss: 0.00002128
Iteration 438/1000 | Loss: 0.00002127
Iteration 439/1000 | Loss: 0.00002127
Iteration 440/1000 | Loss: 0.00002127
Iteration 441/1000 | Loss: 0.00002127
Iteration 442/1000 | Loss: 0.00002127
Iteration 443/1000 | Loss: 0.00002127
Iteration 444/1000 | Loss: 0.00002127
Iteration 445/1000 | Loss: 0.00002127
Iteration 446/1000 | Loss: 0.00002127
Iteration 447/1000 | Loss: 0.00002127
Iteration 448/1000 | Loss: 0.00002127
Iteration 449/1000 | Loss: 0.00002127
Iteration 450/1000 | Loss: 0.00002127
Iteration 451/1000 | Loss: 0.00002127
Iteration 452/1000 | Loss: 0.00002127
Iteration 453/1000 | Loss: 0.00002127
Iteration 454/1000 | Loss: 0.00002126
Iteration 455/1000 | Loss: 0.00002126
Iteration 456/1000 | Loss: 0.00002126
Iteration 457/1000 | Loss: 0.00002126
Iteration 458/1000 | Loss: 0.00002126
Iteration 459/1000 | Loss: 0.00002126
Iteration 460/1000 | Loss: 0.00002125
Iteration 461/1000 | Loss: 0.00002125
Iteration 462/1000 | Loss: 0.00002125
Iteration 463/1000 | Loss: 0.00002125
Iteration 464/1000 | Loss: 0.00002125
Iteration 465/1000 | Loss: 0.00002125
Iteration 466/1000 | Loss: 0.00002125
Iteration 467/1000 | Loss: 0.00002125
Iteration 468/1000 | Loss: 0.00002124
Iteration 469/1000 | Loss: 0.00002124
Iteration 470/1000 | Loss: 0.00002124
Iteration 471/1000 | Loss: 0.00002124
Iteration 472/1000 | Loss: 0.00002124
Iteration 473/1000 | Loss: 0.00002124
Iteration 474/1000 | Loss: 0.00002123
Iteration 475/1000 | Loss: 0.00002123
Iteration 476/1000 | Loss: 0.00013067
Iteration 477/1000 | Loss: 0.00003585
Iteration 478/1000 | Loss: 0.00002138
Iteration 479/1000 | Loss: 0.00004053
Iteration 480/1000 | Loss: 0.00002438
Iteration 481/1000 | Loss: 0.00002903
Iteration 482/1000 | Loss: 0.00002213
Iteration 483/1000 | Loss: 0.00013761
Iteration 484/1000 | Loss: 0.00002206
Iteration 485/1000 | Loss: 0.00002131
Iteration 486/1000 | Loss: 0.00002877
Iteration 487/1000 | Loss: 0.00002128
Iteration 488/1000 | Loss: 0.00002128
Iteration 489/1000 | Loss: 0.00002128
Iteration 490/1000 | Loss: 0.00002127
Iteration 491/1000 | Loss: 0.00002127
Iteration 492/1000 | Loss: 0.00002127
Iteration 493/1000 | Loss: 0.00002126
Iteration 494/1000 | Loss: 0.00002126
Iteration 495/1000 | Loss: 0.00002125
Iteration 496/1000 | Loss: 0.00002124
Iteration 497/1000 | Loss: 0.00002124
Iteration 498/1000 | Loss: 0.00002123
Iteration 499/1000 | Loss: 0.00002123
Iteration 500/1000 | Loss: 0.00002123
Iteration 501/1000 | Loss: 0.00002122
Iteration 502/1000 | Loss: 0.00002122
Iteration 503/1000 | Loss: 0.00002122
Iteration 504/1000 | Loss: 0.00002121
Iteration 505/1000 | Loss: 0.00002121
Iteration 506/1000 | Loss: 0.00002120
Iteration 507/1000 | Loss: 0.00002120
Iteration 508/1000 | Loss: 0.00002119
Iteration 509/1000 | Loss: 0.00002119
Iteration 510/1000 | Loss: 0.00002119
Iteration 511/1000 | Loss: 0.00002119
Iteration 512/1000 | Loss: 0.00002118
Iteration 513/1000 | Loss: 0.00002118
Iteration 514/1000 | Loss: 0.00002118
Iteration 515/1000 | Loss: 0.00002118
Iteration 516/1000 | Loss: 0.00002118
Iteration 517/1000 | Loss: 0.00002118
Iteration 518/1000 | Loss: 0.00002118
Iteration 519/1000 | Loss: 0.00002118
Iteration 520/1000 | Loss: 0.00002118
Iteration 521/1000 | Loss: 0.00002118
Iteration 522/1000 | Loss: 0.00002118
Iteration 523/1000 | Loss: 0.00002117
Iteration 524/1000 | Loss: 0.00002117
Iteration 525/1000 | Loss: 0.00002117
Iteration 526/1000 | Loss: 0.00002117
Iteration 527/1000 | Loss: 0.00002117
Iteration 528/1000 | Loss: 0.00002117
Iteration 529/1000 | Loss: 0.00002117
Iteration 530/1000 | Loss: 0.00002117
Iteration 531/1000 | Loss: 0.00002117
Iteration 532/1000 | Loss: 0.00002116
Iteration 533/1000 | Loss: 0.00002116
Iteration 534/1000 | Loss: 0.00002116
Iteration 535/1000 | Loss: 0.00002116
Iteration 536/1000 | Loss: 0.00002116
Iteration 537/1000 | Loss: 0.00002116
Iteration 538/1000 | Loss: 0.00002116
Iteration 539/1000 | Loss: 0.00002116
Iteration 540/1000 | Loss: 0.00002116
Iteration 541/1000 | Loss: 0.00002116
Iteration 542/1000 | Loss: 0.00002116
Iteration 543/1000 | Loss: 0.00002115
Iteration 544/1000 | Loss: 0.00002115
Iteration 545/1000 | Loss: 0.00002115
Iteration 546/1000 | Loss: 0.00002115
Iteration 547/1000 | Loss: 0.00002115
Iteration 548/1000 | Loss: 0.00002115
Iteration 549/1000 | Loss: 0.00002115
Iteration 550/1000 | Loss: 0.00002115
Iteration 551/1000 | Loss: 0.00002115
Iteration 552/1000 | Loss: 0.00002115
Iteration 553/1000 | Loss: 0.00002115
Iteration 554/1000 | Loss: 0.00002115
Iteration 555/1000 | Loss: 0.00002115
Iteration 556/1000 | Loss: 0.00002115
Iteration 557/1000 | Loss: 0.00002115
Iteration 558/1000 | Loss: 0.00002115
Iteration 559/1000 | Loss: 0.00002115
Iteration 560/1000 | Loss: 0.00002115
Iteration 561/1000 | Loss: 0.00002115
Iteration 562/1000 | Loss: 0.00002115
Iteration 563/1000 | Loss: 0.00002115
Iteration 564/1000 | Loss: 0.00002115
Iteration 565/1000 | Loss: 0.00002115
Iteration 566/1000 | Loss: 0.00002115
Iteration 567/1000 | Loss: 0.00002115
Iteration 568/1000 | Loss: 0.00002115
Iteration 569/1000 | Loss: 0.00002115
Iteration 570/1000 | Loss: 0.00002115
Iteration 571/1000 | Loss: 0.00002115
Iteration 572/1000 | Loss: 0.00002115
Iteration 573/1000 | Loss: 0.00002115
Iteration 574/1000 | Loss: 0.00002115
Iteration 575/1000 | Loss: 0.00002115
Iteration 576/1000 | Loss: 0.00002115
Iteration 577/1000 | Loss: 0.00002115
Iteration 578/1000 | Loss: 0.00002115
Iteration 579/1000 | Loss: 0.00002115
Iteration 580/1000 | Loss: 0.00002115
Iteration 581/1000 | Loss: 0.00002115
Iteration 582/1000 | Loss: 0.00002115
Iteration 583/1000 | Loss: 0.00002115
Iteration 584/1000 | Loss: 0.00002115
Iteration 585/1000 | Loss: 0.00002115
Iteration 586/1000 | Loss: 0.00002115
Iteration 587/1000 | Loss: 0.00002115
Iteration 588/1000 | Loss: 0.00002115
Iteration 589/1000 | Loss: 0.00002115
Iteration 590/1000 | Loss: 0.00002115
Iteration 591/1000 | Loss: 0.00002115
Iteration 592/1000 | Loss: 0.00002115
Iteration 593/1000 | Loss: 0.00002115
Iteration 594/1000 | Loss: 0.00002115
Iteration 595/1000 | Loss: 0.00002115
Iteration 596/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 596. Stopping optimization.
Last 5 losses: [2.1150604879949242e-05, 2.1150604879949242e-05, 2.1150604879949242e-05, 2.1150604879949242e-05, 2.1150604879949242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1150604879949242e-05

Optimization complete. Final v2v error: 3.858527898788452 mm

Highest mean error: 4.889786720275879 mm for frame 237

Lowest mean error: 3.1594977378845215 mm for frame 28

Saving results

Total time: 340.42203068733215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422375
Iteration 2/25 | Loss: 0.00077934
Iteration 3/25 | Loss: 0.00066295
Iteration 4/25 | Loss: 0.00064095
Iteration 5/25 | Loss: 0.00063517
Iteration 6/25 | Loss: 0.00063364
Iteration 7/25 | Loss: 0.00063306
Iteration 8/25 | Loss: 0.00063306
Iteration 9/25 | Loss: 0.00063306
Iteration 10/25 | Loss: 0.00063306
Iteration 11/25 | Loss: 0.00063306
Iteration 12/25 | Loss: 0.00063306
Iteration 13/25 | Loss: 0.00063306
Iteration 14/25 | Loss: 0.00063306
Iteration 15/25 | Loss: 0.00063306
Iteration 16/25 | Loss: 0.00063306
Iteration 17/25 | Loss: 0.00063306
Iteration 18/25 | Loss: 0.00063306
Iteration 19/25 | Loss: 0.00063306
Iteration 20/25 | Loss: 0.00063306
Iteration 21/25 | Loss: 0.00063306
Iteration 22/25 | Loss: 0.00063306
Iteration 23/25 | Loss: 0.00063306
Iteration 24/25 | Loss: 0.00063306
Iteration 25/25 | Loss: 0.00063306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89932656
Iteration 2/25 | Loss: 0.00032843
Iteration 3/25 | Loss: 0.00032841
Iteration 4/25 | Loss: 0.00032841
Iteration 5/25 | Loss: 0.00032841
Iteration 6/25 | Loss: 0.00032841
Iteration 7/25 | Loss: 0.00032840
Iteration 8/25 | Loss: 0.00032840
Iteration 9/25 | Loss: 0.00032840
Iteration 10/25 | Loss: 0.00032840
Iteration 11/25 | Loss: 0.00032840
Iteration 12/25 | Loss: 0.00032840
Iteration 13/25 | Loss: 0.00032840
Iteration 14/25 | Loss: 0.00032840
Iteration 15/25 | Loss: 0.00032840
Iteration 16/25 | Loss: 0.00032840
Iteration 17/25 | Loss: 0.00032840
Iteration 18/25 | Loss: 0.00032840
Iteration 19/25 | Loss: 0.00032840
Iteration 20/25 | Loss: 0.00032840
Iteration 21/25 | Loss: 0.00032840
Iteration 22/25 | Loss: 0.00032840
Iteration 23/25 | Loss: 0.00032840
Iteration 24/25 | Loss: 0.00032840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003284034028183669, 0.0003284034028183669, 0.0003284034028183669, 0.0003284034028183669, 0.0003284034028183669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003284034028183669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032840
Iteration 2/1000 | Loss: 0.00002919
Iteration 3/1000 | Loss: 0.00001937
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001685
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001520
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001484
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001466
Iteration 16/1000 | Loss: 0.00001462
Iteration 17/1000 | Loss: 0.00001460
Iteration 18/1000 | Loss: 0.00001459
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001454
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001446
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001442
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001438
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001433
Iteration 77/1000 | Loss: 0.00001433
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001432
Iteration 80/1000 | Loss: 0.00001432
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001425
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001423
Iteration 137/1000 | Loss: 0.00001423
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001422
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001421
Iteration 150/1000 | Loss: 0.00001421
Iteration 151/1000 | Loss: 0.00001421
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001421
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001421
Iteration 166/1000 | Loss: 0.00001421
Iteration 167/1000 | Loss: 0.00001421
Iteration 168/1000 | Loss: 0.00001421
Iteration 169/1000 | Loss: 0.00001421
Iteration 170/1000 | Loss: 0.00001421
Iteration 171/1000 | Loss: 0.00001421
Iteration 172/1000 | Loss: 0.00001421
Iteration 173/1000 | Loss: 0.00001421
Iteration 174/1000 | Loss: 0.00001421
Iteration 175/1000 | Loss: 0.00001421
Iteration 176/1000 | Loss: 0.00001421
Iteration 177/1000 | Loss: 0.00001421
Iteration 178/1000 | Loss: 0.00001421
Iteration 179/1000 | Loss: 0.00001421
Iteration 180/1000 | Loss: 0.00001421
Iteration 181/1000 | Loss: 0.00001421
Iteration 182/1000 | Loss: 0.00001421
Iteration 183/1000 | Loss: 0.00001421
Iteration 184/1000 | Loss: 0.00001421
Iteration 185/1000 | Loss: 0.00001421
Iteration 186/1000 | Loss: 0.00001421
Iteration 187/1000 | Loss: 0.00001421
Iteration 188/1000 | Loss: 0.00001421
Iteration 189/1000 | Loss: 0.00001421
Iteration 190/1000 | Loss: 0.00001421
Iteration 191/1000 | Loss: 0.00001421
Iteration 192/1000 | Loss: 0.00001421
Iteration 193/1000 | Loss: 0.00001421
Iteration 194/1000 | Loss: 0.00001421
Iteration 195/1000 | Loss: 0.00001421
Iteration 196/1000 | Loss: 0.00001421
Iteration 197/1000 | Loss: 0.00001421
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4211015695764218e-05, 1.4211015695764218e-05, 1.4211015695764218e-05, 1.4211015695764218e-05, 1.4211015695764218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4211015695764218e-05

Optimization complete. Final v2v error: 3.19218373298645 mm

Highest mean error: 3.5356831550598145 mm for frame 84

Lowest mean error: 2.8380680084228516 mm for frame 153

Saving results

Total time: 42.54150652885437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860819
Iteration 2/25 | Loss: 0.00079192
Iteration 3/25 | Loss: 0.00062727
Iteration 4/25 | Loss: 0.00060889
Iteration 5/25 | Loss: 0.00060447
Iteration 6/25 | Loss: 0.00060363
Iteration 7/25 | Loss: 0.00060344
Iteration 8/25 | Loss: 0.00060344
Iteration 9/25 | Loss: 0.00060344
Iteration 10/25 | Loss: 0.00060344
Iteration 11/25 | Loss: 0.00060344
Iteration 12/25 | Loss: 0.00060344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006034436519257724, 0.0006034436519257724, 0.0006034436519257724, 0.0006034436519257724, 0.0006034436519257724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006034436519257724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67409956
Iteration 2/25 | Loss: 0.00029252
Iteration 3/25 | Loss: 0.00029252
Iteration 4/25 | Loss: 0.00029252
Iteration 5/25 | Loss: 0.00029252
Iteration 6/25 | Loss: 0.00029252
Iteration 7/25 | Loss: 0.00029252
Iteration 8/25 | Loss: 0.00029252
Iteration 9/25 | Loss: 0.00029252
Iteration 10/25 | Loss: 0.00029252
Iteration 11/25 | Loss: 0.00029252
Iteration 12/25 | Loss: 0.00029252
Iteration 13/25 | Loss: 0.00029252
Iteration 14/25 | Loss: 0.00029252
Iteration 15/25 | Loss: 0.00029252
Iteration 16/25 | Loss: 0.00029252
Iteration 17/25 | Loss: 0.00029252
Iteration 18/25 | Loss: 0.00029252
Iteration 19/25 | Loss: 0.00029252
Iteration 20/25 | Loss: 0.00029252
Iteration 21/25 | Loss: 0.00029252
Iteration 22/25 | Loss: 0.00029252
Iteration 23/25 | Loss: 0.00029252
Iteration 24/25 | Loss: 0.00029252
Iteration 25/25 | Loss: 0.00029252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029252
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001485
Iteration 4/1000 | Loss: 0.00001343
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001206
Iteration 8/1000 | Loss: 0.00001193
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001170
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001165
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001156
Iteration 29/1000 | Loss: 0.00001155
Iteration 30/1000 | Loss: 0.00001155
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001144
Iteration 58/1000 | Loss: 0.00001144
Iteration 59/1000 | Loss: 0.00001143
Iteration 60/1000 | Loss: 0.00001143
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001142
Iteration 63/1000 | Loss: 0.00001142
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001141
Iteration 70/1000 | Loss: 0.00001141
Iteration 71/1000 | Loss: 0.00001141
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001137
Iteration 85/1000 | Loss: 0.00001137
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001132
Iteration 99/1000 | Loss: 0.00001132
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001131
Iteration 102/1000 | Loss: 0.00001131
Iteration 103/1000 | Loss: 0.00001131
Iteration 104/1000 | Loss: 0.00001131
Iteration 105/1000 | Loss: 0.00001131
Iteration 106/1000 | Loss: 0.00001131
Iteration 107/1000 | Loss: 0.00001131
Iteration 108/1000 | Loss: 0.00001130
Iteration 109/1000 | Loss: 0.00001130
Iteration 110/1000 | Loss: 0.00001130
Iteration 111/1000 | Loss: 0.00001130
Iteration 112/1000 | Loss: 0.00001130
Iteration 113/1000 | Loss: 0.00001130
Iteration 114/1000 | Loss: 0.00001130
Iteration 115/1000 | Loss: 0.00001130
Iteration 116/1000 | Loss: 0.00001130
Iteration 117/1000 | Loss: 0.00001130
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001129
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001129
Iteration 159/1000 | Loss: 0.00001129
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Iteration 163/1000 | Loss: 0.00001129
Iteration 164/1000 | Loss: 0.00001129
Iteration 165/1000 | Loss: 0.00001129
Iteration 166/1000 | Loss: 0.00001129
Iteration 167/1000 | Loss: 0.00001129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1285262189630885e-05, 1.1285262189630885e-05, 1.1285262189630885e-05, 1.1285262189630885e-05, 1.1285262189630885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1285262189630885e-05

Optimization complete. Final v2v error: 2.862630605697632 mm

Highest mean error: 3.051179885864258 mm for frame 52

Lowest mean error: 2.6940271854400635 mm for frame 120

Saving results

Total time: 36.11460566520691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860823
Iteration 2/25 | Loss: 0.00079185
Iteration 3/25 | Loss: 0.00063705
Iteration 4/25 | Loss: 0.00061389
Iteration 5/25 | Loss: 0.00060523
Iteration 6/25 | Loss: 0.00060388
Iteration 7/25 | Loss: 0.00060349
Iteration 8/25 | Loss: 0.00060349
Iteration 9/25 | Loss: 0.00060349
Iteration 10/25 | Loss: 0.00060349
Iteration 11/25 | Loss: 0.00060349
Iteration 12/25 | Loss: 0.00060349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006034897523932159, 0.0006034897523932159, 0.0006034897523932159, 0.0006034897523932159, 0.0006034897523932159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006034897523932159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67404723
Iteration 2/25 | Loss: 0.00029205
Iteration 3/25 | Loss: 0.00029204
Iteration 4/25 | Loss: 0.00029204
Iteration 5/25 | Loss: 0.00029204
Iteration 6/25 | Loss: 0.00029204
Iteration 7/25 | Loss: 0.00029204
Iteration 8/25 | Loss: 0.00029204
Iteration 9/25 | Loss: 0.00029204
Iteration 10/25 | Loss: 0.00029204
Iteration 11/25 | Loss: 0.00029204
Iteration 12/25 | Loss: 0.00029204
Iteration 13/25 | Loss: 0.00029204
Iteration 14/25 | Loss: 0.00029204
Iteration 15/25 | Loss: 0.00029204
Iteration 16/25 | Loss: 0.00029204
Iteration 17/25 | Loss: 0.00029204
Iteration 18/25 | Loss: 0.00029204
Iteration 19/25 | Loss: 0.00029204
Iteration 20/25 | Loss: 0.00029204
Iteration 21/25 | Loss: 0.00029204
Iteration 22/25 | Loss: 0.00029204
Iteration 23/25 | Loss: 0.00029204
Iteration 24/25 | Loss: 0.00029204
Iteration 25/25 | Loss: 0.00029204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029204
Iteration 2/1000 | Loss: 0.00002135
Iteration 3/1000 | Loss: 0.00001529
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001284
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001169
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001168
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001163
Iteration 26/1000 | Loss: 0.00001163
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001156
Iteration 33/1000 | Loss: 0.00001156
Iteration 34/1000 | Loss: 0.00001156
Iteration 35/1000 | Loss: 0.00001155
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001155
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001150
Iteration 48/1000 | Loss: 0.00001150
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001147
Iteration 51/1000 | Loss: 0.00001147
Iteration 52/1000 | Loss: 0.00001147
Iteration 53/1000 | Loss: 0.00001147
Iteration 54/1000 | Loss: 0.00001147
Iteration 55/1000 | Loss: 0.00001147
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001143
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001142
Iteration 77/1000 | Loss: 0.00001142
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001140
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001139
Iteration 89/1000 | Loss: 0.00001139
Iteration 90/1000 | Loss: 0.00001139
Iteration 91/1000 | Loss: 0.00001139
Iteration 92/1000 | Loss: 0.00001139
Iteration 93/1000 | Loss: 0.00001138
Iteration 94/1000 | Loss: 0.00001138
Iteration 95/1000 | Loss: 0.00001138
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001136
Iteration 101/1000 | Loss: 0.00001136
Iteration 102/1000 | Loss: 0.00001136
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001135
Iteration 107/1000 | Loss: 0.00001135
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1282711056992412e-05, 1.1282711056992412e-05, 1.1282711056992412e-05, 1.1282711056992412e-05, 1.1282711056992412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1282711056992412e-05

Optimization complete. Final v2v error: 2.8624091148376465 mm

Highest mean error: 3.0464437007904053 mm for frame 61

Lowest mean error: 2.7090766429901123 mm for frame 101

Saving results

Total time: 34.61039447784424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990606
Iteration 2/25 | Loss: 0.00181286
Iteration 3/25 | Loss: 0.00097244
Iteration 4/25 | Loss: 0.00093431
Iteration 5/25 | Loss: 0.00092485
Iteration 6/25 | Loss: 0.00092237
Iteration 7/25 | Loss: 0.00092193
Iteration 8/25 | Loss: 0.00092193
Iteration 9/25 | Loss: 0.00092193
Iteration 10/25 | Loss: 0.00092193
Iteration 11/25 | Loss: 0.00092193
Iteration 12/25 | Loss: 0.00092193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009219310595653951, 0.0009219310595653951, 0.0009219310595653951, 0.0009219310595653951, 0.0009219310595653951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009219310595653951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66767550
Iteration 2/25 | Loss: 0.00016561
Iteration 3/25 | Loss: 0.00016561
Iteration 4/25 | Loss: 0.00016561
Iteration 5/25 | Loss: 0.00016561
Iteration 6/25 | Loss: 0.00016561
Iteration 7/25 | Loss: 0.00016561
Iteration 8/25 | Loss: 0.00016561
Iteration 9/25 | Loss: 0.00016561
Iteration 10/25 | Loss: 0.00016561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0001656055828789249, 0.0001656055828789249, 0.0001656055828789249, 0.0001656055828789249, 0.0001656055828789249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001656055828789249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016561
Iteration 2/1000 | Loss: 0.00005409
Iteration 3/1000 | Loss: 0.00004132
Iteration 4/1000 | Loss: 0.00003600
Iteration 5/1000 | Loss: 0.00003413
Iteration 6/1000 | Loss: 0.00003305
Iteration 7/1000 | Loss: 0.00003236
Iteration 8/1000 | Loss: 0.00003147
Iteration 9/1000 | Loss: 0.00003087
Iteration 10/1000 | Loss: 0.00003062
Iteration 11/1000 | Loss: 0.00003039
Iteration 12/1000 | Loss: 0.00003017
Iteration 13/1000 | Loss: 0.00003001
Iteration 14/1000 | Loss: 0.00002990
Iteration 15/1000 | Loss: 0.00002989
Iteration 16/1000 | Loss: 0.00002975
Iteration 17/1000 | Loss: 0.00002969
Iteration 18/1000 | Loss: 0.00002963
Iteration 19/1000 | Loss: 0.00002957
Iteration 20/1000 | Loss: 0.00002953
Iteration 21/1000 | Loss: 0.00002952
Iteration 22/1000 | Loss: 0.00002949
Iteration 23/1000 | Loss: 0.00002944
Iteration 24/1000 | Loss: 0.00002942
Iteration 25/1000 | Loss: 0.00002941
Iteration 26/1000 | Loss: 0.00002931
Iteration 27/1000 | Loss: 0.00002930
Iteration 28/1000 | Loss: 0.00002929
Iteration 29/1000 | Loss: 0.00002927
Iteration 30/1000 | Loss: 0.00002925
Iteration 31/1000 | Loss: 0.00002925
Iteration 32/1000 | Loss: 0.00002925
Iteration 33/1000 | Loss: 0.00002924
Iteration 34/1000 | Loss: 0.00002923
Iteration 35/1000 | Loss: 0.00002922
Iteration 36/1000 | Loss: 0.00002922
Iteration 37/1000 | Loss: 0.00002920
Iteration 38/1000 | Loss: 0.00002920
Iteration 39/1000 | Loss: 0.00002920
Iteration 40/1000 | Loss: 0.00002920
Iteration 41/1000 | Loss: 0.00002920
Iteration 42/1000 | Loss: 0.00002920
Iteration 43/1000 | Loss: 0.00002920
Iteration 44/1000 | Loss: 0.00002919
Iteration 45/1000 | Loss: 0.00002919
Iteration 46/1000 | Loss: 0.00002919
Iteration 47/1000 | Loss: 0.00002919
Iteration 48/1000 | Loss: 0.00002919
Iteration 49/1000 | Loss: 0.00002919
Iteration 50/1000 | Loss: 0.00002919
Iteration 51/1000 | Loss: 0.00002919
Iteration 52/1000 | Loss: 0.00002918
Iteration 53/1000 | Loss: 0.00002918
Iteration 54/1000 | Loss: 0.00002918
Iteration 55/1000 | Loss: 0.00002917
Iteration 56/1000 | Loss: 0.00002917
Iteration 57/1000 | Loss: 0.00002917
Iteration 58/1000 | Loss: 0.00002916
Iteration 59/1000 | Loss: 0.00002916
Iteration 60/1000 | Loss: 0.00002915
Iteration 61/1000 | Loss: 0.00002913
Iteration 62/1000 | Loss: 0.00002913
Iteration 63/1000 | Loss: 0.00002913
Iteration 64/1000 | Loss: 0.00002913
Iteration 65/1000 | Loss: 0.00002913
Iteration 66/1000 | Loss: 0.00002913
Iteration 67/1000 | Loss: 0.00002913
Iteration 68/1000 | Loss: 0.00002913
Iteration 69/1000 | Loss: 0.00002912
Iteration 70/1000 | Loss: 0.00002912
Iteration 71/1000 | Loss: 0.00002912
Iteration 72/1000 | Loss: 0.00002912
Iteration 73/1000 | Loss: 0.00002912
Iteration 74/1000 | Loss: 0.00002912
Iteration 75/1000 | Loss: 0.00002912
Iteration 76/1000 | Loss: 0.00002912
Iteration 77/1000 | Loss: 0.00002912
Iteration 78/1000 | Loss: 0.00002912
Iteration 79/1000 | Loss: 0.00002911
Iteration 80/1000 | Loss: 0.00002911
Iteration 81/1000 | Loss: 0.00002911
Iteration 82/1000 | Loss: 0.00002911
Iteration 83/1000 | Loss: 0.00002911
Iteration 84/1000 | Loss: 0.00002911
Iteration 85/1000 | Loss: 0.00002911
Iteration 86/1000 | Loss: 0.00002910
Iteration 87/1000 | Loss: 0.00002910
Iteration 88/1000 | Loss: 0.00002910
Iteration 89/1000 | Loss: 0.00002910
Iteration 90/1000 | Loss: 0.00002910
Iteration 91/1000 | Loss: 0.00002910
Iteration 92/1000 | Loss: 0.00002910
Iteration 93/1000 | Loss: 0.00002909
Iteration 94/1000 | Loss: 0.00002909
Iteration 95/1000 | Loss: 0.00002909
Iteration 96/1000 | Loss: 0.00002909
Iteration 97/1000 | Loss: 0.00002909
Iteration 98/1000 | Loss: 0.00002909
Iteration 99/1000 | Loss: 0.00002908
Iteration 100/1000 | Loss: 0.00002908
Iteration 101/1000 | Loss: 0.00002908
Iteration 102/1000 | Loss: 0.00002908
Iteration 103/1000 | Loss: 0.00002908
Iteration 104/1000 | Loss: 0.00002907
Iteration 105/1000 | Loss: 0.00002907
Iteration 106/1000 | Loss: 0.00002907
Iteration 107/1000 | Loss: 0.00002906
Iteration 108/1000 | Loss: 0.00002906
Iteration 109/1000 | Loss: 0.00002906
Iteration 110/1000 | Loss: 0.00002906
Iteration 111/1000 | Loss: 0.00002905
Iteration 112/1000 | Loss: 0.00002905
Iteration 113/1000 | Loss: 0.00002905
Iteration 114/1000 | Loss: 0.00002905
Iteration 115/1000 | Loss: 0.00002905
Iteration 116/1000 | Loss: 0.00002905
Iteration 117/1000 | Loss: 0.00002905
Iteration 118/1000 | Loss: 0.00002905
Iteration 119/1000 | Loss: 0.00002905
Iteration 120/1000 | Loss: 0.00002905
Iteration 121/1000 | Loss: 0.00002905
Iteration 122/1000 | Loss: 0.00002905
Iteration 123/1000 | Loss: 0.00002904
Iteration 124/1000 | Loss: 0.00002904
Iteration 125/1000 | Loss: 0.00002904
Iteration 126/1000 | Loss: 0.00002904
Iteration 127/1000 | Loss: 0.00002904
Iteration 128/1000 | Loss: 0.00002904
Iteration 129/1000 | Loss: 0.00002904
Iteration 130/1000 | Loss: 0.00002904
Iteration 131/1000 | Loss: 0.00002904
Iteration 132/1000 | Loss: 0.00002904
Iteration 133/1000 | Loss: 0.00002904
Iteration 134/1000 | Loss: 0.00002903
Iteration 135/1000 | Loss: 0.00002903
Iteration 136/1000 | Loss: 0.00002903
Iteration 137/1000 | Loss: 0.00002903
Iteration 138/1000 | Loss: 0.00002903
Iteration 139/1000 | Loss: 0.00002903
Iteration 140/1000 | Loss: 0.00002903
Iteration 141/1000 | Loss: 0.00002903
Iteration 142/1000 | Loss: 0.00002902
Iteration 143/1000 | Loss: 0.00002902
Iteration 144/1000 | Loss: 0.00002902
Iteration 145/1000 | Loss: 0.00002902
Iteration 146/1000 | Loss: 0.00002902
Iteration 147/1000 | Loss: 0.00002901
Iteration 148/1000 | Loss: 0.00002900
Iteration 149/1000 | Loss: 0.00002900
Iteration 150/1000 | Loss: 0.00002900
Iteration 151/1000 | Loss: 0.00002900
Iteration 152/1000 | Loss: 0.00002900
Iteration 153/1000 | Loss: 0.00002900
Iteration 154/1000 | Loss: 0.00002900
Iteration 155/1000 | Loss: 0.00002900
Iteration 156/1000 | Loss: 0.00002900
Iteration 157/1000 | Loss: 0.00002900
Iteration 158/1000 | Loss: 0.00002900
Iteration 159/1000 | Loss: 0.00002899
Iteration 160/1000 | Loss: 0.00002899
Iteration 161/1000 | Loss: 0.00002899
Iteration 162/1000 | Loss: 0.00002899
Iteration 163/1000 | Loss: 0.00002899
Iteration 164/1000 | Loss: 0.00002899
Iteration 165/1000 | Loss: 0.00002899
Iteration 166/1000 | Loss: 0.00002899
Iteration 167/1000 | Loss: 0.00002899
Iteration 168/1000 | Loss: 0.00002899
Iteration 169/1000 | Loss: 0.00002899
Iteration 170/1000 | Loss: 0.00002899
Iteration 171/1000 | Loss: 0.00002899
Iteration 172/1000 | Loss: 0.00002899
Iteration 173/1000 | Loss: 0.00002899
Iteration 174/1000 | Loss: 0.00002899
Iteration 175/1000 | Loss: 0.00002898
Iteration 176/1000 | Loss: 0.00002898
Iteration 177/1000 | Loss: 0.00002898
Iteration 178/1000 | Loss: 0.00002898
Iteration 179/1000 | Loss: 0.00002898
Iteration 180/1000 | Loss: 0.00002898
Iteration 181/1000 | Loss: 0.00002898
Iteration 182/1000 | Loss: 0.00002898
Iteration 183/1000 | Loss: 0.00002898
Iteration 184/1000 | Loss: 0.00002898
Iteration 185/1000 | Loss: 0.00002898
Iteration 186/1000 | Loss: 0.00002898
Iteration 187/1000 | Loss: 0.00002898
Iteration 188/1000 | Loss: 0.00002898
Iteration 189/1000 | Loss: 0.00002898
Iteration 190/1000 | Loss: 0.00002898
Iteration 191/1000 | Loss: 0.00002898
Iteration 192/1000 | Loss: 0.00002898
Iteration 193/1000 | Loss: 0.00002897
Iteration 194/1000 | Loss: 0.00002897
Iteration 195/1000 | Loss: 0.00002897
Iteration 196/1000 | Loss: 0.00002897
Iteration 197/1000 | Loss: 0.00002897
Iteration 198/1000 | Loss: 0.00002897
Iteration 199/1000 | Loss: 0.00002897
Iteration 200/1000 | Loss: 0.00002897
Iteration 201/1000 | Loss: 0.00002897
Iteration 202/1000 | Loss: 0.00002897
Iteration 203/1000 | Loss: 0.00002897
Iteration 204/1000 | Loss: 0.00002896
Iteration 205/1000 | Loss: 0.00002896
Iteration 206/1000 | Loss: 0.00002896
Iteration 207/1000 | Loss: 0.00002896
Iteration 208/1000 | Loss: 0.00002896
Iteration 209/1000 | Loss: 0.00002895
Iteration 210/1000 | Loss: 0.00002895
Iteration 211/1000 | Loss: 0.00002895
Iteration 212/1000 | Loss: 0.00002895
Iteration 213/1000 | Loss: 0.00002895
Iteration 214/1000 | Loss: 0.00002895
Iteration 215/1000 | Loss: 0.00002895
Iteration 216/1000 | Loss: 0.00002895
Iteration 217/1000 | Loss: 0.00002895
Iteration 218/1000 | Loss: 0.00002895
Iteration 219/1000 | Loss: 0.00002895
Iteration 220/1000 | Loss: 0.00002895
Iteration 221/1000 | Loss: 0.00002894
Iteration 222/1000 | Loss: 0.00002894
Iteration 223/1000 | Loss: 0.00002894
Iteration 224/1000 | Loss: 0.00002894
Iteration 225/1000 | Loss: 0.00002894
Iteration 226/1000 | Loss: 0.00002894
Iteration 227/1000 | Loss: 0.00002894
Iteration 228/1000 | Loss: 0.00002894
Iteration 229/1000 | Loss: 0.00002894
Iteration 230/1000 | Loss: 0.00002894
Iteration 231/1000 | Loss: 0.00002894
Iteration 232/1000 | Loss: 0.00002894
Iteration 233/1000 | Loss: 0.00002894
Iteration 234/1000 | Loss: 0.00002894
Iteration 235/1000 | Loss: 0.00002894
Iteration 236/1000 | Loss: 0.00002893
Iteration 237/1000 | Loss: 0.00002893
Iteration 238/1000 | Loss: 0.00002893
Iteration 239/1000 | Loss: 0.00002893
Iteration 240/1000 | Loss: 0.00002893
Iteration 241/1000 | Loss: 0.00002893
Iteration 242/1000 | Loss: 0.00002893
Iteration 243/1000 | Loss: 0.00002893
Iteration 244/1000 | Loss: 0.00002893
Iteration 245/1000 | Loss: 0.00002893
Iteration 246/1000 | Loss: 0.00002893
Iteration 247/1000 | Loss: 0.00002893
Iteration 248/1000 | Loss: 0.00002893
Iteration 249/1000 | Loss: 0.00002893
Iteration 250/1000 | Loss: 0.00002893
Iteration 251/1000 | Loss: 0.00002893
Iteration 252/1000 | Loss: 0.00002893
Iteration 253/1000 | Loss: 0.00002893
Iteration 254/1000 | Loss: 0.00002893
Iteration 255/1000 | Loss: 0.00002893
Iteration 256/1000 | Loss: 0.00002893
Iteration 257/1000 | Loss: 0.00002893
Iteration 258/1000 | Loss: 0.00002893
Iteration 259/1000 | Loss: 0.00002893
Iteration 260/1000 | Loss: 0.00002893
Iteration 261/1000 | Loss: 0.00002893
Iteration 262/1000 | Loss: 0.00002893
Iteration 263/1000 | Loss: 0.00002893
Iteration 264/1000 | Loss: 0.00002893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.8930502594448626e-05, 2.8930502594448626e-05, 2.8930502594448626e-05, 2.8930502594448626e-05, 2.8930502594448626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8930502594448626e-05

Optimization complete. Final v2v error: 4.413388729095459 mm

Highest mean error: 5.3290791511535645 mm for frame 101

Lowest mean error: 3.956965684890747 mm for frame 112

Saving results

Total time: 53.24899196624756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501611
Iteration 2/25 | Loss: 0.00081305
Iteration 3/25 | Loss: 0.00067859
Iteration 4/25 | Loss: 0.00064994
Iteration 5/25 | Loss: 0.00064316
Iteration 6/25 | Loss: 0.00064187
Iteration 7/25 | Loss: 0.00064167
Iteration 8/25 | Loss: 0.00064167
Iteration 9/25 | Loss: 0.00064167
Iteration 10/25 | Loss: 0.00064167
Iteration 11/25 | Loss: 0.00064167
Iteration 12/25 | Loss: 0.00064167
Iteration 13/25 | Loss: 0.00064167
Iteration 14/25 | Loss: 0.00064167
Iteration 15/25 | Loss: 0.00064167
Iteration 16/25 | Loss: 0.00064167
Iteration 17/25 | Loss: 0.00064167
Iteration 18/25 | Loss: 0.00064167
Iteration 19/25 | Loss: 0.00064167
Iteration 20/25 | Loss: 0.00064167
Iteration 21/25 | Loss: 0.00064167
Iteration 22/25 | Loss: 0.00064167
Iteration 23/25 | Loss: 0.00064167
Iteration 24/25 | Loss: 0.00064167
Iteration 25/25 | Loss: 0.00064167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62424612
Iteration 2/25 | Loss: 0.00030826
Iteration 3/25 | Loss: 0.00030821
Iteration 4/25 | Loss: 0.00030821
Iteration 5/25 | Loss: 0.00030821
Iteration 6/25 | Loss: 0.00030821
Iteration 7/25 | Loss: 0.00030821
Iteration 8/25 | Loss: 0.00030821
Iteration 9/25 | Loss: 0.00030821
Iteration 10/25 | Loss: 0.00030821
Iteration 11/25 | Loss: 0.00030821
Iteration 12/25 | Loss: 0.00030821
Iteration 13/25 | Loss: 0.00030821
Iteration 14/25 | Loss: 0.00030821
Iteration 15/25 | Loss: 0.00030821
Iteration 16/25 | Loss: 0.00030821
Iteration 17/25 | Loss: 0.00030821
Iteration 18/25 | Loss: 0.00030821
Iteration 19/25 | Loss: 0.00030821
Iteration 20/25 | Loss: 0.00030821
Iteration 21/25 | Loss: 0.00030821
Iteration 22/25 | Loss: 0.00030821
Iteration 23/25 | Loss: 0.00030821
Iteration 24/25 | Loss: 0.00030821
Iteration 25/25 | Loss: 0.00030821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030821
Iteration 2/1000 | Loss: 0.00002934
Iteration 3/1000 | Loss: 0.00002018
Iteration 4/1000 | Loss: 0.00001768
Iteration 5/1000 | Loss: 0.00001671
Iteration 6/1000 | Loss: 0.00001598
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001483
Iteration 11/1000 | Loss: 0.00001480
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001427
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001423
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001422
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001422
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001421
Iteration 40/1000 | Loss: 0.00001421
Iteration 41/1000 | Loss: 0.00001420
Iteration 42/1000 | Loss: 0.00001420
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001419
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001418
Iteration 50/1000 | Loss: 0.00001418
Iteration 51/1000 | Loss: 0.00001418
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001418
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001407
Iteration 81/1000 | Loss: 0.00001407
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001401
Iteration 125/1000 | Loss: 0.00001401
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001401
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001401
Iteration 136/1000 | Loss: 0.00001401
Iteration 137/1000 | Loss: 0.00001401
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001401
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Iteration 142/1000 | Loss: 0.00001400
Iteration 143/1000 | Loss: 0.00001400
Iteration 144/1000 | Loss: 0.00001400
Iteration 145/1000 | Loss: 0.00001400
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001399
Iteration 151/1000 | Loss: 0.00001399
Iteration 152/1000 | Loss: 0.00001399
Iteration 153/1000 | Loss: 0.00001399
Iteration 154/1000 | Loss: 0.00001399
Iteration 155/1000 | Loss: 0.00001399
Iteration 156/1000 | Loss: 0.00001399
Iteration 157/1000 | Loss: 0.00001399
Iteration 158/1000 | Loss: 0.00001399
Iteration 159/1000 | Loss: 0.00001399
Iteration 160/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.399254506395664e-05, 1.399254506395664e-05, 1.399254506395664e-05, 1.399254506395664e-05, 1.399254506395664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.399254506395664e-05

Optimization complete. Final v2v error: 3.1521260738372803 mm

Highest mean error: 3.5816800594329834 mm for frame 120

Lowest mean error: 2.804966926574707 mm for frame 30

Saving results

Total time: 39.57955455780029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747814
Iteration 2/25 | Loss: 0.00139846
Iteration 3/25 | Loss: 0.00095940
Iteration 4/25 | Loss: 0.00082397
Iteration 5/25 | Loss: 0.00075719
Iteration 6/25 | Loss: 0.00073473
Iteration 7/25 | Loss: 0.00073520
Iteration 8/25 | Loss: 0.00071901
Iteration 9/25 | Loss: 0.00071522
Iteration 10/25 | Loss: 0.00071235
Iteration 11/25 | Loss: 0.00071183
Iteration 12/25 | Loss: 0.00071137
Iteration 13/25 | Loss: 0.00071105
Iteration 14/25 | Loss: 0.00070888
Iteration 15/25 | Loss: 0.00070755
Iteration 16/25 | Loss: 0.00070718
Iteration 17/25 | Loss: 0.00070697
Iteration 18/25 | Loss: 0.00070692
Iteration 19/25 | Loss: 0.00070692
Iteration 20/25 | Loss: 0.00070692
Iteration 21/25 | Loss: 0.00070692
Iteration 22/25 | Loss: 0.00070692
Iteration 23/25 | Loss: 0.00070691
Iteration 24/25 | Loss: 0.00070691
Iteration 25/25 | Loss: 0.00070691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90121901
Iteration 2/25 | Loss: 0.00030315
Iteration 3/25 | Loss: 0.00030315
Iteration 4/25 | Loss: 0.00030315
Iteration 5/25 | Loss: 0.00030314
Iteration 6/25 | Loss: 0.00030314
Iteration 7/25 | Loss: 0.00030314
Iteration 8/25 | Loss: 0.00030314
Iteration 9/25 | Loss: 0.00030314
Iteration 10/25 | Loss: 0.00030314
Iteration 11/25 | Loss: 0.00030314
Iteration 12/25 | Loss: 0.00030314
Iteration 13/25 | Loss: 0.00030314
Iteration 14/25 | Loss: 0.00030314
Iteration 15/25 | Loss: 0.00030314
Iteration 16/25 | Loss: 0.00030314
Iteration 17/25 | Loss: 0.00030314
Iteration 18/25 | Loss: 0.00030314
Iteration 19/25 | Loss: 0.00030314
Iteration 20/25 | Loss: 0.00030314
Iteration 21/25 | Loss: 0.00030314
Iteration 22/25 | Loss: 0.00030314
Iteration 23/25 | Loss: 0.00030314
Iteration 24/25 | Loss: 0.00030314
Iteration 25/25 | Loss: 0.00030314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030314
Iteration 2/1000 | Loss: 0.00004490
Iteration 3/1000 | Loss: 0.00003229
Iteration 4/1000 | Loss: 0.00002879
Iteration 5/1000 | Loss: 0.00002749
Iteration 6/1000 | Loss: 0.00002632
Iteration 7/1000 | Loss: 0.00002559
Iteration 8/1000 | Loss: 0.00002496
Iteration 9/1000 | Loss: 0.00002461
Iteration 10/1000 | Loss: 0.00002442
Iteration 11/1000 | Loss: 0.00002421
Iteration 12/1000 | Loss: 0.00002420
Iteration 13/1000 | Loss: 0.00002419
Iteration 14/1000 | Loss: 0.00002406
Iteration 15/1000 | Loss: 0.00002405
Iteration 16/1000 | Loss: 0.00002405
Iteration 17/1000 | Loss: 0.00002405
Iteration 18/1000 | Loss: 0.00002403
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002395
Iteration 21/1000 | Loss: 0.00002395
Iteration 22/1000 | Loss: 0.00002393
Iteration 23/1000 | Loss: 0.00002392
Iteration 24/1000 | Loss: 0.00002392
Iteration 25/1000 | Loss: 0.00002391
Iteration 26/1000 | Loss: 0.00002391
Iteration 27/1000 | Loss: 0.00002390
Iteration 28/1000 | Loss: 0.00002390
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002387
Iteration 34/1000 | Loss: 0.00002387
Iteration 35/1000 | Loss: 0.00002387
Iteration 36/1000 | Loss: 0.00002386
Iteration 37/1000 | Loss: 0.00002386
Iteration 38/1000 | Loss: 0.00002385
Iteration 39/1000 | Loss: 0.00002385
Iteration 40/1000 | Loss: 0.00002384
Iteration 41/1000 | Loss: 0.00002383
Iteration 42/1000 | Loss: 0.00002383
Iteration 43/1000 | Loss: 0.00002383
Iteration 44/1000 | Loss: 0.00002382
Iteration 45/1000 | Loss: 0.00002381
Iteration 46/1000 | Loss: 0.00002377
Iteration 47/1000 | Loss: 0.00002376
Iteration 48/1000 | Loss: 0.00002375
Iteration 49/1000 | Loss: 0.00002373
Iteration 50/1000 | Loss: 0.00002373
Iteration 51/1000 | Loss: 0.00002373
Iteration 52/1000 | Loss: 0.00002373
Iteration 53/1000 | Loss: 0.00002373
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002373
Iteration 56/1000 | Loss: 0.00002373
Iteration 57/1000 | Loss: 0.00002372
Iteration 58/1000 | Loss: 0.00002372
Iteration 59/1000 | Loss: 0.00002372
Iteration 60/1000 | Loss: 0.00002372
Iteration 61/1000 | Loss: 0.00002371
Iteration 62/1000 | Loss: 0.00002371
Iteration 63/1000 | Loss: 0.00002370
Iteration 64/1000 | Loss: 0.00002370
Iteration 65/1000 | Loss: 0.00002369
Iteration 66/1000 | Loss: 0.00002369
Iteration 67/1000 | Loss: 0.00002369
Iteration 68/1000 | Loss: 0.00002369
Iteration 69/1000 | Loss: 0.00002369
Iteration 70/1000 | Loss: 0.00002368
Iteration 71/1000 | Loss: 0.00002368
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002368
Iteration 74/1000 | Loss: 0.00002368
Iteration 75/1000 | Loss: 0.00002368
Iteration 76/1000 | Loss: 0.00002368
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002367
Iteration 79/1000 | Loss: 0.00002367
Iteration 80/1000 | Loss: 0.00002367
Iteration 81/1000 | Loss: 0.00002367
Iteration 82/1000 | Loss: 0.00002366
Iteration 83/1000 | Loss: 0.00002366
Iteration 84/1000 | Loss: 0.00002366
Iteration 85/1000 | Loss: 0.00002366
Iteration 86/1000 | Loss: 0.00002366
Iteration 87/1000 | Loss: 0.00002366
Iteration 88/1000 | Loss: 0.00002366
Iteration 89/1000 | Loss: 0.00002366
Iteration 90/1000 | Loss: 0.00002366
Iteration 91/1000 | Loss: 0.00002365
Iteration 92/1000 | Loss: 0.00002365
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002365
Iteration 96/1000 | Loss: 0.00002365
Iteration 97/1000 | Loss: 0.00002365
Iteration 98/1000 | Loss: 0.00002365
Iteration 99/1000 | Loss: 0.00002365
Iteration 100/1000 | Loss: 0.00002364
Iteration 101/1000 | Loss: 0.00002364
Iteration 102/1000 | Loss: 0.00002364
Iteration 103/1000 | Loss: 0.00002364
Iteration 104/1000 | Loss: 0.00002364
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002364
Iteration 108/1000 | Loss: 0.00002364
Iteration 109/1000 | Loss: 0.00002363
Iteration 110/1000 | Loss: 0.00002363
Iteration 111/1000 | Loss: 0.00002363
Iteration 112/1000 | Loss: 0.00002363
Iteration 113/1000 | Loss: 0.00002363
Iteration 114/1000 | Loss: 0.00002363
Iteration 115/1000 | Loss: 0.00002363
Iteration 116/1000 | Loss: 0.00002363
Iteration 117/1000 | Loss: 0.00002362
Iteration 118/1000 | Loss: 0.00002362
Iteration 119/1000 | Loss: 0.00002362
Iteration 120/1000 | Loss: 0.00002362
Iteration 121/1000 | Loss: 0.00002361
Iteration 122/1000 | Loss: 0.00002361
Iteration 123/1000 | Loss: 0.00002361
Iteration 124/1000 | Loss: 0.00002361
Iteration 125/1000 | Loss: 0.00002360
Iteration 126/1000 | Loss: 0.00002360
Iteration 127/1000 | Loss: 0.00002360
Iteration 128/1000 | Loss: 0.00002360
Iteration 129/1000 | Loss: 0.00002360
Iteration 130/1000 | Loss: 0.00002360
Iteration 131/1000 | Loss: 0.00002359
Iteration 132/1000 | Loss: 0.00002359
Iteration 133/1000 | Loss: 0.00002359
Iteration 134/1000 | Loss: 0.00002359
Iteration 135/1000 | Loss: 0.00002359
Iteration 136/1000 | Loss: 0.00002359
Iteration 137/1000 | Loss: 0.00002359
Iteration 138/1000 | Loss: 0.00002359
Iteration 139/1000 | Loss: 0.00002359
Iteration 140/1000 | Loss: 0.00002359
Iteration 141/1000 | Loss: 0.00002359
Iteration 142/1000 | Loss: 0.00002359
Iteration 143/1000 | Loss: 0.00002359
Iteration 144/1000 | Loss: 0.00002359
Iteration 145/1000 | Loss: 0.00002358
Iteration 146/1000 | Loss: 0.00002358
Iteration 147/1000 | Loss: 0.00002358
Iteration 148/1000 | Loss: 0.00002358
Iteration 149/1000 | Loss: 0.00002358
Iteration 150/1000 | Loss: 0.00002358
Iteration 151/1000 | Loss: 0.00002358
Iteration 152/1000 | Loss: 0.00002358
Iteration 153/1000 | Loss: 0.00002358
Iteration 154/1000 | Loss: 0.00002358
Iteration 155/1000 | Loss: 0.00002358
Iteration 156/1000 | Loss: 0.00002358
Iteration 157/1000 | Loss: 0.00002358
Iteration 158/1000 | Loss: 0.00002358
Iteration 159/1000 | Loss: 0.00002358
Iteration 160/1000 | Loss: 0.00002358
Iteration 161/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.357643643335905e-05, 2.357643643335905e-05, 2.357643643335905e-05, 2.357643643335905e-05, 2.357643643335905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.357643643335905e-05

Optimization complete. Final v2v error: 4.057427406311035 mm

Highest mean error: 4.421699523925781 mm for frame 102

Lowest mean error: 3.6757938861846924 mm for frame 53

Saving results

Total time: 59.63899755477905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494553
Iteration 2/25 | Loss: 0.00093756
Iteration 3/25 | Loss: 0.00073821
Iteration 4/25 | Loss: 0.00070727
Iteration 5/25 | Loss: 0.00069523
Iteration 6/25 | Loss: 0.00069280
Iteration 7/25 | Loss: 0.00069256
Iteration 8/25 | Loss: 0.00069256
Iteration 9/25 | Loss: 0.00069256
Iteration 10/25 | Loss: 0.00069256
Iteration 11/25 | Loss: 0.00069256
Iteration 12/25 | Loss: 0.00069256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006925596971996129, 0.0006925596971996129, 0.0006925596971996129, 0.0006925596971996129, 0.0006925596971996129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006925596971996129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47411299
Iteration 2/25 | Loss: 0.00036329
Iteration 3/25 | Loss: 0.00036326
Iteration 4/25 | Loss: 0.00036326
Iteration 5/25 | Loss: 0.00036326
Iteration 6/25 | Loss: 0.00036326
Iteration 7/25 | Loss: 0.00036326
Iteration 8/25 | Loss: 0.00036326
Iteration 9/25 | Loss: 0.00036326
Iteration 10/25 | Loss: 0.00036326
Iteration 11/25 | Loss: 0.00036326
Iteration 12/25 | Loss: 0.00036326
Iteration 13/25 | Loss: 0.00036326
Iteration 14/25 | Loss: 0.00036326
Iteration 15/25 | Loss: 0.00036326
Iteration 16/25 | Loss: 0.00036326
Iteration 17/25 | Loss: 0.00036326
Iteration 18/25 | Loss: 0.00036326
Iteration 19/25 | Loss: 0.00036326
Iteration 20/25 | Loss: 0.00036326
Iteration 21/25 | Loss: 0.00036326
Iteration 22/25 | Loss: 0.00036326
Iteration 23/25 | Loss: 0.00036326
Iteration 24/25 | Loss: 0.00036326
Iteration 25/25 | Loss: 0.00036326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003632618463598192, 0.0003632618463598192, 0.0003632618463598192, 0.0003632618463598192, 0.0003632618463598192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003632618463598192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036326
Iteration 2/1000 | Loss: 0.00002887
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001970
Iteration 5/1000 | Loss: 0.00001875
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001791
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001762
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001729
Iteration 12/1000 | Loss: 0.00001728
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001720
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001702
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001699
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001699
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001691
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001686
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001686
Iteration 43/1000 | Loss: 0.00001685
Iteration 44/1000 | Loss: 0.00001685
Iteration 45/1000 | Loss: 0.00001683
Iteration 46/1000 | Loss: 0.00001683
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00001682
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001682
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001681
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001681
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001680
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001678
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001677
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001676
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001675
Iteration 87/1000 | Loss: 0.00001675
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001673
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001672
Iteration 115/1000 | Loss: 0.00001672
Iteration 116/1000 | Loss: 0.00001672
Iteration 117/1000 | Loss: 0.00001672
Iteration 118/1000 | Loss: 0.00001672
Iteration 119/1000 | Loss: 0.00001672
Iteration 120/1000 | Loss: 0.00001671
Iteration 121/1000 | Loss: 0.00001671
Iteration 122/1000 | Loss: 0.00001671
Iteration 123/1000 | Loss: 0.00001671
Iteration 124/1000 | Loss: 0.00001671
Iteration 125/1000 | Loss: 0.00001671
Iteration 126/1000 | Loss: 0.00001671
Iteration 127/1000 | Loss: 0.00001671
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001670
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001670
Iteration 140/1000 | Loss: 0.00001670
Iteration 141/1000 | Loss: 0.00001670
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001669
Iteration 144/1000 | Loss: 0.00001669
Iteration 145/1000 | Loss: 0.00001669
Iteration 146/1000 | Loss: 0.00001669
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001668
Iteration 152/1000 | Loss: 0.00001668
Iteration 153/1000 | Loss: 0.00001668
Iteration 154/1000 | Loss: 0.00001668
Iteration 155/1000 | Loss: 0.00001668
Iteration 156/1000 | Loss: 0.00001668
Iteration 157/1000 | Loss: 0.00001668
Iteration 158/1000 | Loss: 0.00001668
Iteration 159/1000 | Loss: 0.00001668
Iteration 160/1000 | Loss: 0.00001667
Iteration 161/1000 | Loss: 0.00001667
Iteration 162/1000 | Loss: 0.00001667
Iteration 163/1000 | Loss: 0.00001667
Iteration 164/1000 | Loss: 0.00001667
Iteration 165/1000 | Loss: 0.00001667
Iteration 166/1000 | Loss: 0.00001667
Iteration 167/1000 | Loss: 0.00001667
Iteration 168/1000 | Loss: 0.00001667
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001666
Iteration 171/1000 | Loss: 0.00001666
Iteration 172/1000 | Loss: 0.00001666
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Iteration 175/1000 | Loss: 0.00001666
Iteration 176/1000 | Loss: 0.00001666
Iteration 177/1000 | Loss: 0.00001666
Iteration 178/1000 | Loss: 0.00001666
Iteration 179/1000 | Loss: 0.00001666
Iteration 180/1000 | Loss: 0.00001666
Iteration 181/1000 | Loss: 0.00001666
Iteration 182/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.666479693085421e-05, 1.666479693085421e-05, 1.666479693085421e-05, 1.666479693085421e-05, 1.666479693085421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.666479693085421e-05

Optimization complete. Final v2v error: 3.4110190868377686 mm

Highest mean error: 4.146608352661133 mm for frame 30

Lowest mean error: 2.8822922706604004 mm for frame 146

Saving results

Total time: 44.57528781890869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089411
Iteration 2/25 | Loss: 0.01089410
Iteration 3/25 | Loss: 0.01089410
Iteration 4/25 | Loss: 0.01089410
Iteration 5/25 | Loss: 0.01089410
Iteration 6/25 | Loss: 0.01089410
Iteration 7/25 | Loss: 0.01089410
Iteration 8/25 | Loss: 0.01089410
Iteration 9/25 | Loss: 0.01089410
Iteration 10/25 | Loss: 0.01089410
Iteration 11/25 | Loss: 0.01089410
Iteration 12/25 | Loss: 0.01089410
Iteration 13/25 | Loss: 0.01089409
Iteration 14/25 | Loss: 0.01089409
Iteration 15/25 | Loss: 0.01089409
Iteration 16/25 | Loss: 0.01089409
Iteration 17/25 | Loss: 0.01089409
Iteration 18/25 | Loss: 0.01089409
Iteration 19/25 | Loss: 0.01089409
Iteration 20/25 | Loss: 0.01089409
Iteration 21/25 | Loss: 0.01089409
Iteration 22/25 | Loss: 0.01089409
Iteration 23/25 | Loss: 0.01089409
Iteration 24/25 | Loss: 0.01089409
Iteration 25/25 | Loss: 0.01089409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57971120
Iteration 2/25 | Loss: 0.11764983
Iteration 3/25 | Loss: 0.11752591
Iteration 4/25 | Loss: 0.11714330
Iteration 5/25 | Loss: 0.11712176
Iteration 6/25 | Loss: 0.11712173
Iteration 7/25 | Loss: 0.11712173
Iteration 8/25 | Loss: 0.11712173
Iteration 9/25 | Loss: 0.11712173
Iteration 10/25 | Loss: 0.11712171
Iteration 11/25 | Loss: 0.11712171
Iteration 12/25 | Loss: 0.11712171
Iteration 13/25 | Loss: 0.11712173
Iteration 14/25 | Loss: 0.11712173
Iteration 15/25 | Loss: 0.11712171
Iteration 16/25 | Loss: 0.11712171
Iteration 17/25 | Loss: 0.11712171
Iteration 18/25 | Loss: 0.11712171
Iteration 19/25 | Loss: 0.11712171
Iteration 20/25 | Loss: 0.11712171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.11712171137332916, 0.11712171137332916, 0.11712171137332916, 0.11712171137332916, 0.11712171137332916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11712171137332916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11712171
Iteration 2/1000 | Loss: 0.00142883
Iteration 3/1000 | Loss: 0.00077744
Iteration 4/1000 | Loss: 0.00025415
Iteration 5/1000 | Loss: 0.00035894
Iteration 6/1000 | Loss: 0.00085805
Iteration 7/1000 | Loss: 0.00013204
Iteration 8/1000 | Loss: 0.00005856
Iteration 9/1000 | Loss: 0.00004570
Iteration 10/1000 | Loss: 0.00018455
Iteration 11/1000 | Loss: 0.00003461
Iteration 12/1000 | Loss: 0.00004022
Iteration 13/1000 | Loss: 0.00011343
Iteration 14/1000 | Loss: 0.00002219
Iteration 15/1000 | Loss: 0.00002210
Iteration 16/1000 | Loss: 0.00003403
Iteration 17/1000 | Loss: 0.00004396
Iteration 18/1000 | Loss: 0.00004021
Iteration 19/1000 | Loss: 0.00006556
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00006979
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00003621
Iteration 28/1000 | Loss: 0.00002390
Iteration 29/1000 | Loss: 0.00002052
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00005199
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00002108
Iteration 34/1000 | Loss: 0.00001074
Iteration 35/1000 | Loss: 0.00001065
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00003262
Iteration 39/1000 | Loss: 0.00005028
Iteration 40/1000 | Loss: 0.00011365
Iteration 41/1000 | Loss: 0.00003350
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00002468
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001017
Iteration 47/1000 | Loss: 0.00001012
Iteration 48/1000 | Loss: 0.00001012
Iteration 49/1000 | Loss: 0.00001011
Iteration 50/1000 | Loss: 0.00001011
Iteration 51/1000 | Loss: 0.00001011
Iteration 52/1000 | Loss: 0.00001010
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001012
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001011
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001011
Iteration 61/1000 | Loss: 0.00001011
Iteration 62/1000 | Loss: 0.00001011
Iteration 63/1000 | Loss: 0.00001011
Iteration 64/1000 | Loss: 0.00001011
Iteration 65/1000 | Loss: 0.00001011
Iteration 66/1000 | Loss: 0.00001011
Iteration 67/1000 | Loss: 0.00001011
Iteration 68/1000 | Loss: 0.00001011
Iteration 69/1000 | Loss: 0.00001011
Iteration 70/1000 | Loss: 0.00001011
Iteration 71/1000 | Loss: 0.00001011
Iteration 72/1000 | Loss: 0.00001011
Iteration 73/1000 | Loss: 0.00001011
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001011
Iteration 79/1000 | Loss: 0.00001011
Iteration 80/1000 | Loss: 0.00001011
Iteration 81/1000 | Loss: 0.00001011
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001011
Iteration 87/1000 | Loss: 0.00001011
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001011
Iteration 96/1000 | Loss: 0.00001011
Iteration 97/1000 | Loss: 0.00001011
Iteration 98/1000 | Loss: 0.00001011
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.0109930371982045e-05, 1.0109930371982045e-05, 1.0109930371982045e-05, 1.0109930371982045e-05, 1.0109930371982045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0109930371982045e-05

Optimization complete. Final v2v error: 2.7190752029418945 mm

Highest mean error: 3.4966611862182617 mm for frame 16

Lowest mean error: 2.4236831665039062 mm for frame 146

Saving results

Total time: 78.45991611480713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816989
Iteration 2/25 | Loss: 0.00120679
Iteration 3/25 | Loss: 0.00082024
Iteration 4/25 | Loss: 0.00076144
Iteration 5/25 | Loss: 0.00074639
Iteration 6/25 | Loss: 0.00074204
Iteration 7/25 | Loss: 0.00074128
Iteration 8/25 | Loss: 0.00074128
Iteration 9/25 | Loss: 0.00074128
Iteration 10/25 | Loss: 0.00074128
Iteration 11/25 | Loss: 0.00074128
Iteration 12/25 | Loss: 0.00074128
Iteration 13/25 | Loss: 0.00074128
Iteration 14/25 | Loss: 0.00074128
Iteration 15/25 | Loss: 0.00074128
Iteration 16/25 | Loss: 0.00074128
Iteration 17/25 | Loss: 0.00074128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007412757840938866, 0.0007412757840938866, 0.0007412757840938866, 0.0007412757840938866, 0.0007412757840938866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007412757840938866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49194658
Iteration 2/25 | Loss: 0.00040745
Iteration 3/25 | Loss: 0.00040743
Iteration 4/25 | Loss: 0.00040743
Iteration 5/25 | Loss: 0.00040742
Iteration 6/25 | Loss: 0.00040742
Iteration 7/25 | Loss: 0.00040742
Iteration 8/25 | Loss: 0.00040742
Iteration 9/25 | Loss: 0.00040742
Iteration 10/25 | Loss: 0.00040742
Iteration 11/25 | Loss: 0.00040742
Iteration 12/25 | Loss: 0.00040742
Iteration 13/25 | Loss: 0.00040742
Iteration 14/25 | Loss: 0.00040742
Iteration 15/25 | Loss: 0.00040742
Iteration 16/25 | Loss: 0.00040742
Iteration 17/25 | Loss: 0.00040742
Iteration 18/25 | Loss: 0.00040742
Iteration 19/25 | Loss: 0.00040742
Iteration 20/25 | Loss: 0.00040742
Iteration 21/25 | Loss: 0.00040742
Iteration 22/25 | Loss: 0.00040742
Iteration 23/25 | Loss: 0.00040742
Iteration 24/25 | Loss: 0.00040742
Iteration 25/25 | Loss: 0.00040742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040742
Iteration 2/1000 | Loss: 0.00003884
Iteration 3/1000 | Loss: 0.00002937
Iteration 4/1000 | Loss: 0.00002588
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002339
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002235
Iteration 9/1000 | Loss: 0.00002201
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002145
Iteration 12/1000 | Loss: 0.00002132
Iteration 13/1000 | Loss: 0.00002114
Iteration 14/1000 | Loss: 0.00002107
Iteration 15/1000 | Loss: 0.00002105
Iteration 16/1000 | Loss: 0.00002104
Iteration 17/1000 | Loss: 0.00002103
Iteration 18/1000 | Loss: 0.00002101
Iteration 19/1000 | Loss: 0.00002100
Iteration 20/1000 | Loss: 0.00002099
Iteration 21/1000 | Loss: 0.00002099
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002097
Iteration 24/1000 | Loss: 0.00002093
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002090
Iteration 27/1000 | Loss: 0.00002089
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00002089
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002087
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002086
Iteration 34/1000 | Loss: 0.00002086
Iteration 35/1000 | Loss: 0.00002085
Iteration 36/1000 | Loss: 0.00002085
Iteration 37/1000 | Loss: 0.00002085
Iteration 38/1000 | Loss: 0.00002084
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002084
Iteration 46/1000 | Loss: 0.00002084
Iteration 47/1000 | Loss: 0.00002084
Iteration 48/1000 | Loss: 0.00002084
Iteration 49/1000 | Loss: 0.00002084
Iteration 50/1000 | Loss: 0.00002084
Iteration 51/1000 | Loss: 0.00002084
Iteration 52/1000 | Loss: 0.00002084
Iteration 53/1000 | Loss: 0.00002084
Iteration 54/1000 | Loss: 0.00002084
Iteration 55/1000 | Loss: 0.00002084
Iteration 56/1000 | Loss: 0.00002084
Iteration 57/1000 | Loss: 0.00002084
Iteration 58/1000 | Loss: 0.00002084
Iteration 59/1000 | Loss: 0.00002084
Iteration 60/1000 | Loss: 0.00002084
Iteration 61/1000 | Loss: 0.00002084
Iteration 62/1000 | Loss: 0.00002084
Iteration 63/1000 | Loss: 0.00002084
Iteration 64/1000 | Loss: 0.00002084
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00002084
Iteration 68/1000 | Loss: 0.00002084
Iteration 69/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.0838973796344362e-05, 2.0838973796344362e-05, 2.0838973796344362e-05, 2.0838973796344362e-05, 2.0838973796344362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0838973796344362e-05

Optimization complete. Final v2v error: 3.821518659591675 mm

Highest mean error: 4.337310791015625 mm for frame 169

Lowest mean error: 3.1435959339141846 mm for frame 229

Saving results

Total time: 38.716675758361816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830971
Iteration 2/25 | Loss: 0.00082647
Iteration 3/25 | Loss: 0.00072953
Iteration 4/25 | Loss: 0.00068313
Iteration 5/25 | Loss: 0.00066707
Iteration 6/25 | Loss: 0.00066480
Iteration 7/25 | Loss: 0.00066430
Iteration 8/25 | Loss: 0.00066430
Iteration 9/25 | Loss: 0.00066430
Iteration 10/25 | Loss: 0.00066430
Iteration 11/25 | Loss: 0.00066430
Iteration 12/25 | Loss: 0.00066430
Iteration 13/25 | Loss: 0.00066430
Iteration 14/25 | Loss: 0.00066430
Iteration 15/25 | Loss: 0.00066430
Iteration 16/25 | Loss: 0.00066430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006643029628321528, 0.0006643029628321528, 0.0006643029628321528, 0.0006643029628321528, 0.0006643029628321528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006643029628321528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.44060993
Iteration 2/25 | Loss: 0.00030133
Iteration 3/25 | Loss: 0.00030129
Iteration 4/25 | Loss: 0.00030129
Iteration 5/25 | Loss: 0.00030129
Iteration 6/25 | Loss: 0.00030129
Iteration 7/25 | Loss: 0.00030128
Iteration 8/25 | Loss: 0.00030128
Iteration 9/25 | Loss: 0.00030128
Iteration 10/25 | Loss: 0.00030128
Iteration 11/25 | Loss: 0.00030128
Iteration 12/25 | Loss: 0.00030128
Iteration 13/25 | Loss: 0.00030128
Iteration 14/25 | Loss: 0.00030128
Iteration 15/25 | Loss: 0.00030128
Iteration 16/25 | Loss: 0.00030128
Iteration 17/25 | Loss: 0.00030128
Iteration 18/25 | Loss: 0.00030128
Iteration 19/25 | Loss: 0.00030128
Iteration 20/25 | Loss: 0.00030128
Iteration 21/25 | Loss: 0.00030128
Iteration 22/25 | Loss: 0.00030128
Iteration 23/25 | Loss: 0.00030128
Iteration 24/25 | Loss: 0.00030128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003012840752489865, 0.0003012840752489865, 0.0003012840752489865, 0.0003012840752489865, 0.0003012840752489865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003012840752489865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030128
Iteration 2/1000 | Loss: 0.00003746
Iteration 3/1000 | Loss: 0.00002267
Iteration 4/1000 | Loss: 0.00002002
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001754
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001705
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001684
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001678
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001675
Iteration 18/1000 | Loss: 0.00001675
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001672
Iteration 22/1000 | Loss: 0.00001671
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001670
Iteration 26/1000 | Loss: 0.00001670
Iteration 27/1000 | Loss: 0.00001669
Iteration 28/1000 | Loss: 0.00001669
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001667
Iteration 31/1000 | Loss: 0.00001667
Iteration 32/1000 | Loss: 0.00001667
Iteration 33/1000 | Loss: 0.00001667
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001664
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001664
Iteration 51/1000 | Loss: 0.00001664
Iteration 52/1000 | Loss: 0.00001664
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001661
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001661
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001660
Iteration 81/1000 | Loss: 0.00001660
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001659
Iteration 85/1000 | Loss: 0.00001659
Iteration 86/1000 | Loss: 0.00001659
Iteration 87/1000 | Loss: 0.00001659
Iteration 88/1000 | Loss: 0.00001659
Iteration 89/1000 | Loss: 0.00001659
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001657
Iteration 110/1000 | Loss: 0.00001657
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001656
Iteration 114/1000 | Loss: 0.00001656
Iteration 115/1000 | Loss: 0.00001656
Iteration 116/1000 | Loss: 0.00001656
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001655
Iteration 122/1000 | Loss: 0.00001655
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001655
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001654
Iteration 130/1000 | Loss: 0.00001654
Iteration 131/1000 | Loss: 0.00001654
Iteration 132/1000 | Loss: 0.00001654
Iteration 133/1000 | Loss: 0.00001654
Iteration 134/1000 | Loss: 0.00001654
Iteration 135/1000 | Loss: 0.00001654
Iteration 136/1000 | Loss: 0.00001654
Iteration 137/1000 | Loss: 0.00001654
Iteration 138/1000 | Loss: 0.00001654
Iteration 139/1000 | Loss: 0.00001654
Iteration 140/1000 | Loss: 0.00001654
Iteration 141/1000 | Loss: 0.00001654
Iteration 142/1000 | Loss: 0.00001653
Iteration 143/1000 | Loss: 0.00001653
Iteration 144/1000 | Loss: 0.00001653
Iteration 145/1000 | Loss: 0.00001653
Iteration 146/1000 | Loss: 0.00001653
Iteration 147/1000 | Loss: 0.00001653
Iteration 148/1000 | Loss: 0.00001653
Iteration 149/1000 | Loss: 0.00001653
Iteration 150/1000 | Loss: 0.00001653
Iteration 151/1000 | Loss: 0.00001653
Iteration 152/1000 | Loss: 0.00001653
Iteration 153/1000 | Loss: 0.00001653
Iteration 154/1000 | Loss: 0.00001653
Iteration 155/1000 | Loss: 0.00001653
Iteration 156/1000 | Loss: 0.00001653
Iteration 157/1000 | Loss: 0.00001653
Iteration 158/1000 | Loss: 0.00001653
Iteration 159/1000 | Loss: 0.00001653
Iteration 160/1000 | Loss: 0.00001653
Iteration 161/1000 | Loss: 0.00001653
Iteration 162/1000 | Loss: 0.00001653
Iteration 163/1000 | Loss: 0.00001653
Iteration 164/1000 | Loss: 0.00001653
Iteration 165/1000 | Loss: 0.00001653
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001653
Iteration 170/1000 | Loss: 0.00001653
Iteration 171/1000 | Loss: 0.00001653
Iteration 172/1000 | Loss: 0.00001653
Iteration 173/1000 | Loss: 0.00001653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.6532392692170106e-05, 1.6532392692170106e-05, 1.6532392692170106e-05, 1.6532392692170106e-05, 1.6532392692170106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6532392692170106e-05

Optimization complete. Final v2v error: 3.4567041397094727 mm

Highest mean error: 3.9006659984588623 mm for frame 167

Lowest mean error: 3.1756224632263184 mm for frame 52

Saving results

Total time: 42.42083263397217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974858
Iteration 2/25 | Loss: 0.00371555
Iteration 3/25 | Loss: 0.00222212
Iteration 4/25 | Loss: 0.00195037
Iteration 5/25 | Loss: 0.00198447
Iteration 6/25 | Loss: 0.00143923
Iteration 7/25 | Loss: 0.00122205
Iteration 8/25 | Loss: 0.00112061
Iteration 9/25 | Loss: 0.00106165
Iteration 10/25 | Loss: 0.00103330
Iteration 11/25 | Loss: 0.00100726
Iteration 12/25 | Loss: 0.00101083
Iteration 13/25 | Loss: 0.00099798
Iteration 14/25 | Loss: 0.00100174
Iteration 15/25 | Loss: 0.00100751
Iteration 16/25 | Loss: 0.00101146
Iteration 17/25 | Loss: 0.00098286
Iteration 18/25 | Loss: 0.00099418
Iteration 19/25 | Loss: 0.00099216
Iteration 20/25 | Loss: 0.00099333
Iteration 21/25 | Loss: 0.00098641
Iteration 22/25 | Loss: 0.00099220
Iteration 23/25 | Loss: 0.00098274
Iteration 24/25 | Loss: 0.00098010
Iteration 25/25 | Loss: 0.00097988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44731557
Iteration 2/25 | Loss: 0.00289055
Iteration 3/25 | Loss: 0.00172026
Iteration 4/25 | Loss: 0.00171970
Iteration 5/25 | Loss: 0.00171970
Iteration 6/25 | Loss: 0.00171970
Iteration 7/25 | Loss: 0.00171970
Iteration 8/25 | Loss: 0.00171970
Iteration 9/25 | Loss: 0.00171970
Iteration 10/25 | Loss: 0.00171970
Iteration 11/25 | Loss: 0.00171970
Iteration 12/25 | Loss: 0.00171970
Iteration 13/25 | Loss: 0.00171970
Iteration 14/25 | Loss: 0.00171970
Iteration 15/25 | Loss: 0.00171970
Iteration 16/25 | Loss: 0.00171970
Iteration 17/25 | Loss: 0.00171970
Iteration 18/25 | Loss: 0.00171970
Iteration 19/25 | Loss: 0.00171970
Iteration 20/25 | Loss: 0.00171970
Iteration 21/25 | Loss: 0.00171970
Iteration 22/25 | Loss: 0.00171970
Iteration 23/25 | Loss: 0.00171970
Iteration 24/25 | Loss: 0.00171970
Iteration 25/25 | Loss: 0.00171970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001719701336696744, 0.001719701336696744, 0.001719701336696744, 0.001719701336696744, 0.001719701336696744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001719701336696744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171970
Iteration 2/1000 | Loss: 0.00323583
Iteration 3/1000 | Loss: 0.00166938
Iteration 4/1000 | Loss: 0.00112463
Iteration 5/1000 | Loss: 0.00105907
Iteration 6/1000 | Loss: 0.00108262
Iteration 7/1000 | Loss: 0.00132640
Iteration 8/1000 | Loss: 0.00185094
Iteration 9/1000 | Loss: 0.00259517
Iteration 10/1000 | Loss: 0.00082956
Iteration 11/1000 | Loss: 0.00052044
Iteration 12/1000 | Loss: 0.00060289
Iteration 13/1000 | Loss: 0.00069049
Iteration 14/1000 | Loss: 0.00053057
Iteration 15/1000 | Loss: 0.00032983
Iteration 16/1000 | Loss: 0.00035685
Iteration 17/1000 | Loss: 0.00040090
Iteration 18/1000 | Loss: 0.00090191
Iteration 19/1000 | Loss: 0.00048486
Iteration 20/1000 | Loss: 0.00193291
Iteration 21/1000 | Loss: 0.00075005
Iteration 22/1000 | Loss: 0.00074418
Iteration 23/1000 | Loss: 0.00067865
Iteration 24/1000 | Loss: 0.00032614
Iteration 25/1000 | Loss: 0.00028697
Iteration 26/1000 | Loss: 0.00022665
Iteration 27/1000 | Loss: 0.00045180
Iteration 28/1000 | Loss: 0.00012686
Iteration 29/1000 | Loss: 0.00022755
Iteration 30/1000 | Loss: 0.00019700
Iteration 31/1000 | Loss: 0.00030575
Iteration 32/1000 | Loss: 0.00030006
Iteration 33/1000 | Loss: 0.00059540
Iteration 34/1000 | Loss: 0.00128910
Iteration 35/1000 | Loss: 0.00074403
Iteration 36/1000 | Loss: 0.00024352
Iteration 37/1000 | Loss: 0.00133602
Iteration 38/1000 | Loss: 0.00062112
Iteration 39/1000 | Loss: 0.00019094
Iteration 40/1000 | Loss: 0.00026117
Iteration 41/1000 | Loss: 0.00016371
Iteration 42/1000 | Loss: 0.00162708
Iteration 43/1000 | Loss: 0.00023541
Iteration 44/1000 | Loss: 0.00015388
Iteration 45/1000 | Loss: 0.00044561
Iteration 46/1000 | Loss: 0.00144633
Iteration 47/1000 | Loss: 0.00079790
Iteration 48/1000 | Loss: 0.00009901
Iteration 49/1000 | Loss: 0.00010307
Iteration 50/1000 | Loss: 0.00030970
Iteration 51/1000 | Loss: 0.00004366
Iteration 52/1000 | Loss: 0.00012627
Iteration 53/1000 | Loss: 0.00034534
Iteration 54/1000 | Loss: 0.00016774
Iteration 55/1000 | Loss: 0.00011057
Iteration 56/1000 | Loss: 0.00015707
Iteration 57/1000 | Loss: 0.00008473
Iteration 58/1000 | Loss: 0.00031138
Iteration 59/1000 | Loss: 0.00024196
Iteration 60/1000 | Loss: 0.00017355
Iteration 61/1000 | Loss: 0.00012332
Iteration 62/1000 | Loss: 0.00003814
Iteration 63/1000 | Loss: 0.00003295
Iteration 64/1000 | Loss: 0.00002962
Iteration 65/1000 | Loss: 0.00024141
Iteration 66/1000 | Loss: 0.00003159
Iteration 67/1000 | Loss: 0.00002636
Iteration 68/1000 | Loss: 0.00002507
Iteration 69/1000 | Loss: 0.00002427
Iteration 70/1000 | Loss: 0.00002382
Iteration 71/1000 | Loss: 0.00002337
Iteration 72/1000 | Loss: 0.00002310
Iteration 73/1000 | Loss: 0.00002289
Iteration 74/1000 | Loss: 0.00002272
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002255
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002254
Iteration 79/1000 | Loss: 0.00002254
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00002251
Iteration 88/1000 | Loss: 0.00002250
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002249
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002248
Iteration 96/1000 | Loss: 0.00002248
Iteration 97/1000 | Loss: 0.00002248
Iteration 98/1000 | Loss: 0.00002247
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00002247
Iteration 101/1000 | Loss: 0.00002247
Iteration 102/1000 | Loss: 0.00002247
Iteration 103/1000 | Loss: 0.00002247
Iteration 104/1000 | Loss: 0.00002247
Iteration 105/1000 | Loss: 0.00002247
Iteration 106/1000 | Loss: 0.00002247
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002245
Iteration 111/1000 | Loss: 0.00002245
Iteration 112/1000 | Loss: 0.00002245
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002244
Iteration 117/1000 | Loss: 0.00002244
Iteration 118/1000 | Loss: 0.00002243
Iteration 119/1000 | Loss: 0.00002243
Iteration 120/1000 | Loss: 0.00002243
Iteration 121/1000 | Loss: 0.00002243
Iteration 122/1000 | Loss: 0.00002243
Iteration 123/1000 | Loss: 0.00002243
Iteration 124/1000 | Loss: 0.00002243
Iteration 125/1000 | Loss: 0.00002243
Iteration 126/1000 | Loss: 0.00002243
Iteration 127/1000 | Loss: 0.00002243
Iteration 128/1000 | Loss: 0.00002242
Iteration 129/1000 | Loss: 0.00002242
Iteration 130/1000 | Loss: 0.00002242
Iteration 131/1000 | Loss: 0.00002242
Iteration 132/1000 | Loss: 0.00002242
Iteration 133/1000 | Loss: 0.00002242
Iteration 134/1000 | Loss: 0.00002241
Iteration 135/1000 | Loss: 0.00002241
Iteration 136/1000 | Loss: 0.00002241
Iteration 137/1000 | Loss: 0.00002240
Iteration 138/1000 | Loss: 0.00002240
Iteration 139/1000 | Loss: 0.00002240
Iteration 140/1000 | Loss: 0.00002240
Iteration 141/1000 | Loss: 0.00002240
Iteration 142/1000 | Loss: 0.00002240
Iteration 143/1000 | Loss: 0.00002240
Iteration 144/1000 | Loss: 0.00002240
Iteration 145/1000 | Loss: 0.00002240
Iteration 146/1000 | Loss: 0.00002240
Iteration 147/1000 | Loss: 0.00002239
Iteration 148/1000 | Loss: 0.00002239
Iteration 149/1000 | Loss: 0.00002239
Iteration 150/1000 | Loss: 0.00002239
Iteration 151/1000 | Loss: 0.00002239
Iteration 152/1000 | Loss: 0.00002239
Iteration 153/1000 | Loss: 0.00002239
Iteration 154/1000 | Loss: 0.00002239
Iteration 155/1000 | Loss: 0.00002239
Iteration 156/1000 | Loss: 0.00002239
Iteration 157/1000 | Loss: 0.00002238
Iteration 158/1000 | Loss: 0.00002238
Iteration 159/1000 | Loss: 0.00002238
Iteration 160/1000 | Loss: 0.00002238
Iteration 161/1000 | Loss: 0.00002238
Iteration 162/1000 | Loss: 0.00002238
Iteration 163/1000 | Loss: 0.00002238
Iteration 164/1000 | Loss: 0.00002238
Iteration 165/1000 | Loss: 0.00002238
Iteration 166/1000 | Loss: 0.00002238
Iteration 167/1000 | Loss: 0.00002238
Iteration 168/1000 | Loss: 0.00002238
Iteration 169/1000 | Loss: 0.00002238
Iteration 170/1000 | Loss: 0.00002238
Iteration 171/1000 | Loss: 0.00002238
Iteration 172/1000 | Loss: 0.00002238
Iteration 173/1000 | Loss: 0.00002238
Iteration 174/1000 | Loss: 0.00002238
Iteration 175/1000 | Loss: 0.00002238
Iteration 176/1000 | Loss: 0.00002238
Iteration 177/1000 | Loss: 0.00002238
Iteration 178/1000 | Loss: 0.00002238
Iteration 179/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.2381935195880942e-05, 2.2381935195880942e-05, 2.2381935195880942e-05, 2.2381935195880942e-05, 2.2381935195880942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2381935195880942e-05

Optimization complete. Final v2v error: 3.9370391368865967 mm

Highest mean error: 9.760251998901367 mm for frame 100

Lowest mean error: 3.7159552574157715 mm for frame 112

Saving results

Total time: 159.73605799674988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967904
Iteration 2/25 | Loss: 0.00129147
Iteration 3/25 | Loss: 0.00093124
Iteration 4/25 | Loss: 0.00078845
Iteration 5/25 | Loss: 0.00075385
Iteration 6/25 | Loss: 0.00072921
Iteration 7/25 | Loss: 0.00071835
Iteration 8/25 | Loss: 0.00071327
Iteration 9/25 | Loss: 0.00071208
Iteration 10/25 | Loss: 0.00071988
Iteration 11/25 | Loss: 0.00071201
Iteration 12/25 | Loss: 0.00070999
Iteration 13/25 | Loss: 0.00069255
Iteration 14/25 | Loss: 0.00069079
Iteration 15/25 | Loss: 0.00068845
Iteration 16/25 | Loss: 0.00068713
Iteration 17/25 | Loss: 0.00068693
Iteration 18/25 | Loss: 0.00068681
Iteration 19/25 | Loss: 0.00068678
Iteration 20/25 | Loss: 0.00068678
Iteration 21/25 | Loss: 0.00068678
Iteration 22/25 | Loss: 0.00068678
Iteration 23/25 | Loss: 0.00068678
Iteration 24/25 | Loss: 0.00068678
Iteration 25/25 | Loss: 0.00068678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56799531
Iteration 2/25 | Loss: 0.00038930
Iteration 3/25 | Loss: 0.00030593
Iteration 4/25 | Loss: 0.00030593
Iteration 5/25 | Loss: 0.00030593
Iteration 6/25 | Loss: 0.00030593
Iteration 7/25 | Loss: 0.00030593
Iteration 8/25 | Loss: 0.00030593
Iteration 9/25 | Loss: 0.00030593
Iteration 10/25 | Loss: 0.00030593
Iteration 11/25 | Loss: 0.00030593
Iteration 12/25 | Loss: 0.00030593
Iteration 13/25 | Loss: 0.00030593
Iteration 14/25 | Loss: 0.00030593
Iteration 15/25 | Loss: 0.00030593
Iteration 16/25 | Loss: 0.00030593
Iteration 17/25 | Loss: 0.00030593
Iteration 18/25 | Loss: 0.00030593
Iteration 19/25 | Loss: 0.00030593
Iteration 20/25 | Loss: 0.00030593
Iteration 21/25 | Loss: 0.00030593
Iteration 22/25 | Loss: 0.00030593
Iteration 23/25 | Loss: 0.00030593
Iteration 24/25 | Loss: 0.00030593
Iteration 25/25 | Loss: 0.00030593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030593
Iteration 2/1000 | Loss: 0.00003618
Iteration 3/1000 | Loss: 0.00002508
Iteration 4/1000 | Loss: 0.00002319
Iteration 5/1000 | Loss: 0.00002187
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00002049
Iteration 8/1000 | Loss: 0.00002006
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001956
Iteration 11/1000 | Loss: 0.00001945
Iteration 12/1000 | Loss: 0.00001929
Iteration 13/1000 | Loss: 0.00001927
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001916
Iteration 17/1000 | Loss: 0.00001916
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001916
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001912
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001911
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001910
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00001909
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001908
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001907
Iteration 55/1000 | Loss: 0.00001907
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001904
Iteration 78/1000 | Loss: 0.00001904
Iteration 79/1000 | Loss: 0.00001904
Iteration 80/1000 | Loss: 0.00001904
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001903
Iteration 84/1000 | Loss: 0.00001903
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001903
Iteration 87/1000 | Loss: 0.00001903
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.9028457245440222e-05, 1.9028457245440222e-05, 1.9028457245440222e-05, 1.9028457245440222e-05, 1.9028457245440222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9028457245440222e-05

Optimization complete. Final v2v error: 3.691587448120117 mm

Highest mean error: 4.7972731590271 mm for frame 39

Lowest mean error: 3.1352381706237793 mm for frame 200

Saving results

Total time: 59.92670130729675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824116
Iteration 2/25 | Loss: 0.00105759
Iteration 3/25 | Loss: 0.00071422
Iteration 4/25 | Loss: 0.00066231
Iteration 5/25 | Loss: 0.00064708
Iteration 6/25 | Loss: 0.00064491
Iteration 7/25 | Loss: 0.00064451
Iteration 8/25 | Loss: 0.00064451
Iteration 9/25 | Loss: 0.00064451
Iteration 10/25 | Loss: 0.00064451
Iteration 11/25 | Loss: 0.00064451
Iteration 12/25 | Loss: 0.00064451
Iteration 13/25 | Loss: 0.00064451
Iteration 14/25 | Loss: 0.00064451
Iteration 15/25 | Loss: 0.00064451
Iteration 16/25 | Loss: 0.00064451
Iteration 17/25 | Loss: 0.00064451
Iteration 18/25 | Loss: 0.00064451
Iteration 19/25 | Loss: 0.00064451
Iteration 20/25 | Loss: 0.00064451
Iteration 21/25 | Loss: 0.00064451
Iteration 22/25 | Loss: 0.00064451
Iteration 23/25 | Loss: 0.00064451
Iteration 24/25 | Loss: 0.00064451
Iteration 25/25 | Loss: 0.00064451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23570967
Iteration 2/25 | Loss: 0.00025426
Iteration 3/25 | Loss: 0.00025425
Iteration 4/25 | Loss: 0.00025425
Iteration 5/25 | Loss: 0.00025425
Iteration 6/25 | Loss: 0.00025425
Iteration 7/25 | Loss: 0.00025425
Iteration 8/25 | Loss: 0.00025425
Iteration 9/25 | Loss: 0.00025425
Iteration 10/25 | Loss: 0.00025425
Iteration 11/25 | Loss: 0.00025425
Iteration 12/25 | Loss: 0.00025425
Iteration 13/25 | Loss: 0.00025425
Iteration 14/25 | Loss: 0.00025425
Iteration 15/25 | Loss: 0.00025425
Iteration 16/25 | Loss: 0.00025425
Iteration 17/25 | Loss: 0.00025425
Iteration 18/25 | Loss: 0.00025425
Iteration 19/25 | Loss: 0.00025425
Iteration 20/25 | Loss: 0.00025425
Iteration 21/25 | Loss: 0.00025425
Iteration 22/25 | Loss: 0.00025425
Iteration 23/25 | Loss: 0.00025425
Iteration 24/25 | Loss: 0.00025425
Iteration 25/25 | Loss: 0.00025425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025425
Iteration 2/1000 | Loss: 0.00003540
Iteration 3/1000 | Loss: 0.00002678
Iteration 4/1000 | Loss: 0.00002257
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001862
Iteration 11/1000 | Loss: 0.00001859
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001847
Iteration 14/1000 | Loss: 0.00001846
Iteration 15/1000 | Loss: 0.00001845
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001828
Iteration 18/1000 | Loss: 0.00001827
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001819
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001810
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00001797
Iteration 36/1000 | Loss: 0.00001797
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001796
Iteration 39/1000 | Loss: 0.00001796
Iteration 40/1000 | Loss: 0.00001796
Iteration 41/1000 | Loss: 0.00001796
Iteration 42/1000 | Loss: 0.00001796
Iteration 43/1000 | Loss: 0.00001796
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001794
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001789
Iteration 58/1000 | Loss: 0.00001789
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001789
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001788
Iteration 65/1000 | Loss: 0.00001788
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001788
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001787
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001787
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001786
Iteration 76/1000 | Loss: 0.00001786
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001785
Iteration 82/1000 | Loss: 0.00001785
Iteration 83/1000 | Loss: 0.00001785
Iteration 84/1000 | Loss: 0.00001785
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00001783
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001783
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001780
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001779
Iteration 123/1000 | Loss: 0.00001779
Iteration 124/1000 | Loss: 0.00001779
Iteration 125/1000 | Loss: 0.00001778
Iteration 126/1000 | Loss: 0.00001778
Iteration 127/1000 | Loss: 0.00001778
Iteration 128/1000 | Loss: 0.00001777
Iteration 129/1000 | Loss: 0.00001777
Iteration 130/1000 | Loss: 0.00001777
Iteration 131/1000 | Loss: 0.00001776
Iteration 132/1000 | Loss: 0.00001776
Iteration 133/1000 | Loss: 0.00001776
Iteration 134/1000 | Loss: 0.00001776
Iteration 135/1000 | Loss: 0.00001775
Iteration 136/1000 | Loss: 0.00001775
Iteration 137/1000 | Loss: 0.00001774
Iteration 138/1000 | Loss: 0.00001774
Iteration 139/1000 | Loss: 0.00001774
Iteration 140/1000 | Loss: 0.00001774
Iteration 141/1000 | Loss: 0.00001774
Iteration 142/1000 | Loss: 0.00001774
Iteration 143/1000 | Loss: 0.00001774
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001773
Iteration 146/1000 | Loss: 0.00001773
Iteration 147/1000 | Loss: 0.00001773
Iteration 148/1000 | Loss: 0.00001773
Iteration 149/1000 | Loss: 0.00001773
Iteration 150/1000 | Loss: 0.00001773
Iteration 151/1000 | Loss: 0.00001773
Iteration 152/1000 | Loss: 0.00001772
Iteration 153/1000 | Loss: 0.00001772
Iteration 154/1000 | Loss: 0.00001772
Iteration 155/1000 | Loss: 0.00001772
Iteration 156/1000 | Loss: 0.00001771
Iteration 157/1000 | Loss: 0.00001771
Iteration 158/1000 | Loss: 0.00001771
Iteration 159/1000 | Loss: 0.00001771
Iteration 160/1000 | Loss: 0.00001771
Iteration 161/1000 | Loss: 0.00001771
Iteration 162/1000 | Loss: 0.00001770
Iteration 163/1000 | Loss: 0.00001770
Iteration 164/1000 | Loss: 0.00001770
Iteration 165/1000 | Loss: 0.00001770
Iteration 166/1000 | Loss: 0.00001770
Iteration 167/1000 | Loss: 0.00001770
Iteration 168/1000 | Loss: 0.00001770
Iteration 169/1000 | Loss: 0.00001770
Iteration 170/1000 | Loss: 0.00001770
Iteration 171/1000 | Loss: 0.00001770
Iteration 172/1000 | Loss: 0.00001770
Iteration 173/1000 | Loss: 0.00001770
Iteration 174/1000 | Loss: 0.00001770
Iteration 175/1000 | Loss: 0.00001770
Iteration 176/1000 | Loss: 0.00001769
Iteration 177/1000 | Loss: 0.00001769
Iteration 178/1000 | Loss: 0.00001769
Iteration 179/1000 | Loss: 0.00001769
Iteration 180/1000 | Loss: 0.00001769
Iteration 181/1000 | Loss: 0.00001769
Iteration 182/1000 | Loss: 0.00001769
Iteration 183/1000 | Loss: 0.00001769
Iteration 184/1000 | Loss: 0.00001769
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001769
Iteration 187/1000 | Loss: 0.00001769
Iteration 188/1000 | Loss: 0.00001769
Iteration 189/1000 | Loss: 0.00001769
Iteration 190/1000 | Loss: 0.00001769
Iteration 191/1000 | Loss: 0.00001769
Iteration 192/1000 | Loss: 0.00001769
Iteration 193/1000 | Loss: 0.00001769
Iteration 194/1000 | Loss: 0.00001769
Iteration 195/1000 | Loss: 0.00001769
Iteration 196/1000 | Loss: 0.00001769
Iteration 197/1000 | Loss: 0.00001769
Iteration 198/1000 | Loss: 0.00001769
Iteration 199/1000 | Loss: 0.00001769
Iteration 200/1000 | Loss: 0.00001769
Iteration 201/1000 | Loss: 0.00001769
Iteration 202/1000 | Loss: 0.00001769
Iteration 203/1000 | Loss: 0.00001769
Iteration 204/1000 | Loss: 0.00001769
Iteration 205/1000 | Loss: 0.00001769
Iteration 206/1000 | Loss: 0.00001769
Iteration 207/1000 | Loss: 0.00001769
Iteration 208/1000 | Loss: 0.00001769
Iteration 209/1000 | Loss: 0.00001769
Iteration 210/1000 | Loss: 0.00001769
Iteration 211/1000 | Loss: 0.00001769
Iteration 212/1000 | Loss: 0.00001769
Iteration 213/1000 | Loss: 0.00001769
Iteration 214/1000 | Loss: 0.00001769
Iteration 215/1000 | Loss: 0.00001769
Iteration 216/1000 | Loss: 0.00001769
Iteration 217/1000 | Loss: 0.00001769
Iteration 218/1000 | Loss: 0.00001769
Iteration 219/1000 | Loss: 0.00001769
Iteration 220/1000 | Loss: 0.00001769
Iteration 221/1000 | Loss: 0.00001769
Iteration 222/1000 | Loss: 0.00001769
Iteration 223/1000 | Loss: 0.00001769
Iteration 224/1000 | Loss: 0.00001769
Iteration 225/1000 | Loss: 0.00001769
Iteration 226/1000 | Loss: 0.00001769
Iteration 227/1000 | Loss: 0.00001769
Iteration 228/1000 | Loss: 0.00001769
Iteration 229/1000 | Loss: 0.00001769
Iteration 230/1000 | Loss: 0.00001769
Iteration 231/1000 | Loss: 0.00001769
Iteration 232/1000 | Loss: 0.00001769
Iteration 233/1000 | Loss: 0.00001769
Iteration 234/1000 | Loss: 0.00001769
Iteration 235/1000 | Loss: 0.00001769
Iteration 236/1000 | Loss: 0.00001769
Iteration 237/1000 | Loss: 0.00001769
Iteration 238/1000 | Loss: 0.00001769
Iteration 239/1000 | Loss: 0.00001769
Iteration 240/1000 | Loss: 0.00001769
Iteration 241/1000 | Loss: 0.00001769
Iteration 242/1000 | Loss: 0.00001769
Iteration 243/1000 | Loss: 0.00001769
Iteration 244/1000 | Loss: 0.00001769
Iteration 245/1000 | Loss: 0.00001769
Iteration 246/1000 | Loss: 0.00001769
Iteration 247/1000 | Loss: 0.00001769
Iteration 248/1000 | Loss: 0.00001769
Iteration 249/1000 | Loss: 0.00001769
Iteration 250/1000 | Loss: 0.00001769
Iteration 251/1000 | Loss: 0.00001769
Iteration 252/1000 | Loss: 0.00001769
Iteration 253/1000 | Loss: 0.00001769
Iteration 254/1000 | Loss: 0.00001769
Iteration 255/1000 | Loss: 0.00001769
Iteration 256/1000 | Loss: 0.00001769
Iteration 257/1000 | Loss: 0.00001769
Iteration 258/1000 | Loss: 0.00001769
Iteration 259/1000 | Loss: 0.00001769
Iteration 260/1000 | Loss: 0.00001769
Iteration 261/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.768599759088829e-05, 1.768599759088829e-05, 1.768599759088829e-05, 1.768599759088829e-05, 1.768599759088829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.768599759088829e-05

Optimization complete. Final v2v error: 3.5075345039367676 mm

Highest mean error: 4.937623023986816 mm for frame 64

Lowest mean error: 2.856417655944824 mm for frame 95

Saving results

Total time: 45.3226113319397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845699
Iteration 2/25 | Loss: 0.00106557
Iteration 3/25 | Loss: 0.00074090
Iteration 4/25 | Loss: 0.00070906
Iteration 5/25 | Loss: 0.00069864
Iteration 6/25 | Loss: 0.00069643
Iteration 7/25 | Loss: 0.00069595
Iteration 8/25 | Loss: 0.00069595
Iteration 9/25 | Loss: 0.00069595
Iteration 10/25 | Loss: 0.00069595
Iteration 11/25 | Loss: 0.00069595
Iteration 12/25 | Loss: 0.00069595
Iteration 13/25 | Loss: 0.00069595
Iteration 14/25 | Loss: 0.00069595
Iteration 15/25 | Loss: 0.00069595
Iteration 16/25 | Loss: 0.00069595
Iteration 17/25 | Loss: 0.00069595
Iteration 18/25 | Loss: 0.00069595
Iteration 19/25 | Loss: 0.00069595
Iteration 20/25 | Loss: 0.00069595
Iteration 21/25 | Loss: 0.00069595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006959465681575239, 0.0006959465681575239, 0.0006959465681575239, 0.0006959465681575239, 0.0006959465681575239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006959465681575239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89397001
Iteration 2/25 | Loss: 0.00032195
Iteration 3/25 | Loss: 0.00032192
Iteration 4/25 | Loss: 0.00032192
Iteration 5/25 | Loss: 0.00032192
Iteration 6/25 | Loss: 0.00032192
Iteration 7/25 | Loss: 0.00032192
Iteration 8/25 | Loss: 0.00032192
Iteration 9/25 | Loss: 0.00032192
Iteration 10/25 | Loss: 0.00032192
Iteration 11/25 | Loss: 0.00032192
Iteration 12/25 | Loss: 0.00032192
Iteration 13/25 | Loss: 0.00032192
Iteration 14/25 | Loss: 0.00032192
Iteration 15/25 | Loss: 0.00032192
Iteration 16/25 | Loss: 0.00032192
Iteration 17/25 | Loss: 0.00032192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000321915780659765, 0.000321915780659765, 0.000321915780659765, 0.000321915780659765, 0.000321915780659765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000321915780659765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032192
Iteration 2/1000 | Loss: 0.00003665
Iteration 3/1000 | Loss: 0.00002550
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002204
Iteration 7/1000 | Loss: 0.00002152
Iteration 8/1000 | Loss: 0.00002102
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002055
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00002040
Iteration 13/1000 | Loss: 0.00002031
Iteration 14/1000 | Loss: 0.00002030
Iteration 15/1000 | Loss: 0.00002030
Iteration 16/1000 | Loss: 0.00002026
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002026
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00002026
Iteration 25/1000 | Loss: 0.00002026
Iteration 26/1000 | Loss: 0.00002026
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002025
Iteration 30/1000 | Loss: 0.00002025
Iteration 31/1000 | Loss: 0.00002025
Iteration 32/1000 | Loss: 0.00002025
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00002025
Iteration 35/1000 | Loss: 0.00002025
Iteration 36/1000 | Loss: 0.00002025
Iteration 37/1000 | Loss: 0.00002025
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002024
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002024
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002022
Iteration 52/1000 | Loss: 0.00002022
Iteration 53/1000 | Loss: 0.00002022
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002019
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002018
Iteration 67/1000 | Loss: 0.00002018
Iteration 68/1000 | Loss: 0.00002018
Iteration 69/1000 | Loss: 0.00002018
Iteration 70/1000 | Loss: 0.00002017
Iteration 71/1000 | Loss: 0.00002017
Iteration 72/1000 | Loss: 0.00002016
Iteration 73/1000 | Loss: 0.00002016
Iteration 74/1000 | Loss: 0.00002016
Iteration 75/1000 | Loss: 0.00002015
Iteration 76/1000 | Loss: 0.00002015
Iteration 77/1000 | Loss: 0.00002015
Iteration 78/1000 | Loss: 0.00002015
Iteration 79/1000 | Loss: 0.00002015
Iteration 80/1000 | Loss: 0.00002014
Iteration 81/1000 | Loss: 0.00002014
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002013
Iteration 84/1000 | Loss: 0.00002013
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002011
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002011
Iteration 93/1000 | Loss: 0.00002010
Iteration 94/1000 | Loss: 0.00002010
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002009
Iteration 98/1000 | Loss: 0.00002009
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002008
Iteration 101/1000 | Loss: 0.00002008
Iteration 102/1000 | Loss: 0.00002008
Iteration 103/1000 | Loss: 0.00002007
Iteration 104/1000 | Loss: 0.00002007
Iteration 105/1000 | Loss: 0.00002007
Iteration 106/1000 | Loss: 0.00002007
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002005
Iteration 111/1000 | Loss: 0.00002005
Iteration 112/1000 | Loss: 0.00002005
Iteration 113/1000 | Loss: 0.00002005
Iteration 114/1000 | Loss: 0.00002004
Iteration 115/1000 | Loss: 0.00002004
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002003
Iteration 119/1000 | Loss: 0.00002003
Iteration 120/1000 | Loss: 0.00002003
Iteration 121/1000 | Loss: 0.00002003
Iteration 122/1000 | Loss: 0.00002003
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Iteration 127/1000 | Loss: 0.00002002
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002002
Iteration 134/1000 | Loss: 0.00002001
Iteration 135/1000 | Loss: 0.00002001
Iteration 136/1000 | Loss: 0.00002001
Iteration 137/1000 | Loss: 0.00002001
Iteration 138/1000 | Loss: 0.00002001
Iteration 139/1000 | Loss: 0.00002001
Iteration 140/1000 | Loss: 0.00002001
Iteration 141/1000 | Loss: 0.00002001
Iteration 142/1000 | Loss: 0.00002000
Iteration 143/1000 | Loss: 0.00002000
Iteration 144/1000 | Loss: 0.00002000
Iteration 145/1000 | Loss: 0.00002000
Iteration 146/1000 | Loss: 0.00002000
Iteration 147/1000 | Loss: 0.00002000
Iteration 148/1000 | Loss: 0.00002000
Iteration 149/1000 | Loss: 0.00002000
Iteration 150/1000 | Loss: 0.00002000
Iteration 151/1000 | Loss: 0.00002000
Iteration 152/1000 | Loss: 0.00002000
Iteration 153/1000 | Loss: 0.00002000
Iteration 154/1000 | Loss: 0.00002000
Iteration 155/1000 | Loss: 0.00002000
Iteration 156/1000 | Loss: 0.00002000
Iteration 157/1000 | Loss: 0.00002000
Iteration 158/1000 | Loss: 0.00002000
Iteration 159/1000 | Loss: 0.00002000
Iteration 160/1000 | Loss: 0.00002000
Iteration 161/1000 | Loss: 0.00002000
Iteration 162/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.9997998606413603e-05, 1.9997998606413603e-05, 1.9997998606413603e-05, 1.9997998606413603e-05, 1.9997998606413603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9997998606413603e-05

Optimization complete. Final v2v error: 3.7716071605682373 mm

Highest mean error: 4.580880165100098 mm for frame 232

Lowest mean error: 3.291126012802124 mm for frame 60

Saving results

Total time: 43.15261244773865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00335845
Iteration 2/25 | Loss: 0.00091448
Iteration 3/25 | Loss: 0.00067416
Iteration 4/25 | Loss: 0.00063501
Iteration 5/25 | Loss: 0.00062564
Iteration 6/25 | Loss: 0.00062444
Iteration 7/25 | Loss: 0.00062444
Iteration 8/25 | Loss: 0.00062444
Iteration 9/25 | Loss: 0.00062444
Iteration 10/25 | Loss: 0.00062444
Iteration 11/25 | Loss: 0.00062444
Iteration 12/25 | Loss: 0.00062444
Iteration 13/25 | Loss: 0.00062444
Iteration 14/25 | Loss: 0.00062444
Iteration 15/25 | Loss: 0.00062444
Iteration 16/25 | Loss: 0.00062444
Iteration 17/25 | Loss: 0.00062444
Iteration 18/25 | Loss: 0.00062444
Iteration 19/25 | Loss: 0.00062444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006244445103220642, 0.0006244445103220642, 0.0006244445103220642, 0.0006244445103220642, 0.0006244445103220642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006244445103220642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45726728
Iteration 2/25 | Loss: 0.00029563
Iteration 3/25 | Loss: 0.00029562
Iteration 4/25 | Loss: 0.00029562
Iteration 5/25 | Loss: 0.00029562
Iteration 6/25 | Loss: 0.00029562
Iteration 7/25 | Loss: 0.00029562
Iteration 8/25 | Loss: 0.00029562
Iteration 9/25 | Loss: 0.00029562
Iteration 10/25 | Loss: 0.00029562
Iteration 11/25 | Loss: 0.00029562
Iteration 12/25 | Loss: 0.00029562
Iteration 13/25 | Loss: 0.00029562
Iteration 14/25 | Loss: 0.00029562
Iteration 15/25 | Loss: 0.00029562
Iteration 16/25 | Loss: 0.00029562
Iteration 17/25 | Loss: 0.00029562
Iteration 18/25 | Loss: 0.00029562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002956168900709599, 0.0002956168900709599, 0.0002956168900709599, 0.0002956168900709599, 0.0002956168900709599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002956168900709599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029562
Iteration 2/1000 | Loss: 0.00002918
Iteration 3/1000 | Loss: 0.00001948
Iteration 4/1000 | Loss: 0.00001778
Iteration 5/1000 | Loss: 0.00001661
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001443
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001418
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001403
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00001396
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001393
Iteration 85/1000 | Loss: 0.00001393
Iteration 86/1000 | Loss: 0.00001393
Iteration 87/1000 | Loss: 0.00001393
Iteration 88/1000 | Loss: 0.00001393
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.3925133316661231e-05, 1.3925133316661231e-05, 1.3925133316661231e-05, 1.3925133316661231e-05, 1.3925133316661231e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3925133316661231e-05

Optimization complete. Final v2v error: 3.1788763999938965 mm

Highest mean error: 3.4433929920196533 mm for frame 8

Lowest mean error: 2.8560705184936523 mm for frame 50

Saving results

Total time: 38.509352684020996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789653
Iteration 2/25 | Loss: 0.00187991
Iteration 3/25 | Loss: 0.00100406
Iteration 4/25 | Loss: 0.00083531
Iteration 5/25 | Loss: 0.00079485
Iteration 6/25 | Loss: 0.00078348
Iteration 7/25 | Loss: 0.00078832
Iteration 8/25 | Loss: 0.00079162
Iteration 9/25 | Loss: 0.00079433
Iteration 10/25 | Loss: 0.00079531
Iteration 11/25 | Loss: 0.00079459
Iteration 12/25 | Loss: 0.00078435
Iteration 13/25 | Loss: 0.00079128
Iteration 14/25 | Loss: 0.00078340
Iteration 15/25 | Loss: 0.00078483
Iteration 16/25 | Loss: 0.00079095
Iteration 17/25 | Loss: 0.00078813
Iteration 18/25 | Loss: 0.00077835
Iteration 19/25 | Loss: 0.00076989
Iteration 20/25 | Loss: 0.00076486
Iteration 21/25 | Loss: 0.00076394
Iteration 22/25 | Loss: 0.00076213
Iteration 23/25 | Loss: 0.00076107
Iteration 24/25 | Loss: 0.00076126
Iteration 25/25 | Loss: 0.00076200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45519173
Iteration 2/25 | Loss: 0.00037218
Iteration 3/25 | Loss: 0.00037218
Iteration 4/25 | Loss: 0.00037218
Iteration 5/25 | Loss: 0.00037218
Iteration 6/25 | Loss: 0.00037218
Iteration 7/25 | Loss: 0.00037218
Iteration 8/25 | Loss: 0.00037218
Iteration 9/25 | Loss: 0.00037218
Iteration 10/25 | Loss: 0.00037218
Iteration 11/25 | Loss: 0.00037218
Iteration 12/25 | Loss: 0.00037218
Iteration 13/25 | Loss: 0.00037218
Iteration 14/25 | Loss: 0.00037218
Iteration 15/25 | Loss: 0.00037218
Iteration 16/25 | Loss: 0.00037218
Iteration 17/25 | Loss: 0.00037218
Iteration 18/25 | Loss: 0.00037218
Iteration 19/25 | Loss: 0.00037218
Iteration 20/25 | Loss: 0.00037218
Iteration 21/25 | Loss: 0.00037218
Iteration 22/25 | Loss: 0.00037218
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003721787652466446, 0.0003721787652466446, 0.0003721787652466446, 0.0003721787652466446, 0.0003721787652466446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003721787652466446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037218
Iteration 2/1000 | Loss: 0.00003700
Iteration 3/1000 | Loss: 0.00003724
Iteration 4/1000 | Loss: 0.00003744
Iteration 5/1000 | Loss: 0.00002931
Iteration 6/1000 | Loss: 0.00003671
Iteration 7/1000 | Loss: 0.00003412
Iteration 8/1000 | Loss: 0.00003183
Iteration 9/1000 | Loss: 0.00003348
Iteration 10/1000 | Loss: 0.00003741
Iteration 11/1000 | Loss: 0.00003367
Iteration 12/1000 | Loss: 0.00003529
Iteration 13/1000 | Loss: 0.00002989
Iteration 14/1000 | Loss: 0.00002712
Iteration 15/1000 | Loss: 0.00002536
Iteration 16/1000 | Loss: 0.00005583
Iteration 17/1000 | Loss: 0.00004402
Iteration 18/1000 | Loss: 0.00002420
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002357
Iteration 21/1000 | Loss: 0.00002348
Iteration 22/1000 | Loss: 0.00002341
Iteration 23/1000 | Loss: 0.00005500
Iteration 24/1000 | Loss: 0.00004784
Iteration 25/1000 | Loss: 0.00002331
Iteration 26/1000 | Loss: 0.00005568
Iteration 27/1000 | Loss: 0.00004792
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00002331
Iteration 30/1000 | Loss: 0.00002314
Iteration 31/1000 | Loss: 0.00002313
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00008617
Iteration 34/1000 | Loss: 0.00006606
Iteration 35/1000 | Loss: 0.00003589
Iteration 36/1000 | Loss: 0.00002951
Iteration 37/1000 | Loss: 0.00006022
Iteration 38/1000 | Loss: 0.00008554
Iteration 39/1000 | Loss: 0.00007190
Iteration 40/1000 | Loss: 0.00007872
Iteration 41/1000 | Loss: 0.00005346
Iteration 42/1000 | Loss: 0.00005177
Iteration 43/1000 | Loss: 0.00003641
Iteration 44/1000 | Loss: 0.00004232
Iteration 45/1000 | Loss: 0.00004557
Iteration 46/1000 | Loss: 0.00003398
Iteration 47/1000 | Loss: 0.00003734
Iteration 48/1000 | Loss: 0.00003038
Iteration 49/1000 | Loss: 0.00002776
Iteration 50/1000 | Loss: 0.00002549
Iteration 51/1000 | Loss: 0.00002472
Iteration 52/1000 | Loss: 0.00002409
Iteration 53/1000 | Loss: 0.00002350
Iteration 54/1000 | Loss: 0.00002317
Iteration 55/1000 | Loss: 0.00002288
Iteration 56/1000 | Loss: 0.00002270
Iteration 57/1000 | Loss: 0.00002257
Iteration 58/1000 | Loss: 0.00002251
Iteration 59/1000 | Loss: 0.00002251
Iteration 60/1000 | Loss: 0.00002251
Iteration 61/1000 | Loss: 0.00002250
Iteration 62/1000 | Loss: 0.00002250
Iteration 63/1000 | Loss: 0.00002249
Iteration 64/1000 | Loss: 0.00002248
Iteration 65/1000 | Loss: 0.00002247
Iteration 66/1000 | Loss: 0.00002247
Iteration 67/1000 | Loss: 0.00002247
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00002247
Iteration 70/1000 | Loss: 0.00002247
Iteration 71/1000 | Loss: 0.00002247
Iteration 72/1000 | Loss: 0.00002247
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002246
Iteration 76/1000 | Loss: 0.00002246
Iteration 77/1000 | Loss: 0.00002245
Iteration 78/1000 | Loss: 0.00002245
Iteration 79/1000 | Loss: 0.00002245
Iteration 80/1000 | Loss: 0.00002244
Iteration 81/1000 | Loss: 0.00002244
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002243
Iteration 84/1000 | Loss: 0.00002243
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002240
Iteration 87/1000 | Loss: 0.00002239
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002239
Iteration 90/1000 | Loss: 0.00002239
Iteration 91/1000 | Loss: 0.00002239
Iteration 92/1000 | Loss: 0.00002238
Iteration 93/1000 | Loss: 0.00002238
Iteration 94/1000 | Loss: 0.00002238
Iteration 95/1000 | Loss: 0.00002238
Iteration 96/1000 | Loss: 0.00002237
Iteration 97/1000 | Loss: 0.00002237
Iteration 98/1000 | Loss: 0.00002237
Iteration 99/1000 | Loss: 0.00002236
Iteration 100/1000 | Loss: 0.00002236
Iteration 101/1000 | Loss: 0.00002236
Iteration 102/1000 | Loss: 0.00002236
Iteration 103/1000 | Loss: 0.00002236
Iteration 104/1000 | Loss: 0.00002236
Iteration 105/1000 | Loss: 0.00002236
Iteration 106/1000 | Loss: 0.00002236
Iteration 107/1000 | Loss: 0.00002236
Iteration 108/1000 | Loss: 0.00002235
Iteration 109/1000 | Loss: 0.00002235
Iteration 110/1000 | Loss: 0.00002235
Iteration 111/1000 | Loss: 0.00002235
Iteration 112/1000 | Loss: 0.00002235
Iteration 113/1000 | Loss: 0.00002235
Iteration 114/1000 | Loss: 0.00002235
Iteration 115/1000 | Loss: 0.00005320
Iteration 116/1000 | Loss: 0.00002928
Iteration 117/1000 | Loss: 0.00002651
Iteration 118/1000 | Loss: 0.00002445
Iteration 119/1000 | Loss: 0.00002351
Iteration 120/1000 | Loss: 0.00002281
Iteration 121/1000 | Loss: 0.00002254
Iteration 122/1000 | Loss: 0.00002234
Iteration 123/1000 | Loss: 0.00002234
Iteration 124/1000 | Loss: 0.00002232
Iteration 125/1000 | Loss: 0.00002232
Iteration 126/1000 | Loss: 0.00002232
Iteration 127/1000 | Loss: 0.00002232
Iteration 128/1000 | Loss: 0.00002232
Iteration 129/1000 | Loss: 0.00002232
Iteration 130/1000 | Loss: 0.00002232
Iteration 131/1000 | Loss: 0.00002231
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002231
Iteration 134/1000 | Loss: 0.00002231
Iteration 135/1000 | Loss: 0.00002231
Iteration 136/1000 | Loss: 0.00002231
Iteration 137/1000 | Loss: 0.00002231
Iteration 138/1000 | Loss: 0.00002231
Iteration 139/1000 | Loss: 0.00002231
Iteration 140/1000 | Loss: 0.00002231
Iteration 141/1000 | Loss: 0.00002231
Iteration 142/1000 | Loss: 0.00002231
Iteration 143/1000 | Loss: 0.00002230
Iteration 144/1000 | Loss: 0.00002230
Iteration 145/1000 | Loss: 0.00002230
Iteration 146/1000 | Loss: 0.00002230
Iteration 147/1000 | Loss: 0.00002229
Iteration 148/1000 | Loss: 0.00002229
Iteration 149/1000 | Loss: 0.00002229
Iteration 150/1000 | Loss: 0.00002229
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002229
Iteration 153/1000 | Loss: 0.00002229
Iteration 154/1000 | Loss: 0.00002229
Iteration 155/1000 | Loss: 0.00002229
Iteration 156/1000 | Loss: 0.00002229
Iteration 157/1000 | Loss: 0.00002229
Iteration 158/1000 | Loss: 0.00002229
Iteration 159/1000 | Loss: 0.00002229
Iteration 160/1000 | Loss: 0.00002229
Iteration 161/1000 | Loss: 0.00002228
Iteration 162/1000 | Loss: 0.00002228
Iteration 163/1000 | Loss: 0.00002228
Iteration 164/1000 | Loss: 0.00002228
Iteration 165/1000 | Loss: 0.00002228
Iteration 166/1000 | Loss: 0.00002228
Iteration 167/1000 | Loss: 0.00002228
Iteration 168/1000 | Loss: 0.00002228
Iteration 169/1000 | Loss: 0.00002227
Iteration 170/1000 | Loss: 0.00002227
Iteration 171/1000 | Loss: 0.00002226
Iteration 172/1000 | Loss: 0.00002226
Iteration 173/1000 | Loss: 0.00002226
Iteration 174/1000 | Loss: 0.00002225
Iteration 175/1000 | Loss: 0.00002225
Iteration 176/1000 | Loss: 0.00002225
Iteration 177/1000 | Loss: 0.00002225
Iteration 178/1000 | Loss: 0.00002225
Iteration 179/1000 | Loss: 0.00002225
Iteration 180/1000 | Loss: 0.00002225
Iteration 181/1000 | Loss: 0.00002225
Iteration 182/1000 | Loss: 0.00002225
Iteration 183/1000 | Loss: 0.00002224
Iteration 184/1000 | Loss: 0.00002224
Iteration 185/1000 | Loss: 0.00002224
Iteration 186/1000 | Loss: 0.00002224
Iteration 187/1000 | Loss: 0.00002224
Iteration 188/1000 | Loss: 0.00002224
Iteration 189/1000 | Loss: 0.00002224
Iteration 190/1000 | Loss: 0.00002224
Iteration 191/1000 | Loss: 0.00002224
Iteration 192/1000 | Loss: 0.00002224
Iteration 193/1000 | Loss: 0.00002224
Iteration 194/1000 | Loss: 0.00002224
Iteration 195/1000 | Loss: 0.00002224
Iteration 196/1000 | Loss: 0.00002224
Iteration 197/1000 | Loss: 0.00002224
Iteration 198/1000 | Loss: 0.00002224
Iteration 199/1000 | Loss: 0.00002224
Iteration 200/1000 | Loss: 0.00002223
Iteration 201/1000 | Loss: 0.00002223
Iteration 202/1000 | Loss: 0.00002223
Iteration 203/1000 | Loss: 0.00002223
Iteration 204/1000 | Loss: 0.00002223
Iteration 205/1000 | Loss: 0.00002223
Iteration 206/1000 | Loss: 0.00002223
Iteration 207/1000 | Loss: 0.00002223
Iteration 208/1000 | Loss: 0.00002223
Iteration 209/1000 | Loss: 0.00002223
Iteration 210/1000 | Loss: 0.00002223
Iteration 211/1000 | Loss: 0.00002223
Iteration 212/1000 | Loss: 0.00002223
Iteration 213/1000 | Loss: 0.00002223
Iteration 214/1000 | Loss: 0.00002223
Iteration 215/1000 | Loss: 0.00002223
Iteration 216/1000 | Loss: 0.00002223
Iteration 217/1000 | Loss: 0.00002223
Iteration 218/1000 | Loss: 0.00002223
Iteration 219/1000 | Loss: 0.00002223
Iteration 220/1000 | Loss: 0.00002223
Iteration 221/1000 | Loss: 0.00002223
Iteration 222/1000 | Loss: 0.00002223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.2234647985897027e-05, 2.2234647985897027e-05, 2.2234647985897027e-05, 2.2234647985897027e-05, 2.2234647985897027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2234647985897027e-05

Optimization complete. Final v2v error: 3.8936092853546143 mm

Highest mean error: 5.032834529876709 mm for frame 92

Lowest mean error: 3.4409215450286865 mm for frame 186

Saving results

Total time: 162.82036542892456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014474
Iteration 2/25 | Loss: 0.00132960
Iteration 3/25 | Loss: 0.00082666
Iteration 4/25 | Loss: 0.00074007
Iteration 5/25 | Loss: 0.00072035
Iteration 6/25 | Loss: 0.00071641
Iteration 7/25 | Loss: 0.00070883
Iteration 8/25 | Loss: 0.00069851
Iteration 9/25 | Loss: 0.00069327
Iteration 10/25 | Loss: 0.00068694
Iteration 11/25 | Loss: 0.00068187
Iteration 12/25 | Loss: 0.00068045
Iteration 13/25 | Loss: 0.00068900
Iteration 14/25 | Loss: 0.00068362
Iteration 15/25 | Loss: 0.00067967
Iteration 16/25 | Loss: 0.00067727
Iteration 17/25 | Loss: 0.00067315
Iteration 18/25 | Loss: 0.00067219
Iteration 19/25 | Loss: 0.00067198
Iteration 20/25 | Loss: 0.00067198
Iteration 21/25 | Loss: 0.00067198
Iteration 22/25 | Loss: 0.00067198
Iteration 23/25 | Loss: 0.00067197
Iteration 24/25 | Loss: 0.00067197
Iteration 25/25 | Loss: 0.00067197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.59089661
Iteration 2/25 | Loss: 0.00054561
Iteration 3/25 | Loss: 0.00054553
Iteration 4/25 | Loss: 0.00054553
Iteration 5/25 | Loss: 0.00054553
Iteration 6/25 | Loss: 0.00054553
Iteration 7/25 | Loss: 0.00054553
Iteration 8/25 | Loss: 0.00054553
Iteration 9/25 | Loss: 0.00054553
Iteration 10/25 | Loss: 0.00054553
Iteration 11/25 | Loss: 0.00054553
Iteration 12/25 | Loss: 0.00054553
Iteration 13/25 | Loss: 0.00054553
Iteration 14/25 | Loss: 0.00054553
Iteration 15/25 | Loss: 0.00054553
Iteration 16/25 | Loss: 0.00054553
Iteration 17/25 | Loss: 0.00054553
Iteration 18/25 | Loss: 0.00054553
Iteration 19/25 | Loss: 0.00054553
Iteration 20/25 | Loss: 0.00054553
Iteration 21/25 | Loss: 0.00054553
Iteration 22/25 | Loss: 0.00054553
Iteration 23/25 | Loss: 0.00054553
Iteration 24/25 | Loss: 0.00054553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000545528600923717, 0.000545528600923717, 0.000545528600923717, 0.000545528600923717, 0.000545528600923717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000545528600923717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054553
Iteration 2/1000 | Loss: 0.00006573
Iteration 3/1000 | Loss: 0.00003162
Iteration 4/1000 | Loss: 0.00002484
Iteration 5/1000 | Loss: 0.00002236
Iteration 6/1000 | Loss: 0.00002882
Iteration 7/1000 | Loss: 0.00002259
Iteration 8/1000 | Loss: 0.00004589
Iteration 9/1000 | Loss: 0.00005467
Iteration 10/1000 | Loss: 0.00002708
Iteration 11/1000 | Loss: 0.00006149
Iteration 12/1000 | Loss: 0.00003750
Iteration 13/1000 | Loss: 0.00005068
Iteration 14/1000 | Loss: 0.00005895
Iteration 15/1000 | Loss: 0.00002381
Iteration 16/1000 | Loss: 0.00003640
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001753
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001628
Iteration 26/1000 | Loss: 0.00001600
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001569
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001566
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001558
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001551
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001544
Iteration 89/1000 | Loss: 0.00001544
Iteration 90/1000 | Loss: 0.00001544
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001541
Iteration 114/1000 | Loss: 0.00001541
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001538
Iteration 143/1000 | Loss: 0.00001538
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001536
Iteration 164/1000 | Loss: 0.00001536
Iteration 165/1000 | Loss: 0.00001536
Iteration 166/1000 | Loss: 0.00001536
Iteration 167/1000 | Loss: 0.00001536
Iteration 168/1000 | Loss: 0.00001536
Iteration 169/1000 | Loss: 0.00001536
Iteration 170/1000 | Loss: 0.00001536
Iteration 171/1000 | Loss: 0.00001536
Iteration 172/1000 | Loss: 0.00001536
Iteration 173/1000 | Loss: 0.00001536
Iteration 174/1000 | Loss: 0.00001536
Iteration 175/1000 | Loss: 0.00001536
Iteration 176/1000 | Loss: 0.00001536
Iteration 177/1000 | Loss: 0.00001536
Iteration 178/1000 | Loss: 0.00001536
Iteration 179/1000 | Loss: 0.00001536
Iteration 180/1000 | Loss: 0.00001536
Iteration 181/1000 | Loss: 0.00001536
Iteration 182/1000 | Loss: 0.00001536
Iteration 183/1000 | Loss: 0.00001536
Iteration 184/1000 | Loss: 0.00001536
Iteration 185/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.536168383609038e-05, 1.536168383609038e-05, 1.536168383609038e-05, 1.536168383609038e-05, 1.536168383609038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536168383609038e-05

Optimization complete. Final v2v error: 3.241384267807007 mm

Highest mean error: 4.59128475189209 mm for frame 22

Lowest mean error: 2.7198028564453125 mm for frame 71

Saving results

Total time: 84.22631859779358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438712
Iteration 2/25 | Loss: 0.00090315
Iteration 3/25 | Loss: 0.00071083
Iteration 4/25 | Loss: 0.00067733
Iteration 5/25 | Loss: 0.00066970
Iteration 6/25 | Loss: 0.00066884
Iteration 7/25 | Loss: 0.00066880
Iteration 8/25 | Loss: 0.00066880
Iteration 9/25 | Loss: 0.00066880
Iteration 10/25 | Loss: 0.00066880
Iteration 11/25 | Loss: 0.00066880
Iteration 12/25 | Loss: 0.00066880
Iteration 13/25 | Loss: 0.00066880
Iteration 14/25 | Loss: 0.00066880
Iteration 15/25 | Loss: 0.00066880
Iteration 16/25 | Loss: 0.00066880
Iteration 17/25 | Loss: 0.00066880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006687972345389426, 0.0006687972345389426, 0.0006687972345389426, 0.0006687972345389426, 0.0006687972345389426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006687972345389426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05172265
Iteration 2/25 | Loss: 0.00031571
Iteration 3/25 | Loss: 0.00031570
Iteration 4/25 | Loss: 0.00031570
Iteration 5/25 | Loss: 0.00031570
Iteration 6/25 | Loss: 0.00031570
Iteration 7/25 | Loss: 0.00031570
Iteration 8/25 | Loss: 0.00031570
Iteration 9/25 | Loss: 0.00031570
Iteration 10/25 | Loss: 0.00031570
Iteration 11/25 | Loss: 0.00031570
Iteration 12/25 | Loss: 0.00031570
Iteration 13/25 | Loss: 0.00031570
Iteration 14/25 | Loss: 0.00031570
Iteration 15/25 | Loss: 0.00031570
Iteration 16/25 | Loss: 0.00031570
Iteration 17/25 | Loss: 0.00031570
Iteration 18/25 | Loss: 0.00031570
Iteration 19/25 | Loss: 0.00031570
Iteration 20/25 | Loss: 0.00031570
Iteration 21/25 | Loss: 0.00031570
Iteration 22/25 | Loss: 0.00031570
Iteration 23/25 | Loss: 0.00031570
Iteration 24/25 | Loss: 0.00031570
Iteration 25/25 | Loss: 0.00031570

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031570
Iteration 2/1000 | Loss: 0.00003540
Iteration 3/1000 | Loss: 0.00002353
Iteration 4/1000 | Loss: 0.00002079
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001905
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001774
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001771
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001765
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001764
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001763
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001763
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001762
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001761
Iteration 36/1000 | Loss: 0.00001760
Iteration 37/1000 | Loss: 0.00001760
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001757
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001757
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001757
Iteration 67/1000 | Loss: 0.00001757
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.7570187992532738e-05, 1.7570187992532738e-05, 1.7570187992532738e-05, 1.7570187992532738e-05, 1.7570187992532738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7570187992532738e-05

Optimization complete. Final v2v error: 3.531463861465454 mm

Highest mean error: 3.5535316467285156 mm for frame 57

Lowest mean error: 3.49180269241333 mm for frame 106

Saving results

Total time: 28.391380548477173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044835
Iteration 2/25 | Loss: 0.00229034
Iteration 3/25 | Loss: 0.00158660
Iteration 4/25 | Loss: 0.00140185
Iteration 5/25 | Loss: 0.00133211
Iteration 6/25 | Loss: 0.00129020
Iteration 7/25 | Loss: 0.00125514
Iteration 8/25 | Loss: 0.00117628
Iteration 9/25 | Loss: 0.00116060
Iteration 10/25 | Loss: 0.00112670
Iteration 11/25 | Loss: 0.00110111
Iteration 12/25 | Loss: 0.00108687
Iteration 13/25 | Loss: 0.00108228
Iteration 14/25 | Loss: 0.00106044
Iteration 15/25 | Loss: 0.00105662
Iteration 16/25 | Loss: 0.00104865
Iteration 17/25 | Loss: 0.00104747
Iteration 18/25 | Loss: 0.00103842
Iteration 19/25 | Loss: 0.00103330
Iteration 20/25 | Loss: 0.00102870
Iteration 21/25 | Loss: 0.00102968
Iteration 22/25 | Loss: 0.00102822
Iteration 23/25 | Loss: 0.00102469
Iteration 24/25 | Loss: 0.00102321
Iteration 25/25 | Loss: 0.00102268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41662109
Iteration 2/25 | Loss: 0.00417887
Iteration 3/25 | Loss: 0.00417735
Iteration 4/25 | Loss: 0.00417734
Iteration 5/25 | Loss: 0.00417734
Iteration 6/25 | Loss: 0.00417734
Iteration 7/25 | Loss: 0.00417734
Iteration 8/25 | Loss: 0.00417734
Iteration 9/25 | Loss: 0.00417734
Iteration 10/25 | Loss: 0.00417734
Iteration 11/25 | Loss: 0.00417734
Iteration 12/25 | Loss: 0.00417734
Iteration 13/25 | Loss: 0.00417734
Iteration 14/25 | Loss: 0.00417734
Iteration 15/25 | Loss: 0.00417734
Iteration 16/25 | Loss: 0.00417734
Iteration 17/25 | Loss: 0.00417734
Iteration 18/25 | Loss: 0.00417734
Iteration 19/25 | Loss: 0.00417734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004177341237664223, 0.004177341237664223, 0.004177341237664223, 0.004177341237664223, 0.004177341237664223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004177341237664223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00417734
Iteration 2/1000 | Loss: 0.00117025
Iteration 3/1000 | Loss: 0.00322735
Iteration 4/1000 | Loss: 0.00166144
Iteration 5/1000 | Loss: 0.00057998
Iteration 6/1000 | Loss: 0.00032127
Iteration 7/1000 | Loss: 0.00066429
Iteration 8/1000 | Loss: 0.00089321
Iteration 9/1000 | Loss: 0.00021338
Iteration 10/1000 | Loss: 0.00044546
Iteration 11/1000 | Loss: 0.00014658
Iteration 12/1000 | Loss: 0.00012743
Iteration 13/1000 | Loss: 0.00300926
Iteration 14/1000 | Loss: 0.00325935
Iteration 15/1000 | Loss: 0.00037641
Iteration 16/1000 | Loss: 0.00015151
Iteration 17/1000 | Loss: 0.00078181
Iteration 18/1000 | Loss: 0.00014158
Iteration 19/1000 | Loss: 0.00008665
Iteration 20/1000 | Loss: 0.00007317
Iteration 21/1000 | Loss: 0.00094598
Iteration 22/1000 | Loss: 0.00289198
Iteration 23/1000 | Loss: 0.00023403
Iteration 24/1000 | Loss: 0.00015210
Iteration 25/1000 | Loss: 0.00007795
Iteration 26/1000 | Loss: 0.00006194
Iteration 27/1000 | Loss: 0.00093672
Iteration 28/1000 | Loss: 0.00037084
Iteration 29/1000 | Loss: 0.00058605
Iteration 30/1000 | Loss: 0.00006096
Iteration 31/1000 | Loss: 0.00005236
Iteration 32/1000 | Loss: 0.00004702
Iteration 33/1000 | Loss: 0.00004438
Iteration 34/1000 | Loss: 0.00004286
Iteration 35/1000 | Loss: 0.00009580
Iteration 36/1000 | Loss: 0.00016166
Iteration 37/1000 | Loss: 0.00004098
Iteration 38/1000 | Loss: 0.00004004
Iteration 39/1000 | Loss: 0.00011009
Iteration 40/1000 | Loss: 0.00003939
Iteration 41/1000 | Loss: 0.00028976
Iteration 42/1000 | Loss: 0.00087217
Iteration 43/1000 | Loss: 0.00038526
Iteration 44/1000 | Loss: 0.00006954
Iteration 45/1000 | Loss: 0.00004505
Iteration 46/1000 | Loss: 0.00049649
Iteration 47/1000 | Loss: 0.00043950
Iteration 48/1000 | Loss: 0.00060196
Iteration 49/1000 | Loss: 0.00014205
Iteration 50/1000 | Loss: 0.00018409
Iteration 51/1000 | Loss: 0.00004939
Iteration 52/1000 | Loss: 0.00005335
Iteration 53/1000 | Loss: 0.00027227
Iteration 54/1000 | Loss: 0.00017988
Iteration 55/1000 | Loss: 0.00025851
Iteration 56/1000 | Loss: 0.00036375
Iteration 57/1000 | Loss: 0.00017950
Iteration 58/1000 | Loss: 0.00004292
Iteration 59/1000 | Loss: 0.00003590
Iteration 60/1000 | Loss: 0.00008401
Iteration 61/1000 | Loss: 0.00009297
Iteration 62/1000 | Loss: 0.00003085
Iteration 63/1000 | Loss: 0.00004318
Iteration 64/1000 | Loss: 0.00003804
Iteration 65/1000 | Loss: 0.00003118
Iteration 66/1000 | Loss: 0.00002875
Iteration 67/1000 | Loss: 0.00055031
Iteration 68/1000 | Loss: 0.00003092
Iteration 69/1000 | Loss: 0.00002872
Iteration 70/1000 | Loss: 0.00002777
Iteration 71/1000 | Loss: 0.00002676
Iteration 72/1000 | Loss: 0.00002628
Iteration 73/1000 | Loss: 0.00002602
Iteration 74/1000 | Loss: 0.00002594
Iteration 75/1000 | Loss: 0.00006346
Iteration 76/1000 | Loss: 0.00011610
Iteration 77/1000 | Loss: 0.00003975
Iteration 78/1000 | Loss: 0.00002585
Iteration 79/1000 | Loss: 0.00002574
Iteration 80/1000 | Loss: 0.00004852
Iteration 81/1000 | Loss: 0.00003044
Iteration 82/1000 | Loss: 0.00002571
Iteration 83/1000 | Loss: 0.00002571
Iteration 84/1000 | Loss: 0.00002571
Iteration 85/1000 | Loss: 0.00002571
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002571
Iteration 88/1000 | Loss: 0.00002571
Iteration 89/1000 | Loss: 0.00002571
Iteration 90/1000 | Loss: 0.00002571
Iteration 91/1000 | Loss: 0.00002571
Iteration 92/1000 | Loss: 0.00002571
Iteration 93/1000 | Loss: 0.00002570
Iteration 94/1000 | Loss: 0.00002570
Iteration 95/1000 | Loss: 0.00002569
Iteration 96/1000 | Loss: 0.00002568
Iteration 97/1000 | Loss: 0.00002568
Iteration 98/1000 | Loss: 0.00002567
Iteration 99/1000 | Loss: 0.00002567
Iteration 100/1000 | Loss: 0.00002567
Iteration 101/1000 | Loss: 0.00002567
Iteration 102/1000 | Loss: 0.00002566
Iteration 103/1000 | Loss: 0.00002566
Iteration 104/1000 | Loss: 0.00002566
Iteration 105/1000 | Loss: 0.00002566
Iteration 106/1000 | Loss: 0.00002566
Iteration 107/1000 | Loss: 0.00002566
Iteration 108/1000 | Loss: 0.00002566
Iteration 109/1000 | Loss: 0.00002566
Iteration 110/1000 | Loss: 0.00002566
Iteration 111/1000 | Loss: 0.00002566
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002565
Iteration 114/1000 | Loss: 0.00002564
Iteration 115/1000 | Loss: 0.00002564
Iteration 116/1000 | Loss: 0.00002564
Iteration 117/1000 | Loss: 0.00002564
Iteration 118/1000 | Loss: 0.00002564
Iteration 119/1000 | Loss: 0.00002564
Iteration 120/1000 | Loss: 0.00002564
Iteration 121/1000 | Loss: 0.00002564
Iteration 122/1000 | Loss: 0.00002564
Iteration 123/1000 | Loss: 0.00002563
Iteration 124/1000 | Loss: 0.00002563
Iteration 125/1000 | Loss: 0.00002563
Iteration 126/1000 | Loss: 0.00002563
Iteration 127/1000 | Loss: 0.00002563
Iteration 128/1000 | Loss: 0.00002563
Iteration 129/1000 | Loss: 0.00002563
Iteration 130/1000 | Loss: 0.00002563
Iteration 131/1000 | Loss: 0.00002563
Iteration 132/1000 | Loss: 0.00002563
Iteration 133/1000 | Loss: 0.00002563
Iteration 134/1000 | Loss: 0.00002563
Iteration 135/1000 | Loss: 0.00002562
Iteration 136/1000 | Loss: 0.00002562
Iteration 137/1000 | Loss: 0.00002562
Iteration 138/1000 | Loss: 0.00002561
Iteration 139/1000 | Loss: 0.00002561
Iteration 140/1000 | Loss: 0.00002561
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002561
Iteration 143/1000 | Loss: 0.00002561
Iteration 144/1000 | Loss: 0.00002561
Iteration 145/1000 | Loss: 0.00002561
Iteration 146/1000 | Loss: 0.00002561
Iteration 147/1000 | Loss: 0.00002560
Iteration 148/1000 | Loss: 0.00002560
Iteration 149/1000 | Loss: 0.00002560
Iteration 150/1000 | Loss: 0.00002560
Iteration 151/1000 | Loss: 0.00002560
Iteration 152/1000 | Loss: 0.00002560
Iteration 153/1000 | Loss: 0.00002560
Iteration 154/1000 | Loss: 0.00002560
Iteration 155/1000 | Loss: 0.00002560
Iteration 156/1000 | Loss: 0.00002560
Iteration 157/1000 | Loss: 0.00002560
Iteration 158/1000 | Loss: 0.00002559
Iteration 159/1000 | Loss: 0.00002559
Iteration 160/1000 | Loss: 0.00002559
Iteration 161/1000 | Loss: 0.00002559
Iteration 162/1000 | Loss: 0.00002559
Iteration 163/1000 | Loss: 0.00002558
Iteration 164/1000 | Loss: 0.00002558
Iteration 165/1000 | Loss: 0.00002558
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002557
Iteration 168/1000 | Loss: 0.00002557
Iteration 169/1000 | Loss: 0.00002557
Iteration 170/1000 | Loss: 0.00002557
Iteration 171/1000 | Loss: 0.00002556
Iteration 172/1000 | Loss: 0.00002556
Iteration 173/1000 | Loss: 0.00002556
Iteration 174/1000 | Loss: 0.00002556
Iteration 175/1000 | Loss: 0.00002556
Iteration 176/1000 | Loss: 0.00002555
Iteration 177/1000 | Loss: 0.00002555
Iteration 178/1000 | Loss: 0.00002555
Iteration 179/1000 | Loss: 0.00002555
Iteration 180/1000 | Loss: 0.00002555
Iteration 181/1000 | Loss: 0.00002555
Iteration 182/1000 | Loss: 0.00002555
Iteration 183/1000 | Loss: 0.00002555
Iteration 184/1000 | Loss: 0.00002554
Iteration 185/1000 | Loss: 0.00002554
Iteration 186/1000 | Loss: 0.00002554
Iteration 187/1000 | Loss: 0.00002554
Iteration 188/1000 | Loss: 0.00002554
Iteration 189/1000 | Loss: 0.00002554
Iteration 190/1000 | Loss: 0.00002554
Iteration 191/1000 | Loss: 0.00002554
Iteration 192/1000 | Loss: 0.00002554
Iteration 193/1000 | Loss: 0.00002554
Iteration 194/1000 | Loss: 0.00002554
Iteration 195/1000 | Loss: 0.00002554
Iteration 196/1000 | Loss: 0.00002554
Iteration 197/1000 | Loss: 0.00002554
Iteration 198/1000 | Loss: 0.00002554
Iteration 199/1000 | Loss: 0.00002554
Iteration 200/1000 | Loss: 0.00002554
Iteration 201/1000 | Loss: 0.00002554
Iteration 202/1000 | Loss: 0.00002554
Iteration 203/1000 | Loss: 0.00002554
Iteration 204/1000 | Loss: 0.00002554
Iteration 205/1000 | Loss: 0.00002554
Iteration 206/1000 | Loss: 0.00002554
Iteration 207/1000 | Loss: 0.00002554
Iteration 208/1000 | Loss: 0.00002554
Iteration 209/1000 | Loss: 0.00002554
Iteration 210/1000 | Loss: 0.00002554
Iteration 211/1000 | Loss: 0.00002554
Iteration 212/1000 | Loss: 0.00002554
Iteration 213/1000 | Loss: 0.00002554
Iteration 214/1000 | Loss: 0.00002554
Iteration 215/1000 | Loss: 0.00002554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.5540333808748983e-05, 2.5540333808748983e-05, 2.5540333808748983e-05, 2.5540333808748983e-05, 2.5540333808748983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5540333808748983e-05

Optimization complete. Final v2v error: 4.175968647003174 mm

Highest mean error: 5.465963840484619 mm for frame 228

Lowest mean error: 3.666156053543091 mm for frame 11

Saving results

Total time: 191.92898273468018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764690
Iteration 2/25 | Loss: 0.00110308
Iteration 3/25 | Loss: 0.00076234
Iteration 4/25 | Loss: 0.00071268
Iteration 5/25 | Loss: 0.00070181
Iteration 6/25 | Loss: 0.00069070
Iteration 7/25 | Loss: 0.00069915
Iteration 8/25 | Loss: 0.00066971
Iteration 9/25 | Loss: 0.00066405
Iteration 10/25 | Loss: 0.00066346
Iteration 11/25 | Loss: 0.00066331
Iteration 12/25 | Loss: 0.00066328
Iteration 13/25 | Loss: 0.00066328
Iteration 14/25 | Loss: 0.00066328
Iteration 15/25 | Loss: 0.00066327
Iteration 16/25 | Loss: 0.00066327
Iteration 17/25 | Loss: 0.00066327
Iteration 18/25 | Loss: 0.00066327
Iteration 19/25 | Loss: 0.00066327
Iteration 20/25 | Loss: 0.00066327
Iteration 21/25 | Loss: 0.00066327
Iteration 22/25 | Loss: 0.00066327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006632732111029327, 0.0006632732111029327, 0.0006632732111029327, 0.0006632732111029327, 0.0006632732111029327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006632732111029327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.59803867
Iteration 2/25 | Loss: 0.00029313
Iteration 3/25 | Loss: 0.00029298
Iteration 4/25 | Loss: 0.00029298
Iteration 5/25 | Loss: 0.00029298
Iteration 6/25 | Loss: 0.00029298
Iteration 7/25 | Loss: 0.00029297
Iteration 8/25 | Loss: 0.00029297
Iteration 9/25 | Loss: 0.00029297
Iteration 10/25 | Loss: 0.00029297
Iteration 11/25 | Loss: 0.00029297
Iteration 12/25 | Loss: 0.00029297
Iteration 13/25 | Loss: 0.00029297
Iteration 14/25 | Loss: 0.00029297
Iteration 15/25 | Loss: 0.00029297
Iteration 16/25 | Loss: 0.00029297
Iteration 17/25 | Loss: 0.00029297
Iteration 18/25 | Loss: 0.00029297
Iteration 19/25 | Loss: 0.00029297
Iteration 20/25 | Loss: 0.00029297
Iteration 21/25 | Loss: 0.00029297
Iteration 22/25 | Loss: 0.00029297
Iteration 23/25 | Loss: 0.00029297
Iteration 24/25 | Loss: 0.00029297
Iteration 25/25 | Loss: 0.00029297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029297
Iteration 2/1000 | Loss: 0.00002324
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00001488
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001410
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001336
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001335
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001331
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001330
Iteration 54/1000 | Loss: 0.00001330
Iteration 55/1000 | Loss: 0.00001330
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001329
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001329
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001327
Iteration 73/1000 | Loss: 0.00001327
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001326
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001318
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3182568181946408e-05, 1.3182568181946408e-05, 1.3182568181946408e-05, 1.3182568181946408e-05, 1.3182568181946408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3182568181946408e-05

Optimization complete. Final v2v error: 3.092878580093384 mm

Highest mean error: 3.923673152923584 mm for frame 26

Lowest mean error: 2.6174938678741455 mm for frame 131

Saving results

Total time: 50.55956292152405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804736
Iteration 2/25 | Loss: 0.00091601
Iteration 3/25 | Loss: 0.00070932
Iteration 4/25 | Loss: 0.00066463
Iteration 5/25 | Loss: 0.00065702
Iteration 6/25 | Loss: 0.00065622
Iteration 7/25 | Loss: 0.00065622
Iteration 8/25 | Loss: 0.00065622
Iteration 9/25 | Loss: 0.00065622
Iteration 10/25 | Loss: 0.00065622
Iteration 11/25 | Loss: 0.00065622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006562185008078814, 0.0006562185008078814, 0.0006562185008078814, 0.0006562185008078814, 0.0006562185008078814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006562185008078814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43221116
Iteration 2/25 | Loss: 0.00021383
Iteration 3/25 | Loss: 0.00021383
Iteration 4/25 | Loss: 0.00021383
Iteration 5/25 | Loss: 0.00021383
Iteration 6/25 | Loss: 0.00021383
Iteration 7/25 | Loss: 0.00021383
Iteration 8/25 | Loss: 0.00021383
Iteration 9/25 | Loss: 0.00021383
Iteration 10/25 | Loss: 0.00021383
Iteration 11/25 | Loss: 0.00021383
Iteration 12/25 | Loss: 0.00021383
Iteration 13/25 | Loss: 0.00021383
Iteration 14/25 | Loss: 0.00021383
Iteration 15/25 | Loss: 0.00021383
Iteration 16/25 | Loss: 0.00021383
Iteration 17/25 | Loss: 0.00021383
Iteration 18/25 | Loss: 0.00021383
Iteration 19/25 | Loss: 0.00021383
Iteration 20/25 | Loss: 0.00021383
Iteration 21/25 | Loss: 0.00021383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00021383001876529306, 0.00021383001876529306, 0.00021383001876529306, 0.00021383001876529306, 0.00021383001876529306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021383001876529306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021383
Iteration 2/1000 | Loss: 0.00003463
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002062
Iteration 6/1000 | Loss: 0.00001980
Iteration 7/1000 | Loss: 0.00001928
Iteration 8/1000 | Loss: 0.00001859
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001817
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001802
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001790
Iteration 15/1000 | Loss: 0.00001789
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001780
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001770
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001768
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001766
Iteration 33/1000 | Loss: 0.00001766
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001765
Iteration 39/1000 | Loss: 0.00001764
Iteration 40/1000 | Loss: 0.00001764
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001762
Iteration 59/1000 | Loss: 0.00001762
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001761
Iteration 70/1000 | Loss: 0.00001761
Iteration 71/1000 | Loss: 0.00001761
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001761
Iteration 82/1000 | Loss: 0.00001761
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001760
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001760
Iteration 88/1000 | Loss: 0.00001760
Iteration 89/1000 | Loss: 0.00001760
Iteration 90/1000 | Loss: 0.00001760
Iteration 91/1000 | Loss: 0.00001760
Iteration 92/1000 | Loss: 0.00001760
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00001759
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001759
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001759
Iteration 103/1000 | Loss: 0.00001758
Iteration 104/1000 | Loss: 0.00001758
Iteration 105/1000 | Loss: 0.00001758
Iteration 106/1000 | Loss: 0.00001758
Iteration 107/1000 | Loss: 0.00001758
Iteration 108/1000 | Loss: 0.00001758
Iteration 109/1000 | Loss: 0.00001758
Iteration 110/1000 | Loss: 0.00001758
Iteration 111/1000 | Loss: 0.00001758
Iteration 112/1000 | Loss: 0.00001758
Iteration 113/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.7583666704013012e-05, 1.7583666704013012e-05, 1.7583666704013012e-05, 1.7583666704013012e-05, 1.7583666704013012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7583666704013012e-05

Optimization complete. Final v2v error: 3.5413849353790283 mm

Highest mean error: 3.8590550422668457 mm for frame 220

Lowest mean error: 3.269289255142212 mm for frame 60

Saving results

Total time: 37.642462968826294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759810
Iteration 2/25 | Loss: 0.00128390
Iteration 3/25 | Loss: 0.00080673
Iteration 4/25 | Loss: 0.00074166
Iteration 5/25 | Loss: 0.00073558
Iteration 6/25 | Loss: 0.00072892
Iteration 7/25 | Loss: 0.00073918
Iteration 8/25 | Loss: 0.00072903
Iteration 9/25 | Loss: 0.00072653
Iteration 10/25 | Loss: 0.00072049
Iteration 11/25 | Loss: 0.00072454
Iteration 12/25 | Loss: 0.00072429
Iteration 13/25 | Loss: 0.00071997
Iteration 14/25 | Loss: 0.00071996
Iteration 15/25 | Loss: 0.00071996
Iteration 16/25 | Loss: 0.00071995
Iteration 17/25 | Loss: 0.00071995
Iteration 18/25 | Loss: 0.00071995
Iteration 19/25 | Loss: 0.00071995
Iteration 20/25 | Loss: 0.00071995
Iteration 21/25 | Loss: 0.00071995
Iteration 22/25 | Loss: 0.00071995
Iteration 23/25 | Loss: 0.00071995
Iteration 24/25 | Loss: 0.00071995
Iteration 25/25 | Loss: 0.00071995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09605074
Iteration 2/25 | Loss: 0.00040112
Iteration 3/25 | Loss: 0.00040109
Iteration 4/25 | Loss: 0.00040109
Iteration 5/25 | Loss: 0.00040109
Iteration 6/25 | Loss: 0.00040109
Iteration 7/25 | Loss: 0.00040109
Iteration 8/25 | Loss: 0.00040109
Iteration 9/25 | Loss: 0.00040109
Iteration 10/25 | Loss: 0.00040109
Iteration 11/25 | Loss: 0.00040109
Iteration 12/25 | Loss: 0.00040109
Iteration 13/25 | Loss: 0.00040109
Iteration 14/25 | Loss: 0.00040109
Iteration 15/25 | Loss: 0.00040109
Iteration 16/25 | Loss: 0.00040109
Iteration 17/25 | Loss: 0.00040109
Iteration 18/25 | Loss: 0.00040109
Iteration 19/25 | Loss: 0.00040109
Iteration 20/25 | Loss: 0.00040109
Iteration 21/25 | Loss: 0.00040109
Iteration 22/25 | Loss: 0.00040109
Iteration 23/25 | Loss: 0.00040109
Iteration 24/25 | Loss: 0.00040109
Iteration 25/25 | Loss: 0.00040109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040109
Iteration 2/1000 | Loss: 0.00009073
Iteration 3/1000 | Loss: 0.00009516
Iteration 4/1000 | Loss: 0.00002199
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00008218
Iteration 10/1000 | Loss: 0.00002501
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00001921
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001899
Iteration 20/1000 | Loss: 0.00001899
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001893
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001890
Iteration 37/1000 | Loss: 0.00001890
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001887
Iteration 49/1000 | Loss: 0.00001887
Iteration 50/1000 | Loss: 0.00001887
Iteration 51/1000 | Loss: 0.00001887
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001886
Iteration 54/1000 | Loss: 0.00001886
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001885
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001885
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001884
Iteration 66/1000 | Loss: 0.00001884
Iteration 67/1000 | Loss: 0.00001884
Iteration 68/1000 | Loss: 0.00001884
Iteration 69/1000 | Loss: 0.00001884
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001882
Iteration 90/1000 | Loss: 0.00001882
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001882
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001881
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001880
Iteration 113/1000 | Loss: 0.00001880
Iteration 114/1000 | Loss: 0.00001880
Iteration 115/1000 | Loss: 0.00001880
Iteration 116/1000 | Loss: 0.00001880
Iteration 117/1000 | Loss: 0.00001880
Iteration 118/1000 | Loss: 0.00001880
Iteration 119/1000 | Loss: 0.00001880
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001880
Iteration 122/1000 | Loss: 0.00001880
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.8798145902110264e-05, 1.8798145902110264e-05, 1.8798145902110264e-05, 1.8798145902110264e-05, 1.8798145902110264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8798145902110264e-05

Optimization complete. Final v2v error: 3.667860269546509 mm

Highest mean error: 5.968824863433838 mm for frame 202

Lowest mean error: 3.2900545597076416 mm for frame 215

Saving results

Total time: 56.1279673576355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843210
Iteration 2/25 | Loss: 0.00098466
Iteration 3/25 | Loss: 0.00071316
Iteration 4/25 | Loss: 0.00068792
Iteration 5/25 | Loss: 0.00067791
Iteration 6/25 | Loss: 0.00067550
Iteration 7/25 | Loss: 0.00067510
Iteration 8/25 | Loss: 0.00067510
Iteration 9/25 | Loss: 0.00067510
Iteration 10/25 | Loss: 0.00067510
Iteration 11/25 | Loss: 0.00067510
Iteration 12/25 | Loss: 0.00067510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006750999600626528, 0.0006750999600626528, 0.0006750999600626528, 0.0006750999600626528, 0.0006750999600626528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006750999600626528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.44989204
Iteration 2/25 | Loss: 0.00031052
Iteration 3/25 | Loss: 0.00031046
Iteration 4/25 | Loss: 0.00031046
Iteration 5/25 | Loss: 0.00031046
Iteration 6/25 | Loss: 0.00031046
Iteration 7/25 | Loss: 0.00031046
Iteration 8/25 | Loss: 0.00031046
Iteration 9/25 | Loss: 0.00031046
Iteration 10/25 | Loss: 0.00031046
Iteration 11/25 | Loss: 0.00031046
Iteration 12/25 | Loss: 0.00031046
Iteration 13/25 | Loss: 0.00031046
Iteration 14/25 | Loss: 0.00031046
Iteration 15/25 | Loss: 0.00031046
Iteration 16/25 | Loss: 0.00031046
Iteration 17/25 | Loss: 0.00031046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031045617652125657, 0.00031045617652125657, 0.00031045617652125657, 0.00031045617652125657, 0.00031045617652125657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031045617652125657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031046
Iteration 2/1000 | Loss: 0.00003157
Iteration 3/1000 | Loss: 0.00002439
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002107
Iteration 7/1000 | Loss: 0.00002050
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001954
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001950
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001941
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001939
Iteration 32/1000 | Loss: 0.00001939
Iteration 33/1000 | Loss: 0.00001938
Iteration 34/1000 | Loss: 0.00001938
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001937
Iteration 43/1000 | Loss: 0.00001937
Iteration 44/1000 | Loss: 0.00001936
Iteration 45/1000 | Loss: 0.00001936
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001935
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00001935
Iteration 50/1000 | Loss: 0.00001934
Iteration 51/1000 | Loss: 0.00001934
Iteration 52/1000 | Loss: 0.00001934
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001933
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001931
Iteration 60/1000 | Loss: 0.00001931
Iteration 61/1000 | Loss: 0.00001931
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001930
Iteration 64/1000 | Loss: 0.00001930
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001929
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001929
Iteration 74/1000 | Loss: 0.00001929
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.9289329429739155e-05, 1.9289329429739155e-05, 1.9289329429739155e-05, 1.9289329429739155e-05, 1.9289329429739155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9289329429739155e-05

Optimization complete. Final v2v error: 3.7009778022766113 mm

Highest mean error: 4.5572710037231445 mm for frame 65

Lowest mean error: 3.214362144470215 mm for frame 173

Saving results

Total time: 35.87516498565674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451280
Iteration 2/25 | Loss: 0.00080832
Iteration 3/25 | Loss: 0.00065552
Iteration 4/25 | Loss: 0.00062693
Iteration 5/25 | Loss: 0.00061972
Iteration 6/25 | Loss: 0.00061828
Iteration 7/25 | Loss: 0.00061820
Iteration 8/25 | Loss: 0.00061820
Iteration 9/25 | Loss: 0.00061820
Iteration 10/25 | Loss: 0.00061820
Iteration 11/25 | Loss: 0.00061820
Iteration 12/25 | Loss: 0.00061820
Iteration 13/25 | Loss: 0.00061820
Iteration 14/25 | Loss: 0.00061820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006181989447213709, 0.0006181989447213709, 0.0006181989447213709, 0.0006181989447213709, 0.0006181989447213709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006181989447213709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81731081
Iteration 2/25 | Loss: 0.00025677
Iteration 3/25 | Loss: 0.00025675
Iteration 4/25 | Loss: 0.00025675
Iteration 5/25 | Loss: 0.00025675
Iteration 6/25 | Loss: 0.00025675
Iteration 7/25 | Loss: 0.00025675
Iteration 8/25 | Loss: 0.00025675
Iteration 9/25 | Loss: 0.00025675
Iteration 10/25 | Loss: 0.00025675
Iteration 11/25 | Loss: 0.00025675
Iteration 12/25 | Loss: 0.00025675
Iteration 13/25 | Loss: 0.00025675
Iteration 14/25 | Loss: 0.00025675
Iteration 15/25 | Loss: 0.00025675
Iteration 16/25 | Loss: 0.00025675
Iteration 17/25 | Loss: 0.00025675
Iteration 18/25 | Loss: 0.00025675
Iteration 19/25 | Loss: 0.00025675
Iteration 20/25 | Loss: 0.00025675
Iteration 21/25 | Loss: 0.00025675
Iteration 22/25 | Loss: 0.00025675
Iteration 23/25 | Loss: 0.00025675
Iteration 24/25 | Loss: 0.00025675
Iteration 25/25 | Loss: 0.00025675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025675
Iteration 2/1000 | Loss: 0.00002469
Iteration 3/1000 | Loss: 0.00001569
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001331
Iteration 6/1000 | Loss: 0.00001303
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001248
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001207
Iteration 19/1000 | Loss: 0.00001205
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001188
Iteration 107/1000 | Loss: 0.00001188
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001182
Iteration 142/1000 | Loss: 0.00001182
Iteration 143/1000 | Loss: 0.00001182
Iteration 144/1000 | Loss: 0.00001182
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001181
Iteration 149/1000 | Loss: 0.00001181
Iteration 150/1000 | Loss: 0.00001181
Iteration 151/1000 | Loss: 0.00001181
Iteration 152/1000 | Loss: 0.00001181
Iteration 153/1000 | Loss: 0.00001181
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001181
Iteration 156/1000 | Loss: 0.00001181
Iteration 157/1000 | Loss: 0.00001181
Iteration 158/1000 | Loss: 0.00001181
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001181
Iteration 162/1000 | Loss: 0.00001181
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001181
Iteration 169/1000 | Loss: 0.00001181
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001181
Iteration 172/1000 | Loss: 0.00001181
Iteration 173/1000 | Loss: 0.00001181
Iteration 174/1000 | Loss: 0.00001181
Iteration 175/1000 | Loss: 0.00001181
Iteration 176/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.1808274393843021e-05, 1.1808274393843021e-05, 1.1808274393843021e-05, 1.1808274393843021e-05, 1.1808274393843021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1808274393843021e-05

Optimization complete. Final v2v error: 2.9272210597991943 mm

Highest mean error: 3.2620577812194824 mm for frame 131

Lowest mean error: 2.6989352703094482 mm for frame 158

Saving results

Total time: 39.42270612716675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849622
Iteration 2/25 | Loss: 0.00112370
Iteration 3/25 | Loss: 0.00079469
Iteration 4/25 | Loss: 0.00072821
Iteration 5/25 | Loss: 0.00071411
Iteration 6/25 | Loss: 0.00069996
Iteration 7/25 | Loss: 0.00069505
Iteration 8/25 | Loss: 0.00069369
Iteration 9/25 | Loss: 0.00069267
Iteration 10/25 | Loss: 0.00069190
Iteration 11/25 | Loss: 0.00069160
Iteration 12/25 | Loss: 0.00069142
Iteration 13/25 | Loss: 0.00069129
Iteration 14/25 | Loss: 0.00069117
Iteration 15/25 | Loss: 0.00069657
Iteration 16/25 | Loss: 0.00068759
Iteration 17/25 | Loss: 0.00068566
Iteration 18/25 | Loss: 0.00068487
Iteration 19/25 | Loss: 0.00068475
Iteration 20/25 | Loss: 0.00068474
Iteration 21/25 | Loss: 0.00068474
Iteration 22/25 | Loss: 0.00068474
Iteration 23/25 | Loss: 0.00068474
Iteration 24/25 | Loss: 0.00068470
Iteration 25/25 | Loss: 0.00068470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03080916
Iteration 2/25 | Loss: 0.00034732
Iteration 3/25 | Loss: 0.00034732
Iteration 4/25 | Loss: 0.00034731
Iteration 5/25 | Loss: 0.00034731
Iteration 6/25 | Loss: 0.00034731
Iteration 7/25 | Loss: 0.00034731
Iteration 8/25 | Loss: 0.00034731
Iteration 9/25 | Loss: 0.00034731
Iteration 10/25 | Loss: 0.00034731
Iteration 11/25 | Loss: 0.00034731
Iteration 12/25 | Loss: 0.00034731
Iteration 13/25 | Loss: 0.00034731
Iteration 14/25 | Loss: 0.00034731
Iteration 15/25 | Loss: 0.00034731
Iteration 16/25 | Loss: 0.00034731
Iteration 17/25 | Loss: 0.00034731
Iteration 18/25 | Loss: 0.00034731
Iteration 19/25 | Loss: 0.00034731
Iteration 20/25 | Loss: 0.00034731
Iteration 21/25 | Loss: 0.00034731
Iteration 22/25 | Loss: 0.00034731
Iteration 23/25 | Loss: 0.00034731
Iteration 24/25 | Loss: 0.00034731
Iteration 25/25 | Loss: 0.00034731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034731
Iteration 2/1000 | Loss: 0.00003989
Iteration 3/1000 | Loss: 0.00002416
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001735
Iteration 8/1000 | Loss: 0.00001693
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001660
Iteration 12/1000 | Loss: 0.00001652
Iteration 13/1000 | Loss: 0.00001648
Iteration 14/1000 | Loss: 0.00001647
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001632
Iteration 19/1000 | Loss: 0.00001632
Iteration 20/1000 | Loss: 0.00001626
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001608
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001608
Iteration 35/1000 | Loss: 0.00001606
Iteration 36/1000 | Loss: 0.00001606
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001603
Iteration 43/1000 | Loss: 0.00001603
Iteration 44/1000 | Loss: 0.00001603
Iteration 45/1000 | Loss: 0.00001603
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001598
Iteration 55/1000 | Loss: 0.00001598
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001597
Iteration 58/1000 | Loss: 0.00001597
Iteration 59/1000 | Loss: 0.00001597
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001596
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001595
Iteration 65/1000 | Loss: 0.00001595
Iteration 66/1000 | Loss: 0.00001595
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001594
Iteration 69/1000 | Loss: 0.00001594
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001593
Iteration 74/1000 | Loss: 0.00001593
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00001592
Iteration 77/1000 | Loss: 0.00001592
Iteration 78/1000 | Loss: 0.00001592
Iteration 79/1000 | Loss: 0.00001592
Iteration 80/1000 | Loss: 0.00001592
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001592
Iteration 83/1000 | Loss: 0.00001592
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00001591
Iteration 86/1000 | Loss: 0.00001591
Iteration 87/1000 | Loss: 0.00001591
Iteration 88/1000 | Loss: 0.00001591
Iteration 89/1000 | Loss: 0.00001590
Iteration 90/1000 | Loss: 0.00001590
Iteration 91/1000 | Loss: 0.00001590
Iteration 92/1000 | Loss: 0.00001590
Iteration 93/1000 | Loss: 0.00001590
Iteration 94/1000 | Loss: 0.00001590
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001590
Iteration 97/1000 | Loss: 0.00001590
Iteration 98/1000 | Loss: 0.00001590
Iteration 99/1000 | Loss: 0.00001589
Iteration 100/1000 | Loss: 0.00001589
Iteration 101/1000 | Loss: 0.00001589
Iteration 102/1000 | Loss: 0.00001589
Iteration 103/1000 | Loss: 0.00001589
Iteration 104/1000 | Loss: 0.00001589
Iteration 105/1000 | Loss: 0.00001589
Iteration 106/1000 | Loss: 0.00001589
Iteration 107/1000 | Loss: 0.00001588
Iteration 108/1000 | Loss: 0.00001588
Iteration 109/1000 | Loss: 0.00001588
Iteration 110/1000 | Loss: 0.00001588
Iteration 111/1000 | Loss: 0.00001588
Iteration 112/1000 | Loss: 0.00001588
Iteration 113/1000 | Loss: 0.00001587
Iteration 114/1000 | Loss: 0.00001587
Iteration 115/1000 | Loss: 0.00001587
Iteration 116/1000 | Loss: 0.00001587
Iteration 117/1000 | Loss: 0.00001587
Iteration 118/1000 | Loss: 0.00001587
Iteration 119/1000 | Loss: 0.00001587
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001586
Iteration 122/1000 | Loss: 0.00001586
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001586
Iteration 125/1000 | Loss: 0.00001586
Iteration 126/1000 | Loss: 0.00001586
Iteration 127/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5862970030866563e-05, 1.5862970030866563e-05, 1.5862970030866563e-05, 1.5862970030866563e-05, 1.5862970030866563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5862970030866563e-05

Optimization complete. Final v2v error: 3.3586513996124268 mm

Highest mean error: 4.203802585601807 mm for frame 82

Lowest mean error: 2.8675992488861084 mm for frame 126

Saving results

Total time: 61.36805605888367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967360
Iteration 2/25 | Loss: 0.00270322
Iteration 3/25 | Loss: 0.00155821
Iteration 4/25 | Loss: 0.00114178
Iteration 5/25 | Loss: 0.00105443
Iteration 6/25 | Loss: 0.00105811
Iteration 7/25 | Loss: 0.00107510
Iteration 8/25 | Loss: 0.00103596
Iteration 9/25 | Loss: 0.00100420
Iteration 10/25 | Loss: 0.00099578
Iteration 11/25 | Loss: 0.00100282
Iteration 12/25 | Loss: 0.00101136
Iteration 13/25 | Loss: 0.00099745
Iteration 14/25 | Loss: 0.00098702
Iteration 15/25 | Loss: 0.00099014
Iteration 16/25 | Loss: 0.00097484
Iteration 17/25 | Loss: 0.00097862
Iteration 18/25 | Loss: 0.00097337
Iteration 19/25 | Loss: 0.00096820
Iteration 20/25 | Loss: 0.00096317
Iteration 21/25 | Loss: 0.00096355
Iteration 22/25 | Loss: 0.00098564
Iteration 23/25 | Loss: 0.00097996
Iteration 24/25 | Loss: 0.00097673
Iteration 25/25 | Loss: 0.00097121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82983899
Iteration 2/25 | Loss: 0.00330145
Iteration 3/25 | Loss: 0.00325103
Iteration 4/25 | Loss: 0.00325103
Iteration 5/25 | Loss: 0.00325103
Iteration 6/25 | Loss: 0.00325103
Iteration 7/25 | Loss: 0.00325103
Iteration 8/25 | Loss: 0.00325102
Iteration 9/25 | Loss: 0.00325102
Iteration 10/25 | Loss: 0.00325102
Iteration 11/25 | Loss: 0.00325102
Iteration 12/25 | Loss: 0.00325102
Iteration 13/25 | Loss: 0.00325102
Iteration 14/25 | Loss: 0.00325102
Iteration 15/25 | Loss: 0.00325102
Iteration 16/25 | Loss: 0.00325102
Iteration 17/25 | Loss: 0.00325102
Iteration 18/25 | Loss: 0.00325102
Iteration 19/25 | Loss: 0.00325102
Iteration 20/25 | Loss: 0.00325102
Iteration 21/25 | Loss: 0.00325102
Iteration 22/25 | Loss: 0.00325102
Iteration 23/25 | Loss: 0.00325102
Iteration 24/25 | Loss: 0.00325102
Iteration 25/25 | Loss: 0.00325102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325102
Iteration 2/1000 | Loss: 0.00077321
Iteration 3/1000 | Loss: 0.00072883
Iteration 4/1000 | Loss: 0.00097669
Iteration 5/1000 | Loss: 0.00404916
Iteration 6/1000 | Loss: 0.00076369
Iteration 7/1000 | Loss: 0.00147377
Iteration 8/1000 | Loss: 0.00195058
Iteration 9/1000 | Loss: 0.00479597
Iteration 10/1000 | Loss: 0.00216378
Iteration 11/1000 | Loss: 0.00091955
Iteration 12/1000 | Loss: 0.00032839
Iteration 13/1000 | Loss: 0.00138426
Iteration 14/1000 | Loss: 0.00048342
Iteration 15/1000 | Loss: 0.00133996
Iteration 16/1000 | Loss: 0.00058600
Iteration 17/1000 | Loss: 0.00062060
Iteration 18/1000 | Loss: 0.00109042
Iteration 19/1000 | Loss: 0.00130203
Iteration 20/1000 | Loss: 0.00030743
Iteration 21/1000 | Loss: 0.00018223
Iteration 22/1000 | Loss: 0.00012786
Iteration 23/1000 | Loss: 0.00065018
Iteration 24/1000 | Loss: 0.00228505
Iteration 25/1000 | Loss: 0.00100923
Iteration 26/1000 | Loss: 0.00055460
Iteration 27/1000 | Loss: 0.00011291
Iteration 28/1000 | Loss: 0.00029076
Iteration 29/1000 | Loss: 0.00043325
Iteration 30/1000 | Loss: 0.00036312
Iteration 31/1000 | Loss: 0.00029459
Iteration 32/1000 | Loss: 0.00090488
Iteration 33/1000 | Loss: 0.00061342
Iteration 34/1000 | Loss: 0.00062923
Iteration 35/1000 | Loss: 0.00079037
Iteration 36/1000 | Loss: 0.00095794
Iteration 37/1000 | Loss: 0.00023580
Iteration 38/1000 | Loss: 0.00014952
Iteration 39/1000 | Loss: 0.00020474
Iteration 40/1000 | Loss: 0.00066748
Iteration 41/1000 | Loss: 0.00014738
Iteration 42/1000 | Loss: 0.00029932
Iteration 43/1000 | Loss: 0.00033830
Iteration 44/1000 | Loss: 0.00018384
Iteration 45/1000 | Loss: 0.00016119
Iteration 46/1000 | Loss: 0.00022791
Iteration 47/1000 | Loss: 0.00026053
Iteration 48/1000 | Loss: 0.00020552
Iteration 49/1000 | Loss: 0.00023457
Iteration 50/1000 | Loss: 0.00039050
Iteration 51/1000 | Loss: 0.00053095
Iteration 52/1000 | Loss: 0.00050992
Iteration 53/1000 | Loss: 0.00057430
Iteration 54/1000 | Loss: 0.00026171
Iteration 55/1000 | Loss: 0.00050464
Iteration 56/1000 | Loss: 0.00034203
Iteration 57/1000 | Loss: 0.00072519
Iteration 58/1000 | Loss: 0.00038374
Iteration 59/1000 | Loss: 0.00050041
Iteration 60/1000 | Loss: 0.00010617
Iteration 61/1000 | Loss: 0.00008503
Iteration 62/1000 | Loss: 0.00008486
Iteration 63/1000 | Loss: 0.00008088
Iteration 64/1000 | Loss: 0.00008263
Iteration 65/1000 | Loss: 0.00006141
Iteration 66/1000 | Loss: 0.00010792
Iteration 67/1000 | Loss: 0.00007705
Iteration 68/1000 | Loss: 0.00006039
Iteration 69/1000 | Loss: 0.00079755
Iteration 70/1000 | Loss: 0.00106105
Iteration 71/1000 | Loss: 0.00025224
Iteration 72/1000 | Loss: 0.00020276
Iteration 73/1000 | Loss: 0.00006250
Iteration 74/1000 | Loss: 0.00005608
Iteration 75/1000 | Loss: 0.00008108
Iteration 76/1000 | Loss: 0.00006773
Iteration 77/1000 | Loss: 0.00026958
Iteration 78/1000 | Loss: 0.00026696
Iteration 79/1000 | Loss: 0.00046214
Iteration 80/1000 | Loss: 0.00024996
Iteration 81/1000 | Loss: 0.00046484
Iteration 82/1000 | Loss: 0.00006581
Iteration 83/1000 | Loss: 0.00006534
Iteration 84/1000 | Loss: 0.00006319
Iteration 85/1000 | Loss: 0.00007028
Iteration 86/1000 | Loss: 0.00004511
Iteration 87/1000 | Loss: 0.00004307
Iteration 88/1000 | Loss: 0.00032448
Iteration 89/1000 | Loss: 0.00039333
Iteration 90/1000 | Loss: 0.00047880
Iteration 91/1000 | Loss: 0.00039422
Iteration 92/1000 | Loss: 0.00025943
Iteration 93/1000 | Loss: 0.00021982
Iteration 94/1000 | Loss: 0.00024708
Iteration 95/1000 | Loss: 0.00019485
Iteration 96/1000 | Loss: 0.00006030
Iteration 97/1000 | Loss: 0.00022356
Iteration 98/1000 | Loss: 0.00021988
Iteration 99/1000 | Loss: 0.00015004
Iteration 100/1000 | Loss: 0.00020559
Iteration 101/1000 | Loss: 0.00013119
Iteration 102/1000 | Loss: 0.00018565
Iteration 103/1000 | Loss: 0.00025739
Iteration 104/1000 | Loss: 0.00038653
Iteration 105/1000 | Loss: 0.00022212
Iteration 106/1000 | Loss: 0.00016266
Iteration 107/1000 | Loss: 0.00005920
Iteration 108/1000 | Loss: 0.00004746
Iteration 109/1000 | Loss: 0.00006327
Iteration 110/1000 | Loss: 0.00005918
Iteration 111/1000 | Loss: 0.00006158
Iteration 112/1000 | Loss: 0.00006359
Iteration 113/1000 | Loss: 0.00005262
Iteration 114/1000 | Loss: 0.00018468
Iteration 115/1000 | Loss: 0.00013493
Iteration 116/1000 | Loss: 0.00004104
Iteration 117/1000 | Loss: 0.00005001
Iteration 118/1000 | Loss: 0.00005652
Iteration 119/1000 | Loss: 0.00004778
Iteration 120/1000 | Loss: 0.00006304
Iteration 121/1000 | Loss: 0.00005968
Iteration 122/1000 | Loss: 0.00005280
Iteration 123/1000 | Loss: 0.00005385
Iteration 124/1000 | Loss: 0.00005004
Iteration 125/1000 | Loss: 0.00004353
Iteration 126/1000 | Loss: 0.00006602
Iteration 127/1000 | Loss: 0.00005376
Iteration 128/1000 | Loss: 0.00006394
Iteration 129/1000 | Loss: 0.00006574
Iteration 130/1000 | Loss: 0.00016970
Iteration 131/1000 | Loss: 0.00013252
Iteration 132/1000 | Loss: 0.00041389
Iteration 133/1000 | Loss: 0.00038453
Iteration 134/1000 | Loss: 0.00035246
Iteration 135/1000 | Loss: 0.00005661
Iteration 136/1000 | Loss: 0.00005505
Iteration 137/1000 | Loss: 0.00006058
Iteration 138/1000 | Loss: 0.00006334
Iteration 139/1000 | Loss: 0.00006417
Iteration 140/1000 | Loss: 0.00006366
Iteration 141/1000 | Loss: 0.00004646
Iteration 142/1000 | Loss: 0.00024443
Iteration 143/1000 | Loss: 0.00018734
Iteration 144/1000 | Loss: 0.00005039
Iteration 145/1000 | Loss: 0.00006136
Iteration 146/1000 | Loss: 0.00004983
Iteration 147/1000 | Loss: 0.00006302
Iteration 148/1000 | Loss: 0.00041397
Iteration 149/1000 | Loss: 0.00058541
Iteration 150/1000 | Loss: 0.00050226
Iteration 151/1000 | Loss: 0.00041984
Iteration 152/1000 | Loss: 0.00042727
Iteration 153/1000 | Loss: 0.00039590
Iteration 154/1000 | Loss: 0.00034160
Iteration 155/1000 | Loss: 0.00043506
Iteration 156/1000 | Loss: 0.00032386
Iteration 157/1000 | Loss: 0.00040736
Iteration 158/1000 | Loss: 0.00015737
Iteration 159/1000 | Loss: 0.00006854
Iteration 160/1000 | Loss: 0.00005830
Iteration 161/1000 | Loss: 0.00004402
Iteration 162/1000 | Loss: 0.00032308
Iteration 163/1000 | Loss: 0.00046757
Iteration 164/1000 | Loss: 0.00071509
Iteration 165/1000 | Loss: 0.00040363
Iteration 166/1000 | Loss: 0.00011127
Iteration 167/1000 | Loss: 0.00081457
Iteration 168/1000 | Loss: 0.00018237
Iteration 169/1000 | Loss: 0.00021388
Iteration 170/1000 | Loss: 0.00017725
Iteration 171/1000 | Loss: 0.00019357
Iteration 172/1000 | Loss: 0.00018504
Iteration 173/1000 | Loss: 0.00005725
Iteration 174/1000 | Loss: 0.00005214
Iteration 175/1000 | Loss: 0.00005909
Iteration 176/1000 | Loss: 0.00005505
Iteration 177/1000 | Loss: 0.00005036
Iteration 178/1000 | Loss: 0.00004631
Iteration 179/1000 | Loss: 0.00005604
Iteration 180/1000 | Loss: 0.00004952
Iteration 181/1000 | Loss: 0.00042457
Iteration 182/1000 | Loss: 0.00052623
Iteration 183/1000 | Loss: 0.00044759
Iteration 184/1000 | Loss: 0.00063388
Iteration 185/1000 | Loss: 0.00007056
Iteration 186/1000 | Loss: 0.00006216
Iteration 187/1000 | Loss: 0.00006419
Iteration 188/1000 | Loss: 0.00007254
Iteration 189/1000 | Loss: 0.00005716
Iteration 190/1000 | Loss: 0.00005530
Iteration 191/1000 | Loss: 0.00005726
Iteration 192/1000 | Loss: 0.00005311
Iteration 193/1000 | Loss: 0.00004351
Iteration 194/1000 | Loss: 0.00005373
Iteration 195/1000 | Loss: 0.00005355
Iteration 196/1000 | Loss: 0.00005483
Iteration 197/1000 | Loss: 0.00004128
Iteration 198/1000 | Loss: 0.00004946
Iteration 199/1000 | Loss: 0.00005336
Iteration 200/1000 | Loss: 0.00004873
Iteration 201/1000 | Loss: 0.00005550
Iteration 202/1000 | Loss: 0.00004808
Iteration 203/1000 | Loss: 0.00003959
Iteration 204/1000 | Loss: 0.00004166
Iteration 205/1000 | Loss: 0.00005121
Iteration 206/1000 | Loss: 0.00004598
Iteration 207/1000 | Loss: 0.00019829
Iteration 208/1000 | Loss: 0.00030475
Iteration 209/1000 | Loss: 0.00006689
Iteration 210/1000 | Loss: 0.00006641
Iteration 211/1000 | Loss: 0.00004405
Iteration 212/1000 | Loss: 0.00005554
Iteration 213/1000 | Loss: 0.00005852
Iteration 214/1000 | Loss: 0.00005288
Iteration 215/1000 | Loss: 0.00024262
Iteration 216/1000 | Loss: 0.00016456
Iteration 217/1000 | Loss: 0.00005477
Iteration 218/1000 | Loss: 0.00022318
Iteration 219/1000 | Loss: 0.00012486
Iteration 220/1000 | Loss: 0.00019076
Iteration 221/1000 | Loss: 0.00024239
Iteration 222/1000 | Loss: 0.00021775
Iteration 223/1000 | Loss: 0.00023016
Iteration 224/1000 | Loss: 0.00009205
Iteration 225/1000 | Loss: 0.00004865
Iteration 226/1000 | Loss: 0.00020878
Iteration 227/1000 | Loss: 0.00019711
Iteration 228/1000 | Loss: 0.00009245
Iteration 229/1000 | Loss: 0.00018458
Iteration 230/1000 | Loss: 0.00016779
Iteration 231/1000 | Loss: 0.00008498
Iteration 232/1000 | Loss: 0.00013210
Iteration 233/1000 | Loss: 0.00016056
Iteration 234/1000 | Loss: 0.00011857
Iteration 235/1000 | Loss: 0.00017981
Iteration 236/1000 | Loss: 0.00031891
Iteration 237/1000 | Loss: 0.00039876
Iteration 238/1000 | Loss: 0.00022333
Iteration 239/1000 | Loss: 0.00011152
Iteration 240/1000 | Loss: 0.00021531
Iteration 241/1000 | Loss: 0.00011506
Iteration 242/1000 | Loss: 0.00009647
Iteration 243/1000 | Loss: 0.00008493
Iteration 244/1000 | Loss: 0.00013584
Iteration 245/1000 | Loss: 0.00011780
Iteration 246/1000 | Loss: 0.00016036
Iteration 247/1000 | Loss: 0.00005740
Iteration 248/1000 | Loss: 0.00032584
Iteration 249/1000 | Loss: 0.00006803
Iteration 250/1000 | Loss: 0.00005247
Iteration 251/1000 | Loss: 0.00004220
Iteration 252/1000 | Loss: 0.00005147
Iteration 253/1000 | Loss: 0.00008114
Iteration 254/1000 | Loss: 0.00032610
Iteration 255/1000 | Loss: 0.00006270
Iteration 256/1000 | Loss: 0.00005234
Iteration 257/1000 | Loss: 0.00005253
Iteration 258/1000 | Loss: 0.00006813
Iteration 259/1000 | Loss: 0.00006021
Iteration 260/1000 | Loss: 0.00004237
Iteration 261/1000 | Loss: 0.00005247
Iteration 262/1000 | Loss: 0.00004406
Iteration 263/1000 | Loss: 0.00004297
Iteration 264/1000 | Loss: 0.00005639
Iteration 265/1000 | Loss: 0.00006961
Iteration 266/1000 | Loss: 0.00004227
Iteration 267/1000 | Loss: 0.00005196
Iteration 268/1000 | Loss: 0.00004775
Iteration 269/1000 | Loss: 0.00006553
Iteration 270/1000 | Loss: 0.00005254
Iteration 271/1000 | Loss: 0.00006474
Iteration 272/1000 | Loss: 0.00005175
Iteration 273/1000 | Loss: 0.00006733
Iteration 274/1000 | Loss: 0.00005490
Iteration 275/1000 | Loss: 0.00005577
Iteration 276/1000 | Loss: 0.00004766
Iteration 277/1000 | Loss: 0.00005705
Iteration 278/1000 | Loss: 0.00005734
Iteration 279/1000 | Loss: 0.00005479
Iteration 280/1000 | Loss: 0.00004812
Iteration 281/1000 | Loss: 0.00004634
Iteration 282/1000 | Loss: 0.00005469
Iteration 283/1000 | Loss: 0.00004400
Iteration 284/1000 | Loss: 0.00080533
Iteration 285/1000 | Loss: 0.00054038
Iteration 286/1000 | Loss: 0.00064267
Iteration 287/1000 | Loss: 0.00005767
Iteration 288/1000 | Loss: 0.00006546
Iteration 289/1000 | Loss: 0.00005667
Iteration 290/1000 | Loss: 0.00005981
Iteration 291/1000 | Loss: 0.00004981
Iteration 292/1000 | Loss: 0.00006168
Iteration 293/1000 | Loss: 0.00005409
Iteration 294/1000 | Loss: 0.00005474
Iteration 295/1000 | Loss: 0.00004875
Iteration 296/1000 | Loss: 0.00005020
Iteration 297/1000 | Loss: 0.00005271
Iteration 298/1000 | Loss: 0.00003151
Iteration 299/1000 | Loss: 0.00002920
Iteration 300/1000 | Loss: 0.00002796
Iteration 301/1000 | Loss: 0.00002726
Iteration 302/1000 | Loss: 0.00002689
Iteration 303/1000 | Loss: 0.00002639
Iteration 304/1000 | Loss: 0.00002604
Iteration 305/1000 | Loss: 0.00002583
Iteration 306/1000 | Loss: 0.00002560
Iteration 307/1000 | Loss: 0.00002551
Iteration 308/1000 | Loss: 0.00002549
Iteration 309/1000 | Loss: 0.00002546
Iteration 310/1000 | Loss: 0.00002545
Iteration 311/1000 | Loss: 0.00002542
Iteration 312/1000 | Loss: 0.00002542
Iteration 313/1000 | Loss: 0.00002536
Iteration 314/1000 | Loss: 0.00002535
Iteration 315/1000 | Loss: 0.00002534
Iteration 316/1000 | Loss: 0.00002534
Iteration 317/1000 | Loss: 0.00002533
Iteration 318/1000 | Loss: 0.00002533
Iteration 319/1000 | Loss: 0.00002532
Iteration 320/1000 | Loss: 0.00002532
Iteration 321/1000 | Loss: 0.00002531
Iteration 322/1000 | Loss: 0.00002531
Iteration 323/1000 | Loss: 0.00002530
Iteration 324/1000 | Loss: 0.00002530
Iteration 325/1000 | Loss: 0.00002530
Iteration 326/1000 | Loss: 0.00002529
Iteration 327/1000 | Loss: 0.00002529
Iteration 328/1000 | Loss: 0.00002528
Iteration 329/1000 | Loss: 0.00002528
Iteration 330/1000 | Loss: 0.00002528
Iteration 331/1000 | Loss: 0.00002528
Iteration 332/1000 | Loss: 0.00002527
Iteration 333/1000 | Loss: 0.00002527
Iteration 334/1000 | Loss: 0.00002527
Iteration 335/1000 | Loss: 0.00002526
Iteration 336/1000 | Loss: 0.00002526
Iteration 337/1000 | Loss: 0.00002526
Iteration 338/1000 | Loss: 0.00002523
Iteration 339/1000 | Loss: 0.00002522
Iteration 340/1000 | Loss: 0.00002519
Iteration 341/1000 | Loss: 0.00002519
Iteration 342/1000 | Loss: 0.00002518
Iteration 343/1000 | Loss: 0.00002518
Iteration 344/1000 | Loss: 0.00002518
Iteration 345/1000 | Loss: 0.00002517
Iteration 346/1000 | Loss: 0.00002517
Iteration 347/1000 | Loss: 0.00002517
Iteration 348/1000 | Loss: 0.00002517
Iteration 349/1000 | Loss: 0.00002516
Iteration 350/1000 | Loss: 0.00002516
Iteration 351/1000 | Loss: 0.00002516
Iteration 352/1000 | Loss: 0.00002516
Iteration 353/1000 | Loss: 0.00002515
Iteration 354/1000 | Loss: 0.00002515
Iteration 355/1000 | Loss: 0.00002515
Iteration 356/1000 | Loss: 0.00002514
Iteration 357/1000 | Loss: 0.00002514
Iteration 358/1000 | Loss: 0.00002514
Iteration 359/1000 | Loss: 0.00002514
Iteration 360/1000 | Loss: 0.00019897
Iteration 361/1000 | Loss: 0.00003128
Iteration 362/1000 | Loss: 0.00002890
Iteration 363/1000 | Loss: 0.00002777
Iteration 364/1000 | Loss: 0.00002711
Iteration 365/1000 | Loss: 0.00002676
Iteration 366/1000 | Loss: 0.00002650
Iteration 367/1000 | Loss: 0.00002633
Iteration 368/1000 | Loss: 0.00002633
Iteration 369/1000 | Loss: 0.00002623
Iteration 370/1000 | Loss: 0.00002619
Iteration 371/1000 | Loss: 0.00002618
Iteration 372/1000 | Loss: 0.00002617
Iteration 373/1000 | Loss: 0.00002616
Iteration 374/1000 | Loss: 0.00002616
Iteration 375/1000 | Loss: 0.00002598
Iteration 376/1000 | Loss: 0.00002586
Iteration 377/1000 | Loss: 0.00002568
Iteration 378/1000 | Loss: 0.00002567
Iteration 379/1000 | Loss: 0.00002565
Iteration 380/1000 | Loss: 0.00002563
Iteration 381/1000 | Loss: 0.00002559
Iteration 382/1000 | Loss: 0.00002555
Iteration 383/1000 | Loss: 0.00002554
Iteration 384/1000 | Loss: 0.00002554
Iteration 385/1000 | Loss: 0.00002553
Iteration 386/1000 | Loss: 0.00002553
Iteration 387/1000 | Loss: 0.00002553
Iteration 388/1000 | Loss: 0.00002552
Iteration 389/1000 | Loss: 0.00002552
Iteration 390/1000 | Loss: 0.00002552
Iteration 391/1000 | Loss: 0.00002552
Iteration 392/1000 | Loss: 0.00002551
Iteration 393/1000 | Loss: 0.00002551
Iteration 394/1000 | Loss: 0.00002551
Iteration 395/1000 | Loss: 0.00002551
Iteration 396/1000 | Loss: 0.00002551
Iteration 397/1000 | Loss: 0.00002551
Iteration 398/1000 | Loss: 0.00002551
Iteration 399/1000 | Loss: 0.00002551
Iteration 400/1000 | Loss: 0.00002551
Iteration 401/1000 | Loss: 0.00002550
Iteration 402/1000 | Loss: 0.00002550
Iteration 403/1000 | Loss: 0.00002550
Iteration 404/1000 | Loss: 0.00002550
Iteration 405/1000 | Loss: 0.00002550
Iteration 406/1000 | Loss: 0.00002550
Iteration 407/1000 | Loss: 0.00002550
Iteration 408/1000 | Loss: 0.00002550
Iteration 409/1000 | Loss: 0.00002549
Iteration 410/1000 | Loss: 0.00002549
Iteration 411/1000 | Loss: 0.00002549
Iteration 412/1000 | Loss: 0.00002549
Iteration 413/1000 | Loss: 0.00002548
Iteration 414/1000 | Loss: 0.00002548
Iteration 415/1000 | Loss: 0.00002548
Iteration 416/1000 | Loss: 0.00002548
Iteration 417/1000 | Loss: 0.00002547
Iteration 418/1000 | Loss: 0.00002547
Iteration 419/1000 | Loss: 0.00002547
Iteration 420/1000 | Loss: 0.00002547
Iteration 421/1000 | Loss: 0.00002547
Iteration 422/1000 | Loss: 0.00002547
Iteration 423/1000 | Loss: 0.00002546
Iteration 424/1000 | Loss: 0.00002546
Iteration 425/1000 | Loss: 0.00002546
Iteration 426/1000 | Loss: 0.00002545
Iteration 427/1000 | Loss: 0.00002545
Iteration 428/1000 | Loss: 0.00002544
Iteration 429/1000 | Loss: 0.00002544
Iteration 430/1000 | Loss: 0.00002544
Iteration 431/1000 | Loss: 0.00002544
Iteration 432/1000 | Loss: 0.00002544
Iteration 433/1000 | Loss: 0.00002544
Iteration 434/1000 | Loss: 0.00002544
Iteration 435/1000 | Loss: 0.00002544
Iteration 436/1000 | Loss: 0.00002544
Iteration 437/1000 | Loss: 0.00002544
Iteration 438/1000 | Loss: 0.00002544
Iteration 439/1000 | Loss: 0.00002544
Iteration 440/1000 | Loss: 0.00002544
Iteration 441/1000 | Loss: 0.00002543
Iteration 442/1000 | Loss: 0.00002543
Iteration 443/1000 | Loss: 0.00002543
Iteration 444/1000 | Loss: 0.00002543
Iteration 445/1000 | Loss: 0.00002543
Iteration 446/1000 | Loss: 0.00002542
Iteration 447/1000 | Loss: 0.00002541
Iteration 448/1000 | Loss: 0.00002541
Iteration 449/1000 | Loss: 0.00002540
Iteration 450/1000 | Loss: 0.00002540
Iteration 451/1000 | Loss: 0.00002540
Iteration 452/1000 | Loss: 0.00002539
Iteration 453/1000 | Loss: 0.00002539
Iteration 454/1000 | Loss: 0.00002539
Iteration 455/1000 | Loss: 0.00002539
Iteration 456/1000 | Loss: 0.00002539
Iteration 457/1000 | Loss: 0.00002539
Iteration 458/1000 | Loss: 0.00002539
Iteration 459/1000 | Loss: 0.00002539
Iteration 460/1000 | Loss: 0.00002538
Iteration 461/1000 | Loss: 0.00002538
Iteration 462/1000 | Loss: 0.00002538
Iteration 463/1000 | Loss: 0.00002538
Iteration 464/1000 | Loss: 0.00002538
Iteration 465/1000 | Loss: 0.00002537
Iteration 466/1000 | Loss: 0.00002537
Iteration 467/1000 | Loss: 0.00002537
Iteration 468/1000 | Loss: 0.00002537
Iteration 469/1000 | Loss: 0.00002537
Iteration 470/1000 | Loss: 0.00002537
Iteration 471/1000 | Loss: 0.00002537
Iteration 472/1000 | Loss: 0.00002537
Iteration 473/1000 | Loss: 0.00002537
Iteration 474/1000 | Loss: 0.00002537
Iteration 475/1000 | Loss: 0.00002537
Iteration 476/1000 | Loss: 0.00002537
Iteration 477/1000 | Loss: 0.00002537
Iteration 478/1000 | Loss: 0.00002537
Iteration 479/1000 | Loss: 0.00002536
Iteration 480/1000 | Loss: 0.00002536
Iteration 481/1000 | Loss: 0.00002536
Iteration 482/1000 | Loss: 0.00002536
Iteration 483/1000 | Loss: 0.00002536
Iteration 484/1000 | Loss: 0.00002536
Iteration 485/1000 | Loss: 0.00002536
Iteration 486/1000 | Loss: 0.00002536
Iteration 487/1000 | Loss: 0.00002536
Iteration 488/1000 | Loss: 0.00002536
Iteration 489/1000 | Loss: 0.00002535
Iteration 490/1000 | Loss: 0.00002535
Iteration 491/1000 | Loss: 0.00002535
Iteration 492/1000 | Loss: 0.00002535
Iteration 493/1000 | Loss: 0.00002535
Iteration 494/1000 | Loss: 0.00002535
Iteration 495/1000 | Loss: 0.00002535
Iteration 496/1000 | Loss: 0.00002535
Iteration 497/1000 | Loss: 0.00002535
Iteration 498/1000 | Loss: 0.00002535
Iteration 499/1000 | Loss: 0.00002534
Iteration 500/1000 | Loss: 0.00002534
Iteration 501/1000 | Loss: 0.00002534
Iteration 502/1000 | Loss: 0.00002534
Iteration 503/1000 | Loss: 0.00002534
Iteration 504/1000 | Loss: 0.00002534
Iteration 505/1000 | Loss: 0.00002534
Iteration 506/1000 | Loss: 0.00002534
Iteration 507/1000 | Loss: 0.00002534
Iteration 508/1000 | Loss: 0.00002533
Iteration 509/1000 | Loss: 0.00002533
Iteration 510/1000 | Loss: 0.00002533
Iteration 511/1000 | Loss: 0.00002533
Iteration 512/1000 | Loss: 0.00002533
Iteration 513/1000 | Loss: 0.00002533
Iteration 514/1000 | Loss: 0.00002533
Iteration 515/1000 | Loss: 0.00002533
Iteration 516/1000 | Loss: 0.00002533
Iteration 517/1000 | Loss: 0.00002533
Iteration 518/1000 | Loss: 0.00002533
Iteration 519/1000 | Loss: 0.00002533
Iteration 520/1000 | Loss: 0.00002533
Iteration 521/1000 | Loss: 0.00002533
Iteration 522/1000 | Loss: 0.00002533
Iteration 523/1000 | Loss: 0.00002533
Iteration 524/1000 | Loss: 0.00002533
Iteration 525/1000 | Loss: 0.00002533
Iteration 526/1000 | Loss: 0.00002532
Iteration 527/1000 | Loss: 0.00002532
Iteration 528/1000 | Loss: 0.00002532
Iteration 529/1000 | Loss: 0.00002532
Iteration 530/1000 | Loss: 0.00002532
Iteration 531/1000 | Loss: 0.00002531
Iteration 532/1000 | Loss: 0.00002531
Iteration 533/1000 | Loss: 0.00002531
Iteration 534/1000 | Loss: 0.00002531
Iteration 535/1000 | Loss: 0.00002531
Iteration 536/1000 | Loss: 0.00002531
Iteration 537/1000 | Loss: 0.00002530
Iteration 538/1000 | Loss: 0.00002530
Iteration 539/1000 | Loss: 0.00002530
Iteration 540/1000 | Loss: 0.00002530
Iteration 541/1000 | Loss: 0.00002530
Iteration 542/1000 | Loss: 0.00002530
Iteration 543/1000 | Loss: 0.00002530
Iteration 544/1000 | Loss: 0.00002530
Iteration 545/1000 | Loss: 0.00002530
Iteration 546/1000 | Loss: 0.00002530
Iteration 547/1000 | Loss: 0.00002530
Iteration 548/1000 | Loss: 0.00002529
Iteration 549/1000 | Loss: 0.00002529
Iteration 550/1000 | Loss: 0.00002529
Iteration 551/1000 | Loss: 0.00002529
Iteration 552/1000 | Loss: 0.00002529
Iteration 553/1000 | Loss: 0.00002529
Iteration 554/1000 | Loss: 0.00002529
Iteration 555/1000 | Loss: 0.00002529
Iteration 556/1000 | Loss: 0.00002529
Iteration 557/1000 | Loss: 0.00002529
Iteration 558/1000 | Loss: 0.00002529
Iteration 559/1000 | Loss: 0.00002529
Iteration 560/1000 | Loss: 0.00002529
Iteration 561/1000 | Loss: 0.00002529
Iteration 562/1000 | Loss: 0.00002529
Iteration 563/1000 | Loss: 0.00002528
Iteration 564/1000 | Loss: 0.00002528
Iteration 565/1000 | Loss: 0.00002528
Iteration 566/1000 | Loss: 0.00002528
Iteration 567/1000 | Loss: 0.00002527
Iteration 568/1000 | Loss: 0.00002527
Iteration 569/1000 | Loss: 0.00002527
Iteration 570/1000 | Loss: 0.00002527
Iteration 571/1000 | Loss: 0.00002527
Iteration 572/1000 | Loss: 0.00002526
Iteration 573/1000 | Loss: 0.00002526
Iteration 574/1000 | Loss: 0.00002526
Iteration 575/1000 | Loss: 0.00002526
Iteration 576/1000 | Loss: 0.00002526
Iteration 577/1000 | Loss: 0.00002526
Iteration 578/1000 | Loss: 0.00002525
Iteration 579/1000 | Loss: 0.00002525
Iteration 580/1000 | Loss: 0.00002525
Iteration 581/1000 | Loss: 0.00002525
Iteration 582/1000 | Loss: 0.00002525
Iteration 583/1000 | Loss: 0.00002524
Iteration 584/1000 | Loss: 0.00002524
Iteration 585/1000 | Loss: 0.00002524
Iteration 586/1000 | Loss: 0.00002524
Iteration 587/1000 | Loss: 0.00002524
Iteration 588/1000 | Loss: 0.00002524
Iteration 589/1000 | Loss: 0.00002524
Iteration 590/1000 | Loss: 0.00002523
Iteration 591/1000 | Loss: 0.00002523
Iteration 592/1000 | Loss: 0.00002523
Iteration 593/1000 | Loss: 0.00002523
Iteration 594/1000 | Loss: 0.00002523
Iteration 595/1000 | Loss: 0.00002522
Iteration 596/1000 | Loss: 0.00002522
Iteration 597/1000 | Loss: 0.00002522
Iteration 598/1000 | Loss: 0.00002522
Iteration 599/1000 | Loss: 0.00002522
Iteration 600/1000 | Loss: 0.00002522
Iteration 601/1000 | Loss: 0.00002522
Iteration 602/1000 | Loss: 0.00002522
Iteration 603/1000 | Loss: 0.00002522
Iteration 604/1000 | Loss: 0.00002522
Iteration 605/1000 | Loss: 0.00002521
Iteration 606/1000 | Loss: 0.00002521
Iteration 607/1000 | Loss: 0.00002521
Iteration 608/1000 | Loss: 0.00002521
Iteration 609/1000 | Loss: 0.00002520
Iteration 610/1000 | Loss: 0.00002520
Iteration 611/1000 | Loss: 0.00002520
Iteration 612/1000 | Loss: 0.00002520
Iteration 613/1000 | Loss: 0.00002520
Iteration 614/1000 | Loss: 0.00002520
Iteration 615/1000 | Loss: 0.00002520
Iteration 616/1000 | Loss: 0.00002520
Iteration 617/1000 | Loss: 0.00002520
Iteration 618/1000 | Loss: 0.00002520
Iteration 619/1000 | Loss: 0.00002520
Iteration 620/1000 | Loss: 0.00002520
Iteration 621/1000 | Loss: 0.00002519
Iteration 622/1000 | Loss: 0.00002519
Iteration 623/1000 | Loss: 0.00002519
Iteration 624/1000 | Loss: 0.00002518
Iteration 625/1000 | Loss: 0.00002518
Iteration 626/1000 | Loss: 0.00002518
Iteration 627/1000 | Loss: 0.00002517
Iteration 628/1000 | Loss: 0.00002517
Iteration 629/1000 | Loss: 0.00002517
Iteration 630/1000 | Loss: 0.00002517
Iteration 631/1000 | Loss: 0.00002517
Iteration 632/1000 | Loss: 0.00002517
Iteration 633/1000 | Loss: 0.00002517
Iteration 634/1000 | Loss: 0.00002516
Iteration 635/1000 | Loss: 0.00002516
Iteration 636/1000 | Loss: 0.00002516
Iteration 637/1000 | Loss: 0.00002516
Iteration 638/1000 | Loss: 0.00002516
Iteration 639/1000 | Loss: 0.00002516
Iteration 640/1000 | Loss: 0.00002516
Iteration 641/1000 | Loss: 0.00002516
Iteration 642/1000 | Loss: 0.00002516
Iteration 643/1000 | Loss: 0.00002516
Iteration 644/1000 | Loss: 0.00002515
Iteration 645/1000 | Loss: 0.00002515
Iteration 646/1000 | Loss: 0.00002515
Iteration 647/1000 | Loss: 0.00002514
Iteration 648/1000 | Loss: 0.00002514
Iteration 649/1000 | Loss: 0.00002514
Iteration 650/1000 | Loss: 0.00002514
Iteration 651/1000 | Loss: 0.00002513
Iteration 652/1000 | Loss: 0.00002513
Iteration 653/1000 | Loss: 0.00002513
Iteration 654/1000 | Loss: 0.00002513
Iteration 655/1000 | Loss: 0.00002513
Iteration 656/1000 | Loss: 0.00002513
Iteration 657/1000 | Loss: 0.00002512
Iteration 658/1000 | Loss: 0.00002512
Iteration 659/1000 | Loss: 0.00002512
Iteration 660/1000 | Loss: 0.00002512
Iteration 661/1000 | Loss: 0.00002512
Iteration 662/1000 | Loss: 0.00002512
Iteration 663/1000 | Loss: 0.00002512
Iteration 664/1000 | Loss: 0.00002512
Iteration 665/1000 | Loss: 0.00002512
Iteration 666/1000 | Loss: 0.00002512
Iteration 667/1000 | Loss: 0.00002511
Iteration 668/1000 | Loss: 0.00002511
Iteration 669/1000 | Loss: 0.00002511
Iteration 670/1000 | Loss: 0.00002511
Iteration 671/1000 | Loss: 0.00002511
Iteration 672/1000 | Loss: 0.00002511
Iteration 673/1000 | Loss: 0.00002511
Iteration 674/1000 | Loss: 0.00002511
Iteration 675/1000 | Loss: 0.00002510
Iteration 676/1000 | Loss: 0.00002510
Iteration 677/1000 | Loss: 0.00002510
Iteration 678/1000 | Loss: 0.00002510
Iteration 679/1000 | Loss: 0.00002510
Iteration 680/1000 | Loss: 0.00002510
Iteration 681/1000 | Loss: 0.00002510
Iteration 682/1000 | Loss: 0.00002510
Iteration 683/1000 | Loss: 0.00002509
Iteration 684/1000 | Loss: 0.00002509
Iteration 685/1000 | Loss: 0.00002509
Iteration 686/1000 | Loss: 0.00002509
Iteration 687/1000 | Loss: 0.00002509
Iteration 688/1000 | Loss: 0.00002509
Iteration 689/1000 | Loss: 0.00002509
Iteration 690/1000 | Loss: 0.00002509
Iteration 691/1000 | Loss: 0.00002508
Iteration 692/1000 | Loss: 0.00002508
Iteration 693/1000 | Loss: 0.00002508
Iteration 694/1000 | Loss: 0.00002508
Iteration 695/1000 | Loss: 0.00002508
Iteration 696/1000 | Loss: 0.00002507
Iteration 697/1000 | Loss: 0.00002507
Iteration 698/1000 | Loss: 0.00002507
Iteration 699/1000 | Loss: 0.00002507
Iteration 700/1000 | Loss: 0.00002507
Iteration 701/1000 | Loss: 0.00002506
Iteration 702/1000 | Loss: 0.00002506
Iteration 703/1000 | Loss: 0.00002506
Iteration 704/1000 | Loss: 0.00002506
Iteration 705/1000 | Loss: 0.00002506
Iteration 706/1000 | Loss: 0.00002506
Iteration 707/1000 | Loss: 0.00002506
Iteration 708/1000 | Loss: 0.00002506
Iteration 709/1000 | Loss: 0.00002506
Iteration 710/1000 | Loss: 0.00002506
Iteration 711/1000 | Loss: 0.00002506
Iteration 712/1000 | Loss: 0.00002506
Iteration 713/1000 | Loss: 0.00002506
Iteration 714/1000 | Loss: 0.00002505
Iteration 715/1000 | Loss: 0.00002505
Iteration 716/1000 | Loss: 0.00002505
Iteration 717/1000 | Loss: 0.00002505
Iteration 718/1000 | Loss: 0.00002505
Iteration 719/1000 | Loss: 0.00002505
Iteration 720/1000 | Loss: 0.00002505
Iteration 721/1000 | Loss: 0.00002505
Iteration 722/1000 | Loss: 0.00002505
Iteration 723/1000 | Loss: 0.00002505
Iteration 724/1000 | Loss: 0.00002505
Iteration 725/1000 | Loss: 0.00002505
Iteration 726/1000 | Loss: 0.00002505
Iteration 727/1000 | Loss: 0.00002505
Iteration 728/1000 | Loss: 0.00002505
Iteration 729/1000 | Loss: 0.00002505
Iteration 730/1000 | Loss: 0.00002505
Iteration 731/1000 | Loss: 0.00002505
Iteration 732/1000 | Loss: 0.00002505
Iteration 733/1000 | Loss: 0.00002505
Iteration 734/1000 | Loss: 0.00002505
Iteration 735/1000 | Loss: 0.00002504
Iteration 736/1000 | Loss: 0.00002504
Iteration 737/1000 | Loss: 0.00002504
Iteration 738/1000 | Loss: 0.00002504
Iteration 739/1000 | Loss: 0.00002504
Iteration 740/1000 | Loss: 0.00002504
Iteration 741/1000 | Loss: 0.00002504
Iteration 742/1000 | Loss: 0.00002504
Iteration 743/1000 | Loss: 0.00002504
Iteration 744/1000 | Loss: 0.00002504
Iteration 745/1000 | Loss: 0.00002504
Iteration 746/1000 | Loss: 0.00002504
Iteration 747/1000 | Loss: 0.00002504
Iteration 748/1000 | Loss: 0.00002504
Iteration 749/1000 | Loss: 0.00002504
Iteration 750/1000 | Loss: 0.00002504
Iteration 751/1000 | Loss: 0.00002504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 751. Stopping optimization.
Last 5 losses: [2.503913128748536e-05, 2.503913128748536e-05, 2.503913128748536e-05, 2.503913128748536e-05, 2.503913128748536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.503913128748536e-05

Optimization complete. Final v2v error: 3.6018638610839844 mm

Highest mean error: 13.420653343200684 mm for frame 40

Lowest mean error: 2.7684361934661865 mm for frame 160

Saving results

Total time: 546.7886629104614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789082
Iteration 2/25 | Loss: 0.00180166
Iteration 3/25 | Loss: 0.00093488
Iteration 4/25 | Loss: 0.00085926
Iteration 5/25 | Loss: 0.00076565
Iteration 6/25 | Loss: 0.00073963
Iteration 7/25 | Loss: 0.00074206
Iteration 8/25 | Loss: 0.00072162
Iteration 9/25 | Loss: 0.00071531
Iteration 10/25 | Loss: 0.00071119
Iteration 11/25 | Loss: 0.00070547
Iteration 12/25 | Loss: 0.00070111
Iteration 13/25 | Loss: 0.00070168
Iteration 14/25 | Loss: 0.00070126
Iteration 15/25 | Loss: 0.00069933
Iteration 16/25 | Loss: 0.00070179
Iteration 17/25 | Loss: 0.00069908
Iteration 18/25 | Loss: 0.00069907
Iteration 19/25 | Loss: 0.00069907
Iteration 20/25 | Loss: 0.00069907
Iteration 21/25 | Loss: 0.00069907
Iteration 22/25 | Loss: 0.00069907
Iteration 23/25 | Loss: 0.00069907
Iteration 24/25 | Loss: 0.00069906
Iteration 25/25 | Loss: 0.00069906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97914422
Iteration 2/25 | Loss: 0.00036904
Iteration 3/25 | Loss: 0.00033938
Iteration 4/25 | Loss: 0.00033938
Iteration 5/25 | Loss: 0.00033938
Iteration 6/25 | Loss: 0.00033938
Iteration 7/25 | Loss: 0.00033938
Iteration 8/25 | Loss: 0.00033938
Iteration 9/25 | Loss: 0.00033938
Iteration 10/25 | Loss: 0.00033938
Iteration 11/25 | Loss: 0.00033938
Iteration 12/25 | Loss: 0.00033938
Iteration 13/25 | Loss: 0.00033938
Iteration 14/25 | Loss: 0.00033938
Iteration 15/25 | Loss: 0.00033938
Iteration 16/25 | Loss: 0.00033938
Iteration 17/25 | Loss: 0.00033938
Iteration 18/25 | Loss: 0.00033938
Iteration 19/25 | Loss: 0.00033938
Iteration 20/25 | Loss: 0.00033938
Iteration 21/25 | Loss: 0.00033938
Iteration 22/25 | Loss: 0.00033938
Iteration 23/25 | Loss: 0.00033938
Iteration 24/25 | Loss: 0.00033938
Iteration 25/25 | Loss: 0.00033938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033938
Iteration 2/1000 | Loss: 0.00005728
Iteration 3/1000 | Loss: 0.00005900
Iteration 4/1000 | Loss: 0.00003774
Iteration 5/1000 | Loss: 0.00002021
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001831
Iteration 8/1000 | Loss: 0.00001983
Iteration 9/1000 | Loss: 0.00003894
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00003681
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001557
Iteration 25/1000 | Loss: 0.00001557
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00005286
Iteration 29/1000 | Loss: 0.00002224
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001534
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00002968
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001527
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001526
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001526
Iteration 59/1000 | Loss: 0.00001526
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001526
Iteration 64/1000 | Loss: 0.00001526
Iteration 65/1000 | Loss: 0.00001525
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001525
Iteration 71/1000 | Loss: 0.00001525
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00003538
Iteration 81/1000 | Loss: 0.00002460
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001516
Iteration 96/1000 | Loss: 0.00001516
Iteration 97/1000 | Loss: 0.00001516
Iteration 98/1000 | Loss: 0.00001516
Iteration 99/1000 | Loss: 0.00001516
Iteration 100/1000 | Loss: 0.00001516
Iteration 101/1000 | Loss: 0.00001516
Iteration 102/1000 | Loss: 0.00001516
Iteration 103/1000 | Loss: 0.00001516
Iteration 104/1000 | Loss: 0.00001516
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001516
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001516
Iteration 109/1000 | Loss: 0.00001516
Iteration 110/1000 | Loss: 0.00001516
Iteration 111/1000 | Loss: 0.00001516
Iteration 112/1000 | Loss: 0.00001516
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001516
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001516
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5159852409851737e-05, 1.5159852409851737e-05, 1.5159852409851737e-05, 1.5159852409851737e-05, 1.5159852409851737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5159852409851737e-05

Optimization complete. Final v2v error: 3.2388219833374023 mm

Highest mean error: 9.0054292678833 mm for frame 133

Lowest mean error: 2.802696943283081 mm for frame 121

Saving results

Total time: 68.43923926353455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020363
Iteration 2/25 | Loss: 0.00227536
Iteration 3/25 | Loss: 0.00154509
Iteration 4/25 | Loss: 0.00137718
Iteration 5/25 | Loss: 0.00122266
Iteration 6/25 | Loss: 0.00119012
Iteration 7/25 | Loss: 0.00101294
Iteration 8/25 | Loss: 0.00091885
Iteration 9/25 | Loss: 0.00086608
Iteration 10/25 | Loss: 0.00083201
Iteration 11/25 | Loss: 0.00084747
Iteration 12/25 | Loss: 0.00079317
Iteration 13/25 | Loss: 0.00077358
Iteration 14/25 | Loss: 0.00076395
Iteration 15/25 | Loss: 0.00079928
Iteration 16/25 | Loss: 0.00074601
Iteration 17/25 | Loss: 0.00073887
Iteration 18/25 | Loss: 0.00073621
Iteration 19/25 | Loss: 0.00073684
Iteration 20/25 | Loss: 0.00074180
Iteration 21/25 | Loss: 0.00074164
Iteration 22/25 | Loss: 0.00073800
Iteration 23/25 | Loss: 0.00073766
Iteration 24/25 | Loss: 0.00073106
Iteration 25/25 | Loss: 0.00073073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53306258
Iteration 2/25 | Loss: 0.00087901
Iteration 3/25 | Loss: 0.00087901
Iteration 4/25 | Loss: 0.00087901
Iteration 5/25 | Loss: 0.00087901
Iteration 6/25 | Loss: 0.00087901
Iteration 7/25 | Loss: 0.00087901
Iteration 8/25 | Loss: 0.00087901
Iteration 9/25 | Loss: 0.00087901
Iteration 10/25 | Loss: 0.00087901
Iteration 11/25 | Loss: 0.00087901
Iteration 12/25 | Loss: 0.00087901
Iteration 13/25 | Loss: 0.00087901
Iteration 14/25 | Loss: 0.00087901
Iteration 15/25 | Loss: 0.00087901
Iteration 16/25 | Loss: 0.00087901
Iteration 17/25 | Loss: 0.00087901
Iteration 18/25 | Loss: 0.00087901
Iteration 19/25 | Loss: 0.00087901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008790112333372235, 0.0008790112333372235, 0.0008790112333372235, 0.0008790112333372235, 0.0008790112333372235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008790112333372235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087901
Iteration 2/1000 | Loss: 0.00076397
Iteration 3/1000 | Loss: 0.00041565
Iteration 4/1000 | Loss: 0.00025494
Iteration 5/1000 | Loss: 0.00055071
Iteration 6/1000 | Loss: 0.00202642
Iteration 7/1000 | Loss: 0.00090771
Iteration 8/1000 | Loss: 0.00100756
Iteration 9/1000 | Loss: 0.00057753
Iteration 10/1000 | Loss: 0.00072055
Iteration 11/1000 | Loss: 0.00089307
Iteration 12/1000 | Loss: 0.00072539
Iteration 13/1000 | Loss: 0.00051653
Iteration 14/1000 | Loss: 0.00041202
Iteration 15/1000 | Loss: 0.00052844
Iteration 16/1000 | Loss: 0.00093633
Iteration 17/1000 | Loss: 0.00084089
Iteration 18/1000 | Loss: 0.00072257
Iteration 19/1000 | Loss: 0.00075398
Iteration 20/1000 | Loss: 0.00041205
Iteration 21/1000 | Loss: 0.00044987
Iteration 22/1000 | Loss: 0.00082215
Iteration 23/1000 | Loss: 0.00038654
Iteration 24/1000 | Loss: 0.00091430
Iteration 25/1000 | Loss: 0.00094758
Iteration 26/1000 | Loss: 0.00025558
Iteration 27/1000 | Loss: 0.00048275
Iteration 28/1000 | Loss: 0.00102941
Iteration 29/1000 | Loss: 0.00170953
Iteration 30/1000 | Loss: 0.00049634
Iteration 31/1000 | Loss: 0.00046737
Iteration 32/1000 | Loss: 0.00058575
Iteration 33/1000 | Loss: 0.00045707
Iteration 34/1000 | Loss: 0.00093020
Iteration 35/1000 | Loss: 0.00107934
Iteration 36/1000 | Loss: 0.00037050
Iteration 37/1000 | Loss: 0.00005257
Iteration 38/1000 | Loss: 0.00059158
Iteration 39/1000 | Loss: 0.00212222
Iteration 40/1000 | Loss: 0.00162728
Iteration 41/1000 | Loss: 0.00032091
Iteration 42/1000 | Loss: 0.00022176
Iteration 43/1000 | Loss: 0.00044594
Iteration 44/1000 | Loss: 0.00085295
Iteration 45/1000 | Loss: 0.00037832
Iteration 46/1000 | Loss: 0.00080572
Iteration 47/1000 | Loss: 0.00121883
Iteration 48/1000 | Loss: 0.00048406
Iteration 49/1000 | Loss: 0.00036242
Iteration 50/1000 | Loss: 0.00025841
Iteration 51/1000 | Loss: 0.00089825
Iteration 52/1000 | Loss: 0.00051993
Iteration 53/1000 | Loss: 0.00106800
Iteration 54/1000 | Loss: 0.00082345
Iteration 55/1000 | Loss: 0.00173638
Iteration 56/1000 | Loss: 0.00091368
Iteration 57/1000 | Loss: 0.00168987
Iteration 58/1000 | Loss: 0.00139075
Iteration 59/1000 | Loss: 0.00161550
Iteration 60/1000 | Loss: 0.00141647
Iteration 61/1000 | Loss: 0.00094226
Iteration 62/1000 | Loss: 0.00064620
Iteration 63/1000 | Loss: 0.00025596
Iteration 64/1000 | Loss: 0.00032159
Iteration 65/1000 | Loss: 0.00029311
Iteration 66/1000 | Loss: 0.00025984
Iteration 67/1000 | Loss: 0.00027023
Iteration 68/1000 | Loss: 0.00028263
Iteration 69/1000 | Loss: 0.00040980
Iteration 70/1000 | Loss: 0.00063465
Iteration 71/1000 | Loss: 0.00065053
Iteration 72/1000 | Loss: 0.00085921
Iteration 73/1000 | Loss: 0.00088150
Iteration 74/1000 | Loss: 0.00179380
Iteration 75/1000 | Loss: 0.00097783
Iteration 76/1000 | Loss: 0.00091062
Iteration 77/1000 | Loss: 0.00115118
Iteration 78/1000 | Loss: 0.00048984
Iteration 79/1000 | Loss: 0.00033576
Iteration 80/1000 | Loss: 0.00037301
Iteration 81/1000 | Loss: 0.00007895
Iteration 82/1000 | Loss: 0.00006106
Iteration 83/1000 | Loss: 0.00104836
Iteration 84/1000 | Loss: 0.00082448
Iteration 85/1000 | Loss: 0.00020944
Iteration 86/1000 | Loss: 0.00054070
Iteration 87/1000 | Loss: 0.00029595
Iteration 88/1000 | Loss: 0.00025725
Iteration 89/1000 | Loss: 0.00019483
Iteration 90/1000 | Loss: 0.00042991
Iteration 91/1000 | Loss: 0.00011501
Iteration 92/1000 | Loss: 0.00026025
Iteration 93/1000 | Loss: 0.00024064
Iteration 94/1000 | Loss: 0.00025373
Iteration 95/1000 | Loss: 0.00020627
Iteration 96/1000 | Loss: 0.00045830
Iteration 97/1000 | Loss: 0.00026434
Iteration 98/1000 | Loss: 0.00030512
Iteration 99/1000 | Loss: 0.00022659
Iteration 100/1000 | Loss: 0.00011581
Iteration 101/1000 | Loss: 0.00012174
Iteration 102/1000 | Loss: 0.00024151
Iteration 103/1000 | Loss: 0.00022584
Iteration 104/1000 | Loss: 0.00013412
Iteration 105/1000 | Loss: 0.00052296
Iteration 106/1000 | Loss: 0.00012208
Iteration 107/1000 | Loss: 0.00009221
Iteration 108/1000 | Loss: 0.00011464
Iteration 109/1000 | Loss: 0.00018527
Iteration 110/1000 | Loss: 0.00019734
Iteration 111/1000 | Loss: 0.00045504
Iteration 112/1000 | Loss: 0.00045496
Iteration 113/1000 | Loss: 0.00026994
Iteration 114/1000 | Loss: 0.00009155
Iteration 115/1000 | Loss: 0.00016722
Iteration 116/1000 | Loss: 0.00045553
Iteration 117/1000 | Loss: 0.00038081
Iteration 118/1000 | Loss: 0.00010710
Iteration 119/1000 | Loss: 0.00011581
Iteration 120/1000 | Loss: 0.00002855
Iteration 121/1000 | Loss: 0.00037539
Iteration 122/1000 | Loss: 0.00052705
Iteration 123/1000 | Loss: 0.00230344
Iteration 124/1000 | Loss: 0.00082500
Iteration 125/1000 | Loss: 0.00080279
Iteration 126/1000 | Loss: 0.00053530
Iteration 127/1000 | Loss: 0.00040488
Iteration 128/1000 | Loss: 0.00011798
Iteration 129/1000 | Loss: 0.00010694
Iteration 130/1000 | Loss: 0.00006902
Iteration 131/1000 | Loss: 0.00002903
Iteration 132/1000 | Loss: 0.00011399
Iteration 133/1000 | Loss: 0.00009055
Iteration 134/1000 | Loss: 0.00011851
Iteration 135/1000 | Loss: 0.00012602
Iteration 136/1000 | Loss: 0.00008449
Iteration 137/1000 | Loss: 0.00022312
Iteration 138/1000 | Loss: 0.00023359
Iteration 139/1000 | Loss: 0.00008488
Iteration 140/1000 | Loss: 0.00012321
Iteration 141/1000 | Loss: 0.00009504
Iteration 142/1000 | Loss: 0.00022689
Iteration 143/1000 | Loss: 0.00013562
Iteration 144/1000 | Loss: 0.00004844
Iteration 145/1000 | Loss: 0.00002793
Iteration 146/1000 | Loss: 0.00013840
Iteration 147/1000 | Loss: 0.00013389
Iteration 148/1000 | Loss: 0.00019270
Iteration 149/1000 | Loss: 0.00023057
Iteration 150/1000 | Loss: 0.00024797
Iteration 151/1000 | Loss: 0.00023462
Iteration 152/1000 | Loss: 0.00022324
Iteration 153/1000 | Loss: 0.00022097
Iteration 154/1000 | Loss: 0.00017242
Iteration 155/1000 | Loss: 0.00021716
Iteration 156/1000 | Loss: 0.00016896
Iteration 157/1000 | Loss: 0.00024112
Iteration 158/1000 | Loss: 0.00005500
Iteration 159/1000 | Loss: 0.00016548
Iteration 160/1000 | Loss: 0.00026283
Iteration 161/1000 | Loss: 0.00025438
Iteration 162/1000 | Loss: 0.00022490
Iteration 163/1000 | Loss: 0.00016806
Iteration 164/1000 | Loss: 0.00015019
Iteration 165/1000 | Loss: 0.00005537
Iteration 166/1000 | Loss: 0.00038249
Iteration 167/1000 | Loss: 0.00025854
Iteration 168/1000 | Loss: 0.00027878
Iteration 169/1000 | Loss: 0.00021813
Iteration 170/1000 | Loss: 0.00024853
Iteration 171/1000 | Loss: 0.00018515
Iteration 172/1000 | Loss: 0.00028686
Iteration 173/1000 | Loss: 0.00024585
Iteration 174/1000 | Loss: 0.00023721
Iteration 175/1000 | Loss: 0.00016078
Iteration 176/1000 | Loss: 0.00037392
Iteration 177/1000 | Loss: 0.00018707
Iteration 178/1000 | Loss: 0.00002880
Iteration 179/1000 | Loss: 0.00019657
Iteration 180/1000 | Loss: 0.00036360
Iteration 181/1000 | Loss: 0.00024696
Iteration 182/1000 | Loss: 0.00028465
Iteration 183/1000 | Loss: 0.00018102
Iteration 184/1000 | Loss: 0.00025591
Iteration 185/1000 | Loss: 0.00003008
Iteration 186/1000 | Loss: 0.00025931
Iteration 187/1000 | Loss: 0.00041249
Iteration 188/1000 | Loss: 0.00024601
Iteration 189/1000 | Loss: 0.00025482
Iteration 190/1000 | Loss: 0.00024165
Iteration 191/1000 | Loss: 0.00020758
Iteration 192/1000 | Loss: 0.00020869
Iteration 193/1000 | Loss: 0.00023737
Iteration 194/1000 | Loss: 0.00019534
Iteration 195/1000 | Loss: 0.00024408
Iteration 196/1000 | Loss: 0.00016910
Iteration 197/1000 | Loss: 0.00019312
Iteration 198/1000 | Loss: 0.00014292
Iteration 199/1000 | Loss: 0.00003133
Iteration 200/1000 | Loss: 0.00020911
Iteration 201/1000 | Loss: 0.00016133
Iteration 202/1000 | Loss: 0.00013827
Iteration 203/1000 | Loss: 0.00016894
Iteration 204/1000 | Loss: 0.00008895
Iteration 205/1000 | Loss: 0.00013062
Iteration 206/1000 | Loss: 0.00019870
Iteration 207/1000 | Loss: 0.00019651
Iteration 208/1000 | Loss: 0.00006188
Iteration 209/1000 | Loss: 0.00029864
Iteration 210/1000 | Loss: 0.00011357
Iteration 211/1000 | Loss: 0.00023291
Iteration 212/1000 | Loss: 0.00019061
Iteration 213/1000 | Loss: 0.00018560
Iteration 214/1000 | Loss: 0.00045299
Iteration 215/1000 | Loss: 0.00018726
Iteration 216/1000 | Loss: 0.00023705
Iteration 217/1000 | Loss: 0.00024587
Iteration 218/1000 | Loss: 0.00039127
Iteration 219/1000 | Loss: 0.00020381
Iteration 220/1000 | Loss: 0.00020924
Iteration 221/1000 | Loss: 0.00014612
Iteration 222/1000 | Loss: 0.00016731
Iteration 223/1000 | Loss: 0.00029372
Iteration 224/1000 | Loss: 0.00011406
Iteration 225/1000 | Loss: 0.00040430
Iteration 226/1000 | Loss: 0.00002715
Iteration 227/1000 | Loss: 0.00002072
Iteration 228/1000 | Loss: 0.00001972
Iteration 229/1000 | Loss: 0.00001914
Iteration 230/1000 | Loss: 0.00001887
Iteration 231/1000 | Loss: 0.00001865
Iteration 232/1000 | Loss: 0.00001844
Iteration 233/1000 | Loss: 0.00001833
Iteration 234/1000 | Loss: 0.00001825
Iteration 235/1000 | Loss: 0.00001815
Iteration 236/1000 | Loss: 0.00001814
Iteration 237/1000 | Loss: 0.00001813
Iteration 238/1000 | Loss: 0.00001813
Iteration 239/1000 | Loss: 0.00001810
Iteration 240/1000 | Loss: 0.00001805
Iteration 241/1000 | Loss: 0.00001805
Iteration 242/1000 | Loss: 0.00001802
Iteration 243/1000 | Loss: 0.00001801
Iteration 244/1000 | Loss: 0.00001801
Iteration 245/1000 | Loss: 0.00001800
Iteration 246/1000 | Loss: 0.00001800
Iteration 247/1000 | Loss: 0.00001800
Iteration 248/1000 | Loss: 0.00001800
Iteration 249/1000 | Loss: 0.00001800
Iteration 250/1000 | Loss: 0.00001799
Iteration 251/1000 | Loss: 0.00001799
Iteration 252/1000 | Loss: 0.00001799
Iteration 253/1000 | Loss: 0.00001799
Iteration 254/1000 | Loss: 0.00001798
Iteration 255/1000 | Loss: 0.00001798
Iteration 256/1000 | Loss: 0.00001798
Iteration 257/1000 | Loss: 0.00001798
Iteration 258/1000 | Loss: 0.00001797
Iteration 259/1000 | Loss: 0.00001797
Iteration 260/1000 | Loss: 0.00001797
Iteration 261/1000 | Loss: 0.00001796
Iteration 262/1000 | Loss: 0.00001796
Iteration 263/1000 | Loss: 0.00001796
Iteration 264/1000 | Loss: 0.00001796
Iteration 265/1000 | Loss: 0.00001796
Iteration 266/1000 | Loss: 0.00001796
Iteration 267/1000 | Loss: 0.00001795
Iteration 268/1000 | Loss: 0.00001795
Iteration 269/1000 | Loss: 0.00001795
Iteration 270/1000 | Loss: 0.00001795
Iteration 271/1000 | Loss: 0.00001795
Iteration 272/1000 | Loss: 0.00001795
Iteration 273/1000 | Loss: 0.00001795
Iteration 274/1000 | Loss: 0.00001795
Iteration 275/1000 | Loss: 0.00001795
Iteration 276/1000 | Loss: 0.00001795
Iteration 277/1000 | Loss: 0.00001795
Iteration 278/1000 | Loss: 0.00001795
Iteration 279/1000 | Loss: 0.00001794
Iteration 280/1000 | Loss: 0.00001794
Iteration 281/1000 | Loss: 0.00001794
Iteration 282/1000 | Loss: 0.00001794
Iteration 283/1000 | Loss: 0.00001794
Iteration 284/1000 | Loss: 0.00001794
Iteration 285/1000 | Loss: 0.00001794
Iteration 286/1000 | Loss: 0.00001793
Iteration 287/1000 | Loss: 0.00001793
Iteration 288/1000 | Loss: 0.00001793
Iteration 289/1000 | Loss: 0.00001793
Iteration 290/1000 | Loss: 0.00001793
Iteration 291/1000 | Loss: 0.00001793
Iteration 292/1000 | Loss: 0.00001793
Iteration 293/1000 | Loss: 0.00001793
Iteration 294/1000 | Loss: 0.00001793
Iteration 295/1000 | Loss: 0.00001793
Iteration 296/1000 | Loss: 0.00001793
Iteration 297/1000 | Loss: 0.00001793
Iteration 298/1000 | Loss: 0.00001792
Iteration 299/1000 | Loss: 0.00001792
Iteration 300/1000 | Loss: 0.00001792
Iteration 301/1000 | Loss: 0.00001792
Iteration 302/1000 | Loss: 0.00001792
Iteration 303/1000 | Loss: 0.00001792
Iteration 304/1000 | Loss: 0.00001792
Iteration 305/1000 | Loss: 0.00001792
Iteration 306/1000 | Loss: 0.00001792
Iteration 307/1000 | Loss: 0.00001792
Iteration 308/1000 | Loss: 0.00001792
Iteration 309/1000 | Loss: 0.00001792
Iteration 310/1000 | Loss: 0.00001792
Iteration 311/1000 | Loss: 0.00001792
Iteration 312/1000 | Loss: 0.00001792
Iteration 313/1000 | Loss: 0.00001792
Iteration 314/1000 | Loss: 0.00001792
Iteration 315/1000 | Loss: 0.00001792
Iteration 316/1000 | Loss: 0.00001792
Iteration 317/1000 | Loss: 0.00001791
Iteration 318/1000 | Loss: 0.00001791
Iteration 319/1000 | Loss: 0.00001791
Iteration 320/1000 | Loss: 0.00001791
Iteration 321/1000 | Loss: 0.00001791
Iteration 322/1000 | Loss: 0.00001791
Iteration 323/1000 | Loss: 0.00001791
Iteration 324/1000 | Loss: 0.00001790
Iteration 325/1000 | Loss: 0.00001790
Iteration 326/1000 | Loss: 0.00001790
Iteration 327/1000 | Loss: 0.00001790
Iteration 328/1000 | Loss: 0.00001790
Iteration 329/1000 | Loss: 0.00001790
Iteration 330/1000 | Loss: 0.00001790
Iteration 331/1000 | Loss: 0.00001789
Iteration 332/1000 | Loss: 0.00001789
Iteration 333/1000 | Loss: 0.00001789
Iteration 334/1000 | Loss: 0.00001789
Iteration 335/1000 | Loss: 0.00001789
Iteration 336/1000 | Loss: 0.00001789
Iteration 337/1000 | Loss: 0.00001789
Iteration 338/1000 | Loss: 0.00001789
Iteration 339/1000 | Loss: 0.00001789
Iteration 340/1000 | Loss: 0.00001789
Iteration 341/1000 | Loss: 0.00001789
Iteration 342/1000 | Loss: 0.00001788
Iteration 343/1000 | Loss: 0.00001788
Iteration 344/1000 | Loss: 0.00001788
Iteration 345/1000 | Loss: 0.00001788
Iteration 346/1000 | Loss: 0.00001788
Iteration 347/1000 | Loss: 0.00001788
Iteration 348/1000 | Loss: 0.00001788
Iteration 349/1000 | Loss: 0.00001788
Iteration 350/1000 | Loss: 0.00001788
Iteration 351/1000 | Loss: 0.00001788
Iteration 352/1000 | Loss: 0.00001787
Iteration 353/1000 | Loss: 0.00001787
Iteration 354/1000 | Loss: 0.00001787
Iteration 355/1000 | Loss: 0.00001787
Iteration 356/1000 | Loss: 0.00001787
Iteration 357/1000 | Loss: 0.00001787
Iteration 358/1000 | Loss: 0.00001787
Iteration 359/1000 | Loss: 0.00001787
Iteration 360/1000 | Loss: 0.00001787
Iteration 361/1000 | Loss: 0.00001787
Iteration 362/1000 | Loss: 0.00001787
Iteration 363/1000 | Loss: 0.00001787
Iteration 364/1000 | Loss: 0.00001787
Iteration 365/1000 | Loss: 0.00001787
Iteration 366/1000 | Loss: 0.00001787
Iteration 367/1000 | Loss: 0.00001787
Iteration 368/1000 | Loss: 0.00001787
Iteration 369/1000 | Loss: 0.00001787
Iteration 370/1000 | Loss: 0.00001787
Iteration 371/1000 | Loss: 0.00001787
Iteration 372/1000 | Loss: 0.00001787
Iteration 373/1000 | Loss: 0.00001787
Iteration 374/1000 | Loss: 0.00001787
Iteration 375/1000 | Loss: 0.00001787
Iteration 376/1000 | Loss: 0.00001787
Iteration 377/1000 | Loss: 0.00001787
Iteration 378/1000 | Loss: 0.00001787
Iteration 379/1000 | Loss: 0.00001787
Iteration 380/1000 | Loss: 0.00001787
Iteration 381/1000 | Loss: 0.00001787
Iteration 382/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 382. Stopping optimization.
Last 5 losses: [1.7874734112410806e-05, 1.7874734112410806e-05, 1.7874734112410806e-05, 1.7874734112410806e-05, 1.7874734112410806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7874734112410806e-05

Optimization complete. Final v2v error: 3.554227113723755 mm

Highest mean error: 5.979089736938477 mm for frame 49

Lowest mean error: 3.20762038230896 mm for frame 63

Saving results

Total time: 385.89144468307495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436676
Iteration 2/25 | Loss: 0.00083895
Iteration 3/25 | Loss: 0.00073556
Iteration 4/25 | Loss: 0.00071321
Iteration 5/25 | Loss: 0.00070751
Iteration 6/25 | Loss: 0.00070641
Iteration 7/25 | Loss: 0.00070624
Iteration 8/25 | Loss: 0.00070624
Iteration 9/25 | Loss: 0.00070624
Iteration 10/25 | Loss: 0.00070624
Iteration 11/25 | Loss: 0.00070624
Iteration 12/25 | Loss: 0.00070624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007062395452521741, 0.0007062395452521741, 0.0007062395452521741, 0.0007062395452521741, 0.0007062395452521741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007062395452521741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42476189
Iteration 2/25 | Loss: 0.00027513
Iteration 3/25 | Loss: 0.00027513
Iteration 4/25 | Loss: 0.00027513
Iteration 5/25 | Loss: 0.00027513
Iteration 6/25 | Loss: 0.00027513
Iteration 7/25 | Loss: 0.00027513
Iteration 8/25 | Loss: 0.00027513
Iteration 9/25 | Loss: 0.00027513
Iteration 10/25 | Loss: 0.00027513
Iteration 11/25 | Loss: 0.00027513
Iteration 12/25 | Loss: 0.00027513
Iteration 13/25 | Loss: 0.00027513
Iteration 14/25 | Loss: 0.00027513
Iteration 15/25 | Loss: 0.00027513
Iteration 16/25 | Loss: 0.00027513
Iteration 17/25 | Loss: 0.00027513
Iteration 18/25 | Loss: 0.00027513
Iteration 19/25 | Loss: 0.00027513
Iteration 20/25 | Loss: 0.00027513
Iteration 21/25 | Loss: 0.00027513
Iteration 22/25 | Loss: 0.00027513
Iteration 23/25 | Loss: 0.00027513
Iteration 24/25 | Loss: 0.00027513
Iteration 25/25 | Loss: 0.00027513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027513
Iteration 2/1000 | Loss: 0.00004758
Iteration 3/1000 | Loss: 0.00003396
Iteration 4/1000 | Loss: 0.00003155
Iteration 5/1000 | Loss: 0.00002987
Iteration 6/1000 | Loss: 0.00002864
Iteration 7/1000 | Loss: 0.00002778
Iteration 8/1000 | Loss: 0.00002714
Iteration 9/1000 | Loss: 0.00002683
Iteration 10/1000 | Loss: 0.00002658
Iteration 11/1000 | Loss: 0.00002654
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002636
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00002627
Iteration 16/1000 | Loss: 0.00002621
Iteration 17/1000 | Loss: 0.00002618
Iteration 18/1000 | Loss: 0.00002618
Iteration 19/1000 | Loss: 0.00002618
Iteration 20/1000 | Loss: 0.00002617
Iteration 21/1000 | Loss: 0.00002617
Iteration 22/1000 | Loss: 0.00002617
Iteration 23/1000 | Loss: 0.00002617
Iteration 24/1000 | Loss: 0.00002617
Iteration 25/1000 | Loss: 0.00002617
Iteration 26/1000 | Loss: 0.00002617
Iteration 27/1000 | Loss: 0.00002617
Iteration 28/1000 | Loss: 0.00002616
Iteration 29/1000 | Loss: 0.00002616
Iteration 30/1000 | Loss: 0.00002615
Iteration 31/1000 | Loss: 0.00002614
Iteration 32/1000 | Loss: 0.00002613
Iteration 33/1000 | Loss: 0.00002613
Iteration 34/1000 | Loss: 0.00002613
Iteration 35/1000 | Loss: 0.00002612
Iteration 36/1000 | Loss: 0.00002612
Iteration 37/1000 | Loss: 0.00002611
Iteration 38/1000 | Loss: 0.00002610
Iteration 39/1000 | Loss: 0.00002610
Iteration 40/1000 | Loss: 0.00002610
Iteration 41/1000 | Loss: 0.00002609
Iteration 42/1000 | Loss: 0.00002609
Iteration 43/1000 | Loss: 0.00002606
Iteration 44/1000 | Loss: 0.00002598
Iteration 45/1000 | Loss: 0.00002597
Iteration 46/1000 | Loss: 0.00002597
Iteration 47/1000 | Loss: 0.00002597
Iteration 48/1000 | Loss: 0.00002596
Iteration 49/1000 | Loss: 0.00002596
Iteration 50/1000 | Loss: 0.00002593
Iteration 51/1000 | Loss: 0.00002592
Iteration 52/1000 | Loss: 0.00002592
Iteration 53/1000 | Loss: 0.00002592
Iteration 54/1000 | Loss: 0.00002591
Iteration 55/1000 | Loss: 0.00002591
Iteration 56/1000 | Loss: 0.00002590
Iteration 57/1000 | Loss: 0.00002590
Iteration 58/1000 | Loss: 0.00002588
Iteration 59/1000 | Loss: 0.00002588
Iteration 60/1000 | Loss: 0.00002588
Iteration 61/1000 | Loss: 0.00002588
Iteration 62/1000 | Loss: 0.00002588
Iteration 63/1000 | Loss: 0.00002588
Iteration 64/1000 | Loss: 0.00002588
Iteration 65/1000 | Loss: 0.00002587
Iteration 66/1000 | Loss: 0.00002587
Iteration 67/1000 | Loss: 0.00002587
Iteration 68/1000 | Loss: 0.00002587
Iteration 69/1000 | Loss: 0.00002587
Iteration 70/1000 | Loss: 0.00002587
Iteration 71/1000 | Loss: 0.00002587
Iteration 72/1000 | Loss: 0.00002587
Iteration 73/1000 | Loss: 0.00002587
Iteration 74/1000 | Loss: 0.00002586
Iteration 75/1000 | Loss: 0.00002586
Iteration 76/1000 | Loss: 0.00002586
Iteration 77/1000 | Loss: 0.00002586
Iteration 78/1000 | Loss: 0.00002585
Iteration 79/1000 | Loss: 0.00002585
Iteration 80/1000 | Loss: 0.00002585
Iteration 81/1000 | Loss: 0.00002585
Iteration 82/1000 | Loss: 0.00002585
Iteration 83/1000 | Loss: 0.00002585
Iteration 84/1000 | Loss: 0.00002585
Iteration 85/1000 | Loss: 0.00002585
Iteration 86/1000 | Loss: 0.00002585
Iteration 87/1000 | Loss: 0.00002585
Iteration 88/1000 | Loss: 0.00002584
Iteration 89/1000 | Loss: 0.00002584
Iteration 90/1000 | Loss: 0.00002584
Iteration 91/1000 | Loss: 0.00002584
Iteration 92/1000 | Loss: 0.00002584
Iteration 93/1000 | Loss: 0.00002584
Iteration 94/1000 | Loss: 0.00002583
Iteration 95/1000 | Loss: 0.00002583
Iteration 96/1000 | Loss: 0.00002583
Iteration 97/1000 | Loss: 0.00002583
Iteration 98/1000 | Loss: 0.00002583
Iteration 99/1000 | Loss: 0.00002583
Iteration 100/1000 | Loss: 0.00002582
Iteration 101/1000 | Loss: 0.00002582
Iteration 102/1000 | Loss: 0.00002582
Iteration 103/1000 | Loss: 0.00002582
Iteration 104/1000 | Loss: 0.00002581
Iteration 105/1000 | Loss: 0.00002581
Iteration 106/1000 | Loss: 0.00002581
Iteration 107/1000 | Loss: 0.00002580
Iteration 108/1000 | Loss: 0.00002580
Iteration 109/1000 | Loss: 0.00002580
Iteration 110/1000 | Loss: 0.00002580
Iteration 111/1000 | Loss: 0.00002580
Iteration 112/1000 | Loss: 0.00002579
Iteration 113/1000 | Loss: 0.00002579
Iteration 114/1000 | Loss: 0.00002579
Iteration 115/1000 | Loss: 0.00002579
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002579
Iteration 118/1000 | Loss: 0.00002578
Iteration 119/1000 | Loss: 0.00002578
Iteration 120/1000 | Loss: 0.00002578
Iteration 121/1000 | Loss: 0.00002577
Iteration 122/1000 | Loss: 0.00002577
Iteration 123/1000 | Loss: 0.00002577
Iteration 124/1000 | Loss: 0.00002577
Iteration 125/1000 | Loss: 0.00002577
Iteration 126/1000 | Loss: 0.00002577
Iteration 127/1000 | Loss: 0.00002577
Iteration 128/1000 | Loss: 0.00002577
Iteration 129/1000 | Loss: 0.00002577
Iteration 130/1000 | Loss: 0.00002576
Iteration 131/1000 | Loss: 0.00002576
Iteration 132/1000 | Loss: 0.00002576
Iteration 133/1000 | Loss: 0.00002576
Iteration 134/1000 | Loss: 0.00002576
Iteration 135/1000 | Loss: 0.00002576
Iteration 136/1000 | Loss: 0.00002576
Iteration 137/1000 | Loss: 0.00002576
Iteration 138/1000 | Loss: 0.00002576
Iteration 139/1000 | Loss: 0.00002576
Iteration 140/1000 | Loss: 0.00002576
Iteration 141/1000 | Loss: 0.00002576
Iteration 142/1000 | Loss: 0.00002576
Iteration 143/1000 | Loss: 0.00002575
Iteration 144/1000 | Loss: 0.00002575
Iteration 145/1000 | Loss: 0.00002575
Iteration 146/1000 | Loss: 0.00002575
Iteration 147/1000 | Loss: 0.00002575
Iteration 148/1000 | Loss: 0.00002575
Iteration 149/1000 | Loss: 0.00002575
Iteration 150/1000 | Loss: 0.00002575
Iteration 151/1000 | Loss: 0.00002575
Iteration 152/1000 | Loss: 0.00002575
Iteration 153/1000 | Loss: 0.00002575
Iteration 154/1000 | Loss: 0.00002575
Iteration 155/1000 | Loss: 0.00002575
Iteration 156/1000 | Loss: 0.00002575
Iteration 157/1000 | Loss: 0.00002575
Iteration 158/1000 | Loss: 0.00002575
Iteration 159/1000 | Loss: 0.00002574
Iteration 160/1000 | Loss: 0.00002574
Iteration 161/1000 | Loss: 0.00002574
Iteration 162/1000 | Loss: 0.00002574
Iteration 163/1000 | Loss: 0.00002574
Iteration 164/1000 | Loss: 0.00002574
Iteration 165/1000 | Loss: 0.00002574
Iteration 166/1000 | Loss: 0.00002574
Iteration 167/1000 | Loss: 0.00002574
Iteration 168/1000 | Loss: 0.00002574
Iteration 169/1000 | Loss: 0.00002574
Iteration 170/1000 | Loss: 0.00002574
Iteration 171/1000 | Loss: 0.00002574
Iteration 172/1000 | Loss: 0.00002574
Iteration 173/1000 | Loss: 0.00002574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.5741323042893782e-05, 2.5741323042893782e-05, 2.5741323042893782e-05, 2.5741323042893782e-05, 2.5741323042893782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5741323042893782e-05

Optimization complete. Final v2v error: 4.211097717285156 mm

Highest mean error: 4.456226348876953 mm for frame 125

Lowest mean error: 3.88614559173584 mm for frame 47

Saving results

Total time: 39.52037763595581
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00674131
Iteration 2/25 | Loss: 0.00077481
Iteration 3/25 | Loss: 0.00060853
Iteration 4/25 | Loss: 0.00059481
Iteration 5/25 | Loss: 0.00059173
Iteration 6/25 | Loss: 0.00059091
Iteration 7/25 | Loss: 0.00059091
Iteration 8/25 | Loss: 0.00059091
Iteration 9/25 | Loss: 0.00059091
Iteration 10/25 | Loss: 0.00059091
Iteration 11/25 | Loss: 0.00059091
Iteration 12/25 | Loss: 0.00059091
Iteration 13/25 | Loss: 0.00059091
Iteration 14/25 | Loss: 0.00059091
Iteration 15/25 | Loss: 0.00059091
Iteration 16/25 | Loss: 0.00059091
Iteration 17/25 | Loss: 0.00059091
Iteration 18/25 | Loss: 0.00059091
Iteration 19/25 | Loss: 0.00059091
Iteration 20/25 | Loss: 0.00059091
Iteration 21/25 | Loss: 0.00059091
Iteration 22/25 | Loss: 0.00059091
Iteration 23/25 | Loss: 0.00059091
Iteration 24/25 | Loss: 0.00059091
Iteration 25/25 | Loss: 0.00059091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68954349
Iteration 2/25 | Loss: 0.00035672
Iteration 3/25 | Loss: 0.00035671
Iteration 4/25 | Loss: 0.00035671
Iteration 5/25 | Loss: 0.00035671
Iteration 6/25 | Loss: 0.00035671
Iteration 7/25 | Loss: 0.00035671
Iteration 8/25 | Loss: 0.00035671
Iteration 9/25 | Loss: 0.00035671
Iteration 10/25 | Loss: 0.00035671
Iteration 11/25 | Loss: 0.00035671
Iteration 12/25 | Loss: 0.00035671
Iteration 13/25 | Loss: 0.00035671
Iteration 14/25 | Loss: 0.00035671
Iteration 15/25 | Loss: 0.00035671
Iteration 16/25 | Loss: 0.00035671
Iteration 17/25 | Loss: 0.00035671
Iteration 18/25 | Loss: 0.00035671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003567053936421871, 0.0003567053936421871, 0.0003567053936421871, 0.0003567053936421871, 0.0003567053936421871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003567053936421871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035671
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001246
Iteration 4/1000 | Loss: 0.00001118
Iteration 5/1000 | Loss: 0.00001046
Iteration 6/1000 | Loss: 0.00001014
Iteration 7/1000 | Loss: 0.00000997
Iteration 8/1000 | Loss: 0.00000994
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000991
Iteration 11/1000 | Loss: 0.00000989
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000975
Iteration 14/1000 | Loss: 0.00000974
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000971
Iteration 17/1000 | Loss: 0.00000971
Iteration 18/1000 | Loss: 0.00000970
Iteration 19/1000 | Loss: 0.00000970
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000967
Iteration 22/1000 | Loss: 0.00000966
Iteration 23/1000 | Loss: 0.00000966
Iteration 24/1000 | Loss: 0.00000965
Iteration 25/1000 | Loss: 0.00000965
Iteration 26/1000 | Loss: 0.00000965
Iteration 27/1000 | Loss: 0.00000964
Iteration 28/1000 | Loss: 0.00000964
Iteration 29/1000 | Loss: 0.00000963
Iteration 30/1000 | Loss: 0.00000963
Iteration 31/1000 | Loss: 0.00000962
Iteration 32/1000 | Loss: 0.00000962
Iteration 33/1000 | Loss: 0.00000961
Iteration 34/1000 | Loss: 0.00000961
Iteration 35/1000 | Loss: 0.00000960
Iteration 36/1000 | Loss: 0.00000959
Iteration 37/1000 | Loss: 0.00000959
Iteration 38/1000 | Loss: 0.00000959
Iteration 39/1000 | Loss: 0.00000959
Iteration 40/1000 | Loss: 0.00000959
Iteration 41/1000 | Loss: 0.00000959
Iteration 42/1000 | Loss: 0.00000959
Iteration 43/1000 | Loss: 0.00000959
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000958
Iteration 48/1000 | Loss: 0.00000958
Iteration 49/1000 | Loss: 0.00000958
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000958
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000955
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000954
Iteration 58/1000 | Loss: 0.00000954
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000954
Iteration 61/1000 | Loss: 0.00000954
Iteration 62/1000 | Loss: 0.00000954
Iteration 63/1000 | Loss: 0.00000954
Iteration 64/1000 | Loss: 0.00000953
Iteration 65/1000 | Loss: 0.00000953
Iteration 66/1000 | Loss: 0.00000953
Iteration 67/1000 | Loss: 0.00000953
Iteration 68/1000 | Loss: 0.00000952
Iteration 69/1000 | Loss: 0.00000952
Iteration 70/1000 | Loss: 0.00000952
Iteration 71/1000 | Loss: 0.00000952
Iteration 72/1000 | Loss: 0.00000952
Iteration 73/1000 | Loss: 0.00000952
Iteration 74/1000 | Loss: 0.00000952
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000951
Iteration 78/1000 | Loss: 0.00000951
Iteration 79/1000 | Loss: 0.00000951
Iteration 80/1000 | Loss: 0.00000951
Iteration 81/1000 | Loss: 0.00000950
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000948
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000946
Iteration 94/1000 | Loss: 0.00000946
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000945
Iteration 98/1000 | Loss: 0.00000945
Iteration 99/1000 | Loss: 0.00000945
Iteration 100/1000 | Loss: 0.00000945
Iteration 101/1000 | Loss: 0.00000945
Iteration 102/1000 | Loss: 0.00000944
Iteration 103/1000 | Loss: 0.00000944
Iteration 104/1000 | Loss: 0.00000944
Iteration 105/1000 | Loss: 0.00000944
Iteration 106/1000 | Loss: 0.00000944
Iteration 107/1000 | Loss: 0.00000944
Iteration 108/1000 | Loss: 0.00000944
Iteration 109/1000 | Loss: 0.00000943
Iteration 110/1000 | Loss: 0.00000943
Iteration 111/1000 | Loss: 0.00000943
Iteration 112/1000 | Loss: 0.00000943
Iteration 113/1000 | Loss: 0.00000943
Iteration 114/1000 | Loss: 0.00000943
Iteration 115/1000 | Loss: 0.00000943
Iteration 116/1000 | Loss: 0.00000943
Iteration 117/1000 | Loss: 0.00000942
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000942
Iteration 126/1000 | Loss: 0.00000942
Iteration 127/1000 | Loss: 0.00000942
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000941
Iteration 133/1000 | Loss: 0.00000941
Iteration 134/1000 | Loss: 0.00000941
Iteration 135/1000 | Loss: 0.00000941
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000941
Iteration 145/1000 | Loss: 0.00000941
Iteration 146/1000 | Loss: 0.00000941
Iteration 147/1000 | Loss: 0.00000941
Iteration 148/1000 | Loss: 0.00000941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [9.413095540367067e-06, 9.413095540367067e-06, 9.413095540367067e-06, 9.413095540367067e-06, 9.413095540367067e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.413095540367067e-06

Optimization complete. Final v2v error: 2.654538869857788 mm

Highest mean error: 2.8620121479034424 mm for frame 99

Lowest mean error: 2.4567060470581055 mm for frame 132

Saving results

Total time: 31.557023763656616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802155
Iteration 2/25 | Loss: 0.00150182
Iteration 3/25 | Loss: 0.00109917
Iteration 4/25 | Loss: 0.00085880
Iteration 5/25 | Loss: 0.00084600
Iteration 6/25 | Loss: 0.00079985
Iteration 7/25 | Loss: 0.00075653
Iteration 8/25 | Loss: 0.00072609
Iteration 9/25 | Loss: 0.00070389
Iteration 10/25 | Loss: 0.00070124
Iteration 11/25 | Loss: 0.00069028
Iteration 12/25 | Loss: 0.00068630
Iteration 13/25 | Loss: 0.00067379
Iteration 14/25 | Loss: 0.00066900
Iteration 15/25 | Loss: 0.00066740
Iteration 16/25 | Loss: 0.00066620
Iteration 17/25 | Loss: 0.00066696
Iteration 18/25 | Loss: 0.00066460
Iteration 19/25 | Loss: 0.00066348
Iteration 20/25 | Loss: 0.00066311
Iteration 21/25 | Loss: 0.00066300
Iteration 22/25 | Loss: 0.00066300
Iteration 23/25 | Loss: 0.00066300
Iteration 24/25 | Loss: 0.00066300
Iteration 25/25 | Loss: 0.00066300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92216587
Iteration 2/25 | Loss: 0.00034628
Iteration 3/25 | Loss: 0.00034628
Iteration 4/25 | Loss: 0.00034628
Iteration 5/25 | Loss: 0.00034628
Iteration 6/25 | Loss: 0.00034628
Iteration 7/25 | Loss: 0.00034628
Iteration 8/25 | Loss: 0.00034628
Iteration 9/25 | Loss: 0.00034628
Iteration 10/25 | Loss: 0.00034628
Iteration 11/25 | Loss: 0.00034628
Iteration 12/25 | Loss: 0.00034628
Iteration 13/25 | Loss: 0.00034628
Iteration 14/25 | Loss: 0.00034628
Iteration 15/25 | Loss: 0.00034628
Iteration 16/25 | Loss: 0.00034628
Iteration 17/25 | Loss: 0.00034628
Iteration 18/25 | Loss: 0.00034628
Iteration 19/25 | Loss: 0.00034628
Iteration 20/25 | Loss: 0.00034628
Iteration 21/25 | Loss: 0.00034628
Iteration 22/25 | Loss: 0.00034628
Iteration 23/25 | Loss: 0.00034628
Iteration 24/25 | Loss: 0.00034628
Iteration 25/25 | Loss: 0.00034628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034628
Iteration 2/1000 | Loss: 0.00003623
Iteration 3/1000 | Loss: 0.00002521
Iteration 4/1000 | Loss: 0.00002189
Iteration 5/1000 | Loss: 0.00038362
Iteration 6/1000 | Loss: 0.00002062
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001932
Iteration 9/1000 | Loss: 0.00001894
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00013487
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00001817
Iteration 14/1000 | Loss: 0.00001792
Iteration 15/1000 | Loss: 0.00001789
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001779
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001774
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001768
Iteration 61/1000 | Loss: 0.00001768
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001751
Iteration 77/1000 | Loss: 0.00001751
Iteration 78/1000 | Loss: 0.00001751
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001750
Iteration 81/1000 | Loss: 0.00001750
Iteration 82/1000 | Loss: 0.00001750
Iteration 83/1000 | Loss: 0.00001749
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001749
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001748
Iteration 91/1000 | Loss: 0.00001748
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001743
Iteration 105/1000 | Loss: 0.00001743
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001740
Iteration 135/1000 | Loss: 0.00001740
Iteration 136/1000 | Loss: 0.00001740
Iteration 137/1000 | Loss: 0.00001740
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001739
Iteration 141/1000 | Loss: 0.00001739
Iteration 142/1000 | Loss: 0.00001739
Iteration 143/1000 | Loss: 0.00001739
Iteration 144/1000 | Loss: 0.00001739
Iteration 145/1000 | Loss: 0.00001739
Iteration 146/1000 | Loss: 0.00001739
Iteration 147/1000 | Loss: 0.00001739
Iteration 148/1000 | Loss: 0.00001739
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001739
Iteration 153/1000 | Loss: 0.00001739
Iteration 154/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.7389513232046738e-05, 1.7389513232046738e-05, 1.7389513232046738e-05, 1.7389513232046738e-05, 1.7389513232046738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7389513232046738e-05

Optimization complete. Final v2v error: 3.5087215900421143 mm

Highest mean error: 5.326162338256836 mm for frame 163

Lowest mean error: 2.863112211227417 mm for frame 0

Saving results

Total time: 78.61757946014404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114546
Iteration 2/25 | Loss: 0.00263466
Iteration 3/25 | Loss: 0.00152304
Iteration 4/25 | Loss: 0.00132438
Iteration 5/25 | Loss: 0.00143518
Iteration 6/25 | Loss: 0.00159374
Iteration 7/25 | Loss: 0.00146019
Iteration 8/25 | Loss: 0.00130885
Iteration 9/25 | Loss: 0.00117487
Iteration 10/25 | Loss: 0.00110130
Iteration 11/25 | Loss: 0.00104614
Iteration 12/25 | Loss: 0.00098180
Iteration 13/25 | Loss: 0.00092793
Iteration 14/25 | Loss: 0.00085316
Iteration 15/25 | Loss: 0.00083850
Iteration 16/25 | Loss: 0.00081686
Iteration 17/25 | Loss: 0.00079904
Iteration 18/25 | Loss: 0.00078446
Iteration 19/25 | Loss: 0.00077243
Iteration 20/25 | Loss: 0.00076484
Iteration 21/25 | Loss: 0.00076582
Iteration 22/25 | Loss: 0.00076808
Iteration 23/25 | Loss: 0.00076631
Iteration 24/25 | Loss: 0.00076511
Iteration 25/25 | Loss: 0.00076011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06112051
Iteration 2/25 | Loss: 0.00103063
Iteration 3/25 | Loss: 0.00057699
Iteration 4/25 | Loss: 0.00057699
Iteration 5/25 | Loss: 0.00057699
Iteration 6/25 | Loss: 0.00057699
Iteration 7/25 | Loss: 0.00057699
Iteration 8/25 | Loss: 0.00057699
Iteration 9/25 | Loss: 0.00057699
Iteration 10/25 | Loss: 0.00057699
Iteration 11/25 | Loss: 0.00057699
Iteration 12/25 | Loss: 0.00057699
Iteration 13/25 | Loss: 0.00057699
Iteration 14/25 | Loss: 0.00057699
Iteration 15/25 | Loss: 0.00057699
Iteration 16/25 | Loss: 0.00057699
Iteration 17/25 | Loss: 0.00057699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005769857671111822, 0.0005769857671111822, 0.0005769857671111822, 0.0005769857671111822, 0.0005769857671111822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005769857671111822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057699
Iteration 2/1000 | Loss: 0.00058685
Iteration 3/1000 | Loss: 0.00045611
Iteration 4/1000 | Loss: 0.00084222
Iteration 5/1000 | Loss: 0.00027688
Iteration 6/1000 | Loss: 0.00082369
Iteration 7/1000 | Loss: 0.00038700
Iteration 8/1000 | Loss: 0.00011260
Iteration 9/1000 | Loss: 0.00010131
Iteration 10/1000 | Loss: 0.00008614
Iteration 11/1000 | Loss: 0.00012929
Iteration 12/1000 | Loss: 0.00041638
Iteration 13/1000 | Loss: 0.00019323
Iteration 14/1000 | Loss: 0.00011519
Iteration 15/1000 | Loss: 0.00052848
Iteration 16/1000 | Loss: 0.00011445
Iteration 17/1000 | Loss: 0.00009949
Iteration 18/1000 | Loss: 0.00006222
Iteration 19/1000 | Loss: 0.00009138
Iteration 20/1000 | Loss: 0.00022416
Iteration 21/1000 | Loss: 0.00046045
Iteration 22/1000 | Loss: 0.00017155
Iteration 23/1000 | Loss: 0.00010583
Iteration 24/1000 | Loss: 0.00010679
Iteration 25/1000 | Loss: 0.00009192
Iteration 26/1000 | Loss: 0.00006336
Iteration 27/1000 | Loss: 0.00005664
Iteration 28/1000 | Loss: 0.00029326
Iteration 29/1000 | Loss: 0.00012417
Iteration 30/1000 | Loss: 0.00010353
Iteration 31/1000 | Loss: 0.00040508
Iteration 32/1000 | Loss: 0.00009740
Iteration 33/1000 | Loss: 0.00008633
Iteration 34/1000 | Loss: 0.00030093
Iteration 35/1000 | Loss: 0.00031995
Iteration 36/1000 | Loss: 0.00076015
Iteration 37/1000 | Loss: 0.00064841
Iteration 38/1000 | Loss: 0.00061845
Iteration 39/1000 | Loss: 0.00033396
Iteration 40/1000 | Loss: 0.00063335
Iteration 41/1000 | Loss: 0.00016491
Iteration 42/1000 | Loss: 0.00015516
Iteration 43/1000 | Loss: 0.00005024
Iteration 44/1000 | Loss: 0.00004065
Iteration 45/1000 | Loss: 0.00003547
Iteration 46/1000 | Loss: 0.00021753
Iteration 47/1000 | Loss: 0.00076974
Iteration 48/1000 | Loss: 0.00025276
Iteration 49/1000 | Loss: 0.00020674
Iteration 50/1000 | Loss: 0.00015097
Iteration 51/1000 | Loss: 0.00020035
Iteration 52/1000 | Loss: 0.00016975
Iteration 53/1000 | Loss: 0.00018290
Iteration 54/1000 | Loss: 0.00016927
Iteration 55/1000 | Loss: 0.00020464
Iteration 56/1000 | Loss: 0.00003999
Iteration 57/1000 | Loss: 0.00003253
Iteration 58/1000 | Loss: 0.00002968
Iteration 59/1000 | Loss: 0.00002739
Iteration 60/1000 | Loss: 0.00021046
Iteration 61/1000 | Loss: 0.00003435
Iteration 62/1000 | Loss: 0.00003017
Iteration 63/1000 | Loss: 0.00002765
Iteration 64/1000 | Loss: 0.00002601
Iteration 65/1000 | Loss: 0.00005126
Iteration 66/1000 | Loss: 0.00002774
Iteration 67/1000 | Loss: 0.00006267
Iteration 68/1000 | Loss: 0.00006735
Iteration 69/1000 | Loss: 0.00012613
Iteration 70/1000 | Loss: 0.00002766
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002214
Iteration 73/1000 | Loss: 0.00019734
Iteration 74/1000 | Loss: 0.00020912
Iteration 75/1000 | Loss: 0.00020908
Iteration 76/1000 | Loss: 0.00009316
Iteration 77/1000 | Loss: 0.00003440
Iteration 78/1000 | Loss: 0.00002702
Iteration 79/1000 | Loss: 0.00002463
Iteration 80/1000 | Loss: 0.00029351
Iteration 81/1000 | Loss: 0.00004943
Iteration 82/1000 | Loss: 0.00003255
Iteration 83/1000 | Loss: 0.00014052
Iteration 84/1000 | Loss: 0.00005523
Iteration 85/1000 | Loss: 0.00007903
Iteration 86/1000 | Loss: 0.00003872
Iteration 87/1000 | Loss: 0.00002151
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001837
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001777
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001759
Iteration 103/1000 | Loss: 0.00001758
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001752
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001744
Iteration 117/1000 | Loss: 0.00001726
Iteration 118/1000 | Loss: 0.00015335
Iteration 119/1000 | Loss: 0.00014451
Iteration 120/1000 | Loss: 0.00007133
Iteration 121/1000 | Loss: 0.00003282
Iteration 122/1000 | Loss: 0.00002854
Iteration 123/1000 | Loss: 0.00002033
Iteration 124/1000 | Loss: 0.00001775
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001640
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001575
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001540
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001538
Iteration 143/1000 | Loss: 0.00001538
Iteration 144/1000 | Loss: 0.00001538
Iteration 145/1000 | Loss: 0.00001538
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001536
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001536
Iteration 156/1000 | Loss: 0.00001535
Iteration 157/1000 | Loss: 0.00001535
Iteration 158/1000 | Loss: 0.00001535
Iteration 159/1000 | Loss: 0.00001535
Iteration 160/1000 | Loss: 0.00001534
Iteration 161/1000 | Loss: 0.00001534
Iteration 162/1000 | Loss: 0.00001534
Iteration 163/1000 | Loss: 0.00001534
Iteration 164/1000 | Loss: 0.00001534
Iteration 165/1000 | Loss: 0.00001534
Iteration 166/1000 | Loss: 0.00001534
Iteration 167/1000 | Loss: 0.00001534
Iteration 168/1000 | Loss: 0.00001533
Iteration 169/1000 | Loss: 0.00001533
Iteration 170/1000 | Loss: 0.00001533
Iteration 171/1000 | Loss: 0.00001533
Iteration 172/1000 | Loss: 0.00001533
Iteration 173/1000 | Loss: 0.00001533
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001532
Iteration 177/1000 | Loss: 0.00001532
Iteration 178/1000 | Loss: 0.00001531
Iteration 179/1000 | Loss: 0.00001531
Iteration 180/1000 | Loss: 0.00001531
Iteration 181/1000 | Loss: 0.00001530
Iteration 182/1000 | Loss: 0.00001529
Iteration 183/1000 | Loss: 0.00001529
Iteration 184/1000 | Loss: 0.00001529
Iteration 185/1000 | Loss: 0.00001529
Iteration 186/1000 | Loss: 0.00001529
Iteration 187/1000 | Loss: 0.00001528
Iteration 188/1000 | Loss: 0.00001528
Iteration 189/1000 | Loss: 0.00001528
Iteration 190/1000 | Loss: 0.00001528
Iteration 191/1000 | Loss: 0.00001528
Iteration 192/1000 | Loss: 0.00001528
Iteration 193/1000 | Loss: 0.00001528
Iteration 194/1000 | Loss: 0.00001528
Iteration 195/1000 | Loss: 0.00001528
Iteration 196/1000 | Loss: 0.00001528
Iteration 197/1000 | Loss: 0.00001528
Iteration 198/1000 | Loss: 0.00001528
Iteration 199/1000 | Loss: 0.00001528
Iteration 200/1000 | Loss: 0.00001528
Iteration 201/1000 | Loss: 0.00001527
Iteration 202/1000 | Loss: 0.00001527
Iteration 203/1000 | Loss: 0.00001527
Iteration 204/1000 | Loss: 0.00001527
Iteration 205/1000 | Loss: 0.00001527
Iteration 206/1000 | Loss: 0.00001527
Iteration 207/1000 | Loss: 0.00001527
Iteration 208/1000 | Loss: 0.00001526
Iteration 209/1000 | Loss: 0.00001526
Iteration 210/1000 | Loss: 0.00001526
Iteration 211/1000 | Loss: 0.00001526
Iteration 212/1000 | Loss: 0.00001526
Iteration 213/1000 | Loss: 0.00001526
Iteration 214/1000 | Loss: 0.00001526
Iteration 215/1000 | Loss: 0.00001526
Iteration 216/1000 | Loss: 0.00001526
Iteration 217/1000 | Loss: 0.00001526
Iteration 218/1000 | Loss: 0.00001526
Iteration 219/1000 | Loss: 0.00001526
Iteration 220/1000 | Loss: 0.00001526
Iteration 221/1000 | Loss: 0.00001526
Iteration 222/1000 | Loss: 0.00001526
Iteration 223/1000 | Loss: 0.00001526
Iteration 224/1000 | Loss: 0.00001526
Iteration 225/1000 | Loss: 0.00001526
Iteration 226/1000 | Loss: 0.00001526
Iteration 227/1000 | Loss: 0.00001526
Iteration 228/1000 | Loss: 0.00001526
Iteration 229/1000 | Loss: 0.00001526
Iteration 230/1000 | Loss: 0.00001526
Iteration 231/1000 | Loss: 0.00001526
Iteration 232/1000 | Loss: 0.00001526
Iteration 233/1000 | Loss: 0.00001526
Iteration 234/1000 | Loss: 0.00001526
Iteration 235/1000 | Loss: 0.00001526
Iteration 236/1000 | Loss: 0.00001526
Iteration 237/1000 | Loss: 0.00001526
Iteration 238/1000 | Loss: 0.00001526
Iteration 239/1000 | Loss: 0.00001526
Iteration 240/1000 | Loss: 0.00001526
Iteration 241/1000 | Loss: 0.00001526
Iteration 242/1000 | Loss: 0.00001526
Iteration 243/1000 | Loss: 0.00001526
Iteration 244/1000 | Loss: 0.00001526
Iteration 245/1000 | Loss: 0.00001526
Iteration 246/1000 | Loss: 0.00001526
Iteration 247/1000 | Loss: 0.00001526
Iteration 248/1000 | Loss: 0.00001526
Iteration 249/1000 | Loss: 0.00001526
Iteration 250/1000 | Loss: 0.00001526
Iteration 251/1000 | Loss: 0.00001526
Iteration 252/1000 | Loss: 0.00001526
Iteration 253/1000 | Loss: 0.00001526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.5256536244123708e-05, 1.5256536244123708e-05, 1.5256536244123708e-05, 1.5256536244123708e-05, 1.5256536244123708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5256536244123708e-05

Optimization complete. Final v2v error: 3.347212553024292 mm

Highest mean error: 4.330380439758301 mm for frame 20

Lowest mean error: 3.135249376296997 mm for frame 149

Saving results

Total time: 218.70531630516052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831561
Iteration 2/25 | Loss: 0.00133206
Iteration 3/25 | Loss: 0.00085697
Iteration 4/25 | Loss: 0.00080564
Iteration 5/25 | Loss: 0.00078971
Iteration 6/25 | Loss: 0.00078537
Iteration 7/25 | Loss: 0.00078390
Iteration 8/25 | Loss: 0.00078362
Iteration 9/25 | Loss: 0.00078362
Iteration 10/25 | Loss: 0.00078362
Iteration 11/25 | Loss: 0.00078362
Iteration 12/25 | Loss: 0.00078362
Iteration 13/25 | Loss: 0.00078362
Iteration 14/25 | Loss: 0.00078362
Iteration 15/25 | Loss: 0.00078362
Iteration 16/25 | Loss: 0.00078362
Iteration 17/25 | Loss: 0.00078362
Iteration 18/25 | Loss: 0.00078362
Iteration 19/25 | Loss: 0.00078362
Iteration 20/25 | Loss: 0.00078362
Iteration 21/25 | Loss: 0.00078362
Iteration 22/25 | Loss: 0.00078362
Iteration 23/25 | Loss: 0.00078362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007836158620193601, 0.0007836158620193601, 0.0007836158620193601, 0.0007836158620193601, 0.0007836158620193601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007836158620193601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11564326
Iteration 2/25 | Loss: 0.00033251
Iteration 3/25 | Loss: 0.00033251
Iteration 4/25 | Loss: 0.00033251
Iteration 5/25 | Loss: 0.00033251
Iteration 6/25 | Loss: 0.00033251
Iteration 7/25 | Loss: 0.00033251
Iteration 8/25 | Loss: 0.00033251
Iteration 9/25 | Loss: 0.00033251
Iteration 10/25 | Loss: 0.00033251
Iteration 11/25 | Loss: 0.00033251
Iteration 12/25 | Loss: 0.00033251
Iteration 13/25 | Loss: 0.00033251
Iteration 14/25 | Loss: 0.00033251
Iteration 15/25 | Loss: 0.00033251
Iteration 16/25 | Loss: 0.00033251
Iteration 17/25 | Loss: 0.00033251
Iteration 18/25 | Loss: 0.00033251
Iteration 19/25 | Loss: 0.00033251
Iteration 20/25 | Loss: 0.00033251
Iteration 21/25 | Loss: 0.00033251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003325061989016831, 0.0003325061989016831, 0.0003325061989016831, 0.0003325061989016831, 0.0003325061989016831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003325061989016831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033251
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00003841
Iteration 4/1000 | Loss: 0.00003445
Iteration 5/1000 | Loss: 0.00003279
Iteration 6/1000 | Loss: 0.00003173
Iteration 7/1000 | Loss: 0.00003097
Iteration 8/1000 | Loss: 0.00003023
Iteration 9/1000 | Loss: 0.00002959
Iteration 10/1000 | Loss: 0.00002917
Iteration 11/1000 | Loss: 0.00002875
Iteration 12/1000 | Loss: 0.00002841
Iteration 13/1000 | Loss: 0.00002809
Iteration 14/1000 | Loss: 0.00002789
Iteration 15/1000 | Loss: 0.00002769
Iteration 16/1000 | Loss: 0.00002757
Iteration 17/1000 | Loss: 0.00002757
Iteration 18/1000 | Loss: 0.00002757
Iteration 19/1000 | Loss: 0.00002757
Iteration 20/1000 | Loss: 0.00002755
Iteration 21/1000 | Loss: 0.00002753
Iteration 22/1000 | Loss: 0.00002749
Iteration 23/1000 | Loss: 0.00002748
Iteration 24/1000 | Loss: 0.00002745
Iteration 25/1000 | Loss: 0.00002745
Iteration 26/1000 | Loss: 0.00002744
Iteration 27/1000 | Loss: 0.00002743
Iteration 28/1000 | Loss: 0.00002742
Iteration 29/1000 | Loss: 0.00002738
Iteration 30/1000 | Loss: 0.00002731
Iteration 31/1000 | Loss: 0.00002731
Iteration 32/1000 | Loss: 0.00002730
Iteration 33/1000 | Loss: 0.00002729
Iteration 34/1000 | Loss: 0.00002729
Iteration 35/1000 | Loss: 0.00002728
Iteration 36/1000 | Loss: 0.00002727
Iteration 37/1000 | Loss: 0.00002727
Iteration 38/1000 | Loss: 0.00002727
Iteration 39/1000 | Loss: 0.00002727
Iteration 40/1000 | Loss: 0.00002727
Iteration 41/1000 | Loss: 0.00002726
Iteration 42/1000 | Loss: 0.00002726
Iteration 43/1000 | Loss: 0.00002726
Iteration 44/1000 | Loss: 0.00002726
Iteration 45/1000 | Loss: 0.00002726
Iteration 46/1000 | Loss: 0.00002726
Iteration 47/1000 | Loss: 0.00002726
Iteration 48/1000 | Loss: 0.00002726
Iteration 49/1000 | Loss: 0.00002725
Iteration 50/1000 | Loss: 0.00002725
Iteration 51/1000 | Loss: 0.00002725
Iteration 52/1000 | Loss: 0.00002725
Iteration 53/1000 | Loss: 0.00002725
Iteration 54/1000 | Loss: 0.00002724
Iteration 55/1000 | Loss: 0.00002724
Iteration 56/1000 | Loss: 0.00002723
Iteration 57/1000 | Loss: 0.00002723
Iteration 58/1000 | Loss: 0.00002723
Iteration 59/1000 | Loss: 0.00002723
Iteration 60/1000 | Loss: 0.00002723
Iteration 61/1000 | Loss: 0.00002722
Iteration 62/1000 | Loss: 0.00002722
Iteration 63/1000 | Loss: 0.00002722
Iteration 64/1000 | Loss: 0.00002722
Iteration 65/1000 | Loss: 0.00002721
Iteration 66/1000 | Loss: 0.00002721
Iteration 67/1000 | Loss: 0.00002721
Iteration 68/1000 | Loss: 0.00002721
Iteration 69/1000 | Loss: 0.00002720
Iteration 70/1000 | Loss: 0.00002720
Iteration 71/1000 | Loss: 0.00002720
Iteration 72/1000 | Loss: 0.00002720
Iteration 73/1000 | Loss: 0.00002720
Iteration 74/1000 | Loss: 0.00002720
Iteration 75/1000 | Loss: 0.00002720
Iteration 76/1000 | Loss: 0.00002720
Iteration 77/1000 | Loss: 0.00002719
Iteration 78/1000 | Loss: 0.00002719
Iteration 79/1000 | Loss: 0.00002719
Iteration 80/1000 | Loss: 0.00002719
Iteration 81/1000 | Loss: 0.00002719
Iteration 82/1000 | Loss: 0.00002718
Iteration 83/1000 | Loss: 0.00002718
Iteration 84/1000 | Loss: 0.00002718
Iteration 85/1000 | Loss: 0.00002718
Iteration 86/1000 | Loss: 0.00002718
Iteration 87/1000 | Loss: 0.00002718
Iteration 88/1000 | Loss: 0.00002718
Iteration 89/1000 | Loss: 0.00002718
Iteration 90/1000 | Loss: 0.00002718
Iteration 91/1000 | Loss: 0.00002717
Iteration 92/1000 | Loss: 0.00002717
Iteration 93/1000 | Loss: 0.00002717
Iteration 94/1000 | Loss: 0.00002717
Iteration 95/1000 | Loss: 0.00002716
Iteration 96/1000 | Loss: 0.00002716
Iteration 97/1000 | Loss: 0.00002716
Iteration 98/1000 | Loss: 0.00002716
Iteration 99/1000 | Loss: 0.00002716
Iteration 100/1000 | Loss: 0.00002715
Iteration 101/1000 | Loss: 0.00002715
Iteration 102/1000 | Loss: 0.00002715
Iteration 103/1000 | Loss: 0.00002715
Iteration 104/1000 | Loss: 0.00002715
Iteration 105/1000 | Loss: 0.00002715
Iteration 106/1000 | Loss: 0.00002715
Iteration 107/1000 | Loss: 0.00002715
Iteration 108/1000 | Loss: 0.00002715
Iteration 109/1000 | Loss: 0.00002715
Iteration 110/1000 | Loss: 0.00002715
Iteration 111/1000 | Loss: 0.00002715
Iteration 112/1000 | Loss: 0.00002715
Iteration 113/1000 | Loss: 0.00002714
Iteration 114/1000 | Loss: 0.00002714
Iteration 115/1000 | Loss: 0.00002714
Iteration 116/1000 | Loss: 0.00002714
Iteration 117/1000 | Loss: 0.00002714
Iteration 118/1000 | Loss: 0.00002714
Iteration 119/1000 | Loss: 0.00002714
Iteration 120/1000 | Loss: 0.00002714
Iteration 121/1000 | Loss: 0.00002714
Iteration 122/1000 | Loss: 0.00002713
Iteration 123/1000 | Loss: 0.00002713
Iteration 124/1000 | Loss: 0.00002713
Iteration 125/1000 | Loss: 0.00002713
Iteration 126/1000 | Loss: 0.00002713
Iteration 127/1000 | Loss: 0.00002712
Iteration 128/1000 | Loss: 0.00002712
Iteration 129/1000 | Loss: 0.00002712
Iteration 130/1000 | Loss: 0.00002711
Iteration 131/1000 | Loss: 0.00002711
Iteration 132/1000 | Loss: 0.00002711
Iteration 133/1000 | Loss: 0.00002711
Iteration 134/1000 | Loss: 0.00002711
Iteration 135/1000 | Loss: 0.00002711
Iteration 136/1000 | Loss: 0.00002711
Iteration 137/1000 | Loss: 0.00002711
Iteration 138/1000 | Loss: 0.00002710
Iteration 139/1000 | Loss: 0.00002710
Iteration 140/1000 | Loss: 0.00002710
Iteration 141/1000 | Loss: 0.00002710
Iteration 142/1000 | Loss: 0.00002710
Iteration 143/1000 | Loss: 0.00002710
Iteration 144/1000 | Loss: 0.00002710
Iteration 145/1000 | Loss: 0.00002710
Iteration 146/1000 | Loss: 0.00002710
Iteration 147/1000 | Loss: 0.00002710
Iteration 148/1000 | Loss: 0.00002709
Iteration 149/1000 | Loss: 0.00002709
Iteration 150/1000 | Loss: 0.00002709
Iteration 151/1000 | Loss: 0.00002709
Iteration 152/1000 | Loss: 0.00002709
Iteration 153/1000 | Loss: 0.00002709
Iteration 154/1000 | Loss: 0.00002709
Iteration 155/1000 | Loss: 0.00002709
Iteration 156/1000 | Loss: 0.00002709
Iteration 157/1000 | Loss: 0.00002709
Iteration 158/1000 | Loss: 0.00002709
Iteration 159/1000 | Loss: 0.00002708
Iteration 160/1000 | Loss: 0.00002708
Iteration 161/1000 | Loss: 0.00002708
Iteration 162/1000 | Loss: 0.00002708
Iteration 163/1000 | Loss: 0.00002708
Iteration 164/1000 | Loss: 0.00002708
Iteration 165/1000 | Loss: 0.00002708
Iteration 166/1000 | Loss: 0.00002708
Iteration 167/1000 | Loss: 0.00002708
Iteration 168/1000 | Loss: 0.00002708
Iteration 169/1000 | Loss: 0.00002708
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002708
Iteration 172/1000 | Loss: 0.00002708
Iteration 173/1000 | Loss: 0.00002707
Iteration 174/1000 | Loss: 0.00002707
Iteration 175/1000 | Loss: 0.00002707
Iteration 176/1000 | Loss: 0.00002707
Iteration 177/1000 | Loss: 0.00002707
Iteration 178/1000 | Loss: 0.00002707
Iteration 179/1000 | Loss: 0.00002707
Iteration 180/1000 | Loss: 0.00002707
Iteration 181/1000 | Loss: 0.00002707
Iteration 182/1000 | Loss: 0.00002707
Iteration 183/1000 | Loss: 0.00002707
Iteration 184/1000 | Loss: 0.00002707
Iteration 185/1000 | Loss: 0.00002707
Iteration 186/1000 | Loss: 0.00002707
Iteration 187/1000 | Loss: 0.00002707
Iteration 188/1000 | Loss: 0.00002707
Iteration 189/1000 | Loss: 0.00002707
Iteration 190/1000 | Loss: 0.00002706
Iteration 191/1000 | Loss: 0.00002706
Iteration 192/1000 | Loss: 0.00002706
Iteration 193/1000 | Loss: 0.00002706
Iteration 194/1000 | Loss: 0.00002706
Iteration 195/1000 | Loss: 0.00002706
Iteration 196/1000 | Loss: 0.00002706
Iteration 197/1000 | Loss: 0.00002706
Iteration 198/1000 | Loss: 0.00002706
Iteration 199/1000 | Loss: 0.00002706
Iteration 200/1000 | Loss: 0.00002706
Iteration 201/1000 | Loss: 0.00002706
Iteration 202/1000 | Loss: 0.00002706
Iteration 203/1000 | Loss: 0.00002706
Iteration 204/1000 | Loss: 0.00002706
Iteration 205/1000 | Loss: 0.00002706
Iteration 206/1000 | Loss: 0.00002706
Iteration 207/1000 | Loss: 0.00002706
Iteration 208/1000 | Loss: 0.00002706
Iteration 209/1000 | Loss: 0.00002705
Iteration 210/1000 | Loss: 0.00002705
Iteration 211/1000 | Loss: 0.00002705
Iteration 212/1000 | Loss: 0.00002705
Iteration 213/1000 | Loss: 0.00002705
Iteration 214/1000 | Loss: 0.00002705
Iteration 215/1000 | Loss: 0.00002705
Iteration 216/1000 | Loss: 0.00002705
Iteration 217/1000 | Loss: 0.00002705
Iteration 218/1000 | Loss: 0.00002705
Iteration 219/1000 | Loss: 0.00002705
Iteration 220/1000 | Loss: 0.00002705
Iteration 221/1000 | Loss: 0.00002705
Iteration 222/1000 | Loss: 0.00002705
Iteration 223/1000 | Loss: 0.00002705
Iteration 224/1000 | Loss: 0.00002705
Iteration 225/1000 | Loss: 0.00002705
Iteration 226/1000 | Loss: 0.00002705
Iteration 227/1000 | Loss: 0.00002705
Iteration 228/1000 | Loss: 0.00002705
Iteration 229/1000 | Loss: 0.00002705
Iteration 230/1000 | Loss: 0.00002705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.7047222829423845e-05, 2.7047222829423845e-05, 2.7047222829423845e-05, 2.7047222829423845e-05, 2.7047222829423845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7047222829423845e-05

Optimization complete. Final v2v error: 4.2086968421936035 mm

Highest mean error: 5.271302700042725 mm for frame 59

Lowest mean error: 3.5194759368896484 mm for frame 129

Saving results

Total time: 51.5792441368103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514568
Iteration 2/25 | Loss: 0.00087910
Iteration 3/25 | Loss: 0.00070991
Iteration 4/25 | Loss: 0.00067231
Iteration 5/25 | Loss: 0.00066250
Iteration 6/25 | Loss: 0.00066080
Iteration 7/25 | Loss: 0.00066080
Iteration 8/25 | Loss: 0.00066080
Iteration 9/25 | Loss: 0.00066080
Iteration 10/25 | Loss: 0.00066080
Iteration 11/25 | Loss: 0.00066080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006607963587157428, 0.0006607963587157428, 0.0006607963587157428, 0.0006607963587157428, 0.0006607963587157428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006607963587157428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44114566
Iteration 2/25 | Loss: 0.00023307
Iteration 3/25 | Loss: 0.00023303
Iteration 4/25 | Loss: 0.00023303
Iteration 5/25 | Loss: 0.00023303
Iteration 6/25 | Loss: 0.00023303
Iteration 7/25 | Loss: 0.00023303
Iteration 8/25 | Loss: 0.00023303
Iteration 9/25 | Loss: 0.00023303
Iteration 10/25 | Loss: 0.00023303
Iteration 11/25 | Loss: 0.00023303
Iteration 12/25 | Loss: 0.00023303
Iteration 13/25 | Loss: 0.00023303
Iteration 14/25 | Loss: 0.00023303
Iteration 15/25 | Loss: 0.00023303
Iteration 16/25 | Loss: 0.00023303
Iteration 17/25 | Loss: 0.00023303
Iteration 18/25 | Loss: 0.00023303
Iteration 19/25 | Loss: 0.00023303
Iteration 20/25 | Loss: 0.00023303
Iteration 21/25 | Loss: 0.00023303
Iteration 22/25 | Loss: 0.00023303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002330290590180084, 0.0002330290590180084, 0.0002330290590180084, 0.0002330290590180084, 0.0002330290590180084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002330290590180084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023303
Iteration 2/1000 | Loss: 0.00002833
Iteration 3/1000 | Loss: 0.00001675
Iteration 4/1000 | Loss: 0.00001483
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001301
Iteration 8/1000 | Loss: 0.00001286
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001227
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001213
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001212
Iteration 67/1000 | Loss: 0.00001212
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00001211
Iteration 76/1000 | Loss: 0.00001211
Iteration 77/1000 | Loss: 0.00001211
Iteration 78/1000 | Loss: 0.00001211
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001206
Iteration 110/1000 | Loss: 0.00001206
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001202
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Iteration 145/1000 | Loss: 0.00001201
Iteration 146/1000 | Loss: 0.00001201
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.2013286323053762e-05, 1.2013286323053762e-05, 1.2013286323053762e-05, 1.2013286323053762e-05, 1.2013286323053762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2013286323053762e-05

Optimization complete. Final v2v error: 2.9371163845062256 mm

Highest mean error: 3.1270103454589844 mm for frame 240

Lowest mean error: 2.799384355545044 mm for frame 205

Saving results

Total time: 40.0046763420105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698194
Iteration 2/25 | Loss: 0.00126281
Iteration 3/25 | Loss: 0.00085972
Iteration 4/25 | Loss: 0.00077669
Iteration 5/25 | Loss: 0.00075932
Iteration 6/25 | Loss: 0.00074965
Iteration 7/25 | Loss: 0.00073947
Iteration 8/25 | Loss: 0.00073608
Iteration 9/25 | Loss: 0.00072927
Iteration 10/25 | Loss: 0.00072233
Iteration 11/25 | Loss: 0.00072088
Iteration 12/25 | Loss: 0.00072047
Iteration 13/25 | Loss: 0.00071733
Iteration 14/25 | Loss: 0.00071613
Iteration 15/25 | Loss: 0.00071606
Iteration 16/25 | Loss: 0.00071606
Iteration 17/25 | Loss: 0.00071606
Iteration 18/25 | Loss: 0.00071606
Iteration 19/25 | Loss: 0.00071606
Iteration 20/25 | Loss: 0.00071606
Iteration 21/25 | Loss: 0.00071606
Iteration 22/25 | Loss: 0.00071606
Iteration 23/25 | Loss: 0.00071605
Iteration 24/25 | Loss: 0.00071605
Iteration 25/25 | Loss: 0.00071605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20110750
Iteration 2/25 | Loss: 0.00038469
Iteration 3/25 | Loss: 0.00038449
Iteration 4/25 | Loss: 0.00038448
Iteration 5/25 | Loss: 0.00038448
Iteration 6/25 | Loss: 0.00038448
Iteration 7/25 | Loss: 0.00038448
Iteration 8/25 | Loss: 0.00038448
Iteration 9/25 | Loss: 0.00038448
Iteration 10/25 | Loss: 0.00038448
Iteration 11/25 | Loss: 0.00038448
Iteration 12/25 | Loss: 0.00038448
Iteration 13/25 | Loss: 0.00038448
Iteration 14/25 | Loss: 0.00038448
Iteration 15/25 | Loss: 0.00038448
Iteration 16/25 | Loss: 0.00038448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003844828170258552, 0.0003844828170258552, 0.0003844828170258552, 0.0003844828170258552, 0.0003844828170258552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003844828170258552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038448
Iteration 2/1000 | Loss: 0.00005285
Iteration 3/1000 | Loss: 0.00003537
Iteration 4/1000 | Loss: 0.00003091
Iteration 5/1000 | Loss: 0.00002898
Iteration 6/1000 | Loss: 0.00002761
Iteration 7/1000 | Loss: 0.00002681
Iteration 8/1000 | Loss: 0.00002631
Iteration 9/1000 | Loss: 0.00002585
Iteration 10/1000 | Loss: 0.00002541
Iteration 11/1000 | Loss: 0.00002513
Iteration 12/1000 | Loss: 0.00002495
Iteration 13/1000 | Loss: 0.00002477
Iteration 14/1000 | Loss: 0.00002464
Iteration 15/1000 | Loss: 0.00002455
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002438
Iteration 18/1000 | Loss: 0.00002437
Iteration 19/1000 | Loss: 0.00002436
Iteration 20/1000 | Loss: 0.00002430
Iteration 21/1000 | Loss: 0.00002429
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002427
Iteration 24/1000 | Loss: 0.00002427
Iteration 25/1000 | Loss: 0.00002426
Iteration 26/1000 | Loss: 0.00002426
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00002425
Iteration 29/1000 | Loss: 0.00002425
Iteration 30/1000 | Loss: 0.00002424
Iteration 31/1000 | Loss: 0.00002424
Iteration 32/1000 | Loss: 0.00002424
Iteration 33/1000 | Loss: 0.00002423
Iteration 34/1000 | Loss: 0.00002423
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002421
Iteration 37/1000 | Loss: 0.00002421
Iteration 38/1000 | Loss: 0.00002420
Iteration 39/1000 | Loss: 0.00002420
Iteration 40/1000 | Loss: 0.00002419
Iteration 41/1000 | Loss: 0.00002419
Iteration 42/1000 | Loss: 0.00002419
Iteration 43/1000 | Loss: 0.00002418
Iteration 44/1000 | Loss: 0.00002418
Iteration 45/1000 | Loss: 0.00002417
Iteration 46/1000 | Loss: 0.00002417
Iteration 47/1000 | Loss: 0.00002415
Iteration 48/1000 | Loss: 0.00002415
Iteration 49/1000 | Loss: 0.00002414
Iteration 50/1000 | Loss: 0.00002413
Iteration 51/1000 | Loss: 0.00002412
Iteration 52/1000 | Loss: 0.00002412
Iteration 53/1000 | Loss: 0.00002411
Iteration 54/1000 | Loss: 0.00002409
Iteration 55/1000 | Loss: 0.00002407
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00002406
Iteration 58/1000 | Loss: 0.00002406
Iteration 59/1000 | Loss: 0.00002406
Iteration 60/1000 | Loss: 0.00002406
Iteration 61/1000 | Loss: 0.00002405
Iteration 62/1000 | Loss: 0.00002405
Iteration 63/1000 | Loss: 0.00002405
Iteration 64/1000 | Loss: 0.00002405
Iteration 65/1000 | Loss: 0.00002405
Iteration 66/1000 | Loss: 0.00002405
Iteration 67/1000 | Loss: 0.00002405
Iteration 68/1000 | Loss: 0.00002405
Iteration 69/1000 | Loss: 0.00002405
Iteration 70/1000 | Loss: 0.00002402
Iteration 71/1000 | Loss: 0.00002401
Iteration 72/1000 | Loss: 0.00002400
Iteration 73/1000 | Loss: 0.00002400
Iteration 74/1000 | Loss: 0.00002400
Iteration 75/1000 | Loss: 0.00002399
Iteration 76/1000 | Loss: 0.00002399
Iteration 77/1000 | Loss: 0.00002399
Iteration 78/1000 | Loss: 0.00002399
Iteration 79/1000 | Loss: 0.00002399
Iteration 80/1000 | Loss: 0.00002399
Iteration 81/1000 | Loss: 0.00002399
Iteration 82/1000 | Loss: 0.00002399
Iteration 83/1000 | Loss: 0.00002398
Iteration 84/1000 | Loss: 0.00002397
Iteration 85/1000 | Loss: 0.00002397
Iteration 86/1000 | Loss: 0.00002396
Iteration 87/1000 | Loss: 0.00002396
Iteration 88/1000 | Loss: 0.00002395
Iteration 89/1000 | Loss: 0.00002395
Iteration 90/1000 | Loss: 0.00002395
Iteration 91/1000 | Loss: 0.00002395
Iteration 92/1000 | Loss: 0.00002395
Iteration 93/1000 | Loss: 0.00002395
Iteration 94/1000 | Loss: 0.00002395
Iteration 95/1000 | Loss: 0.00002395
Iteration 96/1000 | Loss: 0.00002394
Iteration 97/1000 | Loss: 0.00002394
Iteration 98/1000 | Loss: 0.00002394
Iteration 99/1000 | Loss: 0.00002394
Iteration 100/1000 | Loss: 0.00002394
Iteration 101/1000 | Loss: 0.00002393
Iteration 102/1000 | Loss: 0.00002393
Iteration 103/1000 | Loss: 0.00002393
Iteration 104/1000 | Loss: 0.00002392
Iteration 105/1000 | Loss: 0.00002392
Iteration 106/1000 | Loss: 0.00002392
Iteration 107/1000 | Loss: 0.00002392
Iteration 108/1000 | Loss: 0.00002392
Iteration 109/1000 | Loss: 0.00002391
Iteration 110/1000 | Loss: 0.00002391
Iteration 111/1000 | Loss: 0.00002391
Iteration 112/1000 | Loss: 0.00002391
Iteration 113/1000 | Loss: 0.00002391
Iteration 114/1000 | Loss: 0.00002391
Iteration 115/1000 | Loss: 0.00002391
Iteration 116/1000 | Loss: 0.00002390
Iteration 117/1000 | Loss: 0.00002390
Iteration 118/1000 | Loss: 0.00002390
Iteration 119/1000 | Loss: 0.00002389
Iteration 120/1000 | Loss: 0.00002389
Iteration 121/1000 | Loss: 0.00002389
Iteration 122/1000 | Loss: 0.00002389
Iteration 123/1000 | Loss: 0.00002389
Iteration 124/1000 | Loss: 0.00002389
Iteration 125/1000 | Loss: 0.00002389
Iteration 126/1000 | Loss: 0.00002389
Iteration 127/1000 | Loss: 0.00002389
Iteration 128/1000 | Loss: 0.00002388
Iteration 129/1000 | Loss: 0.00002388
Iteration 130/1000 | Loss: 0.00002388
Iteration 131/1000 | Loss: 0.00002388
Iteration 132/1000 | Loss: 0.00002387
Iteration 133/1000 | Loss: 0.00002387
Iteration 134/1000 | Loss: 0.00002387
Iteration 135/1000 | Loss: 0.00002387
Iteration 136/1000 | Loss: 0.00002387
Iteration 137/1000 | Loss: 0.00002387
Iteration 138/1000 | Loss: 0.00002387
Iteration 139/1000 | Loss: 0.00002386
Iteration 140/1000 | Loss: 0.00002386
Iteration 141/1000 | Loss: 0.00002386
Iteration 142/1000 | Loss: 0.00002386
Iteration 143/1000 | Loss: 0.00002386
Iteration 144/1000 | Loss: 0.00002386
Iteration 145/1000 | Loss: 0.00002386
Iteration 146/1000 | Loss: 0.00002386
Iteration 147/1000 | Loss: 0.00002386
Iteration 148/1000 | Loss: 0.00002386
Iteration 149/1000 | Loss: 0.00002386
Iteration 150/1000 | Loss: 0.00002386
Iteration 151/1000 | Loss: 0.00002386
Iteration 152/1000 | Loss: 0.00002386
Iteration 153/1000 | Loss: 0.00002386
Iteration 154/1000 | Loss: 0.00002386
Iteration 155/1000 | Loss: 0.00002386
Iteration 156/1000 | Loss: 0.00002386
Iteration 157/1000 | Loss: 0.00002386
Iteration 158/1000 | Loss: 0.00002386
Iteration 159/1000 | Loss: 0.00002386
Iteration 160/1000 | Loss: 0.00002386
Iteration 161/1000 | Loss: 0.00002386
Iteration 162/1000 | Loss: 0.00002386
Iteration 163/1000 | Loss: 0.00002386
Iteration 164/1000 | Loss: 0.00002386
Iteration 165/1000 | Loss: 0.00002386
Iteration 166/1000 | Loss: 0.00002386
Iteration 167/1000 | Loss: 0.00002386
Iteration 168/1000 | Loss: 0.00002386
Iteration 169/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.385907464486081e-05, 2.385907464486081e-05, 2.385907464486081e-05, 2.385907464486081e-05, 2.385907464486081e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.385907464486081e-05

Optimization complete. Final v2v error: 3.9878756999969482 mm

Highest mean error: 6.1250152587890625 mm for frame 132

Lowest mean error: 3.138984441757202 mm for frame 172

Saving results

Total time: 68.37295842170715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017185
Iteration 2/25 | Loss: 0.00149445
Iteration 3/25 | Loss: 0.00094400
Iteration 4/25 | Loss: 0.00089822
Iteration 5/25 | Loss: 0.00088457
Iteration 6/25 | Loss: 0.00088061
Iteration 7/25 | Loss: 0.00087994
Iteration 8/25 | Loss: 0.00087994
Iteration 9/25 | Loss: 0.00087994
Iteration 10/25 | Loss: 0.00087994
Iteration 11/25 | Loss: 0.00087994
Iteration 12/25 | Loss: 0.00087994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008799353381618857, 0.0008799353381618857, 0.0008799353381618857, 0.0008799353381618857, 0.0008799353381618857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008799353381618857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86849707
Iteration 2/25 | Loss: 0.00024262
Iteration 3/25 | Loss: 0.00024261
Iteration 4/25 | Loss: 0.00024261
Iteration 5/25 | Loss: 0.00024261
Iteration 6/25 | Loss: 0.00024261
Iteration 7/25 | Loss: 0.00024261
Iteration 8/25 | Loss: 0.00024261
Iteration 9/25 | Loss: 0.00024261
Iteration 10/25 | Loss: 0.00024261
Iteration 11/25 | Loss: 0.00024261
Iteration 12/25 | Loss: 0.00024261
Iteration 13/25 | Loss: 0.00024261
Iteration 14/25 | Loss: 0.00024261
Iteration 15/25 | Loss: 0.00024261
Iteration 16/25 | Loss: 0.00024261
Iteration 17/25 | Loss: 0.00024261
Iteration 18/25 | Loss: 0.00024261
Iteration 19/25 | Loss: 0.00024261
Iteration 20/25 | Loss: 0.00024261
Iteration 21/25 | Loss: 0.00024261
Iteration 22/25 | Loss: 0.00024261
Iteration 23/25 | Loss: 0.00024261
Iteration 24/25 | Loss: 0.00024261
Iteration 25/25 | Loss: 0.00024261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024261
Iteration 2/1000 | Loss: 0.00006337
Iteration 3/1000 | Loss: 0.00004286
Iteration 4/1000 | Loss: 0.00003993
Iteration 5/1000 | Loss: 0.00003809
Iteration 6/1000 | Loss: 0.00003717
Iteration 7/1000 | Loss: 0.00003641
Iteration 8/1000 | Loss: 0.00003581
Iteration 9/1000 | Loss: 0.00003544
Iteration 10/1000 | Loss: 0.00003518
Iteration 11/1000 | Loss: 0.00003497
Iteration 12/1000 | Loss: 0.00003487
Iteration 13/1000 | Loss: 0.00003466
Iteration 14/1000 | Loss: 0.00003448
Iteration 15/1000 | Loss: 0.00003436
Iteration 16/1000 | Loss: 0.00003433
Iteration 17/1000 | Loss: 0.00003422
Iteration 18/1000 | Loss: 0.00003418
Iteration 19/1000 | Loss: 0.00003417
Iteration 20/1000 | Loss: 0.00003416
Iteration 21/1000 | Loss: 0.00003413
Iteration 22/1000 | Loss: 0.00003413
Iteration 23/1000 | Loss: 0.00003408
Iteration 24/1000 | Loss: 0.00003406
Iteration 25/1000 | Loss: 0.00003405
Iteration 26/1000 | Loss: 0.00003404
Iteration 27/1000 | Loss: 0.00003403
Iteration 28/1000 | Loss: 0.00003402
Iteration 29/1000 | Loss: 0.00003401
Iteration 30/1000 | Loss: 0.00003400
Iteration 31/1000 | Loss: 0.00003400
Iteration 32/1000 | Loss: 0.00003400
Iteration 33/1000 | Loss: 0.00003399
Iteration 34/1000 | Loss: 0.00003398
Iteration 35/1000 | Loss: 0.00003397
Iteration 36/1000 | Loss: 0.00003396
Iteration 37/1000 | Loss: 0.00003396
Iteration 38/1000 | Loss: 0.00003396
Iteration 39/1000 | Loss: 0.00003395
Iteration 40/1000 | Loss: 0.00003394
Iteration 41/1000 | Loss: 0.00003394
Iteration 42/1000 | Loss: 0.00003394
Iteration 43/1000 | Loss: 0.00003394
Iteration 44/1000 | Loss: 0.00003394
Iteration 45/1000 | Loss: 0.00003393
Iteration 46/1000 | Loss: 0.00003393
Iteration 47/1000 | Loss: 0.00003392
Iteration 48/1000 | Loss: 0.00003392
Iteration 49/1000 | Loss: 0.00003392
Iteration 50/1000 | Loss: 0.00003392
Iteration 51/1000 | Loss: 0.00003391
Iteration 52/1000 | Loss: 0.00003391
Iteration 53/1000 | Loss: 0.00003391
Iteration 54/1000 | Loss: 0.00003389
Iteration 55/1000 | Loss: 0.00003389
Iteration 56/1000 | Loss: 0.00003389
Iteration 57/1000 | Loss: 0.00003388
Iteration 58/1000 | Loss: 0.00003388
Iteration 59/1000 | Loss: 0.00003388
Iteration 60/1000 | Loss: 0.00003388
Iteration 61/1000 | Loss: 0.00003388
Iteration 62/1000 | Loss: 0.00003388
Iteration 63/1000 | Loss: 0.00003387
Iteration 64/1000 | Loss: 0.00003387
Iteration 65/1000 | Loss: 0.00003387
Iteration 66/1000 | Loss: 0.00003387
Iteration 67/1000 | Loss: 0.00003387
Iteration 68/1000 | Loss: 0.00003387
Iteration 69/1000 | Loss: 0.00003386
Iteration 70/1000 | Loss: 0.00003386
Iteration 71/1000 | Loss: 0.00003386
Iteration 72/1000 | Loss: 0.00003385
Iteration 73/1000 | Loss: 0.00003385
Iteration 74/1000 | Loss: 0.00003384
Iteration 75/1000 | Loss: 0.00003384
Iteration 76/1000 | Loss: 0.00003383
Iteration 77/1000 | Loss: 0.00003383
Iteration 78/1000 | Loss: 0.00003383
Iteration 79/1000 | Loss: 0.00003382
Iteration 80/1000 | Loss: 0.00003382
Iteration 81/1000 | Loss: 0.00003382
Iteration 82/1000 | Loss: 0.00003381
Iteration 83/1000 | Loss: 0.00003381
Iteration 84/1000 | Loss: 0.00003381
Iteration 85/1000 | Loss: 0.00003380
Iteration 86/1000 | Loss: 0.00003380
Iteration 87/1000 | Loss: 0.00003380
Iteration 88/1000 | Loss: 0.00003380
Iteration 89/1000 | Loss: 0.00003380
Iteration 90/1000 | Loss: 0.00003380
Iteration 91/1000 | Loss: 0.00003379
Iteration 92/1000 | Loss: 0.00003379
Iteration 93/1000 | Loss: 0.00003379
Iteration 94/1000 | Loss: 0.00003379
Iteration 95/1000 | Loss: 0.00003379
Iteration 96/1000 | Loss: 0.00003378
Iteration 97/1000 | Loss: 0.00003378
Iteration 98/1000 | Loss: 0.00003378
Iteration 99/1000 | Loss: 0.00003378
Iteration 100/1000 | Loss: 0.00003378
Iteration 101/1000 | Loss: 0.00003378
Iteration 102/1000 | Loss: 0.00003377
Iteration 103/1000 | Loss: 0.00003377
Iteration 104/1000 | Loss: 0.00003377
Iteration 105/1000 | Loss: 0.00003377
Iteration 106/1000 | Loss: 0.00003377
Iteration 107/1000 | Loss: 0.00003377
Iteration 108/1000 | Loss: 0.00003377
Iteration 109/1000 | Loss: 0.00003377
Iteration 110/1000 | Loss: 0.00003376
Iteration 111/1000 | Loss: 0.00003376
Iteration 112/1000 | Loss: 0.00003376
Iteration 113/1000 | Loss: 0.00003375
Iteration 114/1000 | Loss: 0.00003375
Iteration 115/1000 | Loss: 0.00003375
Iteration 116/1000 | Loss: 0.00003375
Iteration 117/1000 | Loss: 0.00003375
Iteration 118/1000 | Loss: 0.00003375
Iteration 119/1000 | Loss: 0.00003374
Iteration 120/1000 | Loss: 0.00003374
Iteration 121/1000 | Loss: 0.00003374
Iteration 122/1000 | Loss: 0.00003374
Iteration 123/1000 | Loss: 0.00003374
Iteration 124/1000 | Loss: 0.00003374
Iteration 125/1000 | Loss: 0.00003373
Iteration 126/1000 | Loss: 0.00003373
Iteration 127/1000 | Loss: 0.00003373
Iteration 128/1000 | Loss: 0.00003373
Iteration 129/1000 | Loss: 0.00003373
Iteration 130/1000 | Loss: 0.00003373
Iteration 131/1000 | Loss: 0.00003373
Iteration 132/1000 | Loss: 0.00003373
Iteration 133/1000 | Loss: 0.00003373
Iteration 134/1000 | Loss: 0.00003372
Iteration 135/1000 | Loss: 0.00003372
Iteration 136/1000 | Loss: 0.00003372
Iteration 137/1000 | Loss: 0.00003372
Iteration 138/1000 | Loss: 0.00003372
Iteration 139/1000 | Loss: 0.00003372
Iteration 140/1000 | Loss: 0.00003372
Iteration 141/1000 | Loss: 0.00003372
Iteration 142/1000 | Loss: 0.00003372
Iteration 143/1000 | Loss: 0.00003372
Iteration 144/1000 | Loss: 0.00003372
Iteration 145/1000 | Loss: 0.00003372
Iteration 146/1000 | Loss: 0.00003371
Iteration 147/1000 | Loss: 0.00003371
Iteration 148/1000 | Loss: 0.00003371
Iteration 149/1000 | Loss: 0.00003371
Iteration 150/1000 | Loss: 0.00003371
Iteration 151/1000 | Loss: 0.00003371
Iteration 152/1000 | Loss: 0.00003371
Iteration 153/1000 | Loss: 0.00003371
Iteration 154/1000 | Loss: 0.00003371
Iteration 155/1000 | Loss: 0.00003371
Iteration 156/1000 | Loss: 0.00003371
Iteration 157/1000 | Loss: 0.00003371
Iteration 158/1000 | Loss: 0.00003371
Iteration 159/1000 | Loss: 0.00003371
Iteration 160/1000 | Loss: 0.00003371
Iteration 161/1000 | Loss: 0.00003371
Iteration 162/1000 | Loss: 0.00003371
Iteration 163/1000 | Loss: 0.00003371
Iteration 164/1000 | Loss: 0.00003371
Iteration 165/1000 | Loss: 0.00003371
Iteration 166/1000 | Loss: 0.00003371
Iteration 167/1000 | Loss: 0.00003371
Iteration 168/1000 | Loss: 0.00003371
Iteration 169/1000 | Loss: 0.00003371
Iteration 170/1000 | Loss: 0.00003371
Iteration 171/1000 | Loss: 0.00003371
Iteration 172/1000 | Loss: 0.00003371
Iteration 173/1000 | Loss: 0.00003371
Iteration 174/1000 | Loss: 0.00003371
Iteration 175/1000 | Loss: 0.00003371
Iteration 176/1000 | Loss: 0.00003371
Iteration 177/1000 | Loss: 0.00003371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [3.370761260157451e-05, 3.370761260157451e-05, 3.370761260157451e-05, 3.370761260157451e-05, 3.370761260157451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.370761260157451e-05

Optimization complete. Final v2v error: 4.707552909851074 mm

Highest mean error: 5.54764986038208 mm for frame 45

Lowest mean error: 3.7449450492858887 mm for frame 29

Saving results

Total time: 47.899747133255005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385420
Iteration 2/25 | Loss: 0.00086988
Iteration 3/25 | Loss: 0.00069344
Iteration 4/25 | Loss: 0.00064856
Iteration 5/25 | Loss: 0.00064228
Iteration 6/25 | Loss: 0.00064104
Iteration 7/25 | Loss: 0.00064055
Iteration 8/25 | Loss: 0.00064055
Iteration 9/25 | Loss: 0.00064055
Iteration 10/25 | Loss: 0.00064055
Iteration 11/25 | Loss: 0.00064055
Iteration 12/25 | Loss: 0.00064055
Iteration 13/25 | Loss: 0.00064055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006405468448065221, 0.0006405468448065221, 0.0006405468448065221, 0.0006405468448065221, 0.0006405468448065221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006405468448065221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46707940
Iteration 2/25 | Loss: 0.00029513
Iteration 3/25 | Loss: 0.00029513
Iteration 4/25 | Loss: 0.00029513
Iteration 5/25 | Loss: 0.00029513
Iteration 6/25 | Loss: 0.00029513
Iteration 7/25 | Loss: 0.00029513
Iteration 8/25 | Loss: 0.00029513
Iteration 9/25 | Loss: 0.00029513
Iteration 10/25 | Loss: 0.00029513
Iteration 11/25 | Loss: 0.00029513
Iteration 12/25 | Loss: 0.00029513
Iteration 13/25 | Loss: 0.00029513
Iteration 14/25 | Loss: 0.00029513
Iteration 15/25 | Loss: 0.00029513
Iteration 16/25 | Loss: 0.00029513
Iteration 17/25 | Loss: 0.00029513
Iteration 18/25 | Loss: 0.00029513
Iteration 19/25 | Loss: 0.00029513
Iteration 20/25 | Loss: 0.00029513
Iteration 21/25 | Loss: 0.00029513
Iteration 22/25 | Loss: 0.00029513
Iteration 23/25 | Loss: 0.00029513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000295127130812034, 0.000295127130812034, 0.000295127130812034, 0.000295127130812034, 0.000295127130812034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000295127130812034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029513
Iteration 2/1000 | Loss: 0.00004196
Iteration 3/1000 | Loss: 0.00002935
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002038
Iteration 6/1000 | Loss: 0.00001903
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001766
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001689
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001660
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001628
Iteration 20/1000 | Loss: 0.00001627
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001621
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00001614
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001612
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001610
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001609
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001607
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001605
Iteration 90/1000 | Loss: 0.00001605
Iteration 91/1000 | Loss: 0.00001605
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001604
Iteration 109/1000 | Loss: 0.00001604
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001604
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001603
Iteration 117/1000 | Loss: 0.00001603
Iteration 118/1000 | Loss: 0.00001603
Iteration 119/1000 | Loss: 0.00001603
Iteration 120/1000 | Loss: 0.00001603
Iteration 121/1000 | Loss: 0.00001603
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001603
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.602798693056684e-05, 1.602798693056684e-05, 1.602798693056684e-05, 1.602798693056684e-05, 1.602798693056684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.602798693056684e-05

Optimization complete. Final v2v error: 3.387587785720825 mm

Highest mean error: 3.758702516555786 mm for frame 74

Lowest mean error: 3.0763766765594482 mm for frame 12

Saving results

Total time: 38.30861186981201
