Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=287, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16072-16127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477713
Iteration 2/25 | Loss: 0.00163544
Iteration 3/25 | Loss: 0.00155330
Iteration 4/25 | Loss: 0.00154358
Iteration 5/25 | Loss: 0.00154089
Iteration 6/25 | Loss: 0.00154089
Iteration 7/25 | Loss: 0.00154089
Iteration 8/25 | Loss: 0.00154089
Iteration 9/25 | Loss: 0.00154089
Iteration 10/25 | Loss: 0.00154089
Iteration 11/25 | Loss: 0.00154089
Iteration 12/25 | Loss: 0.00154089
Iteration 13/25 | Loss: 0.00154089
Iteration 14/25 | Loss: 0.00154089
Iteration 15/25 | Loss: 0.00154089
Iteration 16/25 | Loss: 0.00154089
Iteration 17/25 | Loss: 0.00154089
Iteration 18/25 | Loss: 0.00154089
Iteration 19/25 | Loss: 0.00154089
Iteration 20/25 | Loss: 0.00154089
Iteration 21/25 | Loss: 0.00154089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001540894852951169, 0.001540894852951169, 0.001540894852951169, 0.001540894852951169, 0.001540894852951169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001540894852951169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54822087
Iteration 2/25 | Loss: 0.00162981
Iteration 3/25 | Loss: 0.00162981
Iteration 4/25 | Loss: 0.00162981
Iteration 5/25 | Loss: 0.00162980
Iteration 6/25 | Loss: 0.00162980
Iteration 7/25 | Loss: 0.00162980
Iteration 8/25 | Loss: 0.00162980
Iteration 9/25 | Loss: 0.00162980
Iteration 10/25 | Loss: 0.00162980
Iteration 11/25 | Loss: 0.00162980
Iteration 12/25 | Loss: 0.00162980
Iteration 13/25 | Loss: 0.00162980
Iteration 14/25 | Loss: 0.00162980
Iteration 15/25 | Loss: 0.00162980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016298039117828012, 0.0016298039117828012, 0.0016298039117828012, 0.0016298039117828012, 0.0016298039117828012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016298039117828012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162980
Iteration 2/1000 | Loss: 0.00005115
Iteration 3/1000 | Loss: 0.00003671
Iteration 4/1000 | Loss: 0.00003308
Iteration 5/1000 | Loss: 0.00003113
Iteration 6/1000 | Loss: 0.00002964
Iteration 7/1000 | Loss: 0.00002893
Iteration 8/1000 | Loss: 0.00002845
Iteration 9/1000 | Loss: 0.00002807
Iteration 10/1000 | Loss: 0.00002774
Iteration 11/1000 | Loss: 0.00002744
Iteration 12/1000 | Loss: 0.00002722
Iteration 13/1000 | Loss: 0.00002717
Iteration 14/1000 | Loss: 0.00002701
Iteration 15/1000 | Loss: 0.00002699
Iteration 16/1000 | Loss: 0.00002697
Iteration 17/1000 | Loss: 0.00002696
Iteration 18/1000 | Loss: 0.00002696
Iteration 19/1000 | Loss: 0.00002696
Iteration 20/1000 | Loss: 0.00002695
Iteration 21/1000 | Loss: 0.00002694
Iteration 22/1000 | Loss: 0.00002694
Iteration 23/1000 | Loss: 0.00002693
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00002688
Iteration 26/1000 | Loss: 0.00002683
Iteration 27/1000 | Loss: 0.00002683
Iteration 28/1000 | Loss: 0.00002681
Iteration 29/1000 | Loss: 0.00002681
Iteration 30/1000 | Loss: 0.00002681
Iteration 31/1000 | Loss: 0.00002679
Iteration 32/1000 | Loss: 0.00002676
Iteration 33/1000 | Loss: 0.00002676
Iteration 34/1000 | Loss: 0.00002676
Iteration 35/1000 | Loss: 0.00002674
Iteration 36/1000 | Loss: 0.00002674
Iteration 37/1000 | Loss: 0.00002673
Iteration 38/1000 | Loss: 0.00002673
Iteration 39/1000 | Loss: 0.00002673
Iteration 40/1000 | Loss: 0.00002673
Iteration 41/1000 | Loss: 0.00002672
Iteration 42/1000 | Loss: 0.00002672
Iteration 43/1000 | Loss: 0.00002672
Iteration 44/1000 | Loss: 0.00002672
Iteration 45/1000 | Loss: 0.00002671
Iteration 46/1000 | Loss: 0.00002671
Iteration 47/1000 | Loss: 0.00002671
Iteration 48/1000 | Loss: 0.00002671
Iteration 49/1000 | Loss: 0.00002671
Iteration 50/1000 | Loss: 0.00002670
Iteration 51/1000 | Loss: 0.00002670
Iteration 52/1000 | Loss: 0.00002670
Iteration 53/1000 | Loss: 0.00002670
Iteration 54/1000 | Loss: 0.00002670
Iteration 55/1000 | Loss: 0.00002670
Iteration 56/1000 | Loss: 0.00002670
Iteration 57/1000 | Loss: 0.00002670
Iteration 58/1000 | Loss: 0.00002670
Iteration 59/1000 | Loss: 0.00002670
Iteration 60/1000 | Loss: 0.00002670
Iteration 61/1000 | Loss: 0.00002670
Iteration 62/1000 | Loss: 0.00002669
Iteration 63/1000 | Loss: 0.00002669
Iteration 64/1000 | Loss: 0.00002669
Iteration 65/1000 | Loss: 0.00002669
Iteration 66/1000 | Loss: 0.00002668
Iteration 67/1000 | Loss: 0.00002668
Iteration 68/1000 | Loss: 0.00002668
Iteration 69/1000 | Loss: 0.00002668
Iteration 70/1000 | Loss: 0.00002668
Iteration 71/1000 | Loss: 0.00002668
Iteration 72/1000 | Loss: 0.00002668
Iteration 73/1000 | Loss: 0.00002668
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002667
Iteration 76/1000 | Loss: 0.00002667
Iteration 77/1000 | Loss: 0.00002667
Iteration 78/1000 | Loss: 0.00002667
Iteration 79/1000 | Loss: 0.00002666
Iteration 80/1000 | Loss: 0.00002666
Iteration 81/1000 | Loss: 0.00002666
Iteration 82/1000 | Loss: 0.00002666
Iteration 83/1000 | Loss: 0.00002666
Iteration 84/1000 | Loss: 0.00002666
Iteration 85/1000 | Loss: 0.00002666
Iteration 86/1000 | Loss: 0.00002666
Iteration 87/1000 | Loss: 0.00002665
Iteration 88/1000 | Loss: 0.00002665
Iteration 89/1000 | Loss: 0.00002665
Iteration 90/1000 | Loss: 0.00002665
Iteration 91/1000 | Loss: 0.00002665
Iteration 92/1000 | Loss: 0.00002665
Iteration 93/1000 | Loss: 0.00002665
Iteration 94/1000 | Loss: 0.00002665
Iteration 95/1000 | Loss: 0.00002665
Iteration 96/1000 | Loss: 0.00002664
Iteration 97/1000 | Loss: 0.00002664
Iteration 98/1000 | Loss: 0.00002664
Iteration 99/1000 | Loss: 0.00002664
Iteration 100/1000 | Loss: 0.00002664
Iteration 101/1000 | Loss: 0.00002664
Iteration 102/1000 | Loss: 0.00002664
Iteration 103/1000 | Loss: 0.00002664
Iteration 104/1000 | Loss: 0.00002664
Iteration 105/1000 | Loss: 0.00002664
Iteration 106/1000 | Loss: 0.00002664
Iteration 107/1000 | Loss: 0.00002664
Iteration 108/1000 | Loss: 0.00002663
Iteration 109/1000 | Loss: 0.00002663
Iteration 110/1000 | Loss: 0.00002663
Iteration 111/1000 | Loss: 0.00002663
Iteration 112/1000 | Loss: 0.00002663
Iteration 113/1000 | Loss: 0.00002663
Iteration 114/1000 | Loss: 0.00002663
Iteration 115/1000 | Loss: 0.00002663
Iteration 116/1000 | Loss: 0.00002663
Iteration 117/1000 | Loss: 0.00002663
Iteration 118/1000 | Loss: 0.00002663
Iteration 119/1000 | Loss: 0.00002663
Iteration 120/1000 | Loss: 0.00002663
Iteration 121/1000 | Loss: 0.00002663
Iteration 122/1000 | Loss: 0.00002663
Iteration 123/1000 | Loss: 0.00002663
Iteration 124/1000 | Loss: 0.00002663
Iteration 125/1000 | Loss: 0.00002663
Iteration 126/1000 | Loss: 0.00002663
Iteration 127/1000 | Loss: 0.00002663
Iteration 128/1000 | Loss: 0.00002663
Iteration 129/1000 | Loss: 0.00002663
Iteration 130/1000 | Loss: 0.00002663
Iteration 131/1000 | Loss: 0.00002663
Iteration 132/1000 | Loss: 0.00002663
Iteration 133/1000 | Loss: 0.00002663
Iteration 134/1000 | Loss: 0.00002663
Iteration 135/1000 | Loss: 0.00002663
Iteration 136/1000 | Loss: 0.00002663
Iteration 137/1000 | Loss: 0.00002663
Iteration 138/1000 | Loss: 0.00002663
Iteration 139/1000 | Loss: 0.00002663
Iteration 140/1000 | Loss: 0.00002663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.663079067133367e-05, 2.663079067133367e-05, 2.663079067133367e-05, 2.663079067133367e-05, 2.663079067133367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.663079067133367e-05

Optimization complete. Final v2v error: 4.61005163192749 mm

Highest mean error: 5.324158668518066 mm for frame 191

Lowest mean error: 4.278397560119629 mm for frame 60

Saving results

Total time: 41.56401181221008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003455
Iteration 2/25 | Loss: 0.00290463
Iteration 3/25 | Loss: 0.00222136
Iteration 4/25 | Loss: 0.00207971
Iteration 5/25 | Loss: 0.00212387
Iteration 6/25 | Loss: 0.00202702
Iteration 7/25 | Loss: 0.00190802
Iteration 8/25 | Loss: 0.00183796
Iteration 9/25 | Loss: 0.00181297
Iteration 10/25 | Loss: 0.00180112
Iteration 11/25 | Loss: 0.00179041
Iteration 12/25 | Loss: 0.00179864
Iteration 13/25 | Loss: 0.00180255
Iteration 14/25 | Loss: 0.00179169
Iteration 15/25 | Loss: 0.00179004
Iteration 16/25 | Loss: 0.00178388
Iteration 17/25 | Loss: 0.00178250
Iteration 18/25 | Loss: 0.00178936
Iteration 19/25 | Loss: 0.00178107
Iteration 20/25 | Loss: 0.00178013
Iteration 21/25 | Loss: 0.00179377
Iteration 22/25 | Loss: 0.00178402
Iteration 23/25 | Loss: 0.00177100
Iteration 24/25 | Loss: 0.00176127
Iteration 25/25 | Loss: 0.00175965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58721280
Iteration 2/25 | Loss: 0.00276768
Iteration 3/25 | Loss: 0.00276756
Iteration 4/25 | Loss: 0.00276756
Iteration 5/25 | Loss: 0.00276756
Iteration 6/25 | Loss: 0.00276756
Iteration 7/25 | Loss: 0.00276756
Iteration 8/25 | Loss: 0.00276756
Iteration 9/25 | Loss: 0.00276756
Iteration 10/25 | Loss: 0.00276756
Iteration 11/25 | Loss: 0.00276756
Iteration 12/25 | Loss: 0.00276756
Iteration 13/25 | Loss: 0.00276756
Iteration 14/25 | Loss: 0.00276756
Iteration 15/25 | Loss: 0.00276756
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027675607707351446, 0.0027675607707351446, 0.0027675607707351446, 0.0027675607707351446, 0.0027675607707351446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027675607707351446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276756
Iteration 2/1000 | Loss: 0.00096944
Iteration 3/1000 | Loss: 0.00081447
Iteration 4/1000 | Loss: 0.00029708
Iteration 5/1000 | Loss: 0.00063843
Iteration 6/1000 | Loss: 0.00020650
Iteration 7/1000 | Loss: 0.00021593
Iteration 8/1000 | Loss: 0.00009047
Iteration 9/1000 | Loss: 0.00008319
Iteration 10/1000 | Loss: 0.00007582
Iteration 11/1000 | Loss: 0.00007215
Iteration 12/1000 | Loss: 0.00017013
Iteration 13/1000 | Loss: 0.00021215
Iteration 14/1000 | Loss: 0.00007423
Iteration 15/1000 | Loss: 0.00007013
Iteration 16/1000 | Loss: 0.00027148
Iteration 17/1000 | Loss: 0.00007096
Iteration 18/1000 | Loss: 0.00006699
Iteration 19/1000 | Loss: 0.00006544
Iteration 20/1000 | Loss: 0.00006443
Iteration 21/1000 | Loss: 0.00019177
Iteration 22/1000 | Loss: 0.00060373
Iteration 23/1000 | Loss: 0.00097055
Iteration 24/1000 | Loss: 0.00108400
Iteration 25/1000 | Loss: 0.00140237
Iteration 26/1000 | Loss: 0.00064523
Iteration 27/1000 | Loss: 0.00057104
Iteration 28/1000 | Loss: 0.00017988
Iteration 29/1000 | Loss: 0.00015844
Iteration 30/1000 | Loss: 0.00042123
Iteration 31/1000 | Loss: 0.00011246
Iteration 32/1000 | Loss: 0.00055003
Iteration 33/1000 | Loss: 0.00023907
Iteration 34/1000 | Loss: 0.00009285
Iteration 35/1000 | Loss: 0.00007365
Iteration 36/1000 | Loss: 0.00053725
Iteration 37/1000 | Loss: 0.00020012
Iteration 38/1000 | Loss: 0.00020411
Iteration 39/1000 | Loss: 0.00006986
Iteration 40/1000 | Loss: 0.00012220
Iteration 41/1000 | Loss: 0.00022642
Iteration 42/1000 | Loss: 0.00005731
Iteration 43/1000 | Loss: 0.00005297
Iteration 44/1000 | Loss: 0.00046468
Iteration 45/1000 | Loss: 0.00007540
Iteration 46/1000 | Loss: 0.00031001
Iteration 47/1000 | Loss: 0.00020305
Iteration 48/1000 | Loss: 0.00023119
Iteration 49/1000 | Loss: 0.00012741
Iteration 50/1000 | Loss: 0.00005354
Iteration 51/1000 | Loss: 0.00005964
Iteration 52/1000 | Loss: 0.00038272
Iteration 53/1000 | Loss: 0.00013974
Iteration 54/1000 | Loss: 0.00023167
Iteration 55/1000 | Loss: 0.00030706
Iteration 56/1000 | Loss: 0.00020462
Iteration 57/1000 | Loss: 0.00044347
Iteration 58/1000 | Loss: 0.00024380
Iteration 59/1000 | Loss: 0.00056447
Iteration 60/1000 | Loss: 0.00034743
Iteration 61/1000 | Loss: 0.00012990
Iteration 62/1000 | Loss: 0.00007330
Iteration 63/1000 | Loss: 0.00005510
Iteration 64/1000 | Loss: 0.00011196
Iteration 65/1000 | Loss: 0.00015346
Iteration 66/1000 | Loss: 0.00027099
Iteration 67/1000 | Loss: 0.00032797
Iteration 68/1000 | Loss: 0.00019531
Iteration 69/1000 | Loss: 0.00009057
Iteration 70/1000 | Loss: 0.00014670
Iteration 71/1000 | Loss: 0.00077223
Iteration 72/1000 | Loss: 0.00030991
Iteration 73/1000 | Loss: 0.00005507
Iteration 74/1000 | Loss: 0.00032742
Iteration 75/1000 | Loss: 0.00006091
Iteration 76/1000 | Loss: 0.00004500
Iteration 77/1000 | Loss: 0.00028791
Iteration 78/1000 | Loss: 0.00010663
Iteration 79/1000 | Loss: 0.00004295
Iteration 80/1000 | Loss: 0.00024311
Iteration 81/1000 | Loss: 0.00013399
Iteration 82/1000 | Loss: 0.00025584
Iteration 83/1000 | Loss: 0.00012459
Iteration 84/1000 | Loss: 0.00027848
Iteration 85/1000 | Loss: 0.00011143
Iteration 86/1000 | Loss: 0.00004157
Iteration 87/1000 | Loss: 0.00004099
Iteration 88/1000 | Loss: 0.00004059
Iteration 89/1000 | Loss: 0.00022717
Iteration 90/1000 | Loss: 0.00010671
Iteration 91/1000 | Loss: 0.00026159
Iteration 92/1000 | Loss: 0.00010200
Iteration 93/1000 | Loss: 0.00026043
Iteration 94/1000 | Loss: 0.00024247
Iteration 95/1000 | Loss: 0.00043871
Iteration 96/1000 | Loss: 0.00008159
Iteration 97/1000 | Loss: 0.00010345
Iteration 98/1000 | Loss: 0.00005245
Iteration 99/1000 | Loss: 0.00004387
Iteration 100/1000 | Loss: 0.00004215
Iteration 101/1000 | Loss: 0.00023614
Iteration 102/1000 | Loss: 0.00015067
Iteration 103/1000 | Loss: 0.00013257
Iteration 104/1000 | Loss: 0.00007867
Iteration 105/1000 | Loss: 0.00003805
Iteration 106/1000 | Loss: 0.00003613
Iteration 107/1000 | Loss: 0.00003554
Iteration 108/1000 | Loss: 0.00003514
Iteration 109/1000 | Loss: 0.00003483
Iteration 110/1000 | Loss: 0.00003469
Iteration 111/1000 | Loss: 0.00003468
Iteration 112/1000 | Loss: 0.00003468
Iteration 113/1000 | Loss: 0.00003467
Iteration 114/1000 | Loss: 0.00003466
Iteration 115/1000 | Loss: 0.00003466
Iteration 116/1000 | Loss: 0.00003466
Iteration 117/1000 | Loss: 0.00003466
Iteration 118/1000 | Loss: 0.00003466
Iteration 119/1000 | Loss: 0.00003466
Iteration 120/1000 | Loss: 0.00003466
Iteration 121/1000 | Loss: 0.00003466
Iteration 122/1000 | Loss: 0.00003465
Iteration 123/1000 | Loss: 0.00003463
Iteration 124/1000 | Loss: 0.00003463
Iteration 125/1000 | Loss: 0.00003462
Iteration 126/1000 | Loss: 0.00003461
Iteration 127/1000 | Loss: 0.00003461
Iteration 128/1000 | Loss: 0.00003461
Iteration 129/1000 | Loss: 0.00003461
Iteration 130/1000 | Loss: 0.00003460
Iteration 131/1000 | Loss: 0.00003460
Iteration 132/1000 | Loss: 0.00003460
Iteration 133/1000 | Loss: 0.00003459
Iteration 134/1000 | Loss: 0.00003457
Iteration 135/1000 | Loss: 0.00003453
Iteration 136/1000 | Loss: 0.00003453
Iteration 137/1000 | Loss: 0.00003453
Iteration 138/1000 | Loss: 0.00003453
Iteration 139/1000 | Loss: 0.00003453
Iteration 140/1000 | Loss: 0.00003453
Iteration 141/1000 | Loss: 0.00003452
Iteration 142/1000 | Loss: 0.00003452
Iteration 143/1000 | Loss: 0.00003452
Iteration 144/1000 | Loss: 0.00003451
Iteration 145/1000 | Loss: 0.00003451
Iteration 146/1000 | Loss: 0.00003449
Iteration 147/1000 | Loss: 0.00003449
Iteration 148/1000 | Loss: 0.00003449
Iteration 149/1000 | Loss: 0.00003449
Iteration 150/1000 | Loss: 0.00003449
Iteration 151/1000 | Loss: 0.00003449
Iteration 152/1000 | Loss: 0.00003448
Iteration 153/1000 | Loss: 0.00003448
Iteration 154/1000 | Loss: 0.00003448
Iteration 155/1000 | Loss: 0.00003448
Iteration 156/1000 | Loss: 0.00003448
Iteration 157/1000 | Loss: 0.00003448
Iteration 158/1000 | Loss: 0.00003448
Iteration 159/1000 | Loss: 0.00003448
Iteration 160/1000 | Loss: 0.00003448
Iteration 161/1000 | Loss: 0.00003448
Iteration 162/1000 | Loss: 0.00003448
Iteration 163/1000 | Loss: 0.00003447
Iteration 164/1000 | Loss: 0.00003447
Iteration 165/1000 | Loss: 0.00003447
Iteration 166/1000 | Loss: 0.00003445
Iteration 167/1000 | Loss: 0.00003445
Iteration 168/1000 | Loss: 0.00003445
Iteration 169/1000 | Loss: 0.00003445
Iteration 170/1000 | Loss: 0.00003445
Iteration 171/1000 | Loss: 0.00003445
Iteration 172/1000 | Loss: 0.00003445
Iteration 173/1000 | Loss: 0.00003445
Iteration 174/1000 | Loss: 0.00003445
Iteration 175/1000 | Loss: 0.00003445
Iteration 176/1000 | Loss: 0.00003444
Iteration 177/1000 | Loss: 0.00003444
Iteration 178/1000 | Loss: 0.00003444
Iteration 179/1000 | Loss: 0.00003444
Iteration 180/1000 | Loss: 0.00003444
Iteration 181/1000 | Loss: 0.00003442
Iteration 182/1000 | Loss: 0.00003442
Iteration 183/1000 | Loss: 0.00003442
Iteration 184/1000 | Loss: 0.00003441
Iteration 185/1000 | Loss: 0.00003441
Iteration 186/1000 | Loss: 0.00003441
Iteration 187/1000 | Loss: 0.00003441
Iteration 188/1000 | Loss: 0.00003441
Iteration 189/1000 | Loss: 0.00003441
Iteration 190/1000 | Loss: 0.00003441
Iteration 191/1000 | Loss: 0.00003440
Iteration 192/1000 | Loss: 0.00003440
Iteration 193/1000 | Loss: 0.00003440
Iteration 194/1000 | Loss: 0.00003440
Iteration 195/1000 | Loss: 0.00003440
Iteration 196/1000 | Loss: 0.00003440
Iteration 197/1000 | Loss: 0.00003440
Iteration 198/1000 | Loss: 0.00003440
Iteration 199/1000 | Loss: 0.00003440
Iteration 200/1000 | Loss: 0.00003440
Iteration 201/1000 | Loss: 0.00003440
Iteration 202/1000 | Loss: 0.00003440
Iteration 203/1000 | Loss: 0.00003440
Iteration 204/1000 | Loss: 0.00003440
Iteration 205/1000 | Loss: 0.00003440
Iteration 206/1000 | Loss: 0.00003440
Iteration 207/1000 | Loss: 0.00003440
Iteration 208/1000 | Loss: 0.00003440
Iteration 209/1000 | Loss: 0.00003440
Iteration 210/1000 | Loss: 0.00003440
Iteration 211/1000 | Loss: 0.00003440
Iteration 212/1000 | Loss: 0.00003440
Iteration 213/1000 | Loss: 0.00003440
Iteration 214/1000 | Loss: 0.00003440
Iteration 215/1000 | Loss: 0.00003440
Iteration 216/1000 | Loss: 0.00003440
Iteration 217/1000 | Loss: 0.00003440
Iteration 218/1000 | Loss: 0.00003440
Iteration 219/1000 | Loss: 0.00003440
Iteration 220/1000 | Loss: 0.00003440
Iteration 221/1000 | Loss: 0.00003440
Iteration 222/1000 | Loss: 0.00003440
Iteration 223/1000 | Loss: 0.00003440
Iteration 224/1000 | Loss: 0.00003440
Iteration 225/1000 | Loss: 0.00003440
Iteration 226/1000 | Loss: 0.00003440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [3.439640568103641e-05, 3.439640568103641e-05, 3.439640568103641e-05, 3.439640568103641e-05, 3.439640568103641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.439640568103641e-05

Optimization complete. Final v2v error: 5.166773796081543 mm

Highest mean error: 5.845062732696533 mm for frame 27

Lowest mean error: 4.737679958343506 mm for frame 62

Saving results

Total time: 242.19330739974976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_nl_6132/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_nl_6132/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985437
Iteration 2/25 | Loss: 0.00342280
Iteration 3/25 | Loss: 0.00226909
Iteration 4/25 | Loss: 0.00187389
Iteration 5/25 | Loss: 0.00167119
Iteration 6/25 | Loss: 0.00162444
Iteration 7/25 | Loss: 0.00157263
Iteration 8/25 | Loss: 0.00155501
Iteration 9/25 | Loss: 0.00156568
Iteration 10/25 | Loss: 0.00153108
Iteration 11/25 | Loss: 0.00152201
Iteration 12/25 | Loss: 0.00151806
Iteration 13/25 | Loss: 0.00151796
Iteration 14/25 | Loss: 0.00152156
Iteration 15/25 | Loss: 0.00151350
Iteration 16/25 | Loss: 0.00151748
Iteration 17/25 | Loss: 0.00151578
Iteration 18/25 | Loss: 0.00153422
Iteration 19/25 | Loss: 0.00152458
Iteration 20/25 | Loss: 0.00151579
Iteration 21/25 | Loss: 0.00152363
Iteration 22/25 | Loss: 0.00152171
Iteration 23/25 | Loss: 0.00151166
Iteration 24/25 | Loss: 0.00151371
Iteration 25/25 | Loss: 0.00150408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50787497
Iteration 2/25 | Loss: 0.00349316
Iteration 3/25 | Loss: 0.00342613
Iteration 4/25 | Loss: 0.00342613
Iteration 5/25 | Loss: 0.00342613
Iteration 6/25 | Loss: 0.00342613
Iteration 7/25 | Loss: 0.00342613
Iteration 8/25 | Loss: 0.00342613
Iteration 9/25 | Loss: 0.00342613
Iteration 10/25 | Loss: 0.00342613
Iteration 11/25 | Loss: 0.00342613
Iteration 12/25 | Loss: 0.00342613
Iteration 13/25 | Loss: 0.00342613
Iteration 14/25 | Loss: 0.00342613
Iteration 15/25 | Loss: 0.00342613
Iteration 16/25 | Loss: 0.00342613
Iteration 17/25 | Loss: 0.00342613
Iteration 18/25 | Loss: 0.00342613
Iteration 19/25 | Loss: 0.00342613
Iteration 20/25 | Loss: 0.00342613
Iteration 21/25 | Loss: 0.00342613
Iteration 22/25 | Loss: 0.00342613
Iteration 23/25 | Loss: 0.00342613
Iteration 24/25 | Loss: 0.00342613
Iteration 25/25 | Loss: 0.00342613
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003426128067076206, 0.003426128067076206, 0.003426128067076206, 0.003426128067076206, 0.003426128067076206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003426128067076206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342613
Iteration 2/1000 | Loss: 0.00061446
Iteration 3/1000 | Loss: 0.00119093
Iteration 4/1000 | Loss: 0.00778791
Iteration 5/1000 | Loss: 0.00263139
Iteration 6/1000 | Loss: 0.00463238
Iteration 7/1000 | Loss: 0.00101409
Iteration 8/1000 | Loss: 0.00373286
Iteration 9/1000 | Loss: 0.00054263
Iteration 10/1000 | Loss: 0.00023831
Iteration 11/1000 | Loss: 0.00136289
Iteration 12/1000 | Loss: 0.00112513
Iteration 13/1000 | Loss: 0.00095766
Iteration 14/1000 | Loss: 0.00149806
Iteration 15/1000 | Loss: 0.00346155
Iteration 16/1000 | Loss: 0.00326788
Iteration 17/1000 | Loss: 0.00448815
Iteration 18/1000 | Loss: 0.00255103
Iteration 19/1000 | Loss: 0.00177261
Iteration 20/1000 | Loss: 0.00088346
Iteration 21/1000 | Loss: 0.00163930
Iteration 22/1000 | Loss: 0.00103838
Iteration 23/1000 | Loss: 0.00136742
Iteration 24/1000 | Loss: 0.00213718
Iteration 25/1000 | Loss: 0.00102830
Iteration 26/1000 | Loss: 0.00190005
Iteration 27/1000 | Loss: 0.00180504
Iteration 28/1000 | Loss: 0.00219454
Iteration 29/1000 | Loss: 0.00104799
Iteration 30/1000 | Loss: 0.00098041
Iteration 31/1000 | Loss: 0.00141872
Iteration 32/1000 | Loss: 0.00086659
Iteration 33/1000 | Loss: 0.00079176
Iteration 34/1000 | Loss: 0.00047755
Iteration 35/1000 | Loss: 0.00119453
Iteration 36/1000 | Loss: 0.00052898
Iteration 37/1000 | Loss: 0.00091976
Iteration 38/1000 | Loss: 0.00092605
Iteration 39/1000 | Loss: 0.00079469
Iteration 40/1000 | Loss: 0.00072138
Iteration 41/1000 | Loss: 0.00090553
Iteration 42/1000 | Loss: 0.00122936
Iteration 43/1000 | Loss: 0.00084465
Iteration 44/1000 | Loss: 0.00098052
Iteration 45/1000 | Loss: 0.00038946
Iteration 46/1000 | Loss: 0.00146247
Iteration 47/1000 | Loss: 0.00093370
Iteration 48/1000 | Loss: 0.00152731
Iteration 49/1000 | Loss: 0.00125200
Iteration 50/1000 | Loss: 0.00180568
Iteration 51/1000 | Loss: 0.00089640
Iteration 52/1000 | Loss: 0.00131496
Iteration 53/1000 | Loss: 0.00149990
Iteration 54/1000 | Loss: 0.00141248
Iteration 55/1000 | Loss: 0.00106077
Iteration 56/1000 | Loss: 0.00078034
Iteration 57/1000 | Loss: 0.00050812
Iteration 58/1000 | Loss: 0.00018408
Iteration 59/1000 | Loss: 0.00180623
Iteration 60/1000 | Loss: 0.00104556
Iteration 61/1000 | Loss: 0.00111389
Iteration 62/1000 | Loss: 0.00078502
Iteration 63/1000 | Loss: 0.00024243
Iteration 64/1000 | Loss: 0.00009180
Iteration 65/1000 | Loss: 0.00056844
Iteration 66/1000 | Loss: 0.00010956
Iteration 67/1000 | Loss: 0.00099577
Iteration 68/1000 | Loss: 0.00032500
Iteration 69/1000 | Loss: 0.00007285
Iteration 70/1000 | Loss: 0.00046799
Iteration 71/1000 | Loss: 0.00063343
Iteration 72/1000 | Loss: 0.00044309
Iteration 73/1000 | Loss: 0.00012236
Iteration 74/1000 | Loss: 0.00038929
Iteration 75/1000 | Loss: 0.00005106
Iteration 76/1000 | Loss: 0.00004613
Iteration 77/1000 | Loss: 0.00046882
Iteration 78/1000 | Loss: 0.00017176
Iteration 79/1000 | Loss: 0.00030735
Iteration 80/1000 | Loss: 0.00015546
Iteration 81/1000 | Loss: 0.00008456
Iteration 82/1000 | Loss: 0.00027783
Iteration 83/1000 | Loss: 0.00015560
Iteration 84/1000 | Loss: 0.00035936
Iteration 85/1000 | Loss: 0.00034978
Iteration 86/1000 | Loss: 0.00090386
Iteration 87/1000 | Loss: 0.00059702
Iteration 88/1000 | Loss: 0.00049591
Iteration 89/1000 | Loss: 0.00013217
Iteration 90/1000 | Loss: 0.00020298
Iteration 91/1000 | Loss: 0.00006666
Iteration 92/1000 | Loss: 0.00018196
Iteration 93/1000 | Loss: 0.00005505
Iteration 94/1000 | Loss: 0.00004497
Iteration 95/1000 | Loss: 0.00004058
Iteration 96/1000 | Loss: 0.00003817
Iteration 97/1000 | Loss: 0.00003689
Iteration 98/1000 | Loss: 0.00032550
Iteration 99/1000 | Loss: 0.00003941
Iteration 100/1000 | Loss: 0.00003581
Iteration 101/1000 | Loss: 0.00003557
Iteration 102/1000 | Loss: 0.00003519
Iteration 103/1000 | Loss: 0.00035272
Iteration 104/1000 | Loss: 0.00004201
Iteration 105/1000 | Loss: 0.00021030
Iteration 106/1000 | Loss: 0.00018415
Iteration 107/1000 | Loss: 0.00007542
Iteration 108/1000 | Loss: 0.00005397
Iteration 109/1000 | Loss: 0.00010106
Iteration 110/1000 | Loss: 0.00013730
Iteration 111/1000 | Loss: 0.00093175
Iteration 112/1000 | Loss: 0.00053416
Iteration 113/1000 | Loss: 0.00038352
Iteration 114/1000 | Loss: 0.00020427
Iteration 115/1000 | Loss: 0.00027924
Iteration 116/1000 | Loss: 0.00023006
Iteration 117/1000 | Loss: 0.00042990
Iteration 118/1000 | Loss: 0.00023088
Iteration 119/1000 | Loss: 0.00031460
Iteration 120/1000 | Loss: 0.00010748
Iteration 121/1000 | Loss: 0.00019244
Iteration 122/1000 | Loss: 0.00027031
Iteration 123/1000 | Loss: 0.00024986
Iteration 124/1000 | Loss: 0.00045012
Iteration 125/1000 | Loss: 0.00027704
Iteration 126/1000 | Loss: 0.00029982
Iteration 127/1000 | Loss: 0.00026069
Iteration 128/1000 | Loss: 0.00015347
Iteration 129/1000 | Loss: 0.00029536
Iteration 130/1000 | Loss: 0.00024779
Iteration 131/1000 | Loss: 0.00019987
Iteration 132/1000 | Loss: 0.00016398
Iteration 133/1000 | Loss: 0.00012880
Iteration 134/1000 | Loss: 0.00010522
Iteration 135/1000 | Loss: 0.00013905
Iteration 136/1000 | Loss: 0.00009003
Iteration 137/1000 | Loss: 0.00011108
Iteration 138/1000 | Loss: 0.00007277
Iteration 139/1000 | Loss: 0.00016572
Iteration 140/1000 | Loss: 0.00007517
Iteration 141/1000 | Loss: 0.00006629
Iteration 142/1000 | Loss: 0.00006541
Iteration 143/1000 | Loss: 0.00006380
Iteration 144/1000 | Loss: 0.00005928
Iteration 145/1000 | Loss: 0.00042471
Iteration 146/1000 | Loss: 0.00033756
Iteration 147/1000 | Loss: 0.00037810
Iteration 148/1000 | Loss: 0.00015026
Iteration 149/1000 | Loss: 0.00020490
Iteration 150/1000 | Loss: 0.00019862
Iteration 151/1000 | Loss: 0.00024351
Iteration 152/1000 | Loss: 0.00014091
Iteration 153/1000 | Loss: 0.00004445
Iteration 154/1000 | Loss: 0.00016990
Iteration 155/1000 | Loss: 0.00008208
Iteration 156/1000 | Loss: 0.00016391
Iteration 157/1000 | Loss: 0.00010677
Iteration 158/1000 | Loss: 0.00017476
Iteration 159/1000 | Loss: 0.00010549
Iteration 160/1000 | Loss: 0.00014216
Iteration 161/1000 | Loss: 0.00009668
Iteration 162/1000 | Loss: 0.00011501
Iteration 163/1000 | Loss: 0.00093646
Iteration 164/1000 | Loss: 0.00010606
Iteration 165/1000 | Loss: 0.00012352
Iteration 166/1000 | Loss: 0.00006903
Iteration 167/1000 | Loss: 0.00003768
Iteration 168/1000 | Loss: 0.00003866
Iteration 169/1000 | Loss: 0.00042565
Iteration 170/1000 | Loss: 0.00023261
Iteration 171/1000 | Loss: 0.00015970
Iteration 172/1000 | Loss: 0.00026052
Iteration 173/1000 | Loss: 0.00019339
Iteration 174/1000 | Loss: 0.00091350
Iteration 175/1000 | Loss: 0.00025640
Iteration 176/1000 | Loss: 0.00034883
Iteration 177/1000 | Loss: 0.00027551
Iteration 178/1000 | Loss: 0.00029529
Iteration 179/1000 | Loss: 0.00019201
Iteration 180/1000 | Loss: 0.00019677
Iteration 181/1000 | Loss: 0.00020699
Iteration 182/1000 | Loss: 0.00025962
Iteration 183/1000 | Loss: 0.00014183
Iteration 184/1000 | Loss: 0.00034937
Iteration 185/1000 | Loss: 0.00029800
Iteration 186/1000 | Loss: 0.00034932
Iteration 187/1000 | Loss: 0.00038533
Iteration 188/1000 | Loss: 0.00028277
Iteration 189/1000 | Loss: 0.00011387
Iteration 190/1000 | Loss: 0.00006671
Iteration 191/1000 | Loss: 0.00003628
Iteration 192/1000 | Loss: 0.00003534
Iteration 193/1000 | Loss: 0.00018834
Iteration 194/1000 | Loss: 0.00221374
Iteration 195/1000 | Loss: 0.00021677
Iteration 196/1000 | Loss: 0.00029696
Iteration 197/1000 | Loss: 0.00017775
Iteration 198/1000 | Loss: 0.00033293
Iteration 199/1000 | Loss: 0.00024420
Iteration 200/1000 | Loss: 0.00048726
Iteration 201/1000 | Loss: 0.00006500
Iteration 202/1000 | Loss: 0.00023102
Iteration 203/1000 | Loss: 0.00031109
Iteration 204/1000 | Loss: 0.00004510
Iteration 205/1000 | Loss: 0.00003791
Iteration 206/1000 | Loss: 0.00003640
Iteration 207/1000 | Loss: 0.00003565
Iteration 208/1000 | Loss: 0.00003776
Iteration 209/1000 | Loss: 0.00003568
Iteration 210/1000 | Loss: 0.00003499
Iteration 211/1000 | Loss: 0.00003492
Iteration 212/1000 | Loss: 0.00003491
Iteration 213/1000 | Loss: 0.00003491
Iteration 214/1000 | Loss: 0.00003490
Iteration 215/1000 | Loss: 0.00003488
Iteration 216/1000 | Loss: 0.00003487
Iteration 217/1000 | Loss: 0.00003487
Iteration 218/1000 | Loss: 0.00003478
Iteration 219/1000 | Loss: 0.00069136
Iteration 220/1000 | Loss: 0.00030730
Iteration 221/1000 | Loss: 0.00042888
Iteration 222/1000 | Loss: 0.00030753
Iteration 223/1000 | Loss: 0.00042601
Iteration 224/1000 | Loss: 0.00029066
Iteration 225/1000 | Loss: 0.00029623
Iteration 226/1000 | Loss: 0.00029925
Iteration 227/1000 | Loss: 0.00004312
Iteration 228/1000 | Loss: 0.00003711
Iteration 229/1000 | Loss: 0.00003479
Iteration 230/1000 | Loss: 0.00003311
Iteration 231/1000 | Loss: 0.00003461
Iteration 232/1000 | Loss: 0.00003252
Iteration 233/1000 | Loss: 0.00003154
Iteration 234/1000 | Loss: 0.00003102
Iteration 235/1000 | Loss: 0.00003083
Iteration 236/1000 | Loss: 0.00003296
Iteration 237/1000 | Loss: 0.00003133
Iteration 238/1000 | Loss: 0.00003077
Iteration 239/1000 | Loss: 0.00003073
Iteration 240/1000 | Loss: 0.00003073
Iteration 241/1000 | Loss: 0.00003296
Iteration 242/1000 | Loss: 0.00003152
Iteration 243/1000 | Loss: 0.00003273
Iteration 244/1000 | Loss: 0.00003150
Iteration 245/1000 | Loss: 0.00003272
Iteration 246/1000 | Loss: 0.00003203
Iteration 247/1000 | Loss: 0.00003306
Iteration 248/1000 | Loss: 0.00003249
Iteration 249/1000 | Loss: 0.00003275
Iteration 250/1000 | Loss: 0.00003217
Iteration 251/1000 | Loss: 0.00003262
Iteration 252/1000 | Loss: 0.00003237
Iteration 253/1000 | Loss: 0.00003272
Iteration 254/1000 | Loss: 0.00003247
Iteration 255/1000 | Loss: 0.00003107
Iteration 256/1000 | Loss: 0.00003299
Iteration 257/1000 | Loss: 0.00003093
Iteration 258/1000 | Loss: 0.00003247
Iteration 259/1000 | Loss: 0.00003136
Iteration 260/1000 | Loss: 0.00003257
Iteration 261/1000 | Loss: 0.00003204
Iteration 262/1000 | Loss: 0.00003262
Iteration 263/1000 | Loss: 0.00003246
Iteration 264/1000 | Loss: 0.00003287
Iteration 265/1000 | Loss: 0.00003258
Iteration 266/1000 | Loss: 0.00003282
Iteration 267/1000 | Loss: 0.00003270
Iteration 268/1000 | Loss: 0.00003279
Iteration 269/1000 | Loss: 0.00003266
Iteration 270/1000 | Loss: 0.00003261
Iteration 271/1000 | Loss: 0.00003257
Iteration 272/1000 | Loss: 0.00003276
Iteration 273/1000 | Loss: 0.00003287
Iteration 274/1000 | Loss: 0.00003244
Iteration 275/1000 | Loss: 0.00003248
Iteration 276/1000 | Loss: 0.00003229
Iteration 277/1000 | Loss: 0.00003240
Iteration 278/1000 | Loss: 0.00003251
Iteration 279/1000 | Loss: 0.00003251
Iteration 280/1000 | Loss: 0.00003269
Iteration 281/1000 | Loss: 0.00003279
Iteration 282/1000 | Loss: 0.00003295
Iteration 283/1000 | Loss: 0.00003285
Iteration 284/1000 | Loss: 0.00003287
Iteration 285/1000 | Loss: 0.00003270
Iteration 286/1000 | Loss: 0.00003262
Iteration 287/1000 | Loss: 0.00003279
Iteration 288/1000 | Loss: 0.00003279
Iteration 289/1000 | Loss: 0.00003247
Iteration 290/1000 | Loss: 0.00003267
Iteration 291/1000 | Loss: 0.00003262
Iteration 292/1000 | Loss: 0.00003269
Iteration 293/1000 | Loss: 0.00003267
Iteration 294/1000 | Loss: 0.00003260
Iteration 295/1000 | Loss: 0.00003306
Iteration 296/1000 | Loss: 0.00003261
Iteration 297/1000 | Loss: 0.00003277
Iteration 298/1000 | Loss: 0.00003287
Iteration 299/1000 | Loss: 0.00003298
Iteration 300/1000 | Loss: 0.00003273
Iteration 301/1000 | Loss: 0.00003287
Iteration 302/1000 | Loss: 0.00003268
Iteration 303/1000 | Loss: 0.00003282
Iteration 304/1000 | Loss: 0.00003314
Iteration 305/1000 | Loss: 0.00003283
Iteration 306/1000 | Loss: 0.00003299
Iteration 307/1000 | Loss: 0.00003278
Iteration 308/1000 | Loss: 0.00003247
Iteration 309/1000 | Loss: 0.00003253
Iteration 310/1000 | Loss: 0.00003279
Iteration 311/1000 | Loss: 0.00003269
Iteration 312/1000 | Loss: 0.00003293
Iteration 313/1000 | Loss: 0.00003269
Iteration 314/1000 | Loss: 0.00003299
Iteration 315/1000 | Loss: 0.00003299
Iteration 316/1000 | Loss: 0.00003289
Iteration 317/1000 | Loss: 0.00003258
Iteration 318/1000 | Loss: 0.00003260
Iteration 319/1000 | Loss: 0.00003258
Iteration 320/1000 | Loss: 0.00003233
Iteration 321/1000 | Loss: 0.00003267
Iteration 322/1000 | Loss: 0.00003253
Iteration 323/1000 | Loss: 0.00003275
Iteration 324/1000 | Loss: 0.00003224
Iteration 325/1000 | Loss: 0.00003286
Iteration 326/1000 | Loss: 0.00003284
Iteration 327/1000 | Loss: 0.00003291
Iteration 328/1000 | Loss: 0.00003288
Iteration 329/1000 | Loss: 0.00003292
Iteration 330/1000 | Loss: 0.00003251
Iteration 331/1000 | Loss: 0.00003283
Iteration 332/1000 | Loss: 0.00003172
Iteration 333/1000 | Loss: 0.00003266
Iteration 334/1000 | Loss: 0.00003265
Iteration 335/1000 | Loss: 0.00003296
Iteration 336/1000 | Loss: 0.00003220
Iteration 337/1000 | Loss: 0.00003177
Iteration 338/1000 | Loss: 0.00003278
Iteration 339/1000 | Loss: 0.00003261
Iteration 340/1000 | Loss: 0.00003266
Iteration 341/1000 | Loss: 0.00003260
Iteration 342/1000 | Loss: 0.00003263
Iteration 343/1000 | Loss: 0.00003300
Iteration 344/1000 | Loss: 0.00003300
Iteration 345/1000 | Loss: 0.00003275
Iteration 346/1000 | Loss: 0.00003168
Iteration 347/1000 | Loss: 0.00003152
Iteration 348/1000 | Loss: 0.00003290
Iteration 349/1000 | Loss: 0.00003115
Iteration 350/1000 | Loss: 0.00003237
Iteration 351/1000 | Loss: 0.00003170
Iteration 352/1000 | Loss: 0.00003277
Iteration 353/1000 | Loss: 0.00003278
Iteration 354/1000 | Loss: 0.00003297
Iteration 355/1000 | Loss: 0.00003236
Iteration 356/1000 | Loss: 0.00003279
Iteration 357/1000 | Loss: 0.00003280
Iteration 358/1000 | Loss: 0.00003303
Iteration 359/1000 | Loss: 0.00003304
Iteration 360/1000 | Loss: 0.00003298
Iteration 361/1000 | Loss: 0.00003284
Iteration 362/1000 | Loss: 0.00003436
Iteration 363/1000 | Loss: 0.00003272
Iteration 364/1000 | Loss: 0.00003391
Iteration 365/1000 | Loss: 0.00003243
Iteration 366/1000 | Loss: 0.00003309
Iteration 367/1000 | Loss: 0.00003288
Iteration 368/1000 | Loss: 0.00003290
Iteration 369/1000 | Loss: 0.00003268
Iteration 370/1000 | Loss: 0.00003292
Iteration 371/1000 | Loss: 0.00003288
Iteration 372/1000 | Loss: 0.00003293
Iteration 373/1000 | Loss: 0.00003299
Iteration 374/1000 | Loss: 0.00003116
Iteration 375/1000 | Loss: 0.00003349
Iteration 376/1000 | Loss: 0.00003301
Iteration 377/1000 | Loss: 0.00003379
Iteration 378/1000 | Loss: 0.00003263
Iteration 379/1000 | Loss: 0.00003325
Iteration 380/1000 | Loss: 0.00003315
Iteration 381/1000 | Loss: 0.00003326
Iteration 382/1000 | Loss: 0.00003299
Iteration 383/1000 | Loss: 0.00003321
Iteration 384/1000 | Loss: 0.00003330
Iteration 385/1000 | Loss: 0.00003319
Iteration 386/1000 | Loss: 0.00003284
Iteration 387/1000 | Loss: 0.00003353
Iteration 388/1000 | Loss: 0.00003281
Iteration 389/1000 | Loss: 0.00003321
Iteration 390/1000 | Loss: 0.00003345
Iteration 391/1000 | Loss: 0.00003311
Iteration 392/1000 | Loss: 0.00003320
Iteration 393/1000 | Loss: 0.00003315
Iteration 394/1000 | Loss: 0.00003327
Iteration 395/1000 | Loss: 0.00003314
Iteration 396/1000 | Loss: 0.00003303
Iteration 397/1000 | Loss: 0.00003362
Iteration 398/1000 | Loss: 0.00003296
Iteration 399/1000 | Loss: 0.00003348
Iteration 400/1000 | Loss: 0.00003293
Iteration 401/1000 | Loss: 0.00003437
Iteration 402/1000 | Loss: 0.00003293
Iteration 403/1000 | Loss: 0.00003334
Iteration 404/1000 | Loss: 0.00003360
Iteration 405/1000 | Loss: 0.00003373
Iteration 406/1000 | Loss: 0.00003314
Iteration 407/1000 | Loss: 0.00003397
Iteration 408/1000 | Loss: 0.00003282
Iteration 409/1000 | Loss: 0.00003315
Iteration 410/1000 | Loss: 0.00003272
Iteration 411/1000 | Loss: 0.00003313
Iteration 412/1000 | Loss: 0.00003429
Iteration 413/1000 | Loss: 0.00003329
Iteration 414/1000 | Loss: 0.00003354
Iteration 415/1000 | Loss: 0.00003318
Iteration 416/1000 | Loss: 0.00003352
Iteration 417/1000 | Loss: 0.00003327
Iteration 418/1000 | Loss: 0.00003314
Iteration 419/1000 | Loss: 0.00003360
Iteration 420/1000 | Loss: 0.00003057
Iteration 421/1000 | Loss: 0.00003052
Iteration 422/1000 | Loss: 0.00003052
Iteration 423/1000 | Loss: 0.00003052
Iteration 424/1000 | Loss: 0.00003052
Iteration 425/1000 | Loss: 0.00003052
Iteration 426/1000 | Loss: 0.00003052
Iteration 427/1000 | Loss: 0.00003052
Iteration 428/1000 | Loss: 0.00003052
Iteration 429/1000 | Loss: 0.00003052
Iteration 430/1000 | Loss: 0.00003052
Iteration 431/1000 | Loss: 0.00003052
Iteration 432/1000 | Loss: 0.00003051
Iteration 433/1000 | Loss: 0.00003051
Iteration 434/1000 | Loss: 0.00003051
Iteration 435/1000 | Loss: 0.00003051
Iteration 436/1000 | Loss: 0.00003051
Iteration 437/1000 | Loss: 0.00003051
Iteration 438/1000 | Loss: 0.00003051
Iteration 439/1000 | Loss: 0.00003051
Iteration 440/1000 | Loss: 0.00003051
Iteration 441/1000 | Loss: 0.00003051
Iteration 442/1000 | Loss: 0.00003051
Iteration 443/1000 | Loss: 0.00003051
Iteration 444/1000 | Loss: 0.00003051
Iteration 445/1000 | Loss: 0.00003051
Iteration 446/1000 | Loss: 0.00003051
Iteration 447/1000 | Loss: 0.00003051
Iteration 448/1000 | Loss: 0.00003051
Iteration 449/1000 | Loss: 0.00003051
Iteration 450/1000 | Loss: 0.00003051
Iteration 451/1000 | Loss: 0.00003051
Iteration 452/1000 | Loss: 0.00003051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 452. Stopping optimization.
Last 5 losses: [3.050585110031534e-05, 3.050585110031534e-05, 3.050585110031534e-05, 3.050585110031534e-05, 3.050585110031534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.050585110031534e-05

Optimization complete. Final v2v error: 4.442497253417969 mm

Highest mean error: 10.259648323059082 mm for frame 27

Lowest mean error: 3.4812283515930176 mm for frame 155

Saving results

Total time: 587.4989507198334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380004
Iteration 2/25 | Loss: 0.00109918
Iteration 3/25 | Loss: 0.00096763
Iteration 4/25 | Loss: 0.00094811
Iteration 5/25 | Loss: 0.00094126
Iteration 6/25 | Loss: 0.00093971
Iteration 7/25 | Loss: 0.00093935
Iteration 8/25 | Loss: 0.00093935
Iteration 9/25 | Loss: 0.00093935
Iteration 10/25 | Loss: 0.00093935
Iteration 11/25 | Loss: 0.00093935
Iteration 12/25 | Loss: 0.00093935
Iteration 13/25 | Loss: 0.00093935
Iteration 14/25 | Loss: 0.00093935
Iteration 15/25 | Loss: 0.00093935
Iteration 16/25 | Loss: 0.00093935
Iteration 17/25 | Loss: 0.00093935
Iteration 18/25 | Loss: 0.00093935
Iteration 19/25 | Loss: 0.00093935
Iteration 20/25 | Loss: 0.00093935
Iteration 21/25 | Loss: 0.00093935
Iteration 22/25 | Loss: 0.00093935
Iteration 23/25 | Loss: 0.00093935
Iteration 24/25 | Loss: 0.00093935
Iteration 25/25 | Loss: 0.00093935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33841062
Iteration 2/25 | Loss: 0.00069206
Iteration 3/25 | Loss: 0.00069206
Iteration 4/25 | Loss: 0.00069206
Iteration 5/25 | Loss: 0.00069206
Iteration 6/25 | Loss: 0.00069206
Iteration 7/25 | Loss: 0.00069206
Iteration 8/25 | Loss: 0.00069206
Iteration 9/25 | Loss: 0.00069206
Iteration 10/25 | Loss: 0.00069206
Iteration 11/25 | Loss: 0.00069206
Iteration 12/25 | Loss: 0.00069206
Iteration 13/25 | Loss: 0.00069206
Iteration 14/25 | Loss: 0.00069206
Iteration 15/25 | Loss: 0.00069206
Iteration 16/25 | Loss: 0.00069206
Iteration 17/25 | Loss: 0.00069206
Iteration 18/25 | Loss: 0.00069206
Iteration 19/25 | Loss: 0.00069206
Iteration 20/25 | Loss: 0.00069206
Iteration 21/25 | Loss: 0.00069206
Iteration 22/25 | Loss: 0.00069206
Iteration 23/25 | Loss: 0.00069206
Iteration 24/25 | Loss: 0.00069206
Iteration 25/25 | Loss: 0.00069206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006920595187693834, 0.0006920595187693834, 0.0006920595187693834, 0.0006920595187693834, 0.0006920595187693834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006920595187693834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069206
Iteration 2/1000 | Loss: 0.00004450
Iteration 3/1000 | Loss: 0.00002414
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001211
Iteration 8/1000 | Loss: 0.00001165
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001062
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001048
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001038
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001027
Iteration 24/1000 | Loss: 0.00001026
Iteration 25/1000 | Loss: 0.00001026
Iteration 26/1000 | Loss: 0.00001025
Iteration 27/1000 | Loss: 0.00001024
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001023
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001022
Iteration 33/1000 | Loss: 0.00001019
Iteration 34/1000 | Loss: 0.00001016
Iteration 35/1000 | Loss: 0.00001014
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001008
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001005
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001003
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001002
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001001
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00001000
Iteration 92/1000 | Loss: 0.00001000
Iteration 93/1000 | Loss: 0.00001000
Iteration 94/1000 | Loss: 0.00001000
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000999
Iteration 106/1000 | Loss: 0.00000999
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000998
Iteration 113/1000 | Loss: 0.00000998
Iteration 114/1000 | Loss: 0.00000998
Iteration 115/1000 | Loss: 0.00000998
Iteration 116/1000 | Loss: 0.00000998
Iteration 117/1000 | Loss: 0.00000998
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000998
Iteration 121/1000 | Loss: 0.00000998
Iteration 122/1000 | Loss: 0.00000998
Iteration 123/1000 | Loss: 0.00000998
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000997
Iteration 130/1000 | Loss: 0.00000997
Iteration 131/1000 | Loss: 0.00000997
Iteration 132/1000 | Loss: 0.00000997
Iteration 133/1000 | Loss: 0.00000997
Iteration 134/1000 | Loss: 0.00000997
Iteration 135/1000 | Loss: 0.00000997
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000996
Iteration 139/1000 | Loss: 0.00000996
Iteration 140/1000 | Loss: 0.00000996
Iteration 141/1000 | Loss: 0.00000996
Iteration 142/1000 | Loss: 0.00000996
Iteration 143/1000 | Loss: 0.00000996
Iteration 144/1000 | Loss: 0.00000996
Iteration 145/1000 | Loss: 0.00000996
Iteration 146/1000 | Loss: 0.00000996
Iteration 147/1000 | Loss: 0.00000996
Iteration 148/1000 | Loss: 0.00000996
Iteration 149/1000 | Loss: 0.00000996
Iteration 150/1000 | Loss: 0.00000995
Iteration 151/1000 | Loss: 0.00000995
Iteration 152/1000 | Loss: 0.00000995
Iteration 153/1000 | Loss: 0.00000995
Iteration 154/1000 | Loss: 0.00000995
Iteration 155/1000 | Loss: 0.00000995
Iteration 156/1000 | Loss: 0.00000995
Iteration 157/1000 | Loss: 0.00000995
Iteration 158/1000 | Loss: 0.00000995
Iteration 159/1000 | Loss: 0.00000995
Iteration 160/1000 | Loss: 0.00000995
Iteration 161/1000 | Loss: 0.00000995
Iteration 162/1000 | Loss: 0.00000995
Iteration 163/1000 | Loss: 0.00000995
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000995
Iteration 166/1000 | Loss: 0.00000995
Iteration 167/1000 | Loss: 0.00000995
Iteration 168/1000 | Loss: 0.00000995
Iteration 169/1000 | Loss: 0.00000995
Iteration 170/1000 | Loss: 0.00000995
Iteration 171/1000 | Loss: 0.00000995
Iteration 172/1000 | Loss: 0.00000995
Iteration 173/1000 | Loss: 0.00000994
Iteration 174/1000 | Loss: 0.00000994
Iteration 175/1000 | Loss: 0.00000994
Iteration 176/1000 | Loss: 0.00000994
Iteration 177/1000 | Loss: 0.00000994
Iteration 178/1000 | Loss: 0.00000994
Iteration 179/1000 | Loss: 0.00000994
Iteration 180/1000 | Loss: 0.00000994
Iteration 181/1000 | Loss: 0.00000994
Iteration 182/1000 | Loss: 0.00000994
Iteration 183/1000 | Loss: 0.00000994
Iteration 184/1000 | Loss: 0.00000994
Iteration 185/1000 | Loss: 0.00000994
Iteration 186/1000 | Loss: 0.00000994
Iteration 187/1000 | Loss: 0.00000994
Iteration 188/1000 | Loss: 0.00000994
Iteration 189/1000 | Loss: 0.00000994
Iteration 190/1000 | Loss: 0.00000994
Iteration 191/1000 | Loss: 0.00000994
Iteration 192/1000 | Loss: 0.00000994
Iteration 193/1000 | Loss: 0.00000994
Iteration 194/1000 | Loss: 0.00000994
Iteration 195/1000 | Loss: 0.00000994
Iteration 196/1000 | Loss: 0.00000993
Iteration 197/1000 | Loss: 0.00000993
Iteration 198/1000 | Loss: 0.00000993
Iteration 199/1000 | Loss: 0.00000993
Iteration 200/1000 | Loss: 0.00000993
Iteration 201/1000 | Loss: 0.00000993
Iteration 202/1000 | Loss: 0.00000993
Iteration 203/1000 | Loss: 0.00000993
Iteration 204/1000 | Loss: 0.00000993
Iteration 205/1000 | Loss: 0.00000993
Iteration 206/1000 | Loss: 0.00000993
Iteration 207/1000 | Loss: 0.00000993
Iteration 208/1000 | Loss: 0.00000993
Iteration 209/1000 | Loss: 0.00000993
Iteration 210/1000 | Loss: 0.00000993
Iteration 211/1000 | Loss: 0.00000993
Iteration 212/1000 | Loss: 0.00000993
Iteration 213/1000 | Loss: 0.00000992
Iteration 214/1000 | Loss: 0.00000992
Iteration 215/1000 | Loss: 0.00000992
Iteration 216/1000 | Loss: 0.00000992
Iteration 217/1000 | Loss: 0.00000992
Iteration 218/1000 | Loss: 0.00000992
Iteration 219/1000 | Loss: 0.00000992
Iteration 220/1000 | Loss: 0.00000992
Iteration 221/1000 | Loss: 0.00000992
Iteration 222/1000 | Loss: 0.00000992
Iteration 223/1000 | Loss: 0.00000992
Iteration 224/1000 | Loss: 0.00000992
Iteration 225/1000 | Loss: 0.00000992
Iteration 226/1000 | Loss: 0.00000992
Iteration 227/1000 | Loss: 0.00000992
Iteration 228/1000 | Loss: 0.00000992
Iteration 229/1000 | Loss: 0.00000992
Iteration 230/1000 | Loss: 0.00000992
Iteration 231/1000 | Loss: 0.00000992
Iteration 232/1000 | Loss: 0.00000992
Iteration 233/1000 | Loss: 0.00000992
Iteration 234/1000 | Loss: 0.00000992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [9.922876415657811e-06, 9.922876415657811e-06, 9.922876415657811e-06, 9.922876415657811e-06, 9.922876415657811e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.922876415657811e-06

Optimization complete. Final v2v error: 2.6823246479034424 mm

Highest mean error: 3.1097524166107178 mm for frame 57

Lowest mean error: 2.3612256050109863 mm for frame 101

Saving results

Total time: 44.30618119239807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387072
Iteration 2/25 | Loss: 0.00110170
Iteration 3/25 | Loss: 0.00099398
Iteration 4/25 | Loss: 0.00098611
Iteration 5/25 | Loss: 0.00098237
Iteration 6/25 | Loss: 0.00098127
Iteration 7/25 | Loss: 0.00098127
Iteration 8/25 | Loss: 0.00098127
Iteration 9/25 | Loss: 0.00098127
Iteration 10/25 | Loss: 0.00098127
Iteration 11/25 | Loss: 0.00098127
Iteration 12/25 | Loss: 0.00098127
Iteration 13/25 | Loss: 0.00098127
Iteration 14/25 | Loss: 0.00098127
Iteration 15/25 | Loss: 0.00098127
Iteration 16/25 | Loss: 0.00098127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000981270568445325, 0.000981270568445325, 0.000981270568445325, 0.000981270568445325, 0.000981270568445325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000981270568445325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62047350
Iteration 2/25 | Loss: 0.00057345
Iteration 3/25 | Loss: 0.00057345
Iteration 4/25 | Loss: 0.00057345
Iteration 5/25 | Loss: 0.00057345
Iteration 6/25 | Loss: 0.00057345
Iteration 7/25 | Loss: 0.00057345
Iteration 8/25 | Loss: 0.00057345
Iteration 9/25 | Loss: 0.00057344
Iteration 10/25 | Loss: 0.00057344
Iteration 11/25 | Loss: 0.00057344
Iteration 12/25 | Loss: 0.00057344
Iteration 13/25 | Loss: 0.00057344
Iteration 14/25 | Loss: 0.00057344
Iteration 15/25 | Loss: 0.00057344
Iteration 16/25 | Loss: 0.00057344
Iteration 17/25 | Loss: 0.00057344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005734445876441896, 0.0005734445876441896, 0.0005734445876441896, 0.0005734445876441896, 0.0005734445876441896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005734445876441896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057344
Iteration 2/1000 | Loss: 0.00003029
Iteration 3/1000 | Loss: 0.00001856
Iteration 4/1000 | Loss: 0.00001206
Iteration 5/1000 | Loss: 0.00001063
Iteration 6/1000 | Loss: 0.00000989
Iteration 7/1000 | Loss: 0.00000938
Iteration 8/1000 | Loss: 0.00000909
Iteration 9/1000 | Loss: 0.00000883
Iteration 10/1000 | Loss: 0.00000863
Iteration 11/1000 | Loss: 0.00000844
Iteration 12/1000 | Loss: 0.00000841
Iteration 13/1000 | Loss: 0.00000839
Iteration 14/1000 | Loss: 0.00000836
Iteration 15/1000 | Loss: 0.00000835
Iteration 16/1000 | Loss: 0.00000835
Iteration 17/1000 | Loss: 0.00000834
Iteration 18/1000 | Loss: 0.00000834
Iteration 19/1000 | Loss: 0.00000833
Iteration 20/1000 | Loss: 0.00000828
Iteration 21/1000 | Loss: 0.00000828
Iteration 22/1000 | Loss: 0.00000828
Iteration 23/1000 | Loss: 0.00000828
Iteration 24/1000 | Loss: 0.00000828
Iteration 25/1000 | Loss: 0.00000828
Iteration 26/1000 | Loss: 0.00000827
Iteration 27/1000 | Loss: 0.00000827
Iteration 28/1000 | Loss: 0.00000826
Iteration 29/1000 | Loss: 0.00000826
Iteration 30/1000 | Loss: 0.00000825
Iteration 31/1000 | Loss: 0.00000825
Iteration 32/1000 | Loss: 0.00000825
Iteration 33/1000 | Loss: 0.00000824
Iteration 34/1000 | Loss: 0.00000824
Iteration 35/1000 | Loss: 0.00000824
Iteration 36/1000 | Loss: 0.00000824
Iteration 37/1000 | Loss: 0.00000824
Iteration 38/1000 | Loss: 0.00000823
Iteration 39/1000 | Loss: 0.00000822
Iteration 40/1000 | Loss: 0.00000822
Iteration 41/1000 | Loss: 0.00000821
Iteration 42/1000 | Loss: 0.00000821
Iteration 43/1000 | Loss: 0.00000820
Iteration 44/1000 | Loss: 0.00000820
Iteration 45/1000 | Loss: 0.00000820
Iteration 46/1000 | Loss: 0.00000820
Iteration 47/1000 | Loss: 0.00000820
Iteration 48/1000 | Loss: 0.00000820
Iteration 49/1000 | Loss: 0.00000820
Iteration 50/1000 | Loss: 0.00000820
Iteration 51/1000 | Loss: 0.00000820
Iteration 52/1000 | Loss: 0.00000820
Iteration 53/1000 | Loss: 0.00000820
Iteration 54/1000 | Loss: 0.00000820
Iteration 55/1000 | Loss: 0.00000820
Iteration 56/1000 | Loss: 0.00000820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [8.198280738724861e-06, 8.198280738724861e-06, 8.198280738724861e-06, 8.198280738724861e-06, 8.198280738724861e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.198280738724861e-06

Optimization complete. Final v2v error: 2.4554402828216553 mm

Highest mean error: 2.8534717559814453 mm for frame 108

Lowest mean error: 2.2082746028900146 mm for frame 19

Saving results

Total time: 28.773780822753906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999268
Iteration 2/25 | Loss: 0.00118471
Iteration 3/25 | Loss: 0.00105622
Iteration 4/25 | Loss: 0.00104314
Iteration 5/25 | Loss: 0.00103982
Iteration 6/25 | Loss: 0.00103891
Iteration 7/25 | Loss: 0.00103891
Iteration 8/25 | Loss: 0.00103891
Iteration 9/25 | Loss: 0.00103891
Iteration 10/25 | Loss: 0.00103891
Iteration 11/25 | Loss: 0.00103891
Iteration 12/25 | Loss: 0.00103891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010389069793745875, 0.0010389069793745875, 0.0010389069793745875, 0.0010389069793745875, 0.0010389069793745875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010389069793745875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.06441498
Iteration 2/25 | Loss: 0.00078456
Iteration 3/25 | Loss: 0.00078456
Iteration 4/25 | Loss: 0.00078456
Iteration 5/25 | Loss: 0.00078456
Iteration 6/25 | Loss: 0.00078456
Iteration 7/25 | Loss: 0.00078456
Iteration 8/25 | Loss: 0.00078456
Iteration 9/25 | Loss: 0.00078456
Iteration 10/25 | Loss: 0.00078456
Iteration 11/25 | Loss: 0.00078456
Iteration 12/25 | Loss: 0.00078456
Iteration 13/25 | Loss: 0.00078456
Iteration 14/25 | Loss: 0.00078456
Iteration 15/25 | Loss: 0.00078456
Iteration 16/25 | Loss: 0.00078456
Iteration 17/25 | Loss: 0.00078456
Iteration 18/25 | Loss: 0.00078456
Iteration 19/25 | Loss: 0.00078456
Iteration 20/25 | Loss: 0.00078456
Iteration 21/25 | Loss: 0.00078456
Iteration 22/25 | Loss: 0.00078456
Iteration 23/25 | Loss: 0.00078456
Iteration 24/25 | Loss: 0.00078456
Iteration 25/25 | Loss: 0.00078456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078456
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00001598
Iteration 4/1000 | Loss: 0.00001393
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001156
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001153
Iteration 39/1000 | Loss: 0.00001153
Iteration 40/1000 | Loss: 0.00001153
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001152
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001151
Iteration 45/1000 | Loss: 0.00001150
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001149
Iteration 48/1000 | Loss: 0.00001149
Iteration 49/1000 | Loss: 0.00001149
Iteration 50/1000 | Loss: 0.00001149
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001148
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001147
Iteration 57/1000 | Loss: 0.00001147
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001144
Iteration 72/1000 | Loss: 0.00001144
Iteration 73/1000 | Loss: 0.00001144
Iteration 74/1000 | Loss: 0.00001144
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001143
Iteration 77/1000 | Loss: 0.00001142
Iteration 78/1000 | Loss: 0.00001142
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001141
Iteration 86/1000 | Loss: 0.00001141
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001140
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001139
Iteration 93/1000 | Loss: 0.00001139
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001139
Iteration 96/1000 | Loss: 0.00001138
Iteration 97/1000 | Loss: 0.00001138
Iteration 98/1000 | Loss: 0.00001138
Iteration 99/1000 | Loss: 0.00001138
Iteration 100/1000 | Loss: 0.00001138
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001137
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001137
Iteration 105/1000 | Loss: 0.00001137
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001136
Iteration 112/1000 | Loss: 0.00001136
Iteration 113/1000 | Loss: 0.00001136
Iteration 114/1000 | Loss: 0.00001136
Iteration 115/1000 | Loss: 0.00001136
Iteration 116/1000 | Loss: 0.00001136
Iteration 117/1000 | Loss: 0.00001136
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001135
Iteration 120/1000 | Loss: 0.00001135
Iteration 121/1000 | Loss: 0.00001134
Iteration 122/1000 | Loss: 0.00001134
Iteration 123/1000 | Loss: 0.00001134
Iteration 124/1000 | Loss: 0.00001134
Iteration 125/1000 | Loss: 0.00001134
Iteration 126/1000 | Loss: 0.00001134
Iteration 127/1000 | Loss: 0.00001134
Iteration 128/1000 | Loss: 0.00001134
Iteration 129/1000 | Loss: 0.00001134
Iteration 130/1000 | Loss: 0.00001134
Iteration 131/1000 | Loss: 0.00001134
Iteration 132/1000 | Loss: 0.00001134
Iteration 133/1000 | Loss: 0.00001134
Iteration 134/1000 | Loss: 0.00001133
Iteration 135/1000 | Loss: 0.00001133
Iteration 136/1000 | Loss: 0.00001133
Iteration 137/1000 | Loss: 0.00001133
Iteration 138/1000 | Loss: 0.00001133
Iteration 139/1000 | Loss: 0.00001132
Iteration 140/1000 | Loss: 0.00001132
Iteration 141/1000 | Loss: 0.00001132
Iteration 142/1000 | Loss: 0.00001132
Iteration 143/1000 | Loss: 0.00001132
Iteration 144/1000 | Loss: 0.00001131
Iteration 145/1000 | Loss: 0.00001131
Iteration 146/1000 | Loss: 0.00001131
Iteration 147/1000 | Loss: 0.00001131
Iteration 148/1000 | Loss: 0.00001131
Iteration 149/1000 | Loss: 0.00001131
Iteration 150/1000 | Loss: 0.00001131
Iteration 151/1000 | Loss: 0.00001131
Iteration 152/1000 | Loss: 0.00001131
Iteration 153/1000 | Loss: 0.00001131
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001131
Iteration 157/1000 | Loss: 0.00001130
Iteration 158/1000 | Loss: 0.00001130
Iteration 159/1000 | Loss: 0.00001130
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Iteration 163/1000 | Loss: 0.00001129
Iteration 164/1000 | Loss: 0.00001129
Iteration 165/1000 | Loss: 0.00001129
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Iteration 168/1000 | Loss: 0.00001128
Iteration 169/1000 | Loss: 0.00001128
Iteration 170/1000 | Loss: 0.00001128
Iteration 171/1000 | Loss: 0.00001127
Iteration 172/1000 | Loss: 0.00001127
Iteration 173/1000 | Loss: 0.00001127
Iteration 174/1000 | Loss: 0.00001126
Iteration 175/1000 | Loss: 0.00001126
Iteration 176/1000 | Loss: 0.00001126
Iteration 177/1000 | Loss: 0.00001126
Iteration 178/1000 | Loss: 0.00001126
Iteration 179/1000 | Loss: 0.00001125
Iteration 180/1000 | Loss: 0.00001125
Iteration 181/1000 | Loss: 0.00001125
Iteration 182/1000 | Loss: 0.00001124
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001124
Iteration 185/1000 | Loss: 0.00001124
Iteration 186/1000 | Loss: 0.00001124
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001123
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Iteration 193/1000 | Loss: 0.00001123
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001122
Iteration 196/1000 | Loss: 0.00001122
Iteration 197/1000 | Loss: 0.00001122
Iteration 198/1000 | Loss: 0.00001122
Iteration 199/1000 | Loss: 0.00001122
Iteration 200/1000 | Loss: 0.00001122
Iteration 201/1000 | Loss: 0.00001122
Iteration 202/1000 | Loss: 0.00001122
Iteration 203/1000 | Loss: 0.00001122
Iteration 204/1000 | Loss: 0.00001121
Iteration 205/1000 | Loss: 0.00001121
Iteration 206/1000 | Loss: 0.00001121
Iteration 207/1000 | Loss: 0.00001121
Iteration 208/1000 | Loss: 0.00001121
Iteration 209/1000 | Loss: 0.00001121
Iteration 210/1000 | Loss: 0.00001121
Iteration 211/1000 | Loss: 0.00001120
Iteration 212/1000 | Loss: 0.00001120
Iteration 213/1000 | Loss: 0.00001120
Iteration 214/1000 | Loss: 0.00001120
Iteration 215/1000 | Loss: 0.00001120
Iteration 216/1000 | Loss: 0.00001120
Iteration 217/1000 | Loss: 0.00001120
Iteration 218/1000 | Loss: 0.00001120
Iteration 219/1000 | Loss: 0.00001120
Iteration 220/1000 | Loss: 0.00001120
Iteration 221/1000 | Loss: 0.00001120
Iteration 222/1000 | Loss: 0.00001120
Iteration 223/1000 | Loss: 0.00001120
Iteration 224/1000 | Loss: 0.00001120
Iteration 225/1000 | Loss: 0.00001120
Iteration 226/1000 | Loss: 0.00001120
Iteration 227/1000 | Loss: 0.00001119
Iteration 228/1000 | Loss: 0.00001119
Iteration 229/1000 | Loss: 0.00001119
Iteration 230/1000 | Loss: 0.00001119
Iteration 231/1000 | Loss: 0.00001119
Iteration 232/1000 | Loss: 0.00001119
Iteration 233/1000 | Loss: 0.00001119
Iteration 234/1000 | Loss: 0.00001119
Iteration 235/1000 | Loss: 0.00001119
Iteration 236/1000 | Loss: 0.00001119
Iteration 237/1000 | Loss: 0.00001119
Iteration 238/1000 | Loss: 0.00001119
Iteration 239/1000 | Loss: 0.00001119
Iteration 240/1000 | Loss: 0.00001119
Iteration 241/1000 | Loss: 0.00001119
Iteration 242/1000 | Loss: 0.00001119
Iteration 243/1000 | Loss: 0.00001119
Iteration 244/1000 | Loss: 0.00001119
Iteration 245/1000 | Loss: 0.00001119
Iteration 246/1000 | Loss: 0.00001119
Iteration 247/1000 | Loss: 0.00001119
Iteration 248/1000 | Loss: 0.00001119
Iteration 249/1000 | Loss: 0.00001119
Iteration 250/1000 | Loss: 0.00001119
Iteration 251/1000 | Loss: 0.00001119
Iteration 252/1000 | Loss: 0.00001119
Iteration 253/1000 | Loss: 0.00001119
Iteration 254/1000 | Loss: 0.00001119
Iteration 255/1000 | Loss: 0.00001119
Iteration 256/1000 | Loss: 0.00001119
Iteration 257/1000 | Loss: 0.00001119
Iteration 258/1000 | Loss: 0.00001119
Iteration 259/1000 | Loss: 0.00001119
Iteration 260/1000 | Loss: 0.00001119
Iteration 261/1000 | Loss: 0.00001119
Iteration 262/1000 | Loss: 0.00001119
Iteration 263/1000 | Loss: 0.00001119
Iteration 264/1000 | Loss: 0.00001119
Iteration 265/1000 | Loss: 0.00001119
Iteration 266/1000 | Loss: 0.00001119
Iteration 267/1000 | Loss: 0.00001119
Iteration 268/1000 | Loss: 0.00001119
Iteration 269/1000 | Loss: 0.00001119
Iteration 270/1000 | Loss: 0.00001119
Iteration 271/1000 | Loss: 0.00001119
Iteration 272/1000 | Loss: 0.00001119
Iteration 273/1000 | Loss: 0.00001119
Iteration 274/1000 | Loss: 0.00001119
Iteration 275/1000 | Loss: 0.00001119
Iteration 276/1000 | Loss: 0.00001119
Iteration 277/1000 | Loss: 0.00001119
Iteration 278/1000 | Loss: 0.00001119
Iteration 279/1000 | Loss: 0.00001119
Iteration 280/1000 | Loss: 0.00001119
Iteration 281/1000 | Loss: 0.00001119
Iteration 282/1000 | Loss: 0.00001119
Iteration 283/1000 | Loss: 0.00001119
Iteration 284/1000 | Loss: 0.00001119
Iteration 285/1000 | Loss: 0.00001119
Iteration 286/1000 | Loss: 0.00001119
Iteration 287/1000 | Loss: 0.00001119
Iteration 288/1000 | Loss: 0.00001119
Iteration 289/1000 | Loss: 0.00001119
Iteration 290/1000 | Loss: 0.00001119
Iteration 291/1000 | Loss: 0.00001119
Iteration 292/1000 | Loss: 0.00001119
Iteration 293/1000 | Loss: 0.00001119
Iteration 294/1000 | Loss: 0.00001119
Iteration 295/1000 | Loss: 0.00001119
Iteration 296/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.118834097724175e-05, 1.118834097724175e-05, 1.118834097724175e-05, 1.118834097724175e-05, 1.118834097724175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.118834097724175e-05

Optimization complete. Final v2v error: 2.8210034370422363 mm

Highest mean error: 3.0571184158325195 mm for frame 92

Lowest mean error: 2.3590240478515625 mm for frame 149

Saving results

Total time: 41.07891392707825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057639
Iteration 2/25 | Loss: 0.00210632
Iteration 3/25 | Loss: 0.00169670
Iteration 4/25 | Loss: 0.00149623
Iteration 5/25 | Loss: 0.00137079
Iteration 6/25 | Loss: 0.00129149
Iteration 7/25 | Loss: 0.00123397
Iteration 8/25 | Loss: 0.00120097
Iteration 9/25 | Loss: 0.00117403
Iteration 10/25 | Loss: 0.00115602
Iteration 11/25 | Loss: 0.00115031
Iteration 12/25 | Loss: 0.00115066
Iteration 13/25 | Loss: 0.00115076
Iteration 14/25 | Loss: 0.00115079
Iteration 15/25 | Loss: 0.00115149
Iteration 16/25 | Loss: 0.00115105
Iteration 17/25 | Loss: 0.00115106
Iteration 18/25 | Loss: 0.00115108
Iteration 19/25 | Loss: 0.00114968
Iteration 20/25 | Loss: 0.00114864
Iteration 21/25 | Loss: 0.00115036
Iteration 22/25 | Loss: 0.00115033
Iteration 23/25 | Loss: 0.00115005
Iteration 24/25 | Loss: 0.00114892
Iteration 25/25 | Loss: 0.00115040

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77862763
Iteration 2/25 | Loss: 0.00102757
Iteration 3/25 | Loss: 0.00102733
Iteration 4/25 | Loss: 0.00102732
Iteration 5/25 | Loss: 0.00102732
Iteration 6/25 | Loss: 0.00102732
Iteration 7/25 | Loss: 0.00102732
Iteration 8/25 | Loss: 0.00102732
Iteration 9/25 | Loss: 0.00102732
Iteration 10/25 | Loss: 0.00102732
Iteration 11/25 | Loss: 0.00102732
Iteration 12/25 | Loss: 0.00102732
Iteration 13/25 | Loss: 0.00102732
Iteration 14/25 | Loss: 0.00102732
Iteration 15/25 | Loss: 0.00102732
Iteration 16/25 | Loss: 0.00102732
Iteration 17/25 | Loss: 0.00102732
Iteration 18/25 | Loss: 0.00102732
Iteration 19/25 | Loss: 0.00102732
Iteration 20/25 | Loss: 0.00102732
Iteration 21/25 | Loss: 0.00102732
Iteration 22/25 | Loss: 0.00102732
Iteration 23/25 | Loss: 0.00102732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010273214429616928, 0.0010273214429616928, 0.0010273214429616928, 0.0010273214429616928, 0.0010273214429616928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010273214429616928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102732
Iteration 2/1000 | Loss: 0.00010413
Iteration 3/1000 | Loss: 0.00006867
Iteration 4/1000 | Loss: 0.00006009
Iteration 5/1000 | Loss: 0.00005040
Iteration 6/1000 | Loss: 0.00005010
Iteration 7/1000 | Loss: 0.00004747
Iteration 8/1000 | Loss: 0.00004194
Iteration 9/1000 | Loss: 0.00004943
Iteration 10/1000 | Loss: 0.00004997
Iteration 11/1000 | Loss: 0.00005047
Iteration 12/1000 | Loss: 0.00005393
Iteration 13/1000 | Loss: 0.00005825
Iteration 14/1000 | Loss: 0.00005452
Iteration 15/1000 | Loss: 0.00004770
Iteration 16/1000 | Loss: 0.00005524
Iteration 17/1000 | Loss: 0.00005588
Iteration 18/1000 | Loss: 0.00007272
Iteration 19/1000 | Loss: 0.00005125
Iteration 20/1000 | Loss: 0.00005695
Iteration 21/1000 | Loss: 0.00565459
Iteration 22/1000 | Loss: 0.00012776
Iteration 23/1000 | Loss: 0.00008434
Iteration 24/1000 | Loss: 0.00006008
Iteration 25/1000 | Loss: 0.00004634
Iteration 26/1000 | Loss: 0.00004150
Iteration 27/1000 | Loss: 0.00003909
Iteration 28/1000 | Loss: 0.00003744
Iteration 29/1000 | Loss: 0.00003667
Iteration 30/1000 | Loss: 0.00003619
Iteration 31/1000 | Loss: 0.00003591
Iteration 32/1000 | Loss: 0.00003569
Iteration 33/1000 | Loss: 0.00003551
Iteration 34/1000 | Loss: 0.00003526
Iteration 35/1000 | Loss: 0.00003501
Iteration 36/1000 | Loss: 0.00003479
Iteration 37/1000 | Loss: 0.00003462
Iteration 38/1000 | Loss: 0.00003462
Iteration 39/1000 | Loss: 0.00003451
Iteration 40/1000 | Loss: 0.00003450
Iteration 41/1000 | Loss: 0.00003450
Iteration 42/1000 | Loss: 0.00003449
Iteration 43/1000 | Loss: 0.00003447
Iteration 44/1000 | Loss: 0.00003446
Iteration 45/1000 | Loss: 0.00003446
Iteration 46/1000 | Loss: 0.00003445
Iteration 47/1000 | Loss: 0.00003445
Iteration 48/1000 | Loss: 0.00003444
Iteration 49/1000 | Loss: 0.00003439
Iteration 50/1000 | Loss: 0.00003437
Iteration 51/1000 | Loss: 0.00003436
Iteration 52/1000 | Loss: 0.00003436
Iteration 53/1000 | Loss: 0.00003436
Iteration 54/1000 | Loss: 0.00003436
Iteration 55/1000 | Loss: 0.00003436
Iteration 56/1000 | Loss: 0.00003436
Iteration 57/1000 | Loss: 0.00003436
Iteration 58/1000 | Loss: 0.00003436
Iteration 59/1000 | Loss: 0.00003434
Iteration 60/1000 | Loss: 0.00003428
Iteration 61/1000 | Loss: 0.00003427
Iteration 62/1000 | Loss: 0.00003425
Iteration 63/1000 | Loss: 0.00003422
Iteration 64/1000 | Loss: 0.00003418
Iteration 65/1000 | Loss: 0.00003415
Iteration 66/1000 | Loss: 0.00003414
Iteration 67/1000 | Loss: 0.00003414
Iteration 68/1000 | Loss: 0.00003414
Iteration 69/1000 | Loss: 0.00003414
Iteration 70/1000 | Loss: 0.00003414
Iteration 71/1000 | Loss: 0.00003414
Iteration 72/1000 | Loss: 0.00003414
Iteration 73/1000 | Loss: 0.00003414
Iteration 74/1000 | Loss: 0.00003414
Iteration 75/1000 | Loss: 0.00003414
Iteration 76/1000 | Loss: 0.00003413
Iteration 77/1000 | Loss: 0.00003413
Iteration 78/1000 | Loss: 0.00003413
Iteration 79/1000 | Loss: 0.00003413
Iteration 80/1000 | Loss: 0.00003412
Iteration 81/1000 | Loss: 0.00003412
Iteration 82/1000 | Loss: 0.00003411
Iteration 83/1000 | Loss: 0.00003410
Iteration 84/1000 | Loss: 0.00003410
Iteration 85/1000 | Loss: 0.00003409
Iteration 86/1000 | Loss: 0.00003409
Iteration 87/1000 | Loss: 0.00003408
Iteration 88/1000 | Loss: 0.00003408
Iteration 89/1000 | Loss: 0.00003407
Iteration 90/1000 | Loss: 0.00003407
Iteration 91/1000 | Loss: 0.00003407
Iteration 92/1000 | Loss: 0.00003407
Iteration 93/1000 | Loss: 0.00003407
Iteration 94/1000 | Loss: 0.00003407
Iteration 95/1000 | Loss: 0.00003407
Iteration 96/1000 | Loss: 0.00003407
Iteration 97/1000 | Loss: 0.00003407
Iteration 98/1000 | Loss: 0.00003407
Iteration 99/1000 | Loss: 0.00003407
Iteration 100/1000 | Loss: 0.00003407
Iteration 101/1000 | Loss: 0.00003407
Iteration 102/1000 | Loss: 0.00003407
Iteration 103/1000 | Loss: 0.00003407
Iteration 104/1000 | Loss: 0.00003407
Iteration 105/1000 | Loss: 0.00003407
Iteration 106/1000 | Loss: 0.00003407
Iteration 107/1000 | Loss: 0.00003407
Iteration 108/1000 | Loss: 0.00003407
Iteration 109/1000 | Loss: 0.00003407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [3.406598625588231e-05, 3.406598625588231e-05, 3.406598625588231e-05, 3.406598625588231e-05, 3.406598625588231e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.406598625588231e-05

Optimization complete. Final v2v error: 4.63776969909668 mm

Highest mean error: 6.212691307067871 mm for frame 64

Lowest mean error: 3.037135124206543 mm for frame 0

Saving results

Total time: 107.72427821159363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841256
Iteration 2/25 | Loss: 0.00108009
Iteration 3/25 | Loss: 0.00097558
Iteration 4/25 | Loss: 0.00096588
Iteration 5/25 | Loss: 0.00096362
Iteration 6/25 | Loss: 0.00096330
Iteration 7/25 | Loss: 0.00096330
Iteration 8/25 | Loss: 0.00096330
Iteration 9/25 | Loss: 0.00096330
Iteration 10/25 | Loss: 0.00096330
Iteration 11/25 | Loss: 0.00096330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009633045992814004, 0.0009633045992814004, 0.0009633045992814004, 0.0009633045992814004, 0.0009633045992814004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009633045992814004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37194026
Iteration 2/25 | Loss: 0.00059085
Iteration 3/25 | Loss: 0.00059082
Iteration 4/25 | Loss: 0.00059082
Iteration 5/25 | Loss: 0.00059082
Iteration 6/25 | Loss: 0.00059082
Iteration 7/25 | Loss: 0.00059082
Iteration 8/25 | Loss: 0.00059082
Iteration 9/25 | Loss: 0.00059082
Iteration 10/25 | Loss: 0.00059082
Iteration 11/25 | Loss: 0.00059082
Iteration 12/25 | Loss: 0.00059082
Iteration 13/25 | Loss: 0.00059082
Iteration 14/25 | Loss: 0.00059082
Iteration 15/25 | Loss: 0.00059082
Iteration 16/25 | Loss: 0.00059082
Iteration 17/25 | Loss: 0.00059082
Iteration 18/25 | Loss: 0.00059082
Iteration 19/25 | Loss: 0.00059082
Iteration 20/25 | Loss: 0.00059082
Iteration 21/25 | Loss: 0.00059082
Iteration 22/25 | Loss: 0.00059082
Iteration 23/25 | Loss: 0.00059082
Iteration 24/25 | Loss: 0.00059082
Iteration 25/25 | Loss: 0.00059082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059082
Iteration 2/1000 | Loss: 0.00001739
Iteration 3/1000 | Loss: 0.00001081
Iteration 4/1000 | Loss: 0.00000935
Iteration 5/1000 | Loss: 0.00000854
Iteration 6/1000 | Loss: 0.00000801
Iteration 7/1000 | Loss: 0.00000759
Iteration 8/1000 | Loss: 0.00000740
Iteration 9/1000 | Loss: 0.00000740
Iteration 10/1000 | Loss: 0.00000737
Iteration 11/1000 | Loss: 0.00000729
Iteration 12/1000 | Loss: 0.00000719
Iteration 13/1000 | Loss: 0.00000718
Iteration 14/1000 | Loss: 0.00000716
Iteration 15/1000 | Loss: 0.00000715
Iteration 16/1000 | Loss: 0.00000714
Iteration 17/1000 | Loss: 0.00000714
Iteration 18/1000 | Loss: 0.00000712
Iteration 19/1000 | Loss: 0.00000712
Iteration 20/1000 | Loss: 0.00000712
Iteration 21/1000 | Loss: 0.00000711
Iteration 22/1000 | Loss: 0.00000711
Iteration 23/1000 | Loss: 0.00000711
Iteration 24/1000 | Loss: 0.00000710
Iteration 25/1000 | Loss: 0.00000710
Iteration 26/1000 | Loss: 0.00000708
Iteration 27/1000 | Loss: 0.00000707
Iteration 28/1000 | Loss: 0.00000707
Iteration 29/1000 | Loss: 0.00000707
Iteration 30/1000 | Loss: 0.00000707
Iteration 31/1000 | Loss: 0.00000706
Iteration 32/1000 | Loss: 0.00000705
Iteration 33/1000 | Loss: 0.00000705
Iteration 34/1000 | Loss: 0.00000704
Iteration 35/1000 | Loss: 0.00000704
Iteration 36/1000 | Loss: 0.00000702
Iteration 37/1000 | Loss: 0.00000702
Iteration 38/1000 | Loss: 0.00000702
Iteration 39/1000 | Loss: 0.00000702
Iteration 40/1000 | Loss: 0.00000702
Iteration 41/1000 | Loss: 0.00000702
Iteration 42/1000 | Loss: 0.00000702
Iteration 43/1000 | Loss: 0.00000702
Iteration 44/1000 | Loss: 0.00000702
Iteration 45/1000 | Loss: 0.00000701
Iteration 46/1000 | Loss: 0.00000701
Iteration 47/1000 | Loss: 0.00000701
Iteration 48/1000 | Loss: 0.00000701
Iteration 49/1000 | Loss: 0.00000701
Iteration 50/1000 | Loss: 0.00000700
Iteration 51/1000 | Loss: 0.00000700
Iteration 52/1000 | Loss: 0.00000699
Iteration 53/1000 | Loss: 0.00000699
Iteration 54/1000 | Loss: 0.00000697
Iteration 55/1000 | Loss: 0.00000696
Iteration 56/1000 | Loss: 0.00000696
Iteration 57/1000 | Loss: 0.00000696
Iteration 58/1000 | Loss: 0.00000696
Iteration 59/1000 | Loss: 0.00000696
Iteration 60/1000 | Loss: 0.00000696
Iteration 61/1000 | Loss: 0.00000696
Iteration 62/1000 | Loss: 0.00000696
Iteration 63/1000 | Loss: 0.00000696
Iteration 64/1000 | Loss: 0.00000695
Iteration 65/1000 | Loss: 0.00000693
Iteration 66/1000 | Loss: 0.00000693
Iteration 67/1000 | Loss: 0.00000692
Iteration 68/1000 | Loss: 0.00000692
Iteration 69/1000 | Loss: 0.00000692
Iteration 70/1000 | Loss: 0.00000692
Iteration 71/1000 | Loss: 0.00000692
Iteration 72/1000 | Loss: 0.00000692
Iteration 73/1000 | Loss: 0.00000692
Iteration 74/1000 | Loss: 0.00000692
Iteration 75/1000 | Loss: 0.00000692
Iteration 76/1000 | Loss: 0.00000691
Iteration 77/1000 | Loss: 0.00000691
Iteration 78/1000 | Loss: 0.00000691
Iteration 79/1000 | Loss: 0.00000691
Iteration 80/1000 | Loss: 0.00000690
Iteration 81/1000 | Loss: 0.00000689
Iteration 82/1000 | Loss: 0.00000689
Iteration 83/1000 | Loss: 0.00000689
Iteration 84/1000 | Loss: 0.00000688
Iteration 85/1000 | Loss: 0.00000688
Iteration 86/1000 | Loss: 0.00000688
Iteration 87/1000 | Loss: 0.00000688
Iteration 88/1000 | Loss: 0.00000688
Iteration 89/1000 | Loss: 0.00000688
Iteration 90/1000 | Loss: 0.00000688
Iteration 91/1000 | Loss: 0.00000688
Iteration 92/1000 | Loss: 0.00000688
Iteration 93/1000 | Loss: 0.00000687
Iteration 94/1000 | Loss: 0.00000687
Iteration 95/1000 | Loss: 0.00000687
Iteration 96/1000 | Loss: 0.00000687
Iteration 97/1000 | Loss: 0.00000687
Iteration 98/1000 | Loss: 0.00000687
Iteration 99/1000 | Loss: 0.00000687
Iteration 100/1000 | Loss: 0.00000687
Iteration 101/1000 | Loss: 0.00000687
Iteration 102/1000 | Loss: 0.00000687
Iteration 103/1000 | Loss: 0.00000686
Iteration 104/1000 | Loss: 0.00000686
Iteration 105/1000 | Loss: 0.00000686
Iteration 106/1000 | Loss: 0.00000686
Iteration 107/1000 | Loss: 0.00000686
Iteration 108/1000 | Loss: 0.00000686
Iteration 109/1000 | Loss: 0.00000686
Iteration 110/1000 | Loss: 0.00000686
Iteration 111/1000 | Loss: 0.00000686
Iteration 112/1000 | Loss: 0.00000686
Iteration 113/1000 | Loss: 0.00000686
Iteration 114/1000 | Loss: 0.00000686
Iteration 115/1000 | Loss: 0.00000686
Iteration 116/1000 | Loss: 0.00000686
Iteration 117/1000 | Loss: 0.00000686
Iteration 118/1000 | Loss: 0.00000686
Iteration 119/1000 | Loss: 0.00000686
Iteration 120/1000 | Loss: 0.00000686
Iteration 121/1000 | Loss: 0.00000686
Iteration 122/1000 | Loss: 0.00000686
Iteration 123/1000 | Loss: 0.00000685
Iteration 124/1000 | Loss: 0.00000685
Iteration 125/1000 | Loss: 0.00000685
Iteration 126/1000 | Loss: 0.00000685
Iteration 127/1000 | Loss: 0.00000685
Iteration 128/1000 | Loss: 0.00000685
Iteration 129/1000 | Loss: 0.00000685
Iteration 130/1000 | Loss: 0.00000685
Iteration 131/1000 | Loss: 0.00000685
Iteration 132/1000 | Loss: 0.00000685
Iteration 133/1000 | Loss: 0.00000685
Iteration 134/1000 | Loss: 0.00000685
Iteration 135/1000 | Loss: 0.00000685
Iteration 136/1000 | Loss: 0.00000685
Iteration 137/1000 | Loss: 0.00000685
Iteration 138/1000 | Loss: 0.00000685
Iteration 139/1000 | Loss: 0.00000684
Iteration 140/1000 | Loss: 0.00000684
Iteration 141/1000 | Loss: 0.00000684
Iteration 142/1000 | Loss: 0.00000684
Iteration 143/1000 | Loss: 0.00000684
Iteration 144/1000 | Loss: 0.00000684
Iteration 145/1000 | Loss: 0.00000684
Iteration 146/1000 | Loss: 0.00000684
Iteration 147/1000 | Loss: 0.00000684
Iteration 148/1000 | Loss: 0.00000684
Iteration 149/1000 | Loss: 0.00000684
Iteration 150/1000 | Loss: 0.00000684
Iteration 151/1000 | Loss: 0.00000684
Iteration 152/1000 | Loss: 0.00000684
Iteration 153/1000 | Loss: 0.00000684
Iteration 154/1000 | Loss: 0.00000684
Iteration 155/1000 | Loss: 0.00000684
Iteration 156/1000 | Loss: 0.00000684
Iteration 157/1000 | Loss: 0.00000684
Iteration 158/1000 | Loss: 0.00000684
Iteration 159/1000 | Loss: 0.00000684
Iteration 160/1000 | Loss: 0.00000684
Iteration 161/1000 | Loss: 0.00000684
Iteration 162/1000 | Loss: 0.00000683
Iteration 163/1000 | Loss: 0.00000683
Iteration 164/1000 | Loss: 0.00000683
Iteration 165/1000 | Loss: 0.00000683
Iteration 166/1000 | Loss: 0.00000683
Iteration 167/1000 | Loss: 0.00000683
Iteration 168/1000 | Loss: 0.00000683
Iteration 169/1000 | Loss: 0.00000683
Iteration 170/1000 | Loss: 0.00000683
Iteration 171/1000 | Loss: 0.00000683
Iteration 172/1000 | Loss: 0.00000683
Iteration 173/1000 | Loss: 0.00000683
Iteration 174/1000 | Loss: 0.00000683
Iteration 175/1000 | Loss: 0.00000683
Iteration 176/1000 | Loss: 0.00000683
Iteration 177/1000 | Loss: 0.00000683
Iteration 178/1000 | Loss: 0.00000683
Iteration 179/1000 | Loss: 0.00000683
Iteration 180/1000 | Loss: 0.00000683
Iteration 181/1000 | Loss: 0.00000683
Iteration 182/1000 | Loss: 0.00000683
Iteration 183/1000 | Loss: 0.00000683
Iteration 184/1000 | Loss: 0.00000683
Iteration 185/1000 | Loss: 0.00000683
Iteration 186/1000 | Loss: 0.00000683
Iteration 187/1000 | Loss: 0.00000683
Iteration 188/1000 | Loss: 0.00000683
Iteration 189/1000 | Loss: 0.00000683
Iteration 190/1000 | Loss: 0.00000683
Iteration 191/1000 | Loss: 0.00000683
Iteration 192/1000 | Loss: 0.00000683
Iteration 193/1000 | Loss: 0.00000683
Iteration 194/1000 | Loss: 0.00000683
Iteration 195/1000 | Loss: 0.00000683
Iteration 196/1000 | Loss: 0.00000683
Iteration 197/1000 | Loss: 0.00000683
Iteration 198/1000 | Loss: 0.00000683
Iteration 199/1000 | Loss: 0.00000683
Iteration 200/1000 | Loss: 0.00000683
Iteration 201/1000 | Loss: 0.00000683
Iteration 202/1000 | Loss: 0.00000683
Iteration 203/1000 | Loss: 0.00000683
Iteration 204/1000 | Loss: 0.00000683
Iteration 205/1000 | Loss: 0.00000683
Iteration 206/1000 | Loss: 0.00000683
Iteration 207/1000 | Loss: 0.00000683
Iteration 208/1000 | Loss: 0.00000683
Iteration 209/1000 | Loss: 0.00000683
Iteration 210/1000 | Loss: 0.00000683
Iteration 211/1000 | Loss: 0.00000683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [6.826616754551651e-06, 6.826616754551651e-06, 6.826616754551651e-06, 6.826616754551651e-06, 6.826616754551651e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.826616754551651e-06

Optimization complete. Final v2v error: 2.2638375759124756 mm

Highest mean error: 2.473196029663086 mm for frame 30

Lowest mean error: 2.149094343185425 mm for frame 105

Saving results

Total time: 33.986342668533325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816826
Iteration 2/25 | Loss: 0.00106397
Iteration 3/25 | Loss: 0.00096658
Iteration 4/25 | Loss: 0.00096037
Iteration 5/25 | Loss: 0.00095863
Iteration 6/25 | Loss: 0.00095863
Iteration 7/25 | Loss: 0.00095863
Iteration 8/25 | Loss: 0.00095863
Iteration 9/25 | Loss: 0.00095863
Iteration 10/25 | Loss: 0.00095863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009586284286342561, 0.0009586284286342561, 0.0009586284286342561, 0.0009586284286342561, 0.0009586284286342561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009586284286342561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38464153
Iteration 2/25 | Loss: 0.00069024
Iteration 3/25 | Loss: 0.00069023
Iteration 4/25 | Loss: 0.00069023
Iteration 5/25 | Loss: 0.00069023
Iteration 6/25 | Loss: 0.00069023
Iteration 7/25 | Loss: 0.00069023
Iteration 8/25 | Loss: 0.00069023
Iteration 9/25 | Loss: 0.00069023
Iteration 10/25 | Loss: 0.00069023
Iteration 11/25 | Loss: 0.00069023
Iteration 12/25 | Loss: 0.00069023
Iteration 13/25 | Loss: 0.00069023
Iteration 14/25 | Loss: 0.00069023
Iteration 15/25 | Loss: 0.00069023
Iteration 16/25 | Loss: 0.00069023
Iteration 17/25 | Loss: 0.00069023
Iteration 18/25 | Loss: 0.00069023
Iteration 19/25 | Loss: 0.00069023
Iteration 20/25 | Loss: 0.00069023
Iteration 21/25 | Loss: 0.00069023
Iteration 22/25 | Loss: 0.00069023
Iteration 23/25 | Loss: 0.00069023
Iteration 24/25 | Loss: 0.00069023
Iteration 25/25 | Loss: 0.00069023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069023
Iteration 2/1000 | Loss: 0.00001877
Iteration 3/1000 | Loss: 0.00001124
Iteration 4/1000 | Loss: 0.00001004
Iteration 5/1000 | Loss: 0.00000937
Iteration 6/1000 | Loss: 0.00000892
Iteration 7/1000 | Loss: 0.00000857
Iteration 8/1000 | Loss: 0.00000837
Iteration 9/1000 | Loss: 0.00000836
Iteration 10/1000 | Loss: 0.00000816
Iteration 11/1000 | Loss: 0.00000815
Iteration 12/1000 | Loss: 0.00000812
Iteration 13/1000 | Loss: 0.00000808
Iteration 14/1000 | Loss: 0.00000807
Iteration 15/1000 | Loss: 0.00000806
Iteration 16/1000 | Loss: 0.00000803
Iteration 17/1000 | Loss: 0.00000803
Iteration 18/1000 | Loss: 0.00000803
Iteration 19/1000 | Loss: 0.00000803
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000802
Iteration 23/1000 | Loss: 0.00000799
Iteration 24/1000 | Loss: 0.00000798
Iteration 25/1000 | Loss: 0.00000798
Iteration 26/1000 | Loss: 0.00000798
Iteration 27/1000 | Loss: 0.00000797
Iteration 28/1000 | Loss: 0.00000797
Iteration 29/1000 | Loss: 0.00000797
Iteration 30/1000 | Loss: 0.00000797
Iteration 31/1000 | Loss: 0.00000797
Iteration 32/1000 | Loss: 0.00000797
Iteration 33/1000 | Loss: 0.00000797
Iteration 34/1000 | Loss: 0.00000797
Iteration 35/1000 | Loss: 0.00000797
Iteration 36/1000 | Loss: 0.00000797
Iteration 37/1000 | Loss: 0.00000796
Iteration 38/1000 | Loss: 0.00000796
Iteration 39/1000 | Loss: 0.00000795
Iteration 40/1000 | Loss: 0.00000793
Iteration 41/1000 | Loss: 0.00000793
Iteration 42/1000 | Loss: 0.00000793
Iteration 43/1000 | Loss: 0.00000793
Iteration 44/1000 | Loss: 0.00000793
Iteration 45/1000 | Loss: 0.00000793
Iteration 46/1000 | Loss: 0.00000793
Iteration 47/1000 | Loss: 0.00000793
Iteration 48/1000 | Loss: 0.00000793
Iteration 49/1000 | Loss: 0.00000793
Iteration 50/1000 | Loss: 0.00000792
Iteration 51/1000 | Loss: 0.00000792
Iteration 52/1000 | Loss: 0.00000792
Iteration 53/1000 | Loss: 0.00000792
Iteration 54/1000 | Loss: 0.00000791
Iteration 55/1000 | Loss: 0.00000790
Iteration 56/1000 | Loss: 0.00000790
Iteration 57/1000 | Loss: 0.00000789
Iteration 58/1000 | Loss: 0.00000789
Iteration 59/1000 | Loss: 0.00000788
Iteration 60/1000 | Loss: 0.00000788
Iteration 61/1000 | Loss: 0.00000787
Iteration 62/1000 | Loss: 0.00000787
Iteration 63/1000 | Loss: 0.00000786
Iteration 64/1000 | Loss: 0.00000786
Iteration 65/1000 | Loss: 0.00000785
Iteration 66/1000 | Loss: 0.00000785
Iteration 67/1000 | Loss: 0.00000785
Iteration 68/1000 | Loss: 0.00000785
Iteration 69/1000 | Loss: 0.00000785
Iteration 70/1000 | Loss: 0.00000784
Iteration 71/1000 | Loss: 0.00000784
Iteration 72/1000 | Loss: 0.00000784
Iteration 73/1000 | Loss: 0.00000784
Iteration 74/1000 | Loss: 0.00000784
Iteration 75/1000 | Loss: 0.00000784
Iteration 76/1000 | Loss: 0.00000784
Iteration 77/1000 | Loss: 0.00000783
Iteration 78/1000 | Loss: 0.00000783
Iteration 79/1000 | Loss: 0.00000783
Iteration 80/1000 | Loss: 0.00000783
Iteration 81/1000 | Loss: 0.00000783
Iteration 82/1000 | Loss: 0.00000782
Iteration 83/1000 | Loss: 0.00000782
Iteration 84/1000 | Loss: 0.00000782
Iteration 85/1000 | Loss: 0.00000782
Iteration 86/1000 | Loss: 0.00000782
Iteration 87/1000 | Loss: 0.00000782
Iteration 88/1000 | Loss: 0.00000782
Iteration 89/1000 | Loss: 0.00000782
Iteration 90/1000 | Loss: 0.00000782
Iteration 91/1000 | Loss: 0.00000782
Iteration 92/1000 | Loss: 0.00000781
Iteration 93/1000 | Loss: 0.00000781
Iteration 94/1000 | Loss: 0.00000781
Iteration 95/1000 | Loss: 0.00000781
Iteration 96/1000 | Loss: 0.00000780
Iteration 97/1000 | Loss: 0.00000780
Iteration 98/1000 | Loss: 0.00000780
Iteration 99/1000 | Loss: 0.00000780
Iteration 100/1000 | Loss: 0.00000779
Iteration 101/1000 | Loss: 0.00000779
Iteration 102/1000 | Loss: 0.00000779
Iteration 103/1000 | Loss: 0.00000779
Iteration 104/1000 | Loss: 0.00000779
Iteration 105/1000 | Loss: 0.00000779
Iteration 106/1000 | Loss: 0.00000778
Iteration 107/1000 | Loss: 0.00000778
Iteration 108/1000 | Loss: 0.00000778
Iteration 109/1000 | Loss: 0.00000778
Iteration 110/1000 | Loss: 0.00000778
Iteration 111/1000 | Loss: 0.00000778
Iteration 112/1000 | Loss: 0.00000778
Iteration 113/1000 | Loss: 0.00000778
Iteration 114/1000 | Loss: 0.00000777
Iteration 115/1000 | Loss: 0.00000777
Iteration 116/1000 | Loss: 0.00000777
Iteration 117/1000 | Loss: 0.00000777
Iteration 118/1000 | Loss: 0.00000777
Iteration 119/1000 | Loss: 0.00000776
Iteration 120/1000 | Loss: 0.00000775
Iteration 121/1000 | Loss: 0.00000775
Iteration 122/1000 | Loss: 0.00000774
Iteration 123/1000 | Loss: 0.00000774
Iteration 124/1000 | Loss: 0.00000774
Iteration 125/1000 | Loss: 0.00000774
Iteration 126/1000 | Loss: 0.00000774
Iteration 127/1000 | Loss: 0.00000774
Iteration 128/1000 | Loss: 0.00000774
Iteration 129/1000 | Loss: 0.00000773
Iteration 130/1000 | Loss: 0.00000773
Iteration 131/1000 | Loss: 0.00000773
Iteration 132/1000 | Loss: 0.00000773
Iteration 133/1000 | Loss: 0.00000773
Iteration 134/1000 | Loss: 0.00000773
Iteration 135/1000 | Loss: 0.00000772
Iteration 136/1000 | Loss: 0.00000772
Iteration 137/1000 | Loss: 0.00000772
Iteration 138/1000 | Loss: 0.00000772
Iteration 139/1000 | Loss: 0.00000772
Iteration 140/1000 | Loss: 0.00000771
Iteration 141/1000 | Loss: 0.00000771
Iteration 142/1000 | Loss: 0.00000771
Iteration 143/1000 | Loss: 0.00000771
Iteration 144/1000 | Loss: 0.00000771
Iteration 145/1000 | Loss: 0.00000771
Iteration 146/1000 | Loss: 0.00000770
Iteration 147/1000 | Loss: 0.00000770
Iteration 148/1000 | Loss: 0.00000770
Iteration 149/1000 | Loss: 0.00000770
Iteration 150/1000 | Loss: 0.00000770
Iteration 151/1000 | Loss: 0.00000770
Iteration 152/1000 | Loss: 0.00000770
Iteration 153/1000 | Loss: 0.00000770
Iteration 154/1000 | Loss: 0.00000769
Iteration 155/1000 | Loss: 0.00000769
Iteration 156/1000 | Loss: 0.00000769
Iteration 157/1000 | Loss: 0.00000769
Iteration 158/1000 | Loss: 0.00000769
Iteration 159/1000 | Loss: 0.00000769
Iteration 160/1000 | Loss: 0.00000768
Iteration 161/1000 | Loss: 0.00000768
Iteration 162/1000 | Loss: 0.00000768
Iteration 163/1000 | Loss: 0.00000768
Iteration 164/1000 | Loss: 0.00000767
Iteration 165/1000 | Loss: 0.00000767
Iteration 166/1000 | Loss: 0.00000767
Iteration 167/1000 | Loss: 0.00000767
Iteration 168/1000 | Loss: 0.00000767
Iteration 169/1000 | Loss: 0.00000767
Iteration 170/1000 | Loss: 0.00000767
Iteration 171/1000 | Loss: 0.00000767
Iteration 172/1000 | Loss: 0.00000766
Iteration 173/1000 | Loss: 0.00000766
Iteration 174/1000 | Loss: 0.00000766
Iteration 175/1000 | Loss: 0.00000766
Iteration 176/1000 | Loss: 0.00000766
Iteration 177/1000 | Loss: 0.00000766
Iteration 178/1000 | Loss: 0.00000765
Iteration 179/1000 | Loss: 0.00000765
Iteration 180/1000 | Loss: 0.00000765
Iteration 181/1000 | Loss: 0.00000765
Iteration 182/1000 | Loss: 0.00000765
Iteration 183/1000 | Loss: 0.00000765
Iteration 184/1000 | Loss: 0.00000765
Iteration 185/1000 | Loss: 0.00000764
Iteration 186/1000 | Loss: 0.00000764
Iteration 187/1000 | Loss: 0.00000764
Iteration 188/1000 | Loss: 0.00000764
Iteration 189/1000 | Loss: 0.00000764
Iteration 190/1000 | Loss: 0.00000763
Iteration 191/1000 | Loss: 0.00000763
Iteration 192/1000 | Loss: 0.00000763
Iteration 193/1000 | Loss: 0.00000763
Iteration 194/1000 | Loss: 0.00000763
Iteration 195/1000 | Loss: 0.00000763
Iteration 196/1000 | Loss: 0.00000763
Iteration 197/1000 | Loss: 0.00000763
Iteration 198/1000 | Loss: 0.00000762
Iteration 199/1000 | Loss: 0.00000762
Iteration 200/1000 | Loss: 0.00000762
Iteration 201/1000 | Loss: 0.00000762
Iteration 202/1000 | Loss: 0.00000762
Iteration 203/1000 | Loss: 0.00000762
Iteration 204/1000 | Loss: 0.00000762
Iteration 205/1000 | Loss: 0.00000762
Iteration 206/1000 | Loss: 0.00000762
Iteration 207/1000 | Loss: 0.00000762
Iteration 208/1000 | Loss: 0.00000762
Iteration 209/1000 | Loss: 0.00000762
Iteration 210/1000 | Loss: 0.00000761
Iteration 211/1000 | Loss: 0.00000761
Iteration 212/1000 | Loss: 0.00000761
Iteration 213/1000 | Loss: 0.00000761
Iteration 214/1000 | Loss: 0.00000761
Iteration 215/1000 | Loss: 0.00000761
Iteration 216/1000 | Loss: 0.00000761
Iteration 217/1000 | Loss: 0.00000761
Iteration 218/1000 | Loss: 0.00000761
Iteration 219/1000 | Loss: 0.00000760
Iteration 220/1000 | Loss: 0.00000760
Iteration 221/1000 | Loss: 0.00000760
Iteration 222/1000 | Loss: 0.00000760
Iteration 223/1000 | Loss: 0.00000760
Iteration 224/1000 | Loss: 0.00000760
Iteration 225/1000 | Loss: 0.00000760
Iteration 226/1000 | Loss: 0.00000760
Iteration 227/1000 | Loss: 0.00000760
Iteration 228/1000 | Loss: 0.00000760
Iteration 229/1000 | Loss: 0.00000760
Iteration 230/1000 | Loss: 0.00000760
Iteration 231/1000 | Loss: 0.00000759
Iteration 232/1000 | Loss: 0.00000759
Iteration 233/1000 | Loss: 0.00000759
Iteration 234/1000 | Loss: 0.00000759
Iteration 235/1000 | Loss: 0.00000759
Iteration 236/1000 | Loss: 0.00000759
Iteration 237/1000 | Loss: 0.00000759
Iteration 238/1000 | Loss: 0.00000759
Iteration 239/1000 | Loss: 0.00000759
Iteration 240/1000 | Loss: 0.00000759
Iteration 241/1000 | Loss: 0.00000759
Iteration 242/1000 | Loss: 0.00000759
Iteration 243/1000 | Loss: 0.00000759
Iteration 244/1000 | Loss: 0.00000759
Iteration 245/1000 | Loss: 0.00000759
Iteration 246/1000 | Loss: 0.00000758
Iteration 247/1000 | Loss: 0.00000758
Iteration 248/1000 | Loss: 0.00000758
Iteration 249/1000 | Loss: 0.00000758
Iteration 250/1000 | Loss: 0.00000758
Iteration 251/1000 | Loss: 0.00000758
Iteration 252/1000 | Loss: 0.00000758
Iteration 253/1000 | Loss: 0.00000758
Iteration 254/1000 | Loss: 0.00000758
Iteration 255/1000 | Loss: 0.00000758
Iteration 256/1000 | Loss: 0.00000758
Iteration 257/1000 | Loss: 0.00000758
Iteration 258/1000 | Loss: 0.00000758
Iteration 259/1000 | Loss: 0.00000758
Iteration 260/1000 | Loss: 0.00000757
Iteration 261/1000 | Loss: 0.00000757
Iteration 262/1000 | Loss: 0.00000757
Iteration 263/1000 | Loss: 0.00000757
Iteration 264/1000 | Loss: 0.00000757
Iteration 265/1000 | Loss: 0.00000757
Iteration 266/1000 | Loss: 0.00000757
Iteration 267/1000 | Loss: 0.00000757
Iteration 268/1000 | Loss: 0.00000757
Iteration 269/1000 | Loss: 0.00000757
Iteration 270/1000 | Loss: 0.00000757
Iteration 271/1000 | Loss: 0.00000757
Iteration 272/1000 | Loss: 0.00000757
Iteration 273/1000 | Loss: 0.00000757
Iteration 274/1000 | Loss: 0.00000757
Iteration 275/1000 | Loss: 0.00000757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [7.569644367322326e-06, 7.569644367322326e-06, 7.569644367322326e-06, 7.569644367322326e-06, 7.569644367322326e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.569644367322326e-06

Optimization complete. Final v2v error: 2.335871934890747 mm

Highest mean error: 2.5667030811309814 mm for frame 67

Lowest mean error: 2.189897298812866 mm for frame 185

Saving results

Total time: 41.427330493927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431393
Iteration 2/25 | Loss: 0.00109338
Iteration 3/25 | Loss: 0.00101478
Iteration 4/25 | Loss: 0.00100751
Iteration 5/25 | Loss: 0.00100481
Iteration 6/25 | Loss: 0.00100442
Iteration 7/25 | Loss: 0.00100442
Iteration 8/25 | Loss: 0.00100442
Iteration 9/25 | Loss: 0.00100442
Iteration 10/25 | Loss: 0.00100442
Iteration 11/25 | Loss: 0.00100442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001004422316327691, 0.001004422316327691, 0.001004422316327691, 0.001004422316327691, 0.001004422316327691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001004422316327691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01574564
Iteration 2/25 | Loss: 0.00049773
Iteration 3/25 | Loss: 0.00049773
Iteration 4/25 | Loss: 0.00049773
Iteration 5/25 | Loss: 0.00049773
Iteration 6/25 | Loss: 0.00049773
Iteration 7/25 | Loss: 0.00049773
Iteration 8/25 | Loss: 0.00049773
Iteration 9/25 | Loss: 0.00049773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.000497727538459003, 0.000497727538459003, 0.000497727538459003, 0.000497727538459003, 0.000497727538459003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000497727538459003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049773
Iteration 2/1000 | Loss: 0.00003605
Iteration 3/1000 | Loss: 0.00002116
Iteration 4/1000 | Loss: 0.00001645
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001453
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001225
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001220
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001220
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001218
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001209
Iteration 76/1000 | Loss: 0.00001209
Iteration 77/1000 | Loss: 0.00001209
Iteration 78/1000 | Loss: 0.00001208
Iteration 79/1000 | Loss: 0.00001208
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.2017740118608344e-05, 1.2017740118608344e-05, 1.2017740118608344e-05, 1.2017740118608344e-05, 1.2017740118608344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2017740118608344e-05

Optimization complete. Final v2v error: 2.9682974815368652 mm

Highest mean error: 2.994260549545288 mm for frame 57

Lowest mean error: 2.927476406097412 mm for frame 17

Saving results

Total time: 33.53146290779114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019782
Iteration 2/25 | Loss: 0.00210302
Iteration 3/25 | Loss: 0.00119256
Iteration 4/25 | Loss: 0.00106496
Iteration 5/25 | Loss: 0.00105277
Iteration 6/25 | Loss: 0.00105032
Iteration 7/25 | Loss: 0.00104978
Iteration 8/25 | Loss: 0.00104978
Iteration 9/25 | Loss: 0.00104978
Iteration 10/25 | Loss: 0.00104978
Iteration 11/25 | Loss: 0.00104978
Iteration 12/25 | Loss: 0.00104978
Iteration 13/25 | Loss: 0.00104978
Iteration 14/25 | Loss: 0.00104978
Iteration 15/25 | Loss: 0.00104978
Iteration 16/25 | Loss: 0.00104978
Iteration 17/25 | Loss: 0.00104978
Iteration 18/25 | Loss: 0.00104978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010497849434614182, 0.0010497849434614182, 0.0010497849434614182, 0.0010497849434614182, 0.0010497849434614182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010497849434614182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50024354
Iteration 2/25 | Loss: 0.00103141
Iteration 3/25 | Loss: 0.00103141
Iteration 4/25 | Loss: 0.00103140
Iteration 5/25 | Loss: 0.00103140
Iteration 6/25 | Loss: 0.00103140
Iteration 7/25 | Loss: 0.00103140
Iteration 8/25 | Loss: 0.00103140
Iteration 9/25 | Loss: 0.00103140
Iteration 10/25 | Loss: 0.00103140
Iteration 11/25 | Loss: 0.00103140
Iteration 12/25 | Loss: 0.00103140
Iteration 13/25 | Loss: 0.00103140
Iteration 14/25 | Loss: 0.00103140
Iteration 15/25 | Loss: 0.00103140
Iteration 16/25 | Loss: 0.00103140
Iteration 17/25 | Loss: 0.00103140
Iteration 18/25 | Loss: 0.00103140
Iteration 19/25 | Loss: 0.00103140
Iteration 20/25 | Loss: 0.00103140
Iteration 21/25 | Loss: 0.00103140
Iteration 22/25 | Loss: 0.00103140
Iteration 23/25 | Loss: 0.00103140
Iteration 24/25 | Loss: 0.00103140
Iteration 25/25 | Loss: 0.00103140

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103140
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002645
Iteration 4/1000 | Loss: 0.00002315
Iteration 5/1000 | Loss: 0.00019799
Iteration 6/1000 | Loss: 0.00027195
Iteration 7/1000 | Loss: 0.00023184
Iteration 8/1000 | Loss: 0.00011265
Iteration 9/1000 | Loss: 0.00002843
Iteration 10/1000 | Loss: 0.00002352
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00014517
Iteration 13/1000 | Loss: 0.00012193
Iteration 14/1000 | Loss: 0.00014896
Iteration 15/1000 | Loss: 0.00002972
Iteration 16/1000 | Loss: 0.00002288
Iteration 17/1000 | Loss: 0.00002043
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00014510
Iteration 23/1000 | Loss: 0.00002831
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00015709
Iteration 26/1000 | Loss: 0.00002392
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001736
Iteration 29/1000 | Loss: 0.00002564
Iteration 30/1000 | Loss: 0.00002436
Iteration 31/1000 | Loss: 0.00002534
Iteration 32/1000 | Loss: 0.00002236
Iteration 33/1000 | Loss: 0.00002667
Iteration 34/1000 | Loss: 0.00002082
Iteration 35/1000 | Loss: 0.00002589
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002551
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002441
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001504
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001419
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001416
Iteration 57/1000 | Loss: 0.00001416
Iteration 58/1000 | Loss: 0.00001415
Iteration 59/1000 | Loss: 0.00001415
Iteration 60/1000 | Loss: 0.00001415
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001410
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001409
Iteration 97/1000 | Loss: 0.00001409
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001408
Iteration 101/1000 | Loss: 0.00001408
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001407
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001406
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001406
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001404
Iteration 131/1000 | Loss: 0.00001404
Iteration 132/1000 | Loss: 0.00001404
Iteration 133/1000 | Loss: 0.00001404
Iteration 134/1000 | Loss: 0.00001403
Iteration 135/1000 | Loss: 0.00001403
Iteration 136/1000 | Loss: 0.00001403
Iteration 137/1000 | Loss: 0.00001403
Iteration 138/1000 | Loss: 0.00001403
Iteration 139/1000 | Loss: 0.00001403
Iteration 140/1000 | Loss: 0.00001403
Iteration 141/1000 | Loss: 0.00001403
Iteration 142/1000 | Loss: 0.00001403
Iteration 143/1000 | Loss: 0.00001403
Iteration 144/1000 | Loss: 0.00001403
Iteration 145/1000 | Loss: 0.00001403
Iteration 146/1000 | Loss: 0.00001403
Iteration 147/1000 | Loss: 0.00001403
Iteration 148/1000 | Loss: 0.00001403
Iteration 149/1000 | Loss: 0.00001403
Iteration 150/1000 | Loss: 0.00001403
Iteration 151/1000 | Loss: 0.00001403
Iteration 152/1000 | Loss: 0.00001403
Iteration 153/1000 | Loss: 0.00001403
Iteration 154/1000 | Loss: 0.00001403
Iteration 155/1000 | Loss: 0.00001403
Iteration 156/1000 | Loss: 0.00001403
Iteration 157/1000 | Loss: 0.00001403
Iteration 158/1000 | Loss: 0.00001403
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001403
Iteration 176/1000 | Loss: 0.00001403
Iteration 177/1000 | Loss: 0.00001403
Iteration 178/1000 | Loss: 0.00001403
Iteration 179/1000 | Loss: 0.00001403
Iteration 180/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.4029248632141389e-05, 1.4029248632141389e-05, 1.4029248632141389e-05, 1.4029248632141389e-05, 1.4029248632141389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4029248632141389e-05

Optimization complete. Final v2v error: 3.2031219005584717 mm

Highest mean error: 5.0595197677612305 mm for frame 185

Lowest mean error: 2.7210946083068848 mm for frame 23

Saving results

Total time: 96.68250823020935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905851
Iteration 2/25 | Loss: 0.00134212
Iteration 3/25 | Loss: 0.00116681
Iteration 4/25 | Loss: 0.00115190
Iteration 5/25 | Loss: 0.00115155
Iteration 6/25 | Loss: 0.00115155
Iteration 7/25 | Loss: 0.00115155
Iteration 8/25 | Loss: 0.00115155
Iteration 9/25 | Loss: 0.00115155
Iteration 10/25 | Loss: 0.00115155
Iteration 11/25 | Loss: 0.00115155
Iteration 12/25 | Loss: 0.00115155
Iteration 13/25 | Loss: 0.00115155
Iteration 14/25 | Loss: 0.00115155
Iteration 15/25 | Loss: 0.00115155
Iteration 16/25 | Loss: 0.00115155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011515453225001693, 0.0011515453225001693, 0.0011515453225001693, 0.0011515453225001693, 0.0011515453225001693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011515453225001693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27665043
Iteration 2/25 | Loss: 0.00049708
Iteration 3/25 | Loss: 0.00049701
Iteration 4/25 | Loss: 0.00049701
Iteration 5/25 | Loss: 0.00049700
Iteration 6/25 | Loss: 0.00049700
Iteration 7/25 | Loss: 0.00049700
Iteration 8/25 | Loss: 0.00049700
Iteration 9/25 | Loss: 0.00049700
Iteration 10/25 | Loss: 0.00049700
Iteration 11/25 | Loss: 0.00049700
Iteration 12/25 | Loss: 0.00049700
Iteration 13/25 | Loss: 0.00049700
Iteration 14/25 | Loss: 0.00049700
Iteration 15/25 | Loss: 0.00049700
Iteration 16/25 | Loss: 0.00049700
Iteration 17/25 | Loss: 0.00049700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000497003027703613, 0.000497003027703613, 0.000497003027703613, 0.000497003027703613, 0.000497003027703613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000497003027703613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049700
Iteration 2/1000 | Loss: 0.00003286
Iteration 3/1000 | Loss: 0.00002163
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001806
Iteration 9/1000 | Loss: 0.00001774
Iteration 10/1000 | Loss: 0.00001742
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001707
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001690
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001674
Iteration 22/1000 | Loss: 0.00001670
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001662
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001661
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001660
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001658
Iteration 45/1000 | Loss: 0.00001658
Iteration 46/1000 | Loss: 0.00001658
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001657
Iteration 49/1000 | Loss: 0.00001657
Iteration 50/1000 | Loss: 0.00001657
Iteration 51/1000 | Loss: 0.00001657
Iteration 52/1000 | Loss: 0.00001657
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001657
Iteration 58/1000 | Loss: 0.00001657
Iteration 59/1000 | Loss: 0.00001657
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001656
Iteration 68/1000 | Loss: 0.00001656
Iteration 69/1000 | Loss: 0.00001656
Iteration 70/1000 | Loss: 0.00001656
Iteration 71/1000 | Loss: 0.00001655
Iteration 72/1000 | Loss: 0.00001655
Iteration 73/1000 | Loss: 0.00001655
Iteration 74/1000 | Loss: 0.00001655
Iteration 75/1000 | Loss: 0.00001655
Iteration 76/1000 | Loss: 0.00001655
Iteration 77/1000 | Loss: 0.00001655
Iteration 78/1000 | Loss: 0.00001655
Iteration 79/1000 | Loss: 0.00001655
Iteration 80/1000 | Loss: 0.00001655
Iteration 81/1000 | Loss: 0.00001655
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Iteration 84/1000 | Loss: 0.00001654
Iteration 85/1000 | Loss: 0.00001654
Iteration 86/1000 | Loss: 0.00001654
Iteration 87/1000 | Loss: 0.00001654
Iteration 88/1000 | Loss: 0.00001654
Iteration 89/1000 | Loss: 0.00001654
Iteration 90/1000 | Loss: 0.00001654
Iteration 91/1000 | Loss: 0.00001654
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00001653
Iteration 94/1000 | Loss: 0.00001653
Iteration 95/1000 | Loss: 0.00001653
Iteration 96/1000 | Loss: 0.00001653
Iteration 97/1000 | Loss: 0.00001653
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001652
Iteration 100/1000 | Loss: 0.00001652
Iteration 101/1000 | Loss: 0.00001652
Iteration 102/1000 | Loss: 0.00001652
Iteration 103/1000 | Loss: 0.00001652
Iteration 104/1000 | Loss: 0.00001652
Iteration 105/1000 | Loss: 0.00001651
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001651
Iteration 108/1000 | Loss: 0.00001651
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001648
Iteration 115/1000 | Loss: 0.00001648
Iteration 116/1000 | Loss: 0.00001648
Iteration 117/1000 | Loss: 0.00001648
Iteration 118/1000 | Loss: 0.00001648
Iteration 119/1000 | Loss: 0.00001648
Iteration 120/1000 | Loss: 0.00001648
Iteration 121/1000 | Loss: 0.00001647
Iteration 122/1000 | Loss: 0.00001647
Iteration 123/1000 | Loss: 0.00001647
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001647
Iteration 126/1000 | Loss: 0.00001647
Iteration 127/1000 | Loss: 0.00001647
Iteration 128/1000 | Loss: 0.00001647
Iteration 129/1000 | Loss: 0.00001647
Iteration 130/1000 | Loss: 0.00001647
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001647
Iteration 134/1000 | Loss: 0.00001647
Iteration 135/1000 | Loss: 0.00001647
Iteration 136/1000 | Loss: 0.00001647
Iteration 137/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6471853086841293e-05, 1.6471853086841293e-05, 1.6471853086841293e-05, 1.6471853086841293e-05, 1.6471853086841293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6471853086841293e-05

Optimization complete. Final v2v error: 3.3871233463287354 mm

Highest mean error: 3.532700777053833 mm for frame 3

Lowest mean error: 2.9750423431396484 mm for frame 238

Saving results

Total time: 40.18115830421448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936594
Iteration 2/25 | Loss: 0.00143031
Iteration 3/25 | Loss: 0.00128947
Iteration 4/25 | Loss: 0.00110044
Iteration 5/25 | Loss: 0.00107423
Iteration 6/25 | Loss: 0.00106902
Iteration 7/25 | Loss: 0.00105981
Iteration 8/25 | Loss: 0.00105510
Iteration 9/25 | Loss: 0.00105117
Iteration 10/25 | Loss: 0.00104830
Iteration 11/25 | Loss: 0.00104389
Iteration 12/25 | Loss: 0.00104310
Iteration 13/25 | Loss: 0.00104258
Iteration 14/25 | Loss: 0.00104248
Iteration 15/25 | Loss: 0.00104248
Iteration 16/25 | Loss: 0.00104248
Iteration 17/25 | Loss: 0.00104248
Iteration 18/25 | Loss: 0.00104247
Iteration 19/25 | Loss: 0.00104247
Iteration 20/25 | Loss: 0.00104247
Iteration 21/25 | Loss: 0.00104247
Iteration 22/25 | Loss: 0.00104247
Iteration 23/25 | Loss: 0.00104247
Iteration 24/25 | Loss: 0.00104247
Iteration 25/25 | Loss: 0.00104247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50024426
Iteration 2/25 | Loss: 0.00078342
Iteration 3/25 | Loss: 0.00073122
Iteration 4/25 | Loss: 0.00073122
Iteration 5/25 | Loss: 0.00073122
Iteration 6/25 | Loss: 0.00073122
Iteration 7/25 | Loss: 0.00073122
Iteration 8/25 | Loss: 0.00073122
Iteration 9/25 | Loss: 0.00073122
Iteration 10/25 | Loss: 0.00073122
Iteration 11/25 | Loss: 0.00073122
Iteration 12/25 | Loss: 0.00073122
Iteration 13/25 | Loss: 0.00073122
Iteration 14/25 | Loss: 0.00073122
Iteration 15/25 | Loss: 0.00073122
Iteration 16/25 | Loss: 0.00073122
Iteration 17/25 | Loss: 0.00073122
Iteration 18/25 | Loss: 0.00073122
Iteration 19/25 | Loss: 0.00073122
Iteration 20/25 | Loss: 0.00073122
Iteration 21/25 | Loss: 0.00073122
Iteration 22/25 | Loss: 0.00073122
Iteration 23/25 | Loss: 0.00073122
Iteration 24/25 | Loss: 0.00073122
Iteration 25/25 | Loss: 0.00073122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073122
Iteration 2/1000 | Loss: 0.00003969
Iteration 3/1000 | Loss: 0.00015241
Iteration 4/1000 | Loss: 0.00002048
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001794
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00002867
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00002558
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001689
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00045046
Iteration 15/1000 | Loss: 0.00003598
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00002230
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00004222
Iteration 21/1000 | Loss: 0.00002468
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00002773
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001413
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001407
Iteration 31/1000 | Loss: 0.00002441
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001392
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001391
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001391
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001384
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001381
Iteration 81/1000 | Loss: 0.00001381
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001375
Iteration 112/1000 | Loss: 0.00001375
Iteration 113/1000 | Loss: 0.00001375
Iteration 114/1000 | Loss: 0.00001375
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00002968
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001366
Iteration 139/1000 | Loss: 0.00001366
Iteration 140/1000 | Loss: 0.00001366
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001366
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001365
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001365
Iteration 162/1000 | Loss: 0.00001365
Iteration 163/1000 | Loss: 0.00001365
Iteration 164/1000 | Loss: 0.00001365
Iteration 165/1000 | Loss: 0.00001365
Iteration 166/1000 | Loss: 0.00001365
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001365
Iteration 170/1000 | Loss: 0.00001365
Iteration 171/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3646633306052536e-05, 1.3646633306052536e-05, 1.3646633306052536e-05, 1.3646633306052536e-05, 1.3646633306052536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3646633306052536e-05

Optimization complete. Final v2v error: 3.1128664016723633 mm

Highest mean error: 4.186978340148926 mm for frame 178

Lowest mean error: 2.542856216430664 mm for frame 200

Saving results

Total time: 81.15355348587036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537123
Iteration 2/25 | Loss: 0.00122955
Iteration 3/25 | Loss: 0.00109290
Iteration 4/25 | Loss: 0.00108305
Iteration 5/25 | Loss: 0.00108088
Iteration 6/25 | Loss: 0.00108011
Iteration 7/25 | Loss: 0.00108011
Iteration 8/25 | Loss: 0.00108011
Iteration 9/25 | Loss: 0.00108011
Iteration 10/25 | Loss: 0.00108011
Iteration 11/25 | Loss: 0.00108011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010801111347973347, 0.0010801111347973347, 0.0010801111347973347, 0.0010801111347973347, 0.0010801111347973347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010801111347973347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77262801
Iteration 2/25 | Loss: 0.00074111
Iteration 3/25 | Loss: 0.00074110
Iteration 4/25 | Loss: 0.00074110
Iteration 5/25 | Loss: 0.00074110
Iteration 6/25 | Loss: 0.00074110
Iteration 7/25 | Loss: 0.00074110
Iteration 8/25 | Loss: 0.00074110
Iteration 9/25 | Loss: 0.00074110
Iteration 10/25 | Loss: 0.00074110
Iteration 11/25 | Loss: 0.00074110
Iteration 12/25 | Loss: 0.00074110
Iteration 13/25 | Loss: 0.00074110
Iteration 14/25 | Loss: 0.00074110
Iteration 15/25 | Loss: 0.00074110
Iteration 16/25 | Loss: 0.00074109
Iteration 17/25 | Loss: 0.00074109
Iteration 18/25 | Loss: 0.00074109
Iteration 19/25 | Loss: 0.00074109
Iteration 20/25 | Loss: 0.00074109
Iteration 21/25 | Loss: 0.00074109
Iteration 22/25 | Loss: 0.00074109
Iteration 23/25 | Loss: 0.00074109
Iteration 24/25 | Loss: 0.00074109
Iteration 25/25 | Loss: 0.00074109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074109
Iteration 2/1000 | Loss: 0.00004253
Iteration 3/1000 | Loss: 0.00002790
Iteration 4/1000 | Loss: 0.00002371
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002131
Iteration 7/1000 | Loss: 0.00002083
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001942
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001889
Iteration 14/1000 | Loss: 0.00001868
Iteration 15/1000 | Loss: 0.00001850
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001813
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001776
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001758
Iteration 26/1000 | Loss: 0.00001758
Iteration 27/1000 | Loss: 0.00001757
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001756
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001749
Iteration 36/1000 | Loss: 0.00001749
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001748
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001747
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001746
Iteration 57/1000 | Loss: 0.00001746
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001746
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001744
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001741
Iteration 79/1000 | Loss: 0.00001741
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001741
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001739
Iteration 104/1000 | Loss: 0.00001739
Iteration 105/1000 | Loss: 0.00001739
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001734
Iteration 145/1000 | Loss: 0.00001734
Iteration 146/1000 | Loss: 0.00001734
Iteration 147/1000 | Loss: 0.00001734
Iteration 148/1000 | Loss: 0.00001734
Iteration 149/1000 | Loss: 0.00001734
Iteration 150/1000 | Loss: 0.00001733
Iteration 151/1000 | Loss: 0.00001733
Iteration 152/1000 | Loss: 0.00001733
Iteration 153/1000 | Loss: 0.00001733
Iteration 154/1000 | Loss: 0.00001733
Iteration 155/1000 | Loss: 0.00001733
Iteration 156/1000 | Loss: 0.00001733
Iteration 157/1000 | Loss: 0.00001733
Iteration 158/1000 | Loss: 0.00001733
Iteration 159/1000 | Loss: 0.00001733
Iteration 160/1000 | Loss: 0.00001732
Iteration 161/1000 | Loss: 0.00001732
Iteration 162/1000 | Loss: 0.00001732
Iteration 163/1000 | Loss: 0.00001732
Iteration 164/1000 | Loss: 0.00001731
Iteration 165/1000 | Loss: 0.00001731
Iteration 166/1000 | Loss: 0.00001731
Iteration 167/1000 | Loss: 0.00001731
Iteration 168/1000 | Loss: 0.00001731
Iteration 169/1000 | Loss: 0.00001731
Iteration 170/1000 | Loss: 0.00001730
Iteration 171/1000 | Loss: 0.00001730
Iteration 172/1000 | Loss: 0.00001730
Iteration 173/1000 | Loss: 0.00001730
Iteration 174/1000 | Loss: 0.00001730
Iteration 175/1000 | Loss: 0.00001729
Iteration 176/1000 | Loss: 0.00001729
Iteration 177/1000 | Loss: 0.00001729
Iteration 178/1000 | Loss: 0.00001729
Iteration 179/1000 | Loss: 0.00001729
Iteration 180/1000 | Loss: 0.00001729
Iteration 181/1000 | Loss: 0.00001729
Iteration 182/1000 | Loss: 0.00001729
Iteration 183/1000 | Loss: 0.00001728
Iteration 184/1000 | Loss: 0.00001728
Iteration 185/1000 | Loss: 0.00001728
Iteration 186/1000 | Loss: 0.00001728
Iteration 187/1000 | Loss: 0.00001728
Iteration 188/1000 | Loss: 0.00001728
Iteration 189/1000 | Loss: 0.00001728
Iteration 190/1000 | Loss: 0.00001727
Iteration 191/1000 | Loss: 0.00001727
Iteration 192/1000 | Loss: 0.00001727
Iteration 193/1000 | Loss: 0.00001727
Iteration 194/1000 | Loss: 0.00001727
Iteration 195/1000 | Loss: 0.00001727
Iteration 196/1000 | Loss: 0.00001727
Iteration 197/1000 | Loss: 0.00001727
Iteration 198/1000 | Loss: 0.00001727
Iteration 199/1000 | Loss: 0.00001727
Iteration 200/1000 | Loss: 0.00001727
Iteration 201/1000 | Loss: 0.00001727
Iteration 202/1000 | Loss: 0.00001726
Iteration 203/1000 | Loss: 0.00001726
Iteration 204/1000 | Loss: 0.00001726
Iteration 205/1000 | Loss: 0.00001726
Iteration 206/1000 | Loss: 0.00001726
Iteration 207/1000 | Loss: 0.00001726
Iteration 208/1000 | Loss: 0.00001726
Iteration 209/1000 | Loss: 0.00001726
Iteration 210/1000 | Loss: 0.00001726
Iteration 211/1000 | Loss: 0.00001726
Iteration 212/1000 | Loss: 0.00001726
Iteration 213/1000 | Loss: 0.00001726
Iteration 214/1000 | Loss: 0.00001726
Iteration 215/1000 | Loss: 0.00001726
Iteration 216/1000 | Loss: 0.00001726
Iteration 217/1000 | Loss: 0.00001726
Iteration 218/1000 | Loss: 0.00001726
Iteration 219/1000 | Loss: 0.00001726
Iteration 220/1000 | Loss: 0.00001726
Iteration 221/1000 | Loss: 0.00001726
Iteration 222/1000 | Loss: 0.00001726
Iteration 223/1000 | Loss: 0.00001726
Iteration 224/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.7258262232644483e-05, 1.7258262232644483e-05, 1.7258262232644483e-05, 1.7258262232644483e-05, 1.7258262232644483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7258262232644483e-05

Optimization complete. Final v2v error: 3.4203429222106934 mm

Highest mean error: 3.828549861907959 mm for frame 11

Lowest mean error: 3.1566436290740967 mm for frame 46

Saving results

Total time: 54.05285096168518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482807
Iteration 2/25 | Loss: 0.00116372
Iteration 3/25 | Loss: 0.00106635
Iteration 4/25 | Loss: 0.00105043
Iteration 5/25 | Loss: 0.00104570
Iteration 6/25 | Loss: 0.00104481
Iteration 7/25 | Loss: 0.00104481
Iteration 8/25 | Loss: 0.00104481
Iteration 9/25 | Loss: 0.00104481
Iteration 10/25 | Loss: 0.00104481
Iteration 11/25 | Loss: 0.00104481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010448083048686385, 0.0010448083048686385, 0.0010448083048686385, 0.0010448083048686385, 0.0010448083048686385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010448083048686385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36606646
Iteration 2/25 | Loss: 0.00076529
Iteration 3/25 | Loss: 0.00076528
Iteration 4/25 | Loss: 0.00076528
Iteration 5/25 | Loss: 0.00076528
Iteration 6/25 | Loss: 0.00076528
Iteration 7/25 | Loss: 0.00076528
Iteration 8/25 | Loss: 0.00076528
Iteration 9/25 | Loss: 0.00076528
Iteration 10/25 | Loss: 0.00076528
Iteration 11/25 | Loss: 0.00076528
Iteration 12/25 | Loss: 0.00076528
Iteration 13/25 | Loss: 0.00076528
Iteration 14/25 | Loss: 0.00076528
Iteration 15/25 | Loss: 0.00076528
Iteration 16/25 | Loss: 0.00076528
Iteration 17/25 | Loss: 0.00076528
Iteration 18/25 | Loss: 0.00076528
Iteration 19/25 | Loss: 0.00076528
Iteration 20/25 | Loss: 0.00076528
Iteration 21/25 | Loss: 0.00076528
Iteration 22/25 | Loss: 0.00076528
Iteration 23/25 | Loss: 0.00076528
Iteration 24/25 | Loss: 0.00076528
Iteration 25/25 | Loss: 0.00076528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076528
Iteration 2/1000 | Loss: 0.00003521
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002342
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002162
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001967
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001963
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001960
Iteration 23/1000 | Loss: 0.00001960
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001959
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001956
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001954
Iteration 31/1000 | Loss: 0.00001953
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001952
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001952
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001951
Iteration 44/1000 | Loss: 0.00001951
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001946
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001945
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001942
Iteration 59/1000 | Loss: 0.00001942
Iteration 60/1000 | Loss: 0.00001942
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001940
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001939
Iteration 66/1000 | Loss: 0.00001939
Iteration 67/1000 | Loss: 0.00001939
Iteration 68/1000 | Loss: 0.00001939
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001938
Iteration 72/1000 | Loss: 0.00001938
Iteration 73/1000 | Loss: 0.00001938
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001935
Iteration 82/1000 | Loss: 0.00001935
Iteration 83/1000 | Loss: 0.00001935
Iteration 84/1000 | Loss: 0.00001935
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001935
Iteration 91/1000 | Loss: 0.00001935
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001934
Iteration 97/1000 | Loss: 0.00001934
Iteration 98/1000 | Loss: 0.00001934
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001933
Iteration 102/1000 | Loss: 0.00001933
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001932
Iteration 105/1000 | Loss: 0.00001932
Iteration 106/1000 | Loss: 0.00001932
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001931
Iteration 110/1000 | Loss: 0.00001931
Iteration 111/1000 | Loss: 0.00001931
Iteration 112/1000 | Loss: 0.00001931
Iteration 113/1000 | Loss: 0.00001930
Iteration 114/1000 | Loss: 0.00001930
Iteration 115/1000 | Loss: 0.00001930
Iteration 116/1000 | Loss: 0.00001930
Iteration 117/1000 | Loss: 0.00001929
Iteration 118/1000 | Loss: 0.00001929
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001929
Iteration 121/1000 | Loss: 0.00001929
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001926
Iteration 146/1000 | Loss: 0.00001926
Iteration 147/1000 | Loss: 0.00001926
Iteration 148/1000 | Loss: 0.00001926
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001926
Iteration 155/1000 | Loss: 0.00001926
Iteration 156/1000 | Loss: 0.00001926
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001925
Iteration 159/1000 | Loss: 0.00001925
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001924
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001924
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001923
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.9233428247389384e-05, 1.9233428247389384e-05, 1.9233428247389384e-05, 1.9233428247389384e-05, 1.9233428247389384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9233428247389384e-05

Optimization complete. Final v2v error: 3.3993074893951416 mm

Highest mean error: 4.460165023803711 mm for frame 79

Lowest mean error: 2.9285082817077637 mm for frame 116

Saving results

Total time: 41.467411518096924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400835
Iteration 2/25 | Loss: 0.00109409
Iteration 3/25 | Loss: 0.00099388
Iteration 4/25 | Loss: 0.00097575
Iteration 5/25 | Loss: 0.00096942
Iteration 6/25 | Loss: 0.00096743
Iteration 7/25 | Loss: 0.00096692
Iteration 8/25 | Loss: 0.00096688
Iteration 9/25 | Loss: 0.00096688
Iteration 10/25 | Loss: 0.00096688
Iteration 11/25 | Loss: 0.00096688
Iteration 12/25 | Loss: 0.00096688
Iteration 13/25 | Loss: 0.00096688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000966880761552602, 0.000966880761552602, 0.000966880761552602, 0.000966880761552602, 0.000966880761552602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000966880761552602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38225186
Iteration 2/25 | Loss: 0.00076058
Iteration 3/25 | Loss: 0.00076058
Iteration 4/25 | Loss: 0.00076058
Iteration 5/25 | Loss: 0.00076058
Iteration 6/25 | Loss: 0.00076058
Iteration 7/25 | Loss: 0.00076058
Iteration 8/25 | Loss: 0.00076058
Iteration 9/25 | Loss: 0.00076058
Iteration 10/25 | Loss: 0.00076058
Iteration 11/25 | Loss: 0.00076058
Iteration 12/25 | Loss: 0.00076058
Iteration 13/25 | Loss: 0.00076058
Iteration 14/25 | Loss: 0.00076058
Iteration 15/25 | Loss: 0.00076058
Iteration 16/25 | Loss: 0.00076058
Iteration 17/25 | Loss: 0.00076058
Iteration 18/25 | Loss: 0.00076058
Iteration 19/25 | Loss: 0.00076058
Iteration 20/25 | Loss: 0.00076058
Iteration 21/25 | Loss: 0.00076058
Iteration 22/25 | Loss: 0.00076058
Iteration 23/25 | Loss: 0.00076058
Iteration 24/25 | Loss: 0.00076058
Iteration 25/25 | Loss: 0.00076058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076058
Iteration 2/1000 | Loss: 0.00002964
Iteration 3/1000 | Loss: 0.00001766
Iteration 4/1000 | Loss: 0.00001258
Iteration 5/1000 | Loss: 0.00001108
Iteration 6/1000 | Loss: 0.00001025
Iteration 7/1000 | Loss: 0.00000978
Iteration 8/1000 | Loss: 0.00000939
Iteration 9/1000 | Loss: 0.00000911
Iteration 10/1000 | Loss: 0.00000899
Iteration 11/1000 | Loss: 0.00000885
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000870
Iteration 14/1000 | Loss: 0.00000868
Iteration 15/1000 | Loss: 0.00000868
Iteration 16/1000 | Loss: 0.00000867
Iteration 17/1000 | Loss: 0.00000866
Iteration 18/1000 | Loss: 0.00000864
Iteration 19/1000 | Loss: 0.00000863
Iteration 20/1000 | Loss: 0.00000863
Iteration 21/1000 | Loss: 0.00000862
Iteration 22/1000 | Loss: 0.00000862
Iteration 23/1000 | Loss: 0.00000861
Iteration 24/1000 | Loss: 0.00000861
Iteration 25/1000 | Loss: 0.00000859
Iteration 26/1000 | Loss: 0.00000859
Iteration 27/1000 | Loss: 0.00000858
Iteration 28/1000 | Loss: 0.00000858
Iteration 29/1000 | Loss: 0.00000855
Iteration 30/1000 | Loss: 0.00000855
Iteration 31/1000 | Loss: 0.00000853
Iteration 32/1000 | Loss: 0.00000853
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000853
Iteration 35/1000 | Loss: 0.00000853
Iteration 36/1000 | Loss: 0.00000853
Iteration 37/1000 | Loss: 0.00000853
Iteration 38/1000 | Loss: 0.00000853
Iteration 39/1000 | Loss: 0.00000852
Iteration 40/1000 | Loss: 0.00000852
Iteration 41/1000 | Loss: 0.00000852
Iteration 42/1000 | Loss: 0.00000852
Iteration 43/1000 | Loss: 0.00000852
Iteration 44/1000 | Loss: 0.00000852
Iteration 45/1000 | Loss: 0.00000852
Iteration 46/1000 | Loss: 0.00000851
Iteration 47/1000 | Loss: 0.00000851
Iteration 48/1000 | Loss: 0.00000851
Iteration 49/1000 | Loss: 0.00000851
Iteration 50/1000 | Loss: 0.00000850
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000850
Iteration 53/1000 | Loss: 0.00000850
Iteration 54/1000 | Loss: 0.00000850
Iteration 55/1000 | Loss: 0.00000849
Iteration 56/1000 | Loss: 0.00000849
Iteration 57/1000 | Loss: 0.00000849
Iteration 58/1000 | Loss: 0.00000849
Iteration 59/1000 | Loss: 0.00000848
Iteration 60/1000 | Loss: 0.00000848
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000848
Iteration 63/1000 | Loss: 0.00000848
Iteration 64/1000 | Loss: 0.00000847
Iteration 65/1000 | Loss: 0.00000847
Iteration 66/1000 | Loss: 0.00000846
Iteration 67/1000 | Loss: 0.00000846
Iteration 68/1000 | Loss: 0.00000845
Iteration 69/1000 | Loss: 0.00000845
Iteration 70/1000 | Loss: 0.00000844
Iteration 71/1000 | Loss: 0.00000844
Iteration 72/1000 | Loss: 0.00000844
Iteration 73/1000 | Loss: 0.00000843
Iteration 74/1000 | Loss: 0.00000843
Iteration 75/1000 | Loss: 0.00000842
Iteration 76/1000 | Loss: 0.00000842
Iteration 77/1000 | Loss: 0.00000841
Iteration 78/1000 | Loss: 0.00000841
Iteration 79/1000 | Loss: 0.00000841
Iteration 80/1000 | Loss: 0.00000841
Iteration 81/1000 | Loss: 0.00000841
Iteration 82/1000 | Loss: 0.00000841
Iteration 83/1000 | Loss: 0.00000840
Iteration 84/1000 | Loss: 0.00000840
Iteration 85/1000 | Loss: 0.00000840
Iteration 86/1000 | Loss: 0.00000840
Iteration 87/1000 | Loss: 0.00000840
Iteration 88/1000 | Loss: 0.00000840
Iteration 89/1000 | Loss: 0.00000840
Iteration 90/1000 | Loss: 0.00000840
Iteration 91/1000 | Loss: 0.00000840
Iteration 92/1000 | Loss: 0.00000840
Iteration 93/1000 | Loss: 0.00000839
Iteration 94/1000 | Loss: 0.00000839
Iteration 95/1000 | Loss: 0.00000839
Iteration 96/1000 | Loss: 0.00000839
Iteration 97/1000 | Loss: 0.00000839
Iteration 98/1000 | Loss: 0.00000839
Iteration 99/1000 | Loss: 0.00000839
Iteration 100/1000 | Loss: 0.00000839
Iteration 101/1000 | Loss: 0.00000839
Iteration 102/1000 | Loss: 0.00000839
Iteration 103/1000 | Loss: 0.00000839
Iteration 104/1000 | Loss: 0.00000839
Iteration 105/1000 | Loss: 0.00000838
Iteration 106/1000 | Loss: 0.00000838
Iteration 107/1000 | Loss: 0.00000838
Iteration 108/1000 | Loss: 0.00000838
Iteration 109/1000 | Loss: 0.00000837
Iteration 110/1000 | Loss: 0.00000837
Iteration 111/1000 | Loss: 0.00000837
Iteration 112/1000 | Loss: 0.00000837
Iteration 113/1000 | Loss: 0.00000837
Iteration 114/1000 | Loss: 0.00000837
Iteration 115/1000 | Loss: 0.00000837
Iteration 116/1000 | Loss: 0.00000836
Iteration 117/1000 | Loss: 0.00000836
Iteration 118/1000 | Loss: 0.00000836
Iteration 119/1000 | Loss: 0.00000836
Iteration 120/1000 | Loss: 0.00000836
Iteration 121/1000 | Loss: 0.00000836
Iteration 122/1000 | Loss: 0.00000836
Iteration 123/1000 | Loss: 0.00000836
Iteration 124/1000 | Loss: 0.00000836
Iteration 125/1000 | Loss: 0.00000836
Iteration 126/1000 | Loss: 0.00000836
Iteration 127/1000 | Loss: 0.00000836
Iteration 128/1000 | Loss: 0.00000835
Iteration 129/1000 | Loss: 0.00000835
Iteration 130/1000 | Loss: 0.00000835
Iteration 131/1000 | Loss: 0.00000834
Iteration 132/1000 | Loss: 0.00000834
Iteration 133/1000 | Loss: 0.00000834
Iteration 134/1000 | Loss: 0.00000834
Iteration 135/1000 | Loss: 0.00000834
Iteration 136/1000 | Loss: 0.00000834
Iteration 137/1000 | Loss: 0.00000833
Iteration 138/1000 | Loss: 0.00000833
Iteration 139/1000 | Loss: 0.00000833
Iteration 140/1000 | Loss: 0.00000833
Iteration 141/1000 | Loss: 0.00000833
Iteration 142/1000 | Loss: 0.00000832
Iteration 143/1000 | Loss: 0.00000832
Iteration 144/1000 | Loss: 0.00000832
Iteration 145/1000 | Loss: 0.00000832
Iteration 146/1000 | Loss: 0.00000832
Iteration 147/1000 | Loss: 0.00000832
Iteration 148/1000 | Loss: 0.00000832
Iteration 149/1000 | Loss: 0.00000832
Iteration 150/1000 | Loss: 0.00000832
Iteration 151/1000 | Loss: 0.00000832
Iteration 152/1000 | Loss: 0.00000832
Iteration 153/1000 | Loss: 0.00000832
Iteration 154/1000 | Loss: 0.00000831
Iteration 155/1000 | Loss: 0.00000831
Iteration 156/1000 | Loss: 0.00000831
Iteration 157/1000 | Loss: 0.00000831
Iteration 158/1000 | Loss: 0.00000831
Iteration 159/1000 | Loss: 0.00000831
Iteration 160/1000 | Loss: 0.00000831
Iteration 161/1000 | Loss: 0.00000831
Iteration 162/1000 | Loss: 0.00000831
Iteration 163/1000 | Loss: 0.00000831
Iteration 164/1000 | Loss: 0.00000830
Iteration 165/1000 | Loss: 0.00000830
Iteration 166/1000 | Loss: 0.00000830
Iteration 167/1000 | Loss: 0.00000830
Iteration 168/1000 | Loss: 0.00000830
Iteration 169/1000 | Loss: 0.00000830
Iteration 170/1000 | Loss: 0.00000830
Iteration 171/1000 | Loss: 0.00000830
Iteration 172/1000 | Loss: 0.00000830
Iteration 173/1000 | Loss: 0.00000830
Iteration 174/1000 | Loss: 0.00000830
Iteration 175/1000 | Loss: 0.00000830
Iteration 176/1000 | Loss: 0.00000830
Iteration 177/1000 | Loss: 0.00000830
Iteration 178/1000 | Loss: 0.00000830
Iteration 179/1000 | Loss: 0.00000830
Iteration 180/1000 | Loss: 0.00000830
Iteration 181/1000 | Loss: 0.00000830
Iteration 182/1000 | Loss: 0.00000829
Iteration 183/1000 | Loss: 0.00000829
Iteration 184/1000 | Loss: 0.00000829
Iteration 185/1000 | Loss: 0.00000829
Iteration 186/1000 | Loss: 0.00000829
Iteration 187/1000 | Loss: 0.00000829
Iteration 188/1000 | Loss: 0.00000829
Iteration 189/1000 | Loss: 0.00000829
Iteration 190/1000 | Loss: 0.00000829
Iteration 191/1000 | Loss: 0.00000829
Iteration 192/1000 | Loss: 0.00000829
Iteration 193/1000 | Loss: 0.00000829
Iteration 194/1000 | Loss: 0.00000829
Iteration 195/1000 | Loss: 0.00000829
Iteration 196/1000 | Loss: 0.00000829
Iteration 197/1000 | Loss: 0.00000829
Iteration 198/1000 | Loss: 0.00000829
Iteration 199/1000 | Loss: 0.00000829
Iteration 200/1000 | Loss: 0.00000829
Iteration 201/1000 | Loss: 0.00000829
Iteration 202/1000 | Loss: 0.00000829
Iteration 203/1000 | Loss: 0.00000828
Iteration 204/1000 | Loss: 0.00000828
Iteration 205/1000 | Loss: 0.00000828
Iteration 206/1000 | Loss: 0.00000828
Iteration 207/1000 | Loss: 0.00000828
Iteration 208/1000 | Loss: 0.00000828
Iteration 209/1000 | Loss: 0.00000828
Iteration 210/1000 | Loss: 0.00000828
Iteration 211/1000 | Loss: 0.00000828
Iteration 212/1000 | Loss: 0.00000828
Iteration 213/1000 | Loss: 0.00000828
Iteration 214/1000 | Loss: 0.00000828
Iteration 215/1000 | Loss: 0.00000828
Iteration 216/1000 | Loss: 0.00000828
Iteration 217/1000 | Loss: 0.00000828
Iteration 218/1000 | Loss: 0.00000828
Iteration 219/1000 | Loss: 0.00000828
Iteration 220/1000 | Loss: 0.00000828
Iteration 221/1000 | Loss: 0.00000827
Iteration 222/1000 | Loss: 0.00000827
Iteration 223/1000 | Loss: 0.00000827
Iteration 224/1000 | Loss: 0.00000827
Iteration 225/1000 | Loss: 0.00000827
Iteration 226/1000 | Loss: 0.00000827
Iteration 227/1000 | Loss: 0.00000827
Iteration 228/1000 | Loss: 0.00000827
Iteration 229/1000 | Loss: 0.00000827
Iteration 230/1000 | Loss: 0.00000827
Iteration 231/1000 | Loss: 0.00000827
Iteration 232/1000 | Loss: 0.00000827
Iteration 233/1000 | Loss: 0.00000827
Iteration 234/1000 | Loss: 0.00000827
Iteration 235/1000 | Loss: 0.00000827
Iteration 236/1000 | Loss: 0.00000826
Iteration 237/1000 | Loss: 0.00000826
Iteration 238/1000 | Loss: 0.00000826
Iteration 239/1000 | Loss: 0.00000826
Iteration 240/1000 | Loss: 0.00000826
Iteration 241/1000 | Loss: 0.00000826
Iteration 242/1000 | Loss: 0.00000826
Iteration 243/1000 | Loss: 0.00000826
Iteration 244/1000 | Loss: 0.00000826
Iteration 245/1000 | Loss: 0.00000826
Iteration 246/1000 | Loss: 0.00000826
Iteration 247/1000 | Loss: 0.00000826
Iteration 248/1000 | Loss: 0.00000826
Iteration 249/1000 | Loss: 0.00000825
Iteration 250/1000 | Loss: 0.00000825
Iteration 251/1000 | Loss: 0.00000825
Iteration 252/1000 | Loss: 0.00000825
Iteration 253/1000 | Loss: 0.00000825
Iteration 254/1000 | Loss: 0.00000825
Iteration 255/1000 | Loss: 0.00000825
Iteration 256/1000 | Loss: 0.00000825
Iteration 257/1000 | Loss: 0.00000825
Iteration 258/1000 | Loss: 0.00000825
Iteration 259/1000 | Loss: 0.00000825
Iteration 260/1000 | Loss: 0.00000825
Iteration 261/1000 | Loss: 0.00000825
Iteration 262/1000 | Loss: 0.00000825
Iteration 263/1000 | Loss: 0.00000825
Iteration 264/1000 | Loss: 0.00000825
Iteration 265/1000 | Loss: 0.00000825
Iteration 266/1000 | Loss: 0.00000825
Iteration 267/1000 | Loss: 0.00000825
Iteration 268/1000 | Loss: 0.00000825
Iteration 269/1000 | Loss: 0.00000825
Iteration 270/1000 | Loss: 0.00000825
Iteration 271/1000 | Loss: 0.00000825
Iteration 272/1000 | Loss: 0.00000825
Iteration 273/1000 | Loss: 0.00000825
Iteration 274/1000 | Loss: 0.00000825
Iteration 275/1000 | Loss: 0.00000825
Iteration 276/1000 | Loss: 0.00000825
Iteration 277/1000 | Loss: 0.00000825
Iteration 278/1000 | Loss: 0.00000825
Iteration 279/1000 | Loss: 0.00000825
Iteration 280/1000 | Loss: 0.00000825
Iteration 281/1000 | Loss: 0.00000825
Iteration 282/1000 | Loss: 0.00000825
Iteration 283/1000 | Loss: 0.00000825
Iteration 284/1000 | Loss: 0.00000825
Iteration 285/1000 | Loss: 0.00000825
Iteration 286/1000 | Loss: 0.00000825
Iteration 287/1000 | Loss: 0.00000825
Iteration 288/1000 | Loss: 0.00000825
Iteration 289/1000 | Loss: 0.00000825
Iteration 290/1000 | Loss: 0.00000825
Iteration 291/1000 | Loss: 0.00000825
Iteration 292/1000 | Loss: 0.00000825
Iteration 293/1000 | Loss: 0.00000825
Iteration 294/1000 | Loss: 0.00000825
Iteration 295/1000 | Loss: 0.00000825
Iteration 296/1000 | Loss: 0.00000825
Iteration 297/1000 | Loss: 0.00000825
Iteration 298/1000 | Loss: 0.00000825
Iteration 299/1000 | Loss: 0.00000825
Iteration 300/1000 | Loss: 0.00000825
Iteration 301/1000 | Loss: 0.00000825
Iteration 302/1000 | Loss: 0.00000825
Iteration 303/1000 | Loss: 0.00000825
Iteration 304/1000 | Loss: 0.00000825
Iteration 305/1000 | Loss: 0.00000825
Iteration 306/1000 | Loss: 0.00000825
Iteration 307/1000 | Loss: 0.00000825
Iteration 308/1000 | Loss: 0.00000825
Iteration 309/1000 | Loss: 0.00000825
Iteration 310/1000 | Loss: 0.00000825
Iteration 311/1000 | Loss: 0.00000825
Iteration 312/1000 | Loss: 0.00000825
Iteration 313/1000 | Loss: 0.00000825
Iteration 314/1000 | Loss: 0.00000825
Iteration 315/1000 | Loss: 0.00000825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [8.249649908975698e-06, 8.249649908975698e-06, 8.249649908975698e-06, 8.249649908975698e-06, 8.249649908975698e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.249649908975698e-06

Optimization complete. Final v2v error: 2.4420504570007324 mm

Highest mean error: 3.1796891689300537 mm for frame 29

Lowest mean error: 2.175649642944336 mm for frame 90

Saving results

Total time: 46.205039501190186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418437
Iteration 2/25 | Loss: 0.00113340
Iteration 3/25 | Loss: 0.00101058
Iteration 4/25 | Loss: 0.00099520
Iteration 5/25 | Loss: 0.00099143
Iteration 6/25 | Loss: 0.00099027
Iteration 7/25 | Loss: 0.00099027
Iteration 8/25 | Loss: 0.00099027
Iteration 9/25 | Loss: 0.00099027
Iteration 10/25 | Loss: 0.00099027
Iteration 11/25 | Loss: 0.00099027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009902747115120292, 0.0009902747115120292, 0.0009902747115120292, 0.0009902747115120292, 0.0009902747115120292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009902747115120292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35830688
Iteration 2/25 | Loss: 0.00077725
Iteration 3/25 | Loss: 0.00077725
Iteration 4/25 | Loss: 0.00077725
Iteration 5/25 | Loss: 0.00077724
Iteration 6/25 | Loss: 0.00077724
Iteration 7/25 | Loss: 0.00077724
Iteration 8/25 | Loss: 0.00077724
Iteration 9/25 | Loss: 0.00077724
Iteration 10/25 | Loss: 0.00077724
Iteration 11/25 | Loss: 0.00077724
Iteration 12/25 | Loss: 0.00077724
Iteration 13/25 | Loss: 0.00077724
Iteration 14/25 | Loss: 0.00077724
Iteration 15/25 | Loss: 0.00077724
Iteration 16/25 | Loss: 0.00077724
Iteration 17/25 | Loss: 0.00077724
Iteration 18/25 | Loss: 0.00077724
Iteration 19/25 | Loss: 0.00077724
Iteration 20/25 | Loss: 0.00077724
Iteration 21/25 | Loss: 0.00077724
Iteration 22/25 | Loss: 0.00077724
Iteration 23/25 | Loss: 0.00077724
Iteration 24/25 | Loss: 0.00077724
Iteration 25/25 | Loss: 0.00077724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077724
Iteration 2/1000 | Loss: 0.00002585
Iteration 3/1000 | Loss: 0.00001472
Iteration 4/1000 | Loss: 0.00001229
Iteration 5/1000 | Loss: 0.00001154
Iteration 6/1000 | Loss: 0.00001103
Iteration 7/1000 | Loss: 0.00001059
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00000984
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000980
Iteration 14/1000 | Loss: 0.00000979
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000974
Iteration 17/1000 | Loss: 0.00000973
Iteration 18/1000 | Loss: 0.00000971
Iteration 19/1000 | Loss: 0.00000970
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000967
Iteration 22/1000 | Loss: 0.00000966
Iteration 23/1000 | Loss: 0.00000966
Iteration 24/1000 | Loss: 0.00000965
Iteration 25/1000 | Loss: 0.00000963
Iteration 26/1000 | Loss: 0.00000963
Iteration 27/1000 | Loss: 0.00000963
Iteration 28/1000 | Loss: 0.00000962
Iteration 29/1000 | Loss: 0.00000962
Iteration 30/1000 | Loss: 0.00000961
Iteration 31/1000 | Loss: 0.00000960
Iteration 32/1000 | Loss: 0.00000960
Iteration 33/1000 | Loss: 0.00000959
Iteration 34/1000 | Loss: 0.00000958
Iteration 35/1000 | Loss: 0.00000957
Iteration 36/1000 | Loss: 0.00000957
Iteration 37/1000 | Loss: 0.00000956
Iteration 38/1000 | Loss: 0.00000956
Iteration 39/1000 | Loss: 0.00000955
Iteration 40/1000 | Loss: 0.00000955
Iteration 41/1000 | Loss: 0.00000952
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000950
Iteration 45/1000 | Loss: 0.00000950
Iteration 46/1000 | Loss: 0.00000950
Iteration 47/1000 | Loss: 0.00000949
Iteration 48/1000 | Loss: 0.00000949
Iteration 49/1000 | Loss: 0.00000949
Iteration 50/1000 | Loss: 0.00000948
Iteration 51/1000 | Loss: 0.00000948
Iteration 52/1000 | Loss: 0.00000947
Iteration 53/1000 | Loss: 0.00000947
Iteration 54/1000 | Loss: 0.00000947
Iteration 55/1000 | Loss: 0.00000946
Iteration 56/1000 | Loss: 0.00000946
Iteration 57/1000 | Loss: 0.00000946
Iteration 58/1000 | Loss: 0.00000946
Iteration 59/1000 | Loss: 0.00000945
Iteration 60/1000 | Loss: 0.00000945
Iteration 61/1000 | Loss: 0.00000945
Iteration 62/1000 | Loss: 0.00000945
Iteration 63/1000 | Loss: 0.00000944
Iteration 64/1000 | Loss: 0.00000944
Iteration 65/1000 | Loss: 0.00000944
Iteration 66/1000 | Loss: 0.00000944
Iteration 67/1000 | Loss: 0.00000944
Iteration 68/1000 | Loss: 0.00000944
Iteration 69/1000 | Loss: 0.00000944
Iteration 70/1000 | Loss: 0.00000944
Iteration 71/1000 | Loss: 0.00000944
Iteration 72/1000 | Loss: 0.00000944
Iteration 73/1000 | Loss: 0.00000944
Iteration 74/1000 | Loss: 0.00000944
Iteration 75/1000 | Loss: 0.00000944
Iteration 76/1000 | Loss: 0.00000944
Iteration 77/1000 | Loss: 0.00000943
Iteration 78/1000 | Loss: 0.00000943
Iteration 79/1000 | Loss: 0.00000943
Iteration 80/1000 | Loss: 0.00000943
Iteration 81/1000 | Loss: 0.00000943
Iteration 82/1000 | Loss: 0.00000942
Iteration 83/1000 | Loss: 0.00000942
Iteration 84/1000 | Loss: 0.00000942
Iteration 85/1000 | Loss: 0.00000941
Iteration 86/1000 | Loss: 0.00000941
Iteration 87/1000 | Loss: 0.00000941
Iteration 88/1000 | Loss: 0.00000941
Iteration 89/1000 | Loss: 0.00000941
Iteration 90/1000 | Loss: 0.00000941
Iteration 91/1000 | Loss: 0.00000941
Iteration 92/1000 | Loss: 0.00000941
Iteration 93/1000 | Loss: 0.00000941
Iteration 94/1000 | Loss: 0.00000941
Iteration 95/1000 | Loss: 0.00000941
Iteration 96/1000 | Loss: 0.00000940
Iteration 97/1000 | Loss: 0.00000940
Iteration 98/1000 | Loss: 0.00000940
Iteration 99/1000 | Loss: 0.00000940
Iteration 100/1000 | Loss: 0.00000940
Iteration 101/1000 | Loss: 0.00000940
Iteration 102/1000 | Loss: 0.00000940
Iteration 103/1000 | Loss: 0.00000940
Iteration 104/1000 | Loss: 0.00000940
Iteration 105/1000 | Loss: 0.00000939
Iteration 106/1000 | Loss: 0.00000939
Iteration 107/1000 | Loss: 0.00000939
Iteration 108/1000 | Loss: 0.00000938
Iteration 109/1000 | Loss: 0.00000938
Iteration 110/1000 | Loss: 0.00000938
Iteration 111/1000 | Loss: 0.00000938
Iteration 112/1000 | Loss: 0.00000938
Iteration 113/1000 | Loss: 0.00000938
Iteration 114/1000 | Loss: 0.00000938
Iteration 115/1000 | Loss: 0.00000938
Iteration 116/1000 | Loss: 0.00000938
Iteration 117/1000 | Loss: 0.00000937
Iteration 118/1000 | Loss: 0.00000937
Iteration 119/1000 | Loss: 0.00000937
Iteration 120/1000 | Loss: 0.00000937
Iteration 121/1000 | Loss: 0.00000937
Iteration 122/1000 | Loss: 0.00000937
Iteration 123/1000 | Loss: 0.00000936
Iteration 124/1000 | Loss: 0.00000936
Iteration 125/1000 | Loss: 0.00000936
Iteration 126/1000 | Loss: 0.00000936
Iteration 127/1000 | Loss: 0.00000936
Iteration 128/1000 | Loss: 0.00000936
Iteration 129/1000 | Loss: 0.00000936
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000935
Iteration 132/1000 | Loss: 0.00000935
Iteration 133/1000 | Loss: 0.00000935
Iteration 134/1000 | Loss: 0.00000935
Iteration 135/1000 | Loss: 0.00000935
Iteration 136/1000 | Loss: 0.00000935
Iteration 137/1000 | Loss: 0.00000935
Iteration 138/1000 | Loss: 0.00000935
Iteration 139/1000 | Loss: 0.00000935
Iteration 140/1000 | Loss: 0.00000934
Iteration 141/1000 | Loss: 0.00000934
Iteration 142/1000 | Loss: 0.00000934
Iteration 143/1000 | Loss: 0.00000934
Iteration 144/1000 | Loss: 0.00000934
Iteration 145/1000 | Loss: 0.00000934
Iteration 146/1000 | Loss: 0.00000934
Iteration 147/1000 | Loss: 0.00000934
Iteration 148/1000 | Loss: 0.00000934
Iteration 149/1000 | Loss: 0.00000934
Iteration 150/1000 | Loss: 0.00000934
Iteration 151/1000 | Loss: 0.00000934
Iteration 152/1000 | Loss: 0.00000934
Iteration 153/1000 | Loss: 0.00000933
Iteration 154/1000 | Loss: 0.00000933
Iteration 155/1000 | Loss: 0.00000933
Iteration 156/1000 | Loss: 0.00000932
Iteration 157/1000 | Loss: 0.00000932
Iteration 158/1000 | Loss: 0.00000932
Iteration 159/1000 | Loss: 0.00000932
Iteration 160/1000 | Loss: 0.00000932
Iteration 161/1000 | Loss: 0.00000932
Iteration 162/1000 | Loss: 0.00000932
Iteration 163/1000 | Loss: 0.00000932
Iteration 164/1000 | Loss: 0.00000932
Iteration 165/1000 | Loss: 0.00000932
Iteration 166/1000 | Loss: 0.00000932
Iteration 167/1000 | Loss: 0.00000932
Iteration 168/1000 | Loss: 0.00000932
Iteration 169/1000 | Loss: 0.00000932
Iteration 170/1000 | Loss: 0.00000932
Iteration 171/1000 | Loss: 0.00000932
Iteration 172/1000 | Loss: 0.00000932
Iteration 173/1000 | Loss: 0.00000931
Iteration 174/1000 | Loss: 0.00000931
Iteration 175/1000 | Loss: 0.00000931
Iteration 176/1000 | Loss: 0.00000931
Iteration 177/1000 | Loss: 0.00000931
Iteration 178/1000 | Loss: 0.00000931
Iteration 179/1000 | Loss: 0.00000931
Iteration 180/1000 | Loss: 0.00000931
Iteration 181/1000 | Loss: 0.00000931
Iteration 182/1000 | Loss: 0.00000930
Iteration 183/1000 | Loss: 0.00000930
Iteration 184/1000 | Loss: 0.00000930
Iteration 185/1000 | Loss: 0.00000930
Iteration 186/1000 | Loss: 0.00000930
Iteration 187/1000 | Loss: 0.00000930
Iteration 188/1000 | Loss: 0.00000930
Iteration 189/1000 | Loss: 0.00000930
Iteration 190/1000 | Loss: 0.00000930
Iteration 191/1000 | Loss: 0.00000930
Iteration 192/1000 | Loss: 0.00000930
Iteration 193/1000 | Loss: 0.00000930
Iteration 194/1000 | Loss: 0.00000930
Iteration 195/1000 | Loss: 0.00000930
Iteration 196/1000 | Loss: 0.00000930
Iteration 197/1000 | Loss: 0.00000930
Iteration 198/1000 | Loss: 0.00000930
Iteration 199/1000 | Loss: 0.00000930
Iteration 200/1000 | Loss: 0.00000929
Iteration 201/1000 | Loss: 0.00000929
Iteration 202/1000 | Loss: 0.00000929
Iteration 203/1000 | Loss: 0.00000929
Iteration 204/1000 | Loss: 0.00000929
Iteration 205/1000 | Loss: 0.00000929
Iteration 206/1000 | Loss: 0.00000929
Iteration 207/1000 | Loss: 0.00000929
Iteration 208/1000 | Loss: 0.00000929
Iteration 209/1000 | Loss: 0.00000929
Iteration 210/1000 | Loss: 0.00000929
Iteration 211/1000 | Loss: 0.00000929
Iteration 212/1000 | Loss: 0.00000929
Iteration 213/1000 | Loss: 0.00000929
Iteration 214/1000 | Loss: 0.00000929
Iteration 215/1000 | Loss: 0.00000929
Iteration 216/1000 | Loss: 0.00000929
Iteration 217/1000 | Loss: 0.00000929
Iteration 218/1000 | Loss: 0.00000929
Iteration 219/1000 | Loss: 0.00000929
Iteration 220/1000 | Loss: 0.00000929
Iteration 221/1000 | Loss: 0.00000929
Iteration 222/1000 | Loss: 0.00000929
Iteration 223/1000 | Loss: 0.00000929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [9.29337056732038e-06, 9.29337056732038e-06, 9.29337056732038e-06, 9.29337056732038e-06, 9.29337056732038e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.29337056732038e-06

Optimization complete. Final v2v error: 2.5996062755584717 mm

Highest mean error: 3.6425955295562744 mm for frame 55

Lowest mean error: 2.2782766819000244 mm for frame 174

Saving results

Total time: 45.02233290672302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497626
Iteration 2/25 | Loss: 0.00118364
Iteration 3/25 | Loss: 0.00107242
Iteration 4/25 | Loss: 0.00105516
Iteration 5/25 | Loss: 0.00105009
Iteration 6/25 | Loss: 0.00104942
Iteration 7/25 | Loss: 0.00104942
Iteration 8/25 | Loss: 0.00104942
Iteration 9/25 | Loss: 0.00104942
Iteration 10/25 | Loss: 0.00104942
Iteration 11/25 | Loss: 0.00104942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010494239395484328, 0.0010494239395484328, 0.0010494239395484328, 0.0010494239395484328, 0.0010494239395484328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010494239395484328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84140170
Iteration 2/25 | Loss: 0.00069067
Iteration 3/25 | Loss: 0.00069066
Iteration 4/25 | Loss: 0.00069066
Iteration 5/25 | Loss: 0.00069066
Iteration 6/25 | Loss: 0.00069066
Iteration 7/25 | Loss: 0.00069066
Iteration 8/25 | Loss: 0.00069066
Iteration 9/25 | Loss: 0.00069066
Iteration 10/25 | Loss: 0.00069066
Iteration 11/25 | Loss: 0.00069066
Iteration 12/25 | Loss: 0.00069066
Iteration 13/25 | Loss: 0.00069066
Iteration 14/25 | Loss: 0.00069066
Iteration 15/25 | Loss: 0.00069066
Iteration 16/25 | Loss: 0.00069066
Iteration 17/25 | Loss: 0.00069066
Iteration 18/25 | Loss: 0.00069066
Iteration 19/25 | Loss: 0.00069066
Iteration 20/25 | Loss: 0.00069066
Iteration 21/25 | Loss: 0.00069066
Iteration 22/25 | Loss: 0.00069066
Iteration 23/25 | Loss: 0.00069066
Iteration 24/25 | Loss: 0.00069066
Iteration 25/25 | Loss: 0.00069066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069066
Iteration 2/1000 | Loss: 0.00003277
Iteration 3/1000 | Loss: 0.00002231
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001852
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001715
Iteration 8/1000 | Loss: 0.00001689
Iteration 9/1000 | Loss: 0.00001659
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001554
Iteration 15/1000 | Loss: 0.00001537
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001491
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001485
Iteration 55/1000 | Loss: 0.00001484
Iteration 56/1000 | Loss: 0.00001484
Iteration 57/1000 | Loss: 0.00001484
Iteration 58/1000 | Loss: 0.00001483
Iteration 59/1000 | Loss: 0.00001483
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001476
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001470
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001470
Iteration 123/1000 | Loss: 0.00001469
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00001469
Iteration 126/1000 | Loss: 0.00001469
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001468
Iteration 130/1000 | Loss: 0.00001468
Iteration 131/1000 | Loss: 0.00001468
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001467
Iteration 136/1000 | Loss: 0.00001467
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001466
Iteration 147/1000 | Loss: 0.00001466
Iteration 148/1000 | Loss: 0.00001465
Iteration 149/1000 | Loss: 0.00001465
Iteration 150/1000 | Loss: 0.00001465
Iteration 151/1000 | Loss: 0.00001465
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001464
Iteration 157/1000 | Loss: 0.00001464
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001463
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001463
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001462
Iteration 181/1000 | Loss: 0.00001462
Iteration 182/1000 | Loss: 0.00001462
Iteration 183/1000 | Loss: 0.00001462
Iteration 184/1000 | Loss: 0.00001462
Iteration 185/1000 | Loss: 0.00001462
Iteration 186/1000 | Loss: 0.00001462
Iteration 187/1000 | Loss: 0.00001462
Iteration 188/1000 | Loss: 0.00001462
Iteration 189/1000 | Loss: 0.00001462
Iteration 190/1000 | Loss: 0.00001462
Iteration 191/1000 | Loss: 0.00001462
Iteration 192/1000 | Loss: 0.00001462
Iteration 193/1000 | Loss: 0.00001462
Iteration 194/1000 | Loss: 0.00001462
Iteration 195/1000 | Loss: 0.00001462
Iteration 196/1000 | Loss: 0.00001462
Iteration 197/1000 | Loss: 0.00001462
Iteration 198/1000 | Loss: 0.00001462
Iteration 199/1000 | Loss: 0.00001462
Iteration 200/1000 | Loss: 0.00001462
Iteration 201/1000 | Loss: 0.00001462
Iteration 202/1000 | Loss: 0.00001462
Iteration 203/1000 | Loss: 0.00001462
Iteration 204/1000 | Loss: 0.00001462
Iteration 205/1000 | Loss: 0.00001462
Iteration 206/1000 | Loss: 0.00001462
Iteration 207/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.461805914004799e-05, 1.461805914004799e-05, 1.461805914004799e-05, 1.461805914004799e-05, 1.461805914004799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.461805914004799e-05

Optimization complete. Final v2v error: 3.188887596130371 mm

Highest mean error: 3.452744960784912 mm for frame 0

Lowest mean error: 3.1269242763519287 mm for frame 10

Saving results

Total time: 55.29433226585388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548437
Iteration 2/25 | Loss: 0.00136608
Iteration 3/25 | Loss: 0.00115637
Iteration 4/25 | Loss: 0.00113215
Iteration 5/25 | Loss: 0.00112647
Iteration 6/25 | Loss: 0.00112623
Iteration 7/25 | Loss: 0.00112623
Iteration 8/25 | Loss: 0.00112623
Iteration 9/25 | Loss: 0.00112623
Iteration 10/25 | Loss: 0.00112623
Iteration 11/25 | Loss: 0.00112623
Iteration 12/25 | Loss: 0.00112623
Iteration 13/25 | Loss: 0.00112623
Iteration 14/25 | Loss: 0.00112623
Iteration 15/25 | Loss: 0.00112623
Iteration 16/25 | Loss: 0.00112623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011262266198173165, 0.0011262266198173165, 0.0011262266198173165, 0.0011262266198173165, 0.0011262266198173165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011262266198173165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40006042
Iteration 2/25 | Loss: 0.00088387
Iteration 3/25 | Loss: 0.00088387
Iteration 4/25 | Loss: 0.00088387
Iteration 5/25 | Loss: 0.00088387
Iteration 6/25 | Loss: 0.00088387
Iteration 7/25 | Loss: 0.00088387
Iteration 8/25 | Loss: 0.00088387
Iteration 9/25 | Loss: 0.00088387
Iteration 10/25 | Loss: 0.00088387
Iteration 11/25 | Loss: 0.00088387
Iteration 12/25 | Loss: 0.00088387
Iteration 13/25 | Loss: 0.00088387
Iteration 14/25 | Loss: 0.00088387
Iteration 15/25 | Loss: 0.00088387
Iteration 16/25 | Loss: 0.00088387
Iteration 17/25 | Loss: 0.00088387
Iteration 18/25 | Loss: 0.00088387
Iteration 19/25 | Loss: 0.00088387
Iteration 20/25 | Loss: 0.00088387
Iteration 21/25 | Loss: 0.00088387
Iteration 22/25 | Loss: 0.00088387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008838696521706879, 0.0008838696521706879, 0.0008838696521706879, 0.0008838696521706879, 0.0008838696521706879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008838696521706879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088387
Iteration 2/1000 | Loss: 0.00004883
Iteration 3/1000 | Loss: 0.00003158
Iteration 4/1000 | Loss: 0.00002836
Iteration 5/1000 | Loss: 0.00002746
Iteration 6/1000 | Loss: 0.00002683
Iteration 7/1000 | Loss: 0.00002625
Iteration 8/1000 | Loss: 0.00002591
Iteration 9/1000 | Loss: 0.00002560
Iteration 10/1000 | Loss: 0.00002540
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00002513
Iteration 13/1000 | Loss: 0.00002510
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00002510
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002509
Iteration 18/1000 | Loss: 0.00002509
Iteration 19/1000 | Loss: 0.00002509
Iteration 20/1000 | Loss: 0.00002509
Iteration 21/1000 | Loss: 0.00002509
Iteration 22/1000 | Loss: 0.00002509
Iteration 23/1000 | Loss: 0.00002508
Iteration 24/1000 | Loss: 0.00002508
Iteration 25/1000 | Loss: 0.00002508
Iteration 26/1000 | Loss: 0.00002508
Iteration 27/1000 | Loss: 0.00002506
Iteration 28/1000 | Loss: 0.00002506
Iteration 29/1000 | Loss: 0.00002505
Iteration 30/1000 | Loss: 0.00002505
Iteration 31/1000 | Loss: 0.00002505
Iteration 32/1000 | Loss: 0.00002504
Iteration 33/1000 | Loss: 0.00002504
Iteration 34/1000 | Loss: 0.00002504
Iteration 35/1000 | Loss: 0.00002504
Iteration 36/1000 | Loss: 0.00002504
Iteration 37/1000 | Loss: 0.00002504
Iteration 38/1000 | Loss: 0.00002504
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002504
Iteration 41/1000 | Loss: 0.00002504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [2.5042287234100513e-05, 2.5042287234100513e-05, 2.5042287234100513e-05, 2.5042287234100513e-05, 2.5042287234100513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5042287234100513e-05

Optimization complete. Final v2v error: 4.118556022644043 mm

Highest mean error: 4.768070220947266 mm for frame 88

Lowest mean error: 3.6701555252075195 mm for frame 122

Saving results

Total time: 29.94879126548767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459199
Iteration 2/25 | Loss: 0.00121851
Iteration 3/25 | Loss: 0.00104431
Iteration 4/25 | Loss: 0.00102573
Iteration 5/25 | Loss: 0.00102023
Iteration 6/25 | Loss: 0.00101902
Iteration 7/25 | Loss: 0.00101902
Iteration 8/25 | Loss: 0.00101902
Iteration 9/25 | Loss: 0.00101902
Iteration 10/25 | Loss: 0.00101902
Iteration 11/25 | Loss: 0.00101902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001019017887301743, 0.001019017887301743, 0.001019017887301743, 0.001019017887301743, 0.001019017887301743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001019017887301743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38589060
Iteration 2/25 | Loss: 0.00091386
Iteration 3/25 | Loss: 0.00091386
Iteration 4/25 | Loss: 0.00091386
Iteration 5/25 | Loss: 0.00091386
Iteration 6/25 | Loss: 0.00091386
Iteration 7/25 | Loss: 0.00091386
Iteration 8/25 | Loss: 0.00091386
Iteration 9/25 | Loss: 0.00091386
Iteration 10/25 | Loss: 0.00091386
Iteration 11/25 | Loss: 0.00091386
Iteration 12/25 | Loss: 0.00091386
Iteration 13/25 | Loss: 0.00091386
Iteration 14/25 | Loss: 0.00091386
Iteration 15/25 | Loss: 0.00091386
Iteration 16/25 | Loss: 0.00091386
Iteration 17/25 | Loss: 0.00091386
Iteration 18/25 | Loss: 0.00091386
Iteration 19/25 | Loss: 0.00091386
Iteration 20/25 | Loss: 0.00091386
Iteration 21/25 | Loss: 0.00091386
Iteration 22/25 | Loss: 0.00091386
Iteration 23/25 | Loss: 0.00091386
Iteration 24/25 | Loss: 0.00091386
Iteration 25/25 | Loss: 0.00091386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091386
Iteration 2/1000 | Loss: 0.00003264
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00001953
Iteration 5/1000 | Loss: 0.00001848
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001714
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001603
Iteration 14/1000 | Loss: 0.00001601
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001600
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001586
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001584
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001577
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001576
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001568
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001564
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001559
Iteration 71/1000 | Loss: 0.00001559
Iteration 72/1000 | Loss: 0.00001559
Iteration 73/1000 | Loss: 0.00001559
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001558
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001553
Iteration 93/1000 | Loss: 0.00001553
Iteration 94/1000 | Loss: 0.00001553
Iteration 95/1000 | Loss: 0.00001553
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001550
Iteration 110/1000 | Loss: 0.00001550
Iteration 111/1000 | Loss: 0.00001550
Iteration 112/1000 | Loss: 0.00001550
Iteration 113/1000 | Loss: 0.00001550
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.549905755382497e-05, 1.549905755382497e-05, 1.549905755382497e-05, 1.549905755382497e-05, 1.549905755382497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.549905755382497e-05

Optimization complete. Final v2v error: 3.210317611694336 mm

Highest mean error: 4.1043782234191895 mm for frame 215

Lowest mean error: 2.3902902603149414 mm for frame 53

Saving results

Total time: 42.81537437438965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829427
Iteration 2/25 | Loss: 0.00114885
Iteration 3/25 | Loss: 0.00104653
Iteration 4/25 | Loss: 0.00103358
Iteration 5/25 | Loss: 0.00102961
Iteration 6/25 | Loss: 0.00102863
Iteration 7/25 | Loss: 0.00102863
Iteration 8/25 | Loss: 0.00102863
Iteration 9/25 | Loss: 0.00102863
Iteration 10/25 | Loss: 0.00102863
Iteration 11/25 | Loss: 0.00102863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010286325123161077, 0.0010286325123161077, 0.0010286325123161077, 0.0010286325123161077, 0.0010286325123161077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010286325123161077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64600217
Iteration 2/25 | Loss: 0.00075687
Iteration 3/25 | Loss: 0.00075687
Iteration 4/25 | Loss: 0.00075686
Iteration 5/25 | Loss: 0.00075686
Iteration 6/25 | Loss: 0.00075686
Iteration 7/25 | Loss: 0.00075686
Iteration 8/25 | Loss: 0.00075686
Iteration 9/25 | Loss: 0.00075686
Iteration 10/25 | Loss: 0.00075686
Iteration 11/25 | Loss: 0.00075686
Iteration 12/25 | Loss: 0.00075686
Iteration 13/25 | Loss: 0.00075686
Iteration 14/25 | Loss: 0.00075686
Iteration 15/25 | Loss: 0.00075686
Iteration 16/25 | Loss: 0.00075686
Iteration 17/25 | Loss: 0.00075686
Iteration 18/25 | Loss: 0.00075686
Iteration 19/25 | Loss: 0.00075686
Iteration 20/25 | Loss: 0.00075686
Iteration 21/25 | Loss: 0.00075686
Iteration 22/25 | Loss: 0.00075686
Iteration 23/25 | Loss: 0.00075686
Iteration 24/25 | Loss: 0.00075686
Iteration 25/25 | Loss: 0.00075686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075686
Iteration 2/1000 | Loss: 0.00003557
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001687
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001524
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001416
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001394
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001360
Iteration 36/1000 | Loss: 0.00001360
Iteration 37/1000 | Loss: 0.00001359
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001351
Iteration 50/1000 | Loss: 0.00001351
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001349
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001348
Iteration 58/1000 | Loss: 0.00001348
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001346
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001345
Iteration 76/1000 | Loss: 0.00001345
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001342
Iteration 87/1000 | Loss: 0.00001342
Iteration 88/1000 | Loss: 0.00001342
Iteration 89/1000 | Loss: 0.00001342
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001341
Iteration 96/1000 | Loss: 0.00001341
Iteration 97/1000 | Loss: 0.00001341
Iteration 98/1000 | Loss: 0.00001341
Iteration 99/1000 | Loss: 0.00001341
Iteration 100/1000 | Loss: 0.00001341
Iteration 101/1000 | Loss: 0.00001341
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001340
Iteration 104/1000 | Loss: 0.00001340
Iteration 105/1000 | Loss: 0.00001340
Iteration 106/1000 | Loss: 0.00001340
Iteration 107/1000 | Loss: 0.00001340
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001339
Iteration 111/1000 | Loss: 0.00001339
Iteration 112/1000 | Loss: 0.00001339
Iteration 113/1000 | Loss: 0.00001339
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001338
Iteration 120/1000 | Loss: 0.00001338
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001336
Iteration 128/1000 | Loss: 0.00001336
Iteration 129/1000 | Loss: 0.00001336
Iteration 130/1000 | Loss: 0.00001336
Iteration 131/1000 | Loss: 0.00001336
Iteration 132/1000 | Loss: 0.00001336
Iteration 133/1000 | Loss: 0.00001336
Iteration 134/1000 | Loss: 0.00001336
Iteration 135/1000 | Loss: 0.00001335
Iteration 136/1000 | Loss: 0.00001335
Iteration 137/1000 | Loss: 0.00001335
Iteration 138/1000 | Loss: 0.00001334
Iteration 139/1000 | Loss: 0.00001334
Iteration 140/1000 | Loss: 0.00001334
Iteration 141/1000 | Loss: 0.00001333
Iteration 142/1000 | Loss: 0.00001333
Iteration 143/1000 | Loss: 0.00001333
Iteration 144/1000 | Loss: 0.00001333
Iteration 145/1000 | Loss: 0.00001333
Iteration 146/1000 | Loss: 0.00001333
Iteration 147/1000 | Loss: 0.00001332
Iteration 148/1000 | Loss: 0.00001332
Iteration 149/1000 | Loss: 0.00001332
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001331
Iteration 152/1000 | Loss: 0.00001331
Iteration 153/1000 | Loss: 0.00001331
Iteration 154/1000 | Loss: 0.00001331
Iteration 155/1000 | Loss: 0.00001331
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001331
Iteration 158/1000 | Loss: 0.00001331
Iteration 159/1000 | Loss: 0.00001331
Iteration 160/1000 | Loss: 0.00001331
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001330
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001330
Iteration 167/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3304236745170783e-05, 1.3304236745170783e-05, 1.3304236745170783e-05, 1.3304236745170783e-05, 1.3304236745170783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3304236745170783e-05

Optimization complete. Final v2v error: 3.0354955196380615 mm

Highest mean error: 4.183836936950684 mm for frame 70

Lowest mean error: 2.401155948638916 mm for frame 10

Saving results

Total time: 40.88770031929016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482514
Iteration 2/25 | Loss: 0.00115014
Iteration 3/25 | Loss: 0.00105900
Iteration 4/25 | Loss: 0.00104631
Iteration 5/25 | Loss: 0.00104154
Iteration 6/25 | Loss: 0.00104082
Iteration 7/25 | Loss: 0.00104082
Iteration 8/25 | Loss: 0.00104082
Iteration 9/25 | Loss: 0.00104082
Iteration 10/25 | Loss: 0.00104082
Iteration 11/25 | Loss: 0.00104082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010408181697130203, 0.0010408181697130203, 0.0010408181697130203, 0.0010408181697130203, 0.0010408181697130203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010408181697130203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45270312
Iteration 2/25 | Loss: 0.00075577
Iteration 3/25 | Loss: 0.00075575
Iteration 4/25 | Loss: 0.00075575
Iteration 5/25 | Loss: 0.00075575
Iteration 6/25 | Loss: 0.00075575
Iteration 7/25 | Loss: 0.00075575
Iteration 8/25 | Loss: 0.00075575
Iteration 9/25 | Loss: 0.00075575
Iteration 10/25 | Loss: 0.00075575
Iteration 11/25 | Loss: 0.00075575
Iteration 12/25 | Loss: 0.00075575
Iteration 13/25 | Loss: 0.00075575
Iteration 14/25 | Loss: 0.00075575
Iteration 15/25 | Loss: 0.00075575
Iteration 16/25 | Loss: 0.00075575
Iteration 17/25 | Loss: 0.00075575
Iteration 18/25 | Loss: 0.00075575
Iteration 19/25 | Loss: 0.00075575
Iteration 20/25 | Loss: 0.00075575
Iteration 21/25 | Loss: 0.00075575
Iteration 22/25 | Loss: 0.00075575
Iteration 23/25 | Loss: 0.00075575
Iteration 24/25 | Loss: 0.00075575
Iteration 25/25 | Loss: 0.00075575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075575
Iteration 2/1000 | Loss: 0.00003076
Iteration 3/1000 | Loss: 0.00001684
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001333
Iteration 6/1000 | Loss: 0.00001281
Iteration 7/1000 | Loss: 0.00001248
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001187
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001185
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001176
Iteration 28/1000 | Loss: 0.00001176
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001176
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001166
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001150
Iteration 127/1000 | Loss: 0.00001150
Iteration 128/1000 | Loss: 0.00001150
Iteration 129/1000 | Loss: 0.00001150
Iteration 130/1000 | Loss: 0.00001150
Iteration 131/1000 | Loss: 0.00001150
Iteration 132/1000 | Loss: 0.00001150
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001150
Iteration 139/1000 | Loss: 0.00001150
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001150
Iteration 145/1000 | Loss: 0.00001150
Iteration 146/1000 | Loss: 0.00001150
Iteration 147/1000 | Loss: 0.00001150
Iteration 148/1000 | Loss: 0.00001150
Iteration 149/1000 | Loss: 0.00001150
Iteration 150/1000 | Loss: 0.00001150
Iteration 151/1000 | Loss: 0.00001150
Iteration 152/1000 | Loss: 0.00001150
Iteration 153/1000 | Loss: 0.00001150
Iteration 154/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.150112075265497e-05, 1.150112075265497e-05, 1.150112075265497e-05, 1.150112075265497e-05, 1.150112075265497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.150112075265497e-05

Optimization complete. Final v2v error: 2.890188455581665 mm

Highest mean error: 3.378892660140991 mm for frame 104

Lowest mean error: 2.619938611984253 mm for frame 149

Saving results

Total time: 39.53113293647766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901006
Iteration 2/25 | Loss: 0.00210551
Iteration 3/25 | Loss: 0.00170415
Iteration 4/25 | Loss: 0.00169760
Iteration 5/25 | Loss: 0.00154783
Iteration 6/25 | Loss: 0.00127454
Iteration 7/25 | Loss: 0.00119967
Iteration 8/25 | Loss: 0.00118540
Iteration 9/25 | Loss: 0.00116024
Iteration 10/25 | Loss: 0.00113771
Iteration 11/25 | Loss: 0.00112725
Iteration 12/25 | Loss: 0.00111669
Iteration 13/25 | Loss: 0.00111101
Iteration 14/25 | Loss: 0.00111127
Iteration 15/25 | Loss: 0.00110830
Iteration 16/25 | Loss: 0.00110655
Iteration 17/25 | Loss: 0.00110580
Iteration 18/25 | Loss: 0.00110562
Iteration 19/25 | Loss: 0.00110555
Iteration 20/25 | Loss: 0.00110548
Iteration 21/25 | Loss: 0.00110547
Iteration 22/25 | Loss: 0.00110547
Iteration 23/25 | Loss: 0.00110547
Iteration 24/25 | Loss: 0.00110547
Iteration 25/25 | Loss: 0.00110547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35801864
Iteration 2/25 | Loss: 0.00126372
Iteration 3/25 | Loss: 0.00126371
Iteration 4/25 | Loss: 0.00126371
Iteration 5/25 | Loss: 0.00126371
Iteration 6/25 | Loss: 0.00126371
Iteration 7/25 | Loss: 0.00126371
Iteration 8/25 | Loss: 0.00126371
Iteration 9/25 | Loss: 0.00126371
Iteration 10/25 | Loss: 0.00126371
Iteration 11/25 | Loss: 0.00126371
Iteration 12/25 | Loss: 0.00126371
Iteration 13/25 | Loss: 0.00126371
Iteration 14/25 | Loss: 0.00126371
Iteration 15/25 | Loss: 0.00126371
Iteration 16/25 | Loss: 0.00126371
Iteration 17/25 | Loss: 0.00126371
Iteration 18/25 | Loss: 0.00126371
Iteration 19/25 | Loss: 0.00126371
Iteration 20/25 | Loss: 0.00126371
Iteration 21/25 | Loss: 0.00126371
Iteration 22/25 | Loss: 0.00126371
Iteration 23/25 | Loss: 0.00126371
Iteration 24/25 | Loss: 0.00126371
Iteration 25/25 | Loss: 0.00126371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126371
Iteration 2/1000 | Loss: 0.00147143
Iteration 3/1000 | Loss: 0.00015450
Iteration 4/1000 | Loss: 0.00011216
Iteration 5/1000 | Loss: 0.00008458
Iteration 6/1000 | Loss: 0.00007108
Iteration 7/1000 | Loss: 0.00071261
Iteration 8/1000 | Loss: 0.00009635
Iteration 9/1000 | Loss: 0.00058298
Iteration 10/1000 | Loss: 0.00006949
Iteration 11/1000 | Loss: 0.00121354
Iteration 12/1000 | Loss: 0.00032212
Iteration 13/1000 | Loss: 0.00212325
Iteration 14/1000 | Loss: 0.00107638
Iteration 15/1000 | Loss: 0.00117153
Iteration 16/1000 | Loss: 0.00177671
Iteration 17/1000 | Loss: 0.00118274
Iteration 18/1000 | Loss: 0.00063517
Iteration 19/1000 | Loss: 0.00099268
Iteration 20/1000 | Loss: 0.00116948
Iteration 21/1000 | Loss: 0.00006100
Iteration 22/1000 | Loss: 0.00004948
Iteration 23/1000 | Loss: 0.00004371
Iteration 24/1000 | Loss: 0.00036774
Iteration 25/1000 | Loss: 0.00005466
Iteration 26/1000 | Loss: 0.00003873
Iteration 27/1000 | Loss: 0.00003156
Iteration 28/1000 | Loss: 0.00006394
Iteration 29/1000 | Loss: 0.00004835
Iteration 30/1000 | Loss: 0.00002687
Iteration 31/1000 | Loss: 0.00006701
Iteration 32/1000 | Loss: 0.00016522
Iteration 33/1000 | Loss: 0.00010049
Iteration 34/1000 | Loss: 0.00016354
Iteration 35/1000 | Loss: 0.00018352
Iteration 36/1000 | Loss: 0.00005588
Iteration 37/1000 | Loss: 0.00019985
Iteration 38/1000 | Loss: 0.00011307
Iteration 39/1000 | Loss: 0.00003319
Iteration 40/1000 | Loss: 0.00002885
Iteration 41/1000 | Loss: 0.00002654
Iteration 42/1000 | Loss: 0.00005070
Iteration 43/1000 | Loss: 0.00002874
Iteration 44/1000 | Loss: 0.00002405
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002192
Iteration 47/1000 | Loss: 0.00002139
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002020
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00031331
Iteration 52/1000 | Loss: 0.00029782
Iteration 53/1000 | Loss: 0.00003659
Iteration 54/1000 | Loss: 0.00003201
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002092
Iteration 58/1000 | Loss: 0.00002011
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001768
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001730
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001679
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001671
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001669
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001668
Iteration 91/1000 | Loss: 0.00001668
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001660
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001658
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001656
Iteration 127/1000 | Loss: 0.00001656
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001654
Iteration 130/1000 | Loss: 0.00001654
Iteration 131/1000 | Loss: 0.00001654
Iteration 132/1000 | Loss: 0.00001653
Iteration 133/1000 | Loss: 0.00001653
Iteration 134/1000 | Loss: 0.00001653
Iteration 135/1000 | Loss: 0.00001653
Iteration 136/1000 | Loss: 0.00001653
Iteration 137/1000 | Loss: 0.00001653
Iteration 138/1000 | Loss: 0.00001652
Iteration 139/1000 | Loss: 0.00001652
Iteration 140/1000 | Loss: 0.00001652
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001652
Iteration 143/1000 | Loss: 0.00001652
Iteration 144/1000 | Loss: 0.00001652
Iteration 145/1000 | Loss: 0.00001651
Iteration 146/1000 | Loss: 0.00001651
Iteration 147/1000 | Loss: 0.00001651
Iteration 148/1000 | Loss: 0.00001651
Iteration 149/1000 | Loss: 0.00001651
Iteration 150/1000 | Loss: 0.00001651
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001650
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001648
Iteration 158/1000 | Loss: 0.00001648
Iteration 159/1000 | Loss: 0.00001648
Iteration 160/1000 | Loss: 0.00001648
Iteration 161/1000 | Loss: 0.00001648
Iteration 162/1000 | Loss: 0.00001647
Iteration 163/1000 | Loss: 0.00001647
Iteration 164/1000 | Loss: 0.00001647
Iteration 165/1000 | Loss: 0.00001647
Iteration 166/1000 | Loss: 0.00001647
Iteration 167/1000 | Loss: 0.00001647
Iteration 168/1000 | Loss: 0.00001646
Iteration 169/1000 | Loss: 0.00001646
Iteration 170/1000 | Loss: 0.00001645
Iteration 171/1000 | Loss: 0.00001645
Iteration 172/1000 | Loss: 0.00001645
Iteration 173/1000 | Loss: 0.00001645
Iteration 174/1000 | Loss: 0.00001644
Iteration 175/1000 | Loss: 0.00001644
Iteration 176/1000 | Loss: 0.00001644
Iteration 177/1000 | Loss: 0.00001643
Iteration 178/1000 | Loss: 0.00001643
Iteration 179/1000 | Loss: 0.00001643
Iteration 180/1000 | Loss: 0.00001643
Iteration 181/1000 | Loss: 0.00001643
Iteration 182/1000 | Loss: 0.00001643
Iteration 183/1000 | Loss: 0.00001643
Iteration 184/1000 | Loss: 0.00001642
Iteration 185/1000 | Loss: 0.00001642
Iteration 186/1000 | Loss: 0.00001642
Iteration 187/1000 | Loss: 0.00001642
Iteration 188/1000 | Loss: 0.00001642
Iteration 189/1000 | Loss: 0.00001642
Iteration 190/1000 | Loss: 0.00001642
Iteration 191/1000 | Loss: 0.00001641
Iteration 192/1000 | Loss: 0.00001641
Iteration 193/1000 | Loss: 0.00001641
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001641
Iteration 196/1000 | Loss: 0.00001641
Iteration 197/1000 | Loss: 0.00001641
Iteration 198/1000 | Loss: 0.00001641
Iteration 199/1000 | Loss: 0.00001641
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001640
Iteration 202/1000 | Loss: 0.00001640
Iteration 203/1000 | Loss: 0.00001640
Iteration 204/1000 | Loss: 0.00001640
Iteration 205/1000 | Loss: 0.00001640
Iteration 206/1000 | Loss: 0.00001640
Iteration 207/1000 | Loss: 0.00001640
Iteration 208/1000 | Loss: 0.00001640
Iteration 209/1000 | Loss: 0.00001640
Iteration 210/1000 | Loss: 0.00001640
Iteration 211/1000 | Loss: 0.00001640
Iteration 212/1000 | Loss: 0.00001640
Iteration 213/1000 | Loss: 0.00001640
Iteration 214/1000 | Loss: 0.00001640
Iteration 215/1000 | Loss: 0.00001640
Iteration 216/1000 | Loss: 0.00001640
Iteration 217/1000 | Loss: 0.00001640
Iteration 218/1000 | Loss: 0.00001640
Iteration 219/1000 | Loss: 0.00001640
Iteration 220/1000 | Loss: 0.00001640
Iteration 221/1000 | Loss: 0.00001640
Iteration 222/1000 | Loss: 0.00001640
Iteration 223/1000 | Loss: 0.00001640
Iteration 224/1000 | Loss: 0.00001640
Iteration 225/1000 | Loss: 0.00001640
Iteration 226/1000 | Loss: 0.00001640
Iteration 227/1000 | Loss: 0.00001640
Iteration 228/1000 | Loss: 0.00001640
Iteration 229/1000 | Loss: 0.00001640
Iteration 230/1000 | Loss: 0.00001640
Iteration 231/1000 | Loss: 0.00001640
Iteration 232/1000 | Loss: 0.00001640
Iteration 233/1000 | Loss: 0.00001640
Iteration 234/1000 | Loss: 0.00001640
Iteration 235/1000 | Loss: 0.00001640
Iteration 236/1000 | Loss: 0.00001640
Iteration 237/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.639658148633316e-05, 1.639658148633316e-05, 1.639658148633316e-05, 1.639658148633316e-05, 1.639658148633316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.639658148633316e-05

Optimization complete. Final v2v error: 3.331479787826538 mm

Highest mean error: 5.155862808227539 mm for frame 34

Lowest mean error: 3.0166141986846924 mm for frame 41

Saving results

Total time: 141.61588644981384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427158
Iteration 2/25 | Loss: 0.00121148
Iteration 3/25 | Loss: 0.00104254
Iteration 4/25 | Loss: 0.00103768
Iteration 5/25 | Loss: 0.00103656
Iteration 6/25 | Loss: 0.00103642
Iteration 7/25 | Loss: 0.00103642
Iteration 8/25 | Loss: 0.00103642
Iteration 9/25 | Loss: 0.00103642
Iteration 10/25 | Loss: 0.00103642
Iteration 11/25 | Loss: 0.00103642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001036423142068088, 0.001036423142068088, 0.001036423142068088, 0.001036423142068088, 0.001036423142068088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001036423142068088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82756305
Iteration 2/25 | Loss: 0.00048901
Iteration 3/25 | Loss: 0.00048899
Iteration 4/25 | Loss: 0.00048899
Iteration 5/25 | Loss: 0.00048899
Iteration 6/25 | Loss: 0.00048899
Iteration 7/25 | Loss: 0.00048899
Iteration 8/25 | Loss: 0.00048899
Iteration 9/25 | Loss: 0.00048899
Iteration 10/25 | Loss: 0.00048899
Iteration 11/25 | Loss: 0.00048899
Iteration 12/25 | Loss: 0.00048899
Iteration 13/25 | Loss: 0.00048899
Iteration 14/25 | Loss: 0.00048899
Iteration 15/25 | Loss: 0.00048899
Iteration 16/25 | Loss: 0.00048899
Iteration 17/25 | Loss: 0.00048899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004889867850579321, 0.0004889867850579321, 0.0004889867850579321, 0.0004889867850579321, 0.0004889867850579321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004889867850579321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048899
Iteration 2/1000 | Loss: 0.00002599
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001441
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001161
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001153
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001152
Iteration 49/1000 | Loss: 0.00001150
Iteration 50/1000 | Loss: 0.00001150
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001149
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001143
Iteration 68/1000 | Loss: 0.00001143
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001140
Iteration 79/1000 | Loss: 0.00001140
Iteration 80/1000 | Loss: 0.00001140
Iteration 81/1000 | Loss: 0.00001140
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001138
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001138
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001128
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001125
Iteration 157/1000 | Loss: 0.00001125
Iteration 158/1000 | Loss: 0.00001125
Iteration 159/1000 | Loss: 0.00001125
Iteration 160/1000 | Loss: 0.00001125
Iteration 161/1000 | Loss: 0.00001125
Iteration 162/1000 | Loss: 0.00001125
Iteration 163/1000 | Loss: 0.00001125
Iteration 164/1000 | Loss: 0.00001125
Iteration 165/1000 | Loss: 0.00001125
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001124
Iteration 177/1000 | Loss: 0.00001124
Iteration 178/1000 | Loss: 0.00001124
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001124
Iteration 181/1000 | Loss: 0.00001124
Iteration 182/1000 | Loss: 0.00001124
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001124
Iteration 185/1000 | Loss: 0.00001123
Iteration 186/1000 | Loss: 0.00001123
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001123
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Iteration 193/1000 | Loss: 0.00001123
Iteration 194/1000 | Loss: 0.00001123
Iteration 195/1000 | Loss: 0.00001123
Iteration 196/1000 | Loss: 0.00001123
Iteration 197/1000 | Loss: 0.00001123
Iteration 198/1000 | Loss: 0.00001123
Iteration 199/1000 | Loss: 0.00001123
Iteration 200/1000 | Loss: 0.00001123
Iteration 201/1000 | Loss: 0.00001123
Iteration 202/1000 | Loss: 0.00001123
Iteration 203/1000 | Loss: 0.00001123
Iteration 204/1000 | Loss: 0.00001123
Iteration 205/1000 | Loss: 0.00001123
Iteration 206/1000 | Loss: 0.00001123
Iteration 207/1000 | Loss: 0.00001123
Iteration 208/1000 | Loss: 0.00001123
Iteration 209/1000 | Loss: 0.00001123
Iteration 210/1000 | Loss: 0.00001123
Iteration 211/1000 | Loss: 0.00001123
Iteration 212/1000 | Loss: 0.00001123
Iteration 213/1000 | Loss: 0.00001123
Iteration 214/1000 | Loss: 0.00001123
Iteration 215/1000 | Loss: 0.00001123
Iteration 216/1000 | Loss: 0.00001123
Iteration 217/1000 | Loss: 0.00001123
Iteration 218/1000 | Loss: 0.00001123
Iteration 219/1000 | Loss: 0.00001123
Iteration 220/1000 | Loss: 0.00001123
Iteration 221/1000 | Loss: 0.00001123
Iteration 222/1000 | Loss: 0.00001123
Iteration 223/1000 | Loss: 0.00001123
Iteration 224/1000 | Loss: 0.00001123
Iteration 225/1000 | Loss: 0.00001123
Iteration 226/1000 | Loss: 0.00001123
Iteration 227/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.1228541552554816e-05, 1.1228541552554816e-05, 1.1228541552554816e-05, 1.1228541552554816e-05, 1.1228541552554816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1228541552554816e-05

Optimization complete. Final v2v error: 2.8496577739715576 mm

Highest mean error: 3.3883512020111084 mm for frame 87

Lowest mean error: 2.6006197929382324 mm for frame 53

Saving results

Total time: 38.6550726890564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640593
Iteration 2/25 | Loss: 0.00142938
Iteration 3/25 | Loss: 0.00114522
Iteration 4/25 | Loss: 0.00106840
Iteration 5/25 | Loss: 0.00105828
Iteration 6/25 | Loss: 0.00106208
Iteration 7/25 | Loss: 0.00105274
Iteration 8/25 | Loss: 0.00105208
Iteration 9/25 | Loss: 0.00105521
Iteration 10/25 | Loss: 0.00105094
Iteration 11/25 | Loss: 0.00105073
Iteration 12/25 | Loss: 0.00105065
Iteration 13/25 | Loss: 0.00105065
Iteration 14/25 | Loss: 0.00105065
Iteration 15/25 | Loss: 0.00105064
Iteration 16/25 | Loss: 0.00105064
Iteration 17/25 | Loss: 0.00105064
Iteration 18/25 | Loss: 0.00105064
Iteration 19/25 | Loss: 0.00105064
Iteration 20/25 | Loss: 0.00105064
Iteration 21/25 | Loss: 0.00105064
Iteration 22/25 | Loss: 0.00105064
Iteration 23/25 | Loss: 0.00105064
Iteration 24/25 | Loss: 0.00105064
Iteration 25/25 | Loss: 0.00105064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92095065
Iteration 2/25 | Loss: 0.00075636
Iteration 3/25 | Loss: 0.00075632
Iteration 4/25 | Loss: 0.00075632
Iteration 5/25 | Loss: 0.00075632
Iteration 6/25 | Loss: 0.00075632
Iteration 7/25 | Loss: 0.00075632
Iteration 8/25 | Loss: 0.00075631
Iteration 9/25 | Loss: 0.00075631
Iteration 10/25 | Loss: 0.00075631
Iteration 11/25 | Loss: 0.00075631
Iteration 12/25 | Loss: 0.00075631
Iteration 13/25 | Loss: 0.00075631
Iteration 14/25 | Loss: 0.00075631
Iteration 15/25 | Loss: 0.00075631
Iteration 16/25 | Loss: 0.00075631
Iteration 17/25 | Loss: 0.00075631
Iteration 18/25 | Loss: 0.00075631
Iteration 19/25 | Loss: 0.00075631
Iteration 20/25 | Loss: 0.00075631
Iteration 21/25 | Loss: 0.00075631
Iteration 22/25 | Loss: 0.00075631
Iteration 23/25 | Loss: 0.00075631
Iteration 24/25 | Loss: 0.00075631
Iteration 25/25 | Loss: 0.00075631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075631
Iteration 2/1000 | Loss: 0.00002925
Iteration 3/1000 | Loss: 0.00001889
Iteration 4/1000 | Loss: 0.00001652
Iteration 5/1000 | Loss: 0.00008020
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00001561
Iteration 9/1000 | Loss: 0.00001491
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00004767
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001365
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001337
Iteration 42/1000 | Loss: 0.00001337
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00008010
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001334
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001324
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001314
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001309
Iteration 108/1000 | Loss: 0.00001309
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Iteration 117/1000 | Loss: 0.00001307
Iteration 118/1000 | Loss: 0.00001307
Iteration 119/1000 | Loss: 0.00001307
Iteration 120/1000 | Loss: 0.00001307
Iteration 121/1000 | Loss: 0.00001307
Iteration 122/1000 | Loss: 0.00001307
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001303
Iteration 137/1000 | Loss: 0.00001303
Iteration 138/1000 | Loss: 0.00001303
Iteration 139/1000 | Loss: 0.00001303
Iteration 140/1000 | Loss: 0.00001303
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001303
Iteration 143/1000 | Loss: 0.00001303
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001302
Iteration 146/1000 | Loss: 0.00001302
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Iteration 167/1000 | Loss: 0.00001297
Iteration 168/1000 | Loss: 0.00001297
Iteration 169/1000 | Loss: 0.00001297
Iteration 170/1000 | Loss: 0.00001297
Iteration 171/1000 | Loss: 0.00001297
Iteration 172/1000 | Loss: 0.00001297
Iteration 173/1000 | Loss: 0.00001297
Iteration 174/1000 | Loss: 0.00001297
Iteration 175/1000 | Loss: 0.00001297
Iteration 176/1000 | Loss: 0.00001296
Iteration 177/1000 | Loss: 0.00001296
Iteration 178/1000 | Loss: 0.00001296
Iteration 179/1000 | Loss: 0.00001296
Iteration 180/1000 | Loss: 0.00001296
Iteration 181/1000 | Loss: 0.00001296
Iteration 182/1000 | Loss: 0.00001296
Iteration 183/1000 | Loss: 0.00001296
Iteration 184/1000 | Loss: 0.00001296
Iteration 185/1000 | Loss: 0.00001295
Iteration 186/1000 | Loss: 0.00001295
Iteration 187/1000 | Loss: 0.00001295
Iteration 188/1000 | Loss: 0.00001294
Iteration 189/1000 | Loss: 0.00001294
Iteration 190/1000 | Loss: 0.00001294
Iteration 191/1000 | Loss: 0.00001294
Iteration 192/1000 | Loss: 0.00001294
Iteration 193/1000 | Loss: 0.00001294
Iteration 194/1000 | Loss: 0.00001293
Iteration 195/1000 | Loss: 0.00001293
Iteration 196/1000 | Loss: 0.00001293
Iteration 197/1000 | Loss: 0.00001293
Iteration 198/1000 | Loss: 0.00001293
Iteration 199/1000 | Loss: 0.00001293
Iteration 200/1000 | Loss: 0.00001293
Iteration 201/1000 | Loss: 0.00001293
Iteration 202/1000 | Loss: 0.00001293
Iteration 203/1000 | Loss: 0.00001293
Iteration 204/1000 | Loss: 0.00001292
Iteration 205/1000 | Loss: 0.00001292
Iteration 206/1000 | Loss: 0.00001292
Iteration 207/1000 | Loss: 0.00001292
Iteration 208/1000 | Loss: 0.00001292
Iteration 209/1000 | Loss: 0.00001292
Iteration 210/1000 | Loss: 0.00001292
Iteration 211/1000 | Loss: 0.00001292
Iteration 212/1000 | Loss: 0.00001292
Iteration 213/1000 | Loss: 0.00001292
Iteration 214/1000 | Loss: 0.00001292
Iteration 215/1000 | Loss: 0.00001292
Iteration 216/1000 | Loss: 0.00001292
Iteration 217/1000 | Loss: 0.00001291
Iteration 218/1000 | Loss: 0.00001291
Iteration 219/1000 | Loss: 0.00001291
Iteration 220/1000 | Loss: 0.00001291
Iteration 221/1000 | Loss: 0.00001291
Iteration 222/1000 | Loss: 0.00001290
Iteration 223/1000 | Loss: 0.00001290
Iteration 224/1000 | Loss: 0.00001290
Iteration 225/1000 | Loss: 0.00001290
Iteration 226/1000 | Loss: 0.00001290
Iteration 227/1000 | Loss: 0.00001290
Iteration 228/1000 | Loss: 0.00001290
Iteration 229/1000 | Loss: 0.00001290
Iteration 230/1000 | Loss: 0.00001290
Iteration 231/1000 | Loss: 0.00001290
Iteration 232/1000 | Loss: 0.00001290
Iteration 233/1000 | Loss: 0.00001289
Iteration 234/1000 | Loss: 0.00001289
Iteration 235/1000 | Loss: 0.00001289
Iteration 236/1000 | Loss: 0.00001289
Iteration 237/1000 | Loss: 0.00001289
Iteration 238/1000 | Loss: 0.00001289
Iteration 239/1000 | Loss: 0.00001289
Iteration 240/1000 | Loss: 0.00001289
Iteration 241/1000 | Loss: 0.00001289
Iteration 242/1000 | Loss: 0.00001289
Iteration 243/1000 | Loss: 0.00001289
Iteration 244/1000 | Loss: 0.00001289
Iteration 245/1000 | Loss: 0.00001289
Iteration 246/1000 | Loss: 0.00001289
Iteration 247/1000 | Loss: 0.00001289
Iteration 248/1000 | Loss: 0.00001289
Iteration 249/1000 | Loss: 0.00001289
Iteration 250/1000 | Loss: 0.00001289
Iteration 251/1000 | Loss: 0.00001289
Iteration 252/1000 | Loss: 0.00001289
Iteration 253/1000 | Loss: 0.00001289
Iteration 254/1000 | Loss: 0.00001288
Iteration 255/1000 | Loss: 0.00001288
Iteration 256/1000 | Loss: 0.00001288
Iteration 257/1000 | Loss: 0.00001288
Iteration 258/1000 | Loss: 0.00001288
Iteration 259/1000 | Loss: 0.00001288
Iteration 260/1000 | Loss: 0.00001288
Iteration 261/1000 | Loss: 0.00001288
Iteration 262/1000 | Loss: 0.00001288
Iteration 263/1000 | Loss: 0.00001288
Iteration 264/1000 | Loss: 0.00001288
Iteration 265/1000 | Loss: 0.00001288
Iteration 266/1000 | Loss: 0.00001288
Iteration 267/1000 | Loss: 0.00001288
Iteration 268/1000 | Loss: 0.00001288
Iteration 269/1000 | Loss: 0.00001288
Iteration 270/1000 | Loss: 0.00001288
Iteration 271/1000 | Loss: 0.00001288
Iteration 272/1000 | Loss: 0.00001288
Iteration 273/1000 | Loss: 0.00001288
Iteration 274/1000 | Loss: 0.00001288
Iteration 275/1000 | Loss: 0.00001287
Iteration 276/1000 | Loss: 0.00001287
Iteration 277/1000 | Loss: 0.00001287
Iteration 278/1000 | Loss: 0.00001287
Iteration 279/1000 | Loss: 0.00001287
Iteration 280/1000 | Loss: 0.00001287
Iteration 281/1000 | Loss: 0.00001287
Iteration 282/1000 | Loss: 0.00001287
Iteration 283/1000 | Loss: 0.00001287
Iteration 284/1000 | Loss: 0.00001287
Iteration 285/1000 | Loss: 0.00001287
Iteration 286/1000 | Loss: 0.00001287
Iteration 287/1000 | Loss: 0.00001286
Iteration 288/1000 | Loss: 0.00001286
Iteration 289/1000 | Loss: 0.00001286
Iteration 290/1000 | Loss: 0.00001286
Iteration 291/1000 | Loss: 0.00001286
Iteration 292/1000 | Loss: 0.00001286
Iteration 293/1000 | Loss: 0.00001286
Iteration 294/1000 | Loss: 0.00001286
Iteration 295/1000 | Loss: 0.00001285
Iteration 296/1000 | Loss: 0.00001285
Iteration 297/1000 | Loss: 0.00001285
Iteration 298/1000 | Loss: 0.00001285
Iteration 299/1000 | Loss: 0.00001285
Iteration 300/1000 | Loss: 0.00001285
Iteration 301/1000 | Loss: 0.00001285
Iteration 302/1000 | Loss: 0.00001285
Iteration 303/1000 | Loss: 0.00001285
Iteration 304/1000 | Loss: 0.00001285
Iteration 305/1000 | Loss: 0.00001285
Iteration 306/1000 | Loss: 0.00001285
Iteration 307/1000 | Loss: 0.00001285
Iteration 308/1000 | Loss: 0.00001285
Iteration 309/1000 | Loss: 0.00001284
Iteration 310/1000 | Loss: 0.00001284
Iteration 311/1000 | Loss: 0.00001284
Iteration 312/1000 | Loss: 0.00001284
Iteration 313/1000 | Loss: 0.00001284
Iteration 314/1000 | Loss: 0.00001284
Iteration 315/1000 | Loss: 0.00001284
Iteration 316/1000 | Loss: 0.00001284
Iteration 317/1000 | Loss: 0.00001284
Iteration 318/1000 | Loss: 0.00001284
Iteration 319/1000 | Loss: 0.00001284
Iteration 320/1000 | Loss: 0.00001284
Iteration 321/1000 | Loss: 0.00001284
Iteration 322/1000 | Loss: 0.00001284
Iteration 323/1000 | Loss: 0.00001284
Iteration 324/1000 | Loss: 0.00001284
Iteration 325/1000 | Loss: 0.00001284
Iteration 326/1000 | Loss: 0.00001284
Iteration 327/1000 | Loss: 0.00001284
Iteration 328/1000 | Loss: 0.00001284
Iteration 329/1000 | Loss: 0.00001284
Iteration 330/1000 | Loss: 0.00001283
Iteration 331/1000 | Loss: 0.00001283
Iteration 332/1000 | Loss: 0.00001283
Iteration 333/1000 | Loss: 0.00001283
Iteration 334/1000 | Loss: 0.00001283
Iteration 335/1000 | Loss: 0.00001283
Iteration 336/1000 | Loss: 0.00001283
Iteration 337/1000 | Loss: 0.00001283
Iteration 338/1000 | Loss: 0.00001283
Iteration 339/1000 | Loss: 0.00001283
Iteration 340/1000 | Loss: 0.00001283
Iteration 341/1000 | Loss: 0.00001283
Iteration 342/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [1.2830707419198006e-05, 1.2830707419198006e-05, 1.2830707419198006e-05, 1.2830707419198006e-05, 1.2830707419198006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2830707419198006e-05

Optimization complete. Final v2v error: 2.952878475189209 mm

Highest mean error: 3.7948780059814453 mm for frame 92

Lowest mean error: 2.4900810718536377 mm for frame 52

Saving results

Total time: 82.00054860115051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824443
Iteration 2/25 | Loss: 0.00143481
Iteration 3/25 | Loss: 0.00110071
Iteration 4/25 | Loss: 0.00105632
Iteration 5/25 | Loss: 0.00105257
Iteration 6/25 | Loss: 0.00105163
Iteration 7/25 | Loss: 0.00105163
Iteration 8/25 | Loss: 0.00105163
Iteration 9/25 | Loss: 0.00105163
Iteration 10/25 | Loss: 0.00105163
Iteration 11/25 | Loss: 0.00105163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010516309412196279, 0.0010516309412196279, 0.0010516309412196279, 0.0010516309412196279, 0.0010516309412196279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010516309412196279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34699345
Iteration 2/25 | Loss: 0.00057696
Iteration 3/25 | Loss: 0.00057696
Iteration 4/25 | Loss: 0.00057696
Iteration 5/25 | Loss: 0.00057696
Iteration 6/25 | Loss: 0.00057696
Iteration 7/25 | Loss: 0.00057695
Iteration 8/25 | Loss: 0.00057695
Iteration 9/25 | Loss: 0.00057695
Iteration 10/25 | Loss: 0.00057695
Iteration 11/25 | Loss: 0.00057695
Iteration 12/25 | Loss: 0.00057695
Iteration 13/25 | Loss: 0.00057695
Iteration 14/25 | Loss: 0.00057695
Iteration 15/25 | Loss: 0.00057695
Iteration 16/25 | Loss: 0.00057695
Iteration 17/25 | Loss: 0.00057695
Iteration 18/25 | Loss: 0.00057695
Iteration 19/25 | Loss: 0.00057695
Iteration 20/25 | Loss: 0.00057695
Iteration 21/25 | Loss: 0.00057695
Iteration 22/25 | Loss: 0.00057695
Iteration 23/25 | Loss: 0.00057695
Iteration 24/25 | Loss: 0.00057695
Iteration 25/25 | Loss: 0.00057695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057695
Iteration 2/1000 | Loss: 0.00003112
Iteration 3/1000 | Loss: 0.00002172
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001589
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001470
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001441
Iteration 17/1000 | Loss: 0.00001440
Iteration 18/1000 | Loss: 0.00001440
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001439
Iteration 22/1000 | Loss: 0.00001439
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001438
Iteration 25/1000 | Loss: 0.00001438
Iteration 26/1000 | Loss: 0.00001438
Iteration 27/1000 | Loss: 0.00001438
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001428
Iteration 38/1000 | Loss: 0.00001428
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001416
Iteration 45/1000 | Loss: 0.00001416
Iteration 46/1000 | Loss: 0.00001416
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001415
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001414
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001410
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001407
Iteration 71/1000 | Loss: 0.00001407
Iteration 72/1000 | Loss: 0.00001407
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001403
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001403
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.4029307749297004e-05, 1.4029307749297004e-05, 1.4029307749297004e-05, 1.4029307749297004e-05, 1.4029307749297004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4029307749297004e-05

Optimization complete. Final v2v error: 3.1901211738586426 mm

Highest mean error: 3.3302156925201416 mm for frame 52

Lowest mean error: 2.913280487060547 mm for frame 169

Saving results

Total time: 37.23498201370239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949372
Iteration 2/25 | Loss: 0.00272292
Iteration 3/25 | Loss: 0.00211794
Iteration 4/25 | Loss: 0.00200563
Iteration 5/25 | Loss: 0.00174555
Iteration 6/25 | Loss: 0.00153392
Iteration 7/25 | Loss: 0.00146474
Iteration 8/25 | Loss: 0.00148371
Iteration 9/25 | Loss: 0.00141693
Iteration 10/25 | Loss: 0.00134293
Iteration 11/25 | Loss: 0.00132822
Iteration 12/25 | Loss: 0.00131013
Iteration 13/25 | Loss: 0.00130831
Iteration 14/25 | Loss: 0.00130296
Iteration 15/25 | Loss: 0.00130175
Iteration 16/25 | Loss: 0.00130069
Iteration 17/25 | Loss: 0.00130051
Iteration 18/25 | Loss: 0.00130045
Iteration 19/25 | Loss: 0.00130068
Iteration 20/25 | Loss: 0.00130080
Iteration 21/25 | Loss: 0.00130072
Iteration 22/25 | Loss: 0.00130091
Iteration 23/25 | Loss: 0.00130071
Iteration 24/25 | Loss: 0.00130068
Iteration 25/25 | Loss: 0.00130024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37317657
Iteration 2/25 | Loss: 0.00171516
Iteration 3/25 | Loss: 0.00128935
Iteration 4/25 | Loss: 0.00128934
Iteration 5/25 | Loss: 0.00128934
Iteration 6/25 | Loss: 0.00128934
Iteration 7/25 | Loss: 0.00128934
Iteration 8/25 | Loss: 0.00128934
Iteration 9/25 | Loss: 0.00128934
Iteration 10/25 | Loss: 0.00128934
Iteration 11/25 | Loss: 0.00128934
Iteration 12/25 | Loss: 0.00128934
Iteration 13/25 | Loss: 0.00128934
Iteration 14/25 | Loss: 0.00128934
Iteration 15/25 | Loss: 0.00128934
Iteration 16/25 | Loss: 0.00128934
Iteration 17/25 | Loss: 0.00128934
Iteration 18/25 | Loss: 0.00128934
Iteration 19/25 | Loss: 0.00128934
Iteration 20/25 | Loss: 0.00128934
Iteration 21/25 | Loss: 0.00128934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012893383391201496, 0.0012893383391201496, 0.0012893383391201496, 0.0012893383391201496, 0.0012893383391201496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012893383391201496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128934
Iteration 2/1000 | Loss: 0.00143254
Iteration 3/1000 | Loss: 0.00049795
Iteration 4/1000 | Loss: 0.00022734
Iteration 5/1000 | Loss: 0.00037114
Iteration 6/1000 | Loss: 0.00072446
Iteration 7/1000 | Loss: 0.00077912
Iteration 8/1000 | Loss: 0.00046315
Iteration 9/1000 | Loss: 0.00013884
Iteration 10/1000 | Loss: 0.00016259
Iteration 11/1000 | Loss: 0.00018335
Iteration 12/1000 | Loss: 0.00019848
Iteration 13/1000 | Loss: 0.00017089
Iteration 14/1000 | Loss: 0.00011047
Iteration 15/1000 | Loss: 0.00009171
Iteration 16/1000 | Loss: 0.00005275
Iteration 17/1000 | Loss: 0.00004858
Iteration 18/1000 | Loss: 0.00004580
Iteration 19/1000 | Loss: 0.00004255
Iteration 20/1000 | Loss: 0.00004009
Iteration 21/1000 | Loss: 0.00003787
Iteration 22/1000 | Loss: 0.00033695
Iteration 23/1000 | Loss: 0.00050414
Iteration 24/1000 | Loss: 0.00088625
Iteration 25/1000 | Loss: 0.00004695
Iteration 26/1000 | Loss: 0.00003631
Iteration 27/1000 | Loss: 0.00036538
Iteration 28/1000 | Loss: 0.00002990
Iteration 29/1000 | Loss: 0.00002807
Iteration 30/1000 | Loss: 0.00002676
Iteration 31/1000 | Loss: 0.00002588
Iteration 32/1000 | Loss: 0.00002505
Iteration 33/1000 | Loss: 0.00002457
Iteration 34/1000 | Loss: 0.00002426
Iteration 35/1000 | Loss: 0.00002400
Iteration 36/1000 | Loss: 0.00002382
Iteration 37/1000 | Loss: 0.00002379
Iteration 38/1000 | Loss: 0.00002378
Iteration 39/1000 | Loss: 0.00002377
Iteration 40/1000 | Loss: 0.00002376
Iteration 41/1000 | Loss: 0.00002376
Iteration 42/1000 | Loss: 0.00002373
Iteration 43/1000 | Loss: 0.00002366
Iteration 44/1000 | Loss: 0.00002358
Iteration 45/1000 | Loss: 0.00002355
Iteration 46/1000 | Loss: 0.00004153
Iteration 47/1000 | Loss: 0.00002557
Iteration 48/1000 | Loss: 0.00002469
Iteration 49/1000 | Loss: 0.00002392
Iteration 50/1000 | Loss: 0.00002349
Iteration 51/1000 | Loss: 0.00002301
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002240
Iteration 54/1000 | Loss: 0.00002233
Iteration 55/1000 | Loss: 0.00002230
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002220
Iteration 58/1000 | Loss: 0.00002203
Iteration 59/1000 | Loss: 0.00002203
Iteration 60/1000 | Loss: 0.00002203
Iteration 61/1000 | Loss: 0.00002203
Iteration 62/1000 | Loss: 0.00002203
Iteration 63/1000 | Loss: 0.00002203
Iteration 64/1000 | Loss: 0.00002202
Iteration 65/1000 | Loss: 0.00002202
Iteration 66/1000 | Loss: 0.00002200
Iteration 67/1000 | Loss: 0.00002199
Iteration 68/1000 | Loss: 0.00002199
Iteration 69/1000 | Loss: 0.00002199
Iteration 70/1000 | Loss: 0.00002198
Iteration 71/1000 | Loss: 0.00002198
Iteration 72/1000 | Loss: 0.00002197
Iteration 73/1000 | Loss: 0.00002197
Iteration 74/1000 | Loss: 0.00002195
Iteration 75/1000 | Loss: 0.00002194
Iteration 76/1000 | Loss: 0.00002194
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002193
Iteration 79/1000 | Loss: 0.00002192
Iteration 80/1000 | Loss: 0.00002192
Iteration 81/1000 | Loss: 0.00002191
Iteration 82/1000 | Loss: 0.00002191
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002190
Iteration 86/1000 | Loss: 0.00002189
Iteration 87/1000 | Loss: 0.00002189
Iteration 88/1000 | Loss: 0.00002189
Iteration 89/1000 | Loss: 0.00002189
Iteration 90/1000 | Loss: 0.00002188
Iteration 91/1000 | Loss: 0.00002188
Iteration 92/1000 | Loss: 0.00002187
Iteration 93/1000 | Loss: 0.00002187
Iteration 94/1000 | Loss: 0.00002187
Iteration 95/1000 | Loss: 0.00002187
Iteration 96/1000 | Loss: 0.00002186
Iteration 97/1000 | Loss: 0.00002186
Iteration 98/1000 | Loss: 0.00002186
Iteration 99/1000 | Loss: 0.00002186
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002185
Iteration 110/1000 | Loss: 0.00002185
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002184
Iteration 113/1000 | Loss: 0.00002184
Iteration 114/1000 | Loss: 0.00002184
Iteration 115/1000 | Loss: 0.00002184
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002184
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002184
Iteration 121/1000 | Loss: 0.00002184
Iteration 122/1000 | Loss: 0.00002184
Iteration 123/1000 | Loss: 0.00002184
Iteration 124/1000 | Loss: 0.00002184
Iteration 125/1000 | Loss: 0.00002184
Iteration 126/1000 | Loss: 0.00002184
Iteration 127/1000 | Loss: 0.00002184
Iteration 128/1000 | Loss: 0.00002184
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002184
Iteration 132/1000 | Loss: 0.00002184
Iteration 133/1000 | Loss: 0.00002184
Iteration 134/1000 | Loss: 0.00002184
Iteration 135/1000 | Loss: 0.00002184
Iteration 136/1000 | Loss: 0.00002184
Iteration 137/1000 | Loss: 0.00002184
Iteration 138/1000 | Loss: 0.00002184
Iteration 139/1000 | Loss: 0.00002184
Iteration 140/1000 | Loss: 0.00002184
Iteration 141/1000 | Loss: 0.00002184
Iteration 142/1000 | Loss: 0.00002184
Iteration 143/1000 | Loss: 0.00002184
Iteration 144/1000 | Loss: 0.00002184
Iteration 145/1000 | Loss: 0.00002184
Iteration 146/1000 | Loss: 0.00002184
Iteration 147/1000 | Loss: 0.00002184
Iteration 148/1000 | Loss: 0.00002184
Iteration 149/1000 | Loss: 0.00002184
Iteration 150/1000 | Loss: 0.00002184
Iteration 151/1000 | Loss: 0.00002184
Iteration 152/1000 | Loss: 0.00002184
Iteration 153/1000 | Loss: 0.00002184
Iteration 154/1000 | Loss: 0.00002184
Iteration 155/1000 | Loss: 0.00002184
Iteration 156/1000 | Loss: 0.00002184
Iteration 157/1000 | Loss: 0.00002184
Iteration 158/1000 | Loss: 0.00002184
Iteration 159/1000 | Loss: 0.00002184
Iteration 160/1000 | Loss: 0.00002184
Iteration 161/1000 | Loss: 0.00002184
Iteration 162/1000 | Loss: 0.00002184
Iteration 163/1000 | Loss: 0.00002184
Iteration 164/1000 | Loss: 0.00002184
Iteration 165/1000 | Loss: 0.00002184
Iteration 166/1000 | Loss: 0.00002184
Iteration 167/1000 | Loss: 0.00002184
Iteration 168/1000 | Loss: 0.00002184
Iteration 169/1000 | Loss: 0.00002184
Iteration 170/1000 | Loss: 0.00002184
Iteration 171/1000 | Loss: 0.00002184
Iteration 172/1000 | Loss: 0.00002184
Iteration 173/1000 | Loss: 0.00002184
Iteration 174/1000 | Loss: 0.00002184
Iteration 175/1000 | Loss: 0.00002184
Iteration 176/1000 | Loss: 0.00002184
Iteration 177/1000 | Loss: 0.00002184
Iteration 178/1000 | Loss: 0.00002184
Iteration 179/1000 | Loss: 0.00002184
Iteration 180/1000 | Loss: 0.00002184
Iteration 181/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.183766628149897e-05, 2.183766628149897e-05, 2.183766628149897e-05, 2.183766628149897e-05, 2.183766628149897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.183766628149897e-05

Optimization complete. Final v2v error: 3.892714738845825 mm

Highest mean error: 9.408199310302734 mm for frame 91

Lowest mean error: 3.65824294090271 mm for frame 100

Saving results

Total time: 122.97752237319946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714042
Iteration 2/25 | Loss: 0.00139464
Iteration 3/25 | Loss: 0.00117188
Iteration 4/25 | Loss: 0.00114824
Iteration 5/25 | Loss: 0.00114027
Iteration 6/25 | Loss: 0.00113931
Iteration 7/25 | Loss: 0.00113380
Iteration 8/25 | Loss: 0.00112495
Iteration 9/25 | Loss: 0.00112070
Iteration 10/25 | Loss: 0.00111753
Iteration 11/25 | Loss: 0.00111929
Iteration 12/25 | Loss: 0.00110961
Iteration 13/25 | Loss: 0.00110839
Iteration 14/25 | Loss: 0.00110823
Iteration 15/25 | Loss: 0.00110822
Iteration 16/25 | Loss: 0.00110822
Iteration 17/25 | Loss: 0.00110822
Iteration 18/25 | Loss: 0.00110822
Iteration 19/25 | Loss: 0.00110822
Iteration 20/25 | Loss: 0.00110822
Iteration 21/25 | Loss: 0.00110822
Iteration 22/25 | Loss: 0.00110822
Iteration 23/25 | Loss: 0.00110821
Iteration 24/25 | Loss: 0.00110821
Iteration 25/25 | Loss: 0.00110821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38840616
Iteration 2/25 | Loss: 0.00077236
Iteration 3/25 | Loss: 0.00077235
Iteration 4/25 | Loss: 0.00077235
Iteration 5/25 | Loss: 0.00077235
Iteration 6/25 | Loss: 0.00077235
Iteration 7/25 | Loss: 0.00077235
Iteration 8/25 | Loss: 0.00077235
Iteration 9/25 | Loss: 0.00077235
Iteration 10/25 | Loss: 0.00077235
Iteration 11/25 | Loss: 0.00077235
Iteration 12/25 | Loss: 0.00077235
Iteration 13/25 | Loss: 0.00077235
Iteration 14/25 | Loss: 0.00077235
Iteration 15/25 | Loss: 0.00077235
Iteration 16/25 | Loss: 0.00077235
Iteration 17/25 | Loss: 0.00077235
Iteration 18/25 | Loss: 0.00077235
Iteration 19/25 | Loss: 0.00077235
Iteration 20/25 | Loss: 0.00077235
Iteration 21/25 | Loss: 0.00077235
Iteration 22/25 | Loss: 0.00077235
Iteration 23/25 | Loss: 0.00077235
Iteration 24/25 | Loss: 0.00077235
Iteration 25/25 | Loss: 0.00077235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077235
Iteration 2/1000 | Loss: 0.00004760
Iteration 3/1000 | Loss: 0.00003401
Iteration 4/1000 | Loss: 0.00002972
Iteration 5/1000 | Loss: 0.00002807
Iteration 6/1000 | Loss: 0.00002678
Iteration 7/1000 | Loss: 0.00002606
Iteration 8/1000 | Loss: 0.00002553
Iteration 9/1000 | Loss: 0.00002513
Iteration 10/1000 | Loss: 0.00002482
Iteration 11/1000 | Loss: 0.00002460
Iteration 12/1000 | Loss: 0.00002444
Iteration 13/1000 | Loss: 0.00002432
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002417
Iteration 16/1000 | Loss: 0.00002416
Iteration 17/1000 | Loss: 0.00002413
Iteration 18/1000 | Loss: 0.00002402
Iteration 19/1000 | Loss: 0.00002402
Iteration 20/1000 | Loss: 0.00002401
Iteration 21/1000 | Loss: 0.00002396
Iteration 22/1000 | Loss: 0.00002395
Iteration 23/1000 | Loss: 0.00002393
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002386
Iteration 26/1000 | Loss: 0.00002385
Iteration 27/1000 | Loss: 0.00002384
Iteration 28/1000 | Loss: 0.00002380
Iteration 29/1000 | Loss: 0.00002369
Iteration 30/1000 | Loss: 0.00002365
Iteration 31/1000 | Loss: 0.00002365
Iteration 32/1000 | Loss: 0.00002363
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00002346
Iteration 37/1000 | Loss: 0.00002345
Iteration 38/1000 | Loss: 0.00002343
Iteration 39/1000 | Loss: 0.00002342
Iteration 40/1000 | Loss: 0.00002341
Iteration 41/1000 | Loss: 0.00002341
Iteration 42/1000 | Loss: 0.00002341
Iteration 43/1000 | Loss: 0.00002340
Iteration 44/1000 | Loss: 0.00002337
Iteration 45/1000 | Loss: 0.00002337
Iteration 46/1000 | Loss: 0.00002336
Iteration 47/1000 | Loss: 0.00002335
Iteration 48/1000 | Loss: 0.00002335
Iteration 49/1000 | Loss: 0.00002335
Iteration 50/1000 | Loss: 0.00002335
Iteration 51/1000 | Loss: 0.00002334
Iteration 52/1000 | Loss: 0.00002333
Iteration 53/1000 | Loss: 0.00002333
Iteration 54/1000 | Loss: 0.00002333
Iteration 55/1000 | Loss: 0.00002332
Iteration 56/1000 | Loss: 0.00002332
Iteration 57/1000 | Loss: 0.00002331
Iteration 58/1000 | Loss: 0.00002331
Iteration 59/1000 | Loss: 0.00002331
Iteration 60/1000 | Loss: 0.00002331
Iteration 61/1000 | Loss: 0.00002331
Iteration 62/1000 | Loss: 0.00002331
Iteration 63/1000 | Loss: 0.00002330
Iteration 64/1000 | Loss: 0.00002330
Iteration 65/1000 | Loss: 0.00002330
Iteration 66/1000 | Loss: 0.00002330
Iteration 67/1000 | Loss: 0.00002330
Iteration 68/1000 | Loss: 0.00002329
Iteration 69/1000 | Loss: 0.00002329
Iteration 70/1000 | Loss: 0.00002329
Iteration 71/1000 | Loss: 0.00002329
Iteration 72/1000 | Loss: 0.00002328
Iteration 73/1000 | Loss: 0.00002328
Iteration 74/1000 | Loss: 0.00002328
Iteration 75/1000 | Loss: 0.00002327
Iteration 76/1000 | Loss: 0.00002327
Iteration 77/1000 | Loss: 0.00002327
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002324
Iteration 84/1000 | Loss: 0.00002324
Iteration 85/1000 | Loss: 0.00002324
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002324
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002322
Iteration 98/1000 | Loss: 0.00002322
Iteration 99/1000 | Loss: 0.00002321
Iteration 100/1000 | Loss: 0.00002321
Iteration 101/1000 | Loss: 0.00002321
Iteration 102/1000 | Loss: 0.00002321
Iteration 103/1000 | Loss: 0.00002320
Iteration 104/1000 | Loss: 0.00002320
Iteration 105/1000 | Loss: 0.00002320
Iteration 106/1000 | Loss: 0.00002320
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002320
Iteration 112/1000 | Loss: 0.00002320
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002319
Iteration 115/1000 | Loss: 0.00002319
Iteration 116/1000 | Loss: 0.00002319
Iteration 117/1000 | Loss: 0.00002319
Iteration 118/1000 | Loss: 0.00002319
Iteration 119/1000 | Loss: 0.00002319
Iteration 120/1000 | Loss: 0.00002319
Iteration 121/1000 | Loss: 0.00002319
Iteration 122/1000 | Loss: 0.00002319
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002319
Iteration 128/1000 | Loss: 0.00002319
Iteration 129/1000 | Loss: 0.00002319
Iteration 130/1000 | Loss: 0.00002319
Iteration 131/1000 | Loss: 0.00002319
Iteration 132/1000 | Loss: 0.00002319
Iteration 133/1000 | Loss: 0.00002319
Iteration 134/1000 | Loss: 0.00002319
Iteration 135/1000 | Loss: 0.00002319
Iteration 136/1000 | Loss: 0.00002319
Iteration 137/1000 | Loss: 0.00002319
Iteration 138/1000 | Loss: 0.00002319
Iteration 139/1000 | Loss: 0.00002319
Iteration 140/1000 | Loss: 0.00002319
Iteration 141/1000 | Loss: 0.00002319
Iteration 142/1000 | Loss: 0.00002319
Iteration 143/1000 | Loss: 0.00002319
Iteration 144/1000 | Loss: 0.00002319
Iteration 145/1000 | Loss: 0.00002319
Iteration 146/1000 | Loss: 0.00002319
Iteration 147/1000 | Loss: 0.00002319
Iteration 148/1000 | Loss: 0.00002319
Iteration 149/1000 | Loss: 0.00002319
Iteration 150/1000 | Loss: 0.00002319
Iteration 151/1000 | Loss: 0.00002319
Iteration 152/1000 | Loss: 0.00002319
Iteration 153/1000 | Loss: 0.00002319
Iteration 154/1000 | Loss: 0.00002319
Iteration 155/1000 | Loss: 0.00002319
Iteration 156/1000 | Loss: 0.00002319
Iteration 157/1000 | Loss: 0.00002319
Iteration 158/1000 | Loss: 0.00002319
Iteration 159/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.3193193555925973e-05, 2.3193193555925973e-05, 2.3193193555925973e-05, 2.3193193555925973e-05, 2.3193193555925973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3193193555925973e-05

Optimization complete. Final v2v error: 3.468194007873535 mm

Highest mean error: 12.528676986694336 mm for frame 178

Lowest mean error: 2.6078412532806396 mm for frame 188

Saving results

Total time: 69.50300788879395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389797
Iteration 2/25 | Loss: 0.00113419
Iteration 3/25 | Loss: 0.00102462
Iteration 4/25 | Loss: 0.00101215
Iteration 5/25 | Loss: 0.00100953
Iteration 6/25 | Loss: 0.00100923
Iteration 7/25 | Loss: 0.00100923
Iteration 8/25 | Loss: 0.00100923
Iteration 9/25 | Loss: 0.00100923
Iteration 10/25 | Loss: 0.00100923
Iteration 11/25 | Loss: 0.00100923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001009229919873178, 0.001009229919873178, 0.001009229919873178, 0.001009229919873178, 0.001009229919873178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001009229919873178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38411486
Iteration 2/25 | Loss: 0.00071051
Iteration 3/25 | Loss: 0.00071051
Iteration 4/25 | Loss: 0.00071051
Iteration 5/25 | Loss: 0.00071051
Iteration 6/25 | Loss: 0.00071051
Iteration 7/25 | Loss: 0.00071051
Iteration 8/25 | Loss: 0.00071050
Iteration 9/25 | Loss: 0.00071050
Iteration 10/25 | Loss: 0.00071050
Iteration 11/25 | Loss: 0.00071050
Iteration 12/25 | Loss: 0.00071050
Iteration 13/25 | Loss: 0.00071050
Iteration 14/25 | Loss: 0.00071050
Iteration 15/25 | Loss: 0.00071050
Iteration 16/25 | Loss: 0.00071050
Iteration 17/25 | Loss: 0.00071050
Iteration 18/25 | Loss: 0.00071050
Iteration 19/25 | Loss: 0.00071050
Iteration 20/25 | Loss: 0.00071050
Iteration 21/25 | Loss: 0.00071050
Iteration 22/25 | Loss: 0.00071050
Iteration 23/25 | Loss: 0.00071050
Iteration 24/25 | Loss: 0.00071050
Iteration 25/25 | Loss: 0.00071050

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071050
Iteration 2/1000 | Loss: 0.00002707
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001520
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001333
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001276
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001219
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001180
Iteration 37/1000 | Loss: 0.00001180
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001178
Iteration 47/1000 | Loss: 0.00001178
Iteration 48/1000 | Loss: 0.00001178
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001177
Iteration 51/1000 | Loss: 0.00001177
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001176
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001176
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001166
Iteration 119/1000 | Loss: 0.00001166
Iteration 120/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.166050697065657e-05, 1.166050697065657e-05, 1.166050697065657e-05, 1.166050697065657e-05, 1.166050697065657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.166050697065657e-05

Optimization complete. Final v2v error: 2.8089733123779297 mm

Highest mean error: 3.099423885345459 mm for frame 21

Lowest mean error: 2.4522666931152344 mm for frame 103

Saving results

Total time: 34.2905535697937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816562
Iteration 2/25 | Loss: 0.00133138
Iteration 3/25 | Loss: 0.00102349
Iteration 4/25 | Loss: 0.00099821
Iteration 5/25 | Loss: 0.00099560
Iteration 6/25 | Loss: 0.00099493
Iteration 7/25 | Loss: 0.00099493
Iteration 8/25 | Loss: 0.00099493
Iteration 9/25 | Loss: 0.00099493
Iteration 10/25 | Loss: 0.00099493
Iteration 11/25 | Loss: 0.00099493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000994930393062532, 0.000994930393062532, 0.000994930393062532, 0.000994930393062532, 0.000994930393062532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000994930393062532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37923145
Iteration 2/25 | Loss: 0.00068468
Iteration 3/25 | Loss: 0.00068468
Iteration 4/25 | Loss: 0.00068468
Iteration 5/25 | Loss: 0.00068468
Iteration 6/25 | Loss: 0.00068468
Iteration 7/25 | Loss: 0.00068468
Iteration 8/25 | Loss: 0.00068468
Iteration 9/25 | Loss: 0.00068468
Iteration 10/25 | Loss: 0.00068468
Iteration 11/25 | Loss: 0.00068468
Iteration 12/25 | Loss: 0.00068468
Iteration 13/25 | Loss: 0.00068468
Iteration 14/25 | Loss: 0.00068468
Iteration 15/25 | Loss: 0.00068468
Iteration 16/25 | Loss: 0.00068468
Iteration 17/25 | Loss: 0.00068468
Iteration 18/25 | Loss: 0.00068468
Iteration 19/25 | Loss: 0.00068468
Iteration 20/25 | Loss: 0.00068468
Iteration 21/25 | Loss: 0.00068468
Iteration 22/25 | Loss: 0.00068468
Iteration 23/25 | Loss: 0.00068468
Iteration 24/25 | Loss: 0.00068468
Iteration 25/25 | Loss: 0.00068468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068468
Iteration 2/1000 | Loss: 0.00002359
Iteration 3/1000 | Loss: 0.00001361
Iteration 4/1000 | Loss: 0.00001169
Iteration 5/1000 | Loss: 0.00001072
Iteration 6/1000 | Loss: 0.00001022
Iteration 7/1000 | Loss: 0.00000998
Iteration 8/1000 | Loss: 0.00000968
Iteration 9/1000 | Loss: 0.00000946
Iteration 10/1000 | Loss: 0.00000930
Iteration 11/1000 | Loss: 0.00000928
Iteration 12/1000 | Loss: 0.00000923
Iteration 13/1000 | Loss: 0.00000918
Iteration 14/1000 | Loss: 0.00000911
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000910
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000909
Iteration 19/1000 | Loss: 0.00000908
Iteration 20/1000 | Loss: 0.00000907
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000904
Iteration 24/1000 | Loss: 0.00000904
Iteration 25/1000 | Loss: 0.00000903
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000903
Iteration 28/1000 | Loss: 0.00000902
Iteration 29/1000 | Loss: 0.00000902
Iteration 30/1000 | Loss: 0.00000902
Iteration 31/1000 | Loss: 0.00000901
Iteration 32/1000 | Loss: 0.00000901
Iteration 33/1000 | Loss: 0.00000900
Iteration 34/1000 | Loss: 0.00000900
Iteration 35/1000 | Loss: 0.00000900
Iteration 36/1000 | Loss: 0.00000900
Iteration 37/1000 | Loss: 0.00000899
Iteration 38/1000 | Loss: 0.00000899
Iteration 39/1000 | Loss: 0.00000899
Iteration 40/1000 | Loss: 0.00000899
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000898
Iteration 43/1000 | Loss: 0.00000898
Iteration 44/1000 | Loss: 0.00000897
Iteration 45/1000 | Loss: 0.00000897
Iteration 46/1000 | Loss: 0.00000894
Iteration 47/1000 | Loss: 0.00000893
Iteration 48/1000 | Loss: 0.00000893
Iteration 49/1000 | Loss: 0.00000893
Iteration 50/1000 | Loss: 0.00000892
Iteration 51/1000 | Loss: 0.00000892
Iteration 52/1000 | Loss: 0.00000892
Iteration 53/1000 | Loss: 0.00000891
Iteration 54/1000 | Loss: 0.00000891
Iteration 55/1000 | Loss: 0.00000891
Iteration 56/1000 | Loss: 0.00000891
Iteration 57/1000 | Loss: 0.00000891
Iteration 58/1000 | Loss: 0.00000891
Iteration 59/1000 | Loss: 0.00000890
Iteration 60/1000 | Loss: 0.00000890
Iteration 61/1000 | Loss: 0.00000890
Iteration 62/1000 | Loss: 0.00000890
Iteration 63/1000 | Loss: 0.00000890
Iteration 64/1000 | Loss: 0.00000890
Iteration 65/1000 | Loss: 0.00000890
Iteration 66/1000 | Loss: 0.00000890
Iteration 67/1000 | Loss: 0.00000890
Iteration 68/1000 | Loss: 0.00000889
Iteration 69/1000 | Loss: 0.00000889
Iteration 70/1000 | Loss: 0.00000889
Iteration 71/1000 | Loss: 0.00000889
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000889
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000889
Iteration 77/1000 | Loss: 0.00000889
Iteration 78/1000 | Loss: 0.00000889
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000888
Iteration 83/1000 | Loss: 0.00000888
Iteration 84/1000 | Loss: 0.00000888
Iteration 85/1000 | Loss: 0.00000888
Iteration 86/1000 | Loss: 0.00000888
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000887
Iteration 95/1000 | Loss: 0.00000887
Iteration 96/1000 | Loss: 0.00000887
Iteration 97/1000 | Loss: 0.00000887
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000887
Iteration 101/1000 | Loss: 0.00000887
Iteration 102/1000 | Loss: 0.00000887
Iteration 103/1000 | Loss: 0.00000887
Iteration 104/1000 | Loss: 0.00000886
Iteration 105/1000 | Loss: 0.00000886
Iteration 106/1000 | Loss: 0.00000886
Iteration 107/1000 | Loss: 0.00000886
Iteration 108/1000 | Loss: 0.00000886
Iteration 109/1000 | Loss: 0.00000886
Iteration 110/1000 | Loss: 0.00000885
Iteration 111/1000 | Loss: 0.00000885
Iteration 112/1000 | Loss: 0.00000885
Iteration 113/1000 | Loss: 0.00000885
Iteration 114/1000 | Loss: 0.00000885
Iteration 115/1000 | Loss: 0.00000885
Iteration 116/1000 | Loss: 0.00000885
Iteration 117/1000 | Loss: 0.00000885
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000884
Iteration 121/1000 | Loss: 0.00000884
Iteration 122/1000 | Loss: 0.00000884
Iteration 123/1000 | Loss: 0.00000884
Iteration 124/1000 | Loss: 0.00000884
Iteration 125/1000 | Loss: 0.00000884
Iteration 126/1000 | Loss: 0.00000884
Iteration 127/1000 | Loss: 0.00000884
Iteration 128/1000 | Loss: 0.00000884
Iteration 129/1000 | Loss: 0.00000884
Iteration 130/1000 | Loss: 0.00000884
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000883
Iteration 134/1000 | Loss: 0.00000883
Iteration 135/1000 | Loss: 0.00000883
Iteration 136/1000 | Loss: 0.00000883
Iteration 137/1000 | Loss: 0.00000883
Iteration 138/1000 | Loss: 0.00000883
Iteration 139/1000 | Loss: 0.00000883
Iteration 140/1000 | Loss: 0.00000883
Iteration 141/1000 | Loss: 0.00000883
Iteration 142/1000 | Loss: 0.00000883
Iteration 143/1000 | Loss: 0.00000882
Iteration 144/1000 | Loss: 0.00000882
Iteration 145/1000 | Loss: 0.00000882
Iteration 146/1000 | Loss: 0.00000882
Iteration 147/1000 | Loss: 0.00000882
Iteration 148/1000 | Loss: 0.00000882
Iteration 149/1000 | Loss: 0.00000882
Iteration 150/1000 | Loss: 0.00000882
Iteration 151/1000 | Loss: 0.00000882
Iteration 152/1000 | Loss: 0.00000882
Iteration 153/1000 | Loss: 0.00000882
Iteration 154/1000 | Loss: 0.00000881
Iteration 155/1000 | Loss: 0.00000881
Iteration 156/1000 | Loss: 0.00000881
Iteration 157/1000 | Loss: 0.00000881
Iteration 158/1000 | Loss: 0.00000881
Iteration 159/1000 | Loss: 0.00000881
Iteration 160/1000 | Loss: 0.00000881
Iteration 161/1000 | Loss: 0.00000881
Iteration 162/1000 | Loss: 0.00000881
Iteration 163/1000 | Loss: 0.00000881
Iteration 164/1000 | Loss: 0.00000881
Iteration 165/1000 | Loss: 0.00000880
Iteration 166/1000 | Loss: 0.00000880
Iteration 167/1000 | Loss: 0.00000880
Iteration 168/1000 | Loss: 0.00000880
Iteration 169/1000 | Loss: 0.00000880
Iteration 170/1000 | Loss: 0.00000880
Iteration 171/1000 | Loss: 0.00000880
Iteration 172/1000 | Loss: 0.00000880
Iteration 173/1000 | Loss: 0.00000880
Iteration 174/1000 | Loss: 0.00000880
Iteration 175/1000 | Loss: 0.00000879
Iteration 176/1000 | Loss: 0.00000879
Iteration 177/1000 | Loss: 0.00000879
Iteration 178/1000 | Loss: 0.00000879
Iteration 179/1000 | Loss: 0.00000879
Iteration 180/1000 | Loss: 0.00000879
Iteration 181/1000 | Loss: 0.00000879
Iteration 182/1000 | Loss: 0.00000879
Iteration 183/1000 | Loss: 0.00000879
Iteration 184/1000 | Loss: 0.00000879
Iteration 185/1000 | Loss: 0.00000879
Iteration 186/1000 | Loss: 0.00000879
Iteration 187/1000 | Loss: 0.00000879
Iteration 188/1000 | Loss: 0.00000879
Iteration 189/1000 | Loss: 0.00000879
Iteration 190/1000 | Loss: 0.00000879
Iteration 191/1000 | Loss: 0.00000879
Iteration 192/1000 | Loss: 0.00000879
Iteration 193/1000 | Loss: 0.00000879
Iteration 194/1000 | Loss: 0.00000879
Iteration 195/1000 | Loss: 0.00000879
Iteration 196/1000 | Loss: 0.00000879
Iteration 197/1000 | Loss: 0.00000879
Iteration 198/1000 | Loss: 0.00000879
Iteration 199/1000 | Loss: 0.00000879
Iteration 200/1000 | Loss: 0.00000879
Iteration 201/1000 | Loss: 0.00000879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [8.790550964477006e-06, 8.790550964477006e-06, 8.790550964477006e-06, 8.790550964477006e-06, 8.790550964477006e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.790550964477006e-06

Optimization complete. Final v2v error: 2.550391912460327 mm

Highest mean error: 2.826798915863037 mm for frame 58

Lowest mean error: 2.4316208362579346 mm for frame 4

Saving results

Total time: 36.90778923034668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803466
Iteration 2/25 | Loss: 0.00125443
Iteration 3/25 | Loss: 0.00114594
Iteration 4/25 | Loss: 0.00113230
Iteration 5/25 | Loss: 0.00112745
Iteration 6/25 | Loss: 0.00112634
Iteration 7/25 | Loss: 0.00112634
Iteration 8/25 | Loss: 0.00112634
Iteration 9/25 | Loss: 0.00112634
Iteration 10/25 | Loss: 0.00112634
Iteration 11/25 | Loss: 0.00112634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011263438500463963, 0.0011263438500463963, 0.0011263438500463963, 0.0011263438500463963, 0.0011263438500463963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011263438500463963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22980440
Iteration 2/25 | Loss: 0.00064910
Iteration 3/25 | Loss: 0.00064908
Iteration 4/25 | Loss: 0.00064907
Iteration 5/25 | Loss: 0.00064907
Iteration 6/25 | Loss: 0.00064907
Iteration 7/25 | Loss: 0.00064907
Iteration 8/25 | Loss: 0.00064907
Iteration 9/25 | Loss: 0.00064907
Iteration 10/25 | Loss: 0.00064907
Iteration 11/25 | Loss: 0.00064907
Iteration 12/25 | Loss: 0.00064907
Iteration 13/25 | Loss: 0.00064907
Iteration 14/25 | Loss: 0.00064907
Iteration 15/25 | Loss: 0.00064907
Iteration 16/25 | Loss: 0.00064907
Iteration 17/25 | Loss: 0.00064907
Iteration 18/25 | Loss: 0.00064907
Iteration 19/25 | Loss: 0.00064907
Iteration 20/25 | Loss: 0.00064907
Iteration 21/25 | Loss: 0.00064907
Iteration 22/25 | Loss: 0.00064907
Iteration 23/25 | Loss: 0.00064907
Iteration 24/25 | Loss: 0.00064907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006490714731626213, 0.0006490714731626213, 0.0006490714731626213, 0.0006490714731626213, 0.0006490714731626213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006490714731626213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064907
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00002252
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001905
Iteration 9/1000 | Loss: 0.00001872
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001845
Iteration 14/1000 | Loss: 0.00001843
Iteration 15/1000 | Loss: 0.00001842
Iteration 16/1000 | Loss: 0.00001842
Iteration 17/1000 | Loss: 0.00001835
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001833
Iteration 20/1000 | Loss: 0.00001832
Iteration 21/1000 | Loss: 0.00001831
Iteration 22/1000 | Loss: 0.00001831
Iteration 23/1000 | Loss: 0.00001831
Iteration 24/1000 | Loss: 0.00001831
Iteration 25/1000 | Loss: 0.00001831
Iteration 26/1000 | Loss: 0.00001830
Iteration 27/1000 | Loss: 0.00001830
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001829
Iteration 30/1000 | Loss: 0.00001829
Iteration 31/1000 | Loss: 0.00001828
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001828
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001827
Iteration 37/1000 | Loss: 0.00001827
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001826
Iteration 40/1000 | Loss: 0.00001826
Iteration 41/1000 | Loss: 0.00001826
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001824
Iteration 47/1000 | Loss: 0.00001823
Iteration 48/1000 | Loss: 0.00001823
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001820
Iteration 53/1000 | Loss: 0.00001820
Iteration 54/1000 | Loss: 0.00001819
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001817
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001815
Iteration 61/1000 | Loss: 0.00001815
Iteration 62/1000 | Loss: 0.00001815
Iteration 63/1000 | Loss: 0.00001815
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001814
Iteration 66/1000 | Loss: 0.00001814
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001813
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001811
Iteration 74/1000 | Loss: 0.00001811
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00001811
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001811
Iteration 79/1000 | Loss: 0.00001811
Iteration 80/1000 | Loss: 0.00001811
Iteration 81/1000 | Loss: 0.00001811
Iteration 82/1000 | Loss: 0.00001811
Iteration 83/1000 | Loss: 0.00001811
Iteration 84/1000 | Loss: 0.00001810
Iteration 85/1000 | Loss: 0.00001810
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001809
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001808
Iteration 92/1000 | Loss: 0.00001808
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001805
Iteration 106/1000 | Loss: 0.00001805
Iteration 107/1000 | Loss: 0.00001805
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001804
Iteration 117/1000 | Loss: 0.00001804
Iteration 118/1000 | Loss: 0.00001803
Iteration 119/1000 | Loss: 0.00001803
Iteration 120/1000 | Loss: 0.00001803
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001802
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001800
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001799
Iteration 137/1000 | Loss: 0.00001799
Iteration 138/1000 | Loss: 0.00001799
Iteration 139/1000 | Loss: 0.00001799
Iteration 140/1000 | Loss: 0.00001799
Iteration 141/1000 | Loss: 0.00001799
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001798
Iteration 147/1000 | Loss: 0.00001798
Iteration 148/1000 | Loss: 0.00001798
Iteration 149/1000 | Loss: 0.00001798
Iteration 150/1000 | Loss: 0.00001798
Iteration 151/1000 | Loss: 0.00001798
Iteration 152/1000 | Loss: 0.00001798
Iteration 153/1000 | Loss: 0.00001798
Iteration 154/1000 | Loss: 0.00001798
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001798
Iteration 157/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.7980655684368685e-05, 1.7980655684368685e-05, 1.7980655684368685e-05, 1.7980655684368685e-05, 1.7980655684368685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7980655684368685e-05

Optimization complete. Final v2v error: 3.549175977706909 mm

Highest mean error: 4.430004596710205 mm for frame 52

Lowest mean error: 3.0480093955993652 mm for frame 220

Saving results

Total time: 41.04488468170166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463138
Iteration 2/25 | Loss: 0.00109028
Iteration 3/25 | Loss: 0.00101343
Iteration 4/25 | Loss: 0.00100441
Iteration 5/25 | Loss: 0.00100278
Iteration 6/25 | Loss: 0.00100278
Iteration 7/25 | Loss: 0.00100278
Iteration 8/25 | Loss: 0.00100278
Iteration 9/25 | Loss: 0.00100278
Iteration 10/25 | Loss: 0.00100278
Iteration 11/25 | Loss: 0.00100278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010027849348261952, 0.0010027849348261952, 0.0010027849348261952, 0.0010027849348261952, 0.0010027849348261952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010027849348261952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.87033415
Iteration 2/25 | Loss: 0.00066538
Iteration 3/25 | Loss: 0.00066538
Iteration 4/25 | Loss: 0.00066538
Iteration 5/25 | Loss: 0.00066538
Iteration 6/25 | Loss: 0.00066538
Iteration 7/25 | Loss: 0.00066537
Iteration 8/25 | Loss: 0.00066537
Iteration 9/25 | Loss: 0.00066537
Iteration 10/25 | Loss: 0.00066537
Iteration 11/25 | Loss: 0.00066537
Iteration 12/25 | Loss: 0.00066537
Iteration 13/25 | Loss: 0.00066537
Iteration 14/25 | Loss: 0.00066537
Iteration 15/25 | Loss: 0.00066537
Iteration 16/25 | Loss: 0.00066537
Iteration 17/25 | Loss: 0.00066537
Iteration 18/25 | Loss: 0.00066537
Iteration 19/25 | Loss: 0.00066537
Iteration 20/25 | Loss: 0.00066537
Iteration 21/25 | Loss: 0.00066537
Iteration 22/25 | Loss: 0.00066537
Iteration 23/25 | Loss: 0.00066537
Iteration 24/25 | Loss: 0.00066537
Iteration 25/25 | Loss: 0.00066537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006653731106780469, 0.0006653731106780469, 0.0006653731106780469, 0.0006653731106780469, 0.0006653731106780469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006653731106780469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066537
Iteration 2/1000 | Loss: 0.00001812
Iteration 3/1000 | Loss: 0.00001406
Iteration 4/1000 | Loss: 0.00001301
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001160
Iteration 8/1000 | Loss: 0.00001134
Iteration 9/1000 | Loss: 0.00001121
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001101
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001091
Iteration 19/1000 | Loss: 0.00001091
Iteration 20/1000 | Loss: 0.00001089
Iteration 21/1000 | Loss: 0.00001082
Iteration 22/1000 | Loss: 0.00001079
Iteration 23/1000 | Loss: 0.00001078
Iteration 24/1000 | Loss: 0.00001078
Iteration 25/1000 | Loss: 0.00001077
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001075
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001067
Iteration 34/1000 | Loss: 0.00001067
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001063
Iteration 39/1000 | Loss: 0.00001063
Iteration 40/1000 | Loss: 0.00001062
Iteration 41/1000 | Loss: 0.00001062
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001060
Iteration 44/1000 | Loss: 0.00001060
Iteration 45/1000 | Loss: 0.00001059
Iteration 46/1000 | Loss: 0.00001059
Iteration 47/1000 | Loss: 0.00001059
Iteration 48/1000 | Loss: 0.00001059
Iteration 49/1000 | Loss: 0.00001058
Iteration 50/1000 | Loss: 0.00001058
Iteration 51/1000 | Loss: 0.00001057
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001055
Iteration 54/1000 | Loss: 0.00001055
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001054
Iteration 58/1000 | Loss: 0.00001054
Iteration 59/1000 | Loss: 0.00001053
Iteration 60/1000 | Loss: 0.00001053
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001052
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001051
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001050
Iteration 76/1000 | Loss: 0.00001049
Iteration 77/1000 | Loss: 0.00001049
Iteration 78/1000 | Loss: 0.00001049
Iteration 79/1000 | Loss: 0.00001048
Iteration 80/1000 | Loss: 0.00001048
Iteration 81/1000 | Loss: 0.00001048
Iteration 82/1000 | Loss: 0.00001047
Iteration 83/1000 | Loss: 0.00001047
Iteration 84/1000 | Loss: 0.00001047
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001044
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001043
Iteration 92/1000 | Loss: 0.00001043
Iteration 93/1000 | Loss: 0.00001043
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001043
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.0430338079459034e-05, 1.0430338079459034e-05, 1.0430338079459034e-05, 1.0430338079459034e-05, 1.0430338079459034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0430338079459034e-05

Optimization complete. Final v2v error: 2.7797110080718994 mm

Highest mean error: 2.9905970096588135 mm for frame 92

Lowest mean error: 2.6204051971435547 mm for frame 214

Saving results

Total time: 36.60070848464966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00681076
Iteration 2/25 | Loss: 0.00115526
Iteration 3/25 | Loss: 0.00102252
Iteration 4/25 | Loss: 0.00097543
Iteration 5/25 | Loss: 0.00096785
Iteration 6/25 | Loss: 0.00096631
Iteration 7/25 | Loss: 0.00096581
Iteration 8/25 | Loss: 0.00096566
Iteration 9/25 | Loss: 0.00096556
Iteration 10/25 | Loss: 0.00096555
Iteration 11/25 | Loss: 0.00096554
Iteration 12/25 | Loss: 0.00096554
Iteration 13/25 | Loss: 0.00096554
Iteration 14/25 | Loss: 0.00096554
Iteration 15/25 | Loss: 0.00096554
Iteration 16/25 | Loss: 0.00096554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009655420435592532, 0.0009655420435592532, 0.0009655420435592532, 0.0009655420435592532, 0.0009655420435592532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009655420435592532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32983303
Iteration 2/25 | Loss: 0.00078552
Iteration 3/25 | Loss: 0.00077323
Iteration 4/25 | Loss: 0.00077323
Iteration 5/25 | Loss: 0.00077323
Iteration 6/25 | Loss: 0.00077323
Iteration 7/25 | Loss: 0.00077323
Iteration 8/25 | Loss: 0.00077323
Iteration 9/25 | Loss: 0.00077323
Iteration 10/25 | Loss: 0.00077322
Iteration 11/25 | Loss: 0.00077322
Iteration 12/25 | Loss: 0.00077322
Iteration 13/25 | Loss: 0.00077322
Iteration 14/25 | Loss: 0.00077322
Iteration 15/25 | Loss: 0.00077322
Iteration 16/25 | Loss: 0.00077322
Iteration 17/25 | Loss: 0.00077322
Iteration 18/25 | Loss: 0.00077322
Iteration 19/25 | Loss: 0.00077322
Iteration 20/25 | Loss: 0.00077322
Iteration 21/25 | Loss: 0.00077322
Iteration 22/25 | Loss: 0.00077322
Iteration 23/25 | Loss: 0.00077322
Iteration 24/25 | Loss: 0.00077322
Iteration 25/25 | Loss: 0.00077322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077322
Iteration 2/1000 | Loss: 0.00001430
Iteration 3/1000 | Loss: 0.00001091
Iteration 4/1000 | Loss: 0.00001003
Iteration 5/1000 | Loss: 0.00000956
Iteration 6/1000 | Loss: 0.00000928
Iteration 7/1000 | Loss: 0.00000928
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000890
Iteration 10/1000 | Loss: 0.00000889
Iteration 11/1000 | Loss: 0.00000886
Iteration 12/1000 | Loss: 0.00000883
Iteration 13/1000 | Loss: 0.00000882
Iteration 14/1000 | Loss: 0.00000882
Iteration 15/1000 | Loss: 0.00000881
Iteration 16/1000 | Loss: 0.00000880
Iteration 17/1000 | Loss: 0.00000880
Iteration 18/1000 | Loss: 0.00000879
Iteration 19/1000 | Loss: 0.00000878
Iteration 20/1000 | Loss: 0.00000875
Iteration 21/1000 | Loss: 0.00000874
Iteration 22/1000 | Loss: 0.00000874
Iteration 23/1000 | Loss: 0.00000870
Iteration 24/1000 | Loss: 0.00000867
Iteration 25/1000 | Loss: 0.00000866
Iteration 26/1000 | Loss: 0.00000865
Iteration 27/1000 | Loss: 0.00000864
Iteration 28/1000 | Loss: 0.00000864
Iteration 29/1000 | Loss: 0.00000863
Iteration 30/1000 | Loss: 0.00000862
Iteration 31/1000 | Loss: 0.00000862
Iteration 32/1000 | Loss: 0.00000861
Iteration 33/1000 | Loss: 0.00000861
Iteration 34/1000 | Loss: 0.00000860
Iteration 35/1000 | Loss: 0.00000860
Iteration 36/1000 | Loss: 0.00000860
Iteration 37/1000 | Loss: 0.00000860
Iteration 38/1000 | Loss: 0.00000860
Iteration 39/1000 | Loss: 0.00000859
Iteration 40/1000 | Loss: 0.00000859
Iteration 41/1000 | Loss: 0.00000858
Iteration 42/1000 | Loss: 0.00000858
Iteration 43/1000 | Loss: 0.00000858
Iteration 44/1000 | Loss: 0.00000857
Iteration 45/1000 | Loss: 0.00000857
Iteration 46/1000 | Loss: 0.00000857
Iteration 47/1000 | Loss: 0.00000856
Iteration 48/1000 | Loss: 0.00000856
Iteration 49/1000 | Loss: 0.00000856
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000855
Iteration 52/1000 | Loss: 0.00000855
Iteration 53/1000 | Loss: 0.00000854
Iteration 54/1000 | Loss: 0.00000854
Iteration 55/1000 | Loss: 0.00000852
Iteration 56/1000 | Loss: 0.00000852
Iteration 57/1000 | Loss: 0.00000852
Iteration 58/1000 | Loss: 0.00000852
Iteration 59/1000 | Loss: 0.00000852
Iteration 60/1000 | Loss: 0.00000851
Iteration 61/1000 | Loss: 0.00000851
Iteration 62/1000 | Loss: 0.00000851
Iteration 63/1000 | Loss: 0.00000851
Iteration 64/1000 | Loss: 0.00000851
Iteration 65/1000 | Loss: 0.00000851
Iteration 66/1000 | Loss: 0.00000851
Iteration 67/1000 | Loss: 0.00000851
Iteration 68/1000 | Loss: 0.00000851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [8.512336535204668e-06, 8.512336535204668e-06, 8.512336535204668e-06, 8.512336535204668e-06, 8.512336535204668e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.512336535204668e-06

Optimization complete. Final v2v error: 2.527374744415283 mm

Highest mean error: 2.9333527088165283 mm for frame 180

Lowest mean error: 2.2892024517059326 mm for frame 21

Saving results

Total time: 37.091198682785034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497275
Iteration 2/25 | Loss: 0.00120431
Iteration 3/25 | Loss: 0.00106859
Iteration 4/25 | Loss: 0.00105678
Iteration 5/25 | Loss: 0.00105431
Iteration 6/25 | Loss: 0.00105338
Iteration 7/25 | Loss: 0.00105338
Iteration 8/25 | Loss: 0.00105338
Iteration 9/25 | Loss: 0.00105338
Iteration 10/25 | Loss: 0.00105338
Iteration 11/25 | Loss: 0.00105338
Iteration 12/25 | Loss: 0.00105338
Iteration 13/25 | Loss: 0.00105338
Iteration 14/25 | Loss: 0.00105338
Iteration 15/25 | Loss: 0.00105338
Iteration 16/25 | Loss: 0.00105338
Iteration 17/25 | Loss: 0.00105338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001053377753123641, 0.001053377753123641, 0.001053377753123641, 0.001053377753123641, 0.001053377753123641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001053377753123641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42010045
Iteration 2/25 | Loss: 0.00074249
Iteration 3/25 | Loss: 0.00074246
Iteration 4/25 | Loss: 0.00074246
Iteration 5/25 | Loss: 0.00074246
Iteration 6/25 | Loss: 0.00074246
Iteration 7/25 | Loss: 0.00074246
Iteration 8/25 | Loss: 0.00074246
Iteration 9/25 | Loss: 0.00074246
Iteration 10/25 | Loss: 0.00074246
Iteration 11/25 | Loss: 0.00074246
Iteration 12/25 | Loss: 0.00074246
Iteration 13/25 | Loss: 0.00074246
Iteration 14/25 | Loss: 0.00074246
Iteration 15/25 | Loss: 0.00074246
Iteration 16/25 | Loss: 0.00074246
Iteration 17/25 | Loss: 0.00074246
Iteration 18/25 | Loss: 0.00074246
Iteration 19/25 | Loss: 0.00074246
Iteration 20/25 | Loss: 0.00074246
Iteration 21/25 | Loss: 0.00074246
Iteration 22/25 | Loss: 0.00074246
Iteration 23/25 | Loss: 0.00074246
Iteration 24/25 | Loss: 0.00074246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007424590294249356, 0.0007424590294249356, 0.0007424590294249356, 0.0007424590294249356, 0.0007424590294249356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007424590294249356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074246
Iteration 2/1000 | Loss: 0.00004187
Iteration 3/1000 | Loss: 0.00002682
Iteration 4/1000 | Loss: 0.00002006
Iteration 5/1000 | Loss: 0.00001783
Iteration 6/1000 | Loss: 0.00001687
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001498
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001460
Iteration 32/1000 | Loss: 0.00001459
Iteration 33/1000 | Loss: 0.00001459
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001457
Iteration 39/1000 | Loss: 0.00001457
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001452
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001447
Iteration 64/1000 | Loss: 0.00001447
Iteration 65/1000 | Loss: 0.00001447
Iteration 66/1000 | Loss: 0.00001447
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001446
Iteration 71/1000 | Loss: 0.00001446
Iteration 72/1000 | Loss: 0.00001446
Iteration 73/1000 | Loss: 0.00001446
Iteration 74/1000 | Loss: 0.00001446
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001445
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.445137331756996e-05, 1.445137331756996e-05, 1.445137331756996e-05, 1.445137331756996e-05, 1.445137331756996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.445137331756996e-05

Optimization complete. Final v2v error: 2.963742971420288 mm

Highest mean error: 4.862098693847656 mm for frame 60

Lowest mean error: 2.280691623687744 mm for frame 87

Saving results

Total time: 34.425854206085205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00277415
Iteration 2/25 | Loss: 0.00128564
Iteration 3/25 | Loss: 0.00104699
Iteration 4/25 | Loss: 0.00099783
Iteration 5/25 | Loss: 0.00098388
Iteration 6/25 | Loss: 0.00097882
Iteration 7/25 | Loss: 0.00097708
Iteration 8/25 | Loss: 0.00097640
Iteration 9/25 | Loss: 0.00097617
Iteration 10/25 | Loss: 0.00097603
Iteration 11/25 | Loss: 0.00097601
Iteration 12/25 | Loss: 0.00097599
Iteration 13/25 | Loss: 0.00097599
Iteration 14/25 | Loss: 0.00097598
Iteration 15/25 | Loss: 0.00097598
Iteration 16/25 | Loss: 0.00097598
Iteration 17/25 | Loss: 0.00097598
Iteration 18/25 | Loss: 0.00097598
Iteration 19/25 | Loss: 0.00097598
Iteration 20/25 | Loss: 0.00097598
Iteration 21/25 | Loss: 0.00097597
Iteration 22/25 | Loss: 0.00097597
Iteration 23/25 | Loss: 0.00097597
Iteration 24/25 | Loss: 0.00097597
Iteration 25/25 | Loss: 0.00097597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36767042
Iteration 2/25 | Loss: 0.00153126
Iteration 3/25 | Loss: 0.00153126
Iteration 4/25 | Loss: 0.00153126
Iteration 5/25 | Loss: 0.00153126
Iteration 6/25 | Loss: 0.00153125
Iteration 7/25 | Loss: 0.00153125
Iteration 8/25 | Loss: 0.00153125
Iteration 9/25 | Loss: 0.00153125
Iteration 10/25 | Loss: 0.00153125
Iteration 11/25 | Loss: 0.00153125
Iteration 12/25 | Loss: 0.00153125
Iteration 13/25 | Loss: 0.00153125
Iteration 14/25 | Loss: 0.00153125
Iteration 15/25 | Loss: 0.00153125
Iteration 16/25 | Loss: 0.00153125
Iteration 17/25 | Loss: 0.00153125
Iteration 18/25 | Loss: 0.00153125
Iteration 19/25 | Loss: 0.00153125
Iteration 20/25 | Loss: 0.00153125
Iteration 21/25 | Loss: 0.00153125
Iteration 22/25 | Loss: 0.00153125
Iteration 23/25 | Loss: 0.00153125
Iteration 24/25 | Loss: 0.00153125
Iteration 25/25 | Loss: 0.00153125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153125
Iteration 2/1000 | Loss: 0.00021872
Iteration 3/1000 | Loss: 0.00013762
Iteration 4/1000 | Loss: 0.00009968
Iteration 5/1000 | Loss: 0.00010258
Iteration 6/1000 | Loss: 0.00004784
Iteration 7/1000 | Loss: 0.00010756
Iteration 8/1000 | Loss: 0.00010616
Iteration 9/1000 | Loss: 0.00010446
Iteration 10/1000 | Loss: 0.00012051
Iteration 11/1000 | Loss: 0.00010672
Iteration 12/1000 | Loss: 0.00004962
Iteration 13/1000 | Loss: 0.00007237
Iteration 14/1000 | Loss: 0.00007449
Iteration 15/1000 | Loss: 0.00005258
Iteration 16/1000 | Loss: 0.00004806
Iteration 17/1000 | Loss: 0.00005731
Iteration 18/1000 | Loss: 0.00005414
Iteration 19/1000 | Loss: 0.00009372
Iteration 20/1000 | Loss: 0.00008818
Iteration 21/1000 | Loss: 0.00008214
Iteration 22/1000 | Loss: 0.00007736
Iteration 23/1000 | Loss: 0.00008034
Iteration 24/1000 | Loss: 0.00006435
Iteration 25/1000 | Loss: 0.00010144
Iteration 26/1000 | Loss: 0.00008236
Iteration 27/1000 | Loss: 0.00007629
Iteration 28/1000 | Loss: 0.00009568
Iteration 29/1000 | Loss: 0.00006300
Iteration 30/1000 | Loss: 0.00008486
Iteration 31/1000 | Loss: 0.00008791
Iteration 32/1000 | Loss: 0.00008013
Iteration 33/1000 | Loss: 0.00008295
Iteration 34/1000 | Loss: 0.00011809
Iteration 35/1000 | Loss: 0.00009427
Iteration 36/1000 | Loss: 0.00010798
Iteration 37/1000 | Loss: 0.00008441
Iteration 38/1000 | Loss: 0.00007930
Iteration 39/1000 | Loss: 0.00006215
Iteration 40/1000 | Loss: 0.00004799
Iteration 41/1000 | Loss: 0.00007695
Iteration 42/1000 | Loss: 0.00007188
Iteration 43/1000 | Loss: 0.00004341
Iteration 44/1000 | Loss: 0.00005383
Iteration 45/1000 | Loss: 0.00002664
Iteration 46/1000 | Loss: 0.00003978
Iteration 47/1000 | Loss: 0.00003447
Iteration 48/1000 | Loss: 0.00003776
Iteration 49/1000 | Loss: 0.00003177
Iteration 50/1000 | Loss: 0.00003930
Iteration 51/1000 | Loss: 0.00004950
Iteration 52/1000 | Loss: 0.00004278
Iteration 53/1000 | Loss: 0.00004686
Iteration 54/1000 | Loss: 0.00004583
Iteration 55/1000 | Loss: 0.00003570
Iteration 56/1000 | Loss: 0.00003163
Iteration 57/1000 | Loss: 0.00003200
Iteration 58/1000 | Loss: 0.00002923
Iteration 59/1000 | Loss: 0.00002406
Iteration 60/1000 | Loss: 0.00004814
Iteration 61/1000 | Loss: 0.00003406
Iteration 62/1000 | Loss: 0.00002808
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00011548
Iteration 65/1000 | Loss: 0.00003765
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00002274
Iteration 69/1000 | Loss: 0.00002227
Iteration 70/1000 | Loss: 0.00002827
Iteration 71/1000 | Loss: 0.00002041
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00002062
Iteration 75/1000 | Loss: 0.00002020
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00003175
Iteration 78/1000 | Loss: 0.00003677
Iteration 79/1000 | Loss: 0.00002909
Iteration 80/1000 | Loss: 0.00002952
Iteration 81/1000 | Loss: 0.00002372
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002626
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00003144
Iteration 86/1000 | Loss: 0.00001804
Iteration 87/1000 | Loss: 0.00002095
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001196
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001150
Iteration 129/1000 | Loss: 0.00001150
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001146
Iteration 147/1000 | Loss: 0.00001146
Iteration 148/1000 | Loss: 0.00001146
Iteration 149/1000 | Loss: 0.00001145
Iteration 150/1000 | Loss: 0.00001145
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001144
Iteration 156/1000 | Loss: 0.00001144
Iteration 157/1000 | Loss: 0.00001144
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001141
Iteration 164/1000 | Loss: 0.00001140
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001138
Iteration 168/1000 | Loss: 0.00001138
Iteration 169/1000 | Loss: 0.00001138
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001137
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001134
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001134
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001133
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001132
Iteration 184/1000 | Loss: 0.00001132
Iteration 185/1000 | Loss: 0.00001132
Iteration 186/1000 | Loss: 0.00001131
Iteration 187/1000 | Loss: 0.00001130
Iteration 188/1000 | Loss: 0.00001130
Iteration 189/1000 | Loss: 0.00001130
Iteration 190/1000 | Loss: 0.00001130
Iteration 191/1000 | Loss: 0.00001130
Iteration 192/1000 | Loss: 0.00001130
Iteration 193/1000 | Loss: 0.00001129
Iteration 194/1000 | Loss: 0.00001129
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001127
Iteration 199/1000 | Loss: 0.00001127
Iteration 200/1000 | Loss: 0.00001127
Iteration 201/1000 | Loss: 0.00001126
Iteration 202/1000 | Loss: 0.00001126
Iteration 203/1000 | Loss: 0.00001125
Iteration 204/1000 | Loss: 0.00001125
Iteration 205/1000 | Loss: 0.00001125
Iteration 206/1000 | Loss: 0.00001124
Iteration 207/1000 | Loss: 0.00001124
Iteration 208/1000 | Loss: 0.00001124
Iteration 209/1000 | Loss: 0.00001124
Iteration 210/1000 | Loss: 0.00001123
Iteration 211/1000 | Loss: 0.00001123
Iteration 212/1000 | Loss: 0.00001123
Iteration 213/1000 | Loss: 0.00001122
Iteration 214/1000 | Loss: 0.00001122
Iteration 215/1000 | Loss: 0.00001121
Iteration 216/1000 | Loss: 0.00001121
Iteration 217/1000 | Loss: 0.00001121
Iteration 218/1000 | Loss: 0.00001120
Iteration 219/1000 | Loss: 0.00001120
Iteration 220/1000 | Loss: 0.00001120
Iteration 221/1000 | Loss: 0.00001119
Iteration 222/1000 | Loss: 0.00001119
Iteration 223/1000 | Loss: 0.00001119
Iteration 224/1000 | Loss: 0.00001119
Iteration 225/1000 | Loss: 0.00001119
Iteration 226/1000 | Loss: 0.00001119
Iteration 227/1000 | Loss: 0.00001119
Iteration 228/1000 | Loss: 0.00001119
Iteration 229/1000 | Loss: 0.00001118
Iteration 230/1000 | Loss: 0.00001118
Iteration 231/1000 | Loss: 0.00001118
Iteration 232/1000 | Loss: 0.00001118
Iteration 233/1000 | Loss: 0.00001118
Iteration 234/1000 | Loss: 0.00001118
Iteration 235/1000 | Loss: 0.00001118
Iteration 236/1000 | Loss: 0.00001118
Iteration 237/1000 | Loss: 0.00001117
Iteration 238/1000 | Loss: 0.00001117
Iteration 239/1000 | Loss: 0.00001117
Iteration 240/1000 | Loss: 0.00001117
Iteration 241/1000 | Loss: 0.00001116
Iteration 242/1000 | Loss: 0.00001116
Iteration 243/1000 | Loss: 0.00001116
Iteration 244/1000 | Loss: 0.00001116
Iteration 245/1000 | Loss: 0.00001115
Iteration 246/1000 | Loss: 0.00001115
Iteration 247/1000 | Loss: 0.00001115
Iteration 248/1000 | Loss: 0.00001115
Iteration 249/1000 | Loss: 0.00001114
Iteration 250/1000 | Loss: 0.00001114
Iteration 251/1000 | Loss: 0.00001114
Iteration 252/1000 | Loss: 0.00001114
Iteration 253/1000 | Loss: 0.00001114
Iteration 254/1000 | Loss: 0.00001114
Iteration 255/1000 | Loss: 0.00001114
Iteration 256/1000 | Loss: 0.00001113
Iteration 257/1000 | Loss: 0.00001113
Iteration 258/1000 | Loss: 0.00001113
Iteration 259/1000 | Loss: 0.00001113
Iteration 260/1000 | Loss: 0.00001113
Iteration 261/1000 | Loss: 0.00001113
Iteration 262/1000 | Loss: 0.00001113
Iteration 263/1000 | Loss: 0.00001113
Iteration 264/1000 | Loss: 0.00001113
Iteration 265/1000 | Loss: 0.00001112
Iteration 266/1000 | Loss: 0.00001112
Iteration 267/1000 | Loss: 0.00001112
Iteration 268/1000 | Loss: 0.00001112
Iteration 269/1000 | Loss: 0.00001112
Iteration 270/1000 | Loss: 0.00001112
Iteration 271/1000 | Loss: 0.00001112
Iteration 272/1000 | Loss: 0.00001112
Iteration 273/1000 | Loss: 0.00001112
Iteration 274/1000 | Loss: 0.00001111
Iteration 275/1000 | Loss: 0.00001111
Iteration 276/1000 | Loss: 0.00001111
Iteration 277/1000 | Loss: 0.00001111
Iteration 278/1000 | Loss: 0.00001111
Iteration 279/1000 | Loss: 0.00001111
Iteration 280/1000 | Loss: 0.00001111
Iteration 281/1000 | Loss: 0.00001111
Iteration 282/1000 | Loss: 0.00001111
Iteration 283/1000 | Loss: 0.00001111
Iteration 284/1000 | Loss: 0.00001111
Iteration 285/1000 | Loss: 0.00001111
Iteration 286/1000 | Loss: 0.00001111
Iteration 287/1000 | Loss: 0.00001111
Iteration 288/1000 | Loss: 0.00001110
Iteration 289/1000 | Loss: 0.00001110
Iteration 290/1000 | Loss: 0.00001110
Iteration 291/1000 | Loss: 0.00001110
Iteration 292/1000 | Loss: 0.00001110
Iteration 293/1000 | Loss: 0.00001110
Iteration 294/1000 | Loss: 0.00001110
Iteration 295/1000 | Loss: 0.00001110
Iteration 296/1000 | Loss: 0.00001110
Iteration 297/1000 | Loss: 0.00001110
Iteration 298/1000 | Loss: 0.00001110
Iteration 299/1000 | Loss: 0.00001110
Iteration 300/1000 | Loss: 0.00001110
Iteration 301/1000 | Loss: 0.00001110
Iteration 302/1000 | Loss: 0.00001110
Iteration 303/1000 | Loss: 0.00001110
Iteration 304/1000 | Loss: 0.00001110
Iteration 305/1000 | Loss: 0.00001110
Iteration 306/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [1.1104631994385272e-05, 1.1104631994385272e-05, 1.1104631994385272e-05, 1.1104631994385272e-05, 1.1104631994385272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1104631994385272e-05

Optimization complete. Final v2v error: 2.761967897415161 mm

Highest mean error: 5.887380123138428 mm for frame 48

Lowest mean error: 2.4796550273895264 mm for frame 187

Saving results

Total time: 173.4908468723297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895755
Iteration 2/25 | Loss: 0.00119084
Iteration 3/25 | Loss: 0.00105078
Iteration 4/25 | Loss: 0.00102735
Iteration 5/25 | Loss: 0.00101963
Iteration 6/25 | Loss: 0.00101715
Iteration 7/25 | Loss: 0.00101675
Iteration 8/25 | Loss: 0.00101675
Iteration 9/25 | Loss: 0.00101675
Iteration 10/25 | Loss: 0.00101675
Iteration 11/25 | Loss: 0.00101675
Iteration 12/25 | Loss: 0.00101675
Iteration 13/25 | Loss: 0.00101675
Iteration 14/25 | Loss: 0.00101675
Iteration 15/25 | Loss: 0.00101675
Iteration 16/25 | Loss: 0.00101675
Iteration 17/25 | Loss: 0.00101675
Iteration 18/25 | Loss: 0.00101675
Iteration 19/25 | Loss: 0.00101675
Iteration 20/25 | Loss: 0.00101675
Iteration 21/25 | Loss: 0.00101675
Iteration 22/25 | Loss: 0.00101675
Iteration 23/25 | Loss: 0.00101675
Iteration 24/25 | Loss: 0.00101675
Iteration 25/25 | Loss: 0.00101675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39254022
Iteration 2/25 | Loss: 0.00072875
Iteration 3/25 | Loss: 0.00072874
Iteration 4/25 | Loss: 0.00072874
Iteration 5/25 | Loss: 0.00072874
Iteration 6/25 | Loss: 0.00072874
Iteration 7/25 | Loss: 0.00072874
Iteration 8/25 | Loss: 0.00072873
Iteration 9/25 | Loss: 0.00072873
Iteration 10/25 | Loss: 0.00072873
Iteration 11/25 | Loss: 0.00072873
Iteration 12/25 | Loss: 0.00072873
Iteration 13/25 | Loss: 0.00072873
Iteration 14/25 | Loss: 0.00072873
Iteration 15/25 | Loss: 0.00072873
Iteration 16/25 | Loss: 0.00072873
Iteration 17/25 | Loss: 0.00072873
Iteration 18/25 | Loss: 0.00072873
Iteration 19/25 | Loss: 0.00072873
Iteration 20/25 | Loss: 0.00072873
Iteration 21/25 | Loss: 0.00072873
Iteration 22/25 | Loss: 0.00072873
Iteration 23/25 | Loss: 0.00072873
Iteration 24/25 | Loss: 0.00072873
Iteration 25/25 | Loss: 0.00072873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072873
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00002633
Iteration 4/1000 | Loss: 0.00002001
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001701
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001602
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001555
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001551
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001540
Iteration 17/1000 | Loss: 0.00001539
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001520
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001507
Iteration 53/1000 | Loss: 0.00001507
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001504
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001500
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001500
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001499
Iteration 95/1000 | Loss: 0.00001499
Iteration 96/1000 | Loss: 0.00001499
Iteration 97/1000 | Loss: 0.00001499
Iteration 98/1000 | Loss: 0.00001499
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001499
Iteration 101/1000 | Loss: 0.00001499
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001498
Iteration 108/1000 | Loss: 0.00001498
Iteration 109/1000 | Loss: 0.00001498
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001496
Iteration 116/1000 | Loss: 0.00001496
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001495
Iteration 121/1000 | Loss: 0.00001495
Iteration 122/1000 | Loss: 0.00001495
Iteration 123/1000 | Loss: 0.00001495
Iteration 124/1000 | Loss: 0.00001495
Iteration 125/1000 | Loss: 0.00001495
Iteration 126/1000 | Loss: 0.00001494
Iteration 127/1000 | Loss: 0.00001494
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001494
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001493
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001492
Iteration 137/1000 | Loss: 0.00001492
Iteration 138/1000 | Loss: 0.00001492
Iteration 139/1000 | Loss: 0.00001492
Iteration 140/1000 | Loss: 0.00001492
Iteration 141/1000 | Loss: 0.00001492
Iteration 142/1000 | Loss: 0.00001492
Iteration 143/1000 | Loss: 0.00001492
Iteration 144/1000 | Loss: 0.00001491
Iteration 145/1000 | Loss: 0.00001491
Iteration 146/1000 | Loss: 0.00001491
Iteration 147/1000 | Loss: 0.00001491
Iteration 148/1000 | Loss: 0.00001491
Iteration 149/1000 | Loss: 0.00001491
Iteration 150/1000 | Loss: 0.00001491
Iteration 151/1000 | Loss: 0.00001491
Iteration 152/1000 | Loss: 0.00001491
Iteration 153/1000 | Loss: 0.00001490
Iteration 154/1000 | Loss: 0.00001490
Iteration 155/1000 | Loss: 0.00001490
Iteration 156/1000 | Loss: 0.00001490
Iteration 157/1000 | Loss: 0.00001490
Iteration 158/1000 | Loss: 0.00001490
Iteration 159/1000 | Loss: 0.00001490
Iteration 160/1000 | Loss: 0.00001489
Iteration 161/1000 | Loss: 0.00001489
Iteration 162/1000 | Loss: 0.00001489
Iteration 163/1000 | Loss: 0.00001489
Iteration 164/1000 | Loss: 0.00001489
Iteration 165/1000 | Loss: 0.00001488
Iteration 166/1000 | Loss: 0.00001488
Iteration 167/1000 | Loss: 0.00001488
Iteration 168/1000 | Loss: 0.00001488
Iteration 169/1000 | Loss: 0.00001488
Iteration 170/1000 | Loss: 0.00001488
Iteration 171/1000 | Loss: 0.00001488
Iteration 172/1000 | Loss: 0.00001488
Iteration 173/1000 | Loss: 0.00001488
Iteration 174/1000 | Loss: 0.00001488
Iteration 175/1000 | Loss: 0.00001488
Iteration 176/1000 | Loss: 0.00001488
Iteration 177/1000 | Loss: 0.00001488
Iteration 178/1000 | Loss: 0.00001487
Iteration 179/1000 | Loss: 0.00001487
Iteration 180/1000 | Loss: 0.00001487
Iteration 181/1000 | Loss: 0.00001487
Iteration 182/1000 | Loss: 0.00001487
Iteration 183/1000 | Loss: 0.00001487
Iteration 184/1000 | Loss: 0.00001487
Iteration 185/1000 | Loss: 0.00001487
Iteration 186/1000 | Loss: 0.00001487
Iteration 187/1000 | Loss: 0.00001486
Iteration 188/1000 | Loss: 0.00001486
Iteration 189/1000 | Loss: 0.00001486
Iteration 190/1000 | Loss: 0.00001486
Iteration 191/1000 | Loss: 0.00001486
Iteration 192/1000 | Loss: 0.00001486
Iteration 193/1000 | Loss: 0.00001486
Iteration 194/1000 | Loss: 0.00001486
Iteration 195/1000 | Loss: 0.00001486
Iteration 196/1000 | Loss: 0.00001486
Iteration 197/1000 | Loss: 0.00001486
Iteration 198/1000 | Loss: 0.00001486
Iteration 199/1000 | Loss: 0.00001486
Iteration 200/1000 | Loss: 0.00001486
Iteration 201/1000 | Loss: 0.00001485
Iteration 202/1000 | Loss: 0.00001485
Iteration 203/1000 | Loss: 0.00001485
Iteration 204/1000 | Loss: 0.00001485
Iteration 205/1000 | Loss: 0.00001485
Iteration 206/1000 | Loss: 0.00001485
Iteration 207/1000 | Loss: 0.00001485
Iteration 208/1000 | Loss: 0.00001485
Iteration 209/1000 | Loss: 0.00001484
Iteration 210/1000 | Loss: 0.00001484
Iteration 211/1000 | Loss: 0.00001484
Iteration 212/1000 | Loss: 0.00001484
Iteration 213/1000 | Loss: 0.00001484
Iteration 214/1000 | Loss: 0.00001484
Iteration 215/1000 | Loss: 0.00001484
Iteration 216/1000 | Loss: 0.00001484
Iteration 217/1000 | Loss: 0.00001483
Iteration 218/1000 | Loss: 0.00001483
Iteration 219/1000 | Loss: 0.00001483
Iteration 220/1000 | Loss: 0.00001483
Iteration 221/1000 | Loss: 0.00001482
Iteration 222/1000 | Loss: 0.00001482
Iteration 223/1000 | Loss: 0.00001482
Iteration 224/1000 | Loss: 0.00001482
Iteration 225/1000 | Loss: 0.00001482
Iteration 226/1000 | Loss: 0.00001482
Iteration 227/1000 | Loss: 0.00001482
Iteration 228/1000 | Loss: 0.00001482
Iteration 229/1000 | Loss: 0.00001482
Iteration 230/1000 | Loss: 0.00001482
Iteration 231/1000 | Loss: 0.00001481
Iteration 232/1000 | Loss: 0.00001481
Iteration 233/1000 | Loss: 0.00001481
Iteration 234/1000 | Loss: 0.00001481
Iteration 235/1000 | Loss: 0.00001480
Iteration 236/1000 | Loss: 0.00001480
Iteration 237/1000 | Loss: 0.00001480
Iteration 238/1000 | Loss: 0.00001480
Iteration 239/1000 | Loss: 0.00001480
Iteration 240/1000 | Loss: 0.00001480
Iteration 241/1000 | Loss: 0.00001480
Iteration 242/1000 | Loss: 0.00001480
Iteration 243/1000 | Loss: 0.00001480
Iteration 244/1000 | Loss: 0.00001480
Iteration 245/1000 | Loss: 0.00001480
Iteration 246/1000 | Loss: 0.00001480
Iteration 247/1000 | Loss: 0.00001480
Iteration 248/1000 | Loss: 0.00001480
Iteration 249/1000 | Loss: 0.00001479
Iteration 250/1000 | Loss: 0.00001479
Iteration 251/1000 | Loss: 0.00001479
Iteration 252/1000 | Loss: 0.00001479
Iteration 253/1000 | Loss: 0.00001479
Iteration 254/1000 | Loss: 0.00001479
Iteration 255/1000 | Loss: 0.00001479
Iteration 256/1000 | Loss: 0.00001479
Iteration 257/1000 | Loss: 0.00001479
Iteration 258/1000 | Loss: 0.00001479
Iteration 259/1000 | Loss: 0.00001479
Iteration 260/1000 | Loss: 0.00001478
Iteration 261/1000 | Loss: 0.00001478
Iteration 262/1000 | Loss: 0.00001478
Iteration 263/1000 | Loss: 0.00001478
Iteration 264/1000 | Loss: 0.00001478
Iteration 265/1000 | Loss: 0.00001478
Iteration 266/1000 | Loss: 0.00001478
Iteration 267/1000 | Loss: 0.00001478
Iteration 268/1000 | Loss: 0.00001478
Iteration 269/1000 | Loss: 0.00001478
Iteration 270/1000 | Loss: 0.00001478
Iteration 271/1000 | Loss: 0.00001478
Iteration 272/1000 | Loss: 0.00001478
Iteration 273/1000 | Loss: 0.00001478
Iteration 274/1000 | Loss: 0.00001478
Iteration 275/1000 | Loss: 0.00001477
Iteration 276/1000 | Loss: 0.00001477
Iteration 277/1000 | Loss: 0.00001477
Iteration 278/1000 | Loss: 0.00001477
Iteration 279/1000 | Loss: 0.00001477
Iteration 280/1000 | Loss: 0.00001477
Iteration 281/1000 | Loss: 0.00001477
Iteration 282/1000 | Loss: 0.00001477
Iteration 283/1000 | Loss: 0.00001477
Iteration 284/1000 | Loss: 0.00001477
Iteration 285/1000 | Loss: 0.00001477
Iteration 286/1000 | Loss: 0.00001477
Iteration 287/1000 | Loss: 0.00001476
Iteration 288/1000 | Loss: 0.00001476
Iteration 289/1000 | Loss: 0.00001476
Iteration 290/1000 | Loss: 0.00001476
Iteration 291/1000 | Loss: 0.00001476
Iteration 292/1000 | Loss: 0.00001476
Iteration 293/1000 | Loss: 0.00001476
Iteration 294/1000 | Loss: 0.00001476
Iteration 295/1000 | Loss: 0.00001476
Iteration 296/1000 | Loss: 0.00001476
Iteration 297/1000 | Loss: 0.00001476
Iteration 298/1000 | Loss: 0.00001476
Iteration 299/1000 | Loss: 0.00001476
Iteration 300/1000 | Loss: 0.00001476
Iteration 301/1000 | Loss: 0.00001476
Iteration 302/1000 | Loss: 0.00001476
Iteration 303/1000 | Loss: 0.00001476
Iteration 304/1000 | Loss: 0.00001476
Iteration 305/1000 | Loss: 0.00001476
Iteration 306/1000 | Loss: 0.00001476
Iteration 307/1000 | Loss: 0.00001476
Iteration 308/1000 | Loss: 0.00001476
Iteration 309/1000 | Loss: 0.00001476
Iteration 310/1000 | Loss: 0.00001476
Iteration 311/1000 | Loss: 0.00001476
Iteration 312/1000 | Loss: 0.00001476
Iteration 313/1000 | Loss: 0.00001476
Iteration 314/1000 | Loss: 0.00001476
Iteration 315/1000 | Loss: 0.00001476
Iteration 316/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [1.4762389582756441e-05, 1.4762389582756441e-05, 1.4762389582756441e-05, 1.4762389582756441e-05, 1.4762389582756441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4762389582756441e-05

Optimization complete. Final v2v error: 3.2316958904266357 mm

Highest mean error: 4.930380821228027 mm for frame 69

Lowest mean error: 2.816818952560425 mm for frame 132

Saving results

Total time: 50.35756540298462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859502
Iteration 2/25 | Loss: 0.00159223
Iteration 3/25 | Loss: 0.00118619
Iteration 4/25 | Loss: 0.00112647
Iteration 5/25 | Loss: 0.00111754
Iteration 6/25 | Loss: 0.00111553
Iteration 7/25 | Loss: 0.00111516
Iteration 8/25 | Loss: 0.00111516
Iteration 9/25 | Loss: 0.00111516
Iteration 10/25 | Loss: 0.00111516
Iteration 11/25 | Loss: 0.00111516
Iteration 12/25 | Loss: 0.00111516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011151557555422187, 0.0011151557555422187, 0.0011151557555422187, 0.0011151557555422187, 0.0011151557555422187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011151557555422187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27408063
Iteration 2/25 | Loss: 0.00078438
Iteration 3/25 | Loss: 0.00078436
Iteration 4/25 | Loss: 0.00078436
Iteration 5/25 | Loss: 0.00078436
Iteration 6/25 | Loss: 0.00078436
Iteration 7/25 | Loss: 0.00078436
Iteration 8/25 | Loss: 0.00078436
Iteration 9/25 | Loss: 0.00078436
Iteration 10/25 | Loss: 0.00078436
Iteration 11/25 | Loss: 0.00078436
Iteration 12/25 | Loss: 0.00078436
Iteration 13/25 | Loss: 0.00078436
Iteration 14/25 | Loss: 0.00078436
Iteration 15/25 | Loss: 0.00078436
Iteration 16/25 | Loss: 0.00078436
Iteration 17/25 | Loss: 0.00078436
Iteration 18/25 | Loss: 0.00078436
Iteration 19/25 | Loss: 0.00078436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007843582425266504, 0.0007843582425266504, 0.0007843582425266504, 0.0007843582425266504, 0.0007843582425266504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007843582425266504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078436
Iteration 2/1000 | Loss: 0.00005716
Iteration 3/1000 | Loss: 0.00002919
Iteration 4/1000 | Loss: 0.00002335
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001940
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001820
Iteration 9/1000 | Loss: 0.00001783
Iteration 10/1000 | Loss: 0.00001765
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001750
Iteration 13/1000 | Loss: 0.00001747
Iteration 14/1000 | Loss: 0.00001745
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001726
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00001707
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00001706
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001706
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001706
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001706
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.7051537724910304e-05, 1.7051537724910304e-05, 1.7051537724910304e-05, 1.7051537724910304e-05, 1.7051537724910304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7051537724910304e-05

Optimization complete. Final v2v error: 3.504934787750244 mm

Highest mean error: 3.749624013900757 mm for frame 115

Lowest mean error: 3.0678927898406982 mm for frame 36

Saving results

Total time: 31.56413698196411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838514
Iteration 2/25 | Loss: 0.00130973
Iteration 3/25 | Loss: 0.00112422
Iteration 4/25 | Loss: 0.00109708
Iteration 5/25 | Loss: 0.00109778
Iteration 6/25 | Loss: 0.00108157
Iteration 7/25 | Loss: 0.00106913
Iteration 8/25 | Loss: 0.00105728
Iteration 9/25 | Loss: 0.00105395
Iteration 10/25 | Loss: 0.00105346
Iteration 11/25 | Loss: 0.00105323
Iteration 12/25 | Loss: 0.00105316
Iteration 13/25 | Loss: 0.00105315
Iteration 14/25 | Loss: 0.00105315
Iteration 15/25 | Loss: 0.00105315
Iteration 16/25 | Loss: 0.00105315
Iteration 17/25 | Loss: 0.00105315
Iteration 18/25 | Loss: 0.00105315
Iteration 19/25 | Loss: 0.00105315
Iteration 20/25 | Loss: 0.00105315
Iteration 21/25 | Loss: 0.00105314
Iteration 22/25 | Loss: 0.00105314
Iteration 23/25 | Loss: 0.00105314
Iteration 24/25 | Loss: 0.00105314
Iteration 25/25 | Loss: 0.00105314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23245704
Iteration 2/25 | Loss: 0.00050949
Iteration 3/25 | Loss: 0.00050944
Iteration 4/25 | Loss: 0.00050944
Iteration 5/25 | Loss: 0.00050944
Iteration 6/25 | Loss: 0.00050944
Iteration 7/25 | Loss: 0.00050944
Iteration 8/25 | Loss: 0.00050944
Iteration 9/25 | Loss: 0.00050944
Iteration 10/25 | Loss: 0.00050944
Iteration 11/25 | Loss: 0.00050944
Iteration 12/25 | Loss: 0.00050944
Iteration 13/25 | Loss: 0.00050944
Iteration 14/25 | Loss: 0.00050944
Iteration 15/25 | Loss: 0.00050944
Iteration 16/25 | Loss: 0.00050944
Iteration 17/25 | Loss: 0.00050944
Iteration 18/25 | Loss: 0.00050944
Iteration 19/25 | Loss: 0.00050944
Iteration 20/25 | Loss: 0.00050944
Iteration 21/25 | Loss: 0.00050944
Iteration 22/25 | Loss: 0.00050944
Iteration 23/25 | Loss: 0.00050944
Iteration 24/25 | Loss: 0.00050944
Iteration 25/25 | Loss: 0.00050944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050944
Iteration 2/1000 | Loss: 0.00003853
Iteration 3/1000 | Loss: 0.00002610
Iteration 4/1000 | Loss: 0.00002025
Iteration 5/1000 | Loss: 0.00001766
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001581
Iteration 9/1000 | Loss: 0.00001549
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001473
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001446
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001446
Iteration 61/1000 | Loss: 0.00001445
Iteration 62/1000 | Loss: 0.00001445
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001445
Iteration 65/1000 | Loss: 0.00001445
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001444
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001442
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001442
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001438
Iteration 130/1000 | Loss: 0.00001438
Iteration 131/1000 | Loss: 0.00001438
Iteration 132/1000 | Loss: 0.00001438
Iteration 133/1000 | Loss: 0.00001438
Iteration 134/1000 | Loss: 0.00001438
Iteration 135/1000 | Loss: 0.00001438
Iteration 136/1000 | Loss: 0.00001438
Iteration 137/1000 | Loss: 0.00001438
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001438
Iteration 143/1000 | Loss: 0.00001438
Iteration 144/1000 | Loss: 0.00001438
Iteration 145/1000 | Loss: 0.00001438
Iteration 146/1000 | Loss: 0.00001438
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001438
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001438
Iteration 153/1000 | Loss: 0.00001438
Iteration 154/1000 | Loss: 0.00001438
Iteration 155/1000 | Loss: 0.00001438
Iteration 156/1000 | Loss: 0.00001438
Iteration 157/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.438463823433267e-05, 1.438463823433267e-05, 1.438463823433267e-05, 1.438463823433267e-05, 1.438463823433267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.438463823433267e-05

Optimization complete. Final v2v error: 3.102626323699951 mm

Highest mean error: 3.6142778396606445 mm for frame 99

Lowest mean error: 2.7918107509613037 mm for frame 59

Saving results

Total time: 48.64552330970764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836679
Iteration 2/25 | Loss: 0.00125788
Iteration 3/25 | Loss: 0.00108800
Iteration 4/25 | Loss: 0.00107107
Iteration 5/25 | Loss: 0.00106773
Iteration 6/25 | Loss: 0.00106766
Iteration 7/25 | Loss: 0.00106766
Iteration 8/25 | Loss: 0.00106766
Iteration 9/25 | Loss: 0.00106766
Iteration 10/25 | Loss: 0.00106766
Iteration 11/25 | Loss: 0.00106766
Iteration 12/25 | Loss: 0.00106766
Iteration 13/25 | Loss: 0.00106766
Iteration 14/25 | Loss: 0.00106766
Iteration 15/25 | Loss: 0.00106766
Iteration 16/25 | Loss: 0.00106766
Iteration 17/25 | Loss: 0.00106766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010676566744223237, 0.0010676566744223237, 0.0010676566744223237, 0.0010676566744223237, 0.0010676566744223237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010676566744223237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34425294
Iteration 2/25 | Loss: 0.00065844
Iteration 3/25 | Loss: 0.00065839
Iteration 4/25 | Loss: 0.00065839
Iteration 5/25 | Loss: 0.00065839
Iteration 6/25 | Loss: 0.00065839
Iteration 7/25 | Loss: 0.00065839
Iteration 8/25 | Loss: 0.00065839
Iteration 9/25 | Loss: 0.00065839
Iteration 10/25 | Loss: 0.00065839
Iteration 11/25 | Loss: 0.00065839
Iteration 12/25 | Loss: 0.00065839
Iteration 13/25 | Loss: 0.00065839
Iteration 14/25 | Loss: 0.00065839
Iteration 15/25 | Loss: 0.00065839
Iteration 16/25 | Loss: 0.00065839
Iteration 17/25 | Loss: 0.00065839
Iteration 18/25 | Loss: 0.00065839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006583855138160288, 0.0006583855138160288, 0.0006583855138160288, 0.0006583855138160288, 0.0006583855138160288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006583855138160288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065839
Iteration 2/1000 | Loss: 0.00003876
Iteration 3/1000 | Loss: 0.00002333
Iteration 4/1000 | Loss: 0.00001861
Iteration 5/1000 | Loss: 0.00001723
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001425
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001413
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001405
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001394
Iteration 58/1000 | Loss: 0.00001394
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001392
Iteration 63/1000 | Loss: 0.00001392
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001391
Iteration 66/1000 | Loss: 0.00001391
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001389
Iteration 74/1000 | Loss: 0.00001389
Iteration 75/1000 | Loss: 0.00001388
Iteration 76/1000 | Loss: 0.00001388
Iteration 77/1000 | Loss: 0.00001388
Iteration 78/1000 | Loss: 0.00001388
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001376
Iteration 118/1000 | Loss: 0.00001376
Iteration 119/1000 | Loss: 0.00001376
Iteration 120/1000 | Loss: 0.00001375
Iteration 121/1000 | Loss: 0.00001375
Iteration 122/1000 | Loss: 0.00001375
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001372
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001372
Iteration 134/1000 | Loss: 0.00001372
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00001372
Iteration 144/1000 | Loss: 0.00001372
Iteration 145/1000 | Loss: 0.00001372
Iteration 146/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.3719438356929459e-05, 1.3719438356929459e-05, 1.3719438356929459e-05, 1.3719438356929459e-05, 1.3719438356929459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3719438356929459e-05

Optimization complete. Final v2v error: 3.1228930950164795 mm

Highest mean error: 3.9344098567962646 mm for frame 0

Lowest mean error: 2.688262939453125 mm for frame 208

Saving results

Total time: 43.91321086883545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893461
Iteration 2/25 | Loss: 0.00177645
Iteration 3/25 | Loss: 0.00134239
Iteration 4/25 | Loss: 0.00125416
Iteration 5/25 | Loss: 0.00120326
Iteration 6/25 | Loss: 0.00115413
Iteration 7/25 | Loss: 0.00113028
Iteration 8/25 | Loss: 0.00112654
Iteration 9/25 | Loss: 0.00113274
Iteration 10/25 | Loss: 0.00112918
Iteration 11/25 | Loss: 0.00112514
Iteration 12/25 | Loss: 0.00112359
Iteration 13/25 | Loss: 0.00112319
Iteration 14/25 | Loss: 0.00112316
Iteration 15/25 | Loss: 0.00112316
Iteration 16/25 | Loss: 0.00112316
Iteration 17/25 | Loss: 0.00112315
Iteration 18/25 | Loss: 0.00112315
Iteration 19/25 | Loss: 0.00112315
Iteration 20/25 | Loss: 0.00112315
Iteration 21/25 | Loss: 0.00112315
Iteration 22/25 | Loss: 0.00112315
Iteration 23/25 | Loss: 0.00112315
Iteration 24/25 | Loss: 0.00112315
Iteration 25/25 | Loss: 0.00112315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59935522
Iteration 2/25 | Loss: 0.00055938
Iteration 3/25 | Loss: 0.00055938
Iteration 4/25 | Loss: 0.00055938
Iteration 5/25 | Loss: 0.00055938
Iteration 6/25 | Loss: 0.00055938
Iteration 7/25 | Loss: 0.00055938
Iteration 8/25 | Loss: 0.00055938
Iteration 9/25 | Loss: 0.00055938
Iteration 10/25 | Loss: 0.00055938
Iteration 11/25 | Loss: 0.00055938
Iteration 12/25 | Loss: 0.00055938
Iteration 13/25 | Loss: 0.00055938
Iteration 14/25 | Loss: 0.00055938
Iteration 15/25 | Loss: 0.00055938
Iteration 16/25 | Loss: 0.00055938
Iteration 17/25 | Loss: 0.00055938
Iteration 18/25 | Loss: 0.00055938
Iteration 19/25 | Loss: 0.00055938
Iteration 20/25 | Loss: 0.00055938
Iteration 21/25 | Loss: 0.00055938
Iteration 22/25 | Loss: 0.00055938
Iteration 23/25 | Loss: 0.00055938
Iteration 24/25 | Loss: 0.00055938
Iteration 25/25 | Loss: 0.00055938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055938
Iteration 2/1000 | Loss: 0.00004991
Iteration 3/1000 | Loss: 0.00003172
Iteration 4/1000 | Loss: 0.00002674
Iteration 5/1000 | Loss: 0.00002454
Iteration 6/1000 | Loss: 0.00002373
Iteration 7/1000 | Loss: 0.00002319
Iteration 8/1000 | Loss: 0.00002283
Iteration 9/1000 | Loss: 0.00002255
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002213
Iteration 12/1000 | Loss: 0.00002200
Iteration 13/1000 | Loss: 0.00002195
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002185
Iteration 16/1000 | Loss: 0.00002184
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002178
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002173
Iteration 22/1000 | Loss: 0.00002172
Iteration 23/1000 | Loss: 0.00002166
Iteration 24/1000 | Loss: 0.00002164
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002162
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002161
Iteration 33/1000 | Loss: 0.00002160
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002159
Iteration 36/1000 | Loss: 0.00002159
Iteration 37/1000 | Loss: 0.00002159
Iteration 38/1000 | Loss: 0.00002159
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002158
Iteration 41/1000 | Loss: 0.00002158
Iteration 42/1000 | Loss: 0.00002157
Iteration 43/1000 | Loss: 0.00002156
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002156
Iteration 46/1000 | Loss: 0.00002156
Iteration 47/1000 | Loss: 0.00002156
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002155
Iteration 50/1000 | Loss: 0.00002155
Iteration 51/1000 | Loss: 0.00002155
Iteration 52/1000 | Loss: 0.00002154
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002154
Iteration 55/1000 | Loss: 0.00002154
Iteration 56/1000 | Loss: 0.00002154
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002153
Iteration 62/1000 | Loss: 0.00002153
Iteration 63/1000 | Loss: 0.00002153
Iteration 64/1000 | Loss: 0.00002153
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00002151
Iteration 74/1000 | Loss: 0.00002151
Iteration 75/1000 | Loss: 0.00002151
Iteration 76/1000 | Loss: 0.00002150
Iteration 77/1000 | Loss: 0.00002150
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00002150
Iteration 80/1000 | Loss: 0.00002150
Iteration 81/1000 | Loss: 0.00002150
Iteration 82/1000 | Loss: 0.00002150
Iteration 83/1000 | Loss: 0.00002150
Iteration 84/1000 | Loss: 0.00002150
Iteration 85/1000 | Loss: 0.00002150
Iteration 86/1000 | Loss: 0.00002150
Iteration 87/1000 | Loss: 0.00002150
Iteration 88/1000 | Loss: 0.00002150
Iteration 89/1000 | Loss: 0.00002150
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002150
Iteration 92/1000 | Loss: 0.00002150
Iteration 93/1000 | Loss: 0.00002150
Iteration 94/1000 | Loss: 0.00002150
Iteration 95/1000 | Loss: 0.00002150
Iteration 96/1000 | Loss: 0.00002150
Iteration 97/1000 | Loss: 0.00002150
Iteration 98/1000 | Loss: 0.00002150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.150299769709818e-05, 2.150299769709818e-05, 2.150299769709818e-05, 2.150299769709818e-05, 2.150299769709818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.150299769709818e-05

Optimization complete. Final v2v error: 3.728262424468994 mm

Highest mean error: 5.128085613250732 mm for frame 90

Lowest mean error: 2.659968137741089 mm for frame 3

Saving results

Total time: 48.79326248168945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385587
Iteration 2/25 | Loss: 0.00103715
Iteration 3/25 | Loss: 0.00097115
Iteration 4/25 | Loss: 0.00096247
Iteration 5/25 | Loss: 0.00095973
Iteration 6/25 | Loss: 0.00095917
Iteration 7/25 | Loss: 0.00095917
Iteration 8/25 | Loss: 0.00095917
Iteration 9/25 | Loss: 0.00095917
Iteration 10/25 | Loss: 0.00095917
Iteration 11/25 | Loss: 0.00095917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009591676644049585, 0.0009591676644049585, 0.0009591676644049585, 0.0009591676644049585, 0.0009591676644049585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009591676644049585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63110685
Iteration 2/25 | Loss: 0.00072151
Iteration 3/25 | Loss: 0.00072150
Iteration 4/25 | Loss: 0.00072150
Iteration 5/25 | Loss: 0.00072150
Iteration 6/25 | Loss: 0.00072150
Iteration 7/25 | Loss: 0.00072150
Iteration 8/25 | Loss: 0.00072150
Iteration 9/25 | Loss: 0.00072150
Iteration 10/25 | Loss: 0.00072150
Iteration 11/25 | Loss: 0.00072150
Iteration 12/25 | Loss: 0.00072150
Iteration 13/25 | Loss: 0.00072150
Iteration 14/25 | Loss: 0.00072150
Iteration 15/25 | Loss: 0.00072150
Iteration 16/25 | Loss: 0.00072150
Iteration 17/25 | Loss: 0.00072150
Iteration 18/25 | Loss: 0.00072150
Iteration 19/25 | Loss: 0.00072150
Iteration 20/25 | Loss: 0.00072150
Iteration 21/25 | Loss: 0.00072150
Iteration 22/25 | Loss: 0.00072150
Iteration 23/25 | Loss: 0.00072150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007214999059215188, 0.0007214999059215188, 0.0007214999059215188, 0.0007214999059215188, 0.0007214999059215188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007214999059215188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072150
Iteration 2/1000 | Loss: 0.00001824
Iteration 3/1000 | Loss: 0.00001184
Iteration 4/1000 | Loss: 0.00001059
Iteration 5/1000 | Loss: 0.00000994
Iteration 6/1000 | Loss: 0.00000965
Iteration 7/1000 | Loss: 0.00000939
Iteration 8/1000 | Loss: 0.00000928
Iteration 9/1000 | Loss: 0.00000925
Iteration 10/1000 | Loss: 0.00000923
Iteration 11/1000 | Loss: 0.00000919
Iteration 12/1000 | Loss: 0.00000900
Iteration 13/1000 | Loss: 0.00000897
Iteration 14/1000 | Loss: 0.00000894
Iteration 15/1000 | Loss: 0.00000893
Iteration 16/1000 | Loss: 0.00000893
Iteration 17/1000 | Loss: 0.00000891
Iteration 18/1000 | Loss: 0.00000890
Iteration 19/1000 | Loss: 0.00000890
Iteration 20/1000 | Loss: 0.00000890
Iteration 21/1000 | Loss: 0.00000890
Iteration 22/1000 | Loss: 0.00000888
Iteration 23/1000 | Loss: 0.00000886
Iteration 24/1000 | Loss: 0.00000881
Iteration 25/1000 | Loss: 0.00000879
Iteration 26/1000 | Loss: 0.00000878
Iteration 27/1000 | Loss: 0.00000878
Iteration 28/1000 | Loss: 0.00000878
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000874
Iteration 31/1000 | Loss: 0.00000874
Iteration 32/1000 | Loss: 0.00000873
Iteration 33/1000 | Loss: 0.00000873
Iteration 34/1000 | Loss: 0.00000872
Iteration 35/1000 | Loss: 0.00000872
Iteration 36/1000 | Loss: 0.00000871
Iteration 37/1000 | Loss: 0.00000871
Iteration 38/1000 | Loss: 0.00000870
Iteration 39/1000 | Loss: 0.00000869
Iteration 40/1000 | Loss: 0.00000869
Iteration 41/1000 | Loss: 0.00000869
Iteration 42/1000 | Loss: 0.00000869
Iteration 43/1000 | Loss: 0.00000868
Iteration 44/1000 | Loss: 0.00000868
Iteration 45/1000 | Loss: 0.00000866
Iteration 46/1000 | Loss: 0.00000866
Iteration 47/1000 | Loss: 0.00000865
Iteration 48/1000 | Loss: 0.00000865
Iteration 49/1000 | Loss: 0.00000865
Iteration 50/1000 | Loss: 0.00000865
Iteration 51/1000 | Loss: 0.00000864
Iteration 52/1000 | Loss: 0.00000864
Iteration 53/1000 | Loss: 0.00000863
Iteration 54/1000 | Loss: 0.00000863
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00000861
Iteration 57/1000 | Loss: 0.00000860
Iteration 58/1000 | Loss: 0.00000860
Iteration 59/1000 | Loss: 0.00000860
Iteration 60/1000 | Loss: 0.00000859
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000859
Iteration 65/1000 | Loss: 0.00000858
Iteration 66/1000 | Loss: 0.00000857
Iteration 67/1000 | Loss: 0.00000857
Iteration 68/1000 | Loss: 0.00000857
Iteration 69/1000 | Loss: 0.00000856
Iteration 70/1000 | Loss: 0.00000856
Iteration 71/1000 | Loss: 0.00000856
Iteration 72/1000 | Loss: 0.00000856
Iteration 73/1000 | Loss: 0.00000856
Iteration 74/1000 | Loss: 0.00000856
Iteration 75/1000 | Loss: 0.00000856
Iteration 76/1000 | Loss: 0.00000856
Iteration 77/1000 | Loss: 0.00000856
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000856
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000855
Iteration 82/1000 | Loss: 0.00000855
Iteration 83/1000 | Loss: 0.00000855
Iteration 84/1000 | Loss: 0.00000855
Iteration 85/1000 | Loss: 0.00000855
Iteration 86/1000 | Loss: 0.00000855
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000854
Iteration 89/1000 | Loss: 0.00000854
Iteration 90/1000 | Loss: 0.00000854
Iteration 91/1000 | Loss: 0.00000854
Iteration 92/1000 | Loss: 0.00000853
Iteration 93/1000 | Loss: 0.00000853
Iteration 94/1000 | Loss: 0.00000853
Iteration 95/1000 | Loss: 0.00000852
Iteration 96/1000 | Loss: 0.00000852
Iteration 97/1000 | Loss: 0.00000852
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000850
Iteration 102/1000 | Loss: 0.00000850
Iteration 103/1000 | Loss: 0.00000850
Iteration 104/1000 | Loss: 0.00000850
Iteration 105/1000 | Loss: 0.00000850
Iteration 106/1000 | Loss: 0.00000850
Iteration 107/1000 | Loss: 0.00000850
Iteration 108/1000 | Loss: 0.00000850
Iteration 109/1000 | Loss: 0.00000850
Iteration 110/1000 | Loss: 0.00000849
Iteration 111/1000 | Loss: 0.00000849
Iteration 112/1000 | Loss: 0.00000849
Iteration 113/1000 | Loss: 0.00000849
Iteration 114/1000 | Loss: 0.00000849
Iteration 115/1000 | Loss: 0.00000848
Iteration 116/1000 | Loss: 0.00000848
Iteration 117/1000 | Loss: 0.00000848
Iteration 118/1000 | Loss: 0.00000848
Iteration 119/1000 | Loss: 0.00000847
Iteration 120/1000 | Loss: 0.00000847
Iteration 121/1000 | Loss: 0.00000847
Iteration 122/1000 | Loss: 0.00000847
Iteration 123/1000 | Loss: 0.00000847
Iteration 124/1000 | Loss: 0.00000847
Iteration 125/1000 | Loss: 0.00000846
Iteration 126/1000 | Loss: 0.00000846
Iteration 127/1000 | Loss: 0.00000846
Iteration 128/1000 | Loss: 0.00000846
Iteration 129/1000 | Loss: 0.00000846
Iteration 130/1000 | Loss: 0.00000845
Iteration 131/1000 | Loss: 0.00000845
Iteration 132/1000 | Loss: 0.00000845
Iteration 133/1000 | Loss: 0.00000845
Iteration 134/1000 | Loss: 0.00000845
Iteration 135/1000 | Loss: 0.00000845
Iteration 136/1000 | Loss: 0.00000845
Iteration 137/1000 | Loss: 0.00000845
Iteration 138/1000 | Loss: 0.00000845
Iteration 139/1000 | Loss: 0.00000845
Iteration 140/1000 | Loss: 0.00000845
Iteration 141/1000 | Loss: 0.00000845
Iteration 142/1000 | Loss: 0.00000845
Iteration 143/1000 | Loss: 0.00000845
Iteration 144/1000 | Loss: 0.00000844
Iteration 145/1000 | Loss: 0.00000844
Iteration 146/1000 | Loss: 0.00000844
Iteration 147/1000 | Loss: 0.00000844
Iteration 148/1000 | Loss: 0.00000844
Iteration 149/1000 | Loss: 0.00000843
Iteration 150/1000 | Loss: 0.00000843
Iteration 151/1000 | Loss: 0.00000843
Iteration 152/1000 | Loss: 0.00000842
Iteration 153/1000 | Loss: 0.00000842
Iteration 154/1000 | Loss: 0.00000842
Iteration 155/1000 | Loss: 0.00000842
Iteration 156/1000 | Loss: 0.00000842
Iteration 157/1000 | Loss: 0.00000842
Iteration 158/1000 | Loss: 0.00000842
Iteration 159/1000 | Loss: 0.00000842
Iteration 160/1000 | Loss: 0.00000841
Iteration 161/1000 | Loss: 0.00000841
Iteration 162/1000 | Loss: 0.00000841
Iteration 163/1000 | Loss: 0.00000841
Iteration 164/1000 | Loss: 0.00000841
Iteration 165/1000 | Loss: 0.00000841
Iteration 166/1000 | Loss: 0.00000841
Iteration 167/1000 | Loss: 0.00000841
Iteration 168/1000 | Loss: 0.00000841
Iteration 169/1000 | Loss: 0.00000840
Iteration 170/1000 | Loss: 0.00000840
Iteration 171/1000 | Loss: 0.00000840
Iteration 172/1000 | Loss: 0.00000840
Iteration 173/1000 | Loss: 0.00000840
Iteration 174/1000 | Loss: 0.00000840
Iteration 175/1000 | Loss: 0.00000840
Iteration 176/1000 | Loss: 0.00000840
Iteration 177/1000 | Loss: 0.00000840
Iteration 178/1000 | Loss: 0.00000840
Iteration 179/1000 | Loss: 0.00000840
Iteration 180/1000 | Loss: 0.00000839
Iteration 181/1000 | Loss: 0.00000839
Iteration 182/1000 | Loss: 0.00000839
Iteration 183/1000 | Loss: 0.00000839
Iteration 184/1000 | Loss: 0.00000839
Iteration 185/1000 | Loss: 0.00000839
Iteration 186/1000 | Loss: 0.00000839
Iteration 187/1000 | Loss: 0.00000839
Iteration 188/1000 | Loss: 0.00000838
Iteration 189/1000 | Loss: 0.00000838
Iteration 190/1000 | Loss: 0.00000838
Iteration 191/1000 | Loss: 0.00000838
Iteration 192/1000 | Loss: 0.00000838
Iteration 193/1000 | Loss: 0.00000838
Iteration 194/1000 | Loss: 0.00000838
Iteration 195/1000 | Loss: 0.00000838
Iteration 196/1000 | Loss: 0.00000838
Iteration 197/1000 | Loss: 0.00000838
Iteration 198/1000 | Loss: 0.00000838
Iteration 199/1000 | Loss: 0.00000838
Iteration 200/1000 | Loss: 0.00000838
Iteration 201/1000 | Loss: 0.00000838
Iteration 202/1000 | Loss: 0.00000837
Iteration 203/1000 | Loss: 0.00000837
Iteration 204/1000 | Loss: 0.00000837
Iteration 205/1000 | Loss: 0.00000837
Iteration 206/1000 | Loss: 0.00000837
Iteration 207/1000 | Loss: 0.00000837
Iteration 208/1000 | Loss: 0.00000837
Iteration 209/1000 | Loss: 0.00000837
Iteration 210/1000 | Loss: 0.00000837
Iteration 211/1000 | Loss: 0.00000837
Iteration 212/1000 | Loss: 0.00000837
Iteration 213/1000 | Loss: 0.00000837
Iteration 214/1000 | Loss: 0.00000837
Iteration 215/1000 | Loss: 0.00000837
Iteration 216/1000 | Loss: 0.00000836
Iteration 217/1000 | Loss: 0.00000836
Iteration 218/1000 | Loss: 0.00000836
Iteration 219/1000 | Loss: 0.00000836
Iteration 220/1000 | Loss: 0.00000836
Iteration 221/1000 | Loss: 0.00000836
Iteration 222/1000 | Loss: 0.00000836
Iteration 223/1000 | Loss: 0.00000836
Iteration 224/1000 | Loss: 0.00000836
Iteration 225/1000 | Loss: 0.00000836
Iteration 226/1000 | Loss: 0.00000836
Iteration 227/1000 | Loss: 0.00000836
Iteration 228/1000 | Loss: 0.00000836
Iteration 229/1000 | Loss: 0.00000836
Iteration 230/1000 | Loss: 0.00000836
Iteration 231/1000 | Loss: 0.00000836
Iteration 232/1000 | Loss: 0.00000836
Iteration 233/1000 | Loss: 0.00000836
Iteration 234/1000 | Loss: 0.00000836
Iteration 235/1000 | Loss: 0.00000836
Iteration 236/1000 | Loss: 0.00000836
Iteration 237/1000 | Loss: 0.00000836
Iteration 238/1000 | Loss: 0.00000836
Iteration 239/1000 | Loss: 0.00000836
Iteration 240/1000 | Loss: 0.00000835
Iteration 241/1000 | Loss: 0.00000835
Iteration 242/1000 | Loss: 0.00000835
Iteration 243/1000 | Loss: 0.00000835
Iteration 244/1000 | Loss: 0.00000835
Iteration 245/1000 | Loss: 0.00000835
Iteration 246/1000 | Loss: 0.00000835
Iteration 247/1000 | Loss: 0.00000835
Iteration 248/1000 | Loss: 0.00000835
Iteration 249/1000 | Loss: 0.00000835
Iteration 250/1000 | Loss: 0.00000835
Iteration 251/1000 | Loss: 0.00000834
Iteration 252/1000 | Loss: 0.00000834
Iteration 253/1000 | Loss: 0.00000834
Iteration 254/1000 | Loss: 0.00000834
Iteration 255/1000 | Loss: 0.00000834
Iteration 256/1000 | Loss: 0.00000834
Iteration 257/1000 | Loss: 0.00000834
Iteration 258/1000 | Loss: 0.00000834
Iteration 259/1000 | Loss: 0.00000834
Iteration 260/1000 | Loss: 0.00000834
Iteration 261/1000 | Loss: 0.00000834
Iteration 262/1000 | Loss: 0.00000834
Iteration 263/1000 | Loss: 0.00000834
Iteration 264/1000 | Loss: 0.00000834
Iteration 265/1000 | Loss: 0.00000834
Iteration 266/1000 | Loss: 0.00000834
Iteration 267/1000 | Loss: 0.00000834
Iteration 268/1000 | Loss: 0.00000834
Iteration 269/1000 | Loss: 0.00000834
Iteration 270/1000 | Loss: 0.00000834
Iteration 271/1000 | Loss: 0.00000834
Iteration 272/1000 | Loss: 0.00000834
Iteration 273/1000 | Loss: 0.00000833
Iteration 274/1000 | Loss: 0.00000833
Iteration 275/1000 | Loss: 0.00000833
Iteration 276/1000 | Loss: 0.00000833
Iteration 277/1000 | Loss: 0.00000833
Iteration 278/1000 | Loss: 0.00000833
Iteration 279/1000 | Loss: 0.00000833
Iteration 280/1000 | Loss: 0.00000833
Iteration 281/1000 | Loss: 0.00000833
Iteration 282/1000 | Loss: 0.00000833
Iteration 283/1000 | Loss: 0.00000833
Iteration 284/1000 | Loss: 0.00000833
Iteration 285/1000 | Loss: 0.00000833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [8.332339348271489e-06, 8.332339348271489e-06, 8.332339348271489e-06, 8.332339348271489e-06, 8.332339348271489e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.332339348271489e-06

Optimization complete. Final v2v error: 2.4928486347198486 mm

Highest mean error: 2.8328022956848145 mm for frame 103

Lowest mean error: 2.2662017345428467 mm for frame 68

Saving results

Total time: 43.14492225646973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627278
Iteration 2/25 | Loss: 0.00107284
Iteration 3/25 | Loss: 0.00098570
Iteration 4/25 | Loss: 0.00097543
Iteration 5/25 | Loss: 0.00097180
Iteration 6/25 | Loss: 0.00097068
Iteration 7/25 | Loss: 0.00097068
Iteration 8/25 | Loss: 0.00097068
Iteration 9/25 | Loss: 0.00097068
Iteration 10/25 | Loss: 0.00097068
Iteration 11/25 | Loss: 0.00097068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009706799755804241, 0.0009706799755804241, 0.0009706799755804241, 0.0009706799755804241, 0.0009706799755804241]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009706799755804241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41476297
Iteration 2/25 | Loss: 0.00074832
Iteration 3/25 | Loss: 0.00074832
Iteration 4/25 | Loss: 0.00074832
Iteration 5/25 | Loss: 0.00074832
Iteration 6/25 | Loss: 0.00074832
Iteration 7/25 | Loss: 0.00074832
Iteration 8/25 | Loss: 0.00074832
Iteration 9/25 | Loss: 0.00074832
Iteration 10/25 | Loss: 0.00074832
Iteration 11/25 | Loss: 0.00074832
Iteration 12/25 | Loss: 0.00074832
Iteration 13/25 | Loss: 0.00074832
Iteration 14/25 | Loss: 0.00074832
Iteration 15/25 | Loss: 0.00074832
Iteration 16/25 | Loss: 0.00074832
Iteration 17/25 | Loss: 0.00074832
Iteration 18/25 | Loss: 0.00074832
Iteration 19/25 | Loss: 0.00074832
Iteration 20/25 | Loss: 0.00074832
Iteration 21/25 | Loss: 0.00074832
Iteration 22/25 | Loss: 0.00074832
Iteration 23/25 | Loss: 0.00074832
Iteration 24/25 | Loss: 0.00074832
Iteration 25/25 | Loss: 0.00074832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074832
Iteration 2/1000 | Loss: 0.00001648
Iteration 3/1000 | Loss: 0.00001037
Iteration 4/1000 | Loss: 0.00000922
Iteration 5/1000 | Loss: 0.00000866
Iteration 6/1000 | Loss: 0.00000838
Iteration 7/1000 | Loss: 0.00000810
Iteration 8/1000 | Loss: 0.00000805
Iteration 9/1000 | Loss: 0.00000804
Iteration 10/1000 | Loss: 0.00000795
Iteration 11/1000 | Loss: 0.00000793
Iteration 12/1000 | Loss: 0.00000793
Iteration 13/1000 | Loss: 0.00000782
Iteration 14/1000 | Loss: 0.00000774
Iteration 15/1000 | Loss: 0.00000773
Iteration 16/1000 | Loss: 0.00000773
Iteration 17/1000 | Loss: 0.00000772
Iteration 18/1000 | Loss: 0.00000772
Iteration 19/1000 | Loss: 0.00000772
Iteration 20/1000 | Loss: 0.00000772
Iteration 21/1000 | Loss: 0.00000771
Iteration 22/1000 | Loss: 0.00000769
Iteration 23/1000 | Loss: 0.00000768
Iteration 24/1000 | Loss: 0.00000767
Iteration 25/1000 | Loss: 0.00000767
Iteration 26/1000 | Loss: 0.00000766
Iteration 27/1000 | Loss: 0.00000765
Iteration 28/1000 | Loss: 0.00000763
Iteration 29/1000 | Loss: 0.00000762
Iteration 30/1000 | Loss: 0.00000762
Iteration 31/1000 | Loss: 0.00000762
Iteration 32/1000 | Loss: 0.00000762
Iteration 33/1000 | Loss: 0.00000762
Iteration 34/1000 | Loss: 0.00000762
Iteration 35/1000 | Loss: 0.00000762
Iteration 36/1000 | Loss: 0.00000762
Iteration 37/1000 | Loss: 0.00000761
Iteration 38/1000 | Loss: 0.00000761
Iteration 39/1000 | Loss: 0.00000761
Iteration 40/1000 | Loss: 0.00000760
Iteration 41/1000 | Loss: 0.00000759
Iteration 42/1000 | Loss: 0.00000758
Iteration 43/1000 | Loss: 0.00000758
Iteration 44/1000 | Loss: 0.00000757
Iteration 45/1000 | Loss: 0.00000757
Iteration 46/1000 | Loss: 0.00000756
Iteration 47/1000 | Loss: 0.00000756
Iteration 48/1000 | Loss: 0.00000756
Iteration 49/1000 | Loss: 0.00000755
Iteration 50/1000 | Loss: 0.00000755
Iteration 51/1000 | Loss: 0.00000755
Iteration 52/1000 | Loss: 0.00000755
Iteration 53/1000 | Loss: 0.00000754
Iteration 54/1000 | Loss: 0.00000754
Iteration 55/1000 | Loss: 0.00000754
Iteration 56/1000 | Loss: 0.00000754
Iteration 57/1000 | Loss: 0.00000754
Iteration 58/1000 | Loss: 0.00000754
Iteration 59/1000 | Loss: 0.00000754
Iteration 60/1000 | Loss: 0.00000754
Iteration 61/1000 | Loss: 0.00000754
Iteration 62/1000 | Loss: 0.00000753
Iteration 63/1000 | Loss: 0.00000753
Iteration 64/1000 | Loss: 0.00000753
Iteration 65/1000 | Loss: 0.00000752
Iteration 66/1000 | Loss: 0.00000752
Iteration 67/1000 | Loss: 0.00000752
Iteration 68/1000 | Loss: 0.00000752
Iteration 69/1000 | Loss: 0.00000751
Iteration 70/1000 | Loss: 0.00000751
Iteration 71/1000 | Loss: 0.00000751
Iteration 72/1000 | Loss: 0.00000750
Iteration 73/1000 | Loss: 0.00000750
Iteration 74/1000 | Loss: 0.00000750
Iteration 75/1000 | Loss: 0.00000750
Iteration 76/1000 | Loss: 0.00000749
Iteration 77/1000 | Loss: 0.00000749
Iteration 78/1000 | Loss: 0.00000748
Iteration 79/1000 | Loss: 0.00000748
Iteration 80/1000 | Loss: 0.00000748
Iteration 81/1000 | Loss: 0.00000748
Iteration 82/1000 | Loss: 0.00000746
Iteration 83/1000 | Loss: 0.00000746
Iteration 84/1000 | Loss: 0.00000745
Iteration 85/1000 | Loss: 0.00000745
Iteration 86/1000 | Loss: 0.00000745
Iteration 87/1000 | Loss: 0.00000745
Iteration 88/1000 | Loss: 0.00000745
Iteration 89/1000 | Loss: 0.00000745
Iteration 90/1000 | Loss: 0.00000745
Iteration 91/1000 | Loss: 0.00000745
Iteration 92/1000 | Loss: 0.00000745
Iteration 93/1000 | Loss: 0.00000744
Iteration 94/1000 | Loss: 0.00000744
Iteration 95/1000 | Loss: 0.00000743
Iteration 96/1000 | Loss: 0.00000743
Iteration 97/1000 | Loss: 0.00000743
Iteration 98/1000 | Loss: 0.00000742
Iteration 99/1000 | Loss: 0.00000742
Iteration 100/1000 | Loss: 0.00000742
Iteration 101/1000 | Loss: 0.00000742
Iteration 102/1000 | Loss: 0.00000742
Iteration 103/1000 | Loss: 0.00000742
Iteration 104/1000 | Loss: 0.00000742
Iteration 105/1000 | Loss: 0.00000742
Iteration 106/1000 | Loss: 0.00000742
Iteration 107/1000 | Loss: 0.00000742
Iteration 108/1000 | Loss: 0.00000742
Iteration 109/1000 | Loss: 0.00000742
Iteration 110/1000 | Loss: 0.00000742
Iteration 111/1000 | Loss: 0.00000742
Iteration 112/1000 | Loss: 0.00000741
Iteration 113/1000 | Loss: 0.00000741
Iteration 114/1000 | Loss: 0.00000741
Iteration 115/1000 | Loss: 0.00000741
Iteration 116/1000 | Loss: 0.00000741
Iteration 117/1000 | Loss: 0.00000741
Iteration 118/1000 | Loss: 0.00000741
Iteration 119/1000 | Loss: 0.00000741
Iteration 120/1000 | Loss: 0.00000740
Iteration 121/1000 | Loss: 0.00000740
Iteration 122/1000 | Loss: 0.00000740
Iteration 123/1000 | Loss: 0.00000740
Iteration 124/1000 | Loss: 0.00000740
Iteration 125/1000 | Loss: 0.00000740
Iteration 126/1000 | Loss: 0.00000740
Iteration 127/1000 | Loss: 0.00000740
Iteration 128/1000 | Loss: 0.00000740
Iteration 129/1000 | Loss: 0.00000740
Iteration 130/1000 | Loss: 0.00000740
Iteration 131/1000 | Loss: 0.00000740
Iteration 132/1000 | Loss: 0.00000740
Iteration 133/1000 | Loss: 0.00000740
Iteration 134/1000 | Loss: 0.00000740
Iteration 135/1000 | Loss: 0.00000740
Iteration 136/1000 | Loss: 0.00000740
Iteration 137/1000 | Loss: 0.00000740
Iteration 138/1000 | Loss: 0.00000740
Iteration 139/1000 | Loss: 0.00000740
Iteration 140/1000 | Loss: 0.00000740
Iteration 141/1000 | Loss: 0.00000740
Iteration 142/1000 | Loss: 0.00000740
Iteration 143/1000 | Loss: 0.00000740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [7.397710760415066e-06, 7.397710760415066e-06, 7.397710760415066e-06, 7.397710760415066e-06, 7.397710760415066e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.397710760415066e-06

Optimization complete. Final v2v error: 2.3353424072265625 mm

Highest mean error: 2.9235002994537354 mm for frame 80

Lowest mean error: 2.1536123752593994 mm for frame 105

Saving results

Total time: 31.96680998802185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503533
Iteration 2/25 | Loss: 0.00114152
Iteration 3/25 | Loss: 0.00103162
Iteration 4/25 | Loss: 0.00101406
Iteration 5/25 | Loss: 0.00100912
Iteration 6/25 | Loss: 0.00100861
Iteration 7/25 | Loss: 0.00100861
Iteration 8/25 | Loss: 0.00100861
Iteration 9/25 | Loss: 0.00100861
Iteration 10/25 | Loss: 0.00100861
Iteration 11/25 | Loss: 0.00100861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010086074471473694, 0.0010086074471473694, 0.0010086074471473694, 0.0010086074471473694, 0.0010086074471473694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010086074471473694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35818136
Iteration 2/25 | Loss: 0.00065520
Iteration 3/25 | Loss: 0.00065520
Iteration 4/25 | Loss: 0.00065520
Iteration 5/25 | Loss: 0.00065520
Iteration 6/25 | Loss: 0.00065520
Iteration 7/25 | Loss: 0.00065520
Iteration 8/25 | Loss: 0.00065520
Iteration 9/25 | Loss: 0.00065520
Iteration 10/25 | Loss: 0.00065520
Iteration 11/25 | Loss: 0.00065520
Iteration 12/25 | Loss: 0.00065520
Iteration 13/25 | Loss: 0.00065520
Iteration 14/25 | Loss: 0.00065520
Iteration 15/25 | Loss: 0.00065520
Iteration 16/25 | Loss: 0.00065520
Iteration 17/25 | Loss: 0.00065520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006551992264576256, 0.0006551992264576256, 0.0006551992264576256, 0.0006551992264576256, 0.0006551992264576256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006551992264576256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065520
Iteration 2/1000 | Loss: 0.00001987
Iteration 3/1000 | Loss: 0.00001460
Iteration 4/1000 | Loss: 0.00001281
Iteration 5/1000 | Loss: 0.00001169
Iteration 6/1000 | Loss: 0.00001115
Iteration 7/1000 | Loss: 0.00001085
Iteration 8/1000 | Loss: 0.00001058
Iteration 9/1000 | Loss: 0.00001032
Iteration 10/1000 | Loss: 0.00001030
Iteration 11/1000 | Loss: 0.00001019
Iteration 12/1000 | Loss: 0.00001009
Iteration 13/1000 | Loss: 0.00001006
Iteration 14/1000 | Loss: 0.00001000
Iteration 15/1000 | Loss: 0.00000998
Iteration 16/1000 | Loss: 0.00000997
Iteration 17/1000 | Loss: 0.00000995
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000992
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000991
Iteration 26/1000 | Loss: 0.00000991
Iteration 27/1000 | Loss: 0.00000990
Iteration 28/1000 | Loss: 0.00000988
Iteration 29/1000 | Loss: 0.00000988
Iteration 30/1000 | Loss: 0.00000988
Iteration 31/1000 | Loss: 0.00000988
Iteration 32/1000 | Loss: 0.00000988
Iteration 33/1000 | Loss: 0.00000988
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000988
Iteration 36/1000 | Loss: 0.00000988
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000987
Iteration 42/1000 | Loss: 0.00000987
Iteration 43/1000 | Loss: 0.00000987
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000986
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000983
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000982
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000982
Iteration 69/1000 | Loss: 0.00000982
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000982
Iteration 73/1000 | Loss: 0.00000982
Iteration 74/1000 | Loss: 0.00000982
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000981
Iteration 82/1000 | Loss: 0.00000981
Iteration 83/1000 | Loss: 0.00000981
Iteration 84/1000 | Loss: 0.00000981
Iteration 85/1000 | Loss: 0.00000981
Iteration 86/1000 | Loss: 0.00000981
Iteration 87/1000 | Loss: 0.00000981
Iteration 88/1000 | Loss: 0.00000981
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000980
Iteration 106/1000 | Loss: 0.00000980
Iteration 107/1000 | Loss: 0.00000980
Iteration 108/1000 | Loss: 0.00000980
Iteration 109/1000 | Loss: 0.00000980
Iteration 110/1000 | Loss: 0.00000980
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000980
Iteration 121/1000 | Loss: 0.00000980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [9.802028216654435e-06, 9.802028216654435e-06, 9.802028216654435e-06, 9.802028216654435e-06, 9.802028216654435e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.802028216654435e-06

Optimization complete. Final v2v error: 2.683467149734497 mm

Highest mean error: 2.879127025604248 mm for frame 138

Lowest mean error: 2.5438523292541504 mm for frame 212

Saving results

Total time: 34.73602271080017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042677
Iteration 2/25 | Loss: 0.00138641
Iteration 3/25 | Loss: 0.00114502
Iteration 4/25 | Loss: 0.00111394
Iteration 5/25 | Loss: 0.00110485
Iteration 6/25 | Loss: 0.00110177
Iteration 7/25 | Loss: 0.00110175
Iteration 8/25 | Loss: 0.00110175
Iteration 9/25 | Loss: 0.00110175
Iteration 10/25 | Loss: 0.00110175
Iteration 11/25 | Loss: 0.00110175
Iteration 12/25 | Loss: 0.00110175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011017455253750086, 0.0011017455253750086, 0.0011017455253750086, 0.0011017455253750086, 0.0011017455253750086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011017455253750086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74659848
Iteration 2/25 | Loss: 0.00084624
Iteration 3/25 | Loss: 0.00084615
Iteration 4/25 | Loss: 0.00084615
Iteration 5/25 | Loss: 0.00084615
Iteration 6/25 | Loss: 0.00084615
Iteration 7/25 | Loss: 0.00084615
Iteration 8/25 | Loss: 0.00084615
Iteration 9/25 | Loss: 0.00084615
Iteration 10/25 | Loss: 0.00084615
Iteration 11/25 | Loss: 0.00084615
Iteration 12/25 | Loss: 0.00084615
Iteration 13/25 | Loss: 0.00084615
Iteration 14/25 | Loss: 0.00084615
Iteration 15/25 | Loss: 0.00084615
Iteration 16/25 | Loss: 0.00084615
Iteration 17/25 | Loss: 0.00084615
Iteration 18/25 | Loss: 0.00084615
Iteration 19/25 | Loss: 0.00084615
Iteration 20/25 | Loss: 0.00084615
Iteration 21/25 | Loss: 0.00084615
Iteration 22/25 | Loss: 0.00084615
Iteration 23/25 | Loss: 0.00084615
Iteration 24/25 | Loss: 0.00084615
Iteration 25/25 | Loss: 0.00084615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084615
Iteration 2/1000 | Loss: 0.00006880
Iteration 3/1000 | Loss: 0.00004118
Iteration 4/1000 | Loss: 0.00003056
Iteration 5/1000 | Loss: 0.00002835
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002603
Iteration 8/1000 | Loss: 0.00002540
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002458
Iteration 11/1000 | Loss: 0.00002432
Iteration 12/1000 | Loss: 0.00002418
Iteration 13/1000 | Loss: 0.00002402
Iteration 14/1000 | Loss: 0.00002391
Iteration 15/1000 | Loss: 0.00002375
Iteration 16/1000 | Loss: 0.00002364
Iteration 17/1000 | Loss: 0.00002362
Iteration 18/1000 | Loss: 0.00002361
Iteration 19/1000 | Loss: 0.00002361
Iteration 20/1000 | Loss: 0.00002361
Iteration 21/1000 | Loss: 0.00002355
Iteration 22/1000 | Loss: 0.00002348
Iteration 23/1000 | Loss: 0.00002348
Iteration 24/1000 | Loss: 0.00002347
Iteration 25/1000 | Loss: 0.00002346
Iteration 26/1000 | Loss: 0.00002341
Iteration 27/1000 | Loss: 0.00002340
Iteration 28/1000 | Loss: 0.00002340
Iteration 29/1000 | Loss: 0.00002339
Iteration 30/1000 | Loss: 0.00002339
Iteration 31/1000 | Loss: 0.00002338
Iteration 32/1000 | Loss: 0.00002338
Iteration 33/1000 | Loss: 0.00002338
Iteration 34/1000 | Loss: 0.00002337
Iteration 35/1000 | Loss: 0.00002337
Iteration 36/1000 | Loss: 0.00002337
Iteration 37/1000 | Loss: 0.00002336
Iteration 38/1000 | Loss: 0.00002336
Iteration 39/1000 | Loss: 0.00002335
Iteration 40/1000 | Loss: 0.00002335
Iteration 41/1000 | Loss: 0.00002334
Iteration 42/1000 | Loss: 0.00002334
Iteration 43/1000 | Loss: 0.00002333
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002332
Iteration 46/1000 | Loss: 0.00002332
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002329
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00002329
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002327
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002327
Iteration 62/1000 | Loss: 0.00002326
Iteration 63/1000 | Loss: 0.00002326
Iteration 64/1000 | Loss: 0.00002326
Iteration 65/1000 | Loss: 0.00002326
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002325
Iteration 68/1000 | Loss: 0.00002325
Iteration 69/1000 | Loss: 0.00002325
Iteration 70/1000 | Loss: 0.00002325
Iteration 71/1000 | Loss: 0.00002325
Iteration 72/1000 | Loss: 0.00002325
Iteration 73/1000 | Loss: 0.00002325
Iteration 74/1000 | Loss: 0.00002325
Iteration 75/1000 | Loss: 0.00002324
Iteration 76/1000 | Loss: 0.00002324
Iteration 77/1000 | Loss: 0.00002324
Iteration 78/1000 | Loss: 0.00002324
Iteration 79/1000 | Loss: 0.00002324
Iteration 80/1000 | Loss: 0.00002324
Iteration 81/1000 | Loss: 0.00002324
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002323
Iteration 84/1000 | Loss: 0.00002323
Iteration 85/1000 | Loss: 0.00002323
Iteration 86/1000 | Loss: 0.00002323
Iteration 87/1000 | Loss: 0.00002322
Iteration 88/1000 | Loss: 0.00002322
Iteration 89/1000 | Loss: 0.00002322
Iteration 90/1000 | Loss: 0.00002322
Iteration 91/1000 | Loss: 0.00002321
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002321
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002320
Iteration 98/1000 | Loss: 0.00002320
Iteration 99/1000 | Loss: 0.00002320
Iteration 100/1000 | Loss: 0.00002320
Iteration 101/1000 | Loss: 0.00002319
Iteration 102/1000 | Loss: 0.00002319
Iteration 103/1000 | Loss: 0.00002319
Iteration 104/1000 | Loss: 0.00002319
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002318
Iteration 107/1000 | Loss: 0.00002318
Iteration 108/1000 | Loss: 0.00002317
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002316
Iteration 113/1000 | Loss: 0.00002316
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002316
Iteration 118/1000 | Loss: 0.00002316
Iteration 119/1000 | Loss: 0.00002315
Iteration 120/1000 | Loss: 0.00002315
Iteration 121/1000 | Loss: 0.00002315
Iteration 122/1000 | Loss: 0.00002315
Iteration 123/1000 | Loss: 0.00002315
Iteration 124/1000 | Loss: 0.00002315
Iteration 125/1000 | Loss: 0.00002315
Iteration 126/1000 | Loss: 0.00002315
Iteration 127/1000 | Loss: 0.00002315
Iteration 128/1000 | Loss: 0.00002315
Iteration 129/1000 | Loss: 0.00002314
Iteration 130/1000 | Loss: 0.00002314
Iteration 131/1000 | Loss: 0.00002314
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002314
Iteration 135/1000 | Loss: 0.00002314
Iteration 136/1000 | Loss: 0.00002314
Iteration 137/1000 | Loss: 0.00002314
Iteration 138/1000 | Loss: 0.00002314
Iteration 139/1000 | Loss: 0.00002314
Iteration 140/1000 | Loss: 0.00002314
Iteration 141/1000 | Loss: 0.00002314
Iteration 142/1000 | Loss: 0.00002314
Iteration 143/1000 | Loss: 0.00002314
Iteration 144/1000 | Loss: 0.00002314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.3143951693782583e-05, 2.3143951693782583e-05, 2.3143951693782583e-05, 2.3143951693782583e-05, 2.3143951693782583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3143951693782583e-05

Optimization complete. Final v2v error: 3.984659433364868 mm

Highest mean error: 5.359355449676514 mm for frame 112

Lowest mean error: 3.2858288288116455 mm for frame 68

Saving results

Total time: 48.97036790847778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078703
Iteration 2/25 | Loss: 0.00267738
Iteration 3/25 | Loss: 0.00237681
Iteration 4/25 | Loss: 0.00208496
Iteration 5/25 | Loss: 0.00167888
Iteration 6/25 | Loss: 0.00144313
Iteration 7/25 | Loss: 0.00133724
Iteration 8/25 | Loss: 0.00130797
Iteration 9/25 | Loss: 0.00133728
Iteration 10/25 | Loss: 0.00128118
Iteration 11/25 | Loss: 0.00123554
Iteration 12/25 | Loss: 0.00122414
Iteration 13/25 | Loss: 0.00120138
Iteration 14/25 | Loss: 0.00117877
Iteration 15/25 | Loss: 0.00115922
Iteration 16/25 | Loss: 0.00114722
Iteration 17/25 | Loss: 0.00112964
Iteration 18/25 | Loss: 0.00113158
Iteration 19/25 | Loss: 0.00112215
Iteration 20/25 | Loss: 0.00111702
Iteration 21/25 | Loss: 0.00111597
Iteration 22/25 | Loss: 0.00111594
Iteration 23/25 | Loss: 0.00112465
Iteration 24/25 | Loss: 0.00112547
Iteration 25/25 | Loss: 0.00113188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33374453
Iteration 2/25 | Loss: 0.00397488
Iteration 3/25 | Loss: 0.00178958
Iteration 4/25 | Loss: 0.00178958
Iteration 5/25 | Loss: 0.00178958
Iteration 6/25 | Loss: 0.00178958
Iteration 7/25 | Loss: 0.00178957
Iteration 8/25 | Loss: 0.00178957
Iteration 9/25 | Loss: 0.00178957
Iteration 10/25 | Loss: 0.00178957
Iteration 11/25 | Loss: 0.00178957
Iteration 12/25 | Loss: 0.00178957
Iteration 13/25 | Loss: 0.00178957
Iteration 14/25 | Loss: 0.00178957
Iteration 15/25 | Loss: 0.00178957
Iteration 16/25 | Loss: 0.00178957
Iteration 17/25 | Loss: 0.00178957
Iteration 18/25 | Loss: 0.00178957
Iteration 19/25 | Loss: 0.00178957
Iteration 20/25 | Loss: 0.00178957
Iteration 21/25 | Loss: 0.00178957
Iteration 22/25 | Loss: 0.00178957
Iteration 23/25 | Loss: 0.00178957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001789572648704052, 0.001789572648704052, 0.001789572648704052, 0.001789572648704052, 0.001789572648704052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001789572648704052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178957
Iteration 2/1000 | Loss: 0.00301961
Iteration 3/1000 | Loss: 0.00045321
Iteration 4/1000 | Loss: 0.00053774
Iteration 5/1000 | Loss: 0.00054355
Iteration 6/1000 | Loss: 0.00050114
Iteration 7/1000 | Loss: 0.00067747
Iteration 8/1000 | Loss: 0.00049838
Iteration 9/1000 | Loss: 0.00043515
Iteration 10/1000 | Loss: 0.00046601
Iteration 11/1000 | Loss: 0.00062579
Iteration 12/1000 | Loss: 0.00116677
Iteration 13/1000 | Loss: 0.00039809
Iteration 14/1000 | Loss: 0.00021747
Iteration 15/1000 | Loss: 0.00035819
Iteration 16/1000 | Loss: 0.00033681
Iteration 17/1000 | Loss: 0.00047285
Iteration 18/1000 | Loss: 0.00037213
Iteration 19/1000 | Loss: 0.00029415
Iteration 20/1000 | Loss: 0.00048769
Iteration 21/1000 | Loss: 0.00047208
Iteration 22/1000 | Loss: 0.00064931
Iteration 23/1000 | Loss: 0.00033897
Iteration 24/1000 | Loss: 0.00037552
Iteration 25/1000 | Loss: 0.00026161
Iteration 26/1000 | Loss: 0.00019800
Iteration 27/1000 | Loss: 0.00035070
Iteration 28/1000 | Loss: 0.00024853
Iteration 29/1000 | Loss: 0.00032460
Iteration 30/1000 | Loss: 0.00029062
Iteration 31/1000 | Loss: 0.00026164
Iteration 32/1000 | Loss: 0.00036319
Iteration 33/1000 | Loss: 0.00030070
Iteration 34/1000 | Loss: 0.00030025
Iteration 35/1000 | Loss: 0.00027670
Iteration 36/1000 | Loss: 0.00028371
Iteration 37/1000 | Loss: 0.00030564
Iteration 38/1000 | Loss: 0.00016475
Iteration 39/1000 | Loss: 0.00030490
Iteration 40/1000 | Loss: 0.00036741
Iteration 41/1000 | Loss: 0.00059519
Iteration 42/1000 | Loss: 0.00092611
Iteration 43/1000 | Loss: 0.00033311
Iteration 44/1000 | Loss: 0.00083076
Iteration 45/1000 | Loss: 0.00021827
Iteration 46/1000 | Loss: 0.00047332
Iteration 47/1000 | Loss: 0.00023520
Iteration 48/1000 | Loss: 0.00018958
Iteration 49/1000 | Loss: 0.00026422
Iteration 50/1000 | Loss: 0.00037945
Iteration 51/1000 | Loss: 0.00030297
Iteration 52/1000 | Loss: 0.00034379
Iteration 53/1000 | Loss: 0.00033434
Iteration 54/1000 | Loss: 0.00011657
Iteration 55/1000 | Loss: 0.00065487
Iteration 56/1000 | Loss: 0.00032982
Iteration 57/1000 | Loss: 0.00026597
Iteration 58/1000 | Loss: 0.00025241
Iteration 59/1000 | Loss: 0.00015289
Iteration 60/1000 | Loss: 0.00037753
Iteration 61/1000 | Loss: 0.00017255
Iteration 62/1000 | Loss: 0.00047748
Iteration 63/1000 | Loss: 0.00056192
Iteration 64/1000 | Loss: 0.00023279
Iteration 65/1000 | Loss: 0.00015832
Iteration 66/1000 | Loss: 0.00049603
Iteration 67/1000 | Loss: 0.00020278
Iteration 68/1000 | Loss: 0.00029233
Iteration 69/1000 | Loss: 0.00037606
Iteration 70/1000 | Loss: 0.00023490
Iteration 71/1000 | Loss: 0.00040252
Iteration 72/1000 | Loss: 0.00016276
Iteration 73/1000 | Loss: 0.00009408
Iteration 74/1000 | Loss: 0.00012801
Iteration 75/1000 | Loss: 0.00017128
Iteration 76/1000 | Loss: 0.00019218
Iteration 77/1000 | Loss: 0.00038977
Iteration 78/1000 | Loss: 0.00105015
Iteration 79/1000 | Loss: 0.00087194
Iteration 80/1000 | Loss: 0.00040500
Iteration 81/1000 | Loss: 0.00034475
Iteration 82/1000 | Loss: 0.00017881
Iteration 83/1000 | Loss: 0.00020922
Iteration 84/1000 | Loss: 0.00018722
Iteration 85/1000 | Loss: 0.00012115
Iteration 86/1000 | Loss: 0.00055780
Iteration 87/1000 | Loss: 0.00016621
Iteration 88/1000 | Loss: 0.00010744
Iteration 89/1000 | Loss: 0.00009213
Iteration 90/1000 | Loss: 0.00026180
Iteration 91/1000 | Loss: 0.00022070
Iteration 92/1000 | Loss: 0.00016509
Iteration 93/1000 | Loss: 0.00152983
Iteration 94/1000 | Loss: 0.00041579
Iteration 95/1000 | Loss: 0.00050418
Iteration 96/1000 | Loss: 0.00039319
Iteration 97/1000 | Loss: 0.00084762
Iteration 98/1000 | Loss: 0.00022484
Iteration 99/1000 | Loss: 0.00018738
Iteration 100/1000 | Loss: 0.00045166
Iteration 101/1000 | Loss: 0.00016966
Iteration 102/1000 | Loss: 0.00030970
Iteration 103/1000 | Loss: 0.00036105
Iteration 104/1000 | Loss: 0.00094351
Iteration 105/1000 | Loss: 0.00040904
Iteration 106/1000 | Loss: 0.00036083
Iteration 107/1000 | Loss: 0.00006801
Iteration 108/1000 | Loss: 0.00008614
Iteration 109/1000 | Loss: 0.00050003
Iteration 110/1000 | Loss: 0.00007975
Iteration 111/1000 | Loss: 0.00017636
Iteration 112/1000 | Loss: 0.00057628
Iteration 113/1000 | Loss: 0.00007091
Iteration 114/1000 | Loss: 0.00099203
Iteration 115/1000 | Loss: 0.00014557
Iteration 116/1000 | Loss: 0.00057715
Iteration 117/1000 | Loss: 0.00016290
Iteration 118/1000 | Loss: 0.00015206
Iteration 119/1000 | Loss: 0.00012877
Iteration 120/1000 | Loss: 0.00007340
Iteration 121/1000 | Loss: 0.00007361
Iteration 122/1000 | Loss: 0.00006739
Iteration 123/1000 | Loss: 0.00004126
Iteration 124/1000 | Loss: 0.00006221
Iteration 125/1000 | Loss: 0.00005629
Iteration 126/1000 | Loss: 0.00005180
Iteration 127/1000 | Loss: 0.00007594
Iteration 128/1000 | Loss: 0.00008385
Iteration 129/1000 | Loss: 0.00007795
Iteration 130/1000 | Loss: 0.00048914
Iteration 131/1000 | Loss: 0.00033636
Iteration 132/1000 | Loss: 0.00057156
Iteration 133/1000 | Loss: 0.00006206
Iteration 134/1000 | Loss: 0.00016188
Iteration 135/1000 | Loss: 0.00015736
Iteration 136/1000 | Loss: 0.00005935
Iteration 137/1000 | Loss: 0.00006004
Iteration 138/1000 | Loss: 0.00015709
Iteration 139/1000 | Loss: 0.00016625
Iteration 140/1000 | Loss: 0.00016819
Iteration 141/1000 | Loss: 0.00012978
Iteration 142/1000 | Loss: 0.00015314
Iteration 143/1000 | Loss: 0.00011500
Iteration 144/1000 | Loss: 0.00016598
Iteration 145/1000 | Loss: 0.00014083
Iteration 146/1000 | Loss: 0.00015513
Iteration 147/1000 | Loss: 0.00006533
Iteration 148/1000 | Loss: 0.00005862
Iteration 149/1000 | Loss: 0.00009802
Iteration 150/1000 | Loss: 0.00040349
Iteration 151/1000 | Loss: 0.00026005
Iteration 152/1000 | Loss: 0.00027393
Iteration 153/1000 | Loss: 0.00054972
Iteration 154/1000 | Loss: 0.00071185
Iteration 155/1000 | Loss: 0.00027833
Iteration 156/1000 | Loss: 0.00018985
Iteration 157/1000 | Loss: 0.00012533
Iteration 158/1000 | Loss: 0.00021748
Iteration 159/1000 | Loss: 0.00014665
Iteration 160/1000 | Loss: 0.00017208
Iteration 161/1000 | Loss: 0.00014456
Iteration 162/1000 | Loss: 0.00007655
Iteration 163/1000 | Loss: 0.00009716
Iteration 164/1000 | Loss: 0.00007158
Iteration 165/1000 | Loss: 0.00020383
Iteration 166/1000 | Loss: 0.00013250
Iteration 167/1000 | Loss: 0.00008174
Iteration 168/1000 | Loss: 0.00015492
Iteration 169/1000 | Loss: 0.00015107
Iteration 170/1000 | Loss: 0.00046557
Iteration 171/1000 | Loss: 0.00006562
Iteration 172/1000 | Loss: 0.00006860
Iteration 173/1000 | Loss: 0.00035480
Iteration 174/1000 | Loss: 0.00005522
Iteration 175/1000 | Loss: 0.00005597
Iteration 176/1000 | Loss: 0.00005697
Iteration 177/1000 | Loss: 0.00005824
Iteration 178/1000 | Loss: 0.00004752
Iteration 179/1000 | Loss: 0.00011353
Iteration 180/1000 | Loss: 0.00009133
Iteration 181/1000 | Loss: 0.00004599
Iteration 182/1000 | Loss: 0.00015173
Iteration 183/1000 | Loss: 0.00012846
Iteration 184/1000 | Loss: 0.00006468
Iteration 185/1000 | Loss: 0.00005831
Iteration 186/1000 | Loss: 0.00013531
Iteration 187/1000 | Loss: 0.00011867
Iteration 188/1000 | Loss: 0.00011440
Iteration 189/1000 | Loss: 0.00010759
Iteration 190/1000 | Loss: 0.00010838
Iteration 191/1000 | Loss: 0.00013434
Iteration 192/1000 | Loss: 0.00014217
Iteration 193/1000 | Loss: 0.00014775
Iteration 194/1000 | Loss: 0.00005486
Iteration 195/1000 | Loss: 0.00004299
Iteration 196/1000 | Loss: 0.00004463
Iteration 197/1000 | Loss: 0.00005660
Iteration 198/1000 | Loss: 0.00019839
Iteration 199/1000 | Loss: 0.00008086
Iteration 200/1000 | Loss: 0.00007956
Iteration 201/1000 | Loss: 0.00006739
Iteration 202/1000 | Loss: 0.00007580
Iteration 203/1000 | Loss: 0.00006703
Iteration 204/1000 | Loss: 0.00005998
Iteration 205/1000 | Loss: 0.00006152
Iteration 206/1000 | Loss: 0.00008116
Iteration 207/1000 | Loss: 0.00006563
Iteration 208/1000 | Loss: 0.00007973
Iteration 209/1000 | Loss: 0.00011919
Iteration 210/1000 | Loss: 0.00006557
Iteration 211/1000 | Loss: 0.00006362
Iteration 212/1000 | Loss: 0.00009433
Iteration 213/1000 | Loss: 0.00008845
Iteration 214/1000 | Loss: 0.00006339
Iteration 215/1000 | Loss: 0.00008015
Iteration 216/1000 | Loss: 0.00006643
Iteration 217/1000 | Loss: 0.00007206
Iteration 218/1000 | Loss: 0.00006460
Iteration 219/1000 | Loss: 0.00005175
Iteration 220/1000 | Loss: 0.00004340
Iteration 221/1000 | Loss: 0.00004912
Iteration 222/1000 | Loss: 0.00005825
Iteration 223/1000 | Loss: 0.00007202
Iteration 224/1000 | Loss: 0.00003509
Iteration 225/1000 | Loss: 0.00004900
Iteration 226/1000 | Loss: 0.00005286
Iteration 227/1000 | Loss: 0.00004897
Iteration 228/1000 | Loss: 0.00005546
Iteration 229/1000 | Loss: 0.00005259
Iteration 230/1000 | Loss: 0.00006677
Iteration 231/1000 | Loss: 0.00004596
Iteration 232/1000 | Loss: 0.00003883
Iteration 233/1000 | Loss: 0.00004770
Iteration 234/1000 | Loss: 0.00006711
Iteration 235/1000 | Loss: 0.00005930
Iteration 236/1000 | Loss: 0.00006449
Iteration 237/1000 | Loss: 0.00005955
Iteration 238/1000 | Loss: 0.00006173
Iteration 239/1000 | Loss: 0.00002813
Iteration 240/1000 | Loss: 0.00005755
Iteration 241/1000 | Loss: 0.00006191
Iteration 242/1000 | Loss: 0.00004153
Iteration 243/1000 | Loss: 0.00004782
Iteration 244/1000 | Loss: 0.00005452
Iteration 245/1000 | Loss: 0.00005883
Iteration 246/1000 | Loss: 0.00003118
Iteration 247/1000 | Loss: 0.00003757
Iteration 248/1000 | Loss: 0.00006300
Iteration 249/1000 | Loss: 0.00006093
Iteration 250/1000 | Loss: 0.00006118
Iteration 251/1000 | Loss: 0.00005188
Iteration 252/1000 | Loss: 0.00006067
Iteration 253/1000 | Loss: 0.00006355
Iteration 254/1000 | Loss: 0.00006078
Iteration 255/1000 | Loss: 0.00005003
Iteration 256/1000 | Loss: 0.00005865
Iteration 257/1000 | Loss: 0.00005009
Iteration 258/1000 | Loss: 0.00008136
Iteration 259/1000 | Loss: 0.00006386
Iteration 260/1000 | Loss: 0.00007124
Iteration 261/1000 | Loss: 0.00005853
Iteration 262/1000 | Loss: 0.00006054
Iteration 263/1000 | Loss: 0.00005108
Iteration 264/1000 | Loss: 0.00006993
Iteration 265/1000 | Loss: 0.00006328
Iteration 266/1000 | Loss: 0.00007290
Iteration 267/1000 | Loss: 0.00006535
Iteration 268/1000 | Loss: 0.00007254
Iteration 269/1000 | Loss: 0.00006385
Iteration 270/1000 | Loss: 0.00004042
Iteration 271/1000 | Loss: 0.00003446
Iteration 272/1000 | Loss: 0.00005737
Iteration 273/1000 | Loss: 0.00004840
Iteration 274/1000 | Loss: 0.00006397
Iteration 275/1000 | Loss: 0.00007465
Iteration 276/1000 | Loss: 0.00007145
Iteration 277/1000 | Loss: 0.00007345
Iteration 278/1000 | Loss: 0.00007149
Iteration 279/1000 | Loss: 0.00007239
Iteration 280/1000 | Loss: 0.00007186
Iteration 281/1000 | Loss: 0.00006232
Iteration 282/1000 | Loss: 0.00007408
Iteration 283/1000 | Loss: 0.00007074
Iteration 284/1000 | Loss: 0.00007478
Iteration 285/1000 | Loss: 0.00007320
Iteration 286/1000 | Loss: 0.00007257
Iteration 287/1000 | Loss: 0.00007381
Iteration 288/1000 | Loss: 0.00007338
Iteration 289/1000 | Loss: 0.00005835
Iteration 290/1000 | Loss: 0.00007990
Iteration 291/1000 | Loss: 0.00005951
Iteration 292/1000 | Loss: 0.00008403
Iteration 293/1000 | Loss: 0.00006422
Iteration 294/1000 | Loss: 0.00006989
Iteration 295/1000 | Loss: 0.00007077
Iteration 296/1000 | Loss: 0.00007260
Iteration 297/1000 | Loss: 0.00005492
Iteration 298/1000 | Loss: 0.00006656
Iteration 299/1000 | Loss: 0.00007195
Iteration 300/1000 | Loss: 0.00007268
Iteration 301/1000 | Loss: 0.00006144
Iteration 302/1000 | Loss: 0.00007262
Iteration 303/1000 | Loss: 0.00006932
Iteration 304/1000 | Loss: 0.00007219
Iteration 305/1000 | Loss: 0.00006464
Iteration 306/1000 | Loss: 0.00008459
Iteration 307/1000 | Loss: 0.00007002
Iteration 308/1000 | Loss: 0.00007242
Iteration 309/1000 | Loss: 0.00006974
Iteration 310/1000 | Loss: 0.00007135
Iteration 311/1000 | Loss: 0.00007005
Iteration 312/1000 | Loss: 0.00004536
Iteration 313/1000 | Loss: 0.00004767
Iteration 314/1000 | Loss: 0.00005875
Iteration 315/1000 | Loss: 0.00005173
Iteration 316/1000 | Loss: 0.00005830
Iteration 317/1000 | Loss: 0.00007035
Iteration 318/1000 | Loss: 0.00006127
Iteration 319/1000 | Loss: 0.00004773
Iteration 320/1000 | Loss: 0.00006772
Iteration 321/1000 | Loss: 0.00007075
Iteration 322/1000 | Loss: 0.00006980
Iteration 323/1000 | Loss: 0.00007690
Iteration 324/1000 | Loss: 0.00004626
Iteration 325/1000 | Loss: 0.00005311
Iteration 326/1000 | Loss: 0.00007532
Iteration 327/1000 | Loss: 0.00005566
Iteration 328/1000 | Loss: 0.00006838
Iteration 329/1000 | Loss: 0.00006544
Iteration 330/1000 | Loss: 0.00007145
Iteration 331/1000 | Loss: 0.00005891
Iteration 332/1000 | Loss: 0.00006320
Iteration 333/1000 | Loss: 0.00006275
Iteration 334/1000 | Loss: 0.00006494
Iteration 335/1000 | Loss: 0.00006829
Iteration 336/1000 | Loss: 0.00006942
Iteration 337/1000 | Loss: 0.00006650
Iteration 338/1000 | Loss: 0.00007477
Iteration 339/1000 | Loss: 0.00005822
Iteration 340/1000 | Loss: 0.00006331
Iteration 341/1000 | Loss: 0.00005532
Iteration 342/1000 | Loss: 0.00003703
Iteration 343/1000 | Loss: 0.00005904
Iteration 344/1000 | Loss: 0.00007118
Iteration 345/1000 | Loss: 0.00006238
Iteration 346/1000 | Loss: 0.00006850
Iteration 347/1000 | Loss: 0.00007033
Iteration 348/1000 | Loss: 0.00007186
Iteration 349/1000 | Loss: 0.00007119
Iteration 350/1000 | Loss: 0.00007213
Iteration 351/1000 | Loss: 0.00007060
Iteration 352/1000 | Loss: 0.00007275
Iteration 353/1000 | Loss: 0.00006594
Iteration 354/1000 | Loss: 0.00007392
Iteration 355/1000 | Loss: 0.00006546
Iteration 356/1000 | Loss: 0.00007828
Iteration 357/1000 | Loss: 0.00007720
Iteration 358/1000 | Loss: 0.00007965
Iteration 359/1000 | Loss: 0.00005424
Iteration 360/1000 | Loss: 0.00005472
Iteration 361/1000 | Loss: 0.00007058
Iteration 362/1000 | Loss: 0.00005848
Iteration 363/1000 | Loss: 0.00005933
Iteration 364/1000 | Loss: 0.00006110
Iteration 365/1000 | Loss: 0.00006853
Iteration 366/1000 | Loss: 0.00006230
Iteration 367/1000 | Loss: 0.00007157
Iteration 368/1000 | Loss: 0.00007168
Iteration 369/1000 | Loss: 0.00006145
Iteration 370/1000 | Loss: 0.00006807
Iteration 371/1000 | Loss: 0.00005776
Iteration 372/1000 | Loss: 0.00006123
Iteration 373/1000 | Loss: 0.00006765
Iteration 374/1000 | Loss: 0.00006115
Iteration 375/1000 | Loss: 0.00005907
Iteration 376/1000 | Loss: 0.00006485
Iteration 377/1000 | Loss: 0.00006054
Iteration 378/1000 | Loss: 0.00006022
Iteration 379/1000 | Loss: 0.00006557
Iteration 380/1000 | Loss: 0.00003459
Iteration 381/1000 | Loss: 0.00004579
Iteration 382/1000 | Loss: 0.00004140
Iteration 383/1000 | Loss: 0.00005611
Iteration 384/1000 | Loss: 0.00004215
Iteration 385/1000 | Loss: 0.00004596
Iteration 386/1000 | Loss: 0.00005359
Iteration 387/1000 | Loss: 0.00006572
Iteration 388/1000 | Loss: 0.00006625
Iteration 389/1000 | Loss: 0.00006627
Iteration 390/1000 | Loss: 0.00006681
Iteration 391/1000 | Loss: 0.00004656
Iteration 392/1000 | Loss: 0.00005270
Iteration 393/1000 | Loss: 0.00006186
Iteration 394/1000 | Loss: 0.00007234
Iteration 395/1000 | Loss: 0.00006720
Iteration 396/1000 | Loss: 0.00006657
Iteration 397/1000 | Loss: 0.00006088
Iteration 398/1000 | Loss: 0.00005460
Iteration 399/1000 | Loss: 0.00005645
Iteration 400/1000 | Loss: 0.00005935
Iteration 401/1000 | Loss: 0.00005993
Iteration 402/1000 | Loss: 0.00005392
Iteration 403/1000 | Loss: 0.00005604
Iteration 404/1000 | Loss: 0.00006151
Iteration 405/1000 | Loss: 0.00005815
Iteration 406/1000 | Loss: 0.00006355
Iteration 407/1000 | Loss: 0.00005433
Iteration 408/1000 | Loss: 0.00006200
Iteration 409/1000 | Loss: 0.00006310
Iteration 410/1000 | Loss: 0.00007222
Iteration 411/1000 | Loss: 0.00005796
Iteration 412/1000 | Loss: 0.00005913
Iteration 413/1000 | Loss: 0.00007078
Iteration 414/1000 | Loss: 0.00005348
Iteration 415/1000 | Loss: 0.00005933
Iteration 416/1000 | Loss: 0.00005834
Iteration 417/1000 | Loss: 0.00006531
Iteration 418/1000 | Loss: 0.00006980
Iteration 419/1000 | Loss: 0.00006829
Iteration 420/1000 | Loss: 0.00006172
Iteration 421/1000 | Loss: 0.00006966
Iteration 422/1000 | Loss: 0.00005358
Iteration 423/1000 | Loss: 0.00008468
Iteration 424/1000 | Loss: 0.00003540
Iteration 425/1000 | Loss: 0.00006218
Iteration 426/1000 | Loss: 0.00003420
Iteration 427/1000 | Loss: 0.00004054
Iteration 428/1000 | Loss: 0.00004737
Iteration 429/1000 | Loss: 0.00003962
Iteration 430/1000 | Loss: 0.00003689
Iteration 431/1000 | Loss: 0.00003297
Iteration 432/1000 | Loss: 0.00001727
Iteration 433/1000 | Loss: 0.00002767
Iteration 434/1000 | Loss: 0.00004530
Iteration 435/1000 | Loss: 0.00003906
Iteration 436/1000 | Loss: 0.00002296
Iteration 437/1000 | Loss: 0.00003393
Iteration 438/1000 | Loss: 0.00003787
Iteration 439/1000 | Loss: 0.00004805
Iteration 440/1000 | Loss: 0.00005762
Iteration 441/1000 | Loss: 0.00004437
Iteration 442/1000 | Loss: 0.00003952
Iteration 443/1000 | Loss: 0.00004117
Iteration 444/1000 | Loss: 0.00004041
Iteration 445/1000 | Loss: 0.00004284
Iteration 446/1000 | Loss: 0.00005597
Iteration 447/1000 | Loss: 0.00004309
Iteration 448/1000 | Loss: 0.00005157
Iteration 449/1000 | Loss: 0.00004382
Iteration 450/1000 | Loss: 0.00003969
Iteration 451/1000 | Loss: 0.00004334
Iteration 452/1000 | Loss: 0.00004755
Iteration 453/1000 | Loss: 0.00004264
Iteration 454/1000 | Loss: 0.00003742
Iteration 455/1000 | Loss: 0.00002996
Iteration 456/1000 | Loss: 0.00004152
Iteration 457/1000 | Loss: 0.00004831
Iteration 458/1000 | Loss: 0.00004345
Iteration 459/1000 | Loss: 0.00005234
Iteration 460/1000 | Loss: 0.00003898
Iteration 461/1000 | Loss: 0.00003011
Iteration 462/1000 | Loss: 0.00003241
Iteration 463/1000 | Loss: 0.00003329
Iteration 464/1000 | Loss: 0.00004167
Iteration 465/1000 | Loss: 0.00003873
Iteration 466/1000 | Loss: 0.00004647
Iteration 467/1000 | Loss: 0.00003862
Iteration 468/1000 | Loss: 0.00004171
Iteration 469/1000 | Loss: 0.00004655
Iteration 470/1000 | Loss: 0.00002639
Iteration 471/1000 | Loss: 0.00003316
Iteration 472/1000 | Loss: 0.00003699
Iteration 473/1000 | Loss: 0.00003233
Iteration 474/1000 | Loss: 0.00003512
Iteration 475/1000 | Loss: 0.00004052
Iteration 476/1000 | Loss: 0.00003686
Iteration 477/1000 | Loss: 0.00004246
Iteration 478/1000 | Loss: 0.00003927
Iteration 479/1000 | Loss: 0.00003982
Iteration 480/1000 | Loss: 0.00002064
Iteration 481/1000 | Loss: 0.00003211
Iteration 482/1000 | Loss: 0.00004814
Iteration 483/1000 | Loss: 0.00004624
Iteration 484/1000 | Loss: 0.00004340
Iteration 485/1000 | Loss: 0.00003589
Iteration 486/1000 | Loss: 0.00004081
Iteration 487/1000 | Loss: 0.00004759
Iteration 488/1000 | Loss: 0.00004819
Iteration 489/1000 | Loss: 0.00004843
Iteration 490/1000 | Loss: 0.00004019
Iteration 491/1000 | Loss: 0.00004483
Iteration 492/1000 | Loss: 0.00003951
Iteration 493/1000 | Loss: 0.00004386
Iteration 494/1000 | Loss: 0.00003915
Iteration 495/1000 | Loss: 0.00004537
Iteration 496/1000 | Loss: 0.00004002
Iteration 497/1000 | Loss: 0.00003398
Iteration 498/1000 | Loss: 0.00003720
Iteration 499/1000 | Loss: 0.00004247
Iteration 500/1000 | Loss: 0.00004440
Iteration 501/1000 | Loss: 0.00003725
Iteration 502/1000 | Loss: 0.00004727
Iteration 503/1000 | Loss: 0.00004121
Iteration 504/1000 | Loss: 0.00004386
Iteration 505/1000 | Loss: 0.00003777
Iteration 506/1000 | Loss: 0.00004503
Iteration 507/1000 | Loss: 0.00004246
Iteration 508/1000 | Loss: 0.00004777
Iteration 509/1000 | Loss: 0.00004271
Iteration 510/1000 | Loss: 0.00005266
Iteration 511/1000 | Loss: 0.00004485
Iteration 512/1000 | Loss: 0.00004564
Iteration 513/1000 | Loss: 0.00004254
Iteration 514/1000 | Loss: 0.00004554
Iteration 515/1000 | Loss: 0.00004952
Iteration 516/1000 | Loss: 0.00004561
Iteration 517/1000 | Loss: 0.00004057
Iteration 518/1000 | Loss: 0.00004528
Iteration 519/1000 | Loss: 0.00004026
Iteration 520/1000 | Loss: 0.00004253
Iteration 521/1000 | Loss: 0.00004363
Iteration 522/1000 | Loss: 0.00004084
Iteration 523/1000 | Loss: 0.00004365
Iteration 524/1000 | Loss: 0.00003852
Iteration 525/1000 | Loss: 0.00004194
Iteration 526/1000 | Loss: 0.00004341
Iteration 527/1000 | Loss: 0.00004175
Iteration 528/1000 | Loss: 0.00004206
Iteration 529/1000 | Loss: 0.00002786
Iteration 530/1000 | Loss: 0.00003816
Iteration 531/1000 | Loss: 0.00004022
Iteration 532/1000 | Loss: 0.00003880
Iteration 533/1000 | Loss: 0.00004644
Iteration 534/1000 | Loss: 0.00003529
Iteration 535/1000 | Loss: 0.00003786
Iteration 536/1000 | Loss: 0.00003564
Iteration 537/1000 | Loss: 0.00003992
Iteration 538/1000 | Loss: 0.00002595
Iteration 539/1000 | Loss: 0.00004800
Iteration 540/1000 | Loss: 0.00003571
Iteration 541/1000 | Loss: 0.00004763
Iteration 542/1000 | Loss: 0.00004365
Iteration 543/1000 | Loss: 0.00003790
Iteration 544/1000 | Loss: 0.00002620
Iteration 545/1000 | Loss: 0.00002637
Iteration 546/1000 | Loss: 0.00003380
Iteration 547/1000 | Loss: 0.00005474
Iteration 548/1000 | Loss: 0.00006246
Iteration 549/1000 | Loss: 0.00005032
Iteration 550/1000 | Loss: 0.00003709
Iteration 551/1000 | Loss: 0.00003192
Iteration 552/1000 | Loss: 0.00002140
Iteration 553/1000 | Loss: 0.00002504
Iteration 554/1000 | Loss: 0.00002961
Iteration 555/1000 | Loss: 0.00003571
Iteration 556/1000 | Loss: 0.00003877
Iteration 557/1000 | Loss: 0.00003036
Iteration 558/1000 | Loss: 0.00003582
Iteration 559/1000 | Loss: 0.00003896
Iteration 560/1000 | Loss: 0.00003628
Iteration 561/1000 | Loss: 0.00004136
Iteration 562/1000 | Loss: 0.00003097
Iteration 563/1000 | Loss: 0.00003533
Iteration 564/1000 | Loss: 0.00003525
Iteration 565/1000 | Loss: 0.00004045
Iteration 566/1000 | Loss: 0.00004544
Iteration 567/1000 | Loss: 0.00003747
Iteration 568/1000 | Loss: 0.00003848
Iteration 569/1000 | Loss: 0.00002578
Iteration 570/1000 | Loss: 0.00002714
Iteration 571/1000 | Loss: 0.00002828
Iteration 572/1000 | Loss: 0.00003022
Iteration 573/1000 | Loss: 0.00002555
Iteration 574/1000 | Loss: 0.00002745
Iteration 575/1000 | Loss: 0.00002934
Iteration 576/1000 | Loss: 0.00002946
Iteration 577/1000 | Loss: 0.00002116
Iteration 578/1000 | Loss: 0.00001761
Iteration 579/1000 | Loss: 0.00002298
Iteration 580/1000 | Loss: 0.00002690
Iteration 581/1000 | Loss: 0.00003525
Iteration 582/1000 | Loss: 0.00002760
Iteration 583/1000 | Loss: 0.00003114
Iteration 584/1000 | Loss: 0.00002667
Iteration 585/1000 | Loss: 0.00003254
Iteration 586/1000 | Loss: 0.00002560
Iteration 587/1000 | Loss: 0.00003399
Iteration 588/1000 | Loss: 0.00002791
Iteration 589/1000 | Loss: 0.00001958
Iteration 590/1000 | Loss: 0.00003326
Iteration 591/1000 | Loss: 0.00002584
Iteration 592/1000 | Loss: 0.00003078
Iteration 593/1000 | Loss: 0.00003165
Iteration 594/1000 | Loss: 0.00011438
Iteration 595/1000 | Loss: 0.00002842
Iteration 596/1000 | Loss: 0.00002109
Iteration 597/1000 | Loss: 0.00015886
Iteration 598/1000 | Loss: 0.00001553
Iteration 599/1000 | Loss: 0.00001470
Iteration 600/1000 | Loss: 0.00001435
Iteration 601/1000 | Loss: 0.00001420
Iteration 602/1000 | Loss: 0.00001414
Iteration 603/1000 | Loss: 0.00001412
Iteration 604/1000 | Loss: 0.00001395
Iteration 605/1000 | Loss: 0.00001385
Iteration 606/1000 | Loss: 0.00001381
Iteration 607/1000 | Loss: 0.00001380
Iteration 608/1000 | Loss: 0.00001377
Iteration 609/1000 | Loss: 0.00001377
Iteration 610/1000 | Loss: 0.00001376
Iteration 611/1000 | Loss: 0.00001376
Iteration 612/1000 | Loss: 0.00001375
Iteration 613/1000 | Loss: 0.00001374
Iteration 614/1000 | Loss: 0.00001374
Iteration 615/1000 | Loss: 0.00001373
Iteration 616/1000 | Loss: 0.00001371
Iteration 617/1000 | Loss: 0.00001371
Iteration 618/1000 | Loss: 0.00001370
Iteration 619/1000 | Loss: 0.00001370
Iteration 620/1000 | Loss: 0.00001369
Iteration 621/1000 | Loss: 0.00001368
Iteration 622/1000 | Loss: 0.00001367
Iteration 623/1000 | Loss: 0.00001367
Iteration 624/1000 | Loss: 0.00001366
Iteration 625/1000 | Loss: 0.00001366
Iteration 626/1000 | Loss: 0.00001366
Iteration 627/1000 | Loss: 0.00001366
Iteration 628/1000 | Loss: 0.00001365
Iteration 629/1000 | Loss: 0.00001365
Iteration 630/1000 | Loss: 0.00001365
Iteration 631/1000 | Loss: 0.00001364
Iteration 632/1000 | Loss: 0.00001363
Iteration 633/1000 | Loss: 0.00001359
Iteration 634/1000 | Loss: 0.00001359
Iteration 635/1000 | Loss: 0.00001359
Iteration 636/1000 | Loss: 0.00001358
Iteration 637/1000 | Loss: 0.00001357
Iteration 638/1000 | Loss: 0.00001357
Iteration 639/1000 | Loss: 0.00001357
Iteration 640/1000 | Loss: 0.00001357
Iteration 641/1000 | Loss: 0.00001357
Iteration 642/1000 | Loss: 0.00001356
Iteration 643/1000 | Loss: 0.00001356
Iteration 644/1000 | Loss: 0.00001356
Iteration 645/1000 | Loss: 0.00001356
Iteration 646/1000 | Loss: 0.00001356
Iteration 647/1000 | Loss: 0.00001356
Iteration 648/1000 | Loss: 0.00001356
Iteration 649/1000 | Loss: 0.00001356
Iteration 650/1000 | Loss: 0.00001355
Iteration 651/1000 | Loss: 0.00001355
Iteration 652/1000 | Loss: 0.00001355
Iteration 653/1000 | Loss: 0.00001355
Iteration 654/1000 | Loss: 0.00001355
Iteration 655/1000 | Loss: 0.00001355
Iteration 656/1000 | Loss: 0.00001355
Iteration 657/1000 | Loss: 0.00001355
Iteration 658/1000 | Loss: 0.00001355
Iteration 659/1000 | Loss: 0.00001355
Iteration 660/1000 | Loss: 0.00001354
Iteration 661/1000 | Loss: 0.00001354
Iteration 662/1000 | Loss: 0.00001353
Iteration 663/1000 | Loss: 0.00001353
Iteration 664/1000 | Loss: 0.00001353
Iteration 665/1000 | Loss: 0.00001353
Iteration 666/1000 | Loss: 0.00001353
Iteration 667/1000 | Loss: 0.00001352
Iteration 668/1000 | Loss: 0.00001352
Iteration 669/1000 | Loss: 0.00001352
Iteration 670/1000 | Loss: 0.00001352
Iteration 671/1000 | Loss: 0.00001352
Iteration 672/1000 | Loss: 0.00001352
Iteration 673/1000 | Loss: 0.00001352
Iteration 674/1000 | Loss: 0.00001352
Iteration 675/1000 | Loss: 0.00001351
Iteration 676/1000 | Loss: 0.00001351
Iteration 677/1000 | Loss: 0.00001351
Iteration 678/1000 | Loss: 0.00001351
Iteration 679/1000 | Loss: 0.00001350
Iteration 680/1000 | Loss: 0.00001350
Iteration 681/1000 | Loss: 0.00001349
Iteration 682/1000 | Loss: 0.00001349
Iteration 683/1000 | Loss: 0.00001349
Iteration 684/1000 | Loss: 0.00001349
Iteration 685/1000 | Loss: 0.00001349
Iteration 686/1000 | Loss: 0.00001348
Iteration 687/1000 | Loss: 0.00001348
Iteration 688/1000 | Loss: 0.00001348
Iteration 689/1000 | Loss: 0.00001348
Iteration 690/1000 | Loss: 0.00001348
Iteration 691/1000 | Loss: 0.00001348
Iteration 692/1000 | Loss: 0.00001348
Iteration 693/1000 | Loss: 0.00001345
Iteration 694/1000 | Loss: 0.00001344
Iteration 695/1000 | Loss: 0.00001344
Iteration 696/1000 | Loss: 0.00001344
Iteration 697/1000 | Loss: 0.00002475
Iteration 698/1000 | Loss: 0.00002640
Iteration 699/1000 | Loss: 0.00001754
Iteration 700/1000 | Loss: 0.00001616
Iteration 701/1000 | Loss: 0.00001556
Iteration 702/1000 | Loss: 0.00001510
Iteration 703/1000 | Loss: 0.00001455
Iteration 704/1000 | Loss: 0.00001420
Iteration 705/1000 | Loss: 0.00001378
Iteration 706/1000 | Loss: 0.00001338
Iteration 707/1000 | Loss: 0.00001316
Iteration 708/1000 | Loss: 0.00001284
Iteration 709/1000 | Loss: 0.00001262
Iteration 710/1000 | Loss: 0.00001255
Iteration 711/1000 | Loss: 0.00001254
Iteration 712/1000 | Loss: 0.00001251
Iteration 713/1000 | Loss: 0.00001248
Iteration 714/1000 | Loss: 0.00001247
Iteration 715/1000 | Loss: 0.00001243
Iteration 716/1000 | Loss: 0.00001241
Iteration 717/1000 | Loss: 0.00001240
Iteration 718/1000 | Loss: 0.00001240
Iteration 719/1000 | Loss: 0.00001239
Iteration 720/1000 | Loss: 0.00001238
Iteration 721/1000 | Loss: 0.00001238
Iteration 722/1000 | Loss: 0.00001238
Iteration 723/1000 | Loss: 0.00001238
Iteration 724/1000 | Loss: 0.00001238
Iteration 725/1000 | Loss: 0.00001238
Iteration 726/1000 | Loss: 0.00001238
Iteration 727/1000 | Loss: 0.00001238
Iteration 728/1000 | Loss: 0.00001238
Iteration 729/1000 | Loss: 0.00001238
Iteration 730/1000 | Loss: 0.00001238
Iteration 731/1000 | Loss: 0.00001238
Iteration 732/1000 | Loss: 0.00001237
Iteration 733/1000 | Loss: 0.00001237
Iteration 734/1000 | Loss: 0.00001237
Iteration 735/1000 | Loss: 0.00001237
Iteration 736/1000 | Loss: 0.00001237
Iteration 737/1000 | Loss: 0.00001237
Iteration 738/1000 | Loss: 0.00001236
Iteration 739/1000 | Loss: 0.00001236
Iteration 740/1000 | Loss: 0.00001236
Iteration 741/1000 | Loss: 0.00001235
Iteration 742/1000 | Loss: 0.00001235
Iteration 743/1000 | Loss: 0.00001235
Iteration 744/1000 | Loss: 0.00001235
Iteration 745/1000 | Loss: 0.00001235
Iteration 746/1000 | Loss: 0.00001234
Iteration 747/1000 | Loss: 0.00001234
Iteration 748/1000 | Loss: 0.00001234
Iteration 749/1000 | Loss: 0.00001234
Iteration 750/1000 | Loss: 0.00001234
Iteration 751/1000 | Loss: 0.00001234
Iteration 752/1000 | Loss: 0.00001234
Iteration 753/1000 | Loss: 0.00001234
Iteration 754/1000 | Loss: 0.00001233
Iteration 755/1000 | Loss: 0.00001233
Iteration 756/1000 | Loss: 0.00001233
Iteration 757/1000 | Loss: 0.00001233
Iteration 758/1000 | Loss: 0.00001233
Iteration 759/1000 | Loss: 0.00001233
Iteration 760/1000 | Loss: 0.00001233
Iteration 761/1000 | Loss: 0.00001233
Iteration 762/1000 | Loss: 0.00001233
Iteration 763/1000 | Loss: 0.00001232
Iteration 764/1000 | Loss: 0.00001232
Iteration 765/1000 | Loss: 0.00001232
Iteration 766/1000 | Loss: 0.00001232
Iteration 767/1000 | Loss: 0.00001232
Iteration 768/1000 | Loss: 0.00001232
Iteration 769/1000 | Loss: 0.00001232
Iteration 770/1000 | Loss: 0.00001232
Iteration 771/1000 | Loss: 0.00001232
Iteration 772/1000 | Loss: 0.00001232
Iteration 773/1000 | Loss: 0.00001232
Iteration 774/1000 | Loss: 0.00001232
Iteration 775/1000 | Loss: 0.00001232
Iteration 776/1000 | Loss: 0.00001232
Iteration 777/1000 | Loss: 0.00001232
Iteration 778/1000 | Loss: 0.00001232
Iteration 779/1000 | Loss: 0.00001231
Iteration 780/1000 | Loss: 0.00001231
Iteration 781/1000 | Loss: 0.00001231
Iteration 782/1000 | Loss: 0.00001231
Iteration 783/1000 | Loss: 0.00001231
Iteration 784/1000 | Loss: 0.00001231
Iteration 785/1000 | Loss: 0.00001231
Iteration 786/1000 | Loss: 0.00001231
Iteration 787/1000 | Loss: 0.00001231
Iteration 788/1000 | Loss: 0.00001231
Iteration 789/1000 | Loss: 0.00001231
Iteration 790/1000 | Loss: 0.00001231
Iteration 791/1000 | Loss: 0.00001231
Iteration 792/1000 | Loss: 0.00001231
Iteration 793/1000 | Loss: 0.00001231
Iteration 794/1000 | Loss: 0.00001231
Iteration 795/1000 | Loss: 0.00001231
Iteration 796/1000 | Loss: 0.00001230
Iteration 797/1000 | Loss: 0.00001230
Iteration 798/1000 | Loss: 0.00001230
Iteration 799/1000 | Loss: 0.00001230
Iteration 800/1000 | Loss: 0.00001230
Iteration 801/1000 | Loss: 0.00001230
Iteration 802/1000 | Loss: 0.00001230
Iteration 803/1000 | Loss: 0.00001230
Iteration 804/1000 | Loss: 0.00001230
Iteration 805/1000 | Loss: 0.00001230
Iteration 806/1000 | Loss: 0.00001230
Iteration 807/1000 | Loss: 0.00001230
Iteration 808/1000 | Loss: 0.00001230
Iteration 809/1000 | Loss: 0.00001230
Iteration 810/1000 | Loss: 0.00001230
Iteration 811/1000 | Loss: 0.00001230
Iteration 812/1000 | Loss: 0.00001230
Iteration 813/1000 | Loss: 0.00001230
Iteration 814/1000 | Loss: 0.00001230
Iteration 815/1000 | Loss: 0.00001230
Iteration 816/1000 | Loss: 0.00001230
Iteration 817/1000 | Loss: 0.00001230
Iteration 818/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 818. Stopping optimization.
Last 5 losses: [1.2302956747589633e-05, 1.2302956747589633e-05, 1.2302956747589633e-05, 1.2302956747589633e-05, 1.2302956747589633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2302956747589633e-05

Optimization complete. Final v2v error: 2.9730052947998047 mm

Highest mean error: 3.830603837966919 mm for frame 49

Lowest mean error: 2.7522940635681152 mm for frame 198

Saving results

Total time: 1060.872300863266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031898
Iteration 2/25 | Loss: 0.00285298
Iteration 3/25 | Loss: 0.00206047
Iteration 4/25 | Loss: 0.00198630
Iteration 5/25 | Loss: 0.00179584
Iteration 6/25 | Loss: 0.00165818
Iteration 7/25 | Loss: 0.00147076
Iteration 8/25 | Loss: 0.00140015
Iteration 9/25 | Loss: 0.00136086
Iteration 10/25 | Loss: 0.00135212
Iteration 11/25 | Loss: 0.00135145
Iteration 12/25 | Loss: 0.00135565
Iteration 13/25 | Loss: 0.00133692
Iteration 14/25 | Loss: 0.00132913
Iteration 15/25 | Loss: 0.00132416
Iteration 16/25 | Loss: 0.00132314
Iteration 17/25 | Loss: 0.00131140
Iteration 18/25 | Loss: 0.00130933
Iteration 19/25 | Loss: 0.00130884
Iteration 20/25 | Loss: 0.00130627
Iteration 21/25 | Loss: 0.00130508
Iteration 22/25 | Loss: 0.00130360
Iteration 23/25 | Loss: 0.00130583
Iteration 24/25 | Loss: 0.00130896
Iteration 25/25 | Loss: 0.00130286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49184239
Iteration 2/25 | Loss: 0.00341388
Iteration 3/25 | Loss: 0.00264404
Iteration 4/25 | Loss: 0.00264404
Iteration 5/25 | Loss: 0.00264403
Iteration 6/25 | Loss: 0.00264403
Iteration 7/25 | Loss: 0.00264403
Iteration 8/25 | Loss: 0.00264403
Iteration 9/25 | Loss: 0.00264403
Iteration 10/25 | Loss: 0.00264403
Iteration 11/25 | Loss: 0.00264403
Iteration 12/25 | Loss: 0.00264403
Iteration 13/25 | Loss: 0.00264403
Iteration 14/25 | Loss: 0.00264403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0026440315414220095, 0.0026440315414220095, 0.0026440315414220095, 0.0026440315414220095, 0.0026440315414220095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026440315414220095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264403
Iteration 2/1000 | Loss: 0.00207296
Iteration 3/1000 | Loss: 0.00054719
Iteration 4/1000 | Loss: 0.00231911
Iteration 5/1000 | Loss: 0.00017669
Iteration 6/1000 | Loss: 0.00016217
Iteration 7/1000 | Loss: 0.00059852
Iteration 8/1000 | Loss: 0.00091350
Iteration 9/1000 | Loss: 0.00014919
Iteration 10/1000 | Loss: 0.00014187
Iteration 11/1000 | Loss: 0.00139773
Iteration 12/1000 | Loss: 0.00013786
Iteration 13/1000 | Loss: 0.00040308
Iteration 14/1000 | Loss: 0.00025849
Iteration 15/1000 | Loss: 0.00061271
Iteration 16/1000 | Loss: 0.00018628
Iteration 17/1000 | Loss: 0.00016362
Iteration 18/1000 | Loss: 0.00012441
Iteration 19/1000 | Loss: 0.00012018
Iteration 20/1000 | Loss: 0.00040904
Iteration 21/1000 | Loss: 0.00011801
Iteration 22/1000 | Loss: 0.00189676
Iteration 23/1000 | Loss: 0.00089921
Iteration 24/1000 | Loss: 0.00059669
Iteration 25/1000 | Loss: 0.00038343
Iteration 26/1000 | Loss: 0.00011875
Iteration 27/1000 | Loss: 0.00011068
Iteration 28/1000 | Loss: 0.00010517
Iteration 29/1000 | Loss: 0.00010127
Iteration 30/1000 | Loss: 0.00039238
Iteration 31/1000 | Loss: 0.00009971
Iteration 32/1000 | Loss: 0.00009838
Iteration 33/1000 | Loss: 0.00009723
Iteration 34/1000 | Loss: 0.00028286
Iteration 35/1000 | Loss: 0.00016929
Iteration 36/1000 | Loss: 0.00027019
Iteration 37/1000 | Loss: 0.00016943
Iteration 38/1000 | Loss: 0.00099724
Iteration 39/1000 | Loss: 0.00018485
Iteration 40/1000 | Loss: 0.00031150
Iteration 41/1000 | Loss: 0.00011513
Iteration 42/1000 | Loss: 0.00011715
Iteration 43/1000 | Loss: 0.00011080
Iteration 44/1000 | Loss: 0.00009471
Iteration 45/1000 | Loss: 0.00028711
Iteration 46/1000 | Loss: 0.00044090
Iteration 47/1000 | Loss: 0.00010748
Iteration 48/1000 | Loss: 0.00010746
Iteration 49/1000 | Loss: 0.00010292
Iteration 50/1000 | Loss: 0.00009295
Iteration 51/1000 | Loss: 0.00009127
Iteration 52/1000 | Loss: 0.00009081
Iteration 53/1000 | Loss: 0.00010025
Iteration 54/1000 | Loss: 0.00009788
Iteration 55/1000 | Loss: 0.00053741
Iteration 56/1000 | Loss: 0.00010930
Iteration 57/1000 | Loss: 0.00038209
Iteration 58/1000 | Loss: 0.00013422
Iteration 59/1000 | Loss: 0.00021104
Iteration 60/1000 | Loss: 0.00010332
Iteration 61/1000 | Loss: 0.00009582
Iteration 62/1000 | Loss: 0.00029569
Iteration 63/1000 | Loss: 0.00012748
Iteration 64/1000 | Loss: 0.00023408
Iteration 65/1000 | Loss: 0.00015281
Iteration 66/1000 | Loss: 0.00045093
Iteration 67/1000 | Loss: 0.00017092
Iteration 68/1000 | Loss: 0.00036382
Iteration 69/1000 | Loss: 0.00020974
Iteration 70/1000 | Loss: 0.00022745
Iteration 71/1000 | Loss: 0.00028357
Iteration 72/1000 | Loss: 0.00021675
Iteration 73/1000 | Loss: 0.00036296
Iteration 74/1000 | Loss: 0.00032135
Iteration 75/1000 | Loss: 0.00011045
Iteration 76/1000 | Loss: 0.00009656
Iteration 77/1000 | Loss: 0.00009232
Iteration 78/1000 | Loss: 0.00009109
Iteration 79/1000 | Loss: 0.00009005
Iteration 80/1000 | Loss: 0.00008897
Iteration 81/1000 | Loss: 0.00008771
Iteration 82/1000 | Loss: 0.00050401
Iteration 83/1000 | Loss: 0.00016202
Iteration 84/1000 | Loss: 0.00052937
Iteration 85/1000 | Loss: 0.00021625
Iteration 86/1000 | Loss: 0.00048012
Iteration 87/1000 | Loss: 0.00050088
Iteration 88/1000 | Loss: 0.00013038
Iteration 89/1000 | Loss: 0.00034619
Iteration 90/1000 | Loss: 0.00011655
Iteration 91/1000 | Loss: 0.00028709
Iteration 92/1000 | Loss: 0.00015492
Iteration 93/1000 | Loss: 0.00009870
Iteration 94/1000 | Loss: 0.00008428
Iteration 95/1000 | Loss: 0.00029395
Iteration 96/1000 | Loss: 0.00008822
Iteration 97/1000 | Loss: 0.00008206
Iteration 98/1000 | Loss: 0.00007986
Iteration 99/1000 | Loss: 0.00007859
Iteration 100/1000 | Loss: 0.00007784
Iteration 101/1000 | Loss: 0.00007742
Iteration 102/1000 | Loss: 0.00007685
Iteration 103/1000 | Loss: 0.00007632
Iteration 104/1000 | Loss: 0.00007587
Iteration 105/1000 | Loss: 0.00007541
Iteration 106/1000 | Loss: 0.00007500
Iteration 107/1000 | Loss: 0.00007463
Iteration 108/1000 | Loss: 0.00007435
Iteration 109/1000 | Loss: 0.00007416
Iteration 110/1000 | Loss: 0.00007393
Iteration 111/1000 | Loss: 0.00007390
Iteration 112/1000 | Loss: 0.00007390
Iteration 113/1000 | Loss: 0.00007374
Iteration 114/1000 | Loss: 0.00019597
Iteration 115/1000 | Loss: 0.00007881
Iteration 116/1000 | Loss: 0.00007699
Iteration 117/1000 | Loss: 0.00007542
Iteration 118/1000 | Loss: 0.00007490
Iteration 119/1000 | Loss: 0.00007452
Iteration 120/1000 | Loss: 0.00007430
Iteration 121/1000 | Loss: 0.00007401
Iteration 122/1000 | Loss: 0.00007377
Iteration 123/1000 | Loss: 0.00007370
Iteration 124/1000 | Loss: 0.00007357
Iteration 125/1000 | Loss: 0.00007356
Iteration 126/1000 | Loss: 0.00007356
Iteration 127/1000 | Loss: 0.00007351
Iteration 128/1000 | Loss: 0.00007350
Iteration 129/1000 | Loss: 0.00007349
Iteration 130/1000 | Loss: 0.00007348
Iteration 131/1000 | Loss: 0.00007348
Iteration 132/1000 | Loss: 0.00007348
Iteration 133/1000 | Loss: 0.00007348
Iteration 134/1000 | Loss: 0.00007348
Iteration 135/1000 | Loss: 0.00007347
Iteration 136/1000 | Loss: 0.00007347
Iteration 137/1000 | Loss: 0.00007347
Iteration 138/1000 | Loss: 0.00007347
Iteration 139/1000 | Loss: 0.00007347
Iteration 140/1000 | Loss: 0.00007347
Iteration 141/1000 | Loss: 0.00007347
Iteration 142/1000 | Loss: 0.00007347
Iteration 143/1000 | Loss: 0.00007347
Iteration 144/1000 | Loss: 0.00007347
Iteration 145/1000 | Loss: 0.00007346
Iteration 146/1000 | Loss: 0.00007346
Iteration 147/1000 | Loss: 0.00007346
Iteration 148/1000 | Loss: 0.00007346
Iteration 149/1000 | Loss: 0.00007346
Iteration 150/1000 | Loss: 0.00007346
Iteration 151/1000 | Loss: 0.00007346
Iteration 152/1000 | Loss: 0.00007345
Iteration 153/1000 | Loss: 0.00007345
Iteration 154/1000 | Loss: 0.00007345
Iteration 155/1000 | Loss: 0.00007345
Iteration 156/1000 | Loss: 0.00007345
Iteration 157/1000 | Loss: 0.00007345
Iteration 158/1000 | Loss: 0.00007344
Iteration 159/1000 | Loss: 0.00007344
Iteration 160/1000 | Loss: 0.00007344
Iteration 161/1000 | Loss: 0.00007344
Iteration 162/1000 | Loss: 0.00007343
Iteration 163/1000 | Loss: 0.00007343
Iteration 164/1000 | Loss: 0.00007343
Iteration 165/1000 | Loss: 0.00007343
Iteration 166/1000 | Loss: 0.00007343
Iteration 167/1000 | Loss: 0.00007342
Iteration 168/1000 | Loss: 0.00007342
Iteration 169/1000 | Loss: 0.00007342
Iteration 170/1000 | Loss: 0.00007342
Iteration 171/1000 | Loss: 0.00007342
Iteration 172/1000 | Loss: 0.00007342
Iteration 173/1000 | Loss: 0.00007342
Iteration 174/1000 | Loss: 0.00007341
Iteration 175/1000 | Loss: 0.00007341
Iteration 176/1000 | Loss: 0.00007341
Iteration 177/1000 | Loss: 0.00007341
Iteration 178/1000 | Loss: 0.00007341
Iteration 179/1000 | Loss: 0.00007341
Iteration 180/1000 | Loss: 0.00007341
Iteration 181/1000 | Loss: 0.00007341
Iteration 182/1000 | Loss: 0.00007341
Iteration 183/1000 | Loss: 0.00007341
Iteration 184/1000 | Loss: 0.00007341
Iteration 185/1000 | Loss: 0.00007341
Iteration 186/1000 | Loss: 0.00007340
Iteration 187/1000 | Loss: 0.00007340
Iteration 188/1000 | Loss: 0.00007340
Iteration 189/1000 | Loss: 0.00007340
Iteration 190/1000 | Loss: 0.00007340
Iteration 191/1000 | Loss: 0.00007340
Iteration 192/1000 | Loss: 0.00007340
Iteration 193/1000 | Loss: 0.00007340
Iteration 194/1000 | Loss: 0.00007340
Iteration 195/1000 | Loss: 0.00007340
Iteration 196/1000 | Loss: 0.00007340
Iteration 197/1000 | Loss: 0.00007340
Iteration 198/1000 | Loss: 0.00007340
Iteration 199/1000 | Loss: 0.00007340
Iteration 200/1000 | Loss: 0.00007340
Iteration 201/1000 | Loss: 0.00007340
Iteration 202/1000 | Loss: 0.00007340
Iteration 203/1000 | Loss: 0.00007340
Iteration 204/1000 | Loss: 0.00007340
Iteration 205/1000 | Loss: 0.00007340
Iteration 206/1000 | Loss: 0.00007340
Iteration 207/1000 | Loss: 0.00007340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [7.33955530449748e-05, 7.33955530449748e-05, 7.33955530449748e-05, 7.33955530449748e-05, 7.33955530449748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.33955530449748e-05

Optimization complete. Final v2v error: 4.6767897605896 mm

Highest mean error: 12.27937126159668 mm for frame 53

Lowest mean error: 3.011754274368286 mm for frame 17

Saving results

Total time: 234.01417922973633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509690
Iteration 2/25 | Loss: 0.00110164
Iteration 3/25 | Loss: 0.00099959
Iteration 4/25 | Loss: 0.00098294
Iteration 5/25 | Loss: 0.00097828
Iteration 6/25 | Loss: 0.00097716
Iteration 7/25 | Loss: 0.00097716
Iteration 8/25 | Loss: 0.00097716
Iteration 9/25 | Loss: 0.00097716
Iteration 10/25 | Loss: 0.00097716
Iteration 11/25 | Loss: 0.00097716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000977159827016294, 0.000977159827016294, 0.000977159827016294, 0.000977159827016294, 0.000977159827016294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000977159827016294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41766167
Iteration 2/25 | Loss: 0.00073496
Iteration 3/25 | Loss: 0.00073496
Iteration 4/25 | Loss: 0.00073495
Iteration 5/25 | Loss: 0.00073495
Iteration 6/25 | Loss: 0.00073495
Iteration 7/25 | Loss: 0.00073495
Iteration 8/25 | Loss: 0.00073495
Iteration 9/25 | Loss: 0.00073495
Iteration 10/25 | Loss: 0.00073495
Iteration 11/25 | Loss: 0.00073495
Iteration 12/25 | Loss: 0.00073495
Iteration 13/25 | Loss: 0.00073495
Iteration 14/25 | Loss: 0.00073495
Iteration 15/25 | Loss: 0.00073495
Iteration 16/25 | Loss: 0.00073495
Iteration 17/25 | Loss: 0.00073495
Iteration 18/25 | Loss: 0.00073495
Iteration 19/25 | Loss: 0.00073495
Iteration 20/25 | Loss: 0.00073495
Iteration 21/25 | Loss: 0.00073495
Iteration 22/25 | Loss: 0.00073495
Iteration 23/25 | Loss: 0.00073495
Iteration 24/25 | Loss: 0.00073495
Iteration 25/25 | Loss: 0.00073495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073495
Iteration 2/1000 | Loss: 0.00001742
Iteration 3/1000 | Loss: 0.00001318
Iteration 4/1000 | Loss: 0.00001223
Iteration 5/1000 | Loss: 0.00001165
Iteration 6/1000 | Loss: 0.00001121
Iteration 7/1000 | Loss: 0.00001104
Iteration 8/1000 | Loss: 0.00001069
Iteration 9/1000 | Loss: 0.00001066
Iteration 10/1000 | Loss: 0.00001055
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001054
Iteration 13/1000 | Loss: 0.00001046
Iteration 14/1000 | Loss: 0.00001043
Iteration 15/1000 | Loss: 0.00001040
Iteration 16/1000 | Loss: 0.00001029
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001026
Iteration 19/1000 | Loss: 0.00001025
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001023
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001023
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001018
Iteration 27/1000 | Loss: 0.00001017
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001009
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001007
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001006
Iteration 38/1000 | Loss: 0.00001005
Iteration 39/1000 | Loss: 0.00001005
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001003
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001002
Iteration 44/1000 | Loss: 0.00001002
Iteration 45/1000 | Loss: 0.00001002
Iteration 46/1000 | Loss: 0.00001001
Iteration 47/1000 | Loss: 0.00001001
Iteration 48/1000 | Loss: 0.00001000
Iteration 49/1000 | Loss: 0.00001000
Iteration 50/1000 | Loss: 0.00001000
Iteration 51/1000 | Loss: 0.00001000
Iteration 52/1000 | Loss: 0.00000999
Iteration 53/1000 | Loss: 0.00000998
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000996
Iteration 57/1000 | Loss: 0.00000996
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000995
Iteration 60/1000 | Loss: 0.00000995
Iteration 61/1000 | Loss: 0.00000994
Iteration 62/1000 | Loss: 0.00000994
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000994
Iteration 66/1000 | Loss: 0.00000994
Iteration 67/1000 | Loss: 0.00000994
Iteration 68/1000 | Loss: 0.00000994
Iteration 69/1000 | Loss: 0.00000994
Iteration 70/1000 | Loss: 0.00000994
Iteration 71/1000 | Loss: 0.00000994
Iteration 72/1000 | Loss: 0.00000993
Iteration 73/1000 | Loss: 0.00000993
Iteration 74/1000 | Loss: 0.00000993
Iteration 75/1000 | Loss: 0.00000993
Iteration 76/1000 | Loss: 0.00000993
Iteration 77/1000 | Loss: 0.00000992
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000989
Iteration 84/1000 | Loss: 0.00000989
Iteration 85/1000 | Loss: 0.00000989
Iteration 86/1000 | Loss: 0.00000988
Iteration 87/1000 | Loss: 0.00000988
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000987
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000987
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000985
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000985
Iteration 105/1000 | Loss: 0.00000985
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000985
Iteration 109/1000 | Loss: 0.00000985
Iteration 110/1000 | Loss: 0.00000985
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [9.847668479778804e-06, 9.847668479778804e-06, 9.847668479778804e-06, 9.847668479778804e-06, 9.847668479778804e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.847668479778804e-06

Optimization complete. Final v2v error: 2.7523012161254883 mm

Highest mean error: 2.985769271850586 mm for frame 182

Lowest mean error: 2.5006415843963623 mm for frame 13

Saving results

Total time: 37.90172553062439
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775200
Iteration 2/25 | Loss: 0.00133374
Iteration 3/25 | Loss: 0.00110079
Iteration 4/25 | Loss: 0.00107479
Iteration 5/25 | Loss: 0.00106852
Iteration 6/25 | Loss: 0.00106739
Iteration 7/25 | Loss: 0.00106739
Iteration 8/25 | Loss: 0.00106739
Iteration 9/25 | Loss: 0.00106739
Iteration 10/25 | Loss: 0.00106739
Iteration 11/25 | Loss: 0.00106739
Iteration 12/25 | Loss: 0.00106739
Iteration 13/25 | Loss: 0.00106739
Iteration 14/25 | Loss: 0.00106739
Iteration 15/25 | Loss: 0.00106739
Iteration 16/25 | Loss: 0.00106739
Iteration 17/25 | Loss: 0.00106739
Iteration 18/25 | Loss: 0.00106739
Iteration 19/25 | Loss: 0.00106739
Iteration 20/25 | Loss: 0.00106739
Iteration 21/25 | Loss: 0.00106739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010673945071175694, 0.0010673945071175694, 0.0010673945071175694, 0.0010673945071175694, 0.0010673945071175694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010673945071175694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18496943
Iteration 2/25 | Loss: 0.00065015
Iteration 3/25 | Loss: 0.00065009
Iteration 4/25 | Loss: 0.00065009
Iteration 5/25 | Loss: 0.00065009
Iteration 6/25 | Loss: 0.00065009
Iteration 7/25 | Loss: 0.00065009
Iteration 8/25 | Loss: 0.00065009
Iteration 9/25 | Loss: 0.00065009
Iteration 10/25 | Loss: 0.00065009
Iteration 11/25 | Loss: 0.00065009
Iteration 12/25 | Loss: 0.00065009
Iteration 13/25 | Loss: 0.00065009
Iteration 14/25 | Loss: 0.00065009
Iteration 15/25 | Loss: 0.00065009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006500898161903024, 0.0006500898161903024, 0.0006500898161903024, 0.0006500898161903024, 0.0006500898161903024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006500898161903024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065009
Iteration 2/1000 | Loss: 0.00003860
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00001786
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001512
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001501
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001492
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001489
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001485
Iteration 24/1000 | Loss: 0.00001485
Iteration 25/1000 | Loss: 0.00001484
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001483
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001472
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001466
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001460
Iteration 97/1000 | Loss: 0.00001460
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001459
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001458
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001457
Iteration 118/1000 | Loss: 0.00001457
Iteration 119/1000 | Loss: 0.00001457
Iteration 120/1000 | Loss: 0.00001457
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001457
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001456
Iteration 130/1000 | Loss: 0.00001456
Iteration 131/1000 | Loss: 0.00001456
Iteration 132/1000 | Loss: 0.00001456
Iteration 133/1000 | Loss: 0.00001456
Iteration 134/1000 | Loss: 0.00001456
Iteration 135/1000 | Loss: 0.00001456
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Iteration 138/1000 | Loss: 0.00001455
Iteration 139/1000 | Loss: 0.00001455
Iteration 140/1000 | Loss: 0.00001455
Iteration 141/1000 | Loss: 0.00001455
Iteration 142/1000 | Loss: 0.00001455
Iteration 143/1000 | Loss: 0.00001455
Iteration 144/1000 | Loss: 0.00001455
Iteration 145/1000 | Loss: 0.00001455
Iteration 146/1000 | Loss: 0.00001455
Iteration 147/1000 | Loss: 0.00001455
Iteration 148/1000 | Loss: 0.00001454
Iteration 149/1000 | Loss: 0.00001454
Iteration 150/1000 | Loss: 0.00001454
Iteration 151/1000 | Loss: 0.00001454
Iteration 152/1000 | Loss: 0.00001454
Iteration 153/1000 | Loss: 0.00001454
Iteration 154/1000 | Loss: 0.00001454
Iteration 155/1000 | Loss: 0.00001454
Iteration 156/1000 | Loss: 0.00001454
Iteration 157/1000 | Loss: 0.00001454
Iteration 158/1000 | Loss: 0.00001454
Iteration 159/1000 | Loss: 0.00001454
Iteration 160/1000 | Loss: 0.00001454
Iteration 161/1000 | Loss: 0.00001454
Iteration 162/1000 | Loss: 0.00001454
Iteration 163/1000 | Loss: 0.00001454
Iteration 164/1000 | Loss: 0.00001454
Iteration 165/1000 | Loss: 0.00001454
Iteration 166/1000 | Loss: 0.00001454
Iteration 167/1000 | Loss: 0.00001454
Iteration 168/1000 | Loss: 0.00001454
Iteration 169/1000 | Loss: 0.00001454
Iteration 170/1000 | Loss: 0.00001454
Iteration 171/1000 | Loss: 0.00001454
Iteration 172/1000 | Loss: 0.00001454
Iteration 173/1000 | Loss: 0.00001454
Iteration 174/1000 | Loss: 0.00001454
Iteration 175/1000 | Loss: 0.00001454
Iteration 176/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.453573440812761e-05, 1.453573440812761e-05, 1.453573440812761e-05, 1.453573440812761e-05, 1.453573440812761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.453573440812761e-05

Optimization complete. Final v2v error: 3.1557798385620117 mm

Highest mean error: 4.236532688140869 mm for frame 212

Lowest mean error: 2.580927610397339 mm for frame 145

Saving results

Total time: 41.21539783477783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847405
Iteration 2/25 | Loss: 0.00146784
Iteration 3/25 | Loss: 0.00113983
Iteration 4/25 | Loss: 0.00111363
Iteration 5/25 | Loss: 0.00111092
Iteration 6/25 | Loss: 0.00111092
Iteration 7/25 | Loss: 0.00111092
Iteration 8/25 | Loss: 0.00111092
Iteration 9/25 | Loss: 0.00111092
Iteration 10/25 | Loss: 0.00111092
Iteration 11/25 | Loss: 0.00111092
Iteration 12/25 | Loss: 0.00111092
Iteration 13/25 | Loss: 0.00111092
Iteration 14/25 | Loss: 0.00111092
Iteration 15/25 | Loss: 0.00111092
Iteration 16/25 | Loss: 0.00111092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011109240585938096, 0.0011109240585938096, 0.0011109240585938096, 0.0011109240585938096, 0.0011109240585938096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011109240585938096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.14745550
Iteration 2/25 | Loss: 0.00056523
Iteration 3/25 | Loss: 0.00056523
Iteration 4/25 | Loss: 0.00056523
Iteration 5/25 | Loss: 0.00056523
Iteration 6/25 | Loss: 0.00056523
Iteration 7/25 | Loss: 0.00056523
Iteration 8/25 | Loss: 0.00056523
Iteration 9/25 | Loss: 0.00056523
Iteration 10/25 | Loss: 0.00056523
Iteration 11/25 | Loss: 0.00056523
Iteration 12/25 | Loss: 0.00056523
Iteration 13/25 | Loss: 0.00056523
Iteration 14/25 | Loss: 0.00056523
Iteration 15/25 | Loss: 0.00056523
Iteration 16/25 | Loss: 0.00056523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005652254330925643, 0.0005652254330925643, 0.0005652254330925643, 0.0005652254330925643, 0.0005652254330925643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005652254330925643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056523
Iteration 2/1000 | Loss: 0.00005409
Iteration 3/1000 | Loss: 0.00002835
Iteration 4/1000 | Loss: 0.00002353
Iteration 5/1000 | Loss: 0.00002207
Iteration 6/1000 | Loss: 0.00002110
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002005
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001963
Iteration 12/1000 | Loss: 0.00001946
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001929
Iteration 15/1000 | Loss: 0.00001926
Iteration 16/1000 | Loss: 0.00001926
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001921
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001921
Iteration 23/1000 | Loss: 0.00001921
Iteration 24/1000 | Loss: 0.00001921
Iteration 25/1000 | Loss: 0.00001920
Iteration 26/1000 | Loss: 0.00001920
Iteration 27/1000 | Loss: 0.00001920
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001918
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001915
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001914
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001914
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001914
Iteration 43/1000 | Loss: 0.00001914
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001913
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001913
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001912
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001911
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001908
Iteration 64/1000 | Loss: 0.00001908
Iteration 65/1000 | Loss: 0.00001908
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001907
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001907
Iteration 74/1000 | Loss: 0.00001907
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001906
Iteration 78/1000 | Loss: 0.00001906
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001905
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001905
Iteration 87/1000 | Loss: 0.00001905
Iteration 88/1000 | Loss: 0.00001905
Iteration 89/1000 | Loss: 0.00001905
Iteration 90/1000 | Loss: 0.00001904
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001904
Iteration 99/1000 | Loss: 0.00001904
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001902
Iteration 105/1000 | Loss: 0.00001902
Iteration 106/1000 | Loss: 0.00001901
Iteration 107/1000 | Loss: 0.00001901
Iteration 108/1000 | Loss: 0.00001901
Iteration 109/1000 | Loss: 0.00001900
Iteration 110/1000 | Loss: 0.00001900
Iteration 111/1000 | Loss: 0.00001900
Iteration 112/1000 | Loss: 0.00001900
Iteration 113/1000 | Loss: 0.00001900
Iteration 114/1000 | Loss: 0.00001900
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001900
Iteration 118/1000 | Loss: 0.00001900
Iteration 119/1000 | Loss: 0.00001900
Iteration 120/1000 | Loss: 0.00001900
Iteration 121/1000 | Loss: 0.00001900
Iteration 122/1000 | Loss: 0.00001900
Iteration 123/1000 | Loss: 0.00001900
Iteration 124/1000 | Loss: 0.00001900
Iteration 125/1000 | Loss: 0.00001900
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00001898
Iteration 130/1000 | Loss: 0.00001898
Iteration 131/1000 | Loss: 0.00001898
Iteration 132/1000 | Loss: 0.00001898
Iteration 133/1000 | Loss: 0.00001898
Iteration 134/1000 | Loss: 0.00001898
Iteration 135/1000 | Loss: 0.00001897
Iteration 136/1000 | Loss: 0.00001897
Iteration 137/1000 | Loss: 0.00001897
Iteration 138/1000 | Loss: 0.00001897
Iteration 139/1000 | Loss: 0.00001897
Iteration 140/1000 | Loss: 0.00001897
Iteration 141/1000 | Loss: 0.00001897
Iteration 142/1000 | Loss: 0.00001897
Iteration 143/1000 | Loss: 0.00001897
Iteration 144/1000 | Loss: 0.00001897
Iteration 145/1000 | Loss: 0.00001897
Iteration 146/1000 | Loss: 0.00001896
Iteration 147/1000 | Loss: 0.00001896
Iteration 148/1000 | Loss: 0.00001896
Iteration 149/1000 | Loss: 0.00001896
Iteration 150/1000 | Loss: 0.00001896
Iteration 151/1000 | Loss: 0.00001896
Iteration 152/1000 | Loss: 0.00001896
Iteration 153/1000 | Loss: 0.00001896
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001895
Iteration 156/1000 | Loss: 0.00001895
Iteration 157/1000 | Loss: 0.00001895
Iteration 158/1000 | Loss: 0.00001895
Iteration 159/1000 | Loss: 0.00001894
Iteration 160/1000 | Loss: 0.00001894
Iteration 161/1000 | Loss: 0.00001894
Iteration 162/1000 | Loss: 0.00001894
Iteration 163/1000 | Loss: 0.00001894
Iteration 164/1000 | Loss: 0.00001894
Iteration 165/1000 | Loss: 0.00001894
Iteration 166/1000 | Loss: 0.00001894
Iteration 167/1000 | Loss: 0.00001894
Iteration 168/1000 | Loss: 0.00001894
Iteration 169/1000 | Loss: 0.00001894
Iteration 170/1000 | Loss: 0.00001894
Iteration 171/1000 | Loss: 0.00001894
Iteration 172/1000 | Loss: 0.00001894
Iteration 173/1000 | Loss: 0.00001894
Iteration 174/1000 | Loss: 0.00001894
Iteration 175/1000 | Loss: 0.00001894
Iteration 176/1000 | Loss: 0.00001894
Iteration 177/1000 | Loss: 0.00001894
Iteration 178/1000 | Loss: 0.00001894
Iteration 179/1000 | Loss: 0.00001894
Iteration 180/1000 | Loss: 0.00001894
Iteration 181/1000 | Loss: 0.00001894
Iteration 182/1000 | Loss: 0.00001894
Iteration 183/1000 | Loss: 0.00001894
Iteration 184/1000 | Loss: 0.00001894
Iteration 185/1000 | Loss: 0.00001894
Iteration 186/1000 | Loss: 0.00001894
Iteration 187/1000 | Loss: 0.00001894
Iteration 188/1000 | Loss: 0.00001894
Iteration 189/1000 | Loss: 0.00001894
Iteration 190/1000 | Loss: 0.00001894
Iteration 191/1000 | Loss: 0.00001894
Iteration 192/1000 | Loss: 0.00001894
Iteration 193/1000 | Loss: 0.00001894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.8935088519356214e-05, 1.8935088519356214e-05, 1.8935088519356214e-05, 1.8935088519356214e-05, 1.8935088519356214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8935088519356214e-05

Optimization complete. Final v2v error: 3.7086257934570312 mm

Highest mean error: 3.965404748916626 mm for frame 51

Lowest mean error: 3.259359836578369 mm for frame 102

Saving results

Total time: 43.143386125564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431454
Iteration 2/25 | Loss: 0.00124503
Iteration 3/25 | Loss: 0.00104480
Iteration 4/25 | Loss: 0.00102420
Iteration 5/25 | Loss: 0.00101989
Iteration 6/25 | Loss: 0.00101925
Iteration 7/25 | Loss: 0.00101925
Iteration 8/25 | Loss: 0.00101925
Iteration 9/25 | Loss: 0.00101925
Iteration 10/25 | Loss: 0.00101925
Iteration 11/25 | Loss: 0.00101925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010192451300099492, 0.0010192451300099492, 0.0010192451300099492, 0.0010192451300099492, 0.0010192451300099492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010192451300099492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36997581
Iteration 2/25 | Loss: 0.00060504
Iteration 3/25 | Loss: 0.00060504
Iteration 4/25 | Loss: 0.00060504
Iteration 5/25 | Loss: 0.00060504
Iteration 6/25 | Loss: 0.00060504
Iteration 7/25 | Loss: 0.00060504
Iteration 8/25 | Loss: 0.00060503
Iteration 9/25 | Loss: 0.00060503
Iteration 10/25 | Loss: 0.00060503
Iteration 11/25 | Loss: 0.00060503
Iteration 12/25 | Loss: 0.00060503
Iteration 13/25 | Loss: 0.00060503
Iteration 14/25 | Loss: 0.00060503
Iteration 15/25 | Loss: 0.00060503
Iteration 16/25 | Loss: 0.00060503
Iteration 17/25 | Loss: 0.00060503
Iteration 18/25 | Loss: 0.00060503
Iteration 19/25 | Loss: 0.00060503
Iteration 20/25 | Loss: 0.00060503
Iteration 21/25 | Loss: 0.00060503
Iteration 22/25 | Loss: 0.00060503
Iteration 23/25 | Loss: 0.00060503
Iteration 24/25 | Loss: 0.00060503
Iteration 25/25 | Loss: 0.00060503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060503
Iteration 2/1000 | Loss: 0.00003799
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00001772
Iteration 5/1000 | Loss: 0.00001657
Iteration 6/1000 | Loss: 0.00001577
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001486
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001447
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001405
Iteration 25/1000 | Loss: 0.00001403
Iteration 26/1000 | Loss: 0.00001402
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001399
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001393
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001391
Iteration 40/1000 | Loss: 0.00001391
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001390
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001383
Iteration 54/1000 | Loss: 0.00001382
Iteration 55/1000 | Loss: 0.00001382
Iteration 56/1000 | Loss: 0.00001382
Iteration 57/1000 | Loss: 0.00001382
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001382
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001381
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001378
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001377
Iteration 78/1000 | Loss: 0.00001377
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001376
Iteration 81/1000 | Loss: 0.00001376
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001375
Iteration 84/1000 | Loss: 0.00001375
Iteration 85/1000 | Loss: 0.00001375
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001371
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001367
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001366
Iteration 139/1000 | Loss: 0.00001366
Iteration 140/1000 | Loss: 0.00001366
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001365
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001365
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001365
Iteration 162/1000 | Loss: 0.00001365
Iteration 163/1000 | Loss: 0.00001365
Iteration 164/1000 | Loss: 0.00001365
Iteration 165/1000 | Loss: 0.00001365
Iteration 166/1000 | Loss: 0.00001365
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.3649266293214168e-05, 1.3649266293214168e-05, 1.3649266293214168e-05, 1.3649266293214168e-05, 1.3649266293214168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3649266293214168e-05

Optimization complete. Final v2v error: 3.1033976078033447 mm

Highest mean error: 4.354361534118652 mm for frame 203

Lowest mean error: 2.6391289234161377 mm for frame 97

Saving results

Total time: 42.172723054885864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002011
Iteration 2/25 | Loss: 0.01002011
Iteration 3/25 | Loss: 0.00237783
Iteration 4/25 | Loss: 0.00184425
Iteration 5/25 | Loss: 0.00169889
Iteration 6/25 | Loss: 0.00172939
Iteration 7/25 | Loss: 0.00174409
Iteration 8/25 | Loss: 0.00161216
Iteration 9/25 | Loss: 0.00151654
Iteration 10/25 | Loss: 0.00145551
Iteration 11/25 | Loss: 0.00141098
Iteration 12/25 | Loss: 0.00137436
Iteration 13/25 | Loss: 0.00135814
Iteration 14/25 | Loss: 0.00134611
Iteration 15/25 | Loss: 0.00135111
Iteration 16/25 | Loss: 0.00133668
Iteration 17/25 | Loss: 0.00133377
Iteration 18/25 | Loss: 0.00132483
Iteration 19/25 | Loss: 0.00131237
Iteration 20/25 | Loss: 0.00130706
Iteration 21/25 | Loss: 0.00130792
Iteration 22/25 | Loss: 0.00130618
Iteration 23/25 | Loss: 0.00131066
Iteration 24/25 | Loss: 0.00130759
Iteration 25/25 | Loss: 0.00130774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54321134
Iteration 2/25 | Loss: 0.00381908
Iteration 3/25 | Loss: 0.00220200
Iteration 4/25 | Loss: 0.00221488
Iteration 5/25 | Loss: 0.00201079
Iteration 6/25 | Loss: 0.00201079
Iteration 7/25 | Loss: 0.00201079
Iteration 8/25 | Loss: 0.00201079
Iteration 9/25 | Loss: 0.00201079
Iteration 10/25 | Loss: 0.00201079
Iteration 11/25 | Loss: 0.00201079
Iteration 12/25 | Loss: 0.00201079
Iteration 13/25 | Loss: 0.00201079
Iteration 14/25 | Loss: 0.00201079
Iteration 15/25 | Loss: 0.00201079
Iteration 16/25 | Loss: 0.00201079
Iteration 17/25 | Loss: 0.00201079
Iteration 18/25 | Loss: 0.00201079
Iteration 19/25 | Loss: 0.00201079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020107850432395935, 0.0020107850432395935, 0.0020107850432395935, 0.0020107850432395935, 0.0020107850432395935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020107850432395935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201079
Iteration 2/1000 | Loss: 0.00201986
Iteration 3/1000 | Loss: 0.00196661
Iteration 4/1000 | Loss: 0.00086789
Iteration 5/1000 | Loss: 0.00090956
Iteration 6/1000 | Loss: 0.00058716
Iteration 7/1000 | Loss: 0.00152231
Iteration 8/1000 | Loss: 0.00149925
Iteration 9/1000 | Loss: 0.00032871
Iteration 10/1000 | Loss: 0.00128651
Iteration 11/1000 | Loss: 0.00141443
Iteration 12/1000 | Loss: 0.00048046
Iteration 13/1000 | Loss: 0.00033161
Iteration 14/1000 | Loss: 0.00082528
Iteration 15/1000 | Loss: 0.00065399
Iteration 16/1000 | Loss: 0.00043730
Iteration 17/1000 | Loss: 0.00072127
Iteration 18/1000 | Loss: 0.00085262
Iteration 19/1000 | Loss: 0.00020347
Iteration 20/1000 | Loss: 0.00021132
Iteration 21/1000 | Loss: 0.00023876
Iteration 22/1000 | Loss: 0.00035567
Iteration 23/1000 | Loss: 0.00047576
Iteration 24/1000 | Loss: 0.00040059
Iteration 25/1000 | Loss: 0.00049739
Iteration 26/1000 | Loss: 0.00038647
Iteration 27/1000 | Loss: 0.00034066
Iteration 28/1000 | Loss: 0.00032306
Iteration 29/1000 | Loss: 0.00021829
Iteration 30/1000 | Loss: 0.00034809
Iteration 31/1000 | Loss: 0.00023308
Iteration 32/1000 | Loss: 0.00032599
Iteration 33/1000 | Loss: 0.00029392
Iteration 34/1000 | Loss: 0.00020907
Iteration 35/1000 | Loss: 0.00022718
Iteration 36/1000 | Loss: 0.00023548
Iteration 37/1000 | Loss: 0.00020006
Iteration 38/1000 | Loss: 0.00018868
Iteration 39/1000 | Loss: 0.00054249
Iteration 40/1000 | Loss: 0.00019386
Iteration 41/1000 | Loss: 0.00017317
Iteration 42/1000 | Loss: 0.00042984
Iteration 43/1000 | Loss: 0.00037457
Iteration 44/1000 | Loss: 0.00024425
Iteration 45/1000 | Loss: 0.00016788
Iteration 46/1000 | Loss: 0.00047199
Iteration 47/1000 | Loss: 0.00035581
Iteration 48/1000 | Loss: 0.00029245
Iteration 49/1000 | Loss: 0.00024307
Iteration 50/1000 | Loss: 0.00015092
Iteration 51/1000 | Loss: 0.00015738
Iteration 52/1000 | Loss: 0.00014403
Iteration 53/1000 | Loss: 0.00022986
Iteration 54/1000 | Loss: 0.00027082
Iteration 55/1000 | Loss: 0.00025405
Iteration 56/1000 | Loss: 0.00029041
Iteration 57/1000 | Loss: 0.00028427
Iteration 58/1000 | Loss: 0.00045015
Iteration 59/1000 | Loss: 0.00032927
Iteration 60/1000 | Loss: 0.00026138
Iteration 61/1000 | Loss: 0.00021927
Iteration 62/1000 | Loss: 0.00014481
Iteration 63/1000 | Loss: 0.00021719
Iteration 64/1000 | Loss: 0.00023881
Iteration 65/1000 | Loss: 0.00014502
Iteration 66/1000 | Loss: 0.00020629
Iteration 67/1000 | Loss: 0.00026152
Iteration 68/1000 | Loss: 0.00021624
Iteration 69/1000 | Loss: 0.00019431
Iteration 70/1000 | Loss: 0.00018412
Iteration 71/1000 | Loss: 0.00070815
Iteration 72/1000 | Loss: 0.00103104
Iteration 73/1000 | Loss: 0.00048460
Iteration 74/1000 | Loss: 0.00050573
Iteration 75/1000 | Loss: 0.00038575
Iteration 76/1000 | Loss: 0.00034332
Iteration 77/1000 | Loss: 0.00022063
Iteration 78/1000 | Loss: 0.00024707
Iteration 79/1000 | Loss: 0.00021828
Iteration 80/1000 | Loss: 0.00028481
Iteration 81/1000 | Loss: 0.00020507
Iteration 82/1000 | Loss: 0.00022576
Iteration 83/1000 | Loss: 0.00021645
Iteration 84/1000 | Loss: 0.00021745
Iteration 85/1000 | Loss: 0.00020702
Iteration 86/1000 | Loss: 0.00046251
Iteration 87/1000 | Loss: 0.00063219
Iteration 88/1000 | Loss: 0.00045351
Iteration 89/1000 | Loss: 0.00015161
Iteration 90/1000 | Loss: 0.00011844
Iteration 91/1000 | Loss: 0.00010716
Iteration 92/1000 | Loss: 0.00020342
Iteration 93/1000 | Loss: 0.00014378
Iteration 94/1000 | Loss: 0.00019020
Iteration 95/1000 | Loss: 0.00018782
Iteration 96/1000 | Loss: 0.00022648
Iteration 97/1000 | Loss: 0.00036634
Iteration 98/1000 | Loss: 0.00016354
Iteration 99/1000 | Loss: 0.00017018
Iteration 100/1000 | Loss: 0.00013779
Iteration 101/1000 | Loss: 0.00009941
Iteration 102/1000 | Loss: 0.00010309
Iteration 103/1000 | Loss: 0.00010502
Iteration 104/1000 | Loss: 0.00010033
Iteration 105/1000 | Loss: 0.00009895
Iteration 106/1000 | Loss: 0.00032912
Iteration 107/1000 | Loss: 0.00019326
Iteration 108/1000 | Loss: 0.00052186
Iteration 109/1000 | Loss: 0.00026526
Iteration 110/1000 | Loss: 0.00012403
Iteration 111/1000 | Loss: 0.00010397
Iteration 112/1000 | Loss: 0.00031733
Iteration 113/1000 | Loss: 0.00010720
Iteration 114/1000 | Loss: 0.00010757
Iteration 115/1000 | Loss: 0.00011219
Iteration 116/1000 | Loss: 0.00009321
Iteration 117/1000 | Loss: 0.00009754
Iteration 118/1000 | Loss: 0.00009130
Iteration 119/1000 | Loss: 0.00011037
Iteration 120/1000 | Loss: 0.00011143
Iteration 121/1000 | Loss: 0.00026766
Iteration 122/1000 | Loss: 0.00011364
Iteration 123/1000 | Loss: 0.00010255
Iteration 124/1000 | Loss: 0.00018599
Iteration 125/1000 | Loss: 0.00061098
Iteration 126/1000 | Loss: 0.00028713
Iteration 127/1000 | Loss: 0.00033728
Iteration 128/1000 | Loss: 0.00012569
Iteration 129/1000 | Loss: 0.00009782
Iteration 130/1000 | Loss: 0.00010920
Iteration 131/1000 | Loss: 0.00008485
Iteration 132/1000 | Loss: 0.00015325
Iteration 133/1000 | Loss: 0.00007974
Iteration 134/1000 | Loss: 0.00007793
Iteration 135/1000 | Loss: 0.00068606
Iteration 136/1000 | Loss: 0.00024328
Iteration 137/1000 | Loss: 0.00033026
Iteration 138/1000 | Loss: 0.00016594
Iteration 139/1000 | Loss: 0.00013337
Iteration 140/1000 | Loss: 0.00008309
Iteration 141/1000 | Loss: 0.00021258
Iteration 142/1000 | Loss: 0.00008157
Iteration 143/1000 | Loss: 0.00007691
Iteration 144/1000 | Loss: 0.00016658
Iteration 145/1000 | Loss: 0.00007687
Iteration 146/1000 | Loss: 0.00015672
Iteration 147/1000 | Loss: 0.00007555
Iteration 148/1000 | Loss: 0.00007319
Iteration 149/1000 | Loss: 0.00007246
Iteration 150/1000 | Loss: 0.00007179
Iteration 151/1000 | Loss: 0.00007106
Iteration 152/1000 | Loss: 0.00007045
Iteration 153/1000 | Loss: 0.00042529
Iteration 154/1000 | Loss: 0.00026699
Iteration 155/1000 | Loss: 0.00010696
Iteration 156/1000 | Loss: 0.00006945
Iteration 157/1000 | Loss: 0.00007317
Iteration 158/1000 | Loss: 0.00017894
Iteration 159/1000 | Loss: 0.00017091
Iteration 160/1000 | Loss: 0.00012404
Iteration 161/1000 | Loss: 0.00007368
Iteration 162/1000 | Loss: 0.00009442
Iteration 163/1000 | Loss: 0.00007126
Iteration 164/1000 | Loss: 0.00006989
Iteration 165/1000 | Loss: 0.00015271
Iteration 166/1000 | Loss: 0.00014587
Iteration 167/1000 | Loss: 0.00014736
Iteration 168/1000 | Loss: 0.00016659
Iteration 169/1000 | Loss: 0.00017906
Iteration 170/1000 | Loss: 0.00023903
Iteration 171/1000 | Loss: 0.00007129
Iteration 172/1000 | Loss: 0.00009049
Iteration 173/1000 | Loss: 0.00040056
Iteration 174/1000 | Loss: 0.00016088
Iteration 175/1000 | Loss: 0.00047055
Iteration 176/1000 | Loss: 0.00022973
Iteration 177/1000 | Loss: 0.00009321
Iteration 178/1000 | Loss: 0.00018914
Iteration 179/1000 | Loss: 0.00007528
Iteration 180/1000 | Loss: 0.00008707
Iteration 181/1000 | Loss: 0.00014271
Iteration 182/1000 | Loss: 0.00023828
Iteration 183/1000 | Loss: 0.00017711
Iteration 184/1000 | Loss: 0.00009101
Iteration 185/1000 | Loss: 0.00012912
Iteration 186/1000 | Loss: 0.00008534
Iteration 187/1000 | Loss: 0.00006893
Iteration 188/1000 | Loss: 0.00006850
Iteration 189/1000 | Loss: 0.00013389
Iteration 190/1000 | Loss: 0.00007218
Iteration 191/1000 | Loss: 0.00007074
Iteration 192/1000 | Loss: 0.00008645
Iteration 193/1000 | Loss: 0.00013071
Iteration 194/1000 | Loss: 0.00007745
Iteration 195/1000 | Loss: 0.00046403
Iteration 196/1000 | Loss: 0.00021788
Iteration 197/1000 | Loss: 0.00008576
Iteration 198/1000 | Loss: 0.00007189
Iteration 199/1000 | Loss: 0.00025892
Iteration 200/1000 | Loss: 0.00006988
Iteration 201/1000 | Loss: 0.00006705
Iteration 202/1000 | Loss: 0.00006643
Iteration 203/1000 | Loss: 0.00018376
Iteration 204/1000 | Loss: 0.00009544
Iteration 205/1000 | Loss: 0.00018579
Iteration 206/1000 | Loss: 0.00007751
Iteration 207/1000 | Loss: 0.00007065
Iteration 208/1000 | Loss: 0.00006637
Iteration 209/1000 | Loss: 0.00006584
Iteration 210/1000 | Loss: 0.00006567
Iteration 211/1000 | Loss: 0.00006562
Iteration 212/1000 | Loss: 0.00006547
Iteration 213/1000 | Loss: 0.00006534
Iteration 214/1000 | Loss: 0.00006533
Iteration 215/1000 | Loss: 0.00006533
Iteration 216/1000 | Loss: 0.00006532
Iteration 217/1000 | Loss: 0.00040517
Iteration 218/1000 | Loss: 0.00006930
Iteration 219/1000 | Loss: 0.00006670
Iteration 220/1000 | Loss: 0.00006580
Iteration 221/1000 | Loss: 0.00006508
Iteration 222/1000 | Loss: 0.00006437
Iteration 223/1000 | Loss: 0.00006401
Iteration 224/1000 | Loss: 0.00006382
Iteration 225/1000 | Loss: 0.00006375
Iteration 226/1000 | Loss: 0.00006373
Iteration 227/1000 | Loss: 0.00006373
Iteration 228/1000 | Loss: 0.00006372
Iteration 229/1000 | Loss: 0.00006372
Iteration 230/1000 | Loss: 0.00006371
Iteration 231/1000 | Loss: 0.00006371
Iteration 232/1000 | Loss: 0.00006371
Iteration 233/1000 | Loss: 0.00006371
Iteration 234/1000 | Loss: 0.00006371
Iteration 235/1000 | Loss: 0.00006371
Iteration 236/1000 | Loss: 0.00006371
Iteration 237/1000 | Loss: 0.00006371
Iteration 238/1000 | Loss: 0.00006357
Iteration 239/1000 | Loss: 0.00006356
Iteration 240/1000 | Loss: 0.00006352
Iteration 241/1000 | Loss: 0.00006348
Iteration 242/1000 | Loss: 0.00006348
Iteration 243/1000 | Loss: 0.00006343
Iteration 244/1000 | Loss: 0.00006340
Iteration 245/1000 | Loss: 0.00006337
Iteration 246/1000 | Loss: 0.00006334
Iteration 247/1000 | Loss: 0.00006334
Iteration 248/1000 | Loss: 0.00006334
Iteration 249/1000 | Loss: 0.00006334
Iteration 250/1000 | Loss: 0.00006334
Iteration 251/1000 | Loss: 0.00006333
Iteration 252/1000 | Loss: 0.00006333
Iteration 253/1000 | Loss: 0.00006333
Iteration 254/1000 | Loss: 0.00006333
Iteration 255/1000 | Loss: 0.00006332
Iteration 256/1000 | Loss: 0.00006332
Iteration 257/1000 | Loss: 0.00006332
Iteration 258/1000 | Loss: 0.00006332
Iteration 259/1000 | Loss: 0.00006332
Iteration 260/1000 | Loss: 0.00006332
Iteration 261/1000 | Loss: 0.00006332
Iteration 262/1000 | Loss: 0.00006332
Iteration 263/1000 | Loss: 0.00006332
Iteration 264/1000 | Loss: 0.00006332
Iteration 265/1000 | Loss: 0.00006331
Iteration 266/1000 | Loss: 0.00006331
Iteration 267/1000 | Loss: 0.00006331
Iteration 268/1000 | Loss: 0.00006331
Iteration 269/1000 | Loss: 0.00006331
Iteration 270/1000 | Loss: 0.00006331
Iteration 271/1000 | Loss: 0.00006331
Iteration 272/1000 | Loss: 0.00006331
Iteration 273/1000 | Loss: 0.00006330
Iteration 274/1000 | Loss: 0.00006330
Iteration 275/1000 | Loss: 0.00006330
Iteration 276/1000 | Loss: 0.00006329
Iteration 277/1000 | Loss: 0.00006329
Iteration 278/1000 | Loss: 0.00006329
Iteration 279/1000 | Loss: 0.00006329
Iteration 280/1000 | Loss: 0.00006329
Iteration 281/1000 | Loss: 0.00006329
Iteration 282/1000 | Loss: 0.00006329
Iteration 283/1000 | Loss: 0.00006329
Iteration 284/1000 | Loss: 0.00006329
Iteration 285/1000 | Loss: 0.00006329
Iteration 286/1000 | Loss: 0.00006328
Iteration 287/1000 | Loss: 0.00006328
Iteration 288/1000 | Loss: 0.00006328
Iteration 289/1000 | Loss: 0.00006328
Iteration 290/1000 | Loss: 0.00006328
Iteration 291/1000 | Loss: 0.00006328
Iteration 292/1000 | Loss: 0.00006328
Iteration 293/1000 | Loss: 0.00006328
Iteration 294/1000 | Loss: 0.00006328
Iteration 295/1000 | Loss: 0.00006328
Iteration 296/1000 | Loss: 0.00006328
Iteration 297/1000 | Loss: 0.00006328
Iteration 298/1000 | Loss: 0.00006328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [6.327624578261748e-05, 6.327624578261748e-05, 6.327624578261748e-05, 6.327624578261748e-05, 6.327624578261748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.327624578261748e-05

Optimization complete. Final v2v error: 4.826304912567139 mm

Highest mean error: 11.960487365722656 mm for frame 133

Lowest mean error: 2.9304232597351074 mm for frame 36

Saving results

Total time: 421.95652294158936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975363
Iteration 2/25 | Loss: 0.00975363
Iteration 3/25 | Loss: 0.00975363
Iteration 4/25 | Loss: 0.00975362
Iteration 5/25 | Loss: 0.00975362
Iteration 6/25 | Loss: 0.00975362
Iteration 7/25 | Loss: 0.00975362
Iteration 8/25 | Loss: 0.00975362
Iteration 9/25 | Loss: 0.00975362
Iteration 10/25 | Loss: 0.00975362
Iteration 11/25 | Loss: 0.00975362
Iteration 12/25 | Loss: 0.00975362
Iteration 13/25 | Loss: 0.00975362
Iteration 14/25 | Loss: 0.00975362
Iteration 15/25 | Loss: 0.00975361
Iteration 16/25 | Loss: 0.00975361
Iteration 17/25 | Loss: 0.00975361
Iteration 18/25 | Loss: 0.00975361
Iteration 19/25 | Loss: 0.00975361
Iteration 20/25 | Loss: 0.00975361
Iteration 21/25 | Loss: 0.00975361
Iteration 22/25 | Loss: 0.00975361
Iteration 23/25 | Loss: 0.00975361
Iteration 24/25 | Loss: 0.00975361
Iteration 25/25 | Loss: 0.00975361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69129920
Iteration 2/25 | Loss: 0.15257783
Iteration 3/25 | Loss: 0.14485198
Iteration 4/25 | Loss: 0.14405718
Iteration 5/25 | Loss: 0.14405711
Iteration 6/25 | Loss: 0.14405710
Iteration 7/25 | Loss: 0.14405708
Iteration 8/25 | Loss: 0.14405708
Iteration 9/25 | Loss: 0.14405708
Iteration 10/25 | Loss: 0.14405708
Iteration 11/25 | Loss: 0.14405707
Iteration 12/25 | Loss: 0.14405705
Iteration 13/25 | Loss: 0.14405705
Iteration 14/25 | Loss: 0.14405705
Iteration 15/25 | Loss: 0.14405705
Iteration 16/25 | Loss: 0.14405705
Iteration 17/25 | Loss: 0.14405705
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.14405705034732819, 0.14405705034732819, 0.14405705034732819, 0.14405705034732819, 0.14405705034732819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14405705034732819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14405705
Iteration 2/1000 | Loss: 0.00692949
Iteration 3/1000 | Loss: 0.00496154
Iteration 4/1000 | Loss: 0.00213422
Iteration 5/1000 | Loss: 0.00043304
Iteration 6/1000 | Loss: 0.00046385
Iteration 7/1000 | Loss: 0.00167812
Iteration 8/1000 | Loss: 0.00228177
Iteration 9/1000 | Loss: 0.00154686
Iteration 10/1000 | Loss: 0.00068846
Iteration 11/1000 | Loss: 0.00008897
Iteration 12/1000 | Loss: 0.00062501
Iteration 13/1000 | Loss: 0.00007647
Iteration 14/1000 | Loss: 0.00097148
Iteration 15/1000 | Loss: 0.00453478
Iteration 16/1000 | Loss: 0.00038805
Iteration 17/1000 | Loss: 0.00037087
Iteration 18/1000 | Loss: 0.00012616
Iteration 19/1000 | Loss: 0.00033180
Iteration 20/1000 | Loss: 0.00053648
Iteration 21/1000 | Loss: 0.00027425
Iteration 22/1000 | Loss: 0.00014263
Iteration 23/1000 | Loss: 0.00003937
Iteration 24/1000 | Loss: 0.00004916
Iteration 25/1000 | Loss: 0.00011981
Iteration 26/1000 | Loss: 0.00016760
Iteration 27/1000 | Loss: 0.00040945
Iteration 28/1000 | Loss: 0.00160160
Iteration 29/1000 | Loss: 0.00009810
Iteration 30/1000 | Loss: 0.00003952
Iteration 31/1000 | Loss: 0.00012004
Iteration 32/1000 | Loss: 0.00020572
Iteration 33/1000 | Loss: 0.00016914
Iteration 34/1000 | Loss: 0.00008278
Iteration 35/1000 | Loss: 0.00003234
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00018739
Iteration 38/1000 | Loss: 0.00035343
Iteration 39/1000 | Loss: 0.00005021
Iteration 40/1000 | Loss: 0.00012595
Iteration 41/1000 | Loss: 0.00022960
Iteration 42/1000 | Loss: 0.00001896
Iteration 43/1000 | Loss: 0.00001859
Iteration 44/1000 | Loss: 0.00025292
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00009950
Iteration 47/1000 | Loss: 0.00022308
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00015639
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00009789
Iteration 52/1000 | Loss: 0.00001698
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001672
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001661
Iteration 60/1000 | Loss: 0.00001661
Iteration 61/1000 | Loss: 0.00001649
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001644
Iteration 69/1000 | Loss: 0.00001644
Iteration 70/1000 | Loss: 0.00001643
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00010123
Iteration 75/1000 | Loss: 0.00007416
Iteration 76/1000 | Loss: 0.00035895
Iteration 77/1000 | Loss: 0.00008708
Iteration 78/1000 | Loss: 0.00004173
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00007220
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00003342
Iteration 83/1000 | Loss: 0.00002785
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001629
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001615
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001615
Iteration 108/1000 | Loss: 0.00001615
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001614
Iteration 117/1000 | Loss: 0.00001614
Iteration 118/1000 | Loss: 0.00001614
Iteration 119/1000 | Loss: 0.00001614
Iteration 120/1000 | Loss: 0.00001614
Iteration 121/1000 | Loss: 0.00001614
Iteration 122/1000 | Loss: 0.00001614
Iteration 123/1000 | Loss: 0.00001614
Iteration 124/1000 | Loss: 0.00001614
Iteration 125/1000 | Loss: 0.00001614
Iteration 126/1000 | Loss: 0.00001614
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00001614
Iteration 129/1000 | Loss: 0.00001614
Iteration 130/1000 | Loss: 0.00001614
Iteration 131/1000 | Loss: 0.00001614
Iteration 132/1000 | Loss: 0.00001614
Iteration 133/1000 | Loss: 0.00001614
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001614
Iteration 137/1000 | Loss: 0.00001614
Iteration 138/1000 | Loss: 0.00001614
Iteration 139/1000 | Loss: 0.00001614
Iteration 140/1000 | Loss: 0.00001614
Iteration 141/1000 | Loss: 0.00001614
Iteration 142/1000 | Loss: 0.00001614
Iteration 143/1000 | Loss: 0.00001614
Iteration 144/1000 | Loss: 0.00001614
Iteration 145/1000 | Loss: 0.00001614
Iteration 146/1000 | Loss: 0.00001614
Iteration 147/1000 | Loss: 0.00001614
Iteration 148/1000 | Loss: 0.00001614
Iteration 149/1000 | Loss: 0.00001614
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001614
Iteration 154/1000 | Loss: 0.00001614
Iteration 155/1000 | Loss: 0.00001614
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.6136316844495013e-05, 1.6136316844495013e-05, 1.6136316844495013e-05, 1.6136316844495013e-05, 1.6136316844495013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6136316844495013e-05

Optimization complete. Final v2v error: 3.4139046669006348 mm

Highest mean error: 3.842465877532959 mm for frame 10

Lowest mean error: 3.043787717819214 mm for frame 85

Saving results

Total time: 122.29113721847534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832402
Iteration 2/25 | Loss: 0.00114823
Iteration 3/25 | Loss: 0.00099437
Iteration 4/25 | Loss: 0.00097657
Iteration 5/25 | Loss: 0.00097208
Iteration 6/25 | Loss: 0.00097063
Iteration 7/25 | Loss: 0.00097042
Iteration 8/25 | Loss: 0.00097042
Iteration 9/25 | Loss: 0.00097042
Iteration 10/25 | Loss: 0.00097042
Iteration 11/25 | Loss: 0.00097042
Iteration 12/25 | Loss: 0.00097042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009704176336526871, 0.0009704176336526871, 0.0009704176336526871, 0.0009704176336526871, 0.0009704176336526871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009704176336526871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41740656
Iteration 2/25 | Loss: 0.00077928
Iteration 3/25 | Loss: 0.00077928
Iteration 4/25 | Loss: 0.00077928
Iteration 5/25 | Loss: 0.00077928
Iteration 6/25 | Loss: 0.00077928
Iteration 7/25 | Loss: 0.00077928
Iteration 8/25 | Loss: 0.00077928
Iteration 9/25 | Loss: 0.00077928
Iteration 10/25 | Loss: 0.00077928
Iteration 11/25 | Loss: 0.00077928
Iteration 12/25 | Loss: 0.00077928
Iteration 13/25 | Loss: 0.00077928
Iteration 14/25 | Loss: 0.00077928
Iteration 15/25 | Loss: 0.00077928
Iteration 16/25 | Loss: 0.00077928
Iteration 17/25 | Loss: 0.00077928
Iteration 18/25 | Loss: 0.00077928
Iteration 19/25 | Loss: 0.00077928
Iteration 20/25 | Loss: 0.00077928
Iteration 21/25 | Loss: 0.00077928
Iteration 22/25 | Loss: 0.00077928
Iteration 23/25 | Loss: 0.00077928
Iteration 24/25 | Loss: 0.00077928
Iteration 25/25 | Loss: 0.00077928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077928
Iteration 2/1000 | Loss: 0.00001945
Iteration 3/1000 | Loss: 0.00001227
Iteration 4/1000 | Loss: 0.00001086
Iteration 5/1000 | Loss: 0.00001001
Iteration 6/1000 | Loss: 0.00000944
Iteration 7/1000 | Loss: 0.00000907
Iteration 8/1000 | Loss: 0.00000890
Iteration 9/1000 | Loss: 0.00000888
Iteration 10/1000 | Loss: 0.00000880
Iteration 11/1000 | Loss: 0.00000861
Iteration 12/1000 | Loss: 0.00000851
Iteration 13/1000 | Loss: 0.00000847
Iteration 14/1000 | Loss: 0.00000845
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000843
Iteration 17/1000 | Loss: 0.00000840
Iteration 18/1000 | Loss: 0.00000840
Iteration 19/1000 | Loss: 0.00000839
Iteration 20/1000 | Loss: 0.00000838
Iteration 21/1000 | Loss: 0.00000837
Iteration 22/1000 | Loss: 0.00000837
Iteration 23/1000 | Loss: 0.00000835
Iteration 24/1000 | Loss: 0.00000834
Iteration 25/1000 | Loss: 0.00000832
Iteration 26/1000 | Loss: 0.00000832
Iteration 27/1000 | Loss: 0.00000831
Iteration 28/1000 | Loss: 0.00000831
Iteration 29/1000 | Loss: 0.00000830
Iteration 30/1000 | Loss: 0.00000829
Iteration 31/1000 | Loss: 0.00000828
Iteration 32/1000 | Loss: 0.00000828
Iteration 33/1000 | Loss: 0.00000827
Iteration 34/1000 | Loss: 0.00000827
Iteration 35/1000 | Loss: 0.00000827
Iteration 36/1000 | Loss: 0.00000827
Iteration 37/1000 | Loss: 0.00000826
Iteration 38/1000 | Loss: 0.00000826
Iteration 39/1000 | Loss: 0.00000826
Iteration 40/1000 | Loss: 0.00000825
Iteration 41/1000 | Loss: 0.00000825
Iteration 42/1000 | Loss: 0.00000824
Iteration 43/1000 | Loss: 0.00000824
Iteration 44/1000 | Loss: 0.00000824
Iteration 45/1000 | Loss: 0.00000824
Iteration 46/1000 | Loss: 0.00000824
Iteration 47/1000 | Loss: 0.00000824
Iteration 48/1000 | Loss: 0.00000824
Iteration 49/1000 | Loss: 0.00000824
Iteration 50/1000 | Loss: 0.00000824
Iteration 51/1000 | Loss: 0.00000824
Iteration 52/1000 | Loss: 0.00000823
Iteration 53/1000 | Loss: 0.00000823
Iteration 54/1000 | Loss: 0.00000823
Iteration 55/1000 | Loss: 0.00000823
Iteration 56/1000 | Loss: 0.00000822
Iteration 57/1000 | Loss: 0.00000822
Iteration 58/1000 | Loss: 0.00000822
Iteration 59/1000 | Loss: 0.00000822
Iteration 60/1000 | Loss: 0.00000822
Iteration 61/1000 | Loss: 0.00000822
Iteration 62/1000 | Loss: 0.00000821
Iteration 63/1000 | Loss: 0.00000821
Iteration 64/1000 | Loss: 0.00000821
Iteration 65/1000 | Loss: 0.00000821
Iteration 66/1000 | Loss: 0.00000821
Iteration 67/1000 | Loss: 0.00000821
Iteration 68/1000 | Loss: 0.00000821
Iteration 69/1000 | Loss: 0.00000821
Iteration 70/1000 | Loss: 0.00000821
Iteration 71/1000 | Loss: 0.00000821
Iteration 72/1000 | Loss: 0.00000821
Iteration 73/1000 | Loss: 0.00000821
Iteration 74/1000 | Loss: 0.00000820
Iteration 75/1000 | Loss: 0.00000820
Iteration 76/1000 | Loss: 0.00000820
Iteration 77/1000 | Loss: 0.00000820
Iteration 78/1000 | Loss: 0.00000820
Iteration 79/1000 | Loss: 0.00000820
Iteration 80/1000 | Loss: 0.00000820
Iteration 81/1000 | Loss: 0.00000820
Iteration 82/1000 | Loss: 0.00000819
Iteration 83/1000 | Loss: 0.00000819
Iteration 84/1000 | Loss: 0.00000819
Iteration 85/1000 | Loss: 0.00000819
Iteration 86/1000 | Loss: 0.00000819
Iteration 87/1000 | Loss: 0.00000819
Iteration 88/1000 | Loss: 0.00000819
Iteration 89/1000 | Loss: 0.00000819
Iteration 90/1000 | Loss: 0.00000819
Iteration 91/1000 | Loss: 0.00000819
Iteration 92/1000 | Loss: 0.00000819
Iteration 93/1000 | Loss: 0.00000819
Iteration 94/1000 | Loss: 0.00000819
Iteration 95/1000 | Loss: 0.00000818
Iteration 96/1000 | Loss: 0.00000818
Iteration 97/1000 | Loss: 0.00000818
Iteration 98/1000 | Loss: 0.00000818
Iteration 99/1000 | Loss: 0.00000818
Iteration 100/1000 | Loss: 0.00000818
Iteration 101/1000 | Loss: 0.00000818
Iteration 102/1000 | Loss: 0.00000818
Iteration 103/1000 | Loss: 0.00000818
Iteration 104/1000 | Loss: 0.00000818
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000818
Iteration 108/1000 | Loss: 0.00000818
Iteration 109/1000 | Loss: 0.00000817
Iteration 110/1000 | Loss: 0.00000817
Iteration 111/1000 | Loss: 0.00000817
Iteration 112/1000 | Loss: 0.00000816
Iteration 113/1000 | Loss: 0.00000816
Iteration 114/1000 | Loss: 0.00000816
Iteration 115/1000 | Loss: 0.00000815
Iteration 116/1000 | Loss: 0.00000815
Iteration 117/1000 | Loss: 0.00000815
Iteration 118/1000 | Loss: 0.00000815
Iteration 119/1000 | Loss: 0.00000815
Iteration 120/1000 | Loss: 0.00000815
Iteration 121/1000 | Loss: 0.00000814
Iteration 122/1000 | Loss: 0.00000814
Iteration 123/1000 | Loss: 0.00000814
Iteration 124/1000 | Loss: 0.00000814
Iteration 125/1000 | Loss: 0.00000813
Iteration 126/1000 | Loss: 0.00000813
Iteration 127/1000 | Loss: 0.00000813
Iteration 128/1000 | Loss: 0.00000813
Iteration 129/1000 | Loss: 0.00000813
Iteration 130/1000 | Loss: 0.00000812
Iteration 131/1000 | Loss: 0.00000812
Iteration 132/1000 | Loss: 0.00000812
Iteration 133/1000 | Loss: 0.00000812
Iteration 134/1000 | Loss: 0.00000812
Iteration 135/1000 | Loss: 0.00000811
Iteration 136/1000 | Loss: 0.00000811
Iteration 137/1000 | Loss: 0.00000811
Iteration 138/1000 | Loss: 0.00000810
Iteration 139/1000 | Loss: 0.00000810
Iteration 140/1000 | Loss: 0.00000810
Iteration 141/1000 | Loss: 0.00000810
Iteration 142/1000 | Loss: 0.00000810
Iteration 143/1000 | Loss: 0.00000810
Iteration 144/1000 | Loss: 0.00000810
Iteration 145/1000 | Loss: 0.00000810
Iteration 146/1000 | Loss: 0.00000810
Iteration 147/1000 | Loss: 0.00000810
Iteration 148/1000 | Loss: 0.00000809
Iteration 149/1000 | Loss: 0.00000809
Iteration 150/1000 | Loss: 0.00000809
Iteration 151/1000 | Loss: 0.00000809
Iteration 152/1000 | Loss: 0.00000809
Iteration 153/1000 | Loss: 0.00000809
Iteration 154/1000 | Loss: 0.00000809
Iteration 155/1000 | Loss: 0.00000809
Iteration 156/1000 | Loss: 0.00000809
Iteration 157/1000 | Loss: 0.00000809
Iteration 158/1000 | Loss: 0.00000808
Iteration 159/1000 | Loss: 0.00000808
Iteration 160/1000 | Loss: 0.00000808
Iteration 161/1000 | Loss: 0.00000808
Iteration 162/1000 | Loss: 0.00000808
Iteration 163/1000 | Loss: 0.00000808
Iteration 164/1000 | Loss: 0.00000808
Iteration 165/1000 | Loss: 0.00000808
Iteration 166/1000 | Loss: 0.00000808
Iteration 167/1000 | Loss: 0.00000808
Iteration 168/1000 | Loss: 0.00000808
Iteration 169/1000 | Loss: 0.00000808
Iteration 170/1000 | Loss: 0.00000808
Iteration 171/1000 | Loss: 0.00000808
Iteration 172/1000 | Loss: 0.00000808
Iteration 173/1000 | Loss: 0.00000808
Iteration 174/1000 | Loss: 0.00000808
Iteration 175/1000 | Loss: 0.00000808
Iteration 176/1000 | Loss: 0.00000808
Iteration 177/1000 | Loss: 0.00000808
Iteration 178/1000 | Loss: 0.00000808
Iteration 179/1000 | Loss: 0.00000808
Iteration 180/1000 | Loss: 0.00000808
Iteration 181/1000 | Loss: 0.00000808
Iteration 182/1000 | Loss: 0.00000808
Iteration 183/1000 | Loss: 0.00000808
Iteration 184/1000 | Loss: 0.00000808
Iteration 185/1000 | Loss: 0.00000808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [8.077908205450512e-06, 8.077908205450512e-06, 8.077908205450512e-06, 8.077908205450512e-06, 8.077908205450512e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.077908205450512e-06

Optimization complete. Final v2v error: 2.392547369003296 mm

Highest mean error: 3.548058032989502 mm for frame 91

Lowest mean error: 2.1235098838806152 mm for frame 30

Saving results

Total time: 42.55643439292908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064860
Iteration 2/25 | Loss: 0.01064860
Iteration 3/25 | Loss: 0.00541165
Iteration 4/25 | Loss: 0.00394682
Iteration 5/25 | Loss: 0.00264770
Iteration 6/25 | Loss: 0.00242344
Iteration 7/25 | Loss: 0.00227068
Iteration 8/25 | Loss: 0.00224306
Iteration 9/25 | Loss: 0.00216473
Iteration 10/25 | Loss: 0.00186939
Iteration 11/25 | Loss: 0.00173761
Iteration 12/25 | Loss: 0.00168825
Iteration 13/25 | Loss: 0.00157558
Iteration 14/25 | Loss: 0.00148372
Iteration 15/25 | Loss: 0.00144707
Iteration 16/25 | Loss: 0.00140954
Iteration 17/25 | Loss: 0.00139820
Iteration 18/25 | Loss: 0.00139454
Iteration 19/25 | Loss: 0.00138181
Iteration 20/25 | Loss: 0.00137760
Iteration 21/25 | Loss: 0.00137792
Iteration 22/25 | Loss: 0.00137329
Iteration 23/25 | Loss: 0.00137235
Iteration 24/25 | Loss: 0.00137081
Iteration 25/25 | Loss: 0.00137219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67582572
Iteration 2/25 | Loss: 0.00142609
Iteration 3/25 | Loss: 0.00142609
Iteration 4/25 | Loss: 0.00142608
Iteration 5/25 | Loss: 0.00142608
Iteration 6/25 | Loss: 0.00142608
Iteration 7/25 | Loss: 0.00142608
Iteration 8/25 | Loss: 0.00142608
Iteration 9/25 | Loss: 0.00142608
Iteration 10/25 | Loss: 0.00142608
Iteration 11/25 | Loss: 0.00142608
Iteration 12/25 | Loss: 0.00142608
Iteration 13/25 | Loss: 0.00142608
Iteration 14/25 | Loss: 0.00142608
Iteration 15/25 | Loss: 0.00142608
Iteration 16/25 | Loss: 0.00142608
Iteration 17/25 | Loss: 0.00142608
Iteration 18/25 | Loss: 0.00142608
Iteration 19/25 | Loss: 0.00142608
Iteration 20/25 | Loss: 0.00142608
Iteration 21/25 | Loss: 0.00142608
Iteration 22/25 | Loss: 0.00142608
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001426082686521113, 0.001426082686521113, 0.001426082686521113, 0.001426082686521113, 0.001426082686521113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001426082686521113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142608
Iteration 2/1000 | Loss: 0.00081281
Iteration 3/1000 | Loss: 0.00061426
Iteration 4/1000 | Loss: 0.00059455
Iteration 5/1000 | Loss: 0.00041199
Iteration 6/1000 | Loss: 0.00026000
Iteration 7/1000 | Loss: 0.00014677
Iteration 8/1000 | Loss: 0.00013652
Iteration 9/1000 | Loss: 0.00012449
Iteration 10/1000 | Loss: 0.00009264
Iteration 11/1000 | Loss: 0.00137171
Iteration 12/1000 | Loss: 0.00011024
Iteration 13/1000 | Loss: 0.00008962
Iteration 14/1000 | Loss: 0.00009127
Iteration 15/1000 | Loss: 0.00007008
Iteration 16/1000 | Loss: 0.00009362
Iteration 17/1000 | Loss: 0.00008405
Iteration 18/1000 | Loss: 0.00009364
Iteration 19/1000 | Loss: 0.00008011
Iteration 20/1000 | Loss: 0.00006609
Iteration 21/1000 | Loss: 0.00007973
Iteration 22/1000 | Loss: 0.00051948
Iteration 23/1000 | Loss: 0.00008604
Iteration 24/1000 | Loss: 0.00006060
Iteration 25/1000 | Loss: 0.00006540
Iteration 26/1000 | Loss: 0.00005101
Iteration 27/1000 | Loss: 0.00018428
Iteration 28/1000 | Loss: 0.00021279
Iteration 29/1000 | Loss: 0.00004895
Iteration 30/1000 | Loss: 0.00003917
Iteration 31/1000 | Loss: 0.00003633
Iteration 32/1000 | Loss: 0.00003412
Iteration 33/1000 | Loss: 0.00020882
Iteration 34/1000 | Loss: 0.00035144
Iteration 35/1000 | Loss: 0.00004338
Iteration 36/1000 | Loss: 0.00003630
Iteration 37/1000 | Loss: 0.00023243
Iteration 38/1000 | Loss: 0.00003501
Iteration 39/1000 | Loss: 0.00003082
Iteration 40/1000 | Loss: 0.00002897
Iteration 41/1000 | Loss: 0.00002685
Iteration 42/1000 | Loss: 0.00002598
Iteration 43/1000 | Loss: 0.00002535
Iteration 44/1000 | Loss: 0.00002488
Iteration 45/1000 | Loss: 0.00002450
Iteration 46/1000 | Loss: 0.00024558
Iteration 47/1000 | Loss: 0.00003164
Iteration 48/1000 | Loss: 0.00002751
Iteration 49/1000 | Loss: 0.00002583
Iteration 50/1000 | Loss: 0.00002451
Iteration 51/1000 | Loss: 0.00002369
Iteration 52/1000 | Loss: 0.00002312
Iteration 53/1000 | Loss: 0.00002284
Iteration 54/1000 | Loss: 0.00002279
Iteration 55/1000 | Loss: 0.00002279
Iteration 56/1000 | Loss: 0.00002278
Iteration 57/1000 | Loss: 0.00002278
Iteration 58/1000 | Loss: 0.00002277
Iteration 59/1000 | Loss: 0.00002277
Iteration 60/1000 | Loss: 0.00002277
Iteration 61/1000 | Loss: 0.00002277
Iteration 62/1000 | Loss: 0.00002276
Iteration 63/1000 | Loss: 0.00002276
Iteration 64/1000 | Loss: 0.00002276
Iteration 65/1000 | Loss: 0.00002276
Iteration 66/1000 | Loss: 0.00002276
Iteration 67/1000 | Loss: 0.00002276
Iteration 68/1000 | Loss: 0.00002276
Iteration 69/1000 | Loss: 0.00002276
Iteration 70/1000 | Loss: 0.00002275
Iteration 71/1000 | Loss: 0.00002275
Iteration 72/1000 | Loss: 0.00002275
Iteration 73/1000 | Loss: 0.00002275
Iteration 74/1000 | Loss: 0.00002275
Iteration 75/1000 | Loss: 0.00002275
Iteration 76/1000 | Loss: 0.00002274
Iteration 77/1000 | Loss: 0.00002274
Iteration 78/1000 | Loss: 0.00002274
Iteration 79/1000 | Loss: 0.00002273
Iteration 80/1000 | Loss: 0.00002273
Iteration 81/1000 | Loss: 0.00002273
Iteration 82/1000 | Loss: 0.00002271
Iteration 83/1000 | Loss: 0.00002269
Iteration 84/1000 | Loss: 0.00002269
Iteration 85/1000 | Loss: 0.00002269
Iteration 86/1000 | Loss: 0.00002268
Iteration 87/1000 | Loss: 0.00002268
Iteration 88/1000 | Loss: 0.00002268
Iteration 89/1000 | Loss: 0.00002268
Iteration 90/1000 | Loss: 0.00002268
Iteration 91/1000 | Loss: 0.00002267
Iteration 92/1000 | Loss: 0.00002267
Iteration 93/1000 | Loss: 0.00002267
Iteration 94/1000 | Loss: 0.00002267
Iteration 95/1000 | Loss: 0.00002267
Iteration 96/1000 | Loss: 0.00002267
Iteration 97/1000 | Loss: 0.00002267
Iteration 98/1000 | Loss: 0.00002267
Iteration 99/1000 | Loss: 0.00002267
Iteration 100/1000 | Loss: 0.00002267
Iteration 101/1000 | Loss: 0.00002267
Iteration 102/1000 | Loss: 0.00002267
Iteration 103/1000 | Loss: 0.00002267
Iteration 104/1000 | Loss: 0.00002266
Iteration 105/1000 | Loss: 0.00002266
Iteration 106/1000 | Loss: 0.00002266
Iteration 107/1000 | Loss: 0.00002266
Iteration 108/1000 | Loss: 0.00002266
Iteration 109/1000 | Loss: 0.00002266
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002265
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002265
Iteration 115/1000 | Loss: 0.00002265
Iteration 116/1000 | Loss: 0.00002265
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002265
Iteration 120/1000 | Loss: 0.00002264
Iteration 121/1000 | Loss: 0.00002264
Iteration 122/1000 | Loss: 0.00002264
Iteration 123/1000 | Loss: 0.00002264
Iteration 124/1000 | Loss: 0.00002264
Iteration 125/1000 | Loss: 0.00002264
Iteration 126/1000 | Loss: 0.00002264
Iteration 127/1000 | Loss: 0.00002264
Iteration 128/1000 | Loss: 0.00002264
Iteration 129/1000 | Loss: 0.00002264
Iteration 130/1000 | Loss: 0.00002264
Iteration 131/1000 | Loss: 0.00002264
Iteration 132/1000 | Loss: 0.00002264
Iteration 133/1000 | Loss: 0.00002264
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002263
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002263
Iteration 138/1000 | Loss: 0.00002263
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00002263
Iteration 141/1000 | Loss: 0.00002263
Iteration 142/1000 | Loss: 0.00002263
Iteration 143/1000 | Loss: 0.00002263
Iteration 144/1000 | Loss: 0.00002263
Iteration 145/1000 | Loss: 0.00002263
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002263
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002263
Iteration 150/1000 | Loss: 0.00002263
Iteration 151/1000 | Loss: 0.00002263
Iteration 152/1000 | Loss: 0.00002263
Iteration 153/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.262987254653126e-05, 2.262987254653126e-05, 2.262987254653126e-05, 2.262987254653126e-05, 2.262987254653126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.262987254653126e-05

Optimization complete. Final v2v error: 3.7962334156036377 mm

Highest mean error: 10.611379623413086 mm for frame 232

Lowest mean error: 3.553813934326172 mm for frame 69

Saving results

Total time: 146.29687476158142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072411
Iteration 2/25 | Loss: 0.00471869
Iteration 3/25 | Loss: 0.00337167
Iteration 4/25 | Loss: 0.00312188
Iteration 5/25 | Loss: 0.00259376
Iteration 6/25 | Loss: 0.00253865
Iteration 7/25 | Loss: 0.00215742
Iteration 8/25 | Loss: 0.00202394
Iteration 9/25 | Loss: 0.00202923
Iteration 10/25 | Loss: 0.00204220
Iteration 11/25 | Loss: 0.00190417
Iteration 12/25 | Loss: 0.00178793
Iteration 13/25 | Loss: 0.00180599
Iteration 14/25 | Loss: 0.00181216
Iteration 15/25 | Loss: 0.00175173
Iteration 16/25 | Loss: 0.00170823
Iteration 17/25 | Loss: 0.00170416
Iteration 18/25 | Loss: 0.00169510
Iteration 19/25 | Loss: 0.00170234
Iteration 20/25 | Loss: 0.00168481
Iteration 21/25 | Loss: 0.00167628
Iteration 22/25 | Loss: 0.00167416
Iteration 23/25 | Loss: 0.00167070
Iteration 24/25 | Loss: 0.00167177
Iteration 25/25 | Loss: 0.00166120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40156406
Iteration 2/25 | Loss: 0.00182200
Iteration 3/25 | Loss: 0.00182200
Iteration 4/25 | Loss: 0.00182199
Iteration 5/25 | Loss: 0.00182199
Iteration 6/25 | Loss: 0.00182199
Iteration 7/25 | Loss: 0.00182199
Iteration 8/25 | Loss: 0.00182199
Iteration 9/25 | Loss: 0.00182199
Iteration 10/25 | Loss: 0.00182199
Iteration 11/25 | Loss: 0.00182199
Iteration 12/25 | Loss: 0.00182199
Iteration 13/25 | Loss: 0.00182199
Iteration 14/25 | Loss: 0.00182199
Iteration 15/25 | Loss: 0.00182199
Iteration 16/25 | Loss: 0.00182199
Iteration 17/25 | Loss: 0.00182199
Iteration 18/25 | Loss: 0.00182199
Iteration 19/25 | Loss: 0.00182199
Iteration 20/25 | Loss: 0.00182199
Iteration 21/25 | Loss: 0.00182199
Iteration 22/25 | Loss: 0.00182199
Iteration 23/25 | Loss: 0.00182199
Iteration 24/25 | Loss: 0.00182199
Iteration 25/25 | Loss: 0.00182199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182199
Iteration 2/1000 | Loss: 0.00067007
Iteration 3/1000 | Loss: 0.00098976
Iteration 4/1000 | Loss: 0.00053443
Iteration 5/1000 | Loss: 0.00040231
Iteration 6/1000 | Loss: 0.00072321
Iteration 7/1000 | Loss: 0.00042105
Iteration 8/1000 | Loss: 0.00051512
Iteration 9/1000 | Loss: 0.00055208
Iteration 10/1000 | Loss: 0.00057401
Iteration 11/1000 | Loss: 0.00066028
Iteration 12/1000 | Loss: 0.00106199
Iteration 13/1000 | Loss: 0.00137441
Iteration 14/1000 | Loss: 0.00059934
Iteration 15/1000 | Loss: 0.00046144
Iteration 16/1000 | Loss: 0.00026584
Iteration 17/1000 | Loss: 0.00043877
Iteration 18/1000 | Loss: 0.00040297
Iteration 19/1000 | Loss: 0.00039888
Iteration 20/1000 | Loss: 0.00054172
Iteration 21/1000 | Loss: 0.00050101
Iteration 22/1000 | Loss: 0.00044864
Iteration 23/1000 | Loss: 0.00054658
Iteration 24/1000 | Loss: 0.00052603
Iteration 25/1000 | Loss: 0.00057599
Iteration 26/1000 | Loss: 0.00064949
Iteration 27/1000 | Loss: 0.00072164
Iteration 28/1000 | Loss: 0.00064260
Iteration 29/1000 | Loss: 0.00059918
Iteration 30/1000 | Loss: 0.00068542
Iteration 31/1000 | Loss: 0.00066639
Iteration 32/1000 | Loss: 0.00072130
Iteration 33/1000 | Loss: 0.00070574
Iteration 34/1000 | Loss: 0.00088004
Iteration 35/1000 | Loss: 0.00066669
Iteration 36/1000 | Loss: 0.00067450
Iteration 37/1000 | Loss: 0.00068991
Iteration 38/1000 | Loss: 0.00072171
Iteration 39/1000 | Loss: 0.00064376
Iteration 40/1000 | Loss: 0.00065504
Iteration 41/1000 | Loss: 0.00058571
Iteration 42/1000 | Loss: 0.00054971
Iteration 43/1000 | Loss: 0.00051942
Iteration 44/1000 | Loss: 0.00051558
Iteration 45/1000 | Loss: 0.00051462
Iteration 46/1000 | Loss: 0.00060737
Iteration 47/1000 | Loss: 0.00051689
Iteration 48/1000 | Loss: 0.00052408
Iteration 49/1000 | Loss: 0.00121707
Iteration 50/1000 | Loss: 0.00041208
Iteration 51/1000 | Loss: 0.00033864
Iteration 52/1000 | Loss: 0.00040761
Iteration 53/1000 | Loss: 0.00045563
Iteration 54/1000 | Loss: 0.00044736
Iteration 55/1000 | Loss: 0.00047884
Iteration 56/1000 | Loss: 0.00051040
Iteration 57/1000 | Loss: 0.00061581
Iteration 58/1000 | Loss: 0.00051485
Iteration 59/1000 | Loss: 0.00160789
Iteration 60/1000 | Loss: 0.00090089
Iteration 61/1000 | Loss: 0.00048580
Iteration 62/1000 | Loss: 0.00058476
Iteration 63/1000 | Loss: 0.00093111
Iteration 64/1000 | Loss: 0.00074079
Iteration 65/1000 | Loss: 0.00049395
Iteration 66/1000 | Loss: 0.00053309
Iteration 67/1000 | Loss: 0.00068301
Iteration 68/1000 | Loss: 0.00059428
Iteration 69/1000 | Loss: 0.00057020
Iteration 70/1000 | Loss: 0.00073056
Iteration 71/1000 | Loss: 0.00055913
Iteration 72/1000 | Loss: 0.00045978
Iteration 73/1000 | Loss: 0.00071659
Iteration 74/1000 | Loss: 0.00065751
Iteration 75/1000 | Loss: 0.00056547
Iteration 76/1000 | Loss: 0.00055979
Iteration 77/1000 | Loss: 0.00047290
Iteration 78/1000 | Loss: 0.00042786
Iteration 79/1000 | Loss: 0.00047219
Iteration 80/1000 | Loss: 0.00046689
Iteration 81/1000 | Loss: 0.00045721
Iteration 82/1000 | Loss: 0.00044680
Iteration 83/1000 | Loss: 0.00043546
Iteration 84/1000 | Loss: 0.00046952
Iteration 85/1000 | Loss: 0.00047202
Iteration 86/1000 | Loss: 0.00043802
Iteration 87/1000 | Loss: 0.00045858
Iteration 88/1000 | Loss: 0.00040449
Iteration 89/1000 | Loss: 0.00093330
Iteration 90/1000 | Loss: 0.00111306
Iteration 91/1000 | Loss: 0.00103003
Iteration 92/1000 | Loss: 0.00112419
Iteration 93/1000 | Loss: 0.00096570
Iteration 94/1000 | Loss: 0.00125123
Iteration 95/1000 | Loss: 0.00237737
Iteration 96/1000 | Loss: 0.00086132
Iteration 97/1000 | Loss: 0.00059899
Iteration 98/1000 | Loss: 0.00054995
Iteration 99/1000 | Loss: 0.00075204
Iteration 100/1000 | Loss: 0.00064735
Iteration 101/1000 | Loss: 0.00064823
Iteration 102/1000 | Loss: 0.00061042
Iteration 103/1000 | Loss: 0.00050962
Iteration 104/1000 | Loss: 0.00054632
Iteration 105/1000 | Loss: 0.00055915
Iteration 106/1000 | Loss: 0.00057301
Iteration 107/1000 | Loss: 0.00057405
Iteration 108/1000 | Loss: 0.00048000
Iteration 109/1000 | Loss: 0.00047351
Iteration 110/1000 | Loss: 0.00051284
Iteration 111/1000 | Loss: 0.00054507
Iteration 112/1000 | Loss: 0.00048920
Iteration 113/1000 | Loss: 0.00069576
Iteration 114/1000 | Loss: 0.00052283
Iteration 115/1000 | Loss: 0.00063593
Iteration 116/1000 | Loss: 0.00064631
Iteration 117/1000 | Loss: 0.00056584
Iteration 118/1000 | Loss: 0.00051567
Iteration 119/1000 | Loss: 0.00043771
Iteration 120/1000 | Loss: 0.00044885
Iteration 121/1000 | Loss: 0.00050323
Iteration 122/1000 | Loss: 0.00051724
Iteration 123/1000 | Loss: 0.00075092
Iteration 124/1000 | Loss: 0.00060175
Iteration 125/1000 | Loss: 0.00040754
Iteration 126/1000 | Loss: 0.00065338
Iteration 127/1000 | Loss: 0.00071332
Iteration 128/1000 | Loss: 0.00071670
Iteration 129/1000 | Loss: 0.00060522
Iteration 130/1000 | Loss: 0.00049097
Iteration 131/1000 | Loss: 0.00057300
Iteration 132/1000 | Loss: 0.00056989
Iteration 133/1000 | Loss: 0.00080634
Iteration 134/1000 | Loss: 0.00054966
Iteration 135/1000 | Loss: 0.00113011
Iteration 136/1000 | Loss: 0.00078961
Iteration 137/1000 | Loss: 0.00082043
Iteration 138/1000 | Loss: 0.00062384
Iteration 139/1000 | Loss: 0.00061446
Iteration 140/1000 | Loss: 0.00091942
Iteration 141/1000 | Loss: 0.00076956
Iteration 142/1000 | Loss: 0.00052563
Iteration 143/1000 | Loss: 0.00151318
Iteration 144/1000 | Loss: 0.00054056
Iteration 145/1000 | Loss: 0.00051981
Iteration 146/1000 | Loss: 0.00060920
Iteration 147/1000 | Loss: 0.00054585
Iteration 148/1000 | Loss: 0.00049191
Iteration 149/1000 | Loss: 0.00054785
Iteration 150/1000 | Loss: 0.00046742
Iteration 151/1000 | Loss: 0.00043977
Iteration 152/1000 | Loss: 0.00051134
Iteration 153/1000 | Loss: 0.00050196
Iteration 154/1000 | Loss: 0.00035964
Iteration 155/1000 | Loss: 0.00055487
Iteration 156/1000 | Loss: 0.00056270
Iteration 157/1000 | Loss: 0.00049267
Iteration 158/1000 | Loss: 0.00051261
Iteration 159/1000 | Loss: 0.00047235
Iteration 160/1000 | Loss: 0.00050080
Iteration 161/1000 | Loss: 0.00051291
Iteration 162/1000 | Loss: 0.00059167
Iteration 163/1000 | Loss: 0.00050307
Iteration 164/1000 | Loss: 0.00062185
Iteration 165/1000 | Loss: 0.00055735
Iteration 166/1000 | Loss: 0.00057965
Iteration 167/1000 | Loss: 0.00051014
Iteration 168/1000 | Loss: 0.00048193
Iteration 169/1000 | Loss: 0.00056339
Iteration 170/1000 | Loss: 0.00057006
Iteration 171/1000 | Loss: 0.00026580
Iteration 172/1000 | Loss: 0.00037259
Iteration 173/1000 | Loss: 0.00053635
Iteration 174/1000 | Loss: 0.00061426
Iteration 175/1000 | Loss: 0.00037017
Iteration 176/1000 | Loss: 0.00030445
Iteration 177/1000 | Loss: 0.00036729
Iteration 178/1000 | Loss: 0.00052430
Iteration 179/1000 | Loss: 0.00102206
Iteration 180/1000 | Loss: 0.00058632
Iteration 181/1000 | Loss: 0.00099049
Iteration 182/1000 | Loss: 0.00068294
Iteration 183/1000 | Loss: 0.00075923
Iteration 184/1000 | Loss: 0.00048392
Iteration 185/1000 | Loss: 0.00046288
Iteration 186/1000 | Loss: 0.00050602
Iteration 187/1000 | Loss: 0.00064620
Iteration 188/1000 | Loss: 0.00061361
Iteration 189/1000 | Loss: 0.00095122
Iteration 190/1000 | Loss: 0.00060326
Iteration 191/1000 | Loss: 0.00083653
Iteration 192/1000 | Loss: 0.00059508
Iteration 193/1000 | Loss: 0.00055041
Iteration 194/1000 | Loss: 0.00085787
Iteration 195/1000 | Loss: 0.00094016
Iteration 196/1000 | Loss: 0.00052619
Iteration 197/1000 | Loss: 0.00058187
Iteration 198/1000 | Loss: 0.00078847
Iteration 199/1000 | Loss: 0.00087047
Iteration 200/1000 | Loss: 0.00051453
Iteration 201/1000 | Loss: 0.00049064
Iteration 202/1000 | Loss: 0.00044308
Iteration 203/1000 | Loss: 0.00093581
Iteration 204/1000 | Loss: 0.00057003
Iteration 205/1000 | Loss: 0.00084802
Iteration 206/1000 | Loss: 0.00062174
Iteration 207/1000 | Loss: 0.00078675
Iteration 208/1000 | Loss: 0.00066716
Iteration 209/1000 | Loss: 0.00052030
Iteration 210/1000 | Loss: 0.00052844
Iteration 211/1000 | Loss: 0.00051593
Iteration 212/1000 | Loss: 0.00061327
Iteration 213/1000 | Loss: 0.00049369
Iteration 214/1000 | Loss: 0.00061762
Iteration 215/1000 | Loss: 0.00038961
Iteration 216/1000 | Loss: 0.00066688
Iteration 217/1000 | Loss: 0.00050138
Iteration 218/1000 | Loss: 0.00046866
Iteration 219/1000 | Loss: 0.00040177
Iteration 220/1000 | Loss: 0.00049394
Iteration 221/1000 | Loss: 0.00059341
Iteration 222/1000 | Loss: 0.00041893
Iteration 223/1000 | Loss: 0.00064497
Iteration 224/1000 | Loss: 0.00060443
Iteration 225/1000 | Loss: 0.00056219
Iteration 226/1000 | Loss: 0.00057833
Iteration 227/1000 | Loss: 0.00164774
Iteration 228/1000 | Loss: 0.00099556
Iteration 229/1000 | Loss: 0.00071175
Iteration 230/1000 | Loss: 0.00054592
Iteration 231/1000 | Loss: 0.00090163
Iteration 232/1000 | Loss: 0.00078624
Iteration 233/1000 | Loss: 0.00067601
Iteration 234/1000 | Loss: 0.00070791
Iteration 235/1000 | Loss: 0.00039754
Iteration 236/1000 | Loss: 0.00062046
Iteration 237/1000 | Loss: 0.00033957
Iteration 238/1000 | Loss: 0.00028457
Iteration 239/1000 | Loss: 0.00054720
Iteration 240/1000 | Loss: 0.00050216
Iteration 241/1000 | Loss: 0.00025416
Iteration 242/1000 | Loss: 0.00187076
Iteration 243/1000 | Loss: 0.00048753
Iteration 244/1000 | Loss: 0.00093622
Iteration 245/1000 | Loss: 0.00064790
Iteration 246/1000 | Loss: 0.00066798
Iteration 247/1000 | Loss: 0.00014872
Iteration 248/1000 | Loss: 0.00009946
Iteration 249/1000 | Loss: 0.00009459
Iteration 250/1000 | Loss: 0.00009143
Iteration 251/1000 | Loss: 0.00036587
Iteration 252/1000 | Loss: 0.00009808
Iteration 253/1000 | Loss: 0.00015388
Iteration 254/1000 | Loss: 0.00008804
Iteration 255/1000 | Loss: 0.00008642
Iteration 256/1000 | Loss: 0.00008505
Iteration 257/1000 | Loss: 0.00008355
Iteration 258/1000 | Loss: 0.00017538
Iteration 259/1000 | Loss: 0.00025922
Iteration 260/1000 | Loss: 0.00023102
Iteration 261/1000 | Loss: 0.00017453
Iteration 262/1000 | Loss: 0.00016104
Iteration 263/1000 | Loss: 0.00009592
Iteration 264/1000 | Loss: 0.00008754
Iteration 265/1000 | Loss: 0.00008349
Iteration 266/1000 | Loss: 0.00019114
Iteration 267/1000 | Loss: 0.00024098
Iteration 268/1000 | Loss: 0.00018653
Iteration 269/1000 | Loss: 0.00035128
Iteration 270/1000 | Loss: 0.00024620
Iteration 271/1000 | Loss: 0.00026115
Iteration 272/1000 | Loss: 0.00029769
Iteration 273/1000 | Loss: 0.00023985
Iteration 274/1000 | Loss: 0.00031323
Iteration 275/1000 | Loss: 0.00033129
Iteration 276/1000 | Loss: 0.00027124
Iteration 277/1000 | Loss: 0.00040184
Iteration 278/1000 | Loss: 0.00011757
Iteration 279/1000 | Loss: 0.00022784
Iteration 280/1000 | Loss: 0.00016813
Iteration 281/1000 | Loss: 0.00033033
Iteration 282/1000 | Loss: 0.00021022
Iteration 283/1000 | Loss: 0.00021157
Iteration 284/1000 | Loss: 0.00020571
Iteration 285/1000 | Loss: 0.00022741
Iteration 286/1000 | Loss: 0.00036845
Iteration 287/1000 | Loss: 0.00030703
Iteration 288/1000 | Loss: 0.00013684
Iteration 289/1000 | Loss: 0.00019275
Iteration 290/1000 | Loss: 0.00017078
Iteration 291/1000 | Loss: 0.00008707
Iteration 292/1000 | Loss: 0.00009631
Iteration 293/1000 | Loss: 0.00009562
Iteration 294/1000 | Loss: 0.00009387
Iteration 295/1000 | Loss: 0.00009457
Iteration 296/1000 | Loss: 0.00008468
Iteration 297/1000 | Loss: 0.00009061
Iteration 298/1000 | Loss: 0.00008240
Iteration 299/1000 | Loss: 0.00008159
Iteration 300/1000 | Loss: 0.00008091
Iteration 301/1000 | Loss: 0.00022190
Iteration 302/1000 | Loss: 0.00010196
Iteration 303/1000 | Loss: 0.00016637
Iteration 304/1000 | Loss: 0.00016372
Iteration 305/1000 | Loss: 0.00014325
Iteration 306/1000 | Loss: 0.00008554
Iteration 307/1000 | Loss: 0.00008213
Iteration 308/1000 | Loss: 0.00008029
Iteration 309/1000 | Loss: 0.00007968
Iteration 310/1000 | Loss: 0.00007927
Iteration 311/1000 | Loss: 0.00007901
Iteration 312/1000 | Loss: 0.00007879
Iteration 313/1000 | Loss: 0.00007857
Iteration 314/1000 | Loss: 0.00007838
Iteration 315/1000 | Loss: 0.00007824
Iteration 316/1000 | Loss: 0.00007815
Iteration 317/1000 | Loss: 0.00007797
Iteration 318/1000 | Loss: 0.00007792
Iteration 319/1000 | Loss: 0.00007781
Iteration 320/1000 | Loss: 0.00007775
Iteration 321/1000 | Loss: 0.00007768
Iteration 322/1000 | Loss: 0.00007767
Iteration 323/1000 | Loss: 0.00007766
Iteration 324/1000 | Loss: 0.00007766
Iteration 325/1000 | Loss: 0.00008029
Iteration 326/1000 | Loss: 0.00007764
Iteration 327/1000 | Loss: 0.00007746
Iteration 328/1000 | Loss: 0.00007722
Iteration 329/1000 | Loss: 0.00007713
Iteration 330/1000 | Loss: 0.00007713
Iteration 331/1000 | Loss: 0.00007712
Iteration 332/1000 | Loss: 0.00007712
Iteration 333/1000 | Loss: 0.00007712
Iteration 334/1000 | Loss: 0.00007711
Iteration 335/1000 | Loss: 0.00007710
Iteration 336/1000 | Loss: 0.00007710
Iteration 337/1000 | Loss: 0.00007709
Iteration 338/1000 | Loss: 0.00007709
Iteration 339/1000 | Loss: 0.00007709
Iteration 340/1000 | Loss: 0.00007709
Iteration 341/1000 | Loss: 0.00007709
Iteration 342/1000 | Loss: 0.00007709
Iteration 343/1000 | Loss: 0.00007708
Iteration 344/1000 | Loss: 0.00007708
Iteration 345/1000 | Loss: 0.00007708
Iteration 346/1000 | Loss: 0.00007704
Iteration 347/1000 | Loss: 0.00007701
Iteration 348/1000 | Loss: 0.00007700
Iteration 349/1000 | Loss: 0.00007699
Iteration 350/1000 | Loss: 0.00007698
Iteration 351/1000 | Loss: 0.00007692
Iteration 352/1000 | Loss: 0.00007688
Iteration 353/1000 | Loss: 0.00007688
Iteration 354/1000 | Loss: 0.00007688
Iteration 355/1000 | Loss: 0.00007688
Iteration 356/1000 | Loss: 0.00007688
Iteration 357/1000 | Loss: 0.00007688
Iteration 358/1000 | Loss: 0.00007688
Iteration 359/1000 | Loss: 0.00007687
Iteration 360/1000 | Loss: 0.00007687
Iteration 361/1000 | Loss: 0.00007687
Iteration 362/1000 | Loss: 0.00007687
Iteration 363/1000 | Loss: 0.00007687
Iteration 364/1000 | Loss: 0.00007687
Iteration 365/1000 | Loss: 0.00007687
Iteration 366/1000 | Loss: 0.00007687
Iteration 367/1000 | Loss: 0.00007687
Iteration 368/1000 | Loss: 0.00007686
Iteration 369/1000 | Loss: 0.00007686
Iteration 370/1000 | Loss: 0.00007686
Iteration 371/1000 | Loss: 0.00007685
Iteration 372/1000 | Loss: 0.00007685
Iteration 373/1000 | Loss: 0.00007685
Iteration 374/1000 | Loss: 0.00007685
Iteration 375/1000 | Loss: 0.00007685
Iteration 376/1000 | Loss: 0.00007685
Iteration 377/1000 | Loss: 0.00007685
Iteration 378/1000 | Loss: 0.00007685
Iteration 379/1000 | Loss: 0.00007685
Iteration 380/1000 | Loss: 0.00007685
Iteration 381/1000 | Loss: 0.00007685
Iteration 382/1000 | Loss: 0.00007685
Iteration 383/1000 | Loss: 0.00007685
Iteration 384/1000 | Loss: 0.00007685
Iteration 385/1000 | Loss: 0.00007685
Iteration 386/1000 | Loss: 0.00007685
Iteration 387/1000 | Loss: 0.00007684
Iteration 388/1000 | Loss: 0.00007684
Iteration 389/1000 | Loss: 0.00007684
Iteration 390/1000 | Loss: 0.00007684
Iteration 391/1000 | Loss: 0.00007684
Iteration 392/1000 | Loss: 0.00007684
Iteration 393/1000 | Loss: 0.00007684
Iteration 394/1000 | Loss: 0.00007684
Iteration 395/1000 | Loss: 0.00007684
Iteration 396/1000 | Loss: 0.00007684
Iteration 397/1000 | Loss: 0.00007683
Iteration 398/1000 | Loss: 0.00007683
Iteration 399/1000 | Loss: 0.00007683
Iteration 400/1000 | Loss: 0.00007683
Iteration 401/1000 | Loss: 0.00007683
Iteration 402/1000 | Loss: 0.00007683
Iteration 403/1000 | Loss: 0.00007683
Iteration 404/1000 | Loss: 0.00007683
Iteration 405/1000 | Loss: 0.00007683
Iteration 406/1000 | Loss: 0.00007683
Iteration 407/1000 | Loss: 0.00007683
Iteration 408/1000 | Loss: 0.00007683
Iteration 409/1000 | Loss: 0.00007683
Iteration 410/1000 | Loss: 0.00007682
Iteration 411/1000 | Loss: 0.00007682
Iteration 412/1000 | Loss: 0.00007682
Iteration 413/1000 | Loss: 0.00007682
Iteration 414/1000 | Loss: 0.00007682
Iteration 415/1000 | Loss: 0.00007682
Iteration 416/1000 | Loss: 0.00007682
Iteration 417/1000 | Loss: 0.00007682
Iteration 418/1000 | Loss: 0.00007682
Iteration 419/1000 | Loss: 0.00007682
Iteration 420/1000 | Loss: 0.00007682
Iteration 421/1000 | Loss: 0.00007682
Iteration 422/1000 | Loss: 0.00007682
Iteration 423/1000 | Loss: 0.00007681
Iteration 424/1000 | Loss: 0.00007681
Iteration 425/1000 | Loss: 0.00007681
Iteration 426/1000 | Loss: 0.00007681
Iteration 427/1000 | Loss: 0.00007681
Iteration 428/1000 | Loss: 0.00007681
Iteration 429/1000 | Loss: 0.00007681
Iteration 430/1000 | Loss: 0.00007681
Iteration 431/1000 | Loss: 0.00007681
Iteration 432/1000 | Loss: 0.00007681
Iteration 433/1000 | Loss: 0.00007681
Iteration 434/1000 | Loss: 0.00007681
Iteration 435/1000 | Loss: 0.00007681
Iteration 436/1000 | Loss: 0.00007681
Iteration 437/1000 | Loss: 0.00007681
Iteration 438/1000 | Loss: 0.00007681
Iteration 439/1000 | Loss: 0.00007681
Iteration 440/1000 | Loss: 0.00007681
Iteration 441/1000 | Loss: 0.00007681
Iteration 442/1000 | Loss: 0.00007681
Iteration 443/1000 | Loss: 0.00007681
Iteration 444/1000 | Loss: 0.00007681
Iteration 445/1000 | Loss: 0.00007681
Iteration 446/1000 | Loss: 0.00007681
Iteration 447/1000 | Loss: 0.00007681
Iteration 448/1000 | Loss: 0.00007681
Iteration 449/1000 | Loss: 0.00007681
Iteration 450/1000 | Loss: 0.00007681
Iteration 451/1000 | Loss: 0.00007681
Iteration 452/1000 | Loss: 0.00007681
Iteration 453/1000 | Loss: 0.00007681
Iteration 454/1000 | Loss: 0.00007681
Iteration 455/1000 | Loss: 0.00007681
Iteration 456/1000 | Loss: 0.00007681
Iteration 457/1000 | Loss: 0.00007681
Iteration 458/1000 | Loss: 0.00007681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 458. Stopping optimization.
Last 5 losses: [7.680628186790273e-05, 7.680628186790273e-05, 7.680628186790273e-05, 7.680628186790273e-05, 7.680628186790273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.680628186790273e-05

Optimization complete. Final v2v error: 6.3525214195251465 mm

Highest mean error: 12.220264434814453 mm for frame 149

Lowest mean error: 4.419089317321777 mm for frame 9

Saving results

Total time: 591.4274168014526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_028/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_028/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524015
Iteration 2/25 | Loss: 0.00124422
Iteration 3/25 | Loss: 0.00108329
Iteration 4/25 | Loss: 0.00103886
Iteration 5/25 | Loss: 0.00102079
Iteration 6/25 | Loss: 0.00103156
Iteration 7/25 | Loss: 0.00103359
Iteration 8/25 | Loss: 0.00101472
Iteration 9/25 | Loss: 0.00100020
Iteration 10/25 | Loss: 0.00099147
Iteration 11/25 | Loss: 0.00099556
Iteration 12/25 | Loss: 0.00099214
Iteration 13/25 | Loss: 0.00099375
Iteration 14/25 | Loss: 0.00098884
Iteration 15/25 | Loss: 0.00098754
Iteration 16/25 | Loss: 0.00098848
Iteration 17/25 | Loss: 0.00098843
Iteration 18/25 | Loss: 0.00098654
Iteration 19/25 | Loss: 0.00098511
Iteration 20/25 | Loss: 0.00098454
Iteration 21/25 | Loss: 0.00098425
Iteration 22/25 | Loss: 0.00098423
Iteration 23/25 | Loss: 0.00098422
Iteration 24/25 | Loss: 0.00098422
Iteration 25/25 | Loss: 0.00098422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43354380
Iteration 2/25 | Loss: 0.00090739
Iteration 3/25 | Loss: 0.00090738
Iteration 4/25 | Loss: 0.00090738
Iteration 5/25 | Loss: 0.00090738
Iteration 6/25 | Loss: 0.00090738
Iteration 7/25 | Loss: 0.00090738
Iteration 8/25 | Loss: 0.00090738
Iteration 9/25 | Loss: 0.00090738
Iteration 10/25 | Loss: 0.00090738
Iteration 11/25 | Loss: 0.00090738
Iteration 12/25 | Loss: 0.00090738
Iteration 13/25 | Loss: 0.00090738
Iteration 14/25 | Loss: 0.00090738
Iteration 15/25 | Loss: 0.00090738
Iteration 16/25 | Loss: 0.00090738
Iteration 17/25 | Loss: 0.00090738
Iteration 18/25 | Loss: 0.00090738
Iteration 19/25 | Loss: 0.00090738
Iteration 20/25 | Loss: 0.00090738
Iteration 21/25 | Loss: 0.00090738
Iteration 22/25 | Loss: 0.00090738
Iteration 23/25 | Loss: 0.00090738
Iteration 24/25 | Loss: 0.00090738
Iteration 25/25 | Loss: 0.00090738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090738
Iteration 2/1000 | Loss: 0.00002893
Iteration 3/1000 | Loss: 0.00009391
Iteration 4/1000 | Loss: 0.00009947
Iteration 5/1000 | Loss: 0.00003866
Iteration 6/1000 | Loss: 0.00015370
Iteration 7/1000 | Loss: 0.00013927
Iteration 8/1000 | Loss: 0.00011961
Iteration 9/1000 | Loss: 0.00019494
Iteration 10/1000 | Loss: 0.00011322
Iteration 11/1000 | Loss: 0.00016546
Iteration 12/1000 | Loss: 0.00015100
Iteration 13/1000 | Loss: 0.00002852
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001427
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000957
Iteration 23/1000 | Loss: 0.00000952
Iteration 24/1000 | Loss: 0.00000943
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000931
Iteration 29/1000 | Loss: 0.00000914
Iteration 30/1000 | Loss: 0.00000910
Iteration 31/1000 | Loss: 0.00000905
Iteration 32/1000 | Loss: 0.00000904
Iteration 33/1000 | Loss: 0.00000904
Iteration 34/1000 | Loss: 0.00000904
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000904
Iteration 37/1000 | Loss: 0.00000904
Iteration 38/1000 | Loss: 0.00000904
Iteration 39/1000 | Loss: 0.00000904
Iteration 40/1000 | Loss: 0.00000904
Iteration 41/1000 | Loss: 0.00000904
Iteration 42/1000 | Loss: 0.00000903
Iteration 43/1000 | Loss: 0.00000903
Iteration 44/1000 | Loss: 0.00000903
Iteration 45/1000 | Loss: 0.00000903
Iteration 46/1000 | Loss: 0.00000902
Iteration 47/1000 | Loss: 0.00000902
Iteration 48/1000 | Loss: 0.00000901
Iteration 49/1000 | Loss: 0.00000900
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000898
Iteration 53/1000 | Loss: 0.00000897
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000896
Iteration 56/1000 | Loss: 0.00000896
Iteration 57/1000 | Loss: 0.00000895
Iteration 58/1000 | Loss: 0.00000895
Iteration 59/1000 | Loss: 0.00000894
Iteration 60/1000 | Loss: 0.00000894
Iteration 61/1000 | Loss: 0.00000894
Iteration 62/1000 | Loss: 0.00000894
Iteration 63/1000 | Loss: 0.00000893
Iteration 64/1000 | Loss: 0.00000893
Iteration 65/1000 | Loss: 0.00000893
Iteration 66/1000 | Loss: 0.00000893
Iteration 67/1000 | Loss: 0.00000893
Iteration 68/1000 | Loss: 0.00000892
Iteration 69/1000 | Loss: 0.00000890
Iteration 70/1000 | Loss: 0.00000890
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000889
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000888
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000888
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000887
Iteration 83/1000 | Loss: 0.00000887
Iteration 84/1000 | Loss: 0.00000887
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000886
Iteration 89/1000 | Loss: 0.00000886
Iteration 90/1000 | Loss: 0.00000886
Iteration 91/1000 | Loss: 0.00000885
Iteration 92/1000 | Loss: 0.00000885
Iteration 93/1000 | Loss: 0.00000885
Iteration 94/1000 | Loss: 0.00000884
Iteration 95/1000 | Loss: 0.00000884
Iteration 96/1000 | Loss: 0.00000884
Iteration 97/1000 | Loss: 0.00000884
Iteration 98/1000 | Loss: 0.00000883
Iteration 99/1000 | Loss: 0.00000881
Iteration 100/1000 | Loss: 0.00000881
Iteration 101/1000 | Loss: 0.00000881
Iteration 102/1000 | Loss: 0.00000881
Iteration 103/1000 | Loss: 0.00000881
Iteration 104/1000 | Loss: 0.00000881
Iteration 105/1000 | Loss: 0.00000881
Iteration 106/1000 | Loss: 0.00000881
Iteration 107/1000 | Loss: 0.00000881
Iteration 108/1000 | Loss: 0.00000881
Iteration 109/1000 | Loss: 0.00000880
Iteration 110/1000 | Loss: 0.00000880
Iteration 111/1000 | Loss: 0.00000880
Iteration 112/1000 | Loss: 0.00000880
Iteration 113/1000 | Loss: 0.00000880
Iteration 114/1000 | Loss: 0.00000880
Iteration 115/1000 | Loss: 0.00000880
Iteration 116/1000 | Loss: 0.00000880
Iteration 117/1000 | Loss: 0.00000880
Iteration 118/1000 | Loss: 0.00000880
Iteration 119/1000 | Loss: 0.00000880
Iteration 120/1000 | Loss: 0.00000880
Iteration 121/1000 | Loss: 0.00000880
Iteration 122/1000 | Loss: 0.00000880
Iteration 123/1000 | Loss: 0.00000879
Iteration 124/1000 | Loss: 0.00000879
Iteration 125/1000 | Loss: 0.00000879
Iteration 126/1000 | Loss: 0.00000878
Iteration 127/1000 | Loss: 0.00000878
Iteration 128/1000 | Loss: 0.00000877
Iteration 129/1000 | Loss: 0.00000877
Iteration 130/1000 | Loss: 0.00000877
Iteration 131/1000 | Loss: 0.00000877
Iteration 132/1000 | Loss: 0.00000877
Iteration 133/1000 | Loss: 0.00000877
Iteration 134/1000 | Loss: 0.00000877
Iteration 135/1000 | Loss: 0.00000877
Iteration 136/1000 | Loss: 0.00000877
Iteration 137/1000 | Loss: 0.00000877
Iteration 138/1000 | Loss: 0.00000877
Iteration 139/1000 | Loss: 0.00000876
Iteration 140/1000 | Loss: 0.00000876
Iteration 141/1000 | Loss: 0.00000876
Iteration 142/1000 | Loss: 0.00000876
Iteration 143/1000 | Loss: 0.00000876
Iteration 144/1000 | Loss: 0.00000876
Iteration 145/1000 | Loss: 0.00000876
Iteration 146/1000 | Loss: 0.00000876
Iteration 147/1000 | Loss: 0.00000876
Iteration 148/1000 | Loss: 0.00000876
Iteration 149/1000 | Loss: 0.00000875
Iteration 150/1000 | Loss: 0.00000875
Iteration 151/1000 | Loss: 0.00000875
Iteration 152/1000 | Loss: 0.00000875
Iteration 153/1000 | Loss: 0.00000875
Iteration 154/1000 | Loss: 0.00000875
Iteration 155/1000 | Loss: 0.00000875
Iteration 156/1000 | Loss: 0.00000875
Iteration 157/1000 | Loss: 0.00000875
Iteration 158/1000 | Loss: 0.00000875
Iteration 159/1000 | Loss: 0.00000875
Iteration 160/1000 | Loss: 0.00000875
Iteration 161/1000 | Loss: 0.00000875
Iteration 162/1000 | Loss: 0.00000875
Iteration 163/1000 | Loss: 0.00000875
Iteration 164/1000 | Loss: 0.00000875
Iteration 165/1000 | Loss: 0.00000875
Iteration 166/1000 | Loss: 0.00000875
Iteration 167/1000 | Loss: 0.00000875
Iteration 168/1000 | Loss: 0.00000875
Iteration 169/1000 | Loss: 0.00000875
Iteration 170/1000 | Loss: 0.00000875
Iteration 171/1000 | Loss: 0.00000875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [8.746731509745587e-06, 8.746731509745587e-06, 8.746731509745587e-06, 8.746731509745587e-06, 8.746731509745587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.746731509745587e-06

Optimization complete. Final v2v error: 2.5372586250305176 mm

Highest mean error: 3.8738839626312256 mm for frame 83

Lowest mean error: 2.246617555618286 mm for frame 169

Saving results

Total time: 85.47057294845581
