Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=235, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13160-13215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752059
Iteration 2/25 | Loss: 0.00179065
Iteration 3/25 | Loss: 0.00128602
Iteration 4/25 | Loss: 0.00121730
Iteration 5/25 | Loss: 0.00118375
Iteration 6/25 | Loss: 0.00117271
Iteration 7/25 | Loss: 0.00117045
Iteration 8/25 | Loss: 0.00114859
Iteration 9/25 | Loss: 0.00113735
Iteration 10/25 | Loss: 0.00112717
Iteration 11/25 | Loss: 0.00112402
Iteration 12/25 | Loss: 0.00112186
Iteration 13/25 | Loss: 0.00112374
Iteration 14/25 | Loss: 0.00112266
Iteration 15/25 | Loss: 0.00112204
Iteration 16/25 | Loss: 0.00112005
Iteration 17/25 | Loss: 0.00111919
Iteration 18/25 | Loss: 0.00111874
Iteration 19/25 | Loss: 0.00111866
Iteration 20/25 | Loss: 0.00111865
Iteration 21/25 | Loss: 0.00111865
Iteration 22/25 | Loss: 0.00111865
Iteration 23/25 | Loss: 0.00111865
Iteration 24/25 | Loss: 0.00111865
Iteration 25/25 | Loss: 0.00111865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36380887
Iteration 2/25 | Loss: 0.00076262
Iteration 3/25 | Loss: 0.00076259
Iteration 4/25 | Loss: 0.00076259
Iteration 5/25 | Loss: 0.00076259
Iteration 6/25 | Loss: 0.00076258
Iteration 7/25 | Loss: 0.00076258
Iteration 8/25 | Loss: 0.00076258
Iteration 9/25 | Loss: 0.00076258
Iteration 10/25 | Loss: 0.00076258
Iteration 11/25 | Loss: 0.00076258
Iteration 12/25 | Loss: 0.00076258
Iteration 13/25 | Loss: 0.00076258
Iteration 14/25 | Loss: 0.00076258
Iteration 15/25 | Loss: 0.00076258
Iteration 16/25 | Loss: 0.00076258
Iteration 17/25 | Loss: 0.00076258
Iteration 18/25 | Loss: 0.00076258
Iteration 19/25 | Loss: 0.00076258
Iteration 20/25 | Loss: 0.00076258
Iteration 21/25 | Loss: 0.00076258
Iteration 22/25 | Loss: 0.00076258
Iteration 23/25 | Loss: 0.00076258
Iteration 24/25 | Loss: 0.00076258
Iteration 25/25 | Loss: 0.00076258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076258
Iteration 2/1000 | Loss: 0.00004129
Iteration 3/1000 | Loss: 0.00002993
Iteration 4/1000 | Loss: 0.00002610
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002400
Iteration 7/1000 | Loss: 0.00002347
Iteration 8/1000 | Loss: 0.00002297
Iteration 9/1000 | Loss: 0.00002268
Iteration 10/1000 | Loss: 0.00002238
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002207
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002188
Iteration 17/1000 | Loss: 0.00002186
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002180
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002178
Iteration 22/1000 | Loss: 0.00002174
Iteration 23/1000 | Loss: 0.00002174
Iteration 24/1000 | Loss: 0.00002174
Iteration 25/1000 | Loss: 0.00002173
Iteration 26/1000 | Loss: 0.00002172
Iteration 27/1000 | Loss: 0.00002171
Iteration 28/1000 | Loss: 0.00002171
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002160
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002158
Iteration 41/1000 | Loss: 0.00002157
Iteration 42/1000 | Loss: 0.00002157
Iteration 43/1000 | Loss: 0.00002156
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002156
Iteration 46/1000 | Loss: 0.00002156
Iteration 47/1000 | Loss: 0.00002156
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002155
Iteration 50/1000 | Loss: 0.00002155
Iteration 51/1000 | Loss: 0.00002155
Iteration 52/1000 | Loss: 0.00002155
Iteration 53/1000 | Loss: 0.00002155
Iteration 54/1000 | Loss: 0.00002154
Iteration 55/1000 | Loss: 0.00002154
Iteration 56/1000 | Loss: 0.00002154
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002153
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002151
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002150
Iteration 70/1000 | Loss: 0.00002150
Iteration 71/1000 | Loss: 0.00002150
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002149
Iteration 76/1000 | Loss: 0.00002149
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002148
Iteration 81/1000 | Loss: 0.00002148
Iteration 82/1000 | Loss: 0.00002148
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002147
Iteration 85/1000 | Loss: 0.00002147
Iteration 86/1000 | Loss: 0.00002147
Iteration 87/1000 | Loss: 0.00002147
Iteration 88/1000 | Loss: 0.00002147
Iteration 89/1000 | Loss: 0.00002147
Iteration 90/1000 | Loss: 0.00002147
Iteration 91/1000 | Loss: 0.00002147
Iteration 92/1000 | Loss: 0.00002147
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002146
Iteration 96/1000 | Loss: 0.00002146
Iteration 97/1000 | Loss: 0.00002146
Iteration 98/1000 | Loss: 0.00002146
Iteration 99/1000 | Loss: 0.00002146
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002145
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002145
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002145
Iteration 115/1000 | Loss: 0.00002145
Iteration 116/1000 | Loss: 0.00002145
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002145
Iteration 121/1000 | Loss: 0.00002145
Iteration 122/1000 | Loss: 0.00002145
Iteration 123/1000 | Loss: 0.00002145
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.1449084670166485e-05, 2.1449084670166485e-05, 2.1449084670166485e-05, 2.1449084670166485e-05, 2.1449084670166485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1449084670166485e-05

Optimization complete. Final v2v error: 3.991593599319458 mm

Highest mean error: 4.768846035003662 mm for frame 33

Lowest mean error: 3.3389196395874023 mm for frame 138

Saving results

Total time: 73.26608467102051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880527
Iteration 2/25 | Loss: 0.00123354
Iteration 3/25 | Loss: 0.00108167
Iteration 4/25 | Loss: 0.00105797
Iteration 5/25 | Loss: 0.00105185
Iteration 6/25 | Loss: 0.00104971
Iteration 7/25 | Loss: 0.00104921
Iteration 8/25 | Loss: 0.00104921
Iteration 9/25 | Loss: 0.00104921
Iteration 10/25 | Loss: 0.00104921
Iteration 11/25 | Loss: 0.00104921
Iteration 12/25 | Loss: 0.00104921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010492127621546388, 0.0010492127621546388, 0.0010492127621546388, 0.0010492127621546388, 0.0010492127621546388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010492127621546388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41910267
Iteration 2/25 | Loss: 0.00061509
Iteration 3/25 | Loss: 0.00061509
Iteration 4/25 | Loss: 0.00061509
Iteration 5/25 | Loss: 0.00061509
Iteration 6/25 | Loss: 0.00061508
Iteration 7/25 | Loss: 0.00061508
Iteration 8/25 | Loss: 0.00061508
Iteration 9/25 | Loss: 0.00061508
Iteration 10/25 | Loss: 0.00061508
Iteration 11/25 | Loss: 0.00061508
Iteration 12/25 | Loss: 0.00061508
Iteration 13/25 | Loss: 0.00061508
Iteration 14/25 | Loss: 0.00061508
Iteration 15/25 | Loss: 0.00061508
Iteration 16/25 | Loss: 0.00061508
Iteration 17/25 | Loss: 0.00061508
Iteration 18/25 | Loss: 0.00061508
Iteration 19/25 | Loss: 0.00061508
Iteration 20/25 | Loss: 0.00061508
Iteration 21/25 | Loss: 0.00061508
Iteration 22/25 | Loss: 0.00061508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006150839617475867, 0.0006150839617475867, 0.0006150839617475867, 0.0006150839617475867, 0.0006150839617475867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006150839617475867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061508
Iteration 2/1000 | Loss: 0.00003777
Iteration 3/1000 | Loss: 0.00002716
Iteration 4/1000 | Loss: 0.00002319
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002082
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00001974
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001892
Iteration 12/1000 | Loss: 0.00001885
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001856
Iteration 15/1000 | Loss: 0.00001855
Iteration 16/1000 | Loss: 0.00001854
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001844
Iteration 21/1000 | Loss: 0.00001841
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001839
Iteration 27/1000 | Loss: 0.00001838
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001838
Iteration 30/1000 | Loss: 0.00001838
Iteration 31/1000 | Loss: 0.00001837
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001837
Iteration 34/1000 | Loss: 0.00001837
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001836
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001835
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001834
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001833
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001833
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001832
Iteration 59/1000 | Loss: 0.00001832
Iteration 60/1000 | Loss: 0.00001832
Iteration 61/1000 | Loss: 0.00001832
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001828
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001828
Iteration 76/1000 | Loss: 0.00001827
Iteration 77/1000 | Loss: 0.00001827
Iteration 78/1000 | Loss: 0.00001827
Iteration 79/1000 | Loss: 0.00001827
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001826
Iteration 82/1000 | Loss: 0.00001826
Iteration 83/1000 | Loss: 0.00001826
Iteration 84/1000 | Loss: 0.00001826
Iteration 85/1000 | Loss: 0.00001826
Iteration 86/1000 | Loss: 0.00001826
Iteration 87/1000 | Loss: 0.00001826
Iteration 88/1000 | Loss: 0.00001826
Iteration 89/1000 | Loss: 0.00001826
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001825
Iteration 95/1000 | Loss: 0.00001825
Iteration 96/1000 | Loss: 0.00001825
Iteration 97/1000 | Loss: 0.00001825
Iteration 98/1000 | Loss: 0.00001824
Iteration 99/1000 | Loss: 0.00001824
Iteration 100/1000 | Loss: 0.00001824
Iteration 101/1000 | Loss: 0.00001824
Iteration 102/1000 | Loss: 0.00001824
Iteration 103/1000 | Loss: 0.00001824
Iteration 104/1000 | Loss: 0.00001824
Iteration 105/1000 | Loss: 0.00001824
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001823
Iteration 109/1000 | Loss: 0.00001823
Iteration 110/1000 | Loss: 0.00001823
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001822
Iteration 113/1000 | Loss: 0.00001822
Iteration 114/1000 | Loss: 0.00001822
Iteration 115/1000 | Loss: 0.00001822
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001822
Iteration 121/1000 | Loss: 0.00001822
Iteration 122/1000 | Loss: 0.00001822
Iteration 123/1000 | Loss: 0.00001821
Iteration 124/1000 | Loss: 0.00001821
Iteration 125/1000 | Loss: 0.00001821
Iteration 126/1000 | Loss: 0.00001821
Iteration 127/1000 | Loss: 0.00001821
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001820
Iteration 141/1000 | Loss: 0.00001819
Iteration 142/1000 | Loss: 0.00001819
Iteration 143/1000 | Loss: 0.00001819
Iteration 144/1000 | Loss: 0.00001819
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001819
Iteration 147/1000 | Loss: 0.00001819
Iteration 148/1000 | Loss: 0.00001818
Iteration 149/1000 | Loss: 0.00001818
Iteration 150/1000 | Loss: 0.00001818
Iteration 151/1000 | Loss: 0.00001818
Iteration 152/1000 | Loss: 0.00001818
Iteration 153/1000 | Loss: 0.00001818
Iteration 154/1000 | Loss: 0.00001818
Iteration 155/1000 | Loss: 0.00001818
Iteration 156/1000 | Loss: 0.00001818
Iteration 157/1000 | Loss: 0.00001818
Iteration 158/1000 | Loss: 0.00001818
Iteration 159/1000 | Loss: 0.00001817
Iteration 160/1000 | Loss: 0.00001817
Iteration 161/1000 | Loss: 0.00001817
Iteration 162/1000 | Loss: 0.00001817
Iteration 163/1000 | Loss: 0.00001817
Iteration 164/1000 | Loss: 0.00001817
Iteration 165/1000 | Loss: 0.00001817
Iteration 166/1000 | Loss: 0.00001817
Iteration 167/1000 | Loss: 0.00001817
Iteration 168/1000 | Loss: 0.00001817
Iteration 169/1000 | Loss: 0.00001817
Iteration 170/1000 | Loss: 0.00001817
Iteration 171/1000 | Loss: 0.00001817
Iteration 172/1000 | Loss: 0.00001817
Iteration 173/1000 | Loss: 0.00001817
Iteration 174/1000 | Loss: 0.00001817
Iteration 175/1000 | Loss: 0.00001817
Iteration 176/1000 | Loss: 0.00001816
Iteration 177/1000 | Loss: 0.00001816
Iteration 178/1000 | Loss: 0.00001816
Iteration 179/1000 | Loss: 0.00001816
Iteration 180/1000 | Loss: 0.00001816
Iteration 181/1000 | Loss: 0.00001816
Iteration 182/1000 | Loss: 0.00001816
Iteration 183/1000 | Loss: 0.00001816
Iteration 184/1000 | Loss: 0.00001815
Iteration 185/1000 | Loss: 0.00001815
Iteration 186/1000 | Loss: 0.00001815
Iteration 187/1000 | Loss: 0.00001815
Iteration 188/1000 | Loss: 0.00001815
Iteration 189/1000 | Loss: 0.00001815
Iteration 190/1000 | Loss: 0.00001815
Iteration 191/1000 | Loss: 0.00001815
Iteration 192/1000 | Loss: 0.00001814
Iteration 193/1000 | Loss: 0.00001814
Iteration 194/1000 | Loss: 0.00001814
Iteration 195/1000 | Loss: 0.00001814
Iteration 196/1000 | Loss: 0.00001814
Iteration 197/1000 | Loss: 0.00001814
Iteration 198/1000 | Loss: 0.00001814
Iteration 199/1000 | Loss: 0.00001814
Iteration 200/1000 | Loss: 0.00001814
Iteration 201/1000 | Loss: 0.00001814
Iteration 202/1000 | Loss: 0.00001814
Iteration 203/1000 | Loss: 0.00001814
Iteration 204/1000 | Loss: 0.00001814
Iteration 205/1000 | Loss: 0.00001814
Iteration 206/1000 | Loss: 0.00001814
Iteration 207/1000 | Loss: 0.00001814
Iteration 208/1000 | Loss: 0.00001814
Iteration 209/1000 | Loss: 0.00001814
Iteration 210/1000 | Loss: 0.00001814
Iteration 211/1000 | Loss: 0.00001814
Iteration 212/1000 | Loss: 0.00001814
Iteration 213/1000 | Loss: 0.00001814
Iteration 214/1000 | Loss: 0.00001814
Iteration 215/1000 | Loss: 0.00001814
Iteration 216/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.8143273337045684e-05, 1.8143273337045684e-05, 1.8143273337045684e-05, 1.8143273337045684e-05, 1.8143273337045684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8143273337045684e-05

Optimization complete. Final v2v error: 3.6490111351013184 mm

Highest mean error: 4.759687900543213 mm for frame 90

Lowest mean error: 3.186997890472412 mm for frame 7

Saving results

Total time: 49.248764991760254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128689
Iteration 2/25 | Loss: 0.01128689
Iteration 3/25 | Loss: 0.01128689
Iteration 4/25 | Loss: 0.01128689
Iteration 5/25 | Loss: 0.01128689
Iteration 6/25 | Loss: 0.01128689
Iteration 7/25 | Loss: 0.01128688
Iteration 8/25 | Loss: 0.01128688
Iteration 9/25 | Loss: 0.01128688
Iteration 10/25 | Loss: 0.01128688
Iteration 11/25 | Loss: 0.01128688
Iteration 12/25 | Loss: 0.01128687
Iteration 13/25 | Loss: 0.01128687
Iteration 14/25 | Loss: 0.01128687
Iteration 15/25 | Loss: 0.01128687
Iteration 16/25 | Loss: 0.01128687
Iteration 17/25 | Loss: 0.01128686
Iteration 18/25 | Loss: 0.01128686
Iteration 19/25 | Loss: 0.01128686
Iteration 20/25 | Loss: 0.01128686
Iteration 21/25 | Loss: 0.01128686
Iteration 22/25 | Loss: 0.01128686
Iteration 23/25 | Loss: 0.01128685
Iteration 24/25 | Loss: 0.01128685
Iteration 25/25 | Loss: 0.01128685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55643475
Iteration 2/25 | Loss: 0.17230222
Iteration 3/25 | Loss: 0.16348915
Iteration 4/25 | Loss: 0.15290642
Iteration 5/25 | Loss: 0.15290627
Iteration 6/25 | Loss: 0.15290624
Iteration 7/25 | Loss: 0.15290624
Iteration 8/25 | Loss: 0.15290624
Iteration 9/25 | Loss: 0.15290622
Iteration 10/25 | Loss: 0.15290622
Iteration 11/25 | Loss: 0.15290622
Iteration 12/25 | Loss: 0.15290622
Iteration 13/25 | Loss: 0.15290622
Iteration 14/25 | Loss: 0.15290622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.15290622413158417, 0.15290622413158417, 0.15290622413158417, 0.15290622413158417, 0.15290622413158417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15290622413158417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15290622
Iteration 2/1000 | Loss: 0.01159334
Iteration 3/1000 | Loss: 0.00142033
Iteration 4/1000 | Loss: 0.00069551
Iteration 5/1000 | Loss: 0.00042799
Iteration 6/1000 | Loss: 0.00022500
Iteration 7/1000 | Loss: 0.00019277
Iteration 8/1000 | Loss: 0.00013215
Iteration 9/1000 | Loss: 0.00008495
Iteration 10/1000 | Loss: 0.00008556
Iteration 11/1000 | Loss: 0.00004854
Iteration 12/1000 | Loss: 0.00011776
Iteration 13/1000 | Loss: 0.00015796
Iteration 14/1000 | Loss: 0.00003845
Iteration 15/1000 | Loss: 0.00003640
Iteration 16/1000 | Loss: 0.00003522
Iteration 17/1000 | Loss: 0.00025050
Iteration 18/1000 | Loss: 0.00004610
Iteration 19/1000 | Loss: 0.00003075
Iteration 20/1000 | Loss: 0.00002965
Iteration 21/1000 | Loss: 0.00013076
Iteration 22/1000 | Loss: 0.00002863
Iteration 23/1000 | Loss: 0.00002790
Iteration 24/1000 | Loss: 0.00002734
Iteration 25/1000 | Loss: 0.00002676
Iteration 26/1000 | Loss: 0.00002637
Iteration 27/1000 | Loss: 0.00014736
Iteration 28/1000 | Loss: 0.00014772
Iteration 29/1000 | Loss: 0.00005259
Iteration 30/1000 | Loss: 0.00002631
Iteration 31/1000 | Loss: 0.00008924
Iteration 32/1000 | Loss: 0.00021939
Iteration 33/1000 | Loss: 0.00002685
Iteration 34/1000 | Loss: 0.00019987
Iteration 35/1000 | Loss: 0.00003285
Iteration 36/1000 | Loss: 0.00003267
Iteration 37/1000 | Loss: 0.00004051
Iteration 38/1000 | Loss: 0.00003719
Iteration 39/1000 | Loss: 0.00002583
Iteration 40/1000 | Loss: 0.00002583
Iteration 41/1000 | Loss: 0.00002581
Iteration 42/1000 | Loss: 0.00009672
Iteration 43/1000 | Loss: 0.00004409
Iteration 44/1000 | Loss: 0.00004828
Iteration 45/1000 | Loss: 0.00005155
Iteration 46/1000 | Loss: 0.00013849
Iteration 47/1000 | Loss: 0.00005086
Iteration 48/1000 | Loss: 0.00008047
Iteration 49/1000 | Loss: 0.00003624
Iteration 50/1000 | Loss: 0.00009590
Iteration 51/1000 | Loss: 0.00003895
Iteration 52/1000 | Loss: 0.00002583
Iteration 53/1000 | Loss: 0.00002570
Iteration 54/1000 | Loss: 0.00002565
Iteration 55/1000 | Loss: 0.00002565
Iteration 56/1000 | Loss: 0.00002564
Iteration 57/1000 | Loss: 0.00002556
Iteration 58/1000 | Loss: 0.00002556
Iteration 59/1000 | Loss: 0.00002555
Iteration 60/1000 | Loss: 0.00002555
Iteration 61/1000 | Loss: 0.00002555
Iteration 62/1000 | Loss: 0.00002555
Iteration 63/1000 | Loss: 0.00002555
Iteration 64/1000 | Loss: 0.00002554
Iteration 65/1000 | Loss: 0.00002554
Iteration 66/1000 | Loss: 0.00002554
Iteration 67/1000 | Loss: 0.00002553
Iteration 68/1000 | Loss: 0.00002553
Iteration 69/1000 | Loss: 0.00002553
Iteration 70/1000 | Loss: 0.00002553
Iteration 71/1000 | Loss: 0.00002552
Iteration 72/1000 | Loss: 0.00002552
Iteration 73/1000 | Loss: 0.00002552
Iteration 74/1000 | Loss: 0.00002552
Iteration 75/1000 | Loss: 0.00002552
Iteration 76/1000 | Loss: 0.00002552
Iteration 77/1000 | Loss: 0.00002552
Iteration 78/1000 | Loss: 0.00002552
Iteration 79/1000 | Loss: 0.00002552
Iteration 80/1000 | Loss: 0.00002552
Iteration 81/1000 | Loss: 0.00002552
Iteration 82/1000 | Loss: 0.00002552
Iteration 83/1000 | Loss: 0.00002552
Iteration 84/1000 | Loss: 0.00002552
Iteration 85/1000 | Loss: 0.00002551
Iteration 86/1000 | Loss: 0.00002551
Iteration 87/1000 | Loss: 0.00002551
Iteration 88/1000 | Loss: 0.00002550
Iteration 89/1000 | Loss: 0.00002550
Iteration 90/1000 | Loss: 0.00002550
Iteration 91/1000 | Loss: 0.00002550
Iteration 92/1000 | Loss: 0.00002550
Iteration 93/1000 | Loss: 0.00002550
Iteration 94/1000 | Loss: 0.00002549
Iteration 95/1000 | Loss: 0.00002549
Iteration 96/1000 | Loss: 0.00002549
Iteration 97/1000 | Loss: 0.00002549
Iteration 98/1000 | Loss: 0.00002549
Iteration 99/1000 | Loss: 0.00002548
Iteration 100/1000 | Loss: 0.00002548
Iteration 101/1000 | Loss: 0.00002548
Iteration 102/1000 | Loss: 0.00002548
Iteration 103/1000 | Loss: 0.00002548
Iteration 104/1000 | Loss: 0.00002547
Iteration 105/1000 | Loss: 0.00002547
Iteration 106/1000 | Loss: 0.00002547
Iteration 107/1000 | Loss: 0.00002547
Iteration 108/1000 | Loss: 0.00002547
Iteration 109/1000 | Loss: 0.00002547
Iteration 110/1000 | Loss: 0.00002547
Iteration 111/1000 | Loss: 0.00002547
Iteration 112/1000 | Loss: 0.00002547
Iteration 113/1000 | Loss: 0.00002547
Iteration 114/1000 | Loss: 0.00002545
Iteration 115/1000 | Loss: 0.00002545
Iteration 116/1000 | Loss: 0.00002544
Iteration 117/1000 | Loss: 0.00002544
Iteration 118/1000 | Loss: 0.00002543
Iteration 119/1000 | Loss: 0.00013065
Iteration 120/1000 | Loss: 0.00002731
Iteration 121/1000 | Loss: 0.00016310
Iteration 122/1000 | Loss: 0.00002634
Iteration 123/1000 | Loss: 0.00009411
Iteration 124/1000 | Loss: 0.00005936
Iteration 125/1000 | Loss: 0.00003220
Iteration 126/1000 | Loss: 0.00002570
Iteration 127/1000 | Loss: 0.00002539
Iteration 128/1000 | Loss: 0.00002538
Iteration 129/1000 | Loss: 0.00002537
Iteration 130/1000 | Loss: 0.00002533
Iteration 131/1000 | Loss: 0.00002533
Iteration 132/1000 | Loss: 0.00002533
Iteration 133/1000 | Loss: 0.00002532
Iteration 134/1000 | Loss: 0.00002532
Iteration 135/1000 | Loss: 0.00002531
Iteration 136/1000 | Loss: 0.00002531
Iteration 137/1000 | Loss: 0.00002531
Iteration 138/1000 | Loss: 0.00002531
Iteration 139/1000 | Loss: 0.00002531
Iteration 140/1000 | Loss: 0.00002531
Iteration 141/1000 | Loss: 0.00002531
Iteration 142/1000 | Loss: 0.00002531
Iteration 143/1000 | Loss: 0.00002531
Iteration 144/1000 | Loss: 0.00002531
Iteration 145/1000 | Loss: 0.00002531
Iteration 146/1000 | Loss: 0.00002531
Iteration 147/1000 | Loss: 0.00002531
Iteration 148/1000 | Loss: 0.00002531
Iteration 149/1000 | Loss: 0.00002531
Iteration 150/1000 | Loss: 0.00002531
Iteration 151/1000 | Loss: 0.00002531
Iteration 152/1000 | Loss: 0.00002531
Iteration 153/1000 | Loss: 0.00002530
Iteration 154/1000 | Loss: 0.00002530
Iteration 155/1000 | Loss: 0.00002530
Iteration 156/1000 | Loss: 0.00002530
Iteration 157/1000 | Loss: 0.00002530
Iteration 158/1000 | Loss: 0.00002530
Iteration 159/1000 | Loss: 0.00002530
Iteration 160/1000 | Loss: 0.00002530
Iteration 161/1000 | Loss: 0.00002530
Iteration 162/1000 | Loss: 0.00002530
Iteration 163/1000 | Loss: 0.00002530
Iteration 164/1000 | Loss: 0.00002530
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002530
Iteration 170/1000 | Loss: 0.00002530
Iteration 171/1000 | Loss: 0.00002530
Iteration 172/1000 | Loss: 0.00002530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.5303872462245636e-05, 2.5303872462245636e-05, 2.5303872462245636e-05, 2.5303872462245636e-05, 2.5303872462245636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5303872462245636e-05

Optimization complete. Final v2v error: 4.46788215637207 mm

Highest mean error: 9.896696090698242 mm for frame 184

Lowest mean error: 3.990525007247925 mm for frame 239

Saving results

Total time: 115.05128693580627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113744
Iteration 2/25 | Loss: 0.00221230
Iteration 3/25 | Loss: 0.00168396
Iteration 4/25 | Loss: 0.00181306
Iteration 5/25 | Loss: 0.00169890
Iteration 6/25 | Loss: 0.00146846
Iteration 7/25 | Loss: 0.00125728
Iteration 8/25 | Loss: 0.00113723
Iteration 9/25 | Loss: 0.00110520
Iteration 10/25 | Loss: 0.00109168
Iteration 11/25 | Loss: 0.00108750
Iteration 12/25 | Loss: 0.00108419
Iteration 13/25 | Loss: 0.00108275
Iteration 14/25 | Loss: 0.00108334
Iteration 15/25 | Loss: 0.00108073
Iteration 16/25 | Loss: 0.00108043
Iteration 17/25 | Loss: 0.00108438
Iteration 18/25 | Loss: 0.00107219
Iteration 19/25 | Loss: 0.00106700
Iteration 20/25 | Loss: 0.00106508
Iteration 21/25 | Loss: 0.00106411
Iteration 22/25 | Loss: 0.00106384
Iteration 23/25 | Loss: 0.00106371
Iteration 24/25 | Loss: 0.00106370
Iteration 25/25 | Loss: 0.00106370

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45221996
Iteration 2/25 | Loss: 0.00054025
Iteration 3/25 | Loss: 0.00054025
Iteration 4/25 | Loss: 0.00054025
Iteration 5/25 | Loss: 0.00054024
Iteration 6/25 | Loss: 0.00054024
Iteration 7/25 | Loss: 0.00054024
Iteration 8/25 | Loss: 0.00054024
Iteration 9/25 | Loss: 0.00054024
Iteration 10/25 | Loss: 0.00054024
Iteration 11/25 | Loss: 0.00054024
Iteration 12/25 | Loss: 0.00054024
Iteration 13/25 | Loss: 0.00054024
Iteration 14/25 | Loss: 0.00054024
Iteration 15/25 | Loss: 0.00054024
Iteration 16/25 | Loss: 0.00054024
Iteration 17/25 | Loss: 0.00054024
Iteration 18/25 | Loss: 0.00054024
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005402436363510787, 0.0005402436363510787, 0.0005402436363510787, 0.0005402436363510787, 0.0005402436363510787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005402436363510787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054024
Iteration 2/1000 | Loss: 0.00003889
Iteration 3/1000 | Loss: 0.00003193
Iteration 4/1000 | Loss: 0.00002973
Iteration 5/1000 | Loss: 0.00002821
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002664
Iteration 8/1000 | Loss: 0.00002617
Iteration 9/1000 | Loss: 0.00002609
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00002576
Iteration 13/1000 | Loss: 0.00002572
Iteration 14/1000 | Loss: 0.00002571
Iteration 15/1000 | Loss: 0.00002571
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002566
Iteration 18/1000 | Loss: 0.00002565
Iteration 19/1000 | Loss: 0.00002562
Iteration 20/1000 | Loss: 0.00002562
Iteration 21/1000 | Loss: 0.00002560
Iteration 22/1000 | Loss: 0.00002560
Iteration 23/1000 | Loss: 0.00002559
Iteration 24/1000 | Loss: 0.00002559
Iteration 25/1000 | Loss: 0.00002559
Iteration 26/1000 | Loss: 0.00002559
Iteration 27/1000 | Loss: 0.00002558
Iteration 28/1000 | Loss: 0.00002558
Iteration 29/1000 | Loss: 0.00002558
Iteration 30/1000 | Loss: 0.00002557
Iteration 31/1000 | Loss: 0.00002557
Iteration 32/1000 | Loss: 0.00002557
Iteration 33/1000 | Loss: 0.00002557
Iteration 34/1000 | Loss: 0.00002556
Iteration 35/1000 | Loss: 0.00002556
Iteration 36/1000 | Loss: 0.00002556
Iteration 37/1000 | Loss: 0.00002556
Iteration 38/1000 | Loss: 0.00002555
Iteration 39/1000 | Loss: 0.00002555
Iteration 40/1000 | Loss: 0.00002555
Iteration 41/1000 | Loss: 0.00002554
Iteration 42/1000 | Loss: 0.00002554
Iteration 43/1000 | Loss: 0.00002554
Iteration 44/1000 | Loss: 0.00002553
Iteration 45/1000 | Loss: 0.00002553
Iteration 46/1000 | Loss: 0.00002553
Iteration 47/1000 | Loss: 0.00002553
Iteration 48/1000 | Loss: 0.00002553
Iteration 49/1000 | Loss: 0.00002553
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002553
Iteration 52/1000 | Loss: 0.00002553
Iteration 53/1000 | Loss: 0.00002553
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002552
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002552
Iteration 58/1000 | Loss: 0.00002552
Iteration 59/1000 | Loss: 0.00002552
Iteration 60/1000 | Loss: 0.00002552
Iteration 61/1000 | Loss: 0.00002551
Iteration 62/1000 | Loss: 0.00002551
Iteration 63/1000 | Loss: 0.00002551
Iteration 64/1000 | Loss: 0.00002551
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002551
Iteration 70/1000 | Loss: 0.00002551
Iteration 71/1000 | Loss: 0.00002551
Iteration 72/1000 | Loss: 0.00002551
Iteration 73/1000 | Loss: 0.00002551
Iteration 74/1000 | Loss: 0.00002551
Iteration 75/1000 | Loss: 0.00002551
Iteration 76/1000 | Loss: 0.00002551
Iteration 77/1000 | Loss: 0.00002550
Iteration 78/1000 | Loss: 0.00002550
Iteration 79/1000 | Loss: 0.00002550
Iteration 80/1000 | Loss: 0.00002550
Iteration 81/1000 | Loss: 0.00002550
Iteration 82/1000 | Loss: 0.00002550
Iteration 83/1000 | Loss: 0.00002550
Iteration 84/1000 | Loss: 0.00002550
Iteration 85/1000 | Loss: 0.00002550
Iteration 86/1000 | Loss: 0.00002549
Iteration 87/1000 | Loss: 0.00002549
Iteration 88/1000 | Loss: 0.00002549
Iteration 89/1000 | Loss: 0.00002549
Iteration 90/1000 | Loss: 0.00002549
Iteration 91/1000 | Loss: 0.00002549
Iteration 92/1000 | Loss: 0.00002549
Iteration 93/1000 | Loss: 0.00002548
Iteration 94/1000 | Loss: 0.00002548
Iteration 95/1000 | Loss: 0.00002548
Iteration 96/1000 | Loss: 0.00002548
Iteration 97/1000 | Loss: 0.00002548
Iteration 98/1000 | Loss: 0.00002548
Iteration 99/1000 | Loss: 0.00002548
Iteration 100/1000 | Loss: 0.00002547
Iteration 101/1000 | Loss: 0.00002547
Iteration 102/1000 | Loss: 0.00002547
Iteration 103/1000 | Loss: 0.00002547
Iteration 104/1000 | Loss: 0.00002547
Iteration 105/1000 | Loss: 0.00002547
Iteration 106/1000 | Loss: 0.00002547
Iteration 107/1000 | Loss: 0.00002547
Iteration 108/1000 | Loss: 0.00002547
Iteration 109/1000 | Loss: 0.00002547
Iteration 110/1000 | Loss: 0.00002546
Iteration 111/1000 | Loss: 0.00002546
Iteration 112/1000 | Loss: 0.00002546
Iteration 113/1000 | Loss: 0.00002546
Iteration 114/1000 | Loss: 0.00002546
Iteration 115/1000 | Loss: 0.00002546
Iteration 116/1000 | Loss: 0.00002546
Iteration 117/1000 | Loss: 0.00002546
Iteration 118/1000 | Loss: 0.00002546
Iteration 119/1000 | Loss: 0.00002546
Iteration 120/1000 | Loss: 0.00002546
Iteration 121/1000 | Loss: 0.00002546
Iteration 122/1000 | Loss: 0.00002545
Iteration 123/1000 | Loss: 0.00002545
Iteration 124/1000 | Loss: 0.00002545
Iteration 125/1000 | Loss: 0.00002544
Iteration 126/1000 | Loss: 0.00002544
Iteration 127/1000 | Loss: 0.00002544
Iteration 128/1000 | Loss: 0.00002544
Iteration 129/1000 | Loss: 0.00002544
Iteration 130/1000 | Loss: 0.00002544
Iteration 131/1000 | Loss: 0.00002544
Iteration 132/1000 | Loss: 0.00002544
Iteration 133/1000 | Loss: 0.00002544
Iteration 134/1000 | Loss: 0.00002544
Iteration 135/1000 | Loss: 0.00002544
Iteration 136/1000 | Loss: 0.00002544
Iteration 137/1000 | Loss: 0.00002544
Iteration 138/1000 | Loss: 0.00002543
Iteration 139/1000 | Loss: 0.00002543
Iteration 140/1000 | Loss: 0.00002543
Iteration 141/1000 | Loss: 0.00002543
Iteration 142/1000 | Loss: 0.00002543
Iteration 143/1000 | Loss: 0.00002543
Iteration 144/1000 | Loss: 0.00002543
Iteration 145/1000 | Loss: 0.00002543
Iteration 146/1000 | Loss: 0.00002543
Iteration 147/1000 | Loss: 0.00002543
Iteration 148/1000 | Loss: 0.00002543
Iteration 149/1000 | Loss: 0.00002543
Iteration 150/1000 | Loss: 0.00002543
Iteration 151/1000 | Loss: 0.00002543
Iteration 152/1000 | Loss: 0.00002543
Iteration 153/1000 | Loss: 0.00002543
Iteration 154/1000 | Loss: 0.00002543
Iteration 155/1000 | Loss: 0.00002543
Iteration 156/1000 | Loss: 0.00002543
Iteration 157/1000 | Loss: 0.00002543
Iteration 158/1000 | Loss: 0.00002543
Iteration 159/1000 | Loss: 0.00002543
Iteration 160/1000 | Loss: 0.00002543
Iteration 161/1000 | Loss: 0.00002543
Iteration 162/1000 | Loss: 0.00002543
Iteration 163/1000 | Loss: 0.00002543
Iteration 164/1000 | Loss: 0.00002543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.5434854251216166e-05, 2.5434854251216166e-05, 2.5434854251216166e-05, 2.5434854251216166e-05, 2.5434854251216166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5434854251216166e-05

Optimization complete. Final v2v error: 4.338191509246826 mm

Highest mean error: 9.383441925048828 mm for frame 78

Lowest mean error: 4.000657558441162 mm for frame 114

Saving results

Total time: 66.56347703933716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079837
Iteration 2/25 | Loss: 0.00444187
Iteration 3/25 | Loss: 0.00321176
Iteration 4/25 | Loss: 0.00232218
Iteration 5/25 | Loss: 0.00153907
Iteration 6/25 | Loss: 0.00141165
Iteration 7/25 | Loss: 0.00122635
Iteration 8/25 | Loss: 0.00112440
Iteration 9/25 | Loss: 0.00109392
Iteration 10/25 | Loss: 0.00103170
Iteration 11/25 | Loss: 0.00095101
Iteration 12/25 | Loss: 0.00093164
Iteration 13/25 | Loss: 0.00089558
Iteration 14/25 | Loss: 0.00088928
Iteration 15/25 | Loss: 0.00088534
Iteration 16/25 | Loss: 0.00088077
Iteration 17/25 | Loss: 0.00087459
Iteration 18/25 | Loss: 0.00087808
Iteration 19/25 | Loss: 0.00088078
Iteration 20/25 | Loss: 0.00087937
Iteration 21/25 | Loss: 0.00088491
Iteration 22/25 | Loss: 0.00088051
Iteration 23/25 | Loss: 0.00087380
Iteration 24/25 | Loss: 0.00086785
Iteration 25/25 | Loss: 0.00086940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39369249
Iteration 2/25 | Loss: 0.00058584
Iteration 3/25 | Loss: 0.00057947
Iteration 4/25 | Loss: 0.00057947
Iteration 5/25 | Loss: 0.00057947
Iteration 6/25 | Loss: 0.00057947
Iteration 7/25 | Loss: 0.00057947
Iteration 8/25 | Loss: 0.00057947
Iteration 9/25 | Loss: 0.00057947
Iteration 10/25 | Loss: 0.00057947
Iteration 11/25 | Loss: 0.00057947
Iteration 12/25 | Loss: 0.00057947
Iteration 13/25 | Loss: 0.00057947
Iteration 14/25 | Loss: 0.00057947
Iteration 15/25 | Loss: 0.00057947
Iteration 16/25 | Loss: 0.00057947
Iteration 17/25 | Loss: 0.00057947
Iteration 18/25 | Loss: 0.00057947
Iteration 19/25 | Loss: 0.00057947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005794745520688593, 0.0005794745520688593, 0.0005794745520688593, 0.0005794745520688593, 0.0005794745520688593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005794745520688593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057947
Iteration 2/1000 | Loss: 0.00014582
Iteration 3/1000 | Loss: 0.00010225
Iteration 4/1000 | Loss: 0.00007011
Iteration 5/1000 | Loss: 0.00005225
Iteration 6/1000 | Loss: 0.00004333
Iteration 7/1000 | Loss: 0.00010779
Iteration 8/1000 | Loss: 0.00022899
Iteration 9/1000 | Loss: 0.00004933
Iteration 10/1000 | Loss: 0.00003588
Iteration 11/1000 | Loss: 0.00017541
Iteration 12/1000 | Loss: 0.00088218
Iteration 13/1000 | Loss: 0.00031149
Iteration 14/1000 | Loss: 0.00018377
Iteration 15/1000 | Loss: 0.00004868
Iteration 16/1000 | Loss: 0.00010463
Iteration 17/1000 | Loss: 0.00003481
Iteration 18/1000 | Loss: 0.00010124
Iteration 19/1000 | Loss: 0.00003437
Iteration 20/1000 | Loss: 0.00003400
Iteration 21/1000 | Loss: 0.00003373
Iteration 22/1000 | Loss: 0.00003364
Iteration 23/1000 | Loss: 0.00003364
Iteration 24/1000 | Loss: 0.00003363
Iteration 25/1000 | Loss: 0.00003362
Iteration 26/1000 | Loss: 0.00003356
Iteration 27/1000 | Loss: 0.00003355
Iteration 28/1000 | Loss: 0.00003354
Iteration 29/1000 | Loss: 0.00003353
Iteration 30/1000 | Loss: 0.00003353
Iteration 31/1000 | Loss: 0.00003352
Iteration 32/1000 | Loss: 0.00003352
Iteration 33/1000 | Loss: 0.00003351
Iteration 34/1000 | Loss: 0.00003350
Iteration 35/1000 | Loss: 0.00003348
Iteration 36/1000 | Loss: 0.00003348
Iteration 37/1000 | Loss: 0.00010060
Iteration 38/1000 | Loss: 0.00004059
Iteration 39/1000 | Loss: 0.00003710
Iteration 40/1000 | Loss: 0.00003346
Iteration 41/1000 | Loss: 0.00003344
Iteration 42/1000 | Loss: 0.00003342
Iteration 43/1000 | Loss: 0.00003342
Iteration 44/1000 | Loss: 0.00003338
Iteration 45/1000 | Loss: 0.00003338
Iteration 46/1000 | Loss: 0.00003336
Iteration 47/1000 | Loss: 0.00003335
Iteration 48/1000 | Loss: 0.00003335
Iteration 49/1000 | Loss: 0.00003335
Iteration 50/1000 | Loss: 0.00003335
Iteration 51/1000 | Loss: 0.00003335
Iteration 52/1000 | Loss: 0.00003335
Iteration 53/1000 | Loss: 0.00003335
Iteration 54/1000 | Loss: 0.00003335
Iteration 55/1000 | Loss: 0.00003334
Iteration 56/1000 | Loss: 0.00003334
Iteration 57/1000 | Loss: 0.00003334
Iteration 58/1000 | Loss: 0.00003334
Iteration 59/1000 | Loss: 0.00003334
Iteration 60/1000 | Loss: 0.00003333
Iteration 61/1000 | Loss: 0.00003333
Iteration 62/1000 | Loss: 0.00003333
Iteration 63/1000 | Loss: 0.00003333
Iteration 64/1000 | Loss: 0.00003333
Iteration 65/1000 | Loss: 0.00003332
Iteration 66/1000 | Loss: 0.00003332
Iteration 67/1000 | Loss: 0.00003332
Iteration 68/1000 | Loss: 0.00003332
Iteration 69/1000 | Loss: 0.00003332
Iteration 70/1000 | Loss: 0.00003332
Iteration 71/1000 | Loss: 0.00003332
Iteration 72/1000 | Loss: 0.00003332
Iteration 73/1000 | Loss: 0.00003332
Iteration 74/1000 | Loss: 0.00003331
Iteration 75/1000 | Loss: 0.00003331
Iteration 76/1000 | Loss: 0.00003331
Iteration 77/1000 | Loss: 0.00003331
Iteration 78/1000 | Loss: 0.00003331
Iteration 79/1000 | Loss: 0.00003329
Iteration 80/1000 | Loss: 0.00003329
Iteration 81/1000 | Loss: 0.00003328
Iteration 82/1000 | Loss: 0.00003328
Iteration 83/1000 | Loss: 0.00003328
Iteration 84/1000 | Loss: 0.00003328
Iteration 85/1000 | Loss: 0.00003328
Iteration 86/1000 | Loss: 0.00003328
Iteration 87/1000 | Loss: 0.00003328
Iteration 88/1000 | Loss: 0.00003328
Iteration 89/1000 | Loss: 0.00003328
Iteration 90/1000 | Loss: 0.00003328
Iteration 91/1000 | Loss: 0.00003328
Iteration 92/1000 | Loss: 0.00003328
Iteration 93/1000 | Loss: 0.00003327
Iteration 94/1000 | Loss: 0.00003327
Iteration 95/1000 | Loss: 0.00003327
Iteration 96/1000 | Loss: 0.00003327
Iteration 97/1000 | Loss: 0.00003327
Iteration 98/1000 | Loss: 0.00003327
Iteration 99/1000 | Loss: 0.00003326
Iteration 100/1000 | Loss: 0.00003326
Iteration 101/1000 | Loss: 0.00003326
Iteration 102/1000 | Loss: 0.00003326
Iteration 103/1000 | Loss: 0.00003326
Iteration 104/1000 | Loss: 0.00003326
Iteration 105/1000 | Loss: 0.00003326
Iteration 106/1000 | Loss: 0.00003326
Iteration 107/1000 | Loss: 0.00003325
Iteration 108/1000 | Loss: 0.00003325
Iteration 109/1000 | Loss: 0.00003325
Iteration 110/1000 | Loss: 0.00003325
Iteration 111/1000 | Loss: 0.00003325
Iteration 112/1000 | Loss: 0.00003325
Iteration 113/1000 | Loss: 0.00003325
Iteration 114/1000 | Loss: 0.00003325
Iteration 115/1000 | Loss: 0.00003325
Iteration 116/1000 | Loss: 0.00003325
Iteration 117/1000 | Loss: 0.00003325
Iteration 118/1000 | Loss: 0.00003325
Iteration 119/1000 | Loss: 0.00003325
Iteration 120/1000 | Loss: 0.00003325
Iteration 121/1000 | Loss: 0.00003325
Iteration 122/1000 | Loss: 0.00003325
Iteration 123/1000 | Loss: 0.00003325
Iteration 124/1000 | Loss: 0.00003325
Iteration 125/1000 | Loss: 0.00003325
Iteration 126/1000 | Loss: 0.00003325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [3.324973295093514e-05, 3.324973295093514e-05, 3.324973295093514e-05, 3.324973295093514e-05, 3.324973295093514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.324973295093514e-05

Optimization complete. Final v2v error: 4.695085048675537 mm

Highest mean error: 10.59255313873291 mm for frame 215

Lowest mean error: 3.951683521270752 mm for frame 232

Saving results

Total time: 101.22344207763672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01190487
Iteration 2/25 | Loss: 0.00195938
Iteration 3/25 | Loss: 0.00127136
Iteration 4/25 | Loss: 0.00121988
Iteration 5/25 | Loss: 0.00120653
Iteration 6/25 | Loss: 0.00120177
Iteration 7/25 | Loss: 0.00120067
Iteration 8/25 | Loss: 0.00120067
Iteration 9/25 | Loss: 0.00120067
Iteration 10/25 | Loss: 0.00120067
Iteration 11/25 | Loss: 0.00120067
Iteration 12/25 | Loss: 0.00120067
Iteration 13/25 | Loss: 0.00120067
Iteration 14/25 | Loss: 0.00120067
Iteration 15/25 | Loss: 0.00120067
Iteration 16/25 | Loss: 0.00120067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012006718898192048, 0.0012006718898192048, 0.0012006718898192048, 0.0012006718898192048, 0.0012006718898192048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012006718898192048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93031090
Iteration 2/25 | Loss: 0.00044895
Iteration 3/25 | Loss: 0.00044894
Iteration 4/25 | Loss: 0.00044894
Iteration 5/25 | Loss: 0.00044894
Iteration 6/25 | Loss: 0.00044894
Iteration 7/25 | Loss: 0.00044894
Iteration 8/25 | Loss: 0.00044894
Iteration 9/25 | Loss: 0.00044894
Iteration 10/25 | Loss: 0.00044894
Iteration 11/25 | Loss: 0.00044894
Iteration 12/25 | Loss: 0.00044894
Iteration 13/25 | Loss: 0.00044894
Iteration 14/25 | Loss: 0.00044894
Iteration 15/25 | Loss: 0.00044894
Iteration 16/25 | Loss: 0.00044894
Iteration 17/25 | Loss: 0.00044894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00044894180609844625, 0.00044894180609844625, 0.00044894180609844625, 0.00044894180609844625, 0.00044894180609844625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044894180609844625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044894
Iteration 2/1000 | Loss: 0.00005805
Iteration 3/1000 | Loss: 0.00004586
Iteration 4/1000 | Loss: 0.00004344
Iteration 5/1000 | Loss: 0.00004165
Iteration 6/1000 | Loss: 0.00004042
Iteration 7/1000 | Loss: 0.00003951
Iteration 8/1000 | Loss: 0.00003878
Iteration 9/1000 | Loss: 0.00003842
Iteration 10/1000 | Loss: 0.00003812
Iteration 11/1000 | Loss: 0.00003789
Iteration 12/1000 | Loss: 0.00003782
Iteration 13/1000 | Loss: 0.00003771
Iteration 14/1000 | Loss: 0.00003769
Iteration 15/1000 | Loss: 0.00003768
Iteration 16/1000 | Loss: 0.00003767
Iteration 17/1000 | Loss: 0.00003766
Iteration 18/1000 | Loss: 0.00003765
Iteration 19/1000 | Loss: 0.00003764
Iteration 20/1000 | Loss: 0.00003760
Iteration 21/1000 | Loss: 0.00003757
Iteration 22/1000 | Loss: 0.00003757
Iteration 23/1000 | Loss: 0.00003756
Iteration 24/1000 | Loss: 0.00003756
Iteration 25/1000 | Loss: 0.00003754
Iteration 26/1000 | Loss: 0.00003754
Iteration 27/1000 | Loss: 0.00003753
Iteration 28/1000 | Loss: 0.00003753
Iteration 29/1000 | Loss: 0.00003752
Iteration 30/1000 | Loss: 0.00003752
Iteration 31/1000 | Loss: 0.00003752
Iteration 32/1000 | Loss: 0.00003751
Iteration 33/1000 | Loss: 0.00003751
Iteration 34/1000 | Loss: 0.00003751
Iteration 35/1000 | Loss: 0.00003750
Iteration 36/1000 | Loss: 0.00003750
Iteration 37/1000 | Loss: 0.00003750
Iteration 38/1000 | Loss: 0.00003749
Iteration 39/1000 | Loss: 0.00003749
Iteration 40/1000 | Loss: 0.00003749
Iteration 41/1000 | Loss: 0.00003748
Iteration 42/1000 | Loss: 0.00003748
Iteration 43/1000 | Loss: 0.00003748
Iteration 44/1000 | Loss: 0.00003747
Iteration 45/1000 | Loss: 0.00003747
Iteration 46/1000 | Loss: 0.00003747
Iteration 47/1000 | Loss: 0.00003747
Iteration 48/1000 | Loss: 0.00003747
Iteration 49/1000 | Loss: 0.00003747
Iteration 50/1000 | Loss: 0.00003747
Iteration 51/1000 | Loss: 0.00003747
Iteration 52/1000 | Loss: 0.00003747
Iteration 53/1000 | Loss: 0.00003746
Iteration 54/1000 | Loss: 0.00003746
Iteration 55/1000 | Loss: 0.00003746
Iteration 56/1000 | Loss: 0.00003745
Iteration 57/1000 | Loss: 0.00003745
Iteration 58/1000 | Loss: 0.00003745
Iteration 59/1000 | Loss: 0.00003744
Iteration 60/1000 | Loss: 0.00003744
Iteration 61/1000 | Loss: 0.00003744
Iteration 62/1000 | Loss: 0.00003744
Iteration 63/1000 | Loss: 0.00003744
Iteration 64/1000 | Loss: 0.00003743
Iteration 65/1000 | Loss: 0.00003743
Iteration 66/1000 | Loss: 0.00003743
Iteration 67/1000 | Loss: 0.00003743
Iteration 68/1000 | Loss: 0.00003743
Iteration 69/1000 | Loss: 0.00003743
Iteration 70/1000 | Loss: 0.00003742
Iteration 71/1000 | Loss: 0.00003742
Iteration 72/1000 | Loss: 0.00003742
Iteration 73/1000 | Loss: 0.00003742
Iteration 74/1000 | Loss: 0.00003742
Iteration 75/1000 | Loss: 0.00003742
Iteration 76/1000 | Loss: 0.00003741
Iteration 77/1000 | Loss: 0.00003741
Iteration 78/1000 | Loss: 0.00003741
Iteration 79/1000 | Loss: 0.00003741
Iteration 80/1000 | Loss: 0.00003741
Iteration 81/1000 | Loss: 0.00003741
Iteration 82/1000 | Loss: 0.00003740
Iteration 83/1000 | Loss: 0.00003740
Iteration 84/1000 | Loss: 0.00003740
Iteration 85/1000 | Loss: 0.00003740
Iteration 86/1000 | Loss: 0.00003740
Iteration 87/1000 | Loss: 0.00003740
Iteration 88/1000 | Loss: 0.00003740
Iteration 89/1000 | Loss: 0.00003740
Iteration 90/1000 | Loss: 0.00003740
Iteration 91/1000 | Loss: 0.00003740
Iteration 92/1000 | Loss: 0.00003739
Iteration 93/1000 | Loss: 0.00003739
Iteration 94/1000 | Loss: 0.00003739
Iteration 95/1000 | Loss: 0.00003739
Iteration 96/1000 | Loss: 0.00003739
Iteration 97/1000 | Loss: 0.00003739
Iteration 98/1000 | Loss: 0.00003739
Iteration 99/1000 | Loss: 0.00003739
Iteration 100/1000 | Loss: 0.00003739
Iteration 101/1000 | Loss: 0.00003739
Iteration 102/1000 | Loss: 0.00003739
Iteration 103/1000 | Loss: 0.00003739
Iteration 104/1000 | Loss: 0.00003739
Iteration 105/1000 | Loss: 0.00003739
Iteration 106/1000 | Loss: 0.00003739
Iteration 107/1000 | Loss: 0.00003738
Iteration 108/1000 | Loss: 0.00003738
Iteration 109/1000 | Loss: 0.00003738
Iteration 110/1000 | Loss: 0.00003738
Iteration 111/1000 | Loss: 0.00003738
Iteration 112/1000 | Loss: 0.00003738
Iteration 113/1000 | Loss: 0.00003738
Iteration 114/1000 | Loss: 0.00003738
Iteration 115/1000 | Loss: 0.00003738
Iteration 116/1000 | Loss: 0.00003738
Iteration 117/1000 | Loss: 0.00003738
Iteration 118/1000 | Loss: 0.00003738
Iteration 119/1000 | Loss: 0.00003738
Iteration 120/1000 | Loss: 0.00003738
Iteration 121/1000 | Loss: 0.00003737
Iteration 122/1000 | Loss: 0.00003737
Iteration 123/1000 | Loss: 0.00003737
Iteration 124/1000 | Loss: 0.00003737
Iteration 125/1000 | Loss: 0.00003737
Iteration 126/1000 | Loss: 0.00003737
Iteration 127/1000 | Loss: 0.00003737
Iteration 128/1000 | Loss: 0.00003737
Iteration 129/1000 | Loss: 0.00003737
Iteration 130/1000 | Loss: 0.00003737
Iteration 131/1000 | Loss: 0.00003737
Iteration 132/1000 | Loss: 0.00003737
Iteration 133/1000 | Loss: 0.00003736
Iteration 134/1000 | Loss: 0.00003736
Iteration 135/1000 | Loss: 0.00003736
Iteration 136/1000 | Loss: 0.00003736
Iteration 137/1000 | Loss: 0.00003736
Iteration 138/1000 | Loss: 0.00003736
Iteration 139/1000 | Loss: 0.00003736
Iteration 140/1000 | Loss: 0.00003736
Iteration 141/1000 | Loss: 0.00003736
Iteration 142/1000 | Loss: 0.00003736
Iteration 143/1000 | Loss: 0.00003735
Iteration 144/1000 | Loss: 0.00003735
Iteration 145/1000 | Loss: 0.00003735
Iteration 146/1000 | Loss: 0.00003735
Iteration 147/1000 | Loss: 0.00003735
Iteration 148/1000 | Loss: 0.00003735
Iteration 149/1000 | Loss: 0.00003735
Iteration 150/1000 | Loss: 0.00003735
Iteration 151/1000 | Loss: 0.00003735
Iteration 152/1000 | Loss: 0.00003735
Iteration 153/1000 | Loss: 0.00003735
Iteration 154/1000 | Loss: 0.00003735
Iteration 155/1000 | Loss: 0.00003735
Iteration 156/1000 | Loss: 0.00003735
Iteration 157/1000 | Loss: 0.00003735
Iteration 158/1000 | Loss: 0.00003735
Iteration 159/1000 | Loss: 0.00003735
Iteration 160/1000 | Loss: 0.00003735
Iteration 161/1000 | Loss: 0.00003735
Iteration 162/1000 | Loss: 0.00003735
Iteration 163/1000 | Loss: 0.00003735
Iteration 164/1000 | Loss: 0.00003735
Iteration 165/1000 | Loss: 0.00003735
Iteration 166/1000 | Loss: 0.00003735
Iteration 167/1000 | Loss: 0.00003735
Iteration 168/1000 | Loss: 0.00003735
Iteration 169/1000 | Loss: 0.00003735
Iteration 170/1000 | Loss: 0.00003735
Iteration 171/1000 | Loss: 0.00003735
Iteration 172/1000 | Loss: 0.00003735
Iteration 173/1000 | Loss: 0.00003735
Iteration 174/1000 | Loss: 0.00003735
Iteration 175/1000 | Loss: 0.00003735
Iteration 176/1000 | Loss: 0.00003735
Iteration 177/1000 | Loss: 0.00003735
Iteration 178/1000 | Loss: 0.00003735
Iteration 179/1000 | Loss: 0.00003735
Iteration 180/1000 | Loss: 0.00003735
Iteration 181/1000 | Loss: 0.00003735
Iteration 182/1000 | Loss: 0.00003735
Iteration 183/1000 | Loss: 0.00003735
Iteration 184/1000 | Loss: 0.00003735
Iteration 185/1000 | Loss: 0.00003735
Iteration 186/1000 | Loss: 0.00003735
Iteration 187/1000 | Loss: 0.00003735
Iteration 188/1000 | Loss: 0.00003735
Iteration 189/1000 | Loss: 0.00003735
Iteration 190/1000 | Loss: 0.00003735
Iteration 191/1000 | Loss: 0.00003735
Iteration 192/1000 | Loss: 0.00003735
Iteration 193/1000 | Loss: 0.00003735
Iteration 194/1000 | Loss: 0.00003735
Iteration 195/1000 | Loss: 0.00003735
Iteration 196/1000 | Loss: 0.00003735
Iteration 197/1000 | Loss: 0.00003735
Iteration 198/1000 | Loss: 0.00003735
Iteration 199/1000 | Loss: 0.00003735
Iteration 200/1000 | Loss: 0.00003735
Iteration 201/1000 | Loss: 0.00003735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [3.7354056985350326e-05, 3.7354056985350326e-05, 3.7354056985350326e-05, 3.7354056985350326e-05, 3.7354056985350326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7354056985350326e-05

Optimization complete. Final v2v error: 5.156726360321045 mm

Highest mean error: 5.947686195373535 mm for frame 120

Lowest mean error: 4.266932010650635 mm for frame 41

Saving results

Total time: 40.18189764022827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008096
Iteration 2/25 | Loss: 0.00177721
Iteration 3/25 | Loss: 0.00133771
Iteration 4/25 | Loss: 0.00125122
Iteration 5/25 | Loss: 0.00122467
Iteration 6/25 | Loss: 0.00120267
Iteration 7/25 | Loss: 0.00120633
Iteration 8/25 | Loss: 0.00118709
Iteration 9/25 | Loss: 0.00118534
Iteration 10/25 | Loss: 0.00117944
Iteration 11/25 | Loss: 0.00117848
Iteration 12/25 | Loss: 0.00117836
Iteration 13/25 | Loss: 0.00117836
Iteration 14/25 | Loss: 0.00117835
Iteration 15/25 | Loss: 0.00117835
Iteration 16/25 | Loss: 0.00117835
Iteration 17/25 | Loss: 0.00117835
Iteration 18/25 | Loss: 0.00117835
Iteration 19/25 | Loss: 0.00117835
Iteration 20/25 | Loss: 0.00117835
Iteration 21/25 | Loss: 0.00117835
Iteration 22/25 | Loss: 0.00117834
Iteration 23/25 | Loss: 0.00117834
Iteration 24/25 | Loss: 0.00117834
Iteration 25/25 | Loss: 0.00117834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15474510
Iteration 2/25 | Loss: 0.00067335
Iteration 3/25 | Loss: 0.00067335
Iteration 4/25 | Loss: 0.00067334
Iteration 5/25 | Loss: 0.00067334
Iteration 6/25 | Loss: 0.00067334
Iteration 7/25 | Loss: 0.00067334
Iteration 8/25 | Loss: 0.00067334
Iteration 9/25 | Loss: 0.00067334
Iteration 10/25 | Loss: 0.00067334
Iteration 11/25 | Loss: 0.00067334
Iteration 12/25 | Loss: 0.00067334
Iteration 13/25 | Loss: 0.00067334
Iteration 14/25 | Loss: 0.00067334
Iteration 15/25 | Loss: 0.00067334
Iteration 16/25 | Loss: 0.00067334
Iteration 17/25 | Loss: 0.00067334
Iteration 18/25 | Loss: 0.00067334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006733432528562844, 0.0006733432528562844, 0.0006733432528562844, 0.0006733432528562844, 0.0006733432528562844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006733432528562844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067334
Iteration 2/1000 | Loss: 0.00004841
Iteration 3/1000 | Loss: 0.00003826
Iteration 4/1000 | Loss: 0.00003262
Iteration 5/1000 | Loss: 0.00003082
Iteration 6/1000 | Loss: 0.00002978
Iteration 7/1000 | Loss: 0.00002921
Iteration 8/1000 | Loss: 0.00002859
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002773
Iteration 11/1000 | Loss: 0.00002752
Iteration 12/1000 | Loss: 0.00002748
Iteration 13/1000 | Loss: 0.00002747
Iteration 14/1000 | Loss: 0.00002745
Iteration 15/1000 | Loss: 0.00002745
Iteration 16/1000 | Loss: 0.00002744
Iteration 17/1000 | Loss: 0.00002744
Iteration 18/1000 | Loss: 0.00002741
Iteration 19/1000 | Loss: 0.00002738
Iteration 20/1000 | Loss: 0.00002737
Iteration 21/1000 | Loss: 0.00002730
Iteration 22/1000 | Loss: 0.00002728
Iteration 23/1000 | Loss: 0.00002727
Iteration 24/1000 | Loss: 0.00002726
Iteration 25/1000 | Loss: 0.00002725
Iteration 26/1000 | Loss: 0.00002723
Iteration 27/1000 | Loss: 0.00002722
Iteration 28/1000 | Loss: 0.00002721
Iteration 29/1000 | Loss: 0.00002721
Iteration 30/1000 | Loss: 0.00002720
Iteration 31/1000 | Loss: 0.00002720
Iteration 32/1000 | Loss: 0.00002720
Iteration 33/1000 | Loss: 0.00002719
Iteration 34/1000 | Loss: 0.00002719
Iteration 35/1000 | Loss: 0.00002718
Iteration 36/1000 | Loss: 0.00002718
Iteration 37/1000 | Loss: 0.00002717
Iteration 38/1000 | Loss: 0.00002717
Iteration 39/1000 | Loss: 0.00002717
Iteration 40/1000 | Loss: 0.00002716
Iteration 41/1000 | Loss: 0.00002716
Iteration 42/1000 | Loss: 0.00002716
Iteration 43/1000 | Loss: 0.00002715
Iteration 44/1000 | Loss: 0.00002715
Iteration 45/1000 | Loss: 0.00002715
Iteration 46/1000 | Loss: 0.00002714
Iteration 47/1000 | Loss: 0.00002714
Iteration 48/1000 | Loss: 0.00002713
Iteration 49/1000 | Loss: 0.00002713
Iteration 50/1000 | Loss: 0.00002713
Iteration 51/1000 | Loss: 0.00002712
Iteration 52/1000 | Loss: 0.00002712
Iteration 53/1000 | Loss: 0.00002712
Iteration 54/1000 | Loss: 0.00002711
Iteration 55/1000 | Loss: 0.00002711
Iteration 56/1000 | Loss: 0.00002710
Iteration 57/1000 | Loss: 0.00002709
Iteration 58/1000 | Loss: 0.00002709
Iteration 59/1000 | Loss: 0.00002709
Iteration 60/1000 | Loss: 0.00002709
Iteration 61/1000 | Loss: 0.00002709
Iteration 62/1000 | Loss: 0.00002709
Iteration 63/1000 | Loss: 0.00002709
Iteration 64/1000 | Loss: 0.00002708
Iteration 65/1000 | Loss: 0.00002708
Iteration 66/1000 | Loss: 0.00002708
Iteration 67/1000 | Loss: 0.00002708
Iteration 68/1000 | Loss: 0.00002708
Iteration 69/1000 | Loss: 0.00002707
Iteration 70/1000 | Loss: 0.00002707
Iteration 71/1000 | Loss: 0.00002706
Iteration 72/1000 | Loss: 0.00002706
Iteration 73/1000 | Loss: 0.00002706
Iteration 74/1000 | Loss: 0.00002705
Iteration 75/1000 | Loss: 0.00002705
Iteration 76/1000 | Loss: 0.00002704
Iteration 77/1000 | Loss: 0.00002704
Iteration 78/1000 | Loss: 0.00002704
Iteration 79/1000 | Loss: 0.00002703
Iteration 80/1000 | Loss: 0.00002703
Iteration 81/1000 | Loss: 0.00002703
Iteration 82/1000 | Loss: 0.00002703
Iteration 83/1000 | Loss: 0.00002703
Iteration 84/1000 | Loss: 0.00002703
Iteration 85/1000 | Loss: 0.00002702
Iteration 86/1000 | Loss: 0.00002702
Iteration 87/1000 | Loss: 0.00002702
Iteration 88/1000 | Loss: 0.00002702
Iteration 89/1000 | Loss: 0.00002702
Iteration 90/1000 | Loss: 0.00002702
Iteration 91/1000 | Loss: 0.00002702
Iteration 92/1000 | Loss: 0.00002702
Iteration 93/1000 | Loss: 0.00002702
Iteration 94/1000 | Loss: 0.00002702
Iteration 95/1000 | Loss: 0.00002702
Iteration 96/1000 | Loss: 0.00002702
Iteration 97/1000 | Loss: 0.00002702
Iteration 98/1000 | Loss: 0.00002702
Iteration 99/1000 | Loss: 0.00002702
Iteration 100/1000 | Loss: 0.00002702
Iteration 101/1000 | Loss: 0.00002702
Iteration 102/1000 | Loss: 0.00002702
Iteration 103/1000 | Loss: 0.00002702
Iteration 104/1000 | Loss: 0.00002702
Iteration 105/1000 | Loss: 0.00002702
Iteration 106/1000 | Loss: 0.00002702
Iteration 107/1000 | Loss: 0.00002702
Iteration 108/1000 | Loss: 0.00002702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.701731318666134e-05, 2.701731318666134e-05, 2.701731318666134e-05, 2.701731318666134e-05, 2.701731318666134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.701731318666134e-05

Optimization complete. Final v2v error: 4.442047595977783 mm

Highest mean error: 5.063805103302002 mm for frame 97

Lowest mean error: 4.0743489265441895 mm for frame 184

Saving results

Total time: 47.009063482284546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00579281
Iteration 2/25 | Loss: 0.00139698
Iteration 3/25 | Loss: 0.00114347
Iteration 4/25 | Loss: 0.00112212
Iteration 5/25 | Loss: 0.00111765
Iteration 6/25 | Loss: 0.00111692
Iteration 7/25 | Loss: 0.00111692
Iteration 8/25 | Loss: 0.00111692
Iteration 9/25 | Loss: 0.00111689
Iteration 10/25 | Loss: 0.00111689
Iteration 11/25 | Loss: 0.00111689
Iteration 12/25 | Loss: 0.00111689
Iteration 13/25 | Loss: 0.00111689
Iteration 14/25 | Loss: 0.00111689
Iteration 15/25 | Loss: 0.00111689
Iteration 16/25 | Loss: 0.00111689
Iteration 17/25 | Loss: 0.00111689
Iteration 18/25 | Loss: 0.00111689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011168946512043476, 0.0011168946512043476, 0.0011168946512043476, 0.0011168946512043476, 0.0011168946512043476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011168946512043476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36607111
Iteration 2/25 | Loss: 0.00066187
Iteration 3/25 | Loss: 0.00066187
Iteration 4/25 | Loss: 0.00066187
Iteration 5/25 | Loss: 0.00066187
Iteration 6/25 | Loss: 0.00066187
Iteration 7/25 | Loss: 0.00066187
Iteration 8/25 | Loss: 0.00066187
Iteration 9/25 | Loss: 0.00066187
Iteration 10/25 | Loss: 0.00066187
Iteration 11/25 | Loss: 0.00066187
Iteration 12/25 | Loss: 0.00066187
Iteration 13/25 | Loss: 0.00066187
Iteration 14/25 | Loss: 0.00066187
Iteration 15/25 | Loss: 0.00066187
Iteration 16/25 | Loss: 0.00066187
Iteration 17/25 | Loss: 0.00066187
Iteration 18/25 | Loss: 0.00066187
Iteration 19/25 | Loss: 0.00066187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006618686602450907, 0.0006618686602450907, 0.0006618686602450907, 0.0006618686602450907, 0.0006618686602450907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006618686602450907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066187
Iteration 2/1000 | Loss: 0.00004369
Iteration 3/1000 | Loss: 0.00003131
Iteration 4/1000 | Loss: 0.00002897
Iteration 5/1000 | Loss: 0.00002787
Iteration 6/1000 | Loss: 0.00002694
Iteration 7/1000 | Loss: 0.00002632
Iteration 8/1000 | Loss: 0.00002591
Iteration 9/1000 | Loss: 0.00002562
Iteration 10/1000 | Loss: 0.00002542
Iteration 11/1000 | Loss: 0.00002527
Iteration 12/1000 | Loss: 0.00002526
Iteration 13/1000 | Loss: 0.00002526
Iteration 14/1000 | Loss: 0.00002525
Iteration 15/1000 | Loss: 0.00002524
Iteration 16/1000 | Loss: 0.00002512
Iteration 17/1000 | Loss: 0.00002508
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002504
Iteration 20/1000 | Loss: 0.00002504
Iteration 21/1000 | Loss: 0.00002504
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002503
Iteration 24/1000 | Loss: 0.00002503
Iteration 25/1000 | Loss: 0.00002503
Iteration 26/1000 | Loss: 0.00002503
Iteration 27/1000 | Loss: 0.00002503
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002502
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002502
Iteration 33/1000 | Loss: 0.00002501
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002501
Iteration 37/1000 | Loss: 0.00002501
Iteration 38/1000 | Loss: 0.00002500
Iteration 39/1000 | Loss: 0.00002499
Iteration 40/1000 | Loss: 0.00002499
Iteration 41/1000 | Loss: 0.00002499
Iteration 42/1000 | Loss: 0.00002499
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002499
Iteration 45/1000 | Loss: 0.00002499
Iteration 46/1000 | Loss: 0.00002499
Iteration 47/1000 | Loss: 0.00002499
Iteration 48/1000 | Loss: 0.00002499
Iteration 49/1000 | Loss: 0.00002499
Iteration 50/1000 | Loss: 0.00002498
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00002498
Iteration 53/1000 | Loss: 0.00002497
Iteration 54/1000 | Loss: 0.00002497
Iteration 55/1000 | Loss: 0.00002497
Iteration 56/1000 | Loss: 0.00002497
Iteration 57/1000 | Loss: 0.00002497
Iteration 58/1000 | Loss: 0.00002497
Iteration 59/1000 | Loss: 0.00002497
Iteration 60/1000 | Loss: 0.00002496
Iteration 61/1000 | Loss: 0.00002496
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002496
Iteration 64/1000 | Loss: 0.00002495
Iteration 65/1000 | Loss: 0.00002495
Iteration 66/1000 | Loss: 0.00002495
Iteration 67/1000 | Loss: 0.00002495
Iteration 68/1000 | Loss: 0.00002495
Iteration 69/1000 | Loss: 0.00002495
Iteration 70/1000 | Loss: 0.00002495
Iteration 71/1000 | Loss: 0.00002495
Iteration 72/1000 | Loss: 0.00002495
Iteration 73/1000 | Loss: 0.00002495
Iteration 74/1000 | Loss: 0.00002494
Iteration 75/1000 | Loss: 0.00002494
Iteration 76/1000 | Loss: 0.00002494
Iteration 77/1000 | Loss: 0.00002494
Iteration 78/1000 | Loss: 0.00002493
Iteration 79/1000 | Loss: 0.00002493
Iteration 80/1000 | Loss: 0.00002493
Iteration 81/1000 | Loss: 0.00002493
Iteration 82/1000 | Loss: 0.00002492
Iteration 83/1000 | Loss: 0.00002492
Iteration 84/1000 | Loss: 0.00002492
Iteration 85/1000 | Loss: 0.00002492
Iteration 86/1000 | Loss: 0.00002492
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002491
Iteration 89/1000 | Loss: 0.00002491
Iteration 90/1000 | Loss: 0.00002491
Iteration 91/1000 | Loss: 0.00002491
Iteration 92/1000 | Loss: 0.00002491
Iteration 93/1000 | Loss: 0.00002491
Iteration 94/1000 | Loss: 0.00002491
Iteration 95/1000 | Loss: 0.00002491
Iteration 96/1000 | Loss: 0.00002491
Iteration 97/1000 | Loss: 0.00002491
Iteration 98/1000 | Loss: 0.00002491
Iteration 99/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.490652332198806e-05, 2.490652332198806e-05, 2.490652332198806e-05, 2.490652332198806e-05, 2.490652332198806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.490652332198806e-05

Optimization complete. Final v2v error: 4.286201000213623 mm

Highest mean error: 4.718899250030518 mm for frame 60

Lowest mean error: 3.8369944095611572 mm for frame 0

Saving results

Total time: 34.15193581581116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893253
Iteration 2/25 | Loss: 0.00127524
Iteration 3/25 | Loss: 0.00112727
Iteration 4/25 | Loss: 0.00110214
Iteration 5/25 | Loss: 0.00109322
Iteration 6/25 | Loss: 0.00109165
Iteration 7/25 | Loss: 0.00109137
Iteration 8/25 | Loss: 0.00109137
Iteration 9/25 | Loss: 0.00109137
Iteration 10/25 | Loss: 0.00109137
Iteration 11/25 | Loss: 0.00109137
Iteration 12/25 | Loss: 0.00109137
Iteration 13/25 | Loss: 0.00109137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001091374084353447, 0.001091374084353447, 0.001091374084353447, 0.001091374084353447, 0.001091374084353447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001091374084353447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.38758755
Iteration 2/25 | Loss: 0.00057795
Iteration 3/25 | Loss: 0.00057795
Iteration 4/25 | Loss: 0.00057795
Iteration 5/25 | Loss: 0.00057794
Iteration 6/25 | Loss: 0.00057794
Iteration 7/25 | Loss: 0.00057794
Iteration 8/25 | Loss: 0.00057794
Iteration 9/25 | Loss: 0.00057794
Iteration 10/25 | Loss: 0.00057794
Iteration 11/25 | Loss: 0.00057794
Iteration 12/25 | Loss: 0.00057794
Iteration 13/25 | Loss: 0.00057794
Iteration 14/25 | Loss: 0.00057794
Iteration 15/25 | Loss: 0.00057794
Iteration 16/25 | Loss: 0.00057794
Iteration 17/25 | Loss: 0.00057794
Iteration 18/25 | Loss: 0.00057794
Iteration 19/25 | Loss: 0.00057794
Iteration 20/25 | Loss: 0.00057794
Iteration 21/25 | Loss: 0.00057794
Iteration 22/25 | Loss: 0.00057794
Iteration 23/25 | Loss: 0.00057794
Iteration 24/25 | Loss: 0.00057794
Iteration 25/25 | Loss: 0.00057794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057794
Iteration 2/1000 | Loss: 0.00004341
Iteration 3/1000 | Loss: 0.00003047
Iteration 4/1000 | Loss: 0.00002769
Iteration 5/1000 | Loss: 0.00002624
Iteration 6/1000 | Loss: 0.00002552
Iteration 7/1000 | Loss: 0.00002496
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002416
Iteration 10/1000 | Loss: 0.00002399
Iteration 11/1000 | Loss: 0.00002389
Iteration 12/1000 | Loss: 0.00002383
Iteration 13/1000 | Loss: 0.00002377
Iteration 14/1000 | Loss: 0.00002377
Iteration 15/1000 | Loss: 0.00002376
Iteration 16/1000 | Loss: 0.00002376
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002375
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002373
Iteration 22/1000 | Loss: 0.00002373
Iteration 23/1000 | Loss: 0.00002372
Iteration 24/1000 | Loss: 0.00002371
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002369
Iteration 27/1000 | Loss: 0.00002368
Iteration 28/1000 | Loss: 0.00002368
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002367
Iteration 31/1000 | Loss: 0.00002366
Iteration 32/1000 | Loss: 0.00002366
Iteration 33/1000 | Loss: 0.00002365
Iteration 34/1000 | Loss: 0.00002365
Iteration 35/1000 | Loss: 0.00002364
Iteration 36/1000 | Loss: 0.00002364
Iteration 37/1000 | Loss: 0.00002363
Iteration 38/1000 | Loss: 0.00002363
Iteration 39/1000 | Loss: 0.00002362
Iteration 40/1000 | Loss: 0.00002362
Iteration 41/1000 | Loss: 0.00002361
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002360
Iteration 44/1000 | Loss: 0.00002359
Iteration 45/1000 | Loss: 0.00002359
Iteration 46/1000 | Loss: 0.00002359
Iteration 47/1000 | Loss: 0.00002358
Iteration 48/1000 | Loss: 0.00002358
Iteration 49/1000 | Loss: 0.00002358
Iteration 50/1000 | Loss: 0.00002358
Iteration 51/1000 | Loss: 0.00002357
Iteration 52/1000 | Loss: 0.00002357
Iteration 53/1000 | Loss: 0.00002357
Iteration 54/1000 | Loss: 0.00002357
Iteration 55/1000 | Loss: 0.00002355
Iteration 56/1000 | Loss: 0.00002355
Iteration 57/1000 | Loss: 0.00002355
Iteration 58/1000 | Loss: 0.00002354
Iteration 59/1000 | Loss: 0.00002354
Iteration 60/1000 | Loss: 0.00002354
Iteration 61/1000 | Loss: 0.00002354
Iteration 62/1000 | Loss: 0.00002354
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002353
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002351
Iteration 69/1000 | Loss: 0.00002351
Iteration 70/1000 | Loss: 0.00002350
Iteration 71/1000 | Loss: 0.00002350
Iteration 72/1000 | Loss: 0.00002350
Iteration 73/1000 | Loss: 0.00002350
Iteration 74/1000 | Loss: 0.00002350
Iteration 75/1000 | Loss: 0.00002350
Iteration 76/1000 | Loss: 0.00002350
Iteration 77/1000 | Loss: 0.00002349
Iteration 78/1000 | Loss: 0.00002349
Iteration 79/1000 | Loss: 0.00002348
Iteration 80/1000 | Loss: 0.00002348
Iteration 81/1000 | Loss: 0.00002348
Iteration 82/1000 | Loss: 0.00002347
Iteration 83/1000 | Loss: 0.00002347
Iteration 84/1000 | Loss: 0.00002347
Iteration 85/1000 | Loss: 0.00002347
Iteration 86/1000 | Loss: 0.00002347
Iteration 87/1000 | Loss: 0.00002347
Iteration 88/1000 | Loss: 0.00002346
Iteration 89/1000 | Loss: 0.00002346
Iteration 90/1000 | Loss: 0.00002345
Iteration 91/1000 | Loss: 0.00002345
Iteration 92/1000 | Loss: 0.00002345
Iteration 93/1000 | Loss: 0.00002345
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002344
Iteration 96/1000 | Loss: 0.00002344
Iteration 97/1000 | Loss: 0.00002344
Iteration 98/1000 | Loss: 0.00002344
Iteration 99/1000 | Loss: 0.00002344
Iteration 100/1000 | Loss: 0.00002344
Iteration 101/1000 | Loss: 0.00002344
Iteration 102/1000 | Loss: 0.00002343
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00002343
Iteration 105/1000 | Loss: 0.00002343
Iteration 106/1000 | Loss: 0.00002343
Iteration 107/1000 | Loss: 0.00002343
Iteration 108/1000 | Loss: 0.00002343
Iteration 109/1000 | Loss: 0.00002343
Iteration 110/1000 | Loss: 0.00002343
Iteration 111/1000 | Loss: 0.00002343
Iteration 112/1000 | Loss: 0.00002343
Iteration 113/1000 | Loss: 0.00002342
Iteration 114/1000 | Loss: 0.00002342
Iteration 115/1000 | Loss: 0.00002342
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002342
Iteration 118/1000 | Loss: 0.00002342
Iteration 119/1000 | Loss: 0.00002341
Iteration 120/1000 | Loss: 0.00002341
Iteration 121/1000 | Loss: 0.00002341
Iteration 122/1000 | Loss: 0.00002340
Iteration 123/1000 | Loss: 0.00002340
Iteration 124/1000 | Loss: 0.00002340
Iteration 125/1000 | Loss: 0.00002340
Iteration 126/1000 | Loss: 0.00002339
Iteration 127/1000 | Loss: 0.00002339
Iteration 128/1000 | Loss: 0.00002339
Iteration 129/1000 | Loss: 0.00002339
Iteration 130/1000 | Loss: 0.00002339
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00002339
Iteration 133/1000 | Loss: 0.00002339
Iteration 134/1000 | Loss: 0.00002339
Iteration 135/1000 | Loss: 0.00002339
Iteration 136/1000 | Loss: 0.00002339
Iteration 137/1000 | Loss: 0.00002338
Iteration 138/1000 | Loss: 0.00002338
Iteration 139/1000 | Loss: 0.00002338
Iteration 140/1000 | Loss: 0.00002338
Iteration 141/1000 | Loss: 0.00002338
Iteration 142/1000 | Loss: 0.00002338
Iteration 143/1000 | Loss: 0.00002338
Iteration 144/1000 | Loss: 0.00002337
Iteration 145/1000 | Loss: 0.00002337
Iteration 146/1000 | Loss: 0.00002337
Iteration 147/1000 | Loss: 0.00002337
Iteration 148/1000 | Loss: 0.00002337
Iteration 149/1000 | Loss: 0.00002337
Iteration 150/1000 | Loss: 0.00002337
Iteration 151/1000 | Loss: 0.00002337
Iteration 152/1000 | Loss: 0.00002337
Iteration 153/1000 | Loss: 0.00002337
Iteration 154/1000 | Loss: 0.00002337
Iteration 155/1000 | Loss: 0.00002337
Iteration 156/1000 | Loss: 0.00002337
Iteration 157/1000 | Loss: 0.00002337
Iteration 158/1000 | Loss: 0.00002337
Iteration 159/1000 | Loss: 0.00002337
Iteration 160/1000 | Loss: 0.00002337
Iteration 161/1000 | Loss: 0.00002337
Iteration 162/1000 | Loss: 0.00002337
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002337
Iteration 165/1000 | Loss: 0.00002337
Iteration 166/1000 | Loss: 0.00002337
Iteration 167/1000 | Loss: 0.00002337
Iteration 168/1000 | Loss: 0.00002337
Iteration 169/1000 | Loss: 0.00002337
Iteration 170/1000 | Loss: 0.00002337
Iteration 171/1000 | Loss: 0.00002337
Iteration 172/1000 | Loss: 0.00002337
Iteration 173/1000 | Loss: 0.00002337
Iteration 174/1000 | Loss: 0.00002337
Iteration 175/1000 | Loss: 0.00002337
Iteration 176/1000 | Loss: 0.00002337
Iteration 177/1000 | Loss: 0.00002337
Iteration 178/1000 | Loss: 0.00002337
Iteration 179/1000 | Loss: 0.00002337
Iteration 180/1000 | Loss: 0.00002337
Iteration 181/1000 | Loss: 0.00002337
Iteration 182/1000 | Loss: 0.00002337
Iteration 183/1000 | Loss: 0.00002337
Iteration 184/1000 | Loss: 0.00002337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.3366466848528944e-05, 2.3366466848528944e-05, 2.3366466848528944e-05, 2.3366466848528944e-05, 2.3366466848528944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3366466848528944e-05

Optimization complete. Final v2v error: 4.2127299308776855 mm

Highest mean error: 5.260133266448975 mm for frame 84

Lowest mean error: 3.7168853282928467 mm for frame 107

Saving results

Total time: 37.96440005302429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848335
Iteration 2/25 | Loss: 0.00206685
Iteration 3/25 | Loss: 0.00127204
Iteration 4/25 | Loss: 0.00117244
Iteration 5/25 | Loss: 0.00111388
Iteration 6/25 | Loss: 0.00111399
Iteration 7/25 | Loss: 0.00107336
Iteration 8/25 | Loss: 0.00104005
Iteration 9/25 | Loss: 0.00102877
Iteration 10/25 | Loss: 0.00101574
Iteration 11/25 | Loss: 0.00101759
Iteration 12/25 | Loss: 0.00099895
Iteration 13/25 | Loss: 0.00099894
Iteration 14/25 | Loss: 0.00098987
Iteration 15/25 | Loss: 0.00098370
Iteration 16/25 | Loss: 0.00098191
Iteration 17/25 | Loss: 0.00098153
Iteration 18/25 | Loss: 0.00098138
Iteration 19/25 | Loss: 0.00098127
Iteration 20/25 | Loss: 0.00098125
Iteration 21/25 | Loss: 0.00098125
Iteration 22/25 | Loss: 0.00098125
Iteration 23/25 | Loss: 0.00098125
Iteration 24/25 | Loss: 0.00098125
Iteration 25/25 | Loss: 0.00098125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07467771
Iteration 2/25 | Loss: 0.00054634
Iteration 3/25 | Loss: 0.00054634
Iteration 4/25 | Loss: 0.00054634
Iteration 5/25 | Loss: 0.00054633
Iteration 6/25 | Loss: 0.00054633
Iteration 7/25 | Loss: 0.00054633
Iteration 8/25 | Loss: 0.00054633
Iteration 9/25 | Loss: 0.00054633
Iteration 10/25 | Loss: 0.00054633
Iteration 11/25 | Loss: 0.00054633
Iteration 12/25 | Loss: 0.00054633
Iteration 13/25 | Loss: 0.00054633
Iteration 14/25 | Loss: 0.00054633
Iteration 15/25 | Loss: 0.00054633
Iteration 16/25 | Loss: 0.00054633
Iteration 17/25 | Loss: 0.00054633
Iteration 18/25 | Loss: 0.00054633
Iteration 19/25 | Loss: 0.00054633
Iteration 20/25 | Loss: 0.00054633
Iteration 21/25 | Loss: 0.00054633
Iteration 22/25 | Loss: 0.00054633
Iteration 23/25 | Loss: 0.00054633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005463334964588284, 0.0005463334964588284, 0.0005463334964588284, 0.0005463334964588284, 0.0005463334964588284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005463334964588284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054633
Iteration 2/1000 | Loss: 0.00003396
Iteration 3/1000 | Loss: 0.00002607
Iteration 4/1000 | Loss: 0.00002384
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002232
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002163
Iteration 9/1000 | Loss: 0.00002156
Iteration 10/1000 | Loss: 0.00002147
Iteration 11/1000 | Loss: 0.00002146
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002136
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002130
Iteration 19/1000 | Loss: 0.00002130
Iteration 20/1000 | Loss: 0.00002130
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002130
Iteration 23/1000 | Loss: 0.00002130
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002125
Iteration 28/1000 | Loss: 0.00002125
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00002125
Iteration 31/1000 | Loss: 0.00002125
Iteration 32/1000 | Loss: 0.00002125
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002125
Iteration 35/1000 | Loss: 0.00002125
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002122
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002119
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002116
Iteration 49/1000 | Loss: 0.00002116
Iteration 50/1000 | Loss: 0.00002116
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002115
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00002114
Iteration 56/1000 | Loss: 0.00002113
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00002113
Iteration 59/1000 | Loss: 0.00002113
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002112
Iteration 62/1000 | Loss: 0.00002112
Iteration 63/1000 | Loss: 0.00002112
Iteration 64/1000 | Loss: 0.00002111
Iteration 65/1000 | Loss: 0.00002111
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002110
Iteration 70/1000 | Loss: 0.00002110
Iteration 71/1000 | Loss: 0.00002110
Iteration 72/1000 | Loss: 0.00002109
Iteration 73/1000 | Loss: 0.00002109
Iteration 74/1000 | Loss: 0.00002109
Iteration 75/1000 | Loss: 0.00002109
Iteration 76/1000 | Loss: 0.00002108
Iteration 77/1000 | Loss: 0.00002108
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00002108
Iteration 80/1000 | Loss: 0.00002108
Iteration 81/1000 | Loss: 0.00002108
Iteration 82/1000 | Loss: 0.00002107
Iteration 83/1000 | Loss: 0.00002107
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002107
Iteration 86/1000 | Loss: 0.00002107
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002106
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002106
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00002105
Iteration 95/1000 | Loss: 0.00002105
Iteration 96/1000 | Loss: 0.00002105
Iteration 97/1000 | Loss: 0.00002105
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002104
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002104
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002104
Iteration 107/1000 | Loss: 0.00002104
Iteration 108/1000 | Loss: 0.00002104
Iteration 109/1000 | Loss: 0.00002103
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002103
Iteration 112/1000 | Loss: 0.00002103
Iteration 113/1000 | Loss: 0.00002103
Iteration 114/1000 | Loss: 0.00002103
Iteration 115/1000 | Loss: 0.00002103
Iteration 116/1000 | Loss: 0.00002103
Iteration 117/1000 | Loss: 0.00002103
Iteration 118/1000 | Loss: 0.00002103
Iteration 119/1000 | Loss: 0.00002103
Iteration 120/1000 | Loss: 0.00002102
Iteration 121/1000 | Loss: 0.00002102
Iteration 122/1000 | Loss: 0.00002102
Iteration 123/1000 | Loss: 0.00002102
Iteration 124/1000 | Loss: 0.00002102
Iteration 125/1000 | Loss: 0.00002102
Iteration 126/1000 | Loss: 0.00002102
Iteration 127/1000 | Loss: 0.00002102
Iteration 128/1000 | Loss: 0.00002102
Iteration 129/1000 | Loss: 0.00002102
Iteration 130/1000 | Loss: 0.00002101
Iteration 131/1000 | Loss: 0.00002101
Iteration 132/1000 | Loss: 0.00002101
Iteration 133/1000 | Loss: 0.00002101
Iteration 134/1000 | Loss: 0.00002101
Iteration 135/1000 | Loss: 0.00002101
Iteration 136/1000 | Loss: 0.00002101
Iteration 137/1000 | Loss: 0.00002101
Iteration 138/1000 | Loss: 0.00002101
Iteration 139/1000 | Loss: 0.00002101
Iteration 140/1000 | Loss: 0.00002101
Iteration 141/1000 | Loss: 0.00002101
Iteration 142/1000 | Loss: 0.00002101
Iteration 143/1000 | Loss: 0.00002101
Iteration 144/1000 | Loss: 0.00002101
Iteration 145/1000 | Loss: 0.00002101
Iteration 146/1000 | Loss: 0.00002101
Iteration 147/1000 | Loss: 0.00002101
Iteration 148/1000 | Loss: 0.00002101
Iteration 149/1000 | Loss: 0.00002101
Iteration 150/1000 | Loss: 0.00002101
Iteration 151/1000 | Loss: 0.00002101
Iteration 152/1000 | Loss: 0.00002101
Iteration 153/1000 | Loss: 0.00002101
Iteration 154/1000 | Loss: 0.00002101
Iteration 155/1000 | Loss: 0.00002101
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002101
Iteration 158/1000 | Loss: 0.00002101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.1012285287724808e-05, 2.1012285287724808e-05, 2.1012285287724808e-05, 2.1012285287724808e-05, 2.1012285287724808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1012285287724808e-05

Optimization complete. Final v2v error: 3.942899465560913 mm

Highest mean error: 9.813946723937988 mm for frame 159

Lowest mean error: 3.503760576248169 mm for frame 62

Saving results

Total time: 60.2888445854187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856100
Iteration 2/25 | Loss: 0.00135931
Iteration 3/25 | Loss: 0.00115135
Iteration 4/25 | Loss: 0.00112067
Iteration 5/25 | Loss: 0.00110844
Iteration 6/25 | Loss: 0.00110504
Iteration 7/25 | Loss: 0.00110374
Iteration 8/25 | Loss: 0.00110373
Iteration 9/25 | Loss: 0.00110373
Iteration 10/25 | Loss: 0.00110373
Iteration 11/25 | Loss: 0.00110373
Iteration 12/25 | Loss: 0.00110373
Iteration 13/25 | Loss: 0.00110373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011037264484912157, 0.0011037264484912157, 0.0011037264484912157, 0.0011037264484912157, 0.0011037264484912157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011037264484912157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37931752
Iteration 2/25 | Loss: 0.00064566
Iteration 3/25 | Loss: 0.00064565
Iteration 4/25 | Loss: 0.00064565
Iteration 5/25 | Loss: 0.00064565
Iteration 6/25 | Loss: 0.00064565
Iteration 7/25 | Loss: 0.00064565
Iteration 8/25 | Loss: 0.00064565
Iteration 9/25 | Loss: 0.00064565
Iteration 10/25 | Loss: 0.00064565
Iteration 11/25 | Loss: 0.00064565
Iteration 12/25 | Loss: 0.00064565
Iteration 13/25 | Loss: 0.00064565
Iteration 14/25 | Loss: 0.00064565
Iteration 15/25 | Loss: 0.00064565
Iteration 16/25 | Loss: 0.00064565
Iteration 17/25 | Loss: 0.00064565
Iteration 18/25 | Loss: 0.00064565
Iteration 19/25 | Loss: 0.00064565
Iteration 20/25 | Loss: 0.00064565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000645646417979151, 0.000645646417979151, 0.000645646417979151, 0.000645646417979151, 0.000645646417979151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000645646417979151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064565
Iteration 2/1000 | Loss: 0.00004557
Iteration 3/1000 | Loss: 0.00003381
Iteration 4/1000 | Loss: 0.00003019
Iteration 5/1000 | Loss: 0.00002834
Iteration 6/1000 | Loss: 0.00002759
Iteration 7/1000 | Loss: 0.00002678
Iteration 8/1000 | Loss: 0.00002622
Iteration 9/1000 | Loss: 0.00002584
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002521
Iteration 13/1000 | Loss: 0.00002508
Iteration 14/1000 | Loss: 0.00002499
Iteration 15/1000 | Loss: 0.00002496
Iteration 16/1000 | Loss: 0.00002495
Iteration 17/1000 | Loss: 0.00002494
Iteration 18/1000 | Loss: 0.00002493
Iteration 19/1000 | Loss: 0.00002489
Iteration 20/1000 | Loss: 0.00002489
Iteration 21/1000 | Loss: 0.00002488
Iteration 22/1000 | Loss: 0.00002487
Iteration 23/1000 | Loss: 0.00002487
Iteration 24/1000 | Loss: 0.00002486
Iteration 25/1000 | Loss: 0.00002485
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00002484
Iteration 28/1000 | Loss: 0.00002483
Iteration 29/1000 | Loss: 0.00002483
Iteration 30/1000 | Loss: 0.00002482
Iteration 31/1000 | Loss: 0.00002482
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002480
Iteration 34/1000 | Loss: 0.00002480
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002478
Iteration 37/1000 | Loss: 0.00002478
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002474
Iteration 40/1000 | Loss: 0.00002473
Iteration 41/1000 | Loss: 0.00002473
Iteration 42/1000 | Loss: 0.00002473
Iteration 43/1000 | Loss: 0.00002473
Iteration 44/1000 | Loss: 0.00002473
Iteration 45/1000 | Loss: 0.00002472
Iteration 46/1000 | Loss: 0.00002472
Iteration 47/1000 | Loss: 0.00002471
Iteration 48/1000 | Loss: 0.00002471
Iteration 49/1000 | Loss: 0.00002471
Iteration 50/1000 | Loss: 0.00002470
Iteration 51/1000 | Loss: 0.00002470
Iteration 52/1000 | Loss: 0.00002470
Iteration 53/1000 | Loss: 0.00002470
Iteration 54/1000 | Loss: 0.00002470
Iteration 55/1000 | Loss: 0.00002470
Iteration 56/1000 | Loss: 0.00002470
Iteration 57/1000 | Loss: 0.00002470
Iteration 58/1000 | Loss: 0.00002470
Iteration 59/1000 | Loss: 0.00002470
Iteration 60/1000 | Loss: 0.00002469
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00002469
Iteration 63/1000 | Loss: 0.00002469
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00002468
Iteration 66/1000 | Loss: 0.00002468
Iteration 67/1000 | Loss: 0.00002468
Iteration 68/1000 | Loss: 0.00002468
Iteration 69/1000 | Loss: 0.00002467
Iteration 70/1000 | Loss: 0.00002467
Iteration 71/1000 | Loss: 0.00002467
Iteration 72/1000 | Loss: 0.00002467
Iteration 73/1000 | Loss: 0.00002467
Iteration 74/1000 | Loss: 0.00002467
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002467
Iteration 79/1000 | Loss: 0.00002467
Iteration 80/1000 | Loss: 0.00002467
Iteration 81/1000 | Loss: 0.00002467
Iteration 82/1000 | Loss: 0.00002466
Iteration 83/1000 | Loss: 0.00002466
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002466
Iteration 86/1000 | Loss: 0.00002466
Iteration 87/1000 | Loss: 0.00002466
Iteration 88/1000 | Loss: 0.00002466
Iteration 89/1000 | Loss: 0.00002466
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002465
Iteration 92/1000 | Loss: 0.00002465
Iteration 93/1000 | Loss: 0.00002464
Iteration 94/1000 | Loss: 0.00002464
Iteration 95/1000 | Loss: 0.00002464
Iteration 96/1000 | Loss: 0.00002464
Iteration 97/1000 | Loss: 0.00002464
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002464
Iteration 100/1000 | Loss: 0.00002463
Iteration 101/1000 | Loss: 0.00002463
Iteration 102/1000 | Loss: 0.00002463
Iteration 103/1000 | Loss: 0.00002463
Iteration 104/1000 | Loss: 0.00002463
Iteration 105/1000 | Loss: 0.00002463
Iteration 106/1000 | Loss: 0.00002463
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002462
Iteration 109/1000 | Loss: 0.00002462
Iteration 110/1000 | Loss: 0.00002462
Iteration 111/1000 | Loss: 0.00002462
Iteration 112/1000 | Loss: 0.00002462
Iteration 113/1000 | Loss: 0.00002462
Iteration 114/1000 | Loss: 0.00002462
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Iteration 117/1000 | Loss: 0.00002462
Iteration 118/1000 | Loss: 0.00002462
Iteration 119/1000 | Loss: 0.00002462
Iteration 120/1000 | Loss: 0.00002462
Iteration 121/1000 | Loss: 0.00002462
Iteration 122/1000 | Loss: 0.00002462
Iteration 123/1000 | Loss: 0.00002462
Iteration 124/1000 | Loss: 0.00002462
Iteration 125/1000 | Loss: 0.00002462
Iteration 126/1000 | Loss: 0.00002462
Iteration 127/1000 | Loss: 0.00002462
Iteration 128/1000 | Loss: 0.00002462
Iteration 129/1000 | Loss: 0.00002462
Iteration 130/1000 | Loss: 0.00002462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.46183462877525e-05, 2.46183462877525e-05, 2.46183462877525e-05, 2.46183462877525e-05, 2.46183462877525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.46183462877525e-05

Optimization complete. Final v2v error: 4.293025016784668 mm

Highest mean error: 4.860055923461914 mm for frame 108

Lowest mean error: 3.809718132019043 mm for frame 166

Saving results

Total time: 43.578038692474365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841112
Iteration 2/25 | Loss: 0.00183434
Iteration 3/25 | Loss: 0.00126844
Iteration 4/25 | Loss: 0.00116110
Iteration 5/25 | Loss: 0.00112945
Iteration 6/25 | Loss: 0.00113305
Iteration 7/25 | Loss: 0.00112188
Iteration 8/25 | Loss: 0.00111218
Iteration 9/25 | Loss: 0.00110359
Iteration 10/25 | Loss: 0.00109887
Iteration 11/25 | Loss: 0.00109565
Iteration 12/25 | Loss: 0.00109399
Iteration 13/25 | Loss: 0.00109350
Iteration 14/25 | Loss: 0.00109334
Iteration 15/25 | Loss: 0.00109332
Iteration 16/25 | Loss: 0.00109331
Iteration 17/25 | Loss: 0.00109331
Iteration 18/25 | Loss: 0.00109331
Iteration 19/25 | Loss: 0.00109330
Iteration 20/25 | Loss: 0.00109330
Iteration 21/25 | Loss: 0.00109330
Iteration 22/25 | Loss: 0.00109330
Iteration 23/25 | Loss: 0.00109330
Iteration 24/25 | Loss: 0.00109330
Iteration 25/25 | Loss: 0.00109329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89878607
Iteration 2/25 | Loss: 0.00063661
Iteration 3/25 | Loss: 0.00063661
Iteration 4/25 | Loss: 0.00063661
Iteration 5/25 | Loss: 0.00063661
Iteration 6/25 | Loss: 0.00063661
Iteration 7/25 | Loss: 0.00063661
Iteration 8/25 | Loss: 0.00063661
Iteration 9/25 | Loss: 0.00063661
Iteration 10/25 | Loss: 0.00063661
Iteration 11/25 | Loss: 0.00063661
Iteration 12/25 | Loss: 0.00063661
Iteration 13/25 | Loss: 0.00063661
Iteration 14/25 | Loss: 0.00063661
Iteration 15/25 | Loss: 0.00063661
Iteration 16/25 | Loss: 0.00063661
Iteration 17/25 | Loss: 0.00063661
Iteration 18/25 | Loss: 0.00063661
Iteration 19/25 | Loss: 0.00063661
Iteration 20/25 | Loss: 0.00063661
Iteration 21/25 | Loss: 0.00063661
Iteration 22/25 | Loss: 0.00063661
Iteration 23/25 | Loss: 0.00063661
Iteration 24/25 | Loss: 0.00063661
Iteration 25/25 | Loss: 0.00063661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063661
Iteration 2/1000 | Loss: 0.00003612
Iteration 3/1000 | Loss: 0.00003016
Iteration 4/1000 | Loss: 0.00002848
Iteration 5/1000 | Loss: 0.00002764
Iteration 6/1000 | Loss: 0.00002691
Iteration 7/1000 | Loss: 0.00002644
Iteration 8/1000 | Loss: 0.00002632
Iteration 9/1000 | Loss: 0.00002620
Iteration 10/1000 | Loss: 0.00002618
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002611
Iteration 13/1000 | Loss: 0.00002607
Iteration 14/1000 | Loss: 0.00002606
Iteration 15/1000 | Loss: 0.00002601
Iteration 16/1000 | Loss: 0.00002600
Iteration 17/1000 | Loss: 0.00002600
Iteration 18/1000 | Loss: 0.00002592
Iteration 19/1000 | Loss: 0.00002591
Iteration 20/1000 | Loss: 0.00002591
Iteration 21/1000 | Loss: 0.00002590
Iteration 22/1000 | Loss: 0.00002590
Iteration 23/1000 | Loss: 0.00002589
Iteration 24/1000 | Loss: 0.00002589
Iteration 25/1000 | Loss: 0.00002588
Iteration 26/1000 | Loss: 0.00002588
Iteration 27/1000 | Loss: 0.00002588
Iteration 28/1000 | Loss: 0.00002588
Iteration 29/1000 | Loss: 0.00002588
Iteration 30/1000 | Loss: 0.00002587
Iteration 31/1000 | Loss: 0.00002587
Iteration 32/1000 | Loss: 0.00002587
Iteration 33/1000 | Loss: 0.00002587
Iteration 34/1000 | Loss: 0.00002587
Iteration 35/1000 | Loss: 0.00002587
Iteration 36/1000 | Loss: 0.00002587
Iteration 37/1000 | Loss: 0.00002587
Iteration 38/1000 | Loss: 0.00002587
Iteration 39/1000 | Loss: 0.00002587
Iteration 40/1000 | Loss: 0.00002587
Iteration 41/1000 | Loss: 0.00002586
Iteration 42/1000 | Loss: 0.00002586
Iteration 43/1000 | Loss: 0.00002586
Iteration 44/1000 | Loss: 0.00002586
Iteration 45/1000 | Loss: 0.00002586
Iteration 46/1000 | Loss: 0.00002586
Iteration 47/1000 | Loss: 0.00002586
Iteration 48/1000 | Loss: 0.00002586
Iteration 49/1000 | Loss: 0.00002585
Iteration 50/1000 | Loss: 0.00002585
Iteration 51/1000 | Loss: 0.00002584
Iteration 52/1000 | Loss: 0.00002584
Iteration 53/1000 | Loss: 0.00002584
Iteration 54/1000 | Loss: 0.00002584
Iteration 55/1000 | Loss: 0.00002584
Iteration 56/1000 | Loss: 0.00002584
Iteration 57/1000 | Loss: 0.00002584
Iteration 58/1000 | Loss: 0.00002583
Iteration 59/1000 | Loss: 0.00002583
Iteration 60/1000 | Loss: 0.00002583
Iteration 61/1000 | Loss: 0.00002583
Iteration 62/1000 | Loss: 0.00002583
Iteration 63/1000 | Loss: 0.00002582
Iteration 64/1000 | Loss: 0.00002582
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002581
Iteration 67/1000 | Loss: 0.00002581
Iteration 68/1000 | Loss: 0.00002581
Iteration 69/1000 | Loss: 0.00002581
Iteration 70/1000 | Loss: 0.00002580
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002579
Iteration 73/1000 | Loss: 0.00002579
Iteration 74/1000 | Loss: 0.00002579
Iteration 75/1000 | Loss: 0.00002579
Iteration 76/1000 | Loss: 0.00002579
Iteration 77/1000 | Loss: 0.00002579
Iteration 78/1000 | Loss: 0.00002579
Iteration 79/1000 | Loss: 0.00002579
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002578
Iteration 82/1000 | Loss: 0.00002578
Iteration 83/1000 | Loss: 0.00002578
Iteration 84/1000 | Loss: 0.00002578
Iteration 85/1000 | Loss: 0.00002578
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002578
Iteration 89/1000 | Loss: 0.00002577
Iteration 90/1000 | Loss: 0.00002577
Iteration 91/1000 | Loss: 0.00002577
Iteration 92/1000 | Loss: 0.00002577
Iteration 93/1000 | Loss: 0.00002577
Iteration 94/1000 | Loss: 0.00002577
Iteration 95/1000 | Loss: 0.00002577
Iteration 96/1000 | Loss: 0.00002577
Iteration 97/1000 | Loss: 0.00002577
Iteration 98/1000 | Loss: 0.00002577
Iteration 99/1000 | Loss: 0.00002577
Iteration 100/1000 | Loss: 0.00002577
Iteration 101/1000 | Loss: 0.00002577
Iteration 102/1000 | Loss: 0.00002576
Iteration 103/1000 | Loss: 0.00002576
Iteration 104/1000 | Loss: 0.00002576
Iteration 105/1000 | Loss: 0.00002576
Iteration 106/1000 | Loss: 0.00002576
Iteration 107/1000 | Loss: 0.00002576
Iteration 108/1000 | Loss: 0.00002576
Iteration 109/1000 | Loss: 0.00002576
Iteration 110/1000 | Loss: 0.00002576
Iteration 111/1000 | Loss: 0.00002576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.5764666133909486e-05, 2.5764666133909486e-05, 2.5764666133909486e-05, 2.5764666133909486e-05, 2.5764666133909486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5764666133909486e-05

Optimization complete. Final v2v error: 4.359018802642822 mm

Highest mean error: 4.746829509735107 mm for frame 106

Lowest mean error: 3.533729076385498 mm for frame 7

Saving results

Total time: 49.293893575668335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472199
Iteration 2/25 | Loss: 0.00119244
Iteration 3/25 | Loss: 0.00111653
Iteration 4/25 | Loss: 0.00109130
Iteration 5/25 | Loss: 0.00108182
Iteration 6/25 | Loss: 0.00108008
Iteration 7/25 | Loss: 0.00107983
Iteration 8/25 | Loss: 0.00107983
Iteration 9/25 | Loss: 0.00107983
Iteration 10/25 | Loss: 0.00107983
Iteration 11/25 | Loss: 0.00107983
Iteration 12/25 | Loss: 0.00107983
Iteration 13/25 | Loss: 0.00107983
Iteration 14/25 | Loss: 0.00107983
Iteration 15/25 | Loss: 0.00107983
Iteration 16/25 | Loss: 0.00107983
Iteration 17/25 | Loss: 0.00107983
Iteration 18/25 | Loss: 0.00107983
Iteration 19/25 | Loss: 0.00107983
Iteration 20/25 | Loss: 0.00107983
Iteration 21/25 | Loss: 0.00107983
Iteration 22/25 | Loss: 0.00107983
Iteration 23/25 | Loss: 0.00107983
Iteration 24/25 | Loss: 0.00107983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010798268485814333, 0.0010798268485814333, 0.0010798268485814333, 0.0010798268485814333, 0.0010798268485814333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010798268485814333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55796552
Iteration 2/25 | Loss: 0.00056462
Iteration 3/25 | Loss: 0.00056462
Iteration 4/25 | Loss: 0.00056462
Iteration 5/25 | Loss: 0.00056462
Iteration 6/25 | Loss: 0.00056462
Iteration 7/25 | Loss: 0.00056462
Iteration 8/25 | Loss: 0.00056462
Iteration 9/25 | Loss: 0.00056462
Iteration 10/25 | Loss: 0.00056462
Iteration 11/25 | Loss: 0.00056462
Iteration 12/25 | Loss: 0.00056462
Iteration 13/25 | Loss: 0.00056462
Iteration 14/25 | Loss: 0.00056462
Iteration 15/25 | Loss: 0.00056462
Iteration 16/25 | Loss: 0.00056462
Iteration 17/25 | Loss: 0.00056462
Iteration 18/25 | Loss: 0.00056462
Iteration 19/25 | Loss: 0.00056462
Iteration 20/25 | Loss: 0.00056462
Iteration 21/25 | Loss: 0.00056462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005646176869049668, 0.0005646176869049668, 0.0005646176869049668, 0.0005646176869049668, 0.0005646176869049668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005646176869049668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056462
Iteration 2/1000 | Loss: 0.00003511
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00002474
Iteration 5/1000 | Loss: 0.00002394
Iteration 6/1000 | Loss: 0.00002360
Iteration 7/1000 | Loss: 0.00002303
Iteration 8/1000 | Loss: 0.00002257
Iteration 9/1000 | Loss: 0.00002235
Iteration 10/1000 | Loss: 0.00002224
Iteration 11/1000 | Loss: 0.00002209
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002202
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002199
Iteration 16/1000 | Loss: 0.00002198
Iteration 17/1000 | Loss: 0.00002198
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00002196
Iteration 20/1000 | Loss: 0.00002196
Iteration 21/1000 | Loss: 0.00002193
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002193
Iteration 26/1000 | Loss: 0.00002193
Iteration 27/1000 | Loss: 0.00002189
Iteration 28/1000 | Loss: 0.00002189
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002188
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002188
Iteration 38/1000 | Loss: 0.00002188
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002185
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002183
Iteration 46/1000 | Loss: 0.00002183
Iteration 47/1000 | Loss: 0.00002183
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002183
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002182
Iteration 52/1000 | Loss: 0.00002182
Iteration 53/1000 | Loss: 0.00002182
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002181
Iteration 56/1000 | Loss: 0.00002181
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002179
Iteration 62/1000 | Loss: 0.00002179
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002174
Iteration 70/1000 | Loss: 0.00002174
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002172
Iteration 73/1000 | Loss: 0.00002171
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002170
Iteration 77/1000 | Loss: 0.00002170
Iteration 78/1000 | Loss: 0.00002169
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00002169
Iteration 81/1000 | Loss: 0.00002169
Iteration 82/1000 | Loss: 0.00002169
Iteration 83/1000 | Loss: 0.00002168
Iteration 84/1000 | Loss: 0.00002168
Iteration 85/1000 | Loss: 0.00002168
Iteration 86/1000 | Loss: 0.00002168
Iteration 87/1000 | Loss: 0.00002168
Iteration 88/1000 | Loss: 0.00002168
Iteration 89/1000 | Loss: 0.00002168
Iteration 90/1000 | Loss: 0.00002168
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002167
Iteration 93/1000 | Loss: 0.00002167
Iteration 94/1000 | Loss: 0.00002167
Iteration 95/1000 | Loss: 0.00002167
Iteration 96/1000 | Loss: 0.00002167
Iteration 97/1000 | Loss: 0.00002167
Iteration 98/1000 | Loss: 0.00002167
Iteration 99/1000 | Loss: 0.00002167
Iteration 100/1000 | Loss: 0.00002167
Iteration 101/1000 | Loss: 0.00002167
Iteration 102/1000 | Loss: 0.00002167
Iteration 103/1000 | Loss: 0.00002166
Iteration 104/1000 | Loss: 0.00002166
Iteration 105/1000 | Loss: 0.00002166
Iteration 106/1000 | Loss: 0.00002166
Iteration 107/1000 | Loss: 0.00002166
Iteration 108/1000 | Loss: 0.00002166
Iteration 109/1000 | Loss: 0.00002166
Iteration 110/1000 | Loss: 0.00002166
Iteration 111/1000 | Loss: 0.00002166
Iteration 112/1000 | Loss: 0.00002166
Iteration 113/1000 | Loss: 0.00002166
Iteration 114/1000 | Loss: 0.00002166
Iteration 115/1000 | Loss: 0.00002166
Iteration 116/1000 | Loss: 0.00002166
Iteration 117/1000 | Loss: 0.00002166
Iteration 118/1000 | Loss: 0.00002165
Iteration 119/1000 | Loss: 0.00002165
Iteration 120/1000 | Loss: 0.00002165
Iteration 121/1000 | Loss: 0.00002165
Iteration 122/1000 | Loss: 0.00002165
Iteration 123/1000 | Loss: 0.00002165
Iteration 124/1000 | Loss: 0.00002164
Iteration 125/1000 | Loss: 0.00002164
Iteration 126/1000 | Loss: 0.00002164
Iteration 127/1000 | Loss: 0.00002164
Iteration 128/1000 | Loss: 0.00002164
Iteration 129/1000 | Loss: 0.00002164
Iteration 130/1000 | Loss: 0.00002164
Iteration 131/1000 | Loss: 0.00002164
Iteration 132/1000 | Loss: 0.00002164
Iteration 133/1000 | Loss: 0.00002164
Iteration 134/1000 | Loss: 0.00002164
Iteration 135/1000 | Loss: 0.00002164
Iteration 136/1000 | Loss: 0.00002164
Iteration 137/1000 | Loss: 0.00002164
Iteration 138/1000 | Loss: 0.00002164
Iteration 139/1000 | Loss: 0.00002164
Iteration 140/1000 | Loss: 0.00002164
Iteration 141/1000 | Loss: 0.00002164
Iteration 142/1000 | Loss: 0.00002164
Iteration 143/1000 | Loss: 0.00002164
Iteration 144/1000 | Loss: 0.00002163
Iteration 145/1000 | Loss: 0.00002163
Iteration 146/1000 | Loss: 0.00002163
Iteration 147/1000 | Loss: 0.00002163
Iteration 148/1000 | Loss: 0.00002163
Iteration 149/1000 | Loss: 0.00002163
Iteration 150/1000 | Loss: 0.00002163
Iteration 151/1000 | Loss: 0.00002163
Iteration 152/1000 | Loss: 0.00002163
Iteration 153/1000 | Loss: 0.00002163
Iteration 154/1000 | Loss: 0.00002163
Iteration 155/1000 | Loss: 0.00002163
Iteration 156/1000 | Loss: 0.00002163
Iteration 157/1000 | Loss: 0.00002163
Iteration 158/1000 | Loss: 0.00002162
Iteration 159/1000 | Loss: 0.00002162
Iteration 160/1000 | Loss: 0.00002162
Iteration 161/1000 | Loss: 0.00002162
Iteration 162/1000 | Loss: 0.00002162
Iteration 163/1000 | Loss: 0.00002162
Iteration 164/1000 | Loss: 0.00002162
Iteration 165/1000 | Loss: 0.00002162
Iteration 166/1000 | Loss: 0.00002162
Iteration 167/1000 | Loss: 0.00002162
Iteration 168/1000 | Loss: 0.00002162
Iteration 169/1000 | Loss: 0.00002162
Iteration 170/1000 | Loss: 0.00002162
Iteration 171/1000 | Loss: 0.00002162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.1623232896672562e-05, 2.1623232896672562e-05, 2.1623232896672562e-05, 2.1623232896672562e-05, 2.1623232896672562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1623232896672562e-05

Optimization complete. Final v2v error: 4.072167873382568 mm

Highest mean error: 4.442512512207031 mm for frame 112

Lowest mean error: 3.777545928955078 mm for frame 152

Saving results

Total time: 36.32588291168213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583443
Iteration 2/25 | Loss: 0.00137118
Iteration 3/25 | Loss: 0.00117183
Iteration 4/25 | Loss: 0.00114861
Iteration 5/25 | Loss: 0.00113956
Iteration 6/25 | Loss: 0.00113709
Iteration 7/25 | Loss: 0.00113691
Iteration 8/25 | Loss: 0.00113691
Iteration 9/25 | Loss: 0.00113691
Iteration 10/25 | Loss: 0.00113691
Iteration 11/25 | Loss: 0.00113691
Iteration 12/25 | Loss: 0.00113691
Iteration 13/25 | Loss: 0.00113691
Iteration 14/25 | Loss: 0.00113691
Iteration 15/25 | Loss: 0.00113691
Iteration 16/25 | Loss: 0.00113691
Iteration 17/25 | Loss: 0.00113691
Iteration 18/25 | Loss: 0.00113691
Iteration 19/25 | Loss: 0.00113691
Iteration 20/25 | Loss: 0.00113691
Iteration 21/25 | Loss: 0.00113691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011369077255949378, 0.0011369077255949378, 0.0011369077255949378, 0.0011369077255949378, 0.0011369077255949378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011369077255949378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88593721
Iteration 2/25 | Loss: 0.00069904
Iteration 3/25 | Loss: 0.00069902
Iteration 4/25 | Loss: 0.00069902
Iteration 5/25 | Loss: 0.00069902
Iteration 6/25 | Loss: 0.00069902
Iteration 7/25 | Loss: 0.00069902
Iteration 8/25 | Loss: 0.00069902
Iteration 9/25 | Loss: 0.00069902
Iteration 10/25 | Loss: 0.00069902
Iteration 11/25 | Loss: 0.00069902
Iteration 12/25 | Loss: 0.00069902
Iteration 13/25 | Loss: 0.00069902
Iteration 14/25 | Loss: 0.00069902
Iteration 15/25 | Loss: 0.00069902
Iteration 16/25 | Loss: 0.00069902
Iteration 17/25 | Loss: 0.00069902
Iteration 18/25 | Loss: 0.00069902
Iteration 19/25 | Loss: 0.00069902
Iteration 20/25 | Loss: 0.00069902
Iteration 21/25 | Loss: 0.00069902
Iteration 22/25 | Loss: 0.00069902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006990195834077895, 0.0006990195834077895, 0.0006990195834077895, 0.0006990195834077895, 0.0006990195834077895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006990195834077895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069902
Iteration 2/1000 | Loss: 0.00004260
Iteration 3/1000 | Loss: 0.00003100
Iteration 4/1000 | Loss: 0.00002871
Iteration 5/1000 | Loss: 0.00002759
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002645
Iteration 8/1000 | Loss: 0.00002603
Iteration 9/1000 | Loss: 0.00002575
Iteration 10/1000 | Loss: 0.00002571
Iteration 11/1000 | Loss: 0.00002549
Iteration 12/1000 | Loss: 0.00002547
Iteration 13/1000 | Loss: 0.00002538
Iteration 14/1000 | Loss: 0.00002533
Iteration 15/1000 | Loss: 0.00002531
Iteration 16/1000 | Loss: 0.00002525
Iteration 17/1000 | Loss: 0.00002521
Iteration 18/1000 | Loss: 0.00002521
Iteration 19/1000 | Loss: 0.00002519
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00002515
Iteration 22/1000 | Loss: 0.00002515
Iteration 23/1000 | Loss: 0.00002514
Iteration 24/1000 | Loss: 0.00002513
Iteration 25/1000 | Loss: 0.00002512
Iteration 26/1000 | Loss: 0.00002512
Iteration 27/1000 | Loss: 0.00002511
Iteration 28/1000 | Loss: 0.00002511
Iteration 29/1000 | Loss: 0.00002511
Iteration 30/1000 | Loss: 0.00002511
Iteration 31/1000 | Loss: 0.00002511
Iteration 32/1000 | Loss: 0.00002511
Iteration 33/1000 | Loss: 0.00002511
Iteration 34/1000 | Loss: 0.00002511
Iteration 35/1000 | Loss: 0.00002511
Iteration 36/1000 | Loss: 0.00002511
Iteration 37/1000 | Loss: 0.00002511
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002508
Iteration 43/1000 | Loss: 0.00002508
Iteration 44/1000 | Loss: 0.00002508
Iteration 45/1000 | Loss: 0.00002507
Iteration 46/1000 | Loss: 0.00002507
Iteration 47/1000 | Loss: 0.00002507
Iteration 48/1000 | Loss: 0.00002507
Iteration 49/1000 | Loss: 0.00002507
Iteration 50/1000 | Loss: 0.00002506
Iteration 51/1000 | Loss: 0.00002506
Iteration 52/1000 | Loss: 0.00002506
Iteration 53/1000 | Loss: 0.00002506
Iteration 54/1000 | Loss: 0.00002506
Iteration 55/1000 | Loss: 0.00002506
Iteration 56/1000 | Loss: 0.00002506
Iteration 57/1000 | Loss: 0.00002506
Iteration 58/1000 | Loss: 0.00002505
Iteration 59/1000 | Loss: 0.00002505
Iteration 60/1000 | Loss: 0.00002505
Iteration 61/1000 | Loss: 0.00002505
Iteration 62/1000 | Loss: 0.00002505
Iteration 63/1000 | Loss: 0.00002504
Iteration 64/1000 | Loss: 0.00002504
Iteration 65/1000 | Loss: 0.00002504
Iteration 66/1000 | Loss: 0.00002504
Iteration 67/1000 | Loss: 0.00002504
Iteration 68/1000 | Loss: 0.00002504
Iteration 69/1000 | Loss: 0.00002504
Iteration 70/1000 | Loss: 0.00002504
Iteration 71/1000 | Loss: 0.00002504
Iteration 72/1000 | Loss: 0.00002503
Iteration 73/1000 | Loss: 0.00002503
Iteration 74/1000 | Loss: 0.00002503
Iteration 75/1000 | Loss: 0.00002503
Iteration 76/1000 | Loss: 0.00002503
Iteration 77/1000 | Loss: 0.00002503
Iteration 78/1000 | Loss: 0.00002503
Iteration 79/1000 | Loss: 0.00002503
Iteration 80/1000 | Loss: 0.00002503
Iteration 81/1000 | Loss: 0.00002503
Iteration 82/1000 | Loss: 0.00002503
Iteration 83/1000 | Loss: 0.00002503
Iteration 84/1000 | Loss: 0.00002503
Iteration 85/1000 | Loss: 0.00002502
Iteration 86/1000 | Loss: 0.00002502
Iteration 87/1000 | Loss: 0.00002502
Iteration 88/1000 | Loss: 0.00002502
Iteration 89/1000 | Loss: 0.00002502
Iteration 90/1000 | Loss: 0.00002502
Iteration 91/1000 | Loss: 0.00002502
Iteration 92/1000 | Loss: 0.00002502
Iteration 93/1000 | Loss: 0.00002502
Iteration 94/1000 | Loss: 0.00002502
Iteration 95/1000 | Loss: 0.00002501
Iteration 96/1000 | Loss: 0.00002501
Iteration 97/1000 | Loss: 0.00002501
Iteration 98/1000 | Loss: 0.00002501
Iteration 99/1000 | Loss: 0.00002501
Iteration 100/1000 | Loss: 0.00002501
Iteration 101/1000 | Loss: 0.00002501
Iteration 102/1000 | Loss: 0.00002501
Iteration 103/1000 | Loss: 0.00002501
Iteration 104/1000 | Loss: 0.00002501
Iteration 105/1000 | Loss: 0.00002501
Iteration 106/1000 | Loss: 0.00002501
Iteration 107/1000 | Loss: 0.00002501
Iteration 108/1000 | Loss: 0.00002501
Iteration 109/1000 | Loss: 0.00002501
Iteration 110/1000 | Loss: 0.00002501
Iteration 111/1000 | Loss: 0.00002501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.5010189347085543e-05, 2.5010189347085543e-05, 2.5010189347085543e-05, 2.5010189347085543e-05, 2.5010189347085543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5010189347085543e-05

Optimization complete. Final v2v error: 4.369317054748535 mm

Highest mean error: 4.799719333648682 mm for frame 162

Lowest mean error: 3.910771608352661 mm for frame 0

Saving results

Total time: 38.77205538749695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01163830
Iteration 2/25 | Loss: 0.01163830
Iteration 3/25 | Loss: 0.00593919
Iteration 4/25 | Loss: 0.00375018
Iteration 5/25 | Loss: 0.00329232
Iteration 6/25 | Loss: 0.00279623
Iteration 7/25 | Loss: 0.00247604
Iteration 8/25 | Loss: 0.00232112
Iteration 9/25 | Loss: 0.00227798
Iteration 10/25 | Loss: 0.00220396
Iteration 11/25 | Loss: 0.00216648
Iteration 12/25 | Loss: 0.00205193
Iteration 13/25 | Loss: 0.00190720
Iteration 14/25 | Loss: 0.00186697
Iteration 15/25 | Loss: 0.00183660
Iteration 16/25 | Loss: 0.00183116
Iteration 17/25 | Loss: 0.00181758
Iteration 18/25 | Loss: 0.00180880
Iteration 19/25 | Loss: 0.00179942
Iteration 20/25 | Loss: 0.00179920
Iteration 21/25 | Loss: 0.00179894
Iteration 22/25 | Loss: 0.00179912
Iteration 23/25 | Loss: 0.00179379
Iteration 24/25 | Loss: 0.00179170
Iteration 25/25 | Loss: 0.00179523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40462205
Iteration 2/25 | Loss: 0.00605897
Iteration 3/25 | Loss: 0.00482675
Iteration 4/25 | Loss: 0.00482574
Iteration 5/25 | Loss: 0.00482574
Iteration 6/25 | Loss: 0.00482574
Iteration 7/25 | Loss: 0.00482574
Iteration 8/25 | Loss: 0.00482574
Iteration 9/25 | Loss: 0.00482574
Iteration 10/25 | Loss: 0.00482574
Iteration 11/25 | Loss: 0.00482574
Iteration 12/25 | Loss: 0.00482574
Iteration 13/25 | Loss: 0.00482574
Iteration 14/25 | Loss: 0.00482574
Iteration 15/25 | Loss: 0.00482574
Iteration 16/25 | Loss: 0.00482574
Iteration 17/25 | Loss: 0.00482574
Iteration 18/25 | Loss: 0.00482574
Iteration 19/25 | Loss: 0.00482574
Iteration 20/25 | Loss: 0.00482574
Iteration 21/25 | Loss: 0.00482574
Iteration 22/25 | Loss: 0.00482574
Iteration 23/25 | Loss: 0.00482574
Iteration 24/25 | Loss: 0.00482574
Iteration 25/25 | Loss: 0.00482574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00482574
Iteration 2/1000 | Loss: 0.00118634
Iteration 3/1000 | Loss: 0.00171291
Iteration 4/1000 | Loss: 0.00287152
Iteration 5/1000 | Loss: 0.00072657
Iteration 6/1000 | Loss: 0.00047658
Iteration 7/1000 | Loss: 0.00113676
Iteration 8/1000 | Loss: 0.00055227
Iteration 9/1000 | Loss: 0.00145891
Iteration 10/1000 | Loss: 0.00058241
Iteration 11/1000 | Loss: 0.00055293
Iteration 12/1000 | Loss: 0.00049782
Iteration 13/1000 | Loss: 0.00863647
Iteration 14/1000 | Loss: 0.02416972
Iteration 15/1000 | Loss: 0.00808106
Iteration 16/1000 | Loss: 0.00215856
Iteration 17/1000 | Loss: 0.00043255
Iteration 18/1000 | Loss: 0.00093559
Iteration 19/1000 | Loss: 0.00101500
Iteration 20/1000 | Loss: 0.00453173
Iteration 21/1000 | Loss: 0.00168643
Iteration 22/1000 | Loss: 0.00611435
Iteration 23/1000 | Loss: 0.00639547
Iteration 24/1000 | Loss: 0.01452737
Iteration 25/1000 | Loss: 0.01058736
Iteration 26/1000 | Loss: 0.00281011
Iteration 27/1000 | Loss: 0.00368402
Iteration 28/1000 | Loss: 0.00287800
Iteration 29/1000 | Loss: 0.00294567
Iteration 30/1000 | Loss: 0.00214610
Iteration 31/1000 | Loss: 0.00154450
Iteration 32/1000 | Loss: 0.00170686
Iteration 33/1000 | Loss: 0.00293786
Iteration 34/1000 | Loss: 0.00215968
Iteration 35/1000 | Loss: 0.00343862
Iteration 36/1000 | Loss: 0.00296023
Iteration 37/1000 | Loss: 0.00027281
Iteration 38/1000 | Loss: 0.00108087
Iteration 39/1000 | Loss: 0.00101580
Iteration 40/1000 | Loss: 0.00045472
Iteration 41/1000 | Loss: 0.00099329
Iteration 42/1000 | Loss: 0.00093828
Iteration 43/1000 | Loss: 0.00294310
Iteration 44/1000 | Loss: 0.00173344
Iteration 45/1000 | Loss: 0.00112999
Iteration 46/1000 | Loss: 0.00063983
Iteration 47/1000 | Loss: 0.00073604
Iteration 48/1000 | Loss: 0.00074792
Iteration 49/1000 | Loss: 0.00034049
Iteration 50/1000 | Loss: 0.00033981
Iteration 51/1000 | Loss: 0.00053978
Iteration 52/1000 | Loss: 0.00032413
Iteration 53/1000 | Loss: 0.00017930
Iteration 54/1000 | Loss: 0.00018255
Iteration 55/1000 | Loss: 0.00203353
Iteration 56/1000 | Loss: 0.00104004
Iteration 57/1000 | Loss: 0.00027663
Iteration 58/1000 | Loss: 0.00024357
Iteration 59/1000 | Loss: 0.00017572
Iteration 60/1000 | Loss: 0.00200993
Iteration 61/1000 | Loss: 0.00080812
Iteration 62/1000 | Loss: 0.00261651
Iteration 63/1000 | Loss: 0.00060778
Iteration 64/1000 | Loss: 0.00036883
Iteration 65/1000 | Loss: 0.00053587
Iteration 66/1000 | Loss: 0.00050553
Iteration 67/1000 | Loss: 0.00186344
Iteration 68/1000 | Loss: 0.00177455
Iteration 69/1000 | Loss: 0.00236639
Iteration 70/1000 | Loss: 0.00183890
Iteration 71/1000 | Loss: 0.00147576
Iteration 72/1000 | Loss: 0.00103720
Iteration 73/1000 | Loss: 0.00084458
Iteration 74/1000 | Loss: 0.00100587
Iteration 75/1000 | Loss: 0.00070311
Iteration 76/1000 | Loss: 0.00019256
Iteration 77/1000 | Loss: 0.00035461
Iteration 78/1000 | Loss: 0.00032044
Iteration 79/1000 | Loss: 0.00033912
Iteration 80/1000 | Loss: 0.00027800
Iteration 81/1000 | Loss: 0.00042153
Iteration 82/1000 | Loss: 0.00037483
Iteration 83/1000 | Loss: 0.00038873
Iteration 84/1000 | Loss: 0.00045359
Iteration 85/1000 | Loss: 0.00063831
Iteration 86/1000 | Loss: 0.00057431
Iteration 87/1000 | Loss: 0.00060864
Iteration 88/1000 | Loss: 0.00052970
Iteration 89/1000 | Loss: 0.00046146
Iteration 90/1000 | Loss: 0.00029887
Iteration 91/1000 | Loss: 0.00015870
Iteration 92/1000 | Loss: 0.00104452
Iteration 93/1000 | Loss: 0.00120938
Iteration 94/1000 | Loss: 0.00088175
Iteration 95/1000 | Loss: 0.00107725
Iteration 96/1000 | Loss: 0.00016329
Iteration 97/1000 | Loss: 0.00014757
Iteration 98/1000 | Loss: 0.00013981
Iteration 99/1000 | Loss: 0.00026543
Iteration 100/1000 | Loss: 0.00031067
Iteration 101/1000 | Loss: 0.00066751
Iteration 102/1000 | Loss: 0.00017820
Iteration 103/1000 | Loss: 0.00058924
Iteration 104/1000 | Loss: 0.00061652
Iteration 105/1000 | Loss: 0.00072748
Iteration 106/1000 | Loss: 0.00113206
Iteration 107/1000 | Loss: 0.00079802
Iteration 108/1000 | Loss: 0.00078750
Iteration 109/1000 | Loss: 0.00049063
Iteration 110/1000 | Loss: 0.00018141
Iteration 111/1000 | Loss: 0.00068592
Iteration 112/1000 | Loss: 0.00034228
Iteration 113/1000 | Loss: 0.00074569
Iteration 114/1000 | Loss: 0.00034436
Iteration 115/1000 | Loss: 0.00032479
Iteration 116/1000 | Loss: 0.00037544
Iteration 117/1000 | Loss: 0.00085634
Iteration 118/1000 | Loss: 0.00022141
Iteration 119/1000 | Loss: 0.00013770
Iteration 120/1000 | Loss: 0.00028832
Iteration 121/1000 | Loss: 0.00064817
Iteration 122/1000 | Loss: 0.00013912
Iteration 123/1000 | Loss: 0.00028537
Iteration 124/1000 | Loss: 0.00014021
Iteration 125/1000 | Loss: 0.00028564
Iteration 126/1000 | Loss: 0.00012592
Iteration 127/1000 | Loss: 0.00011662
Iteration 128/1000 | Loss: 0.00011040
Iteration 129/1000 | Loss: 0.00137651
Iteration 130/1000 | Loss: 0.00189743
Iteration 131/1000 | Loss: 0.00014225
Iteration 132/1000 | Loss: 0.00011322
Iteration 133/1000 | Loss: 0.00010460
Iteration 134/1000 | Loss: 0.00009744
Iteration 135/1000 | Loss: 0.00009372
Iteration 136/1000 | Loss: 0.00009063
Iteration 137/1000 | Loss: 0.00021881
Iteration 138/1000 | Loss: 0.00010121
Iteration 139/1000 | Loss: 0.00016327
Iteration 140/1000 | Loss: 0.00017408
Iteration 141/1000 | Loss: 0.00050542
Iteration 142/1000 | Loss: 0.00046721
Iteration 143/1000 | Loss: 0.00056852
Iteration 144/1000 | Loss: 0.00076890
Iteration 145/1000 | Loss: 0.00065835
Iteration 146/1000 | Loss: 0.00011085
Iteration 147/1000 | Loss: 0.00009394
Iteration 148/1000 | Loss: 0.00009100
Iteration 149/1000 | Loss: 0.00008935
Iteration 150/1000 | Loss: 0.00008820
Iteration 151/1000 | Loss: 0.00008636
Iteration 152/1000 | Loss: 0.00008490
Iteration 153/1000 | Loss: 0.00008418
Iteration 154/1000 | Loss: 0.00015608
Iteration 155/1000 | Loss: 0.00009866
Iteration 156/1000 | Loss: 0.00008368
Iteration 157/1000 | Loss: 0.00008339
Iteration 158/1000 | Loss: 0.00008329
Iteration 159/1000 | Loss: 0.00008315
Iteration 160/1000 | Loss: 0.00008308
Iteration 161/1000 | Loss: 0.00008308
Iteration 162/1000 | Loss: 0.00008308
Iteration 163/1000 | Loss: 0.00008308
Iteration 164/1000 | Loss: 0.00008308
Iteration 165/1000 | Loss: 0.00008307
Iteration 166/1000 | Loss: 0.00008306
Iteration 167/1000 | Loss: 0.00056770
Iteration 168/1000 | Loss: 0.00008750
Iteration 169/1000 | Loss: 0.00008354
Iteration 170/1000 | Loss: 0.00008195
Iteration 171/1000 | Loss: 0.00008128
Iteration 172/1000 | Loss: 0.00008080
Iteration 173/1000 | Loss: 0.00008051
Iteration 174/1000 | Loss: 0.00008026
Iteration 175/1000 | Loss: 0.00008017
Iteration 176/1000 | Loss: 0.00008017
Iteration 177/1000 | Loss: 0.00008015
Iteration 178/1000 | Loss: 0.00008013
Iteration 179/1000 | Loss: 0.00008012
Iteration 180/1000 | Loss: 0.00008012
Iteration 181/1000 | Loss: 0.00008012
Iteration 182/1000 | Loss: 0.00008012
Iteration 183/1000 | Loss: 0.00008012
Iteration 184/1000 | Loss: 0.00008012
Iteration 185/1000 | Loss: 0.00008012
Iteration 186/1000 | Loss: 0.00008012
Iteration 187/1000 | Loss: 0.00008012
Iteration 188/1000 | Loss: 0.00008012
Iteration 189/1000 | Loss: 0.00008011
Iteration 190/1000 | Loss: 0.00008011
Iteration 191/1000 | Loss: 0.00008011
Iteration 192/1000 | Loss: 0.00008011
Iteration 193/1000 | Loss: 0.00008011
Iteration 194/1000 | Loss: 0.00008011
Iteration 195/1000 | Loss: 0.00008011
Iteration 196/1000 | Loss: 0.00008011
Iteration 197/1000 | Loss: 0.00008011
Iteration 198/1000 | Loss: 0.00008011
Iteration 199/1000 | Loss: 0.00008011
Iteration 200/1000 | Loss: 0.00008011
Iteration 201/1000 | Loss: 0.00008010
Iteration 202/1000 | Loss: 0.00008010
Iteration 203/1000 | Loss: 0.00008010
Iteration 204/1000 | Loss: 0.00008010
Iteration 205/1000 | Loss: 0.00008009
Iteration 206/1000 | Loss: 0.00008009
Iteration 207/1000 | Loss: 0.00008009
Iteration 208/1000 | Loss: 0.00008008
Iteration 209/1000 | Loss: 0.00008008
Iteration 210/1000 | Loss: 0.00008008
Iteration 211/1000 | Loss: 0.00008008
Iteration 212/1000 | Loss: 0.00008008
Iteration 213/1000 | Loss: 0.00008008
Iteration 214/1000 | Loss: 0.00008008
Iteration 215/1000 | Loss: 0.00008008
Iteration 216/1000 | Loss: 0.00008008
Iteration 217/1000 | Loss: 0.00008008
Iteration 218/1000 | Loss: 0.00008008
Iteration 219/1000 | Loss: 0.00008008
Iteration 220/1000 | Loss: 0.00008008
Iteration 221/1000 | Loss: 0.00008008
Iteration 222/1000 | Loss: 0.00008007
Iteration 223/1000 | Loss: 0.00008007
Iteration 224/1000 | Loss: 0.00008007
Iteration 225/1000 | Loss: 0.00008007
Iteration 226/1000 | Loss: 0.00008007
Iteration 227/1000 | Loss: 0.00008007
Iteration 228/1000 | Loss: 0.00008006
Iteration 229/1000 | Loss: 0.00008006
Iteration 230/1000 | Loss: 0.00008006
Iteration 231/1000 | Loss: 0.00008006
Iteration 232/1000 | Loss: 0.00008006
Iteration 233/1000 | Loss: 0.00008006
Iteration 234/1000 | Loss: 0.00008006
Iteration 235/1000 | Loss: 0.00008006
Iteration 236/1000 | Loss: 0.00008005
Iteration 237/1000 | Loss: 0.00008005
Iteration 238/1000 | Loss: 0.00008005
Iteration 239/1000 | Loss: 0.00008005
Iteration 240/1000 | Loss: 0.00008005
Iteration 241/1000 | Loss: 0.00008004
Iteration 242/1000 | Loss: 0.00008004
Iteration 243/1000 | Loss: 0.00008004
Iteration 244/1000 | Loss: 0.00008004
Iteration 245/1000 | Loss: 0.00008004
Iteration 246/1000 | Loss: 0.00008004
Iteration 247/1000 | Loss: 0.00008004
Iteration 248/1000 | Loss: 0.00008004
Iteration 249/1000 | Loss: 0.00008004
Iteration 250/1000 | Loss: 0.00008004
Iteration 251/1000 | Loss: 0.00008004
Iteration 252/1000 | Loss: 0.00008003
Iteration 253/1000 | Loss: 0.00008003
Iteration 254/1000 | Loss: 0.00008003
Iteration 255/1000 | Loss: 0.00008003
Iteration 256/1000 | Loss: 0.00008003
Iteration 257/1000 | Loss: 0.00008002
Iteration 258/1000 | Loss: 0.00008002
Iteration 259/1000 | Loss: 0.00008002
Iteration 260/1000 | Loss: 0.00008002
Iteration 261/1000 | Loss: 0.00008002
Iteration 262/1000 | Loss: 0.00008001
Iteration 263/1000 | Loss: 0.00008001
Iteration 264/1000 | Loss: 0.00008001
Iteration 265/1000 | Loss: 0.00008001
Iteration 266/1000 | Loss: 0.00008001
Iteration 267/1000 | Loss: 0.00008001
Iteration 268/1000 | Loss: 0.00008001
Iteration 269/1000 | Loss: 0.00008001
Iteration 270/1000 | Loss: 0.00008001
Iteration 271/1000 | Loss: 0.00008001
Iteration 272/1000 | Loss: 0.00008001
Iteration 273/1000 | Loss: 0.00008001
Iteration 274/1000 | Loss: 0.00008001
Iteration 275/1000 | Loss: 0.00008001
Iteration 276/1000 | Loss: 0.00008000
Iteration 277/1000 | Loss: 0.00008000
Iteration 278/1000 | Loss: 0.00008000
Iteration 279/1000 | Loss: 0.00008000
Iteration 280/1000 | Loss: 0.00008000
Iteration 281/1000 | Loss: 0.00008000
Iteration 282/1000 | Loss: 0.00008000
Iteration 283/1000 | Loss: 0.00008000
Iteration 284/1000 | Loss: 0.00008000
Iteration 285/1000 | Loss: 0.00008000
Iteration 286/1000 | Loss: 0.00008000
Iteration 287/1000 | Loss: 0.00008000
Iteration 288/1000 | Loss: 0.00008000
Iteration 289/1000 | Loss: 0.00008000
Iteration 290/1000 | Loss: 0.00008000
Iteration 291/1000 | Loss: 0.00008000
Iteration 292/1000 | Loss: 0.00008000
Iteration 293/1000 | Loss: 0.00008000
Iteration 294/1000 | Loss: 0.00008000
Iteration 295/1000 | Loss: 0.00008000
Iteration 296/1000 | Loss: 0.00008000
Iteration 297/1000 | Loss: 0.00007999
Iteration 298/1000 | Loss: 0.00007999
Iteration 299/1000 | Loss: 0.00007999
Iteration 300/1000 | Loss: 0.00007999
Iteration 301/1000 | Loss: 0.00007999
Iteration 302/1000 | Loss: 0.00007999
Iteration 303/1000 | Loss: 0.00007999
Iteration 304/1000 | Loss: 0.00007999
Iteration 305/1000 | Loss: 0.00007999
Iteration 306/1000 | Loss: 0.00007999
Iteration 307/1000 | Loss: 0.00007999
Iteration 308/1000 | Loss: 0.00007999
Iteration 309/1000 | Loss: 0.00007999
Iteration 310/1000 | Loss: 0.00007999
Iteration 311/1000 | Loss: 0.00007999
Iteration 312/1000 | Loss: 0.00007999
Iteration 313/1000 | Loss: 0.00007999
Iteration 314/1000 | Loss: 0.00007999
Iteration 315/1000 | Loss: 0.00007999
Iteration 316/1000 | Loss: 0.00007999
Iteration 317/1000 | Loss: 0.00007999
Iteration 318/1000 | Loss: 0.00007999
Iteration 319/1000 | Loss: 0.00007999
Iteration 320/1000 | Loss: 0.00007999
Iteration 321/1000 | Loss: 0.00007999
Iteration 322/1000 | Loss: 0.00007999
Iteration 323/1000 | Loss: 0.00007999
Iteration 324/1000 | Loss: 0.00007999
Iteration 325/1000 | Loss: 0.00007999
Iteration 326/1000 | Loss: 0.00007999
Iteration 327/1000 | Loss: 0.00007999
Iteration 328/1000 | Loss: 0.00007999
Iteration 329/1000 | Loss: 0.00007999
Iteration 330/1000 | Loss: 0.00007999
Iteration 331/1000 | Loss: 0.00007999
Iteration 332/1000 | Loss: 0.00007999
Iteration 333/1000 | Loss: 0.00007999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [7.999214722076431e-05, 7.999214722076431e-05, 7.999214722076431e-05, 7.999214722076431e-05, 7.999214722076431e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.999214722076431e-05

Optimization complete. Final v2v error: 6.3731303215026855 mm

Highest mean error: 19.53832244873047 mm for frame 18

Lowest mean error: 5.0454535484313965 mm for frame 13

Saving results

Total time: 338.7761552333832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772692
Iteration 2/25 | Loss: 0.00149153
Iteration 3/25 | Loss: 0.00128508
Iteration 4/25 | Loss: 0.00125812
Iteration 5/25 | Loss: 0.00126179
Iteration 6/25 | Loss: 0.00125400
Iteration 7/25 | Loss: 0.00124309
Iteration 8/25 | Loss: 0.00124176
Iteration 9/25 | Loss: 0.00124167
Iteration 10/25 | Loss: 0.00123829
Iteration 11/25 | Loss: 0.00123956
Iteration 12/25 | Loss: 0.00124158
Iteration 13/25 | Loss: 0.00124280
Iteration 14/25 | Loss: 0.00124379
Iteration 15/25 | Loss: 0.00124191
Iteration 16/25 | Loss: 0.00123938
Iteration 17/25 | Loss: 0.00123860
Iteration 18/25 | Loss: 0.00123801
Iteration 19/25 | Loss: 0.00123939
Iteration 20/25 | Loss: 0.00124188
Iteration 21/25 | Loss: 0.00124167
Iteration 22/25 | Loss: 0.00123809
Iteration 23/25 | Loss: 0.00123558
Iteration 24/25 | Loss: 0.00123642
Iteration 25/25 | Loss: 0.00123833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88333261
Iteration 2/25 | Loss: 0.00115417
Iteration 3/25 | Loss: 0.00115416
Iteration 4/25 | Loss: 0.00115416
Iteration 5/25 | Loss: 0.00115416
Iteration 6/25 | Loss: 0.00115416
Iteration 7/25 | Loss: 0.00115416
Iteration 8/25 | Loss: 0.00115416
Iteration 9/25 | Loss: 0.00115416
Iteration 10/25 | Loss: 0.00115416
Iteration 11/25 | Loss: 0.00115416
Iteration 12/25 | Loss: 0.00115416
Iteration 13/25 | Loss: 0.00115416
Iteration 14/25 | Loss: 0.00115416
Iteration 15/25 | Loss: 0.00115416
Iteration 16/25 | Loss: 0.00115416
Iteration 17/25 | Loss: 0.00115416
Iteration 18/25 | Loss: 0.00115416
Iteration 19/25 | Loss: 0.00115416
Iteration 20/25 | Loss: 0.00115416
Iteration 21/25 | Loss: 0.00115416
Iteration 22/25 | Loss: 0.00115416
Iteration 23/25 | Loss: 0.00115416
Iteration 24/25 | Loss: 0.00115416
Iteration 25/25 | Loss: 0.00115416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115416
Iteration 2/1000 | Loss: 0.00013860
Iteration 3/1000 | Loss: 0.00005375
Iteration 4/1000 | Loss: 0.00011501
Iteration 5/1000 | Loss: 0.00002529
Iteration 6/1000 | Loss: 0.00004075
Iteration 7/1000 | Loss: 0.00002242
Iteration 8/1000 | Loss: 0.00002111
Iteration 9/1000 | Loss: 0.00015613
Iteration 10/1000 | Loss: 0.00010736
Iteration 11/1000 | Loss: 0.00011969
Iteration 12/1000 | Loss: 0.00010002
Iteration 13/1000 | Loss: 0.00017262
Iteration 14/1000 | Loss: 0.00012572
Iteration 15/1000 | Loss: 0.00002711
Iteration 16/1000 | Loss: 0.00002388
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00001939
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00012280
Iteration 21/1000 | Loss: 0.00008206
Iteration 22/1000 | Loss: 0.00002224
Iteration 23/1000 | Loss: 0.00007664
Iteration 24/1000 | Loss: 0.00005763
Iteration 25/1000 | Loss: 0.00008043
Iteration 26/1000 | Loss: 0.00009069
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001641
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001528
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001519
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001510
Iteration 44/1000 | Loss: 0.00001509
Iteration 45/1000 | Loss: 0.00001509
Iteration 46/1000 | Loss: 0.00001508
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001504
Iteration 51/1000 | Loss: 0.00001503
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001499
Iteration 56/1000 | Loss: 0.00001499
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001498
Iteration 59/1000 | Loss: 0.00001498
Iteration 60/1000 | Loss: 0.00001498
Iteration 61/1000 | Loss: 0.00001498
Iteration 62/1000 | Loss: 0.00001497
Iteration 63/1000 | Loss: 0.00001497
Iteration 64/1000 | Loss: 0.00001497
Iteration 65/1000 | Loss: 0.00001497
Iteration 66/1000 | Loss: 0.00001496
Iteration 67/1000 | Loss: 0.00001496
Iteration 68/1000 | Loss: 0.00001496
Iteration 69/1000 | Loss: 0.00001495
Iteration 70/1000 | Loss: 0.00001495
Iteration 71/1000 | Loss: 0.00001495
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001494
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001494
Iteration 77/1000 | Loss: 0.00001493
Iteration 78/1000 | Loss: 0.00001492
Iteration 79/1000 | Loss: 0.00001492
Iteration 80/1000 | Loss: 0.00001492
Iteration 81/1000 | Loss: 0.00001492
Iteration 82/1000 | Loss: 0.00001492
Iteration 83/1000 | Loss: 0.00001491
Iteration 84/1000 | Loss: 0.00001491
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001488
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001488
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001486
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001480
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001477
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001477
Iteration 122/1000 | Loss: 0.00001477
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001476
Iteration 130/1000 | Loss: 0.00001476
Iteration 131/1000 | Loss: 0.00001476
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00001476
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001475
Iteration 139/1000 | Loss: 0.00001475
Iteration 140/1000 | Loss: 0.00001475
Iteration 141/1000 | Loss: 0.00001475
Iteration 142/1000 | Loss: 0.00001475
Iteration 143/1000 | Loss: 0.00001475
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001474
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001472
Iteration 161/1000 | Loss: 0.00001472
Iteration 162/1000 | Loss: 0.00001472
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Iteration 165/1000 | Loss: 0.00001472
Iteration 166/1000 | Loss: 0.00001472
Iteration 167/1000 | Loss: 0.00001472
Iteration 168/1000 | Loss: 0.00001472
Iteration 169/1000 | Loss: 0.00001472
Iteration 170/1000 | Loss: 0.00001472
Iteration 171/1000 | Loss: 0.00001472
Iteration 172/1000 | Loss: 0.00001472
Iteration 173/1000 | Loss: 0.00001472
Iteration 174/1000 | Loss: 0.00001472
Iteration 175/1000 | Loss: 0.00001472
Iteration 176/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4721625120728277e-05, 1.4721625120728277e-05, 1.4721625120728277e-05, 1.4721625120728277e-05, 1.4721625120728277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4721625120728277e-05

Optimization complete. Final v2v error: 3.214160919189453 mm

Highest mean error: 3.9951746463775635 mm for frame 38

Lowest mean error: 2.8340213298797607 mm for frame 188

Saving results

Total time: 123.60083818435669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784640
Iteration 2/25 | Loss: 0.00171403
Iteration 3/25 | Loss: 0.00131730
Iteration 4/25 | Loss: 0.00124489
Iteration 5/25 | Loss: 0.00123687
Iteration 6/25 | Loss: 0.00123529
Iteration 7/25 | Loss: 0.00123490
Iteration 8/25 | Loss: 0.00123490
Iteration 9/25 | Loss: 0.00123490
Iteration 10/25 | Loss: 0.00123490
Iteration 11/25 | Loss: 0.00123490
Iteration 12/25 | Loss: 0.00123490
Iteration 13/25 | Loss: 0.00123490
Iteration 14/25 | Loss: 0.00123490
Iteration 15/25 | Loss: 0.00123490
Iteration 16/25 | Loss: 0.00123490
Iteration 17/25 | Loss: 0.00123490
Iteration 18/25 | Loss: 0.00123490
Iteration 19/25 | Loss: 0.00123490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001234904513694346, 0.001234904513694346, 0.001234904513694346, 0.001234904513694346, 0.001234904513694346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001234904513694346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33031726
Iteration 2/25 | Loss: 0.00087324
Iteration 3/25 | Loss: 0.00087324
Iteration 4/25 | Loss: 0.00087324
Iteration 5/25 | Loss: 0.00087324
Iteration 6/25 | Loss: 0.00087324
Iteration 7/25 | Loss: 0.00087324
Iteration 8/25 | Loss: 0.00087324
Iteration 9/25 | Loss: 0.00087324
Iteration 10/25 | Loss: 0.00087324
Iteration 11/25 | Loss: 0.00087324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008732366259209812, 0.0008732366259209812, 0.0008732366259209812, 0.0008732366259209812, 0.0008732366259209812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008732366259209812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087324
Iteration 2/1000 | Loss: 0.00003094
Iteration 3/1000 | Loss: 0.00002140
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001598
Iteration 8/1000 | Loss: 0.00001563
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001476
Iteration 13/1000 | Loss: 0.00001470
Iteration 14/1000 | Loss: 0.00001461
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001456
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001455
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001451
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001451
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001450
Iteration 36/1000 | Loss: 0.00001450
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001449
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001449
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001433
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001429
Iteration 124/1000 | Loss: 0.00001429
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001427
Iteration 136/1000 | Loss: 0.00001427
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001427
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001426
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00001426
Iteration 147/1000 | Loss: 0.00001426
Iteration 148/1000 | Loss: 0.00001426
Iteration 149/1000 | Loss: 0.00001426
Iteration 150/1000 | Loss: 0.00001426
Iteration 151/1000 | Loss: 0.00001426
Iteration 152/1000 | Loss: 0.00001426
Iteration 153/1000 | Loss: 0.00001425
Iteration 154/1000 | Loss: 0.00001425
Iteration 155/1000 | Loss: 0.00001425
Iteration 156/1000 | Loss: 0.00001425
Iteration 157/1000 | Loss: 0.00001424
Iteration 158/1000 | Loss: 0.00001424
Iteration 159/1000 | Loss: 0.00001424
Iteration 160/1000 | Loss: 0.00001424
Iteration 161/1000 | Loss: 0.00001424
Iteration 162/1000 | Loss: 0.00001424
Iteration 163/1000 | Loss: 0.00001424
Iteration 164/1000 | Loss: 0.00001424
Iteration 165/1000 | Loss: 0.00001423
Iteration 166/1000 | Loss: 0.00001423
Iteration 167/1000 | Loss: 0.00001423
Iteration 168/1000 | Loss: 0.00001423
Iteration 169/1000 | Loss: 0.00001423
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001422
Iteration 172/1000 | Loss: 0.00001422
Iteration 173/1000 | Loss: 0.00001422
Iteration 174/1000 | Loss: 0.00001422
Iteration 175/1000 | Loss: 0.00001422
Iteration 176/1000 | Loss: 0.00001422
Iteration 177/1000 | Loss: 0.00001422
Iteration 178/1000 | Loss: 0.00001422
Iteration 179/1000 | Loss: 0.00001422
Iteration 180/1000 | Loss: 0.00001421
Iteration 181/1000 | Loss: 0.00001421
Iteration 182/1000 | Loss: 0.00001421
Iteration 183/1000 | Loss: 0.00001421
Iteration 184/1000 | Loss: 0.00001421
Iteration 185/1000 | Loss: 0.00001421
Iteration 186/1000 | Loss: 0.00001421
Iteration 187/1000 | Loss: 0.00001421
Iteration 188/1000 | Loss: 0.00001421
Iteration 189/1000 | Loss: 0.00001421
Iteration 190/1000 | Loss: 0.00001420
Iteration 191/1000 | Loss: 0.00001420
Iteration 192/1000 | Loss: 0.00001420
Iteration 193/1000 | Loss: 0.00001420
Iteration 194/1000 | Loss: 0.00001420
Iteration 195/1000 | Loss: 0.00001420
Iteration 196/1000 | Loss: 0.00001420
Iteration 197/1000 | Loss: 0.00001419
Iteration 198/1000 | Loss: 0.00001419
Iteration 199/1000 | Loss: 0.00001419
Iteration 200/1000 | Loss: 0.00001419
Iteration 201/1000 | Loss: 0.00001419
Iteration 202/1000 | Loss: 0.00001419
Iteration 203/1000 | Loss: 0.00001419
Iteration 204/1000 | Loss: 0.00001419
Iteration 205/1000 | Loss: 0.00001418
Iteration 206/1000 | Loss: 0.00001418
Iteration 207/1000 | Loss: 0.00001418
Iteration 208/1000 | Loss: 0.00001418
Iteration 209/1000 | Loss: 0.00001418
Iteration 210/1000 | Loss: 0.00001418
Iteration 211/1000 | Loss: 0.00001418
Iteration 212/1000 | Loss: 0.00001417
Iteration 213/1000 | Loss: 0.00001417
Iteration 214/1000 | Loss: 0.00001417
Iteration 215/1000 | Loss: 0.00001417
Iteration 216/1000 | Loss: 0.00001417
Iteration 217/1000 | Loss: 0.00001417
Iteration 218/1000 | Loss: 0.00001417
Iteration 219/1000 | Loss: 0.00001417
Iteration 220/1000 | Loss: 0.00001416
Iteration 221/1000 | Loss: 0.00001416
Iteration 222/1000 | Loss: 0.00001416
Iteration 223/1000 | Loss: 0.00001416
Iteration 224/1000 | Loss: 0.00001416
Iteration 225/1000 | Loss: 0.00001416
Iteration 226/1000 | Loss: 0.00001416
Iteration 227/1000 | Loss: 0.00001416
Iteration 228/1000 | Loss: 0.00001416
Iteration 229/1000 | Loss: 0.00001416
Iteration 230/1000 | Loss: 0.00001416
Iteration 231/1000 | Loss: 0.00001416
Iteration 232/1000 | Loss: 0.00001416
Iteration 233/1000 | Loss: 0.00001416
Iteration 234/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.4160577848087996e-05, 1.4160577848087996e-05, 1.4160577848087996e-05, 1.4160577848087996e-05, 1.4160577848087996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4160577848087996e-05

Optimization complete. Final v2v error: 3.215838670730591 mm

Highest mean error: 3.6608378887176514 mm for frame 31

Lowest mean error: 2.7332561016082764 mm for frame 3

Saving results

Total time: 52.87189435958862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022489
Iteration 2/25 | Loss: 0.00243803
Iteration 3/25 | Loss: 0.00178332
Iteration 4/25 | Loss: 0.00161290
Iteration 5/25 | Loss: 0.00147801
Iteration 6/25 | Loss: 0.00142901
Iteration 7/25 | Loss: 0.00136553
Iteration 8/25 | Loss: 0.00132248
Iteration 9/25 | Loss: 0.00130868
Iteration 10/25 | Loss: 0.00128165
Iteration 11/25 | Loss: 0.00126883
Iteration 12/25 | Loss: 0.00125314
Iteration 13/25 | Loss: 0.00125731
Iteration 14/25 | Loss: 0.00124203
Iteration 15/25 | Loss: 0.00124042
Iteration 16/25 | Loss: 0.00123991
Iteration 17/25 | Loss: 0.00123925
Iteration 18/25 | Loss: 0.00123809
Iteration 19/25 | Loss: 0.00123768
Iteration 20/25 | Loss: 0.00123736
Iteration 21/25 | Loss: 0.00124007
Iteration 22/25 | Loss: 0.00123472
Iteration 23/25 | Loss: 0.00123428
Iteration 24/25 | Loss: 0.00123414
Iteration 25/25 | Loss: 0.00123410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57164586
Iteration 2/25 | Loss: 0.00105204
Iteration 3/25 | Loss: 0.00105204
Iteration 4/25 | Loss: 0.00105204
Iteration 5/25 | Loss: 0.00105204
Iteration 6/25 | Loss: 0.00105204
Iteration 7/25 | Loss: 0.00105203
Iteration 8/25 | Loss: 0.00105203
Iteration 9/25 | Loss: 0.00105203
Iteration 10/25 | Loss: 0.00105203
Iteration 11/25 | Loss: 0.00105203
Iteration 12/25 | Loss: 0.00105203
Iteration 13/25 | Loss: 0.00105203
Iteration 14/25 | Loss: 0.00105203
Iteration 15/25 | Loss: 0.00105203
Iteration 16/25 | Loss: 0.00105203
Iteration 17/25 | Loss: 0.00105203
Iteration 18/25 | Loss: 0.00105203
Iteration 19/25 | Loss: 0.00105203
Iteration 20/25 | Loss: 0.00105203
Iteration 21/25 | Loss: 0.00105203
Iteration 22/25 | Loss: 0.00105203
Iteration 23/25 | Loss: 0.00105203
Iteration 24/25 | Loss: 0.00105203
Iteration 25/25 | Loss: 0.00105203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105203
Iteration 2/1000 | Loss: 0.00004523
Iteration 3/1000 | Loss: 0.00020935
Iteration 4/1000 | Loss: 0.00004154
Iteration 5/1000 | Loss: 0.00002912
Iteration 6/1000 | Loss: 0.00002632
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00023773
Iteration 9/1000 | Loss: 0.00294586
Iteration 10/1000 | Loss: 0.00029656
Iteration 11/1000 | Loss: 0.00050619
Iteration 12/1000 | Loss: 0.00003058
Iteration 13/1000 | Loss: 0.00002658
Iteration 14/1000 | Loss: 0.00002360
Iteration 15/1000 | Loss: 0.00002258
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002173
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00095071
Iteration 20/1000 | Loss: 0.00002680
Iteration 21/1000 | Loss: 0.00002146
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00021143
Iteration 24/1000 | Loss: 0.00108497
Iteration 25/1000 | Loss: 0.00001679
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00021292
Iteration 38/1000 | Loss: 0.00002574
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001337
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001335
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001325
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001321
Iteration 59/1000 | Loss: 0.00001321
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001320
Iteration 62/1000 | Loss: 0.00001320
Iteration 63/1000 | Loss: 0.00001319
Iteration 64/1000 | Loss: 0.00001319
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001318
Iteration 77/1000 | Loss: 0.00001318
Iteration 78/1000 | Loss: 0.00001317
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001315
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001315
Iteration 103/1000 | Loss: 0.00001315
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001314
Iteration 106/1000 | Loss: 0.00001314
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001314
Iteration 109/1000 | Loss: 0.00001314
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001314
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.313820030190982e-05, 1.313820030190982e-05, 1.313820030190982e-05, 1.313820030190982e-05, 1.313820030190982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.313820030190982e-05

Optimization complete. Final v2v error: 3.093256711959839 mm

Highest mean error: 3.6215367317199707 mm for frame 49

Lowest mean error: 2.8213844299316406 mm for frame 30

Saving results

Total time: 100.41898465156555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384597
Iteration 2/25 | Loss: 0.00129706
Iteration 3/25 | Loss: 0.00121148
Iteration 4/25 | Loss: 0.00119919
Iteration 5/25 | Loss: 0.00119507
Iteration 6/25 | Loss: 0.00119398
Iteration 7/25 | Loss: 0.00119393
Iteration 8/25 | Loss: 0.00119393
Iteration 9/25 | Loss: 0.00119393
Iteration 10/25 | Loss: 0.00119393
Iteration 11/25 | Loss: 0.00119393
Iteration 12/25 | Loss: 0.00119393
Iteration 13/25 | Loss: 0.00119393
Iteration 14/25 | Loss: 0.00119393
Iteration 15/25 | Loss: 0.00119393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011939293472096324, 0.0011939293472096324, 0.0011939293472096324, 0.0011939293472096324, 0.0011939293472096324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011939293472096324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31525671
Iteration 2/25 | Loss: 0.00102714
Iteration 3/25 | Loss: 0.00102714
Iteration 4/25 | Loss: 0.00102713
Iteration 5/25 | Loss: 0.00102713
Iteration 6/25 | Loss: 0.00102713
Iteration 7/25 | Loss: 0.00102713
Iteration 8/25 | Loss: 0.00102713
Iteration 9/25 | Loss: 0.00102713
Iteration 10/25 | Loss: 0.00102713
Iteration 11/25 | Loss: 0.00102713
Iteration 12/25 | Loss: 0.00102713
Iteration 13/25 | Loss: 0.00102713
Iteration 14/25 | Loss: 0.00102713
Iteration 15/25 | Loss: 0.00102713
Iteration 16/25 | Loss: 0.00102713
Iteration 17/25 | Loss: 0.00102713
Iteration 18/25 | Loss: 0.00102713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010271321516484022, 0.0010271321516484022, 0.0010271321516484022, 0.0010271321516484022, 0.0010271321516484022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010271321516484022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102713
Iteration 2/1000 | Loss: 0.00005216
Iteration 3/1000 | Loss: 0.00003488
Iteration 4/1000 | Loss: 0.00002769
Iteration 5/1000 | Loss: 0.00002464
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00001987
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00001895
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001813
Iteration 14/1000 | Loss: 0.00001793
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001743
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001728
Iteration 45/1000 | Loss: 0.00001728
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001727
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00001727
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001724
Iteration 69/1000 | Loss: 0.00001724
Iteration 70/1000 | Loss: 0.00001724
Iteration 71/1000 | Loss: 0.00001724
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001723
Iteration 75/1000 | Loss: 0.00001723
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001716
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001716
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001713
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001712
Iteration 148/1000 | Loss: 0.00001712
Iteration 149/1000 | Loss: 0.00001712
Iteration 150/1000 | Loss: 0.00001712
Iteration 151/1000 | Loss: 0.00001712
Iteration 152/1000 | Loss: 0.00001712
Iteration 153/1000 | Loss: 0.00001712
Iteration 154/1000 | Loss: 0.00001712
Iteration 155/1000 | Loss: 0.00001712
Iteration 156/1000 | Loss: 0.00001712
Iteration 157/1000 | Loss: 0.00001712
Iteration 158/1000 | Loss: 0.00001712
Iteration 159/1000 | Loss: 0.00001711
Iteration 160/1000 | Loss: 0.00001711
Iteration 161/1000 | Loss: 0.00001711
Iteration 162/1000 | Loss: 0.00001711
Iteration 163/1000 | Loss: 0.00001711
Iteration 164/1000 | Loss: 0.00001711
Iteration 165/1000 | Loss: 0.00001711
Iteration 166/1000 | Loss: 0.00001711
Iteration 167/1000 | Loss: 0.00001711
Iteration 168/1000 | Loss: 0.00001711
Iteration 169/1000 | Loss: 0.00001711
Iteration 170/1000 | Loss: 0.00001711
Iteration 171/1000 | Loss: 0.00001711
Iteration 172/1000 | Loss: 0.00001711
Iteration 173/1000 | Loss: 0.00001711
Iteration 174/1000 | Loss: 0.00001711
Iteration 175/1000 | Loss: 0.00001711
Iteration 176/1000 | Loss: 0.00001711
Iteration 177/1000 | Loss: 0.00001711
Iteration 178/1000 | Loss: 0.00001711
Iteration 179/1000 | Loss: 0.00001711
Iteration 180/1000 | Loss: 0.00001711
Iteration 181/1000 | Loss: 0.00001711
Iteration 182/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.71114534168737e-05, 1.71114534168737e-05, 1.71114534168737e-05, 1.71114534168737e-05, 1.71114534168737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.71114534168737e-05

Optimization complete. Final v2v error: 3.477698802947998 mm

Highest mean error: 4.226039886474609 mm for frame 51

Lowest mean error: 2.8387033939361572 mm for frame 9

Saving results

Total time: 46.610504150390625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815285
Iteration 2/25 | Loss: 0.00149091
Iteration 3/25 | Loss: 0.00128395
Iteration 4/25 | Loss: 0.00127458
Iteration 5/25 | Loss: 0.00127330
Iteration 6/25 | Loss: 0.00127330
Iteration 7/25 | Loss: 0.00127330
Iteration 8/25 | Loss: 0.00127330
Iteration 9/25 | Loss: 0.00127330
Iteration 10/25 | Loss: 0.00127330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012733032926917076, 0.0012733032926917076, 0.0012733032926917076, 0.0012733032926917076, 0.0012733032926917076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012733032926917076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97298151
Iteration 2/25 | Loss: 0.00057912
Iteration 3/25 | Loss: 0.00057912
Iteration 4/25 | Loss: 0.00057912
Iteration 5/25 | Loss: 0.00057911
Iteration 6/25 | Loss: 0.00057911
Iteration 7/25 | Loss: 0.00057911
Iteration 8/25 | Loss: 0.00057911
Iteration 9/25 | Loss: 0.00057911
Iteration 10/25 | Loss: 0.00057911
Iteration 11/25 | Loss: 0.00057911
Iteration 12/25 | Loss: 0.00057911
Iteration 13/25 | Loss: 0.00057911
Iteration 14/25 | Loss: 0.00057911
Iteration 15/25 | Loss: 0.00057911
Iteration 16/25 | Loss: 0.00057911
Iteration 17/25 | Loss: 0.00057911
Iteration 18/25 | Loss: 0.00057911
Iteration 19/25 | Loss: 0.00057911
Iteration 20/25 | Loss: 0.00057911
Iteration 21/25 | Loss: 0.00057911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005791131989099085, 0.0005791131989099085, 0.0005791131989099085, 0.0005791131989099085, 0.0005791131989099085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005791131989099085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057911
Iteration 2/1000 | Loss: 0.00003446
Iteration 3/1000 | Loss: 0.00002663
Iteration 4/1000 | Loss: 0.00002459
Iteration 5/1000 | Loss: 0.00002353
Iteration 6/1000 | Loss: 0.00002288
Iteration 7/1000 | Loss: 0.00002228
Iteration 8/1000 | Loss: 0.00002200
Iteration 9/1000 | Loss: 0.00002173
Iteration 10/1000 | Loss: 0.00002145
Iteration 11/1000 | Loss: 0.00002126
Iteration 12/1000 | Loss: 0.00002125
Iteration 13/1000 | Loss: 0.00002118
Iteration 14/1000 | Loss: 0.00002106
Iteration 15/1000 | Loss: 0.00002103
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002102
Iteration 19/1000 | Loss: 0.00002101
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00002100
Iteration 22/1000 | Loss: 0.00002094
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002091
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002083
Iteration 40/1000 | Loss: 0.00002083
Iteration 41/1000 | Loss: 0.00002083
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002082
Iteration 50/1000 | Loss: 0.00002082
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002082
Iteration 55/1000 | Loss: 0.00002081
Iteration 56/1000 | Loss: 0.00002081
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00002080
Iteration 59/1000 | Loss: 0.00002080
Iteration 60/1000 | Loss: 0.00002080
Iteration 61/1000 | Loss: 0.00002079
Iteration 62/1000 | Loss: 0.00002079
Iteration 63/1000 | Loss: 0.00002079
Iteration 64/1000 | Loss: 0.00002078
Iteration 65/1000 | Loss: 0.00002078
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002075
Iteration 69/1000 | Loss: 0.00002075
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002069
Iteration 84/1000 | Loss: 0.00002069
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002067
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002067
Iteration 93/1000 | Loss: 0.00002065
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002062
Iteration 97/1000 | Loss: 0.00002062
Iteration 98/1000 | Loss: 0.00002062
Iteration 99/1000 | Loss: 0.00002062
Iteration 100/1000 | Loss: 0.00002062
Iteration 101/1000 | Loss: 0.00002062
Iteration 102/1000 | Loss: 0.00002062
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002061
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002060
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002060
Iteration 120/1000 | Loss: 0.00002060
Iteration 121/1000 | Loss: 0.00002060
Iteration 122/1000 | Loss: 0.00002060
Iteration 123/1000 | Loss: 0.00002059
Iteration 124/1000 | Loss: 0.00002059
Iteration 125/1000 | Loss: 0.00002059
Iteration 126/1000 | Loss: 0.00002059
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002058
Iteration 133/1000 | Loss: 0.00002058
Iteration 134/1000 | Loss: 0.00002058
Iteration 135/1000 | Loss: 0.00002058
Iteration 136/1000 | Loss: 0.00002058
Iteration 137/1000 | Loss: 0.00002058
Iteration 138/1000 | Loss: 0.00002057
Iteration 139/1000 | Loss: 0.00002057
Iteration 140/1000 | Loss: 0.00002057
Iteration 141/1000 | Loss: 0.00002057
Iteration 142/1000 | Loss: 0.00002057
Iteration 143/1000 | Loss: 0.00002057
Iteration 144/1000 | Loss: 0.00002057
Iteration 145/1000 | Loss: 0.00002057
Iteration 146/1000 | Loss: 0.00002057
Iteration 147/1000 | Loss: 0.00002057
Iteration 148/1000 | Loss: 0.00002057
Iteration 149/1000 | Loss: 0.00002057
Iteration 150/1000 | Loss: 0.00002057
Iteration 151/1000 | Loss: 0.00002056
Iteration 152/1000 | Loss: 0.00002056
Iteration 153/1000 | Loss: 0.00002056
Iteration 154/1000 | Loss: 0.00002056
Iteration 155/1000 | Loss: 0.00002056
Iteration 156/1000 | Loss: 0.00002056
Iteration 157/1000 | Loss: 0.00002056
Iteration 158/1000 | Loss: 0.00002056
Iteration 159/1000 | Loss: 0.00002056
Iteration 160/1000 | Loss: 0.00002055
Iteration 161/1000 | Loss: 0.00002055
Iteration 162/1000 | Loss: 0.00002055
Iteration 163/1000 | Loss: 0.00002055
Iteration 164/1000 | Loss: 0.00002055
Iteration 165/1000 | Loss: 0.00002055
Iteration 166/1000 | Loss: 0.00002055
Iteration 167/1000 | Loss: 0.00002055
Iteration 168/1000 | Loss: 0.00002055
Iteration 169/1000 | Loss: 0.00002055
Iteration 170/1000 | Loss: 0.00002055
Iteration 171/1000 | Loss: 0.00002055
Iteration 172/1000 | Loss: 0.00002055
Iteration 173/1000 | Loss: 0.00002055
Iteration 174/1000 | Loss: 0.00002055
Iteration 175/1000 | Loss: 0.00002055
Iteration 176/1000 | Loss: 0.00002055
Iteration 177/1000 | Loss: 0.00002055
Iteration 178/1000 | Loss: 0.00002055
Iteration 179/1000 | Loss: 0.00002055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.0548182874335907e-05, 2.0548182874335907e-05, 2.0548182874335907e-05, 2.0548182874335907e-05, 2.0548182874335907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0548182874335907e-05

Optimization complete. Final v2v error: 3.771677017211914 mm

Highest mean error: 3.8599493503570557 mm for frame 3

Lowest mean error: 3.6582083702087402 mm for frame 95

Saving results

Total time: 39.913153886795044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418583
Iteration 2/25 | Loss: 0.00128296
Iteration 3/25 | Loss: 0.00121398
Iteration 4/25 | Loss: 0.00120674
Iteration 5/25 | Loss: 0.00120492
Iteration 6/25 | Loss: 0.00120492
Iteration 7/25 | Loss: 0.00120492
Iteration 8/25 | Loss: 0.00120492
Iteration 9/25 | Loss: 0.00120492
Iteration 10/25 | Loss: 0.00120492
Iteration 11/25 | Loss: 0.00120492
Iteration 12/25 | Loss: 0.00120492
Iteration 13/25 | Loss: 0.00120492
Iteration 14/25 | Loss: 0.00120492
Iteration 15/25 | Loss: 0.00120492
Iteration 16/25 | Loss: 0.00120492
Iteration 17/25 | Loss: 0.00120492
Iteration 18/25 | Loss: 0.00120492
Iteration 19/25 | Loss: 0.00120492
Iteration 20/25 | Loss: 0.00120492
Iteration 21/25 | Loss: 0.00120492
Iteration 22/25 | Loss: 0.00120492
Iteration 23/25 | Loss: 0.00120492
Iteration 24/25 | Loss: 0.00120492
Iteration 25/25 | Loss: 0.00120492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36237884
Iteration 2/25 | Loss: 0.00093680
Iteration 3/25 | Loss: 0.00093680
Iteration 4/25 | Loss: 0.00093680
Iteration 5/25 | Loss: 0.00093680
Iteration 6/25 | Loss: 0.00093680
Iteration 7/25 | Loss: 0.00093680
Iteration 8/25 | Loss: 0.00093680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0009367979946546257, 0.0009367979946546257, 0.0009367979946546257, 0.0009367979946546257, 0.0009367979946546257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009367979946546257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093680
Iteration 2/1000 | Loss: 0.00001721
Iteration 3/1000 | Loss: 0.00001432
Iteration 4/1000 | Loss: 0.00001327
Iteration 5/1000 | Loss: 0.00001258
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00001110
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001105
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001104
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001103
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001091
Iteration 34/1000 | Loss: 0.00001089
Iteration 35/1000 | Loss: 0.00001089
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001071
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001067
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001066
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001065
Iteration 47/1000 | Loss: 0.00001065
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001064
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001060
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001059
Iteration 60/1000 | Loss: 0.00001059
Iteration 61/1000 | Loss: 0.00001059
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001052
Iteration 79/1000 | Loss: 0.00001052
Iteration 80/1000 | Loss: 0.00001052
Iteration 81/1000 | Loss: 0.00001052
Iteration 82/1000 | Loss: 0.00001052
Iteration 83/1000 | Loss: 0.00001051
Iteration 84/1000 | Loss: 0.00001051
Iteration 85/1000 | Loss: 0.00001051
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001051
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001050
Iteration 91/1000 | Loss: 0.00001050
Iteration 92/1000 | Loss: 0.00001050
Iteration 93/1000 | Loss: 0.00001050
Iteration 94/1000 | Loss: 0.00001050
Iteration 95/1000 | Loss: 0.00001050
Iteration 96/1000 | Loss: 0.00001049
Iteration 97/1000 | Loss: 0.00001049
Iteration 98/1000 | Loss: 0.00001049
Iteration 99/1000 | Loss: 0.00001049
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001048
Iteration 102/1000 | Loss: 0.00001048
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001047
Iteration 105/1000 | Loss: 0.00001047
Iteration 106/1000 | Loss: 0.00001047
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001047
Iteration 110/1000 | Loss: 0.00001047
Iteration 111/1000 | Loss: 0.00001047
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001044
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001043
Iteration 125/1000 | Loss: 0.00001043
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001043
Iteration 133/1000 | Loss: 0.00001043
Iteration 134/1000 | Loss: 0.00001043
Iteration 135/1000 | Loss: 0.00001043
Iteration 136/1000 | Loss: 0.00001043
Iteration 137/1000 | Loss: 0.00001043
Iteration 138/1000 | Loss: 0.00001043
Iteration 139/1000 | Loss: 0.00001043
Iteration 140/1000 | Loss: 0.00001043
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001043
Iteration 159/1000 | Loss: 0.00001043
Iteration 160/1000 | Loss: 0.00001043
Iteration 161/1000 | Loss: 0.00001043
Iteration 162/1000 | Loss: 0.00001043
Iteration 163/1000 | Loss: 0.00001043
Iteration 164/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.0432027011120226e-05, 1.0432027011120226e-05, 1.0432027011120226e-05, 1.0432027011120226e-05, 1.0432027011120226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0432027011120226e-05

Optimization complete. Final v2v error: 2.82269549369812 mm

Highest mean error: 2.98284912109375 mm for frame 236

Lowest mean error: 2.740673780441284 mm for frame 197

Saving results

Total time: 40.6876060962677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436301
Iteration 2/25 | Loss: 0.00129059
Iteration 3/25 | Loss: 0.00121078
Iteration 4/25 | Loss: 0.00119877
Iteration 5/25 | Loss: 0.00119555
Iteration 6/25 | Loss: 0.00119431
Iteration 7/25 | Loss: 0.00119431
Iteration 8/25 | Loss: 0.00119431
Iteration 9/25 | Loss: 0.00119431
Iteration 10/25 | Loss: 0.00119431
Iteration 11/25 | Loss: 0.00119431
Iteration 12/25 | Loss: 0.00119431
Iteration 13/25 | Loss: 0.00119431
Iteration 14/25 | Loss: 0.00119431
Iteration 15/25 | Loss: 0.00119431
Iteration 16/25 | Loss: 0.00119431
Iteration 17/25 | Loss: 0.00119431
Iteration 18/25 | Loss: 0.00119431
Iteration 19/25 | Loss: 0.00119431
Iteration 20/25 | Loss: 0.00119431
Iteration 21/25 | Loss: 0.00119431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001194306998513639, 0.001194306998513639, 0.001194306998513639, 0.001194306998513639, 0.001194306998513639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194306998513639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33544052
Iteration 2/25 | Loss: 0.00100723
Iteration 3/25 | Loss: 0.00100720
Iteration 4/25 | Loss: 0.00100720
Iteration 5/25 | Loss: 0.00100720
Iteration 6/25 | Loss: 0.00100720
Iteration 7/25 | Loss: 0.00100720
Iteration 8/25 | Loss: 0.00100720
Iteration 9/25 | Loss: 0.00100720
Iteration 10/25 | Loss: 0.00100720
Iteration 11/25 | Loss: 0.00100720
Iteration 12/25 | Loss: 0.00100720
Iteration 13/25 | Loss: 0.00100720
Iteration 14/25 | Loss: 0.00100720
Iteration 15/25 | Loss: 0.00100720
Iteration 16/25 | Loss: 0.00100720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010071999859064817, 0.0010071999859064817, 0.0010071999859064817, 0.0010071999859064817, 0.0010071999859064817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010071999859064817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100720
Iteration 2/1000 | Loss: 0.00002797
Iteration 3/1000 | Loss: 0.00001680
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001246
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001211
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001205
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001200
Iteration 17/1000 | Loss: 0.00001200
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001196
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001164
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001162
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001157
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001154
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001150
Iteration 124/1000 | Loss: 0.00001150
Iteration 125/1000 | Loss: 0.00001150
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001148
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001145
Iteration 148/1000 | Loss: 0.00001145
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001144
Iteration 156/1000 | Loss: 0.00001144
Iteration 157/1000 | Loss: 0.00001144
Iteration 158/1000 | Loss: 0.00001144
Iteration 159/1000 | Loss: 0.00001144
Iteration 160/1000 | Loss: 0.00001144
Iteration 161/1000 | Loss: 0.00001144
Iteration 162/1000 | Loss: 0.00001144
Iteration 163/1000 | Loss: 0.00001144
Iteration 164/1000 | Loss: 0.00001144
Iteration 165/1000 | Loss: 0.00001144
Iteration 166/1000 | Loss: 0.00001144
Iteration 167/1000 | Loss: 0.00001144
Iteration 168/1000 | Loss: 0.00001144
Iteration 169/1000 | Loss: 0.00001144
Iteration 170/1000 | Loss: 0.00001144
Iteration 171/1000 | Loss: 0.00001144
Iteration 172/1000 | Loss: 0.00001144
Iteration 173/1000 | Loss: 0.00001144
Iteration 174/1000 | Loss: 0.00001144
Iteration 175/1000 | Loss: 0.00001144
Iteration 176/1000 | Loss: 0.00001144
Iteration 177/1000 | Loss: 0.00001144
Iteration 178/1000 | Loss: 0.00001144
Iteration 179/1000 | Loss: 0.00001144
Iteration 180/1000 | Loss: 0.00001144
Iteration 181/1000 | Loss: 0.00001144
Iteration 182/1000 | Loss: 0.00001144
Iteration 183/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1436538443376776e-05, 1.1436538443376776e-05, 1.1436538443376776e-05, 1.1436538443376776e-05, 1.1436538443376776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1436538443376776e-05

Optimization complete. Final v2v error: 2.8878040313720703 mm

Highest mean error: 3.6894073486328125 mm for frame 62

Lowest mean error: 2.4835891723632812 mm for frame 29

Saving results

Total time: 41.8510525226593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985198
Iteration 2/25 | Loss: 0.00321136
Iteration 3/25 | Loss: 0.00246506
Iteration 4/25 | Loss: 0.00213752
Iteration 5/25 | Loss: 0.00228619
Iteration 6/25 | Loss: 0.00204759
Iteration 7/25 | Loss: 0.00190017
Iteration 8/25 | Loss: 0.00175837
Iteration 9/25 | Loss: 0.00170608
Iteration 10/25 | Loss: 0.00169330
Iteration 11/25 | Loss: 0.00168531
Iteration 12/25 | Loss: 0.00167015
Iteration 13/25 | Loss: 0.00166594
Iteration 14/25 | Loss: 0.00167541
Iteration 15/25 | Loss: 0.00165644
Iteration 16/25 | Loss: 0.00165376
Iteration 17/25 | Loss: 0.00165242
Iteration 18/25 | Loss: 0.00165139
Iteration 19/25 | Loss: 0.00165495
Iteration 20/25 | Loss: 0.00165056
Iteration 21/25 | Loss: 0.00165126
Iteration 22/25 | Loss: 0.00165161
Iteration 23/25 | Loss: 0.00165035
Iteration 24/25 | Loss: 0.00164603
Iteration 25/25 | Loss: 0.00164452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37072110
Iteration 2/25 | Loss: 0.00877578
Iteration 3/25 | Loss: 0.00485854
Iteration 4/25 | Loss: 0.00485854
Iteration 5/25 | Loss: 0.00485854
Iteration 6/25 | Loss: 0.00485853
Iteration 7/25 | Loss: 0.00485853
Iteration 8/25 | Loss: 0.00485853
Iteration 9/25 | Loss: 0.00485853
Iteration 10/25 | Loss: 0.00485853
Iteration 11/25 | Loss: 0.00485853
Iteration 12/25 | Loss: 0.00485853
Iteration 13/25 | Loss: 0.00485853
Iteration 14/25 | Loss: 0.00485853
Iteration 15/25 | Loss: 0.00485853
Iteration 16/25 | Loss: 0.00485853
Iteration 17/25 | Loss: 0.00485853
Iteration 18/25 | Loss: 0.00485853
Iteration 19/25 | Loss: 0.00485853
Iteration 20/25 | Loss: 0.00485853
Iteration 21/25 | Loss: 0.00485853
Iteration 22/25 | Loss: 0.00485853
Iteration 23/25 | Loss: 0.00485853
Iteration 24/25 | Loss: 0.00485853
Iteration 25/25 | Loss: 0.00485853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00485853
Iteration 2/1000 | Loss: 0.00106856
Iteration 3/1000 | Loss: 0.00096257
Iteration 4/1000 | Loss: 0.00069532
Iteration 5/1000 | Loss: 0.00176894
Iteration 6/1000 | Loss: 0.00338621
Iteration 7/1000 | Loss: 0.00032948
Iteration 8/1000 | Loss: 0.00204359
Iteration 9/1000 | Loss: 0.00025514
Iteration 10/1000 | Loss: 0.00545658
Iteration 11/1000 | Loss: 0.00024570
Iteration 12/1000 | Loss: 0.00166746
Iteration 13/1000 | Loss: 0.00072125
Iteration 14/1000 | Loss: 0.00392762
Iteration 15/1000 | Loss: 0.00128714
Iteration 16/1000 | Loss: 0.00046092
Iteration 17/1000 | Loss: 0.00018239
Iteration 18/1000 | Loss: 0.00137339
Iteration 19/1000 | Loss: 0.00243911
Iteration 20/1000 | Loss: 0.00437143
Iteration 21/1000 | Loss: 0.00297370
Iteration 22/1000 | Loss: 0.00178878
Iteration 23/1000 | Loss: 0.00087322
Iteration 24/1000 | Loss: 0.00141430
Iteration 25/1000 | Loss: 0.00080574
Iteration 26/1000 | Loss: 0.00049380
Iteration 27/1000 | Loss: 0.00023363
Iteration 28/1000 | Loss: 0.00166608
Iteration 29/1000 | Loss: 0.00193926
Iteration 30/1000 | Loss: 0.00265419
Iteration 31/1000 | Loss: 0.00072839
Iteration 32/1000 | Loss: 0.00020976
Iteration 33/1000 | Loss: 0.00101093
Iteration 34/1000 | Loss: 0.00120465
Iteration 35/1000 | Loss: 0.00099308
Iteration 36/1000 | Loss: 0.00068494
Iteration 37/1000 | Loss: 0.00044873
Iteration 38/1000 | Loss: 0.00073410
Iteration 39/1000 | Loss: 0.00082435
Iteration 40/1000 | Loss: 0.00042752
Iteration 41/1000 | Loss: 0.00118302
Iteration 42/1000 | Loss: 0.00041151
Iteration 43/1000 | Loss: 0.00108021
Iteration 44/1000 | Loss: 0.00043199
Iteration 45/1000 | Loss: 0.00038488
Iteration 46/1000 | Loss: 0.00093181
Iteration 47/1000 | Loss: 0.00028986
Iteration 48/1000 | Loss: 0.00029063
Iteration 49/1000 | Loss: 0.00139503
Iteration 50/1000 | Loss: 0.00055976
Iteration 51/1000 | Loss: 0.00059105
Iteration 52/1000 | Loss: 0.00086193
Iteration 53/1000 | Loss: 0.00056675
Iteration 54/1000 | Loss: 0.00037986
Iteration 55/1000 | Loss: 0.00009861
Iteration 56/1000 | Loss: 0.00009237
Iteration 57/1000 | Loss: 0.00045036
Iteration 58/1000 | Loss: 0.00054182
Iteration 59/1000 | Loss: 0.00019102
Iteration 60/1000 | Loss: 0.00008532
Iteration 61/1000 | Loss: 0.00038388
Iteration 62/1000 | Loss: 0.00008412
Iteration 63/1000 | Loss: 0.00062476
Iteration 64/1000 | Loss: 0.00065556
Iteration 65/1000 | Loss: 0.00164662
Iteration 66/1000 | Loss: 0.00163104
Iteration 67/1000 | Loss: 0.00018843
Iteration 68/1000 | Loss: 0.00016033
Iteration 69/1000 | Loss: 0.00007279
Iteration 70/1000 | Loss: 0.00093086
Iteration 71/1000 | Loss: 0.00036166
Iteration 72/1000 | Loss: 0.00033702
Iteration 73/1000 | Loss: 0.00006452
Iteration 74/1000 | Loss: 0.00006051
Iteration 75/1000 | Loss: 0.00005746
Iteration 76/1000 | Loss: 0.00005553
Iteration 77/1000 | Loss: 0.00022809
Iteration 78/1000 | Loss: 0.00005821
Iteration 79/1000 | Loss: 0.00005386
Iteration 80/1000 | Loss: 0.00005273
Iteration 81/1000 | Loss: 0.00025196
Iteration 82/1000 | Loss: 0.00014138
Iteration 83/1000 | Loss: 0.00046108
Iteration 84/1000 | Loss: 0.00054140
Iteration 85/1000 | Loss: 0.00042917
Iteration 86/1000 | Loss: 0.00006628
Iteration 87/1000 | Loss: 0.00075638
Iteration 88/1000 | Loss: 0.00053747
Iteration 89/1000 | Loss: 0.00005955
Iteration 90/1000 | Loss: 0.00005346
Iteration 91/1000 | Loss: 0.00004958
Iteration 92/1000 | Loss: 0.00004697
Iteration 93/1000 | Loss: 0.00004595
Iteration 94/1000 | Loss: 0.00057515
Iteration 95/1000 | Loss: 0.00045412
Iteration 96/1000 | Loss: 0.00045862
Iteration 97/1000 | Loss: 0.00048117
Iteration 98/1000 | Loss: 0.00031101
Iteration 99/1000 | Loss: 0.00006035
Iteration 100/1000 | Loss: 0.00005139
Iteration 101/1000 | Loss: 0.00004675
Iteration 102/1000 | Loss: 0.00004496
Iteration 103/1000 | Loss: 0.00004414
Iteration 104/1000 | Loss: 0.00004341
Iteration 105/1000 | Loss: 0.00004257
Iteration 106/1000 | Loss: 0.00030830
Iteration 107/1000 | Loss: 0.00004486
Iteration 108/1000 | Loss: 0.00004151
Iteration 109/1000 | Loss: 0.00004061
Iteration 110/1000 | Loss: 0.00004009
Iteration 111/1000 | Loss: 0.00003966
Iteration 112/1000 | Loss: 0.00003952
Iteration 113/1000 | Loss: 0.00003943
Iteration 114/1000 | Loss: 0.00003943
Iteration 115/1000 | Loss: 0.00003943
Iteration 116/1000 | Loss: 0.00003942
Iteration 117/1000 | Loss: 0.00003942
Iteration 118/1000 | Loss: 0.00003942
Iteration 119/1000 | Loss: 0.00003942
Iteration 120/1000 | Loss: 0.00003942
Iteration 121/1000 | Loss: 0.00003939
Iteration 122/1000 | Loss: 0.00003938
Iteration 123/1000 | Loss: 0.00003922
Iteration 124/1000 | Loss: 0.00003904
Iteration 125/1000 | Loss: 0.00003891
Iteration 126/1000 | Loss: 0.00003890
Iteration 127/1000 | Loss: 0.00003890
Iteration 128/1000 | Loss: 0.00003890
Iteration 129/1000 | Loss: 0.00003890
Iteration 130/1000 | Loss: 0.00003888
Iteration 131/1000 | Loss: 0.00003887
Iteration 132/1000 | Loss: 0.00003886
Iteration 133/1000 | Loss: 0.00003885
Iteration 134/1000 | Loss: 0.00003884
Iteration 135/1000 | Loss: 0.00003884
Iteration 136/1000 | Loss: 0.00003883
Iteration 137/1000 | Loss: 0.00003882
Iteration 138/1000 | Loss: 0.00003882
Iteration 139/1000 | Loss: 0.00003881
Iteration 140/1000 | Loss: 0.00003880
Iteration 141/1000 | Loss: 0.00003879
Iteration 142/1000 | Loss: 0.00003879
Iteration 143/1000 | Loss: 0.00003879
Iteration 144/1000 | Loss: 0.00003879
Iteration 145/1000 | Loss: 0.00003878
Iteration 146/1000 | Loss: 0.00003878
Iteration 147/1000 | Loss: 0.00003877
Iteration 148/1000 | Loss: 0.00003877
Iteration 149/1000 | Loss: 0.00003876
Iteration 150/1000 | Loss: 0.00003876
Iteration 151/1000 | Loss: 0.00003876
Iteration 152/1000 | Loss: 0.00003875
Iteration 153/1000 | Loss: 0.00003875
Iteration 154/1000 | Loss: 0.00003874
Iteration 155/1000 | Loss: 0.00003873
Iteration 156/1000 | Loss: 0.00003873
Iteration 157/1000 | Loss: 0.00003872
Iteration 158/1000 | Loss: 0.00003872
Iteration 159/1000 | Loss: 0.00003872
Iteration 160/1000 | Loss: 0.00003871
Iteration 161/1000 | Loss: 0.00003871
Iteration 162/1000 | Loss: 0.00003871
Iteration 163/1000 | Loss: 0.00003870
Iteration 164/1000 | Loss: 0.00003865
Iteration 165/1000 | Loss: 0.00003864
Iteration 166/1000 | Loss: 0.00003864
Iteration 167/1000 | Loss: 0.00003863
Iteration 168/1000 | Loss: 0.00003862
Iteration 169/1000 | Loss: 0.00003862
Iteration 170/1000 | Loss: 0.00003859
Iteration 171/1000 | Loss: 0.00003845
Iteration 172/1000 | Loss: 0.00003828
Iteration 173/1000 | Loss: 0.00003805
Iteration 174/1000 | Loss: 0.00003787
Iteration 175/1000 | Loss: 0.00003786
Iteration 176/1000 | Loss: 0.00003784
Iteration 177/1000 | Loss: 0.00003767
Iteration 178/1000 | Loss: 0.00003757
Iteration 179/1000 | Loss: 0.00003747
Iteration 180/1000 | Loss: 0.00003745
Iteration 181/1000 | Loss: 0.00003745
Iteration 182/1000 | Loss: 0.00003743
Iteration 183/1000 | Loss: 0.00003728
Iteration 184/1000 | Loss: 0.00003715
Iteration 185/1000 | Loss: 0.00003710
Iteration 186/1000 | Loss: 0.00003703
Iteration 187/1000 | Loss: 0.00003699
Iteration 188/1000 | Loss: 0.00003698
Iteration 189/1000 | Loss: 0.00003697
Iteration 190/1000 | Loss: 0.00003695
Iteration 191/1000 | Loss: 0.00003695
Iteration 192/1000 | Loss: 0.00003695
Iteration 193/1000 | Loss: 0.00003695
Iteration 194/1000 | Loss: 0.00003695
Iteration 195/1000 | Loss: 0.00003695
Iteration 196/1000 | Loss: 0.00003694
Iteration 197/1000 | Loss: 0.00003694
Iteration 198/1000 | Loss: 0.00003692
Iteration 199/1000 | Loss: 0.00003691
Iteration 200/1000 | Loss: 0.00003690
Iteration 201/1000 | Loss: 0.00003686
Iteration 202/1000 | Loss: 0.00003684
Iteration 203/1000 | Loss: 0.00003684
Iteration 204/1000 | Loss: 0.00003684
Iteration 205/1000 | Loss: 0.00003683
Iteration 206/1000 | Loss: 0.00003682
Iteration 207/1000 | Loss: 0.00003681
Iteration 208/1000 | Loss: 0.00003681
Iteration 209/1000 | Loss: 0.00003679
Iteration 210/1000 | Loss: 0.00003679
Iteration 211/1000 | Loss: 0.00003678
Iteration 212/1000 | Loss: 0.00003678
Iteration 213/1000 | Loss: 0.00003677
Iteration 214/1000 | Loss: 0.00003677
Iteration 215/1000 | Loss: 0.00003677
Iteration 216/1000 | Loss: 0.00003676
Iteration 217/1000 | Loss: 0.00003676
Iteration 218/1000 | Loss: 0.00003676
Iteration 219/1000 | Loss: 0.00003675
Iteration 220/1000 | Loss: 0.00003675
Iteration 221/1000 | Loss: 0.00003675
Iteration 222/1000 | Loss: 0.00003674
Iteration 223/1000 | Loss: 0.00003674
Iteration 224/1000 | Loss: 0.00003673
Iteration 225/1000 | Loss: 0.00003673
Iteration 226/1000 | Loss: 0.00003673
Iteration 227/1000 | Loss: 0.00003673
Iteration 228/1000 | Loss: 0.00003672
Iteration 229/1000 | Loss: 0.00003672
Iteration 230/1000 | Loss: 0.00003672
Iteration 231/1000 | Loss: 0.00003672
Iteration 232/1000 | Loss: 0.00003672
Iteration 233/1000 | Loss: 0.00003672
Iteration 234/1000 | Loss: 0.00003672
Iteration 235/1000 | Loss: 0.00003671
Iteration 236/1000 | Loss: 0.00003671
Iteration 237/1000 | Loss: 0.00003671
Iteration 238/1000 | Loss: 0.00003671
Iteration 239/1000 | Loss: 0.00003670
Iteration 240/1000 | Loss: 0.00003670
Iteration 241/1000 | Loss: 0.00003670
Iteration 242/1000 | Loss: 0.00003669
Iteration 243/1000 | Loss: 0.00003669
Iteration 244/1000 | Loss: 0.00003669
Iteration 245/1000 | Loss: 0.00003669
Iteration 246/1000 | Loss: 0.00003669
Iteration 247/1000 | Loss: 0.00003669
Iteration 248/1000 | Loss: 0.00003669
Iteration 249/1000 | Loss: 0.00003669
Iteration 250/1000 | Loss: 0.00003668
Iteration 251/1000 | Loss: 0.00003668
Iteration 252/1000 | Loss: 0.00003668
Iteration 253/1000 | Loss: 0.00003668
Iteration 254/1000 | Loss: 0.00003668
Iteration 255/1000 | Loss: 0.00003668
Iteration 256/1000 | Loss: 0.00003668
Iteration 257/1000 | Loss: 0.00003668
Iteration 258/1000 | Loss: 0.00003668
Iteration 259/1000 | Loss: 0.00003668
Iteration 260/1000 | Loss: 0.00003668
Iteration 261/1000 | Loss: 0.00003668
Iteration 262/1000 | Loss: 0.00003668
Iteration 263/1000 | Loss: 0.00003668
Iteration 264/1000 | Loss: 0.00003668
Iteration 265/1000 | Loss: 0.00003668
Iteration 266/1000 | Loss: 0.00003668
Iteration 267/1000 | Loss: 0.00003668
Iteration 268/1000 | Loss: 0.00003668
Iteration 269/1000 | Loss: 0.00003668
Iteration 270/1000 | Loss: 0.00003668
Iteration 271/1000 | Loss: 0.00003668
Iteration 272/1000 | Loss: 0.00003668
Iteration 273/1000 | Loss: 0.00003668
Iteration 274/1000 | Loss: 0.00003668
Iteration 275/1000 | Loss: 0.00003668
Iteration 276/1000 | Loss: 0.00003668
Iteration 277/1000 | Loss: 0.00003668
Iteration 278/1000 | Loss: 0.00003668
Iteration 279/1000 | Loss: 0.00003668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [3.667592682177201e-05, 3.667592682177201e-05, 3.667592682177201e-05, 3.667592682177201e-05, 3.667592682177201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.667592682177201e-05

Optimization complete. Final v2v error: 3.9580132961273193 mm

Highest mean error: 12.726044654846191 mm for frame 91

Lowest mean error: 2.9030582904815674 mm for frame 201

Saving results

Total time: 252.08663964271545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422338
Iteration 2/25 | Loss: 0.00133420
Iteration 3/25 | Loss: 0.00124688
Iteration 4/25 | Loss: 0.00123212
Iteration 5/25 | Loss: 0.00122736
Iteration 6/25 | Loss: 0.00122668
Iteration 7/25 | Loss: 0.00122661
Iteration 8/25 | Loss: 0.00122661
Iteration 9/25 | Loss: 0.00122661
Iteration 10/25 | Loss: 0.00122661
Iteration 11/25 | Loss: 0.00122661
Iteration 12/25 | Loss: 0.00122661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001226607826538384, 0.001226607826538384, 0.001226607826538384, 0.001226607826538384, 0.001226607826538384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001226607826538384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40373445
Iteration 2/25 | Loss: 0.00098349
Iteration 3/25 | Loss: 0.00098349
Iteration 4/25 | Loss: 0.00098349
Iteration 5/25 | Loss: 0.00098349
Iteration 6/25 | Loss: 0.00098349
Iteration 7/25 | Loss: 0.00098349
Iteration 8/25 | Loss: 0.00098349
Iteration 9/25 | Loss: 0.00098349
Iteration 10/25 | Loss: 0.00098349
Iteration 11/25 | Loss: 0.00098349
Iteration 12/25 | Loss: 0.00098349
Iteration 13/25 | Loss: 0.00098349
Iteration 14/25 | Loss: 0.00098349
Iteration 15/25 | Loss: 0.00098349
Iteration 16/25 | Loss: 0.00098349
Iteration 17/25 | Loss: 0.00098349
Iteration 18/25 | Loss: 0.00098349
Iteration 19/25 | Loss: 0.00098349
Iteration 20/25 | Loss: 0.00098349
Iteration 21/25 | Loss: 0.00098349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000983487581834197, 0.000983487581834197, 0.000983487581834197, 0.000983487581834197, 0.000983487581834197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983487581834197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098349
Iteration 2/1000 | Loss: 0.00003235
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001945
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001843
Iteration 9/1000 | Loss: 0.00001808
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00001732
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001714
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001698
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001698
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001696
Iteration 25/1000 | Loss: 0.00001696
Iteration 26/1000 | Loss: 0.00001694
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001693
Iteration 30/1000 | Loss: 0.00001692
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001677
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001671
Iteration 46/1000 | Loss: 0.00001671
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001667
Iteration 52/1000 | Loss: 0.00001667
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001665
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00001664
Iteration 62/1000 | Loss: 0.00001663
Iteration 63/1000 | Loss: 0.00001663
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001655
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001654
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001654
Iteration 107/1000 | Loss: 0.00001654
Iteration 108/1000 | Loss: 0.00001654
Iteration 109/1000 | Loss: 0.00001654
Iteration 110/1000 | Loss: 0.00001654
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001653
Iteration 114/1000 | Loss: 0.00001653
Iteration 115/1000 | Loss: 0.00001653
Iteration 116/1000 | Loss: 0.00001653
Iteration 117/1000 | Loss: 0.00001653
Iteration 118/1000 | Loss: 0.00001653
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001652
Iteration 121/1000 | Loss: 0.00001652
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001651
Iteration 124/1000 | Loss: 0.00001651
Iteration 125/1000 | Loss: 0.00001651
Iteration 126/1000 | Loss: 0.00001651
Iteration 127/1000 | Loss: 0.00001651
Iteration 128/1000 | Loss: 0.00001651
Iteration 129/1000 | Loss: 0.00001651
Iteration 130/1000 | Loss: 0.00001651
Iteration 131/1000 | Loss: 0.00001651
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001651
Iteration 134/1000 | Loss: 0.00001651
Iteration 135/1000 | Loss: 0.00001651
Iteration 136/1000 | Loss: 0.00001651
Iteration 137/1000 | Loss: 0.00001651
Iteration 138/1000 | Loss: 0.00001651
Iteration 139/1000 | Loss: 0.00001651
Iteration 140/1000 | Loss: 0.00001651
Iteration 141/1000 | Loss: 0.00001651
Iteration 142/1000 | Loss: 0.00001651
Iteration 143/1000 | Loss: 0.00001651
Iteration 144/1000 | Loss: 0.00001650
Iteration 145/1000 | Loss: 0.00001650
Iteration 146/1000 | Loss: 0.00001650
Iteration 147/1000 | Loss: 0.00001650
Iteration 148/1000 | Loss: 0.00001650
Iteration 149/1000 | Loss: 0.00001650
Iteration 150/1000 | Loss: 0.00001650
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001650
Iteration 153/1000 | Loss: 0.00001650
Iteration 154/1000 | Loss: 0.00001650
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Iteration 162/1000 | Loss: 0.00001649
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001649
Iteration 165/1000 | Loss: 0.00001649
Iteration 166/1000 | Loss: 0.00001649
Iteration 167/1000 | Loss: 0.00001649
Iteration 168/1000 | Loss: 0.00001649
Iteration 169/1000 | Loss: 0.00001649
Iteration 170/1000 | Loss: 0.00001649
Iteration 171/1000 | Loss: 0.00001649
Iteration 172/1000 | Loss: 0.00001649
Iteration 173/1000 | Loss: 0.00001649
Iteration 174/1000 | Loss: 0.00001649
Iteration 175/1000 | Loss: 0.00001649
Iteration 176/1000 | Loss: 0.00001648
Iteration 177/1000 | Loss: 0.00001648
Iteration 178/1000 | Loss: 0.00001648
Iteration 179/1000 | Loss: 0.00001648
Iteration 180/1000 | Loss: 0.00001648
Iteration 181/1000 | Loss: 0.00001648
Iteration 182/1000 | Loss: 0.00001648
Iteration 183/1000 | Loss: 0.00001648
Iteration 184/1000 | Loss: 0.00001648
Iteration 185/1000 | Loss: 0.00001648
Iteration 186/1000 | Loss: 0.00001648
Iteration 187/1000 | Loss: 0.00001648
Iteration 188/1000 | Loss: 0.00001648
Iteration 189/1000 | Loss: 0.00001648
Iteration 190/1000 | Loss: 0.00001648
Iteration 191/1000 | Loss: 0.00001648
Iteration 192/1000 | Loss: 0.00001648
Iteration 193/1000 | Loss: 0.00001648
Iteration 194/1000 | Loss: 0.00001648
Iteration 195/1000 | Loss: 0.00001648
Iteration 196/1000 | Loss: 0.00001648
Iteration 197/1000 | Loss: 0.00001648
Iteration 198/1000 | Loss: 0.00001648
Iteration 199/1000 | Loss: 0.00001648
Iteration 200/1000 | Loss: 0.00001648
Iteration 201/1000 | Loss: 0.00001648
Iteration 202/1000 | Loss: 0.00001648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.647901262913365e-05, 1.647901262913365e-05, 1.647901262913365e-05, 1.647901262913365e-05, 1.647901262913365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.647901262913365e-05

Optimization complete. Final v2v error: 3.464614152908325 mm

Highest mean error: 4.18403434753418 mm for frame 20

Lowest mean error: 3.2585017681121826 mm for frame 8

Saving results

Total time: 43.7524471282959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997605
Iteration 2/25 | Loss: 0.00356664
Iteration 3/25 | Loss: 0.00272530
Iteration 4/25 | Loss: 0.00226318
Iteration 5/25 | Loss: 0.00218339
Iteration 6/25 | Loss: 0.00210495
Iteration 7/25 | Loss: 0.00208189
Iteration 8/25 | Loss: 0.00188699
Iteration 9/25 | Loss: 0.00177136
Iteration 10/25 | Loss: 0.00171762
Iteration 11/25 | Loss: 0.00168465
Iteration 12/25 | Loss: 0.00165918
Iteration 13/25 | Loss: 0.00165244
Iteration 14/25 | Loss: 0.00161958
Iteration 15/25 | Loss: 0.00162213
Iteration 16/25 | Loss: 0.00163559
Iteration 17/25 | Loss: 0.00160378
Iteration 18/25 | Loss: 0.00161196
Iteration 19/25 | Loss: 0.00160565
Iteration 20/25 | Loss: 0.00160189
Iteration 21/25 | Loss: 0.00159750
Iteration 22/25 | Loss: 0.00159707
Iteration 23/25 | Loss: 0.00159705
Iteration 24/25 | Loss: 0.00159705
Iteration 25/25 | Loss: 0.00159705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36652911
Iteration 2/25 | Loss: 0.00319119
Iteration 3/25 | Loss: 0.00198216
Iteration 4/25 | Loss: 0.00198216
Iteration 5/25 | Loss: 0.00198216
Iteration 6/25 | Loss: 0.00198216
Iteration 7/25 | Loss: 0.00198216
Iteration 8/25 | Loss: 0.00198216
Iteration 9/25 | Loss: 0.00198216
Iteration 10/25 | Loss: 0.00198216
Iteration 11/25 | Loss: 0.00198216
Iteration 12/25 | Loss: 0.00198216
Iteration 13/25 | Loss: 0.00198216
Iteration 14/25 | Loss: 0.00198216
Iteration 15/25 | Loss: 0.00198215
Iteration 16/25 | Loss: 0.00198215
Iteration 17/25 | Loss: 0.00198215
Iteration 18/25 | Loss: 0.00198215
Iteration 19/25 | Loss: 0.00198215
Iteration 20/25 | Loss: 0.00198215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019821547903120518, 0.0019821547903120518, 0.0019821547903120518, 0.0019821547903120518, 0.0019821547903120518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019821547903120518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198215
Iteration 2/1000 | Loss: 0.00145225
Iteration 3/1000 | Loss: 0.00179892
Iteration 4/1000 | Loss: 0.00015605
Iteration 5/1000 | Loss: 0.00030363
Iteration 6/1000 | Loss: 0.00013403
Iteration 7/1000 | Loss: 0.00135542
Iteration 8/1000 | Loss: 0.00062170
Iteration 9/1000 | Loss: 0.00012393
Iteration 10/1000 | Loss: 0.00012018
Iteration 11/1000 | Loss: 0.00011715
Iteration 12/1000 | Loss: 0.00011487
Iteration 13/1000 | Loss: 0.00011342
Iteration 14/1000 | Loss: 0.00050369
Iteration 15/1000 | Loss: 0.00011243
Iteration 16/1000 | Loss: 0.00041103
Iteration 17/1000 | Loss: 0.00040812
Iteration 18/1000 | Loss: 0.00011206
Iteration 19/1000 | Loss: 0.00011031
Iteration 20/1000 | Loss: 0.00022525
Iteration 21/1000 | Loss: 0.00046992
Iteration 22/1000 | Loss: 0.00011016
Iteration 23/1000 | Loss: 0.00046253
Iteration 24/1000 | Loss: 0.00010920
Iteration 25/1000 | Loss: 0.00020518
Iteration 26/1000 | Loss: 0.00010850
Iteration 27/1000 | Loss: 0.00010587
Iteration 28/1000 | Loss: 0.00025681
Iteration 29/1000 | Loss: 0.00010571
Iteration 30/1000 | Loss: 0.00010316
Iteration 31/1000 | Loss: 0.00010233
Iteration 32/1000 | Loss: 0.00052554
Iteration 33/1000 | Loss: 0.00051812
Iteration 34/1000 | Loss: 0.00036503
Iteration 35/1000 | Loss: 0.00010087
Iteration 36/1000 | Loss: 0.00010009
Iteration 37/1000 | Loss: 0.00009948
Iteration 38/1000 | Loss: 0.00009885
Iteration 39/1000 | Loss: 0.00009836
Iteration 40/1000 | Loss: 0.00009791
Iteration 41/1000 | Loss: 0.00009752
Iteration 42/1000 | Loss: 0.00046286
Iteration 43/1000 | Loss: 0.00009765
Iteration 44/1000 | Loss: 0.00027263
Iteration 45/1000 | Loss: 0.00009774
Iteration 46/1000 | Loss: 0.00009676
Iteration 47/1000 | Loss: 0.00009652
Iteration 48/1000 | Loss: 0.00009643
Iteration 49/1000 | Loss: 0.00009642
Iteration 50/1000 | Loss: 0.00009623
Iteration 51/1000 | Loss: 0.00009609
Iteration 52/1000 | Loss: 0.00009593
Iteration 53/1000 | Loss: 0.00009583
Iteration 54/1000 | Loss: 0.00009572
Iteration 55/1000 | Loss: 0.00009571
Iteration 56/1000 | Loss: 0.00009570
Iteration 57/1000 | Loss: 0.00009569
Iteration 58/1000 | Loss: 0.00009568
Iteration 59/1000 | Loss: 0.00009568
Iteration 60/1000 | Loss: 0.00009567
Iteration 61/1000 | Loss: 0.00009567
Iteration 62/1000 | Loss: 0.00009567
Iteration 63/1000 | Loss: 0.00009567
Iteration 64/1000 | Loss: 0.00009566
Iteration 65/1000 | Loss: 0.00009566
Iteration 66/1000 | Loss: 0.00009566
Iteration 67/1000 | Loss: 0.00009565
Iteration 68/1000 | Loss: 0.00009565
Iteration 69/1000 | Loss: 0.00009565
Iteration 70/1000 | Loss: 0.00009565
Iteration 71/1000 | Loss: 0.00009564
Iteration 72/1000 | Loss: 0.00009564
Iteration 73/1000 | Loss: 0.00009564
Iteration 74/1000 | Loss: 0.00009564
Iteration 75/1000 | Loss: 0.00009564
Iteration 76/1000 | Loss: 0.00009564
Iteration 77/1000 | Loss: 0.00009564
Iteration 78/1000 | Loss: 0.00009564
Iteration 79/1000 | Loss: 0.00009564
Iteration 80/1000 | Loss: 0.00009563
Iteration 81/1000 | Loss: 0.00009563
Iteration 82/1000 | Loss: 0.00009563
Iteration 83/1000 | Loss: 0.00009563
Iteration 84/1000 | Loss: 0.00009562
Iteration 85/1000 | Loss: 0.00009562
Iteration 86/1000 | Loss: 0.00009562
Iteration 87/1000 | Loss: 0.00009562
Iteration 88/1000 | Loss: 0.00009561
Iteration 89/1000 | Loss: 0.00009561
Iteration 90/1000 | Loss: 0.00009561
Iteration 91/1000 | Loss: 0.00009560
Iteration 92/1000 | Loss: 0.00009560
Iteration 93/1000 | Loss: 0.00009560
Iteration 94/1000 | Loss: 0.00009560
Iteration 95/1000 | Loss: 0.00009560
Iteration 96/1000 | Loss: 0.00009560
Iteration 97/1000 | Loss: 0.00009560
Iteration 98/1000 | Loss: 0.00009560
Iteration 99/1000 | Loss: 0.00009560
Iteration 100/1000 | Loss: 0.00009560
Iteration 101/1000 | Loss: 0.00009560
Iteration 102/1000 | Loss: 0.00009560
Iteration 103/1000 | Loss: 0.00009560
Iteration 104/1000 | Loss: 0.00009560
Iteration 105/1000 | Loss: 0.00009560
Iteration 106/1000 | Loss: 0.00009560
Iteration 107/1000 | Loss: 0.00009560
Iteration 108/1000 | Loss: 0.00009560
Iteration 109/1000 | Loss: 0.00009560
Iteration 110/1000 | Loss: 0.00009560
Iteration 111/1000 | Loss: 0.00009560
Iteration 112/1000 | Loss: 0.00009560
Iteration 113/1000 | Loss: 0.00009560
Iteration 114/1000 | Loss: 0.00009560
Iteration 115/1000 | Loss: 0.00009560
Iteration 116/1000 | Loss: 0.00009560
Iteration 117/1000 | Loss: 0.00009560
Iteration 118/1000 | Loss: 0.00009560
Iteration 119/1000 | Loss: 0.00009560
Iteration 120/1000 | Loss: 0.00009560
Iteration 121/1000 | Loss: 0.00009560
Iteration 122/1000 | Loss: 0.00009560
Iteration 123/1000 | Loss: 0.00009560
Iteration 124/1000 | Loss: 0.00009560
Iteration 125/1000 | Loss: 0.00009560
Iteration 126/1000 | Loss: 0.00009560
Iteration 127/1000 | Loss: 0.00009560
Iteration 128/1000 | Loss: 0.00009560
Iteration 129/1000 | Loss: 0.00009560
Iteration 130/1000 | Loss: 0.00009560
Iteration 131/1000 | Loss: 0.00009560
Iteration 132/1000 | Loss: 0.00009560
Iteration 133/1000 | Loss: 0.00009560
Iteration 134/1000 | Loss: 0.00009560
Iteration 135/1000 | Loss: 0.00009560
Iteration 136/1000 | Loss: 0.00009560
Iteration 137/1000 | Loss: 0.00009560
Iteration 138/1000 | Loss: 0.00009560
Iteration 139/1000 | Loss: 0.00009560
Iteration 140/1000 | Loss: 0.00009560
Iteration 141/1000 | Loss: 0.00009560
Iteration 142/1000 | Loss: 0.00009560
Iteration 143/1000 | Loss: 0.00009560
Iteration 144/1000 | Loss: 0.00009560
Iteration 145/1000 | Loss: 0.00009560
Iteration 146/1000 | Loss: 0.00009560
Iteration 147/1000 | Loss: 0.00009560
Iteration 148/1000 | Loss: 0.00009560
Iteration 149/1000 | Loss: 0.00009560
Iteration 150/1000 | Loss: 0.00009560
Iteration 151/1000 | Loss: 0.00009560
Iteration 152/1000 | Loss: 0.00009560
Iteration 153/1000 | Loss: 0.00009560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [9.55963769229129e-05, 9.55963769229129e-05, 9.55963769229129e-05, 9.55963769229129e-05, 9.55963769229129e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.55963769229129e-05

Optimization complete. Final v2v error: 5.966297626495361 mm

Highest mean error: 11.512577056884766 mm for frame 10

Lowest mean error: 3.82806396484375 mm for frame 108

Saving results

Total time: 118.14888620376587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028739
Iteration 2/25 | Loss: 0.00317512
Iteration 3/25 | Loss: 0.00185962
Iteration 4/25 | Loss: 0.00164238
Iteration 5/25 | Loss: 0.00167149
Iteration 6/25 | Loss: 0.00154162
Iteration 7/25 | Loss: 0.00149888
Iteration 8/25 | Loss: 0.00139424
Iteration 9/25 | Loss: 0.00135408
Iteration 10/25 | Loss: 0.00133993
Iteration 11/25 | Loss: 0.00132571
Iteration 12/25 | Loss: 0.00131446
Iteration 13/25 | Loss: 0.00131031
Iteration 14/25 | Loss: 0.00131515
Iteration 15/25 | Loss: 0.00131069
Iteration 16/25 | Loss: 0.00130640
Iteration 17/25 | Loss: 0.00130454
Iteration 18/25 | Loss: 0.00130383
Iteration 19/25 | Loss: 0.00130958
Iteration 20/25 | Loss: 0.00130574
Iteration 21/25 | Loss: 0.00130184
Iteration 22/25 | Loss: 0.00129746
Iteration 23/25 | Loss: 0.00129615
Iteration 24/25 | Loss: 0.00129569
Iteration 25/25 | Loss: 0.00129535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70793521
Iteration 2/25 | Loss: 0.00141709
Iteration 3/25 | Loss: 0.00140469
Iteration 4/25 | Loss: 0.00140469
Iteration 5/25 | Loss: 0.00140469
Iteration 6/25 | Loss: 0.00140469
Iteration 7/25 | Loss: 0.00140469
Iteration 8/25 | Loss: 0.00140469
Iteration 9/25 | Loss: 0.00140469
Iteration 10/25 | Loss: 0.00140469
Iteration 11/25 | Loss: 0.00140469
Iteration 12/25 | Loss: 0.00140469
Iteration 13/25 | Loss: 0.00140469
Iteration 14/25 | Loss: 0.00140469
Iteration 15/25 | Loss: 0.00140469
Iteration 16/25 | Loss: 0.00140469
Iteration 17/25 | Loss: 0.00140469
Iteration 18/25 | Loss: 0.00140469
Iteration 19/25 | Loss: 0.00140469
Iteration 20/25 | Loss: 0.00140469
Iteration 21/25 | Loss: 0.00140469
Iteration 22/25 | Loss: 0.00140469
Iteration 23/25 | Loss: 0.00140469
Iteration 24/25 | Loss: 0.00140469
Iteration 25/25 | Loss: 0.00140469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140469
Iteration 2/1000 | Loss: 0.00011212
Iteration 3/1000 | Loss: 0.00028428
Iteration 4/1000 | Loss: 0.00023221
Iteration 5/1000 | Loss: 0.00019358
Iteration 6/1000 | Loss: 0.00013318
Iteration 7/1000 | Loss: 0.00032271
Iteration 8/1000 | Loss: 0.00007058
Iteration 9/1000 | Loss: 0.00032142
Iteration 10/1000 | Loss: 0.00006123
Iteration 11/1000 | Loss: 0.00020981
Iteration 12/1000 | Loss: 0.00006271
Iteration 13/1000 | Loss: 0.00063152
Iteration 14/1000 | Loss: 0.00036629
Iteration 15/1000 | Loss: 0.00034476
Iteration 16/1000 | Loss: 0.00040172
Iteration 17/1000 | Loss: 0.00064535
Iteration 18/1000 | Loss: 0.00091035
Iteration 19/1000 | Loss: 0.00008119
Iteration 20/1000 | Loss: 0.00005806
Iteration 21/1000 | Loss: 0.00030126
Iteration 22/1000 | Loss: 0.00025968
Iteration 23/1000 | Loss: 0.00021417
Iteration 24/1000 | Loss: 0.00159398
Iteration 25/1000 | Loss: 0.00111721
Iteration 26/1000 | Loss: 0.00290035
Iteration 27/1000 | Loss: 0.00168483
Iteration 28/1000 | Loss: 0.00231353
Iteration 29/1000 | Loss: 0.00130584
Iteration 30/1000 | Loss: 0.00118681
Iteration 31/1000 | Loss: 0.00039481
Iteration 32/1000 | Loss: 0.00023230
Iteration 33/1000 | Loss: 0.00006488
Iteration 34/1000 | Loss: 0.00005540
Iteration 35/1000 | Loss: 0.00006540
Iteration 36/1000 | Loss: 0.00007232
Iteration 37/1000 | Loss: 0.00004394
Iteration 38/1000 | Loss: 0.00003142
Iteration 39/1000 | Loss: 0.00002419
Iteration 40/1000 | Loss: 0.00023033
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002400
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00003094
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00004626
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001396
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001395
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001395
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001391
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001390
Iteration 117/1000 | Loss: 0.00001390
Iteration 118/1000 | Loss: 0.00001390
Iteration 119/1000 | Loss: 0.00001390
Iteration 120/1000 | Loss: 0.00001390
Iteration 121/1000 | Loss: 0.00001389
Iteration 122/1000 | Loss: 0.00001389
Iteration 123/1000 | Loss: 0.00001389
Iteration 124/1000 | Loss: 0.00001389
Iteration 125/1000 | Loss: 0.00001389
Iteration 126/1000 | Loss: 0.00001389
Iteration 127/1000 | Loss: 0.00001389
Iteration 128/1000 | Loss: 0.00001389
Iteration 129/1000 | Loss: 0.00001389
Iteration 130/1000 | Loss: 0.00001389
Iteration 131/1000 | Loss: 0.00001389
Iteration 132/1000 | Loss: 0.00001389
Iteration 133/1000 | Loss: 0.00001389
Iteration 134/1000 | Loss: 0.00001389
Iteration 135/1000 | Loss: 0.00001389
Iteration 136/1000 | Loss: 0.00001389
Iteration 137/1000 | Loss: 0.00001389
Iteration 138/1000 | Loss: 0.00001389
Iteration 139/1000 | Loss: 0.00001389
Iteration 140/1000 | Loss: 0.00001389
Iteration 141/1000 | Loss: 0.00001389
Iteration 142/1000 | Loss: 0.00001389
Iteration 143/1000 | Loss: 0.00001389
Iteration 144/1000 | Loss: 0.00001389
Iteration 145/1000 | Loss: 0.00001389
Iteration 146/1000 | Loss: 0.00001389
Iteration 147/1000 | Loss: 0.00001389
Iteration 148/1000 | Loss: 0.00001389
Iteration 149/1000 | Loss: 0.00001389
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.3889154615753796e-05, 1.3889154615753796e-05, 1.3889154615753796e-05, 1.3889154615753796e-05, 1.3889154615753796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3889154615753796e-05

Optimization complete. Final v2v error: 3.1510493755340576 mm

Highest mean error: 4.382776737213135 mm for frame 78

Lowest mean error: 2.8190982341766357 mm for frame 25

Saving results

Total time: 130.2299611568451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954992
Iteration 2/25 | Loss: 0.00347314
Iteration 3/25 | Loss: 0.00174316
Iteration 4/25 | Loss: 0.00153101
Iteration 5/25 | Loss: 0.00151147
Iteration 6/25 | Loss: 0.00143916
Iteration 7/25 | Loss: 0.00136337
Iteration 8/25 | Loss: 0.00132407
Iteration 9/25 | Loss: 0.00130691
Iteration 10/25 | Loss: 0.00130522
Iteration 11/25 | Loss: 0.00130299
Iteration 12/25 | Loss: 0.00130678
Iteration 13/25 | Loss: 0.00130371
Iteration 14/25 | Loss: 0.00129806
Iteration 15/25 | Loss: 0.00129710
Iteration 16/25 | Loss: 0.00129569
Iteration 17/25 | Loss: 0.00129431
Iteration 18/25 | Loss: 0.00129417
Iteration 19/25 | Loss: 0.00129414
Iteration 20/25 | Loss: 0.00129414
Iteration 21/25 | Loss: 0.00129414
Iteration 22/25 | Loss: 0.00129414
Iteration 23/25 | Loss: 0.00129414
Iteration 24/25 | Loss: 0.00129414
Iteration 25/25 | Loss: 0.00129413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34878802
Iteration 2/25 | Loss: 0.00154263
Iteration 3/25 | Loss: 0.00103248
Iteration 4/25 | Loss: 0.00103248
Iteration 5/25 | Loss: 0.00103248
Iteration 6/25 | Loss: 0.00103248
Iteration 7/25 | Loss: 0.00103248
Iteration 8/25 | Loss: 0.00103248
Iteration 9/25 | Loss: 0.00103248
Iteration 10/25 | Loss: 0.00103248
Iteration 11/25 | Loss: 0.00103248
Iteration 12/25 | Loss: 0.00103248
Iteration 13/25 | Loss: 0.00103248
Iteration 14/25 | Loss: 0.00103248
Iteration 15/25 | Loss: 0.00103248
Iteration 16/25 | Loss: 0.00103248
Iteration 17/25 | Loss: 0.00103248
Iteration 18/25 | Loss: 0.00103248
Iteration 19/25 | Loss: 0.00103248
Iteration 20/25 | Loss: 0.00103248
Iteration 21/25 | Loss: 0.00103248
Iteration 22/25 | Loss: 0.00103248
Iteration 23/25 | Loss: 0.00103248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001032477244734764, 0.001032477244734764, 0.001032477244734764, 0.001032477244734764, 0.001032477244734764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001032477244734764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103248
Iteration 2/1000 | Loss: 0.00051312
Iteration 3/1000 | Loss: 0.00004737
Iteration 4/1000 | Loss: 0.00003856
Iteration 5/1000 | Loss: 0.00003534
Iteration 6/1000 | Loss: 0.00003325
Iteration 7/1000 | Loss: 0.00003195
Iteration 8/1000 | Loss: 0.00003119
Iteration 9/1000 | Loss: 0.00003054
Iteration 10/1000 | Loss: 0.00003004
Iteration 11/1000 | Loss: 0.00002961
Iteration 12/1000 | Loss: 0.00002922
Iteration 13/1000 | Loss: 0.00002874
Iteration 14/1000 | Loss: 0.00002842
Iteration 15/1000 | Loss: 0.00002808
Iteration 16/1000 | Loss: 0.00002786
Iteration 17/1000 | Loss: 0.00035931
Iteration 18/1000 | Loss: 0.00020349
Iteration 19/1000 | Loss: 0.00003538
Iteration 20/1000 | Loss: 0.00003266
Iteration 21/1000 | Loss: 0.00003072
Iteration 22/1000 | Loss: 0.00002812
Iteration 23/1000 | Loss: 0.00002757
Iteration 24/1000 | Loss: 0.00002725
Iteration 25/1000 | Loss: 0.00129917
Iteration 26/1000 | Loss: 0.00005436
Iteration 27/1000 | Loss: 0.00003047
Iteration 28/1000 | Loss: 0.00027739
Iteration 29/1000 | Loss: 0.00018779
Iteration 30/1000 | Loss: 0.00059577
Iteration 31/1000 | Loss: 0.00055564
Iteration 32/1000 | Loss: 0.00050052
Iteration 33/1000 | Loss: 0.00041659
Iteration 34/1000 | Loss: 0.00042236
Iteration 35/1000 | Loss: 0.00006544
Iteration 36/1000 | Loss: 0.00012056
Iteration 37/1000 | Loss: 0.00003853
Iteration 38/1000 | Loss: 0.00003418
Iteration 39/1000 | Loss: 0.00003183
Iteration 40/1000 | Loss: 0.00003075
Iteration 41/1000 | Loss: 0.00003012
Iteration 42/1000 | Loss: 0.00002942
Iteration 43/1000 | Loss: 0.00002875
Iteration 44/1000 | Loss: 0.00018231
Iteration 45/1000 | Loss: 0.00004052
Iteration 46/1000 | Loss: 0.00002810
Iteration 47/1000 | Loss: 0.00079968
Iteration 48/1000 | Loss: 0.00089575
Iteration 49/1000 | Loss: 0.00047603
Iteration 50/1000 | Loss: 0.00029968
Iteration 51/1000 | Loss: 0.00003590
Iteration 52/1000 | Loss: 0.00002826
Iteration 53/1000 | Loss: 0.00002511
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002349
Iteration 56/1000 | Loss: 0.00002299
Iteration 57/1000 | Loss: 0.00002242
Iteration 58/1000 | Loss: 0.00002196
Iteration 59/1000 | Loss: 0.00002169
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002127
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002103
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002092
Iteration 68/1000 | Loss: 0.00002090
Iteration 69/1000 | Loss: 0.00002089
Iteration 70/1000 | Loss: 0.00002089
Iteration 71/1000 | Loss: 0.00002088
Iteration 72/1000 | Loss: 0.00002088
Iteration 73/1000 | Loss: 0.00002088
Iteration 74/1000 | Loss: 0.00002087
Iteration 75/1000 | Loss: 0.00002084
Iteration 76/1000 | Loss: 0.00002083
Iteration 77/1000 | Loss: 0.00002082
Iteration 78/1000 | Loss: 0.00002081
Iteration 79/1000 | Loss: 0.00002080
Iteration 80/1000 | Loss: 0.00002075
Iteration 81/1000 | Loss: 0.00002075
Iteration 82/1000 | Loss: 0.00002074
Iteration 83/1000 | Loss: 0.00002074
Iteration 84/1000 | Loss: 0.00002073
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002072
Iteration 88/1000 | Loss: 0.00002072
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002071
Iteration 91/1000 | Loss: 0.00002071
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002070
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002069
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002066
Iteration 109/1000 | Loss: 0.00002066
Iteration 110/1000 | Loss: 0.00002066
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002066
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002066
Iteration 121/1000 | Loss: 0.00002066
Iteration 122/1000 | Loss: 0.00002066
Iteration 123/1000 | Loss: 0.00002066
Iteration 124/1000 | Loss: 0.00002066
Iteration 125/1000 | Loss: 0.00002066
Iteration 126/1000 | Loss: 0.00002066
Iteration 127/1000 | Loss: 0.00002066
Iteration 128/1000 | Loss: 0.00002066
Iteration 129/1000 | Loss: 0.00002066
Iteration 130/1000 | Loss: 0.00002066
Iteration 131/1000 | Loss: 0.00002066
Iteration 132/1000 | Loss: 0.00002066
Iteration 133/1000 | Loss: 0.00002066
Iteration 134/1000 | Loss: 0.00002066
Iteration 135/1000 | Loss: 0.00002066
Iteration 136/1000 | Loss: 0.00002066
Iteration 137/1000 | Loss: 0.00002066
Iteration 138/1000 | Loss: 0.00002066
Iteration 139/1000 | Loss: 0.00002066
Iteration 140/1000 | Loss: 0.00002066
Iteration 141/1000 | Loss: 0.00002066
Iteration 142/1000 | Loss: 0.00002066
Iteration 143/1000 | Loss: 0.00002066
Iteration 144/1000 | Loss: 0.00002066
Iteration 145/1000 | Loss: 0.00002066
Iteration 146/1000 | Loss: 0.00002066
Iteration 147/1000 | Loss: 0.00002066
Iteration 148/1000 | Loss: 0.00002066
Iteration 149/1000 | Loss: 0.00002066
Iteration 150/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.065554326691199e-05, 2.065554326691199e-05, 2.065554326691199e-05, 2.065554326691199e-05, 2.065554326691199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.065554326691199e-05

Optimization complete. Final v2v error: 3.4079606533050537 mm

Highest mean error: 11.691122055053711 mm for frame 18

Lowest mean error: 2.8443474769592285 mm for frame 141

Saving results

Total time: 151.23765063285828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875394
Iteration 2/25 | Loss: 0.00177247
Iteration 3/25 | Loss: 0.00140737
Iteration 4/25 | Loss: 0.00134683
Iteration 5/25 | Loss: 0.00133181
Iteration 6/25 | Loss: 0.00133643
Iteration 7/25 | Loss: 0.00131860
Iteration 8/25 | Loss: 0.00130826
Iteration 9/25 | Loss: 0.00129771
Iteration 10/25 | Loss: 0.00129963
Iteration 11/25 | Loss: 0.00129349
Iteration 12/25 | Loss: 0.00129221
Iteration 13/25 | Loss: 0.00129201
Iteration 14/25 | Loss: 0.00129272
Iteration 15/25 | Loss: 0.00129748
Iteration 16/25 | Loss: 0.00129509
Iteration 17/25 | Loss: 0.00128992
Iteration 18/25 | Loss: 0.00128875
Iteration 19/25 | Loss: 0.00128854
Iteration 20/25 | Loss: 0.00128847
Iteration 21/25 | Loss: 0.00128847
Iteration 22/25 | Loss: 0.00128846
Iteration 23/25 | Loss: 0.00128846
Iteration 24/25 | Loss: 0.00128846
Iteration 25/25 | Loss: 0.00128846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.70967770
Iteration 2/25 | Loss: 0.00087746
Iteration 3/25 | Loss: 0.00087736
Iteration 4/25 | Loss: 0.00087736
Iteration 5/25 | Loss: 0.00087735
Iteration 6/25 | Loss: 0.00087735
Iteration 7/25 | Loss: 0.00087735
Iteration 8/25 | Loss: 0.00087735
Iteration 9/25 | Loss: 0.00087735
Iteration 10/25 | Loss: 0.00087735
Iteration 11/25 | Loss: 0.00087735
Iteration 12/25 | Loss: 0.00087735
Iteration 13/25 | Loss: 0.00087735
Iteration 14/25 | Loss: 0.00087735
Iteration 15/25 | Loss: 0.00087735
Iteration 16/25 | Loss: 0.00087735
Iteration 17/25 | Loss: 0.00087735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008773512090556324, 0.0008773512090556324, 0.0008773512090556324, 0.0008773512090556324, 0.0008773512090556324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008773512090556324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087735
Iteration 2/1000 | Loss: 0.00032635
Iteration 3/1000 | Loss: 0.00020738
Iteration 4/1000 | Loss: 0.00023967
Iteration 5/1000 | Loss: 0.00015301
Iteration 6/1000 | Loss: 0.00002936
Iteration 7/1000 | Loss: 0.00016852
Iteration 8/1000 | Loss: 0.00002711
Iteration 9/1000 | Loss: 0.00002501
Iteration 10/1000 | Loss: 0.00022311
Iteration 11/1000 | Loss: 0.00020994
Iteration 12/1000 | Loss: 0.00008259
Iteration 13/1000 | Loss: 0.00002956
Iteration 14/1000 | Loss: 0.00002673
Iteration 15/1000 | Loss: 0.00002547
Iteration 16/1000 | Loss: 0.00002441
Iteration 17/1000 | Loss: 0.00018526
Iteration 18/1000 | Loss: 0.00002344
Iteration 19/1000 | Loss: 0.00022142
Iteration 20/1000 | Loss: 0.00017644
Iteration 21/1000 | Loss: 0.00007439
Iteration 22/1000 | Loss: 0.00015299
Iteration 23/1000 | Loss: 0.00012384
Iteration 24/1000 | Loss: 0.00020764
Iteration 25/1000 | Loss: 0.00011031
Iteration 26/1000 | Loss: 0.00020943
Iteration 27/1000 | Loss: 0.00012620
Iteration 28/1000 | Loss: 0.00018048
Iteration 29/1000 | Loss: 0.00002222
Iteration 30/1000 | Loss: 0.00002171
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00002148
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00030761
Iteration 36/1000 | Loss: 0.00021205
Iteration 37/1000 | Loss: 0.00004619
Iteration 38/1000 | Loss: 0.00003074
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00006029
Iteration 42/1000 | Loss: 0.00002257
Iteration 43/1000 | Loss: 0.00003199
Iteration 44/1000 | Loss: 0.00002500
Iteration 45/1000 | Loss: 0.00002174
Iteration 46/1000 | Loss: 0.00002174
Iteration 47/1000 | Loss: 0.00002174
Iteration 48/1000 | Loss: 0.00002174
Iteration 49/1000 | Loss: 0.00002174
Iteration 50/1000 | Loss: 0.00002174
Iteration 51/1000 | Loss: 0.00002174
Iteration 52/1000 | Loss: 0.00002174
Iteration 53/1000 | Loss: 0.00002174
Iteration 54/1000 | Loss: 0.00002174
Iteration 55/1000 | Loss: 0.00002174
Iteration 56/1000 | Loss: 0.00002173
Iteration 57/1000 | Loss: 0.00002173
Iteration 58/1000 | Loss: 0.00002173
Iteration 59/1000 | Loss: 0.00002171
Iteration 60/1000 | Loss: 0.00002171
Iteration 61/1000 | Loss: 0.00002170
Iteration 62/1000 | Loss: 0.00002170
Iteration 63/1000 | Loss: 0.00002169
Iteration 64/1000 | Loss: 0.00002169
Iteration 65/1000 | Loss: 0.00002168
Iteration 66/1000 | Loss: 0.00002168
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002167
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002165
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002993
Iteration 73/1000 | Loss: 0.00002644
Iteration 74/1000 | Loss: 0.00003008
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00007981
Iteration 77/1000 | Loss: 0.00002718
Iteration 78/1000 | Loss: 0.00007293
Iteration 79/1000 | Loss: 0.00002636
Iteration 80/1000 | Loss: 0.00006937
Iteration 81/1000 | Loss: 0.00002497
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002144
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002144
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002143
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00003728
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00009960
Iteration 107/1000 | Loss: 0.00002350
Iteration 108/1000 | Loss: 0.00002140
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00005905
Iteration 111/1000 | Loss: 0.00002158
Iteration 112/1000 | Loss: 0.00003899
Iteration 113/1000 | Loss: 0.00002149
Iteration 114/1000 | Loss: 0.00002145
Iteration 115/1000 | Loss: 0.00002142
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00005636
Iteration 118/1000 | Loss: 0.00005636
Iteration 119/1000 | Loss: 0.00002196
Iteration 120/1000 | Loss: 0.00002139
Iteration 121/1000 | Loss: 0.00010188
Iteration 122/1000 | Loss: 0.00002142
Iteration 123/1000 | Loss: 0.00002106
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002072
Iteration 126/1000 | Loss: 0.00029825
Iteration 127/1000 | Loss: 0.00005766
Iteration 128/1000 | Loss: 0.00002447
Iteration 129/1000 | Loss: 0.00002333
Iteration 130/1000 | Loss: 0.00026394
Iteration 131/1000 | Loss: 0.00048352
Iteration 132/1000 | Loss: 0.00004097
Iteration 133/1000 | Loss: 0.00002541
Iteration 134/1000 | Loss: 0.00002300
Iteration 135/1000 | Loss: 0.00002190
Iteration 136/1000 | Loss: 0.00002132
Iteration 137/1000 | Loss: 0.00002090
Iteration 138/1000 | Loss: 0.00002050
Iteration 139/1000 | Loss: 0.00002024
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002012
Iteration 142/1000 | Loss: 0.00002005
Iteration 143/1000 | Loss: 0.00002004
Iteration 144/1000 | Loss: 0.00002004
Iteration 145/1000 | Loss: 0.00002003
Iteration 146/1000 | Loss: 0.00002003
Iteration 147/1000 | Loss: 0.00001999
Iteration 148/1000 | Loss: 0.00001996
Iteration 149/1000 | Loss: 0.00001996
Iteration 150/1000 | Loss: 0.00001985
Iteration 151/1000 | Loss: 0.00001983
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Iteration 155/1000 | Loss: 0.00001980
Iteration 156/1000 | Loss: 0.00001979
Iteration 157/1000 | Loss: 0.00001978
Iteration 158/1000 | Loss: 0.00006878
Iteration 159/1000 | Loss: 0.00005491
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001972
Iteration 162/1000 | Loss: 0.00001972
Iteration 163/1000 | Loss: 0.00001972
Iteration 164/1000 | Loss: 0.00001972
Iteration 165/1000 | Loss: 0.00001972
Iteration 166/1000 | Loss: 0.00001972
Iteration 167/1000 | Loss: 0.00001972
Iteration 168/1000 | Loss: 0.00001972
Iteration 169/1000 | Loss: 0.00001971
Iteration 170/1000 | Loss: 0.00001971
Iteration 171/1000 | Loss: 0.00001971
Iteration 172/1000 | Loss: 0.00001971
Iteration 173/1000 | Loss: 0.00001971
Iteration 174/1000 | Loss: 0.00001971
Iteration 175/1000 | Loss: 0.00001971
Iteration 176/1000 | Loss: 0.00001971
Iteration 177/1000 | Loss: 0.00001971
Iteration 178/1000 | Loss: 0.00001971
Iteration 179/1000 | Loss: 0.00001971
Iteration 180/1000 | Loss: 0.00001970
Iteration 181/1000 | Loss: 0.00001970
Iteration 182/1000 | Loss: 0.00001970
Iteration 183/1000 | Loss: 0.00001969
Iteration 184/1000 | Loss: 0.00004833
Iteration 185/1000 | Loss: 0.00002707
Iteration 186/1000 | Loss: 0.00001970
Iteration 187/1000 | Loss: 0.00001969
Iteration 188/1000 | Loss: 0.00001969
Iteration 189/1000 | Loss: 0.00001969
Iteration 190/1000 | Loss: 0.00001968
Iteration 191/1000 | Loss: 0.00001968
Iteration 192/1000 | Loss: 0.00001968
Iteration 193/1000 | Loss: 0.00002154
Iteration 194/1000 | Loss: 0.00002154
Iteration 195/1000 | Loss: 0.00001965
Iteration 196/1000 | Loss: 0.00001965
Iteration 197/1000 | Loss: 0.00001965
Iteration 198/1000 | Loss: 0.00001965
Iteration 199/1000 | Loss: 0.00001965
Iteration 200/1000 | Loss: 0.00001964
Iteration 201/1000 | Loss: 0.00001964
Iteration 202/1000 | Loss: 0.00001964
Iteration 203/1000 | Loss: 0.00001964
Iteration 204/1000 | Loss: 0.00001964
Iteration 205/1000 | Loss: 0.00001964
Iteration 206/1000 | Loss: 0.00001964
Iteration 207/1000 | Loss: 0.00001964
Iteration 208/1000 | Loss: 0.00001964
Iteration 209/1000 | Loss: 0.00001964
Iteration 210/1000 | Loss: 0.00001964
Iteration 211/1000 | Loss: 0.00001963
Iteration 212/1000 | Loss: 0.00001963
Iteration 213/1000 | Loss: 0.00001963
Iteration 214/1000 | Loss: 0.00001963
Iteration 215/1000 | Loss: 0.00001963
Iteration 216/1000 | Loss: 0.00001963
Iteration 217/1000 | Loss: 0.00001963
Iteration 218/1000 | Loss: 0.00001963
Iteration 219/1000 | Loss: 0.00001963
Iteration 220/1000 | Loss: 0.00001963
Iteration 221/1000 | Loss: 0.00001963
Iteration 222/1000 | Loss: 0.00001963
Iteration 223/1000 | Loss: 0.00001963
Iteration 224/1000 | Loss: 0.00001962
Iteration 225/1000 | Loss: 0.00001962
Iteration 226/1000 | Loss: 0.00001962
Iteration 227/1000 | Loss: 0.00001962
Iteration 228/1000 | Loss: 0.00001962
Iteration 229/1000 | Loss: 0.00001962
Iteration 230/1000 | Loss: 0.00001961
Iteration 231/1000 | Loss: 0.00001961
Iteration 232/1000 | Loss: 0.00001961
Iteration 233/1000 | Loss: 0.00001961
Iteration 234/1000 | Loss: 0.00001961
Iteration 235/1000 | Loss: 0.00001961
Iteration 236/1000 | Loss: 0.00001961
Iteration 237/1000 | Loss: 0.00001960
Iteration 238/1000 | Loss: 0.00001960
Iteration 239/1000 | Loss: 0.00001960
Iteration 240/1000 | Loss: 0.00001960
Iteration 241/1000 | Loss: 0.00001959
Iteration 242/1000 | Loss: 0.00001959
Iteration 243/1000 | Loss: 0.00001959
Iteration 244/1000 | Loss: 0.00001959
Iteration 245/1000 | Loss: 0.00001959
Iteration 246/1000 | Loss: 0.00001959
Iteration 247/1000 | Loss: 0.00001959
Iteration 248/1000 | Loss: 0.00001959
Iteration 249/1000 | Loss: 0.00001959
Iteration 250/1000 | Loss: 0.00001959
Iteration 251/1000 | Loss: 0.00001958
Iteration 252/1000 | Loss: 0.00001958
Iteration 253/1000 | Loss: 0.00001958
Iteration 254/1000 | Loss: 0.00001958
Iteration 255/1000 | Loss: 0.00001958
Iteration 256/1000 | Loss: 0.00001958
Iteration 257/1000 | Loss: 0.00001957
Iteration 258/1000 | Loss: 0.00001957
Iteration 259/1000 | Loss: 0.00001956
Iteration 260/1000 | Loss: 0.00001956
Iteration 261/1000 | Loss: 0.00001956
Iteration 262/1000 | Loss: 0.00001956
Iteration 263/1000 | Loss: 0.00001956
Iteration 264/1000 | Loss: 0.00001955
Iteration 265/1000 | Loss: 0.00001955
Iteration 266/1000 | Loss: 0.00001955
Iteration 267/1000 | Loss: 0.00001954
Iteration 268/1000 | Loss: 0.00001954
Iteration 269/1000 | Loss: 0.00001954
Iteration 270/1000 | Loss: 0.00001954
Iteration 271/1000 | Loss: 0.00001954
Iteration 272/1000 | Loss: 0.00001954
Iteration 273/1000 | Loss: 0.00001954
Iteration 274/1000 | Loss: 0.00001954
Iteration 275/1000 | Loss: 0.00001954
Iteration 276/1000 | Loss: 0.00001954
Iteration 277/1000 | Loss: 0.00001954
Iteration 278/1000 | Loss: 0.00001954
Iteration 279/1000 | Loss: 0.00001954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.9535513274604455e-05, 1.9535513274604455e-05, 1.9535513274604455e-05, 1.9535513274604455e-05, 1.9535513274604455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9535513274604455e-05

Optimization complete. Final v2v error: 3.6311986446380615 mm

Highest mean error: 7.354488849639893 mm for frame 93

Lowest mean error: 2.9400949478149414 mm for frame 18

Saving results

Total time: 170.73757243156433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792349
Iteration 2/25 | Loss: 0.00127276
Iteration 3/25 | Loss: 0.00119578
Iteration 4/25 | Loss: 0.00118323
Iteration 5/25 | Loss: 0.00118162
Iteration 6/25 | Loss: 0.00118162
Iteration 7/25 | Loss: 0.00118162
Iteration 8/25 | Loss: 0.00118162
Iteration 9/25 | Loss: 0.00118162
Iteration 10/25 | Loss: 0.00118162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011816160986199975, 0.0011816160986199975, 0.0011816160986199975, 0.0011816160986199975, 0.0011816160986199975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011816160986199975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40455937
Iteration 2/25 | Loss: 0.00091265
Iteration 3/25 | Loss: 0.00091264
Iteration 4/25 | Loss: 0.00091264
Iteration 5/25 | Loss: 0.00091264
Iteration 6/25 | Loss: 0.00091264
Iteration 7/25 | Loss: 0.00091264
Iteration 8/25 | Loss: 0.00091264
Iteration 9/25 | Loss: 0.00091264
Iteration 10/25 | Loss: 0.00091264
Iteration 11/25 | Loss: 0.00091264
Iteration 12/25 | Loss: 0.00091264
Iteration 13/25 | Loss: 0.00091264
Iteration 14/25 | Loss: 0.00091264
Iteration 15/25 | Loss: 0.00091264
Iteration 16/25 | Loss: 0.00091264
Iteration 17/25 | Loss: 0.00091264
Iteration 18/25 | Loss: 0.00091264
Iteration 19/25 | Loss: 0.00091264
Iteration 20/25 | Loss: 0.00091264
Iteration 21/25 | Loss: 0.00091264
Iteration 22/25 | Loss: 0.00091264
Iteration 23/25 | Loss: 0.00091264
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009126401855610311, 0.0009126401855610311, 0.0009126401855610311, 0.0009126401855610311, 0.0009126401855610311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009126401855610311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091264
Iteration 2/1000 | Loss: 0.00002648
Iteration 3/1000 | Loss: 0.00001997
Iteration 4/1000 | Loss: 0.00001806
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001614
Iteration 7/1000 | Loss: 0.00001550
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001370
Iteration 19/1000 | Loss: 0.00001370
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001344
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001328
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001324
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001323
Iteration 130/1000 | Loss: 0.00001323
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.3200557077652775e-05, 1.3200557077652775e-05, 1.3200557077652775e-05, 1.3200557077652775e-05, 1.3200557077652775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3200557077652775e-05

Optimization complete. Final v2v error: 3.140986204147339 mm

Highest mean error: 3.4229421615600586 mm for frame 156

Lowest mean error: 2.9491798877716064 mm for frame 65

Saving results

Total time: 45.372506618499756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901422
Iteration 2/25 | Loss: 0.00195007
Iteration 3/25 | Loss: 0.00157961
Iteration 4/25 | Loss: 0.00151003
Iteration 5/25 | Loss: 0.00149800
Iteration 6/25 | Loss: 0.00141801
Iteration 7/25 | Loss: 0.00137596
Iteration 8/25 | Loss: 0.00136011
Iteration 9/25 | Loss: 0.00135485
Iteration 10/25 | Loss: 0.00139835
Iteration 11/25 | Loss: 0.00135829
Iteration 12/25 | Loss: 0.00133766
Iteration 13/25 | Loss: 0.00132816
Iteration 14/25 | Loss: 0.00132538
Iteration 15/25 | Loss: 0.00132463
Iteration 16/25 | Loss: 0.00132883
Iteration 17/25 | Loss: 0.00132460
Iteration 18/25 | Loss: 0.00132356
Iteration 19/25 | Loss: 0.00132330
Iteration 20/25 | Loss: 0.00132322
Iteration 21/25 | Loss: 0.00132320
Iteration 22/25 | Loss: 0.00132320
Iteration 23/25 | Loss: 0.00132319
Iteration 24/25 | Loss: 0.00132319
Iteration 25/25 | Loss: 0.00132319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34707308
Iteration 2/25 | Loss: 0.00109260
Iteration 3/25 | Loss: 0.00108739
Iteration 4/25 | Loss: 0.00108739
Iteration 5/25 | Loss: 0.00108739
Iteration 6/25 | Loss: 0.00108739
Iteration 7/25 | Loss: 0.00108739
Iteration 8/25 | Loss: 0.00108739
Iteration 9/25 | Loss: 0.00108739
Iteration 10/25 | Loss: 0.00108739
Iteration 11/25 | Loss: 0.00108739
Iteration 12/25 | Loss: 0.00108739
Iteration 13/25 | Loss: 0.00108739
Iteration 14/25 | Loss: 0.00108739
Iteration 15/25 | Loss: 0.00108739
Iteration 16/25 | Loss: 0.00108739
Iteration 17/25 | Loss: 0.00108739
Iteration 18/25 | Loss: 0.00108739
Iteration 19/25 | Loss: 0.00108739
Iteration 20/25 | Loss: 0.00108739
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010873855790123343, 0.0010873855790123343, 0.0010873855790123343, 0.0010873855790123343, 0.0010873855790123343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010873855790123343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108739
Iteration 2/1000 | Loss: 0.00032709
Iteration 3/1000 | Loss: 0.00036793
Iteration 4/1000 | Loss: 0.00021003
Iteration 5/1000 | Loss: 0.00015303
Iteration 6/1000 | Loss: 0.00011419
Iteration 7/1000 | Loss: 0.00008841
Iteration 8/1000 | Loss: 0.00007780
Iteration 9/1000 | Loss: 0.00007206
Iteration 10/1000 | Loss: 0.00017550
Iteration 11/1000 | Loss: 0.00014939
Iteration 12/1000 | Loss: 0.00024775
Iteration 13/1000 | Loss: 0.00016871
Iteration 14/1000 | Loss: 0.00009939
Iteration 15/1000 | Loss: 0.00008000
Iteration 16/1000 | Loss: 0.00006639
Iteration 17/1000 | Loss: 0.00008798
Iteration 18/1000 | Loss: 0.00006238
Iteration 19/1000 | Loss: 0.00005710
Iteration 20/1000 | Loss: 0.00004937
Iteration 21/1000 | Loss: 0.00004544
Iteration 22/1000 | Loss: 0.00004272
Iteration 23/1000 | Loss: 0.00004035
Iteration 24/1000 | Loss: 0.00007060
Iteration 25/1000 | Loss: 0.00006570
Iteration 26/1000 | Loss: 0.00004258
Iteration 27/1000 | Loss: 0.00003586
Iteration 28/1000 | Loss: 0.00004062
Iteration 29/1000 | Loss: 0.00004326
Iteration 30/1000 | Loss: 0.00005898
Iteration 31/1000 | Loss: 0.00004476
Iteration 32/1000 | Loss: 0.00006017
Iteration 33/1000 | Loss: 0.00004771
Iteration 34/1000 | Loss: 0.00004517
Iteration 35/1000 | Loss: 0.00006700
Iteration 36/1000 | Loss: 0.00005407
Iteration 37/1000 | Loss: 0.00003672
Iteration 38/1000 | Loss: 0.00004199
Iteration 39/1000 | Loss: 0.00003653
Iteration 40/1000 | Loss: 0.00004518
Iteration 41/1000 | Loss: 0.00005179
Iteration 42/1000 | Loss: 0.00004036
Iteration 43/1000 | Loss: 0.00003180
Iteration 44/1000 | Loss: 0.00004182
Iteration 45/1000 | Loss: 0.00003901
Iteration 46/1000 | Loss: 0.00006831
Iteration 47/1000 | Loss: 0.00005841
Iteration 48/1000 | Loss: 0.00004671
Iteration 49/1000 | Loss: 0.00004789
Iteration 50/1000 | Loss: 0.00004324
Iteration 51/1000 | Loss: 0.00004618
Iteration 52/1000 | Loss: 0.00003752
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00003936
Iteration 55/1000 | Loss: 0.00003632
Iteration 56/1000 | Loss: 0.00002888
Iteration 57/1000 | Loss: 0.00003605
Iteration 58/1000 | Loss: 0.00004211
Iteration 59/1000 | Loss: 0.00004011
Iteration 60/1000 | Loss: 0.00002975
Iteration 61/1000 | Loss: 0.00003602
Iteration 62/1000 | Loss: 0.00004416
Iteration 63/1000 | Loss: 0.00004492
Iteration 64/1000 | Loss: 0.00005260
Iteration 65/1000 | Loss: 0.00004466
Iteration 66/1000 | Loss: 0.00005100
Iteration 67/1000 | Loss: 0.00005354
Iteration 68/1000 | Loss: 0.00005005
Iteration 69/1000 | Loss: 0.00003670
Iteration 70/1000 | Loss: 0.00003493
Iteration 71/1000 | Loss: 0.00003678
Iteration 72/1000 | Loss: 0.00002876
Iteration 73/1000 | Loss: 0.00003164
Iteration 74/1000 | Loss: 0.00003230
Iteration 75/1000 | Loss: 0.00003250
Iteration 76/1000 | Loss: 0.00003386
Iteration 77/1000 | Loss: 0.00005704
Iteration 78/1000 | Loss: 0.00003571
Iteration 79/1000 | Loss: 0.00003247
Iteration 80/1000 | Loss: 0.00005374
Iteration 81/1000 | Loss: 0.00004638
Iteration 82/1000 | Loss: 0.00005910
Iteration 83/1000 | Loss: 0.00005391
Iteration 84/1000 | Loss: 0.00004661
Iteration 85/1000 | Loss: 0.00002828
Iteration 86/1000 | Loss: 0.00004727
Iteration 87/1000 | Loss: 0.00003796
Iteration 88/1000 | Loss: 0.00004381
Iteration 89/1000 | Loss: 0.00003759
Iteration 90/1000 | Loss: 0.00003827
Iteration 91/1000 | Loss: 0.00003289
Iteration 92/1000 | Loss: 0.00002856
Iteration 93/1000 | Loss: 0.00003602
Iteration 94/1000 | Loss: 0.00004524
Iteration 95/1000 | Loss: 0.00003732
Iteration 96/1000 | Loss: 0.00002888
Iteration 97/1000 | Loss: 0.00002825
Iteration 98/1000 | Loss: 0.00003103
Iteration 99/1000 | Loss: 0.00003279
Iteration 100/1000 | Loss: 0.00003369
Iteration 101/1000 | Loss: 0.00003843
Iteration 102/1000 | Loss: 0.00003472
Iteration 103/1000 | Loss: 0.00003844
Iteration 104/1000 | Loss: 0.00003248
Iteration 105/1000 | Loss: 0.00003471
Iteration 106/1000 | Loss: 0.00003243
Iteration 107/1000 | Loss: 0.00003498
Iteration 108/1000 | Loss: 0.00003347
Iteration 109/1000 | Loss: 0.00003116
Iteration 110/1000 | Loss: 0.00003016
Iteration 111/1000 | Loss: 0.00003055
Iteration 112/1000 | Loss: 0.00003023
Iteration 113/1000 | Loss: 0.00003139
Iteration 114/1000 | Loss: 0.00002645
Iteration 115/1000 | Loss: 0.00002642
Iteration 116/1000 | Loss: 0.00002642
Iteration 117/1000 | Loss: 0.00002641
Iteration 118/1000 | Loss: 0.00002641
Iteration 119/1000 | Loss: 0.00002641
Iteration 120/1000 | Loss: 0.00002639
Iteration 121/1000 | Loss: 0.00002639
Iteration 122/1000 | Loss: 0.00002638
Iteration 123/1000 | Loss: 0.00002638
Iteration 124/1000 | Loss: 0.00002635
Iteration 125/1000 | Loss: 0.00002635
Iteration 126/1000 | Loss: 0.00002635
Iteration 127/1000 | Loss: 0.00002634
Iteration 128/1000 | Loss: 0.00002634
Iteration 129/1000 | Loss: 0.00002633
Iteration 130/1000 | Loss: 0.00002632
Iteration 131/1000 | Loss: 0.00002630
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00003391
Iteration 134/1000 | Loss: 0.00005974
Iteration 135/1000 | Loss: 0.00004370
Iteration 136/1000 | Loss: 0.00006368
Iteration 137/1000 | Loss: 0.00005659
Iteration 138/1000 | Loss: 0.00004142
Iteration 139/1000 | Loss: 0.00003407
Iteration 140/1000 | Loss: 0.00003271
Iteration 141/1000 | Loss: 0.00003125
Iteration 142/1000 | Loss: 0.00002941
Iteration 143/1000 | Loss: 0.00003229
Iteration 144/1000 | Loss: 0.00005712
Iteration 145/1000 | Loss: 0.00003936
Iteration 146/1000 | Loss: 0.00002958
Iteration 147/1000 | Loss: 0.00003298
Iteration 148/1000 | Loss: 0.00003220
Iteration 149/1000 | Loss: 0.00003252
Iteration 150/1000 | Loss: 0.00003342
Iteration 151/1000 | Loss: 0.00003247
Iteration 152/1000 | Loss: 0.00003258
Iteration 153/1000 | Loss: 0.00003223
Iteration 154/1000 | Loss: 0.00002840
Iteration 155/1000 | Loss: 0.00003159
Iteration 156/1000 | Loss: 0.00003272
Iteration 157/1000 | Loss: 0.00003192
Iteration 158/1000 | Loss: 0.00003222
Iteration 159/1000 | Loss: 0.00003274
Iteration 160/1000 | Loss: 0.00002971
Iteration 161/1000 | Loss: 0.00005730
Iteration 162/1000 | Loss: 0.00003977
Iteration 163/1000 | Loss: 0.00003767
Iteration 164/1000 | Loss: 0.00003420
Iteration 165/1000 | Loss: 0.00003600
Iteration 166/1000 | Loss: 0.00003038
Iteration 167/1000 | Loss: 0.00002972
Iteration 168/1000 | Loss: 0.00003292
Iteration 169/1000 | Loss: 0.00003032
Iteration 170/1000 | Loss: 0.00002591
Iteration 171/1000 | Loss: 0.00002580
Iteration 172/1000 | Loss: 0.00002580
Iteration 173/1000 | Loss: 0.00002579
Iteration 174/1000 | Loss: 0.00002575
Iteration 175/1000 | Loss: 0.00002558
Iteration 176/1000 | Loss: 0.00002555
Iteration 177/1000 | Loss: 0.00002547
Iteration 178/1000 | Loss: 0.00005650
Iteration 179/1000 | Loss: 0.00005456
Iteration 180/1000 | Loss: 0.00005825
Iteration 181/1000 | Loss: 0.00003859
Iteration 182/1000 | Loss: 0.00002956
Iteration 183/1000 | Loss: 0.00003103
Iteration 184/1000 | Loss: 0.00002798
Iteration 185/1000 | Loss: 0.00002564
Iteration 186/1000 | Loss: 0.00005563
Iteration 187/1000 | Loss: 0.00004035
Iteration 188/1000 | Loss: 0.00006508
Iteration 189/1000 | Loss: 0.00003822
Iteration 190/1000 | Loss: 0.00006573
Iteration 191/1000 | Loss: 0.00003956
Iteration 192/1000 | Loss: 0.00006641
Iteration 193/1000 | Loss: 0.00005854
Iteration 194/1000 | Loss: 0.00003966
Iteration 195/1000 | Loss: 0.00003465
Iteration 196/1000 | Loss: 0.00004786
Iteration 197/1000 | Loss: 0.00004791
Iteration 198/1000 | Loss: 0.00004645
Iteration 199/1000 | Loss: 0.00003348
Iteration 200/1000 | Loss: 0.00003121
Iteration 201/1000 | Loss: 0.00002907
Iteration 202/1000 | Loss: 0.00002778
Iteration 203/1000 | Loss: 0.00002711
Iteration 204/1000 | Loss: 0.00002676
Iteration 205/1000 | Loss: 0.00002673
Iteration 206/1000 | Loss: 0.00002670
Iteration 207/1000 | Loss: 0.00002639
Iteration 208/1000 | Loss: 0.00002597
Iteration 209/1000 | Loss: 0.00002564
Iteration 210/1000 | Loss: 0.00002531
Iteration 211/1000 | Loss: 0.00002510
Iteration 212/1000 | Loss: 0.00002503
Iteration 213/1000 | Loss: 0.00002500
Iteration 214/1000 | Loss: 0.00002499
Iteration 215/1000 | Loss: 0.00002499
Iteration 216/1000 | Loss: 0.00002499
Iteration 217/1000 | Loss: 0.00002496
Iteration 218/1000 | Loss: 0.00002493
Iteration 219/1000 | Loss: 0.00002490
Iteration 220/1000 | Loss: 0.00002488
Iteration 221/1000 | Loss: 0.00002480
Iteration 222/1000 | Loss: 0.00002478
Iteration 223/1000 | Loss: 0.00002477
Iteration 224/1000 | Loss: 0.00002476
Iteration 225/1000 | Loss: 0.00002476
Iteration 226/1000 | Loss: 0.00002476
Iteration 227/1000 | Loss: 0.00002476
Iteration 228/1000 | Loss: 0.00002476
Iteration 229/1000 | Loss: 0.00002475
Iteration 230/1000 | Loss: 0.00002475
Iteration 231/1000 | Loss: 0.00002475
Iteration 232/1000 | Loss: 0.00002475
Iteration 233/1000 | Loss: 0.00002475
Iteration 234/1000 | Loss: 0.00002474
Iteration 235/1000 | Loss: 0.00002474
Iteration 236/1000 | Loss: 0.00002474
Iteration 237/1000 | Loss: 0.00002474
Iteration 238/1000 | Loss: 0.00002474
Iteration 239/1000 | Loss: 0.00002474
Iteration 240/1000 | Loss: 0.00002474
Iteration 241/1000 | Loss: 0.00002474
Iteration 242/1000 | Loss: 0.00002474
Iteration 243/1000 | Loss: 0.00002473
Iteration 244/1000 | Loss: 0.00002473
Iteration 245/1000 | Loss: 0.00002473
Iteration 246/1000 | Loss: 0.00002473
Iteration 247/1000 | Loss: 0.00002473
Iteration 248/1000 | Loss: 0.00002473
Iteration 249/1000 | Loss: 0.00002473
Iteration 250/1000 | Loss: 0.00002473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.473432141414378e-05, 2.473432141414378e-05, 2.473432141414378e-05, 2.473432141414378e-05, 2.473432141414378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.473432141414378e-05

Optimization complete. Final v2v error: 4.008604049682617 mm

Highest mean error: 7.910899639129639 mm for frame 115

Lowest mean error: 3.0746922492980957 mm for frame 74

Saving results

Total time: 325.2860186100006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00345373
Iteration 2/25 | Loss: 0.00147267
Iteration 3/25 | Loss: 0.00128492
Iteration 4/25 | Loss: 0.00121907
Iteration 5/25 | Loss: 0.00120446
Iteration 6/25 | Loss: 0.00120068
Iteration 7/25 | Loss: 0.00120248
Iteration 8/25 | Loss: 0.00119935
Iteration 9/25 | Loss: 0.00119866
Iteration 10/25 | Loss: 0.00119826
Iteration 11/25 | Loss: 0.00119647
Iteration 12/25 | Loss: 0.00119596
Iteration 13/25 | Loss: 0.00119584
Iteration 14/25 | Loss: 0.00119584
Iteration 15/25 | Loss: 0.00119583
Iteration 16/25 | Loss: 0.00119583
Iteration 17/25 | Loss: 0.00119583
Iteration 18/25 | Loss: 0.00119583
Iteration 19/25 | Loss: 0.00119583
Iteration 20/25 | Loss: 0.00119583
Iteration 21/25 | Loss: 0.00119583
Iteration 22/25 | Loss: 0.00119583
Iteration 23/25 | Loss: 0.00119583
Iteration 24/25 | Loss: 0.00119583
Iteration 25/25 | Loss: 0.00119583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35841680
Iteration 2/25 | Loss: 0.00115326
Iteration 3/25 | Loss: 0.00115326
Iteration 4/25 | Loss: 0.00115326
Iteration 5/25 | Loss: 0.00115326
Iteration 6/25 | Loss: 0.00115326
Iteration 7/25 | Loss: 0.00115326
Iteration 8/25 | Loss: 0.00115326
Iteration 9/25 | Loss: 0.00115326
Iteration 10/25 | Loss: 0.00115326
Iteration 11/25 | Loss: 0.00115325
Iteration 12/25 | Loss: 0.00115325
Iteration 13/25 | Loss: 0.00115325
Iteration 14/25 | Loss: 0.00115325
Iteration 15/25 | Loss: 0.00115325
Iteration 16/25 | Loss: 0.00115325
Iteration 17/25 | Loss: 0.00115325
Iteration 18/25 | Loss: 0.00115325
Iteration 19/25 | Loss: 0.00115325
Iteration 20/25 | Loss: 0.00115325
Iteration 21/25 | Loss: 0.00115325
Iteration 22/25 | Loss: 0.00115325
Iteration 23/25 | Loss: 0.00115325
Iteration 24/25 | Loss: 0.00115325
Iteration 25/25 | Loss: 0.00115325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115325
Iteration 2/1000 | Loss: 0.00004909
Iteration 3/1000 | Loss: 0.00002925
Iteration 4/1000 | Loss: 0.00002194
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001896
Iteration 7/1000 | Loss: 0.00001808
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001620
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001566
Iteration 20/1000 | Loss: 0.00001564
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001561
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001551
Iteration 35/1000 | Loss: 0.00001550
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001549
Iteration 39/1000 | Loss: 0.00001549
Iteration 40/1000 | Loss: 0.00001549
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001546
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001544
Iteration 54/1000 | Loss: 0.00001544
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001542
Iteration 63/1000 | Loss: 0.00001541
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001540
Iteration 68/1000 | Loss: 0.00001540
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001539
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001536
Iteration 87/1000 | Loss: 0.00001536
Iteration 88/1000 | Loss: 0.00001536
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001535
Iteration 91/1000 | Loss: 0.00001535
Iteration 92/1000 | Loss: 0.00001535
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001534
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001533
Iteration 102/1000 | Loss: 0.00001533
Iteration 103/1000 | Loss: 0.00001533
Iteration 104/1000 | Loss: 0.00001533
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001532
Iteration 107/1000 | Loss: 0.00001532
Iteration 108/1000 | Loss: 0.00001532
Iteration 109/1000 | Loss: 0.00001532
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001531
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001530
Iteration 120/1000 | Loss: 0.00001530
Iteration 121/1000 | Loss: 0.00001530
Iteration 122/1000 | Loss: 0.00001530
Iteration 123/1000 | Loss: 0.00001530
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001529
Iteration 128/1000 | Loss: 0.00001529
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001529
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001528
Iteration 141/1000 | Loss: 0.00001528
Iteration 142/1000 | Loss: 0.00001528
Iteration 143/1000 | Loss: 0.00001528
Iteration 144/1000 | Loss: 0.00001528
Iteration 145/1000 | Loss: 0.00001528
Iteration 146/1000 | Loss: 0.00001528
Iteration 147/1000 | Loss: 0.00001528
Iteration 148/1000 | Loss: 0.00001528
Iteration 149/1000 | Loss: 0.00001527
Iteration 150/1000 | Loss: 0.00001527
Iteration 151/1000 | Loss: 0.00001527
Iteration 152/1000 | Loss: 0.00001527
Iteration 153/1000 | Loss: 0.00001527
Iteration 154/1000 | Loss: 0.00001527
Iteration 155/1000 | Loss: 0.00001527
Iteration 156/1000 | Loss: 0.00001527
Iteration 157/1000 | Loss: 0.00001527
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001526
Iteration 161/1000 | Loss: 0.00001526
Iteration 162/1000 | Loss: 0.00001526
Iteration 163/1000 | Loss: 0.00001526
Iteration 164/1000 | Loss: 0.00001526
Iteration 165/1000 | Loss: 0.00001526
Iteration 166/1000 | Loss: 0.00001526
Iteration 167/1000 | Loss: 0.00001526
Iteration 168/1000 | Loss: 0.00001526
Iteration 169/1000 | Loss: 0.00001526
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001525
Iteration 175/1000 | Loss: 0.00001525
Iteration 176/1000 | Loss: 0.00001525
Iteration 177/1000 | Loss: 0.00001525
Iteration 178/1000 | Loss: 0.00001525
Iteration 179/1000 | Loss: 0.00001525
Iteration 180/1000 | Loss: 0.00001525
Iteration 181/1000 | Loss: 0.00001525
Iteration 182/1000 | Loss: 0.00001525
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001525
Iteration 185/1000 | Loss: 0.00001525
Iteration 186/1000 | Loss: 0.00001525
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001525
Iteration 189/1000 | Loss: 0.00001525
Iteration 190/1000 | Loss: 0.00001525
Iteration 191/1000 | Loss: 0.00001525
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001525
Iteration 195/1000 | Loss: 0.00001525
Iteration 196/1000 | Loss: 0.00001525
Iteration 197/1000 | Loss: 0.00001525
Iteration 198/1000 | Loss: 0.00001525
Iteration 199/1000 | Loss: 0.00001525
Iteration 200/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.5254559002642054e-05, 1.5254559002642054e-05, 1.5254559002642054e-05, 1.5254559002642054e-05, 1.5254559002642054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5254559002642054e-05

Optimization complete. Final v2v error: 3.30698823928833 mm

Highest mean error: 4.125386714935303 mm for frame 70

Lowest mean error: 2.7088747024536133 mm for frame 4

Saving results

Total time: 55.172980546951294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421949
Iteration 2/25 | Loss: 0.00128120
Iteration 3/25 | Loss: 0.00121531
Iteration 4/25 | Loss: 0.00119941
Iteration 5/25 | Loss: 0.00119469
Iteration 6/25 | Loss: 0.00119379
Iteration 7/25 | Loss: 0.00119379
Iteration 8/25 | Loss: 0.00119379
Iteration 9/25 | Loss: 0.00119379
Iteration 10/25 | Loss: 0.00119379
Iteration 11/25 | Loss: 0.00119379
Iteration 12/25 | Loss: 0.00119379
Iteration 13/25 | Loss: 0.00119379
Iteration 14/25 | Loss: 0.00119379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011937877861782908, 0.0011937877861782908, 0.0011937877861782908, 0.0011937877861782908, 0.0011937877861782908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011937877861782908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37656593
Iteration 2/25 | Loss: 0.00090718
Iteration 3/25 | Loss: 0.00090718
Iteration 4/25 | Loss: 0.00090718
Iteration 5/25 | Loss: 0.00090718
Iteration 6/25 | Loss: 0.00090718
Iteration 7/25 | Loss: 0.00090718
Iteration 8/25 | Loss: 0.00090718
Iteration 9/25 | Loss: 0.00090717
Iteration 10/25 | Loss: 0.00090717
Iteration 11/25 | Loss: 0.00090717
Iteration 12/25 | Loss: 0.00090717
Iteration 13/25 | Loss: 0.00090717
Iteration 14/25 | Loss: 0.00090717
Iteration 15/25 | Loss: 0.00090717
Iteration 16/25 | Loss: 0.00090717
Iteration 17/25 | Loss: 0.00090717
Iteration 18/25 | Loss: 0.00090717
Iteration 19/25 | Loss: 0.00090717
Iteration 20/25 | Loss: 0.00090717
Iteration 21/25 | Loss: 0.00090717
Iteration 22/25 | Loss: 0.00090717
Iteration 23/25 | Loss: 0.00090717
Iteration 24/25 | Loss: 0.00090717
Iteration 25/25 | Loss: 0.00090717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090717
Iteration 2/1000 | Loss: 0.00002660
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001465
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001369
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001283
Iteration 25/1000 | Loss: 0.00001282
Iteration 26/1000 | Loss: 0.00001281
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001269
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001267
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001266
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001251
Iteration 69/1000 | Loss: 0.00001250
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001246
Iteration 75/1000 | Loss: 0.00001246
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001243
Iteration 90/1000 | Loss: 0.00001243
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001243
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001240
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001239
Iteration 110/1000 | Loss: 0.00001239
Iteration 111/1000 | Loss: 0.00001239
Iteration 112/1000 | Loss: 0.00001239
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001238
Iteration 116/1000 | Loss: 0.00001238
Iteration 117/1000 | Loss: 0.00001238
Iteration 118/1000 | Loss: 0.00001238
Iteration 119/1000 | Loss: 0.00001238
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001236
Iteration 131/1000 | Loss: 0.00001236
Iteration 132/1000 | Loss: 0.00001236
Iteration 133/1000 | Loss: 0.00001236
Iteration 134/1000 | Loss: 0.00001236
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001235
Iteration 140/1000 | Loss: 0.00001235
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001235
Iteration 143/1000 | Loss: 0.00001235
Iteration 144/1000 | Loss: 0.00001235
Iteration 145/1000 | Loss: 0.00001235
Iteration 146/1000 | Loss: 0.00001235
Iteration 147/1000 | Loss: 0.00001235
Iteration 148/1000 | Loss: 0.00001235
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001234
Iteration 151/1000 | Loss: 0.00001234
Iteration 152/1000 | Loss: 0.00001234
Iteration 153/1000 | Loss: 0.00001234
Iteration 154/1000 | Loss: 0.00001234
Iteration 155/1000 | Loss: 0.00001234
Iteration 156/1000 | Loss: 0.00001234
Iteration 157/1000 | Loss: 0.00001234
Iteration 158/1000 | Loss: 0.00001234
Iteration 159/1000 | Loss: 0.00001234
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001234
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001234
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001234
Iteration 183/1000 | Loss: 0.00001234
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2335235624050256e-05, 1.2335235624050256e-05, 1.2335235624050256e-05, 1.2335235624050256e-05, 1.2335235624050256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2335235624050256e-05

Optimization complete. Final v2v error: 3.0249686241149902 mm

Highest mean error: 3.237598419189453 mm for frame 89

Lowest mean error: 2.8180365562438965 mm for frame 117

Saving results

Total time: 42.28581380844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391800
Iteration 2/25 | Loss: 0.00133347
Iteration 3/25 | Loss: 0.00124244
Iteration 4/25 | Loss: 0.00123192
Iteration 5/25 | Loss: 0.00122955
Iteration 6/25 | Loss: 0.00122946
Iteration 7/25 | Loss: 0.00122946
Iteration 8/25 | Loss: 0.00122946
Iteration 9/25 | Loss: 0.00122946
Iteration 10/25 | Loss: 0.00122946
Iteration 11/25 | Loss: 0.00122946
Iteration 12/25 | Loss: 0.00122946
Iteration 13/25 | Loss: 0.00122946
Iteration 14/25 | Loss: 0.00122946
Iteration 15/25 | Loss: 0.00122946
Iteration 16/25 | Loss: 0.00122946
Iteration 17/25 | Loss: 0.00122946
Iteration 18/25 | Loss: 0.00122946
Iteration 19/25 | Loss: 0.00122946
Iteration 20/25 | Loss: 0.00122946
Iteration 21/25 | Loss: 0.00122946
Iteration 22/25 | Loss: 0.00122946
Iteration 23/25 | Loss: 0.00122946
Iteration 24/25 | Loss: 0.00122946
Iteration 25/25 | Loss: 0.00122946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36071563
Iteration 2/25 | Loss: 0.00106146
Iteration 3/25 | Loss: 0.00106146
Iteration 4/25 | Loss: 0.00106146
Iteration 5/25 | Loss: 0.00106146
Iteration 6/25 | Loss: 0.00106146
Iteration 7/25 | Loss: 0.00106146
Iteration 8/25 | Loss: 0.00106146
Iteration 9/25 | Loss: 0.00106146
Iteration 10/25 | Loss: 0.00106146
Iteration 11/25 | Loss: 0.00106146
Iteration 12/25 | Loss: 0.00106146
Iteration 13/25 | Loss: 0.00106146
Iteration 14/25 | Loss: 0.00106146
Iteration 15/25 | Loss: 0.00106146
Iteration 16/25 | Loss: 0.00106146
Iteration 17/25 | Loss: 0.00106146
Iteration 18/25 | Loss: 0.00106146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010614582570269704, 0.0010614582570269704, 0.0010614582570269704, 0.0010614582570269704, 0.0010614582570269704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010614582570269704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106146
Iteration 2/1000 | Loss: 0.00003556
Iteration 3/1000 | Loss: 0.00002390
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001527
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001445
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001437
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001434
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001427
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001426
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001426
Iteration 36/1000 | Loss: 0.00001426
Iteration 37/1000 | Loss: 0.00001425
Iteration 38/1000 | Loss: 0.00001425
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001424
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001421
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001420
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001419
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001416
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001415
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001411
Iteration 73/1000 | Loss: 0.00001411
Iteration 74/1000 | Loss: 0.00001411
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001409
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001408
Iteration 94/1000 | Loss: 0.00001408
Iteration 95/1000 | Loss: 0.00001408
Iteration 96/1000 | Loss: 0.00001408
Iteration 97/1000 | Loss: 0.00001408
Iteration 98/1000 | Loss: 0.00001408
Iteration 99/1000 | Loss: 0.00001407
Iteration 100/1000 | Loss: 0.00001407
Iteration 101/1000 | Loss: 0.00001407
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001406
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001401
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001400
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001400
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001399
Iteration 147/1000 | Loss: 0.00001399
Iteration 148/1000 | Loss: 0.00001399
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001397
Iteration 154/1000 | Loss: 0.00001397
Iteration 155/1000 | Loss: 0.00001397
Iteration 156/1000 | Loss: 0.00001397
Iteration 157/1000 | Loss: 0.00001397
Iteration 158/1000 | Loss: 0.00001397
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001397
Iteration 162/1000 | Loss: 0.00001397
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001396
Iteration 169/1000 | Loss: 0.00001396
Iteration 170/1000 | Loss: 0.00001396
Iteration 171/1000 | Loss: 0.00001396
Iteration 172/1000 | Loss: 0.00001396
Iteration 173/1000 | Loss: 0.00001396
Iteration 174/1000 | Loss: 0.00001396
Iteration 175/1000 | Loss: 0.00001396
Iteration 176/1000 | Loss: 0.00001396
Iteration 177/1000 | Loss: 0.00001396
Iteration 178/1000 | Loss: 0.00001395
Iteration 179/1000 | Loss: 0.00001395
Iteration 180/1000 | Loss: 0.00001395
Iteration 181/1000 | Loss: 0.00001395
Iteration 182/1000 | Loss: 0.00001395
Iteration 183/1000 | Loss: 0.00001395
Iteration 184/1000 | Loss: 0.00001395
Iteration 185/1000 | Loss: 0.00001395
Iteration 186/1000 | Loss: 0.00001395
Iteration 187/1000 | Loss: 0.00001395
Iteration 188/1000 | Loss: 0.00001395
Iteration 189/1000 | Loss: 0.00001395
Iteration 190/1000 | Loss: 0.00001395
Iteration 191/1000 | Loss: 0.00001394
Iteration 192/1000 | Loss: 0.00001394
Iteration 193/1000 | Loss: 0.00001394
Iteration 194/1000 | Loss: 0.00001394
Iteration 195/1000 | Loss: 0.00001394
Iteration 196/1000 | Loss: 0.00001394
Iteration 197/1000 | Loss: 0.00001394
Iteration 198/1000 | Loss: 0.00001393
Iteration 199/1000 | Loss: 0.00001393
Iteration 200/1000 | Loss: 0.00001393
Iteration 201/1000 | Loss: 0.00001393
Iteration 202/1000 | Loss: 0.00001393
Iteration 203/1000 | Loss: 0.00001393
Iteration 204/1000 | Loss: 0.00001393
Iteration 205/1000 | Loss: 0.00001393
Iteration 206/1000 | Loss: 0.00001393
Iteration 207/1000 | Loss: 0.00001393
Iteration 208/1000 | Loss: 0.00001393
Iteration 209/1000 | Loss: 0.00001393
Iteration 210/1000 | Loss: 0.00001393
Iteration 211/1000 | Loss: 0.00001392
Iteration 212/1000 | Loss: 0.00001392
Iteration 213/1000 | Loss: 0.00001392
Iteration 214/1000 | Loss: 0.00001392
Iteration 215/1000 | Loss: 0.00001392
Iteration 216/1000 | Loss: 0.00001392
Iteration 217/1000 | Loss: 0.00001392
Iteration 218/1000 | Loss: 0.00001392
Iteration 219/1000 | Loss: 0.00001392
Iteration 220/1000 | Loss: 0.00001392
Iteration 221/1000 | Loss: 0.00001392
Iteration 222/1000 | Loss: 0.00001392
Iteration 223/1000 | Loss: 0.00001392
Iteration 224/1000 | Loss: 0.00001392
Iteration 225/1000 | Loss: 0.00001392
Iteration 226/1000 | Loss: 0.00001392
Iteration 227/1000 | Loss: 0.00001392
Iteration 228/1000 | Loss: 0.00001392
Iteration 229/1000 | Loss: 0.00001391
Iteration 230/1000 | Loss: 0.00001391
Iteration 231/1000 | Loss: 0.00001391
Iteration 232/1000 | Loss: 0.00001391
Iteration 233/1000 | Loss: 0.00001391
Iteration 234/1000 | Loss: 0.00001391
Iteration 235/1000 | Loss: 0.00001391
Iteration 236/1000 | Loss: 0.00001391
Iteration 237/1000 | Loss: 0.00001391
Iteration 238/1000 | Loss: 0.00001391
Iteration 239/1000 | Loss: 0.00001391
Iteration 240/1000 | Loss: 0.00001391
Iteration 241/1000 | Loss: 0.00001391
Iteration 242/1000 | Loss: 0.00001391
Iteration 243/1000 | Loss: 0.00001391
Iteration 244/1000 | Loss: 0.00001391
Iteration 245/1000 | Loss: 0.00001391
Iteration 246/1000 | Loss: 0.00001391
Iteration 247/1000 | Loss: 0.00001391
Iteration 248/1000 | Loss: 0.00001391
Iteration 249/1000 | Loss: 0.00001391
Iteration 250/1000 | Loss: 0.00001391
Iteration 251/1000 | Loss: 0.00001390
Iteration 252/1000 | Loss: 0.00001390
Iteration 253/1000 | Loss: 0.00001390
Iteration 254/1000 | Loss: 0.00001390
Iteration 255/1000 | Loss: 0.00001390
Iteration 256/1000 | Loss: 0.00001390
Iteration 257/1000 | Loss: 0.00001390
Iteration 258/1000 | Loss: 0.00001390
Iteration 259/1000 | Loss: 0.00001390
Iteration 260/1000 | Loss: 0.00001390
Iteration 261/1000 | Loss: 0.00001390
Iteration 262/1000 | Loss: 0.00001390
Iteration 263/1000 | Loss: 0.00001390
Iteration 264/1000 | Loss: 0.00001390
Iteration 265/1000 | Loss: 0.00001390
Iteration 266/1000 | Loss: 0.00001390
Iteration 267/1000 | Loss: 0.00001390
Iteration 268/1000 | Loss: 0.00001390
Iteration 269/1000 | Loss: 0.00001389
Iteration 270/1000 | Loss: 0.00001389
Iteration 271/1000 | Loss: 0.00001389
Iteration 272/1000 | Loss: 0.00001389
Iteration 273/1000 | Loss: 0.00001389
Iteration 274/1000 | Loss: 0.00001389
Iteration 275/1000 | Loss: 0.00001389
Iteration 276/1000 | Loss: 0.00001389
Iteration 277/1000 | Loss: 0.00001389
Iteration 278/1000 | Loss: 0.00001389
Iteration 279/1000 | Loss: 0.00001389
Iteration 280/1000 | Loss: 0.00001389
Iteration 281/1000 | Loss: 0.00001389
Iteration 282/1000 | Loss: 0.00001389
Iteration 283/1000 | Loss: 0.00001389
Iteration 284/1000 | Loss: 0.00001389
Iteration 285/1000 | Loss: 0.00001389
Iteration 286/1000 | Loss: 0.00001389
Iteration 287/1000 | Loss: 0.00001388
Iteration 288/1000 | Loss: 0.00001388
Iteration 289/1000 | Loss: 0.00001388
Iteration 290/1000 | Loss: 0.00001388
Iteration 291/1000 | Loss: 0.00001388
Iteration 292/1000 | Loss: 0.00001388
Iteration 293/1000 | Loss: 0.00001388
Iteration 294/1000 | Loss: 0.00001388
Iteration 295/1000 | Loss: 0.00001388
Iteration 296/1000 | Loss: 0.00001388
Iteration 297/1000 | Loss: 0.00001388
Iteration 298/1000 | Loss: 0.00001388
Iteration 299/1000 | Loss: 0.00001388
Iteration 300/1000 | Loss: 0.00001388
Iteration 301/1000 | Loss: 0.00001388
Iteration 302/1000 | Loss: 0.00001387
Iteration 303/1000 | Loss: 0.00001387
Iteration 304/1000 | Loss: 0.00001387
Iteration 305/1000 | Loss: 0.00001387
Iteration 306/1000 | Loss: 0.00001387
Iteration 307/1000 | Loss: 0.00001387
Iteration 308/1000 | Loss: 0.00001387
Iteration 309/1000 | Loss: 0.00001387
Iteration 310/1000 | Loss: 0.00001387
Iteration 311/1000 | Loss: 0.00001387
Iteration 312/1000 | Loss: 0.00001386
Iteration 313/1000 | Loss: 0.00001386
Iteration 314/1000 | Loss: 0.00001386
Iteration 315/1000 | Loss: 0.00001386
Iteration 316/1000 | Loss: 0.00001386
Iteration 317/1000 | Loss: 0.00001386
Iteration 318/1000 | Loss: 0.00001386
Iteration 319/1000 | Loss: 0.00001386
Iteration 320/1000 | Loss: 0.00001386
Iteration 321/1000 | Loss: 0.00001386
Iteration 322/1000 | Loss: 0.00001386
Iteration 323/1000 | Loss: 0.00001386
Iteration 324/1000 | Loss: 0.00001386
Iteration 325/1000 | Loss: 0.00001386
Iteration 326/1000 | Loss: 0.00001386
Iteration 327/1000 | Loss: 0.00001386
Iteration 328/1000 | Loss: 0.00001386
Iteration 329/1000 | Loss: 0.00001386
Iteration 330/1000 | Loss: 0.00001386
Iteration 331/1000 | Loss: 0.00001386
Iteration 332/1000 | Loss: 0.00001386
Iteration 333/1000 | Loss: 0.00001386
Iteration 334/1000 | Loss: 0.00001385
Iteration 335/1000 | Loss: 0.00001385
Iteration 336/1000 | Loss: 0.00001385
Iteration 337/1000 | Loss: 0.00001385
Iteration 338/1000 | Loss: 0.00001385
Iteration 339/1000 | Loss: 0.00001385
Iteration 340/1000 | Loss: 0.00001385
Iteration 341/1000 | Loss: 0.00001385
Iteration 342/1000 | Loss: 0.00001385
Iteration 343/1000 | Loss: 0.00001385
Iteration 344/1000 | Loss: 0.00001385
Iteration 345/1000 | Loss: 0.00001385
Iteration 346/1000 | Loss: 0.00001385
Iteration 347/1000 | Loss: 0.00001385
Iteration 348/1000 | Loss: 0.00001385
Iteration 349/1000 | Loss: 0.00001385
Iteration 350/1000 | Loss: 0.00001385
Iteration 351/1000 | Loss: 0.00001385
Iteration 352/1000 | Loss: 0.00001385
Iteration 353/1000 | Loss: 0.00001385
Iteration 354/1000 | Loss: 0.00001385
Iteration 355/1000 | Loss: 0.00001384
Iteration 356/1000 | Loss: 0.00001384
Iteration 357/1000 | Loss: 0.00001384
Iteration 358/1000 | Loss: 0.00001384
Iteration 359/1000 | Loss: 0.00001384
Iteration 360/1000 | Loss: 0.00001384
Iteration 361/1000 | Loss: 0.00001384
Iteration 362/1000 | Loss: 0.00001384
Iteration 363/1000 | Loss: 0.00001384
Iteration 364/1000 | Loss: 0.00001384
Iteration 365/1000 | Loss: 0.00001384
Iteration 366/1000 | Loss: 0.00001384
Iteration 367/1000 | Loss: 0.00001384
Iteration 368/1000 | Loss: 0.00001384
Iteration 369/1000 | Loss: 0.00001384
Iteration 370/1000 | Loss: 0.00001384
Iteration 371/1000 | Loss: 0.00001384
Iteration 372/1000 | Loss: 0.00001384
Iteration 373/1000 | Loss: 0.00001384
Iteration 374/1000 | Loss: 0.00001384
Iteration 375/1000 | Loss: 0.00001384
Iteration 376/1000 | Loss: 0.00001384
Iteration 377/1000 | Loss: 0.00001384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [1.3842128282703925e-05, 1.3842128282703925e-05, 1.3842128282703925e-05, 1.3842128282703925e-05, 1.3842128282703925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3842128282703925e-05

Optimization complete. Final v2v error: 3.0675907135009766 mm

Highest mean error: 3.5210845470428467 mm for frame 88

Lowest mean error: 2.5396828651428223 mm for frame 16

Saving results

Total time: 52.47934556007385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493245
Iteration 2/25 | Loss: 0.00162349
Iteration 3/25 | Loss: 0.00132553
Iteration 4/25 | Loss: 0.00128083
Iteration 5/25 | Loss: 0.00127436
Iteration 6/25 | Loss: 0.00127257
Iteration 7/25 | Loss: 0.00127237
Iteration 8/25 | Loss: 0.00127237
Iteration 9/25 | Loss: 0.00127237
Iteration 10/25 | Loss: 0.00127237
Iteration 11/25 | Loss: 0.00127237
Iteration 12/25 | Loss: 0.00127237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012723691761493683, 0.0012723691761493683, 0.0012723691761493683, 0.0012723691761493683, 0.0012723691761493683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012723691761493683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33384919
Iteration 2/25 | Loss: 0.00078860
Iteration 3/25 | Loss: 0.00078860
Iteration 4/25 | Loss: 0.00078859
Iteration 5/25 | Loss: 0.00078859
Iteration 6/25 | Loss: 0.00078859
Iteration 7/25 | Loss: 0.00078859
Iteration 8/25 | Loss: 0.00078859
Iteration 9/25 | Loss: 0.00078859
Iteration 10/25 | Loss: 0.00078859
Iteration 11/25 | Loss: 0.00078859
Iteration 12/25 | Loss: 0.00078859
Iteration 13/25 | Loss: 0.00078859
Iteration 14/25 | Loss: 0.00078859
Iteration 15/25 | Loss: 0.00078859
Iteration 16/25 | Loss: 0.00078859
Iteration 17/25 | Loss: 0.00078859
Iteration 18/25 | Loss: 0.00078859
Iteration 19/25 | Loss: 0.00078859
Iteration 20/25 | Loss: 0.00078859
Iteration 21/25 | Loss: 0.00078859
Iteration 22/25 | Loss: 0.00078859
Iteration 23/25 | Loss: 0.00078859
Iteration 24/25 | Loss: 0.00078859
Iteration 25/25 | Loss: 0.00078859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007885923841968179, 0.0007885923841968179, 0.0007885923841968179, 0.0007885923841968179, 0.0007885923841968179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007885923841968179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078859
Iteration 2/1000 | Loss: 0.00004727
Iteration 3/1000 | Loss: 0.00002729
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00001955
Iteration 6/1000 | Loss: 0.00001898
Iteration 7/1000 | Loss: 0.00001844
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001787
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001746
Iteration 20/1000 | Loss: 0.00001745
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001739
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001717
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001713
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001708
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001707
Iteration 52/1000 | Loss: 0.00001707
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001706
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001703
Iteration 82/1000 | Loss: 0.00001703
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001703
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001702
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.7007036149152555e-05, 1.7007036149152555e-05, 1.7007036149152555e-05, 1.7007036149152555e-05, 1.7007036149152555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7007036149152555e-05

Optimization complete. Final v2v error: 3.5753731727600098 mm

Highest mean error: 3.695272922515869 mm for frame 50

Lowest mean error: 3.2142367362976074 mm for frame 1

Saving results

Total time: 34.043222427368164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648222
Iteration 2/25 | Loss: 0.00151288
Iteration 3/25 | Loss: 0.00130315
Iteration 4/25 | Loss: 0.00127043
Iteration 5/25 | Loss: 0.00126078
Iteration 6/25 | Loss: 0.00125221
Iteration 7/25 | Loss: 0.00126115
Iteration 8/25 | Loss: 0.00124983
Iteration 9/25 | Loss: 0.00124379
Iteration 10/25 | Loss: 0.00123903
Iteration 11/25 | Loss: 0.00123842
Iteration 12/25 | Loss: 0.00123838
Iteration 13/25 | Loss: 0.00123838
Iteration 14/25 | Loss: 0.00123837
Iteration 15/25 | Loss: 0.00123837
Iteration 16/25 | Loss: 0.00123837
Iteration 17/25 | Loss: 0.00123837
Iteration 18/25 | Loss: 0.00123837
Iteration 19/25 | Loss: 0.00123837
Iteration 20/25 | Loss: 0.00123837
Iteration 21/25 | Loss: 0.00123837
Iteration 22/25 | Loss: 0.00123837
Iteration 23/25 | Loss: 0.00123837
Iteration 24/25 | Loss: 0.00123837
Iteration 25/25 | Loss: 0.00123837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99353075
Iteration 2/25 | Loss: 0.00115112
Iteration 3/25 | Loss: 0.00115081
Iteration 4/25 | Loss: 0.00115080
Iteration 5/25 | Loss: 0.00115080
Iteration 6/25 | Loss: 0.00115080
Iteration 7/25 | Loss: 0.00115080
Iteration 8/25 | Loss: 0.00115080
Iteration 9/25 | Loss: 0.00115080
Iteration 10/25 | Loss: 0.00115080
Iteration 11/25 | Loss: 0.00115080
Iteration 12/25 | Loss: 0.00115080
Iteration 13/25 | Loss: 0.00115080
Iteration 14/25 | Loss: 0.00115080
Iteration 15/25 | Loss: 0.00115080
Iteration 16/25 | Loss: 0.00115080
Iteration 17/25 | Loss: 0.00115080
Iteration 18/25 | Loss: 0.00115080
Iteration 19/25 | Loss: 0.00115080
Iteration 20/25 | Loss: 0.00115080
Iteration 21/25 | Loss: 0.00115080
Iteration 22/25 | Loss: 0.00115080
Iteration 23/25 | Loss: 0.00115080
Iteration 24/25 | Loss: 0.00115080
Iteration 25/25 | Loss: 0.00115080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115080
Iteration 2/1000 | Loss: 0.00006552
Iteration 3/1000 | Loss: 0.00003997
Iteration 4/1000 | Loss: 0.00003191
Iteration 5/1000 | Loss: 0.00002865
Iteration 6/1000 | Loss: 0.00002635
Iteration 7/1000 | Loss: 0.00002516
Iteration 8/1000 | Loss: 0.00002434
Iteration 9/1000 | Loss: 0.00002371
Iteration 10/1000 | Loss: 0.00002320
Iteration 11/1000 | Loss: 0.00002276
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00002195
Iteration 15/1000 | Loss: 0.00002188
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002162
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002135
Iteration 25/1000 | Loss: 0.00002133
Iteration 26/1000 | Loss: 0.00002132
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002130
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002122
Iteration 35/1000 | Loss: 0.00002122
Iteration 36/1000 | Loss: 0.00002121
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002119
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002119
Iteration 43/1000 | Loss: 0.00002118
Iteration 44/1000 | Loss: 0.00002117
Iteration 45/1000 | Loss: 0.00002116
Iteration 46/1000 | Loss: 0.00002116
Iteration 47/1000 | Loss: 0.00002116
Iteration 48/1000 | Loss: 0.00002115
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00002113
Iteration 56/1000 | Loss: 0.00002113
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00002112
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002109
Iteration 62/1000 | Loss: 0.00002109
Iteration 63/1000 | Loss: 0.00002109
Iteration 64/1000 | Loss: 0.00002108
Iteration 65/1000 | Loss: 0.00002108
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002107
Iteration 68/1000 | Loss: 0.00002107
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002106
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002106
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002105
Iteration 76/1000 | Loss: 0.00002105
Iteration 77/1000 | Loss: 0.00002104
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002103
Iteration 80/1000 | Loss: 0.00002103
Iteration 81/1000 | Loss: 0.00002103
Iteration 82/1000 | Loss: 0.00002103
Iteration 83/1000 | Loss: 0.00002102
Iteration 84/1000 | Loss: 0.00002102
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002101
Iteration 87/1000 | Loss: 0.00002101
Iteration 88/1000 | Loss: 0.00002101
Iteration 89/1000 | Loss: 0.00002101
Iteration 90/1000 | Loss: 0.00002100
Iteration 91/1000 | Loss: 0.00002100
Iteration 92/1000 | Loss: 0.00002100
Iteration 93/1000 | Loss: 0.00002100
Iteration 94/1000 | Loss: 0.00002100
Iteration 95/1000 | Loss: 0.00002100
Iteration 96/1000 | Loss: 0.00002100
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002100
Iteration 101/1000 | Loss: 0.00002100
Iteration 102/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.0999295884394087e-05, 2.0999295884394087e-05, 2.0999295884394087e-05, 2.0999295884394087e-05, 2.0999295884394087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0999295884394087e-05

Optimization complete. Final v2v error: 3.684462308883667 mm

Highest mean error: 6.116942882537842 mm for frame 120

Lowest mean error: 2.8079912662506104 mm for frame 9

Saving results

Total time: 61.53763127326965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037131
Iteration 2/25 | Loss: 0.01037131
Iteration 3/25 | Loss: 0.01037131
Iteration 4/25 | Loss: 0.01037131
Iteration 5/25 | Loss: 0.01037130
Iteration 6/25 | Loss: 0.01037130
Iteration 7/25 | Loss: 0.01037130
Iteration 8/25 | Loss: 0.01037130
Iteration 9/25 | Loss: 0.01037130
Iteration 10/25 | Loss: 0.01037130
Iteration 11/25 | Loss: 0.01037130
Iteration 12/25 | Loss: 0.01037130
Iteration 13/25 | Loss: 0.01037130
Iteration 14/25 | Loss: 0.01037130
Iteration 15/25 | Loss: 0.01037129
Iteration 16/25 | Loss: 0.01037129
Iteration 17/25 | Loss: 0.01037129
Iteration 18/25 | Loss: 0.01037129
Iteration 19/25 | Loss: 0.01037129
Iteration 20/25 | Loss: 0.01037129
Iteration 21/25 | Loss: 0.01037129
Iteration 22/25 | Loss: 0.01037129
Iteration 23/25 | Loss: 0.01037129
Iteration 24/25 | Loss: 0.01037129
Iteration 25/25 | Loss: 0.01037128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47388315
Iteration 2/25 | Loss: 0.12255054
Iteration 3/25 | Loss: 0.12073787
Iteration 4/25 | Loss: 0.12073787
Iteration 5/25 | Loss: 0.12073787
Iteration 6/25 | Loss: 0.12073787
Iteration 7/25 | Loss: 0.12073785
Iteration 8/25 | Loss: 0.12073785
Iteration 9/25 | Loss: 0.12073785
Iteration 10/25 | Loss: 0.12073785
Iteration 11/25 | Loss: 0.12073785
Iteration 12/25 | Loss: 0.12073785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.12073785066604614, 0.12073785066604614, 0.12073785066604614, 0.12073785066604614, 0.12073785066604614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12073785066604614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12073785
Iteration 2/1000 | Loss: 0.00128610
Iteration 3/1000 | Loss: 0.00069652
Iteration 4/1000 | Loss: 0.00015997
Iteration 5/1000 | Loss: 0.00020513
Iteration 6/1000 | Loss: 0.00007894
Iteration 7/1000 | Loss: 0.00008020
Iteration 8/1000 | Loss: 0.00002824
Iteration 9/1000 | Loss: 0.00002286
Iteration 10/1000 | Loss: 0.00002536
Iteration 11/1000 | Loss: 0.00002404
Iteration 12/1000 | Loss: 0.00001607
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001142
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001038
Iteration 25/1000 | Loss: 0.00001025
Iteration 26/1000 | Loss: 0.00001006
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000989
Iteration 32/1000 | Loss: 0.00000986
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000985
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000978
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000975
Iteration 46/1000 | Loss: 0.00000975
Iteration 47/1000 | Loss: 0.00000975
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000974
Iteration 51/1000 | Loss: 0.00000972
Iteration 52/1000 | Loss: 0.00000972
Iteration 53/1000 | Loss: 0.00000972
Iteration 54/1000 | Loss: 0.00000971
Iteration 55/1000 | Loss: 0.00000970
Iteration 56/1000 | Loss: 0.00000969
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000967
Iteration 59/1000 | Loss: 0.00000966
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000965
Iteration 62/1000 | Loss: 0.00000964
Iteration 63/1000 | Loss: 0.00000964
Iteration 64/1000 | Loss: 0.00000961
Iteration 65/1000 | Loss: 0.00000960
Iteration 66/1000 | Loss: 0.00000959
Iteration 67/1000 | Loss: 0.00000959
Iteration 68/1000 | Loss: 0.00000959
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000958
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000957
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000956
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000955
Iteration 82/1000 | Loss: 0.00000955
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000952
Iteration 86/1000 | Loss: 0.00000952
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000950
Iteration 90/1000 | Loss: 0.00000950
Iteration 91/1000 | Loss: 0.00000950
Iteration 92/1000 | Loss: 0.00000950
Iteration 93/1000 | Loss: 0.00000950
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000949
Iteration 96/1000 | Loss: 0.00000949
Iteration 97/1000 | Loss: 0.00000949
Iteration 98/1000 | Loss: 0.00000948
Iteration 99/1000 | Loss: 0.00000948
Iteration 100/1000 | Loss: 0.00000948
Iteration 101/1000 | Loss: 0.00000948
Iteration 102/1000 | Loss: 0.00000948
Iteration 103/1000 | Loss: 0.00000947
Iteration 104/1000 | Loss: 0.00000947
Iteration 105/1000 | Loss: 0.00000947
Iteration 106/1000 | Loss: 0.00000947
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000945
Iteration 109/1000 | Loss: 0.00000945
Iteration 110/1000 | Loss: 0.00000945
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000943
Iteration 113/1000 | Loss: 0.00000943
Iteration 114/1000 | Loss: 0.00000942
Iteration 115/1000 | Loss: 0.00000942
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000941
Iteration 118/1000 | Loss: 0.00000941
Iteration 119/1000 | Loss: 0.00000941
Iteration 120/1000 | Loss: 0.00000941
Iteration 121/1000 | Loss: 0.00000941
Iteration 122/1000 | Loss: 0.00000941
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000940
Iteration 125/1000 | Loss: 0.00000940
Iteration 126/1000 | Loss: 0.00000940
Iteration 127/1000 | Loss: 0.00000940
Iteration 128/1000 | Loss: 0.00000940
Iteration 129/1000 | Loss: 0.00000940
Iteration 130/1000 | Loss: 0.00000940
Iteration 131/1000 | Loss: 0.00000940
Iteration 132/1000 | Loss: 0.00000940
Iteration 133/1000 | Loss: 0.00000939
Iteration 134/1000 | Loss: 0.00000939
Iteration 135/1000 | Loss: 0.00000939
Iteration 136/1000 | Loss: 0.00000939
Iteration 137/1000 | Loss: 0.00000938
Iteration 138/1000 | Loss: 0.00000938
Iteration 139/1000 | Loss: 0.00000938
Iteration 140/1000 | Loss: 0.00000938
Iteration 141/1000 | Loss: 0.00000938
Iteration 142/1000 | Loss: 0.00000937
Iteration 143/1000 | Loss: 0.00000937
Iteration 144/1000 | Loss: 0.00000937
Iteration 145/1000 | Loss: 0.00000937
Iteration 146/1000 | Loss: 0.00000937
Iteration 147/1000 | Loss: 0.00000937
Iteration 148/1000 | Loss: 0.00000937
Iteration 149/1000 | Loss: 0.00000936
Iteration 150/1000 | Loss: 0.00000936
Iteration 151/1000 | Loss: 0.00000936
Iteration 152/1000 | Loss: 0.00000936
Iteration 153/1000 | Loss: 0.00000936
Iteration 154/1000 | Loss: 0.00000936
Iteration 155/1000 | Loss: 0.00000936
Iteration 156/1000 | Loss: 0.00000935
Iteration 157/1000 | Loss: 0.00000935
Iteration 158/1000 | Loss: 0.00000935
Iteration 159/1000 | Loss: 0.00000935
Iteration 160/1000 | Loss: 0.00000935
Iteration 161/1000 | Loss: 0.00000935
Iteration 162/1000 | Loss: 0.00000935
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000934
Iteration 165/1000 | Loss: 0.00000934
Iteration 166/1000 | Loss: 0.00000934
Iteration 167/1000 | Loss: 0.00000934
Iteration 168/1000 | Loss: 0.00000934
Iteration 169/1000 | Loss: 0.00000934
Iteration 170/1000 | Loss: 0.00000934
Iteration 171/1000 | Loss: 0.00000934
Iteration 172/1000 | Loss: 0.00000934
Iteration 173/1000 | Loss: 0.00000934
Iteration 174/1000 | Loss: 0.00000934
Iteration 175/1000 | Loss: 0.00000934
Iteration 176/1000 | Loss: 0.00000934
Iteration 177/1000 | Loss: 0.00000934
Iteration 178/1000 | Loss: 0.00000934
Iteration 179/1000 | Loss: 0.00000934
Iteration 180/1000 | Loss: 0.00000934
Iteration 181/1000 | Loss: 0.00000934
Iteration 182/1000 | Loss: 0.00000934
Iteration 183/1000 | Loss: 0.00000933
Iteration 184/1000 | Loss: 0.00000933
Iteration 185/1000 | Loss: 0.00000933
Iteration 186/1000 | Loss: 0.00000933
Iteration 187/1000 | Loss: 0.00000933
Iteration 188/1000 | Loss: 0.00000933
Iteration 189/1000 | Loss: 0.00000933
Iteration 190/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [9.332145964435767e-06, 9.332145964435767e-06, 9.332145964435767e-06, 9.332145964435767e-06, 9.332145964435767e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.332145964435767e-06

Optimization complete. Final v2v error: 2.640836715698242 mm

Highest mean error: 2.9224236011505127 mm for frame 79

Lowest mean error: 2.469996213912964 mm for frame 180

Saving results

Total time: 65.02544522285461
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565245
Iteration 2/25 | Loss: 0.00127477
Iteration 3/25 | Loss: 0.00120052
Iteration 4/25 | Loss: 0.00118837
Iteration 5/25 | Loss: 0.00118346
Iteration 6/25 | Loss: 0.00118235
Iteration 7/25 | Loss: 0.00118235
Iteration 8/25 | Loss: 0.00118235
Iteration 9/25 | Loss: 0.00118235
Iteration 10/25 | Loss: 0.00118235
Iteration 11/25 | Loss: 0.00118235
Iteration 12/25 | Loss: 0.00118235
Iteration 13/25 | Loss: 0.00118235
Iteration 14/25 | Loss: 0.00118235
Iteration 15/25 | Loss: 0.00118235
Iteration 16/25 | Loss: 0.00118235
Iteration 17/25 | Loss: 0.00118235
Iteration 18/25 | Loss: 0.00118235
Iteration 19/25 | Loss: 0.00118235
Iteration 20/25 | Loss: 0.00118235
Iteration 21/25 | Loss: 0.00118235
Iteration 22/25 | Loss: 0.00118235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011823507957160473, 0.0011823507957160473, 0.0011823507957160473, 0.0011823507957160473, 0.0011823507957160473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011823507957160473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70171762
Iteration 2/25 | Loss: 0.00099367
Iteration 3/25 | Loss: 0.00099367
Iteration 4/25 | Loss: 0.00099367
Iteration 5/25 | Loss: 0.00099367
Iteration 6/25 | Loss: 0.00099367
Iteration 7/25 | Loss: 0.00099367
Iteration 8/25 | Loss: 0.00099367
Iteration 9/25 | Loss: 0.00099367
Iteration 10/25 | Loss: 0.00099367
Iteration 11/25 | Loss: 0.00099367
Iteration 12/25 | Loss: 0.00099367
Iteration 13/25 | Loss: 0.00099367
Iteration 14/25 | Loss: 0.00099367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009936690330505371, 0.0009936690330505371, 0.0009936690330505371, 0.0009936690330505371, 0.0009936690330505371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009936690330505371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099367
Iteration 2/1000 | Loss: 0.00001987
Iteration 3/1000 | Loss: 0.00001452
Iteration 4/1000 | Loss: 0.00001324
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001218
Iteration 7/1000 | Loss: 0.00001179
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001124
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001117
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001106
Iteration 22/1000 | Loss: 0.00001105
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001104
Iteration 25/1000 | Loss: 0.00001097
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001076
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001076
Iteration 45/1000 | Loss: 0.00001076
Iteration 46/1000 | Loss: 0.00001076
Iteration 47/1000 | Loss: 0.00001076
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001074
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001072
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001071
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001070
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001067
Iteration 65/1000 | Loss: 0.00001066
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001062
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001059
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001058
Iteration 75/1000 | Loss: 0.00001058
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001058
Iteration 78/1000 | Loss: 0.00001058
Iteration 79/1000 | Loss: 0.00001058
Iteration 80/1000 | Loss: 0.00001058
Iteration 81/1000 | Loss: 0.00001058
Iteration 82/1000 | Loss: 0.00001057
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001057
Iteration 85/1000 | Loss: 0.00001056
Iteration 86/1000 | Loss: 0.00001055
Iteration 87/1000 | Loss: 0.00001055
Iteration 88/1000 | Loss: 0.00001055
Iteration 89/1000 | Loss: 0.00001054
Iteration 90/1000 | Loss: 0.00001054
Iteration 91/1000 | Loss: 0.00001054
Iteration 92/1000 | Loss: 0.00001054
Iteration 93/1000 | Loss: 0.00001054
Iteration 94/1000 | Loss: 0.00001054
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001053
Iteration 97/1000 | Loss: 0.00001051
Iteration 98/1000 | Loss: 0.00001051
Iteration 99/1000 | Loss: 0.00001051
Iteration 100/1000 | Loss: 0.00001051
Iteration 101/1000 | Loss: 0.00001050
Iteration 102/1000 | Loss: 0.00001050
Iteration 103/1000 | Loss: 0.00001050
Iteration 104/1000 | Loss: 0.00001050
Iteration 105/1000 | Loss: 0.00001049
Iteration 106/1000 | Loss: 0.00001049
Iteration 107/1000 | Loss: 0.00001049
Iteration 108/1000 | Loss: 0.00001049
Iteration 109/1000 | Loss: 0.00001048
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001048
Iteration 112/1000 | Loss: 0.00001048
Iteration 113/1000 | Loss: 0.00001048
Iteration 114/1000 | Loss: 0.00001048
Iteration 115/1000 | Loss: 0.00001048
Iteration 116/1000 | Loss: 0.00001048
Iteration 117/1000 | Loss: 0.00001048
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001047
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001046
Iteration 124/1000 | Loss: 0.00001046
Iteration 125/1000 | Loss: 0.00001046
Iteration 126/1000 | Loss: 0.00001046
Iteration 127/1000 | Loss: 0.00001045
Iteration 128/1000 | Loss: 0.00001045
Iteration 129/1000 | Loss: 0.00001045
Iteration 130/1000 | Loss: 0.00001045
Iteration 131/1000 | Loss: 0.00001045
Iteration 132/1000 | Loss: 0.00001045
Iteration 133/1000 | Loss: 0.00001045
Iteration 134/1000 | Loss: 0.00001045
Iteration 135/1000 | Loss: 0.00001045
Iteration 136/1000 | Loss: 0.00001045
Iteration 137/1000 | Loss: 0.00001045
Iteration 138/1000 | Loss: 0.00001044
Iteration 139/1000 | Loss: 0.00001044
Iteration 140/1000 | Loss: 0.00001044
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001042
Iteration 164/1000 | Loss: 0.00001042
Iteration 165/1000 | Loss: 0.00001042
Iteration 166/1000 | Loss: 0.00001042
Iteration 167/1000 | Loss: 0.00001042
Iteration 168/1000 | Loss: 0.00001042
Iteration 169/1000 | Loss: 0.00001042
Iteration 170/1000 | Loss: 0.00001042
Iteration 171/1000 | Loss: 0.00001041
Iteration 172/1000 | Loss: 0.00001041
Iteration 173/1000 | Loss: 0.00001041
Iteration 174/1000 | Loss: 0.00001041
Iteration 175/1000 | Loss: 0.00001041
Iteration 176/1000 | Loss: 0.00001041
Iteration 177/1000 | Loss: 0.00001041
Iteration 178/1000 | Loss: 0.00001041
Iteration 179/1000 | Loss: 0.00001041
Iteration 180/1000 | Loss: 0.00001041
Iteration 181/1000 | Loss: 0.00001041
Iteration 182/1000 | Loss: 0.00001041
Iteration 183/1000 | Loss: 0.00001041
Iteration 184/1000 | Loss: 0.00001040
Iteration 185/1000 | Loss: 0.00001040
Iteration 186/1000 | Loss: 0.00001040
Iteration 187/1000 | Loss: 0.00001040
Iteration 188/1000 | Loss: 0.00001040
Iteration 189/1000 | Loss: 0.00001040
Iteration 190/1000 | Loss: 0.00001040
Iteration 191/1000 | Loss: 0.00001040
Iteration 192/1000 | Loss: 0.00001040
Iteration 193/1000 | Loss: 0.00001040
Iteration 194/1000 | Loss: 0.00001040
Iteration 195/1000 | Loss: 0.00001040
Iteration 196/1000 | Loss: 0.00001040
Iteration 197/1000 | Loss: 0.00001040
Iteration 198/1000 | Loss: 0.00001040
Iteration 199/1000 | Loss: 0.00001040
Iteration 200/1000 | Loss: 0.00001040
Iteration 201/1000 | Loss: 0.00001040
Iteration 202/1000 | Loss: 0.00001040
Iteration 203/1000 | Loss: 0.00001040
Iteration 204/1000 | Loss: 0.00001040
Iteration 205/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.0400266546639614e-05, 1.0400266546639614e-05, 1.0400266546639614e-05, 1.0400266546639614e-05, 1.0400266546639614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0400266546639614e-05

Optimization complete. Final v2v error: 2.772115468978882 mm

Highest mean error: 3.010922908782959 mm for frame 58

Lowest mean error: 2.623802423477173 mm for frame 30

Saving results

Total time: 40.029781103134155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426165
Iteration 2/25 | Loss: 0.00129394
Iteration 3/25 | Loss: 0.00122081
Iteration 4/25 | Loss: 0.00120610
Iteration 5/25 | Loss: 0.00120243
Iteration 6/25 | Loss: 0.00120217
Iteration 7/25 | Loss: 0.00120217
Iteration 8/25 | Loss: 0.00120217
Iteration 9/25 | Loss: 0.00120217
Iteration 10/25 | Loss: 0.00120217
Iteration 11/25 | Loss: 0.00120217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001202165149152279, 0.001202165149152279, 0.001202165149152279, 0.001202165149152279, 0.001202165149152279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001202165149152279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.09842253
Iteration 2/25 | Loss: 0.00087554
Iteration 3/25 | Loss: 0.00087553
Iteration 4/25 | Loss: 0.00087552
Iteration 5/25 | Loss: 0.00087552
Iteration 6/25 | Loss: 0.00087552
Iteration 7/25 | Loss: 0.00087552
Iteration 8/25 | Loss: 0.00087552
Iteration 9/25 | Loss: 0.00087552
Iteration 10/25 | Loss: 0.00087552
Iteration 11/25 | Loss: 0.00087552
Iteration 12/25 | Loss: 0.00087552
Iteration 13/25 | Loss: 0.00087552
Iteration 14/25 | Loss: 0.00087552
Iteration 15/25 | Loss: 0.00087552
Iteration 16/25 | Loss: 0.00087552
Iteration 17/25 | Loss: 0.00087552
Iteration 18/25 | Loss: 0.00087552
Iteration 19/25 | Loss: 0.00087552
Iteration 20/25 | Loss: 0.00087552
Iteration 21/25 | Loss: 0.00087552
Iteration 22/25 | Loss: 0.00087552
Iteration 23/25 | Loss: 0.00087552
Iteration 24/25 | Loss: 0.00087552
Iteration 25/25 | Loss: 0.00087552
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008755212766118348, 0.0008755212766118348, 0.0008755212766118348, 0.0008755212766118348, 0.0008755212766118348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008755212766118348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087552
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001589
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001497
Iteration 12/1000 | Loss: 0.00001482
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001464
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001431
Iteration 28/1000 | Loss: 0.00001431
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001431
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001431
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001418
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001408
Iteration 54/1000 | Loss: 0.00001408
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001407
Iteration 61/1000 | Loss: 0.00001407
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001405
Iteration 73/1000 | Loss: 0.00001405
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001404
Iteration 77/1000 | Loss: 0.00001404
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.403634360030992e-05, 1.403634360030992e-05, 1.403634360030992e-05, 1.403634360030992e-05, 1.403634360030992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.403634360030992e-05

Optimization complete. Final v2v error: 3.1834399700164795 mm

Highest mean error: 3.9936671257019043 mm for frame 235

Lowest mean error: 2.9733238220214844 mm for frame 11

Saving results

Total time: 42.06722855567932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754230
Iteration 2/25 | Loss: 0.00190910
Iteration 3/25 | Loss: 0.00133579
Iteration 4/25 | Loss: 0.00125614
Iteration 5/25 | Loss: 0.00123372
Iteration 6/25 | Loss: 0.00122734
Iteration 7/25 | Loss: 0.00122869
Iteration 8/25 | Loss: 0.00122788
Iteration 9/25 | Loss: 0.00120876
Iteration 10/25 | Loss: 0.00119318
Iteration 11/25 | Loss: 0.00119236
Iteration 12/25 | Loss: 0.00118720
Iteration 13/25 | Loss: 0.00118704
Iteration 14/25 | Loss: 0.00119044
Iteration 15/25 | Loss: 0.00119814
Iteration 16/25 | Loss: 0.00120664
Iteration 17/25 | Loss: 0.00121145
Iteration 18/25 | Loss: 0.00120261
Iteration 19/25 | Loss: 0.00118372
Iteration 20/25 | Loss: 0.00117966
Iteration 21/25 | Loss: 0.00117877
Iteration 22/25 | Loss: 0.00117865
Iteration 23/25 | Loss: 0.00117858
Iteration 24/25 | Loss: 0.00117858
Iteration 25/25 | Loss: 0.00117858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18245935
Iteration 2/25 | Loss: 0.00091022
Iteration 3/25 | Loss: 0.00091021
Iteration 4/25 | Loss: 0.00091021
Iteration 5/25 | Loss: 0.00091021
Iteration 6/25 | Loss: 0.00091021
Iteration 7/25 | Loss: 0.00091021
Iteration 8/25 | Loss: 0.00091021
Iteration 9/25 | Loss: 0.00091021
Iteration 10/25 | Loss: 0.00091021
Iteration 11/25 | Loss: 0.00091021
Iteration 12/25 | Loss: 0.00091021
Iteration 13/25 | Loss: 0.00091021
Iteration 14/25 | Loss: 0.00091021
Iteration 15/25 | Loss: 0.00091021
Iteration 16/25 | Loss: 0.00091021
Iteration 17/25 | Loss: 0.00091021
Iteration 18/25 | Loss: 0.00091021
Iteration 19/25 | Loss: 0.00091021
Iteration 20/25 | Loss: 0.00091021
Iteration 21/25 | Loss: 0.00091021
Iteration 22/25 | Loss: 0.00091021
Iteration 23/25 | Loss: 0.00091021
Iteration 24/25 | Loss: 0.00091021
Iteration 25/25 | Loss: 0.00091021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091021
Iteration 2/1000 | Loss: 0.00002250
Iteration 3/1000 | Loss: 0.00006796
Iteration 4/1000 | Loss: 0.00001650
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00007704
Iteration 7/1000 | Loss: 0.00001905
Iteration 8/1000 | Loss: 0.00002377
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00008724
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001295
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001294
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001257
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001315
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001216
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1864068255817983e-05, 1.1864068255817983e-05, 1.1864068255817983e-05, 1.1864068255817983e-05, 1.1864068255817983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1864068255817983e-05

Optimization complete. Final v2v error: 2.9307916164398193 mm

Highest mean error: 5.673282146453857 mm for frame 130

Lowest mean error: 2.5565502643585205 mm for frame 217

Saving results

Total time: 94.26808071136475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922620
Iteration 2/25 | Loss: 0.00362370
Iteration 3/25 | Loss: 0.00274980
Iteration 4/25 | Loss: 0.00253891
Iteration 5/25 | Loss: 0.00278887
Iteration 6/25 | Loss: 0.00250911
Iteration 7/25 | Loss: 0.00236139
Iteration 8/25 | Loss: 0.00230456
Iteration 9/25 | Loss: 0.00215733
Iteration 10/25 | Loss: 0.00207910
Iteration 11/25 | Loss: 0.00211769
Iteration 12/25 | Loss: 0.00189479
Iteration 13/25 | Loss: 0.00183853
Iteration 14/25 | Loss: 0.00177855
Iteration 15/25 | Loss: 0.00170783
Iteration 16/25 | Loss: 0.00170788
Iteration 17/25 | Loss: 0.00169965
Iteration 18/25 | Loss: 0.00173738
Iteration 19/25 | Loss: 0.00171030
Iteration 20/25 | Loss: 0.00171601
Iteration 21/25 | Loss: 0.00169523
Iteration 22/25 | Loss: 0.00168632
Iteration 23/25 | Loss: 0.00169245
Iteration 24/25 | Loss: 0.00168946
Iteration 25/25 | Loss: 0.00169392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90586543
Iteration 2/25 | Loss: 0.00836536
Iteration 3/25 | Loss: 0.00583461
Iteration 4/25 | Loss: 0.00583064
Iteration 5/25 | Loss: 0.00583064
Iteration 6/25 | Loss: 0.00583064
Iteration 7/25 | Loss: 0.00583064
Iteration 8/25 | Loss: 0.00583064
Iteration 9/25 | Loss: 0.00583064
Iteration 10/25 | Loss: 0.00583064
Iteration 11/25 | Loss: 0.00583064
Iteration 12/25 | Loss: 0.00583064
Iteration 13/25 | Loss: 0.00583064
Iteration 14/25 | Loss: 0.00583064
Iteration 15/25 | Loss: 0.00583064
Iteration 16/25 | Loss: 0.00583064
Iteration 17/25 | Loss: 0.00583064
Iteration 18/25 | Loss: 0.00583064
Iteration 19/25 | Loss: 0.00583064
Iteration 20/25 | Loss: 0.00583064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00583063717931509, 0.00583063717931509, 0.00583063717931509, 0.00583063717931509, 0.00583063717931509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00583063717931509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00583064
Iteration 2/1000 | Loss: 0.00375028
Iteration 3/1000 | Loss: 0.00351569
Iteration 4/1000 | Loss: 0.00290399
Iteration 5/1000 | Loss: 0.00322727
Iteration 6/1000 | Loss: 0.00665795
Iteration 7/1000 | Loss: 0.00457268
Iteration 8/1000 | Loss: 0.00998006
Iteration 9/1000 | Loss: 0.00341570
Iteration 10/1000 | Loss: 0.00946035
Iteration 11/1000 | Loss: 0.00710692
Iteration 12/1000 | Loss: 0.00452218
Iteration 13/1000 | Loss: 0.00275245
Iteration 14/1000 | Loss: 0.00379193
Iteration 15/1000 | Loss: 0.00308460
Iteration 16/1000 | Loss: 0.00274558
Iteration 17/1000 | Loss: 0.00309337
Iteration 18/1000 | Loss: 0.00484900
Iteration 19/1000 | Loss: 0.00424376
Iteration 20/1000 | Loss: 0.00229841
Iteration 21/1000 | Loss: 0.00267318
Iteration 22/1000 | Loss: 0.00172419
Iteration 23/1000 | Loss: 0.00501663
Iteration 24/1000 | Loss: 0.00318173
Iteration 25/1000 | Loss: 0.00387940
Iteration 26/1000 | Loss: 0.00223263
Iteration 27/1000 | Loss: 0.00159718
Iteration 28/1000 | Loss: 0.00145230
Iteration 29/1000 | Loss: 0.00240666
Iteration 30/1000 | Loss: 0.00183000
Iteration 31/1000 | Loss: 0.00291255
Iteration 32/1000 | Loss: 0.00163692
Iteration 33/1000 | Loss: 0.00202603
Iteration 34/1000 | Loss: 0.00192254
Iteration 35/1000 | Loss: 0.00170821
Iteration 36/1000 | Loss: 0.00227669
Iteration 37/1000 | Loss: 0.00177068
Iteration 38/1000 | Loss: 0.00156589
Iteration 39/1000 | Loss: 0.00228633
Iteration 40/1000 | Loss: 0.00166602
Iteration 41/1000 | Loss: 0.00199609
Iteration 42/1000 | Loss: 0.00224244
Iteration 43/1000 | Loss: 0.00159564
Iteration 44/1000 | Loss: 0.00125896
Iteration 45/1000 | Loss: 0.00117467
Iteration 46/1000 | Loss: 0.00435627
Iteration 47/1000 | Loss: 0.00184430
Iteration 48/1000 | Loss: 0.00275290
Iteration 49/1000 | Loss: 0.00121012
Iteration 50/1000 | Loss: 0.00069454
Iteration 51/1000 | Loss: 0.00261651
Iteration 52/1000 | Loss: 0.00150902
Iteration 53/1000 | Loss: 0.00074486
Iteration 54/1000 | Loss: 0.00083029
Iteration 55/1000 | Loss: 0.00237390
Iteration 56/1000 | Loss: 0.00064281
Iteration 57/1000 | Loss: 0.00065377
Iteration 58/1000 | Loss: 0.00022589
Iteration 59/1000 | Loss: 0.00028218
Iteration 60/1000 | Loss: 0.00270938
Iteration 61/1000 | Loss: 0.00126934
Iteration 62/1000 | Loss: 0.00109569
Iteration 63/1000 | Loss: 0.00068528
Iteration 64/1000 | Loss: 0.00039595
Iteration 65/1000 | Loss: 0.00017621
Iteration 66/1000 | Loss: 0.00052703
Iteration 67/1000 | Loss: 0.00045962
Iteration 68/1000 | Loss: 0.00106737
Iteration 69/1000 | Loss: 0.00072028
Iteration 70/1000 | Loss: 0.00024328
Iteration 71/1000 | Loss: 0.00045959
Iteration 72/1000 | Loss: 0.00039968
Iteration 73/1000 | Loss: 0.00148841
Iteration 74/1000 | Loss: 0.00036164
Iteration 75/1000 | Loss: 0.00054121
Iteration 76/1000 | Loss: 0.00030240
Iteration 77/1000 | Loss: 0.00026456
Iteration 78/1000 | Loss: 0.00025664
Iteration 79/1000 | Loss: 0.00026714
Iteration 80/1000 | Loss: 0.00024009
Iteration 81/1000 | Loss: 0.00058911
Iteration 82/1000 | Loss: 0.00031898
Iteration 83/1000 | Loss: 0.00036154
Iteration 84/1000 | Loss: 0.00041719
Iteration 85/1000 | Loss: 0.00034221
Iteration 86/1000 | Loss: 0.00033175
Iteration 87/1000 | Loss: 0.00024501
Iteration 88/1000 | Loss: 0.00096696
Iteration 89/1000 | Loss: 0.00057625
Iteration 90/1000 | Loss: 0.00027888
Iteration 91/1000 | Loss: 0.00053789
Iteration 92/1000 | Loss: 0.00015165
Iteration 93/1000 | Loss: 0.00013054
Iteration 94/1000 | Loss: 0.00012814
Iteration 95/1000 | Loss: 0.00103047
Iteration 96/1000 | Loss: 0.00261696
Iteration 97/1000 | Loss: 0.00196341
Iteration 98/1000 | Loss: 0.00730337
Iteration 99/1000 | Loss: 0.00571689
Iteration 100/1000 | Loss: 0.00096160
Iteration 101/1000 | Loss: 0.00033126
Iteration 102/1000 | Loss: 0.00043555
Iteration 103/1000 | Loss: 0.00020891
Iteration 104/1000 | Loss: 0.00023183
Iteration 105/1000 | Loss: 0.00207606
Iteration 106/1000 | Loss: 0.00034725
Iteration 107/1000 | Loss: 0.00160616
Iteration 108/1000 | Loss: 0.00056559
Iteration 109/1000 | Loss: 0.00029442
Iteration 110/1000 | Loss: 0.00012170
Iteration 111/1000 | Loss: 0.00011235
Iteration 112/1000 | Loss: 0.00010687
Iteration 113/1000 | Loss: 0.00054183
Iteration 114/1000 | Loss: 0.00011712
Iteration 115/1000 | Loss: 0.00018654
Iteration 116/1000 | Loss: 0.00010797
Iteration 117/1000 | Loss: 0.00010088
Iteration 118/1000 | Loss: 0.00009902
Iteration 119/1000 | Loss: 0.00018868
Iteration 120/1000 | Loss: 0.00011762
Iteration 121/1000 | Loss: 0.00009805
Iteration 122/1000 | Loss: 0.00009946
Iteration 123/1000 | Loss: 0.00009563
Iteration 124/1000 | Loss: 0.00014863
Iteration 125/1000 | Loss: 0.00009673
Iteration 126/1000 | Loss: 0.00009466
Iteration 127/1000 | Loss: 0.00009344
Iteration 128/1000 | Loss: 0.00019568
Iteration 129/1000 | Loss: 0.00009948
Iteration 130/1000 | Loss: 0.00017694
Iteration 131/1000 | Loss: 0.00011174
Iteration 132/1000 | Loss: 0.00011384
Iteration 133/1000 | Loss: 0.00009113
Iteration 134/1000 | Loss: 0.00013061
Iteration 135/1000 | Loss: 0.00012357
Iteration 136/1000 | Loss: 0.00009085
Iteration 137/1000 | Loss: 0.00008891
Iteration 138/1000 | Loss: 0.00008808
Iteration 139/1000 | Loss: 0.00012581
Iteration 140/1000 | Loss: 0.00008740
Iteration 141/1000 | Loss: 0.00008657
Iteration 142/1000 | Loss: 0.00031199
Iteration 143/1000 | Loss: 0.00013595
Iteration 144/1000 | Loss: 0.00023076
Iteration 145/1000 | Loss: 0.00009187
Iteration 146/1000 | Loss: 0.00008752
Iteration 147/1000 | Loss: 0.00008541
Iteration 148/1000 | Loss: 0.00008317
Iteration 149/1000 | Loss: 0.00008195
Iteration 150/1000 | Loss: 0.00019887
Iteration 151/1000 | Loss: 0.00008827
Iteration 152/1000 | Loss: 0.00008387
Iteration 153/1000 | Loss: 0.00064307
Iteration 154/1000 | Loss: 0.00093796
Iteration 155/1000 | Loss: 0.00045620
Iteration 156/1000 | Loss: 0.00011106
Iteration 157/1000 | Loss: 0.00008186
Iteration 158/1000 | Loss: 0.00008000
Iteration 159/1000 | Loss: 0.00038172
Iteration 160/1000 | Loss: 0.00008672
Iteration 161/1000 | Loss: 0.00007938
Iteration 162/1000 | Loss: 0.00007828
Iteration 163/1000 | Loss: 0.00007780
Iteration 164/1000 | Loss: 0.00007729
Iteration 165/1000 | Loss: 0.00007680
Iteration 166/1000 | Loss: 0.00043212
Iteration 167/1000 | Loss: 0.00011364
Iteration 168/1000 | Loss: 0.00008644
Iteration 169/1000 | Loss: 0.00007618
Iteration 170/1000 | Loss: 0.00007587
Iteration 171/1000 | Loss: 0.00007568
Iteration 172/1000 | Loss: 0.00007541
Iteration 173/1000 | Loss: 0.00007534
Iteration 174/1000 | Loss: 0.00007530
Iteration 175/1000 | Loss: 0.00007523
Iteration 176/1000 | Loss: 0.00007502
Iteration 177/1000 | Loss: 0.00007491
Iteration 178/1000 | Loss: 0.00007488
Iteration 179/1000 | Loss: 0.00007478
Iteration 180/1000 | Loss: 0.00007471
Iteration 181/1000 | Loss: 0.00007470
Iteration 182/1000 | Loss: 0.00007457
Iteration 183/1000 | Loss: 0.00061016
Iteration 184/1000 | Loss: 0.00039782
Iteration 185/1000 | Loss: 0.00120190
Iteration 186/1000 | Loss: 0.00083286
Iteration 187/1000 | Loss: 0.00037298
Iteration 188/1000 | Loss: 0.00009439
Iteration 189/1000 | Loss: 0.00106131
Iteration 190/1000 | Loss: 0.00012352
Iteration 191/1000 | Loss: 0.00040924
Iteration 192/1000 | Loss: 0.00015142
Iteration 193/1000 | Loss: 0.00007654
Iteration 194/1000 | Loss: 0.00035312
Iteration 195/1000 | Loss: 0.00015055
Iteration 196/1000 | Loss: 0.00034581
Iteration 197/1000 | Loss: 0.00007521
Iteration 198/1000 | Loss: 0.00007015
Iteration 199/1000 | Loss: 0.00006888
Iteration 200/1000 | Loss: 0.00021379
Iteration 201/1000 | Loss: 0.00017716
Iteration 202/1000 | Loss: 0.00020657
Iteration 203/1000 | Loss: 0.00019731
Iteration 204/1000 | Loss: 0.00019531
Iteration 205/1000 | Loss: 0.00030454
Iteration 206/1000 | Loss: 0.00021654
Iteration 207/1000 | Loss: 0.00007583
Iteration 208/1000 | Loss: 0.00007015
Iteration 209/1000 | Loss: 0.00006800
Iteration 210/1000 | Loss: 0.00006700
Iteration 211/1000 | Loss: 0.00006654
Iteration 212/1000 | Loss: 0.00006615
Iteration 213/1000 | Loss: 0.00006589
Iteration 214/1000 | Loss: 0.00006564
Iteration 215/1000 | Loss: 0.00012111
Iteration 216/1000 | Loss: 0.00007384
Iteration 217/1000 | Loss: 0.00006841
Iteration 218/1000 | Loss: 0.00006572
Iteration 219/1000 | Loss: 0.00006523
Iteration 220/1000 | Loss: 0.00006521
Iteration 221/1000 | Loss: 0.00006517
Iteration 222/1000 | Loss: 0.00006516
Iteration 223/1000 | Loss: 0.00006514
Iteration 224/1000 | Loss: 0.00006514
Iteration 225/1000 | Loss: 0.00006513
Iteration 226/1000 | Loss: 0.00006513
Iteration 227/1000 | Loss: 0.00006506
Iteration 228/1000 | Loss: 0.00006503
Iteration 229/1000 | Loss: 0.00015663
Iteration 230/1000 | Loss: 0.00006519
Iteration 231/1000 | Loss: 0.00006481
Iteration 232/1000 | Loss: 0.00006481
Iteration 233/1000 | Loss: 0.00006480
Iteration 234/1000 | Loss: 0.00006480
Iteration 235/1000 | Loss: 0.00006480
Iteration 236/1000 | Loss: 0.00006479
Iteration 237/1000 | Loss: 0.00006479
Iteration 238/1000 | Loss: 0.00006479
Iteration 239/1000 | Loss: 0.00006479
Iteration 240/1000 | Loss: 0.00006479
Iteration 241/1000 | Loss: 0.00006479
Iteration 242/1000 | Loss: 0.00006479
Iteration 243/1000 | Loss: 0.00006479
Iteration 244/1000 | Loss: 0.00006479
Iteration 245/1000 | Loss: 0.00006479
Iteration 246/1000 | Loss: 0.00006479
Iteration 247/1000 | Loss: 0.00006478
Iteration 248/1000 | Loss: 0.00006478
Iteration 249/1000 | Loss: 0.00006478
Iteration 250/1000 | Loss: 0.00006478
Iteration 251/1000 | Loss: 0.00006478
Iteration 252/1000 | Loss: 0.00006477
Iteration 253/1000 | Loss: 0.00006477
Iteration 254/1000 | Loss: 0.00006477
Iteration 255/1000 | Loss: 0.00006477
Iteration 256/1000 | Loss: 0.00006477
Iteration 257/1000 | Loss: 0.00006477
Iteration 258/1000 | Loss: 0.00006477
Iteration 259/1000 | Loss: 0.00006477
Iteration 260/1000 | Loss: 0.00006477
Iteration 261/1000 | Loss: 0.00006476
Iteration 262/1000 | Loss: 0.00006476
Iteration 263/1000 | Loss: 0.00006476
Iteration 264/1000 | Loss: 0.00006476
Iteration 265/1000 | Loss: 0.00006476
Iteration 266/1000 | Loss: 0.00006476
Iteration 267/1000 | Loss: 0.00006476
Iteration 268/1000 | Loss: 0.00006476
Iteration 269/1000 | Loss: 0.00006476
Iteration 270/1000 | Loss: 0.00006476
Iteration 271/1000 | Loss: 0.00006476
Iteration 272/1000 | Loss: 0.00006476
Iteration 273/1000 | Loss: 0.00006476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [6.475999543908983e-05, 6.475999543908983e-05, 6.475999543908983e-05, 6.475999543908983e-05, 6.475999543908983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.475999543908983e-05

Optimization complete. Final v2v error: 4.379953384399414 mm

Highest mean error: 12.175751686096191 mm for frame 76

Lowest mean error: 2.790194034576416 mm for frame 154

Saving results

Total time: 411.0361773967743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440981
Iteration 2/25 | Loss: 0.00140484
Iteration 3/25 | Loss: 0.00126104
Iteration 4/25 | Loss: 0.00124158
Iteration 5/25 | Loss: 0.00123702
Iteration 6/25 | Loss: 0.00123643
Iteration 7/25 | Loss: 0.00123643
Iteration 8/25 | Loss: 0.00123643
Iteration 9/25 | Loss: 0.00123643
Iteration 10/25 | Loss: 0.00123643
Iteration 11/25 | Loss: 0.00123643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012364332797005773, 0.0012364332797005773, 0.0012364332797005773, 0.0012364332797005773, 0.0012364332797005773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012364332797005773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35955250
Iteration 2/25 | Loss: 0.00079275
Iteration 3/25 | Loss: 0.00079275
Iteration 4/25 | Loss: 0.00079274
Iteration 5/25 | Loss: 0.00079274
Iteration 6/25 | Loss: 0.00079274
Iteration 7/25 | Loss: 0.00079274
Iteration 8/25 | Loss: 0.00079274
Iteration 9/25 | Loss: 0.00079274
Iteration 10/25 | Loss: 0.00079274
Iteration 11/25 | Loss: 0.00079274
Iteration 12/25 | Loss: 0.00079274
Iteration 13/25 | Loss: 0.00079274
Iteration 14/25 | Loss: 0.00079274
Iteration 15/25 | Loss: 0.00079274
Iteration 16/25 | Loss: 0.00079274
Iteration 17/25 | Loss: 0.00079274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007927425904199481, 0.0007927425904199481, 0.0007927425904199481, 0.0007927425904199481, 0.0007927425904199481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007927425904199481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079274
Iteration 2/1000 | Loss: 0.00003264
Iteration 3/1000 | Loss: 0.00002251
Iteration 4/1000 | Loss: 0.00002052
Iteration 5/1000 | Loss: 0.00001966
Iteration 6/1000 | Loss: 0.00001888
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001774
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001720
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001699
Iteration 16/1000 | Loss: 0.00001697
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001681
Iteration 23/1000 | Loss: 0.00001680
Iteration 24/1000 | Loss: 0.00001679
Iteration 25/1000 | Loss: 0.00001679
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001678
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001672
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001670
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001668
Iteration 42/1000 | Loss: 0.00001668
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001666
Iteration 47/1000 | Loss: 0.00001666
Iteration 48/1000 | Loss: 0.00001665
Iteration 49/1000 | Loss: 0.00001665
Iteration 50/1000 | Loss: 0.00001665
Iteration 51/1000 | Loss: 0.00001664
Iteration 52/1000 | Loss: 0.00001664
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001664
Iteration 56/1000 | Loss: 0.00001664
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001662
Iteration 68/1000 | Loss: 0.00001662
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001661
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001661
Iteration 78/1000 | Loss: 0.00001661
Iteration 79/1000 | Loss: 0.00001661
Iteration 80/1000 | Loss: 0.00001661
Iteration 81/1000 | Loss: 0.00001661
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001659
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001658
Iteration 93/1000 | Loss: 0.00001658
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001657
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001657
Iteration 110/1000 | Loss: 0.00001657
Iteration 111/1000 | Loss: 0.00001657
Iteration 112/1000 | Loss: 0.00001657
Iteration 113/1000 | Loss: 0.00001657
Iteration 114/1000 | Loss: 0.00001657
Iteration 115/1000 | Loss: 0.00001657
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.656828317209147e-05, 1.656828317209147e-05, 1.656828317209147e-05, 1.656828317209147e-05, 1.656828317209147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.656828317209147e-05

Optimization complete. Final v2v error: 3.423577308654785 mm

Highest mean error: 4.506024360656738 mm for frame 229

Lowest mean error: 2.8905415534973145 mm for frame 91

Saving results

Total time: 40.32818078994751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991196
Iteration 2/25 | Loss: 0.00411177
Iteration 3/25 | Loss: 0.00297872
Iteration 4/25 | Loss: 0.00217841
Iteration 5/25 | Loss: 0.00200240
Iteration 6/25 | Loss: 0.00166071
Iteration 7/25 | Loss: 0.00140775
Iteration 8/25 | Loss: 0.00131739
Iteration 9/25 | Loss: 0.00129340
Iteration 10/25 | Loss: 0.00122924
Iteration 11/25 | Loss: 0.00120818
Iteration 12/25 | Loss: 0.00120229
Iteration 13/25 | Loss: 0.00119541
Iteration 14/25 | Loss: 0.00119367
Iteration 15/25 | Loss: 0.00119313
Iteration 16/25 | Loss: 0.00119296
Iteration 17/25 | Loss: 0.00119285
Iteration 18/25 | Loss: 0.00119283
Iteration 19/25 | Loss: 0.00119283
Iteration 20/25 | Loss: 0.00119282
Iteration 21/25 | Loss: 0.00119282
Iteration 22/25 | Loss: 0.00119282
Iteration 23/25 | Loss: 0.00119282
Iteration 24/25 | Loss: 0.00119282
Iteration 25/25 | Loss: 0.00119282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35735261
Iteration 2/25 | Loss: 0.00090870
Iteration 3/25 | Loss: 0.00090870
Iteration 4/25 | Loss: 0.00090870
Iteration 5/25 | Loss: 0.00090870
Iteration 6/25 | Loss: 0.00090870
Iteration 7/25 | Loss: 0.00090870
Iteration 8/25 | Loss: 0.00090870
Iteration 9/25 | Loss: 0.00090870
Iteration 10/25 | Loss: 0.00090870
Iteration 11/25 | Loss: 0.00090870
Iteration 12/25 | Loss: 0.00090870
Iteration 13/25 | Loss: 0.00090870
Iteration 14/25 | Loss: 0.00090870
Iteration 15/25 | Loss: 0.00090870
Iteration 16/25 | Loss: 0.00090870
Iteration 17/25 | Loss: 0.00090869
Iteration 18/25 | Loss: 0.00090869
Iteration 19/25 | Loss: 0.00090869
Iteration 20/25 | Loss: 0.00090869
Iteration 21/25 | Loss: 0.00090869
Iteration 22/25 | Loss: 0.00090869
Iteration 23/25 | Loss: 0.00090869
Iteration 24/25 | Loss: 0.00090869
Iteration 25/25 | Loss: 0.00090869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090869
Iteration 2/1000 | Loss: 0.00003713
Iteration 3/1000 | Loss: 0.00002961
Iteration 4/1000 | Loss: 0.00002729
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002484
Iteration 7/1000 | Loss: 0.00002402
Iteration 8/1000 | Loss: 0.00002366
Iteration 9/1000 | Loss: 0.00002326
Iteration 10/1000 | Loss: 0.00069117
Iteration 11/1000 | Loss: 0.00228830
Iteration 12/1000 | Loss: 0.00246629
Iteration 13/1000 | Loss: 0.00006373
Iteration 14/1000 | Loss: 0.00003183
Iteration 15/1000 | Loss: 0.00002503
Iteration 16/1000 | Loss: 0.00002351
Iteration 17/1000 | Loss: 0.00002292
Iteration 18/1000 | Loss: 0.00164702
Iteration 19/1000 | Loss: 0.00111876
Iteration 20/1000 | Loss: 0.00124406
Iteration 21/1000 | Loss: 0.00099305
Iteration 22/1000 | Loss: 0.00107664
Iteration 23/1000 | Loss: 0.00101530
Iteration 24/1000 | Loss: 0.00102196
Iteration 25/1000 | Loss: 0.00031720
Iteration 26/1000 | Loss: 0.00005250
Iteration 27/1000 | Loss: 0.00003186
Iteration 28/1000 | Loss: 0.00039248
Iteration 29/1000 | Loss: 0.00010845
Iteration 30/1000 | Loss: 0.00056822
Iteration 31/1000 | Loss: 0.00053008
Iteration 32/1000 | Loss: 0.00086947
Iteration 33/1000 | Loss: 0.00025193
Iteration 34/1000 | Loss: 0.00003287
Iteration 35/1000 | Loss: 0.00061496
Iteration 36/1000 | Loss: 0.00028546
Iteration 37/1000 | Loss: 0.00002566
Iteration 38/1000 | Loss: 0.00047588
Iteration 39/1000 | Loss: 0.00016748
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00042936
Iteration 42/1000 | Loss: 0.00030393
Iteration 43/1000 | Loss: 0.00013214
Iteration 44/1000 | Loss: 0.00002621
Iteration 45/1000 | Loss: 0.00042862
Iteration 46/1000 | Loss: 0.00005181
Iteration 47/1000 | Loss: 0.00044881
Iteration 48/1000 | Loss: 0.00006137
Iteration 49/1000 | Loss: 0.00022464
Iteration 50/1000 | Loss: 0.00029474
Iteration 51/1000 | Loss: 0.00027788
Iteration 52/1000 | Loss: 0.00036515
Iteration 53/1000 | Loss: 0.00025143
Iteration 54/1000 | Loss: 0.00013918
Iteration 55/1000 | Loss: 0.00002146
Iteration 56/1000 | Loss: 0.00014841
Iteration 57/1000 | Loss: 0.00047800
Iteration 58/1000 | Loss: 0.00009116
Iteration 59/1000 | Loss: 0.00079692
Iteration 60/1000 | Loss: 0.00002485
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001604
Iteration 63/1000 | Loss: 0.00012431
Iteration 64/1000 | Loss: 0.00002304
Iteration 65/1000 | Loss: 0.00061621
Iteration 66/1000 | Loss: 0.00002541
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001092
Iteration 77/1000 | Loss: 0.00001090
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001087
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001087
Iteration 82/1000 | Loss: 0.00001087
Iteration 83/1000 | Loss: 0.00001087
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001083
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001080
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001079
Iteration 93/1000 | Loss: 0.00001079
Iteration 94/1000 | Loss: 0.00001079
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001078
Iteration 97/1000 | Loss: 0.00001078
Iteration 98/1000 | Loss: 0.00001078
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001077
Iteration 102/1000 | Loss: 0.00001077
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001075
Iteration 113/1000 | Loss: 0.00001075
Iteration 114/1000 | Loss: 0.00001075
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001075
Iteration 139/1000 | Loss: 0.00001075
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001075
Iteration 142/1000 | Loss: 0.00001075
Iteration 143/1000 | Loss: 0.00001075
Iteration 144/1000 | Loss: 0.00001075
Iteration 145/1000 | Loss: 0.00001075
Iteration 146/1000 | Loss: 0.00001075
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001075
Iteration 149/1000 | Loss: 0.00001075
Iteration 150/1000 | Loss: 0.00001075
Iteration 151/1000 | Loss: 0.00001075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.0751155969046522e-05, 1.0751155969046522e-05, 1.0751155969046522e-05, 1.0751155969046522e-05, 1.0751155969046522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0751155969046522e-05

Optimization complete. Final v2v error: 2.8318679332733154 mm

Highest mean error: 3.763033390045166 mm for frame 139

Lowest mean error: 2.6709423065185547 mm for frame 97

Saving results

Total time: 142.64421582221985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833061
Iteration 2/25 | Loss: 0.00131617
Iteration 3/25 | Loss: 0.00121686
Iteration 4/25 | Loss: 0.00119011
Iteration 5/25 | Loss: 0.00118096
Iteration 6/25 | Loss: 0.00117906
Iteration 7/25 | Loss: 0.00117906
Iteration 8/25 | Loss: 0.00117906
Iteration 9/25 | Loss: 0.00117906
Iteration 10/25 | Loss: 0.00117906
Iteration 11/25 | Loss: 0.00117906
Iteration 12/25 | Loss: 0.00117906
Iteration 13/25 | Loss: 0.00117906
Iteration 14/25 | Loss: 0.00117906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011790557764470577, 0.0011790557764470577, 0.0011790557764470577, 0.0011790557764470577, 0.0011790557764470577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011790557764470577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29069281
Iteration 2/25 | Loss: 0.00123635
Iteration 3/25 | Loss: 0.00123635
Iteration 4/25 | Loss: 0.00123635
Iteration 5/25 | Loss: 0.00123635
Iteration 6/25 | Loss: 0.00123635
Iteration 7/25 | Loss: 0.00123635
Iteration 8/25 | Loss: 0.00123635
Iteration 9/25 | Loss: 0.00123635
Iteration 10/25 | Loss: 0.00123635
Iteration 11/25 | Loss: 0.00123635
Iteration 12/25 | Loss: 0.00123635
Iteration 13/25 | Loss: 0.00123635
Iteration 14/25 | Loss: 0.00123635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012363491114228964, 0.0012363491114228964, 0.0012363491114228964, 0.0012363491114228964, 0.0012363491114228964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012363491114228964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123635
Iteration 2/1000 | Loss: 0.00004333
Iteration 3/1000 | Loss: 0.00002978
Iteration 4/1000 | Loss: 0.00002574
Iteration 5/1000 | Loss: 0.00002427
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002178
Iteration 8/1000 | Loss: 0.00002110
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002007
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001942
Iteration 13/1000 | Loss: 0.00001922
Iteration 14/1000 | Loss: 0.00001917
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001905
Iteration 17/1000 | Loss: 0.00001890
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001862
Iteration 32/1000 | Loss: 0.00001862
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001862
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001860
Iteration 40/1000 | Loss: 0.00001860
Iteration 41/1000 | Loss: 0.00001860
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001857
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001853
Iteration 69/1000 | Loss: 0.00001853
Iteration 70/1000 | Loss: 0.00001852
Iteration 71/1000 | Loss: 0.00001852
Iteration 72/1000 | Loss: 0.00001852
Iteration 73/1000 | Loss: 0.00001852
Iteration 74/1000 | Loss: 0.00001851
Iteration 75/1000 | Loss: 0.00001851
Iteration 76/1000 | Loss: 0.00001851
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001851
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001849
Iteration 89/1000 | Loss: 0.00001849
Iteration 90/1000 | Loss: 0.00001849
Iteration 91/1000 | Loss: 0.00001849
Iteration 92/1000 | Loss: 0.00001849
Iteration 93/1000 | Loss: 0.00001849
Iteration 94/1000 | Loss: 0.00001849
Iteration 95/1000 | Loss: 0.00001848
Iteration 96/1000 | Loss: 0.00001848
Iteration 97/1000 | Loss: 0.00001848
Iteration 98/1000 | Loss: 0.00001848
Iteration 99/1000 | Loss: 0.00001847
Iteration 100/1000 | Loss: 0.00001847
Iteration 101/1000 | Loss: 0.00001847
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001846
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001845
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001845
Iteration 142/1000 | Loss: 0.00001845
Iteration 143/1000 | Loss: 0.00001845
Iteration 144/1000 | Loss: 0.00001845
Iteration 145/1000 | Loss: 0.00001845
Iteration 146/1000 | Loss: 0.00001845
Iteration 147/1000 | Loss: 0.00001845
Iteration 148/1000 | Loss: 0.00001845
Iteration 149/1000 | Loss: 0.00001845
Iteration 150/1000 | Loss: 0.00001845
Iteration 151/1000 | Loss: 0.00001845
Iteration 152/1000 | Loss: 0.00001845
Iteration 153/1000 | Loss: 0.00001845
Iteration 154/1000 | Loss: 0.00001845
Iteration 155/1000 | Loss: 0.00001845
Iteration 156/1000 | Loss: 0.00001845
Iteration 157/1000 | Loss: 0.00001845
Iteration 158/1000 | Loss: 0.00001845
Iteration 159/1000 | Loss: 0.00001845
Iteration 160/1000 | Loss: 0.00001845
Iteration 161/1000 | Loss: 0.00001845
Iteration 162/1000 | Loss: 0.00001845
Iteration 163/1000 | Loss: 0.00001845
Iteration 164/1000 | Loss: 0.00001845
Iteration 165/1000 | Loss: 0.00001845
Iteration 166/1000 | Loss: 0.00001845
Iteration 167/1000 | Loss: 0.00001845
Iteration 168/1000 | Loss: 0.00001845
Iteration 169/1000 | Loss: 0.00001845
Iteration 170/1000 | Loss: 0.00001845
Iteration 171/1000 | Loss: 0.00001845
Iteration 172/1000 | Loss: 0.00001845
Iteration 173/1000 | Loss: 0.00001845
Iteration 174/1000 | Loss: 0.00001845
Iteration 175/1000 | Loss: 0.00001845
Iteration 176/1000 | Loss: 0.00001845
Iteration 177/1000 | Loss: 0.00001845
Iteration 178/1000 | Loss: 0.00001845
Iteration 179/1000 | Loss: 0.00001845
Iteration 180/1000 | Loss: 0.00001845
Iteration 181/1000 | Loss: 0.00001845
Iteration 182/1000 | Loss: 0.00001845
Iteration 183/1000 | Loss: 0.00001845
Iteration 184/1000 | Loss: 0.00001845
Iteration 185/1000 | Loss: 0.00001845
Iteration 186/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.845026417868212e-05, 1.845026417868212e-05, 1.845026417868212e-05, 1.845026417868212e-05, 1.845026417868212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.845026417868212e-05

Optimization complete. Final v2v error: 3.601990222930908 mm

Highest mean error: 4.446897983551025 mm for frame 125

Lowest mean error: 2.9938950538635254 mm for frame 56

Saving results

Total time: 48.34027981758118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898209
Iteration 2/25 | Loss: 0.00170720
Iteration 3/25 | Loss: 0.00140478
Iteration 4/25 | Loss: 0.00134766
Iteration 5/25 | Loss: 0.00132970
Iteration 6/25 | Loss: 0.00129661
Iteration 7/25 | Loss: 0.00129803
Iteration 8/25 | Loss: 0.00129310
Iteration 9/25 | Loss: 0.00129198
Iteration 10/25 | Loss: 0.00129079
Iteration 11/25 | Loss: 0.00129042
Iteration 12/25 | Loss: 0.00129024
Iteration 13/25 | Loss: 0.00129016
Iteration 14/25 | Loss: 0.00129016
Iteration 15/25 | Loss: 0.00129016
Iteration 16/25 | Loss: 0.00129015
Iteration 17/25 | Loss: 0.00129015
Iteration 18/25 | Loss: 0.00129015
Iteration 19/25 | Loss: 0.00129014
Iteration 20/25 | Loss: 0.00129013
Iteration 21/25 | Loss: 0.00129013
Iteration 22/25 | Loss: 0.00129013
Iteration 23/25 | Loss: 0.00129013
Iteration 24/25 | Loss: 0.00129013
Iteration 25/25 | Loss: 0.00129012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12736464
Iteration 2/25 | Loss: 0.00100263
Iteration 3/25 | Loss: 0.00100263
Iteration 4/25 | Loss: 0.00100263
Iteration 5/25 | Loss: 0.00100263
Iteration 6/25 | Loss: 0.00100263
Iteration 7/25 | Loss: 0.00100263
Iteration 8/25 | Loss: 0.00100263
Iteration 9/25 | Loss: 0.00100263
Iteration 10/25 | Loss: 0.00100263
Iteration 11/25 | Loss: 0.00100263
Iteration 12/25 | Loss: 0.00100263
Iteration 13/25 | Loss: 0.00100263
Iteration 14/25 | Loss: 0.00100263
Iteration 15/25 | Loss: 0.00100263
Iteration 16/25 | Loss: 0.00100263
Iteration 17/25 | Loss: 0.00100263
Iteration 18/25 | Loss: 0.00100263
Iteration 19/25 | Loss: 0.00100263
Iteration 20/25 | Loss: 0.00100263
Iteration 21/25 | Loss: 0.00100263
Iteration 22/25 | Loss: 0.00100263
Iteration 23/25 | Loss: 0.00100263
Iteration 24/25 | Loss: 0.00100263
Iteration 25/25 | Loss: 0.00100263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001002626377157867, 0.001002626377157867, 0.001002626377157867, 0.001002626377157867, 0.001002626377157867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001002626377157867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100263
Iteration 2/1000 | Loss: 0.00004621
Iteration 3/1000 | Loss: 0.00003152
Iteration 4/1000 | Loss: 0.00002808
Iteration 5/1000 | Loss: 0.00023680
Iteration 6/1000 | Loss: 0.00002572
Iteration 7/1000 | Loss: 0.00002481
Iteration 8/1000 | Loss: 0.00026325
Iteration 9/1000 | Loss: 0.00003272
Iteration 10/1000 | Loss: 0.00002697
Iteration 11/1000 | Loss: 0.00002495
Iteration 12/1000 | Loss: 0.00019991
Iteration 13/1000 | Loss: 0.00002305
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002161
Iteration 17/1000 | Loss: 0.00002141
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002106
Iteration 20/1000 | Loss: 0.00002093
Iteration 21/1000 | Loss: 0.00002089
Iteration 22/1000 | Loss: 0.00002087
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002079
Iteration 25/1000 | Loss: 0.00002078
Iteration 26/1000 | Loss: 0.00002074
Iteration 27/1000 | Loss: 0.00002073
Iteration 28/1000 | Loss: 0.00002068
Iteration 29/1000 | Loss: 0.00002067
Iteration 30/1000 | Loss: 0.00002067
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00002066
Iteration 33/1000 | Loss: 0.00002066
Iteration 34/1000 | Loss: 0.00002065
Iteration 35/1000 | Loss: 0.00002065
Iteration 36/1000 | Loss: 0.00002065
Iteration 37/1000 | Loss: 0.00002065
Iteration 38/1000 | Loss: 0.00002064
Iteration 39/1000 | Loss: 0.00002064
Iteration 40/1000 | Loss: 0.00002064
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002064
Iteration 43/1000 | Loss: 0.00002064
Iteration 44/1000 | Loss: 0.00002063
Iteration 45/1000 | Loss: 0.00002063
Iteration 46/1000 | Loss: 0.00002063
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002062
Iteration 49/1000 | Loss: 0.00002062
Iteration 50/1000 | Loss: 0.00002062
Iteration 51/1000 | Loss: 0.00002062
Iteration 52/1000 | Loss: 0.00002061
Iteration 53/1000 | Loss: 0.00002061
Iteration 54/1000 | Loss: 0.00002061
Iteration 55/1000 | Loss: 0.00002061
Iteration 56/1000 | Loss: 0.00002061
Iteration 57/1000 | Loss: 0.00002061
Iteration 58/1000 | Loss: 0.00002060
Iteration 59/1000 | Loss: 0.00002060
Iteration 60/1000 | Loss: 0.00002060
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [2.06041459023254e-05, 2.06041459023254e-05, 2.06041459023254e-05, 2.06041459023254e-05, 2.06041459023254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.06041459023254e-05

Optimization complete. Final v2v error: 3.793574571609497 mm

Highest mean error: 4.828067779541016 mm for frame 68

Lowest mean error: 3.3364675045013428 mm for frame 138

Saving results

Total time: 59.273276805877686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828758
Iteration 2/25 | Loss: 0.00232839
Iteration 3/25 | Loss: 0.00190034
Iteration 4/25 | Loss: 0.00176325
Iteration 5/25 | Loss: 0.00157674
Iteration 6/25 | Loss: 0.00142841
Iteration 7/25 | Loss: 0.00140861
Iteration 8/25 | Loss: 0.00139907
Iteration 9/25 | Loss: 0.00139398
Iteration 10/25 | Loss: 0.00139136
Iteration 11/25 | Loss: 0.00138953
Iteration 12/25 | Loss: 0.00138841
Iteration 13/25 | Loss: 0.00138778
Iteration 14/25 | Loss: 0.00138677
Iteration 15/25 | Loss: 0.00138913
Iteration 16/25 | Loss: 0.00138904
Iteration 17/25 | Loss: 0.00138771
Iteration 18/25 | Loss: 0.00138879
Iteration 19/25 | Loss: 0.00138733
Iteration 20/25 | Loss: 0.00138603
Iteration 21/25 | Loss: 0.00138429
Iteration 22/25 | Loss: 0.00138545
Iteration 23/25 | Loss: 0.00138522
Iteration 24/25 | Loss: 0.00138551
Iteration 25/25 | Loss: 0.00138618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35089338
Iteration 2/25 | Loss: 0.00076976
Iteration 3/25 | Loss: 0.00076974
Iteration 4/25 | Loss: 0.00076974
Iteration 5/25 | Loss: 0.00076974
Iteration 6/25 | Loss: 0.00076973
Iteration 7/25 | Loss: 0.00076973
Iteration 8/25 | Loss: 0.00076973
Iteration 9/25 | Loss: 0.00076973
Iteration 10/25 | Loss: 0.00076973
Iteration 11/25 | Loss: 0.00076973
Iteration 12/25 | Loss: 0.00076973
Iteration 13/25 | Loss: 0.00076973
Iteration 14/25 | Loss: 0.00076973
Iteration 15/25 | Loss: 0.00076973
Iteration 16/25 | Loss: 0.00076973
Iteration 17/25 | Loss: 0.00076973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007697336259298027, 0.0007697336259298027, 0.0007697336259298027, 0.0007697336259298027, 0.0007697336259298027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007697336259298027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076973
Iteration 2/1000 | Loss: 0.00004584
Iteration 3/1000 | Loss: 0.00003572
Iteration 4/1000 | Loss: 0.00002934
Iteration 5/1000 | Loss: 0.00003964
Iteration 6/1000 | Loss: 0.00005442
Iteration 7/1000 | Loss: 0.00003399
Iteration 8/1000 | Loss: 0.00003663
Iteration 9/1000 | Loss: 0.00005544
Iteration 10/1000 | Loss: 0.00004832
Iteration 11/1000 | Loss: 0.00004862
Iteration 12/1000 | Loss: 0.00003398
Iteration 13/1000 | Loss: 0.00004158
Iteration 14/1000 | Loss: 0.00002517
Iteration 15/1000 | Loss: 0.00003094
Iteration 16/1000 | Loss: 0.00002323
Iteration 17/1000 | Loss: 0.00003732
Iteration 18/1000 | Loss: 0.00003472
Iteration 19/1000 | Loss: 0.00004223
Iteration 20/1000 | Loss: 0.00003722
Iteration 21/1000 | Loss: 0.00003247
Iteration 22/1000 | Loss: 0.00003640
Iteration 23/1000 | Loss: 0.00003956
Iteration 24/1000 | Loss: 0.00003827
Iteration 25/1000 | Loss: 0.00004151
Iteration 26/1000 | Loss: 0.00003700
Iteration 27/1000 | Loss: 0.00004466
Iteration 28/1000 | Loss: 0.00003035
Iteration 29/1000 | Loss: 0.00002914
Iteration 30/1000 | Loss: 0.00004001
Iteration 31/1000 | Loss: 0.00004272
Iteration 32/1000 | Loss: 0.00003348
Iteration 33/1000 | Loss: 0.00004685
Iteration 34/1000 | Loss: 0.00002669
Iteration 35/1000 | Loss: 0.00003525
Iteration 36/1000 | Loss: 0.00003558
Iteration 37/1000 | Loss: 0.00004055
Iteration 38/1000 | Loss: 0.00004068
Iteration 39/1000 | Loss: 0.00004516
Iteration 40/1000 | Loss: 0.00003263
Iteration 41/1000 | Loss: 0.00004447
Iteration 42/1000 | Loss: 0.00003370
Iteration 43/1000 | Loss: 0.00002761
Iteration 44/1000 | Loss: 0.00002406
Iteration 45/1000 | Loss: 0.00002270
Iteration 46/1000 | Loss: 0.00002216
Iteration 47/1000 | Loss: 0.00002770
Iteration 48/1000 | Loss: 0.00003920
Iteration 49/1000 | Loss: 0.00004862
Iteration 50/1000 | Loss: 0.00002823
Iteration 51/1000 | Loss: 0.00002687
Iteration 52/1000 | Loss: 0.00004065
Iteration 53/1000 | Loss: 0.00002671
Iteration 54/1000 | Loss: 0.00004365
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002186
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002145
Iteration 60/1000 | Loss: 0.00002144
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002142
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002141
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002140
Iteration 69/1000 | Loss: 0.00002140
Iteration 70/1000 | Loss: 0.00002139
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00002139
Iteration 73/1000 | Loss: 0.00002139
Iteration 74/1000 | Loss: 0.00002139
Iteration 75/1000 | Loss: 0.00002139
Iteration 76/1000 | Loss: 0.00002137
Iteration 77/1000 | Loss: 0.00002137
Iteration 78/1000 | Loss: 0.00002137
Iteration 79/1000 | Loss: 0.00002137
Iteration 80/1000 | Loss: 0.00002137
Iteration 81/1000 | Loss: 0.00002137
Iteration 82/1000 | Loss: 0.00002137
Iteration 83/1000 | Loss: 0.00002136
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002133
Iteration 87/1000 | Loss: 0.00002133
Iteration 88/1000 | Loss: 0.00002133
Iteration 89/1000 | Loss: 0.00002133
Iteration 90/1000 | Loss: 0.00002133
Iteration 91/1000 | Loss: 0.00002131
Iteration 92/1000 | Loss: 0.00002127
Iteration 93/1000 | Loss: 0.00002127
Iteration 94/1000 | Loss: 0.00002127
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002123
Iteration 97/1000 | Loss: 0.00002123
Iteration 98/1000 | Loss: 0.00002122
Iteration 99/1000 | Loss: 0.00002121
Iteration 100/1000 | Loss: 0.00002121
Iteration 101/1000 | Loss: 0.00002121
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002117
Iteration 104/1000 | Loss: 0.00002117
Iteration 105/1000 | Loss: 0.00002116
Iteration 106/1000 | Loss: 0.00002115
Iteration 107/1000 | Loss: 0.00002115
Iteration 108/1000 | Loss: 0.00002114
Iteration 109/1000 | Loss: 0.00002114
Iteration 110/1000 | Loss: 0.00002109
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002094
Iteration 113/1000 | Loss: 0.00002094
Iteration 114/1000 | Loss: 0.00002091
Iteration 115/1000 | Loss: 0.00002091
Iteration 116/1000 | Loss: 0.00002090
Iteration 117/1000 | Loss: 0.00002089
Iteration 118/1000 | Loss: 0.00002089
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002088
Iteration 122/1000 | Loss: 0.00002088
Iteration 123/1000 | Loss: 0.00002088
Iteration 124/1000 | Loss: 0.00002088
Iteration 125/1000 | Loss: 0.00002087
Iteration 126/1000 | Loss: 0.00002087
Iteration 127/1000 | Loss: 0.00002087
Iteration 128/1000 | Loss: 0.00002086
Iteration 129/1000 | Loss: 0.00002086
Iteration 130/1000 | Loss: 0.00002085
Iteration 131/1000 | Loss: 0.00002085
Iteration 132/1000 | Loss: 0.00002085
Iteration 133/1000 | Loss: 0.00002085
Iteration 134/1000 | Loss: 0.00002084
Iteration 135/1000 | Loss: 0.00002084
Iteration 136/1000 | Loss: 0.00002084
Iteration 137/1000 | Loss: 0.00002084
Iteration 138/1000 | Loss: 0.00002083
Iteration 139/1000 | Loss: 0.00002083
Iteration 140/1000 | Loss: 0.00002083
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002082
Iteration 147/1000 | Loss: 0.00002082
Iteration 148/1000 | Loss: 0.00002082
Iteration 149/1000 | Loss: 0.00002082
Iteration 150/1000 | Loss: 0.00002082
Iteration 151/1000 | Loss: 0.00002082
Iteration 152/1000 | Loss: 0.00002082
Iteration 153/1000 | Loss: 0.00002082
Iteration 154/1000 | Loss: 0.00002082
Iteration 155/1000 | Loss: 0.00002082
Iteration 156/1000 | Loss: 0.00002082
Iteration 157/1000 | Loss: 0.00002081
Iteration 158/1000 | Loss: 0.00002081
Iteration 159/1000 | Loss: 0.00002081
Iteration 160/1000 | Loss: 0.00002081
Iteration 161/1000 | Loss: 0.00002081
Iteration 162/1000 | Loss: 0.00002081
Iteration 163/1000 | Loss: 0.00002081
Iteration 164/1000 | Loss: 0.00002081
Iteration 165/1000 | Loss: 0.00002080
Iteration 166/1000 | Loss: 0.00002080
Iteration 167/1000 | Loss: 0.00002080
Iteration 168/1000 | Loss: 0.00002080
Iteration 169/1000 | Loss: 0.00002080
Iteration 170/1000 | Loss: 0.00002080
Iteration 171/1000 | Loss: 0.00002080
Iteration 172/1000 | Loss: 0.00002080
Iteration 173/1000 | Loss: 0.00002080
Iteration 174/1000 | Loss: 0.00002080
Iteration 175/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.0801639038836583e-05, 2.0801639038836583e-05, 2.0801639038836583e-05, 2.0801639038836583e-05, 2.0801639038836583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0801639038836583e-05

Optimization complete. Final v2v error: 3.84486985206604 mm

Highest mean error: 4.970513343811035 mm for frame 162

Lowest mean error: 3.6144745349884033 mm for frame 44

Saving results

Total time: 160.1674883365631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917860
Iteration 2/25 | Loss: 0.00199076
Iteration 3/25 | Loss: 0.00145643
Iteration 4/25 | Loss: 0.00144131
Iteration 5/25 | Loss: 0.00143629
Iteration 6/25 | Loss: 0.00143525
Iteration 7/25 | Loss: 0.00143525
Iteration 8/25 | Loss: 0.00143525
Iteration 9/25 | Loss: 0.00143525
Iteration 10/25 | Loss: 0.00143525
Iteration 11/25 | Loss: 0.00143525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014352549333125353, 0.0014352549333125353, 0.0014352549333125353, 0.0014352549333125353, 0.0014352549333125353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014352549333125353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52830970
Iteration 2/25 | Loss: 0.00097482
Iteration 3/25 | Loss: 0.00097482
Iteration 4/25 | Loss: 0.00097482
Iteration 5/25 | Loss: 0.00097482
Iteration 6/25 | Loss: 0.00097482
Iteration 7/25 | Loss: 0.00097482
Iteration 8/25 | Loss: 0.00097482
Iteration 9/25 | Loss: 0.00097482
Iteration 10/25 | Loss: 0.00097482
Iteration 11/25 | Loss: 0.00097482
Iteration 12/25 | Loss: 0.00097482
Iteration 13/25 | Loss: 0.00097482
Iteration 14/25 | Loss: 0.00097482
Iteration 15/25 | Loss: 0.00097482
Iteration 16/25 | Loss: 0.00097482
Iteration 17/25 | Loss: 0.00097482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009748167358338833, 0.0009748167358338833, 0.0009748167358338833, 0.0009748167358338833, 0.0009748167358338833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009748167358338833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097482
Iteration 2/1000 | Loss: 0.00007813
Iteration 3/1000 | Loss: 0.00005829
Iteration 4/1000 | Loss: 0.00005157
Iteration 5/1000 | Loss: 0.00004888
Iteration 6/1000 | Loss: 0.00004732
Iteration 7/1000 | Loss: 0.00004621
Iteration 8/1000 | Loss: 0.00004499
Iteration 9/1000 | Loss: 0.00004389
Iteration 10/1000 | Loss: 0.00004291
Iteration 11/1000 | Loss: 0.00004198
Iteration 12/1000 | Loss: 0.00004140
Iteration 13/1000 | Loss: 0.00004075
Iteration 14/1000 | Loss: 0.00004013
Iteration 15/1000 | Loss: 0.00003968
Iteration 16/1000 | Loss: 0.00003929
Iteration 17/1000 | Loss: 0.00003877
Iteration 18/1000 | Loss: 0.00003846
Iteration 19/1000 | Loss: 0.00003822
Iteration 20/1000 | Loss: 0.00003795
Iteration 21/1000 | Loss: 0.00003773
Iteration 22/1000 | Loss: 0.00003752
Iteration 23/1000 | Loss: 0.00003736
Iteration 24/1000 | Loss: 0.00003722
Iteration 25/1000 | Loss: 0.00003716
Iteration 26/1000 | Loss: 0.00003715
Iteration 27/1000 | Loss: 0.00003711
Iteration 28/1000 | Loss: 0.00003711
Iteration 29/1000 | Loss: 0.00003711
Iteration 30/1000 | Loss: 0.00003709
Iteration 31/1000 | Loss: 0.00003707
Iteration 32/1000 | Loss: 0.00003705
Iteration 33/1000 | Loss: 0.00003699
Iteration 34/1000 | Loss: 0.00003699
Iteration 35/1000 | Loss: 0.00003699
Iteration 36/1000 | Loss: 0.00003697
Iteration 37/1000 | Loss: 0.00003697
Iteration 38/1000 | Loss: 0.00003697
Iteration 39/1000 | Loss: 0.00003696
Iteration 40/1000 | Loss: 0.00003696
Iteration 41/1000 | Loss: 0.00003696
Iteration 42/1000 | Loss: 0.00003694
Iteration 43/1000 | Loss: 0.00003694
Iteration 44/1000 | Loss: 0.00003694
Iteration 45/1000 | Loss: 0.00003694
Iteration 46/1000 | Loss: 0.00003694
Iteration 47/1000 | Loss: 0.00003686
Iteration 48/1000 | Loss: 0.00003686
Iteration 49/1000 | Loss: 0.00003685
Iteration 50/1000 | Loss: 0.00003685
Iteration 51/1000 | Loss: 0.00003685
Iteration 52/1000 | Loss: 0.00003685
Iteration 53/1000 | Loss: 0.00003685
Iteration 54/1000 | Loss: 0.00003685
Iteration 55/1000 | Loss: 0.00003685
Iteration 56/1000 | Loss: 0.00003685
Iteration 57/1000 | Loss: 0.00003685
Iteration 58/1000 | Loss: 0.00003684
Iteration 59/1000 | Loss: 0.00003683
Iteration 60/1000 | Loss: 0.00003683
Iteration 61/1000 | Loss: 0.00003683
Iteration 62/1000 | Loss: 0.00003683
Iteration 63/1000 | Loss: 0.00003683
Iteration 64/1000 | Loss: 0.00003683
Iteration 65/1000 | Loss: 0.00003682
Iteration 66/1000 | Loss: 0.00003682
Iteration 67/1000 | Loss: 0.00003682
Iteration 68/1000 | Loss: 0.00003682
Iteration 69/1000 | Loss: 0.00003682
Iteration 70/1000 | Loss: 0.00003681
Iteration 71/1000 | Loss: 0.00003679
Iteration 72/1000 | Loss: 0.00003679
Iteration 73/1000 | Loss: 0.00003677
Iteration 74/1000 | Loss: 0.00003677
Iteration 75/1000 | Loss: 0.00003677
Iteration 76/1000 | Loss: 0.00003677
Iteration 77/1000 | Loss: 0.00003677
Iteration 78/1000 | Loss: 0.00003677
Iteration 79/1000 | Loss: 0.00003676
Iteration 80/1000 | Loss: 0.00003676
Iteration 81/1000 | Loss: 0.00003676
Iteration 82/1000 | Loss: 0.00003676
Iteration 83/1000 | Loss: 0.00003676
Iteration 84/1000 | Loss: 0.00003676
Iteration 85/1000 | Loss: 0.00003676
Iteration 86/1000 | Loss: 0.00003676
Iteration 87/1000 | Loss: 0.00003676
Iteration 88/1000 | Loss: 0.00003675
Iteration 89/1000 | Loss: 0.00003675
Iteration 90/1000 | Loss: 0.00003675
Iteration 91/1000 | Loss: 0.00003674
Iteration 92/1000 | Loss: 0.00003674
Iteration 93/1000 | Loss: 0.00003674
Iteration 94/1000 | Loss: 0.00003674
Iteration 95/1000 | Loss: 0.00003674
Iteration 96/1000 | Loss: 0.00003674
Iteration 97/1000 | Loss: 0.00003674
Iteration 98/1000 | Loss: 0.00003674
Iteration 99/1000 | Loss: 0.00003674
Iteration 100/1000 | Loss: 0.00003674
Iteration 101/1000 | Loss: 0.00003674
Iteration 102/1000 | Loss: 0.00003673
Iteration 103/1000 | Loss: 0.00003673
Iteration 104/1000 | Loss: 0.00003673
Iteration 105/1000 | Loss: 0.00003673
Iteration 106/1000 | Loss: 0.00003673
Iteration 107/1000 | Loss: 0.00003673
Iteration 108/1000 | Loss: 0.00003672
Iteration 109/1000 | Loss: 0.00003672
Iteration 110/1000 | Loss: 0.00003672
Iteration 111/1000 | Loss: 0.00003672
Iteration 112/1000 | Loss: 0.00003672
Iteration 113/1000 | Loss: 0.00003672
Iteration 114/1000 | Loss: 0.00003672
Iteration 115/1000 | Loss: 0.00003672
Iteration 116/1000 | Loss: 0.00003672
Iteration 117/1000 | Loss: 0.00003672
Iteration 118/1000 | Loss: 0.00003672
Iteration 119/1000 | Loss: 0.00003671
Iteration 120/1000 | Loss: 0.00003671
Iteration 121/1000 | Loss: 0.00003671
Iteration 122/1000 | Loss: 0.00003671
Iteration 123/1000 | Loss: 0.00003671
Iteration 124/1000 | Loss: 0.00003671
Iteration 125/1000 | Loss: 0.00003670
Iteration 126/1000 | Loss: 0.00003670
Iteration 127/1000 | Loss: 0.00003670
Iteration 128/1000 | Loss: 0.00003670
Iteration 129/1000 | Loss: 0.00003670
Iteration 130/1000 | Loss: 0.00003670
Iteration 131/1000 | Loss: 0.00003670
Iteration 132/1000 | Loss: 0.00003670
Iteration 133/1000 | Loss: 0.00003670
Iteration 134/1000 | Loss: 0.00003670
Iteration 135/1000 | Loss: 0.00003670
Iteration 136/1000 | Loss: 0.00003669
Iteration 137/1000 | Loss: 0.00003669
Iteration 138/1000 | Loss: 0.00003669
Iteration 139/1000 | Loss: 0.00003669
Iteration 140/1000 | Loss: 0.00003669
Iteration 141/1000 | Loss: 0.00003669
Iteration 142/1000 | Loss: 0.00003669
Iteration 143/1000 | Loss: 0.00003669
Iteration 144/1000 | Loss: 0.00003669
Iteration 145/1000 | Loss: 0.00003669
Iteration 146/1000 | Loss: 0.00003669
Iteration 147/1000 | Loss: 0.00003669
Iteration 148/1000 | Loss: 0.00003669
Iteration 149/1000 | Loss: 0.00003669
Iteration 150/1000 | Loss: 0.00003669
Iteration 151/1000 | Loss: 0.00003669
Iteration 152/1000 | Loss: 0.00003669
Iteration 153/1000 | Loss: 0.00003669
Iteration 154/1000 | Loss: 0.00003669
Iteration 155/1000 | Loss: 0.00003669
Iteration 156/1000 | Loss: 0.00003669
Iteration 157/1000 | Loss: 0.00003669
Iteration 158/1000 | Loss: 0.00003669
Iteration 159/1000 | Loss: 0.00003669
Iteration 160/1000 | Loss: 0.00003669
Iteration 161/1000 | Loss: 0.00003669
Iteration 162/1000 | Loss: 0.00003669
Iteration 163/1000 | Loss: 0.00003669
Iteration 164/1000 | Loss: 0.00003669
Iteration 165/1000 | Loss: 0.00003669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.6690104025183246e-05, 3.6690104025183246e-05, 3.6690104025183246e-05, 3.6690104025183246e-05, 3.6690104025183246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6690104025183246e-05

Optimization complete. Final v2v error: 5.008852481842041 mm

Highest mean error: 5.4442830085754395 mm for frame 69

Lowest mean error: 4.646378517150879 mm for frame 183

Saving results

Total time: 63.965935707092285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388265
Iteration 2/25 | Loss: 0.00126166
Iteration 3/25 | Loss: 0.00119532
Iteration 4/25 | Loss: 0.00118519
Iteration 5/25 | Loss: 0.00118297
Iteration 6/25 | Loss: 0.00118297
Iteration 7/25 | Loss: 0.00118297
Iteration 8/25 | Loss: 0.00118297
Iteration 9/25 | Loss: 0.00118297
Iteration 10/25 | Loss: 0.00118297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001182971871457994, 0.001182971871457994, 0.001182971871457994, 0.001182971871457994, 0.001182971871457994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001182971871457994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37125170
Iteration 2/25 | Loss: 0.00090752
Iteration 3/25 | Loss: 0.00090752
Iteration 4/25 | Loss: 0.00090752
Iteration 5/25 | Loss: 0.00090752
Iteration 6/25 | Loss: 0.00090752
Iteration 7/25 | Loss: 0.00090752
Iteration 8/25 | Loss: 0.00090752
Iteration 9/25 | Loss: 0.00090752
Iteration 10/25 | Loss: 0.00090752
Iteration 11/25 | Loss: 0.00090752
Iteration 12/25 | Loss: 0.00090752
Iteration 13/25 | Loss: 0.00090752
Iteration 14/25 | Loss: 0.00090752
Iteration 15/25 | Loss: 0.00090752
Iteration 16/25 | Loss: 0.00090752
Iteration 17/25 | Loss: 0.00090752
Iteration 18/25 | Loss: 0.00090752
Iteration 19/25 | Loss: 0.00090752
Iteration 20/25 | Loss: 0.00090752
Iteration 21/25 | Loss: 0.00090752
Iteration 22/25 | Loss: 0.00090752
Iteration 23/25 | Loss: 0.00090752
Iteration 24/25 | Loss: 0.00090752
Iteration 25/25 | Loss: 0.00090752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090752
Iteration 2/1000 | Loss: 0.00001958
Iteration 3/1000 | Loss: 0.00001553
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001328
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001209
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001204
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001179
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001166
Iteration 123/1000 | Loss: 0.00001166
Iteration 124/1000 | Loss: 0.00001165
Iteration 125/1000 | Loss: 0.00001165
Iteration 126/1000 | Loss: 0.00001165
Iteration 127/1000 | Loss: 0.00001164
Iteration 128/1000 | Loss: 0.00001164
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001156
Iteration 174/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1562416148080956e-05, 1.1562416148080956e-05, 1.1562416148080956e-05, 1.1562416148080956e-05, 1.1562416148080956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1562416148080956e-05

Optimization complete. Final v2v error: 2.911268472671509 mm

Highest mean error: 3.0257463455200195 mm for frame 182

Lowest mean error: 2.8508081436157227 mm for frame 208

Saving results

Total time: 42.859164237976074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078923
Iteration 2/25 | Loss: 0.00326822
Iteration 3/25 | Loss: 0.00169966
Iteration 4/25 | Loss: 0.00139643
Iteration 5/25 | Loss: 0.00139295
Iteration 6/25 | Loss: 0.00136449
Iteration 7/25 | Loss: 0.00138082
Iteration 8/25 | Loss: 0.00132810
Iteration 9/25 | Loss: 0.00132180
Iteration 10/25 | Loss: 0.00131002
Iteration 11/25 | Loss: 0.00130143
Iteration 12/25 | Loss: 0.00129235
Iteration 13/25 | Loss: 0.00128681
Iteration 14/25 | Loss: 0.00128951
Iteration 15/25 | Loss: 0.00129205
Iteration 16/25 | Loss: 0.00130068
Iteration 17/25 | Loss: 0.00129467
Iteration 18/25 | Loss: 0.00130274
Iteration 19/25 | Loss: 0.00130298
Iteration 20/25 | Loss: 0.00129538
Iteration 21/25 | Loss: 0.00128790
Iteration 22/25 | Loss: 0.00129191
Iteration 23/25 | Loss: 0.00128722
Iteration 24/25 | Loss: 0.00128212
Iteration 25/25 | Loss: 0.00127958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39805508
Iteration 2/25 | Loss: 0.00138999
Iteration 3/25 | Loss: 0.00138999
Iteration 4/25 | Loss: 0.00138999
Iteration 5/25 | Loss: 0.00138999
Iteration 6/25 | Loss: 0.00138999
Iteration 7/25 | Loss: 0.00138999
Iteration 8/25 | Loss: 0.00138999
Iteration 9/25 | Loss: 0.00138999
Iteration 10/25 | Loss: 0.00138999
Iteration 11/25 | Loss: 0.00138999
Iteration 12/25 | Loss: 0.00138999
Iteration 13/25 | Loss: 0.00138999
Iteration 14/25 | Loss: 0.00138999
Iteration 15/25 | Loss: 0.00138999
Iteration 16/25 | Loss: 0.00138999
Iteration 17/25 | Loss: 0.00138999
Iteration 18/25 | Loss: 0.00138999
Iteration 19/25 | Loss: 0.00138999
Iteration 20/25 | Loss: 0.00138999
Iteration 21/25 | Loss: 0.00138999
Iteration 22/25 | Loss: 0.00138999
Iteration 23/25 | Loss: 0.00138999
Iteration 24/25 | Loss: 0.00138999
Iteration 25/25 | Loss: 0.00138999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138999
Iteration 2/1000 | Loss: 0.00018232
Iteration 3/1000 | Loss: 0.00041785
Iteration 4/1000 | Loss: 0.00044428
Iteration 5/1000 | Loss: 0.00026447
Iteration 6/1000 | Loss: 0.00036781
Iteration 7/1000 | Loss: 0.00028355
Iteration 8/1000 | Loss: 0.00004069
Iteration 9/1000 | Loss: 0.00017355
Iteration 10/1000 | Loss: 0.00037335
Iteration 11/1000 | Loss: 0.00008160
Iteration 12/1000 | Loss: 0.00017287
Iteration 13/1000 | Loss: 0.00020384
Iteration 14/1000 | Loss: 0.00003082
Iteration 15/1000 | Loss: 0.00002850
Iteration 16/1000 | Loss: 0.00006109
Iteration 17/1000 | Loss: 0.00043509
Iteration 18/1000 | Loss: 0.00049791
Iteration 19/1000 | Loss: 0.00016226
Iteration 20/1000 | Loss: 0.00015492
Iteration 21/1000 | Loss: 0.00011712
Iteration 22/1000 | Loss: 0.00006408
Iteration 23/1000 | Loss: 0.00004223
Iteration 24/1000 | Loss: 0.00006265
Iteration 25/1000 | Loss: 0.00002635
Iteration 26/1000 | Loss: 0.00003389
Iteration 27/1000 | Loss: 0.00002200
Iteration 28/1000 | Loss: 0.00005768
Iteration 29/1000 | Loss: 0.00013693
Iteration 30/1000 | Loss: 0.00006365
Iteration 31/1000 | Loss: 0.00034817
Iteration 32/1000 | Loss: 0.00029606
Iteration 33/1000 | Loss: 0.00007625
Iteration 34/1000 | Loss: 0.00006656
Iteration 35/1000 | Loss: 0.00003366
Iteration 36/1000 | Loss: 0.00016949
Iteration 37/1000 | Loss: 0.00029728
Iteration 38/1000 | Loss: 0.00030414
Iteration 39/1000 | Loss: 0.00023188
Iteration 40/1000 | Loss: 0.00028234
Iteration 41/1000 | Loss: 0.00039728
Iteration 42/1000 | Loss: 0.00021254
Iteration 43/1000 | Loss: 0.00030310
Iteration 44/1000 | Loss: 0.00019328
Iteration 45/1000 | Loss: 0.00030882
Iteration 46/1000 | Loss: 0.00018369
Iteration 47/1000 | Loss: 0.00016389
Iteration 48/1000 | Loss: 0.00012776
Iteration 49/1000 | Loss: 0.00017371
Iteration 50/1000 | Loss: 0.00017068
Iteration 51/1000 | Loss: 0.00035106
Iteration 52/1000 | Loss: 0.00027333
Iteration 53/1000 | Loss: 0.00015265
Iteration 54/1000 | Loss: 0.00009217
Iteration 55/1000 | Loss: 0.00015304
Iteration 56/1000 | Loss: 0.00007682
Iteration 57/1000 | Loss: 0.00003053
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00014511
Iteration 60/1000 | Loss: 0.00007007
Iteration 61/1000 | Loss: 0.00012924
Iteration 62/1000 | Loss: 0.00006886
Iteration 63/1000 | Loss: 0.00016589
Iteration 64/1000 | Loss: 0.00014473
Iteration 65/1000 | Loss: 0.00046203
Iteration 66/1000 | Loss: 0.00029911
Iteration 67/1000 | Loss: 0.00025681
Iteration 68/1000 | Loss: 0.00008701
Iteration 69/1000 | Loss: 0.00021843
Iteration 70/1000 | Loss: 0.00023217
Iteration 71/1000 | Loss: 0.00020186
Iteration 72/1000 | Loss: 0.00018956
Iteration 73/1000 | Loss: 0.00019402
Iteration 74/1000 | Loss: 0.00017610
Iteration 75/1000 | Loss: 0.00013780
Iteration 76/1000 | Loss: 0.00023936
Iteration 77/1000 | Loss: 0.00031836
Iteration 78/1000 | Loss: 0.00016033
Iteration 79/1000 | Loss: 0.00034890
Iteration 80/1000 | Loss: 0.00049639
Iteration 81/1000 | Loss: 0.00016176
Iteration 82/1000 | Loss: 0.00003746
Iteration 83/1000 | Loss: 0.00021447
Iteration 84/1000 | Loss: 0.00036150
Iteration 85/1000 | Loss: 0.00034532
Iteration 86/1000 | Loss: 0.00022282
Iteration 87/1000 | Loss: 0.00017945
Iteration 88/1000 | Loss: 0.00017174
Iteration 89/1000 | Loss: 0.00025131
Iteration 90/1000 | Loss: 0.00035603
Iteration 91/1000 | Loss: 0.00012542
Iteration 92/1000 | Loss: 0.00004499
Iteration 93/1000 | Loss: 0.00007112
Iteration 94/1000 | Loss: 0.00008582
Iteration 95/1000 | Loss: 0.00003805
Iteration 96/1000 | Loss: 0.00006976
Iteration 97/1000 | Loss: 0.00005077
Iteration 98/1000 | Loss: 0.00013091
Iteration 99/1000 | Loss: 0.00005551
Iteration 100/1000 | Loss: 0.00003170
Iteration 101/1000 | Loss: 0.00002357
Iteration 102/1000 | Loss: 0.00002080
Iteration 103/1000 | Loss: 0.00003554
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00001989
Iteration 106/1000 | Loss: 0.00001945
Iteration 107/1000 | Loss: 0.00002291
Iteration 108/1000 | Loss: 0.00010552
Iteration 109/1000 | Loss: 0.00001900
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001851
Iteration 112/1000 | Loss: 0.00007702
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001799
Iteration 115/1000 | Loss: 0.00005840
Iteration 116/1000 | Loss: 0.00002273
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00003264
Iteration 122/1000 | Loss: 0.00001756
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00002736
Iteration 125/1000 | Loss: 0.00001742
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00002436
Iteration 129/1000 | Loss: 0.00001733
Iteration 130/1000 | Loss: 0.00001732
Iteration 131/1000 | Loss: 0.00001730
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001729
Iteration 136/1000 | Loss: 0.00001729
Iteration 137/1000 | Loss: 0.00001728
Iteration 138/1000 | Loss: 0.00001728
Iteration 139/1000 | Loss: 0.00001728
Iteration 140/1000 | Loss: 0.00001728
Iteration 141/1000 | Loss: 0.00001727
Iteration 142/1000 | Loss: 0.00001727
Iteration 143/1000 | Loss: 0.00001726
Iteration 144/1000 | Loss: 0.00001726
Iteration 145/1000 | Loss: 0.00001726
Iteration 146/1000 | Loss: 0.00001726
Iteration 147/1000 | Loss: 0.00001726
Iteration 148/1000 | Loss: 0.00001726
Iteration 149/1000 | Loss: 0.00001726
Iteration 150/1000 | Loss: 0.00001725
Iteration 151/1000 | Loss: 0.00001725
Iteration 152/1000 | Loss: 0.00001725
Iteration 153/1000 | Loss: 0.00001725
Iteration 154/1000 | Loss: 0.00001725
Iteration 155/1000 | Loss: 0.00001725
Iteration 156/1000 | Loss: 0.00001725
Iteration 157/1000 | Loss: 0.00001725
Iteration 158/1000 | Loss: 0.00001725
Iteration 159/1000 | Loss: 0.00001724
Iteration 160/1000 | Loss: 0.00001724
Iteration 161/1000 | Loss: 0.00001724
Iteration 162/1000 | Loss: 0.00001724
Iteration 163/1000 | Loss: 0.00001724
Iteration 164/1000 | Loss: 0.00001724
Iteration 165/1000 | Loss: 0.00001724
Iteration 166/1000 | Loss: 0.00001724
Iteration 167/1000 | Loss: 0.00001723
Iteration 168/1000 | Loss: 0.00001723
Iteration 169/1000 | Loss: 0.00001723
Iteration 170/1000 | Loss: 0.00001723
Iteration 171/1000 | Loss: 0.00001723
Iteration 172/1000 | Loss: 0.00001723
Iteration 173/1000 | Loss: 0.00001723
Iteration 174/1000 | Loss: 0.00001723
Iteration 175/1000 | Loss: 0.00001722
Iteration 176/1000 | Loss: 0.00001722
Iteration 177/1000 | Loss: 0.00001722
Iteration 178/1000 | Loss: 0.00001722
Iteration 179/1000 | Loss: 0.00001722
Iteration 180/1000 | Loss: 0.00001722
Iteration 181/1000 | Loss: 0.00001722
Iteration 182/1000 | Loss: 0.00001722
Iteration 183/1000 | Loss: 0.00001721
Iteration 184/1000 | Loss: 0.00001721
Iteration 185/1000 | Loss: 0.00001721
Iteration 186/1000 | Loss: 0.00001721
Iteration 187/1000 | Loss: 0.00001721
Iteration 188/1000 | Loss: 0.00001721
Iteration 189/1000 | Loss: 0.00001721
Iteration 190/1000 | Loss: 0.00001721
Iteration 191/1000 | Loss: 0.00001721
Iteration 192/1000 | Loss: 0.00001721
Iteration 193/1000 | Loss: 0.00001721
Iteration 194/1000 | Loss: 0.00001721
Iteration 195/1000 | Loss: 0.00001721
Iteration 196/1000 | Loss: 0.00001721
Iteration 197/1000 | Loss: 0.00001721
Iteration 198/1000 | Loss: 0.00001721
Iteration 199/1000 | Loss: 0.00001721
Iteration 200/1000 | Loss: 0.00001721
Iteration 201/1000 | Loss: 0.00001720
Iteration 202/1000 | Loss: 0.00001720
Iteration 203/1000 | Loss: 0.00001720
Iteration 204/1000 | Loss: 0.00001720
Iteration 205/1000 | Loss: 0.00001720
Iteration 206/1000 | Loss: 0.00001720
Iteration 207/1000 | Loss: 0.00001720
Iteration 208/1000 | Loss: 0.00001720
Iteration 209/1000 | Loss: 0.00001720
Iteration 210/1000 | Loss: 0.00001720
Iteration 211/1000 | Loss: 0.00001720
Iteration 212/1000 | Loss: 0.00001720
Iteration 213/1000 | Loss: 0.00001720
Iteration 214/1000 | Loss: 0.00001720
Iteration 215/1000 | Loss: 0.00001720
Iteration 216/1000 | Loss: 0.00001720
Iteration 217/1000 | Loss: 0.00001720
Iteration 218/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.7199001376866363e-05, 1.7199001376866363e-05, 1.7199001376866363e-05, 1.7199001376866363e-05, 1.7199001376866363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7199001376866363e-05

Optimization complete. Final v2v error: 3.42629337310791 mm

Highest mean error: 7.548482894897461 mm for frame 105

Lowest mean error: 2.947598695755005 mm for frame 48

Saving results

Total time: 228.69756436347961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432696
Iteration 2/25 | Loss: 0.00130091
Iteration 3/25 | Loss: 0.00123017
Iteration 4/25 | Loss: 0.00121914
Iteration 5/25 | Loss: 0.00121558
Iteration 6/25 | Loss: 0.00121558
Iteration 7/25 | Loss: 0.00121558
Iteration 8/25 | Loss: 0.00121558
Iteration 9/25 | Loss: 0.00121558
Iteration 10/25 | Loss: 0.00121558
Iteration 11/25 | Loss: 0.00121558
Iteration 12/25 | Loss: 0.00121558
Iteration 13/25 | Loss: 0.00121558
Iteration 14/25 | Loss: 0.00121558
Iteration 15/25 | Loss: 0.00121558
Iteration 16/25 | Loss: 0.00121558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012155820149928331, 0.0012155820149928331, 0.0012155820149928331, 0.0012155820149928331, 0.0012155820149928331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012155820149928331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39496434
Iteration 2/25 | Loss: 0.00086921
Iteration 3/25 | Loss: 0.00086921
Iteration 4/25 | Loss: 0.00086921
Iteration 5/25 | Loss: 0.00086920
Iteration 6/25 | Loss: 0.00086920
Iteration 7/25 | Loss: 0.00086920
Iteration 8/25 | Loss: 0.00086920
Iteration 9/25 | Loss: 0.00086920
Iteration 10/25 | Loss: 0.00086920
Iteration 11/25 | Loss: 0.00086920
Iteration 12/25 | Loss: 0.00086920
Iteration 13/25 | Loss: 0.00086920
Iteration 14/25 | Loss: 0.00086920
Iteration 15/25 | Loss: 0.00086920
Iteration 16/25 | Loss: 0.00086920
Iteration 17/25 | Loss: 0.00086920
Iteration 18/25 | Loss: 0.00086920
Iteration 19/25 | Loss: 0.00086920
Iteration 20/25 | Loss: 0.00086920
Iteration 21/25 | Loss: 0.00086920
Iteration 22/25 | Loss: 0.00086920
Iteration 23/25 | Loss: 0.00086920
Iteration 24/25 | Loss: 0.00086920
Iteration 25/25 | Loss: 0.00086920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086920
Iteration 2/1000 | Loss: 0.00002609
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001664
Iteration 8/1000 | Loss: 0.00001633
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001564
Iteration 13/1000 | Loss: 0.00001550
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001528
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00001519
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001502
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001468
Iteration 88/1000 | Loss: 0.00001468
Iteration 89/1000 | Loss: 0.00001468
Iteration 90/1000 | Loss: 0.00001468
Iteration 91/1000 | Loss: 0.00001468
Iteration 92/1000 | Loss: 0.00001468
Iteration 93/1000 | Loss: 0.00001468
Iteration 94/1000 | Loss: 0.00001468
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001462
Iteration 126/1000 | Loss: 0.00001462
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001461
Iteration 141/1000 | Loss: 0.00001461
Iteration 142/1000 | Loss: 0.00001461
Iteration 143/1000 | Loss: 0.00001461
Iteration 144/1000 | Loss: 0.00001461
Iteration 145/1000 | Loss: 0.00001461
Iteration 146/1000 | Loss: 0.00001461
Iteration 147/1000 | Loss: 0.00001461
Iteration 148/1000 | Loss: 0.00001461
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001461
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001461
Iteration 157/1000 | Loss: 0.00001461
Iteration 158/1000 | Loss: 0.00001460
Iteration 159/1000 | Loss: 0.00001460
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001460
Iteration 177/1000 | Loss: 0.00001460
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Iteration 180/1000 | Loss: 0.00001460
Iteration 181/1000 | Loss: 0.00001460
Iteration 182/1000 | Loss: 0.00001460
Iteration 183/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.4602739611291327e-05, 1.4602739611291327e-05, 1.4602739611291327e-05, 1.4602739611291327e-05, 1.4602739611291327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4602739611291327e-05

Optimization complete. Final v2v error: 3.261441946029663 mm

Highest mean error: 3.6002585887908936 mm for frame 69

Lowest mean error: 2.959465265274048 mm for frame 83

Saving results

Total time: 47.81358003616333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765328
Iteration 2/25 | Loss: 0.00155829
Iteration 3/25 | Loss: 0.00127849
Iteration 4/25 | Loss: 0.00122379
Iteration 5/25 | Loss: 0.00121374
Iteration 6/25 | Loss: 0.00121566
Iteration 7/25 | Loss: 0.00121260
Iteration 8/25 | Loss: 0.00120930
Iteration 9/25 | Loss: 0.00120642
Iteration 10/25 | Loss: 0.00120656
Iteration 11/25 | Loss: 0.00119582
Iteration 12/25 | Loss: 0.00119457
Iteration 13/25 | Loss: 0.00119448
Iteration 14/25 | Loss: 0.00119447
Iteration 15/25 | Loss: 0.00119447
Iteration 16/25 | Loss: 0.00119447
Iteration 17/25 | Loss: 0.00119447
Iteration 18/25 | Loss: 0.00119447
Iteration 19/25 | Loss: 0.00119446
Iteration 20/25 | Loss: 0.00119446
Iteration 21/25 | Loss: 0.00119446
Iteration 22/25 | Loss: 0.00119446
Iteration 23/25 | Loss: 0.00119446
Iteration 24/25 | Loss: 0.00119446
Iteration 25/25 | Loss: 0.00119446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88796437
Iteration 2/25 | Loss: 0.00093259
Iteration 3/25 | Loss: 0.00093259
Iteration 4/25 | Loss: 0.00093258
Iteration 5/25 | Loss: 0.00093258
Iteration 6/25 | Loss: 0.00093258
Iteration 7/25 | Loss: 0.00093258
Iteration 8/25 | Loss: 0.00093258
Iteration 9/25 | Loss: 0.00093258
Iteration 10/25 | Loss: 0.00093258
Iteration 11/25 | Loss: 0.00093258
Iteration 12/25 | Loss: 0.00093258
Iteration 13/25 | Loss: 0.00093258
Iteration 14/25 | Loss: 0.00093258
Iteration 15/25 | Loss: 0.00093258
Iteration 16/25 | Loss: 0.00093258
Iteration 17/25 | Loss: 0.00093258
Iteration 18/25 | Loss: 0.00093258
Iteration 19/25 | Loss: 0.00093258
Iteration 20/25 | Loss: 0.00093258
Iteration 21/25 | Loss: 0.00093258
Iteration 22/25 | Loss: 0.00093258
Iteration 23/25 | Loss: 0.00093258
Iteration 24/25 | Loss: 0.00093258
Iteration 25/25 | Loss: 0.00093258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093258
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001899
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001517
Iteration 8/1000 | Loss: 0.00001482
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001403
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001345
Iteration 37/1000 | Loss: 0.00001345
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001345
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001342
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.3416371075436473e-05, 1.3416371075436473e-05, 1.3416371075436473e-05, 1.3416371075436473e-05, 1.3416371075436473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3416371075436473e-05

Optimization complete. Final v2v error: 3.1523470878601074 mm

Highest mean error: 3.589487314224243 mm for frame 82

Lowest mean error: 2.940626859664917 mm for frame 8

Saving results

Total time: 48.16111421585083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955138
Iteration 2/25 | Loss: 0.00187168
Iteration 3/25 | Loss: 0.00143914
Iteration 4/25 | Loss: 0.00134310
Iteration 5/25 | Loss: 0.00131168
Iteration 6/25 | Loss: 0.00131667
Iteration 7/25 | Loss: 0.00128846
Iteration 8/25 | Loss: 0.00128708
Iteration 9/25 | Loss: 0.00128679
Iteration 10/25 | Loss: 0.00128667
Iteration 11/25 | Loss: 0.00128665
Iteration 12/25 | Loss: 0.00128665
Iteration 13/25 | Loss: 0.00128665
Iteration 14/25 | Loss: 0.00128665
Iteration 15/25 | Loss: 0.00128664
Iteration 16/25 | Loss: 0.00128664
Iteration 17/25 | Loss: 0.00128664
Iteration 18/25 | Loss: 0.00128664
Iteration 19/25 | Loss: 0.00128664
Iteration 20/25 | Loss: 0.00128664
Iteration 21/25 | Loss: 0.00128664
Iteration 22/25 | Loss: 0.00128664
Iteration 23/25 | Loss: 0.00128664
Iteration 24/25 | Loss: 0.00128664
Iteration 25/25 | Loss: 0.00128664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51458418
Iteration 2/25 | Loss: 0.00107622
Iteration 3/25 | Loss: 0.00107622
Iteration 4/25 | Loss: 0.00107622
Iteration 5/25 | Loss: 0.00107622
Iteration 6/25 | Loss: 0.00107622
Iteration 7/25 | Loss: 0.00107622
Iteration 8/25 | Loss: 0.00107622
Iteration 9/25 | Loss: 0.00107622
Iteration 10/25 | Loss: 0.00107622
Iteration 11/25 | Loss: 0.00107622
Iteration 12/25 | Loss: 0.00107622
Iteration 13/25 | Loss: 0.00107622
Iteration 14/25 | Loss: 0.00107622
Iteration 15/25 | Loss: 0.00107622
Iteration 16/25 | Loss: 0.00107622
Iteration 17/25 | Loss: 0.00107622
Iteration 18/25 | Loss: 0.00107622
Iteration 19/25 | Loss: 0.00107622
Iteration 20/25 | Loss: 0.00107622
Iteration 21/25 | Loss: 0.00107622
Iteration 22/25 | Loss: 0.00107622
Iteration 23/25 | Loss: 0.00107622
Iteration 24/25 | Loss: 0.00107622
Iteration 25/25 | Loss: 0.00107622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107622
Iteration 2/1000 | Loss: 0.00004238
Iteration 3/1000 | Loss: 0.00003076
Iteration 4/1000 | Loss: 0.00002540
Iteration 5/1000 | Loss: 0.00002360
Iteration 6/1000 | Loss: 0.00002239
Iteration 7/1000 | Loss: 0.00002165
Iteration 8/1000 | Loss: 0.00002109
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00002029
Iteration 11/1000 | Loss: 0.00002002
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001963
Iteration 16/1000 | Loss: 0.00001962
Iteration 17/1000 | Loss: 0.00001958
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001953
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001948
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001945
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001940
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001935
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00001935
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001934
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001933
Iteration 69/1000 | Loss: 0.00001933
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001931
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.9271981727797538e-05, 1.9271981727797538e-05, 1.9271981727797538e-05, 1.9271981727797538e-05, 1.9271981727797538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9271981727797538e-05

Optimization complete. Final v2v error: 3.701543092727661 mm

Highest mean error: 4.712558746337891 mm for frame 165

Lowest mean error: 3.2662713527679443 mm for frame 138

Saving results

Total time: 52.511662006378174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814392
Iteration 2/25 | Loss: 0.00135917
Iteration 3/25 | Loss: 0.00126567
Iteration 4/25 | Loss: 0.00125756
Iteration 5/25 | Loss: 0.00125483
Iteration 6/25 | Loss: 0.00125465
Iteration 7/25 | Loss: 0.00125458
Iteration 8/25 | Loss: 0.00125458
Iteration 9/25 | Loss: 0.00125458
Iteration 10/25 | Loss: 0.00125458
Iteration 11/25 | Loss: 0.00125458
Iteration 12/25 | Loss: 0.00125458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012545835925266147, 0.0012545835925266147, 0.0012545835925266147, 0.0012545835925266147, 0.0012545835925266147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012545835925266147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52484941
Iteration 2/25 | Loss: 0.00096839
Iteration 3/25 | Loss: 0.00096838
Iteration 4/25 | Loss: 0.00096838
Iteration 5/25 | Loss: 0.00096838
Iteration 6/25 | Loss: 0.00096838
Iteration 7/25 | Loss: 0.00096838
Iteration 8/25 | Loss: 0.00096838
Iteration 9/25 | Loss: 0.00096838
Iteration 10/25 | Loss: 0.00096838
Iteration 11/25 | Loss: 0.00096838
Iteration 12/25 | Loss: 0.00096838
Iteration 13/25 | Loss: 0.00096838
Iteration 14/25 | Loss: 0.00096838
Iteration 15/25 | Loss: 0.00096838
Iteration 16/25 | Loss: 0.00096838
Iteration 17/25 | Loss: 0.00096838
Iteration 18/25 | Loss: 0.00096838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009683790267445147, 0.0009683790267445147, 0.0009683790267445147, 0.0009683790267445147, 0.0009683790267445147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009683790267445147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096838
Iteration 2/1000 | Loss: 0.00003773
Iteration 3/1000 | Loss: 0.00002665
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00002028
Iteration 6/1000 | Loss: 0.00001936
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001704
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001682
Iteration 24/1000 | Loss: 0.00001681
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001675
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001671
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001665
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001664
Iteration 44/1000 | Loss: 0.00001663
Iteration 45/1000 | Loss: 0.00001663
Iteration 46/1000 | Loss: 0.00001662
Iteration 47/1000 | Loss: 0.00001662
Iteration 48/1000 | Loss: 0.00001661
Iteration 49/1000 | Loss: 0.00001661
Iteration 50/1000 | Loss: 0.00001661
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001660
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001658
Iteration 60/1000 | Loss: 0.00001658
Iteration 61/1000 | Loss: 0.00001657
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00001651
Iteration 73/1000 | Loss: 0.00001651
Iteration 74/1000 | Loss: 0.00001651
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001650
Iteration 77/1000 | Loss: 0.00001650
Iteration 78/1000 | Loss: 0.00001650
Iteration 79/1000 | Loss: 0.00001650
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001649
Iteration 83/1000 | Loss: 0.00001648
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00001648
Iteration 86/1000 | Loss: 0.00001648
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001647
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001646
Iteration 96/1000 | Loss: 0.00001646
Iteration 97/1000 | Loss: 0.00001646
Iteration 98/1000 | Loss: 0.00001646
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Iteration 101/1000 | Loss: 0.00001645
Iteration 102/1000 | Loss: 0.00001645
Iteration 103/1000 | Loss: 0.00001645
Iteration 104/1000 | Loss: 0.00001645
Iteration 105/1000 | Loss: 0.00001645
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001644
Iteration 109/1000 | Loss: 0.00001644
Iteration 110/1000 | Loss: 0.00001644
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001643
Iteration 117/1000 | Loss: 0.00001643
Iteration 118/1000 | Loss: 0.00001643
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001642
Iteration 134/1000 | Loss: 0.00001642
Iteration 135/1000 | Loss: 0.00001642
Iteration 136/1000 | Loss: 0.00001642
Iteration 137/1000 | Loss: 0.00001642
Iteration 138/1000 | Loss: 0.00001642
Iteration 139/1000 | Loss: 0.00001642
Iteration 140/1000 | Loss: 0.00001642
Iteration 141/1000 | Loss: 0.00001642
Iteration 142/1000 | Loss: 0.00001642
Iteration 143/1000 | Loss: 0.00001642
Iteration 144/1000 | Loss: 0.00001642
Iteration 145/1000 | Loss: 0.00001642
Iteration 146/1000 | Loss: 0.00001642
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001642
Iteration 149/1000 | Loss: 0.00001642
Iteration 150/1000 | Loss: 0.00001642
Iteration 151/1000 | Loss: 0.00001642
Iteration 152/1000 | Loss: 0.00001642
Iteration 153/1000 | Loss: 0.00001642
Iteration 154/1000 | Loss: 0.00001642
Iteration 155/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.6421412510680966e-05, 1.6421412510680966e-05, 1.6421412510680966e-05, 1.6421412510680966e-05, 1.6421412510680966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6421412510680966e-05

Optimization complete. Final v2v error: 3.4149155616760254 mm

Highest mean error: 4.332365989685059 mm for frame 86

Lowest mean error: 2.871427297592163 mm for frame 147

Saving results

Total time: 42.017444372177124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758687
Iteration 2/25 | Loss: 0.00148576
Iteration 3/25 | Loss: 0.00130658
Iteration 4/25 | Loss: 0.00129518
Iteration 5/25 | Loss: 0.00129406
Iteration 6/25 | Loss: 0.00129406
Iteration 7/25 | Loss: 0.00129406
Iteration 8/25 | Loss: 0.00129406
Iteration 9/25 | Loss: 0.00129406
Iteration 10/25 | Loss: 0.00129406
Iteration 11/25 | Loss: 0.00129406
Iteration 12/25 | Loss: 0.00129406
Iteration 13/25 | Loss: 0.00129406
Iteration 14/25 | Loss: 0.00129406
Iteration 15/25 | Loss: 0.00129406
Iteration 16/25 | Loss: 0.00129406
Iteration 17/25 | Loss: 0.00129406
Iteration 18/25 | Loss: 0.00129406
Iteration 19/25 | Loss: 0.00129406
Iteration 20/25 | Loss: 0.00129406
Iteration 21/25 | Loss: 0.00129406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012940560700371861, 0.0012940560700371861, 0.0012940560700371861, 0.0012940560700371861, 0.0012940560700371861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012940560700371861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27138710
Iteration 2/25 | Loss: 0.00080776
Iteration 3/25 | Loss: 0.00080774
Iteration 4/25 | Loss: 0.00080774
Iteration 5/25 | Loss: 0.00080773
Iteration 6/25 | Loss: 0.00080773
Iteration 7/25 | Loss: 0.00080773
Iteration 8/25 | Loss: 0.00080773
Iteration 9/25 | Loss: 0.00080773
Iteration 10/25 | Loss: 0.00080773
Iteration 11/25 | Loss: 0.00080773
Iteration 12/25 | Loss: 0.00080773
Iteration 13/25 | Loss: 0.00080773
Iteration 14/25 | Loss: 0.00080773
Iteration 15/25 | Loss: 0.00080773
Iteration 16/25 | Loss: 0.00080773
Iteration 17/25 | Loss: 0.00080773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008077332749962807, 0.0008077332749962807, 0.0008077332749962807, 0.0008077332749962807, 0.0008077332749962807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008077332749962807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080773
Iteration 2/1000 | Loss: 0.00003449
Iteration 3/1000 | Loss: 0.00002481
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002089
Iteration 6/1000 | Loss: 0.00002038
Iteration 7/1000 | Loss: 0.00001987
Iteration 8/1000 | Loss: 0.00001949
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001859
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001835
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001823
Iteration 30/1000 | Loss: 0.00001823
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001822
Iteration 33/1000 | Loss: 0.00001822
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001821
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001820
Iteration 39/1000 | Loss: 0.00001820
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001817
Iteration 42/1000 | Loss: 0.00001817
Iteration 43/1000 | Loss: 0.00001817
Iteration 44/1000 | Loss: 0.00001816
Iteration 45/1000 | Loss: 0.00001816
Iteration 46/1000 | Loss: 0.00001814
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001813
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001803
Iteration 62/1000 | Loss: 0.00001801
Iteration 63/1000 | Loss: 0.00001801
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001795
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001794
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001793
Iteration 79/1000 | Loss: 0.00001793
Iteration 80/1000 | Loss: 0.00001793
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001792
Iteration 84/1000 | Loss: 0.00001792
Iteration 85/1000 | Loss: 0.00001792
Iteration 86/1000 | Loss: 0.00001792
Iteration 87/1000 | Loss: 0.00001792
Iteration 88/1000 | Loss: 0.00001792
Iteration 89/1000 | Loss: 0.00001792
Iteration 90/1000 | Loss: 0.00001792
Iteration 91/1000 | Loss: 0.00001792
Iteration 92/1000 | Loss: 0.00001792
Iteration 93/1000 | Loss: 0.00001792
Iteration 94/1000 | Loss: 0.00001792
Iteration 95/1000 | Loss: 0.00001792
Iteration 96/1000 | Loss: 0.00001792
Iteration 97/1000 | Loss: 0.00001792
Iteration 98/1000 | Loss: 0.00001791
Iteration 99/1000 | Loss: 0.00001791
Iteration 100/1000 | Loss: 0.00001791
Iteration 101/1000 | Loss: 0.00001791
Iteration 102/1000 | Loss: 0.00001791
Iteration 103/1000 | Loss: 0.00001791
Iteration 104/1000 | Loss: 0.00001791
Iteration 105/1000 | Loss: 0.00001791
Iteration 106/1000 | Loss: 0.00001791
Iteration 107/1000 | Loss: 0.00001791
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00001791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.7913253032020293e-05, 1.7913253032020293e-05, 1.7913253032020293e-05, 1.7913253032020293e-05, 1.7913253032020293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7913253032020293e-05

Optimization complete. Final v2v error: 3.561081886291504 mm

Highest mean error: 3.7806334495544434 mm for frame 30

Lowest mean error: 3.4387855529785156 mm for frame 38

Saving results

Total time: 36.84718728065491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898184
Iteration 2/25 | Loss: 0.00162104
Iteration 3/25 | Loss: 0.00134719
Iteration 4/25 | Loss: 0.00132097
Iteration 5/25 | Loss: 0.00131343
Iteration 6/25 | Loss: 0.00131154
Iteration 7/25 | Loss: 0.00131154
Iteration 8/25 | Loss: 0.00131154
Iteration 9/25 | Loss: 0.00131154
Iteration 10/25 | Loss: 0.00131154
Iteration 11/25 | Loss: 0.00131154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013115426991134882, 0.0013115426991134882, 0.0013115426991134882, 0.0013115426991134882, 0.0013115426991134882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013115426991134882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07291603
Iteration 2/25 | Loss: 0.00089970
Iteration 3/25 | Loss: 0.00089969
Iteration 4/25 | Loss: 0.00089969
Iteration 5/25 | Loss: 0.00089969
Iteration 6/25 | Loss: 0.00089969
Iteration 7/25 | Loss: 0.00089969
Iteration 8/25 | Loss: 0.00089969
Iteration 9/25 | Loss: 0.00089969
Iteration 10/25 | Loss: 0.00089969
Iteration 11/25 | Loss: 0.00089969
Iteration 12/25 | Loss: 0.00089969
Iteration 13/25 | Loss: 0.00089969
Iteration 14/25 | Loss: 0.00089969
Iteration 15/25 | Loss: 0.00089969
Iteration 16/25 | Loss: 0.00089969
Iteration 17/25 | Loss: 0.00089969
Iteration 18/25 | Loss: 0.00089969
Iteration 19/25 | Loss: 0.00089969
Iteration 20/25 | Loss: 0.00089969
Iteration 21/25 | Loss: 0.00089969
Iteration 22/25 | Loss: 0.00089969
Iteration 23/25 | Loss: 0.00089969
Iteration 24/25 | Loss: 0.00089969
Iteration 25/25 | Loss: 0.00089969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089969
Iteration 2/1000 | Loss: 0.00005699
Iteration 3/1000 | Loss: 0.00003781
Iteration 4/1000 | Loss: 0.00002986
Iteration 5/1000 | Loss: 0.00002805
Iteration 6/1000 | Loss: 0.00002681
Iteration 7/1000 | Loss: 0.00002620
Iteration 8/1000 | Loss: 0.00002548
Iteration 9/1000 | Loss: 0.00002494
Iteration 10/1000 | Loss: 0.00002459
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002401
Iteration 13/1000 | Loss: 0.00002376
Iteration 14/1000 | Loss: 0.00002353
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002309
Iteration 18/1000 | Loss: 0.00002291
Iteration 19/1000 | Loss: 0.00002284
Iteration 20/1000 | Loss: 0.00002279
Iteration 21/1000 | Loss: 0.00002277
Iteration 22/1000 | Loss: 0.00002276
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002266
Iteration 26/1000 | Loss: 0.00002258
Iteration 27/1000 | Loss: 0.00002257
Iteration 28/1000 | Loss: 0.00002257
Iteration 29/1000 | Loss: 0.00002249
Iteration 30/1000 | Loss: 0.00002249
Iteration 31/1000 | Loss: 0.00002249
Iteration 32/1000 | Loss: 0.00002248
Iteration 33/1000 | Loss: 0.00002247
Iteration 34/1000 | Loss: 0.00002246
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00002245
Iteration 37/1000 | Loss: 0.00002245
Iteration 38/1000 | Loss: 0.00002245
Iteration 39/1000 | Loss: 0.00002245
Iteration 40/1000 | Loss: 0.00002245
Iteration 41/1000 | Loss: 0.00002244
Iteration 42/1000 | Loss: 0.00002244
Iteration 43/1000 | Loss: 0.00002244
Iteration 44/1000 | Loss: 0.00002244
Iteration 45/1000 | Loss: 0.00002244
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00002243
Iteration 48/1000 | Loss: 0.00002243
Iteration 49/1000 | Loss: 0.00002242
Iteration 50/1000 | Loss: 0.00002242
Iteration 51/1000 | Loss: 0.00002242
Iteration 52/1000 | Loss: 0.00002241
Iteration 53/1000 | Loss: 0.00002241
Iteration 54/1000 | Loss: 0.00002241
Iteration 55/1000 | Loss: 0.00002241
Iteration 56/1000 | Loss: 0.00002240
Iteration 57/1000 | Loss: 0.00002240
Iteration 58/1000 | Loss: 0.00002240
Iteration 59/1000 | Loss: 0.00002240
Iteration 60/1000 | Loss: 0.00002240
Iteration 61/1000 | Loss: 0.00002240
Iteration 62/1000 | Loss: 0.00002240
Iteration 63/1000 | Loss: 0.00002240
Iteration 64/1000 | Loss: 0.00002240
Iteration 65/1000 | Loss: 0.00002240
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00002240
Iteration 68/1000 | Loss: 0.00002240
Iteration 69/1000 | Loss: 0.00002240
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002240
Iteration 72/1000 | Loss: 0.00002239
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002239
Iteration 75/1000 | Loss: 0.00002239
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00002239
Iteration 82/1000 | Loss: 0.00002239
Iteration 83/1000 | Loss: 0.00002239
Iteration 84/1000 | Loss: 0.00002239
Iteration 85/1000 | Loss: 0.00002239
Iteration 86/1000 | Loss: 0.00002239
Iteration 87/1000 | Loss: 0.00002239
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002239
Iteration 90/1000 | Loss: 0.00002239
Iteration 91/1000 | Loss: 0.00002238
Iteration 92/1000 | Loss: 0.00002238
Iteration 93/1000 | Loss: 0.00002238
Iteration 94/1000 | Loss: 0.00002238
Iteration 95/1000 | Loss: 0.00002238
Iteration 96/1000 | Loss: 0.00002238
Iteration 97/1000 | Loss: 0.00002238
Iteration 98/1000 | Loss: 0.00002238
Iteration 99/1000 | Loss: 0.00002238
Iteration 100/1000 | Loss: 0.00002238
Iteration 101/1000 | Loss: 0.00002238
Iteration 102/1000 | Loss: 0.00002238
Iteration 103/1000 | Loss: 0.00002238
Iteration 104/1000 | Loss: 0.00002238
Iteration 105/1000 | Loss: 0.00002238
Iteration 106/1000 | Loss: 0.00002238
Iteration 107/1000 | Loss: 0.00002238
Iteration 108/1000 | Loss: 0.00002238
Iteration 109/1000 | Loss: 0.00002238
Iteration 110/1000 | Loss: 0.00002238
Iteration 111/1000 | Loss: 0.00002238
Iteration 112/1000 | Loss: 0.00002238
Iteration 113/1000 | Loss: 0.00002238
Iteration 114/1000 | Loss: 0.00002238
Iteration 115/1000 | Loss: 0.00002238
Iteration 116/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.2384281692211516e-05, 2.2384281692211516e-05, 2.2384281692211516e-05, 2.2384281692211516e-05, 2.2384281692211516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2384281692211516e-05

Optimization complete. Final v2v error: 3.9102635383605957 mm

Highest mean error: 5.200043201446533 mm for frame 103

Lowest mean error: 3.1064131259918213 mm for frame 122

Saving results

Total time: 45.00147604942322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954474
Iteration 2/25 | Loss: 0.00354936
Iteration 3/25 | Loss: 0.00236045
Iteration 4/25 | Loss: 0.00231767
Iteration 5/25 | Loss: 0.00224076
Iteration 6/25 | Loss: 0.00218615
Iteration 7/25 | Loss: 0.00216916
Iteration 8/25 | Loss: 0.00210319
Iteration 9/25 | Loss: 0.00204748
Iteration 10/25 | Loss: 0.00201460
Iteration 11/25 | Loss: 0.00200034
Iteration 12/25 | Loss: 0.00199328
Iteration 13/25 | Loss: 0.00199281
Iteration 14/25 | Loss: 0.00198930
Iteration 15/25 | Loss: 0.00198831
Iteration 16/25 | Loss: 0.00199116
Iteration 17/25 | Loss: 0.00198594
Iteration 18/25 | Loss: 0.00198462
Iteration 19/25 | Loss: 0.00198404
Iteration 20/25 | Loss: 0.00198349
Iteration 21/25 | Loss: 0.00198701
Iteration 22/25 | Loss: 0.00198272
Iteration 23/25 | Loss: 0.00198159
Iteration 24/25 | Loss: 0.00198146
Iteration 25/25 | Loss: 0.00198140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32497704
Iteration 2/25 | Loss: 0.00525844
Iteration 3/25 | Loss: 0.00525844
Iteration 4/25 | Loss: 0.00525844
Iteration 5/25 | Loss: 0.00525844
Iteration 6/25 | Loss: 0.00525844
Iteration 7/25 | Loss: 0.00525844
Iteration 8/25 | Loss: 0.00525844
Iteration 9/25 | Loss: 0.00525844
Iteration 10/25 | Loss: 0.00525844
Iteration 11/25 | Loss: 0.00525844
Iteration 12/25 | Loss: 0.00525844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.005258435849100351, 0.005258435849100351, 0.005258435849100351, 0.005258435849100351, 0.005258435849100351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005258435849100351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00525844
Iteration 2/1000 | Loss: 0.00064727
Iteration 3/1000 | Loss: 0.00052422
Iteration 4/1000 | Loss: 0.00046582
Iteration 5/1000 | Loss: 0.00042180
Iteration 6/1000 | Loss: 0.00039410
Iteration 7/1000 | Loss: 0.00036107
Iteration 8/1000 | Loss: 0.00034541
Iteration 9/1000 | Loss: 0.00033330
Iteration 10/1000 | Loss: 0.00032558
Iteration 11/1000 | Loss: 0.00632881
Iteration 12/1000 | Loss: 0.01483343
Iteration 13/1000 | Loss: 0.00056576
Iteration 14/1000 | Loss: 0.00036149
Iteration 15/1000 | Loss: 0.00026460
Iteration 16/1000 | Loss: 0.00017238
Iteration 17/1000 | Loss: 0.00011244
Iteration 18/1000 | Loss: 0.00008616
Iteration 19/1000 | Loss: 0.00006292
Iteration 20/1000 | Loss: 0.00005268
Iteration 21/1000 | Loss: 0.00003816
Iteration 22/1000 | Loss: 0.00003150
Iteration 23/1000 | Loss: 0.00002820
Iteration 24/1000 | Loss: 0.00002498
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002125
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001803
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001628
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001618
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001615
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001615
Iteration 46/1000 | Loss: 0.00001615
Iteration 47/1000 | Loss: 0.00001615
Iteration 48/1000 | Loss: 0.00001615
Iteration 49/1000 | Loss: 0.00001615
Iteration 50/1000 | Loss: 0.00001615
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001610
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001609
Iteration 68/1000 | Loss: 0.00001609
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001607
Iteration 78/1000 | Loss: 0.00001607
Iteration 79/1000 | Loss: 0.00001607
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001607
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001606
Iteration 87/1000 | Loss: 0.00001606
Iteration 88/1000 | Loss: 0.00001606
Iteration 89/1000 | Loss: 0.00001606
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001603
Iteration 106/1000 | Loss: 0.00001603
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001603
Iteration 111/1000 | Loss: 0.00001603
Iteration 112/1000 | Loss: 0.00001603
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001603
Iteration 117/1000 | Loss: 0.00001603
Iteration 118/1000 | Loss: 0.00001603
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001601
Iteration 135/1000 | Loss: 0.00001601
Iteration 136/1000 | Loss: 0.00001601
Iteration 137/1000 | Loss: 0.00001601
Iteration 138/1000 | Loss: 0.00001601
Iteration 139/1000 | Loss: 0.00001601
Iteration 140/1000 | Loss: 0.00001601
Iteration 141/1000 | Loss: 0.00001601
Iteration 142/1000 | Loss: 0.00001601
Iteration 143/1000 | Loss: 0.00001601
Iteration 144/1000 | Loss: 0.00001601
Iteration 145/1000 | Loss: 0.00001601
Iteration 146/1000 | Loss: 0.00001601
Iteration 147/1000 | Loss: 0.00001601
Iteration 148/1000 | Loss: 0.00001601
Iteration 149/1000 | Loss: 0.00001601
Iteration 150/1000 | Loss: 0.00001601
Iteration 151/1000 | Loss: 0.00001601
Iteration 152/1000 | Loss: 0.00001600
Iteration 153/1000 | Loss: 0.00001600
Iteration 154/1000 | Loss: 0.00001600
Iteration 155/1000 | Loss: 0.00001600
Iteration 156/1000 | Loss: 0.00001600
Iteration 157/1000 | Loss: 0.00001600
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001600
Iteration 160/1000 | Loss: 0.00001600
Iteration 161/1000 | Loss: 0.00001600
Iteration 162/1000 | Loss: 0.00001600
Iteration 163/1000 | Loss: 0.00001600
Iteration 164/1000 | Loss: 0.00001600
Iteration 165/1000 | Loss: 0.00001600
Iteration 166/1000 | Loss: 0.00001600
Iteration 167/1000 | Loss: 0.00001600
Iteration 168/1000 | Loss: 0.00001600
Iteration 169/1000 | Loss: 0.00001600
Iteration 170/1000 | Loss: 0.00001600
Iteration 171/1000 | Loss: 0.00001600
Iteration 172/1000 | Loss: 0.00001600
Iteration 173/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.5995055946405046e-05, 1.5995055946405046e-05, 1.5995055946405046e-05, 1.5995055946405046e-05, 1.5995055946405046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5995055946405046e-05

Optimization complete. Final v2v error: 3.442783832550049 mm

Highest mean error: 3.6638543605804443 mm for frame 76

Lowest mean error: 3.3402533531188965 mm for frame 150

Saving results

Total time: 114.49733352661133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899238
Iteration 2/25 | Loss: 0.00143115
Iteration 3/25 | Loss: 0.00131900
Iteration 4/25 | Loss: 0.00130537
Iteration 5/25 | Loss: 0.00130169
Iteration 6/25 | Loss: 0.00130169
Iteration 7/25 | Loss: 0.00130169
Iteration 8/25 | Loss: 0.00130169
Iteration 9/25 | Loss: 0.00130169
Iteration 10/25 | Loss: 0.00130169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013016900047659874, 0.0013016900047659874, 0.0013016900047659874, 0.0013016900047659874, 0.0013016900047659874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013016900047659874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28981102
Iteration 2/25 | Loss: 0.00099322
Iteration 3/25 | Loss: 0.00099321
Iteration 4/25 | Loss: 0.00099321
Iteration 5/25 | Loss: 0.00099321
Iteration 6/25 | Loss: 0.00099320
Iteration 7/25 | Loss: 0.00099320
Iteration 8/25 | Loss: 0.00099320
Iteration 9/25 | Loss: 0.00099320
Iteration 10/25 | Loss: 0.00099320
Iteration 11/25 | Loss: 0.00099320
Iteration 12/25 | Loss: 0.00099320
Iteration 13/25 | Loss: 0.00099320
Iteration 14/25 | Loss: 0.00099320
Iteration 15/25 | Loss: 0.00099320
Iteration 16/25 | Loss: 0.00099320
Iteration 17/25 | Loss: 0.00099320
Iteration 18/25 | Loss: 0.00099320
Iteration 19/25 | Loss: 0.00099320
Iteration 20/25 | Loss: 0.00099320
Iteration 21/25 | Loss: 0.00099320
Iteration 22/25 | Loss: 0.00099320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009932024404406548, 0.0009932024404406548, 0.0009932024404406548, 0.0009932024404406548, 0.0009932024404406548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009932024404406548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099320
Iteration 2/1000 | Loss: 0.00004754
Iteration 3/1000 | Loss: 0.00003677
Iteration 4/1000 | Loss: 0.00003299
Iteration 5/1000 | Loss: 0.00003174
Iteration 6/1000 | Loss: 0.00003056
Iteration 7/1000 | Loss: 0.00002995
Iteration 8/1000 | Loss: 0.00002940
Iteration 9/1000 | Loss: 0.00002903
Iteration 10/1000 | Loss: 0.00002867
Iteration 11/1000 | Loss: 0.00002836
Iteration 12/1000 | Loss: 0.00002818
Iteration 13/1000 | Loss: 0.00002797
Iteration 14/1000 | Loss: 0.00002790
Iteration 15/1000 | Loss: 0.00002773
Iteration 16/1000 | Loss: 0.00002767
Iteration 17/1000 | Loss: 0.00002763
Iteration 18/1000 | Loss: 0.00002752
Iteration 19/1000 | Loss: 0.00002751
Iteration 20/1000 | Loss: 0.00002751
Iteration 21/1000 | Loss: 0.00002745
Iteration 22/1000 | Loss: 0.00002744
Iteration 23/1000 | Loss: 0.00002744
Iteration 24/1000 | Loss: 0.00002744
Iteration 25/1000 | Loss: 0.00002743
Iteration 26/1000 | Loss: 0.00002742
Iteration 27/1000 | Loss: 0.00002736
Iteration 28/1000 | Loss: 0.00002732
Iteration 29/1000 | Loss: 0.00002731
Iteration 30/1000 | Loss: 0.00002731
Iteration 31/1000 | Loss: 0.00002731
Iteration 32/1000 | Loss: 0.00002729
Iteration 33/1000 | Loss: 0.00002728
Iteration 34/1000 | Loss: 0.00002728
Iteration 35/1000 | Loss: 0.00002728
Iteration 36/1000 | Loss: 0.00002728
Iteration 37/1000 | Loss: 0.00002727
Iteration 38/1000 | Loss: 0.00002727
Iteration 39/1000 | Loss: 0.00002727
Iteration 40/1000 | Loss: 0.00002727
Iteration 41/1000 | Loss: 0.00002727
Iteration 42/1000 | Loss: 0.00002726
Iteration 43/1000 | Loss: 0.00002723
Iteration 44/1000 | Loss: 0.00002723
Iteration 45/1000 | Loss: 0.00002722
Iteration 46/1000 | Loss: 0.00002722
Iteration 47/1000 | Loss: 0.00002721
Iteration 48/1000 | Loss: 0.00002720
Iteration 49/1000 | Loss: 0.00002719
Iteration 50/1000 | Loss: 0.00002719
Iteration 51/1000 | Loss: 0.00002718
Iteration 52/1000 | Loss: 0.00002718
Iteration 53/1000 | Loss: 0.00002718
Iteration 54/1000 | Loss: 0.00002717
Iteration 55/1000 | Loss: 0.00002717
Iteration 56/1000 | Loss: 0.00002717
Iteration 57/1000 | Loss: 0.00002716
Iteration 58/1000 | Loss: 0.00002716
Iteration 59/1000 | Loss: 0.00002715
Iteration 60/1000 | Loss: 0.00002715
Iteration 61/1000 | Loss: 0.00002715
Iteration 62/1000 | Loss: 0.00002714
Iteration 63/1000 | Loss: 0.00002714
Iteration 64/1000 | Loss: 0.00002714
Iteration 65/1000 | Loss: 0.00002714
Iteration 66/1000 | Loss: 0.00002714
Iteration 67/1000 | Loss: 0.00002714
Iteration 68/1000 | Loss: 0.00002713
Iteration 69/1000 | Loss: 0.00002713
Iteration 70/1000 | Loss: 0.00002713
Iteration 71/1000 | Loss: 0.00002713
Iteration 72/1000 | Loss: 0.00002713
Iteration 73/1000 | Loss: 0.00002713
Iteration 74/1000 | Loss: 0.00002713
Iteration 75/1000 | Loss: 0.00002713
Iteration 76/1000 | Loss: 0.00002713
Iteration 77/1000 | Loss: 0.00002713
Iteration 78/1000 | Loss: 0.00002713
Iteration 79/1000 | Loss: 0.00002713
Iteration 80/1000 | Loss: 0.00002713
Iteration 81/1000 | Loss: 0.00002713
Iteration 82/1000 | Loss: 0.00002713
Iteration 83/1000 | Loss: 0.00002713
Iteration 84/1000 | Loss: 0.00002713
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002713
Iteration 87/1000 | Loss: 0.00002713
Iteration 88/1000 | Loss: 0.00002713
Iteration 89/1000 | Loss: 0.00002713
Iteration 90/1000 | Loss: 0.00002713
Iteration 91/1000 | Loss: 0.00002713
Iteration 92/1000 | Loss: 0.00002713
Iteration 93/1000 | Loss: 0.00002713
Iteration 94/1000 | Loss: 0.00002713
Iteration 95/1000 | Loss: 0.00002713
Iteration 96/1000 | Loss: 0.00002713
Iteration 97/1000 | Loss: 0.00002713
Iteration 98/1000 | Loss: 0.00002713
Iteration 99/1000 | Loss: 0.00002713
Iteration 100/1000 | Loss: 0.00002713
Iteration 101/1000 | Loss: 0.00002713
Iteration 102/1000 | Loss: 0.00002713
Iteration 103/1000 | Loss: 0.00002713
Iteration 104/1000 | Loss: 0.00002713
Iteration 105/1000 | Loss: 0.00002713
Iteration 106/1000 | Loss: 0.00002713
Iteration 107/1000 | Loss: 0.00002713
Iteration 108/1000 | Loss: 0.00002713
Iteration 109/1000 | Loss: 0.00002713
Iteration 110/1000 | Loss: 0.00002713
Iteration 111/1000 | Loss: 0.00002713
Iteration 112/1000 | Loss: 0.00002713
Iteration 113/1000 | Loss: 0.00002713
Iteration 114/1000 | Loss: 0.00002713
Iteration 115/1000 | Loss: 0.00002713
Iteration 116/1000 | Loss: 0.00002713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.7133097319165245e-05, 2.7133097319165245e-05, 2.7133097319165245e-05, 2.7133097319165245e-05, 2.7133097319165245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7133097319165245e-05

Optimization complete. Final v2v error: 4.291090965270996 mm

Highest mean error: 4.995507717132568 mm for frame 0

Lowest mean error: 3.9515554904937744 mm for frame 66

Saving results

Total time: 44.46827960014343
