Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=179, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10024-10079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733053
Iteration 2/25 | Loss: 0.00214984
Iteration 3/25 | Loss: 0.00164564
Iteration 4/25 | Loss: 0.00160950
Iteration 5/25 | Loss: 0.00150246
Iteration 6/25 | Loss: 0.00142302
Iteration 7/25 | Loss: 0.00139215
Iteration 8/25 | Loss: 0.00138228
Iteration 9/25 | Loss: 0.00137942
Iteration 10/25 | Loss: 0.00137892
Iteration 11/25 | Loss: 0.00137870
Iteration 12/25 | Loss: 0.00137859
Iteration 13/25 | Loss: 0.00137857
Iteration 14/25 | Loss: 0.00137857
Iteration 15/25 | Loss: 0.00137856
Iteration 16/25 | Loss: 0.00137856
Iteration 17/25 | Loss: 0.00137856
Iteration 18/25 | Loss: 0.00137855
Iteration 19/25 | Loss: 0.00137855
Iteration 20/25 | Loss: 0.00137855
Iteration 21/25 | Loss: 0.00137855
Iteration 22/25 | Loss: 0.00137855
Iteration 23/25 | Loss: 0.00137855
Iteration 24/25 | Loss: 0.00137855
Iteration 25/25 | Loss: 0.00137855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34661007
Iteration 2/25 | Loss: 0.00111186
Iteration 3/25 | Loss: 0.00111185
Iteration 4/25 | Loss: 0.00111185
Iteration 5/25 | Loss: 0.00111184
Iteration 6/25 | Loss: 0.00111184
Iteration 7/25 | Loss: 0.00111184
Iteration 8/25 | Loss: 0.00111184
Iteration 9/25 | Loss: 0.00111184
Iteration 10/25 | Loss: 0.00111184
Iteration 11/25 | Loss: 0.00111184
Iteration 12/25 | Loss: 0.00111184
Iteration 13/25 | Loss: 0.00111184
Iteration 14/25 | Loss: 0.00111184
Iteration 15/25 | Loss: 0.00111184
Iteration 16/25 | Loss: 0.00111184
Iteration 17/25 | Loss: 0.00111184
Iteration 18/25 | Loss: 0.00111184
Iteration 19/25 | Loss: 0.00111184
Iteration 20/25 | Loss: 0.00111184
Iteration 21/25 | Loss: 0.00111184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011118418769910932, 0.0011118418769910932, 0.0011118418769910932, 0.0011118418769910932, 0.0011118418769910932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011118418769910932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111184
Iteration 2/1000 | Loss: 0.00010668
Iteration 3/1000 | Loss: 0.00006958
Iteration 4/1000 | Loss: 0.00005848
Iteration 5/1000 | Loss: 0.00005513
Iteration 6/1000 | Loss: 0.00005288
Iteration 7/1000 | Loss: 0.00005132
Iteration 8/1000 | Loss: 0.00005027
Iteration 9/1000 | Loss: 0.00004950
Iteration 10/1000 | Loss: 0.00004894
Iteration 11/1000 | Loss: 0.00004841
Iteration 12/1000 | Loss: 0.00004809
Iteration 13/1000 | Loss: 0.00004792
Iteration 14/1000 | Loss: 0.00004778
Iteration 15/1000 | Loss: 0.00004765
Iteration 16/1000 | Loss: 0.00004764
Iteration 17/1000 | Loss: 0.00004762
Iteration 18/1000 | Loss: 0.00004758
Iteration 19/1000 | Loss: 0.00004755
Iteration 20/1000 | Loss: 0.00004755
Iteration 21/1000 | Loss: 0.00004754
Iteration 22/1000 | Loss: 0.00004753
Iteration 23/1000 | Loss: 0.00004753
Iteration 24/1000 | Loss: 0.00004753
Iteration 25/1000 | Loss: 0.00004751
Iteration 26/1000 | Loss: 0.00004751
Iteration 27/1000 | Loss: 0.00004751
Iteration 28/1000 | Loss: 0.00004750
Iteration 29/1000 | Loss: 0.00004749
Iteration 30/1000 | Loss: 0.00004749
Iteration 31/1000 | Loss: 0.00004747
Iteration 32/1000 | Loss: 0.00004747
Iteration 33/1000 | Loss: 0.00004747
Iteration 34/1000 | Loss: 0.00004747
Iteration 35/1000 | Loss: 0.00004747
Iteration 36/1000 | Loss: 0.00004747
Iteration 37/1000 | Loss: 0.00004747
Iteration 38/1000 | Loss: 0.00004747
Iteration 39/1000 | Loss: 0.00004747
Iteration 40/1000 | Loss: 0.00004747
Iteration 41/1000 | Loss: 0.00004747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [4.747065031551756e-05, 4.747065031551756e-05, 4.747065031551756e-05, 4.747065031551756e-05, 4.747065031551756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.747065031551756e-05

Optimization complete. Final v2v error: 5.607029438018799 mm

Highest mean error: 6.550061225891113 mm for frame 38

Lowest mean error: 4.849494457244873 mm for frame 217

Saving results

Total time: 54.68141746520996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835211
Iteration 2/25 | Loss: 0.00125960
Iteration 3/25 | Loss: 0.00113512
Iteration 4/25 | Loss: 0.00112457
Iteration 5/25 | Loss: 0.00112216
Iteration 6/25 | Loss: 0.00112199
Iteration 7/25 | Loss: 0.00112199
Iteration 8/25 | Loss: 0.00112199
Iteration 9/25 | Loss: 0.00112199
Iteration 10/25 | Loss: 0.00112199
Iteration 11/25 | Loss: 0.00112199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001121989800594747, 0.001121989800594747, 0.001121989800594747, 0.001121989800594747, 0.001121989800594747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001121989800594747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44757795
Iteration 2/25 | Loss: 0.00080766
Iteration 3/25 | Loss: 0.00080765
Iteration 4/25 | Loss: 0.00080765
Iteration 5/25 | Loss: 0.00080765
Iteration 6/25 | Loss: 0.00080765
Iteration 7/25 | Loss: 0.00080765
Iteration 8/25 | Loss: 0.00080765
Iteration 9/25 | Loss: 0.00080765
Iteration 10/25 | Loss: 0.00080765
Iteration 11/25 | Loss: 0.00080765
Iteration 12/25 | Loss: 0.00080765
Iteration 13/25 | Loss: 0.00080765
Iteration 14/25 | Loss: 0.00080765
Iteration 15/25 | Loss: 0.00080765
Iteration 16/25 | Loss: 0.00080765
Iteration 17/25 | Loss: 0.00080765
Iteration 18/25 | Loss: 0.00080765
Iteration 19/25 | Loss: 0.00080765
Iteration 20/25 | Loss: 0.00080765
Iteration 21/25 | Loss: 0.00080765
Iteration 22/25 | Loss: 0.00080765
Iteration 23/25 | Loss: 0.00080765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008076510857790709, 0.0008076510857790709, 0.0008076510857790709, 0.0008076510857790709, 0.0008076510857790709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008076510857790709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080765
Iteration 2/1000 | Loss: 0.00003256
Iteration 3/1000 | Loss: 0.00001603
Iteration 4/1000 | Loss: 0.00001440
Iteration 5/1000 | Loss: 0.00001354
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001253
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001191
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001164
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001162
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001158
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001154
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001135
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001130
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001127
Iteration 80/1000 | Loss: 0.00001127
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001126
Iteration 90/1000 | Loss: 0.00001126
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001125
Iteration 96/1000 | Loss: 0.00001125
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001125
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001124
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001124
Iteration 113/1000 | Loss: 0.00001124
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001123
Iteration 117/1000 | Loss: 0.00001123
Iteration 118/1000 | Loss: 0.00001123
Iteration 119/1000 | Loss: 0.00001123
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001122
Iteration 122/1000 | Loss: 0.00001122
Iteration 123/1000 | Loss: 0.00001122
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001119
Iteration 137/1000 | Loss: 0.00001119
Iteration 138/1000 | Loss: 0.00001119
Iteration 139/1000 | Loss: 0.00001119
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001118
Iteration 142/1000 | Loss: 0.00001118
Iteration 143/1000 | Loss: 0.00001118
Iteration 144/1000 | Loss: 0.00001118
Iteration 145/1000 | Loss: 0.00001118
Iteration 146/1000 | Loss: 0.00001118
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001116
Iteration 151/1000 | Loss: 0.00001116
Iteration 152/1000 | Loss: 0.00001116
Iteration 153/1000 | Loss: 0.00001116
Iteration 154/1000 | Loss: 0.00001116
Iteration 155/1000 | Loss: 0.00001116
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001115
Iteration 163/1000 | Loss: 0.00001115
Iteration 164/1000 | Loss: 0.00001115
Iteration 165/1000 | Loss: 0.00001115
Iteration 166/1000 | Loss: 0.00001115
Iteration 167/1000 | Loss: 0.00001115
Iteration 168/1000 | Loss: 0.00001115
Iteration 169/1000 | Loss: 0.00001115
Iteration 170/1000 | Loss: 0.00001115
Iteration 171/1000 | Loss: 0.00001115
Iteration 172/1000 | Loss: 0.00001115
Iteration 173/1000 | Loss: 0.00001114
Iteration 174/1000 | Loss: 0.00001114
Iteration 175/1000 | Loss: 0.00001114
Iteration 176/1000 | Loss: 0.00001114
Iteration 177/1000 | Loss: 0.00001114
Iteration 178/1000 | Loss: 0.00001114
Iteration 179/1000 | Loss: 0.00001114
Iteration 180/1000 | Loss: 0.00001114
Iteration 181/1000 | Loss: 0.00001114
Iteration 182/1000 | Loss: 0.00001114
Iteration 183/1000 | Loss: 0.00001114
Iteration 184/1000 | Loss: 0.00001114
Iteration 185/1000 | Loss: 0.00001114
Iteration 186/1000 | Loss: 0.00001114
Iteration 187/1000 | Loss: 0.00001113
Iteration 188/1000 | Loss: 0.00001113
Iteration 189/1000 | Loss: 0.00001113
Iteration 190/1000 | Loss: 0.00001113
Iteration 191/1000 | Loss: 0.00001113
Iteration 192/1000 | Loss: 0.00001113
Iteration 193/1000 | Loss: 0.00001113
Iteration 194/1000 | Loss: 0.00001113
Iteration 195/1000 | Loss: 0.00001113
Iteration 196/1000 | Loss: 0.00001113
Iteration 197/1000 | Loss: 0.00001113
Iteration 198/1000 | Loss: 0.00001112
Iteration 199/1000 | Loss: 0.00001112
Iteration 200/1000 | Loss: 0.00001112
Iteration 201/1000 | Loss: 0.00001112
Iteration 202/1000 | Loss: 0.00001112
Iteration 203/1000 | Loss: 0.00001112
Iteration 204/1000 | Loss: 0.00001112
Iteration 205/1000 | Loss: 0.00001112
Iteration 206/1000 | Loss: 0.00001112
Iteration 207/1000 | Loss: 0.00001112
Iteration 208/1000 | Loss: 0.00001112
Iteration 209/1000 | Loss: 0.00001112
Iteration 210/1000 | Loss: 0.00001112
Iteration 211/1000 | Loss: 0.00001112
Iteration 212/1000 | Loss: 0.00001112
Iteration 213/1000 | Loss: 0.00001112
Iteration 214/1000 | Loss: 0.00001111
Iteration 215/1000 | Loss: 0.00001111
Iteration 216/1000 | Loss: 0.00001111
Iteration 217/1000 | Loss: 0.00001111
Iteration 218/1000 | Loss: 0.00001111
Iteration 219/1000 | Loss: 0.00001111
Iteration 220/1000 | Loss: 0.00001111
Iteration 221/1000 | Loss: 0.00001111
Iteration 222/1000 | Loss: 0.00001111
Iteration 223/1000 | Loss: 0.00001111
Iteration 224/1000 | Loss: 0.00001111
Iteration 225/1000 | Loss: 0.00001111
Iteration 226/1000 | Loss: 0.00001111
Iteration 227/1000 | Loss: 0.00001111
Iteration 228/1000 | Loss: 0.00001111
Iteration 229/1000 | Loss: 0.00001111
Iteration 230/1000 | Loss: 0.00001111
Iteration 231/1000 | Loss: 0.00001110
Iteration 232/1000 | Loss: 0.00001110
Iteration 233/1000 | Loss: 0.00001110
Iteration 234/1000 | Loss: 0.00001110
Iteration 235/1000 | Loss: 0.00001110
Iteration 236/1000 | Loss: 0.00001110
Iteration 237/1000 | Loss: 0.00001110
Iteration 238/1000 | Loss: 0.00001110
Iteration 239/1000 | Loss: 0.00001110
Iteration 240/1000 | Loss: 0.00001110
Iteration 241/1000 | Loss: 0.00001110
Iteration 242/1000 | Loss: 0.00001110
Iteration 243/1000 | Loss: 0.00001110
Iteration 244/1000 | Loss: 0.00001109
Iteration 245/1000 | Loss: 0.00001109
Iteration 246/1000 | Loss: 0.00001109
Iteration 247/1000 | Loss: 0.00001109
Iteration 248/1000 | Loss: 0.00001109
Iteration 249/1000 | Loss: 0.00001109
Iteration 250/1000 | Loss: 0.00001109
Iteration 251/1000 | Loss: 0.00001109
Iteration 252/1000 | Loss: 0.00001109
Iteration 253/1000 | Loss: 0.00001108
Iteration 254/1000 | Loss: 0.00001108
Iteration 255/1000 | Loss: 0.00001108
Iteration 256/1000 | Loss: 0.00001108
Iteration 257/1000 | Loss: 0.00001108
Iteration 258/1000 | Loss: 0.00001108
Iteration 259/1000 | Loss: 0.00001108
Iteration 260/1000 | Loss: 0.00001108
Iteration 261/1000 | Loss: 0.00001108
Iteration 262/1000 | Loss: 0.00001108
Iteration 263/1000 | Loss: 0.00001108
Iteration 264/1000 | Loss: 0.00001108
Iteration 265/1000 | Loss: 0.00001108
Iteration 266/1000 | Loss: 0.00001108
Iteration 267/1000 | Loss: 0.00001108
Iteration 268/1000 | Loss: 0.00001108
Iteration 269/1000 | Loss: 0.00001108
Iteration 270/1000 | Loss: 0.00001108
Iteration 271/1000 | Loss: 0.00001108
Iteration 272/1000 | Loss: 0.00001108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.1078667739639059e-05, 1.1078667739639059e-05, 1.1078667739639059e-05, 1.1078667739639059e-05, 1.1078667739639059e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1078667739639059e-05

Optimization complete. Final v2v error: 2.799611806869507 mm

Highest mean error: 3.6999893188476562 mm for frame 119

Lowest mean error: 2.508449077606201 mm for frame 31

Saving results

Total time: 45.44799017906189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688525
Iteration 2/25 | Loss: 0.00149997
Iteration 3/25 | Loss: 0.00120232
Iteration 4/25 | Loss: 0.00117249
Iteration 5/25 | Loss: 0.00119224
Iteration 6/25 | Loss: 0.00115065
Iteration 7/25 | Loss: 0.00114201
Iteration 8/25 | Loss: 0.00114063
Iteration 9/25 | Loss: 0.00114013
Iteration 10/25 | Loss: 0.00113968
Iteration 11/25 | Loss: 0.00114306
Iteration 12/25 | Loss: 0.00114227
Iteration 13/25 | Loss: 0.00114115
Iteration 14/25 | Loss: 0.00114019
Iteration 15/25 | Loss: 0.00113972
Iteration 16/25 | Loss: 0.00114117
Iteration 17/25 | Loss: 0.00113946
Iteration 18/25 | Loss: 0.00113859
Iteration 19/25 | Loss: 0.00113827
Iteration 20/25 | Loss: 0.00113813
Iteration 21/25 | Loss: 0.00113810
Iteration 22/25 | Loss: 0.00113810
Iteration 23/25 | Loss: 0.00113810
Iteration 24/25 | Loss: 0.00113810
Iteration 25/25 | Loss: 0.00113810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28350377
Iteration 2/25 | Loss: 0.00140699
Iteration 3/25 | Loss: 0.00140696
Iteration 4/25 | Loss: 0.00140696
Iteration 5/25 | Loss: 0.00140696
Iteration 6/25 | Loss: 0.00140696
Iteration 7/25 | Loss: 0.00140696
Iteration 8/25 | Loss: 0.00140696
Iteration 9/25 | Loss: 0.00140696
Iteration 10/25 | Loss: 0.00140696
Iteration 11/25 | Loss: 0.00140696
Iteration 12/25 | Loss: 0.00140696
Iteration 13/25 | Loss: 0.00140696
Iteration 14/25 | Loss: 0.00140696
Iteration 15/25 | Loss: 0.00140696
Iteration 16/25 | Loss: 0.00140696
Iteration 17/25 | Loss: 0.00140696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001406958093866706, 0.001406958093866706, 0.001406958093866706, 0.001406958093866706, 0.001406958093866706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001406958093866706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140696
Iteration 2/1000 | Loss: 0.00009969
Iteration 3/1000 | Loss: 0.00008403
Iteration 4/1000 | Loss: 0.00004836
Iteration 5/1000 | Loss: 0.00002944
Iteration 6/1000 | Loss: 0.00002316
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00015023
Iteration 15/1000 | Loss: 0.00015152
Iteration 16/1000 | Loss: 0.00002510
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00001835
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001627
Iteration 21/1000 | Loss: 0.00001581
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001505
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00014993
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001522
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00002112
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00001353
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001285
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001280
Iteration 52/1000 | Loss: 0.00001280
Iteration 53/1000 | Loss: 0.00001279
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001270
Iteration 56/1000 | Loss: 0.00001269
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001253
Iteration 60/1000 | Loss: 0.00001253
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001232
Iteration 133/1000 | Loss: 0.00001232
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Iteration 137/1000 | Loss: 0.00001232
Iteration 138/1000 | Loss: 0.00001232
Iteration 139/1000 | Loss: 0.00001231
Iteration 140/1000 | Loss: 0.00001231
Iteration 141/1000 | Loss: 0.00001231
Iteration 142/1000 | Loss: 0.00001231
Iteration 143/1000 | Loss: 0.00001231
Iteration 144/1000 | Loss: 0.00001231
Iteration 145/1000 | Loss: 0.00001231
Iteration 146/1000 | Loss: 0.00001231
Iteration 147/1000 | Loss: 0.00001231
Iteration 148/1000 | Loss: 0.00001231
Iteration 149/1000 | Loss: 0.00001231
Iteration 150/1000 | Loss: 0.00001231
Iteration 151/1000 | Loss: 0.00001231
Iteration 152/1000 | Loss: 0.00001231
Iteration 153/1000 | Loss: 0.00001231
Iteration 154/1000 | Loss: 0.00001231
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001231
Iteration 157/1000 | Loss: 0.00001231
Iteration 158/1000 | Loss: 0.00001231
Iteration 159/1000 | Loss: 0.00001231
Iteration 160/1000 | Loss: 0.00001231
Iteration 161/1000 | Loss: 0.00001231
Iteration 162/1000 | Loss: 0.00001231
Iteration 163/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2307532415434252e-05, 1.2307532415434252e-05, 1.2307532415434252e-05, 1.2307532415434252e-05, 1.2307532415434252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2307532415434252e-05

Optimization complete. Final v2v error: 2.9946351051330566 mm

Highest mean error: 5.082422733306885 mm for frame 118

Lowest mean error: 2.72851824760437 mm for frame 12

Saving results

Total time: 123.86158323287964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814851
Iteration 2/25 | Loss: 0.00155373
Iteration 3/25 | Loss: 0.00116260
Iteration 4/25 | Loss: 0.00113345
Iteration 5/25 | Loss: 0.00113199
Iteration 6/25 | Loss: 0.00113199
Iteration 7/25 | Loss: 0.00113199
Iteration 8/25 | Loss: 0.00113199
Iteration 9/25 | Loss: 0.00113199
Iteration 10/25 | Loss: 0.00113199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011319865006953478, 0.0011319865006953478, 0.0011319865006953478, 0.0011319865006953478, 0.0011319865006953478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011319865006953478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32417512
Iteration 2/25 | Loss: 0.00054071
Iteration 3/25 | Loss: 0.00054070
Iteration 4/25 | Loss: 0.00054070
Iteration 5/25 | Loss: 0.00054070
Iteration 6/25 | Loss: 0.00054070
Iteration 7/25 | Loss: 0.00054070
Iteration 8/25 | Loss: 0.00054070
Iteration 9/25 | Loss: 0.00054070
Iteration 10/25 | Loss: 0.00054070
Iteration 11/25 | Loss: 0.00054070
Iteration 12/25 | Loss: 0.00054070
Iteration 13/25 | Loss: 0.00054070
Iteration 14/25 | Loss: 0.00054070
Iteration 15/25 | Loss: 0.00054070
Iteration 16/25 | Loss: 0.00054070
Iteration 17/25 | Loss: 0.00054070
Iteration 18/25 | Loss: 0.00054070
Iteration 19/25 | Loss: 0.00054070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005406962591223419, 0.0005406962591223419, 0.0005406962591223419, 0.0005406962591223419, 0.0005406962591223419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005406962591223419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054070
Iteration 2/1000 | Loss: 0.00002531
Iteration 3/1000 | Loss: 0.00001842
Iteration 4/1000 | Loss: 0.00001679
Iteration 5/1000 | Loss: 0.00001591
Iteration 6/1000 | Loss: 0.00001542
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001443
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001375
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001341
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001333
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001330
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001327
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001326
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.2956029422639403e-05, 1.2956029422639403e-05, 1.2956029422639403e-05, 1.2956029422639403e-05, 1.2956029422639403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2956029422639403e-05

Optimization complete. Final v2v error: 3.020430088043213 mm

Highest mean error: 3.2576606273651123 mm for frame 176

Lowest mean error: 2.880958318710327 mm for frame 142

Saving results

Total time: 37.0501651763916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460954
Iteration 2/25 | Loss: 0.00131654
Iteration 3/25 | Loss: 0.00113044
Iteration 4/25 | Loss: 0.00111767
Iteration 5/25 | Loss: 0.00111418
Iteration 6/25 | Loss: 0.00111311
Iteration 7/25 | Loss: 0.00111298
Iteration 8/25 | Loss: 0.00111298
Iteration 9/25 | Loss: 0.00111298
Iteration 10/25 | Loss: 0.00111298
Iteration 11/25 | Loss: 0.00111298
Iteration 12/25 | Loss: 0.00111298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011129779741168022, 0.0011129779741168022, 0.0011129779741168022, 0.0011129779741168022, 0.0011129779741168022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011129779741168022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45500839
Iteration 2/25 | Loss: 0.00096786
Iteration 3/25 | Loss: 0.00096786
Iteration 4/25 | Loss: 0.00096786
Iteration 5/25 | Loss: 0.00096786
Iteration 6/25 | Loss: 0.00096786
Iteration 7/25 | Loss: 0.00096786
Iteration 8/25 | Loss: 0.00096786
Iteration 9/25 | Loss: 0.00096786
Iteration 10/25 | Loss: 0.00096786
Iteration 11/25 | Loss: 0.00096786
Iteration 12/25 | Loss: 0.00096786
Iteration 13/25 | Loss: 0.00096786
Iteration 14/25 | Loss: 0.00096786
Iteration 15/25 | Loss: 0.00096786
Iteration 16/25 | Loss: 0.00096786
Iteration 17/25 | Loss: 0.00096786
Iteration 18/25 | Loss: 0.00096786
Iteration 19/25 | Loss: 0.00096786
Iteration 20/25 | Loss: 0.00096786
Iteration 21/25 | Loss: 0.00096786
Iteration 22/25 | Loss: 0.00096786
Iteration 23/25 | Loss: 0.00096786
Iteration 24/25 | Loss: 0.00096786
Iteration 25/25 | Loss: 0.00096786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096786
Iteration 2/1000 | Loss: 0.00002532
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001669
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001331
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001325
Iteration 21/1000 | Loss: 0.00001325
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001323
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001321
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001319
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001318
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001302
Iteration 76/1000 | Loss: 0.00001301
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001290
Iteration 116/1000 | Loss: 0.00001290
Iteration 117/1000 | Loss: 0.00001290
Iteration 118/1000 | Loss: 0.00001290
Iteration 119/1000 | Loss: 0.00001290
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001289
Iteration 125/1000 | Loss: 0.00001289
Iteration 126/1000 | Loss: 0.00001289
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001288
Iteration 129/1000 | Loss: 0.00001288
Iteration 130/1000 | Loss: 0.00001288
Iteration 131/1000 | Loss: 0.00001288
Iteration 132/1000 | Loss: 0.00001288
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001288
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001287
Iteration 141/1000 | Loss: 0.00001287
Iteration 142/1000 | Loss: 0.00001287
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001286
Iteration 145/1000 | Loss: 0.00001286
Iteration 146/1000 | Loss: 0.00001286
Iteration 147/1000 | Loss: 0.00001286
Iteration 148/1000 | Loss: 0.00001286
Iteration 149/1000 | Loss: 0.00001286
Iteration 150/1000 | Loss: 0.00001286
Iteration 151/1000 | Loss: 0.00001286
Iteration 152/1000 | Loss: 0.00001286
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001285
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Iteration 157/1000 | Loss: 0.00001285
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001285
Iteration 160/1000 | Loss: 0.00001285
Iteration 161/1000 | Loss: 0.00001285
Iteration 162/1000 | Loss: 0.00001284
Iteration 163/1000 | Loss: 0.00001284
Iteration 164/1000 | Loss: 0.00001284
Iteration 165/1000 | Loss: 0.00001284
Iteration 166/1000 | Loss: 0.00001284
Iteration 167/1000 | Loss: 0.00001284
Iteration 168/1000 | Loss: 0.00001284
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001283
Iteration 175/1000 | Loss: 0.00001283
Iteration 176/1000 | Loss: 0.00001283
Iteration 177/1000 | Loss: 0.00001282
Iteration 178/1000 | Loss: 0.00001282
Iteration 179/1000 | Loss: 0.00001282
Iteration 180/1000 | Loss: 0.00001282
Iteration 181/1000 | Loss: 0.00001282
Iteration 182/1000 | Loss: 0.00001282
Iteration 183/1000 | Loss: 0.00001282
Iteration 184/1000 | Loss: 0.00001282
Iteration 185/1000 | Loss: 0.00001282
Iteration 186/1000 | Loss: 0.00001282
Iteration 187/1000 | Loss: 0.00001282
Iteration 188/1000 | Loss: 0.00001282
Iteration 189/1000 | Loss: 0.00001281
Iteration 190/1000 | Loss: 0.00001281
Iteration 191/1000 | Loss: 0.00001281
Iteration 192/1000 | Loss: 0.00001281
Iteration 193/1000 | Loss: 0.00001281
Iteration 194/1000 | Loss: 0.00001281
Iteration 195/1000 | Loss: 0.00001281
Iteration 196/1000 | Loss: 0.00001281
Iteration 197/1000 | Loss: 0.00001281
Iteration 198/1000 | Loss: 0.00001281
Iteration 199/1000 | Loss: 0.00001280
Iteration 200/1000 | Loss: 0.00001280
Iteration 201/1000 | Loss: 0.00001280
Iteration 202/1000 | Loss: 0.00001280
Iteration 203/1000 | Loss: 0.00001280
Iteration 204/1000 | Loss: 0.00001280
Iteration 205/1000 | Loss: 0.00001280
Iteration 206/1000 | Loss: 0.00001280
Iteration 207/1000 | Loss: 0.00001280
Iteration 208/1000 | Loss: 0.00001280
Iteration 209/1000 | Loss: 0.00001280
Iteration 210/1000 | Loss: 0.00001280
Iteration 211/1000 | Loss: 0.00001280
Iteration 212/1000 | Loss: 0.00001280
Iteration 213/1000 | Loss: 0.00001280
Iteration 214/1000 | Loss: 0.00001280
Iteration 215/1000 | Loss: 0.00001280
Iteration 216/1000 | Loss: 0.00001279
Iteration 217/1000 | Loss: 0.00001279
Iteration 218/1000 | Loss: 0.00001279
Iteration 219/1000 | Loss: 0.00001279
Iteration 220/1000 | Loss: 0.00001279
Iteration 221/1000 | Loss: 0.00001279
Iteration 222/1000 | Loss: 0.00001279
Iteration 223/1000 | Loss: 0.00001279
Iteration 224/1000 | Loss: 0.00001279
Iteration 225/1000 | Loss: 0.00001279
Iteration 226/1000 | Loss: 0.00001279
Iteration 227/1000 | Loss: 0.00001279
Iteration 228/1000 | Loss: 0.00001279
Iteration 229/1000 | Loss: 0.00001279
Iteration 230/1000 | Loss: 0.00001279
Iteration 231/1000 | Loss: 0.00001279
Iteration 232/1000 | Loss: 0.00001279
Iteration 233/1000 | Loss: 0.00001279
Iteration 234/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.2794207577826455e-05, 1.2794207577826455e-05, 1.2794207577826455e-05, 1.2794207577826455e-05, 1.2794207577826455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2794207577826455e-05

Optimization complete. Final v2v error: 2.932309865951538 mm

Highest mean error: 3.5029797554016113 mm for frame 16

Lowest mean error: 2.3086676597595215 mm for frame 107

Saving results

Total time: 45.953840017318726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903989
Iteration 2/25 | Loss: 0.00151570
Iteration 3/25 | Loss: 0.00123218
Iteration 4/25 | Loss: 0.00120659
Iteration 5/25 | Loss: 0.00119927
Iteration 6/25 | Loss: 0.00119730
Iteration 7/25 | Loss: 0.00119712
Iteration 8/25 | Loss: 0.00119712
Iteration 9/25 | Loss: 0.00119712
Iteration 10/25 | Loss: 0.00119712
Iteration 11/25 | Loss: 0.00119712
Iteration 12/25 | Loss: 0.00119712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011971245985478163, 0.0011971245985478163, 0.0011971245985478163, 0.0011971245985478163, 0.0011971245985478163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011971245985478163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07651174
Iteration 2/25 | Loss: 0.00077138
Iteration 3/25 | Loss: 0.00077136
Iteration 4/25 | Loss: 0.00077136
Iteration 5/25 | Loss: 0.00077136
Iteration 6/25 | Loss: 0.00077136
Iteration 7/25 | Loss: 0.00077136
Iteration 8/25 | Loss: 0.00077136
Iteration 9/25 | Loss: 0.00077136
Iteration 10/25 | Loss: 0.00077136
Iteration 11/25 | Loss: 0.00077136
Iteration 12/25 | Loss: 0.00077136
Iteration 13/25 | Loss: 0.00077136
Iteration 14/25 | Loss: 0.00077136
Iteration 15/25 | Loss: 0.00077136
Iteration 16/25 | Loss: 0.00077136
Iteration 17/25 | Loss: 0.00077136
Iteration 18/25 | Loss: 0.00077136
Iteration 19/25 | Loss: 0.00077136
Iteration 20/25 | Loss: 0.00077136
Iteration 21/25 | Loss: 0.00077136
Iteration 22/25 | Loss: 0.00077136
Iteration 23/25 | Loss: 0.00077136
Iteration 24/25 | Loss: 0.00077136
Iteration 25/25 | Loss: 0.00077136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077136
Iteration 2/1000 | Loss: 0.00004304
Iteration 3/1000 | Loss: 0.00003283
Iteration 4/1000 | Loss: 0.00002646
Iteration 5/1000 | Loss: 0.00002514
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002376
Iteration 8/1000 | Loss: 0.00002322
Iteration 9/1000 | Loss: 0.00002269
Iteration 10/1000 | Loss: 0.00002234
Iteration 11/1000 | Loss: 0.00002207
Iteration 12/1000 | Loss: 0.00002190
Iteration 13/1000 | Loss: 0.00002171
Iteration 14/1000 | Loss: 0.00002149
Iteration 15/1000 | Loss: 0.00002136
Iteration 16/1000 | Loss: 0.00002121
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002102
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002081
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002075
Iteration 25/1000 | Loss: 0.00002072
Iteration 26/1000 | Loss: 0.00002071
Iteration 27/1000 | Loss: 0.00002068
Iteration 28/1000 | Loss: 0.00002068
Iteration 29/1000 | Loss: 0.00002067
Iteration 30/1000 | Loss: 0.00002066
Iteration 31/1000 | Loss: 0.00002064
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002056
Iteration 35/1000 | Loss: 0.00002056
Iteration 36/1000 | Loss: 0.00002055
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002055
Iteration 39/1000 | Loss: 0.00002054
Iteration 40/1000 | Loss: 0.00002054
Iteration 41/1000 | Loss: 0.00002054
Iteration 42/1000 | Loss: 0.00002054
Iteration 43/1000 | Loss: 0.00002054
Iteration 44/1000 | Loss: 0.00002054
Iteration 45/1000 | Loss: 0.00002054
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00002054
Iteration 48/1000 | Loss: 0.00002054
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002053
Iteration 51/1000 | Loss: 0.00002052
Iteration 52/1000 | Loss: 0.00002052
Iteration 53/1000 | Loss: 0.00002052
Iteration 54/1000 | Loss: 0.00002051
Iteration 55/1000 | Loss: 0.00002051
Iteration 56/1000 | Loss: 0.00002051
Iteration 57/1000 | Loss: 0.00002051
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002050
Iteration 60/1000 | Loss: 0.00002050
Iteration 61/1000 | Loss: 0.00002050
Iteration 62/1000 | Loss: 0.00002050
Iteration 63/1000 | Loss: 0.00002050
Iteration 64/1000 | Loss: 0.00002050
Iteration 65/1000 | Loss: 0.00002050
Iteration 66/1000 | Loss: 0.00002050
Iteration 67/1000 | Loss: 0.00002049
Iteration 68/1000 | Loss: 0.00002049
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002049
Iteration 74/1000 | Loss: 0.00002049
Iteration 75/1000 | Loss: 0.00002049
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002047
Iteration 96/1000 | Loss: 0.00002047
Iteration 97/1000 | Loss: 0.00002047
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002046
Iteration 102/1000 | Loss: 0.00002046
Iteration 103/1000 | Loss: 0.00002046
Iteration 104/1000 | Loss: 0.00002046
Iteration 105/1000 | Loss: 0.00002046
Iteration 106/1000 | Loss: 0.00002046
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002046
Iteration 116/1000 | Loss: 0.00002046
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002046
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002046
Iteration 122/1000 | Loss: 0.00002046
Iteration 123/1000 | Loss: 0.00002046
Iteration 124/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.0456802303669974e-05, 2.0456802303669974e-05, 2.0456802303669974e-05, 2.0456802303669974e-05, 2.0456802303669974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0456802303669974e-05

Optimization complete. Final v2v error: 3.7140891551971436 mm

Highest mean error: 5.042690753936768 mm for frame 103

Lowest mean error: 2.903550148010254 mm for frame 122

Saving results

Total time: 47.036762714385986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782518
Iteration 2/25 | Loss: 0.00138955
Iteration 3/25 | Loss: 0.00117473
Iteration 4/25 | Loss: 0.00115581
Iteration 5/25 | Loss: 0.00116156
Iteration 6/25 | Loss: 0.00114894
Iteration 7/25 | Loss: 0.00114336
Iteration 8/25 | Loss: 0.00114516
Iteration 9/25 | Loss: 0.00114273
Iteration 10/25 | Loss: 0.00114202
Iteration 11/25 | Loss: 0.00114196
Iteration 12/25 | Loss: 0.00114167
Iteration 13/25 | Loss: 0.00114040
Iteration 14/25 | Loss: 0.00113950
Iteration 15/25 | Loss: 0.00114041
Iteration 16/25 | Loss: 0.00113990
Iteration 17/25 | Loss: 0.00113822
Iteration 18/25 | Loss: 0.00113747
Iteration 19/25 | Loss: 0.00113728
Iteration 20/25 | Loss: 0.00113728
Iteration 21/25 | Loss: 0.00113726
Iteration 22/25 | Loss: 0.00113726
Iteration 23/25 | Loss: 0.00113726
Iteration 24/25 | Loss: 0.00113726
Iteration 25/25 | Loss: 0.00113726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.53763723
Iteration 2/25 | Loss: 0.00070095
Iteration 3/25 | Loss: 0.00070089
Iteration 4/25 | Loss: 0.00070089
Iteration 5/25 | Loss: 0.00070089
Iteration 6/25 | Loss: 0.00070089
Iteration 7/25 | Loss: 0.00070089
Iteration 8/25 | Loss: 0.00070089
Iteration 9/25 | Loss: 0.00070089
Iteration 10/25 | Loss: 0.00070089
Iteration 11/25 | Loss: 0.00070089
Iteration 12/25 | Loss: 0.00070089
Iteration 13/25 | Loss: 0.00070089
Iteration 14/25 | Loss: 0.00070089
Iteration 15/25 | Loss: 0.00070089
Iteration 16/25 | Loss: 0.00070089
Iteration 17/25 | Loss: 0.00070089
Iteration 18/25 | Loss: 0.00070089
Iteration 19/25 | Loss: 0.00070089
Iteration 20/25 | Loss: 0.00070089
Iteration 21/25 | Loss: 0.00070089
Iteration 22/25 | Loss: 0.00070089
Iteration 23/25 | Loss: 0.00070089
Iteration 24/25 | Loss: 0.00070089
Iteration 25/25 | Loss: 0.00070089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070089
Iteration 2/1000 | Loss: 0.00002258
Iteration 3/1000 | Loss: 0.00001675
Iteration 4/1000 | Loss: 0.00040159
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00002354
Iteration 7/1000 | Loss: 0.00001576
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001350
Iteration 45/1000 | Loss: 0.00001350
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001341
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001325
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001321
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001319
Iteration 150/1000 | Loss: 0.00001319
Iteration 151/1000 | Loss: 0.00001319
Iteration 152/1000 | Loss: 0.00001319
Iteration 153/1000 | Loss: 0.00001319
Iteration 154/1000 | Loss: 0.00001318
Iteration 155/1000 | Loss: 0.00001318
Iteration 156/1000 | Loss: 0.00001318
Iteration 157/1000 | Loss: 0.00001318
Iteration 158/1000 | Loss: 0.00001318
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Iteration 161/1000 | Loss: 0.00001317
Iteration 162/1000 | Loss: 0.00001317
Iteration 163/1000 | Loss: 0.00001317
Iteration 164/1000 | Loss: 0.00001317
Iteration 165/1000 | Loss: 0.00001316
Iteration 166/1000 | Loss: 0.00001316
Iteration 167/1000 | Loss: 0.00001316
Iteration 168/1000 | Loss: 0.00001316
Iteration 169/1000 | Loss: 0.00001316
Iteration 170/1000 | Loss: 0.00001316
Iteration 171/1000 | Loss: 0.00001315
Iteration 172/1000 | Loss: 0.00001315
Iteration 173/1000 | Loss: 0.00001315
Iteration 174/1000 | Loss: 0.00001315
Iteration 175/1000 | Loss: 0.00001315
Iteration 176/1000 | Loss: 0.00001315
Iteration 177/1000 | Loss: 0.00001315
Iteration 178/1000 | Loss: 0.00001315
Iteration 179/1000 | Loss: 0.00001315
Iteration 180/1000 | Loss: 0.00001315
Iteration 181/1000 | Loss: 0.00001315
Iteration 182/1000 | Loss: 0.00001315
Iteration 183/1000 | Loss: 0.00001315
Iteration 184/1000 | Loss: 0.00001315
Iteration 185/1000 | Loss: 0.00001315
Iteration 186/1000 | Loss: 0.00001315
Iteration 187/1000 | Loss: 0.00001315
Iteration 188/1000 | Loss: 0.00001315
Iteration 189/1000 | Loss: 0.00001315
Iteration 190/1000 | Loss: 0.00001315
Iteration 191/1000 | Loss: 0.00001315
Iteration 192/1000 | Loss: 0.00001315
Iteration 193/1000 | Loss: 0.00001315
Iteration 194/1000 | Loss: 0.00001315
Iteration 195/1000 | Loss: 0.00001315
Iteration 196/1000 | Loss: 0.00001315
Iteration 197/1000 | Loss: 0.00001315
Iteration 198/1000 | Loss: 0.00001315
Iteration 199/1000 | Loss: 0.00001315
Iteration 200/1000 | Loss: 0.00001315
Iteration 201/1000 | Loss: 0.00001315
Iteration 202/1000 | Loss: 0.00001315
Iteration 203/1000 | Loss: 0.00001315
Iteration 204/1000 | Loss: 0.00001315
Iteration 205/1000 | Loss: 0.00001315
Iteration 206/1000 | Loss: 0.00001315
Iteration 207/1000 | Loss: 0.00001315
Iteration 208/1000 | Loss: 0.00001315
Iteration 209/1000 | Loss: 0.00001315
Iteration 210/1000 | Loss: 0.00001315
Iteration 211/1000 | Loss: 0.00001315
Iteration 212/1000 | Loss: 0.00001315
Iteration 213/1000 | Loss: 0.00001315
Iteration 214/1000 | Loss: 0.00001315
Iteration 215/1000 | Loss: 0.00001315
Iteration 216/1000 | Loss: 0.00001315
Iteration 217/1000 | Loss: 0.00001315
Iteration 218/1000 | Loss: 0.00001315
Iteration 219/1000 | Loss: 0.00001315
Iteration 220/1000 | Loss: 0.00001315
Iteration 221/1000 | Loss: 0.00001315
Iteration 222/1000 | Loss: 0.00001315
Iteration 223/1000 | Loss: 0.00001315
Iteration 224/1000 | Loss: 0.00001315
Iteration 225/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.3146482160664164e-05, 1.3146482160664164e-05, 1.3146482160664164e-05, 1.3146482160664164e-05, 1.3146482160664164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3146482160664164e-05

Optimization complete. Final v2v error: 3.042762279510498 mm

Highest mean error: 3.531618595123291 mm for frame 195

Lowest mean error: 2.6249749660491943 mm for frame 15

Saving results

Total time: 77.38842606544495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785553
Iteration 2/25 | Loss: 0.00125097
Iteration 3/25 | Loss: 0.00109538
Iteration 4/25 | Loss: 0.00107853
Iteration 5/25 | Loss: 0.00107487
Iteration 6/25 | Loss: 0.00107466
Iteration 7/25 | Loss: 0.00107466
Iteration 8/25 | Loss: 0.00107466
Iteration 9/25 | Loss: 0.00107466
Iteration 10/25 | Loss: 0.00107466
Iteration 11/25 | Loss: 0.00107466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010746556799858809, 0.0010746556799858809, 0.0010746556799858809, 0.0010746556799858809, 0.0010746556799858809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010746556799858809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34849095
Iteration 2/25 | Loss: 0.00082022
Iteration 3/25 | Loss: 0.00082022
Iteration 4/25 | Loss: 0.00082022
Iteration 5/25 | Loss: 0.00082022
Iteration 6/25 | Loss: 0.00082022
Iteration 7/25 | Loss: 0.00082022
Iteration 8/25 | Loss: 0.00082022
Iteration 9/25 | Loss: 0.00082022
Iteration 10/25 | Loss: 0.00082022
Iteration 11/25 | Loss: 0.00082022
Iteration 12/25 | Loss: 0.00082022
Iteration 13/25 | Loss: 0.00082022
Iteration 14/25 | Loss: 0.00082022
Iteration 15/25 | Loss: 0.00082022
Iteration 16/25 | Loss: 0.00082022
Iteration 17/25 | Loss: 0.00082022
Iteration 18/25 | Loss: 0.00082022
Iteration 19/25 | Loss: 0.00082022
Iteration 20/25 | Loss: 0.00082022
Iteration 21/25 | Loss: 0.00082022
Iteration 22/25 | Loss: 0.00082022
Iteration 23/25 | Loss: 0.00082022
Iteration 24/25 | Loss: 0.00082022
Iteration 25/25 | Loss: 0.00082022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082022
Iteration 2/1000 | Loss: 0.00002448
Iteration 3/1000 | Loss: 0.00001509
Iteration 4/1000 | Loss: 0.00001247
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001101
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00000990
Iteration 11/1000 | Loss: 0.00000987
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000983
Iteration 14/1000 | Loss: 0.00000979
Iteration 15/1000 | Loss: 0.00000977
Iteration 16/1000 | Loss: 0.00000972
Iteration 17/1000 | Loss: 0.00000971
Iteration 18/1000 | Loss: 0.00000970
Iteration 19/1000 | Loss: 0.00000970
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000967
Iteration 22/1000 | Loss: 0.00000961
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000960
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000958
Iteration 28/1000 | Loss: 0.00000957
Iteration 29/1000 | Loss: 0.00000957
Iteration 30/1000 | Loss: 0.00000957
Iteration 31/1000 | Loss: 0.00000956
Iteration 32/1000 | Loss: 0.00000955
Iteration 33/1000 | Loss: 0.00000955
Iteration 34/1000 | Loss: 0.00000954
Iteration 35/1000 | Loss: 0.00000954
Iteration 36/1000 | Loss: 0.00000953
Iteration 37/1000 | Loss: 0.00000953
Iteration 38/1000 | Loss: 0.00000953
Iteration 39/1000 | Loss: 0.00000953
Iteration 40/1000 | Loss: 0.00000952
Iteration 41/1000 | Loss: 0.00000952
Iteration 42/1000 | Loss: 0.00000952
Iteration 43/1000 | Loss: 0.00000952
Iteration 44/1000 | Loss: 0.00000952
Iteration 45/1000 | Loss: 0.00000951
Iteration 46/1000 | Loss: 0.00000951
Iteration 47/1000 | Loss: 0.00000951
Iteration 48/1000 | Loss: 0.00000951
Iteration 49/1000 | Loss: 0.00000951
Iteration 50/1000 | Loss: 0.00000951
Iteration 51/1000 | Loss: 0.00000951
Iteration 52/1000 | Loss: 0.00000951
Iteration 53/1000 | Loss: 0.00000951
Iteration 54/1000 | Loss: 0.00000951
Iteration 55/1000 | Loss: 0.00000951
Iteration 56/1000 | Loss: 0.00000950
Iteration 57/1000 | Loss: 0.00000950
Iteration 58/1000 | Loss: 0.00000950
Iteration 59/1000 | Loss: 0.00000950
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000948
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000948
Iteration 69/1000 | Loss: 0.00000946
Iteration 70/1000 | Loss: 0.00000946
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000945
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000945
Iteration 77/1000 | Loss: 0.00000944
Iteration 78/1000 | Loss: 0.00000944
Iteration 79/1000 | Loss: 0.00000944
Iteration 80/1000 | Loss: 0.00000944
Iteration 81/1000 | Loss: 0.00000944
Iteration 82/1000 | Loss: 0.00000943
Iteration 83/1000 | Loss: 0.00000943
Iteration 84/1000 | Loss: 0.00000943
Iteration 85/1000 | Loss: 0.00000943
Iteration 86/1000 | Loss: 0.00000943
Iteration 87/1000 | Loss: 0.00000942
Iteration 88/1000 | Loss: 0.00000942
Iteration 89/1000 | Loss: 0.00000942
Iteration 90/1000 | Loss: 0.00000941
Iteration 91/1000 | Loss: 0.00000941
Iteration 92/1000 | Loss: 0.00000940
Iteration 93/1000 | Loss: 0.00000940
Iteration 94/1000 | Loss: 0.00000940
Iteration 95/1000 | Loss: 0.00000939
Iteration 96/1000 | Loss: 0.00000939
Iteration 97/1000 | Loss: 0.00000939
Iteration 98/1000 | Loss: 0.00000939
Iteration 99/1000 | Loss: 0.00000939
Iteration 100/1000 | Loss: 0.00000938
Iteration 101/1000 | Loss: 0.00000938
Iteration 102/1000 | Loss: 0.00000938
Iteration 103/1000 | Loss: 0.00000938
Iteration 104/1000 | Loss: 0.00000937
Iteration 105/1000 | Loss: 0.00000937
Iteration 106/1000 | Loss: 0.00000937
Iteration 107/1000 | Loss: 0.00000937
Iteration 108/1000 | Loss: 0.00000937
Iteration 109/1000 | Loss: 0.00000937
Iteration 110/1000 | Loss: 0.00000937
Iteration 111/1000 | Loss: 0.00000937
Iteration 112/1000 | Loss: 0.00000937
Iteration 113/1000 | Loss: 0.00000937
Iteration 114/1000 | Loss: 0.00000936
Iteration 115/1000 | Loss: 0.00000936
Iteration 116/1000 | Loss: 0.00000936
Iteration 117/1000 | Loss: 0.00000936
Iteration 118/1000 | Loss: 0.00000936
Iteration 119/1000 | Loss: 0.00000936
Iteration 120/1000 | Loss: 0.00000936
Iteration 121/1000 | Loss: 0.00000936
Iteration 122/1000 | Loss: 0.00000936
Iteration 123/1000 | Loss: 0.00000935
Iteration 124/1000 | Loss: 0.00000935
Iteration 125/1000 | Loss: 0.00000935
Iteration 126/1000 | Loss: 0.00000935
Iteration 127/1000 | Loss: 0.00000935
Iteration 128/1000 | Loss: 0.00000935
Iteration 129/1000 | Loss: 0.00000935
Iteration 130/1000 | Loss: 0.00000935
Iteration 131/1000 | Loss: 0.00000935
Iteration 132/1000 | Loss: 0.00000935
Iteration 133/1000 | Loss: 0.00000934
Iteration 134/1000 | Loss: 0.00000934
Iteration 135/1000 | Loss: 0.00000934
Iteration 136/1000 | Loss: 0.00000934
Iteration 137/1000 | Loss: 0.00000934
Iteration 138/1000 | Loss: 0.00000934
Iteration 139/1000 | Loss: 0.00000934
Iteration 140/1000 | Loss: 0.00000934
Iteration 141/1000 | Loss: 0.00000934
Iteration 142/1000 | Loss: 0.00000934
Iteration 143/1000 | Loss: 0.00000933
Iteration 144/1000 | Loss: 0.00000933
Iteration 145/1000 | Loss: 0.00000933
Iteration 146/1000 | Loss: 0.00000933
Iteration 147/1000 | Loss: 0.00000933
Iteration 148/1000 | Loss: 0.00000932
Iteration 149/1000 | Loss: 0.00000932
Iteration 150/1000 | Loss: 0.00000932
Iteration 151/1000 | Loss: 0.00000932
Iteration 152/1000 | Loss: 0.00000932
Iteration 153/1000 | Loss: 0.00000932
Iteration 154/1000 | Loss: 0.00000932
Iteration 155/1000 | Loss: 0.00000932
Iteration 156/1000 | Loss: 0.00000932
Iteration 157/1000 | Loss: 0.00000932
Iteration 158/1000 | Loss: 0.00000932
Iteration 159/1000 | Loss: 0.00000932
Iteration 160/1000 | Loss: 0.00000932
Iteration 161/1000 | Loss: 0.00000932
Iteration 162/1000 | Loss: 0.00000931
Iteration 163/1000 | Loss: 0.00000931
Iteration 164/1000 | Loss: 0.00000931
Iteration 165/1000 | Loss: 0.00000931
Iteration 166/1000 | Loss: 0.00000931
Iteration 167/1000 | Loss: 0.00000930
Iteration 168/1000 | Loss: 0.00000930
Iteration 169/1000 | Loss: 0.00000930
Iteration 170/1000 | Loss: 0.00000930
Iteration 171/1000 | Loss: 0.00000930
Iteration 172/1000 | Loss: 0.00000930
Iteration 173/1000 | Loss: 0.00000930
Iteration 174/1000 | Loss: 0.00000929
Iteration 175/1000 | Loss: 0.00000929
Iteration 176/1000 | Loss: 0.00000929
Iteration 177/1000 | Loss: 0.00000929
Iteration 178/1000 | Loss: 0.00000929
Iteration 179/1000 | Loss: 0.00000929
Iteration 180/1000 | Loss: 0.00000929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [9.29258021642454e-06, 9.29258021642454e-06, 9.29258021642454e-06, 9.29258021642454e-06, 9.29258021642454e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.29258021642454e-06

Optimization complete. Final v2v error: 2.5921895503997803 mm

Highest mean error: 2.838365316390991 mm for frame 88

Lowest mean error: 2.407663106918335 mm for frame 18

Saving results

Total time: 42.490142822265625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063665
Iteration 2/25 | Loss: 0.00151928
Iteration 3/25 | Loss: 0.00126236
Iteration 4/25 | Loss: 0.00115411
Iteration 5/25 | Loss: 0.00114417
Iteration 6/25 | Loss: 0.00112342
Iteration 7/25 | Loss: 0.00112635
Iteration 8/25 | Loss: 0.00112535
Iteration 9/25 | Loss: 0.00111986
Iteration 10/25 | Loss: 0.00111917
Iteration 11/25 | Loss: 0.00111901
Iteration 12/25 | Loss: 0.00111901
Iteration 13/25 | Loss: 0.00111898
Iteration 14/25 | Loss: 0.00111898
Iteration 15/25 | Loss: 0.00111898
Iteration 16/25 | Loss: 0.00111898
Iteration 17/25 | Loss: 0.00111898
Iteration 18/25 | Loss: 0.00111898
Iteration 19/25 | Loss: 0.00111898
Iteration 20/25 | Loss: 0.00111897
Iteration 21/25 | Loss: 0.00111897
Iteration 22/25 | Loss: 0.00111897
Iteration 23/25 | Loss: 0.00111897
Iteration 24/25 | Loss: 0.00111897
Iteration 25/25 | Loss: 0.00111897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.42982960
Iteration 2/25 | Loss: 0.00088069
Iteration 3/25 | Loss: 0.00088069
Iteration 4/25 | Loss: 0.00088069
Iteration 5/25 | Loss: 0.00088069
Iteration 6/25 | Loss: 0.00088069
Iteration 7/25 | Loss: 0.00088069
Iteration 8/25 | Loss: 0.00088069
Iteration 9/25 | Loss: 0.00088069
Iteration 10/25 | Loss: 0.00088069
Iteration 11/25 | Loss: 0.00088069
Iteration 12/25 | Loss: 0.00088069
Iteration 13/25 | Loss: 0.00088069
Iteration 14/25 | Loss: 0.00088069
Iteration 15/25 | Loss: 0.00088069
Iteration 16/25 | Loss: 0.00088069
Iteration 17/25 | Loss: 0.00088069
Iteration 18/25 | Loss: 0.00088069
Iteration 19/25 | Loss: 0.00088069
Iteration 20/25 | Loss: 0.00088069
Iteration 21/25 | Loss: 0.00088069
Iteration 22/25 | Loss: 0.00088069
Iteration 23/25 | Loss: 0.00088069
Iteration 24/25 | Loss: 0.00088069
Iteration 25/25 | Loss: 0.00088069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088069
Iteration 2/1000 | Loss: 0.00002154
Iteration 3/1000 | Loss: 0.00007410
Iteration 4/1000 | Loss: 0.00012364
Iteration 5/1000 | Loss: 0.00001451
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001248
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001194
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00009104
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001123
Iteration 15/1000 | Loss: 0.00001121
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001109
Iteration 26/1000 | Loss: 0.00001109
Iteration 27/1000 | Loss: 0.00009234
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00003598
Iteration 30/1000 | Loss: 0.00001108
Iteration 31/1000 | Loss: 0.00015049
Iteration 32/1000 | Loss: 0.00017674
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001096
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001096
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001092
Iteration 53/1000 | Loss: 0.00014833
Iteration 54/1000 | Loss: 0.00001594
Iteration 55/1000 | Loss: 0.00001313
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001062
Iteration 63/1000 | Loss: 0.00001045
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001036
Iteration 67/1000 | Loss: 0.00001033
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001029
Iteration 70/1000 | Loss: 0.00001029
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001024
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001021
Iteration 81/1000 | Loss: 0.00001021
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001020
Iteration 100/1000 | Loss: 0.00001020
Iteration 101/1000 | Loss: 0.00001020
Iteration 102/1000 | Loss: 0.00001020
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001020
Iteration 105/1000 | Loss: 0.00001020
Iteration 106/1000 | Loss: 0.00001020
Iteration 107/1000 | Loss: 0.00001020
Iteration 108/1000 | Loss: 0.00001020
Iteration 109/1000 | Loss: 0.00001020
Iteration 110/1000 | Loss: 0.00001020
Iteration 111/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.0204962563875597e-05, 1.0204962563875597e-05, 1.0204962563875597e-05, 1.0204962563875597e-05, 1.0204962563875597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0204962563875597e-05

Optimization complete. Final v2v error: 2.6840851306915283 mm

Highest mean error: 5.103522300720215 mm for frame 95

Lowest mean error: 2.483973741531372 mm for frame 160

Saving results

Total time: 76.77928805351257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402236
Iteration 2/25 | Loss: 0.00119386
Iteration 3/25 | Loss: 0.00108743
Iteration 4/25 | Loss: 0.00107563
Iteration 5/25 | Loss: 0.00107311
Iteration 6/25 | Loss: 0.00107254
Iteration 7/25 | Loss: 0.00107254
Iteration 8/25 | Loss: 0.00107254
Iteration 9/25 | Loss: 0.00107254
Iteration 10/25 | Loss: 0.00107254
Iteration 11/25 | Loss: 0.00107254
Iteration 12/25 | Loss: 0.00107254
Iteration 13/25 | Loss: 0.00107254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010725362226366997, 0.0010725362226366997, 0.0010725362226366997, 0.0010725362226366997, 0.0010725362226366997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010725362226366997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43977809
Iteration 2/25 | Loss: 0.00087209
Iteration 3/25 | Loss: 0.00087209
Iteration 4/25 | Loss: 0.00087209
Iteration 5/25 | Loss: 0.00087209
Iteration 6/25 | Loss: 0.00087209
Iteration 7/25 | Loss: 0.00087208
Iteration 8/25 | Loss: 0.00087208
Iteration 9/25 | Loss: 0.00087208
Iteration 10/25 | Loss: 0.00087208
Iteration 11/25 | Loss: 0.00087208
Iteration 12/25 | Loss: 0.00087208
Iteration 13/25 | Loss: 0.00087208
Iteration 14/25 | Loss: 0.00087208
Iteration 15/25 | Loss: 0.00087208
Iteration 16/25 | Loss: 0.00087208
Iteration 17/25 | Loss: 0.00087208
Iteration 18/25 | Loss: 0.00087208
Iteration 19/25 | Loss: 0.00087208
Iteration 20/25 | Loss: 0.00087208
Iteration 21/25 | Loss: 0.00087208
Iteration 22/25 | Loss: 0.00087208
Iteration 23/25 | Loss: 0.00087208
Iteration 24/25 | Loss: 0.00087208
Iteration 25/25 | Loss: 0.00087208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087208
Iteration 2/1000 | Loss: 0.00001961
Iteration 3/1000 | Loss: 0.00001173
Iteration 4/1000 | Loss: 0.00001047
Iteration 5/1000 | Loss: 0.00000992
Iteration 6/1000 | Loss: 0.00000964
Iteration 7/1000 | Loss: 0.00000937
Iteration 8/1000 | Loss: 0.00000926
Iteration 9/1000 | Loss: 0.00000925
Iteration 10/1000 | Loss: 0.00000915
Iteration 11/1000 | Loss: 0.00000912
Iteration 12/1000 | Loss: 0.00000905
Iteration 13/1000 | Loss: 0.00000903
Iteration 14/1000 | Loss: 0.00000903
Iteration 15/1000 | Loss: 0.00000894
Iteration 16/1000 | Loss: 0.00000893
Iteration 17/1000 | Loss: 0.00000889
Iteration 18/1000 | Loss: 0.00000887
Iteration 19/1000 | Loss: 0.00000886
Iteration 20/1000 | Loss: 0.00000884
Iteration 21/1000 | Loss: 0.00000884
Iteration 22/1000 | Loss: 0.00000875
Iteration 23/1000 | Loss: 0.00000873
Iteration 24/1000 | Loss: 0.00000870
Iteration 25/1000 | Loss: 0.00000867
Iteration 26/1000 | Loss: 0.00000866
Iteration 27/1000 | Loss: 0.00000866
Iteration 28/1000 | Loss: 0.00000860
Iteration 29/1000 | Loss: 0.00000860
Iteration 30/1000 | Loss: 0.00000859
Iteration 31/1000 | Loss: 0.00000859
Iteration 32/1000 | Loss: 0.00000858
Iteration 33/1000 | Loss: 0.00000857
Iteration 34/1000 | Loss: 0.00000856
Iteration 35/1000 | Loss: 0.00000856
Iteration 36/1000 | Loss: 0.00000855
Iteration 37/1000 | Loss: 0.00000855
Iteration 38/1000 | Loss: 0.00000855
Iteration 39/1000 | Loss: 0.00000855
Iteration 40/1000 | Loss: 0.00000854
Iteration 41/1000 | Loss: 0.00000854
Iteration 42/1000 | Loss: 0.00000852
Iteration 43/1000 | Loss: 0.00000852
Iteration 44/1000 | Loss: 0.00000852
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000851
Iteration 47/1000 | Loss: 0.00000851
Iteration 48/1000 | Loss: 0.00000851
Iteration 49/1000 | Loss: 0.00000851
Iteration 50/1000 | Loss: 0.00000851
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000850
Iteration 53/1000 | Loss: 0.00000849
Iteration 54/1000 | Loss: 0.00000849
Iteration 55/1000 | Loss: 0.00000849
Iteration 56/1000 | Loss: 0.00000849
Iteration 57/1000 | Loss: 0.00000849
Iteration 58/1000 | Loss: 0.00000849
Iteration 59/1000 | Loss: 0.00000849
Iteration 60/1000 | Loss: 0.00000848
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000848
Iteration 63/1000 | Loss: 0.00000848
Iteration 64/1000 | Loss: 0.00000848
Iteration 65/1000 | Loss: 0.00000848
Iteration 66/1000 | Loss: 0.00000848
Iteration 67/1000 | Loss: 0.00000847
Iteration 68/1000 | Loss: 0.00000847
Iteration 69/1000 | Loss: 0.00000847
Iteration 70/1000 | Loss: 0.00000847
Iteration 71/1000 | Loss: 0.00000847
Iteration 72/1000 | Loss: 0.00000846
Iteration 73/1000 | Loss: 0.00000846
Iteration 74/1000 | Loss: 0.00000846
Iteration 75/1000 | Loss: 0.00000846
Iteration 76/1000 | Loss: 0.00000846
Iteration 77/1000 | Loss: 0.00000846
Iteration 78/1000 | Loss: 0.00000846
Iteration 79/1000 | Loss: 0.00000845
Iteration 80/1000 | Loss: 0.00000845
Iteration 81/1000 | Loss: 0.00000845
Iteration 82/1000 | Loss: 0.00000845
Iteration 83/1000 | Loss: 0.00000845
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000845
Iteration 86/1000 | Loss: 0.00000845
Iteration 87/1000 | Loss: 0.00000845
Iteration 88/1000 | Loss: 0.00000845
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000843
Iteration 97/1000 | Loss: 0.00000843
Iteration 98/1000 | Loss: 0.00000843
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000842
Iteration 101/1000 | Loss: 0.00000842
Iteration 102/1000 | Loss: 0.00000842
Iteration 103/1000 | Loss: 0.00000842
Iteration 104/1000 | Loss: 0.00000842
Iteration 105/1000 | Loss: 0.00000842
Iteration 106/1000 | Loss: 0.00000841
Iteration 107/1000 | Loss: 0.00000841
Iteration 108/1000 | Loss: 0.00000841
Iteration 109/1000 | Loss: 0.00000841
Iteration 110/1000 | Loss: 0.00000840
Iteration 111/1000 | Loss: 0.00000840
Iteration 112/1000 | Loss: 0.00000840
Iteration 113/1000 | Loss: 0.00000840
Iteration 114/1000 | Loss: 0.00000840
Iteration 115/1000 | Loss: 0.00000840
Iteration 116/1000 | Loss: 0.00000839
Iteration 117/1000 | Loss: 0.00000839
Iteration 118/1000 | Loss: 0.00000839
Iteration 119/1000 | Loss: 0.00000839
Iteration 120/1000 | Loss: 0.00000838
Iteration 121/1000 | Loss: 0.00000838
Iteration 122/1000 | Loss: 0.00000837
Iteration 123/1000 | Loss: 0.00000837
Iteration 124/1000 | Loss: 0.00000837
Iteration 125/1000 | Loss: 0.00000836
Iteration 126/1000 | Loss: 0.00000836
Iteration 127/1000 | Loss: 0.00000836
Iteration 128/1000 | Loss: 0.00000835
Iteration 129/1000 | Loss: 0.00000835
Iteration 130/1000 | Loss: 0.00000835
Iteration 131/1000 | Loss: 0.00000834
Iteration 132/1000 | Loss: 0.00000834
Iteration 133/1000 | Loss: 0.00000834
Iteration 134/1000 | Loss: 0.00000834
Iteration 135/1000 | Loss: 0.00000834
Iteration 136/1000 | Loss: 0.00000834
Iteration 137/1000 | Loss: 0.00000833
Iteration 138/1000 | Loss: 0.00000833
Iteration 139/1000 | Loss: 0.00000833
Iteration 140/1000 | Loss: 0.00000833
Iteration 141/1000 | Loss: 0.00000832
Iteration 142/1000 | Loss: 0.00000832
Iteration 143/1000 | Loss: 0.00000832
Iteration 144/1000 | Loss: 0.00000831
Iteration 145/1000 | Loss: 0.00000831
Iteration 146/1000 | Loss: 0.00000831
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000830
Iteration 151/1000 | Loss: 0.00000830
Iteration 152/1000 | Loss: 0.00000830
Iteration 153/1000 | Loss: 0.00000830
Iteration 154/1000 | Loss: 0.00000830
Iteration 155/1000 | Loss: 0.00000830
Iteration 156/1000 | Loss: 0.00000829
Iteration 157/1000 | Loss: 0.00000829
Iteration 158/1000 | Loss: 0.00000829
Iteration 159/1000 | Loss: 0.00000829
Iteration 160/1000 | Loss: 0.00000829
Iteration 161/1000 | Loss: 0.00000828
Iteration 162/1000 | Loss: 0.00000828
Iteration 163/1000 | Loss: 0.00000828
Iteration 164/1000 | Loss: 0.00000828
Iteration 165/1000 | Loss: 0.00000828
Iteration 166/1000 | Loss: 0.00000828
Iteration 167/1000 | Loss: 0.00000828
Iteration 168/1000 | Loss: 0.00000828
Iteration 169/1000 | Loss: 0.00000828
Iteration 170/1000 | Loss: 0.00000828
Iteration 171/1000 | Loss: 0.00000828
Iteration 172/1000 | Loss: 0.00000828
Iteration 173/1000 | Loss: 0.00000827
Iteration 174/1000 | Loss: 0.00000827
Iteration 175/1000 | Loss: 0.00000827
Iteration 176/1000 | Loss: 0.00000827
Iteration 177/1000 | Loss: 0.00000827
Iteration 178/1000 | Loss: 0.00000827
Iteration 179/1000 | Loss: 0.00000827
Iteration 180/1000 | Loss: 0.00000827
Iteration 181/1000 | Loss: 0.00000827
Iteration 182/1000 | Loss: 0.00000827
Iteration 183/1000 | Loss: 0.00000827
Iteration 184/1000 | Loss: 0.00000826
Iteration 185/1000 | Loss: 0.00000826
Iteration 186/1000 | Loss: 0.00000826
Iteration 187/1000 | Loss: 0.00000826
Iteration 188/1000 | Loss: 0.00000826
Iteration 189/1000 | Loss: 0.00000826
Iteration 190/1000 | Loss: 0.00000826
Iteration 191/1000 | Loss: 0.00000826
Iteration 192/1000 | Loss: 0.00000825
Iteration 193/1000 | Loss: 0.00000825
Iteration 194/1000 | Loss: 0.00000825
Iteration 195/1000 | Loss: 0.00000825
Iteration 196/1000 | Loss: 0.00000825
Iteration 197/1000 | Loss: 0.00000825
Iteration 198/1000 | Loss: 0.00000825
Iteration 199/1000 | Loss: 0.00000825
Iteration 200/1000 | Loss: 0.00000825
Iteration 201/1000 | Loss: 0.00000825
Iteration 202/1000 | Loss: 0.00000825
Iteration 203/1000 | Loss: 0.00000825
Iteration 204/1000 | Loss: 0.00000825
Iteration 205/1000 | Loss: 0.00000825
Iteration 206/1000 | Loss: 0.00000825
Iteration 207/1000 | Loss: 0.00000825
Iteration 208/1000 | Loss: 0.00000825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [8.252388397522736e-06, 8.252388397522736e-06, 8.252388397522736e-06, 8.252388397522736e-06, 8.252388397522736e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.252388397522736e-06

Optimization complete. Final v2v error: 2.4787943363189697 mm

Highest mean error: 3.308619737625122 mm for frame 59

Lowest mean error: 2.1749074459075928 mm for frame 84

Saving results

Total time: 39.67282032966614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764645
Iteration 2/25 | Loss: 0.00156493
Iteration 3/25 | Loss: 0.00140252
Iteration 4/25 | Loss: 0.00137544
Iteration 5/25 | Loss: 0.00136786
Iteration 6/25 | Loss: 0.00136650
Iteration 7/25 | Loss: 0.00136650
Iteration 8/25 | Loss: 0.00136650
Iteration 9/25 | Loss: 0.00136650
Iteration 10/25 | Loss: 0.00136650
Iteration 11/25 | Loss: 0.00136650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001366496435366571, 0.001366496435366571, 0.001366496435366571, 0.001366496435366571, 0.001366496435366571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001366496435366571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93574584
Iteration 2/25 | Loss: 0.00129671
Iteration 3/25 | Loss: 0.00129671
Iteration 4/25 | Loss: 0.00129671
Iteration 5/25 | Loss: 0.00129671
Iteration 6/25 | Loss: 0.00129671
Iteration 7/25 | Loss: 0.00129671
Iteration 8/25 | Loss: 0.00129671
Iteration 9/25 | Loss: 0.00129671
Iteration 10/25 | Loss: 0.00129671
Iteration 11/25 | Loss: 0.00129671
Iteration 12/25 | Loss: 0.00129671
Iteration 13/25 | Loss: 0.00129671
Iteration 14/25 | Loss: 0.00129671
Iteration 15/25 | Loss: 0.00129671
Iteration 16/25 | Loss: 0.00129671
Iteration 17/25 | Loss: 0.00129671
Iteration 18/25 | Loss: 0.00129671
Iteration 19/25 | Loss: 0.00129671
Iteration 20/25 | Loss: 0.00129671
Iteration 21/25 | Loss: 0.00129671
Iteration 22/25 | Loss: 0.00129671
Iteration 23/25 | Loss: 0.00129671
Iteration 24/25 | Loss: 0.00129671
Iteration 25/25 | Loss: 0.00129671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129671
Iteration 2/1000 | Loss: 0.00009033
Iteration 3/1000 | Loss: 0.00005867
Iteration 4/1000 | Loss: 0.00005158
Iteration 5/1000 | Loss: 0.00004963
Iteration 6/1000 | Loss: 0.00004770
Iteration 7/1000 | Loss: 0.00004665
Iteration 8/1000 | Loss: 0.00004598
Iteration 9/1000 | Loss: 0.00004550
Iteration 10/1000 | Loss: 0.00004507
Iteration 11/1000 | Loss: 0.00004472
Iteration 12/1000 | Loss: 0.00004448
Iteration 13/1000 | Loss: 0.00004418
Iteration 14/1000 | Loss: 0.00004392
Iteration 15/1000 | Loss: 0.00004373
Iteration 16/1000 | Loss: 0.00004369
Iteration 17/1000 | Loss: 0.00004364
Iteration 18/1000 | Loss: 0.00004352
Iteration 19/1000 | Loss: 0.00004347
Iteration 20/1000 | Loss: 0.00004333
Iteration 21/1000 | Loss: 0.00004328
Iteration 22/1000 | Loss: 0.00004322
Iteration 23/1000 | Loss: 0.00004321
Iteration 24/1000 | Loss: 0.00004320
Iteration 25/1000 | Loss: 0.00004319
Iteration 26/1000 | Loss: 0.00004318
Iteration 27/1000 | Loss: 0.00004318
Iteration 28/1000 | Loss: 0.00004317
Iteration 29/1000 | Loss: 0.00004317
Iteration 30/1000 | Loss: 0.00004316
Iteration 31/1000 | Loss: 0.00004316
Iteration 32/1000 | Loss: 0.00004315
Iteration 33/1000 | Loss: 0.00004314
Iteration 34/1000 | Loss: 0.00004314
Iteration 35/1000 | Loss: 0.00004312
Iteration 36/1000 | Loss: 0.00004312
Iteration 37/1000 | Loss: 0.00004311
Iteration 38/1000 | Loss: 0.00004308
Iteration 39/1000 | Loss: 0.00004308
Iteration 40/1000 | Loss: 0.00004302
Iteration 41/1000 | Loss: 0.00004302
Iteration 42/1000 | Loss: 0.00004302
Iteration 43/1000 | Loss: 0.00004301
Iteration 44/1000 | Loss: 0.00004301
Iteration 45/1000 | Loss: 0.00004301
Iteration 46/1000 | Loss: 0.00004300
Iteration 47/1000 | Loss: 0.00004300
Iteration 48/1000 | Loss: 0.00004299
Iteration 49/1000 | Loss: 0.00004299
Iteration 50/1000 | Loss: 0.00004299
Iteration 51/1000 | Loss: 0.00004299
Iteration 52/1000 | Loss: 0.00004299
Iteration 53/1000 | Loss: 0.00004299
Iteration 54/1000 | Loss: 0.00004299
Iteration 55/1000 | Loss: 0.00004299
Iteration 56/1000 | Loss: 0.00004299
Iteration 57/1000 | Loss: 0.00004299
Iteration 58/1000 | Loss: 0.00004298
Iteration 59/1000 | Loss: 0.00004298
Iteration 60/1000 | Loss: 0.00004298
Iteration 61/1000 | Loss: 0.00004298
Iteration 62/1000 | Loss: 0.00004298
Iteration 63/1000 | Loss: 0.00004297
Iteration 64/1000 | Loss: 0.00004297
Iteration 65/1000 | Loss: 0.00004297
Iteration 66/1000 | Loss: 0.00004296
Iteration 67/1000 | Loss: 0.00004296
Iteration 68/1000 | Loss: 0.00004296
Iteration 69/1000 | Loss: 0.00004296
Iteration 70/1000 | Loss: 0.00004296
Iteration 71/1000 | Loss: 0.00004296
Iteration 72/1000 | Loss: 0.00004296
Iteration 73/1000 | Loss: 0.00004296
Iteration 74/1000 | Loss: 0.00004295
Iteration 75/1000 | Loss: 0.00004295
Iteration 76/1000 | Loss: 0.00004295
Iteration 77/1000 | Loss: 0.00004295
Iteration 78/1000 | Loss: 0.00004295
Iteration 79/1000 | Loss: 0.00004295
Iteration 80/1000 | Loss: 0.00004295
Iteration 81/1000 | Loss: 0.00004295
Iteration 82/1000 | Loss: 0.00004295
Iteration 83/1000 | Loss: 0.00004295
Iteration 84/1000 | Loss: 0.00004295
Iteration 85/1000 | Loss: 0.00004294
Iteration 86/1000 | Loss: 0.00004292
Iteration 87/1000 | Loss: 0.00004292
Iteration 88/1000 | Loss: 0.00004292
Iteration 89/1000 | Loss: 0.00004292
Iteration 90/1000 | Loss: 0.00004292
Iteration 91/1000 | Loss: 0.00004292
Iteration 92/1000 | Loss: 0.00004292
Iteration 93/1000 | Loss: 0.00004292
Iteration 94/1000 | Loss: 0.00004292
Iteration 95/1000 | Loss: 0.00004292
Iteration 96/1000 | Loss: 0.00004291
Iteration 97/1000 | Loss: 0.00004291
Iteration 98/1000 | Loss: 0.00004291
Iteration 99/1000 | Loss: 0.00004291
Iteration 100/1000 | Loss: 0.00004291
Iteration 101/1000 | Loss: 0.00004291
Iteration 102/1000 | Loss: 0.00004291
Iteration 103/1000 | Loss: 0.00004291
Iteration 104/1000 | Loss: 0.00004290
Iteration 105/1000 | Loss: 0.00004290
Iteration 106/1000 | Loss: 0.00004290
Iteration 107/1000 | Loss: 0.00004290
Iteration 108/1000 | Loss: 0.00004290
Iteration 109/1000 | Loss: 0.00004290
Iteration 110/1000 | Loss: 0.00004290
Iteration 111/1000 | Loss: 0.00004290
Iteration 112/1000 | Loss: 0.00004290
Iteration 113/1000 | Loss: 0.00004290
Iteration 114/1000 | Loss: 0.00004290
Iteration 115/1000 | Loss: 0.00004290
Iteration 116/1000 | Loss: 0.00004289
Iteration 117/1000 | Loss: 0.00004289
Iteration 118/1000 | Loss: 0.00004289
Iteration 119/1000 | Loss: 0.00004289
Iteration 120/1000 | Loss: 0.00004289
Iteration 121/1000 | Loss: 0.00004289
Iteration 122/1000 | Loss: 0.00004289
Iteration 123/1000 | Loss: 0.00004288
Iteration 124/1000 | Loss: 0.00004288
Iteration 125/1000 | Loss: 0.00004288
Iteration 126/1000 | Loss: 0.00004288
Iteration 127/1000 | Loss: 0.00004288
Iteration 128/1000 | Loss: 0.00004288
Iteration 129/1000 | Loss: 0.00004288
Iteration 130/1000 | Loss: 0.00004288
Iteration 131/1000 | Loss: 0.00004287
Iteration 132/1000 | Loss: 0.00004287
Iteration 133/1000 | Loss: 0.00004287
Iteration 134/1000 | Loss: 0.00004287
Iteration 135/1000 | Loss: 0.00004287
Iteration 136/1000 | Loss: 0.00004287
Iteration 137/1000 | Loss: 0.00004287
Iteration 138/1000 | Loss: 0.00004287
Iteration 139/1000 | Loss: 0.00004287
Iteration 140/1000 | Loss: 0.00004287
Iteration 141/1000 | Loss: 0.00004287
Iteration 142/1000 | Loss: 0.00004286
Iteration 143/1000 | Loss: 0.00004286
Iteration 144/1000 | Loss: 0.00004286
Iteration 145/1000 | Loss: 0.00004286
Iteration 146/1000 | Loss: 0.00004286
Iteration 147/1000 | Loss: 0.00004286
Iteration 148/1000 | Loss: 0.00004285
Iteration 149/1000 | Loss: 0.00004285
Iteration 150/1000 | Loss: 0.00004285
Iteration 151/1000 | Loss: 0.00004285
Iteration 152/1000 | Loss: 0.00004285
Iteration 153/1000 | Loss: 0.00004285
Iteration 154/1000 | Loss: 0.00004285
Iteration 155/1000 | Loss: 0.00004284
Iteration 156/1000 | Loss: 0.00004284
Iteration 157/1000 | Loss: 0.00004283
Iteration 158/1000 | Loss: 0.00004283
Iteration 159/1000 | Loss: 0.00004283
Iteration 160/1000 | Loss: 0.00004283
Iteration 161/1000 | Loss: 0.00004283
Iteration 162/1000 | Loss: 0.00004283
Iteration 163/1000 | Loss: 0.00004283
Iteration 164/1000 | Loss: 0.00004283
Iteration 165/1000 | Loss: 0.00004283
Iteration 166/1000 | Loss: 0.00004283
Iteration 167/1000 | Loss: 0.00004283
Iteration 168/1000 | Loss: 0.00004283
Iteration 169/1000 | Loss: 0.00004283
Iteration 170/1000 | Loss: 0.00004283
Iteration 171/1000 | Loss: 0.00004283
Iteration 172/1000 | Loss: 0.00004282
Iteration 173/1000 | Loss: 0.00004282
Iteration 174/1000 | Loss: 0.00004282
Iteration 175/1000 | Loss: 0.00004282
Iteration 176/1000 | Loss: 0.00004282
Iteration 177/1000 | Loss: 0.00004282
Iteration 178/1000 | Loss: 0.00004282
Iteration 179/1000 | Loss: 0.00004281
Iteration 180/1000 | Loss: 0.00004281
Iteration 181/1000 | Loss: 0.00004281
Iteration 182/1000 | Loss: 0.00004281
Iteration 183/1000 | Loss: 0.00004281
Iteration 184/1000 | Loss: 0.00004281
Iteration 185/1000 | Loss: 0.00004281
Iteration 186/1000 | Loss: 0.00004281
Iteration 187/1000 | Loss: 0.00004281
Iteration 188/1000 | Loss: 0.00004281
Iteration 189/1000 | Loss: 0.00004281
Iteration 190/1000 | Loss: 0.00004281
Iteration 191/1000 | Loss: 0.00004281
Iteration 192/1000 | Loss: 0.00004281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [4.2811912862816826e-05, 4.2811912862816826e-05, 4.2811912862816826e-05, 4.2811912862816826e-05, 4.2811912862816826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2811912862816826e-05

Optimization complete. Final v2v error: 5.341240882873535 mm

Highest mean error: 6.9766974449157715 mm for frame 66

Lowest mean error: 4.483165740966797 mm for frame 145

Saving results

Total time: 56.27414846420288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369237
Iteration 2/25 | Loss: 0.00110535
Iteration 3/25 | Loss: 0.00105305
Iteration 4/25 | Loss: 0.00104424
Iteration 5/25 | Loss: 0.00104134
Iteration 6/25 | Loss: 0.00104095
Iteration 7/25 | Loss: 0.00104095
Iteration 8/25 | Loss: 0.00104095
Iteration 9/25 | Loss: 0.00104095
Iteration 10/25 | Loss: 0.00104095
Iteration 11/25 | Loss: 0.00104095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001040946808643639, 0.001040946808643639, 0.001040946808643639, 0.001040946808643639, 0.001040946808643639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001040946808643639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44819224
Iteration 2/25 | Loss: 0.00084942
Iteration 3/25 | Loss: 0.00084942
Iteration 4/25 | Loss: 0.00084942
Iteration 5/25 | Loss: 0.00084942
Iteration 6/25 | Loss: 0.00084942
Iteration 7/25 | Loss: 0.00084942
Iteration 8/25 | Loss: 0.00084942
Iteration 9/25 | Loss: 0.00084942
Iteration 10/25 | Loss: 0.00084942
Iteration 11/25 | Loss: 0.00084942
Iteration 12/25 | Loss: 0.00084942
Iteration 13/25 | Loss: 0.00084942
Iteration 14/25 | Loss: 0.00084942
Iteration 15/25 | Loss: 0.00084942
Iteration 16/25 | Loss: 0.00084942
Iteration 17/25 | Loss: 0.00084942
Iteration 18/25 | Loss: 0.00084942
Iteration 19/25 | Loss: 0.00084942
Iteration 20/25 | Loss: 0.00084942
Iteration 21/25 | Loss: 0.00084942
Iteration 22/25 | Loss: 0.00084942
Iteration 23/25 | Loss: 0.00084942
Iteration 24/25 | Loss: 0.00084942
Iteration 25/25 | Loss: 0.00084942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084942
Iteration 2/1000 | Loss: 0.00002033
Iteration 3/1000 | Loss: 0.00001128
Iteration 4/1000 | Loss: 0.00000979
Iteration 5/1000 | Loss: 0.00000907
Iteration 6/1000 | Loss: 0.00000875
Iteration 7/1000 | Loss: 0.00000870
Iteration 8/1000 | Loss: 0.00000841
Iteration 9/1000 | Loss: 0.00000832
Iteration 10/1000 | Loss: 0.00000831
Iteration 11/1000 | Loss: 0.00000831
Iteration 12/1000 | Loss: 0.00000825
Iteration 13/1000 | Loss: 0.00000824
Iteration 14/1000 | Loss: 0.00000817
Iteration 15/1000 | Loss: 0.00000804
Iteration 16/1000 | Loss: 0.00000800
Iteration 17/1000 | Loss: 0.00000799
Iteration 18/1000 | Loss: 0.00000797
Iteration 19/1000 | Loss: 0.00000796
Iteration 20/1000 | Loss: 0.00000795
Iteration 21/1000 | Loss: 0.00000795
Iteration 22/1000 | Loss: 0.00000794
Iteration 23/1000 | Loss: 0.00000794
Iteration 24/1000 | Loss: 0.00000787
Iteration 25/1000 | Loss: 0.00000787
Iteration 26/1000 | Loss: 0.00000787
Iteration 27/1000 | Loss: 0.00000787
Iteration 28/1000 | Loss: 0.00000786
Iteration 29/1000 | Loss: 0.00000785
Iteration 30/1000 | Loss: 0.00000783
Iteration 31/1000 | Loss: 0.00000782
Iteration 32/1000 | Loss: 0.00000778
Iteration 33/1000 | Loss: 0.00000778
Iteration 34/1000 | Loss: 0.00000776
Iteration 35/1000 | Loss: 0.00000776
Iteration 36/1000 | Loss: 0.00000776
Iteration 37/1000 | Loss: 0.00000776
Iteration 38/1000 | Loss: 0.00000776
Iteration 39/1000 | Loss: 0.00000776
Iteration 40/1000 | Loss: 0.00000776
Iteration 41/1000 | Loss: 0.00000776
Iteration 42/1000 | Loss: 0.00000776
Iteration 43/1000 | Loss: 0.00000776
Iteration 44/1000 | Loss: 0.00000776
Iteration 45/1000 | Loss: 0.00000776
Iteration 46/1000 | Loss: 0.00000776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [7.756766535749193e-06, 7.756766535749193e-06, 7.756766535749193e-06, 7.756766535749193e-06, 7.756766535749193e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.756766535749193e-06

Optimization complete. Final v2v error: 2.4227402210235596 mm

Highest mean error: 2.597644090652466 mm for frame 51

Lowest mean error: 2.2923471927642822 mm for frame 32

Saving results

Total time: 26.204108953475952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00267368
Iteration 2/25 | Loss: 0.00126900
Iteration 3/25 | Loss: 0.00107405
Iteration 4/25 | Loss: 0.00104919
Iteration 5/25 | Loss: 0.00104321
Iteration 6/25 | Loss: 0.00104036
Iteration 7/25 | Loss: 0.00103949
Iteration 8/25 | Loss: 0.00103937
Iteration 9/25 | Loss: 0.00103937
Iteration 10/25 | Loss: 0.00103937
Iteration 11/25 | Loss: 0.00103937
Iteration 12/25 | Loss: 0.00103937
Iteration 13/25 | Loss: 0.00103937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010393736883997917, 0.0010393736883997917, 0.0010393736883997917, 0.0010393736883997917, 0.0010393736883997917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010393736883997917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33840764
Iteration 2/25 | Loss: 0.00101704
Iteration 3/25 | Loss: 0.00101703
Iteration 4/25 | Loss: 0.00101703
Iteration 5/25 | Loss: 0.00101703
Iteration 6/25 | Loss: 0.00101703
Iteration 7/25 | Loss: 0.00101703
Iteration 8/25 | Loss: 0.00101703
Iteration 9/25 | Loss: 0.00101703
Iteration 10/25 | Loss: 0.00101703
Iteration 11/25 | Loss: 0.00101703
Iteration 12/25 | Loss: 0.00101703
Iteration 13/25 | Loss: 0.00101703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010170318419113755, 0.0010170318419113755, 0.0010170318419113755, 0.0010170318419113755, 0.0010170318419113755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010170318419113755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101703
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00001576
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001170
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001038
Iteration 8/1000 | Loss: 0.00001000
Iteration 9/1000 | Loss: 0.00000971
Iteration 10/1000 | Loss: 0.00000946
Iteration 11/1000 | Loss: 0.00000941
Iteration 12/1000 | Loss: 0.00000932
Iteration 13/1000 | Loss: 0.00000929
Iteration 14/1000 | Loss: 0.00000924
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000909
Iteration 17/1000 | Loss: 0.00000907
Iteration 18/1000 | Loss: 0.00000906
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000905
Iteration 21/1000 | Loss: 0.00000904
Iteration 22/1000 | Loss: 0.00000903
Iteration 23/1000 | Loss: 0.00000902
Iteration 24/1000 | Loss: 0.00000902
Iteration 25/1000 | Loss: 0.00000901
Iteration 26/1000 | Loss: 0.00000901
Iteration 27/1000 | Loss: 0.00000899
Iteration 28/1000 | Loss: 0.00000899
Iteration 29/1000 | Loss: 0.00000899
Iteration 30/1000 | Loss: 0.00000899
Iteration 31/1000 | Loss: 0.00000899
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000899
Iteration 34/1000 | Loss: 0.00000899
Iteration 35/1000 | Loss: 0.00000899
Iteration 36/1000 | Loss: 0.00000898
Iteration 37/1000 | Loss: 0.00000898
Iteration 38/1000 | Loss: 0.00000898
Iteration 39/1000 | Loss: 0.00000898
Iteration 40/1000 | Loss: 0.00000898
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000898
Iteration 43/1000 | Loss: 0.00000898
Iteration 44/1000 | Loss: 0.00000898
Iteration 45/1000 | Loss: 0.00000897
Iteration 46/1000 | Loss: 0.00000897
Iteration 47/1000 | Loss: 0.00000897
Iteration 48/1000 | Loss: 0.00000897
Iteration 49/1000 | Loss: 0.00000895
Iteration 50/1000 | Loss: 0.00000894
Iteration 51/1000 | Loss: 0.00000894
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000893
Iteration 54/1000 | Loss: 0.00000893
Iteration 55/1000 | Loss: 0.00000893
Iteration 56/1000 | Loss: 0.00000893
Iteration 57/1000 | Loss: 0.00000893
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000892
Iteration 60/1000 | Loss: 0.00000892
Iteration 61/1000 | Loss: 0.00000892
Iteration 62/1000 | Loss: 0.00000892
Iteration 63/1000 | Loss: 0.00000892
Iteration 64/1000 | Loss: 0.00000892
Iteration 65/1000 | Loss: 0.00000891
Iteration 66/1000 | Loss: 0.00000891
Iteration 67/1000 | Loss: 0.00000891
Iteration 68/1000 | Loss: 0.00000891
Iteration 69/1000 | Loss: 0.00000891
Iteration 70/1000 | Loss: 0.00000890
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000889
Iteration 77/1000 | Loss: 0.00000889
Iteration 78/1000 | Loss: 0.00000889
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000888
Iteration 83/1000 | Loss: 0.00000888
Iteration 84/1000 | Loss: 0.00000888
Iteration 85/1000 | Loss: 0.00000888
Iteration 86/1000 | Loss: 0.00000888
Iteration 87/1000 | Loss: 0.00000887
Iteration 88/1000 | Loss: 0.00000887
Iteration 89/1000 | Loss: 0.00000887
Iteration 90/1000 | Loss: 0.00000887
Iteration 91/1000 | Loss: 0.00000887
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000887
Iteration 95/1000 | Loss: 0.00000887
Iteration 96/1000 | Loss: 0.00000887
Iteration 97/1000 | Loss: 0.00000887
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000886
Iteration 101/1000 | Loss: 0.00000885
Iteration 102/1000 | Loss: 0.00000885
Iteration 103/1000 | Loss: 0.00000885
Iteration 104/1000 | Loss: 0.00000884
Iteration 105/1000 | Loss: 0.00000884
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000884
Iteration 108/1000 | Loss: 0.00000884
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000883
Iteration 113/1000 | Loss: 0.00000882
Iteration 114/1000 | Loss: 0.00000882
Iteration 115/1000 | Loss: 0.00000882
Iteration 116/1000 | Loss: 0.00000881
Iteration 117/1000 | Loss: 0.00000881
Iteration 118/1000 | Loss: 0.00000881
Iteration 119/1000 | Loss: 0.00000881
Iteration 120/1000 | Loss: 0.00000880
Iteration 121/1000 | Loss: 0.00000880
Iteration 122/1000 | Loss: 0.00000880
Iteration 123/1000 | Loss: 0.00000879
Iteration 124/1000 | Loss: 0.00000879
Iteration 125/1000 | Loss: 0.00000879
Iteration 126/1000 | Loss: 0.00000878
Iteration 127/1000 | Loss: 0.00000878
Iteration 128/1000 | Loss: 0.00000877
Iteration 129/1000 | Loss: 0.00000877
Iteration 130/1000 | Loss: 0.00000877
Iteration 131/1000 | Loss: 0.00000876
Iteration 132/1000 | Loss: 0.00000876
Iteration 133/1000 | Loss: 0.00000876
Iteration 134/1000 | Loss: 0.00000876
Iteration 135/1000 | Loss: 0.00000876
Iteration 136/1000 | Loss: 0.00000876
Iteration 137/1000 | Loss: 0.00000876
Iteration 138/1000 | Loss: 0.00000876
Iteration 139/1000 | Loss: 0.00000876
Iteration 140/1000 | Loss: 0.00000876
Iteration 141/1000 | Loss: 0.00000876
Iteration 142/1000 | Loss: 0.00000876
Iteration 143/1000 | Loss: 0.00000875
Iteration 144/1000 | Loss: 0.00000875
Iteration 145/1000 | Loss: 0.00000874
Iteration 146/1000 | Loss: 0.00000874
Iteration 147/1000 | Loss: 0.00000874
Iteration 148/1000 | Loss: 0.00000874
Iteration 149/1000 | Loss: 0.00000873
Iteration 150/1000 | Loss: 0.00000873
Iteration 151/1000 | Loss: 0.00000873
Iteration 152/1000 | Loss: 0.00000873
Iteration 153/1000 | Loss: 0.00000873
Iteration 154/1000 | Loss: 0.00000873
Iteration 155/1000 | Loss: 0.00000873
Iteration 156/1000 | Loss: 0.00000873
Iteration 157/1000 | Loss: 0.00000873
Iteration 158/1000 | Loss: 0.00000873
Iteration 159/1000 | Loss: 0.00000873
Iteration 160/1000 | Loss: 0.00000873
Iteration 161/1000 | Loss: 0.00000873
Iteration 162/1000 | Loss: 0.00000873
Iteration 163/1000 | Loss: 0.00000872
Iteration 164/1000 | Loss: 0.00000872
Iteration 165/1000 | Loss: 0.00000872
Iteration 166/1000 | Loss: 0.00000872
Iteration 167/1000 | Loss: 0.00000872
Iteration 168/1000 | Loss: 0.00000872
Iteration 169/1000 | Loss: 0.00000872
Iteration 170/1000 | Loss: 0.00000872
Iteration 171/1000 | Loss: 0.00000872
Iteration 172/1000 | Loss: 0.00000872
Iteration 173/1000 | Loss: 0.00000872
Iteration 174/1000 | Loss: 0.00000872
Iteration 175/1000 | Loss: 0.00000872
Iteration 176/1000 | Loss: 0.00000872
Iteration 177/1000 | Loss: 0.00000872
Iteration 178/1000 | Loss: 0.00000872
Iteration 179/1000 | Loss: 0.00000872
Iteration 180/1000 | Loss: 0.00000872
Iteration 181/1000 | Loss: 0.00000872
Iteration 182/1000 | Loss: 0.00000872
Iteration 183/1000 | Loss: 0.00000872
Iteration 184/1000 | Loss: 0.00000872
Iteration 185/1000 | Loss: 0.00000872
Iteration 186/1000 | Loss: 0.00000872
Iteration 187/1000 | Loss: 0.00000872
Iteration 188/1000 | Loss: 0.00000872
Iteration 189/1000 | Loss: 0.00000872
Iteration 190/1000 | Loss: 0.00000872
Iteration 191/1000 | Loss: 0.00000872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [8.723169230506755e-06, 8.723169230506755e-06, 8.723169230506755e-06, 8.723169230506755e-06, 8.723169230506755e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.723169230506755e-06

Optimization complete. Final v2v error: 2.5718026161193848 mm

Highest mean error: 2.6911814212799072 mm for frame 203

Lowest mean error: 2.3860490322113037 mm for frame 140

Saving results

Total time: 46.525803327560425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881674
Iteration 2/25 | Loss: 0.00133355
Iteration 3/25 | Loss: 0.00116116
Iteration 4/25 | Loss: 0.00115837
Iteration 5/25 | Loss: 0.00116613
Iteration 6/25 | Loss: 0.00114352
Iteration 7/25 | Loss: 0.00114112
Iteration 8/25 | Loss: 0.00114069
Iteration 9/25 | Loss: 0.00114062
Iteration 10/25 | Loss: 0.00114061
Iteration 11/25 | Loss: 0.00114061
Iteration 12/25 | Loss: 0.00114061
Iteration 13/25 | Loss: 0.00114061
Iteration 14/25 | Loss: 0.00114061
Iteration 15/25 | Loss: 0.00114061
Iteration 16/25 | Loss: 0.00114061
Iteration 17/25 | Loss: 0.00114061
Iteration 18/25 | Loss: 0.00114060
Iteration 19/25 | Loss: 0.00114060
Iteration 20/25 | Loss: 0.00114060
Iteration 21/25 | Loss: 0.00114060
Iteration 22/25 | Loss: 0.00114060
Iteration 23/25 | Loss: 0.00114060
Iteration 24/25 | Loss: 0.00114060
Iteration 25/25 | Loss: 0.00114060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31628990
Iteration 2/25 | Loss: 0.00085402
Iteration 3/25 | Loss: 0.00085401
Iteration 4/25 | Loss: 0.00085401
Iteration 5/25 | Loss: 0.00085401
Iteration 6/25 | Loss: 0.00085401
Iteration 7/25 | Loss: 0.00085401
Iteration 8/25 | Loss: 0.00085401
Iteration 9/25 | Loss: 0.00085401
Iteration 10/25 | Loss: 0.00085401
Iteration 11/25 | Loss: 0.00085401
Iteration 12/25 | Loss: 0.00085401
Iteration 13/25 | Loss: 0.00085401
Iteration 14/25 | Loss: 0.00085401
Iteration 15/25 | Loss: 0.00085401
Iteration 16/25 | Loss: 0.00085401
Iteration 17/25 | Loss: 0.00085401
Iteration 18/25 | Loss: 0.00085401
Iteration 19/25 | Loss: 0.00085401
Iteration 20/25 | Loss: 0.00085401
Iteration 21/25 | Loss: 0.00085401
Iteration 22/25 | Loss: 0.00085401
Iteration 23/25 | Loss: 0.00085401
Iteration 24/25 | Loss: 0.00085401
Iteration 25/25 | Loss: 0.00085401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085401
Iteration 2/1000 | Loss: 0.00003208
Iteration 3/1000 | Loss: 0.00001989
Iteration 4/1000 | Loss: 0.00001733
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001476
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001412
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00011538
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001370
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001366
Iteration 37/1000 | Loss: 0.00001366
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001362
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001360
Iteration 61/1000 | Loss: 0.00001360
Iteration 62/1000 | Loss: 0.00001360
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001356
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00013444
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001355
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001352
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001351
Iteration 96/1000 | Loss: 0.00001351
Iteration 97/1000 | Loss: 0.00001351
Iteration 98/1000 | Loss: 0.00001351
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001350
Iteration 102/1000 | Loss: 0.00001350
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001349
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001347
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001347
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00010395
Iteration 133/1000 | Loss: 0.00002795
Iteration 134/1000 | Loss: 0.00003296
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001348
Iteration 138/1000 | Loss: 0.00001347
Iteration 139/1000 | Loss: 0.00001346
Iteration 140/1000 | Loss: 0.00001346
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001345
Iteration 146/1000 | Loss: 0.00001345
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001344
Iteration 150/1000 | Loss: 0.00001344
Iteration 151/1000 | Loss: 0.00001344
Iteration 152/1000 | Loss: 0.00001344
Iteration 153/1000 | Loss: 0.00001344
Iteration 154/1000 | Loss: 0.00001343
Iteration 155/1000 | Loss: 0.00001343
Iteration 156/1000 | Loss: 0.00001343
Iteration 157/1000 | Loss: 0.00001343
Iteration 158/1000 | Loss: 0.00001343
Iteration 159/1000 | Loss: 0.00001343
Iteration 160/1000 | Loss: 0.00001343
Iteration 161/1000 | Loss: 0.00001343
Iteration 162/1000 | Loss: 0.00001343
Iteration 163/1000 | Loss: 0.00001342
Iteration 164/1000 | Loss: 0.00001342
Iteration 165/1000 | Loss: 0.00001342
Iteration 166/1000 | Loss: 0.00001342
Iteration 167/1000 | Loss: 0.00001342
Iteration 168/1000 | Loss: 0.00001342
Iteration 169/1000 | Loss: 0.00001342
Iteration 170/1000 | Loss: 0.00001342
Iteration 171/1000 | Loss: 0.00001342
Iteration 172/1000 | Loss: 0.00001342
Iteration 173/1000 | Loss: 0.00001342
Iteration 174/1000 | Loss: 0.00001342
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001341
Iteration 178/1000 | Loss: 0.00001341
Iteration 179/1000 | Loss: 0.00001341
Iteration 180/1000 | Loss: 0.00001341
Iteration 181/1000 | Loss: 0.00001341
Iteration 182/1000 | Loss: 0.00001341
Iteration 183/1000 | Loss: 0.00001341
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Iteration 189/1000 | Loss: 0.00001340
Iteration 190/1000 | Loss: 0.00001340
Iteration 191/1000 | Loss: 0.00001340
Iteration 192/1000 | Loss: 0.00001340
Iteration 193/1000 | Loss: 0.00001339
Iteration 194/1000 | Loss: 0.00001339
Iteration 195/1000 | Loss: 0.00001339
Iteration 196/1000 | Loss: 0.00001339
Iteration 197/1000 | Loss: 0.00001338
Iteration 198/1000 | Loss: 0.00001338
Iteration 199/1000 | Loss: 0.00001338
Iteration 200/1000 | Loss: 0.00001338
Iteration 201/1000 | Loss: 0.00001338
Iteration 202/1000 | Loss: 0.00001338
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001338
Iteration 205/1000 | Loss: 0.00001338
Iteration 206/1000 | Loss: 0.00001338
Iteration 207/1000 | Loss: 0.00001338
Iteration 208/1000 | Loss: 0.00001338
Iteration 209/1000 | Loss: 0.00001338
Iteration 210/1000 | Loss: 0.00001337
Iteration 211/1000 | Loss: 0.00001337
Iteration 212/1000 | Loss: 0.00001337
Iteration 213/1000 | Loss: 0.00001337
Iteration 214/1000 | Loss: 0.00001337
Iteration 215/1000 | Loss: 0.00001337
Iteration 216/1000 | Loss: 0.00001337
Iteration 217/1000 | Loss: 0.00001337
Iteration 218/1000 | Loss: 0.00001337
Iteration 219/1000 | Loss: 0.00001337
Iteration 220/1000 | Loss: 0.00001337
Iteration 221/1000 | Loss: 0.00001337
Iteration 222/1000 | Loss: 0.00001337
Iteration 223/1000 | Loss: 0.00001337
Iteration 224/1000 | Loss: 0.00001337
Iteration 225/1000 | Loss: 0.00001337
Iteration 226/1000 | Loss: 0.00001337
Iteration 227/1000 | Loss: 0.00001337
Iteration 228/1000 | Loss: 0.00001337
Iteration 229/1000 | Loss: 0.00001336
Iteration 230/1000 | Loss: 0.00001336
Iteration 231/1000 | Loss: 0.00001336
Iteration 232/1000 | Loss: 0.00001336
Iteration 233/1000 | Loss: 0.00001336
Iteration 234/1000 | Loss: 0.00001336
Iteration 235/1000 | Loss: 0.00001336
Iteration 236/1000 | Loss: 0.00001336
Iteration 237/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.3363426660362165e-05, 1.3363426660362165e-05, 1.3363426660362165e-05, 1.3363426660362165e-05, 1.3363426660362165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3363426660362165e-05

Optimization complete. Final v2v error: 3.0538735389709473 mm

Highest mean error: 3.8155486583709717 mm for frame 183

Lowest mean error: 2.625493288040161 mm for frame 204

Saving results

Total time: 65.82173442840576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989293
Iteration 2/25 | Loss: 0.00219994
Iteration 3/25 | Loss: 0.00194509
Iteration 4/25 | Loss: 0.00214127
Iteration 5/25 | Loss: 0.00132611
Iteration 6/25 | Loss: 0.00123348
Iteration 7/25 | Loss: 0.00121314
Iteration 8/25 | Loss: 0.00119657
Iteration 9/25 | Loss: 0.00117867
Iteration 10/25 | Loss: 0.00117380
Iteration 11/25 | Loss: 0.00117831
Iteration 12/25 | Loss: 0.00116720
Iteration 13/25 | Loss: 0.00117169
Iteration 14/25 | Loss: 0.00116440
Iteration 15/25 | Loss: 0.00115848
Iteration 16/25 | Loss: 0.00116414
Iteration 17/25 | Loss: 0.00116110
Iteration 18/25 | Loss: 0.00115531
Iteration 19/25 | Loss: 0.00115466
Iteration 20/25 | Loss: 0.00115457
Iteration 21/25 | Loss: 0.00115457
Iteration 22/25 | Loss: 0.00115457
Iteration 23/25 | Loss: 0.00115457
Iteration 24/25 | Loss: 0.00115457
Iteration 25/25 | Loss: 0.00115456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33806252
Iteration 2/25 | Loss: 0.00086082
Iteration 3/25 | Loss: 0.00086082
Iteration 4/25 | Loss: 0.00086082
Iteration 5/25 | Loss: 0.00086082
Iteration 6/25 | Loss: 0.00086082
Iteration 7/25 | Loss: 0.00086082
Iteration 8/25 | Loss: 0.00086082
Iteration 9/25 | Loss: 0.00086082
Iteration 10/25 | Loss: 0.00086082
Iteration 11/25 | Loss: 0.00086082
Iteration 12/25 | Loss: 0.00086082
Iteration 13/25 | Loss: 0.00086082
Iteration 14/25 | Loss: 0.00086082
Iteration 15/25 | Loss: 0.00086082
Iteration 16/25 | Loss: 0.00086082
Iteration 17/25 | Loss: 0.00086082
Iteration 18/25 | Loss: 0.00086082
Iteration 19/25 | Loss: 0.00086082
Iteration 20/25 | Loss: 0.00086082
Iteration 21/25 | Loss: 0.00086082
Iteration 22/25 | Loss: 0.00086082
Iteration 23/25 | Loss: 0.00086082
Iteration 24/25 | Loss: 0.00086082
Iteration 25/25 | Loss: 0.00086082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086082
Iteration 2/1000 | Loss: 0.00016154
Iteration 3/1000 | Loss: 0.00005385
Iteration 4/1000 | Loss: 0.00004614
Iteration 5/1000 | Loss: 0.00004203
Iteration 6/1000 | Loss: 0.00003710
Iteration 7/1000 | Loss: 0.00003420
Iteration 8/1000 | Loss: 0.00003131
Iteration 9/1000 | Loss: 0.00002994
Iteration 10/1000 | Loss: 0.00002873
Iteration 11/1000 | Loss: 0.00002771
Iteration 12/1000 | Loss: 0.00108522
Iteration 13/1000 | Loss: 0.00003995
Iteration 14/1000 | Loss: 0.00002960
Iteration 15/1000 | Loss: 0.00002558
Iteration 16/1000 | Loss: 0.00002053
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001652
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001535
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001523
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001505
Iteration 38/1000 | Loss: 0.00001505
Iteration 39/1000 | Loss: 0.00001505
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001500
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001499
Iteration 72/1000 | Loss: 0.00001499
Iteration 73/1000 | Loss: 0.00001499
Iteration 74/1000 | Loss: 0.00001499
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001499
Iteration 77/1000 | Loss: 0.00001499
Iteration 78/1000 | Loss: 0.00001499
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.499349582445575e-05, 1.499349582445575e-05, 1.499349582445575e-05, 1.499349582445575e-05, 1.499349582445575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.499349582445575e-05

Optimization complete. Final v2v error: 3.3107285499572754 mm

Highest mean error: 3.435612678527832 mm for frame 0

Lowest mean error: 3.1927027702331543 mm for frame 5

Saving results

Total time: 75.18981838226318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964881
Iteration 2/25 | Loss: 0.00304281
Iteration 3/25 | Loss: 0.00205787
Iteration 4/25 | Loss: 0.00177422
Iteration 5/25 | Loss: 0.00141224
Iteration 6/25 | Loss: 0.00125253
Iteration 7/25 | Loss: 0.00123246
Iteration 8/25 | Loss: 0.00123848
Iteration 9/25 | Loss: 0.00121414
Iteration 10/25 | Loss: 0.00119603
Iteration 11/25 | Loss: 0.00119235
Iteration 12/25 | Loss: 0.00119043
Iteration 13/25 | Loss: 0.00119344
Iteration 14/25 | Loss: 0.00119162
Iteration 15/25 | Loss: 0.00118502
Iteration 16/25 | Loss: 0.00118374
Iteration 17/25 | Loss: 0.00118352
Iteration 18/25 | Loss: 0.00118343
Iteration 19/25 | Loss: 0.00118343
Iteration 20/25 | Loss: 0.00118343
Iteration 21/25 | Loss: 0.00118343
Iteration 22/25 | Loss: 0.00118343
Iteration 23/25 | Loss: 0.00118342
Iteration 24/25 | Loss: 0.00118342
Iteration 25/25 | Loss: 0.00118342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35958898
Iteration 2/25 | Loss: 0.00053598
Iteration 3/25 | Loss: 0.00053598
Iteration 4/25 | Loss: 0.00053598
Iteration 5/25 | Loss: 0.00053598
Iteration 6/25 | Loss: 0.00053598
Iteration 7/25 | Loss: 0.00053598
Iteration 8/25 | Loss: 0.00053598
Iteration 9/25 | Loss: 0.00053597
Iteration 10/25 | Loss: 0.00053597
Iteration 11/25 | Loss: 0.00053597
Iteration 12/25 | Loss: 0.00053597
Iteration 13/25 | Loss: 0.00053597
Iteration 14/25 | Loss: 0.00053597
Iteration 15/25 | Loss: 0.00053597
Iteration 16/25 | Loss: 0.00053597
Iteration 17/25 | Loss: 0.00053597
Iteration 18/25 | Loss: 0.00053597
Iteration 19/25 | Loss: 0.00053597
Iteration 20/25 | Loss: 0.00053597
Iteration 21/25 | Loss: 0.00053597
Iteration 22/25 | Loss: 0.00053597
Iteration 23/25 | Loss: 0.00053597
Iteration 24/25 | Loss: 0.00053597
Iteration 25/25 | Loss: 0.00053597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053597
Iteration 2/1000 | Loss: 0.00003523
Iteration 3/1000 | Loss: 0.00002366
Iteration 4/1000 | Loss: 0.00002192
Iteration 5/1000 | Loss: 0.00002070
Iteration 6/1000 | Loss: 0.00002013
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001944
Iteration 9/1000 | Loss: 0.00001928
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001885
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001878
Iteration 14/1000 | Loss: 0.00001877
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00001863
Iteration 21/1000 | Loss: 0.00001863
Iteration 22/1000 | Loss: 0.00001862
Iteration 23/1000 | Loss: 0.00001862
Iteration 24/1000 | Loss: 0.00001862
Iteration 25/1000 | Loss: 0.00001857
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00001856
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001854
Iteration 33/1000 | Loss: 0.00001853
Iteration 34/1000 | Loss: 0.00001850
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001850
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001848
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001847
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001847
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001846
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001844
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001843
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001842
Iteration 73/1000 | Loss: 0.00001842
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001836
Iteration 121/1000 | Loss: 0.00001836
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001836
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001836
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001835
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001834
Iteration 134/1000 | Loss: 0.00001834
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001833
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001832
Iteration 144/1000 | Loss: 0.00001832
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001832
Iteration 147/1000 | Loss: 0.00001832
Iteration 148/1000 | Loss: 0.00001832
Iteration 149/1000 | Loss: 0.00001832
Iteration 150/1000 | Loss: 0.00001832
Iteration 151/1000 | Loss: 0.00001832
Iteration 152/1000 | Loss: 0.00001832
Iteration 153/1000 | Loss: 0.00001832
Iteration 154/1000 | Loss: 0.00001832
Iteration 155/1000 | Loss: 0.00001832
Iteration 156/1000 | Loss: 0.00001832
Iteration 157/1000 | Loss: 0.00001832
Iteration 158/1000 | Loss: 0.00001831
Iteration 159/1000 | Loss: 0.00001831
Iteration 160/1000 | Loss: 0.00001831
Iteration 161/1000 | Loss: 0.00001831
Iteration 162/1000 | Loss: 0.00001831
Iteration 163/1000 | Loss: 0.00001831
Iteration 164/1000 | Loss: 0.00001831
Iteration 165/1000 | Loss: 0.00001831
Iteration 166/1000 | Loss: 0.00001831
Iteration 167/1000 | Loss: 0.00001831
Iteration 168/1000 | Loss: 0.00001831
Iteration 169/1000 | Loss: 0.00001831
Iteration 170/1000 | Loss: 0.00001831
Iteration 171/1000 | Loss: 0.00001831
Iteration 172/1000 | Loss: 0.00001831
Iteration 173/1000 | Loss: 0.00001831
Iteration 174/1000 | Loss: 0.00001830
Iteration 175/1000 | Loss: 0.00001830
Iteration 176/1000 | Loss: 0.00001830
Iteration 177/1000 | Loss: 0.00001830
Iteration 178/1000 | Loss: 0.00001830
Iteration 179/1000 | Loss: 0.00001830
Iteration 180/1000 | Loss: 0.00001830
Iteration 181/1000 | Loss: 0.00001830
Iteration 182/1000 | Loss: 0.00001830
Iteration 183/1000 | Loss: 0.00001830
Iteration 184/1000 | Loss: 0.00001830
Iteration 185/1000 | Loss: 0.00001829
Iteration 186/1000 | Loss: 0.00001829
Iteration 187/1000 | Loss: 0.00001829
Iteration 188/1000 | Loss: 0.00001828
Iteration 189/1000 | Loss: 0.00001828
Iteration 190/1000 | Loss: 0.00001828
Iteration 191/1000 | Loss: 0.00001828
Iteration 192/1000 | Loss: 0.00001828
Iteration 193/1000 | Loss: 0.00001828
Iteration 194/1000 | Loss: 0.00001828
Iteration 195/1000 | Loss: 0.00001828
Iteration 196/1000 | Loss: 0.00001828
Iteration 197/1000 | Loss: 0.00001828
Iteration 198/1000 | Loss: 0.00001828
Iteration 199/1000 | Loss: 0.00001828
Iteration 200/1000 | Loss: 0.00001828
Iteration 201/1000 | Loss: 0.00001828
Iteration 202/1000 | Loss: 0.00001828
Iteration 203/1000 | Loss: 0.00001827
Iteration 204/1000 | Loss: 0.00001827
Iteration 205/1000 | Loss: 0.00001827
Iteration 206/1000 | Loss: 0.00001827
Iteration 207/1000 | Loss: 0.00001827
Iteration 208/1000 | Loss: 0.00001826
Iteration 209/1000 | Loss: 0.00001826
Iteration 210/1000 | Loss: 0.00001826
Iteration 211/1000 | Loss: 0.00001826
Iteration 212/1000 | Loss: 0.00001826
Iteration 213/1000 | Loss: 0.00001826
Iteration 214/1000 | Loss: 0.00001826
Iteration 215/1000 | Loss: 0.00001826
Iteration 216/1000 | Loss: 0.00001826
Iteration 217/1000 | Loss: 0.00001826
Iteration 218/1000 | Loss: 0.00001826
Iteration 219/1000 | Loss: 0.00001826
Iteration 220/1000 | Loss: 0.00001826
Iteration 221/1000 | Loss: 0.00001826
Iteration 222/1000 | Loss: 0.00001826
Iteration 223/1000 | Loss: 0.00001826
Iteration 224/1000 | Loss: 0.00001826
Iteration 225/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.8261836885358207e-05, 1.8261836885358207e-05, 1.8261836885358207e-05, 1.8261836885358207e-05, 1.8261836885358207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8261836885358207e-05

Optimization complete. Final v2v error: 3.633970022201538 mm

Highest mean error: 3.7716727256774902 mm for frame 0

Lowest mean error: 3.4014036655426025 mm for frame 18

Saving results

Total time: 62.57426571846008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398217
Iteration 2/25 | Loss: 0.00113753
Iteration 3/25 | Loss: 0.00106326
Iteration 4/25 | Loss: 0.00105122
Iteration 5/25 | Loss: 0.00104713
Iteration 6/25 | Loss: 0.00104679
Iteration 7/25 | Loss: 0.00104679
Iteration 8/25 | Loss: 0.00104679
Iteration 9/25 | Loss: 0.00104679
Iteration 10/25 | Loss: 0.00104679
Iteration 11/25 | Loss: 0.00104679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010467899264767766, 0.0010467899264767766, 0.0010467899264767766, 0.0010467899264767766, 0.0010467899264767766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010467899264767766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.96222782
Iteration 2/25 | Loss: 0.00074680
Iteration 3/25 | Loss: 0.00074680
Iteration 4/25 | Loss: 0.00074680
Iteration 5/25 | Loss: 0.00074680
Iteration 6/25 | Loss: 0.00074680
Iteration 7/25 | Loss: 0.00074680
Iteration 8/25 | Loss: 0.00074680
Iteration 9/25 | Loss: 0.00074679
Iteration 10/25 | Loss: 0.00074679
Iteration 11/25 | Loss: 0.00074679
Iteration 12/25 | Loss: 0.00074679
Iteration 13/25 | Loss: 0.00074679
Iteration 14/25 | Loss: 0.00074679
Iteration 15/25 | Loss: 0.00074679
Iteration 16/25 | Loss: 0.00074679
Iteration 17/25 | Loss: 0.00074679
Iteration 18/25 | Loss: 0.00074679
Iteration 19/25 | Loss: 0.00074679
Iteration 20/25 | Loss: 0.00074679
Iteration 21/25 | Loss: 0.00074679
Iteration 22/25 | Loss: 0.00074679
Iteration 23/25 | Loss: 0.00074679
Iteration 24/25 | Loss: 0.00074679
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000746794103179127, 0.000746794103179127, 0.000746794103179127, 0.000746794103179127, 0.000746794103179127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000746794103179127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074679
Iteration 2/1000 | Loss: 0.00001639
Iteration 3/1000 | Loss: 0.00001289
Iteration 4/1000 | Loss: 0.00001191
Iteration 5/1000 | Loss: 0.00001144
Iteration 6/1000 | Loss: 0.00001084
Iteration 7/1000 | Loss: 0.00001059
Iteration 8/1000 | Loss: 0.00001025
Iteration 9/1000 | Loss: 0.00001013
Iteration 10/1000 | Loss: 0.00000991
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000988
Iteration 13/1000 | Loss: 0.00000979
Iteration 14/1000 | Loss: 0.00000964
Iteration 15/1000 | Loss: 0.00000963
Iteration 16/1000 | Loss: 0.00000958
Iteration 17/1000 | Loss: 0.00000957
Iteration 18/1000 | Loss: 0.00000956
Iteration 19/1000 | Loss: 0.00000953
Iteration 20/1000 | Loss: 0.00000952
Iteration 21/1000 | Loss: 0.00000951
Iteration 22/1000 | Loss: 0.00000951
Iteration 23/1000 | Loss: 0.00000951
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000946
Iteration 26/1000 | Loss: 0.00000945
Iteration 27/1000 | Loss: 0.00000941
Iteration 28/1000 | Loss: 0.00000941
Iteration 29/1000 | Loss: 0.00000941
Iteration 30/1000 | Loss: 0.00000940
Iteration 31/1000 | Loss: 0.00000935
Iteration 32/1000 | Loss: 0.00000934
Iteration 33/1000 | Loss: 0.00000933
Iteration 34/1000 | Loss: 0.00000932
Iteration 35/1000 | Loss: 0.00000932
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000931
Iteration 38/1000 | Loss: 0.00000930
Iteration 39/1000 | Loss: 0.00000930
Iteration 40/1000 | Loss: 0.00000929
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000927
Iteration 43/1000 | Loss: 0.00000927
Iteration 44/1000 | Loss: 0.00000927
Iteration 45/1000 | Loss: 0.00000926
Iteration 46/1000 | Loss: 0.00000926
Iteration 47/1000 | Loss: 0.00000925
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000924
Iteration 51/1000 | Loss: 0.00000924
Iteration 52/1000 | Loss: 0.00000923
Iteration 53/1000 | Loss: 0.00000923
Iteration 54/1000 | Loss: 0.00000923
Iteration 55/1000 | Loss: 0.00000923
Iteration 56/1000 | Loss: 0.00000923
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000923
Iteration 59/1000 | Loss: 0.00000923
Iteration 60/1000 | Loss: 0.00000923
Iteration 61/1000 | Loss: 0.00000923
Iteration 62/1000 | Loss: 0.00000923
Iteration 63/1000 | Loss: 0.00000923
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000923
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000922
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000922
Iteration 74/1000 | Loss: 0.00000922
Iteration 75/1000 | Loss: 0.00000921
Iteration 76/1000 | Loss: 0.00000921
Iteration 77/1000 | Loss: 0.00000921
Iteration 78/1000 | Loss: 0.00000921
Iteration 79/1000 | Loss: 0.00000920
Iteration 80/1000 | Loss: 0.00000920
Iteration 81/1000 | Loss: 0.00000920
Iteration 82/1000 | Loss: 0.00000920
Iteration 83/1000 | Loss: 0.00000920
Iteration 84/1000 | Loss: 0.00000920
Iteration 85/1000 | Loss: 0.00000919
Iteration 86/1000 | Loss: 0.00000919
Iteration 87/1000 | Loss: 0.00000919
Iteration 88/1000 | Loss: 0.00000919
Iteration 89/1000 | Loss: 0.00000919
Iteration 90/1000 | Loss: 0.00000919
Iteration 91/1000 | Loss: 0.00000919
Iteration 92/1000 | Loss: 0.00000919
Iteration 93/1000 | Loss: 0.00000919
Iteration 94/1000 | Loss: 0.00000918
Iteration 95/1000 | Loss: 0.00000918
Iteration 96/1000 | Loss: 0.00000918
Iteration 97/1000 | Loss: 0.00000918
Iteration 98/1000 | Loss: 0.00000917
Iteration 99/1000 | Loss: 0.00000917
Iteration 100/1000 | Loss: 0.00000917
Iteration 101/1000 | Loss: 0.00000917
Iteration 102/1000 | Loss: 0.00000917
Iteration 103/1000 | Loss: 0.00000917
Iteration 104/1000 | Loss: 0.00000917
Iteration 105/1000 | Loss: 0.00000916
Iteration 106/1000 | Loss: 0.00000916
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000916
Iteration 109/1000 | Loss: 0.00000915
Iteration 110/1000 | Loss: 0.00000915
Iteration 111/1000 | Loss: 0.00000915
Iteration 112/1000 | Loss: 0.00000914
Iteration 113/1000 | Loss: 0.00000912
Iteration 114/1000 | Loss: 0.00000912
Iteration 115/1000 | Loss: 0.00000912
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000912
Iteration 119/1000 | Loss: 0.00000912
Iteration 120/1000 | Loss: 0.00000912
Iteration 121/1000 | Loss: 0.00000912
Iteration 122/1000 | Loss: 0.00000912
Iteration 123/1000 | Loss: 0.00000912
Iteration 124/1000 | Loss: 0.00000911
Iteration 125/1000 | Loss: 0.00000911
Iteration 126/1000 | Loss: 0.00000911
Iteration 127/1000 | Loss: 0.00000911
Iteration 128/1000 | Loss: 0.00000911
Iteration 129/1000 | Loss: 0.00000911
Iteration 130/1000 | Loss: 0.00000911
Iteration 131/1000 | Loss: 0.00000911
Iteration 132/1000 | Loss: 0.00000910
Iteration 133/1000 | Loss: 0.00000910
Iteration 134/1000 | Loss: 0.00000909
Iteration 135/1000 | Loss: 0.00000908
Iteration 136/1000 | Loss: 0.00000908
Iteration 137/1000 | Loss: 0.00000908
Iteration 138/1000 | Loss: 0.00000908
Iteration 139/1000 | Loss: 0.00000908
Iteration 140/1000 | Loss: 0.00000907
Iteration 141/1000 | Loss: 0.00000907
Iteration 142/1000 | Loss: 0.00000907
Iteration 143/1000 | Loss: 0.00000907
Iteration 144/1000 | Loss: 0.00000907
Iteration 145/1000 | Loss: 0.00000907
Iteration 146/1000 | Loss: 0.00000907
Iteration 147/1000 | Loss: 0.00000906
Iteration 148/1000 | Loss: 0.00000905
Iteration 149/1000 | Loss: 0.00000905
Iteration 150/1000 | Loss: 0.00000905
Iteration 151/1000 | Loss: 0.00000905
Iteration 152/1000 | Loss: 0.00000905
Iteration 153/1000 | Loss: 0.00000904
Iteration 154/1000 | Loss: 0.00000904
Iteration 155/1000 | Loss: 0.00000904
Iteration 156/1000 | Loss: 0.00000904
Iteration 157/1000 | Loss: 0.00000904
Iteration 158/1000 | Loss: 0.00000904
Iteration 159/1000 | Loss: 0.00000904
Iteration 160/1000 | Loss: 0.00000904
Iteration 161/1000 | Loss: 0.00000904
Iteration 162/1000 | Loss: 0.00000904
Iteration 163/1000 | Loss: 0.00000904
Iteration 164/1000 | Loss: 0.00000904
Iteration 165/1000 | Loss: 0.00000904
Iteration 166/1000 | Loss: 0.00000904
Iteration 167/1000 | Loss: 0.00000904
Iteration 168/1000 | Loss: 0.00000904
Iteration 169/1000 | Loss: 0.00000904
Iteration 170/1000 | Loss: 0.00000903
Iteration 171/1000 | Loss: 0.00000903
Iteration 172/1000 | Loss: 0.00000903
Iteration 173/1000 | Loss: 0.00000903
Iteration 174/1000 | Loss: 0.00000903
Iteration 175/1000 | Loss: 0.00000903
Iteration 176/1000 | Loss: 0.00000903
Iteration 177/1000 | Loss: 0.00000903
Iteration 178/1000 | Loss: 0.00000903
Iteration 179/1000 | Loss: 0.00000903
Iteration 180/1000 | Loss: 0.00000903
Iteration 181/1000 | Loss: 0.00000903
Iteration 182/1000 | Loss: 0.00000903
Iteration 183/1000 | Loss: 0.00000903
Iteration 184/1000 | Loss: 0.00000903
Iteration 185/1000 | Loss: 0.00000903
Iteration 186/1000 | Loss: 0.00000903
Iteration 187/1000 | Loss: 0.00000903
Iteration 188/1000 | Loss: 0.00000903
Iteration 189/1000 | Loss: 0.00000903
Iteration 190/1000 | Loss: 0.00000903
Iteration 191/1000 | Loss: 0.00000903
Iteration 192/1000 | Loss: 0.00000903
Iteration 193/1000 | Loss: 0.00000903
Iteration 194/1000 | Loss: 0.00000903
Iteration 195/1000 | Loss: 0.00000903
Iteration 196/1000 | Loss: 0.00000903
Iteration 197/1000 | Loss: 0.00000903
Iteration 198/1000 | Loss: 0.00000903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [9.032106390804984e-06, 9.032106390804984e-06, 9.032106390804984e-06, 9.032106390804984e-06, 9.032106390804984e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.032106390804984e-06

Optimization complete. Final v2v error: 2.6057004928588867 mm

Highest mean error: 2.863969326019287 mm for frame 139

Lowest mean error: 2.491602897644043 mm for frame 201

Saving results

Total time: 45.418580770492554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532606
Iteration 2/25 | Loss: 0.00115120
Iteration 3/25 | Loss: 0.00107942
Iteration 4/25 | Loss: 0.00106834
Iteration 5/25 | Loss: 0.00106424
Iteration 6/25 | Loss: 0.00106352
Iteration 7/25 | Loss: 0.00106352
Iteration 8/25 | Loss: 0.00106352
Iteration 9/25 | Loss: 0.00106352
Iteration 10/25 | Loss: 0.00106352
Iteration 11/25 | Loss: 0.00106352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010635211365297437, 0.0010635211365297437, 0.0010635211365297437, 0.0010635211365297437, 0.0010635211365297437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010635211365297437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62478018
Iteration 2/25 | Loss: 0.00079033
Iteration 3/25 | Loss: 0.00079033
Iteration 4/25 | Loss: 0.00079033
Iteration 5/25 | Loss: 0.00079033
Iteration 6/25 | Loss: 0.00079033
Iteration 7/25 | Loss: 0.00079033
Iteration 8/25 | Loss: 0.00079033
Iteration 9/25 | Loss: 0.00079033
Iteration 10/25 | Loss: 0.00079033
Iteration 11/25 | Loss: 0.00079033
Iteration 12/25 | Loss: 0.00079033
Iteration 13/25 | Loss: 0.00079033
Iteration 14/25 | Loss: 0.00079033
Iteration 15/25 | Loss: 0.00079033
Iteration 16/25 | Loss: 0.00079033
Iteration 17/25 | Loss: 0.00079033
Iteration 18/25 | Loss: 0.00079033
Iteration 19/25 | Loss: 0.00079033
Iteration 20/25 | Loss: 0.00079033
Iteration 21/25 | Loss: 0.00079033
Iteration 22/25 | Loss: 0.00079033
Iteration 23/25 | Loss: 0.00079033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007903258083388209, 0.0007903258083388209, 0.0007903258083388209, 0.0007903258083388209, 0.0007903258083388209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007903258083388209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079033
Iteration 2/1000 | Loss: 0.00002004
Iteration 3/1000 | Loss: 0.00001371
Iteration 4/1000 | Loss: 0.00001271
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00001135
Iteration 8/1000 | Loss: 0.00001115
Iteration 9/1000 | Loss: 0.00001086
Iteration 10/1000 | Loss: 0.00001061
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001033
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001026
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001025
Iteration 23/1000 | Loss: 0.00001019
Iteration 24/1000 | Loss: 0.00001019
Iteration 25/1000 | Loss: 0.00001019
Iteration 26/1000 | Loss: 0.00001019
Iteration 27/1000 | Loss: 0.00001019
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001018
Iteration 30/1000 | Loss: 0.00001018
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001018
Iteration 33/1000 | Loss: 0.00001018
Iteration 34/1000 | Loss: 0.00001018
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001018
Iteration 37/1000 | Loss: 0.00001015
Iteration 38/1000 | Loss: 0.00001014
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001013
Iteration 41/1000 | Loss: 0.00001013
Iteration 42/1000 | Loss: 0.00001012
Iteration 43/1000 | Loss: 0.00001012
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001008
Iteration 50/1000 | Loss: 0.00001007
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001004
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001000
Iteration 62/1000 | Loss: 0.00001000
Iteration 63/1000 | Loss: 0.00000998
Iteration 64/1000 | Loss: 0.00000998
Iteration 65/1000 | Loss: 0.00000997
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000995
Iteration 68/1000 | Loss: 0.00000994
Iteration 69/1000 | Loss: 0.00000994
Iteration 70/1000 | Loss: 0.00000993
Iteration 71/1000 | Loss: 0.00000993
Iteration 72/1000 | Loss: 0.00000992
Iteration 73/1000 | Loss: 0.00000992
Iteration 74/1000 | Loss: 0.00000992
Iteration 75/1000 | Loss: 0.00000992
Iteration 76/1000 | Loss: 0.00000992
Iteration 77/1000 | Loss: 0.00000992
Iteration 78/1000 | Loss: 0.00000992
Iteration 79/1000 | Loss: 0.00000992
Iteration 80/1000 | Loss: 0.00000991
Iteration 81/1000 | Loss: 0.00000991
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000989
Iteration 85/1000 | Loss: 0.00000989
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000985
Iteration 93/1000 | Loss: 0.00000985
Iteration 94/1000 | Loss: 0.00000984
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000983
Iteration 102/1000 | Loss: 0.00000983
Iteration 103/1000 | Loss: 0.00000983
Iteration 104/1000 | Loss: 0.00000982
Iteration 105/1000 | Loss: 0.00000982
Iteration 106/1000 | Loss: 0.00000982
Iteration 107/1000 | Loss: 0.00000982
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000981
Iteration 110/1000 | Loss: 0.00000981
Iteration 111/1000 | Loss: 0.00000981
Iteration 112/1000 | Loss: 0.00000981
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000979
Iteration 119/1000 | Loss: 0.00000979
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000978
Iteration 129/1000 | Loss: 0.00000978
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000977
Iteration 136/1000 | Loss: 0.00000977
Iteration 137/1000 | Loss: 0.00000977
Iteration 138/1000 | Loss: 0.00000977
Iteration 139/1000 | Loss: 0.00000977
Iteration 140/1000 | Loss: 0.00000976
Iteration 141/1000 | Loss: 0.00000976
Iteration 142/1000 | Loss: 0.00000976
Iteration 143/1000 | Loss: 0.00000976
Iteration 144/1000 | Loss: 0.00000976
Iteration 145/1000 | Loss: 0.00000976
Iteration 146/1000 | Loss: 0.00000976
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000975
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000975
Iteration 152/1000 | Loss: 0.00000975
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000974
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000974
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000972
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000972
Iteration 168/1000 | Loss: 0.00000972
Iteration 169/1000 | Loss: 0.00000972
Iteration 170/1000 | Loss: 0.00000972
Iteration 171/1000 | Loss: 0.00000972
Iteration 172/1000 | Loss: 0.00000972
Iteration 173/1000 | Loss: 0.00000972
Iteration 174/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [9.717075954540633e-06, 9.717075954540633e-06, 9.717075954540633e-06, 9.717075954540633e-06, 9.717075954540633e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.717075954540633e-06

Optimization complete. Final v2v error: 2.698458671569824 mm

Highest mean error: 3.088369131088257 mm for frame 150

Lowest mean error: 2.4771552085876465 mm for frame 39

Saving results

Total time: 40.04759883880615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015391
Iteration 2/25 | Loss: 0.00132883
Iteration 3/25 | Loss: 0.00113172
Iteration 4/25 | Loss: 0.00110555
Iteration 5/25 | Loss: 0.00110148
Iteration 6/25 | Loss: 0.00110080
Iteration 7/25 | Loss: 0.00110080
Iteration 8/25 | Loss: 0.00110080
Iteration 9/25 | Loss: 0.00110080
Iteration 10/25 | Loss: 0.00110080
Iteration 11/25 | Loss: 0.00110080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00110079743899405, 0.00110079743899405, 0.00110079743899405, 0.00110079743899405, 0.00110079743899405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00110079743899405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64386725
Iteration 2/25 | Loss: 0.00081944
Iteration 3/25 | Loss: 0.00081944
Iteration 4/25 | Loss: 0.00081943
Iteration 5/25 | Loss: 0.00081943
Iteration 6/25 | Loss: 0.00081943
Iteration 7/25 | Loss: 0.00081943
Iteration 8/25 | Loss: 0.00081943
Iteration 9/25 | Loss: 0.00081943
Iteration 10/25 | Loss: 0.00081943
Iteration 11/25 | Loss: 0.00081943
Iteration 12/25 | Loss: 0.00081943
Iteration 13/25 | Loss: 0.00081943
Iteration 14/25 | Loss: 0.00081943
Iteration 15/25 | Loss: 0.00081943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008194330730475485, 0.0008194330730475485, 0.0008194330730475485, 0.0008194330730475485, 0.0008194330730475485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008194330730475485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081943
Iteration 2/1000 | Loss: 0.00002086
Iteration 3/1000 | Loss: 0.00001526
Iteration 4/1000 | Loss: 0.00001406
Iteration 5/1000 | Loss: 0.00001302
Iteration 6/1000 | Loss: 0.00001248
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001177
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001137
Iteration 11/1000 | Loss: 0.00001123
Iteration 12/1000 | Loss: 0.00001119
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001110
Iteration 15/1000 | Loss: 0.00001109
Iteration 16/1000 | Loss: 0.00001108
Iteration 17/1000 | Loss: 0.00001107
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001107
Iteration 20/1000 | Loss: 0.00001107
Iteration 21/1000 | Loss: 0.00001106
Iteration 22/1000 | Loss: 0.00001105
Iteration 23/1000 | Loss: 0.00001105
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001104
Iteration 35/1000 | Loss: 0.00001104
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001100
Iteration 43/1000 | Loss: 0.00001100
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001096
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001095
Iteration 52/1000 | Loss: 0.00001095
Iteration 53/1000 | Loss: 0.00001095
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001091
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001090
Iteration 70/1000 | Loss: 0.00001090
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001089
Iteration 77/1000 | Loss: 0.00001089
Iteration 78/1000 | Loss: 0.00001089
Iteration 79/1000 | Loss: 0.00001089
Iteration 80/1000 | Loss: 0.00001089
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001088
Iteration 83/1000 | Loss: 0.00001088
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001088
Iteration 90/1000 | Loss: 0.00001088
Iteration 91/1000 | Loss: 0.00001088
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001087
Iteration 95/1000 | Loss: 0.00001087
Iteration 96/1000 | Loss: 0.00001087
Iteration 97/1000 | Loss: 0.00001087
Iteration 98/1000 | Loss: 0.00001087
Iteration 99/1000 | Loss: 0.00001087
Iteration 100/1000 | Loss: 0.00001086
Iteration 101/1000 | Loss: 0.00001086
Iteration 102/1000 | Loss: 0.00001086
Iteration 103/1000 | Loss: 0.00001086
Iteration 104/1000 | Loss: 0.00001086
Iteration 105/1000 | Loss: 0.00001086
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001085
Iteration 113/1000 | Loss: 0.00001085
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001084
Iteration 120/1000 | Loss: 0.00001084
Iteration 121/1000 | Loss: 0.00001084
Iteration 122/1000 | Loss: 0.00001084
Iteration 123/1000 | Loss: 0.00001084
Iteration 124/1000 | Loss: 0.00001084
Iteration 125/1000 | Loss: 0.00001084
Iteration 126/1000 | Loss: 0.00001083
Iteration 127/1000 | Loss: 0.00001083
Iteration 128/1000 | Loss: 0.00001083
Iteration 129/1000 | Loss: 0.00001083
Iteration 130/1000 | Loss: 0.00001083
Iteration 131/1000 | Loss: 0.00001083
Iteration 132/1000 | Loss: 0.00001083
Iteration 133/1000 | Loss: 0.00001083
Iteration 134/1000 | Loss: 0.00001083
Iteration 135/1000 | Loss: 0.00001083
Iteration 136/1000 | Loss: 0.00001083
Iteration 137/1000 | Loss: 0.00001082
Iteration 138/1000 | Loss: 0.00001082
Iteration 139/1000 | Loss: 0.00001082
Iteration 140/1000 | Loss: 0.00001082
Iteration 141/1000 | Loss: 0.00001082
Iteration 142/1000 | Loss: 0.00001082
Iteration 143/1000 | Loss: 0.00001082
Iteration 144/1000 | Loss: 0.00001082
Iteration 145/1000 | Loss: 0.00001082
Iteration 146/1000 | Loss: 0.00001081
Iteration 147/1000 | Loss: 0.00001081
Iteration 148/1000 | Loss: 0.00001081
Iteration 149/1000 | Loss: 0.00001081
Iteration 150/1000 | Loss: 0.00001081
Iteration 151/1000 | Loss: 0.00001081
Iteration 152/1000 | Loss: 0.00001081
Iteration 153/1000 | Loss: 0.00001081
Iteration 154/1000 | Loss: 0.00001081
Iteration 155/1000 | Loss: 0.00001081
Iteration 156/1000 | Loss: 0.00001081
Iteration 157/1000 | Loss: 0.00001081
Iteration 158/1000 | Loss: 0.00001081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.0814927918545436e-05, 1.0814927918545436e-05, 1.0814927918545436e-05, 1.0814927918545436e-05, 1.0814927918545436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0814927918545436e-05

Optimization complete. Final v2v error: 2.809234619140625 mm

Highest mean error: 3.2075483798980713 mm for frame 95

Lowest mean error: 2.6126911640167236 mm for frame 248

Saving results

Total time: 41.56528186798096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810456
Iteration 2/25 | Loss: 0.00144541
Iteration 3/25 | Loss: 0.00114841
Iteration 4/25 | Loss: 0.00112291
Iteration 5/25 | Loss: 0.00111941
Iteration 6/25 | Loss: 0.00111937
Iteration 7/25 | Loss: 0.00111937
Iteration 8/25 | Loss: 0.00111937
Iteration 9/25 | Loss: 0.00111937
Iteration 10/25 | Loss: 0.00111937
Iteration 11/25 | Loss: 0.00111937
Iteration 12/25 | Loss: 0.00111937
Iteration 13/25 | Loss: 0.00111937
Iteration 14/25 | Loss: 0.00111937
Iteration 15/25 | Loss: 0.00111937
Iteration 16/25 | Loss: 0.00111937
Iteration 17/25 | Loss: 0.00111937
Iteration 18/25 | Loss: 0.00111937
Iteration 19/25 | Loss: 0.00111937
Iteration 20/25 | Loss: 0.00111937
Iteration 21/25 | Loss: 0.00111937
Iteration 22/25 | Loss: 0.00111937
Iteration 23/25 | Loss: 0.00111937
Iteration 24/25 | Loss: 0.00111937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011193652171641588, 0.0011193652171641588, 0.0011193652171641588, 0.0011193652171641588, 0.0011193652171641588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011193652171641588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32433355
Iteration 2/25 | Loss: 0.00062003
Iteration 3/25 | Loss: 0.00062003
Iteration 4/25 | Loss: 0.00062002
Iteration 5/25 | Loss: 0.00062002
Iteration 6/25 | Loss: 0.00062002
Iteration 7/25 | Loss: 0.00062002
Iteration 8/25 | Loss: 0.00062002
Iteration 9/25 | Loss: 0.00062002
Iteration 10/25 | Loss: 0.00062002
Iteration 11/25 | Loss: 0.00062002
Iteration 12/25 | Loss: 0.00062002
Iteration 13/25 | Loss: 0.00062002
Iteration 14/25 | Loss: 0.00062002
Iteration 15/25 | Loss: 0.00062002
Iteration 16/25 | Loss: 0.00062002
Iteration 17/25 | Loss: 0.00062002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006200209027156234, 0.0006200209027156234, 0.0006200209027156234, 0.0006200209027156234, 0.0006200209027156234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006200209027156234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062002
Iteration 2/1000 | Loss: 0.00002593
Iteration 3/1000 | Loss: 0.00001925
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001497
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001460
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001355
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001335
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001334
Iteration 32/1000 | Loss: 0.00001334
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001321
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001320
Iteration 82/1000 | Loss: 0.00001320
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001320
Iteration 88/1000 | Loss: 0.00001320
Iteration 89/1000 | Loss: 0.00001320
Iteration 90/1000 | Loss: 0.00001320
Iteration 91/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.3197806765674613e-05, 1.3197806765674613e-05, 1.3197806765674613e-05, 1.3197806765674613e-05, 1.3197806765674613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3197806765674613e-05

Optimization complete. Final v2v error: 3.0283150672912598 mm

Highest mean error: 3.151571035385132 mm for frame 35

Lowest mean error: 2.9490599632263184 mm for frame 61

Saving results

Total time: 34.35305690765381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824484
Iteration 2/25 | Loss: 0.00119460
Iteration 3/25 | Loss: 0.00107558
Iteration 4/25 | Loss: 0.00106257
Iteration 5/25 | Loss: 0.00106048
Iteration 6/25 | Loss: 0.00106027
Iteration 7/25 | Loss: 0.00106027
Iteration 8/25 | Loss: 0.00106027
Iteration 9/25 | Loss: 0.00106027
Iteration 10/25 | Loss: 0.00106027
Iteration 11/25 | Loss: 0.00106027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010602695401757956, 0.0010602695401757956, 0.0010602695401757956, 0.0010602695401757956, 0.0010602695401757956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010602695401757956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34727848
Iteration 2/25 | Loss: 0.00069013
Iteration 3/25 | Loss: 0.00069011
Iteration 4/25 | Loss: 0.00069011
Iteration 5/25 | Loss: 0.00069011
Iteration 6/25 | Loss: 0.00069010
Iteration 7/25 | Loss: 0.00069010
Iteration 8/25 | Loss: 0.00069010
Iteration 9/25 | Loss: 0.00069010
Iteration 10/25 | Loss: 0.00069010
Iteration 11/25 | Loss: 0.00069010
Iteration 12/25 | Loss: 0.00069010
Iteration 13/25 | Loss: 0.00069010
Iteration 14/25 | Loss: 0.00069010
Iteration 15/25 | Loss: 0.00069010
Iteration 16/25 | Loss: 0.00069010
Iteration 17/25 | Loss: 0.00069010
Iteration 18/25 | Loss: 0.00069010
Iteration 19/25 | Loss: 0.00069010
Iteration 20/25 | Loss: 0.00069010
Iteration 21/25 | Loss: 0.00069010
Iteration 22/25 | Loss: 0.00069010
Iteration 23/25 | Loss: 0.00069010
Iteration 24/25 | Loss: 0.00069010
Iteration 25/25 | Loss: 0.00069010

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069010
Iteration 2/1000 | Loss: 0.00001830
Iteration 3/1000 | Loss: 0.00001243
Iteration 4/1000 | Loss: 0.00001126
Iteration 5/1000 | Loss: 0.00001030
Iteration 6/1000 | Loss: 0.00000979
Iteration 7/1000 | Loss: 0.00000942
Iteration 8/1000 | Loss: 0.00000918
Iteration 9/1000 | Loss: 0.00000917
Iteration 10/1000 | Loss: 0.00000911
Iteration 11/1000 | Loss: 0.00000893
Iteration 12/1000 | Loss: 0.00000881
Iteration 13/1000 | Loss: 0.00000878
Iteration 14/1000 | Loss: 0.00000876
Iteration 15/1000 | Loss: 0.00000872
Iteration 16/1000 | Loss: 0.00000872
Iteration 17/1000 | Loss: 0.00000870
Iteration 18/1000 | Loss: 0.00000869
Iteration 19/1000 | Loss: 0.00000868
Iteration 20/1000 | Loss: 0.00000866
Iteration 21/1000 | Loss: 0.00000866
Iteration 22/1000 | Loss: 0.00000865
Iteration 23/1000 | Loss: 0.00000864
Iteration 24/1000 | Loss: 0.00000862
Iteration 25/1000 | Loss: 0.00000859
Iteration 26/1000 | Loss: 0.00000858
Iteration 27/1000 | Loss: 0.00000857
Iteration 28/1000 | Loss: 0.00000857
Iteration 29/1000 | Loss: 0.00000856
Iteration 30/1000 | Loss: 0.00000854
Iteration 31/1000 | Loss: 0.00000853
Iteration 32/1000 | Loss: 0.00000852
Iteration 33/1000 | Loss: 0.00000852
Iteration 34/1000 | Loss: 0.00000852
Iteration 35/1000 | Loss: 0.00000851
Iteration 36/1000 | Loss: 0.00000851
Iteration 37/1000 | Loss: 0.00000849
Iteration 38/1000 | Loss: 0.00000848
Iteration 39/1000 | Loss: 0.00000846
Iteration 40/1000 | Loss: 0.00000846
Iteration 41/1000 | Loss: 0.00000846
Iteration 42/1000 | Loss: 0.00000845
Iteration 43/1000 | Loss: 0.00000845
Iteration 44/1000 | Loss: 0.00000844
Iteration 45/1000 | Loss: 0.00000843
Iteration 46/1000 | Loss: 0.00000843
Iteration 47/1000 | Loss: 0.00000842
Iteration 48/1000 | Loss: 0.00000842
Iteration 49/1000 | Loss: 0.00000842
Iteration 50/1000 | Loss: 0.00000841
Iteration 51/1000 | Loss: 0.00000841
Iteration 52/1000 | Loss: 0.00000841
Iteration 53/1000 | Loss: 0.00000840
Iteration 54/1000 | Loss: 0.00000840
Iteration 55/1000 | Loss: 0.00000840
Iteration 56/1000 | Loss: 0.00000839
Iteration 57/1000 | Loss: 0.00000839
Iteration 58/1000 | Loss: 0.00000839
Iteration 59/1000 | Loss: 0.00000839
Iteration 60/1000 | Loss: 0.00000839
Iteration 61/1000 | Loss: 0.00000838
Iteration 62/1000 | Loss: 0.00000838
Iteration 63/1000 | Loss: 0.00000838
Iteration 64/1000 | Loss: 0.00000837
Iteration 65/1000 | Loss: 0.00000837
Iteration 66/1000 | Loss: 0.00000837
Iteration 67/1000 | Loss: 0.00000837
Iteration 68/1000 | Loss: 0.00000837
Iteration 69/1000 | Loss: 0.00000837
Iteration 70/1000 | Loss: 0.00000836
Iteration 71/1000 | Loss: 0.00000836
Iteration 72/1000 | Loss: 0.00000836
Iteration 73/1000 | Loss: 0.00000835
Iteration 74/1000 | Loss: 0.00000835
Iteration 75/1000 | Loss: 0.00000835
Iteration 76/1000 | Loss: 0.00000835
Iteration 77/1000 | Loss: 0.00000835
Iteration 78/1000 | Loss: 0.00000834
Iteration 79/1000 | Loss: 0.00000834
Iteration 80/1000 | Loss: 0.00000834
Iteration 81/1000 | Loss: 0.00000834
Iteration 82/1000 | Loss: 0.00000834
Iteration 83/1000 | Loss: 0.00000833
Iteration 84/1000 | Loss: 0.00000833
Iteration 85/1000 | Loss: 0.00000833
Iteration 86/1000 | Loss: 0.00000833
Iteration 87/1000 | Loss: 0.00000832
Iteration 88/1000 | Loss: 0.00000832
Iteration 89/1000 | Loss: 0.00000832
Iteration 90/1000 | Loss: 0.00000832
Iteration 91/1000 | Loss: 0.00000832
Iteration 92/1000 | Loss: 0.00000831
Iteration 93/1000 | Loss: 0.00000831
Iteration 94/1000 | Loss: 0.00000831
Iteration 95/1000 | Loss: 0.00000831
Iteration 96/1000 | Loss: 0.00000831
Iteration 97/1000 | Loss: 0.00000831
Iteration 98/1000 | Loss: 0.00000831
Iteration 99/1000 | Loss: 0.00000831
Iteration 100/1000 | Loss: 0.00000831
Iteration 101/1000 | Loss: 0.00000830
Iteration 102/1000 | Loss: 0.00000830
Iteration 103/1000 | Loss: 0.00000830
Iteration 104/1000 | Loss: 0.00000830
Iteration 105/1000 | Loss: 0.00000829
Iteration 106/1000 | Loss: 0.00000829
Iteration 107/1000 | Loss: 0.00000829
Iteration 108/1000 | Loss: 0.00000829
Iteration 109/1000 | Loss: 0.00000829
Iteration 110/1000 | Loss: 0.00000828
Iteration 111/1000 | Loss: 0.00000828
Iteration 112/1000 | Loss: 0.00000828
Iteration 113/1000 | Loss: 0.00000828
Iteration 114/1000 | Loss: 0.00000828
Iteration 115/1000 | Loss: 0.00000828
Iteration 116/1000 | Loss: 0.00000827
Iteration 117/1000 | Loss: 0.00000827
Iteration 118/1000 | Loss: 0.00000827
Iteration 119/1000 | Loss: 0.00000827
Iteration 120/1000 | Loss: 0.00000827
Iteration 121/1000 | Loss: 0.00000827
Iteration 122/1000 | Loss: 0.00000827
Iteration 123/1000 | Loss: 0.00000827
Iteration 124/1000 | Loss: 0.00000826
Iteration 125/1000 | Loss: 0.00000826
Iteration 126/1000 | Loss: 0.00000824
Iteration 127/1000 | Loss: 0.00000824
Iteration 128/1000 | Loss: 0.00000824
Iteration 129/1000 | Loss: 0.00000824
Iteration 130/1000 | Loss: 0.00000824
Iteration 131/1000 | Loss: 0.00000824
Iteration 132/1000 | Loss: 0.00000824
Iteration 133/1000 | Loss: 0.00000824
Iteration 134/1000 | Loss: 0.00000824
Iteration 135/1000 | Loss: 0.00000824
Iteration 136/1000 | Loss: 0.00000823
Iteration 137/1000 | Loss: 0.00000823
Iteration 138/1000 | Loss: 0.00000822
Iteration 139/1000 | Loss: 0.00000822
Iteration 140/1000 | Loss: 0.00000821
Iteration 141/1000 | Loss: 0.00000821
Iteration 142/1000 | Loss: 0.00000821
Iteration 143/1000 | Loss: 0.00000821
Iteration 144/1000 | Loss: 0.00000821
Iteration 145/1000 | Loss: 0.00000821
Iteration 146/1000 | Loss: 0.00000821
Iteration 147/1000 | Loss: 0.00000821
Iteration 148/1000 | Loss: 0.00000820
Iteration 149/1000 | Loss: 0.00000820
Iteration 150/1000 | Loss: 0.00000820
Iteration 151/1000 | Loss: 0.00000820
Iteration 152/1000 | Loss: 0.00000820
Iteration 153/1000 | Loss: 0.00000820
Iteration 154/1000 | Loss: 0.00000820
Iteration 155/1000 | Loss: 0.00000820
Iteration 156/1000 | Loss: 0.00000820
Iteration 157/1000 | Loss: 0.00000820
Iteration 158/1000 | Loss: 0.00000820
Iteration 159/1000 | Loss: 0.00000820
Iteration 160/1000 | Loss: 0.00000820
Iteration 161/1000 | Loss: 0.00000820
Iteration 162/1000 | Loss: 0.00000819
Iteration 163/1000 | Loss: 0.00000819
Iteration 164/1000 | Loss: 0.00000819
Iteration 165/1000 | Loss: 0.00000819
Iteration 166/1000 | Loss: 0.00000819
Iteration 167/1000 | Loss: 0.00000819
Iteration 168/1000 | Loss: 0.00000819
Iteration 169/1000 | Loss: 0.00000819
Iteration 170/1000 | Loss: 0.00000819
Iteration 171/1000 | Loss: 0.00000819
Iteration 172/1000 | Loss: 0.00000819
Iteration 173/1000 | Loss: 0.00000819
Iteration 174/1000 | Loss: 0.00000819
Iteration 175/1000 | Loss: 0.00000819
Iteration 176/1000 | Loss: 0.00000819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [8.185014849004801e-06, 8.185014849004801e-06, 8.185014849004801e-06, 8.185014849004801e-06, 8.185014849004801e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.185014849004801e-06

Optimization complete. Final v2v error: 2.4694602489471436 mm

Highest mean error: 2.6036548614501953 mm for frame 24

Lowest mean error: 2.3843936920166016 mm for frame 43

Saving results

Total time: 35.860169410705566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955213
Iteration 2/25 | Loss: 0.00458242
Iteration 3/25 | Loss: 0.00343996
Iteration 4/25 | Loss: 0.00286246
Iteration 5/25 | Loss: 0.00238902
Iteration 6/25 | Loss: 0.00216365
Iteration 7/25 | Loss: 0.00210136
Iteration 8/25 | Loss: 0.00197289
Iteration 9/25 | Loss: 0.00185494
Iteration 10/25 | Loss: 0.00179762
Iteration 11/25 | Loss: 0.00174849
Iteration 12/25 | Loss: 0.00170589
Iteration 13/25 | Loss: 0.00170533
Iteration 14/25 | Loss: 0.00169421
Iteration 15/25 | Loss: 0.00167492
Iteration 16/25 | Loss: 0.00166064
Iteration 17/25 | Loss: 0.00165360
Iteration 18/25 | Loss: 0.00166212
Iteration 19/25 | Loss: 0.00165870
Iteration 20/25 | Loss: 0.00165676
Iteration 21/25 | Loss: 0.00165094
Iteration 22/25 | Loss: 0.00164632
Iteration 23/25 | Loss: 0.00164119
Iteration 24/25 | Loss: 0.00164280
Iteration 25/25 | Loss: 0.00164118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33061838
Iteration 2/25 | Loss: 0.00532696
Iteration 3/25 | Loss: 0.00530710
Iteration 4/25 | Loss: 0.00530710
Iteration 5/25 | Loss: 0.00530710
Iteration 6/25 | Loss: 0.00530710
Iteration 7/25 | Loss: 0.00530710
Iteration 8/25 | Loss: 0.00530710
Iteration 9/25 | Loss: 0.00530710
Iteration 10/25 | Loss: 0.00530710
Iteration 11/25 | Loss: 0.00530710
Iteration 12/25 | Loss: 0.00530710
Iteration 13/25 | Loss: 0.00530710
Iteration 14/25 | Loss: 0.00530710
Iteration 15/25 | Loss: 0.00530710
Iteration 16/25 | Loss: 0.00530710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005307098384946585, 0.005307098384946585, 0.005307098384946585, 0.005307098384946585, 0.005307098384946585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005307098384946585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00530710
Iteration 2/1000 | Loss: 0.00068372
Iteration 3/1000 | Loss: 0.00051540
Iteration 4/1000 | Loss: 0.00053907
Iteration 5/1000 | Loss: 0.00046566
Iteration 6/1000 | Loss: 0.00037029
Iteration 7/1000 | Loss: 0.00034248
Iteration 8/1000 | Loss: 0.00032065
Iteration 9/1000 | Loss: 0.00030458
Iteration 10/1000 | Loss: 0.00029695
Iteration 11/1000 | Loss: 0.00028878
Iteration 12/1000 | Loss: 0.00028292
Iteration 13/1000 | Loss: 0.00027652
Iteration 14/1000 | Loss: 0.00092761
Iteration 15/1000 | Loss: 0.01273509
Iteration 16/1000 | Loss: 0.01118269
Iteration 17/1000 | Loss: 0.00547063
Iteration 18/1000 | Loss: 0.01514621
Iteration 19/1000 | Loss: 0.00136533
Iteration 20/1000 | Loss: 0.00038051
Iteration 21/1000 | Loss: 0.00026988
Iteration 22/1000 | Loss: 0.00022402
Iteration 23/1000 | Loss: 0.00018900
Iteration 24/1000 | Loss: 0.00016776
Iteration 25/1000 | Loss: 0.00015692
Iteration 26/1000 | Loss: 0.00014679
Iteration 27/1000 | Loss: 0.00014119
Iteration 28/1000 | Loss: 0.00013621
Iteration 29/1000 | Loss: 0.00013182
Iteration 30/1000 | Loss: 0.00012879
Iteration 31/1000 | Loss: 0.00012527
Iteration 32/1000 | Loss: 0.00012168
Iteration 33/1000 | Loss: 0.00011828
Iteration 34/1000 | Loss: 0.00011588
Iteration 35/1000 | Loss: 0.00011300
Iteration 36/1000 | Loss: 0.00011076
Iteration 37/1000 | Loss: 0.00010884
Iteration 38/1000 | Loss: 0.00010682
Iteration 39/1000 | Loss: 0.00010493
Iteration 40/1000 | Loss: 0.00010346
Iteration 41/1000 | Loss: 0.00010218
Iteration 42/1000 | Loss: 0.00010138
Iteration 43/1000 | Loss: 0.00010053
Iteration 44/1000 | Loss: 0.00009974
Iteration 45/1000 | Loss: 0.00009909
Iteration 46/1000 | Loss: 0.00009856
Iteration 47/1000 | Loss: 0.00009819
Iteration 48/1000 | Loss: 0.00009788
Iteration 49/1000 | Loss: 0.00009765
Iteration 50/1000 | Loss: 0.00009744
Iteration 51/1000 | Loss: 0.00009738
Iteration 52/1000 | Loss: 0.00009731
Iteration 53/1000 | Loss: 0.00009730
Iteration 54/1000 | Loss: 0.00009728
Iteration 55/1000 | Loss: 0.00009718
Iteration 56/1000 | Loss: 0.00009715
Iteration 57/1000 | Loss: 0.00009714
Iteration 58/1000 | Loss: 0.00009712
Iteration 59/1000 | Loss: 0.00009712
Iteration 60/1000 | Loss: 0.00009711
Iteration 61/1000 | Loss: 0.00009710
Iteration 62/1000 | Loss: 0.00009710
Iteration 63/1000 | Loss: 0.00009709
Iteration 64/1000 | Loss: 0.00009707
Iteration 65/1000 | Loss: 0.00009707
Iteration 66/1000 | Loss: 0.00009706
Iteration 67/1000 | Loss: 0.00009706
Iteration 68/1000 | Loss: 0.00009705
Iteration 69/1000 | Loss: 0.00009705
Iteration 70/1000 | Loss: 0.00009704
Iteration 71/1000 | Loss: 0.00009702
Iteration 72/1000 | Loss: 0.00009702
Iteration 73/1000 | Loss: 0.00009701
Iteration 74/1000 | Loss: 0.00009701
Iteration 75/1000 | Loss: 0.00009700
Iteration 76/1000 | Loss: 0.00009700
Iteration 77/1000 | Loss: 0.00009699
Iteration 78/1000 | Loss: 0.00009699
Iteration 79/1000 | Loss: 0.00009699
Iteration 80/1000 | Loss: 0.00009698
Iteration 81/1000 | Loss: 0.00009698
Iteration 82/1000 | Loss: 0.00009698
Iteration 83/1000 | Loss: 0.00009698
Iteration 84/1000 | Loss: 0.00009697
Iteration 85/1000 | Loss: 0.00009697
Iteration 86/1000 | Loss: 0.00009697
Iteration 87/1000 | Loss: 0.00009697
Iteration 88/1000 | Loss: 0.00009696
Iteration 89/1000 | Loss: 0.00009696
Iteration 90/1000 | Loss: 0.00009695
Iteration 91/1000 | Loss: 0.00009695
Iteration 92/1000 | Loss: 0.00009694
Iteration 93/1000 | Loss: 0.00009694
Iteration 94/1000 | Loss: 0.00009694
Iteration 95/1000 | Loss: 0.00009694
Iteration 96/1000 | Loss: 0.00009694
Iteration 97/1000 | Loss: 0.00009694
Iteration 98/1000 | Loss: 0.00009694
Iteration 99/1000 | Loss: 0.00009693
Iteration 100/1000 | Loss: 0.00009693
Iteration 101/1000 | Loss: 0.00009693
Iteration 102/1000 | Loss: 0.00009693
Iteration 103/1000 | Loss: 0.00009693
Iteration 104/1000 | Loss: 0.00009692
Iteration 105/1000 | Loss: 0.00009692
Iteration 106/1000 | Loss: 0.00009692
Iteration 107/1000 | Loss: 0.00009692
Iteration 108/1000 | Loss: 0.00009692
Iteration 109/1000 | Loss: 0.00009692
Iteration 110/1000 | Loss: 0.00009692
Iteration 111/1000 | Loss: 0.00009692
Iteration 112/1000 | Loss: 0.00009692
Iteration 113/1000 | Loss: 0.00009692
Iteration 114/1000 | Loss: 0.00009692
Iteration 115/1000 | Loss: 0.00009692
Iteration 116/1000 | Loss: 0.00009692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [9.69164539128542e-05, 9.69164539128542e-05, 9.69164539128542e-05, 9.69164539128542e-05, 9.69164539128542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.69164539128542e-05

Optimization complete. Final v2v error: 5.279330253601074 mm

Highest mean error: 10.751465797424316 mm for frame 218

Lowest mean error: 3.6438777446746826 mm for frame 234

Saving results

Total time: 143.2034764289856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967065
Iteration 2/25 | Loss: 0.00967065
Iteration 3/25 | Loss: 0.00967065
Iteration 4/25 | Loss: 0.00967065
Iteration 5/25 | Loss: 0.00967065
Iteration 6/25 | Loss: 0.00967064
Iteration 7/25 | Loss: 0.00967064
Iteration 8/25 | Loss: 0.00967064
Iteration 9/25 | Loss: 0.00967064
Iteration 10/25 | Loss: 0.00967064
Iteration 11/25 | Loss: 0.00967064
Iteration 12/25 | Loss: 0.00967063
Iteration 13/25 | Loss: 0.00967063
Iteration 14/25 | Loss: 0.00967063
Iteration 15/25 | Loss: 0.00967063
Iteration 16/25 | Loss: 0.00967063
Iteration 17/25 | Loss: 0.00967062
Iteration 18/25 | Loss: 0.00967062
Iteration 19/25 | Loss: 0.00967062
Iteration 20/25 | Loss: 0.00967062
Iteration 21/25 | Loss: 0.00967062
Iteration 22/25 | Loss: 0.00967062
Iteration 23/25 | Loss: 0.00967061
Iteration 24/25 | Loss: 0.00967061
Iteration 25/25 | Loss: 0.00967061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63836741
Iteration 2/25 | Loss: 0.19791602
Iteration 3/25 | Loss: 0.19789977
Iteration 4/25 | Loss: 0.19789970
Iteration 5/25 | Loss: 0.19789970
Iteration 6/25 | Loss: 0.19789970
Iteration 7/25 | Loss: 0.19789965
Iteration 8/25 | Loss: 0.19789965
Iteration 9/25 | Loss: 0.19789965
Iteration 10/25 | Loss: 0.19789965
Iteration 11/25 | Loss: 0.19789965
Iteration 12/25 | Loss: 0.19789965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.19789965450763702, 0.19789965450763702, 0.19789965450763702, 0.19789965450763702, 0.19789965450763702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.19789965450763702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.19789965
Iteration 2/1000 | Loss: 0.00300775
Iteration 3/1000 | Loss: 0.00099245
Iteration 4/1000 | Loss: 0.00043491
Iteration 5/1000 | Loss: 0.00027038
Iteration 6/1000 | Loss: 0.00016884
Iteration 7/1000 | Loss: 0.00063834
Iteration 8/1000 | Loss: 0.00029262
Iteration 9/1000 | Loss: 0.00006665
Iteration 10/1000 | Loss: 0.00005485
Iteration 11/1000 | Loss: 0.00004569
Iteration 12/1000 | Loss: 0.00003918
Iteration 13/1000 | Loss: 0.00003402
Iteration 14/1000 | Loss: 0.00003027
Iteration 15/1000 | Loss: 0.00002824
Iteration 16/1000 | Loss: 0.00002619
Iteration 17/1000 | Loss: 0.00002389
Iteration 18/1000 | Loss: 0.00002137
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001836
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001594
Iteration 29/1000 | Loss: 0.00001573
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001541
Iteration 53/1000 | Loss: 0.00001541
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001540
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001540
Iteration 59/1000 | Loss: 0.00001540
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001539
Iteration 63/1000 | Loss: 0.00001539
Iteration 64/1000 | Loss: 0.00001539
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001536
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001535
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001533
Iteration 94/1000 | Loss: 0.00001533
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001532
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001530
Iteration 113/1000 | Loss: 0.00001530
Iteration 114/1000 | Loss: 0.00001530
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001530
Iteration 119/1000 | Loss: 0.00001530
Iteration 120/1000 | Loss: 0.00001530
Iteration 121/1000 | Loss: 0.00001530
Iteration 122/1000 | Loss: 0.00001530
Iteration 123/1000 | Loss: 0.00001530
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001530
Iteration 129/1000 | Loss: 0.00001530
Iteration 130/1000 | Loss: 0.00001530
Iteration 131/1000 | Loss: 0.00001530
Iteration 132/1000 | Loss: 0.00001530
Iteration 133/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.530173722130712e-05, 1.530173722130712e-05, 1.530173722130712e-05, 1.530173722130712e-05, 1.530173722130712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530173722130712e-05

Optimization complete. Final v2v error: 3.384103775024414 mm

Highest mean error: 3.5882227420806885 mm for frame 238

Lowest mean error: 3.179231643676758 mm for frame 124

Saving results

Total time: 64.47625350952148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01179168
Iteration 2/25 | Loss: 0.00287698
Iteration 3/25 | Loss: 0.00193404
Iteration 4/25 | Loss: 0.00184134
Iteration 5/25 | Loss: 0.00203897
Iteration 6/25 | Loss: 0.00177647
Iteration 7/25 | Loss: 0.00170882
Iteration 8/25 | Loss: 0.00172608
Iteration 9/25 | Loss: 0.00162380
Iteration 10/25 | Loss: 0.00161364
Iteration 11/25 | Loss: 0.00163017
Iteration 12/25 | Loss: 0.00159848
Iteration 13/25 | Loss: 0.00159221
Iteration 14/25 | Loss: 0.00156380
Iteration 15/25 | Loss: 0.00156294
Iteration 16/25 | Loss: 0.00156225
Iteration 17/25 | Loss: 0.00156957
Iteration 18/25 | Loss: 0.00155964
Iteration 19/25 | Loss: 0.00155418
Iteration 20/25 | Loss: 0.00155277
Iteration 21/25 | Loss: 0.00155247
Iteration 22/25 | Loss: 0.00155234
Iteration 23/25 | Loss: 0.00155224
Iteration 24/25 | Loss: 0.00155220
Iteration 25/25 | Loss: 0.00155219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75975692
Iteration 2/25 | Loss: 0.00262938
Iteration 3/25 | Loss: 0.00262937
Iteration 4/25 | Loss: 0.00262937
Iteration 5/25 | Loss: 0.00262937
Iteration 6/25 | Loss: 0.00262937
Iteration 7/25 | Loss: 0.00262937
Iteration 8/25 | Loss: 0.00262937
Iteration 9/25 | Loss: 0.00262937
Iteration 10/25 | Loss: 0.00262937
Iteration 11/25 | Loss: 0.00262937
Iteration 12/25 | Loss: 0.00262937
Iteration 13/25 | Loss: 0.00262937
Iteration 14/25 | Loss: 0.00262937
Iteration 15/25 | Loss: 0.00262937
Iteration 16/25 | Loss: 0.00262937
Iteration 17/25 | Loss: 0.00262937
Iteration 18/25 | Loss: 0.00262937
Iteration 19/25 | Loss: 0.00262937
Iteration 20/25 | Loss: 0.00262937
Iteration 21/25 | Loss: 0.00262937
Iteration 22/25 | Loss: 0.00262937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026293655391782522, 0.0026293655391782522, 0.0026293655391782522, 0.0026293655391782522, 0.0026293655391782522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026293655391782522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262937
Iteration 2/1000 | Loss: 0.00112973
Iteration 3/1000 | Loss: 0.00021953
Iteration 4/1000 | Loss: 0.00017908
Iteration 5/1000 | Loss: 0.00015979
Iteration 6/1000 | Loss: 0.00014939
Iteration 7/1000 | Loss: 0.00014071
Iteration 8/1000 | Loss: 0.00041430
Iteration 9/1000 | Loss: 0.00013565
Iteration 10/1000 | Loss: 0.00090574
Iteration 11/1000 | Loss: 0.00468684
Iteration 12/1000 | Loss: 0.00217327
Iteration 13/1000 | Loss: 0.00253597
Iteration 14/1000 | Loss: 0.00094451
Iteration 15/1000 | Loss: 0.00069042
Iteration 16/1000 | Loss: 0.00405598
Iteration 17/1000 | Loss: 0.00237304
Iteration 18/1000 | Loss: 0.00278574
Iteration 19/1000 | Loss: 0.00201706
Iteration 20/1000 | Loss: 0.00169527
Iteration 21/1000 | Loss: 0.00044921
Iteration 22/1000 | Loss: 0.00094176
Iteration 23/1000 | Loss: 0.00282552
Iteration 24/1000 | Loss: 0.00216061
Iteration 25/1000 | Loss: 0.00247659
Iteration 26/1000 | Loss: 0.00375013
Iteration 27/1000 | Loss: 0.00208828
Iteration 28/1000 | Loss: 0.00161286
Iteration 29/1000 | Loss: 0.00024778
Iteration 30/1000 | Loss: 0.00064707
Iteration 31/1000 | Loss: 0.00015329
Iteration 32/1000 | Loss: 0.00065308
Iteration 33/1000 | Loss: 0.00133431
Iteration 34/1000 | Loss: 0.00083835
Iteration 35/1000 | Loss: 0.00060568
Iteration 36/1000 | Loss: 0.00098512
Iteration 37/1000 | Loss: 0.00053516
Iteration 38/1000 | Loss: 0.00019567
Iteration 39/1000 | Loss: 0.00012770
Iteration 40/1000 | Loss: 0.00011622
Iteration 41/1000 | Loss: 0.00010881
Iteration 42/1000 | Loss: 0.00010451
Iteration 43/1000 | Loss: 0.00010076
Iteration 44/1000 | Loss: 0.00009809
Iteration 45/1000 | Loss: 0.00009587
Iteration 46/1000 | Loss: 0.00009422
Iteration 47/1000 | Loss: 0.00009282
Iteration 48/1000 | Loss: 0.00074228
Iteration 49/1000 | Loss: 0.00030912
Iteration 50/1000 | Loss: 0.00009635
Iteration 51/1000 | Loss: 0.00067125
Iteration 52/1000 | Loss: 0.00024556
Iteration 53/1000 | Loss: 0.00009788
Iteration 54/1000 | Loss: 0.00037003
Iteration 55/1000 | Loss: 0.00026754
Iteration 56/1000 | Loss: 0.00025646
Iteration 57/1000 | Loss: 0.00032527
Iteration 58/1000 | Loss: 0.00055474
Iteration 59/1000 | Loss: 0.00142322
Iteration 60/1000 | Loss: 0.00104526
Iteration 61/1000 | Loss: 0.00009794
Iteration 62/1000 | Loss: 0.00009429
Iteration 63/1000 | Loss: 0.00009296
Iteration 64/1000 | Loss: 0.00085734
Iteration 65/1000 | Loss: 0.00058205
Iteration 66/1000 | Loss: 0.00063388
Iteration 67/1000 | Loss: 0.00009193
Iteration 68/1000 | Loss: 0.00009043
Iteration 69/1000 | Loss: 0.00008874
Iteration 70/1000 | Loss: 0.00008753
Iteration 71/1000 | Loss: 0.00008618
Iteration 72/1000 | Loss: 0.00008542
Iteration 73/1000 | Loss: 0.00008487
Iteration 74/1000 | Loss: 0.00008421
Iteration 75/1000 | Loss: 0.00008378
Iteration 76/1000 | Loss: 0.00008321
Iteration 77/1000 | Loss: 0.00008271
Iteration 78/1000 | Loss: 0.00070424
Iteration 79/1000 | Loss: 0.00008457
Iteration 80/1000 | Loss: 0.00008113
Iteration 81/1000 | Loss: 0.00007971
Iteration 82/1000 | Loss: 0.00007898
Iteration 83/1000 | Loss: 0.00007794
Iteration 84/1000 | Loss: 0.00007732
Iteration 85/1000 | Loss: 0.00007681
Iteration 86/1000 | Loss: 0.00007636
Iteration 87/1000 | Loss: 0.00007602
Iteration 88/1000 | Loss: 0.00007572
Iteration 89/1000 | Loss: 0.00007545
Iteration 90/1000 | Loss: 0.00007531
Iteration 91/1000 | Loss: 0.00007513
Iteration 92/1000 | Loss: 0.00007488
Iteration 93/1000 | Loss: 0.00007464
Iteration 94/1000 | Loss: 0.00007445
Iteration 95/1000 | Loss: 0.00007432
Iteration 96/1000 | Loss: 0.00007431
Iteration 97/1000 | Loss: 0.00007428
Iteration 98/1000 | Loss: 0.00007417
Iteration 99/1000 | Loss: 0.00007414
Iteration 100/1000 | Loss: 0.00007403
Iteration 101/1000 | Loss: 0.00007392
Iteration 102/1000 | Loss: 0.00007386
Iteration 103/1000 | Loss: 0.00007383
Iteration 104/1000 | Loss: 0.00007382
Iteration 105/1000 | Loss: 0.00007379
Iteration 106/1000 | Loss: 0.00007376
Iteration 107/1000 | Loss: 0.00007376
Iteration 108/1000 | Loss: 0.00007376
Iteration 109/1000 | Loss: 0.00007376
Iteration 110/1000 | Loss: 0.00007376
Iteration 111/1000 | Loss: 0.00007376
Iteration 112/1000 | Loss: 0.00007376
Iteration 113/1000 | Loss: 0.00007375
Iteration 114/1000 | Loss: 0.00007375
Iteration 115/1000 | Loss: 0.00007375
Iteration 116/1000 | Loss: 0.00007375
Iteration 117/1000 | Loss: 0.00007375
Iteration 118/1000 | Loss: 0.00007374
Iteration 119/1000 | Loss: 0.00007374
Iteration 120/1000 | Loss: 0.00007374
Iteration 121/1000 | Loss: 0.00007373
Iteration 122/1000 | Loss: 0.00007373
Iteration 123/1000 | Loss: 0.00007373
Iteration 124/1000 | Loss: 0.00007373
Iteration 125/1000 | Loss: 0.00007373
Iteration 126/1000 | Loss: 0.00007373
Iteration 127/1000 | Loss: 0.00007373
Iteration 128/1000 | Loss: 0.00007373
Iteration 129/1000 | Loss: 0.00007373
Iteration 130/1000 | Loss: 0.00007372
Iteration 131/1000 | Loss: 0.00007372
Iteration 132/1000 | Loss: 0.00007372
Iteration 133/1000 | Loss: 0.00007372
Iteration 134/1000 | Loss: 0.00007371
Iteration 135/1000 | Loss: 0.00007371
Iteration 136/1000 | Loss: 0.00007371
Iteration 137/1000 | Loss: 0.00007370
Iteration 138/1000 | Loss: 0.00007370
Iteration 139/1000 | Loss: 0.00007370
Iteration 140/1000 | Loss: 0.00007368
Iteration 141/1000 | Loss: 0.00007368
Iteration 142/1000 | Loss: 0.00007368
Iteration 143/1000 | Loss: 0.00007367
Iteration 144/1000 | Loss: 0.00007366
Iteration 145/1000 | Loss: 0.00007366
Iteration 146/1000 | Loss: 0.00007366
Iteration 147/1000 | Loss: 0.00007366
Iteration 148/1000 | Loss: 0.00007366
Iteration 149/1000 | Loss: 0.00007366
Iteration 150/1000 | Loss: 0.00007366
Iteration 151/1000 | Loss: 0.00007365
Iteration 152/1000 | Loss: 0.00007365
Iteration 153/1000 | Loss: 0.00007365
Iteration 154/1000 | Loss: 0.00007364
Iteration 155/1000 | Loss: 0.00007363
Iteration 156/1000 | Loss: 0.00007363
Iteration 157/1000 | Loss: 0.00007363
Iteration 158/1000 | Loss: 0.00007362
Iteration 159/1000 | Loss: 0.00007362
Iteration 160/1000 | Loss: 0.00007362
Iteration 161/1000 | Loss: 0.00007361
Iteration 162/1000 | Loss: 0.00007361
Iteration 163/1000 | Loss: 0.00007361
Iteration 164/1000 | Loss: 0.00007361
Iteration 165/1000 | Loss: 0.00007361
Iteration 166/1000 | Loss: 0.00007361
Iteration 167/1000 | Loss: 0.00007361
Iteration 168/1000 | Loss: 0.00007360
Iteration 169/1000 | Loss: 0.00007360
Iteration 170/1000 | Loss: 0.00007360
Iteration 171/1000 | Loss: 0.00007359
Iteration 172/1000 | Loss: 0.00007359
Iteration 173/1000 | Loss: 0.00007359
Iteration 174/1000 | Loss: 0.00007359
Iteration 175/1000 | Loss: 0.00007359
Iteration 176/1000 | Loss: 0.00007358
Iteration 177/1000 | Loss: 0.00007358
Iteration 178/1000 | Loss: 0.00007358
Iteration 179/1000 | Loss: 0.00007358
Iteration 180/1000 | Loss: 0.00007357
Iteration 181/1000 | Loss: 0.00007357
Iteration 182/1000 | Loss: 0.00007357
Iteration 183/1000 | Loss: 0.00007356
Iteration 184/1000 | Loss: 0.00007356
Iteration 185/1000 | Loss: 0.00007356
Iteration 186/1000 | Loss: 0.00007356
Iteration 187/1000 | Loss: 0.00007356
Iteration 188/1000 | Loss: 0.00007356
Iteration 189/1000 | Loss: 0.00007355
Iteration 190/1000 | Loss: 0.00007355
Iteration 191/1000 | Loss: 0.00007355
Iteration 192/1000 | Loss: 0.00007355
Iteration 193/1000 | Loss: 0.00007355
Iteration 194/1000 | Loss: 0.00007355
Iteration 195/1000 | Loss: 0.00007355
Iteration 196/1000 | Loss: 0.00007355
Iteration 197/1000 | Loss: 0.00007355
Iteration 198/1000 | Loss: 0.00007355
Iteration 199/1000 | Loss: 0.00007354
Iteration 200/1000 | Loss: 0.00007354
Iteration 201/1000 | Loss: 0.00007354
Iteration 202/1000 | Loss: 0.00007354
Iteration 203/1000 | Loss: 0.00007353
Iteration 204/1000 | Loss: 0.00007353
Iteration 205/1000 | Loss: 0.00007353
Iteration 206/1000 | Loss: 0.00007353
Iteration 207/1000 | Loss: 0.00007353
Iteration 208/1000 | Loss: 0.00007353
Iteration 209/1000 | Loss: 0.00007353
Iteration 210/1000 | Loss: 0.00007353
Iteration 211/1000 | Loss: 0.00007353
Iteration 212/1000 | Loss: 0.00007353
Iteration 213/1000 | Loss: 0.00007353
Iteration 214/1000 | Loss: 0.00007352
Iteration 215/1000 | Loss: 0.00007352
Iteration 216/1000 | Loss: 0.00007352
Iteration 217/1000 | Loss: 0.00007352
Iteration 218/1000 | Loss: 0.00007352
Iteration 219/1000 | Loss: 0.00007352
Iteration 220/1000 | Loss: 0.00007352
Iteration 221/1000 | Loss: 0.00007352
Iteration 222/1000 | Loss: 0.00007352
Iteration 223/1000 | Loss: 0.00007352
Iteration 224/1000 | Loss: 0.00007352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [7.3523391620256e-05, 7.3523391620256e-05, 7.3523391620256e-05, 7.3523391620256e-05, 7.3523391620256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.3523391620256e-05

Optimization complete. Final v2v error: 5.519657611846924 mm

Highest mean error: 10.684606552124023 mm for frame 13

Lowest mean error: 4.470881938934326 mm for frame 134

Saving results

Total time: 189.1165418624878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059520
Iteration 2/25 | Loss: 0.00299497
Iteration 3/25 | Loss: 0.00158015
Iteration 4/25 | Loss: 0.00145773
Iteration 5/25 | Loss: 0.00135313
Iteration 6/25 | Loss: 0.00128264
Iteration 7/25 | Loss: 0.00123686
Iteration 8/25 | Loss: 0.00118872
Iteration 9/25 | Loss: 0.00116960
Iteration 10/25 | Loss: 0.00116886
Iteration 11/25 | Loss: 0.00116671
Iteration 12/25 | Loss: 0.00115501
Iteration 13/25 | Loss: 0.00115097
Iteration 14/25 | Loss: 0.00114561
Iteration 15/25 | Loss: 0.00114279
Iteration 16/25 | Loss: 0.00113948
Iteration 17/25 | Loss: 0.00113814
Iteration 18/25 | Loss: 0.00113809
Iteration 19/25 | Loss: 0.00114071
Iteration 20/25 | Loss: 0.00113679
Iteration 21/25 | Loss: 0.00113261
Iteration 22/25 | Loss: 0.00113207
Iteration 23/25 | Loss: 0.00113187
Iteration 24/25 | Loss: 0.00113187
Iteration 25/25 | Loss: 0.00113186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36156583
Iteration 2/25 | Loss: 0.00090929
Iteration 3/25 | Loss: 0.00090929
Iteration 4/25 | Loss: 0.00090929
Iteration 5/25 | Loss: 0.00090929
Iteration 6/25 | Loss: 0.00090929
Iteration 7/25 | Loss: 0.00090929
Iteration 8/25 | Loss: 0.00090929
Iteration 9/25 | Loss: 0.00090929
Iteration 10/25 | Loss: 0.00090929
Iteration 11/25 | Loss: 0.00090929
Iteration 12/25 | Loss: 0.00090929
Iteration 13/25 | Loss: 0.00090929
Iteration 14/25 | Loss: 0.00090929
Iteration 15/25 | Loss: 0.00090929
Iteration 16/25 | Loss: 0.00090929
Iteration 17/25 | Loss: 0.00090929
Iteration 18/25 | Loss: 0.00090929
Iteration 19/25 | Loss: 0.00090929
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000909285678062588, 0.000909285678062588, 0.000909285678062588, 0.000909285678062588, 0.000909285678062588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000909285678062588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090929
Iteration 2/1000 | Loss: 0.00003431
Iteration 3/1000 | Loss: 0.00024762
Iteration 4/1000 | Loss: 0.00002667
Iteration 5/1000 | Loss: 0.00002401
Iteration 6/1000 | Loss: 0.00020800
Iteration 7/1000 | Loss: 0.00043697
Iteration 8/1000 | Loss: 0.00035099
Iteration 9/1000 | Loss: 0.00037735
Iteration 10/1000 | Loss: 0.00022535
Iteration 11/1000 | Loss: 0.00030064
Iteration 12/1000 | Loss: 0.00029760
Iteration 13/1000 | Loss: 0.00026868
Iteration 14/1000 | Loss: 0.00011702
Iteration 15/1000 | Loss: 0.00011093
Iteration 16/1000 | Loss: 0.00003983
Iteration 17/1000 | Loss: 0.00003173
Iteration 18/1000 | Loss: 0.00002624
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00002246
Iteration 21/1000 | Loss: 0.00020080
Iteration 22/1000 | Loss: 0.00073211
Iteration 23/1000 | Loss: 0.00031521
Iteration 24/1000 | Loss: 0.00039395
Iteration 25/1000 | Loss: 0.00062253
Iteration 26/1000 | Loss: 0.00042174
Iteration 27/1000 | Loss: 0.00017714
Iteration 28/1000 | Loss: 0.00004601
Iteration 29/1000 | Loss: 0.00003833
Iteration 30/1000 | Loss: 0.00012089
Iteration 31/1000 | Loss: 0.00003258
Iteration 32/1000 | Loss: 0.00002671
Iteration 33/1000 | Loss: 0.00002512
Iteration 34/1000 | Loss: 0.00003488
Iteration 35/1000 | Loss: 0.00002465
Iteration 36/1000 | Loss: 0.00002297
Iteration 37/1000 | Loss: 0.00022863
Iteration 38/1000 | Loss: 0.00016317
Iteration 39/1000 | Loss: 0.00021454
Iteration 40/1000 | Loss: 0.00002703
Iteration 41/1000 | Loss: 0.00002330
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00022958
Iteration 45/1000 | Loss: 0.00013671
Iteration 46/1000 | Loss: 0.00021587
Iteration 47/1000 | Loss: 0.00011229
Iteration 48/1000 | Loss: 0.00021060
Iteration 49/1000 | Loss: 0.00003046
Iteration 50/1000 | Loss: 0.00002294
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001851
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00002298
Iteration 60/1000 | Loss: 0.00001667
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00002018
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001585
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001555
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001540
Iteration 87/1000 | Loss: 0.00001534
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001531
Iteration 90/1000 | Loss: 0.00001530
Iteration 91/1000 | Loss: 0.00001530
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001513
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.5084653568919748e-05, 1.5084653568919748e-05, 1.5084653568919748e-05, 1.5084653568919748e-05, 1.5084653568919748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5084653568919748e-05

Optimization complete. Final v2v error: 3.2060508728027344 mm

Highest mean error: 8.814018249511719 mm for frame 127

Lowest mean error: 2.955005407333374 mm for frame 28

Saving results

Total time: 142.4361219406128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775222
Iteration 2/25 | Loss: 0.00156541
Iteration 3/25 | Loss: 0.00124943
Iteration 4/25 | Loss: 0.00118000
Iteration 5/25 | Loss: 0.00117174
Iteration 6/25 | Loss: 0.00112959
Iteration 7/25 | Loss: 0.00110276
Iteration 8/25 | Loss: 0.00109677
Iteration 9/25 | Loss: 0.00108521
Iteration 10/25 | Loss: 0.00108310
Iteration 11/25 | Loss: 0.00108302
Iteration 12/25 | Loss: 0.00109449
Iteration 13/25 | Loss: 0.00107482
Iteration 14/25 | Loss: 0.00107209
Iteration 15/25 | Loss: 0.00106883
Iteration 16/25 | Loss: 0.00106872
Iteration 17/25 | Loss: 0.00106872
Iteration 18/25 | Loss: 0.00106871
Iteration 19/25 | Loss: 0.00106871
Iteration 20/25 | Loss: 0.00106871
Iteration 21/25 | Loss: 0.00106871
Iteration 22/25 | Loss: 0.00106871
Iteration 23/25 | Loss: 0.00106870
Iteration 24/25 | Loss: 0.00106870
Iteration 25/25 | Loss: 0.00106870

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88309824
Iteration 2/25 | Loss: 0.00092748
Iteration 3/25 | Loss: 0.00084691
Iteration 4/25 | Loss: 0.00084691
Iteration 5/25 | Loss: 0.00084691
Iteration 6/25 | Loss: 0.00084691
Iteration 7/25 | Loss: 0.00084691
Iteration 8/25 | Loss: 0.00084691
Iteration 9/25 | Loss: 0.00084691
Iteration 10/25 | Loss: 0.00084691
Iteration 11/25 | Loss: 0.00084691
Iteration 12/25 | Loss: 0.00084691
Iteration 13/25 | Loss: 0.00084691
Iteration 14/25 | Loss: 0.00084691
Iteration 15/25 | Loss: 0.00084691
Iteration 16/25 | Loss: 0.00084691
Iteration 17/25 | Loss: 0.00084691
Iteration 18/25 | Loss: 0.00084691
Iteration 19/25 | Loss: 0.00084691
Iteration 20/25 | Loss: 0.00084691
Iteration 21/25 | Loss: 0.00084691
Iteration 22/25 | Loss: 0.00084691
Iteration 23/25 | Loss: 0.00084691
Iteration 24/25 | Loss: 0.00084691
Iteration 25/25 | Loss: 0.00084691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084691
Iteration 2/1000 | Loss: 0.00007400
Iteration 3/1000 | Loss: 0.00010254
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00001388
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00001187
Iteration 8/1000 | Loss: 0.00006487
Iteration 9/1000 | Loss: 0.00001139
Iteration 10/1000 | Loss: 0.00002140
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001045
Iteration 16/1000 | Loss: 0.00001039
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001021
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001018
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001016
Iteration 25/1000 | Loss: 0.00001016
Iteration 26/1000 | Loss: 0.00001014
Iteration 27/1000 | Loss: 0.00001014
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001011
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001010
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001006
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001005
Iteration 36/1000 | Loss: 0.00001004
Iteration 37/1000 | Loss: 0.00001003
Iteration 38/1000 | Loss: 0.00001002
Iteration 39/1000 | Loss: 0.00001001
Iteration 40/1000 | Loss: 0.00001001
Iteration 41/1000 | Loss: 0.00001000
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000998
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000997
Iteration 48/1000 | Loss: 0.00000997
Iteration 49/1000 | Loss: 0.00000996
Iteration 50/1000 | Loss: 0.00000996
Iteration 51/1000 | Loss: 0.00000996
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000995
Iteration 55/1000 | Loss: 0.00000995
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000994
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000989
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000987
Iteration 70/1000 | Loss: 0.00000987
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000986
Iteration 73/1000 | Loss: 0.00000986
Iteration 74/1000 | Loss: 0.00000986
Iteration 75/1000 | Loss: 0.00000986
Iteration 76/1000 | Loss: 0.00000985
Iteration 77/1000 | Loss: 0.00000985
Iteration 78/1000 | Loss: 0.00000985
Iteration 79/1000 | Loss: 0.00000985
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000983
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000979
Iteration 97/1000 | Loss: 0.00000979
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000977
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000974
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000973
Iteration 140/1000 | Loss: 0.00000973
Iteration 141/1000 | Loss: 0.00000973
Iteration 142/1000 | Loss: 0.00000973
Iteration 143/1000 | Loss: 0.00000973
Iteration 144/1000 | Loss: 0.00000973
Iteration 145/1000 | Loss: 0.00000973
Iteration 146/1000 | Loss: 0.00000972
Iteration 147/1000 | Loss: 0.00000972
Iteration 148/1000 | Loss: 0.00000972
Iteration 149/1000 | Loss: 0.00000972
Iteration 150/1000 | Loss: 0.00000972
Iteration 151/1000 | Loss: 0.00000972
Iteration 152/1000 | Loss: 0.00000972
Iteration 153/1000 | Loss: 0.00000971
Iteration 154/1000 | Loss: 0.00000971
Iteration 155/1000 | Loss: 0.00000971
Iteration 156/1000 | Loss: 0.00000971
Iteration 157/1000 | Loss: 0.00000971
Iteration 158/1000 | Loss: 0.00000971
Iteration 159/1000 | Loss: 0.00000971
Iteration 160/1000 | Loss: 0.00000971
Iteration 161/1000 | Loss: 0.00000971
Iteration 162/1000 | Loss: 0.00000971
Iteration 163/1000 | Loss: 0.00000970
Iteration 164/1000 | Loss: 0.00000970
Iteration 165/1000 | Loss: 0.00000970
Iteration 166/1000 | Loss: 0.00000970
Iteration 167/1000 | Loss: 0.00000970
Iteration 168/1000 | Loss: 0.00000970
Iteration 169/1000 | Loss: 0.00000970
Iteration 170/1000 | Loss: 0.00000970
Iteration 171/1000 | Loss: 0.00000970
Iteration 172/1000 | Loss: 0.00000970
Iteration 173/1000 | Loss: 0.00000970
Iteration 174/1000 | Loss: 0.00000970
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000969
Iteration 177/1000 | Loss: 0.00000969
Iteration 178/1000 | Loss: 0.00000969
Iteration 179/1000 | Loss: 0.00000969
Iteration 180/1000 | Loss: 0.00000969
Iteration 181/1000 | Loss: 0.00000969
Iteration 182/1000 | Loss: 0.00000969
Iteration 183/1000 | Loss: 0.00000969
Iteration 184/1000 | Loss: 0.00000969
Iteration 185/1000 | Loss: 0.00000969
Iteration 186/1000 | Loss: 0.00000969
Iteration 187/1000 | Loss: 0.00000969
Iteration 188/1000 | Loss: 0.00000969
Iteration 189/1000 | Loss: 0.00000969
Iteration 190/1000 | Loss: 0.00000969
Iteration 191/1000 | Loss: 0.00000969
Iteration 192/1000 | Loss: 0.00000969
Iteration 193/1000 | Loss: 0.00000968
Iteration 194/1000 | Loss: 0.00000968
Iteration 195/1000 | Loss: 0.00000968
Iteration 196/1000 | Loss: 0.00000968
Iteration 197/1000 | Loss: 0.00000968
Iteration 198/1000 | Loss: 0.00000968
Iteration 199/1000 | Loss: 0.00000968
Iteration 200/1000 | Loss: 0.00000968
Iteration 201/1000 | Loss: 0.00000968
Iteration 202/1000 | Loss: 0.00000968
Iteration 203/1000 | Loss: 0.00000968
Iteration 204/1000 | Loss: 0.00000968
Iteration 205/1000 | Loss: 0.00000968
Iteration 206/1000 | Loss: 0.00000968
Iteration 207/1000 | Loss: 0.00000968
Iteration 208/1000 | Loss: 0.00000968
Iteration 209/1000 | Loss: 0.00000968
Iteration 210/1000 | Loss: 0.00000968
Iteration 211/1000 | Loss: 0.00000968
Iteration 212/1000 | Loss: 0.00000968
Iteration 213/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [9.681459232524503e-06, 9.681459232524503e-06, 9.681459232524503e-06, 9.681459232524503e-06, 9.681459232524503e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.681459232524503e-06

Optimization complete. Final v2v error: 2.6801328659057617 mm

Highest mean error: 3.0247132778167725 mm for frame 77

Lowest mean error: 2.457273006439209 mm for frame 126

Saving results

Total time: 66.95121359825134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749661
Iteration 2/25 | Loss: 0.00125144
Iteration 3/25 | Loss: 0.00087045
Iteration 4/25 | Loss: 0.00079735
Iteration 5/25 | Loss: 0.00078154
Iteration 6/25 | Loss: 0.00077778
Iteration 7/25 | Loss: 0.00077744
Iteration 8/25 | Loss: 0.00077744
Iteration 9/25 | Loss: 0.00077744
Iteration 10/25 | Loss: 0.00077744
Iteration 11/25 | Loss: 0.00077744
Iteration 12/25 | Loss: 0.00077744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007774372934363782, 0.0007774372934363782, 0.0007774372934363782, 0.0007774372934363782, 0.0007774372934363782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007774372934363782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28898072
Iteration 2/25 | Loss: 0.00043960
Iteration 3/25 | Loss: 0.00043959
Iteration 4/25 | Loss: 0.00043959
Iteration 5/25 | Loss: 0.00043959
Iteration 6/25 | Loss: 0.00043959
Iteration 7/25 | Loss: 0.00043959
Iteration 8/25 | Loss: 0.00043959
Iteration 9/25 | Loss: 0.00043959
Iteration 10/25 | Loss: 0.00043959
Iteration 11/25 | Loss: 0.00043959
Iteration 12/25 | Loss: 0.00043959
Iteration 13/25 | Loss: 0.00043959
Iteration 14/25 | Loss: 0.00043959
Iteration 15/25 | Loss: 0.00043959
Iteration 16/25 | Loss: 0.00043959
Iteration 17/25 | Loss: 0.00043959
Iteration 18/25 | Loss: 0.00043959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004395871073938906, 0.0004395871073938906, 0.0004395871073938906, 0.0004395871073938906, 0.0004395871073938906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004395871073938906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043959
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00003090
Iteration 4/1000 | Loss: 0.00002837
Iteration 5/1000 | Loss: 0.00002716
Iteration 6/1000 | Loss: 0.00002640
Iteration 7/1000 | Loss: 0.00002546
Iteration 8/1000 | Loss: 0.00002488
Iteration 9/1000 | Loss: 0.00002447
Iteration 10/1000 | Loss: 0.00002435
Iteration 11/1000 | Loss: 0.00002421
Iteration 12/1000 | Loss: 0.00002410
Iteration 13/1000 | Loss: 0.00002400
Iteration 14/1000 | Loss: 0.00002399
Iteration 15/1000 | Loss: 0.00002398
Iteration 16/1000 | Loss: 0.00002395
Iteration 17/1000 | Loss: 0.00002394
Iteration 18/1000 | Loss: 0.00002392
Iteration 19/1000 | Loss: 0.00002391
Iteration 20/1000 | Loss: 0.00002388
Iteration 21/1000 | Loss: 0.00002387
Iteration 22/1000 | Loss: 0.00002386
Iteration 23/1000 | Loss: 0.00002386
Iteration 24/1000 | Loss: 0.00002385
Iteration 25/1000 | Loss: 0.00002385
Iteration 26/1000 | Loss: 0.00002385
Iteration 27/1000 | Loss: 0.00002384
Iteration 28/1000 | Loss: 0.00002384
Iteration 29/1000 | Loss: 0.00002383
Iteration 30/1000 | Loss: 0.00002383
Iteration 31/1000 | Loss: 0.00002383
Iteration 32/1000 | Loss: 0.00002382
Iteration 33/1000 | Loss: 0.00002382
Iteration 34/1000 | Loss: 0.00002382
Iteration 35/1000 | Loss: 0.00002382
Iteration 36/1000 | Loss: 0.00002381
Iteration 37/1000 | Loss: 0.00002380
Iteration 38/1000 | Loss: 0.00002379
Iteration 39/1000 | Loss: 0.00002377
Iteration 40/1000 | Loss: 0.00002377
Iteration 41/1000 | Loss: 0.00002377
Iteration 42/1000 | Loss: 0.00002377
Iteration 43/1000 | Loss: 0.00002376
Iteration 44/1000 | Loss: 0.00002376
Iteration 45/1000 | Loss: 0.00002374
Iteration 46/1000 | Loss: 0.00002374
Iteration 47/1000 | Loss: 0.00002374
Iteration 48/1000 | Loss: 0.00002374
Iteration 49/1000 | Loss: 0.00002374
Iteration 50/1000 | Loss: 0.00002374
Iteration 51/1000 | Loss: 0.00002374
Iteration 52/1000 | Loss: 0.00002373
Iteration 53/1000 | Loss: 0.00002373
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002373
Iteration 56/1000 | Loss: 0.00002373
Iteration 57/1000 | Loss: 0.00002373
Iteration 58/1000 | Loss: 0.00002371
Iteration 59/1000 | Loss: 0.00002371
Iteration 60/1000 | Loss: 0.00002370
Iteration 61/1000 | Loss: 0.00002370
Iteration 62/1000 | Loss: 0.00002369
Iteration 63/1000 | Loss: 0.00002369
Iteration 64/1000 | Loss: 0.00002369
Iteration 65/1000 | Loss: 0.00002368
Iteration 66/1000 | Loss: 0.00002368
Iteration 67/1000 | Loss: 0.00002368
Iteration 68/1000 | Loss: 0.00002367
Iteration 69/1000 | Loss: 0.00002367
Iteration 70/1000 | Loss: 0.00002366
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002366
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002365
Iteration 78/1000 | Loss: 0.00002365
Iteration 79/1000 | Loss: 0.00002365
Iteration 80/1000 | Loss: 0.00002365
Iteration 81/1000 | Loss: 0.00002365
Iteration 82/1000 | Loss: 0.00002365
Iteration 83/1000 | Loss: 0.00002365
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00002365
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.3646056433790363e-05, 2.3646056433790363e-05, 2.3646056433790363e-05, 2.3646056433790363e-05, 2.3646056433790363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3646056433790363e-05

Optimization complete. Final v2v error: 4.204261302947998 mm

Highest mean error: 4.49455451965332 mm for frame 24

Lowest mean error: 3.907155752182007 mm for frame 266

Saving results

Total time: 38.400264501571655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125128
Iteration 2/25 | Loss: 0.00279923
Iteration 3/25 | Loss: 0.00164985
Iteration 4/25 | Loss: 0.00163430
Iteration 5/25 | Loss: 0.00158931
Iteration 6/25 | Loss: 0.00137558
Iteration 7/25 | Loss: 0.00128437
Iteration 8/25 | Loss: 0.00122077
Iteration 9/25 | Loss: 0.00118484
Iteration 10/25 | Loss: 0.00114286
Iteration 11/25 | Loss: 0.00113945
Iteration 12/25 | Loss: 0.00111548
Iteration 13/25 | Loss: 0.00112365
Iteration 14/25 | Loss: 0.00109692
Iteration 15/25 | Loss: 0.00108339
Iteration 16/25 | Loss: 0.00106293
Iteration 17/25 | Loss: 0.00106175
Iteration 18/25 | Loss: 0.00108019
Iteration 19/25 | Loss: 0.00106018
Iteration 20/25 | Loss: 0.00106644
Iteration 21/25 | Loss: 0.00105346
Iteration 22/25 | Loss: 0.00105691
Iteration 23/25 | Loss: 0.00106335
Iteration 24/25 | Loss: 0.00106072
Iteration 25/25 | Loss: 0.00103521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38091087
Iteration 2/25 | Loss: 0.00807097
Iteration 3/25 | Loss: 0.00529388
Iteration 4/25 | Loss: 0.00529386
Iteration 5/25 | Loss: 0.00529386
Iteration 6/25 | Loss: 0.00529385
Iteration 7/25 | Loss: 0.00529385
Iteration 8/25 | Loss: 0.00529385
Iteration 9/25 | Loss: 0.00529385
Iteration 10/25 | Loss: 0.00529385
Iteration 11/25 | Loss: 0.00529385
Iteration 12/25 | Loss: 0.00529385
Iteration 13/25 | Loss: 0.00529385
Iteration 14/25 | Loss: 0.00529385
Iteration 15/25 | Loss: 0.00529385
Iteration 16/25 | Loss: 0.00529385
Iteration 17/25 | Loss: 0.00529385
Iteration 18/25 | Loss: 0.00529385
Iteration 19/25 | Loss: 0.00529385
Iteration 20/25 | Loss: 0.00529385
Iteration 21/25 | Loss: 0.00529385
Iteration 22/25 | Loss: 0.00529385
Iteration 23/25 | Loss: 0.00529385
Iteration 24/25 | Loss: 0.00529385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.005293852183967829, 0.005293852183967829, 0.005293852183967829, 0.005293852183967829, 0.005293852183967829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005293852183967829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00529385
Iteration 2/1000 | Loss: 0.00374844
Iteration 3/1000 | Loss: 0.00494851
Iteration 4/1000 | Loss: 0.00663606
Iteration 5/1000 | Loss: 0.00471226
Iteration 6/1000 | Loss: 0.00429867
Iteration 7/1000 | Loss: 0.00702136
Iteration 8/1000 | Loss: 0.00953563
Iteration 9/1000 | Loss: 0.00457954
Iteration 10/1000 | Loss: 0.00467100
Iteration 11/1000 | Loss: 0.00427912
Iteration 12/1000 | Loss: 0.00323031
Iteration 13/1000 | Loss: 0.00780364
Iteration 14/1000 | Loss: 0.00852865
Iteration 15/1000 | Loss: 0.00649981
Iteration 16/1000 | Loss: 0.00684424
Iteration 17/1000 | Loss: 0.00720002
Iteration 18/1000 | Loss: 0.00894199
Iteration 19/1000 | Loss: 0.00635159
Iteration 20/1000 | Loss: 0.00802342
Iteration 21/1000 | Loss: 0.00552709
Iteration 22/1000 | Loss: 0.00933715
Iteration 23/1000 | Loss: 0.00610118
Iteration 24/1000 | Loss: 0.00769408
Iteration 25/1000 | Loss: 0.00732361
Iteration 26/1000 | Loss: 0.00658321
Iteration 27/1000 | Loss: 0.00471229
Iteration 28/1000 | Loss: 0.00431131
Iteration 29/1000 | Loss: 0.00708776
Iteration 30/1000 | Loss: 0.00364528
Iteration 31/1000 | Loss: 0.00383488
Iteration 32/1000 | Loss: 0.00399256
Iteration 33/1000 | Loss: 0.00388358
Iteration 34/1000 | Loss: 0.00397499
Iteration 35/1000 | Loss: 0.00597475
Iteration 36/1000 | Loss: 0.00334189
Iteration 37/1000 | Loss: 0.00427268
Iteration 38/1000 | Loss: 0.00218486
Iteration 39/1000 | Loss: 0.00226786
Iteration 40/1000 | Loss: 0.00298518
Iteration 41/1000 | Loss: 0.00241833
Iteration 42/1000 | Loss: 0.00227850
Iteration 43/1000 | Loss: 0.00206198
Iteration 44/1000 | Loss: 0.00287327
Iteration 45/1000 | Loss: 0.00201959
Iteration 46/1000 | Loss: 0.00503100
Iteration 47/1000 | Loss: 0.00519398
Iteration 48/1000 | Loss: 0.00209446
Iteration 49/1000 | Loss: 0.00188667
Iteration 50/1000 | Loss: 0.00331939
Iteration 51/1000 | Loss: 0.00222351
Iteration 52/1000 | Loss: 0.00405497
Iteration 53/1000 | Loss: 0.00390792
Iteration 54/1000 | Loss: 0.00697185
Iteration 55/1000 | Loss: 0.00199432
Iteration 56/1000 | Loss: 0.00275666
Iteration 57/1000 | Loss: 0.00456002
Iteration 58/1000 | Loss: 0.00419334
Iteration 59/1000 | Loss: 0.00407336
Iteration 60/1000 | Loss: 0.00527208
Iteration 61/1000 | Loss: 0.00550843
Iteration 62/1000 | Loss: 0.00469379
Iteration 63/1000 | Loss: 0.00484163
Iteration 64/1000 | Loss: 0.00390021
Iteration 65/1000 | Loss: 0.00510153
Iteration 66/1000 | Loss: 0.00402664
Iteration 67/1000 | Loss: 0.00176739
Iteration 68/1000 | Loss: 0.00167361
Iteration 69/1000 | Loss: 0.00204717
Iteration 70/1000 | Loss: 0.00221949
Iteration 71/1000 | Loss: 0.00454505
Iteration 72/1000 | Loss: 0.00325970
Iteration 73/1000 | Loss: 0.00193442
Iteration 74/1000 | Loss: 0.00193032
Iteration 75/1000 | Loss: 0.00315338
Iteration 76/1000 | Loss: 0.00233142
Iteration 77/1000 | Loss: 0.00359726
Iteration 78/1000 | Loss: 0.00505026
Iteration 79/1000 | Loss: 0.00226729
Iteration 80/1000 | Loss: 0.00191105
Iteration 81/1000 | Loss: 0.00273993
Iteration 82/1000 | Loss: 0.00184579
Iteration 83/1000 | Loss: 0.00239935
Iteration 84/1000 | Loss: 0.00255987
Iteration 85/1000 | Loss: 0.00238171
Iteration 86/1000 | Loss: 0.00163123
Iteration 87/1000 | Loss: 0.00409656
Iteration 88/1000 | Loss: 0.00234646
Iteration 89/1000 | Loss: 0.00201788
Iteration 90/1000 | Loss: 0.00137211
Iteration 91/1000 | Loss: 0.00156994
Iteration 92/1000 | Loss: 0.00492521
Iteration 93/1000 | Loss: 0.00312610
Iteration 94/1000 | Loss: 0.00122215
Iteration 95/1000 | Loss: 0.00353971
Iteration 96/1000 | Loss: 0.00155746
Iteration 97/1000 | Loss: 0.00184703
Iteration 98/1000 | Loss: 0.00166566
Iteration 99/1000 | Loss: 0.00231131
Iteration 100/1000 | Loss: 0.00207150
Iteration 101/1000 | Loss: 0.00187253
Iteration 102/1000 | Loss: 0.00189435
Iteration 103/1000 | Loss: 0.00175545
Iteration 104/1000 | Loss: 0.00180316
Iteration 105/1000 | Loss: 0.00352011
Iteration 106/1000 | Loss: 0.00200223
Iteration 107/1000 | Loss: 0.00355302
Iteration 108/1000 | Loss: 0.00212127
Iteration 109/1000 | Loss: 0.00432291
Iteration 110/1000 | Loss: 0.00205727
Iteration 111/1000 | Loss: 0.00326606
Iteration 112/1000 | Loss: 0.00205509
Iteration 113/1000 | Loss: 0.00354180
Iteration 114/1000 | Loss: 0.00815468
Iteration 115/1000 | Loss: 0.00281574
Iteration 116/1000 | Loss: 0.00673949
Iteration 117/1000 | Loss: 0.00327340
Iteration 118/1000 | Loss: 0.00571336
Iteration 119/1000 | Loss: 0.00385784
Iteration 120/1000 | Loss: 0.00417905
Iteration 121/1000 | Loss: 0.00306723
Iteration 122/1000 | Loss: 0.00349336
Iteration 123/1000 | Loss: 0.00577702
Iteration 124/1000 | Loss: 0.00193343
Iteration 125/1000 | Loss: 0.00608612
Iteration 126/1000 | Loss: 0.00477376
Iteration 127/1000 | Loss: 0.00294512
Iteration 128/1000 | Loss: 0.00430824
Iteration 129/1000 | Loss: 0.00249072
Iteration 130/1000 | Loss: 0.00459814
Iteration 131/1000 | Loss: 0.00370017
Iteration 132/1000 | Loss: 0.00640882
Iteration 133/1000 | Loss: 0.00341030
Iteration 134/1000 | Loss: 0.00273076
Iteration 135/1000 | Loss: 0.00393424
Iteration 136/1000 | Loss: 0.00254943
Iteration 137/1000 | Loss: 0.00396087
Iteration 138/1000 | Loss: 0.00247567
Iteration 139/1000 | Loss: 0.00267373
Iteration 140/1000 | Loss: 0.00165463
Iteration 141/1000 | Loss: 0.00337563
Iteration 142/1000 | Loss: 0.00284080
Iteration 143/1000 | Loss: 0.00190194
Iteration 144/1000 | Loss: 0.00427061
Iteration 145/1000 | Loss: 0.00249114
Iteration 146/1000 | Loss: 0.00176360
Iteration 147/1000 | Loss: 0.00200672
Iteration 148/1000 | Loss: 0.00278995
Iteration 149/1000 | Loss: 0.00187906
Iteration 150/1000 | Loss: 0.00191733
Iteration 151/1000 | Loss: 0.00199994
Iteration 152/1000 | Loss: 0.00258706
Iteration 153/1000 | Loss: 0.00249855
Iteration 154/1000 | Loss: 0.00306883
Iteration 155/1000 | Loss: 0.00236562
Iteration 156/1000 | Loss: 0.00316673
Iteration 157/1000 | Loss: 0.00234908
Iteration 158/1000 | Loss: 0.00192593
Iteration 159/1000 | Loss: 0.00198818
Iteration 160/1000 | Loss: 0.00187264
Iteration 161/1000 | Loss: 0.00210281
Iteration 162/1000 | Loss: 0.00186602
Iteration 163/1000 | Loss: 0.00255472
Iteration 164/1000 | Loss: 0.00160734
Iteration 165/1000 | Loss: 0.00178347
Iteration 166/1000 | Loss: 0.00315539
Iteration 167/1000 | Loss: 0.00397177
Iteration 168/1000 | Loss: 0.00224726
Iteration 169/1000 | Loss: 0.00387094
Iteration 170/1000 | Loss: 0.00347986
Iteration 171/1000 | Loss: 0.00405954
Iteration 172/1000 | Loss: 0.00220507
Iteration 173/1000 | Loss: 0.00219514
Iteration 174/1000 | Loss: 0.00188478
Iteration 175/1000 | Loss: 0.00218332
Iteration 176/1000 | Loss: 0.00162716
Iteration 177/1000 | Loss: 0.00290715
Iteration 178/1000 | Loss: 0.00350653
Iteration 179/1000 | Loss: 0.00142707
Iteration 180/1000 | Loss: 0.00237605
Iteration 181/1000 | Loss: 0.00164173
Iteration 182/1000 | Loss: 0.00237566
Iteration 183/1000 | Loss: 0.00212245
Iteration 184/1000 | Loss: 0.00190831
Iteration 185/1000 | Loss: 0.00204738
Iteration 186/1000 | Loss: 0.00207892
Iteration 187/1000 | Loss: 0.00252230
Iteration 188/1000 | Loss: 0.00306212
Iteration 189/1000 | Loss: 0.00220362
Iteration 190/1000 | Loss: 0.00346294
Iteration 191/1000 | Loss: 0.00200119
Iteration 192/1000 | Loss: 0.00189806
Iteration 193/1000 | Loss: 0.00366225
Iteration 194/1000 | Loss: 0.00201377
Iteration 195/1000 | Loss: 0.00215451
Iteration 196/1000 | Loss: 0.00186751
Iteration 197/1000 | Loss: 0.00223218
Iteration 198/1000 | Loss: 0.00192662
Iteration 199/1000 | Loss: 0.00198324
Iteration 200/1000 | Loss: 0.00369969
Iteration 201/1000 | Loss: 0.00317275
Iteration 202/1000 | Loss: 0.00326180
Iteration 203/1000 | Loss: 0.00348506
Iteration 204/1000 | Loss: 0.00269036
Iteration 205/1000 | Loss: 0.00211934
Iteration 206/1000 | Loss: 0.00252292
Iteration 207/1000 | Loss: 0.00314017
Iteration 208/1000 | Loss: 0.00175020
Iteration 209/1000 | Loss: 0.00166150
Iteration 210/1000 | Loss: 0.00201724
Iteration 211/1000 | Loss: 0.00232997
Iteration 212/1000 | Loss: 0.00241081
Iteration 213/1000 | Loss: 0.00235974
Iteration 214/1000 | Loss: 0.00258134
Iteration 215/1000 | Loss: 0.00233897
Iteration 216/1000 | Loss: 0.00248653
Iteration 217/1000 | Loss: 0.00184890
Iteration 218/1000 | Loss: 0.00208773
Iteration 219/1000 | Loss: 0.00167986
Iteration 220/1000 | Loss: 0.00202496
Iteration 221/1000 | Loss: 0.00157149
Iteration 222/1000 | Loss: 0.00235960
Iteration 223/1000 | Loss: 0.00195341
Iteration 224/1000 | Loss: 0.00300064
Iteration 225/1000 | Loss: 0.00266196
Iteration 226/1000 | Loss: 0.00323224
Iteration 227/1000 | Loss: 0.00291698
Iteration 228/1000 | Loss: 0.00269092
Iteration 229/1000 | Loss: 0.00329183
Iteration 230/1000 | Loss: 0.00276802
Iteration 231/1000 | Loss: 0.00216664
Iteration 232/1000 | Loss: 0.00206487
Iteration 233/1000 | Loss: 0.00179891
Iteration 234/1000 | Loss: 0.00186485
Iteration 235/1000 | Loss: 0.00356624
Iteration 236/1000 | Loss: 0.00331410
Iteration 237/1000 | Loss: 0.00404857
Iteration 238/1000 | Loss: 0.00181694
Iteration 239/1000 | Loss: 0.00210136
Iteration 240/1000 | Loss: 0.00292508
Iteration 241/1000 | Loss: 0.00185666
Iteration 242/1000 | Loss: 0.00321907
Iteration 243/1000 | Loss: 0.00202080
Iteration 244/1000 | Loss: 0.00193382
Iteration 245/1000 | Loss: 0.00190256
Iteration 246/1000 | Loss: 0.00178241
Iteration 247/1000 | Loss: 0.00208215
Iteration 248/1000 | Loss: 0.00178354
Iteration 249/1000 | Loss: 0.00350103
Iteration 250/1000 | Loss: 0.00218505
Iteration 251/1000 | Loss: 0.00189314
Iteration 252/1000 | Loss: 0.00184581
Iteration 253/1000 | Loss: 0.00174653
Iteration 254/1000 | Loss: 0.00187000
Iteration 255/1000 | Loss: 0.00172234
Iteration 256/1000 | Loss: 0.00191070
Iteration 257/1000 | Loss: 0.00198795
Iteration 258/1000 | Loss: 0.00191308
Iteration 259/1000 | Loss: 0.00173077
Iteration 260/1000 | Loss: 0.00403710
Iteration 261/1000 | Loss: 0.00155285
Iteration 262/1000 | Loss: 0.00268699
Iteration 263/1000 | Loss: 0.00174777
Iteration 264/1000 | Loss: 0.00179434
Iteration 265/1000 | Loss: 0.00181759
Iteration 266/1000 | Loss: 0.00164887
Iteration 267/1000 | Loss: 0.00177023
Iteration 268/1000 | Loss: 0.00339287
Iteration 269/1000 | Loss: 0.00195790
Iteration 270/1000 | Loss: 0.00162344
Iteration 271/1000 | Loss: 0.00517628
Iteration 272/1000 | Loss: 0.00261448
Iteration 273/1000 | Loss: 0.00215979
Iteration 274/1000 | Loss: 0.00195742
Iteration 275/1000 | Loss: 0.00323215
Iteration 276/1000 | Loss: 0.00165246
Iteration 277/1000 | Loss: 0.00171496
Iteration 278/1000 | Loss: 0.00174747
Iteration 279/1000 | Loss: 0.00255252
Iteration 280/1000 | Loss: 0.00165860
Iteration 281/1000 | Loss: 0.00227713
Iteration 282/1000 | Loss: 0.00293588
Iteration 283/1000 | Loss: 0.00220105
Iteration 284/1000 | Loss: 0.00184985
Iteration 285/1000 | Loss: 0.00200250
Iteration 286/1000 | Loss: 0.00187249
Iteration 287/1000 | Loss: 0.00177997
Iteration 288/1000 | Loss: 0.00163299
Iteration 289/1000 | Loss: 0.00145075
Iteration 290/1000 | Loss: 0.00147142
Iteration 291/1000 | Loss: 0.00152647
Iteration 292/1000 | Loss: 0.00140534
Iteration 293/1000 | Loss: 0.00145684
Iteration 294/1000 | Loss: 0.00282767
Iteration 295/1000 | Loss: 0.00150832
Iteration 296/1000 | Loss: 0.00492007
Iteration 297/1000 | Loss: 0.00127450
Iteration 298/1000 | Loss: 0.00186149
Iteration 299/1000 | Loss: 0.00182732
Iteration 300/1000 | Loss: 0.00154493
Iteration 301/1000 | Loss: 0.00166592
Iteration 302/1000 | Loss: 0.00167832
Iteration 303/1000 | Loss: 0.00188591
Iteration 304/1000 | Loss: 0.00250043
Iteration 305/1000 | Loss: 0.00285358
Iteration 306/1000 | Loss: 0.00159017
Iteration 307/1000 | Loss: 0.00200368
Iteration 308/1000 | Loss: 0.00169559
Iteration 309/1000 | Loss: 0.00138719
Iteration 310/1000 | Loss: 0.00181735
Iteration 311/1000 | Loss: 0.00133832
Iteration 312/1000 | Loss: 0.00157162
Iteration 313/1000 | Loss: 0.00266537
Iteration 314/1000 | Loss: 0.00281412
Iteration 315/1000 | Loss: 0.00135772
Iteration 316/1000 | Loss: 0.00418499
Iteration 317/1000 | Loss: 0.00402598
Iteration 318/1000 | Loss: 0.00164366
Iteration 319/1000 | Loss: 0.00168514
Iteration 320/1000 | Loss: 0.00124510
Iteration 321/1000 | Loss: 0.00130326
Iteration 322/1000 | Loss: 0.00265452
Iteration 323/1000 | Loss: 0.00217609
Iteration 324/1000 | Loss: 0.00229425
Iteration 325/1000 | Loss: 0.00453026
Iteration 326/1000 | Loss: 0.00256146
Iteration 327/1000 | Loss: 0.00361637
Iteration 328/1000 | Loss: 0.00294596
Iteration 329/1000 | Loss: 0.00369595
Iteration 330/1000 | Loss: 0.00348629
Iteration 331/1000 | Loss: 0.00179104
Iteration 332/1000 | Loss: 0.00182869
Iteration 333/1000 | Loss: 0.00206015
Iteration 334/1000 | Loss: 0.00140756
Iteration 335/1000 | Loss: 0.00336497
Iteration 336/1000 | Loss: 0.00188597
Iteration 337/1000 | Loss: 0.00302516
Iteration 338/1000 | Loss: 0.00374890
Iteration 339/1000 | Loss: 0.00330431
Iteration 340/1000 | Loss: 0.00239098
Iteration 341/1000 | Loss: 0.00175997
Iteration 342/1000 | Loss: 0.00138065
Iteration 343/1000 | Loss: 0.00154779
Iteration 344/1000 | Loss: 0.00133556
Iteration 345/1000 | Loss: 0.00144207
Iteration 346/1000 | Loss: 0.00225498
Iteration 347/1000 | Loss: 0.00144948
Iteration 348/1000 | Loss: 0.00138761
Iteration 349/1000 | Loss: 0.00267963
Iteration 350/1000 | Loss: 0.00138522
Iteration 351/1000 | Loss: 0.00214188
Iteration 352/1000 | Loss: 0.00149169
Iteration 353/1000 | Loss: 0.00280096
Iteration 354/1000 | Loss: 0.00180799
Iteration 355/1000 | Loss: 0.00159031
Iteration 356/1000 | Loss: 0.00150263
Iteration 357/1000 | Loss: 0.00145164
Iteration 358/1000 | Loss: 0.00125413
Iteration 359/1000 | Loss: 0.00203946
Iteration 360/1000 | Loss: 0.00153381
Iteration 361/1000 | Loss: 0.00166539
Iteration 362/1000 | Loss: 0.00138093
Iteration 363/1000 | Loss: 0.00204169
Iteration 364/1000 | Loss: 0.00154559
Iteration 365/1000 | Loss: 0.00185639
Iteration 366/1000 | Loss: 0.00180577
Iteration 367/1000 | Loss: 0.00138438
Iteration 368/1000 | Loss: 0.00165304
Iteration 369/1000 | Loss: 0.00132843
Iteration 370/1000 | Loss: 0.00207034
Iteration 371/1000 | Loss: 0.00143596
Iteration 372/1000 | Loss: 0.00186271
Iteration 373/1000 | Loss: 0.00262079
Iteration 374/1000 | Loss: 0.00135889
Iteration 375/1000 | Loss: 0.00123297
Iteration 376/1000 | Loss: 0.00171950
Iteration 377/1000 | Loss: 0.00133292
Iteration 378/1000 | Loss: 0.00160160
Iteration 379/1000 | Loss: 0.00148479
Iteration 380/1000 | Loss: 0.00138828
Iteration 381/1000 | Loss: 0.00139339
Iteration 382/1000 | Loss: 0.00134917
Iteration 383/1000 | Loss: 0.00137854
Iteration 384/1000 | Loss: 0.00199755
Iteration 385/1000 | Loss: 0.00145541
Iteration 386/1000 | Loss: 0.00176657
Iteration 387/1000 | Loss: 0.00264048
Iteration 388/1000 | Loss: 0.00139303
Iteration 389/1000 | Loss: 0.00145728
Iteration 390/1000 | Loss: 0.00137445
Iteration 391/1000 | Loss: 0.00140336
Iteration 392/1000 | Loss: 0.00153809
Iteration 393/1000 | Loss: 0.00133404
Iteration 394/1000 | Loss: 0.00159553
Iteration 395/1000 | Loss: 0.00299860
Iteration 396/1000 | Loss: 0.00117053
Iteration 397/1000 | Loss: 0.00153182
Iteration 398/1000 | Loss: 0.00167535
Iteration 399/1000 | Loss: 0.00166442
Iteration 400/1000 | Loss: 0.00167138
Iteration 401/1000 | Loss: 0.00148785
Iteration 402/1000 | Loss: 0.00171842
Iteration 403/1000 | Loss: 0.00156049
Iteration 404/1000 | Loss: 0.00177686
Iteration 405/1000 | Loss: 0.00156895
Iteration 406/1000 | Loss: 0.00240615
Iteration 407/1000 | Loss: 0.00150592
Iteration 408/1000 | Loss: 0.00187700
Iteration 409/1000 | Loss: 0.00164171
Iteration 410/1000 | Loss: 0.00138000
Iteration 411/1000 | Loss: 0.00176248
Iteration 412/1000 | Loss: 0.00140453
Iteration 413/1000 | Loss: 0.00167824
Iteration 414/1000 | Loss: 0.00166588
Iteration 415/1000 | Loss: 0.00138264
Iteration 416/1000 | Loss: 0.00145023
Iteration 417/1000 | Loss: 0.00154929
Iteration 418/1000 | Loss: 0.00223627
Iteration 419/1000 | Loss: 0.00158308
Iteration 420/1000 | Loss: 0.00140956
Iteration 421/1000 | Loss: 0.00145652
Iteration 422/1000 | Loss: 0.00174807
Iteration 423/1000 | Loss: 0.00160036
Iteration 424/1000 | Loss: 0.00249123
Iteration 425/1000 | Loss: 0.00167497
Iteration 426/1000 | Loss: 0.00264851
Iteration 427/1000 | Loss: 0.00143657
Iteration 428/1000 | Loss: 0.00147243
Iteration 429/1000 | Loss: 0.00184701
Iteration 430/1000 | Loss: 0.00134948
Iteration 431/1000 | Loss: 0.00139557
Iteration 432/1000 | Loss: 0.00139202
Iteration 433/1000 | Loss: 0.00122844
Iteration 434/1000 | Loss: 0.00161247
Iteration 435/1000 | Loss: 0.00188416
Iteration 436/1000 | Loss: 0.00172166
Iteration 437/1000 | Loss: 0.00215311
Iteration 438/1000 | Loss: 0.00179875
Iteration 439/1000 | Loss: 0.00169562
Iteration 440/1000 | Loss: 0.00184576
Iteration 441/1000 | Loss: 0.00242118
Iteration 442/1000 | Loss: 0.00186196
Iteration 443/1000 | Loss: 0.00162915
Iteration 444/1000 | Loss: 0.00054395
Iteration 445/1000 | Loss: 0.00058157
Iteration 446/1000 | Loss: 0.00077464
Iteration 447/1000 | Loss: 0.00062607
Iteration 448/1000 | Loss: 0.00058481
Iteration 449/1000 | Loss: 0.00083222
Iteration 450/1000 | Loss: 0.00071424
Iteration 451/1000 | Loss: 0.00080516
Iteration 452/1000 | Loss: 0.00092712
Iteration 453/1000 | Loss: 0.00135217
Iteration 454/1000 | Loss: 0.00037067
Iteration 455/1000 | Loss: 0.00051510
Iteration 456/1000 | Loss: 0.00045264
Iteration 457/1000 | Loss: 0.00036714
Iteration 458/1000 | Loss: 0.00032225
Iteration 459/1000 | Loss: 0.00052362
Iteration 460/1000 | Loss: 0.00037484
Iteration 461/1000 | Loss: 0.00043865
Iteration 462/1000 | Loss: 0.00042700
Iteration 463/1000 | Loss: 0.00042074
Iteration 464/1000 | Loss: 0.00042797
Iteration 465/1000 | Loss: 0.00075042
Iteration 466/1000 | Loss: 0.00055954
Iteration 467/1000 | Loss: 0.00039865
Iteration 468/1000 | Loss: 0.00084337
Iteration 469/1000 | Loss: 0.00087500
Iteration 470/1000 | Loss: 0.00099990
Iteration 471/1000 | Loss: 0.00041891
Iteration 472/1000 | Loss: 0.00054158
Iteration 473/1000 | Loss: 0.00040764
Iteration 474/1000 | Loss: 0.00043907
Iteration 475/1000 | Loss: 0.00034345
Iteration 476/1000 | Loss: 0.00045401
Iteration 477/1000 | Loss: 0.00034509
Iteration 478/1000 | Loss: 0.00055627
Iteration 479/1000 | Loss: 0.00033656
Iteration 480/1000 | Loss: 0.00049290
Iteration 481/1000 | Loss: 0.00032583
Iteration 482/1000 | Loss: 0.00031553
Iteration 483/1000 | Loss: 0.00061664
Iteration 484/1000 | Loss: 0.00097503
Iteration 485/1000 | Loss: 0.00031575
Iteration 486/1000 | Loss: 0.00056024
Iteration 487/1000 | Loss: 0.00034532
Iteration 488/1000 | Loss: 0.00030178
Iteration 489/1000 | Loss: 0.00045578
Iteration 490/1000 | Loss: 0.00037071
Iteration 491/1000 | Loss: 0.00032915
Iteration 492/1000 | Loss: 0.00040928
Iteration 493/1000 | Loss: 0.00041468
Iteration 494/1000 | Loss: 0.00037690
Iteration 495/1000 | Loss: 0.00026044
Iteration 496/1000 | Loss: 0.00049306
Iteration 497/1000 | Loss: 0.00110284
Iteration 498/1000 | Loss: 0.00056005
Iteration 499/1000 | Loss: 0.00052302
Iteration 500/1000 | Loss: 0.00031981
Iteration 501/1000 | Loss: 0.00026848
Iteration 502/1000 | Loss: 0.00020185
Iteration 503/1000 | Loss: 0.00036595
Iteration 504/1000 | Loss: 0.00031948
Iteration 505/1000 | Loss: 0.00065905
Iteration 506/1000 | Loss: 0.00089954
Iteration 507/1000 | Loss: 0.00046810
Iteration 508/1000 | Loss: 0.00036627
Iteration 509/1000 | Loss: 0.00044267
Iteration 510/1000 | Loss: 0.00035694
Iteration 511/1000 | Loss: 0.00034485
Iteration 512/1000 | Loss: 0.00073345
Iteration 513/1000 | Loss: 0.00030619
Iteration 514/1000 | Loss: 0.00040622
Iteration 515/1000 | Loss: 0.00046567
Iteration 516/1000 | Loss: 0.00077607
Iteration 517/1000 | Loss: 0.00045148
Iteration 518/1000 | Loss: 0.00048330
Iteration 519/1000 | Loss: 0.00065939
Iteration 520/1000 | Loss: 0.00039285
Iteration 521/1000 | Loss: 0.00058267
Iteration 522/1000 | Loss: 0.00037851
Iteration 523/1000 | Loss: 0.00030885
Iteration 524/1000 | Loss: 0.00035607
Iteration 525/1000 | Loss: 0.00055275
Iteration 526/1000 | Loss: 0.00046222
Iteration 527/1000 | Loss: 0.00045159
Iteration 528/1000 | Loss: 0.00044003
Iteration 529/1000 | Loss: 0.00051830
Iteration 530/1000 | Loss: 0.00045255
Iteration 531/1000 | Loss: 0.00062327
Iteration 532/1000 | Loss: 0.00045929
Iteration 533/1000 | Loss: 0.00044772
Iteration 534/1000 | Loss: 0.00042580
Iteration 535/1000 | Loss: 0.00042371
Iteration 536/1000 | Loss: 0.00042793
Iteration 537/1000 | Loss: 0.00044255
Iteration 538/1000 | Loss: 0.00055915
Iteration 539/1000 | Loss: 0.00042874
Iteration 540/1000 | Loss: 0.00041281
Iteration 541/1000 | Loss: 0.00053286
Iteration 542/1000 | Loss: 0.00033987
Iteration 543/1000 | Loss: 0.00040193
Iteration 544/1000 | Loss: 0.00043497
Iteration 545/1000 | Loss: 0.00038682
Iteration 546/1000 | Loss: 0.00075363
Iteration 547/1000 | Loss: 0.00041375
Iteration 548/1000 | Loss: 0.00040950
Iteration 549/1000 | Loss: 0.00035878
Iteration 550/1000 | Loss: 0.00039728
Iteration 551/1000 | Loss: 0.00071632
Iteration 552/1000 | Loss: 0.00050396
Iteration 553/1000 | Loss: 0.00045384
Iteration 554/1000 | Loss: 0.00043945
Iteration 555/1000 | Loss: 0.00057072
Iteration 556/1000 | Loss: 0.00056090
Iteration 557/1000 | Loss: 0.00030628
Iteration 558/1000 | Loss: 0.00045234
Iteration 559/1000 | Loss: 0.00043264
Iteration 560/1000 | Loss: 0.00044800
Iteration 561/1000 | Loss: 0.00048041
Iteration 562/1000 | Loss: 0.00066179
Iteration 563/1000 | Loss: 0.00035824
Iteration 564/1000 | Loss: 0.00094724
Iteration 565/1000 | Loss: 0.00052009
Iteration 566/1000 | Loss: 0.00063682
Iteration 567/1000 | Loss: 0.00082349
Iteration 568/1000 | Loss: 0.00041855
Iteration 569/1000 | Loss: 0.00034282
Iteration 570/1000 | Loss: 0.00036498
Iteration 571/1000 | Loss: 0.00063747
Iteration 572/1000 | Loss: 0.00047296
Iteration 573/1000 | Loss: 0.00037418
Iteration 574/1000 | Loss: 0.00030974
Iteration 575/1000 | Loss: 0.00041410
Iteration 576/1000 | Loss: 0.00047253
Iteration 577/1000 | Loss: 0.00070038
Iteration 578/1000 | Loss: 0.00024540
Iteration 579/1000 | Loss: 0.00027388
Iteration 580/1000 | Loss: 0.00044491
Iteration 581/1000 | Loss: 0.00027402
Iteration 582/1000 | Loss: 0.00029340
Iteration 583/1000 | Loss: 0.00026058
Iteration 584/1000 | Loss: 0.00040586
Iteration 585/1000 | Loss: 0.00025994
Iteration 586/1000 | Loss: 0.00029974
Iteration 587/1000 | Loss: 0.00027340
Iteration 588/1000 | Loss: 0.00024977
Iteration 589/1000 | Loss: 0.00022010
Iteration 590/1000 | Loss: 0.00035882
Iteration 591/1000 | Loss: 0.00025426
Iteration 592/1000 | Loss: 0.00063001
Iteration 593/1000 | Loss: 0.00020243
Iteration 594/1000 | Loss: 0.00033427
Iteration 595/1000 | Loss: 0.00012569
Iteration 596/1000 | Loss: 0.00020137
Iteration 597/1000 | Loss: 0.00024361
Iteration 598/1000 | Loss: 0.00024284
Iteration 599/1000 | Loss: 0.00040784
Iteration 600/1000 | Loss: 0.00012093
Iteration 601/1000 | Loss: 0.00013345
Iteration 602/1000 | Loss: 0.00013994
Iteration 603/1000 | Loss: 0.00013960
Iteration 604/1000 | Loss: 0.00012932
Iteration 605/1000 | Loss: 0.00012353
Iteration 606/1000 | Loss: 0.00012443
Iteration 607/1000 | Loss: 0.00018383
Iteration 608/1000 | Loss: 0.00013265
Iteration 609/1000 | Loss: 0.00013909
Iteration 610/1000 | Loss: 0.00013129
Iteration 611/1000 | Loss: 0.00012870
Iteration 612/1000 | Loss: 0.00012577
Iteration 613/1000 | Loss: 0.00018993
Iteration 614/1000 | Loss: 0.00012209
Iteration 615/1000 | Loss: 0.00013257
Iteration 616/1000 | Loss: 0.00029660
Iteration 617/1000 | Loss: 0.00013618
Iteration 618/1000 | Loss: 0.00016788
Iteration 619/1000 | Loss: 0.00044314
Iteration 620/1000 | Loss: 0.00014822
Iteration 621/1000 | Loss: 0.00021572
Iteration 622/1000 | Loss: 0.00013217
Iteration 623/1000 | Loss: 0.00023536
Iteration 624/1000 | Loss: 0.00014724
Iteration 625/1000 | Loss: 0.00003076
Iteration 626/1000 | Loss: 0.00002901
Iteration 627/1000 | Loss: 0.00002797
Iteration 628/1000 | Loss: 0.00002751
Iteration 629/1000 | Loss: 0.00002750
Iteration 630/1000 | Loss: 0.00002727
Iteration 631/1000 | Loss: 0.00002725
Iteration 632/1000 | Loss: 0.00002701
Iteration 633/1000 | Loss: 0.00026496
Iteration 634/1000 | Loss: 0.00002705
Iteration 635/1000 | Loss: 0.00012274
Iteration 636/1000 | Loss: 0.00002665
Iteration 637/1000 | Loss: 0.00002638
Iteration 638/1000 | Loss: 0.00002636
Iteration 639/1000 | Loss: 0.00002622
Iteration 640/1000 | Loss: 0.00002618
Iteration 641/1000 | Loss: 0.00002612
Iteration 642/1000 | Loss: 0.00002606
Iteration 643/1000 | Loss: 0.00002598
Iteration 644/1000 | Loss: 0.00002596
Iteration 645/1000 | Loss: 0.00002596
Iteration 646/1000 | Loss: 0.00002595
Iteration 647/1000 | Loss: 0.00002594
Iteration 648/1000 | Loss: 0.00002594
Iteration 649/1000 | Loss: 0.00002593
Iteration 650/1000 | Loss: 0.00002593
Iteration 651/1000 | Loss: 0.00002593
Iteration 652/1000 | Loss: 0.00002593
Iteration 653/1000 | Loss: 0.00002593
Iteration 654/1000 | Loss: 0.00002593
Iteration 655/1000 | Loss: 0.00002593
Iteration 656/1000 | Loss: 0.00002593
Iteration 657/1000 | Loss: 0.00002593
Iteration 658/1000 | Loss: 0.00002593
Iteration 659/1000 | Loss: 0.00002593
Iteration 660/1000 | Loss: 0.00002592
Iteration 661/1000 | Loss: 0.00002592
Iteration 662/1000 | Loss: 0.00002592
Iteration 663/1000 | Loss: 0.00002592
Iteration 664/1000 | Loss: 0.00002592
Iteration 665/1000 | Loss: 0.00002592
Iteration 666/1000 | Loss: 0.00002592
Iteration 667/1000 | Loss: 0.00002591
Iteration 668/1000 | Loss: 0.00002591
Iteration 669/1000 | Loss: 0.00002590
Iteration 670/1000 | Loss: 0.00002588
Iteration 671/1000 | Loss: 0.00002588
Iteration 672/1000 | Loss: 0.00002588
Iteration 673/1000 | Loss: 0.00002588
Iteration 674/1000 | Loss: 0.00002587
Iteration 675/1000 | Loss: 0.00002587
Iteration 676/1000 | Loss: 0.00002587
Iteration 677/1000 | Loss: 0.00002587
Iteration 678/1000 | Loss: 0.00002586
Iteration 679/1000 | Loss: 0.00002586
Iteration 680/1000 | Loss: 0.00002586
Iteration 681/1000 | Loss: 0.00002586
Iteration 682/1000 | Loss: 0.00002585
Iteration 683/1000 | Loss: 0.00002584
Iteration 684/1000 | Loss: 0.00002584
Iteration 685/1000 | Loss: 0.00002583
Iteration 686/1000 | Loss: 0.00002583
Iteration 687/1000 | Loss: 0.00002583
Iteration 688/1000 | Loss: 0.00002582
Iteration 689/1000 | Loss: 0.00002582
Iteration 690/1000 | Loss: 0.00002581
Iteration 691/1000 | Loss: 0.00002581
Iteration 692/1000 | Loss: 0.00002581
Iteration 693/1000 | Loss: 0.00002581
Iteration 694/1000 | Loss: 0.00002580
Iteration 695/1000 | Loss: 0.00002580
Iteration 696/1000 | Loss: 0.00002580
Iteration 697/1000 | Loss: 0.00002579
Iteration 698/1000 | Loss: 0.00002579
Iteration 699/1000 | Loss: 0.00002578
Iteration 700/1000 | Loss: 0.00002578
Iteration 701/1000 | Loss: 0.00002578
Iteration 702/1000 | Loss: 0.00002577
Iteration 703/1000 | Loss: 0.00002577
Iteration 704/1000 | Loss: 0.00002577
Iteration 705/1000 | Loss: 0.00002577
Iteration 706/1000 | Loss: 0.00002577
Iteration 707/1000 | Loss: 0.00002576
Iteration 708/1000 | Loss: 0.00002576
Iteration 709/1000 | Loss: 0.00002576
Iteration 710/1000 | Loss: 0.00002576
Iteration 711/1000 | Loss: 0.00002576
Iteration 712/1000 | Loss: 0.00002576
Iteration 713/1000 | Loss: 0.00002576
Iteration 714/1000 | Loss: 0.00002575
Iteration 715/1000 | Loss: 0.00002575
Iteration 716/1000 | Loss: 0.00002575
Iteration 717/1000 | Loss: 0.00002575
Iteration 718/1000 | Loss: 0.00002575
Iteration 719/1000 | Loss: 0.00002575
Iteration 720/1000 | Loss: 0.00002575
Iteration 721/1000 | Loss: 0.00002575
Iteration 722/1000 | Loss: 0.00002575
Iteration 723/1000 | Loss: 0.00002575
Iteration 724/1000 | Loss: 0.00002575
Iteration 725/1000 | Loss: 0.00002575
Iteration 726/1000 | Loss: 0.00002575
Iteration 727/1000 | Loss: 0.00002575
Iteration 728/1000 | Loss: 0.00002575
Iteration 729/1000 | Loss: 0.00002574
Iteration 730/1000 | Loss: 0.00002574
Iteration 731/1000 | Loss: 0.00002574
Iteration 732/1000 | Loss: 0.00002574
Iteration 733/1000 | Loss: 0.00002574
Iteration 734/1000 | Loss: 0.00002574
Iteration 735/1000 | Loss: 0.00002574
Iteration 736/1000 | Loss: 0.00002574
Iteration 737/1000 | Loss: 0.00002574
Iteration 738/1000 | Loss: 0.00002574
Iteration 739/1000 | Loss: 0.00002574
Iteration 740/1000 | Loss: 0.00002574
Iteration 741/1000 | Loss: 0.00002574
Iteration 742/1000 | Loss: 0.00002574
Iteration 743/1000 | Loss: 0.00002574
Iteration 744/1000 | Loss: 0.00002574
Iteration 745/1000 | Loss: 0.00002574
Iteration 746/1000 | Loss: 0.00002574
Iteration 747/1000 | Loss: 0.00002574
Iteration 748/1000 | Loss: 0.00002574
Iteration 749/1000 | Loss: 0.00002574
Iteration 750/1000 | Loss: 0.00002574
Iteration 751/1000 | Loss: 0.00002574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 751. Stopping optimization.
Last 5 losses: [2.5740180717548355e-05, 2.5740180717548355e-05, 2.5740180717548355e-05, 2.5740180717548355e-05, 2.5740180717548355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5740180717548355e-05

Optimization complete. Final v2v error: 3.8391270637512207 mm

Highest mean error: 22.161813735961914 mm for frame 164

Lowest mean error: 3.002718687057495 mm for frame 2

Saving results

Total time: 1078.8402378559113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432964
Iteration 2/25 | Loss: 0.00111464
Iteration 3/25 | Loss: 0.00080260
Iteration 4/25 | Loss: 0.00074463
Iteration 5/25 | Loss: 0.00073541
Iteration 6/25 | Loss: 0.00073211
Iteration 7/25 | Loss: 0.00073152
Iteration 8/25 | Loss: 0.00073152
Iteration 9/25 | Loss: 0.00073152
Iteration 10/25 | Loss: 0.00073152
Iteration 11/25 | Loss: 0.00073152
Iteration 12/25 | Loss: 0.00073152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007315217517316341, 0.0007315217517316341, 0.0007315217517316341, 0.0007315217517316341, 0.0007315217517316341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007315217517316341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.85456753
Iteration 2/25 | Loss: 0.00038476
Iteration 3/25 | Loss: 0.00038476
Iteration 4/25 | Loss: 0.00038476
Iteration 5/25 | Loss: 0.00038476
Iteration 6/25 | Loss: 0.00038476
Iteration 7/25 | Loss: 0.00038476
Iteration 8/25 | Loss: 0.00038476
Iteration 9/25 | Loss: 0.00038476
Iteration 10/25 | Loss: 0.00038476
Iteration 11/25 | Loss: 0.00038476
Iteration 12/25 | Loss: 0.00038476
Iteration 13/25 | Loss: 0.00038476
Iteration 14/25 | Loss: 0.00038476
Iteration 15/25 | Loss: 0.00038476
Iteration 16/25 | Loss: 0.00038476
Iteration 17/25 | Loss: 0.00038476
Iteration 18/25 | Loss: 0.00038476
Iteration 19/25 | Loss: 0.00038476
Iteration 20/25 | Loss: 0.00038476
Iteration 21/25 | Loss: 0.00038476
Iteration 22/25 | Loss: 0.00038476
Iteration 23/25 | Loss: 0.00038476
Iteration 24/25 | Loss: 0.00038476
Iteration 25/25 | Loss: 0.00038476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038476
Iteration 2/1000 | Loss: 0.00004256
Iteration 3/1000 | Loss: 0.00003008
Iteration 4/1000 | Loss: 0.00002769
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00002490
Iteration 7/1000 | Loss: 0.00002419
Iteration 8/1000 | Loss: 0.00002359
Iteration 9/1000 | Loss: 0.00002329
Iteration 10/1000 | Loss: 0.00002315
Iteration 11/1000 | Loss: 0.00002310
Iteration 12/1000 | Loss: 0.00002306
Iteration 13/1000 | Loss: 0.00002301
Iteration 14/1000 | Loss: 0.00002300
Iteration 15/1000 | Loss: 0.00002291
Iteration 16/1000 | Loss: 0.00002286
Iteration 17/1000 | Loss: 0.00002286
Iteration 18/1000 | Loss: 0.00002285
Iteration 19/1000 | Loss: 0.00002285
Iteration 20/1000 | Loss: 0.00002284
Iteration 21/1000 | Loss: 0.00002283
Iteration 22/1000 | Loss: 0.00002283
Iteration 23/1000 | Loss: 0.00002282
Iteration 24/1000 | Loss: 0.00002281
Iteration 25/1000 | Loss: 0.00002281
Iteration 26/1000 | Loss: 0.00002280
Iteration 27/1000 | Loss: 0.00002280
Iteration 28/1000 | Loss: 0.00002280
Iteration 29/1000 | Loss: 0.00002278
Iteration 30/1000 | Loss: 0.00002278
Iteration 31/1000 | Loss: 0.00002275
Iteration 32/1000 | Loss: 0.00002275
Iteration 33/1000 | Loss: 0.00002275
Iteration 34/1000 | Loss: 0.00002275
Iteration 35/1000 | Loss: 0.00002275
Iteration 36/1000 | Loss: 0.00002275
Iteration 37/1000 | Loss: 0.00002274
Iteration 38/1000 | Loss: 0.00002274
Iteration 39/1000 | Loss: 0.00002273
Iteration 40/1000 | Loss: 0.00002273
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002272
Iteration 43/1000 | Loss: 0.00002272
Iteration 44/1000 | Loss: 0.00002272
Iteration 45/1000 | Loss: 0.00002272
Iteration 46/1000 | Loss: 0.00002271
Iteration 47/1000 | Loss: 0.00002270
Iteration 48/1000 | Loss: 0.00002270
Iteration 49/1000 | Loss: 0.00002269
Iteration 50/1000 | Loss: 0.00002269
Iteration 51/1000 | Loss: 0.00002269
Iteration 52/1000 | Loss: 0.00002269
Iteration 53/1000 | Loss: 0.00002269
Iteration 54/1000 | Loss: 0.00002269
Iteration 55/1000 | Loss: 0.00002269
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002268
Iteration 58/1000 | Loss: 0.00002265
Iteration 59/1000 | Loss: 0.00002265
Iteration 60/1000 | Loss: 0.00002265
Iteration 61/1000 | Loss: 0.00002265
Iteration 62/1000 | Loss: 0.00002265
Iteration 63/1000 | Loss: 0.00002265
Iteration 64/1000 | Loss: 0.00002265
Iteration 65/1000 | Loss: 0.00002264
Iteration 66/1000 | Loss: 0.00002264
Iteration 67/1000 | Loss: 0.00002264
Iteration 68/1000 | Loss: 0.00002264
Iteration 69/1000 | Loss: 0.00002262
Iteration 70/1000 | Loss: 0.00002262
Iteration 71/1000 | Loss: 0.00002262
Iteration 72/1000 | Loss: 0.00002261
Iteration 73/1000 | Loss: 0.00002261
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002260
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002259
Iteration 78/1000 | Loss: 0.00002259
Iteration 79/1000 | Loss: 0.00002259
Iteration 80/1000 | Loss: 0.00002258
Iteration 81/1000 | Loss: 0.00002258
Iteration 82/1000 | Loss: 0.00002258
Iteration 83/1000 | Loss: 0.00002258
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002256
Iteration 91/1000 | Loss: 0.00002256
Iteration 92/1000 | Loss: 0.00002256
Iteration 93/1000 | Loss: 0.00002256
Iteration 94/1000 | Loss: 0.00002256
Iteration 95/1000 | Loss: 0.00002256
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002255
Iteration 101/1000 | Loss: 0.00002255
Iteration 102/1000 | Loss: 0.00002255
Iteration 103/1000 | Loss: 0.00002254
Iteration 104/1000 | Loss: 0.00002254
Iteration 105/1000 | Loss: 0.00002254
Iteration 106/1000 | Loss: 0.00002254
Iteration 107/1000 | Loss: 0.00002254
Iteration 108/1000 | Loss: 0.00002254
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002253
Iteration 112/1000 | Loss: 0.00002253
Iteration 113/1000 | Loss: 0.00002253
Iteration 114/1000 | Loss: 0.00002253
Iteration 115/1000 | Loss: 0.00002253
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Iteration 118/1000 | Loss: 0.00002253
Iteration 119/1000 | Loss: 0.00002253
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002252
Iteration 122/1000 | Loss: 0.00002252
Iteration 123/1000 | Loss: 0.00002252
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002251
Iteration 128/1000 | Loss: 0.00002251
Iteration 129/1000 | Loss: 0.00002251
Iteration 130/1000 | Loss: 0.00002251
Iteration 131/1000 | Loss: 0.00002251
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002250
Iteration 140/1000 | Loss: 0.00002250
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002250
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002250
Iteration 146/1000 | Loss: 0.00002250
Iteration 147/1000 | Loss: 0.00002250
Iteration 148/1000 | Loss: 0.00002250
Iteration 149/1000 | Loss: 0.00002250
Iteration 150/1000 | Loss: 0.00002250
Iteration 151/1000 | Loss: 0.00002250
Iteration 152/1000 | Loss: 0.00002250
Iteration 153/1000 | Loss: 0.00002250
Iteration 154/1000 | Loss: 0.00002250
Iteration 155/1000 | Loss: 0.00002250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.2502967112814076e-05, 2.2502967112814076e-05, 2.2502967112814076e-05, 2.2502967112814076e-05, 2.2502967112814076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2502967112814076e-05

Optimization complete. Final v2v error: 4.102451324462891 mm

Highest mean error: 4.4960808753967285 mm for frame 90

Lowest mean error: 3.707683563232422 mm for frame 0

Saving results

Total time: 37.422544956207275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113693
Iteration 2/25 | Loss: 0.00232619
Iteration 3/25 | Loss: 0.00243559
Iteration 4/25 | Loss: 0.00194646
Iteration 5/25 | Loss: 0.00128848
Iteration 6/25 | Loss: 0.00121305
Iteration 7/25 | Loss: 0.00111559
Iteration 8/25 | Loss: 0.00112211
Iteration 9/25 | Loss: 0.00098996
Iteration 10/25 | Loss: 0.00096245
Iteration 11/25 | Loss: 0.00111799
Iteration 12/25 | Loss: 0.00128665
Iteration 13/25 | Loss: 0.00108019
Iteration 14/25 | Loss: 0.00104725
Iteration 15/25 | Loss: 0.00099349
Iteration 16/25 | Loss: 0.00092652
Iteration 17/25 | Loss: 0.00092343
Iteration 18/25 | Loss: 0.00100436
Iteration 19/25 | Loss: 0.00095387
Iteration 20/25 | Loss: 0.00094744
Iteration 21/25 | Loss: 0.00094641
Iteration 22/25 | Loss: 0.00098499
Iteration 23/25 | Loss: 0.00091071
Iteration 24/25 | Loss: 0.00089260
Iteration 25/25 | Loss: 0.00088710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43974698
Iteration 2/25 | Loss: 0.00550661
Iteration 3/25 | Loss: 0.00343880
Iteration 4/25 | Loss: 0.00282821
Iteration 5/25 | Loss: 0.00279390
Iteration 6/25 | Loss: 0.00279390
Iteration 7/25 | Loss: 0.00279390
Iteration 8/25 | Loss: 0.00279390
Iteration 9/25 | Loss: 0.00279390
Iteration 10/25 | Loss: 0.00279390
Iteration 11/25 | Loss: 0.00279390
Iteration 12/25 | Loss: 0.00279390
Iteration 13/25 | Loss: 0.00279390
Iteration 14/25 | Loss: 0.00279390
Iteration 15/25 | Loss: 0.00279390
Iteration 16/25 | Loss: 0.00279390
Iteration 17/25 | Loss: 0.00279390
Iteration 18/25 | Loss: 0.00279390
Iteration 19/25 | Loss: 0.00279390
Iteration 20/25 | Loss: 0.00279390
Iteration 21/25 | Loss: 0.00279390
Iteration 22/25 | Loss: 0.00279390
Iteration 23/25 | Loss: 0.00279390
Iteration 24/25 | Loss: 0.00279390
Iteration 25/25 | Loss: 0.00279390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279390
Iteration 2/1000 | Loss: 0.00689395
Iteration 3/1000 | Loss: 0.00294689
Iteration 4/1000 | Loss: 0.00265451
Iteration 5/1000 | Loss: 0.00134119
Iteration 6/1000 | Loss: 0.00450811
Iteration 7/1000 | Loss: 0.00570294
Iteration 8/1000 | Loss: 0.00690479
Iteration 9/1000 | Loss: 0.00966091
Iteration 10/1000 | Loss: 0.00772337
Iteration 11/1000 | Loss: 0.00730064
Iteration 12/1000 | Loss: 0.00491092
Iteration 13/1000 | Loss: 0.00195750
Iteration 14/1000 | Loss: 0.00458674
Iteration 15/1000 | Loss: 0.00332413
Iteration 16/1000 | Loss: 0.00380869
Iteration 17/1000 | Loss: 0.00342199
Iteration 18/1000 | Loss: 0.00597318
Iteration 19/1000 | Loss: 0.00398709
Iteration 20/1000 | Loss: 0.00213223
Iteration 21/1000 | Loss: 0.00431887
Iteration 22/1000 | Loss: 0.00163755
Iteration 23/1000 | Loss: 0.00297620
Iteration 24/1000 | Loss: 0.00413622
Iteration 25/1000 | Loss: 0.00276671
Iteration 26/1000 | Loss: 0.00237265
Iteration 27/1000 | Loss: 0.00054256
Iteration 28/1000 | Loss: 0.00177990
Iteration 29/1000 | Loss: 0.00145587
Iteration 30/1000 | Loss: 0.00177582
Iteration 31/1000 | Loss: 0.00370293
Iteration 32/1000 | Loss: 0.00232041
Iteration 33/1000 | Loss: 0.00146500
Iteration 34/1000 | Loss: 0.00266930
Iteration 35/1000 | Loss: 0.00344001
Iteration 36/1000 | Loss: 0.00295085
Iteration 37/1000 | Loss: 0.00479081
Iteration 38/1000 | Loss: 0.00073920
Iteration 39/1000 | Loss: 0.00068403
Iteration 40/1000 | Loss: 0.00034176
Iteration 41/1000 | Loss: 0.00057167
Iteration 42/1000 | Loss: 0.00084198
Iteration 43/1000 | Loss: 0.00104081
Iteration 44/1000 | Loss: 0.00054643
Iteration 45/1000 | Loss: 0.00056732
Iteration 46/1000 | Loss: 0.00034067
Iteration 47/1000 | Loss: 0.00021924
Iteration 48/1000 | Loss: 0.00010146
Iteration 49/1000 | Loss: 0.00033874
Iteration 50/1000 | Loss: 0.00023277
Iteration 51/1000 | Loss: 0.00058544
Iteration 52/1000 | Loss: 0.00030298
Iteration 53/1000 | Loss: 0.00042566
Iteration 54/1000 | Loss: 0.00028150
Iteration 55/1000 | Loss: 0.00103666
Iteration 56/1000 | Loss: 0.00028836
Iteration 57/1000 | Loss: 0.00082235
Iteration 58/1000 | Loss: 0.00022121
Iteration 59/1000 | Loss: 0.00024345
Iteration 60/1000 | Loss: 0.00008150
Iteration 61/1000 | Loss: 0.00022399
Iteration 62/1000 | Loss: 0.00052874
Iteration 63/1000 | Loss: 0.00022302
Iteration 64/1000 | Loss: 0.00055714
Iteration 65/1000 | Loss: 0.00040632
Iteration 66/1000 | Loss: 0.00027124
Iteration 67/1000 | Loss: 0.00020100
Iteration 68/1000 | Loss: 0.00005799
Iteration 69/1000 | Loss: 0.00022373
Iteration 70/1000 | Loss: 0.00024937
Iteration 71/1000 | Loss: 0.00027085
Iteration 72/1000 | Loss: 0.00009690
Iteration 73/1000 | Loss: 0.00051986
Iteration 74/1000 | Loss: 0.00024107
Iteration 75/1000 | Loss: 0.00025131
Iteration 76/1000 | Loss: 0.00027944
Iteration 77/1000 | Loss: 0.00022501
Iteration 78/1000 | Loss: 0.00038137
Iteration 79/1000 | Loss: 0.00036118
Iteration 80/1000 | Loss: 0.00025428
Iteration 81/1000 | Loss: 0.00023289
Iteration 82/1000 | Loss: 0.00020963
Iteration 83/1000 | Loss: 0.00014384
Iteration 84/1000 | Loss: 0.00016507
Iteration 85/1000 | Loss: 0.00012679
Iteration 86/1000 | Loss: 0.00011384
Iteration 87/1000 | Loss: 0.00006571
Iteration 88/1000 | Loss: 0.00038385
Iteration 89/1000 | Loss: 0.00026787
Iteration 90/1000 | Loss: 0.00009740
Iteration 91/1000 | Loss: 0.00036340
Iteration 92/1000 | Loss: 0.00032536
Iteration 93/1000 | Loss: 0.00007473
Iteration 94/1000 | Loss: 0.00005401
Iteration 95/1000 | Loss: 0.00004243
Iteration 96/1000 | Loss: 0.00003625
Iteration 97/1000 | Loss: 0.00035361
Iteration 98/1000 | Loss: 0.00016982
Iteration 99/1000 | Loss: 0.00004138
Iteration 100/1000 | Loss: 0.00031279
Iteration 101/1000 | Loss: 0.00018608
Iteration 102/1000 | Loss: 0.00019558
Iteration 103/1000 | Loss: 0.00006526
Iteration 104/1000 | Loss: 0.00023723
Iteration 105/1000 | Loss: 0.00020960
Iteration 106/1000 | Loss: 0.00032124
Iteration 107/1000 | Loss: 0.00045297
Iteration 108/1000 | Loss: 0.00015892
Iteration 109/1000 | Loss: 0.00016241
Iteration 110/1000 | Loss: 0.00017246
Iteration 111/1000 | Loss: 0.00004338
Iteration 112/1000 | Loss: 0.00036369
Iteration 113/1000 | Loss: 0.00005218
Iteration 114/1000 | Loss: 0.00003982
Iteration 115/1000 | Loss: 0.00003710
Iteration 116/1000 | Loss: 0.00003496
Iteration 117/1000 | Loss: 0.00003082
Iteration 118/1000 | Loss: 0.00002964
Iteration 119/1000 | Loss: 0.00002929
Iteration 120/1000 | Loss: 0.00002900
Iteration 121/1000 | Loss: 0.00126159
Iteration 122/1000 | Loss: 0.00062092
Iteration 123/1000 | Loss: 0.00005559
Iteration 124/1000 | Loss: 0.00004067
Iteration 125/1000 | Loss: 0.00041798
Iteration 126/1000 | Loss: 0.00003375
Iteration 127/1000 | Loss: 0.00002660
Iteration 128/1000 | Loss: 0.00002463
Iteration 129/1000 | Loss: 0.00002390
Iteration 130/1000 | Loss: 0.00002356
Iteration 131/1000 | Loss: 0.00002328
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002307
Iteration 134/1000 | Loss: 0.00002306
Iteration 135/1000 | Loss: 0.00002304
Iteration 136/1000 | Loss: 0.00002304
Iteration 137/1000 | Loss: 0.00002303
Iteration 138/1000 | Loss: 0.00002303
Iteration 139/1000 | Loss: 0.00002303
Iteration 140/1000 | Loss: 0.00002303
Iteration 141/1000 | Loss: 0.00002303
Iteration 142/1000 | Loss: 0.00002303
Iteration 143/1000 | Loss: 0.00002303
Iteration 144/1000 | Loss: 0.00002302
Iteration 145/1000 | Loss: 0.00002302
Iteration 146/1000 | Loss: 0.00002302
Iteration 147/1000 | Loss: 0.00002302
Iteration 148/1000 | Loss: 0.00002302
Iteration 149/1000 | Loss: 0.00002302
Iteration 150/1000 | Loss: 0.00002301
Iteration 151/1000 | Loss: 0.00002301
Iteration 152/1000 | Loss: 0.00002301
Iteration 153/1000 | Loss: 0.00002301
Iteration 154/1000 | Loss: 0.00002300
Iteration 155/1000 | Loss: 0.00002300
Iteration 156/1000 | Loss: 0.00002300
Iteration 157/1000 | Loss: 0.00002300
Iteration 158/1000 | Loss: 0.00002300
Iteration 159/1000 | Loss: 0.00002300
Iteration 160/1000 | Loss: 0.00002300
Iteration 161/1000 | Loss: 0.00002300
Iteration 162/1000 | Loss: 0.00002300
Iteration 163/1000 | Loss: 0.00002300
Iteration 164/1000 | Loss: 0.00002299
Iteration 165/1000 | Loss: 0.00002299
Iteration 166/1000 | Loss: 0.00002299
Iteration 167/1000 | Loss: 0.00002298
Iteration 168/1000 | Loss: 0.00002298
Iteration 169/1000 | Loss: 0.00002298
Iteration 170/1000 | Loss: 0.00002298
Iteration 171/1000 | Loss: 0.00002298
Iteration 172/1000 | Loss: 0.00002297
Iteration 173/1000 | Loss: 0.00002297
Iteration 174/1000 | Loss: 0.00002296
Iteration 175/1000 | Loss: 0.00002296
Iteration 176/1000 | Loss: 0.00002295
Iteration 177/1000 | Loss: 0.00002295
Iteration 178/1000 | Loss: 0.00002293
Iteration 179/1000 | Loss: 0.00002293
Iteration 180/1000 | Loss: 0.00002293
Iteration 181/1000 | Loss: 0.00002293
Iteration 182/1000 | Loss: 0.00002293
Iteration 183/1000 | Loss: 0.00002293
Iteration 184/1000 | Loss: 0.00002293
Iteration 185/1000 | Loss: 0.00002293
Iteration 186/1000 | Loss: 0.00002293
Iteration 187/1000 | Loss: 0.00002293
Iteration 188/1000 | Loss: 0.00002292
Iteration 189/1000 | Loss: 0.00002292
Iteration 190/1000 | Loss: 0.00002292
Iteration 191/1000 | Loss: 0.00002291
Iteration 192/1000 | Loss: 0.00002291
Iteration 193/1000 | Loss: 0.00002291
Iteration 194/1000 | Loss: 0.00002291
Iteration 195/1000 | Loss: 0.00002291
Iteration 196/1000 | Loss: 0.00002290
Iteration 197/1000 | Loss: 0.00002290
Iteration 198/1000 | Loss: 0.00002290
Iteration 199/1000 | Loss: 0.00002290
Iteration 200/1000 | Loss: 0.00002290
Iteration 201/1000 | Loss: 0.00002290
Iteration 202/1000 | Loss: 0.00002289
Iteration 203/1000 | Loss: 0.00002289
Iteration 204/1000 | Loss: 0.00002289
Iteration 205/1000 | Loss: 0.00002289
Iteration 206/1000 | Loss: 0.00002289
Iteration 207/1000 | Loss: 0.00002289
Iteration 208/1000 | Loss: 0.00002289
Iteration 209/1000 | Loss: 0.00002289
Iteration 210/1000 | Loss: 0.00002288
Iteration 211/1000 | Loss: 0.00002288
Iteration 212/1000 | Loss: 0.00002288
Iteration 213/1000 | Loss: 0.00002288
Iteration 214/1000 | Loss: 0.00002287
Iteration 215/1000 | Loss: 0.00002287
Iteration 216/1000 | Loss: 0.00002287
Iteration 217/1000 | Loss: 0.00002286
Iteration 218/1000 | Loss: 0.00002286
Iteration 219/1000 | Loss: 0.00002286
Iteration 220/1000 | Loss: 0.00002286
Iteration 221/1000 | Loss: 0.00002285
Iteration 222/1000 | Loss: 0.00002285
Iteration 223/1000 | Loss: 0.00002285
Iteration 224/1000 | Loss: 0.00002285
Iteration 225/1000 | Loss: 0.00002285
Iteration 226/1000 | Loss: 0.00002284
Iteration 227/1000 | Loss: 0.00002284
Iteration 228/1000 | Loss: 0.00002284
Iteration 229/1000 | Loss: 0.00002284
Iteration 230/1000 | Loss: 0.00002284
Iteration 231/1000 | Loss: 0.00002284
Iteration 232/1000 | Loss: 0.00002284
Iteration 233/1000 | Loss: 0.00002284
Iteration 234/1000 | Loss: 0.00002284
Iteration 235/1000 | Loss: 0.00002284
Iteration 236/1000 | Loss: 0.00002284
Iteration 237/1000 | Loss: 0.00002283
Iteration 238/1000 | Loss: 0.00002283
Iteration 239/1000 | Loss: 0.00002283
Iteration 240/1000 | Loss: 0.00002283
Iteration 241/1000 | Loss: 0.00002283
Iteration 242/1000 | Loss: 0.00002283
Iteration 243/1000 | Loss: 0.00002283
Iteration 244/1000 | Loss: 0.00002283
Iteration 245/1000 | Loss: 0.00002283
Iteration 246/1000 | Loss: 0.00002283
Iteration 247/1000 | Loss: 0.00002282
Iteration 248/1000 | Loss: 0.00002282
Iteration 249/1000 | Loss: 0.00002282
Iteration 250/1000 | Loss: 0.00002281
Iteration 251/1000 | Loss: 0.00002281
Iteration 252/1000 | Loss: 0.00002280
Iteration 253/1000 | Loss: 0.00002280
Iteration 254/1000 | Loss: 0.00002279
Iteration 255/1000 | Loss: 0.00002279
Iteration 256/1000 | Loss: 0.00002279
Iteration 257/1000 | Loss: 0.00002279
Iteration 258/1000 | Loss: 0.00002279
Iteration 259/1000 | Loss: 0.00002278
Iteration 260/1000 | Loss: 0.00002278
Iteration 261/1000 | Loss: 0.00002278
Iteration 262/1000 | Loss: 0.00002278
Iteration 263/1000 | Loss: 0.00027170
Iteration 264/1000 | Loss: 0.00002934
Iteration 265/1000 | Loss: 0.00002558
Iteration 266/1000 | Loss: 0.00002396
Iteration 267/1000 | Loss: 0.00002313
Iteration 268/1000 | Loss: 0.00002275
Iteration 269/1000 | Loss: 0.00002253
Iteration 270/1000 | Loss: 0.00002245
Iteration 271/1000 | Loss: 0.00002231
Iteration 272/1000 | Loss: 0.00002230
Iteration 273/1000 | Loss: 0.00002229
Iteration 274/1000 | Loss: 0.00002228
Iteration 275/1000 | Loss: 0.00002227
Iteration 276/1000 | Loss: 0.00002227
Iteration 277/1000 | Loss: 0.00002226
Iteration 278/1000 | Loss: 0.00002226
Iteration 279/1000 | Loss: 0.00002225
Iteration 280/1000 | Loss: 0.00002225
Iteration 281/1000 | Loss: 0.00002224
Iteration 282/1000 | Loss: 0.00002224
Iteration 283/1000 | Loss: 0.00002224
Iteration 284/1000 | Loss: 0.00002223
Iteration 285/1000 | Loss: 0.00002221
Iteration 286/1000 | Loss: 0.00002221
Iteration 287/1000 | Loss: 0.00002221
Iteration 288/1000 | Loss: 0.00002220
Iteration 289/1000 | Loss: 0.00002220
Iteration 290/1000 | Loss: 0.00002220
Iteration 291/1000 | Loss: 0.00002220
Iteration 292/1000 | Loss: 0.00002220
Iteration 293/1000 | Loss: 0.00002220
Iteration 294/1000 | Loss: 0.00002219
Iteration 295/1000 | Loss: 0.00002219
Iteration 296/1000 | Loss: 0.00002219
Iteration 297/1000 | Loss: 0.00002219
Iteration 298/1000 | Loss: 0.00002219
Iteration 299/1000 | Loss: 0.00002219
Iteration 300/1000 | Loss: 0.00002219
Iteration 301/1000 | Loss: 0.00002219
Iteration 302/1000 | Loss: 0.00002219
Iteration 303/1000 | Loss: 0.00002219
Iteration 304/1000 | Loss: 0.00002219
Iteration 305/1000 | Loss: 0.00002219
Iteration 306/1000 | Loss: 0.00002219
Iteration 307/1000 | Loss: 0.00002219
Iteration 308/1000 | Loss: 0.00002219
Iteration 309/1000 | Loss: 0.00002218
Iteration 310/1000 | Loss: 0.00002218
Iteration 311/1000 | Loss: 0.00002218
Iteration 312/1000 | Loss: 0.00002218
Iteration 313/1000 | Loss: 0.00002218
Iteration 314/1000 | Loss: 0.00002218
Iteration 315/1000 | Loss: 0.00002218
Iteration 316/1000 | Loss: 0.00002218
Iteration 317/1000 | Loss: 0.00002218
Iteration 318/1000 | Loss: 0.00002218
Iteration 319/1000 | Loss: 0.00002217
Iteration 320/1000 | Loss: 0.00002217
Iteration 321/1000 | Loss: 0.00002217
Iteration 322/1000 | Loss: 0.00002217
Iteration 323/1000 | Loss: 0.00002216
Iteration 324/1000 | Loss: 0.00002216
Iteration 325/1000 | Loss: 0.00002216
Iteration 326/1000 | Loss: 0.00002216
Iteration 327/1000 | Loss: 0.00002216
Iteration 328/1000 | Loss: 0.00002216
Iteration 329/1000 | Loss: 0.00002216
Iteration 330/1000 | Loss: 0.00002216
Iteration 331/1000 | Loss: 0.00002216
Iteration 332/1000 | Loss: 0.00002216
Iteration 333/1000 | Loss: 0.00002216
Iteration 334/1000 | Loss: 0.00002216
Iteration 335/1000 | Loss: 0.00002216
Iteration 336/1000 | Loss: 0.00002215
Iteration 337/1000 | Loss: 0.00002215
Iteration 338/1000 | Loss: 0.00002215
Iteration 339/1000 | Loss: 0.00002215
Iteration 340/1000 | Loss: 0.00002215
Iteration 341/1000 | Loss: 0.00002215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [2.2154943508212455e-05, 2.2154943508212455e-05, 2.2154943508212455e-05, 2.2154943508212455e-05, 2.2154943508212455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2154943508212455e-05

Optimization complete. Final v2v error: 4.047664642333984 mm

Highest mean error: 4.992762088775635 mm for frame 100

Lowest mean error: 3.802372694015503 mm for frame 107

Saving results

Total time: 262.3792688846588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082871
Iteration 2/25 | Loss: 0.01082871
Iteration 3/25 | Loss: 0.00211280
Iteration 4/25 | Loss: 0.00123764
Iteration 5/25 | Loss: 0.00102394
Iteration 6/25 | Loss: 0.00100781
Iteration 7/25 | Loss: 0.00093822
Iteration 8/25 | Loss: 0.00091302
Iteration 9/25 | Loss: 0.00089086
Iteration 10/25 | Loss: 0.00087019
Iteration 11/25 | Loss: 0.00084512
Iteration 12/25 | Loss: 0.00082549
Iteration 13/25 | Loss: 0.00081907
Iteration 14/25 | Loss: 0.00080463
Iteration 15/25 | Loss: 0.00080433
Iteration 16/25 | Loss: 0.00079772
Iteration 17/25 | Loss: 0.00078733
Iteration 18/25 | Loss: 0.00078237
Iteration 19/25 | Loss: 0.00078052
Iteration 20/25 | Loss: 0.00077874
Iteration 21/25 | Loss: 0.00078410
Iteration 22/25 | Loss: 0.00078009
Iteration 23/25 | Loss: 0.00077612
Iteration 24/25 | Loss: 0.00077476
Iteration 25/25 | Loss: 0.00077172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26869977
Iteration 2/25 | Loss: 0.00152381
Iteration 3/25 | Loss: 0.00139001
Iteration 4/25 | Loss: 0.00139001
Iteration 5/25 | Loss: 0.00139001
Iteration 6/25 | Loss: 0.00139001
Iteration 7/25 | Loss: 0.00139001
Iteration 8/25 | Loss: 0.00139001
Iteration 9/25 | Loss: 0.00139001
Iteration 10/25 | Loss: 0.00139001
Iteration 11/25 | Loss: 0.00139001
Iteration 12/25 | Loss: 0.00139001
Iteration 13/25 | Loss: 0.00139001
Iteration 14/25 | Loss: 0.00139001
Iteration 15/25 | Loss: 0.00139001
Iteration 16/25 | Loss: 0.00139001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013900063931941986, 0.0013900063931941986, 0.0013900063931941986, 0.0013900063931941986, 0.0013900063931941986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013900063931941986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139001
Iteration 2/1000 | Loss: 0.00098774
Iteration 3/1000 | Loss: 0.00090126
Iteration 4/1000 | Loss: 0.00023001
Iteration 5/1000 | Loss: 0.00052520
Iteration 6/1000 | Loss: 0.00038383
Iteration 7/1000 | Loss: 0.00026266
Iteration 8/1000 | Loss: 0.00135148
Iteration 9/1000 | Loss: 0.00010447
Iteration 10/1000 | Loss: 0.00166250
Iteration 11/1000 | Loss: 0.00077448
Iteration 12/1000 | Loss: 0.00173401
Iteration 13/1000 | Loss: 0.00078107
Iteration 14/1000 | Loss: 0.00019564
Iteration 15/1000 | Loss: 0.00153222
Iteration 16/1000 | Loss: 0.00100597
Iteration 17/1000 | Loss: 0.00088342
Iteration 18/1000 | Loss: 0.00039037
Iteration 19/1000 | Loss: 0.00174740
Iteration 20/1000 | Loss: 0.00075590
Iteration 21/1000 | Loss: 0.00044010
Iteration 22/1000 | Loss: 0.00264766
Iteration 23/1000 | Loss: 0.00041562
Iteration 24/1000 | Loss: 0.00045847
Iteration 25/1000 | Loss: 0.00036864
Iteration 26/1000 | Loss: 0.00047855
Iteration 27/1000 | Loss: 0.00011535
Iteration 28/1000 | Loss: 0.00069869
Iteration 29/1000 | Loss: 0.00079735
Iteration 30/1000 | Loss: 0.00111550
Iteration 31/1000 | Loss: 0.00064229
Iteration 32/1000 | Loss: 0.00104839
Iteration 33/1000 | Loss: 0.00076222
Iteration 34/1000 | Loss: 0.00056052
Iteration 35/1000 | Loss: 0.00037562
Iteration 36/1000 | Loss: 0.00111395
Iteration 37/1000 | Loss: 0.00115157
Iteration 38/1000 | Loss: 0.00115232
Iteration 39/1000 | Loss: 0.00069354
Iteration 40/1000 | Loss: 0.00024680
Iteration 41/1000 | Loss: 0.00020465
Iteration 42/1000 | Loss: 0.00019519
Iteration 43/1000 | Loss: 0.00023316
Iteration 44/1000 | Loss: 0.00046914
Iteration 45/1000 | Loss: 0.00102019
Iteration 46/1000 | Loss: 0.00041043
Iteration 47/1000 | Loss: 0.00048944
Iteration 48/1000 | Loss: 0.00015596
Iteration 49/1000 | Loss: 0.00071610
Iteration 50/1000 | Loss: 0.00010706
Iteration 51/1000 | Loss: 0.00040006
Iteration 52/1000 | Loss: 0.00030902
Iteration 53/1000 | Loss: 0.00114893
Iteration 54/1000 | Loss: 0.00012817
Iteration 55/1000 | Loss: 0.00041461
Iteration 56/1000 | Loss: 0.00003642
Iteration 57/1000 | Loss: 0.00008436
Iteration 58/1000 | Loss: 0.00060414
Iteration 59/1000 | Loss: 0.00013173
Iteration 60/1000 | Loss: 0.00004861
Iteration 61/1000 | Loss: 0.00012418
Iteration 62/1000 | Loss: 0.00048641
Iteration 63/1000 | Loss: 0.00064457
Iteration 64/1000 | Loss: 0.00051801
Iteration 65/1000 | Loss: 0.00021962
Iteration 66/1000 | Loss: 0.00032657
Iteration 67/1000 | Loss: 0.00005066
Iteration 68/1000 | Loss: 0.00026858
Iteration 69/1000 | Loss: 0.00016244
Iteration 70/1000 | Loss: 0.00003299
Iteration 71/1000 | Loss: 0.00046002
Iteration 72/1000 | Loss: 0.00015787
Iteration 73/1000 | Loss: 0.00008598
Iteration 74/1000 | Loss: 0.00003202
Iteration 75/1000 | Loss: 0.00013461
Iteration 76/1000 | Loss: 0.00006127
Iteration 77/1000 | Loss: 0.00019989
Iteration 78/1000 | Loss: 0.00009689
Iteration 79/1000 | Loss: 0.00007744
Iteration 80/1000 | Loss: 0.00005547
Iteration 81/1000 | Loss: 0.00003994
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00020682
Iteration 84/1000 | Loss: 0.00014580
Iteration 85/1000 | Loss: 0.00005209
Iteration 86/1000 | Loss: 0.00002503
Iteration 87/1000 | Loss: 0.00003862
Iteration 88/1000 | Loss: 0.00025535
Iteration 89/1000 | Loss: 0.00004730
Iteration 90/1000 | Loss: 0.00032964
Iteration 91/1000 | Loss: 0.00035438
Iteration 92/1000 | Loss: 0.00020503
Iteration 93/1000 | Loss: 0.00003233
Iteration 94/1000 | Loss: 0.00016247
Iteration 95/1000 | Loss: 0.00025742
Iteration 96/1000 | Loss: 0.00035192
Iteration 97/1000 | Loss: 0.00015675
Iteration 98/1000 | Loss: 0.00046989
Iteration 99/1000 | Loss: 0.00125390
Iteration 100/1000 | Loss: 0.00010413
Iteration 101/1000 | Loss: 0.00006182
Iteration 102/1000 | Loss: 0.00005030
Iteration 103/1000 | Loss: 0.00004308
Iteration 104/1000 | Loss: 0.00005222
Iteration 105/1000 | Loss: 0.00002579
Iteration 106/1000 | Loss: 0.00007137
Iteration 107/1000 | Loss: 0.00018124
Iteration 108/1000 | Loss: 0.00046483
Iteration 109/1000 | Loss: 0.00044900
Iteration 110/1000 | Loss: 0.00027661
Iteration 111/1000 | Loss: 0.00028121
Iteration 112/1000 | Loss: 0.00002789
Iteration 113/1000 | Loss: 0.00004976
Iteration 114/1000 | Loss: 0.00015475
Iteration 115/1000 | Loss: 0.00009193
Iteration 116/1000 | Loss: 0.00005486
Iteration 117/1000 | Loss: 0.00008116
Iteration 118/1000 | Loss: 0.00004146
Iteration 119/1000 | Loss: 0.00005419
Iteration 120/1000 | Loss: 0.00018020
Iteration 121/1000 | Loss: 0.00004086
Iteration 122/1000 | Loss: 0.00031128
Iteration 123/1000 | Loss: 0.00028767
Iteration 124/1000 | Loss: 0.00020471
Iteration 125/1000 | Loss: 0.00026011
Iteration 126/1000 | Loss: 0.00017660
Iteration 127/1000 | Loss: 0.00016753
Iteration 128/1000 | Loss: 0.00023222
Iteration 129/1000 | Loss: 0.00027930
Iteration 130/1000 | Loss: 0.00030527
Iteration 131/1000 | Loss: 0.00006958
Iteration 132/1000 | Loss: 0.00002758
Iteration 133/1000 | Loss: 0.00003434
Iteration 134/1000 | Loss: 0.00027370
Iteration 135/1000 | Loss: 0.00025295
Iteration 136/1000 | Loss: 0.00018428
Iteration 137/1000 | Loss: 0.00005486
Iteration 138/1000 | Loss: 0.00002030
Iteration 139/1000 | Loss: 0.00001975
Iteration 140/1000 | Loss: 0.00003310
Iteration 141/1000 | Loss: 0.00021680
Iteration 142/1000 | Loss: 0.00005642
Iteration 143/1000 | Loss: 0.00032372
Iteration 144/1000 | Loss: 0.00002056
Iteration 145/1000 | Loss: 0.00006050
Iteration 146/1000 | Loss: 0.00002725
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001900
Iteration 150/1000 | Loss: 0.00001898
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001893
Iteration 153/1000 | Loss: 0.00001893
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001889
Iteration 156/1000 | Loss: 0.00001887
Iteration 157/1000 | Loss: 0.00001887
Iteration 158/1000 | Loss: 0.00003564
Iteration 159/1000 | Loss: 0.00001960
Iteration 160/1000 | Loss: 0.00001876
Iteration 161/1000 | Loss: 0.00002251
Iteration 162/1000 | Loss: 0.00009829
Iteration 163/1000 | Loss: 0.00002740
Iteration 164/1000 | Loss: 0.00002165
Iteration 165/1000 | Loss: 0.00001873
Iteration 166/1000 | Loss: 0.00002659
Iteration 167/1000 | Loss: 0.00001977
Iteration 168/1000 | Loss: 0.00001865
Iteration 169/1000 | Loss: 0.00001865
Iteration 170/1000 | Loss: 0.00001865
Iteration 171/1000 | Loss: 0.00001865
Iteration 172/1000 | Loss: 0.00001865
Iteration 173/1000 | Loss: 0.00001865
Iteration 174/1000 | Loss: 0.00001865
Iteration 175/1000 | Loss: 0.00001865
Iteration 176/1000 | Loss: 0.00001865
Iteration 177/1000 | Loss: 0.00001864
Iteration 178/1000 | Loss: 0.00001864
Iteration 179/1000 | Loss: 0.00001864
Iteration 180/1000 | Loss: 0.00001860
Iteration 181/1000 | Loss: 0.00001859
Iteration 182/1000 | Loss: 0.00001859
Iteration 183/1000 | Loss: 0.00001858
Iteration 184/1000 | Loss: 0.00001858
Iteration 185/1000 | Loss: 0.00001858
Iteration 186/1000 | Loss: 0.00001858
Iteration 187/1000 | Loss: 0.00001857
Iteration 188/1000 | Loss: 0.00001857
Iteration 189/1000 | Loss: 0.00001857
Iteration 190/1000 | Loss: 0.00001857
Iteration 191/1000 | Loss: 0.00001857
Iteration 192/1000 | Loss: 0.00001857
Iteration 193/1000 | Loss: 0.00001856
Iteration 194/1000 | Loss: 0.00001856
Iteration 195/1000 | Loss: 0.00001855
Iteration 196/1000 | Loss: 0.00001853
Iteration 197/1000 | Loss: 0.00001853
Iteration 198/1000 | Loss: 0.00001853
Iteration 199/1000 | Loss: 0.00001853
Iteration 200/1000 | Loss: 0.00001852
Iteration 201/1000 | Loss: 0.00001852
Iteration 202/1000 | Loss: 0.00001851
Iteration 203/1000 | Loss: 0.00001850
Iteration 204/1000 | Loss: 0.00001849
Iteration 205/1000 | Loss: 0.00001849
Iteration 206/1000 | Loss: 0.00007813
Iteration 207/1000 | Loss: 0.00002109
Iteration 208/1000 | Loss: 0.00002080
Iteration 209/1000 | Loss: 0.00006067
Iteration 210/1000 | Loss: 0.00001853
Iteration 211/1000 | Loss: 0.00002024
Iteration 212/1000 | Loss: 0.00001842
Iteration 213/1000 | Loss: 0.00001841
Iteration 214/1000 | Loss: 0.00001841
Iteration 215/1000 | Loss: 0.00001841
Iteration 216/1000 | Loss: 0.00001841
Iteration 217/1000 | Loss: 0.00001841
Iteration 218/1000 | Loss: 0.00001841
Iteration 219/1000 | Loss: 0.00001841
Iteration 220/1000 | Loss: 0.00001840
Iteration 221/1000 | Loss: 0.00001840
Iteration 222/1000 | Loss: 0.00001840
Iteration 223/1000 | Loss: 0.00001840
Iteration 224/1000 | Loss: 0.00001840
Iteration 225/1000 | Loss: 0.00001840
Iteration 226/1000 | Loss: 0.00001840
Iteration 227/1000 | Loss: 0.00001840
Iteration 228/1000 | Loss: 0.00001840
Iteration 229/1000 | Loss: 0.00001840
Iteration 230/1000 | Loss: 0.00001840
Iteration 231/1000 | Loss: 0.00001840
Iteration 232/1000 | Loss: 0.00001840
Iteration 233/1000 | Loss: 0.00001840
Iteration 234/1000 | Loss: 0.00001840
Iteration 235/1000 | Loss: 0.00001840
Iteration 236/1000 | Loss: 0.00001840
Iteration 237/1000 | Loss: 0.00001840
Iteration 238/1000 | Loss: 0.00001840
Iteration 239/1000 | Loss: 0.00001840
Iteration 240/1000 | Loss: 0.00001840
Iteration 241/1000 | Loss: 0.00001840
Iteration 242/1000 | Loss: 0.00001840
Iteration 243/1000 | Loss: 0.00001840
Iteration 244/1000 | Loss: 0.00001840
Iteration 245/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.8397669919067994e-05, 1.8397669919067994e-05, 1.8397669919067994e-05, 1.8397669919067994e-05, 1.8397669919067994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8397669919067994e-05

Optimization complete. Final v2v error: 3.3936338424682617 mm

Highest mean error: 14.299092292785645 mm for frame 6

Lowest mean error: 2.803631544113159 mm for frame 98

Saving results

Total time: 317.4665651321411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903507
Iteration 2/25 | Loss: 0.00128397
Iteration 3/25 | Loss: 0.00091547
Iteration 4/25 | Loss: 0.00085934
Iteration 5/25 | Loss: 0.00084576
Iteration 6/25 | Loss: 0.00084386
Iteration 7/25 | Loss: 0.00084361
Iteration 8/25 | Loss: 0.00084361
Iteration 9/25 | Loss: 0.00084361
Iteration 10/25 | Loss: 0.00084361
Iteration 11/25 | Loss: 0.00084361
Iteration 12/25 | Loss: 0.00084361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008436105563305318, 0.0008436105563305318, 0.0008436105563305318, 0.0008436105563305318, 0.0008436105563305318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008436105563305318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86513752
Iteration 2/25 | Loss: 0.00043811
Iteration 3/25 | Loss: 0.00043810
Iteration 4/25 | Loss: 0.00043810
Iteration 5/25 | Loss: 0.00043810
Iteration 6/25 | Loss: 0.00043810
Iteration 7/25 | Loss: 0.00043810
Iteration 8/25 | Loss: 0.00043810
Iteration 9/25 | Loss: 0.00043810
Iteration 10/25 | Loss: 0.00043810
Iteration 11/25 | Loss: 0.00043810
Iteration 12/25 | Loss: 0.00043809
Iteration 13/25 | Loss: 0.00043809
Iteration 14/25 | Loss: 0.00043809
Iteration 15/25 | Loss: 0.00043809
Iteration 16/25 | Loss: 0.00043809
Iteration 17/25 | Loss: 0.00043809
Iteration 18/25 | Loss: 0.00043809
Iteration 19/25 | Loss: 0.00043809
Iteration 20/25 | Loss: 0.00043809
Iteration 21/25 | Loss: 0.00043809
Iteration 22/25 | Loss: 0.00043809
Iteration 23/25 | Loss: 0.00043809
Iteration 24/25 | Loss: 0.00043809
Iteration 25/25 | Loss: 0.00043809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043809
Iteration 2/1000 | Loss: 0.00006341
Iteration 3/1000 | Loss: 0.00004421
Iteration 4/1000 | Loss: 0.00003885
Iteration 5/1000 | Loss: 0.00003544
Iteration 6/1000 | Loss: 0.00003387
Iteration 7/1000 | Loss: 0.00003256
Iteration 8/1000 | Loss: 0.00003184
Iteration 9/1000 | Loss: 0.00003124
Iteration 10/1000 | Loss: 0.00003084
Iteration 11/1000 | Loss: 0.00003059
Iteration 12/1000 | Loss: 0.00003049
Iteration 13/1000 | Loss: 0.00003046
Iteration 14/1000 | Loss: 0.00003046
Iteration 15/1000 | Loss: 0.00003043
Iteration 16/1000 | Loss: 0.00003039
Iteration 17/1000 | Loss: 0.00003038
Iteration 18/1000 | Loss: 0.00003029
Iteration 19/1000 | Loss: 0.00003026
Iteration 20/1000 | Loss: 0.00003026
Iteration 21/1000 | Loss: 0.00003026
Iteration 22/1000 | Loss: 0.00003026
Iteration 23/1000 | Loss: 0.00003025
Iteration 24/1000 | Loss: 0.00003024
Iteration 25/1000 | Loss: 0.00003024
Iteration 26/1000 | Loss: 0.00003024
Iteration 27/1000 | Loss: 0.00003024
Iteration 28/1000 | Loss: 0.00003024
Iteration 29/1000 | Loss: 0.00003023
Iteration 30/1000 | Loss: 0.00003023
Iteration 31/1000 | Loss: 0.00003023
Iteration 32/1000 | Loss: 0.00003023
Iteration 33/1000 | Loss: 0.00003023
Iteration 34/1000 | Loss: 0.00003023
Iteration 35/1000 | Loss: 0.00003023
Iteration 36/1000 | Loss: 0.00003023
Iteration 37/1000 | Loss: 0.00003023
Iteration 38/1000 | Loss: 0.00003023
Iteration 39/1000 | Loss: 0.00003023
Iteration 40/1000 | Loss: 0.00003023
Iteration 41/1000 | Loss: 0.00003023
Iteration 42/1000 | Loss: 0.00003023
Iteration 43/1000 | Loss: 0.00003023
Iteration 44/1000 | Loss: 0.00003023
Iteration 45/1000 | Loss: 0.00003022
Iteration 46/1000 | Loss: 0.00003022
Iteration 47/1000 | Loss: 0.00003022
Iteration 48/1000 | Loss: 0.00003022
Iteration 49/1000 | Loss: 0.00003022
Iteration 50/1000 | Loss: 0.00003022
Iteration 51/1000 | Loss: 0.00003022
Iteration 52/1000 | Loss: 0.00003022
Iteration 53/1000 | Loss: 0.00003022
Iteration 54/1000 | Loss: 0.00003022
Iteration 55/1000 | Loss: 0.00003022
Iteration 56/1000 | Loss: 0.00003022
Iteration 57/1000 | Loss: 0.00003022
Iteration 58/1000 | Loss: 0.00003022
Iteration 59/1000 | Loss: 0.00003022
Iteration 60/1000 | Loss: 0.00003022
Iteration 61/1000 | Loss: 0.00003022
Iteration 62/1000 | Loss: 0.00003022
Iteration 63/1000 | Loss: 0.00003022
Iteration 64/1000 | Loss: 0.00003022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [3.0222770874388516e-05, 3.0222770874388516e-05, 3.0222770874388516e-05, 3.0222770874388516e-05, 3.0222770874388516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0222770874388516e-05

Optimization complete. Final v2v error: 4.660930633544922 mm

Highest mean error: 5.327753067016602 mm for frame 0

Lowest mean error: 4.480856418609619 mm for frame 93

Saving results

Total time: 31.693438291549683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592684
Iteration 2/25 | Loss: 0.00086818
Iteration 3/25 | Loss: 0.00074645
Iteration 4/25 | Loss: 0.00072287
Iteration 5/25 | Loss: 0.00071560
Iteration 6/25 | Loss: 0.00071441
Iteration 7/25 | Loss: 0.00071426
Iteration 8/25 | Loss: 0.00071426
Iteration 9/25 | Loss: 0.00071426
Iteration 10/25 | Loss: 0.00071426
Iteration 11/25 | Loss: 0.00071426
Iteration 12/25 | Loss: 0.00071426
Iteration 13/25 | Loss: 0.00071426
Iteration 14/25 | Loss: 0.00071426
Iteration 15/25 | Loss: 0.00071426
Iteration 16/25 | Loss: 0.00071426
Iteration 17/25 | Loss: 0.00071426
Iteration 18/25 | Loss: 0.00071426
Iteration 19/25 | Loss: 0.00071426
Iteration 20/25 | Loss: 0.00071426
Iteration 21/25 | Loss: 0.00071426
Iteration 22/25 | Loss: 0.00071426
Iteration 23/25 | Loss: 0.00071426
Iteration 24/25 | Loss: 0.00071426
Iteration 25/25 | Loss: 0.00071426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61474299
Iteration 2/25 | Loss: 0.00037363
Iteration 3/25 | Loss: 0.00037362
Iteration 4/25 | Loss: 0.00037362
Iteration 5/25 | Loss: 0.00037362
Iteration 6/25 | Loss: 0.00037362
Iteration 7/25 | Loss: 0.00037362
Iteration 8/25 | Loss: 0.00037362
Iteration 9/25 | Loss: 0.00037362
Iteration 10/25 | Loss: 0.00037362
Iteration 11/25 | Loss: 0.00037362
Iteration 12/25 | Loss: 0.00037362
Iteration 13/25 | Loss: 0.00037362
Iteration 14/25 | Loss: 0.00037362
Iteration 15/25 | Loss: 0.00037362
Iteration 16/25 | Loss: 0.00037362
Iteration 17/25 | Loss: 0.00037362
Iteration 18/25 | Loss: 0.00037362
Iteration 19/25 | Loss: 0.00037362
Iteration 20/25 | Loss: 0.00037362
Iteration 21/25 | Loss: 0.00037362
Iteration 22/25 | Loss: 0.00037362
Iteration 23/25 | Loss: 0.00037362
Iteration 24/25 | Loss: 0.00037362
Iteration 25/25 | Loss: 0.00037362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037362
Iteration 2/1000 | Loss: 0.00003540
Iteration 3/1000 | Loss: 0.00002637
Iteration 4/1000 | Loss: 0.00002472
Iteration 5/1000 | Loss: 0.00002322
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002171
Iteration 8/1000 | Loss: 0.00002140
Iteration 9/1000 | Loss: 0.00002121
Iteration 10/1000 | Loss: 0.00002111
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002109
Iteration 13/1000 | Loss: 0.00002108
Iteration 14/1000 | Loss: 0.00002099
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002090
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002086
Iteration 20/1000 | Loss: 0.00002086
Iteration 21/1000 | Loss: 0.00002086
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00002086
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002081
Iteration 32/1000 | Loss: 0.00002081
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002081
Iteration 35/1000 | Loss: 0.00002081
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002081
Iteration 39/1000 | Loss: 0.00002081
Iteration 40/1000 | Loss: 0.00002080
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002078
Iteration 43/1000 | Loss: 0.00002078
Iteration 44/1000 | Loss: 0.00002078
Iteration 45/1000 | Loss: 0.00002078
Iteration 46/1000 | Loss: 0.00002077
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002077
Iteration 49/1000 | Loss: 0.00002077
Iteration 50/1000 | Loss: 0.00002077
Iteration 51/1000 | Loss: 0.00002077
Iteration 52/1000 | Loss: 0.00002077
Iteration 53/1000 | Loss: 0.00002076
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002074
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002070
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002069
Iteration 77/1000 | Loss: 0.00002069
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002068
Iteration 82/1000 | Loss: 0.00002068
Iteration 83/1000 | Loss: 0.00002068
Iteration 84/1000 | Loss: 0.00002068
Iteration 85/1000 | Loss: 0.00002068
Iteration 86/1000 | Loss: 0.00002068
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002067
Iteration 89/1000 | Loss: 0.00002067
Iteration 90/1000 | Loss: 0.00002066
Iteration 91/1000 | Loss: 0.00002066
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002065
Iteration 98/1000 | Loss: 0.00002065
Iteration 99/1000 | Loss: 0.00002064
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002064
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002063
Iteration 109/1000 | Loss: 0.00002063
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002062
Iteration 116/1000 | Loss: 0.00002062
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002062
Iteration 120/1000 | Loss: 0.00002062
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002062
Iteration 123/1000 | Loss: 0.00002061
Iteration 124/1000 | Loss: 0.00002061
Iteration 125/1000 | Loss: 0.00002061
Iteration 126/1000 | Loss: 0.00002061
Iteration 127/1000 | Loss: 0.00002061
Iteration 128/1000 | Loss: 0.00002061
Iteration 129/1000 | Loss: 0.00002061
Iteration 130/1000 | Loss: 0.00002061
Iteration 131/1000 | Loss: 0.00002061
Iteration 132/1000 | Loss: 0.00002061
Iteration 133/1000 | Loss: 0.00002061
Iteration 134/1000 | Loss: 0.00002061
Iteration 135/1000 | Loss: 0.00002061
Iteration 136/1000 | Loss: 0.00002061
Iteration 137/1000 | Loss: 0.00002061
Iteration 138/1000 | Loss: 0.00002060
Iteration 139/1000 | Loss: 0.00002060
Iteration 140/1000 | Loss: 0.00002060
Iteration 141/1000 | Loss: 0.00002060
Iteration 142/1000 | Loss: 0.00002060
Iteration 143/1000 | Loss: 0.00002060
Iteration 144/1000 | Loss: 0.00002060
Iteration 145/1000 | Loss: 0.00002060
Iteration 146/1000 | Loss: 0.00002060
Iteration 147/1000 | Loss: 0.00002060
Iteration 148/1000 | Loss: 0.00002060
Iteration 149/1000 | Loss: 0.00002060
Iteration 150/1000 | Loss: 0.00002060
Iteration 151/1000 | Loss: 0.00002060
Iteration 152/1000 | Loss: 0.00002060
Iteration 153/1000 | Loss: 0.00002060
Iteration 154/1000 | Loss: 0.00002060
Iteration 155/1000 | Loss: 0.00002060
Iteration 156/1000 | Loss: 0.00002060
Iteration 157/1000 | Loss: 0.00002060
Iteration 158/1000 | Loss: 0.00002059
Iteration 159/1000 | Loss: 0.00002059
Iteration 160/1000 | Loss: 0.00002059
Iteration 161/1000 | Loss: 0.00002059
Iteration 162/1000 | Loss: 0.00002059
Iteration 163/1000 | Loss: 0.00002059
Iteration 164/1000 | Loss: 0.00002059
Iteration 165/1000 | Loss: 0.00002059
Iteration 166/1000 | Loss: 0.00002059
Iteration 167/1000 | Loss: 0.00002059
Iteration 168/1000 | Loss: 0.00002059
Iteration 169/1000 | Loss: 0.00002059
Iteration 170/1000 | Loss: 0.00002059
Iteration 171/1000 | Loss: 0.00002059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.0592951841535978e-05, 2.0592951841535978e-05, 2.0592951841535978e-05, 2.0592951841535978e-05, 2.0592951841535978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0592951841535978e-05

Optimization complete. Final v2v error: 3.8816051483154297 mm

Highest mean error: 4.089110851287842 mm for frame 36

Lowest mean error: 3.585846424102783 mm for frame 1

Saving results

Total time: 36.79175853729248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381724
Iteration 2/25 | Loss: 0.00109027
Iteration 3/25 | Loss: 0.00081830
Iteration 4/25 | Loss: 0.00077410
Iteration 5/25 | Loss: 0.00076165
Iteration 6/25 | Loss: 0.00075894
Iteration 7/25 | Loss: 0.00075802
Iteration 8/25 | Loss: 0.00075802
Iteration 9/25 | Loss: 0.00075802
Iteration 10/25 | Loss: 0.00075802
Iteration 11/25 | Loss: 0.00075802
Iteration 12/25 | Loss: 0.00075802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007580217788927257, 0.0007580217788927257, 0.0007580217788927257, 0.0007580217788927257, 0.0007580217788927257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007580217788927257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76811349
Iteration 2/25 | Loss: 0.00024330
Iteration 3/25 | Loss: 0.00024330
Iteration 4/25 | Loss: 0.00024330
Iteration 5/25 | Loss: 0.00024330
Iteration 6/25 | Loss: 0.00024330
Iteration 7/25 | Loss: 0.00024330
Iteration 8/25 | Loss: 0.00024330
Iteration 9/25 | Loss: 0.00024330
Iteration 10/25 | Loss: 0.00024330
Iteration 11/25 | Loss: 0.00024330
Iteration 12/25 | Loss: 0.00024330
Iteration 13/25 | Loss: 0.00024330
Iteration 14/25 | Loss: 0.00024330
Iteration 15/25 | Loss: 0.00024330
Iteration 16/25 | Loss: 0.00024330
Iteration 17/25 | Loss: 0.00024330
Iteration 18/25 | Loss: 0.00024330
Iteration 19/25 | Loss: 0.00024330
Iteration 20/25 | Loss: 0.00024330
Iteration 21/25 | Loss: 0.00024330
Iteration 22/25 | Loss: 0.00024330
Iteration 23/25 | Loss: 0.00024330
Iteration 24/25 | Loss: 0.00024330
Iteration 25/25 | Loss: 0.00024330

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024330
Iteration 2/1000 | Loss: 0.00002672
Iteration 3/1000 | Loss: 0.00002175
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001922
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001842
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001789
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001785
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001782
Iteration 22/1000 | Loss: 0.00001782
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00001780
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001777
Iteration 29/1000 | Loss: 0.00001775
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001775
Iteration 32/1000 | Loss: 0.00001775
Iteration 33/1000 | Loss: 0.00001775
Iteration 34/1000 | Loss: 0.00001775
Iteration 35/1000 | Loss: 0.00001774
Iteration 36/1000 | Loss: 0.00001774
Iteration 37/1000 | Loss: 0.00001774
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001773
Iteration 41/1000 | Loss: 0.00001773
Iteration 42/1000 | Loss: 0.00001773
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001772
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001772
Iteration 61/1000 | Loss: 0.00001772
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001768
Iteration 86/1000 | Loss: 0.00001768
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001767
Iteration 113/1000 | Loss: 0.00001767
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001767
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001767
Iteration 137/1000 | Loss: 0.00001767
Iteration 138/1000 | Loss: 0.00001767
Iteration 139/1000 | Loss: 0.00001767
Iteration 140/1000 | Loss: 0.00001767
Iteration 141/1000 | Loss: 0.00001767
Iteration 142/1000 | Loss: 0.00001767
Iteration 143/1000 | Loss: 0.00001767
Iteration 144/1000 | Loss: 0.00001767
Iteration 145/1000 | Loss: 0.00001767
Iteration 146/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.766630884958431e-05, 1.766630884958431e-05, 1.766630884958431e-05, 1.766630884958431e-05, 1.766630884958431e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.766630884958431e-05

Optimization complete. Final v2v error: 3.6504502296447754 mm

Highest mean error: 4.2149810791015625 mm for frame 197

Lowest mean error: 3.122476816177368 mm for frame 258

Saving results

Total time: 38.11290383338928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660484
Iteration 2/25 | Loss: 0.00084335
Iteration 3/25 | Loss: 0.00072600
Iteration 4/25 | Loss: 0.00070621
Iteration 5/25 | Loss: 0.00069782
Iteration 6/25 | Loss: 0.00069696
Iteration 7/25 | Loss: 0.00069696
Iteration 8/25 | Loss: 0.00069696
Iteration 9/25 | Loss: 0.00069696
Iteration 10/25 | Loss: 0.00069696
Iteration 11/25 | Loss: 0.00069696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006969576934352517, 0.0006969576934352517, 0.0006969576934352517, 0.0006969576934352517, 0.0006969576934352517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006969576934352517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32541358
Iteration 2/25 | Loss: 0.00035804
Iteration 3/25 | Loss: 0.00035804
Iteration 4/25 | Loss: 0.00035803
Iteration 5/25 | Loss: 0.00035803
Iteration 6/25 | Loss: 0.00035803
Iteration 7/25 | Loss: 0.00035803
Iteration 8/25 | Loss: 0.00035803
Iteration 9/25 | Loss: 0.00035803
Iteration 10/25 | Loss: 0.00035803
Iteration 11/25 | Loss: 0.00035803
Iteration 12/25 | Loss: 0.00035803
Iteration 13/25 | Loss: 0.00035803
Iteration 14/25 | Loss: 0.00035803
Iteration 15/25 | Loss: 0.00035803
Iteration 16/25 | Loss: 0.00035803
Iteration 17/25 | Loss: 0.00035803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00035803287755697966, 0.00035803287755697966, 0.00035803287755697966, 0.00035803287755697966, 0.00035803287755697966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035803287755697966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035803
Iteration 2/1000 | Loss: 0.00003174
Iteration 3/1000 | Loss: 0.00002335
Iteration 4/1000 | Loss: 0.00002187
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001920
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001901
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001879
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001876
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001873
Iteration 25/1000 | Loss: 0.00001872
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001868
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001862
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001860
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001858
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001857
Iteration 58/1000 | Loss: 0.00001857
Iteration 59/1000 | Loss: 0.00001857
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001853
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001852
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001848
Iteration 82/1000 | Loss: 0.00001848
Iteration 83/1000 | Loss: 0.00001848
Iteration 84/1000 | Loss: 0.00001848
Iteration 85/1000 | Loss: 0.00001848
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001846
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001846
Iteration 95/1000 | Loss: 0.00001845
Iteration 96/1000 | Loss: 0.00001845
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001844
Iteration 99/1000 | Loss: 0.00001844
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001844
Iteration 104/1000 | Loss: 0.00001844
Iteration 105/1000 | Loss: 0.00001844
Iteration 106/1000 | Loss: 0.00001844
Iteration 107/1000 | Loss: 0.00001844
Iteration 108/1000 | Loss: 0.00001844
Iteration 109/1000 | Loss: 0.00001844
Iteration 110/1000 | Loss: 0.00001844
Iteration 111/1000 | Loss: 0.00001843
Iteration 112/1000 | Loss: 0.00001843
Iteration 113/1000 | Loss: 0.00001843
Iteration 114/1000 | Loss: 0.00001843
Iteration 115/1000 | Loss: 0.00001843
Iteration 116/1000 | Loss: 0.00001843
Iteration 117/1000 | Loss: 0.00001843
Iteration 118/1000 | Loss: 0.00001843
Iteration 119/1000 | Loss: 0.00001843
Iteration 120/1000 | Loss: 0.00001843
Iteration 121/1000 | Loss: 0.00001843
Iteration 122/1000 | Loss: 0.00001843
Iteration 123/1000 | Loss: 0.00001843
Iteration 124/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8432056094752625e-05, 1.8432056094752625e-05, 1.8432056094752625e-05, 1.8432056094752625e-05, 1.8432056094752625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8432056094752625e-05

Optimization complete. Final v2v error: 3.697175979614258 mm

Highest mean error: 3.892728567123413 mm for frame 45

Lowest mean error: 3.436209201812744 mm for frame 66

Saving results

Total time: 31.165467739105225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409483
Iteration 2/25 | Loss: 0.00084156
Iteration 3/25 | Loss: 0.00070224
Iteration 4/25 | Loss: 0.00068350
Iteration 5/25 | Loss: 0.00067747
Iteration 6/25 | Loss: 0.00067582
Iteration 7/25 | Loss: 0.00067545
Iteration 8/25 | Loss: 0.00067545
Iteration 9/25 | Loss: 0.00067545
Iteration 10/25 | Loss: 0.00067545
Iteration 11/25 | Loss: 0.00067545
Iteration 12/25 | Loss: 0.00067545
Iteration 13/25 | Loss: 0.00067545
Iteration 14/25 | Loss: 0.00067545
Iteration 15/25 | Loss: 0.00067545
Iteration 16/25 | Loss: 0.00067545
Iteration 17/25 | Loss: 0.00067545
Iteration 18/25 | Loss: 0.00067545
Iteration 19/25 | Loss: 0.00067545
Iteration 20/25 | Loss: 0.00067545
Iteration 21/25 | Loss: 0.00067545
Iteration 22/25 | Loss: 0.00067545
Iteration 23/25 | Loss: 0.00067545
Iteration 24/25 | Loss: 0.00067545
Iteration 25/25 | Loss: 0.00067545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27724254
Iteration 2/25 | Loss: 0.00029274
Iteration 3/25 | Loss: 0.00029272
Iteration 4/25 | Loss: 0.00029272
Iteration 5/25 | Loss: 0.00029272
Iteration 6/25 | Loss: 0.00029272
Iteration 7/25 | Loss: 0.00029272
Iteration 8/25 | Loss: 0.00029272
Iteration 9/25 | Loss: 0.00029272
Iteration 10/25 | Loss: 0.00029272
Iteration 11/25 | Loss: 0.00029272
Iteration 12/25 | Loss: 0.00029272
Iteration 13/25 | Loss: 0.00029272
Iteration 14/25 | Loss: 0.00029272
Iteration 15/25 | Loss: 0.00029272
Iteration 16/25 | Loss: 0.00029272
Iteration 17/25 | Loss: 0.00029272
Iteration 18/25 | Loss: 0.00029272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00029271579114720225, 0.00029271579114720225, 0.00029271579114720225, 0.00029271579114720225, 0.00029271579114720225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029271579114720225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029272
Iteration 2/1000 | Loss: 0.00002300
Iteration 3/1000 | Loss: 0.00001671
Iteration 4/1000 | Loss: 0.00001539
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001438
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001371
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001348
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001339
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001338
Iteration 39/1000 | Loss: 0.00001338
Iteration 40/1000 | Loss: 0.00001337
Iteration 41/1000 | Loss: 0.00001337
Iteration 42/1000 | Loss: 0.00001337
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001336
Iteration 45/1000 | Loss: 0.00001335
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001322
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001322
Iteration 108/1000 | Loss: 0.00001322
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3217163541412447e-05, 1.3217163541412447e-05, 1.3217163541412447e-05, 1.3217163541412447e-05, 1.3217163541412447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3217163541412447e-05

Optimization complete. Final v2v error: 3.189889669418335 mm

Highest mean error: 3.465477228164673 mm for frame 70

Lowest mean error: 2.960407257080078 mm for frame 89

Saving results

Total time: 33.382094621658325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424686
Iteration 2/25 | Loss: 0.00095091
Iteration 3/25 | Loss: 0.00083855
Iteration 4/25 | Loss: 0.00081385
Iteration 5/25 | Loss: 0.00080473
Iteration 6/25 | Loss: 0.00080353
Iteration 7/25 | Loss: 0.00080352
Iteration 8/25 | Loss: 0.00080352
Iteration 9/25 | Loss: 0.00080352
Iteration 10/25 | Loss: 0.00080352
Iteration 11/25 | Loss: 0.00080352
Iteration 12/25 | Loss: 0.00080352
Iteration 13/25 | Loss: 0.00080352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008035172941163182, 0.0008035172941163182, 0.0008035172941163182, 0.0008035172941163182, 0.0008035172941163182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008035172941163182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27055705
Iteration 2/25 | Loss: 0.00054366
Iteration 3/25 | Loss: 0.00054366
Iteration 4/25 | Loss: 0.00054366
Iteration 5/25 | Loss: 0.00054366
Iteration 6/25 | Loss: 0.00054366
Iteration 7/25 | Loss: 0.00054366
Iteration 8/25 | Loss: 0.00054366
Iteration 9/25 | Loss: 0.00054366
Iteration 10/25 | Loss: 0.00054366
Iteration 11/25 | Loss: 0.00054366
Iteration 12/25 | Loss: 0.00054366
Iteration 13/25 | Loss: 0.00054366
Iteration 14/25 | Loss: 0.00054366
Iteration 15/25 | Loss: 0.00054366
Iteration 16/25 | Loss: 0.00054366
Iteration 17/25 | Loss: 0.00054366
Iteration 18/25 | Loss: 0.00054366
Iteration 19/25 | Loss: 0.00054366
Iteration 20/25 | Loss: 0.00054366
Iteration 21/25 | Loss: 0.00054366
Iteration 22/25 | Loss: 0.00054366
Iteration 23/25 | Loss: 0.00054366
Iteration 24/25 | Loss: 0.00054366
Iteration 25/25 | Loss: 0.00054366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054366
Iteration 2/1000 | Loss: 0.00006340
Iteration 3/1000 | Loss: 0.00004764
Iteration 4/1000 | Loss: 0.00004327
Iteration 5/1000 | Loss: 0.00004117
Iteration 6/1000 | Loss: 0.00003914
Iteration 7/1000 | Loss: 0.00003771
Iteration 8/1000 | Loss: 0.00003691
Iteration 9/1000 | Loss: 0.00003648
Iteration 10/1000 | Loss: 0.00003621
Iteration 11/1000 | Loss: 0.00003608
Iteration 12/1000 | Loss: 0.00003599
Iteration 13/1000 | Loss: 0.00003589
Iteration 14/1000 | Loss: 0.00003582
Iteration 15/1000 | Loss: 0.00003578
Iteration 16/1000 | Loss: 0.00003571
Iteration 17/1000 | Loss: 0.00003570
Iteration 18/1000 | Loss: 0.00003570
Iteration 19/1000 | Loss: 0.00003569
Iteration 20/1000 | Loss: 0.00003569
Iteration 21/1000 | Loss: 0.00003568
Iteration 22/1000 | Loss: 0.00003566
Iteration 23/1000 | Loss: 0.00003566
Iteration 24/1000 | Loss: 0.00003565
Iteration 25/1000 | Loss: 0.00003565
Iteration 26/1000 | Loss: 0.00003565
Iteration 27/1000 | Loss: 0.00003565
Iteration 28/1000 | Loss: 0.00003565
Iteration 29/1000 | Loss: 0.00003565
Iteration 30/1000 | Loss: 0.00003563
Iteration 31/1000 | Loss: 0.00003563
Iteration 32/1000 | Loss: 0.00003562
Iteration 33/1000 | Loss: 0.00003562
Iteration 34/1000 | Loss: 0.00003562
Iteration 35/1000 | Loss: 0.00003562
Iteration 36/1000 | Loss: 0.00003561
Iteration 37/1000 | Loss: 0.00003561
Iteration 38/1000 | Loss: 0.00003560
Iteration 39/1000 | Loss: 0.00003560
Iteration 40/1000 | Loss: 0.00003559
Iteration 41/1000 | Loss: 0.00003559
Iteration 42/1000 | Loss: 0.00003559
Iteration 43/1000 | Loss: 0.00003559
Iteration 44/1000 | Loss: 0.00003559
Iteration 45/1000 | Loss: 0.00003558
Iteration 46/1000 | Loss: 0.00003558
Iteration 47/1000 | Loss: 0.00003558
Iteration 48/1000 | Loss: 0.00003558
Iteration 49/1000 | Loss: 0.00003557
Iteration 50/1000 | Loss: 0.00003557
Iteration 51/1000 | Loss: 0.00003557
Iteration 52/1000 | Loss: 0.00003557
Iteration 53/1000 | Loss: 0.00003557
Iteration 54/1000 | Loss: 0.00003557
Iteration 55/1000 | Loss: 0.00003557
Iteration 56/1000 | Loss: 0.00003556
Iteration 57/1000 | Loss: 0.00003556
Iteration 58/1000 | Loss: 0.00003556
Iteration 59/1000 | Loss: 0.00003556
Iteration 60/1000 | Loss: 0.00003556
Iteration 61/1000 | Loss: 0.00003556
Iteration 62/1000 | Loss: 0.00003556
Iteration 63/1000 | Loss: 0.00003556
Iteration 64/1000 | Loss: 0.00003556
Iteration 65/1000 | Loss: 0.00003556
Iteration 66/1000 | Loss: 0.00003555
Iteration 67/1000 | Loss: 0.00003555
Iteration 68/1000 | Loss: 0.00003555
Iteration 69/1000 | Loss: 0.00003555
Iteration 70/1000 | Loss: 0.00003555
Iteration 71/1000 | Loss: 0.00003555
Iteration 72/1000 | Loss: 0.00003555
Iteration 73/1000 | Loss: 0.00003555
Iteration 74/1000 | Loss: 0.00003555
Iteration 75/1000 | Loss: 0.00003555
Iteration 76/1000 | Loss: 0.00003554
Iteration 77/1000 | Loss: 0.00003554
Iteration 78/1000 | Loss: 0.00003554
Iteration 79/1000 | Loss: 0.00003554
Iteration 80/1000 | Loss: 0.00003554
Iteration 81/1000 | Loss: 0.00003554
Iteration 82/1000 | Loss: 0.00003554
Iteration 83/1000 | Loss: 0.00003553
Iteration 84/1000 | Loss: 0.00003553
Iteration 85/1000 | Loss: 0.00003553
Iteration 86/1000 | Loss: 0.00003553
Iteration 87/1000 | Loss: 0.00003553
Iteration 88/1000 | Loss: 0.00003553
Iteration 89/1000 | Loss: 0.00003553
Iteration 90/1000 | Loss: 0.00003553
Iteration 91/1000 | Loss: 0.00003552
Iteration 92/1000 | Loss: 0.00003552
Iteration 93/1000 | Loss: 0.00003552
Iteration 94/1000 | Loss: 0.00003552
Iteration 95/1000 | Loss: 0.00003552
Iteration 96/1000 | Loss: 0.00003551
Iteration 97/1000 | Loss: 0.00003551
Iteration 98/1000 | Loss: 0.00003551
Iteration 99/1000 | Loss: 0.00003551
Iteration 100/1000 | Loss: 0.00003551
Iteration 101/1000 | Loss: 0.00003551
Iteration 102/1000 | Loss: 0.00003551
Iteration 103/1000 | Loss: 0.00003551
Iteration 104/1000 | Loss: 0.00003551
Iteration 105/1000 | Loss: 0.00003551
Iteration 106/1000 | Loss: 0.00003551
Iteration 107/1000 | Loss: 0.00003551
Iteration 108/1000 | Loss: 0.00003551
Iteration 109/1000 | Loss: 0.00003551
Iteration 110/1000 | Loss: 0.00003551
Iteration 111/1000 | Loss: 0.00003551
Iteration 112/1000 | Loss: 0.00003551
Iteration 113/1000 | Loss: 0.00003550
Iteration 114/1000 | Loss: 0.00003550
Iteration 115/1000 | Loss: 0.00003550
Iteration 116/1000 | Loss: 0.00003550
Iteration 117/1000 | Loss: 0.00003550
Iteration 118/1000 | Loss: 0.00003550
Iteration 119/1000 | Loss: 0.00003550
Iteration 120/1000 | Loss: 0.00003550
Iteration 121/1000 | Loss: 0.00003550
Iteration 122/1000 | Loss: 0.00003550
Iteration 123/1000 | Loss: 0.00003550
Iteration 124/1000 | Loss: 0.00003550
Iteration 125/1000 | Loss: 0.00003550
Iteration 126/1000 | Loss: 0.00003550
Iteration 127/1000 | Loss: 0.00003550
Iteration 128/1000 | Loss: 0.00003550
Iteration 129/1000 | Loss: 0.00003550
Iteration 130/1000 | Loss: 0.00003550
Iteration 131/1000 | Loss: 0.00003550
Iteration 132/1000 | Loss: 0.00003550
Iteration 133/1000 | Loss: 0.00003550
Iteration 134/1000 | Loss: 0.00003550
Iteration 135/1000 | Loss: 0.00003550
Iteration 136/1000 | Loss: 0.00003550
Iteration 137/1000 | Loss: 0.00003550
Iteration 138/1000 | Loss: 0.00003550
Iteration 139/1000 | Loss: 0.00003550
Iteration 140/1000 | Loss: 0.00003550
Iteration 141/1000 | Loss: 0.00003550
Iteration 142/1000 | Loss: 0.00003550
Iteration 143/1000 | Loss: 0.00003550
Iteration 144/1000 | Loss: 0.00003550
Iteration 145/1000 | Loss: 0.00003550
Iteration 146/1000 | Loss: 0.00003550
Iteration 147/1000 | Loss: 0.00003550
Iteration 148/1000 | Loss: 0.00003550
Iteration 149/1000 | Loss: 0.00003550
Iteration 150/1000 | Loss: 0.00003550
Iteration 151/1000 | Loss: 0.00003550
Iteration 152/1000 | Loss: 0.00003550
Iteration 153/1000 | Loss: 0.00003550
Iteration 154/1000 | Loss: 0.00003550
Iteration 155/1000 | Loss: 0.00003550
Iteration 156/1000 | Loss: 0.00003550
Iteration 157/1000 | Loss: 0.00003550
Iteration 158/1000 | Loss: 0.00003550
Iteration 159/1000 | Loss: 0.00003550
Iteration 160/1000 | Loss: 0.00003550
Iteration 161/1000 | Loss: 0.00003550
Iteration 162/1000 | Loss: 0.00003550
Iteration 163/1000 | Loss: 0.00003550
Iteration 164/1000 | Loss: 0.00003550
Iteration 165/1000 | Loss: 0.00003550
Iteration 166/1000 | Loss: 0.00003550
Iteration 167/1000 | Loss: 0.00003550
Iteration 168/1000 | Loss: 0.00003550
Iteration 169/1000 | Loss: 0.00003550
Iteration 170/1000 | Loss: 0.00003550
Iteration 171/1000 | Loss: 0.00003550
Iteration 172/1000 | Loss: 0.00003550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [3.5499208024702966e-05, 3.5499208024702966e-05, 3.5499208024702966e-05, 3.5499208024702966e-05, 3.5499208024702966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5499208024702966e-05

Optimization complete. Final v2v error: 4.9964280128479 mm

Highest mean error: 5.229798316955566 mm for frame 141

Lowest mean error: 4.447957515716553 mm for frame 46

Saving results

Total time: 36.794323205947876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096146
Iteration 2/25 | Loss: 0.00135775
Iteration 3/25 | Loss: 0.00103130
Iteration 4/25 | Loss: 0.00073898
Iteration 5/25 | Loss: 0.00068510
Iteration 6/25 | Loss: 0.00067429
Iteration 7/25 | Loss: 0.00067956
Iteration 8/25 | Loss: 0.00068123
Iteration 9/25 | Loss: 0.00069554
Iteration 10/25 | Loss: 0.00068926
Iteration 11/25 | Loss: 0.00068561
Iteration 12/25 | Loss: 0.00068931
Iteration 13/25 | Loss: 0.00068454
Iteration 14/25 | Loss: 0.00067412
Iteration 15/25 | Loss: 0.00067153
Iteration 16/25 | Loss: 0.00066969
Iteration 17/25 | Loss: 0.00066675
Iteration 18/25 | Loss: 0.00066722
Iteration 19/25 | Loss: 0.00066766
Iteration 20/25 | Loss: 0.00066895
Iteration 21/25 | Loss: 0.00066727
Iteration 22/25 | Loss: 0.00066726
Iteration 23/25 | Loss: 0.00066746
Iteration 24/25 | Loss: 0.00066994
Iteration 25/25 | Loss: 0.00066694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34225428
Iteration 2/25 | Loss: 0.00043952
Iteration 3/25 | Loss: 0.00043951
Iteration 4/25 | Loss: 0.00043951
Iteration 5/25 | Loss: 0.00043951
Iteration 6/25 | Loss: 0.00043951
Iteration 7/25 | Loss: 0.00043951
Iteration 8/25 | Loss: 0.00043951
Iteration 9/25 | Loss: 0.00043951
Iteration 10/25 | Loss: 0.00043951
Iteration 11/25 | Loss: 0.00043951
Iteration 12/25 | Loss: 0.00043951
Iteration 13/25 | Loss: 0.00043951
Iteration 14/25 | Loss: 0.00043951
Iteration 15/25 | Loss: 0.00043951
Iteration 16/25 | Loss: 0.00043951
Iteration 17/25 | Loss: 0.00043951
Iteration 18/25 | Loss: 0.00043951
Iteration 19/25 | Loss: 0.00043951
Iteration 20/25 | Loss: 0.00043951
Iteration 21/25 | Loss: 0.00043951
Iteration 22/25 | Loss: 0.00043951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00043951242696493864, 0.00043951242696493864, 0.00043951242696493864, 0.00043951242696493864, 0.00043951242696493864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043951242696493864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043951
Iteration 2/1000 | Loss: 0.00025240
Iteration 3/1000 | Loss: 0.00020763
Iteration 4/1000 | Loss: 0.00005072
Iteration 5/1000 | Loss: 0.00034290
Iteration 6/1000 | Loss: 0.00020204
Iteration 7/1000 | Loss: 0.00037744
Iteration 8/1000 | Loss: 0.00044422
Iteration 9/1000 | Loss: 0.00013940
Iteration 10/1000 | Loss: 0.00024789
Iteration 11/1000 | Loss: 0.00003700
Iteration 12/1000 | Loss: 0.00002985
Iteration 13/1000 | Loss: 0.00002548
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00040060
Iteration 16/1000 | Loss: 0.00003026
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00044782
Iteration 19/1000 | Loss: 0.00002387
Iteration 20/1000 | Loss: 0.00002088
Iteration 21/1000 | Loss: 0.00003828
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001897
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001885
Iteration 29/1000 | Loss: 0.00001884
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001603
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001457
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001427
Iteration 47/1000 | Loss: 0.00001427
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001422
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001418
Iteration 68/1000 | Loss: 0.00001418
Iteration 69/1000 | Loss: 0.00001418
Iteration 70/1000 | Loss: 0.00001417
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001415
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001410
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001409
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001406
Iteration 127/1000 | Loss: 0.00001406
Iteration 128/1000 | Loss: 0.00001406
Iteration 129/1000 | Loss: 0.00001406
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001406
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001405
Iteration 136/1000 | Loss: 0.00001405
Iteration 137/1000 | Loss: 0.00001405
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001405
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001404
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001403
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001403
Iteration 176/1000 | Loss: 0.00001403
Iteration 177/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.4029430531081744e-05, 1.4029430531081744e-05, 1.4029430531081744e-05, 1.4029430531081744e-05, 1.4029430531081744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4029430531081744e-05

Optimization complete. Final v2v error: 3.2162272930145264 mm

Highest mean error: 3.9357216358184814 mm for frame 81

Lowest mean error: 2.7929930686950684 mm for frame 120

Saving results

Total time: 105.18085360527039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623156
Iteration 2/25 | Loss: 0.00121885
Iteration 3/25 | Loss: 0.00106861
Iteration 4/25 | Loss: 0.00099514
Iteration 5/25 | Loss: 0.00098450
Iteration 6/25 | Loss: 0.00098127
Iteration 7/25 | Loss: 0.00098028
Iteration 8/25 | Loss: 0.00098023
Iteration 9/25 | Loss: 0.00098023
Iteration 10/25 | Loss: 0.00098023
Iteration 11/25 | Loss: 0.00098023
Iteration 12/25 | Loss: 0.00098023
Iteration 13/25 | Loss: 0.00098023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009802347049117088, 0.0009802347049117088, 0.0009802347049117088, 0.0009802347049117088, 0.0009802347049117088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009802347049117088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27561545
Iteration 2/25 | Loss: 0.00056807
Iteration 3/25 | Loss: 0.00056802
Iteration 4/25 | Loss: 0.00056802
Iteration 5/25 | Loss: 0.00056802
Iteration 6/25 | Loss: 0.00056802
Iteration 7/25 | Loss: 0.00056802
Iteration 8/25 | Loss: 0.00056802
Iteration 9/25 | Loss: 0.00056802
Iteration 10/25 | Loss: 0.00056802
Iteration 11/25 | Loss: 0.00056802
Iteration 12/25 | Loss: 0.00056802
Iteration 13/25 | Loss: 0.00056802
Iteration 14/25 | Loss: 0.00056802
Iteration 15/25 | Loss: 0.00056802
Iteration 16/25 | Loss: 0.00056802
Iteration 17/25 | Loss: 0.00056802
Iteration 18/25 | Loss: 0.00056802
Iteration 19/25 | Loss: 0.00056802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005680222529917955, 0.0005680222529917955, 0.0005680222529917955, 0.0005680222529917955, 0.0005680222529917955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005680222529917955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056802
Iteration 2/1000 | Loss: 0.00008970
Iteration 3/1000 | Loss: 0.00007130
Iteration 4/1000 | Loss: 0.00005864
Iteration 5/1000 | Loss: 0.00005499
Iteration 6/1000 | Loss: 0.00005307
Iteration 7/1000 | Loss: 0.00005189
Iteration 8/1000 | Loss: 0.00005087
Iteration 9/1000 | Loss: 0.00004998
Iteration 10/1000 | Loss: 0.00004926
Iteration 11/1000 | Loss: 0.00004880
Iteration 12/1000 | Loss: 0.00004846
Iteration 13/1000 | Loss: 0.00004824
Iteration 14/1000 | Loss: 0.00004805
Iteration 15/1000 | Loss: 0.00004800
Iteration 16/1000 | Loss: 0.00004799
Iteration 17/1000 | Loss: 0.00004785
Iteration 18/1000 | Loss: 0.00004783
Iteration 19/1000 | Loss: 0.00004780
Iteration 20/1000 | Loss: 0.00004780
Iteration 21/1000 | Loss: 0.00004779
Iteration 22/1000 | Loss: 0.00004779
Iteration 23/1000 | Loss: 0.00004779
Iteration 24/1000 | Loss: 0.00004778
Iteration 25/1000 | Loss: 0.00004778
Iteration 26/1000 | Loss: 0.00004777
Iteration 27/1000 | Loss: 0.00004777
Iteration 28/1000 | Loss: 0.00004777
Iteration 29/1000 | Loss: 0.00004774
Iteration 30/1000 | Loss: 0.00004773
Iteration 31/1000 | Loss: 0.00004772
Iteration 32/1000 | Loss: 0.00004772
Iteration 33/1000 | Loss: 0.00004772
Iteration 34/1000 | Loss: 0.00004771
Iteration 35/1000 | Loss: 0.00004771
Iteration 36/1000 | Loss: 0.00004771
Iteration 37/1000 | Loss: 0.00004771
Iteration 38/1000 | Loss: 0.00004771
Iteration 39/1000 | Loss: 0.00004770
Iteration 40/1000 | Loss: 0.00004770
Iteration 41/1000 | Loss: 0.00004769
Iteration 42/1000 | Loss: 0.00004769
Iteration 43/1000 | Loss: 0.00004769
Iteration 44/1000 | Loss: 0.00004769
Iteration 45/1000 | Loss: 0.00004768
Iteration 46/1000 | Loss: 0.00004768
Iteration 47/1000 | Loss: 0.00004768
Iteration 48/1000 | Loss: 0.00004768
Iteration 49/1000 | Loss: 0.00004768
Iteration 50/1000 | Loss: 0.00004768
Iteration 51/1000 | Loss: 0.00004768
Iteration 52/1000 | Loss: 0.00004768
Iteration 53/1000 | Loss: 0.00004768
Iteration 54/1000 | Loss: 0.00004767
Iteration 55/1000 | Loss: 0.00004767
Iteration 56/1000 | Loss: 0.00004767
Iteration 57/1000 | Loss: 0.00004767
Iteration 58/1000 | Loss: 0.00004767
Iteration 59/1000 | Loss: 0.00004767
Iteration 60/1000 | Loss: 0.00004767
Iteration 61/1000 | Loss: 0.00004767
Iteration 62/1000 | Loss: 0.00004767
Iteration 63/1000 | Loss: 0.00004766
Iteration 64/1000 | Loss: 0.00004766
Iteration 65/1000 | Loss: 0.00004766
Iteration 66/1000 | Loss: 0.00004766
Iteration 67/1000 | Loss: 0.00004765
Iteration 68/1000 | Loss: 0.00004765
Iteration 69/1000 | Loss: 0.00004765
Iteration 70/1000 | Loss: 0.00004765
Iteration 71/1000 | Loss: 0.00004764
Iteration 72/1000 | Loss: 0.00004764
Iteration 73/1000 | Loss: 0.00004764
Iteration 74/1000 | Loss: 0.00004764
Iteration 75/1000 | Loss: 0.00004764
Iteration 76/1000 | Loss: 0.00004764
Iteration 77/1000 | Loss: 0.00004764
Iteration 78/1000 | Loss: 0.00004764
Iteration 79/1000 | Loss: 0.00004764
Iteration 80/1000 | Loss: 0.00004763
Iteration 81/1000 | Loss: 0.00004763
Iteration 82/1000 | Loss: 0.00004763
Iteration 83/1000 | Loss: 0.00004763
Iteration 84/1000 | Loss: 0.00004763
Iteration 85/1000 | Loss: 0.00004763
Iteration 86/1000 | Loss: 0.00004763
Iteration 87/1000 | Loss: 0.00004763
Iteration 88/1000 | Loss: 0.00004763
Iteration 89/1000 | Loss: 0.00004763
Iteration 90/1000 | Loss: 0.00004763
Iteration 91/1000 | Loss: 0.00004763
Iteration 92/1000 | Loss: 0.00004763
Iteration 93/1000 | Loss: 0.00004762
Iteration 94/1000 | Loss: 0.00004762
Iteration 95/1000 | Loss: 0.00004762
Iteration 96/1000 | Loss: 0.00004762
Iteration 97/1000 | Loss: 0.00004762
Iteration 98/1000 | Loss: 0.00004762
Iteration 99/1000 | Loss: 0.00004762
Iteration 100/1000 | Loss: 0.00004762
Iteration 101/1000 | Loss: 0.00004761
Iteration 102/1000 | Loss: 0.00004761
Iteration 103/1000 | Loss: 0.00004761
Iteration 104/1000 | Loss: 0.00004761
Iteration 105/1000 | Loss: 0.00004761
Iteration 106/1000 | Loss: 0.00004761
Iteration 107/1000 | Loss: 0.00004761
Iteration 108/1000 | Loss: 0.00004760
Iteration 109/1000 | Loss: 0.00004760
Iteration 110/1000 | Loss: 0.00004760
Iteration 111/1000 | Loss: 0.00004760
Iteration 112/1000 | Loss: 0.00004760
Iteration 113/1000 | Loss: 0.00004760
Iteration 114/1000 | Loss: 0.00004760
Iteration 115/1000 | Loss: 0.00004759
Iteration 116/1000 | Loss: 0.00004759
Iteration 117/1000 | Loss: 0.00004759
Iteration 118/1000 | Loss: 0.00004759
Iteration 119/1000 | Loss: 0.00004759
Iteration 120/1000 | Loss: 0.00004759
Iteration 121/1000 | Loss: 0.00004759
Iteration 122/1000 | Loss: 0.00004758
Iteration 123/1000 | Loss: 0.00004758
Iteration 124/1000 | Loss: 0.00004758
Iteration 125/1000 | Loss: 0.00004758
Iteration 126/1000 | Loss: 0.00004758
Iteration 127/1000 | Loss: 0.00004758
Iteration 128/1000 | Loss: 0.00004757
Iteration 129/1000 | Loss: 0.00004757
Iteration 130/1000 | Loss: 0.00004757
Iteration 131/1000 | Loss: 0.00004757
Iteration 132/1000 | Loss: 0.00004757
Iteration 133/1000 | Loss: 0.00004757
Iteration 134/1000 | Loss: 0.00004757
Iteration 135/1000 | Loss: 0.00004757
Iteration 136/1000 | Loss: 0.00004757
Iteration 137/1000 | Loss: 0.00004756
Iteration 138/1000 | Loss: 0.00004756
Iteration 139/1000 | Loss: 0.00004756
Iteration 140/1000 | Loss: 0.00004756
Iteration 141/1000 | Loss: 0.00004756
Iteration 142/1000 | Loss: 0.00004756
Iteration 143/1000 | Loss: 0.00004756
Iteration 144/1000 | Loss: 0.00004756
Iteration 145/1000 | Loss: 0.00004756
Iteration 146/1000 | Loss: 0.00004756
Iteration 147/1000 | Loss: 0.00004756
Iteration 148/1000 | Loss: 0.00004756
Iteration 149/1000 | Loss: 0.00004756
Iteration 150/1000 | Loss: 0.00004756
Iteration 151/1000 | Loss: 0.00004756
Iteration 152/1000 | Loss: 0.00004756
Iteration 153/1000 | Loss: 0.00004755
Iteration 154/1000 | Loss: 0.00004755
Iteration 155/1000 | Loss: 0.00004755
Iteration 156/1000 | Loss: 0.00004755
Iteration 157/1000 | Loss: 0.00004755
Iteration 158/1000 | Loss: 0.00004755
Iteration 159/1000 | Loss: 0.00004755
Iteration 160/1000 | Loss: 0.00004754
Iteration 161/1000 | Loss: 0.00004754
Iteration 162/1000 | Loss: 0.00004754
Iteration 163/1000 | Loss: 0.00004754
Iteration 164/1000 | Loss: 0.00004754
Iteration 165/1000 | Loss: 0.00004754
Iteration 166/1000 | Loss: 0.00004754
Iteration 167/1000 | Loss: 0.00004754
Iteration 168/1000 | Loss: 0.00004754
Iteration 169/1000 | Loss: 0.00004754
Iteration 170/1000 | Loss: 0.00004754
Iteration 171/1000 | Loss: 0.00004754
Iteration 172/1000 | Loss: 0.00004753
Iteration 173/1000 | Loss: 0.00004753
Iteration 174/1000 | Loss: 0.00004753
Iteration 175/1000 | Loss: 0.00004753
Iteration 176/1000 | Loss: 0.00004753
Iteration 177/1000 | Loss: 0.00004752
Iteration 178/1000 | Loss: 0.00004752
Iteration 179/1000 | Loss: 0.00004752
Iteration 180/1000 | Loss: 0.00004752
Iteration 181/1000 | Loss: 0.00004752
Iteration 182/1000 | Loss: 0.00004752
Iteration 183/1000 | Loss: 0.00004752
Iteration 184/1000 | Loss: 0.00004752
Iteration 185/1000 | Loss: 0.00004752
Iteration 186/1000 | Loss: 0.00004752
Iteration 187/1000 | Loss: 0.00004752
Iteration 188/1000 | Loss: 0.00004752
Iteration 189/1000 | Loss: 0.00004752
Iteration 190/1000 | Loss: 0.00004752
Iteration 191/1000 | Loss: 0.00004751
Iteration 192/1000 | Loss: 0.00004751
Iteration 193/1000 | Loss: 0.00004751
Iteration 194/1000 | Loss: 0.00004751
Iteration 195/1000 | Loss: 0.00004751
Iteration 196/1000 | Loss: 0.00004751
Iteration 197/1000 | Loss: 0.00004751
Iteration 198/1000 | Loss: 0.00004751
Iteration 199/1000 | Loss: 0.00004751
Iteration 200/1000 | Loss: 0.00004751
Iteration 201/1000 | Loss: 0.00004751
Iteration 202/1000 | Loss: 0.00004751
Iteration 203/1000 | Loss: 0.00004751
Iteration 204/1000 | Loss: 0.00004751
Iteration 205/1000 | Loss: 0.00004751
Iteration 206/1000 | Loss: 0.00004751
Iteration 207/1000 | Loss: 0.00004751
Iteration 208/1000 | Loss: 0.00004751
Iteration 209/1000 | Loss: 0.00004751
Iteration 210/1000 | Loss: 0.00004751
Iteration 211/1000 | Loss: 0.00004751
Iteration 212/1000 | Loss: 0.00004751
Iteration 213/1000 | Loss: 0.00004750
Iteration 214/1000 | Loss: 0.00004750
Iteration 215/1000 | Loss: 0.00004750
Iteration 216/1000 | Loss: 0.00004750
Iteration 217/1000 | Loss: 0.00004750
Iteration 218/1000 | Loss: 0.00004750
Iteration 219/1000 | Loss: 0.00004750
Iteration 220/1000 | Loss: 0.00004750
Iteration 221/1000 | Loss: 0.00004750
Iteration 222/1000 | Loss: 0.00004750
Iteration 223/1000 | Loss: 0.00004750
Iteration 224/1000 | Loss: 0.00004750
Iteration 225/1000 | Loss: 0.00004750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [4.7502642701147124e-05, 4.7502642701147124e-05, 4.7502642701147124e-05, 4.7502642701147124e-05, 4.7502642701147124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7502642701147124e-05

Optimization complete. Final v2v error: 5.606276512145996 mm

Highest mean error: 6.545608997344971 mm for frame 119

Lowest mean error: 4.595876216888428 mm for frame 16

Saving results

Total time: 45.726749658584595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878614
Iteration 2/25 | Loss: 0.00144279
Iteration 3/25 | Loss: 0.00090403
Iteration 4/25 | Loss: 0.00084801
Iteration 5/25 | Loss: 0.00084421
Iteration 6/25 | Loss: 0.00084394
Iteration 7/25 | Loss: 0.00084394
Iteration 8/25 | Loss: 0.00084394
Iteration 9/25 | Loss: 0.00084394
Iteration 10/25 | Loss: 0.00084394
Iteration 11/25 | Loss: 0.00084394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008439371013082564, 0.0008439371013082564, 0.0008439371013082564, 0.0008439371013082564, 0.0008439371013082564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008439371013082564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25821459
Iteration 2/25 | Loss: 0.00061062
Iteration 3/25 | Loss: 0.00061062
Iteration 4/25 | Loss: 0.00061062
Iteration 5/25 | Loss: 0.00061062
Iteration 6/25 | Loss: 0.00061062
Iteration 7/25 | Loss: 0.00061062
Iteration 8/25 | Loss: 0.00061062
Iteration 9/25 | Loss: 0.00061062
Iteration 10/25 | Loss: 0.00061062
Iteration 11/25 | Loss: 0.00061062
Iteration 12/25 | Loss: 0.00061062
Iteration 13/25 | Loss: 0.00061062
Iteration 14/25 | Loss: 0.00061062
Iteration 15/25 | Loss: 0.00061062
Iteration 16/25 | Loss: 0.00061062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006106198416091502, 0.0006106198416091502, 0.0006106198416091502, 0.0006106198416091502, 0.0006106198416091502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006106198416091502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061062
Iteration 2/1000 | Loss: 0.00005600
Iteration 3/1000 | Loss: 0.00003916
Iteration 4/1000 | Loss: 0.00003571
Iteration 5/1000 | Loss: 0.00003380
Iteration 6/1000 | Loss: 0.00003189
Iteration 7/1000 | Loss: 0.00003034
Iteration 8/1000 | Loss: 0.00002945
Iteration 9/1000 | Loss: 0.00002895
Iteration 10/1000 | Loss: 0.00002863
Iteration 11/1000 | Loss: 0.00002841
Iteration 12/1000 | Loss: 0.00002818
Iteration 13/1000 | Loss: 0.00002807
Iteration 14/1000 | Loss: 0.00002802
Iteration 15/1000 | Loss: 0.00002799
Iteration 16/1000 | Loss: 0.00002799
Iteration 17/1000 | Loss: 0.00002799
Iteration 18/1000 | Loss: 0.00002799
Iteration 19/1000 | Loss: 0.00002799
Iteration 20/1000 | Loss: 0.00002798
Iteration 21/1000 | Loss: 0.00002798
Iteration 22/1000 | Loss: 0.00002797
Iteration 23/1000 | Loss: 0.00002797
Iteration 24/1000 | Loss: 0.00002796
Iteration 25/1000 | Loss: 0.00002796
Iteration 26/1000 | Loss: 0.00002796
Iteration 27/1000 | Loss: 0.00002795
Iteration 28/1000 | Loss: 0.00002795
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00002790
Iteration 31/1000 | Loss: 0.00002790
Iteration 32/1000 | Loss: 0.00002790
Iteration 33/1000 | Loss: 0.00002790
Iteration 34/1000 | Loss: 0.00002788
Iteration 35/1000 | Loss: 0.00002787
Iteration 36/1000 | Loss: 0.00002784
Iteration 37/1000 | Loss: 0.00002784
Iteration 38/1000 | Loss: 0.00002784
Iteration 39/1000 | Loss: 0.00002782
Iteration 40/1000 | Loss: 0.00002782
Iteration 41/1000 | Loss: 0.00002781
Iteration 42/1000 | Loss: 0.00002781
Iteration 43/1000 | Loss: 0.00002781
Iteration 44/1000 | Loss: 0.00002780
Iteration 45/1000 | Loss: 0.00002780
Iteration 46/1000 | Loss: 0.00002780
Iteration 47/1000 | Loss: 0.00002780
Iteration 48/1000 | Loss: 0.00002780
Iteration 49/1000 | Loss: 0.00002780
Iteration 50/1000 | Loss: 0.00002780
Iteration 51/1000 | Loss: 0.00002779
Iteration 52/1000 | Loss: 0.00002779
Iteration 53/1000 | Loss: 0.00002779
Iteration 54/1000 | Loss: 0.00002779
Iteration 55/1000 | Loss: 0.00002778
Iteration 56/1000 | Loss: 0.00002778
Iteration 57/1000 | Loss: 0.00002778
Iteration 58/1000 | Loss: 0.00002778
Iteration 59/1000 | Loss: 0.00002777
Iteration 60/1000 | Loss: 0.00002777
Iteration 61/1000 | Loss: 0.00002777
Iteration 62/1000 | Loss: 0.00002777
Iteration 63/1000 | Loss: 0.00002777
Iteration 64/1000 | Loss: 0.00002777
Iteration 65/1000 | Loss: 0.00002777
Iteration 66/1000 | Loss: 0.00002777
Iteration 67/1000 | Loss: 0.00002777
Iteration 68/1000 | Loss: 0.00002777
Iteration 69/1000 | Loss: 0.00002777
Iteration 70/1000 | Loss: 0.00002777
Iteration 71/1000 | Loss: 0.00002777
Iteration 72/1000 | Loss: 0.00002776
Iteration 73/1000 | Loss: 0.00002776
Iteration 74/1000 | Loss: 0.00002776
Iteration 75/1000 | Loss: 0.00002776
Iteration 76/1000 | Loss: 0.00002776
Iteration 77/1000 | Loss: 0.00002776
Iteration 78/1000 | Loss: 0.00002776
Iteration 79/1000 | Loss: 0.00002776
Iteration 80/1000 | Loss: 0.00002776
Iteration 81/1000 | Loss: 0.00002775
Iteration 82/1000 | Loss: 0.00002775
Iteration 83/1000 | Loss: 0.00002775
Iteration 84/1000 | Loss: 0.00002775
Iteration 85/1000 | Loss: 0.00002775
Iteration 86/1000 | Loss: 0.00002775
Iteration 87/1000 | Loss: 0.00002775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.7751701054512523e-05, 2.7751701054512523e-05, 2.7751701054512523e-05, 2.7751701054512523e-05, 2.7751701054512523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7751701054512523e-05

Optimization complete. Final v2v error: 4.436726093292236 mm

Highest mean error: 4.614016532897949 mm for frame 157

Lowest mean error: 4.148219585418701 mm for frame 2

Saving results

Total time: 38.05858254432678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947867
Iteration 2/25 | Loss: 0.00123279
Iteration 3/25 | Loss: 0.00088285
Iteration 4/25 | Loss: 0.00084853
Iteration 5/25 | Loss: 0.00083840
Iteration 6/25 | Loss: 0.00083646
Iteration 7/25 | Loss: 0.00083578
Iteration 8/25 | Loss: 0.00083572
Iteration 9/25 | Loss: 0.00083572
Iteration 10/25 | Loss: 0.00083572
Iteration 11/25 | Loss: 0.00083572
Iteration 12/25 | Loss: 0.00083572
Iteration 13/25 | Loss: 0.00083572
Iteration 14/25 | Loss: 0.00083572
Iteration 15/25 | Loss: 0.00083572
Iteration 16/25 | Loss: 0.00083572
Iteration 17/25 | Loss: 0.00083572
Iteration 18/25 | Loss: 0.00083572
Iteration 19/25 | Loss: 0.00083572
Iteration 20/25 | Loss: 0.00083572
Iteration 21/25 | Loss: 0.00083572
Iteration 22/25 | Loss: 0.00083572
Iteration 23/25 | Loss: 0.00083572
Iteration 24/25 | Loss: 0.00083572
Iteration 25/25 | Loss: 0.00083572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26743388
Iteration 2/25 | Loss: 0.00039165
Iteration 3/25 | Loss: 0.00039162
Iteration 4/25 | Loss: 0.00039162
Iteration 5/25 | Loss: 0.00039162
Iteration 6/25 | Loss: 0.00039162
Iteration 7/25 | Loss: 0.00039162
Iteration 8/25 | Loss: 0.00039162
Iteration 9/25 | Loss: 0.00039162
Iteration 10/25 | Loss: 0.00039162
Iteration 11/25 | Loss: 0.00039162
Iteration 12/25 | Loss: 0.00039162
Iteration 13/25 | Loss: 0.00039162
Iteration 14/25 | Loss: 0.00039162
Iteration 15/25 | Loss: 0.00039162
Iteration 16/25 | Loss: 0.00039162
Iteration 17/25 | Loss: 0.00039162
Iteration 18/25 | Loss: 0.00039162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00039162172470241785, 0.00039162172470241785, 0.00039162172470241785, 0.00039162172470241785, 0.00039162172470241785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039162172470241785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039162
Iteration 2/1000 | Loss: 0.00003671
Iteration 3/1000 | Loss: 0.00003004
Iteration 4/1000 | Loss: 0.00002717
Iteration 5/1000 | Loss: 0.00002612
Iteration 6/1000 | Loss: 0.00002533
Iteration 7/1000 | Loss: 0.00002473
Iteration 8/1000 | Loss: 0.00002449
Iteration 9/1000 | Loss: 0.00002449
Iteration 10/1000 | Loss: 0.00002426
Iteration 11/1000 | Loss: 0.00002415
Iteration 12/1000 | Loss: 0.00002415
Iteration 13/1000 | Loss: 0.00002414
Iteration 14/1000 | Loss: 0.00002414
Iteration 15/1000 | Loss: 0.00002414
Iteration 16/1000 | Loss: 0.00002414
Iteration 17/1000 | Loss: 0.00002413
Iteration 18/1000 | Loss: 0.00002412
Iteration 19/1000 | Loss: 0.00002409
Iteration 20/1000 | Loss: 0.00002405
Iteration 21/1000 | Loss: 0.00002405
Iteration 22/1000 | Loss: 0.00002397
Iteration 23/1000 | Loss: 0.00002396
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002395
Iteration 26/1000 | Loss: 0.00002395
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002395
Iteration 30/1000 | Loss: 0.00002395
Iteration 31/1000 | Loss: 0.00002394
Iteration 32/1000 | Loss: 0.00002394
Iteration 33/1000 | Loss: 0.00002394
Iteration 34/1000 | Loss: 0.00002393
Iteration 35/1000 | Loss: 0.00002393
Iteration 36/1000 | Loss: 0.00002393
Iteration 37/1000 | Loss: 0.00002393
Iteration 38/1000 | Loss: 0.00002392
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002392
Iteration 41/1000 | Loss: 0.00002392
Iteration 42/1000 | Loss: 0.00002391
Iteration 43/1000 | Loss: 0.00002391
Iteration 44/1000 | Loss: 0.00002391
Iteration 45/1000 | Loss: 0.00002390
Iteration 46/1000 | Loss: 0.00002390
Iteration 47/1000 | Loss: 0.00002389
Iteration 48/1000 | Loss: 0.00002389
Iteration 49/1000 | Loss: 0.00002389
Iteration 50/1000 | Loss: 0.00002389
Iteration 51/1000 | Loss: 0.00002389
Iteration 52/1000 | Loss: 0.00002389
Iteration 53/1000 | Loss: 0.00002389
Iteration 54/1000 | Loss: 0.00002389
Iteration 55/1000 | Loss: 0.00002388
Iteration 56/1000 | Loss: 0.00002388
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002388
Iteration 60/1000 | Loss: 0.00002388
Iteration 61/1000 | Loss: 0.00002388
Iteration 62/1000 | Loss: 0.00002388
Iteration 63/1000 | Loss: 0.00002388
Iteration 64/1000 | Loss: 0.00002388
Iteration 65/1000 | Loss: 0.00002388
Iteration 66/1000 | Loss: 0.00002388
Iteration 67/1000 | Loss: 0.00002388
Iteration 68/1000 | Loss: 0.00002388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.387914173596073e-05, 2.387914173596073e-05, 2.387914173596073e-05, 2.387914173596073e-05, 2.387914173596073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.387914173596073e-05

Optimization complete. Final v2v error: 4.137434959411621 mm

Highest mean error: 4.28745174407959 mm for frame 140

Lowest mean error: 3.8599636554718018 mm for frame 3

Saving results

Total time: 30.44888663291931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098321
Iteration 2/25 | Loss: 0.01098321
Iteration 3/25 | Loss: 0.01098321
Iteration 4/25 | Loss: 0.01098321
Iteration 5/25 | Loss: 0.01098321
Iteration 6/25 | Loss: 0.00223514
Iteration 7/25 | Loss: 0.00144895
Iteration 8/25 | Loss: 0.00128376
Iteration 9/25 | Loss: 0.00115098
Iteration 10/25 | Loss: 0.00118025
Iteration 11/25 | Loss: 0.00115977
Iteration 12/25 | Loss: 0.00106317
Iteration 13/25 | Loss: 0.00101630
Iteration 14/25 | Loss: 0.00097539
Iteration 15/25 | Loss: 0.00093863
Iteration 16/25 | Loss: 0.00093493
Iteration 17/25 | Loss: 0.00092327
Iteration 18/25 | Loss: 0.00091349
Iteration 19/25 | Loss: 0.00089645
Iteration 20/25 | Loss: 0.00089215
Iteration 21/25 | Loss: 0.00089213
Iteration 22/25 | Loss: 0.00088622
Iteration 23/25 | Loss: 0.00089470
Iteration 24/25 | Loss: 0.00090323
Iteration 25/25 | Loss: 0.00089080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26033998
Iteration 2/25 | Loss: 0.00207530
Iteration 3/25 | Loss: 0.00134819
Iteration 4/25 | Loss: 0.00134818
Iteration 5/25 | Loss: 0.00134818
Iteration 6/25 | Loss: 0.00134818
Iteration 7/25 | Loss: 0.00134818
Iteration 8/25 | Loss: 0.00134818
Iteration 9/25 | Loss: 0.00134818
Iteration 10/25 | Loss: 0.00134818
Iteration 11/25 | Loss: 0.00134818
Iteration 12/25 | Loss: 0.00134818
Iteration 13/25 | Loss: 0.00134818
Iteration 14/25 | Loss: 0.00134818
Iteration 15/25 | Loss: 0.00134818
Iteration 16/25 | Loss: 0.00134818
Iteration 17/25 | Loss: 0.00134818
Iteration 18/25 | Loss: 0.00134818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013481801142916083, 0.0013481801142916083, 0.0013481801142916083, 0.0013481801142916083, 0.0013481801142916083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013481801142916083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134818
Iteration 2/1000 | Loss: 0.00082356
Iteration 3/1000 | Loss: 0.00068962
Iteration 4/1000 | Loss: 0.00071839
Iteration 5/1000 | Loss: 0.00038508
Iteration 6/1000 | Loss: 0.00027708
Iteration 7/1000 | Loss: 0.00025688
Iteration 8/1000 | Loss: 0.00030284
Iteration 9/1000 | Loss: 0.00027561
Iteration 10/1000 | Loss: 0.00014456
Iteration 11/1000 | Loss: 0.00025847
Iteration 12/1000 | Loss: 0.00039853
Iteration 13/1000 | Loss: 0.00022992
Iteration 14/1000 | Loss: 0.00026611
Iteration 15/1000 | Loss: 0.00023962
Iteration 16/1000 | Loss: 0.00043443
Iteration 17/1000 | Loss: 0.00017296
Iteration 18/1000 | Loss: 0.00022166
Iteration 19/1000 | Loss: 0.00028687
Iteration 20/1000 | Loss: 0.00059508
Iteration 21/1000 | Loss: 0.00073346
Iteration 22/1000 | Loss: 0.00046658
Iteration 23/1000 | Loss: 0.00021078
Iteration 24/1000 | Loss: 0.00027622
Iteration 25/1000 | Loss: 0.00031701
Iteration 26/1000 | Loss: 0.00015303
Iteration 27/1000 | Loss: 0.00035825
Iteration 28/1000 | Loss: 0.00029695
Iteration 29/1000 | Loss: 0.00034862
Iteration 30/1000 | Loss: 0.00013274
Iteration 31/1000 | Loss: 0.00093269
Iteration 32/1000 | Loss: 0.00023968
Iteration 33/1000 | Loss: 0.00024877
Iteration 34/1000 | Loss: 0.00205760
Iteration 35/1000 | Loss: 0.00124847
Iteration 36/1000 | Loss: 0.00119767
Iteration 37/1000 | Loss: 0.00190997
Iteration 38/1000 | Loss: 0.00066625
Iteration 39/1000 | Loss: 0.00016042
Iteration 40/1000 | Loss: 0.00008020
Iteration 41/1000 | Loss: 0.00006349
Iteration 42/1000 | Loss: 0.00090529
Iteration 43/1000 | Loss: 0.00084248
Iteration 44/1000 | Loss: 0.00035226
Iteration 45/1000 | Loss: 0.00024065
Iteration 46/1000 | Loss: 0.00023019
Iteration 47/1000 | Loss: 0.00058375
Iteration 48/1000 | Loss: 0.00086028
Iteration 49/1000 | Loss: 0.00082413
Iteration 50/1000 | Loss: 0.00204874
Iteration 51/1000 | Loss: 0.00173310
Iteration 52/1000 | Loss: 0.00182614
Iteration 53/1000 | Loss: 0.00244472
Iteration 54/1000 | Loss: 0.00208425
Iteration 55/1000 | Loss: 0.00164313
Iteration 56/1000 | Loss: 0.00102084
Iteration 57/1000 | Loss: 0.00089642
Iteration 58/1000 | Loss: 0.00039077
Iteration 59/1000 | Loss: 0.00026324
Iteration 60/1000 | Loss: 0.00044841
Iteration 61/1000 | Loss: 0.00032419
Iteration 62/1000 | Loss: 0.00045636
Iteration 63/1000 | Loss: 0.00028741
Iteration 64/1000 | Loss: 0.00015774
Iteration 65/1000 | Loss: 0.00088286
Iteration 66/1000 | Loss: 0.00095494
Iteration 67/1000 | Loss: 0.00084153
Iteration 68/1000 | Loss: 0.00107676
Iteration 69/1000 | Loss: 0.00080481
Iteration 70/1000 | Loss: 0.00077220
Iteration 71/1000 | Loss: 0.00060898
Iteration 72/1000 | Loss: 0.00079431
Iteration 73/1000 | Loss: 0.00095671
Iteration 74/1000 | Loss: 0.00059908
Iteration 75/1000 | Loss: 0.00117199
Iteration 76/1000 | Loss: 0.00027209
Iteration 77/1000 | Loss: 0.00072377
Iteration 78/1000 | Loss: 0.00147815
Iteration 79/1000 | Loss: 0.00041276
Iteration 80/1000 | Loss: 0.00019838
Iteration 81/1000 | Loss: 0.00016774
Iteration 82/1000 | Loss: 0.00015119
Iteration 83/1000 | Loss: 0.00013106
Iteration 84/1000 | Loss: 0.00015962
Iteration 85/1000 | Loss: 0.00010256
Iteration 86/1000 | Loss: 0.00007170
Iteration 87/1000 | Loss: 0.00006004
Iteration 88/1000 | Loss: 0.00023516
Iteration 89/1000 | Loss: 0.00006526
Iteration 90/1000 | Loss: 0.00004957
Iteration 91/1000 | Loss: 0.00021216
Iteration 92/1000 | Loss: 0.00005341
Iteration 93/1000 | Loss: 0.00005158
Iteration 94/1000 | Loss: 0.00004382
Iteration 95/1000 | Loss: 0.00030528
Iteration 96/1000 | Loss: 0.00018194
Iteration 97/1000 | Loss: 0.00030593
Iteration 98/1000 | Loss: 0.00019587
Iteration 99/1000 | Loss: 0.00026747
Iteration 100/1000 | Loss: 0.00044575
Iteration 101/1000 | Loss: 0.00081785
Iteration 102/1000 | Loss: 0.00084687
Iteration 103/1000 | Loss: 0.00021254
Iteration 104/1000 | Loss: 0.00020357
Iteration 105/1000 | Loss: 0.00018220
Iteration 106/1000 | Loss: 0.00005402
Iteration 107/1000 | Loss: 0.00019969
Iteration 108/1000 | Loss: 0.00005386
Iteration 109/1000 | Loss: 0.00005755
Iteration 110/1000 | Loss: 0.00004844
Iteration 111/1000 | Loss: 0.00020221
Iteration 112/1000 | Loss: 0.00031973
Iteration 113/1000 | Loss: 0.00015544
Iteration 114/1000 | Loss: 0.00024509
Iteration 115/1000 | Loss: 0.00019528
Iteration 116/1000 | Loss: 0.00006204
Iteration 117/1000 | Loss: 0.00038506
Iteration 118/1000 | Loss: 0.00032734
Iteration 119/1000 | Loss: 0.00029488
Iteration 120/1000 | Loss: 0.00062251
Iteration 121/1000 | Loss: 0.00022523
Iteration 122/1000 | Loss: 0.00008430
Iteration 123/1000 | Loss: 0.00022145
Iteration 124/1000 | Loss: 0.00035066
Iteration 125/1000 | Loss: 0.00062996
Iteration 126/1000 | Loss: 0.00031188
Iteration 127/1000 | Loss: 0.00041079
Iteration 128/1000 | Loss: 0.00017894
Iteration 129/1000 | Loss: 0.00014112
Iteration 130/1000 | Loss: 0.00011621
Iteration 131/1000 | Loss: 0.00005993
Iteration 132/1000 | Loss: 0.00034445
Iteration 133/1000 | Loss: 0.00012111
Iteration 134/1000 | Loss: 0.00020379
Iteration 135/1000 | Loss: 0.00024656
Iteration 136/1000 | Loss: 0.00008674
Iteration 137/1000 | Loss: 0.00005783
Iteration 138/1000 | Loss: 0.00018968
Iteration 139/1000 | Loss: 0.00009875
Iteration 140/1000 | Loss: 0.00027432
Iteration 141/1000 | Loss: 0.00012505
Iteration 142/1000 | Loss: 0.00013197
Iteration 143/1000 | Loss: 0.00025323
Iteration 144/1000 | Loss: 0.00014079
Iteration 145/1000 | Loss: 0.00007359
Iteration 146/1000 | Loss: 0.00030389
Iteration 147/1000 | Loss: 0.00012433
Iteration 148/1000 | Loss: 0.00015294
Iteration 149/1000 | Loss: 0.00015003
Iteration 150/1000 | Loss: 0.00030359
Iteration 151/1000 | Loss: 0.00017286
Iteration 152/1000 | Loss: 0.00030228
Iteration 153/1000 | Loss: 0.00011174
Iteration 154/1000 | Loss: 0.00028800
Iteration 155/1000 | Loss: 0.00014777
Iteration 156/1000 | Loss: 0.00019643
Iteration 157/1000 | Loss: 0.00042566
Iteration 158/1000 | Loss: 0.00005652
Iteration 159/1000 | Loss: 0.00020071
Iteration 160/1000 | Loss: 0.00026730
Iteration 161/1000 | Loss: 0.00033003
Iteration 162/1000 | Loss: 0.00006232
Iteration 163/1000 | Loss: 0.00003957
Iteration 164/1000 | Loss: 0.00004599
Iteration 165/1000 | Loss: 0.00015039
Iteration 166/1000 | Loss: 0.00003716
Iteration 167/1000 | Loss: 0.00021555
Iteration 168/1000 | Loss: 0.00038255
Iteration 169/1000 | Loss: 0.00024203
Iteration 170/1000 | Loss: 0.00029645
Iteration 171/1000 | Loss: 0.00012212
Iteration 172/1000 | Loss: 0.00034138
Iteration 173/1000 | Loss: 0.00011525
Iteration 174/1000 | Loss: 0.00012989
Iteration 175/1000 | Loss: 0.00011415
Iteration 176/1000 | Loss: 0.00011453
Iteration 177/1000 | Loss: 0.00018043
Iteration 178/1000 | Loss: 0.00005023
Iteration 179/1000 | Loss: 0.00004870
Iteration 180/1000 | Loss: 0.00013733
Iteration 181/1000 | Loss: 0.00006350
Iteration 182/1000 | Loss: 0.00004419
Iteration 183/1000 | Loss: 0.00018398
Iteration 184/1000 | Loss: 0.00006907
Iteration 185/1000 | Loss: 0.00003349
Iteration 186/1000 | Loss: 0.00012170
Iteration 187/1000 | Loss: 0.00004212
Iteration 188/1000 | Loss: 0.00004459
Iteration 189/1000 | Loss: 0.00003639
Iteration 190/1000 | Loss: 0.00004011
Iteration 191/1000 | Loss: 0.00003531
Iteration 192/1000 | Loss: 0.00005012
Iteration 193/1000 | Loss: 0.00004606
Iteration 194/1000 | Loss: 0.00003495
Iteration 195/1000 | Loss: 0.00005400
Iteration 196/1000 | Loss: 0.00004574
Iteration 197/1000 | Loss: 0.00076540
Iteration 198/1000 | Loss: 0.00023398
Iteration 199/1000 | Loss: 0.00012221
Iteration 200/1000 | Loss: 0.00006669
Iteration 201/1000 | Loss: 0.00003924
Iteration 202/1000 | Loss: 0.00004200
Iteration 203/1000 | Loss: 0.00003519
Iteration 204/1000 | Loss: 0.00005344
Iteration 205/1000 | Loss: 0.00026254
Iteration 206/1000 | Loss: 0.00045699
Iteration 207/1000 | Loss: 0.00003283
Iteration 208/1000 | Loss: 0.00029082
Iteration 209/1000 | Loss: 0.00003821
Iteration 210/1000 | Loss: 0.00003387
Iteration 211/1000 | Loss: 0.00003144
Iteration 212/1000 | Loss: 0.00003019
Iteration 213/1000 | Loss: 0.00002956
Iteration 214/1000 | Loss: 0.00002897
Iteration 215/1000 | Loss: 0.00002855
Iteration 216/1000 | Loss: 0.00002822
Iteration 217/1000 | Loss: 0.00002795
Iteration 218/1000 | Loss: 0.00002776
Iteration 219/1000 | Loss: 0.00002768
Iteration 220/1000 | Loss: 0.00002763
Iteration 221/1000 | Loss: 0.00002756
Iteration 222/1000 | Loss: 0.00002747
Iteration 223/1000 | Loss: 0.00002723
Iteration 224/1000 | Loss: 0.00002700
Iteration 225/1000 | Loss: 0.00002679
Iteration 226/1000 | Loss: 0.00002661
Iteration 227/1000 | Loss: 0.00002651
Iteration 228/1000 | Loss: 0.00002643
Iteration 229/1000 | Loss: 0.00002639
Iteration 230/1000 | Loss: 0.00002638
Iteration 231/1000 | Loss: 0.00002638
Iteration 232/1000 | Loss: 0.00002637
Iteration 233/1000 | Loss: 0.00002637
Iteration 234/1000 | Loss: 0.00002634
Iteration 235/1000 | Loss: 0.00002632
Iteration 236/1000 | Loss: 0.00002629
Iteration 237/1000 | Loss: 0.00002628
Iteration 238/1000 | Loss: 0.00002628
Iteration 239/1000 | Loss: 0.00002628
Iteration 240/1000 | Loss: 0.00002628
Iteration 241/1000 | Loss: 0.00002628
Iteration 242/1000 | Loss: 0.00002628
Iteration 243/1000 | Loss: 0.00002628
Iteration 244/1000 | Loss: 0.00002627
Iteration 245/1000 | Loss: 0.00002627
Iteration 246/1000 | Loss: 0.00002627
Iteration 247/1000 | Loss: 0.00002627
Iteration 248/1000 | Loss: 0.00002626
Iteration 249/1000 | Loss: 0.00002625
Iteration 250/1000 | Loss: 0.00002625
Iteration 251/1000 | Loss: 0.00002625
Iteration 252/1000 | Loss: 0.00002624
Iteration 253/1000 | Loss: 0.00002624
Iteration 254/1000 | Loss: 0.00002622
Iteration 255/1000 | Loss: 0.00002620
Iteration 256/1000 | Loss: 0.00002620
Iteration 257/1000 | Loss: 0.00002620
Iteration 258/1000 | Loss: 0.00002619
Iteration 259/1000 | Loss: 0.00002618
Iteration 260/1000 | Loss: 0.00002618
Iteration 261/1000 | Loss: 0.00002617
Iteration 262/1000 | Loss: 0.00002617
Iteration 263/1000 | Loss: 0.00002617
Iteration 264/1000 | Loss: 0.00002616
Iteration 265/1000 | Loss: 0.00002616
Iteration 266/1000 | Loss: 0.00002616
Iteration 267/1000 | Loss: 0.00002616
Iteration 268/1000 | Loss: 0.00002615
Iteration 269/1000 | Loss: 0.00002615
Iteration 270/1000 | Loss: 0.00018293
Iteration 271/1000 | Loss: 0.00030748
Iteration 272/1000 | Loss: 0.00020068
Iteration 273/1000 | Loss: 0.00013993
Iteration 274/1000 | Loss: 0.00008433
Iteration 275/1000 | Loss: 0.00003725
Iteration 276/1000 | Loss: 0.00003316
Iteration 277/1000 | Loss: 0.00003101
Iteration 278/1000 | Loss: 0.00016573
Iteration 279/1000 | Loss: 0.00008729
Iteration 280/1000 | Loss: 0.00018822
Iteration 281/1000 | Loss: 0.00009083
Iteration 282/1000 | Loss: 0.00009695
Iteration 283/1000 | Loss: 0.00019245
Iteration 284/1000 | Loss: 0.00004149
Iteration 285/1000 | Loss: 0.00021903
Iteration 286/1000 | Loss: 0.00013366
Iteration 287/1000 | Loss: 0.00007466
Iteration 288/1000 | Loss: 0.00027695
Iteration 289/1000 | Loss: 0.00003501
Iteration 290/1000 | Loss: 0.00003103
Iteration 291/1000 | Loss: 0.00002980
Iteration 292/1000 | Loss: 0.00002942
Iteration 293/1000 | Loss: 0.00002935
Iteration 294/1000 | Loss: 0.00002934
Iteration 295/1000 | Loss: 0.00002914
Iteration 296/1000 | Loss: 0.00002869
Iteration 297/1000 | Loss: 0.00004393
Iteration 298/1000 | Loss: 0.00004190
Iteration 299/1000 | Loss: 0.00003022
Iteration 300/1000 | Loss: 0.00002951
Iteration 301/1000 | Loss: 0.00004274
Iteration 302/1000 | Loss: 0.00003136
Iteration 303/1000 | Loss: 0.00002876
Iteration 304/1000 | Loss: 0.00004284
Iteration 305/1000 | Loss: 0.00003242
Iteration 306/1000 | Loss: 0.00004203
Iteration 307/1000 | Loss: 0.00003534
Iteration 308/1000 | Loss: 0.00004172
Iteration 309/1000 | Loss: 0.00002998
Iteration 310/1000 | Loss: 0.00002829
Iteration 311/1000 | Loss: 0.00002736
Iteration 312/1000 | Loss: 0.00003213
Iteration 313/1000 | Loss: 0.00002734
Iteration 314/1000 | Loss: 0.00003109
Iteration 315/1000 | Loss: 0.00002691
Iteration 316/1000 | Loss: 0.00002664
Iteration 317/1000 | Loss: 0.00002628
Iteration 318/1000 | Loss: 0.00002616
Iteration 319/1000 | Loss: 0.00002610
Iteration 320/1000 | Loss: 0.00002596
Iteration 321/1000 | Loss: 0.00002595
Iteration 322/1000 | Loss: 0.00002593
Iteration 323/1000 | Loss: 0.00002592
Iteration 324/1000 | Loss: 0.00002590
Iteration 325/1000 | Loss: 0.00002589
Iteration 326/1000 | Loss: 0.00002588
Iteration 327/1000 | Loss: 0.00002588
Iteration 328/1000 | Loss: 0.00002588
Iteration 329/1000 | Loss: 0.00002587
Iteration 330/1000 | Loss: 0.00002587
Iteration 331/1000 | Loss: 0.00002587
Iteration 332/1000 | Loss: 0.00002587
Iteration 333/1000 | Loss: 0.00002587
Iteration 334/1000 | Loss: 0.00002586
Iteration 335/1000 | Loss: 0.00002586
Iteration 336/1000 | Loss: 0.00002586
Iteration 337/1000 | Loss: 0.00002586
Iteration 338/1000 | Loss: 0.00002586
Iteration 339/1000 | Loss: 0.00002586
Iteration 340/1000 | Loss: 0.00002585
Iteration 341/1000 | Loss: 0.00002585
Iteration 342/1000 | Loss: 0.00002585
Iteration 343/1000 | Loss: 0.00002585
Iteration 344/1000 | Loss: 0.00002585
Iteration 345/1000 | Loss: 0.00002585
Iteration 346/1000 | Loss: 0.00002585
Iteration 347/1000 | Loss: 0.00002585
Iteration 348/1000 | Loss: 0.00002585
Iteration 349/1000 | Loss: 0.00002585
Iteration 350/1000 | Loss: 0.00002584
Iteration 351/1000 | Loss: 0.00002584
Iteration 352/1000 | Loss: 0.00002584
Iteration 353/1000 | Loss: 0.00002584
Iteration 354/1000 | Loss: 0.00002584
Iteration 355/1000 | Loss: 0.00002584
Iteration 356/1000 | Loss: 0.00002583
Iteration 357/1000 | Loss: 0.00002582
Iteration 358/1000 | Loss: 0.00002581
Iteration 359/1000 | Loss: 0.00002581
Iteration 360/1000 | Loss: 0.00002581
Iteration 361/1000 | Loss: 0.00002581
Iteration 362/1000 | Loss: 0.00002581
Iteration 363/1000 | Loss: 0.00002581
Iteration 364/1000 | Loss: 0.00002581
Iteration 365/1000 | Loss: 0.00002580
Iteration 366/1000 | Loss: 0.00002580
Iteration 367/1000 | Loss: 0.00002580
Iteration 368/1000 | Loss: 0.00002580
Iteration 369/1000 | Loss: 0.00002580
Iteration 370/1000 | Loss: 0.00002580
Iteration 371/1000 | Loss: 0.00002580
Iteration 372/1000 | Loss: 0.00002580
Iteration 373/1000 | Loss: 0.00002580
Iteration 374/1000 | Loss: 0.00002579
Iteration 375/1000 | Loss: 0.00002579
Iteration 376/1000 | Loss: 0.00002578
Iteration 377/1000 | Loss: 0.00002578
Iteration 378/1000 | Loss: 0.00002578
Iteration 379/1000 | Loss: 0.00002578
Iteration 380/1000 | Loss: 0.00002577
Iteration 381/1000 | Loss: 0.00002577
Iteration 382/1000 | Loss: 0.00002577
Iteration 383/1000 | Loss: 0.00002576
Iteration 384/1000 | Loss: 0.00002576
Iteration 385/1000 | Loss: 0.00002576
Iteration 386/1000 | Loss: 0.00002576
Iteration 387/1000 | Loss: 0.00002576
Iteration 388/1000 | Loss: 0.00002576
Iteration 389/1000 | Loss: 0.00002575
Iteration 390/1000 | Loss: 0.00002575
Iteration 391/1000 | Loss: 0.00002575
Iteration 392/1000 | Loss: 0.00002575
Iteration 393/1000 | Loss: 0.00002575
Iteration 394/1000 | Loss: 0.00002574
Iteration 395/1000 | Loss: 0.00002574
Iteration 396/1000 | Loss: 0.00002574
Iteration 397/1000 | Loss: 0.00002574
Iteration 398/1000 | Loss: 0.00002574
Iteration 399/1000 | Loss: 0.00002574
Iteration 400/1000 | Loss: 0.00002574
Iteration 401/1000 | Loss: 0.00002573
Iteration 402/1000 | Loss: 0.00002573
Iteration 403/1000 | Loss: 0.00002573
Iteration 404/1000 | Loss: 0.00002573
Iteration 405/1000 | Loss: 0.00002573
Iteration 406/1000 | Loss: 0.00002573
Iteration 407/1000 | Loss: 0.00002573
Iteration 408/1000 | Loss: 0.00002573
Iteration 409/1000 | Loss: 0.00002573
Iteration 410/1000 | Loss: 0.00002573
Iteration 411/1000 | Loss: 0.00002573
Iteration 412/1000 | Loss: 0.00002573
Iteration 413/1000 | Loss: 0.00002572
Iteration 414/1000 | Loss: 0.00002572
Iteration 415/1000 | Loss: 0.00002572
Iteration 416/1000 | Loss: 0.00002572
Iteration 417/1000 | Loss: 0.00002572
Iteration 418/1000 | Loss: 0.00002572
Iteration 419/1000 | Loss: 0.00002572
Iteration 420/1000 | Loss: 0.00002572
Iteration 421/1000 | Loss: 0.00002572
Iteration 422/1000 | Loss: 0.00002572
Iteration 423/1000 | Loss: 0.00002572
Iteration 424/1000 | Loss: 0.00002572
Iteration 425/1000 | Loss: 0.00002572
Iteration 426/1000 | Loss: 0.00002572
Iteration 427/1000 | Loss: 0.00002571
Iteration 428/1000 | Loss: 0.00002571
Iteration 429/1000 | Loss: 0.00002571
Iteration 430/1000 | Loss: 0.00002571
Iteration 431/1000 | Loss: 0.00002571
Iteration 432/1000 | Loss: 0.00002571
Iteration 433/1000 | Loss: 0.00002571
Iteration 434/1000 | Loss: 0.00002571
Iteration 435/1000 | Loss: 0.00002571
Iteration 436/1000 | Loss: 0.00002571
Iteration 437/1000 | Loss: 0.00002571
Iteration 438/1000 | Loss: 0.00002571
Iteration 439/1000 | Loss: 0.00002571
Iteration 440/1000 | Loss: 0.00002571
Iteration 441/1000 | Loss: 0.00002571
Iteration 442/1000 | Loss: 0.00002571
Iteration 443/1000 | Loss: 0.00002571
Iteration 444/1000 | Loss: 0.00002571
Iteration 445/1000 | Loss: 0.00002571
Iteration 446/1000 | Loss: 0.00002570
Iteration 447/1000 | Loss: 0.00002570
Iteration 448/1000 | Loss: 0.00002570
Iteration 449/1000 | Loss: 0.00002570
Iteration 450/1000 | Loss: 0.00002570
Iteration 451/1000 | Loss: 0.00002570
Iteration 452/1000 | Loss: 0.00002570
Iteration 453/1000 | Loss: 0.00002570
Iteration 454/1000 | Loss: 0.00002570
Iteration 455/1000 | Loss: 0.00002570
Iteration 456/1000 | Loss: 0.00002570
Iteration 457/1000 | Loss: 0.00002570
Iteration 458/1000 | Loss: 0.00002570
Iteration 459/1000 | Loss: 0.00002570
Iteration 460/1000 | Loss: 0.00002569
Iteration 461/1000 | Loss: 0.00002569
Iteration 462/1000 | Loss: 0.00002569
Iteration 463/1000 | Loss: 0.00002569
Iteration 464/1000 | Loss: 0.00002569
Iteration 465/1000 | Loss: 0.00002569
Iteration 466/1000 | Loss: 0.00002569
Iteration 467/1000 | Loss: 0.00002569
Iteration 468/1000 | Loss: 0.00002568
Iteration 469/1000 | Loss: 0.00002568
Iteration 470/1000 | Loss: 0.00002568
Iteration 471/1000 | Loss: 0.00002568
Iteration 472/1000 | Loss: 0.00002568
Iteration 473/1000 | Loss: 0.00002568
Iteration 474/1000 | Loss: 0.00002567
Iteration 475/1000 | Loss: 0.00002567
Iteration 476/1000 | Loss: 0.00002567
Iteration 477/1000 | Loss: 0.00002567
Iteration 478/1000 | Loss: 0.00002567
Iteration 479/1000 | Loss: 0.00002567
Iteration 480/1000 | Loss: 0.00002567
Iteration 481/1000 | Loss: 0.00002567
Iteration 482/1000 | Loss: 0.00002567
Iteration 483/1000 | Loss: 0.00002566
Iteration 484/1000 | Loss: 0.00002566
Iteration 485/1000 | Loss: 0.00019281
Iteration 486/1000 | Loss: 0.00011496
Iteration 487/1000 | Loss: 0.00003322
Iteration 488/1000 | Loss: 0.00002824
Iteration 489/1000 | Loss: 0.00002692
Iteration 490/1000 | Loss: 0.00002623
Iteration 491/1000 | Loss: 0.00002573
Iteration 492/1000 | Loss: 0.00002549
Iteration 493/1000 | Loss: 0.00002543
Iteration 494/1000 | Loss: 0.00002537
Iteration 495/1000 | Loss: 0.00002536
Iteration 496/1000 | Loss: 0.00002532
Iteration 497/1000 | Loss: 0.00002527
Iteration 498/1000 | Loss: 0.00002523
Iteration 499/1000 | Loss: 0.00002521
Iteration 500/1000 | Loss: 0.00002521
Iteration 501/1000 | Loss: 0.00002520
Iteration 502/1000 | Loss: 0.00002519
Iteration 503/1000 | Loss: 0.00002519
Iteration 504/1000 | Loss: 0.00002518
Iteration 505/1000 | Loss: 0.00002518
Iteration 506/1000 | Loss: 0.00002516
Iteration 507/1000 | Loss: 0.00002516
Iteration 508/1000 | Loss: 0.00002515
Iteration 509/1000 | Loss: 0.00002515
Iteration 510/1000 | Loss: 0.00002515
Iteration 511/1000 | Loss: 0.00002514
Iteration 512/1000 | Loss: 0.00002514
Iteration 513/1000 | Loss: 0.00002513
Iteration 514/1000 | Loss: 0.00002511
Iteration 515/1000 | Loss: 0.00002511
Iteration 516/1000 | Loss: 0.00002511
Iteration 517/1000 | Loss: 0.00002511
Iteration 518/1000 | Loss: 0.00002510
Iteration 519/1000 | Loss: 0.00002510
Iteration 520/1000 | Loss: 0.00002509
Iteration 521/1000 | Loss: 0.00002509
Iteration 522/1000 | Loss: 0.00002509
Iteration 523/1000 | Loss: 0.00002508
Iteration 524/1000 | Loss: 0.00002508
Iteration 525/1000 | Loss: 0.00002508
Iteration 526/1000 | Loss: 0.00002508
Iteration 527/1000 | Loss: 0.00002507
Iteration 528/1000 | Loss: 0.00002507
Iteration 529/1000 | Loss: 0.00002507
Iteration 530/1000 | Loss: 0.00002506
Iteration 531/1000 | Loss: 0.00002506
Iteration 532/1000 | Loss: 0.00002506
Iteration 533/1000 | Loss: 0.00002505
Iteration 534/1000 | Loss: 0.00002505
Iteration 535/1000 | Loss: 0.00002505
Iteration 536/1000 | Loss: 0.00002505
Iteration 537/1000 | Loss: 0.00002504
Iteration 538/1000 | Loss: 0.00002504
Iteration 539/1000 | Loss: 0.00002504
Iteration 540/1000 | Loss: 0.00002504
Iteration 541/1000 | Loss: 0.00002504
Iteration 542/1000 | Loss: 0.00002504
Iteration 543/1000 | Loss: 0.00002504
Iteration 544/1000 | Loss: 0.00002503
Iteration 545/1000 | Loss: 0.00002503
Iteration 546/1000 | Loss: 0.00002503
Iteration 547/1000 | Loss: 0.00002503
Iteration 548/1000 | Loss: 0.00002503
Iteration 549/1000 | Loss: 0.00002502
Iteration 550/1000 | Loss: 0.00002502
Iteration 551/1000 | Loss: 0.00002502
Iteration 552/1000 | Loss: 0.00002502
Iteration 553/1000 | Loss: 0.00002502
Iteration 554/1000 | Loss: 0.00002502
Iteration 555/1000 | Loss: 0.00002502
Iteration 556/1000 | Loss: 0.00002502
Iteration 557/1000 | Loss: 0.00002502
Iteration 558/1000 | Loss: 0.00002502
Iteration 559/1000 | Loss: 0.00002501
Iteration 560/1000 | Loss: 0.00002501
Iteration 561/1000 | Loss: 0.00002501
Iteration 562/1000 | Loss: 0.00002501
Iteration 563/1000 | Loss: 0.00002501
Iteration 564/1000 | Loss: 0.00002501
Iteration 565/1000 | Loss: 0.00002500
Iteration 566/1000 | Loss: 0.00002500
Iteration 567/1000 | Loss: 0.00002500
Iteration 568/1000 | Loss: 0.00002500
Iteration 569/1000 | Loss: 0.00002500
Iteration 570/1000 | Loss: 0.00002500
Iteration 571/1000 | Loss: 0.00002500
Iteration 572/1000 | Loss: 0.00002500
Iteration 573/1000 | Loss: 0.00002499
Iteration 574/1000 | Loss: 0.00002499
Iteration 575/1000 | Loss: 0.00002499
Iteration 576/1000 | Loss: 0.00002499
Iteration 577/1000 | Loss: 0.00002499
Iteration 578/1000 | Loss: 0.00002499
Iteration 579/1000 | Loss: 0.00002499
Iteration 580/1000 | Loss: 0.00002499
Iteration 581/1000 | Loss: 0.00002498
Iteration 582/1000 | Loss: 0.00002498
Iteration 583/1000 | Loss: 0.00002498
Iteration 584/1000 | Loss: 0.00002498
Iteration 585/1000 | Loss: 0.00002498
Iteration 586/1000 | Loss: 0.00002498
Iteration 587/1000 | Loss: 0.00002498
Iteration 588/1000 | Loss: 0.00002498
Iteration 589/1000 | Loss: 0.00002498
Iteration 590/1000 | Loss: 0.00002498
Iteration 591/1000 | Loss: 0.00002498
Iteration 592/1000 | Loss: 0.00002498
Iteration 593/1000 | Loss: 0.00002498
Iteration 594/1000 | Loss: 0.00002498
Iteration 595/1000 | Loss: 0.00002498
Iteration 596/1000 | Loss: 0.00002497
Iteration 597/1000 | Loss: 0.00002497
Iteration 598/1000 | Loss: 0.00002497
Iteration 599/1000 | Loss: 0.00002497
Iteration 600/1000 | Loss: 0.00002497
Iteration 601/1000 | Loss: 0.00002497
Iteration 602/1000 | Loss: 0.00002497
Iteration 603/1000 | Loss: 0.00002497
Iteration 604/1000 | Loss: 0.00002497
Iteration 605/1000 | Loss: 0.00002497
Iteration 606/1000 | Loss: 0.00002497
Iteration 607/1000 | Loss: 0.00002497
Iteration 608/1000 | Loss: 0.00002497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 608. Stopping optimization.
Last 5 losses: [2.4971992388600484e-05, 2.4971992388600484e-05, 2.4971992388600484e-05, 2.4971992388600484e-05, 2.4971992388600484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4971992388600484e-05

Optimization complete. Final v2v error: 4.191240310668945 mm

Highest mean error: 4.820223808288574 mm for frame 160

Lowest mean error: 3.6374754905700684 mm for frame 20

Saving results

Total time: 529.1324763298035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504741
Iteration 2/25 | Loss: 0.00144059
Iteration 3/25 | Loss: 0.00093044
Iteration 4/25 | Loss: 0.00080926
Iteration 5/25 | Loss: 0.00078131
Iteration 6/25 | Loss: 0.00077745
Iteration 7/25 | Loss: 0.00077719
Iteration 8/25 | Loss: 0.00077719
Iteration 9/25 | Loss: 0.00077719
Iteration 10/25 | Loss: 0.00077719
Iteration 11/25 | Loss: 0.00077719
Iteration 12/25 | Loss: 0.00077719
Iteration 13/25 | Loss: 0.00077719
Iteration 14/25 | Loss: 0.00077719
Iteration 15/25 | Loss: 0.00077719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007771914824843407, 0.0007771914824843407, 0.0007771914824843407, 0.0007771914824843407, 0.0007771914824843407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007771914824843407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36227477
Iteration 2/25 | Loss: 0.00042331
Iteration 3/25 | Loss: 0.00042331
Iteration 4/25 | Loss: 0.00042330
Iteration 5/25 | Loss: 0.00042330
Iteration 6/25 | Loss: 0.00042330
Iteration 7/25 | Loss: 0.00042330
Iteration 8/25 | Loss: 0.00042330
Iteration 9/25 | Loss: 0.00042330
Iteration 10/25 | Loss: 0.00042330
Iteration 11/25 | Loss: 0.00042330
Iteration 12/25 | Loss: 0.00042330
Iteration 13/25 | Loss: 0.00042330
Iteration 14/25 | Loss: 0.00042330
Iteration 15/25 | Loss: 0.00042330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004233033105265349, 0.0004233033105265349, 0.0004233033105265349, 0.0004233033105265349, 0.0004233033105265349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004233033105265349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042330
Iteration 2/1000 | Loss: 0.00003544
Iteration 3/1000 | Loss: 0.00002724
Iteration 4/1000 | Loss: 0.00002511
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002307
Iteration 7/1000 | Loss: 0.00002243
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002166
Iteration 10/1000 | Loss: 0.00002154
Iteration 11/1000 | Loss: 0.00002145
Iteration 12/1000 | Loss: 0.00002144
Iteration 13/1000 | Loss: 0.00002128
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002122
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002111
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00002111
Iteration 23/1000 | Loss: 0.00002111
Iteration 24/1000 | Loss: 0.00002111
Iteration 25/1000 | Loss: 0.00002110
Iteration 26/1000 | Loss: 0.00002110
Iteration 27/1000 | Loss: 0.00002110
Iteration 28/1000 | Loss: 0.00002110
Iteration 29/1000 | Loss: 0.00002110
Iteration 30/1000 | Loss: 0.00002109
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002107
Iteration 33/1000 | Loss: 0.00002107
Iteration 34/1000 | Loss: 0.00002106
Iteration 35/1000 | Loss: 0.00002106
Iteration 36/1000 | Loss: 0.00002106
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002105
Iteration 39/1000 | Loss: 0.00002105
Iteration 40/1000 | Loss: 0.00002103
Iteration 41/1000 | Loss: 0.00002103
Iteration 42/1000 | Loss: 0.00002102
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002102
Iteration 45/1000 | Loss: 0.00002100
Iteration 46/1000 | Loss: 0.00002100
Iteration 47/1000 | Loss: 0.00002100
Iteration 48/1000 | Loss: 0.00002100
Iteration 49/1000 | Loss: 0.00002100
Iteration 50/1000 | Loss: 0.00002100
Iteration 51/1000 | Loss: 0.00002100
Iteration 52/1000 | Loss: 0.00002100
Iteration 53/1000 | Loss: 0.00002100
Iteration 54/1000 | Loss: 0.00002100
Iteration 55/1000 | Loss: 0.00002100
Iteration 56/1000 | Loss: 0.00002099
Iteration 57/1000 | Loss: 0.00002099
Iteration 58/1000 | Loss: 0.00002099
Iteration 59/1000 | Loss: 0.00002099
Iteration 60/1000 | Loss: 0.00002099
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002099
Iteration 63/1000 | Loss: 0.00002099
Iteration 64/1000 | Loss: 0.00002099
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002095
Iteration 67/1000 | Loss: 0.00002095
Iteration 68/1000 | Loss: 0.00002095
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00002095
Iteration 71/1000 | Loss: 0.00002095
Iteration 72/1000 | Loss: 0.00002095
Iteration 73/1000 | Loss: 0.00002095
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002094
Iteration 80/1000 | Loss: 0.00002094
Iteration 81/1000 | Loss: 0.00002094
Iteration 82/1000 | Loss: 0.00002094
Iteration 83/1000 | Loss: 0.00002094
Iteration 84/1000 | Loss: 0.00002093
Iteration 85/1000 | Loss: 0.00002092
Iteration 86/1000 | Loss: 0.00002092
Iteration 87/1000 | Loss: 0.00002092
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002090
Iteration 92/1000 | Loss: 0.00002090
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002089
Iteration 95/1000 | Loss: 0.00002089
Iteration 96/1000 | Loss: 0.00002089
Iteration 97/1000 | Loss: 0.00002089
Iteration 98/1000 | Loss: 0.00002088
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002087
Iteration 101/1000 | Loss: 0.00002087
Iteration 102/1000 | Loss: 0.00002087
Iteration 103/1000 | Loss: 0.00002086
Iteration 104/1000 | Loss: 0.00002086
Iteration 105/1000 | Loss: 0.00002086
Iteration 106/1000 | Loss: 0.00002085
Iteration 107/1000 | Loss: 0.00002085
Iteration 108/1000 | Loss: 0.00002085
Iteration 109/1000 | Loss: 0.00002085
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002083
Iteration 115/1000 | Loss: 0.00002083
Iteration 116/1000 | Loss: 0.00002083
Iteration 117/1000 | Loss: 0.00002083
Iteration 118/1000 | Loss: 0.00002082
Iteration 119/1000 | Loss: 0.00002082
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002082
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002082
Iteration 124/1000 | Loss: 0.00002081
Iteration 125/1000 | Loss: 0.00002081
Iteration 126/1000 | Loss: 0.00002081
Iteration 127/1000 | Loss: 0.00002080
Iteration 128/1000 | Loss: 0.00002080
Iteration 129/1000 | Loss: 0.00002080
Iteration 130/1000 | Loss: 0.00002080
Iteration 131/1000 | Loss: 0.00002080
Iteration 132/1000 | Loss: 0.00002080
Iteration 133/1000 | Loss: 0.00002080
Iteration 134/1000 | Loss: 0.00002079
Iteration 135/1000 | Loss: 0.00002079
Iteration 136/1000 | Loss: 0.00002079
Iteration 137/1000 | Loss: 0.00002079
Iteration 138/1000 | Loss: 0.00002079
Iteration 139/1000 | Loss: 0.00002079
Iteration 140/1000 | Loss: 0.00002078
Iteration 141/1000 | Loss: 0.00002078
Iteration 142/1000 | Loss: 0.00002078
Iteration 143/1000 | Loss: 0.00002078
Iteration 144/1000 | Loss: 0.00002078
Iteration 145/1000 | Loss: 0.00002078
Iteration 146/1000 | Loss: 0.00002078
Iteration 147/1000 | Loss: 0.00002078
Iteration 148/1000 | Loss: 0.00002078
Iteration 149/1000 | Loss: 0.00002078
Iteration 150/1000 | Loss: 0.00002078
Iteration 151/1000 | Loss: 0.00002077
Iteration 152/1000 | Loss: 0.00002077
Iteration 153/1000 | Loss: 0.00002077
Iteration 154/1000 | Loss: 0.00002077
Iteration 155/1000 | Loss: 0.00002077
Iteration 156/1000 | Loss: 0.00002077
Iteration 157/1000 | Loss: 0.00002077
Iteration 158/1000 | Loss: 0.00002077
Iteration 159/1000 | Loss: 0.00002077
Iteration 160/1000 | Loss: 0.00002076
Iteration 161/1000 | Loss: 0.00002076
Iteration 162/1000 | Loss: 0.00002076
Iteration 163/1000 | Loss: 0.00002076
Iteration 164/1000 | Loss: 0.00002075
Iteration 165/1000 | Loss: 0.00002075
Iteration 166/1000 | Loss: 0.00002075
Iteration 167/1000 | Loss: 0.00002075
Iteration 168/1000 | Loss: 0.00002075
Iteration 169/1000 | Loss: 0.00002075
Iteration 170/1000 | Loss: 0.00002075
Iteration 171/1000 | Loss: 0.00002075
Iteration 172/1000 | Loss: 0.00002075
Iteration 173/1000 | Loss: 0.00002075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.0751629563164897e-05, 2.0751629563164897e-05, 2.0751629563164897e-05, 2.0751629563164897e-05, 2.0751629563164897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0751629563164897e-05

Optimization complete. Final v2v error: 3.854552984237671 mm

Highest mean error: 4.326053142547607 mm for frame 45

Lowest mean error: 3.4806196689605713 mm for frame 157

Saving results

Total time: 41.89031791687012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889155
Iteration 2/25 | Loss: 0.00091206
Iteration 3/25 | Loss: 0.00075882
Iteration 4/25 | Loss: 0.00073463
Iteration 5/25 | Loss: 0.00072654
Iteration 6/25 | Loss: 0.00072496
Iteration 7/25 | Loss: 0.00072484
Iteration 8/25 | Loss: 0.00072484
Iteration 9/25 | Loss: 0.00072484
Iteration 10/25 | Loss: 0.00072484
Iteration 11/25 | Loss: 0.00072484
Iteration 12/25 | Loss: 0.00072484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007248445763252676, 0.0007248445763252676, 0.0007248445763252676, 0.0007248445763252676, 0.0007248445763252676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007248445763252676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28147340
Iteration 2/25 | Loss: 0.00040307
Iteration 3/25 | Loss: 0.00040305
Iteration 4/25 | Loss: 0.00040305
Iteration 5/25 | Loss: 0.00040305
Iteration 6/25 | Loss: 0.00040305
Iteration 7/25 | Loss: 0.00040305
Iteration 8/25 | Loss: 0.00040305
Iteration 9/25 | Loss: 0.00040305
Iteration 10/25 | Loss: 0.00040305
Iteration 11/25 | Loss: 0.00040305
Iteration 12/25 | Loss: 0.00040305
Iteration 13/25 | Loss: 0.00040305
Iteration 14/25 | Loss: 0.00040305
Iteration 15/25 | Loss: 0.00040305
Iteration 16/25 | Loss: 0.00040305
Iteration 17/25 | Loss: 0.00040305
Iteration 18/25 | Loss: 0.00040305
Iteration 19/25 | Loss: 0.00040305
Iteration 20/25 | Loss: 0.00040305
Iteration 21/25 | Loss: 0.00040305
Iteration 22/25 | Loss: 0.00040305
Iteration 23/25 | Loss: 0.00040305
Iteration 24/25 | Loss: 0.00040305
Iteration 25/25 | Loss: 0.00040305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040305
Iteration 2/1000 | Loss: 0.00003791
Iteration 3/1000 | Loss: 0.00002858
Iteration 4/1000 | Loss: 0.00002465
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002092
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00002007
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001975
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00001961
Iteration 18/1000 | Loss: 0.00001961
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00001957
Iteration 24/1000 | Loss: 0.00001957
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001954
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001953
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001953
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001953
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001952
Iteration 45/1000 | Loss: 0.00001952
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001951
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001949
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001948
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001948
Iteration 69/1000 | Loss: 0.00001948
Iteration 70/1000 | Loss: 0.00001948
Iteration 71/1000 | Loss: 0.00001948
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001948
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001948
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.9477098248898983e-05, 1.9477098248898983e-05, 1.9477098248898983e-05, 1.9477098248898983e-05, 1.9477098248898983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9477098248898983e-05

Optimization complete. Final v2v error: 3.823197603225708 mm

Highest mean error: 4.209125518798828 mm for frame 121

Lowest mean error: 3.5073463916778564 mm for frame 91

Saving results

Total time: 31.13592219352722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076812
Iteration 2/25 | Loss: 0.00233394
Iteration 3/25 | Loss: 0.00148257
Iteration 4/25 | Loss: 0.00130082
Iteration 5/25 | Loss: 0.00126424
Iteration 6/25 | Loss: 0.00124765
Iteration 7/25 | Loss: 0.00123509
Iteration 8/25 | Loss: 0.00122497
Iteration 9/25 | Loss: 0.00121924
Iteration 10/25 | Loss: 0.00123218
Iteration 11/25 | Loss: 0.00121360
Iteration 12/25 | Loss: 0.00120888
Iteration 13/25 | Loss: 0.00121200
Iteration 14/25 | Loss: 0.00120766
Iteration 15/25 | Loss: 0.00120570
Iteration 16/25 | Loss: 0.00120554
Iteration 17/25 | Loss: 0.00120554
Iteration 18/25 | Loss: 0.00120554
Iteration 19/25 | Loss: 0.00120554
Iteration 20/25 | Loss: 0.00120553
Iteration 21/25 | Loss: 0.00120553
Iteration 22/25 | Loss: 0.00120553
Iteration 23/25 | Loss: 0.00120553
Iteration 24/25 | Loss: 0.00120553
Iteration 25/25 | Loss: 0.00120553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24136484
Iteration 2/25 | Loss: 0.00431844
Iteration 3/25 | Loss: 0.00431844
Iteration 4/25 | Loss: 0.00431844
Iteration 5/25 | Loss: 0.00431844
Iteration 6/25 | Loss: 0.00431844
Iteration 7/25 | Loss: 0.00431844
Iteration 8/25 | Loss: 0.00431844
Iteration 9/25 | Loss: 0.00431844
Iteration 10/25 | Loss: 0.00431844
Iteration 11/25 | Loss: 0.00431844
Iteration 12/25 | Loss: 0.00431844
Iteration 13/25 | Loss: 0.00431844
Iteration 14/25 | Loss: 0.00431844
Iteration 15/25 | Loss: 0.00431844
Iteration 16/25 | Loss: 0.00431844
Iteration 17/25 | Loss: 0.00431844
Iteration 18/25 | Loss: 0.00431844
Iteration 19/25 | Loss: 0.00431844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004318438470363617, 0.004318438470363617, 0.004318438470363617, 0.004318438470363617, 0.004318438470363617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004318438470363617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00431844
Iteration 2/1000 | Loss: 0.00076270
Iteration 3/1000 | Loss: 0.00054953
Iteration 4/1000 | Loss: 0.00042879
Iteration 5/1000 | Loss: 0.00046153
Iteration 6/1000 | Loss: 0.00039415
Iteration 7/1000 | Loss: 0.00283205
Iteration 8/1000 | Loss: 0.00038647
Iteration 9/1000 | Loss: 0.00035325
Iteration 10/1000 | Loss: 0.00026870
Iteration 11/1000 | Loss: 0.00025895
Iteration 12/1000 | Loss: 0.00721583
Iteration 13/1000 | Loss: 0.01087764
Iteration 14/1000 | Loss: 0.00050757
Iteration 15/1000 | Loss: 0.00029853
Iteration 16/1000 | Loss: 0.00021733
Iteration 17/1000 | Loss: 0.00013871
Iteration 18/1000 | Loss: 0.00009852
Iteration 19/1000 | Loss: 0.00006844
Iteration 20/1000 | Loss: 0.00005136
Iteration 21/1000 | Loss: 0.00004980
Iteration 22/1000 | Loss: 0.00003596
Iteration 23/1000 | Loss: 0.00003103
Iteration 24/1000 | Loss: 0.00002752
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002911
Iteration 27/1000 | Loss: 0.00002651
Iteration 28/1000 | Loss: 0.00002675
Iteration 29/1000 | Loss: 0.00002182
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002057
Iteration 32/1000 | Loss: 0.00002013
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001956
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001951
Iteration 39/1000 | Loss: 0.00001950
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001944
Iteration 45/1000 | Loss: 0.00001943
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001942
Iteration 49/1000 | Loss: 0.00001941
Iteration 50/1000 | Loss: 0.00001941
Iteration 51/1000 | Loss: 0.00001941
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001941
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001940
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001939
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001937
Iteration 96/1000 | Loss: 0.00001937
Iteration 97/1000 | Loss: 0.00001937
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.9366269043530338e-05, 1.9366269043530338e-05, 1.9366269043530338e-05, 1.9366269043530338e-05, 1.9366269043530338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9366269043530338e-05

Optimization complete. Final v2v error: 3.8050670623779297 mm

Highest mean error: 10.2900972366333 mm for frame 193

Lowest mean error: 3.4200360774993896 mm for frame 203

Saving results

Total time: 92.3382568359375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053332
Iteration 2/25 | Loss: 0.00186387
Iteration 3/25 | Loss: 0.00106532
Iteration 4/25 | Loss: 0.00089117
Iteration 5/25 | Loss: 0.00084617
Iteration 6/25 | Loss: 0.00080448
Iteration 7/25 | Loss: 0.00076880
Iteration 8/25 | Loss: 0.00075007
Iteration 9/25 | Loss: 0.00074022
Iteration 10/25 | Loss: 0.00074198
Iteration 11/25 | Loss: 0.00074303
Iteration 12/25 | Loss: 0.00073703
Iteration 13/25 | Loss: 0.00073376
Iteration 14/25 | Loss: 0.00073301
Iteration 15/25 | Loss: 0.00073246
Iteration 16/25 | Loss: 0.00073596
Iteration 17/25 | Loss: 0.00073588
Iteration 18/25 | Loss: 0.00073072
Iteration 19/25 | Loss: 0.00073036
Iteration 20/25 | Loss: 0.00072762
Iteration 21/25 | Loss: 0.00072707
Iteration 22/25 | Loss: 0.00072681
Iteration 23/25 | Loss: 0.00072675
Iteration 24/25 | Loss: 0.00072675
Iteration 25/25 | Loss: 0.00072675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30356860
Iteration 2/25 | Loss: 0.00039381
Iteration 3/25 | Loss: 0.00038342
Iteration 4/25 | Loss: 0.00038342
Iteration 5/25 | Loss: 0.00038342
Iteration 6/25 | Loss: 0.00038342
Iteration 7/25 | Loss: 0.00038342
Iteration 8/25 | Loss: 0.00038342
Iteration 9/25 | Loss: 0.00038342
Iteration 10/25 | Loss: 0.00038342
Iteration 11/25 | Loss: 0.00038342
Iteration 12/25 | Loss: 0.00038342
Iteration 13/25 | Loss: 0.00038342
Iteration 14/25 | Loss: 0.00038342
Iteration 15/25 | Loss: 0.00038342
Iteration 16/25 | Loss: 0.00038342
Iteration 17/25 | Loss: 0.00038342
Iteration 18/25 | Loss: 0.00038342
Iteration 19/25 | Loss: 0.00038342
Iteration 20/25 | Loss: 0.00038342
Iteration 21/25 | Loss: 0.00038342
Iteration 22/25 | Loss: 0.00038342
Iteration 23/25 | Loss: 0.00038342
Iteration 24/25 | Loss: 0.00038342
Iteration 25/25 | Loss: 0.00038342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038342
Iteration 2/1000 | Loss: 0.00022871
Iteration 3/1000 | Loss: 0.00006194
Iteration 4/1000 | Loss: 0.00005082
Iteration 5/1000 | Loss: 0.00017644
Iteration 6/1000 | Loss: 0.00005615
Iteration 7/1000 | Loss: 0.00004631
Iteration 8/1000 | Loss: 0.00004199
Iteration 9/1000 | Loss: 0.00004317
Iteration 10/1000 | Loss: 0.00004287
Iteration 11/1000 | Loss: 0.00003708
Iteration 12/1000 | Loss: 0.00024692
Iteration 13/1000 | Loss: 0.00028825
Iteration 14/1000 | Loss: 0.00023098
Iteration 15/1000 | Loss: 0.00014253
Iteration 16/1000 | Loss: 0.00012360
Iteration 17/1000 | Loss: 0.00010464
Iteration 18/1000 | Loss: 0.00021602
Iteration 19/1000 | Loss: 0.00004822
Iteration 20/1000 | Loss: 0.00003416
Iteration 21/1000 | Loss: 0.00003214
Iteration 22/1000 | Loss: 0.00011116
Iteration 23/1000 | Loss: 0.00018146
Iteration 24/1000 | Loss: 0.00010058
Iteration 25/1000 | Loss: 0.00005856
Iteration 26/1000 | Loss: 0.00003416
Iteration 27/1000 | Loss: 0.00003200
Iteration 28/1000 | Loss: 0.00003150
Iteration 29/1000 | Loss: 0.00011367
Iteration 30/1000 | Loss: 0.00006425
Iteration 31/1000 | Loss: 0.00003876
Iteration 32/1000 | Loss: 0.00010701
Iteration 33/1000 | Loss: 0.00043827
Iteration 34/1000 | Loss: 0.00007391
Iteration 35/1000 | Loss: 0.00004698
Iteration 36/1000 | Loss: 0.00004275
Iteration 37/1000 | Loss: 0.00002934
Iteration 38/1000 | Loss: 0.00002847
Iteration 39/1000 | Loss: 0.00002938
Iteration 40/1000 | Loss: 0.00003349
Iteration 41/1000 | Loss: 0.00002874
Iteration 42/1000 | Loss: 0.00004131
Iteration 43/1000 | Loss: 0.00003292
Iteration 44/1000 | Loss: 0.00002934
Iteration 45/1000 | Loss: 0.00002768
Iteration 46/1000 | Loss: 0.00002760
Iteration 47/1000 | Loss: 0.00002760
Iteration 48/1000 | Loss: 0.00002759
Iteration 49/1000 | Loss: 0.00002759
Iteration 50/1000 | Loss: 0.00002758
Iteration 51/1000 | Loss: 0.00002757
Iteration 52/1000 | Loss: 0.00002756
Iteration 53/1000 | Loss: 0.00002753
Iteration 54/1000 | Loss: 0.00002753
Iteration 55/1000 | Loss: 0.00002753
Iteration 56/1000 | Loss: 0.00002753
Iteration 57/1000 | Loss: 0.00002753
Iteration 58/1000 | Loss: 0.00002753
Iteration 59/1000 | Loss: 0.00002753
Iteration 60/1000 | Loss: 0.00002753
Iteration 61/1000 | Loss: 0.00002753
Iteration 62/1000 | Loss: 0.00002752
Iteration 63/1000 | Loss: 0.00002752
Iteration 64/1000 | Loss: 0.00002749
Iteration 65/1000 | Loss: 0.00002748
Iteration 66/1000 | Loss: 0.00002747
Iteration 67/1000 | Loss: 0.00003866
Iteration 68/1000 | Loss: 0.00002745
Iteration 69/1000 | Loss: 0.00002743
Iteration 70/1000 | Loss: 0.00002823
Iteration 71/1000 | Loss: 0.00002820
Iteration 72/1000 | Loss: 0.00002767
Iteration 73/1000 | Loss: 0.00002737
Iteration 74/1000 | Loss: 0.00002737
Iteration 75/1000 | Loss: 0.00002737
Iteration 76/1000 | Loss: 0.00002737
Iteration 77/1000 | Loss: 0.00002737
Iteration 78/1000 | Loss: 0.00002737
Iteration 79/1000 | Loss: 0.00002737
Iteration 80/1000 | Loss: 0.00002736
Iteration 81/1000 | Loss: 0.00002736
Iteration 82/1000 | Loss: 0.00002736
Iteration 83/1000 | Loss: 0.00002736
Iteration 84/1000 | Loss: 0.00002736
Iteration 85/1000 | Loss: 0.00002736
Iteration 86/1000 | Loss: 0.00002736
Iteration 87/1000 | Loss: 0.00002736
Iteration 88/1000 | Loss: 0.00002736
Iteration 89/1000 | Loss: 0.00002736
Iteration 90/1000 | Loss: 0.00002735
Iteration 91/1000 | Loss: 0.00002735
Iteration 92/1000 | Loss: 0.00002735
Iteration 93/1000 | Loss: 0.00002734
Iteration 94/1000 | Loss: 0.00002734
Iteration 95/1000 | Loss: 0.00002734
Iteration 96/1000 | Loss: 0.00002733
Iteration 97/1000 | Loss: 0.00002733
Iteration 98/1000 | Loss: 0.00002733
Iteration 99/1000 | Loss: 0.00002732
Iteration 100/1000 | Loss: 0.00002732
Iteration 101/1000 | Loss: 0.00002732
Iteration 102/1000 | Loss: 0.00002731
Iteration 103/1000 | Loss: 0.00002731
Iteration 104/1000 | Loss: 0.00002731
Iteration 105/1000 | Loss: 0.00002730
Iteration 106/1000 | Loss: 0.00002730
Iteration 107/1000 | Loss: 0.00002730
Iteration 108/1000 | Loss: 0.00002730
Iteration 109/1000 | Loss: 0.00002730
Iteration 110/1000 | Loss: 0.00002729
Iteration 111/1000 | Loss: 0.00002729
Iteration 112/1000 | Loss: 0.00002729
Iteration 113/1000 | Loss: 0.00003625
Iteration 114/1000 | Loss: 0.00002849
Iteration 115/1000 | Loss: 0.00002757
Iteration 116/1000 | Loss: 0.00002803
Iteration 117/1000 | Loss: 0.00002804
Iteration 118/1000 | Loss: 0.00002751
Iteration 119/1000 | Loss: 0.00002768
Iteration 120/1000 | Loss: 0.00002744
Iteration 121/1000 | Loss: 0.00002731
Iteration 122/1000 | Loss: 0.00002731
Iteration 123/1000 | Loss: 0.00002814
Iteration 124/1000 | Loss: 0.00002736
Iteration 125/1000 | Loss: 0.00002746
Iteration 126/1000 | Loss: 0.00002732
Iteration 127/1000 | Loss: 0.00002725
Iteration 128/1000 | Loss: 0.00002725
Iteration 129/1000 | Loss: 0.00002725
Iteration 130/1000 | Loss: 0.00002724
Iteration 131/1000 | Loss: 0.00002724
Iteration 132/1000 | Loss: 0.00002724
Iteration 133/1000 | Loss: 0.00002724
Iteration 134/1000 | Loss: 0.00002741
Iteration 135/1000 | Loss: 0.00002730
Iteration 136/1000 | Loss: 0.00002730
Iteration 137/1000 | Loss: 0.00002729
Iteration 138/1000 | Loss: 0.00002725
Iteration 139/1000 | Loss: 0.00002725
Iteration 140/1000 | Loss: 0.00002724
Iteration 141/1000 | Loss: 0.00002738
Iteration 142/1000 | Loss: 0.00002730
Iteration 143/1000 | Loss: 0.00002729
Iteration 144/1000 | Loss: 0.00002729
Iteration 145/1000 | Loss: 0.00002735
Iteration 146/1000 | Loss: 0.00002735
Iteration 147/1000 | Loss: 0.00002730
Iteration 148/1000 | Loss: 0.00002733
Iteration 149/1000 | Loss: 0.00002732
Iteration 150/1000 | Loss: 0.00002731
Iteration 151/1000 | Loss: 0.00002729
Iteration 152/1000 | Loss: 0.00002728
Iteration 153/1000 | Loss: 0.00002728
Iteration 154/1000 | Loss: 0.00002727
Iteration 155/1000 | Loss: 0.00002727
Iteration 156/1000 | Loss: 0.00002727
Iteration 157/1000 | Loss: 0.00002727
Iteration 158/1000 | Loss: 0.00002726
Iteration 159/1000 | Loss: 0.00002726
Iteration 160/1000 | Loss: 0.00002726
Iteration 161/1000 | Loss: 0.00002726
Iteration 162/1000 | Loss: 0.00002725
Iteration 163/1000 | Loss: 0.00002725
Iteration 164/1000 | Loss: 0.00002725
Iteration 165/1000 | Loss: 0.00002725
Iteration 166/1000 | Loss: 0.00002724
Iteration 167/1000 | Loss: 0.00002724
Iteration 168/1000 | Loss: 0.00002724
Iteration 169/1000 | Loss: 0.00002724
Iteration 170/1000 | Loss: 0.00002724
Iteration 171/1000 | Loss: 0.00002724
Iteration 172/1000 | Loss: 0.00002724
Iteration 173/1000 | Loss: 0.00002724
Iteration 174/1000 | Loss: 0.00002724
Iteration 175/1000 | Loss: 0.00002723
Iteration 176/1000 | Loss: 0.00002723
Iteration 177/1000 | Loss: 0.00002723
Iteration 178/1000 | Loss: 0.00002723
Iteration 179/1000 | Loss: 0.00002723
Iteration 180/1000 | Loss: 0.00002723
Iteration 181/1000 | Loss: 0.00002723
Iteration 182/1000 | Loss: 0.00002723
Iteration 183/1000 | Loss: 0.00002723
Iteration 184/1000 | Loss: 0.00002790
Iteration 185/1000 | Loss: 0.00002724
Iteration 186/1000 | Loss: 0.00002724
Iteration 187/1000 | Loss: 0.00002724
Iteration 188/1000 | Loss: 0.00002724
Iteration 189/1000 | Loss: 0.00002724
Iteration 190/1000 | Loss: 0.00002723
Iteration 191/1000 | Loss: 0.00002723
Iteration 192/1000 | Loss: 0.00002723
Iteration 193/1000 | Loss: 0.00002733
Iteration 194/1000 | Loss: 0.00002733
Iteration 195/1000 | Loss: 0.00002721
Iteration 196/1000 | Loss: 0.00002721
Iteration 197/1000 | Loss: 0.00002721
Iteration 198/1000 | Loss: 0.00002721
Iteration 199/1000 | Loss: 0.00002721
Iteration 200/1000 | Loss: 0.00002721
Iteration 201/1000 | Loss: 0.00002721
Iteration 202/1000 | Loss: 0.00002721
Iteration 203/1000 | Loss: 0.00002721
Iteration 204/1000 | Loss: 0.00002721
Iteration 205/1000 | Loss: 0.00002720
Iteration 206/1000 | Loss: 0.00002720
Iteration 207/1000 | Loss: 0.00002785
Iteration 208/1000 | Loss: 0.00002730
Iteration 209/1000 | Loss: 0.00002767
Iteration 210/1000 | Loss: 0.00002731
Iteration 211/1000 | Loss: 0.00002731
Iteration 212/1000 | Loss: 0.00002731
Iteration 213/1000 | Loss: 0.00002730
Iteration 214/1000 | Loss: 0.00002730
Iteration 215/1000 | Loss: 0.00002730
Iteration 216/1000 | Loss: 0.00002730
Iteration 217/1000 | Loss: 0.00002729
Iteration 218/1000 | Loss: 0.00002729
Iteration 219/1000 | Loss: 0.00002728
Iteration 220/1000 | Loss: 0.00002728
Iteration 221/1000 | Loss: 0.00002727
Iteration 222/1000 | Loss: 0.00002726
Iteration 223/1000 | Loss: 0.00002724
Iteration 224/1000 | Loss: 0.00002723
Iteration 225/1000 | Loss: 0.00002723
Iteration 226/1000 | Loss: 0.00002722
Iteration 227/1000 | Loss: 0.00002722
Iteration 228/1000 | Loss: 0.00002722
Iteration 229/1000 | Loss: 0.00002722
Iteration 230/1000 | Loss: 0.00002722
Iteration 231/1000 | Loss: 0.00002722
Iteration 232/1000 | Loss: 0.00002722
Iteration 233/1000 | Loss: 0.00002722
Iteration 234/1000 | Loss: 0.00002722
Iteration 235/1000 | Loss: 0.00002722
Iteration 236/1000 | Loss: 0.00002722
Iteration 237/1000 | Loss: 0.00002721
Iteration 238/1000 | Loss: 0.00002721
Iteration 239/1000 | Loss: 0.00002721
Iteration 240/1000 | Loss: 0.00002721
Iteration 241/1000 | Loss: 0.00002721
Iteration 242/1000 | Loss: 0.00002720
Iteration 243/1000 | Loss: 0.00002720
Iteration 244/1000 | Loss: 0.00002721
Iteration 245/1000 | Loss: 0.00002721
Iteration 246/1000 | Loss: 0.00002720
Iteration 247/1000 | Loss: 0.00002720
Iteration 248/1000 | Loss: 0.00002719
Iteration 249/1000 | Loss: 0.00002719
Iteration 250/1000 | Loss: 0.00002719
Iteration 251/1000 | Loss: 0.00002719
Iteration 252/1000 | Loss: 0.00002718
Iteration 253/1000 | Loss: 0.00002718
Iteration 254/1000 | Loss: 0.00002718
Iteration 255/1000 | Loss: 0.00002718
Iteration 256/1000 | Loss: 0.00002718
Iteration 257/1000 | Loss: 0.00002718
Iteration 258/1000 | Loss: 0.00002718
Iteration 259/1000 | Loss: 0.00002718
Iteration 260/1000 | Loss: 0.00002718
Iteration 261/1000 | Loss: 0.00002718
Iteration 262/1000 | Loss: 0.00002718
Iteration 263/1000 | Loss: 0.00002718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.7182129997527227e-05, 2.7182129997527227e-05, 2.7182129997527227e-05, 2.7182129997527227e-05, 2.7182129997527227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7182129997527227e-05

Optimization complete. Final v2v error: 4.084497451782227 mm

Highest mean error: 21.615205764770508 mm for frame 9

Lowest mean error: 3.3830695152282715 mm for frame 236

Saving results

Total time: 161.2185401916504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055908
Iteration 2/25 | Loss: 0.00263841
Iteration 3/25 | Loss: 0.00210194
Iteration 4/25 | Loss: 0.00184221
Iteration 5/25 | Loss: 0.00179850
Iteration 6/25 | Loss: 0.00143048
Iteration 7/25 | Loss: 0.00115724
Iteration 8/25 | Loss: 0.00099831
Iteration 9/25 | Loss: 0.00093779
Iteration 10/25 | Loss: 0.00092199
Iteration 11/25 | Loss: 0.00090628
Iteration 12/25 | Loss: 0.00089158
Iteration 13/25 | Loss: 0.00087390
Iteration 14/25 | Loss: 0.00085026
Iteration 15/25 | Loss: 0.00083465
Iteration 16/25 | Loss: 0.00082998
Iteration 17/25 | Loss: 0.00082850
Iteration 18/25 | Loss: 0.00083144
Iteration 19/25 | Loss: 0.00083096
Iteration 20/25 | Loss: 0.00083031
Iteration 21/25 | Loss: 0.00082628
Iteration 22/25 | Loss: 0.00082512
Iteration 23/25 | Loss: 0.00082466
Iteration 24/25 | Loss: 0.00082426
Iteration 25/25 | Loss: 0.00082399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29040873
Iteration 2/25 | Loss: 0.00129407
Iteration 3/25 | Loss: 0.00129405
Iteration 4/25 | Loss: 0.00129405
Iteration 5/25 | Loss: 0.00129405
Iteration 6/25 | Loss: 0.00129405
Iteration 7/25 | Loss: 0.00129405
Iteration 8/25 | Loss: 0.00129405
Iteration 9/25 | Loss: 0.00129405
Iteration 10/25 | Loss: 0.00129405
Iteration 11/25 | Loss: 0.00129405
Iteration 12/25 | Loss: 0.00129405
Iteration 13/25 | Loss: 0.00129405
Iteration 14/25 | Loss: 0.00129405
Iteration 15/25 | Loss: 0.00129405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012940495507791638, 0.0012940495507791638, 0.0012940495507791638, 0.0012940495507791638, 0.0012940495507791638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012940495507791638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129405
Iteration 2/1000 | Loss: 0.00073716
Iteration 3/1000 | Loss: 0.00049018
Iteration 4/1000 | Loss: 0.00012454
Iteration 5/1000 | Loss: 0.00017029
Iteration 6/1000 | Loss: 0.00008324
Iteration 7/1000 | Loss: 0.00005550
Iteration 8/1000 | Loss: 0.00003988
Iteration 9/1000 | Loss: 0.00003430
Iteration 10/1000 | Loss: 0.00003093
Iteration 11/1000 | Loss: 0.00002863
Iteration 12/1000 | Loss: 0.00002693
Iteration 13/1000 | Loss: 0.00002578
Iteration 14/1000 | Loss: 0.00002491
Iteration 15/1000 | Loss: 0.00002453
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002378
Iteration 18/1000 | Loss: 0.00002362
Iteration 19/1000 | Loss: 0.00002358
Iteration 20/1000 | Loss: 0.00002352
Iteration 21/1000 | Loss: 0.00002351
Iteration 22/1000 | Loss: 0.00002338
Iteration 23/1000 | Loss: 0.00002333
Iteration 24/1000 | Loss: 0.00002331
Iteration 25/1000 | Loss: 0.00002327
Iteration 26/1000 | Loss: 0.00002325
Iteration 27/1000 | Loss: 0.00002320
Iteration 28/1000 | Loss: 0.00002319
Iteration 29/1000 | Loss: 0.00002318
Iteration 30/1000 | Loss: 0.00002317
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002316
Iteration 33/1000 | Loss: 0.00002315
Iteration 34/1000 | Loss: 0.00002314
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002313
Iteration 38/1000 | Loss: 0.00002313
Iteration 39/1000 | Loss: 0.00002313
Iteration 40/1000 | Loss: 0.00002312
Iteration 41/1000 | Loss: 0.00002312
Iteration 42/1000 | Loss: 0.00002312
Iteration 43/1000 | Loss: 0.00002312
Iteration 44/1000 | Loss: 0.00002311
Iteration 45/1000 | Loss: 0.00002311
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002311
Iteration 48/1000 | Loss: 0.00002311
Iteration 49/1000 | Loss: 0.00002311
Iteration 50/1000 | Loss: 0.00002311
Iteration 51/1000 | Loss: 0.00002311
Iteration 52/1000 | Loss: 0.00002311
Iteration 53/1000 | Loss: 0.00002310
Iteration 54/1000 | Loss: 0.00002308
Iteration 55/1000 | Loss: 0.00002308
Iteration 56/1000 | Loss: 0.00002308
Iteration 57/1000 | Loss: 0.00002306
Iteration 58/1000 | Loss: 0.00002306
Iteration 59/1000 | Loss: 0.00002306
Iteration 60/1000 | Loss: 0.00002306
Iteration 61/1000 | Loss: 0.00002306
Iteration 62/1000 | Loss: 0.00002306
Iteration 63/1000 | Loss: 0.00002305
Iteration 64/1000 | Loss: 0.00002304
Iteration 65/1000 | Loss: 0.00002304
Iteration 66/1000 | Loss: 0.00002304
Iteration 67/1000 | Loss: 0.00002304
Iteration 68/1000 | Loss: 0.00002304
Iteration 69/1000 | Loss: 0.00002304
Iteration 70/1000 | Loss: 0.00002304
Iteration 71/1000 | Loss: 0.00002303
Iteration 72/1000 | Loss: 0.00002303
Iteration 73/1000 | Loss: 0.00002303
Iteration 74/1000 | Loss: 0.00002303
Iteration 75/1000 | Loss: 0.00002303
Iteration 76/1000 | Loss: 0.00002303
Iteration 77/1000 | Loss: 0.00002303
Iteration 78/1000 | Loss: 0.00002303
Iteration 79/1000 | Loss: 0.00002303
Iteration 80/1000 | Loss: 0.00002302
Iteration 81/1000 | Loss: 0.00002302
Iteration 82/1000 | Loss: 0.00002302
Iteration 83/1000 | Loss: 0.00002302
Iteration 84/1000 | Loss: 0.00002302
Iteration 85/1000 | Loss: 0.00002302
Iteration 86/1000 | Loss: 0.00002302
Iteration 87/1000 | Loss: 0.00002302
Iteration 88/1000 | Loss: 0.00002302
Iteration 89/1000 | Loss: 0.00002301
Iteration 90/1000 | Loss: 0.00002301
Iteration 91/1000 | Loss: 0.00002301
Iteration 92/1000 | Loss: 0.00002301
Iteration 93/1000 | Loss: 0.00002301
Iteration 94/1000 | Loss: 0.00002301
Iteration 95/1000 | Loss: 0.00002300
Iteration 96/1000 | Loss: 0.00002300
Iteration 97/1000 | Loss: 0.00002300
Iteration 98/1000 | Loss: 0.00002300
Iteration 99/1000 | Loss: 0.00002300
Iteration 100/1000 | Loss: 0.00002300
Iteration 101/1000 | Loss: 0.00002300
Iteration 102/1000 | Loss: 0.00002300
Iteration 103/1000 | Loss: 0.00002300
Iteration 104/1000 | Loss: 0.00002300
Iteration 105/1000 | Loss: 0.00002300
Iteration 106/1000 | Loss: 0.00002299
Iteration 107/1000 | Loss: 0.00002299
Iteration 108/1000 | Loss: 0.00002299
Iteration 109/1000 | Loss: 0.00002299
Iteration 110/1000 | Loss: 0.00002299
Iteration 111/1000 | Loss: 0.00002299
Iteration 112/1000 | Loss: 0.00002299
Iteration 113/1000 | Loss: 0.00002299
Iteration 114/1000 | Loss: 0.00002299
Iteration 115/1000 | Loss: 0.00002299
Iteration 116/1000 | Loss: 0.00002299
Iteration 117/1000 | Loss: 0.00002299
Iteration 118/1000 | Loss: 0.00002299
Iteration 119/1000 | Loss: 0.00002299
Iteration 120/1000 | Loss: 0.00002299
Iteration 121/1000 | Loss: 0.00002299
Iteration 122/1000 | Loss: 0.00002299
Iteration 123/1000 | Loss: 0.00002299
Iteration 124/1000 | Loss: 0.00002299
Iteration 125/1000 | Loss: 0.00002299
Iteration 126/1000 | Loss: 0.00002299
Iteration 127/1000 | Loss: 0.00002299
Iteration 128/1000 | Loss: 0.00002299
Iteration 129/1000 | Loss: 0.00002299
Iteration 130/1000 | Loss: 0.00002299
Iteration 131/1000 | Loss: 0.00002299
Iteration 132/1000 | Loss: 0.00002299
Iteration 133/1000 | Loss: 0.00002299
Iteration 134/1000 | Loss: 0.00002299
Iteration 135/1000 | Loss: 0.00002299
Iteration 136/1000 | Loss: 0.00002299
Iteration 137/1000 | Loss: 0.00002299
Iteration 138/1000 | Loss: 0.00002299
Iteration 139/1000 | Loss: 0.00002299
Iteration 140/1000 | Loss: 0.00002299
Iteration 141/1000 | Loss: 0.00002299
Iteration 142/1000 | Loss: 0.00002299
Iteration 143/1000 | Loss: 0.00002299
Iteration 144/1000 | Loss: 0.00002299
Iteration 145/1000 | Loss: 0.00002299
Iteration 146/1000 | Loss: 0.00002299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.298600520589389e-05, 2.298600520589389e-05, 2.298600520589389e-05, 2.298600520589389e-05, 2.298600520589389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.298600520589389e-05

Optimization complete. Final v2v error: 4.085079669952393 mm

Highest mean error: 5.190591335296631 mm for frame 192

Lowest mean error: 3.261260509490967 mm for frame 89

Saving results

Total time: 94.76205372810364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541948
Iteration 2/25 | Loss: 0.00120403
Iteration 3/25 | Loss: 0.00088546
Iteration 4/25 | Loss: 0.00083315
Iteration 5/25 | Loss: 0.00081978
Iteration 6/25 | Loss: 0.00081713
Iteration 7/25 | Loss: 0.00081654
Iteration 8/25 | Loss: 0.00081654
Iteration 9/25 | Loss: 0.00081654
Iteration 10/25 | Loss: 0.00081654
Iteration 11/25 | Loss: 0.00081654
Iteration 12/25 | Loss: 0.00081654
Iteration 13/25 | Loss: 0.00081654
Iteration 14/25 | Loss: 0.00081654
Iteration 15/25 | Loss: 0.00081654
Iteration 16/25 | Loss: 0.00081654
Iteration 17/25 | Loss: 0.00081654
Iteration 18/25 | Loss: 0.00081654
Iteration 19/25 | Loss: 0.00081654
Iteration 20/25 | Loss: 0.00081654
Iteration 21/25 | Loss: 0.00081654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008165406761690974, 0.0008165406761690974, 0.0008165406761690974, 0.0008165406761690974, 0.0008165406761690974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008165406761690974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26556063
Iteration 2/25 | Loss: 0.00039656
Iteration 3/25 | Loss: 0.00039654
Iteration 4/25 | Loss: 0.00039654
Iteration 5/25 | Loss: 0.00039654
Iteration 6/25 | Loss: 0.00039654
Iteration 7/25 | Loss: 0.00039654
Iteration 8/25 | Loss: 0.00039654
Iteration 9/25 | Loss: 0.00039654
Iteration 10/25 | Loss: 0.00039654
Iteration 11/25 | Loss: 0.00039654
Iteration 12/25 | Loss: 0.00039654
Iteration 13/25 | Loss: 0.00039654
Iteration 14/25 | Loss: 0.00039654
Iteration 15/25 | Loss: 0.00039654
Iteration 16/25 | Loss: 0.00039654
Iteration 17/25 | Loss: 0.00039654
Iteration 18/25 | Loss: 0.00039654
Iteration 19/25 | Loss: 0.00039654
Iteration 20/25 | Loss: 0.00039654
Iteration 21/25 | Loss: 0.00039654
Iteration 22/25 | Loss: 0.00039654
Iteration 23/25 | Loss: 0.00039654
Iteration 24/25 | Loss: 0.00039654
Iteration 25/25 | Loss: 0.00039654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039654
Iteration 2/1000 | Loss: 0.00005394
Iteration 3/1000 | Loss: 0.00003894
Iteration 4/1000 | Loss: 0.00003536
Iteration 5/1000 | Loss: 0.00003353
Iteration 6/1000 | Loss: 0.00003225
Iteration 7/1000 | Loss: 0.00003145
Iteration 8/1000 | Loss: 0.00003061
Iteration 9/1000 | Loss: 0.00003036
Iteration 10/1000 | Loss: 0.00003003
Iteration 11/1000 | Loss: 0.00002976
Iteration 12/1000 | Loss: 0.00002962
Iteration 13/1000 | Loss: 0.00002951
Iteration 14/1000 | Loss: 0.00002949
Iteration 15/1000 | Loss: 0.00002945
Iteration 16/1000 | Loss: 0.00002943
Iteration 17/1000 | Loss: 0.00002937
Iteration 18/1000 | Loss: 0.00002936
Iteration 19/1000 | Loss: 0.00002936
Iteration 20/1000 | Loss: 0.00002935
Iteration 21/1000 | Loss: 0.00002934
Iteration 22/1000 | Loss: 0.00002934
Iteration 23/1000 | Loss: 0.00002934
Iteration 24/1000 | Loss: 0.00002932
Iteration 25/1000 | Loss: 0.00002932
Iteration 26/1000 | Loss: 0.00002932
Iteration 27/1000 | Loss: 0.00002932
Iteration 28/1000 | Loss: 0.00002931
Iteration 29/1000 | Loss: 0.00002931
Iteration 30/1000 | Loss: 0.00002931
Iteration 31/1000 | Loss: 0.00002931
Iteration 32/1000 | Loss: 0.00002931
Iteration 33/1000 | Loss: 0.00002930
Iteration 34/1000 | Loss: 0.00002930
Iteration 35/1000 | Loss: 0.00002930
Iteration 36/1000 | Loss: 0.00002930
Iteration 37/1000 | Loss: 0.00002929
Iteration 38/1000 | Loss: 0.00002929
Iteration 39/1000 | Loss: 0.00002929
Iteration 40/1000 | Loss: 0.00002928
Iteration 41/1000 | Loss: 0.00002928
Iteration 42/1000 | Loss: 0.00002928
Iteration 43/1000 | Loss: 0.00002927
Iteration 44/1000 | Loss: 0.00002927
Iteration 45/1000 | Loss: 0.00002927
Iteration 46/1000 | Loss: 0.00002926
Iteration 47/1000 | Loss: 0.00002926
Iteration 48/1000 | Loss: 0.00002926
Iteration 49/1000 | Loss: 0.00002926
Iteration 50/1000 | Loss: 0.00002926
Iteration 51/1000 | Loss: 0.00002926
Iteration 52/1000 | Loss: 0.00002926
Iteration 53/1000 | Loss: 0.00002925
Iteration 54/1000 | Loss: 0.00002925
Iteration 55/1000 | Loss: 0.00002925
Iteration 56/1000 | Loss: 0.00002924
Iteration 57/1000 | Loss: 0.00002924
Iteration 58/1000 | Loss: 0.00002924
Iteration 59/1000 | Loss: 0.00002923
Iteration 60/1000 | Loss: 0.00002923
Iteration 61/1000 | Loss: 0.00002923
Iteration 62/1000 | Loss: 0.00002923
Iteration 63/1000 | Loss: 0.00002923
Iteration 64/1000 | Loss: 0.00002922
Iteration 65/1000 | Loss: 0.00002922
Iteration 66/1000 | Loss: 0.00002922
Iteration 67/1000 | Loss: 0.00002922
Iteration 68/1000 | Loss: 0.00002922
Iteration 69/1000 | Loss: 0.00002922
Iteration 70/1000 | Loss: 0.00002922
Iteration 71/1000 | Loss: 0.00002922
Iteration 72/1000 | Loss: 0.00002922
Iteration 73/1000 | Loss: 0.00002922
Iteration 74/1000 | Loss: 0.00002921
Iteration 75/1000 | Loss: 0.00002921
Iteration 76/1000 | Loss: 0.00002921
Iteration 77/1000 | Loss: 0.00002920
Iteration 78/1000 | Loss: 0.00002920
Iteration 79/1000 | Loss: 0.00002920
Iteration 80/1000 | Loss: 0.00002920
Iteration 81/1000 | Loss: 0.00002920
Iteration 82/1000 | Loss: 0.00002920
Iteration 83/1000 | Loss: 0.00002920
Iteration 84/1000 | Loss: 0.00002920
Iteration 85/1000 | Loss: 0.00002920
Iteration 86/1000 | Loss: 0.00002920
Iteration 87/1000 | Loss: 0.00002920
Iteration 88/1000 | Loss: 0.00002920
Iteration 89/1000 | Loss: 0.00002920
Iteration 90/1000 | Loss: 0.00002920
Iteration 91/1000 | Loss: 0.00002920
Iteration 92/1000 | Loss: 0.00002920
Iteration 93/1000 | Loss: 0.00002920
Iteration 94/1000 | Loss: 0.00002920
Iteration 95/1000 | Loss: 0.00002920
Iteration 96/1000 | Loss: 0.00002920
Iteration 97/1000 | Loss: 0.00002920
Iteration 98/1000 | Loss: 0.00002920
Iteration 99/1000 | Loss: 0.00002920
Iteration 100/1000 | Loss: 0.00002920
Iteration 101/1000 | Loss: 0.00002920
Iteration 102/1000 | Loss: 0.00002920
Iteration 103/1000 | Loss: 0.00002920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.919597682193853e-05, 2.919597682193853e-05, 2.919597682193853e-05, 2.919597682193853e-05, 2.919597682193853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.919597682193853e-05

Optimization complete. Final v2v error: 4.622970104217529 mm

Highest mean error: 5.113504886627197 mm for frame 10

Lowest mean error: 4.175667762756348 mm for frame 98

Saving results

Total time: 35.069793701171875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069312
Iteration 2/25 | Loss: 0.00628361
Iteration 3/25 | Loss: 0.00216193
Iteration 4/25 | Loss: 0.00172861
Iteration 5/25 | Loss: 0.00159035
Iteration 6/25 | Loss: 0.00160091
Iteration 7/25 | Loss: 0.00154922
Iteration 8/25 | Loss: 0.00152882
Iteration 9/25 | Loss: 0.00144921
Iteration 10/25 | Loss: 0.00137332
Iteration 11/25 | Loss: 0.00130529
Iteration 12/25 | Loss: 0.00124893
Iteration 13/25 | Loss: 0.00121299
Iteration 14/25 | Loss: 0.00117445
Iteration 15/25 | Loss: 0.00116132
Iteration 16/25 | Loss: 0.00113582
Iteration 17/25 | Loss: 0.00111010
Iteration 18/25 | Loss: 0.00110596
Iteration 19/25 | Loss: 0.00109671
Iteration 20/25 | Loss: 0.00108481
Iteration 21/25 | Loss: 0.00108149
Iteration 22/25 | Loss: 0.00107872
Iteration 23/25 | Loss: 0.00107499
Iteration 24/25 | Loss: 0.00106813
Iteration 25/25 | Loss: 0.00106736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32192457
Iteration 2/25 | Loss: 0.00348536
Iteration 3/25 | Loss: 0.00319555
Iteration 4/25 | Loss: 0.00319555
Iteration 5/25 | Loss: 0.00319555
Iteration 6/25 | Loss: 0.00319555
Iteration 7/25 | Loss: 0.00319555
Iteration 8/25 | Loss: 0.00319555
Iteration 9/25 | Loss: 0.00319555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.003195547265931964, 0.003195547265931964, 0.003195547265931964, 0.003195547265931964, 0.003195547265931964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003195547265931964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319555
Iteration 2/1000 | Loss: 0.00261888
Iteration 3/1000 | Loss: 0.00171343
Iteration 4/1000 | Loss: 0.00266382
Iteration 5/1000 | Loss: 0.00260705
Iteration 6/1000 | Loss: 0.00089525
Iteration 7/1000 | Loss: 0.00101326
Iteration 8/1000 | Loss: 0.00101102
Iteration 9/1000 | Loss: 0.00216297
Iteration 10/1000 | Loss: 0.00166285
Iteration 11/1000 | Loss: 0.00184920
Iteration 12/1000 | Loss: 0.00245693
Iteration 13/1000 | Loss: 0.00331411
Iteration 14/1000 | Loss: 0.00232365
Iteration 15/1000 | Loss: 0.00189631
Iteration 16/1000 | Loss: 0.00192457
Iteration 17/1000 | Loss: 0.00107650
Iteration 18/1000 | Loss: 0.00193938
Iteration 19/1000 | Loss: 0.00083650
Iteration 20/1000 | Loss: 0.00250421
Iteration 21/1000 | Loss: 0.00358855
Iteration 22/1000 | Loss: 0.00079735
Iteration 23/1000 | Loss: 0.00244594
Iteration 24/1000 | Loss: 0.00074032
Iteration 25/1000 | Loss: 0.00030865
Iteration 26/1000 | Loss: 0.00081949
Iteration 27/1000 | Loss: 0.00051016
Iteration 28/1000 | Loss: 0.00184737
Iteration 29/1000 | Loss: 0.00105611
Iteration 30/1000 | Loss: 0.00250458
Iteration 31/1000 | Loss: 0.00220225
Iteration 32/1000 | Loss: 0.00211138
Iteration 33/1000 | Loss: 0.00184992
Iteration 34/1000 | Loss: 0.00095819
Iteration 35/1000 | Loss: 0.00145893
Iteration 36/1000 | Loss: 0.00196225
Iteration 37/1000 | Loss: 0.00193700
Iteration 38/1000 | Loss: 0.00108229
Iteration 39/1000 | Loss: 0.00103423
Iteration 40/1000 | Loss: 0.00065392
Iteration 41/1000 | Loss: 0.00090125
Iteration 42/1000 | Loss: 0.00026686
Iteration 43/1000 | Loss: 0.00078078
Iteration 44/1000 | Loss: 0.00145314
Iteration 45/1000 | Loss: 0.00109553
Iteration 46/1000 | Loss: 0.00100991
Iteration 47/1000 | Loss: 0.00064440
Iteration 48/1000 | Loss: 0.00065370
Iteration 49/1000 | Loss: 0.00162209
Iteration 50/1000 | Loss: 0.00249076
Iteration 51/1000 | Loss: 0.00227332
Iteration 52/1000 | Loss: 0.00166615
Iteration 53/1000 | Loss: 0.00160998
Iteration 54/1000 | Loss: 0.00168878
Iteration 55/1000 | Loss: 0.00179529
Iteration 56/1000 | Loss: 0.00131528
Iteration 57/1000 | Loss: 0.00122972
Iteration 58/1000 | Loss: 0.00162656
Iteration 59/1000 | Loss: 0.00111914
Iteration 60/1000 | Loss: 0.00140111
Iteration 61/1000 | Loss: 0.00139326
Iteration 62/1000 | Loss: 0.00166709
Iteration 63/1000 | Loss: 0.00203464
Iteration 64/1000 | Loss: 0.00228308
Iteration 65/1000 | Loss: 0.00340223
Iteration 66/1000 | Loss: 0.00156700
Iteration 67/1000 | Loss: 0.00104911
Iteration 68/1000 | Loss: 0.00058434
Iteration 69/1000 | Loss: 0.00048819
Iteration 70/1000 | Loss: 0.00052488
Iteration 71/1000 | Loss: 0.00050763
Iteration 72/1000 | Loss: 0.00097995
Iteration 73/1000 | Loss: 0.00061612
Iteration 74/1000 | Loss: 0.00066179
Iteration 75/1000 | Loss: 0.00094638
Iteration 76/1000 | Loss: 0.00103259
Iteration 77/1000 | Loss: 0.00103597
Iteration 78/1000 | Loss: 0.00087402
Iteration 79/1000 | Loss: 0.00077295
Iteration 80/1000 | Loss: 0.00064899
Iteration 81/1000 | Loss: 0.00085328
Iteration 82/1000 | Loss: 0.00052810
Iteration 83/1000 | Loss: 0.00084919
Iteration 84/1000 | Loss: 0.00023680
Iteration 85/1000 | Loss: 0.00013262
Iteration 86/1000 | Loss: 0.00018909
Iteration 87/1000 | Loss: 0.00096450
Iteration 88/1000 | Loss: 0.00090287
Iteration 89/1000 | Loss: 0.00048107
Iteration 90/1000 | Loss: 0.00054208
Iteration 91/1000 | Loss: 0.00132307
Iteration 92/1000 | Loss: 0.00063965
Iteration 93/1000 | Loss: 0.00044795
Iteration 94/1000 | Loss: 0.00045742
Iteration 95/1000 | Loss: 0.00034131
Iteration 96/1000 | Loss: 0.00052774
Iteration 97/1000 | Loss: 0.00035616
Iteration 98/1000 | Loss: 0.00033819
Iteration 99/1000 | Loss: 0.00029059
Iteration 100/1000 | Loss: 0.00041663
Iteration 101/1000 | Loss: 0.00050184
Iteration 102/1000 | Loss: 0.00026259
Iteration 103/1000 | Loss: 0.00023477
Iteration 104/1000 | Loss: 0.00009080
Iteration 105/1000 | Loss: 0.00026993
Iteration 106/1000 | Loss: 0.00042222
Iteration 107/1000 | Loss: 0.00017728
Iteration 108/1000 | Loss: 0.00021882
Iteration 109/1000 | Loss: 0.00096590
Iteration 110/1000 | Loss: 0.00083666
Iteration 111/1000 | Loss: 0.00010742
Iteration 112/1000 | Loss: 0.00010618
Iteration 113/1000 | Loss: 0.00014748
Iteration 114/1000 | Loss: 0.00037653
Iteration 115/1000 | Loss: 0.00024573
Iteration 116/1000 | Loss: 0.00014697
Iteration 117/1000 | Loss: 0.00016043
Iteration 118/1000 | Loss: 0.00009288
Iteration 119/1000 | Loss: 0.00030161
Iteration 120/1000 | Loss: 0.00022663
Iteration 121/1000 | Loss: 0.00009001
Iteration 122/1000 | Loss: 0.00012015
Iteration 123/1000 | Loss: 0.00024884
Iteration 124/1000 | Loss: 0.00032578
Iteration 125/1000 | Loss: 0.00012398
Iteration 126/1000 | Loss: 0.00014889
Iteration 127/1000 | Loss: 0.00009589
Iteration 128/1000 | Loss: 0.00012817
Iteration 129/1000 | Loss: 0.00009638
Iteration 130/1000 | Loss: 0.00009454
Iteration 131/1000 | Loss: 0.00011225
Iteration 132/1000 | Loss: 0.00055634
Iteration 133/1000 | Loss: 0.00035850
Iteration 134/1000 | Loss: 0.00097477
Iteration 135/1000 | Loss: 0.00051805
Iteration 136/1000 | Loss: 0.00028047
Iteration 137/1000 | Loss: 0.00036826
Iteration 138/1000 | Loss: 0.00046065
Iteration 139/1000 | Loss: 0.00019871
Iteration 140/1000 | Loss: 0.00016697
Iteration 141/1000 | Loss: 0.00030674
Iteration 142/1000 | Loss: 0.00106804
Iteration 143/1000 | Loss: 0.00009326
Iteration 144/1000 | Loss: 0.00046192
Iteration 145/1000 | Loss: 0.00074526
Iteration 146/1000 | Loss: 0.00042472
Iteration 147/1000 | Loss: 0.00071988
Iteration 148/1000 | Loss: 0.00055165
Iteration 149/1000 | Loss: 0.00024126
Iteration 150/1000 | Loss: 0.00016817
Iteration 151/1000 | Loss: 0.00017927
Iteration 152/1000 | Loss: 0.00013574
Iteration 153/1000 | Loss: 0.00016155
Iteration 154/1000 | Loss: 0.00007163
Iteration 155/1000 | Loss: 0.00006076
Iteration 156/1000 | Loss: 0.00007444
Iteration 157/1000 | Loss: 0.00017054
Iteration 158/1000 | Loss: 0.00021889
Iteration 159/1000 | Loss: 0.00008031
Iteration 160/1000 | Loss: 0.00023792
Iteration 161/1000 | Loss: 0.00043607
Iteration 162/1000 | Loss: 0.00042838
Iteration 163/1000 | Loss: 0.00035984
Iteration 164/1000 | Loss: 0.00037082
Iteration 165/1000 | Loss: 0.00055910
Iteration 166/1000 | Loss: 0.00008803
Iteration 167/1000 | Loss: 0.00006633
Iteration 168/1000 | Loss: 0.00008012
Iteration 169/1000 | Loss: 0.00007519
Iteration 170/1000 | Loss: 0.00024326
Iteration 171/1000 | Loss: 0.00025749
Iteration 172/1000 | Loss: 0.00015519
Iteration 173/1000 | Loss: 0.00021692
Iteration 174/1000 | Loss: 0.00029396
Iteration 175/1000 | Loss: 0.00025327
Iteration 176/1000 | Loss: 0.00019812
Iteration 177/1000 | Loss: 0.00007094
Iteration 178/1000 | Loss: 0.00022580
Iteration 179/1000 | Loss: 0.00022329
Iteration 180/1000 | Loss: 0.00008895
Iteration 181/1000 | Loss: 0.00021585
Iteration 182/1000 | Loss: 0.00016926
Iteration 183/1000 | Loss: 0.00023551
Iteration 184/1000 | Loss: 0.00007311
Iteration 185/1000 | Loss: 0.00007429
Iteration 186/1000 | Loss: 0.00006544
Iteration 187/1000 | Loss: 0.00007830
Iteration 188/1000 | Loss: 0.00007505
Iteration 189/1000 | Loss: 0.00005087
Iteration 190/1000 | Loss: 0.00005756
Iteration 191/1000 | Loss: 0.00006628
Iteration 192/1000 | Loss: 0.00007349
Iteration 193/1000 | Loss: 0.00021121
Iteration 194/1000 | Loss: 0.00008552
Iteration 195/1000 | Loss: 0.00007884
Iteration 196/1000 | Loss: 0.00008569
Iteration 197/1000 | Loss: 0.00007726
Iteration 198/1000 | Loss: 0.00008282
Iteration 199/1000 | Loss: 0.00007707
Iteration 200/1000 | Loss: 0.00007634
Iteration 201/1000 | Loss: 0.00007772
Iteration 202/1000 | Loss: 0.00007537
Iteration 203/1000 | Loss: 0.00007833
Iteration 204/1000 | Loss: 0.00007431
Iteration 205/1000 | Loss: 0.00007633
Iteration 206/1000 | Loss: 0.00007306
Iteration 207/1000 | Loss: 0.00025772
Iteration 208/1000 | Loss: 0.00026076
Iteration 209/1000 | Loss: 0.00029327
Iteration 210/1000 | Loss: 0.00010523
Iteration 211/1000 | Loss: 0.00033207
Iteration 212/1000 | Loss: 0.00037402
Iteration 213/1000 | Loss: 0.00064629
Iteration 214/1000 | Loss: 0.00072486
Iteration 215/1000 | Loss: 0.00008785
Iteration 216/1000 | Loss: 0.00006274
Iteration 217/1000 | Loss: 0.00004700
Iteration 218/1000 | Loss: 0.00028113
Iteration 219/1000 | Loss: 0.00017595
Iteration 220/1000 | Loss: 0.00030340
Iteration 221/1000 | Loss: 0.00053419
Iteration 222/1000 | Loss: 0.00025815
Iteration 223/1000 | Loss: 0.00035539
Iteration 224/1000 | Loss: 0.00013903
Iteration 225/1000 | Loss: 0.00022525
Iteration 226/1000 | Loss: 0.00035387
Iteration 227/1000 | Loss: 0.00037905
Iteration 228/1000 | Loss: 0.00030225
Iteration 229/1000 | Loss: 0.00039250
Iteration 230/1000 | Loss: 0.00061809
Iteration 231/1000 | Loss: 0.00098396
Iteration 232/1000 | Loss: 0.00007423
Iteration 233/1000 | Loss: 0.00041767
Iteration 234/1000 | Loss: 0.00009287
Iteration 235/1000 | Loss: 0.00005920
Iteration 236/1000 | Loss: 0.00030102
Iteration 237/1000 | Loss: 0.00031721
Iteration 238/1000 | Loss: 0.00006950
Iteration 239/1000 | Loss: 0.00029447
Iteration 240/1000 | Loss: 0.00027692
Iteration 241/1000 | Loss: 0.00006944
Iteration 242/1000 | Loss: 0.00007198
Iteration 243/1000 | Loss: 0.00007118
Iteration 244/1000 | Loss: 0.00017651
Iteration 245/1000 | Loss: 0.00010347
Iteration 246/1000 | Loss: 0.00034243
Iteration 247/1000 | Loss: 0.00034257
Iteration 248/1000 | Loss: 0.00040755
Iteration 249/1000 | Loss: 0.00024588
Iteration 250/1000 | Loss: 0.00027000
Iteration 251/1000 | Loss: 0.00011644
Iteration 252/1000 | Loss: 0.00006716
Iteration 253/1000 | Loss: 0.00006928
Iteration 254/1000 | Loss: 0.00007429
Iteration 255/1000 | Loss: 0.00007232
Iteration 256/1000 | Loss: 0.00007300
Iteration 257/1000 | Loss: 0.00007286
Iteration 258/1000 | Loss: 0.00025553
Iteration 259/1000 | Loss: 0.00013780
Iteration 260/1000 | Loss: 0.00019282
Iteration 261/1000 | Loss: 0.00030480
Iteration 262/1000 | Loss: 0.00035422
Iteration 263/1000 | Loss: 0.00011077
Iteration 264/1000 | Loss: 0.00007327
Iteration 265/1000 | Loss: 0.00017517
Iteration 266/1000 | Loss: 0.00018451
Iteration 267/1000 | Loss: 0.00018996
Iteration 268/1000 | Loss: 0.00013122
Iteration 269/1000 | Loss: 0.00019504
Iteration 270/1000 | Loss: 0.00017898
Iteration 271/1000 | Loss: 0.00023096
Iteration 272/1000 | Loss: 0.00024523
Iteration 273/1000 | Loss: 0.00006713
Iteration 274/1000 | Loss: 0.00004641
Iteration 275/1000 | Loss: 0.00005821
Iteration 276/1000 | Loss: 0.00011150
Iteration 277/1000 | Loss: 0.00004584
Iteration 278/1000 | Loss: 0.00003425
Iteration 279/1000 | Loss: 0.00003044
Iteration 280/1000 | Loss: 0.00002842
Iteration 281/1000 | Loss: 0.00002641
Iteration 282/1000 | Loss: 0.00002530
Iteration 283/1000 | Loss: 0.00002461
Iteration 284/1000 | Loss: 0.00002425
Iteration 285/1000 | Loss: 0.00002404
Iteration 286/1000 | Loss: 0.00002400
Iteration 287/1000 | Loss: 0.00002396
Iteration 288/1000 | Loss: 0.00002396
Iteration 289/1000 | Loss: 0.00002392
Iteration 290/1000 | Loss: 0.00002391
Iteration 291/1000 | Loss: 0.00002390
Iteration 292/1000 | Loss: 0.00002390
Iteration 293/1000 | Loss: 0.00002389
Iteration 294/1000 | Loss: 0.00002386
Iteration 295/1000 | Loss: 0.00002380
Iteration 296/1000 | Loss: 0.00002379
Iteration 297/1000 | Loss: 0.00002378
Iteration 298/1000 | Loss: 0.00002377
Iteration 299/1000 | Loss: 0.00002377
Iteration 300/1000 | Loss: 0.00002377
Iteration 301/1000 | Loss: 0.00002377
Iteration 302/1000 | Loss: 0.00002375
Iteration 303/1000 | Loss: 0.00002374
Iteration 304/1000 | Loss: 0.00002374
Iteration 305/1000 | Loss: 0.00002374
Iteration 306/1000 | Loss: 0.00002373
Iteration 307/1000 | Loss: 0.00002373
Iteration 308/1000 | Loss: 0.00002372
Iteration 309/1000 | Loss: 0.00002366
Iteration 310/1000 | Loss: 0.00002366
Iteration 311/1000 | Loss: 0.00002366
Iteration 312/1000 | Loss: 0.00002366
Iteration 313/1000 | Loss: 0.00002378
Iteration 314/1000 | Loss: 0.00002373
Iteration 315/1000 | Loss: 0.00002373
Iteration 316/1000 | Loss: 0.00002373
Iteration 317/1000 | Loss: 0.00002373
Iteration 318/1000 | Loss: 0.00002372
Iteration 319/1000 | Loss: 0.00002372
Iteration 320/1000 | Loss: 0.00002372
Iteration 321/1000 | Loss: 0.00002366
Iteration 322/1000 | Loss: 0.00002366
Iteration 323/1000 | Loss: 0.00002365
Iteration 324/1000 | Loss: 0.00002365
Iteration 325/1000 | Loss: 0.00002364
Iteration 326/1000 | Loss: 0.00002364
Iteration 327/1000 | Loss: 0.00002364
Iteration 328/1000 | Loss: 0.00002364
Iteration 329/1000 | Loss: 0.00002364
Iteration 330/1000 | Loss: 0.00002364
Iteration 331/1000 | Loss: 0.00002374
Iteration 332/1000 | Loss: 0.00002374
Iteration 333/1000 | Loss: 0.00002373
Iteration 334/1000 | Loss: 0.00002373
Iteration 335/1000 | Loss: 0.00002370
Iteration 336/1000 | Loss: 0.00002370
Iteration 337/1000 | Loss: 0.00002369
Iteration 338/1000 | Loss: 0.00002362
Iteration 339/1000 | Loss: 0.00002362
Iteration 340/1000 | Loss: 0.00002361
Iteration 341/1000 | Loss: 0.00002360
Iteration 342/1000 | Loss: 0.00002360
Iteration 343/1000 | Loss: 0.00002359
Iteration 344/1000 | Loss: 0.00002358
Iteration 345/1000 | Loss: 0.00002358
Iteration 346/1000 | Loss: 0.00002357
Iteration 347/1000 | Loss: 0.00002364
Iteration 348/1000 | Loss: 0.00002363
Iteration 349/1000 | Loss: 0.00002363
Iteration 350/1000 | Loss: 0.00002363
Iteration 351/1000 | Loss: 0.00002363
Iteration 352/1000 | Loss: 0.00002363
Iteration 353/1000 | Loss: 0.00002362
Iteration 354/1000 | Loss: 0.00002362
Iteration 355/1000 | Loss: 0.00002362
Iteration 356/1000 | Loss: 0.00002361
Iteration 357/1000 | Loss: 0.00002361
Iteration 358/1000 | Loss: 0.00002361
Iteration 359/1000 | Loss: 0.00002360
Iteration 360/1000 | Loss: 0.00002354
Iteration 361/1000 | Loss: 0.00002354
Iteration 362/1000 | Loss: 0.00002353
Iteration 363/1000 | Loss: 0.00002353
Iteration 364/1000 | Loss: 0.00002353
Iteration 365/1000 | Loss: 0.00002352
Iteration 366/1000 | Loss: 0.00002351
Iteration 367/1000 | Loss: 0.00002351
Iteration 368/1000 | Loss: 0.00002351
Iteration 369/1000 | Loss: 0.00002350
Iteration 370/1000 | Loss: 0.00002350
Iteration 371/1000 | Loss: 0.00002350
Iteration 372/1000 | Loss: 0.00002351
Iteration 373/1000 | Loss: 0.00002351
Iteration 374/1000 | Loss: 0.00002351
Iteration 375/1000 | Loss: 0.00002351
Iteration 376/1000 | Loss: 0.00002351
Iteration 377/1000 | Loss: 0.00002351
Iteration 378/1000 | Loss: 0.00002351
Iteration 379/1000 | Loss: 0.00002351
Iteration 380/1000 | Loss: 0.00002351
Iteration 381/1000 | Loss: 0.00002351
Iteration 382/1000 | Loss: 0.00002351
Iteration 383/1000 | Loss: 0.00002351
Iteration 384/1000 | Loss: 0.00002351
Iteration 385/1000 | Loss: 0.00002351
Iteration 386/1000 | Loss: 0.00002351
Iteration 387/1000 | Loss: 0.00002351
Iteration 388/1000 | Loss: 0.00002351
Iteration 389/1000 | Loss: 0.00002351
Iteration 390/1000 | Loss: 0.00002351
Iteration 391/1000 | Loss: 0.00002351
Iteration 392/1000 | Loss: 0.00002351
Iteration 393/1000 | Loss: 0.00002351
Iteration 394/1000 | Loss: 0.00002351
Iteration 395/1000 | Loss: 0.00002351
Iteration 396/1000 | Loss: 0.00002351
Iteration 397/1000 | Loss: 0.00002351
Iteration 398/1000 | Loss: 0.00002351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [2.351263538002968e-05, 2.351263538002968e-05, 2.351263538002968e-05, 2.351263538002968e-05, 2.351263538002968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.351263538002968e-05

Optimization complete. Final v2v error: 3.732980728149414 mm

Highest mean error: 20.798051834106445 mm for frame 129

Lowest mean error: 3.1107749938964844 mm for frame 171

Saving results

Total time: 524.4064626693726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869106
Iteration 2/25 | Loss: 0.00104096
Iteration 3/25 | Loss: 0.00083754
Iteration 4/25 | Loss: 0.00079859
Iteration 5/25 | Loss: 0.00078706
Iteration 6/25 | Loss: 0.00078358
Iteration 7/25 | Loss: 0.00078242
Iteration 8/25 | Loss: 0.00078635
Iteration 9/25 | Loss: 0.00078912
Iteration 10/25 | Loss: 0.00078561
Iteration 11/25 | Loss: 0.00078602
Iteration 12/25 | Loss: 0.00078469
Iteration 13/25 | Loss: 0.00077887
Iteration 14/25 | Loss: 0.00077474
Iteration 15/25 | Loss: 0.00077337
Iteration 16/25 | Loss: 0.00077290
Iteration 17/25 | Loss: 0.00077275
Iteration 18/25 | Loss: 0.00077275
Iteration 19/25 | Loss: 0.00077274
Iteration 20/25 | Loss: 0.00077274
Iteration 21/25 | Loss: 0.00077274
Iteration 22/25 | Loss: 0.00077274
Iteration 23/25 | Loss: 0.00077274
Iteration 24/25 | Loss: 0.00077274
Iteration 25/25 | Loss: 0.00077274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28270411
Iteration 2/25 | Loss: 0.00149969
Iteration 3/25 | Loss: 0.00149968
Iteration 4/25 | Loss: 0.00149968
Iteration 5/25 | Loss: 0.00149968
Iteration 6/25 | Loss: 0.00149968
Iteration 7/25 | Loss: 0.00149968
Iteration 8/25 | Loss: 0.00149968
Iteration 9/25 | Loss: 0.00149968
Iteration 10/25 | Loss: 0.00149968
Iteration 11/25 | Loss: 0.00149968
Iteration 12/25 | Loss: 0.00149968
Iteration 13/25 | Loss: 0.00149968
Iteration 14/25 | Loss: 0.00149968
Iteration 15/25 | Loss: 0.00149968
Iteration 16/25 | Loss: 0.00149968
Iteration 17/25 | Loss: 0.00149968
Iteration 18/25 | Loss: 0.00149968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014996780082583427, 0.0014996780082583427, 0.0014996780082583427, 0.0014996780082583427, 0.0014996780082583427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014996780082583427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149968
Iteration 2/1000 | Loss: 0.00015213
Iteration 3/1000 | Loss: 0.00011404
Iteration 4/1000 | Loss: 0.00010012
Iteration 5/1000 | Loss: 0.00008923
Iteration 6/1000 | Loss: 0.00008418
Iteration 7/1000 | Loss: 0.00008089
Iteration 8/1000 | Loss: 0.00007696
Iteration 9/1000 | Loss: 0.00024952
Iteration 10/1000 | Loss: 0.00007092
Iteration 11/1000 | Loss: 0.00006863
Iteration 12/1000 | Loss: 0.00006698
Iteration 13/1000 | Loss: 0.00047001
Iteration 14/1000 | Loss: 0.00007392
Iteration 15/1000 | Loss: 0.00006309
Iteration 16/1000 | Loss: 0.00006162
Iteration 17/1000 | Loss: 0.00204258
Iteration 18/1000 | Loss: 0.00460307
Iteration 19/1000 | Loss: 0.00133119
Iteration 20/1000 | Loss: 0.00203939
Iteration 21/1000 | Loss: 0.00504948
Iteration 22/1000 | Loss: 0.00017883
Iteration 23/1000 | Loss: 0.00008204
Iteration 24/1000 | Loss: 0.00005131
Iteration 25/1000 | Loss: 0.00003951
Iteration 26/1000 | Loss: 0.00003168
Iteration 27/1000 | Loss: 0.00002770
Iteration 28/1000 | Loss: 0.00002473
Iteration 29/1000 | Loss: 0.00002313
Iteration 30/1000 | Loss: 0.00002185
Iteration 31/1000 | Loss: 0.00002090
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00001957
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001790
Iteration 38/1000 | Loss: 0.00001773
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001751
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001745
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001745
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00058780
Iteration 106/1000 | Loss: 0.00002686
Iteration 107/1000 | Loss: 0.00002151
Iteration 108/1000 | Loss: 0.00001923
Iteration 109/1000 | Loss: 0.00001722
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001518
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001514
Iteration 115/1000 | Loss: 0.00001514
Iteration 116/1000 | Loss: 0.00001511
Iteration 117/1000 | Loss: 0.00001511
Iteration 118/1000 | Loss: 0.00001511
Iteration 119/1000 | Loss: 0.00001510
Iteration 120/1000 | Loss: 0.00001509
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001499
Iteration 123/1000 | Loss: 0.00001498
Iteration 124/1000 | Loss: 0.00001498
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001485
Iteration 132/1000 | Loss: 0.00001485
Iteration 133/1000 | Loss: 0.00001484
Iteration 134/1000 | Loss: 0.00001484
Iteration 135/1000 | Loss: 0.00001484
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001482
Iteration 140/1000 | Loss: 0.00001482
Iteration 141/1000 | Loss: 0.00001482
Iteration 142/1000 | Loss: 0.00001482
Iteration 143/1000 | Loss: 0.00001482
Iteration 144/1000 | Loss: 0.00001482
Iteration 145/1000 | Loss: 0.00001481
Iteration 146/1000 | Loss: 0.00001481
Iteration 147/1000 | Loss: 0.00001481
Iteration 148/1000 | Loss: 0.00001481
Iteration 149/1000 | Loss: 0.00001481
Iteration 150/1000 | Loss: 0.00001481
Iteration 151/1000 | Loss: 0.00001481
Iteration 152/1000 | Loss: 0.00001481
Iteration 153/1000 | Loss: 0.00001480
Iteration 154/1000 | Loss: 0.00001480
Iteration 155/1000 | Loss: 0.00001480
Iteration 156/1000 | Loss: 0.00001479
Iteration 157/1000 | Loss: 0.00001479
Iteration 158/1000 | Loss: 0.00001479
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Iteration 162/1000 | Loss: 0.00001479
Iteration 163/1000 | Loss: 0.00001479
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001479
Iteration 170/1000 | Loss: 0.00001479
Iteration 171/1000 | Loss: 0.00001479
Iteration 172/1000 | Loss: 0.00001479
Iteration 173/1000 | Loss: 0.00001479
Iteration 174/1000 | Loss: 0.00001479
Iteration 175/1000 | Loss: 0.00001479
Iteration 176/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4789643500989769e-05, 1.4789643500989769e-05, 1.4789643500989769e-05, 1.4789643500989769e-05, 1.4789643500989769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4789643500989769e-05

Optimization complete. Final v2v error: 3.2288146018981934 mm

Highest mean error: 4.946142196655273 mm for frame 85

Lowest mean error: 2.7782976627349854 mm for frame 197

Saving results

Total time: 112.9354100227356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_it_4603/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_it_4603/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141998
Iteration 2/25 | Loss: 0.00245983
Iteration 3/25 | Loss: 0.00159831
Iteration 4/25 | Loss: 0.00192389
Iteration 5/25 | Loss: 0.00104903
Iteration 6/25 | Loss: 0.00089373
Iteration 7/25 | Loss: 0.00087231
Iteration 8/25 | Loss: 0.00087060
Iteration 9/25 | Loss: 0.00087035
Iteration 10/25 | Loss: 0.00087035
Iteration 11/25 | Loss: 0.00087035
Iteration 12/25 | Loss: 0.00087035
Iteration 13/25 | Loss: 0.00087035
Iteration 14/25 | Loss: 0.00087035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000870351679623127, 0.000870351679623127, 0.000870351679623127, 0.000870351679623127, 0.000870351679623127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000870351679623127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13674116
Iteration 2/25 | Loss: 0.00035427
Iteration 3/25 | Loss: 0.00035427
Iteration 4/25 | Loss: 0.00035427
Iteration 5/25 | Loss: 0.00035427
Iteration 6/25 | Loss: 0.00035427
Iteration 7/25 | Loss: 0.00035427
Iteration 8/25 | Loss: 0.00035427
Iteration 9/25 | Loss: 0.00035427
Iteration 10/25 | Loss: 0.00035427
Iteration 11/25 | Loss: 0.00035427
Iteration 12/25 | Loss: 0.00035427
Iteration 13/25 | Loss: 0.00035427
Iteration 14/25 | Loss: 0.00035427
Iteration 15/25 | Loss: 0.00035427
Iteration 16/25 | Loss: 0.00035427
Iteration 17/25 | Loss: 0.00035427
Iteration 18/25 | Loss: 0.00035427
Iteration 19/25 | Loss: 0.00035427
Iteration 20/25 | Loss: 0.00035427
Iteration 21/25 | Loss: 0.00035427
Iteration 22/25 | Loss: 0.00035427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00035426794784143567, 0.00035426794784143567, 0.00035426794784143567, 0.00035426794784143567, 0.00035426794784143567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035426794784143567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035427
Iteration 2/1000 | Loss: 0.00004268
Iteration 3/1000 | Loss: 0.00003465
Iteration 4/1000 | Loss: 0.00003260
Iteration 5/1000 | Loss: 0.00003156
Iteration 6/1000 | Loss: 0.00003067
Iteration 7/1000 | Loss: 0.00003005
Iteration 8/1000 | Loss: 0.00002964
Iteration 9/1000 | Loss: 0.00002927
Iteration 10/1000 | Loss: 0.00002912
Iteration 11/1000 | Loss: 0.00002908
Iteration 12/1000 | Loss: 0.00002891
Iteration 13/1000 | Loss: 0.00002887
Iteration 14/1000 | Loss: 0.00002885
Iteration 15/1000 | Loss: 0.00002885
Iteration 16/1000 | Loss: 0.00002884
Iteration 17/1000 | Loss: 0.00002884
Iteration 18/1000 | Loss: 0.00002884
Iteration 19/1000 | Loss: 0.00002883
Iteration 20/1000 | Loss: 0.00002883
Iteration 21/1000 | Loss: 0.00002882
Iteration 22/1000 | Loss: 0.00002882
Iteration 23/1000 | Loss: 0.00002881
Iteration 24/1000 | Loss: 0.00002880
Iteration 25/1000 | Loss: 0.00002880
Iteration 26/1000 | Loss: 0.00002880
Iteration 27/1000 | Loss: 0.00002880
Iteration 28/1000 | Loss: 0.00002880
Iteration 29/1000 | Loss: 0.00002880
Iteration 30/1000 | Loss: 0.00002880
Iteration 31/1000 | Loss: 0.00002880
Iteration 32/1000 | Loss: 0.00002879
Iteration 33/1000 | Loss: 0.00002879
Iteration 34/1000 | Loss: 0.00002879
Iteration 35/1000 | Loss: 0.00002879
Iteration 36/1000 | Loss: 0.00002879
Iteration 37/1000 | Loss: 0.00002879
Iteration 38/1000 | Loss: 0.00002879
Iteration 39/1000 | Loss: 0.00002879
Iteration 40/1000 | Loss: 0.00002879
Iteration 41/1000 | Loss: 0.00002879
Iteration 42/1000 | Loss: 0.00002878
Iteration 43/1000 | Loss: 0.00002878
Iteration 44/1000 | Loss: 0.00002877
Iteration 45/1000 | Loss: 0.00002877
Iteration 46/1000 | Loss: 0.00002877
Iteration 47/1000 | Loss: 0.00002877
Iteration 48/1000 | Loss: 0.00002877
Iteration 49/1000 | Loss: 0.00002877
Iteration 50/1000 | Loss: 0.00002877
Iteration 51/1000 | Loss: 0.00002877
Iteration 52/1000 | Loss: 0.00002877
Iteration 53/1000 | Loss: 0.00002876
Iteration 54/1000 | Loss: 0.00002876
Iteration 55/1000 | Loss: 0.00002876
Iteration 56/1000 | Loss: 0.00002876
Iteration 57/1000 | Loss: 0.00002876
Iteration 58/1000 | Loss: 0.00002876
Iteration 59/1000 | Loss: 0.00002875
Iteration 60/1000 | Loss: 0.00002875
Iteration 61/1000 | Loss: 0.00002875
Iteration 62/1000 | Loss: 0.00002874
Iteration 63/1000 | Loss: 0.00002873
Iteration 64/1000 | Loss: 0.00002872
Iteration 65/1000 | Loss: 0.00002872
Iteration 66/1000 | Loss: 0.00002872
Iteration 67/1000 | Loss: 0.00002871
Iteration 68/1000 | Loss: 0.00002871
Iteration 69/1000 | Loss: 0.00002871
Iteration 70/1000 | Loss: 0.00002871
Iteration 71/1000 | Loss: 0.00002871
Iteration 72/1000 | Loss: 0.00002870
Iteration 73/1000 | Loss: 0.00002870
Iteration 74/1000 | Loss: 0.00002869
Iteration 75/1000 | Loss: 0.00002868
Iteration 76/1000 | Loss: 0.00002868
Iteration 77/1000 | Loss: 0.00002868
Iteration 78/1000 | Loss: 0.00002867
Iteration 79/1000 | Loss: 0.00002867
Iteration 80/1000 | Loss: 0.00002867
Iteration 81/1000 | Loss: 0.00002867
Iteration 82/1000 | Loss: 0.00002867
Iteration 83/1000 | Loss: 0.00002866
Iteration 84/1000 | Loss: 0.00002866
Iteration 85/1000 | Loss: 0.00002865
Iteration 86/1000 | Loss: 0.00002865
Iteration 87/1000 | Loss: 0.00002865
Iteration 88/1000 | Loss: 0.00002865
Iteration 89/1000 | Loss: 0.00002864
Iteration 90/1000 | Loss: 0.00002864
Iteration 91/1000 | Loss: 0.00002864
Iteration 92/1000 | Loss: 0.00002864
Iteration 93/1000 | Loss: 0.00002864
Iteration 94/1000 | Loss: 0.00002864
Iteration 95/1000 | Loss: 0.00002864
Iteration 96/1000 | Loss: 0.00002864
Iteration 97/1000 | Loss: 0.00002864
Iteration 98/1000 | Loss: 0.00002864
Iteration 99/1000 | Loss: 0.00002864
Iteration 100/1000 | Loss: 0.00002864
Iteration 101/1000 | Loss: 0.00002864
Iteration 102/1000 | Loss: 0.00002864
Iteration 103/1000 | Loss: 0.00002864
Iteration 104/1000 | Loss: 0.00002863
Iteration 105/1000 | Loss: 0.00002863
Iteration 106/1000 | Loss: 0.00002863
Iteration 107/1000 | Loss: 0.00002863
Iteration 108/1000 | Loss: 0.00002863
Iteration 109/1000 | Loss: 0.00002863
Iteration 110/1000 | Loss: 0.00002863
Iteration 111/1000 | Loss: 0.00002863
Iteration 112/1000 | Loss: 0.00002863
Iteration 113/1000 | Loss: 0.00002863
Iteration 114/1000 | Loss: 0.00002863
Iteration 115/1000 | Loss: 0.00002863
Iteration 116/1000 | Loss: 0.00002863
Iteration 117/1000 | Loss: 0.00002863
Iteration 118/1000 | Loss: 0.00002863
Iteration 119/1000 | Loss: 0.00002863
Iteration 120/1000 | Loss: 0.00002863
Iteration 121/1000 | Loss: 0.00002863
Iteration 122/1000 | Loss: 0.00002863
Iteration 123/1000 | Loss: 0.00002863
Iteration 124/1000 | Loss: 0.00002863
Iteration 125/1000 | Loss: 0.00002863
Iteration 126/1000 | Loss: 0.00002863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.86332106043119e-05, 2.86332106043119e-05, 2.86332106043119e-05, 2.86332106043119e-05, 2.86332106043119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.86332106043119e-05

Optimization complete. Final v2v error: 4.392512798309326 mm

Highest mean error: 4.8014326095581055 mm for frame 171

Lowest mean error: 3.8873629570007324 mm for frame 0

Saving results

Total time: 42.90363359451294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731169
Iteration 2/25 | Loss: 0.00150033
Iteration 3/25 | Loss: 0.00103070
Iteration 4/25 | Loss: 0.00088689
Iteration 5/25 | Loss: 0.00084075
Iteration 6/25 | Loss: 0.00078186
Iteration 7/25 | Loss: 0.00074843
Iteration 8/25 | Loss: 0.00074459
Iteration 9/25 | Loss: 0.00071429
Iteration 10/25 | Loss: 0.00070145
Iteration 11/25 | Loss: 0.00069777
Iteration 12/25 | Loss: 0.00069600
Iteration 13/25 | Loss: 0.00069472
Iteration 14/25 | Loss: 0.00069395
Iteration 15/25 | Loss: 0.00069347
Iteration 16/25 | Loss: 0.00069285
Iteration 17/25 | Loss: 0.00069203
Iteration 18/25 | Loss: 0.00069052
Iteration 19/25 | Loss: 0.00073253
Iteration 20/25 | Loss: 0.00069999
Iteration 21/25 | Loss: 0.00065153
Iteration 22/25 | Loss: 0.00062159
Iteration 23/25 | Loss: 0.00061039
Iteration 24/25 | Loss: 0.00060800
Iteration 25/25 | Loss: 0.00060750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.73422384
Iteration 2/25 | Loss: 0.00072152
Iteration 3/25 | Loss: 0.00072138
Iteration 4/25 | Loss: 0.00072138
Iteration 5/25 | Loss: 0.00072138
Iteration 6/25 | Loss: 0.00072138
Iteration 7/25 | Loss: 0.00072138
Iteration 8/25 | Loss: 0.00072138
Iteration 9/25 | Loss: 0.00072138
Iteration 10/25 | Loss: 0.00072138
Iteration 11/25 | Loss: 0.00072138
Iteration 12/25 | Loss: 0.00072138
Iteration 13/25 | Loss: 0.00072138
Iteration 14/25 | Loss: 0.00072138
Iteration 15/25 | Loss: 0.00072138
Iteration 16/25 | Loss: 0.00072138
Iteration 17/25 | Loss: 0.00072138
Iteration 18/25 | Loss: 0.00072138
Iteration 19/25 | Loss: 0.00072138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007213785429485142, 0.0007213785429485142, 0.0007213785429485142, 0.0007213785429485142, 0.0007213785429485142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007213785429485142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072138
Iteration 2/1000 | Loss: 0.00011072
Iteration 3/1000 | Loss: 0.00037426
Iteration 4/1000 | Loss: 0.00033721
Iteration 5/1000 | Loss: 0.00043837
Iteration 6/1000 | Loss: 0.00032243
Iteration 7/1000 | Loss: 0.00057932
Iteration 8/1000 | Loss: 0.00028613
Iteration 9/1000 | Loss: 0.00003483
Iteration 10/1000 | Loss: 0.00002936
Iteration 11/1000 | Loss: 0.00002600
Iteration 12/1000 | Loss: 0.00002375
Iteration 13/1000 | Loss: 0.00002274
Iteration 14/1000 | Loss: 0.00002219
Iteration 15/1000 | Loss: 0.00002167
Iteration 16/1000 | Loss: 0.00037504
Iteration 17/1000 | Loss: 0.00002159
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001795
Iteration 23/1000 | Loss: 0.00001787
Iteration 24/1000 | Loss: 0.00001780
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001777
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001771
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001764
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001763
Iteration 37/1000 | Loss: 0.00001763
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001759
Iteration 42/1000 | Loss: 0.00001758
Iteration 43/1000 | Loss: 0.00001758
Iteration 44/1000 | Loss: 0.00001758
Iteration 45/1000 | Loss: 0.00001758
Iteration 46/1000 | Loss: 0.00001758
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001757
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001756
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001756
Iteration 59/1000 | Loss: 0.00001756
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001755
Iteration 62/1000 | Loss: 0.00001755
Iteration 63/1000 | Loss: 0.00001755
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001754
Iteration 67/1000 | Loss: 0.00001754
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001752
Iteration 88/1000 | Loss: 0.00001752
Iteration 89/1000 | Loss: 0.00001752
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001750
Iteration 96/1000 | Loss: 0.00001750
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001749
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001748
Iteration 105/1000 | Loss: 0.00001748
Iteration 106/1000 | Loss: 0.00001748
Iteration 107/1000 | Loss: 0.00001748
Iteration 108/1000 | Loss: 0.00001748
Iteration 109/1000 | Loss: 0.00001748
Iteration 110/1000 | Loss: 0.00001748
Iteration 111/1000 | Loss: 0.00001748
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001747
Iteration 114/1000 | Loss: 0.00001747
Iteration 115/1000 | Loss: 0.00001747
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001747
Iteration 119/1000 | Loss: 0.00001747
Iteration 120/1000 | Loss: 0.00001747
Iteration 121/1000 | Loss: 0.00001747
Iteration 122/1000 | Loss: 0.00001747
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.7465808923589066e-05, 1.7465808923589066e-05, 1.7465808923589066e-05, 1.7465808923589066e-05, 1.7465808923589066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7465808923589066e-05

Optimization complete. Final v2v error: 3.5727851390838623 mm

Highest mean error: 4.4310221672058105 mm for frame 7

Lowest mean error: 2.999520778656006 mm for frame 225

Saving results

Total time: 98.79434943199158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139960
Iteration 2/25 | Loss: 0.01139960
Iteration 3/25 | Loss: 0.00238796
Iteration 4/25 | Loss: 0.00184284
Iteration 5/25 | Loss: 0.00165285
Iteration 6/25 | Loss: 0.00205405
Iteration 7/25 | Loss: 0.00157733
Iteration 8/25 | Loss: 0.00103231
Iteration 9/25 | Loss: 0.00087177
Iteration 10/25 | Loss: 0.00082322
Iteration 11/25 | Loss: 0.00080595
Iteration 12/25 | Loss: 0.00079571
Iteration 13/25 | Loss: 0.00078604
Iteration 14/25 | Loss: 0.00078857
Iteration 15/25 | Loss: 0.00078302
Iteration 16/25 | Loss: 0.00078285
Iteration 17/25 | Loss: 0.00078102
Iteration 18/25 | Loss: 0.00077843
Iteration 19/25 | Loss: 0.00077619
Iteration 20/25 | Loss: 0.00077871
Iteration 21/25 | Loss: 0.00077911
Iteration 22/25 | Loss: 0.00077792
Iteration 23/25 | Loss: 0.00077670
Iteration 24/25 | Loss: 0.00077722
Iteration 25/25 | Loss: 0.00077753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24483013
Iteration 2/25 | Loss: 0.00037624
Iteration 3/25 | Loss: 0.00037623
Iteration 4/25 | Loss: 0.00037623
Iteration 5/25 | Loss: 0.00037623
Iteration 6/25 | Loss: 0.00037623
Iteration 7/25 | Loss: 0.00037623
Iteration 8/25 | Loss: 0.00037623
Iteration 9/25 | Loss: 0.00037623
Iteration 10/25 | Loss: 0.00037623
Iteration 11/25 | Loss: 0.00037623
Iteration 12/25 | Loss: 0.00037623
Iteration 13/25 | Loss: 0.00037623
Iteration 14/25 | Loss: 0.00037623
Iteration 15/25 | Loss: 0.00037623
Iteration 16/25 | Loss: 0.00037623
Iteration 17/25 | Loss: 0.00037623
Iteration 18/25 | Loss: 0.00037623
Iteration 19/25 | Loss: 0.00037623
Iteration 20/25 | Loss: 0.00037623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003762312699109316, 0.0003762312699109316, 0.0003762312699109316, 0.0003762312699109316, 0.0003762312699109316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003762312699109316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037623
Iteration 2/1000 | Loss: 0.00016843
Iteration 3/1000 | Loss: 0.00016668
Iteration 4/1000 | Loss: 0.00013488
Iteration 5/1000 | Loss: 0.00014473
Iteration 6/1000 | Loss: 0.00017435
Iteration 7/1000 | Loss: 0.00011855
Iteration 8/1000 | Loss: 0.00007927
Iteration 9/1000 | Loss: 0.00011917
Iteration 10/1000 | Loss: 0.00013939
Iteration 11/1000 | Loss: 0.00018420
Iteration 12/1000 | Loss: 0.00013098
Iteration 13/1000 | Loss: 0.00018857
Iteration 14/1000 | Loss: 0.00016914
Iteration 15/1000 | Loss: 0.00018447
Iteration 16/1000 | Loss: 0.00015097
Iteration 17/1000 | Loss: 0.00016653
Iteration 18/1000 | Loss: 0.00017322
Iteration 19/1000 | Loss: 0.00006758
Iteration 20/1000 | Loss: 0.00006924
Iteration 21/1000 | Loss: 0.00009358
Iteration 22/1000 | Loss: 0.00014952
Iteration 23/1000 | Loss: 0.00008691
Iteration 24/1000 | Loss: 0.00015240
Iteration 25/1000 | Loss: 0.00010194
Iteration 26/1000 | Loss: 0.00017268
Iteration 27/1000 | Loss: 0.00013921
Iteration 28/1000 | Loss: 0.00010499
Iteration 29/1000 | Loss: 0.00014374
Iteration 30/1000 | Loss: 0.00017701
Iteration 31/1000 | Loss: 0.00009876
Iteration 32/1000 | Loss: 0.00005938
Iteration 33/1000 | Loss: 0.00015440
Iteration 34/1000 | Loss: 0.00018587
Iteration 35/1000 | Loss: 0.00016602
Iteration 36/1000 | Loss: 0.00014728
Iteration 37/1000 | Loss: 0.00012277
Iteration 38/1000 | Loss: 0.00014148
Iteration 39/1000 | Loss: 0.00011844
Iteration 40/1000 | Loss: 0.00016999
Iteration 41/1000 | Loss: 0.00016189
Iteration 42/1000 | Loss: 0.00014840
Iteration 43/1000 | Loss: 0.00018610
Iteration 44/1000 | Loss: 0.00011826
Iteration 45/1000 | Loss: 0.00025299
Iteration 46/1000 | Loss: 0.00011723
Iteration 47/1000 | Loss: 0.00018906
Iteration 48/1000 | Loss: 0.00010038
Iteration 49/1000 | Loss: 0.00004977
Iteration 50/1000 | Loss: 0.00005030
Iteration 51/1000 | Loss: 0.00009944
Iteration 52/1000 | Loss: 0.00005375
Iteration 53/1000 | Loss: 0.00017920
Iteration 54/1000 | Loss: 0.00006317
Iteration 55/1000 | Loss: 0.00004723
Iteration 56/1000 | Loss: 0.00007678
Iteration 57/1000 | Loss: 0.00005623
Iteration 58/1000 | Loss: 0.00004514
Iteration 59/1000 | Loss: 0.00007243
Iteration 60/1000 | Loss: 0.00005427
Iteration 61/1000 | Loss: 0.00006078
Iteration 62/1000 | Loss: 0.00005299
Iteration 63/1000 | Loss: 0.00003747
Iteration 64/1000 | Loss: 0.00005022
Iteration 65/1000 | Loss: 0.00005307
Iteration 66/1000 | Loss: 0.00003748
Iteration 67/1000 | Loss: 0.00004473
Iteration 68/1000 | Loss: 0.00005521
Iteration 69/1000 | Loss: 0.00005858
Iteration 70/1000 | Loss: 0.00005414
Iteration 71/1000 | Loss: 0.00005236
Iteration 72/1000 | Loss: 0.00005026
Iteration 73/1000 | Loss: 0.00005811
Iteration 74/1000 | Loss: 0.00008016
Iteration 75/1000 | Loss: 0.00004329
Iteration 76/1000 | Loss: 0.00005214
Iteration 77/1000 | Loss: 0.00004779
Iteration 78/1000 | Loss: 0.00005460
Iteration 79/1000 | Loss: 0.00005136
Iteration 80/1000 | Loss: 0.00005283
Iteration 81/1000 | Loss: 0.00004869
Iteration 82/1000 | Loss: 0.00005745
Iteration 83/1000 | Loss: 0.00005430
Iteration 84/1000 | Loss: 0.00005055
Iteration 85/1000 | Loss: 0.00004725
Iteration 86/1000 | Loss: 0.00004680
Iteration 87/1000 | Loss: 0.00005648
Iteration 88/1000 | Loss: 0.00005245
Iteration 89/1000 | Loss: 0.00003406
Iteration 90/1000 | Loss: 0.00004397
Iteration 91/1000 | Loss: 0.00004780
Iteration 92/1000 | Loss: 0.00004364
Iteration 93/1000 | Loss: 0.00004357
Iteration 94/1000 | Loss: 0.00004261
Iteration 95/1000 | Loss: 0.00004529
Iteration 96/1000 | Loss: 0.00004186
Iteration 97/1000 | Loss: 0.00005888
Iteration 98/1000 | Loss: 0.00004135
Iteration 99/1000 | Loss: 0.00003507
Iteration 100/1000 | Loss: 0.00004470
Iteration 101/1000 | Loss: 0.00005698
Iteration 102/1000 | Loss: 0.00005883
Iteration 103/1000 | Loss: 0.00006456
Iteration 104/1000 | Loss: 0.00004572
Iteration 105/1000 | Loss: 0.00003773
Iteration 106/1000 | Loss: 0.00005298
Iteration 107/1000 | Loss: 0.00005261
Iteration 108/1000 | Loss: 0.00003935
Iteration 109/1000 | Loss: 0.00003571
Iteration 110/1000 | Loss: 0.00003395
Iteration 111/1000 | Loss: 0.00003326
Iteration 112/1000 | Loss: 0.00003259
Iteration 113/1000 | Loss: 0.00003207
Iteration 114/1000 | Loss: 0.00003176
Iteration 115/1000 | Loss: 0.00003162
Iteration 116/1000 | Loss: 0.00003160
Iteration 117/1000 | Loss: 0.00003146
Iteration 118/1000 | Loss: 0.00003141
Iteration 119/1000 | Loss: 0.00003141
Iteration 120/1000 | Loss: 0.00003133
Iteration 121/1000 | Loss: 0.00003131
Iteration 122/1000 | Loss: 0.00003129
Iteration 123/1000 | Loss: 0.00003127
Iteration 124/1000 | Loss: 0.00003127
Iteration 125/1000 | Loss: 0.00003126
Iteration 126/1000 | Loss: 0.00003124
Iteration 127/1000 | Loss: 0.00003124
Iteration 128/1000 | Loss: 0.00003123
Iteration 129/1000 | Loss: 0.00003122
Iteration 130/1000 | Loss: 0.00003122
Iteration 131/1000 | Loss: 0.00003121
Iteration 132/1000 | Loss: 0.00003121
Iteration 133/1000 | Loss: 0.00003121
Iteration 134/1000 | Loss: 0.00003119
Iteration 135/1000 | Loss: 0.00003118
Iteration 136/1000 | Loss: 0.00003118
Iteration 137/1000 | Loss: 0.00003118
Iteration 138/1000 | Loss: 0.00003118
Iteration 139/1000 | Loss: 0.00003117
Iteration 140/1000 | Loss: 0.00003117
Iteration 141/1000 | Loss: 0.00003117
Iteration 142/1000 | Loss: 0.00003116
Iteration 143/1000 | Loss: 0.00003112
Iteration 144/1000 | Loss: 0.00003112
Iteration 145/1000 | Loss: 0.00003112
Iteration 146/1000 | Loss: 0.00003111
Iteration 147/1000 | Loss: 0.00003111
Iteration 148/1000 | Loss: 0.00003110
Iteration 149/1000 | Loss: 0.00003110
Iteration 150/1000 | Loss: 0.00003110
Iteration 151/1000 | Loss: 0.00003110
Iteration 152/1000 | Loss: 0.00003110
Iteration 153/1000 | Loss: 0.00003109
Iteration 154/1000 | Loss: 0.00003109
Iteration 155/1000 | Loss: 0.00003109
Iteration 156/1000 | Loss: 0.00003109
Iteration 157/1000 | Loss: 0.00003109
Iteration 158/1000 | Loss: 0.00003109
Iteration 159/1000 | Loss: 0.00003109
Iteration 160/1000 | Loss: 0.00003109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.1093411962501705e-05, 3.1093411962501705e-05, 3.1093411962501705e-05, 3.1093411962501705e-05, 3.1093411962501705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1093411962501705e-05

Optimization complete. Final v2v error: 4.429668426513672 mm

Highest mean error: 5.443653583526611 mm for frame 4

Lowest mean error: 4.067752838134766 mm for frame 0

Saving results

Total time: 244.50518655776978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590863
Iteration 2/25 | Loss: 0.00111708
Iteration 3/25 | Loss: 0.00059846
Iteration 4/25 | Loss: 0.00059549
Iteration 5/25 | Loss: 0.00049832
Iteration 6/25 | Loss: 0.00049068
Iteration 7/25 | Loss: 0.00048990
Iteration 8/25 | Loss: 0.00048831
Iteration 9/25 | Loss: 0.00048775
Iteration 10/25 | Loss: 0.00048762
Iteration 11/25 | Loss: 0.00048749
Iteration 12/25 | Loss: 0.00048741
Iteration 13/25 | Loss: 0.00048741
Iteration 14/25 | Loss: 0.00048741
Iteration 15/25 | Loss: 0.00048741
Iteration 16/25 | Loss: 0.00048741
Iteration 17/25 | Loss: 0.00048741
Iteration 18/25 | Loss: 0.00048741
Iteration 19/25 | Loss: 0.00048741
Iteration 20/25 | Loss: 0.00048741
Iteration 21/25 | Loss: 0.00048741
Iteration 22/25 | Loss: 0.00048741
Iteration 23/25 | Loss: 0.00048740
Iteration 24/25 | Loss: 0.00048740
Iteration 25/25 | Loss: 0.00048740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.54314709
Iteration 2/25 | Loss: 0.00009020
Iteration 3/25 | Loss: 0.00009016
Iteration 4/25 | Loss: 0.00009016
Iteration 5/25 | Loss: 0.00009016
Iteration 6/25 | Loss: 0.00009016
Iteration 7/25 | Loss: 0.00009016
Iteration 8/25 | Loss: 0.00009016
Iteration 9/25 | Loss: 0.00009016
Iteration 10/25 | Loss: 0.00009016
Iteration 11/25 | Loss: 0.00009016
Iteration 12/25 | Loss: 0.00009016
Iteration 13/25 | Loss: 0.00009016
Iteration 14/25 | Loss: 0.00009016
Iteration 15/25 | Loss: 0.00009016
Iteration 16/25 | Loss: 0.00009016
Iteration 17/25 | Loss: 0.00009016
Iteration 18/25 | Loss: 0.00009016
Iteration 19/25 | Loss: 0.00009016
Iteration 20/25 | Loss: 0.00009016
Iteration 21/25 | Loss: 0.00009016
Iteration 22/25 | Loss: 0.00009016
Iteration 23/25 | Loss: 0.00009016
Iteration 24/25 | Loss: 0.00009016
Iteration 25/25 | Loss: 0.00009016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009016
Iteration 2/1000 | Loss: 0.00001754
Iteration 3/1000 | Loss: 0.00001479
Iteration 4/1000 | Loss: 0.00001401
Iteration 5/1000 | Loss: 0.00032811
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001230
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001107
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001080
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001068
Iteration 15/1000 | Loss: 0.00001066
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001026
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00000999
Iteration 25/1000 | Loss: 0.00000996
Iteration 26/1000 | Loss: 0.00000996
Iteration 27/1000 | Loss: 0.00000995
Iteration 28/1000 | Loss: 0.00000995
Iteration 29/1000 | Loss: 0.00000995
Iteration 30/1000 | Loss: 0.00000995
Iteration 31/1000 | Loss: 0.00000995
Iteration 32/1000 | Loss: 0.00000995
Iteration 33/1000 | Loss: 0.00000993
Iteration 34/1000 | Loss: 0.00000993
Iteration 35/1000 | Loss: 0.00000992
Iteration 36/1000 | Loss: 0.00000992
Iteration 37/1000 | Loss: 0.00000992
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000990
Iteration 40/1000 | Loss: 0.00000990
Iteration 41/1000 | Loss: 0.00000989
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000988
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000986
Iteration 47/1000 | Loss: 0.00000986
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000986
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000982
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000979
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000978
Iteration 67/1000 | Loss: 0.00000978
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000975
Iteration 74/1000 | Loss: 0.00000974
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000973
Iteration 77/1000 | Loss: 0.00000973
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000971
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000971
Iteration 86/1000 | Loss: 0.00000970
Iteration 87/1000 | Loss: 0.00000970
Iteration 88/1000 | Loss: 0.00000970
Iteration 89/1000 | Loss: 0.00000970
Iteration 90/1000 | Loss: 0.00000970
Iteration 91/1000 | Loss: 0.00000970
Iteration 92/1000 | Loss: 0.00000970
Iteration 93/1000 | Loss: 0.00000970
Iteration 94/1000 | Loss: 0.00000970
Iteration 95/1000 | Loss: 0.00000970
Iteration 96/1000 | Loss: 0.00000969
Iteration 97/1000 | Loss: 0.00000969
Iteration 98/1000 | Loss: 0.00000969
Iteration 99/1000 | Loss: 0.00000969
Iteration 100/1000 | Loss: 0.00000969
Iteration 101/1000 | Loss: 0.00000969
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000968
Iteration 110/1000 | Loss: 0.00000968
Iteration 111/1000 | Loss: 0.00000968
Iteration 112/1000 | Loss: 0.00000968
Iteration 113/1000 | Loss: 0.00000968
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000968
Iteration 116/1000 | Loss: 0.00000968
Iteration 117/1000 | Loss: 0.00000968
Iteration 118/1000 | Loss: 0.00000968
Iteration 119/1000 | Loss: 0.00000968
Iteration 120/1000 | Loss: 0.00000968
Iteration 121/1000 | Loss: 0.00000968
Iteration 122/1000 | Loss: 0.00000968
Iteration 123/1000 | Loss: 0.00000968
Iteration 124/1000 | Loss: 0.00000968
Iteration 125/1000 | Loss: 0.00000968
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000968
Iteration 143/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [9.676577064965386e-06, 9.676577064965386e-06, 9.676577064965386e-06, 9.676577064965386e-06, 9.676577064965386e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.676577064965386e-06

Optimization complete. Final v2v error: 2.652639389038086 mm

Highest mean error: 4.108615398406982 mm for frame 209

Lowest mean error: 2.2056069374084473 mm for frame 127

Saving results

Total time: 59.03628730773926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820565
Iteration 2/25 | Loss: 0.00101816
Iteration 3/25 | Loss: 0.00069070
Iteration 4/25 | Loss: 0.00062543
Iteration 5/25 | Loss: 0.00060846
Iteration 6/25 | Loss: 0.00060353
Iteration 7/25 | Loss: 0.00060224
Iteration 8/25 | Loss: 0.00060209
Iteration 9/25 | Loss: 0.00060209
Iteration 10/25 | Loss: 0.00060209
Iteration 11/25 | Loss: 0.00060209
Iteration 12/25 | Loss: 0.00060209
Iteration 13/25 | Loss: 0.00060209
Iteration 14/25 | Loss: 0.00060209
Iteration 15/25 | Loss: 0.00060209
Iteration 16/25 | Loss: 0.00060209
Iteration 17/25 | Loss: 0.00060209
Iteration 18/25 | Loss: 0.00060209
Iteration 19/25 | Loss: 0.00060209
Iteration 20/25 | Loss: 0.00060209
Iteration 21/25 | Loss: 0.00060209
Iteration 22/25 | Loss: 0.00060209
Iteration 23/25 | Loss: 0.00060209
Iteration 24/25 | Loss: 0.00060209
Iteration 25/25 | Loss: 0.00060209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36096287
Iteration 2/25 | Loss: 0.00010776
Iteration 3/25 | Loss: 0.00010776
Iteration 4/25 | Loss: 0.00010776
Iteration 5/25 | Loss: 0.00010776
Iteration 6/25 | Loss: 0.00010776
Iteration 7/25 | Loss: 0.00010776
Iteration 8/25 | Loss: 0.00010776
Iteration 9/25 | Loss: 0.00010776
Iteration 10/25 | Loss: 0.00010776
Iteration 11/25 | Loss: 0.00010775
Iteration 12/25 | Loss: 0.00010775
Iteration 13/25 | Loss: 0.00010775
Iteration 14/25 | Loss: 0.00010775
Iteration 15/25 | Loss: 0.00010775
Iteration 16/25 | Loss: 0.00010775
Iteration 17/25 | Loss: 0.00010775
Iteration 18/25 | Loss: 0.00010775
Iteration 19/25 | Loss: 0.00010775
Iteration 20/25 | Loss: 0.00010775
Iteration 21/25 | Loss: 0.00010775
Iteration 22/25 | Loss: 0.00010775
Iteration 23/25 | Loss: 0.00010775
Iteration 24/25 | Loss: 0.00010775
Iteration 25/25 | Loss: 0.00010775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010775
Iteration 2/1000 | Loss: 0.00003118
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001830
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001789
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001783
Iteration 21/1000 | Loss: 0.00001782
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001780
Iteration 24/1000 | Loss: 0.00001780
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001778
Iteration 29/1000 | Loss: 0.00001778
Iteration 30/1000 | Loss: 0.00001777
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001777
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001775
Iteration 35/1000 | Loss: 0.00001775
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001773
Iteration 43/1000 | Loss: 0.00001772
Iteration 44/1000 | Loss: 0.00001772
Iteration 45/1000 | Loss: 0.00001772
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001771
Iteration 53/1000 | Loss: 0.00001771
Iteration 54/1000 | Loss: 0.00001770
Iteration 55/1000 | Loss: 0.00001770
Iteration 56/1000 | Loss: 0.00001770
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001770
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001770
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001769
Iteration 68/1000 | Loss: 0.00001769
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001769
Iteration 71/1000 | Loss: 0.00001769
Iteration 72/1000 | Loss: 0.00001769
Iteration 73/1000 | Loss: 0.00001769
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001768
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001768
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001766
Iteration 90/1000 | Loss: 0.00001766
Iteration 91/1000 | Loss: 0.00001766
Iteration 92/1000 | Loss: 0.00001766
Iteration 93/1000 | Loss: 0.00001766
Iteration 94/1000 | Loss: 0.00001766
Iteration 95/1000 | Loss: 0.00001766
Iteration 96/1000 | Loss: 0.00001766
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001765
Iteration 99/1000 | Loss: 0.00001765
Iteration 100/1000 | Loss: 0.00001765
Iteration 101/1000 | Loss: 0.00001765
Iteration 102/1000 | Loss: 0.00001765
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001765
Iteration 105/1000 | Loss: 0.00001765
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001764
Iteration 110/1000 | Loss: 0.00001764
Iteration 111/1000 | Loss: 0.00001764
Iteration 112/1000 | Loss: 0.00001764
Iteration 113/1000 | Loss: 0.00001764
Iteration 114/1000 | Loss: 0.00001764
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001763
Iteration 125/1000 | Loss: 0.00001763
Iteration 126/1000 | Loss: 0.00001763
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001763
Iteration 131/1000 | Loss: 0.00001763
Iteration 132/1000 | Loss: 0.00001763
Iteration 133/1000 | Loss: 0.00001763
Iteration 134/1000 | Loss: 0.00001763
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001763
Iteration 138/1000 | Loss: 0.00001763
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001763
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001762
Iteration 146/1000 | Loss: 0.00001762
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001762
Iteration 149/1000 | Loss: 0.00001762
Iteration 150/1000 | Loss: 0.00001762
Iteration 151/1000 | Loss: 0.00001762
Iteration 152/1000 | Loss: 0.00001762
Iteration 153/1000 | Loss: 0.00001762
Iteration 154/1000 | Loss: 0.00001762
Iteration 155/1000 | Loss: 0.00001762
Iteration 156/1000 | Loss: 0.00001762
Iteration 157/1000 | Loss: 0.00001761
Iteration 158/1000 | Loss: 0.00001761
Iteration 159/1000 | Loss: 0.00001761
Iteration 160/1000 | Loss: 0.00001761
Iteration 161/1000 | Loss: 0.00001761
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001760
Iteration 167/1000 | Loss: 0.00001760
Iteration 168/1000 | Loss: 0.00001760
Iteration 169/1000 | Loss: 0.00001760
Iteration 170/1000 | Loss: 0.00001760
Iteration 171/1000 | Loss: 0.00001760
Iteration 172/1000 | Loss: 0.00001760
Iteration 173/1000 | Loss: 0.00001760
Iteration 174/1000 | Loss: 0.00001760
Iteration 175/1000 | Loss: 0.00001759
Iteration 176/1000 | Loss: 0.00001759
Iteration 177/1000 | Loss: 0.00001759
Iteration 178/1000 | Loss: 0.00001759
Iteration 179/1000 | Loss: 0.00001759
Iteration 180/1000 | Loss: 0.00001759
Iteration 181/1000 | Loss: 0.00001759
Iteration 182/1000 | Loss: 0.00001759
Iteration 183/1000 | Loss: 0.00001759
Iteration 184/1000 | Loss: 0.00001759
Iteration 185/1000 | Loss: 0.00001759
Iteration 186/1000 | Loss: 0.00001759
Iteration 187/1000 | Loss: 0.00001759
Iteration 188/1000 | Loss: 0.00001759
Iteration 189/1000 | Loss: 0.00001759
Iteration 190/1000 | Loss: 0.00001759
Iteration 191/1000 | Loss: 0.00001759
Iteration 192/1000 | Loss: 0.00001759
Iteration 193/1000 | Loss: 0.00001759
Iteration 194/1000 | Loss: 0.00001759
Iteration 195/1000 | Loss: 0.00001759
Iteration 196/1000 | Loss: 0.00001759
Iteration 197/1000 | Loss: 0.00001759
Iteration 198/1000 | Loss: 0.00001759
Iteration 199/1000 | Loss: 0.00001759
Iteration 200/1000 | Loss: 0.00001759
Iteration 201/1000 | Loss: 0.00001759
Iteration 202/1000 | Loss: 0.00001759
Iteration 203/1000 | Loss: 0.00001759
Iteration 204/1000 | Loss: 0.00001759
Iteration 205/1000 | Loss: 0.00001759
Iteration 206/1000 | Loss: 0.00001759
Iteration 207/1000 | Loss: 0.00001759
Iteration 208/1000 | Loss: 0.00001759
Iteration 209/1000 | Loss: 0.00001759
Iteration 210/1000 | Loss: 0.00001759
Iteration 211/1000 | Loss: 0.00001759
Iteration 212/1000 | Loss: 0.00001759
Iteration 213/1000 | Loss: 0.00001759
Iteration 214/1000 | Loss: 0.00001759
Iteration 215/1000 | Loss: 0.00001759
Iteration 216/1000 | Loss: 0.00001759
Iteration 217/1000 | Loss: 0.00001759
Iteration 218/1000 | Loss: 0.00001759
Iteration 219/1000 | Loss: 0.00001759
Iteration 220/1000 | Loss: 0.00001759
Iteration 221/1000 | Loss: 0.00001759
Iteration 222/1000 | Loss: 0.00001759
Iteration 223/1000 | Loss: 0.00001759
Iteration 224/1000 | Loss: 0.00001759
Iteration 225/1000 | Loss: 0.00001759
Iteration 226/1000 | Loss: 0.00001759
Iteration 227/1000 | Loss: 0.00001759
Iteration 228/1000 | Loss: 0.00001759
Iteration 229/1000 | Loss: 0.00001759
Iteration 230/1000 | Loss: 0.00001759
Iteration 231/1000 | Loss: 0.00001759
Iteration 232/1000 | Loss: 0.00001759
Iteration 233/1000 | Loss: 0.00001759
Iteration 234/1000 | Loss: 0.00001759
Iteration 235/1000 | Loss: 0.00001759
Iteration 236/1000 | Loss: 0.00001759
Iteration 237/1000 | Loss: 0.00001759
Iteration 238/1000 | Loss: 0.00001759
Iteration 239/1000 | Loss: 0.00001759
Iteration 240/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.7588536138646305e-05, 1.7588536138646305e-05, 1.7588536138646305e-05, 1.7588536138646305e-05, 1.7588536138646305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7588536138646305e-05

Optimization complete. Final v2v error: 3.507694959640503 mm

Highest mean error: 3.820427417755127 mm for frame 27

Lowest mean error: 3.178398847579956 mm for frame 124

Saving results

Total time: 41.3151319026947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_0514/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_0514/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524846
Iteration 2/25 | Loss: 0.00134391
Iteration 3/25 | Loss: 0.00080678
Iteration 4/25 | Loss: 0.00065908
Iteration 5/25 | Loss: 0.00063353
Iteration 6/25 | Loss: 0.00062454
Iteration 7/25 | Loss: 0.00062185
Iteration 8/25 | Loss: 0.00062098
Iteration 9/25 | Loss: 0.00062060
Iteration 10/25 | Loss: 0.00062055
Iteration 11/25 | Loss: 0.00062055
Iteration 12/25 | Loss: 0.00062055
Iteration 13/25 | Loss: 0.00062055
Iteration 14/25 | Loss: 0.00062055
Iteration 15/25 | Loss: 0.00062055
Iteration 16/25 | Loss: 0.00062055
Iteration 17/25 | Loss: 0.00062055
Iteration 18/25 | Loss: 0.00062055
Iteration 19/25 | Loss: 0.00062055
Iteration 20/25 | Loss: 0.00062055
Iteration 21/25 | Loss: 0.00062055
Iteration 22/25 | Loss: 0.00062055
Iteration 23/25 | Loss: 0.00062055
Iteration 24/25 | Loss: 0.00062055
Iteration 25/25 | Loss: 0.00062055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41173637
Iteration 2/25 | Loss: 0.00015923
Iteration 3/25 | Loss: 0.00015921
Iteration 4/25 | Loss: 0.00015920
Iteration 5/25 | Loss: 0.00015920
Iteration 6/25 | Loss: 0.00015920
Iteration 7/25 | Loss: 0.00015920
Iteration 8/25 | Loss: 0.00015920
Iteration 9/25 | Loss: 0.00015920
Iteration 10/25 | Loss: 0.00015920
Iteration 11/25 | Loss: 0.00015920
Iteration 12/25 | Loss: 0.00015920
Iteration 13/25 | Loss: 0.00015920
Iteration 14/25 | Loss: 0.00015920
Iteration 15/25 | Loss: 0.00015920
Iteration 16/25 | Loss: 0.00015920
Iteration 17/25 | Loss: 0.00015920
Iteration 18/25 | Loss: 0.00015920
Iteration 19/25 | Loss: 0.00015920
Iteration 20/25 | Loss: 0.00015920
Iteration 21/25 | Loss: 0.00015920
Iteration 22/25 | Loss: 0.00015920
Iteration 23/25 | Loss: 0.00015920
Iteration 24/25 | Loss: 0.00015920
Iteration 25/25 | Loss: 0.00015920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015920
Iteration 2/1000 | Loss: 0.00003474
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002500
Iteration 5/1000 | Loss: 0.00002433
Iteration 6/1000 | Loss: 0.00002370
Iteration 7/1000 | Loss: 0.00002326
Iteration 8/1000 | Loss: 0.00002280
Iteration 9/1000 | Loss: 0.00002246
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002219
Iteration 12/1000 | Loss: 0.00002216
Iteration 13/1000 | Loss: 0.00002208
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002202
Iteration 16/1000 | Loss: 0.00002202
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002201
Iteration 19/1000 | Loss: 0.00002198
Iteration 20/1000 | Loss: 0.00002198
Iteration 21/1000 | Loss: 0.00002195
Iteration 22/1000 | Loss: 0.00002194
Iteration 23/1000 | Loss: 0.00002194
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002190
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002189
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002188
Iteration 31/1000 | Loss: 0.00002187
Iteration 32/1000 | Loss: 0.00002187
Iteration 33/1000 | Loss: 0.00002187
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00002185
Iteration 36/1000 | Loss: 0.00002185
Iteration 37/1000 | Loss: 0.00002185
Iteration 38/1000 | Loss: 0.00002184
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002183
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002180
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002178
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002177
Iteration 59/1000 | Loss: 0.00002177
Iteration 60/1000 | Loss: 0.00002177
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002174
Iteration 68/1000 | Loss: 0.00002174
Iteration 69/1000 | Loss: 0.00002174
Iteration 70/1000 | Loss: 0.00002173
Iteration 71/1000 | Loss: 0.00002173
Iteration 72/1000 | Loss: 0.00002173
Iteration 73/1000 | Loss: 0.00002172
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002171
Iteration 78/1000 | Loss: 0.00002171
Iteration 79/1000 | Loss: 0.00002171
Iteration 80/1000 | Loss: 0.00002170
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002170
Iteration 83/1000 | Loss: 0.00002170
Iteration 84/1000 | Loss: 0.00002170
Iteration 85/1000 | Loss: 0.00002169
Iteration 86/1000 | Loss: 0.00002169
Iteration 87/1000 | Loss: 0.00002169
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002168
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002168
Iteration 93/1000 | Loss: 0.00002168
Iteration 94/1000 | Loss: 0.00002168
Iteration 95/1000 | Loss: 0.00002168
Iteration 96/1000 | Loss: 0.00002167
Iteration 97/1000 | Loss: 0.00002167
Iteration 98/1000 | Loss: 0.00002167
Iteration 99/1000 | Loss: 0.00002167
Iteration 100/1000 | Loss: 0.00002167
Iteration 101/1000 | Loss: 0.00002166
Iteration 102/1000 | Loss: 0.00002166
Iteration 103/1000 | Loss: 0.00002166
Iteration 104/1000 | Loss: 0.00002166
Iteration 105/1000 | Loss: 0.00002165
Iteration 106/1000 | Loss: 0.00002165
Iteration 107/1000 | Loss: 0.00002165
Iteration 108/1000 | Loss: 0.00002165
Iteration 109/1000 | Loss: 0.00002165
Iteration 110/1000 | Loss: 0.00002165
Iteration 111/1000 | Loss: 0.00002164
Iteration 112/1000 | Loss: 0.00002164
Iteration 113/1000 | Loss: 0.00002164
Iteration 114/1000 | Loss: 0.00002163
Iteration 115/1000 | Loss: 0.00002163
Iteration 116/1000 | Loss: 0.00002163
Iteration 117/1000 | Loss: 0.00002163
Iteration 118/1000 | Loss: 0.00002163
Iteration 119/1000 | Loss: 0.00002163
Iteration 120/1000 | Loss: 0.00002163
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002163
Iteration 123/1000 | Loss: 0.00002162
Iteration 124/1000 | Loss: 0.00002162
Iteration 125/1000 | Loss: 0.00002162
Iteration 126/1000 | Loss: 0.00002162
Iteration 127/1000 | Loss: 0.00002162
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002161
Iteration 132/1000 | Loss: 0.00002161
Iteration 133/1000 | Loss: 0.00002161
Iteration 134/1000 | Loss: 0.00002161
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002160
Iteration 137/1000 | Loss: 0.00002160
Iteration 138/1000 | Loss: 0.00002160
Iteration 139/1000 | Loss: 0.00002160
Iteration 140/1000 | Loss: 0.00002160
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002159
Iteration 147/1000 | Loss: 0.00002159
Iteration 148/1000 | Loss: 0.00002159
Iteration 149/1000 | Loss: 0.00002159
Iteration 150/1000 | Loss: 0.00002159
Iteration 151/1000 | Loss: 0.00002159
Iteration 152/1000 | Loss: 0.00002158
Iteration 153/1000 | Loss: 0.00002158
Iteration 154/1000 | Loss: 0.00002158
Iteration 155/1000 | Loss: 0.00002158
Iteration 156/1000 | Loss: 0.00002158
Iteration 157/1000 | Loss: 0.00002158
Iteration 158/1000 | Loss: 0.00002158
Iteration 159/1000 | Loss: 0.00002157
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002157
Iteration 165/1000 | Loss: 0.00002157
Iteration 166/1000 | Loss: 0.00002157
Iteration 167/1000 | Loss: 0.00002157
Iteration 168/1000 | Loss: 0.00002157
Iteration 169/1000 | Loss: 0.00002157
Iteration 170/1000 | Loss: 0.00002157
Iteration 171/1000 | Loss: 0.00002157
Iteration 172/1000 | Loss: 0.00002157
Iteration 173/1000 | Loss: 0.00002157
Iteration 174/1000 | Loss: 0.00002157
Iteration 175/1000 | Loss: 0.00002157
Iteration 176/1000 | Loss: 0.00002157
Iteration 177/1000 | Loss: 0.00002157
Iteration 178/1000 | Loss: 0.00002157
Iteration 179/1000 | Loss: 0.00002157
Iteration 180/1000 | Loss: 0.00002157
Iteration 181/1000 | Loss: 0.00002157
Iteration 182/1000 | Loss: 0.00002157
Iteration 183/1000 | Loss: 0.00002157
Iteration 184/1000 | Loss: 0.00002157
Iteration 185/1000 | Loss: 0.00002157
Iteration 186/1000 | Loss: 0.00002157
Iteration 187/1000 | Loss: 0.00002157
Iteration 188/1000 | Loss: 0.00002157
Iteration 189/1000 | Loss: 0.00002157
Iteration 190/1000 | Loss: 0.00002157
Iteration 191/1000 | Loss: 0.00002157
Iteration 192/1000 | Loss: 0.00002157
Iteration 193/1000 | Loss: 0.00002157
Iteration 194/1000 | Loss: 0.00002157
Iteration 195/1000 | Loss: 0.00002157
Iteration 196/1000 | Loss: 0.00002157
Iteration 197/1000 | Loss: 0.00002157
Iteration 198/1000 | Loss: 0.00002157
Iteration 199/1000 | Loss: 0.00002157
Iteration 200/1000 | Loss: 0.00002157
Iteration 201/1000 | Loss: 0.00002157
Iteration 202/1000 | Loss: 0.00002157
Iteration 203/1000 | Loss: 0.00002157
Iteration 204/1000 | Loss: 0.00002157
Iteration 205/1000 | Loss: 0.00002157
Iteration 206/1000 | Loss: 0.00002157
Iteration 207/1000 | Loss: 0.00002157
Iteration 208/1000 | Loss: 0.00002157
Iteration 209/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.1567651856457815e-05, 2.1567651856457815e-05, 2.1567651856457815e-05, 2.1567651856457815e-05, 2.1567651856457815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1567651856457815e-05

Optimization complete. Final v2v error: 3.7414042949676514 mm

Highest mean error: 5.9253621101379395 mm for frame 59

Lowest mean error: 2.53452205657959 mm for frame 0

Saving results

Total time: 47.07803273200989
