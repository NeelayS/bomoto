Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=187, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10472-10527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487936
Iteration 2/25 | Loss: 0.00159030
Iteration 3/25 | Loss: 0.00151807
Iteration 4/25 | Loss: 0.00150264
Iteration 5/25 | Loss: 0.00149744
Iteration 6/25 | Loss: 0.00149612
Iteration 7/25 | Loss: 0.00149612
Iteration 8/25 | Loss: 0.00149612
Iteration 9/25 | Loss: 0.00149612
Iteration 10/25 | Loss: 0.00149612
Iteration 11/25 | Loss: 0.00149612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014961239648982882, 0.0014961239648982882, 0.0014961239648982882, 0.0014961239648982882, 0.0014961239648982882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014961239648982882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.15338278
Iteration 2/25 | Loss: 0.00283585
Iteration 3/25 | Loss: 0.00283584
Iteration 4/25 | Loss: 0.00283584
Iteration 5/25 | Loss: 0.00283584
Iteration 6/25 | Loss: 0.00283584
Iteration 7/25 | Loss: 0.00283584
Iteration 8/25 | Loss: 0.00283584
Iteration 9/25 | Loss: 0.00283584
Iteration 10/25 | Loss: 0.00283584
Iteration 11/25 | Loss: 0.00283584
Iteration 12/25 | Loss: 0.00283583
Iteration 13/25 | Loss: 0.00283583
Iteration 14/25 | Loss: 0.00283583
Iteration 15/25 | Loss: 0.00283583
Iteration 16/25 | Loss: 0.00283583
Iteration 17/25 | Loss: 0.00283583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028358346316963434, 0.0028358346316963434, 0.0028358346316963434, 0.0028358346316963434, 0.0028358346316963434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028358346316963434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00283583
Iteration 2/1000 | Loss: 0.00004140
Iteration 3/1000 | Loss: 0.00003038
Iteration 4/1000 | Loss: 0.00002662
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002428
Iteration 7/1000 | Loss: 0.00002362
Iteration 8/1000 | Loss: 0.00002311
Iteration 9/1000 | Loss: 0.00002272
Iteration 10/1000 | Loss: 0.00002239
Iteration 11/1000 | Loss: 0.00002198
Iteration 12/1000 | Loss: 0.00002164
Iteration 13/1000 | Loss: 0.00002142
Iteration 14/1000 | Loss: 0.00002112
Iteration 15/1000 | Loss: 0.00002089
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002079
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002069
Iteration 20/1000 | Loss: 0.00002068
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002057
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002053
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002048
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002047
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002047
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002042
Iteration 49/1000 | Loss: 0.00002042
Iteration 50/1000 | Loss: 0.00002042
Iteration 51/1000 | Loss: 0.00002041
Iteration 52/1000 | Loss: 0.00002040
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002036
Iteration 56/1000 | Loss: 0.00002036
Iteration 57/1000 | Loss: 0.00002036
Iteration 58/1000 | Loss: 0.00002036
Iteration 59/1000 | Loss: 0.00002035
Iteration 60/1000 | Loss: 0.00002035
Iteration 61/1000 | Loss: 0.00002035
Iteration 62/1000 | Loss: 0.00002034
Iteration 63/1000 | Loss: 0.00002034
Iteration 64/1000 | Loss: 0.00002034
Iteration 65/1000 | Loss: 0.00002034
Iteration 66/1000 | Loss: 0.00002033
Iteration 67/1000 | Loss: 0.00002033
Iteration 68/1000 | Loss: 0.00002033
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00002032
Iteration 71/1000 | Loss: 0.00002032
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002032
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002031
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002031
Iteration 82/1000 | Loss: 0.00002031
Iteration 83/1000 | Loss: 0.00002031
Iteration 84/1000 | Loss: 0.00002031
Iteration 85/1000 | Loss: 0.00002031
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002030
Iteration 90/1000 | Loss: 0.00002030
Iteration 91/1000 | Loss: 0.00002030
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002028
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002025
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002025
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002024
Iteration 130/1000 | Loss: 0.00002024
Iteration 131/1000 | Loss: 0.00002024
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002024
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00002023
Iteration 137/1000 | Loss: 0.00002023
Iteration 138/1000 | Loss: 0.00002023
Iteration 139/1000 | Loss: 0.00002023
Iteration 140/1000 | Loss: 0.00002023
Iteration 141/1000 | Loss: 0.00002023
Iteration 142/1000 | Loss: 0.00002023
Iteration 143/1000 | Loss: 0.00002023
Iteration 144/1000 | Loss: 0.00002023
Iteration 145/1000 | Loss: 0.00002023
Iteration 146/1000 | Loss: 0.00002023
Iteration 147/1000 | Loss: 0.00002023
Iteration 148/1000 | Loss: 0.00002023
Iteration 149/1000 | Loss: 0.00002023
Iteration 150/1000 | Loss: 0.00002023
Iteration 151/1000 | Loss: 0.00002022
Iteration 152/1000 | Loss: 0.00002022
Iteration 153/1000 | Loss: 0.00002022
Iteration 154/1000 | Loss: 0.00002022
Iteration 155/1000 | Loss: 0.00002022
Iteration 156/1000 | Loss: 0.00002021
Iteration 157/1000 | Loss: 0.00002021
Iteration 158/1000 | Loss: 0.00002021
Iteration 159/1000 | Loss: 0.00002021
Iteration 160/1000 | Loss: 0.00002021
Iteration 161/1000 | Loss: 0.00002021
Iteration 162/1000 | Loss: 0.00002021
Iteration 163/1000 | Loss: 0.00002021
Iteration 164/1000 | Loss: 0.00002021
Iteration 165/1000 | Loss: 0.00002021
Iteration 166/1000 | Loss: 0.00002021
Iteration 167/1000 | Loss: 0.00002020
Iteration 168/1000 | Loss: 0.00002020
Iteration 169/1000 | Loss: 0.00002020
Iteration 170/1000 | Loss: 0.00002020
Iteration 171/1000 | Loss: 0.00002020
Iteration 172/1000 | Loss: 0.00002020
Iteration 173/1000 | Loss: 0.00002019
Iteration 174/1000 | Loss: 0.00002019
Iteration 175/1000 | Loss: 0.00002019
Iteration 176/1000 | Loss: 0.00002019
Iteration 177/1000 | Loss: 0.00002019
Iteration 178/1000 | Loss: 0.00002019
Iteration 179/1000 | Loss: 0.00002019
Iteration 180/1000 | Loss: 0.00002019
Iteration 181/1000 | Loss: 0.00002019
Iteration 182/1000 | Loss: 0.00002019
Iteration 183/1000 | Loss: 0.00002019
Iteration 184/1000 | Loss: 0.00002019
Iteration 185/1000 | Loss: 0.00002019
Iteration 186/1000 | Loss: 0.00002019
Iteration 187/1000 | Loss: 0.00002019
Iteration 188/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.0187173504382372e-05, 2.0187173504382372e-05, 2.0187173504382372e-05, 2.0187173504382372e-05, 2.0187173504382372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0187173504382372e-05

Optimization complete. Final v2v error: 3.824068307876587 mm

Highest mean error: 4.352639675140381 mm for frame 17

Lowest mean error: 3.2189197540283203 mm for frame 2

Saving results

Total time: 47.33671832084656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976146
Iteration 2/25 | Loss: 0.00255977
Iteration 3/25 | Loss: 0.00181911
Iteration 4/25 | Loss: 0.00170878
Iteration 5/25 | Loss: 0.00168514
Iteration 6/25 | Loss: 0.00163173
Iteration 7/25 | Loss: 0.00157345
Iteration 8/25 | Loss: 0.00156420
Iteration 9/25 | Loss: 0.00155914
Iteration 10/25 | Loss: 0.00155271
Iteration 11/25 | Loss: 0.00154341
Iteration 12/25 | Loss: 0.00154579
Iteration 13/25 | Loss: 0.00154460
Iteration 14/25 | Loss: 0.00154279
Iteration 15/25 | Loss: 0.00153988
Iteration 16/25 | Loss: 0.00153992
Iteration 17/25 | Loss: 0.00154242
Iteration 18/25 | Loss: 0.00154288
Iteration 19/25 | Loss: 0.00154470
Iteration 20/25 | Loss: 0.00154283
Iteration 21/25 | Loss: 0.00153836
Iteration 22/25 | Loss: 0.00153692
Iteration 23/25 | Loss: 0.00153681
Iteration 24/25 | Loss: 0.00153676
Iteration 25/25 | Loss: 0.00153676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38304961
Iteration 2/25 | Loss: 0.00245010
Iteration 3/25 | Loss: 0.00239278
Iteration 4/25 | Loss: 0.00239278
Iteration 5/25 | Loss: 0.00239278
Iteration 6/25 | Loss: 0.00239278
Iteration 7/25 | Loss: 0.00239278
Iteration 8/25 | Loss: 0.00239278
Iteration 9/25 | Loss: 0.00239278
Iteration 10/25 | Loss: 0.00239278
Iteration 11/25 | Loss: 0.00239278
Iteration 12/25 | Loss: 0.00239278
Iteration 13/25 | Loss: 0.00239278
Iteration 14/25 | Loss: 0.00239278
Iteration 15/25 | Loss: 0.00239278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002392775844782591, 0.002392775844782591, 0.002392775844782591, 0.002392775844782591, 0.002392775844782591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002392775844782591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239278
Iteration 2/1000 | Loss: 0.00008381
Iteration 3/1000 | Loss: 0.00007445
Iteration 4/1000 | Loss: 0.00003630
Iteration 5/1000 | Loss: 0.00005135
Iteration 6/1000 | Loss: 0.00055594
Iteration 7/1000 | Loss: 0.00005862
Iteration 8/1000 | Loss: 0.00012838
Iteration 9/1000 | Loss: 0.00004463
Iteration 10/1000 | Loss: 0.00002847
Iteration 11/1000 | Loss: 0.00003127
Iteration 12/1000 | Loss: 0.00002471
Iteration 13/1000 | Loss: 0.00004949
Iteration 14/1000 | Loss: 0.00003663
Iteration 15/1000 | Loss: 0.00004224
Iteration 16/1000 | Loss: 0.00002156
Iteration 17/1000 | Loss: 0.00003500
Iteration 18/1000 | Loss: 0.00002917
Iteration 19/1000 | Loss: 0.00003496
Iteration 20/1000 | Loss: 0.00002088
Iteration 21/1000 | Loss: 0.00002268
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00005823
Iteration 24/1000 | Loss: 0.00002561
Iteration 25/1000 | Loss: 0.00001998
Iteration 26/1000 | Loss: 0.00002939
Iteration 27/1000 | Loss: 0.00002001
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001977
Iteration 36/1000 | Loss: 0.00002372
Iteration 37/1000 | Loss: 0.00003440
Iteration 38/1000 | Loss: 0.00002727
Iteration 39/1000 | Loss: 0.00002588
Iteration 40/1000 | Loss: 0.00009087
Iteration 41/1000 | Loss: 0.00003476
Iteration 42/1000 | Loss: 0.00002134
Iteration 43/1000 | Loss: 0.00002151
Iteration 44/1000 | Loss: 0.00002840
Iteration 45/1000 | Loss: 0.00001949
Iteration 46/1000 | Loss: 0.00001949
Iteration 47/1000 | Loss: 0.00001949
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001947
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00002610
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00001957
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.9372579117771238e-05, 1.9372579117771238e-05, 1.9372579117771238e-05, 1.9372579117771238e-05, 1.9372579117771238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9372579117771238e-05

Optimization complete. Final v2v error: 3.5547189712524414 mm

Highest mean error: 10.59791374206543 mm for frame 182

Lowest mean error: 3.1537394523620605 mm for frame 10

Saving results

Total time: 106.88617134094238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559516
Iteration 2/25 | Loss: 0.00152029
Iteration 3/25 | Loss: 0.00146304
Iteration 4/25 | Loss: 0.00145575
Iteration 5/25 | Loss: 0.00145289
Iteration 6/25 | Loss: 0.00145276
Iteration 7/25 | Loss: 0.00145276
Iteration 8/25 | Loss: 0.00145276
Iteration 9/25 | Loss: 0.00145276
Iteration 10/25 | Loss: 0.00145276
Iteration 11/25 | Loss: 0.00145276
Iteration 12/25 | Loss: 0.00145276
Iteration 13/25 | Loss: 0.00145276
Iteration 14/25 | Loss: 0.00145276
Iteration 15/25 | Loss: 0.00145276
Iteration 16/25 | Loss: 0.00145276
Iteration 17/25 | Loss: 0.00145276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001452756579965353, 0.001452756579965353, 0.001452756579965353, 0.001452756579965353, 0.001452756579965353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001452756579965353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60473299
Iteration 2/25 | Loss: 0.00232754
Iteration 3/25 | Loss: 0.00232754
Iteration 4/25 | Loss: 0.00232754
Iteration 5/25 | Loss: 0.00232754
Iteration 6/25 | Loss: 0.00232754
Iteration 7/25 | Loss: 0.00232754
Iteration 8/25 | Loss: 0.00232754
Iteration 9/25 | Loss: 0.00232754
Iteration 10/25 | Loss: 0.00232754
Iteration 11/25 | Loss: 0.00232754
Iteration 12/25 | Loss: 0.00232754
Iteration 13/25 | Loss: 0.00232754
Iteration 14/25 | Loss: 0.00232754
Iteration 15/25 | Loss: 0.00232754
Iteration 16/25 | Loss: 0.00232754
Iteration 17/25 | Loss: 0.00232754
Iteration 18/25 | Loss: 0.00232754
Iteration 19/25 | Loss: 0.00232754
Iteration 20/25 | Loss: 0.00232754
Iteration 21/25 | Loss: 0.00232754
Iteration 22/25 | Loss: 0.00232754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0023275362327694893, 0.0023275362327694893, 0.0023275362327694893, 0.0023275362327694893, 0.0023275362327694893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023275362327694893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232754
Iteration 2/1000 | Loss: 0.00002471
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001439
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001327
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001264
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001230
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001199
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001197
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001191
Iteration 54/1000 | Loss: 0.00001191
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001191
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001188
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001185
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001175
Iteration 131/1000 | Loss: 0.00001175
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001173
Iteration 135/1000 | Loss: 0.00001173
Iteration 136/1000 | Loss: 0.00001173
Iteration 137/1000 | Loss: 0.00001173
Iteration 138/1000 | Loss: 0.00001173
Iteration 139/1000 | Loss: 0.00001173
Iteration 140/1000 | Loss: 0.00001173
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001168
Iteration 173/1000 | Loss: 0.00001168
Iteration 174/1000 | Loss: 0.00001167
Iteration 175/1000 | Loss: 0.00001167
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001167
Iteration 179/1000 | Loss: 0.00001167
Iteration 180/1000 | Loss: 0.00001167
Iteration 181/1000 | Loss: 0.00001167
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001165
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001165
Iteration 194/1000 | Loss: 0.00001165
Iteration 195/1000 | Loss: 0.00001165
Iteration 196/1000 | Loss: 0.00001165
Iteration 197/1000 | Loss: 0.00001165
Iteration 198/1000 | Loss: 0.00001165
Iteration 199/1000 | Loss: 0.00001165
Iteration 200/1000 | Loss: 0.00001165
Iteration 201/1000 | Loss: 0.00001165
Iteration 202/1000 | Loss: 0.00001165
Iteration 203/1000 | Loss: 0.00001165
Iteration 204/1000 | Loss: 0.00001165
Iteration 205/1000 | Loss: 0.00001165
Iteration 206/1000 | Loss: 0.00001165
Iteration 207/1000 | Loss: 0.00001165
Iteration 208/1000 | Loss: 0.00001165
Iteration 209/1000 | Loss: 0.00001165
Iteration 210/1000 | Loss: 0.00001165
Iteration 211/1000 | Loss: 0.00001165
Iteration 212/1000 | Loss: 0.00001165
Iteration 213/1000 | Loss: 0.00001165
Iteration 214/1000 | Loss: 0.00001165
Iteration 215/1000 | Loss: 0.00001165
Iteration 216/1000 | Loss: 0.00001165
Iteration 217/1000 | Loss: 0.00001165
Iteration 218/1000 | Loss: 0.00001165
Iteration 219/1000 | Loss: 0.00001165
Iteration 220/1000 | Loss: 0.00001165
Iteration 221/1000 | Loss: 0.00001165
Iteration 222/1000 | Loss: 0.00001165
Iteration 223/1000 | Loss: 0.00001165
Iteration 224/1000 | Loss: 0.00001165
Iteration 225/1000 | Loss: 0.00001165
Iteration 226/1000 | Loss: 0.00001165
Iteration 227/1000 | Loss: 0.00001165
Iteration 228/1000 | Loss: 0.00001165
Iteration 229/1000 | Loss: 0.00001165
Iteration 230/1000 | Loss: 0.00001165
Iteration 231/1000 | Loss: 0.00001165
Iteration 232/1000 | Loss: 0.00001165
Iteration 233/1000 | Loss: 0.00001165
Iteration 234/1000 | Loss: 0.00001165
Iteration 235/1000 | Loss: 0.00001165
Iteration 236/1000 | Loss: 0.00001165
Iteration 237/1000 | Loss: 0.00001165
Iteration 238/1000 | Loss: 0.00001165
Iteration 239/1000 | Loss: 0.00001165
Iteration 240/1000 | Loss: 0.00001165
Iteration 241/1000 | Loss: 0.00001165
Iteration 242/1000 | Loss: 0.00001165
Iteration 243/1000 | Loss: 0.00001165
Iteration 244/1000 | Loss: 0.00001165
Iteration 245/1000 | Loss: 0.00001165
Iteration 246/1000 | Loss: 0.00001165
Iteration 247/1000 | Loss: 0.00001165
Iteration 248/1000 | Loss: 0.00001165
Iteration 249/1000 | Loss: 0.00001165
Iteration 250/1000 | Loss: 0.00001165
Iteration 251/1000 | Loss: 0.00001165
Iteration 252/1000 | Loss: 0.00001165
Iteration 253/1000 | Loss: 0.00001165
Iteration 254/1000 | Loss: 0.00001165
Iteration 255/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.1648065992631018e-05, 1.1648065992631018e-05, 1.1648065992631018e-05, 1.1648065992631018e-05, 1.1648065992631018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1648065992631018e-05

Optimization complete. Final v2v error: 2.984011650085449 mm

Highest mean error: 3.310570001602173 mm for frame 112

Lowest mean error: 2.8569774627685547 mm for frame 40

Saving results

Total time: 46.14544105529785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992148
Iteration 2/25 | Loss: 0.00992148
Iteration 3/25 | Loss: 0.00992147
Iteration 4/25 | Loss: 0.00992147
Iteration 5/25 | Loss: 0.00992147
Iteration 6/25 | Loss: 0.00992147
Iteration 7/25 | Loss: 0.00992147
Iteration 8/25 | Loss: 0.00992147
Iteration 9/25 | Loss: 0.00992147
Iteration 10/25 | Loss: 0.00992146
Iteration 11/25 | Loss: 0.00992146
Iteration 12/25 | Loss: 0.00992146
Iteration 13/25 | Loss: 0.00992146
Iteration 14/25 | Loss: 0.00992146
Iteration 15/25 | Loss: 0.00992146
Iteration 16/25 | Loss: 0.00992146
Iteration 17/25 | Loss: 0.00992146
Iteration 18/25 | Loss: 0.00992145
Iteration 19/25 | Loss: 0.00992145
Iteration 20/25 | Loss: 0.00992145
Iteration 21/25 | Loss: 0.00992145
Iteration 22/25 | Loss: 0.00992145
Iteration 23/25 | Loss: 0.00992145
Iteration 24/25 | Loss: 0.00992145
Iteration 25/25 | Loss: 0.00992144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46726596
Iteration 2/25 | Loss: 0.18388088
Iteration 3/25 | Loss: 0.18387423
Iteration 4/25 | Loss: 0.18387423
Iteration 5/25 | Loss: 0.18387423
Iteration 6/25 | Loss: 0.18387419
Iteration 7/25 | Loss: 0.18387419
Iteration 8/25 | Loss: 0.18387419
Iteration 9/25 | Loss: 0.18387419
Iteration 10/25 | Loss: 0.18387419
Iteration 11/25 | Loss: 0.18387419
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.1838741898536682, 0.1838741898536682, 0.1838741898536682, 0.1838741898536682, 0.1838741898536682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1838741898536682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18387419
Iteration 2/1000 | Loss: 0.00619590
Iteration 3/1000 | Loss: 0.00229826
Iteration 4/1000 | Loss: 0.00080792
Iteration 5/1000 | Loss: 0.00087676
Iteration 6/1000 | Loss: 0.00061814
Iteration 7/1000 | Loss: 0.00312038
Iteration 8/1000 | Loss: 0.00021507
Iteration 9/1000 | Loss: 0.00019158
Iteration 10/1000 | Loss: 0.00077387
Iteration 11/1000 | Loss: 0.00006492
Iteration 12/1000 | Loss: 0.00064908
Iteration 13/1000 | Loss: 0.00006024
Iteration 14/1000 | Loss: 0.00004586
Iteration 15/1000 | Loss: 0.00016341
Iteration 16/1000 | Loss: 0.00021769
Iteration 17/1000 | Loss: 0.00007121
Iteration 18/1000 | Loss: 0.00003090
Iteration 19/1000 | Loss: 0.00099654
Iteration 20/1000 | Loss: 0.00014119
Iteration 21/1000 | Loss: 0.00004900
Iteration 22/1000 | Loss: 0.00018313
Iteration 23/1000 | Loss: 0.00003424
Iteration 24/1000 | Loss: 0.00003264
Iteration 25/1000 | Loss: 0.00002409
Iteration 26/1000 | Loss: 0.00006265
Iteration 27/1000 | Loss: 0.00002314
Iteration 28/1000 | Loss: 0.00003708
Iteration 29/1000 | Loss: 0.00002166
Iteration 30/1000 | Loss: 0.00003218
Iteration 31/1000 | Loss: 0.00003627
Iteration 32/1000 | Loss: 0.00004570
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00014028
Iteration 35/1000 | Loss: 0.00004881
Iteration 36/1000 | Loss: 0.00001903
Iteration 37/1000 | Loss: 0.00001874
Iteration 38/1000 | Loss: 0.00011328
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001842
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001831
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001830
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001826
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001821
Iteration 62/1000 | Loss: 0.00001821
Iteration 63/1000 | Loss: 0.00001817
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001816
Iteration 66/1000 | Loss: 0.00001816
Iteration 67/1000 | Loss: 0.00001815
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001813
Iteration 72/1000 | Loss: 0.00001813
Iteration 73/1000 | Loss: 0.00001812
Iteration 74/1000 | Loss: 0.00001812
Iteration 75/1000 | Loss: 0.00001810
Iteration 76/1000 | Loss: 0.00001810
Iteration 77/1000 | Loss: 0.00001810
Iteration 78/1000 | Loss: 0.00001810
Iteration 79/1000 | Loss: 0.00001810
Iteration 80/1000 | Loss: 0.00001810
Iteration 81/1000 | Loss: 0.00001810
Iteration 82/1000 | Loss: 0.00001809
Iteration 83/1000 | Loss: 0.00001809
Iteration 84/1000 | Loss: 0.00001809
Iteration 85/1000 | Loss: 0.00001809
Iteration 86/1000 | Loss: 0.00001809
Iteration 87/1000 | Loss: 0.00001809
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001807
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001801
Iteration 99/1000 | Loss: 0.00001800
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00001799
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001798
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001798
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001796
Iteration 111/1000 | Loss: 0.00001795
Iteration 112/1000 | Loss: 0.00001795
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001793
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.7928690795088187e-05, 1.7928690795088187e-05, 1.7928690795088187e-05, 1.7928690795088187e-05, 1.7928690795088187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7928690795088187e-05

Optimization complete. Final v2v error: 3.6071126461029053 mm

Highest mean error: 4.064888954162598 mm for frame 29

Lowest mean error: 3.3292441368103027 mm for frame 138

Saving results

Total time: 80.20533394813538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797665
Iteration 2/25 | Loss: 0.00115070
Iteration 3/25 | Loss: 0.00104227
Iteration 4/25 | Loss: 0.00102516
Iteration 5/25 | Loss: 0.00101729
Iteration 6/25 | Loss: 0.00101573
Iteration 7/25 | Loss: 0.00101573
Iteration 8/25 | Loss: 0.00101573
Iteration 9/25 | Loss: 0.00101573
Iteration 10/25 | Loss: 0.00101573
Iteration 11/25 | Loss: 0.00101573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010157300857827067, 0.0010157300857827067, 0.0010157300857827067, 0.0010157300857827067, 0.0010157300857827067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010157300857827067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35925436
Iteration 2/25 | Loss: 0.00081636
Iteration 3/25 | Loss: 0.00081636
Iteration 4/25 | Loss: 0.00081636
Iteration 5/25 | Loss: 0.00081636
Iteration 6/25 | Loss: 0.00081636
Iteration 7/25 | Loss: 0.00081636
Iteration 8/25 | Loss: 0.00081636
Iteration 9/25 | Loss: 0.00081636
Iteration 10/25 | Loss: 0.00081636
Iteration 11/25 | Loss: 0.00081636
Iteration 12/25 | Loss: 0.00081636
Iteration 13/25 | Loss: 0.00081636
Iteration 14/25 | Loss: 0.00081636
Iteration 15/25 | Loss: 0.00081636
Iteration 16/25 | Loss: 0.00081636
Iteration 17/25 | Loss: 0.00081636
Iteration 18/25 | Loss: 0.00081636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008163577294908464, 0.0008163577294908464, 0.0008163577294908464, 0.0008163577294908464, 0.0008163577294908464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008163577294908464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081636
Iteration 2/1000 | Loss: 0.00004209
Iteration 3/1000 | Loss: 0.00002629
Iteration 4/1000 | Loss: 0.00002021
Iteration 5/1000 | Loss: 0.00001716
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001184
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001183
Iteration 123/1000 | Loss: 0.00001183
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.1829893992398866e-05, 1.1829893992398866e-05, 1.1829893992398866e-05, 1.1829893992398866e-05, 1.1829893992398866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1829893992398866e-05

Optimization complete. Final v2v error: 2.835392713546753 mm

Highest mean error: 4.767780303955078 mm for frame 25

Lowest mean error: 2.2069649696350098 mm for frame 78

Saving results

Total time: 43.2525098323822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068657
Iteration 2/25 | Loss: 0.00161395
Iteration 3/25 | Loss: 0.00128071
Iteration 4/25 | Loss: 0.00125651
Iteration 5/25 | Loss: 0.00124990
Iteration 6/25 | Loss: 0.00124862
Iteration 7/25 | Loss: 0.00124862
Iteration 8/25 | Loss: 0.00124862
Iteration 9/25 | Loss: 0.00124862
Iteration 10/25 | Loss: 0.00124862
Iteration 11/25 | Loss: 0.00124862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012486204504966736, 0.0012486204504966736, 0.0012486204504966736, 0.0012486204504966736, 0.0012486204504966736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012486204504966736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93886483
Iteration 2/25 | Loss: 0.00082112
Iteration 3/25 | Loss: 0.00082112
Iteration 4/25 | Loss: 0.00082112
Iteration 5/25 | Loss: 0.00082112
Iteration 6/25 | Loss: 0.00082111
Iteration 7/25 | Loss: 0.00082111
Iteration 8/25 | Loss: 0.00082111
Iteration 9/25 | Loss: 0.00082111
Iteration 10/25 | Loss: 0.00082111
Iteration 11/25 | Loss: 0.00082111
Iteration 12/25 | Loss: 0.00082111
Iteration 13/25 | Loss: 0.00082111
Iteration 14/25 | Loss: 0.00082111
Iteration 15/25 | Loss: 0.00082111
Iteration 16/25 | Loss: 0.00082111
Iteration 17/25 | Loss: 0.00082111
Iteration 18/25 | Loss: 0.00082111
Iteration 19/25 | Loss: 0.00082111
Iteration 20/25 | Loss: 0.00082111
Iteration 21/25 | Loss: 0.00082111
Iteration 22/25 | Loss: 0.00082111
Iteration 23/25 | Loss: 0.00082111
Iteration 24/25 | Loss: 0.00082111
Iteration 25/25 | Loss: 0.00082111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082111
Iteration 2/1000 | Loss: 0.00004220
Iteration 3/1000 | Loss: 0.00003145
Iteration 4/1000 | Loss: 0.00002910
Iteration 5/1000 | Loss: 0.00002760
Iteration 6/1000 | Loss: 0.00002681
Iteration 7/1000 | Loss: 0.00002631
Iteration 8/1000 | Loss: 0.00002594
Iteration 9/1000 | Loss: 0.00002568
Iteration 10/1000 | Loss: 0.00002530
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002477
Iteration 13/1000 | Loss: 0.00002459
Iteration 14/1000 | Loss: 0.00002450
Iteration 15/1000 | Loss: 0.00002432
Iteration 16/1000 | Loss: 0.00002416
Iteration 17/1000 | Loss: 0.00002407
Iteration 18/1000 | Loss: 0.00002401
Iteration 19/1000 | Loss: 0.00002399
Iteration 20/1000 | Loss: 0.00002399
Iteration 21/1000 | Loss: 0.00002396
Iteration 22/1000 | Loss: 0.00002390
Iteration 23/1000 | Loss: 0.00002387
Iteration 24/1000 | Loss: 0.00002386
Iteration 25/1000 | Loss: 0.00002384
Iteration 26/1000 | Loss: 0.00002381
Iteration 27/1000 | Loss: 0.00002381
Iteration 28/1000 | Loss: 0.00002381
Iteration 29/1000 | Loss: 0.00002381
Iteration 30/1000 | Loss: 0.00002377
Iteration 31/1000 | Loss: 0.00002377
Iteration 32/1000 | Loss: 0.00002376
Iteration 33/1000 | Loss: 0.00002376
Iteration 34/1000 | Loss: 0.00002376
Iteration 35/1000 | Loss: 0.00002376
Iteration 36/1000 | Loss: 0.00002375
Iteration 37/1000 | Loss: 0.00002375
Iteration 38/1000 | Loss: 0.00002373
Iteration 39/1000 | Loss: 0.00002373
Iteration 40/1000 | Loss: 0.00002373
Iteration 41/1000 | Loss: 0.00002372
Iteration 42/1000 | Loss: 0.00002371
Iteration 43/1000 | Loss: 0.00002371
Iteration 44/1000 | Loss: 0.00002371
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002371
Iteration 48/1000 | Loss: 0.00002371
Iteration 49/1000 | Loss: 0.00002371
Iteration 50/1000 | Loss: 0.00002371
Iteration 51/1000 | Loss: 0.00002371
Iteration 52/1000 | Loss: 0.00002371
Iteration 53/1000 | Loss: 0.00002370
Iteration 54/1000 | Loss: 0.00002370
Iteration 55/1000 | Loss: 0.00002369
Iteration 56/1000 | Loss: 0.00002369
Iteration 57/1000 | Loss: 0.00002369
Iteration 58/1000 | Loss: 0.00002368
Iteration 59/1000 | Loss: 0.00002368
Iteration 60/1000 | Loss: 0.00002368
Iteration 61/1000 | Loss: 0.00002368
Iteration 62/1000 | Loss: 0.00002367
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002367
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002366
Iteration 67/1000 | Loss: 0.00002366
Iteration 68/1000 | Loss: 0.00002366
Iteration 69/1000 | Loss: 0.00002365
Iteration 70/1000 | Loss: 0.00002365
Iteration 71/1000 | Loss: 0.00002365
Iteration 72/1000 | Loss: 0.00002365
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002364
Iteration 82/1000 | Loss: 0.00002364
Iteration 83/1000 | Loss: 0.00002364
Iteration 84/1000 | Loss: 0.00002364
Iteration 85/1000 | Loss: 0.00002364
Iteration 86/1000 | Loss: 0.00002364
Iteration 87/1000 | Loss: 0.00002364
Iteration 88/1000 | Loss: 0.00002364
Iteration 89/1000 | Loss: 0.00002364
Iteration 90/1000 | Loss: 0.00002363
Iteration 91/1000 | Loss: 0.00002363
Iteration 92/1000 | Loss: 0.00002363
Iteration 93/1000 | Loss: 0.00002363
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002362
Iteration 96/1000 | Loss: 0.00002362
Iteration 97/1000 | Loss: 0.00002362
Iteration 98/1000 | Loss: 0.00002362
Iteration 99/1000 | Loss: 0.00002362
Iteration 100/1000 | Loss: 0.00002362
Iteration 101/1000 | Loss: 0.00002362
Iteration 102/1000 | Loss: 0.00002362
Iteration 103/1000 | Loss: 0.00002362
Iteration 104/1000 | Loss: 0.00002362
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002361
Iteration 107/1000 | Loss: 0.00002361
Iteration 108/1000 | Loss: 0.00002361
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002361
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002360
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002360
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002360
Iteration 123/1000 | Loss: 0.00002360
Iteration 124/1000 | Loss: 0.00002360
Iteration 125/1000 | Loss: 0.00002360
Iteration 126/1000 | Loss: 0.00002359
Iteration 127/1000 | Loss: 0.00002359
Iteration 128/1000 | Loss: 0.00002359
Iteration 129/1000 | Loss: 0.00002359
Iteration 130/1000 | Loss: 0.00002359
Iteration 131/1000 | Loss: 0.00002359
Iteration 132/1000 | Loss: 0.00002359
Iteration 133/1000 | Loss: 0.00002359
Iteration 134/1000 | Loss: 0.00002359
Iteration 135/1000 | Loss: 0.00002359
Iteration 136/1000 | Loss: 0.00002359
Iteration 137/1000 | Loss: 0.00002359
Iteration 138/1000 | Loss: 0.00002359
Iteration 139/1000 | Loss: 0.00002359
Iteration 140/1000 | Loss: 0.00002359
Iteration 141/1000 | Loss: 0.00002359
Iteration 142/1000 | Loss: 0.00002359
Iteration 143/1000 | Loss: 0.00002359
Iteration 144/1000 | Loss: 0.00002359
Iteration 145/1000 | Loss: 0.00002359
Iteration 146/1000 | Loss: 0.00002359
Iteration 147/1000 | Loss: 0.00002359
Iteration 148/1000 | Loss: 0.00002359
Iteration 149/1000 | Loss: 0.00002359
Iteration 150/1000 | Loss: 0.00002359
Iteration 151/1000 | Loss: 0.00002359
Iteration 152/1000 | Loss: 0.00002359
Iteration 153/1000 | Loss: 0.00002359
Iteration 154/1000 | Loss: 0.00002359
Iteration 155/1000 | Loss: 0.00002359
Iteration 156/1000 | Loss: 0.00002359
Iteration 157/1000 | Loss: 0.00002359
Iteration 158/1000 | Loss: 0.00002359
Iteration 159/1000 | Loss: 0.00002359
Iteration 160/1000 | Loss: 0.00002359
Iteration 161/1000 | Loss: 0.00002359
Iteration 162/1000 | Loss: 0.00002359
Iteration 163/1000 | Loss: 0.00002359
Iteration 164/1000 | Loss: 0.00002359
Iteration 165/1000 | Loss: 0.00002359
Iteration 166/1000 | Loss: 0.00002359
Iteration 167/1000 | Loss: 0.00002359
Iteration 168/1000 | Loss: 0.00002359
Iteration 169/1000 | Loss: 0.00002359
Iteration 170/1000 | Loss: 0.00002359
Iteration 171/1000 | Loss: 0.00002359
Iteration 172/1000 | Loss: 0.00002359
Iteration 173/1000 | Loss: 0.00002359
Iteration 174/1000 | Loss: 0.00002359
Iteration 175/1000 | Loss: 0.00002359
Iteration 176/1000 | Loss: 0.00002359
Iteration 177/1000 | Loss: 0.00002359
Iteration 178/1000 | Loss: 0.00002359
Iteration 179/1000 | Loss: 0.00002359
Iteration 180/1000 | Loss: 0.00002359
Iteration 181/1000 | Loss: 0.00002359
Iteration 182/1000 | Loss: 0.00002359
Iteration 183/1000 | Loss: 0.00002359
Iteration 184/1000 | Loss: 0.00002359
Iteration 185/1000 | Loss: 0.00002359
Iteration 186/1000 | Loss: 0.00002359
Iteration 187/1000 | Loss: 0.00002359
Iteration 188/1000 | Loss: 0.00002359
Iteration 189/1000 | Loss: 0.00002359
Iteration 190/1000 | Loss: 0.00002359
Iteration 191/1000 | Loss: 0.00002359
Iteration 192/1000 | Loss: 0.00002359
Iteration 193/1000 | Loss: 0.00002359
Iteration 194/1000 | Loss: 0.00002359
Iteration 195/1000 | Loss: 0.00002359
Iteration 196/1000 | Loss: 0.00002359
Iteration 197/1000 | Loss: 0.00002359
Iteration 198/1000 | Loss: 0.00002359
Iteration 199/1000 | Loss: 0.00002359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.359335485380143e-05, 2.359335485380143e-05, 2.359335485380143e-05, 2.359335485380143e-05, 2.359335485380143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.359335485380143e-05

Optimization complete. Final v2v error: 3.9989328384399414 mm

Highest mean error: 4.930320739746094 mm for frame 139

Lowest mean error: 3.2878525257110596 mm for frame 24

Saving results

Total time: 45.83613586425781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590603
Iteration 2/25 | Loss: 0.00138163
Iteration 3/25 | Loss: 0.00123309
Iteration 4/25 | Loss: 0.00109460
Iteration 5/25 | Loss: 0.00108287
Iteration 6/25 | Loss: 0.00108545
Iteration 7/25 | Loss: 0.00107410
Iteration 8/25 | Loss: 0.00107149
Iteration 9/25 | Loss: 0.00106965
Iteration 10/25 | Loss: 0.00106842
Iteration 11/25 | Loss: 0.00106787
Iteration 12/25 | Loss: 0.00107032
Iteration 13/25 | Loss: 0.00106670
Iteration 14/25 | Loss: 0.00106531
Iteration 15/25 | Loss: 0.00106468
Iteration 16/25 | Loss: 0.00106450
Iteration 17/25 | Loss: 0.00106449
Iteration 18/25 | Loss: 0.00106449
Iteration 19/25 | Loss: 0.00106449
Iteration 20/25 | Loss: 0.00106449
Iteration 21/25 | Loss: 0.00106449
Iteration 22/25 | Loss: 0.00106449
Iteration 23/25 | Loss: 0.00106449
Iteration 24/25 | Loss: 0.00106449
Iteration 25/25 | Loss: 0.00106449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.21828628
Iteration 2/25 | Loss: 0.00069312
Iteration 3/25 | Loss: 0.00069312
Iteration 4/25 | Loss: 0.00069311
Iteration 5/25 | Loss: 0.00069311
Iteration 6/25 | Loss: 0.00069311
Iteration 7/25 | Loss: 0.00069311
Iteration 8/25 | Loss: 0.00069311
Iteration 9/25 | Loss: 0.00069311
Iteration 10/25 | Loss: 0.00069311
Iteration 11/25 | Loss: 0.00069311
Iteration 12/25 | Loss: 0.00069311
Iteration 13/25 | Loss: 0.00069311
Iteration 14/25 | Loss: 0.00069311
Iteration 15/25 | Loss: 0.00069311
Iteration 16/25 | Loss: 0.00069311
Iteration 17/25 | Loss: 0.00069311
Iteration 18/25 | Loss: 0.00069311
Iteration 19/25 | Loss: 0.00069311
Iteration 20/25 | Loss: 0.00069311
Iteration 21/25 | Loss: 0.00069311
Iteration 22/25 | Loss: 0.00069311
Iteration 23/25 | Loss: 0.00069311
Iteration 24/25 | Loss: 0.00069311
Iteration 25/25 | Loss: 0.00069311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069311
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001298
Iteration 5/1000 | Loss: 0.00001230
Iteration 6/1000 | Loss: 0.00001174
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001099
Iteration 11/1000 | Loss: 0.00001087
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001072
Iteration 16/1000 | Loss: 0.00001071
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001065
Iteration 23/1000 | Loss: 0.00001065
Iteration 24/1000 | Loss: 0.00001065
Iteration 25/1000 | Loss: 0.00001065
Iteration 26/1000 | Loss: 0.00001065
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001064
Iteration 29/1000 | Loss: 0.00001064
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001063
Iteration 32/1000 | Loss: 0.00001063
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001062
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001061
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001060
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001059
Iteration 45/1000 | Loss: 0.00001059
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001058
Iteration 48/1000 | Loss: 0.00001058
Iteration 49/1000 | Loss: 0.00001057
Iteration 50/1000 | Loss: 0.00001057
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001056
Iteration 56/1000 | Loss: 0.00001056
Iteration 57/1000 | Loss: 0.00001056
Iteration 58/1000 | Loss: 0.00001056
Iteration 59/1000 | Loss: 0.00001055
Iteration 60/1000 | Loss: 0.00001055
Iteration 61/1000 | Loss: 0.00001055
Iteration 62/1000 | Loss: 0.00001055
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001055
Iteration 65/1000 | Loss: 0.00001055
Iteration 66/1000 | Loss: 0.00001055
Iteration 67/1000 | Loss: 0.00001055
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001054
Iteration 71/1000 | Loss: 0.00001054
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001052
Iteration 82/1000 | Loss: 0.00001052
Iteration 83/1000 | Loss: 0.00001052
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001052
Iteration 86/1000 | Loss: 0.00001052
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001051
Iteration 89/1000 | Loss: 0.00001051
Iteration 90/1000 | Loss: 0.00001051
Iteration 91/1000 | Loss: 0.00001050
Iteration 92/1000 | Loss: 0.00001050
Iteration 93/1000 | Loss: 0.00001050
Iteration 94/1000 | Loss: 0.00001050
Iteration 95/1000 | Loss: 0.00001050
Iteration 96/1000 | Loss: 0.00001050
Iteration 97/1000 | Loss: 0.00001050
Iteration 98/1000 | Loss: 0.00001049
Iteration 99/1000 | Loss: 0.00001049
Iteration 100/1000 | Loss: 0.00001049
Iteration 101/1000 | Loss: 0.00001049
Iteration 102/1000 | Loss: 0.00001049
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001048
Iteration 108/1000 | Loss: 0.00001048
Iteration 109/1000 | Loss: 0.00001048
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001048
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001047
Iteration 116/1000 | Loss: 0.00001047
Iteration 117/1000 | Loss: 0.00001047
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001046
Iteration 121/1000 | Loss: 0.00001046
Iteration 122/1000 | Loss: 0.00001046
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001045
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00001045
Iteration 127/1000 | Loss: 0.00001045
Iteration 128/1000 | Loss: 0.00001045
Iteration 129/1000 | Loss: 0.00001045
Iteration 130/1000 | Loss: 0.00001045
Iteration 131/1000 | Loss: 0.00001045
Iteration 132/1000 | Loss: 0.00001045
Iteration 133/1000 | Loss: 0.00001045
Iteration 134/1000 | Loss: 0.00001045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.0449425644765142e-05, 1.0449425644765142e-05, 1.0449425644765142e-05, 1.0449425644765142e-05, 1.0449425644765142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0449425644765142e-05

Optimization complete. Final v2v error: 2.7450973987579346 mm

Highest mean error: 3.131589889526367 mm for frame 66

Lowest mean error: 2.440425157546997 mm for frame 129

Saving results

Total time: 58.27299237251282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030181
Iteration 2/25 | Loss: 0.00219166
Iteration 3/25 | Loss: 0.00167629
Iteration 4/25 | Loss: 0.00157678
Iteration 5/25 | Loss: 0.00149074
Iteration 6/25 | Loss: 0.00136886
Iteration 7/25 | Loss: 0.00134773
Iteration 8/25 | Loss: 0.00132683
Iteration 9/25 | Loss: 0.00132226
Iteration 10/25 | Loss: 0.00132890
Iteration 11/25 | Loss: 0.00131149
Iteration 12/25 | Loss: 0.00132879
Iteration 13/25 | Loss: 0.00130677
Iteration 14/25 | Loss: 0.00128249
Iteration 15/25 | Loss: 0.00128670
Iteration 16/25 | Loss: 0.00124956
Iteration 17/25 | Loss: 0.00124555
Iteration 18/25 | Loss: 0.00123958
Iteration 19/25 | Loss: 0.00122364
Iteration 20/25 | Loss: 0.00121361
Iteration 21/25 | Loss: 0.00120807
Iteration 22/25 | Loss: 0.00120239
Iteration 23/25 | Loss: 0.00120480
Iteration 24/25 | Loss: 0.00120066
Iteration 25/25 | Loss: 0.00120117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39102077
Iteration 2/25 | Loss: 0.00127277
Iteration 3/25 | Loss: 0.00102095
Iteration 4/25 | Loss: 0.00102093
Iteration 5/25 | Loss: 0.00102093
Iteration 6/25 | Loss: 0.00102093
Iteration 7/25 | Loss: 0.00102093
Iteration 8/25 | Loss: 0.00102092
Iteration 9/25 | Loss: 0.00102092
Iteration 10/25 | Loss: 0.00102092
Iteration 11/25 | Loss: 0.00102092
Iteration 12/25 | Loss: 0.00102092
Iteration 13/25 | Loss: 0.00102092
Iteration 14/25 | Loss: 0.00102092
Iteration 15/25 | Loss: 0.00102092
Iteration 16/25 | Loss: 0.00102092
Iteration 17/25 | Loss: 0.00102092
Iteration 18/25 | Loss: 0.00102092
Iteration 19/25 | Loss: 0.00102092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010209239553660154, 0.0010209239553660154, 0.0010209239553660154, 0.0010209239553660154, 0.0010209239553660154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010209239553660154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102092
Iteration 2/1000 | Loss: 0.00041349
Iteration 3/1000 | Loss: 0.00006731
Iteration 4/1000 | Loss: 0.00036171
Iteration 5/1000 | Loss: 0.00026349
Iteration 6/1000 | Loss: 0.00014036
Iteration 7/1000 | Loss: 0.00031859
Iteration 8/1000 | Loss: 0.00022602
Iteration 9/1000 | Loss: 0.00006461
Iteration 10/1000 | Loss: 0.00004007
Iteration 11/1000 | Loss: 0.00022406
Iteration 12/1000 | Loss: 0.00056503
Iteration 13/1000 | Loss: 0.00006408
Iteration 14/1000 | Loss: 0.00022051
Iteration 15/1000 | Loss: 0.00004695
Iteration 16/1000 | Loss: 0.00003900
Iteration 17/1000 | Loss: 0.00002977
Iteration 18/1000 | Loss: 0.00030074
Iteration 19/1000 | Loss: 0.00039603
Iteration 20/1000 | Loss: 0.00021088
Iteration 21/1000 | Loss: 0.00004069
Iteration 22/1000 | Loss: 0.00005314
Iteration 23/1000 | Loss: 0.00014602
Iteration 24/1000 | Loss: 0.00020730
Iteration 25/1000 | Loss: 0.00004899
Iteration 26/1000 | Loss: 0.00003489
Iteration 27/1000 | Loss: 0.00003738
Iteration 28/1000 | Loss: 0.00003633
Iteration 29/1000 | Loss: 0.00003921
Iteration 30/1000 | Loss: 0.00004073
Iteration 31/1000 | Loss: 0.00015855
Iteration 32/1000 | Loss: 0.00003324
Iteration 33/1000 | Loss: 0.00009677
Iteration 34/1000 | Loss: 0.00013254
Iteration 35/1000 | Loss: 0.00009631
Iteration 36/1000 | Loss: 0.00003965
Iteration 37/1000 | Loss: 0.00017032
Iteration 38/1000 | Loss: 0.00114647
Iteration 39/1000 | Loss: 0.00021627
Iteration 40/1000 | Loss: 0.00004328
Iteration 41/1000 | Loss: 0.00008756
Iteration 42/1000 | Loss: 0.00002593
Iteration 43/1000 | Loss: 0.00002488
Iteration 44/1000 | Loss: 0.00016276
Iteration 45/1000 | Loss: 0.00014116
Iteration 46/1000 | Loss: 0.00037605
Iteration 47/1000 | Loss: 0.00023770
Iteration 48/1000 | Loss: 0.00006889
Iteration 49/1000 | Loss: 0.00002409
Iteration 50/1000 | Loss: 0.00002383
Iteration 51/1000 | Loss: 0.00013054
Iteration 52/1000 | Loss: 0.00002774
Iteration 53/1000 | Loss: 0.00013781
Iteration 54/1000 | Loss: 0.00006458
Iteration 55/1000 | Loss: 0.00002341
Iteration 56/1000 | Loss: 0.00005900
Iteration 57/1000 | Loss: 0.00003556
Iteration 58/1000 | Loss: 0.00005697
Iteration 59/1000 | Loss: 0.00004174
Iteration 60/1000 | Loss: 0.00006947
Iteration 61/1000 | Loss: 0.00003221
Iteration 62/1000 | Loss: 0.00017574
Iteration 63/1000 | Loss: 0.00003078
Iteration 64/1000 | Loss: 0.00002368
Iteration 65/1000 | Loss: 0.00013362
Iteration 66/1000 | Loss: 0.00002306
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002277
Iteration 69/1000 | Loss: 0.00002274
Iteration 70/1000 | Loss: 0.00002264
Iteration 71/1000 | Loss: 0.00002251
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002242
Iteration 74/1000 | Loss: 0.00002237
Iteration 75/1000 | Loss: 0.00002233
Iteration 76/1000 | Loss: 0.00002232
Iteration 77/1000 | Loss: 0.00002232
Iteration 78/1000 | Loss: 0.00002230
Iteration 79/1000 | Loss: 0.00002229
Iteration 80/1000 | Loss: 0.00010620
Iteration 81/1000 | Loss: 0.00038694
Iteration 82/1000 | Loss: 0.00033336
Iteration 83/1000 | Loss: 0.00004587
Iteration 84/1000 | Loss: 0.00002330
Iteration 85/1000 | Loss: 0.00013771
Iteration 86/1000 | Loss: 0.00009926
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00012924
Iteration 89/1000 | Loss: 0.00010248
Iteration 90/1000 | Loss: 0.00019598
Iteration 91/1000 | Loss: 0.00004619
Iteration 92/1000 | Loss: 0.00008464
Iteration 93/1000 | Loss: 0.00007925
Iteration 94/1000 | Loss: 0.00002697
Iteration 95/1000 | Loss: 0.00002227
Iteration 96/1000 | Loss: 0.00002225
Iteration 97/1000 | Loss: 0.00002220
Iteration 98/1000 | Loss: 0.00002219
Iteration 99/1000 | Loss: 0.00002219
Iteration 100/1000 | Loss: 0.00002218
Iteration 101/1000 | Loss: 0.00002218
Iteration 102/1000 | Loss: 0.00002218
Iteration 103/1000 | Loss: 0.00002217
Iteration 104/1000 | Loss: 0.00002217
Iteration 105/1000 | Loss: 0.00002217
Iteration 106/1000 | Loss: 0.00002217
Iteration 107/1000 | Loss: 0.00002216
Iteration 108/1000 | Loss: 0.00002216
Iteration 109/1000 | Loss: 0.00002216
Iteration 110/1000 | Loss: 0.00004665
Iteration 111/1000 | Loss: 0.00003073
Iteration 112/1000 | Loss: 0.00003805
Iteration 113/1000 | Loss: 0.00002274
Iteration 114/1000 | Loss: 0.00002220
Iteration 115/1000 | Loss: 0.00002212
Iteration 116/1000 | Loss: 0.00002212
Iteration 117/1000 | Loss: 0.00002212
Iteration 118/1000 | Loss: 0.00002212
Iteration 119/1000 | Loss: 0.00002212
Iteration 120/1000 | Loss: 0.00002212
Iteration 121/1000 | Loss: 0.00002211
Iteration 122/1000 | Loss: 0.00002211
Iteration 123/1000 | Loss: 0.00002211
Iteration 124/1000 | Loss: 0.00002211
Iteration 125/1000 | Loss: 0.00002211
Iteration 126/1000 | Loss: 0.00002211
Iteration 127/1000 | Loss: 0.00002211
Iteration 128/1000 | Loss: 0.00002211
Iteration 129/1000 | Loss: 0.00002210
Iteration 130/1000 | Loss: 0.00002210
Iteration 131/1000 | Loss: 0.00002210
Iteration 132/1000 | Loss: 0.00002210
Iteration 133/1000 | Loss: 0.00002210
Iteration 134/1000 | Loss: 0.00002210
Iteration 135/1000 | Loss: 0.00002210
Iteration 136/1000 | Loss: 0.00002209
Iteration 137/1000 | Loss: 0.00002209
Iteration 138/1000 | Loss: 0.00002209
Iteration 139/1000 | Loss: 0.00002209
Iteration 140/1000 | Loss: 0.00002208
Iteration 141/1000 | Loss: 0.00002208
Iteration 142/1000 | Loss: 0.00002208
Iteration 143/1000 | Loss: 0.00002208
Iteration 144/1000 | Loss: 0.00002207
Iteration 145/1000 | Loss: 0.00002207
Iteration 146/1000 | Loss: 0.00002207
Iteration 147/1000 | Loss: 0.00002207
Iteration 148/1000 | Loss: 0.00002207
Iteration 149/1000 | Loss: 0.00002207
Iteration 150/1000 | Loss: 0.00002207
Iteration 151/1000 | Loss: 0.00002207
Iteration 152/1000 | Loss: 0.00002207
Iteration 153/1000 | Loss: 0.00002207
Iteration 154/1000 | Loss: 0.00002207
Iteration 155/1000 | Loss: 0.00002207
Iteration 156/1000 | Loss: 0.00002207
Iteration 157/1000 | Loss: 0.00002207
Iteration 158/1000 | Loss: 0.00002207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.2069765691412613e-05, 2.2069765691412613e-05, 2.2069765691412613e-05, 2.2069765691412613e-05, 2.2069765691412613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2069765691412613e-05

Optimization complete. Final v2v error: 3.98520565032959 mm

Highest mean error: 5.801968097686768 mm for frame 65

Lowest mean error: 3.4006993770599365 mm for frame 189

Saving results

Total time: 184.3582146167755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762439
Iteration 2/25 | Loss: 0.00131325
Iteration 3/25 | Loss: 0.00113571
Iteration 4/25 | Loss: 0.00110631
Iteration 5/25 | Loss: 0.00109921
Iteration 6/25 | Loss: 0.00109785
Iteration 7/25 | Loss: 0.00109785
Iteration 8/25 | Loss: 0.00109785
Iteration 9/25 | Loss: 0.00109785
Iteration 10/25 | Loss: 0.00109785
Iteration 11/25 | Loss: 0.00109785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001097852480597794, 0.001097852480597794, 0.001097852480597794, 0.001097852480597794, 0.001097852480597794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001097852480597794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42811775
Iteration 2/25 | Loss: 0.00086487
Iteration 3/25 | Loss: 0.00086487
Iteration 4/25 | Loss: 0.00086487
Iteration 5/25 | Loss: 0.00086486
Iteration 6/25 | Loss: 0.00086486
Iteration 7/25 | Loss: 0.00086486
Iteration 8/25 | Loss: 0.00086486
Iteration 9/25 | Loss: 0.00086486
Iteration 10/25 | Loss: 0.00086486
Iteration 11/25 | Loss: 0.00086486
Iteration 12/25 | Loss: 0.00086486
Iteration 13/25 | Loss: 0.00086486
Iteration 14/25 | Loss: 0.00086486
Iteration 15/25 | Loss: 0.00086486
Iteration 16/25 | Loss: 0.00086486
Iteration 17/25 | Loss: 0.00086486
Iteration 18/25 | Loss: 0.00086486
Iteration 19/25 | Loss: 0.00086486
Iteration 20/25 | Loss: 0.00086486
Iteration 21/25 | Loss: 0.00086486
Iteration 22/25 | Loss: 0.00086486
Iteration 23/25 | Loss: 0.00086486
Iteration 24/25 | Loss: 0.00086486
Iteration 25/25 | Loss: 0.00086486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086486
Iteration 2/1000 | Loss: 0.00003221
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001498
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001384
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001362
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001353
Iteration 37/1000 | Loss: 0.00001353
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001351
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001350
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001350
Iteration 48/1000 | Loss: 0.00001350
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001347
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001346
Iteration 61/1000 | Loss: 0.00001346
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001345
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00001339
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001338
Iteration 103/1000 | Loss: 0.00001338
Iteration 104/1000 | Loss: 0.00001338
Iteration 105/1000 | Loss: 0.00001338
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.3377257346292026e-05, 1.3377257346292026e-05, 1.3377257346292026e-05, 1.3377257346292026e-05, 1.3377257346292026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3377257346292026e-05

Optimization complete. Final v2v error: 3.116118907928467 mm

Highest mean error: 3.4480841159820557 mm for frame 25

Lowest mean error: 2.443392038345337 mm for frame 168

Saving results

Total time: 40.24039936065674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966454
Iteration 2/25 | Loss: 0.00168832
Iteration 3/25 | Loss: 0.00130595
Iteration 4/25 | Loss: 0.00127905
Iteration 5/25 | Loss: 0.00126999
Iteration 6/25 | Loss: 0.00126910
Iteration 7/25 | Loss: 0.00126910
Iteration 8/25 | Loss: 0.00126910
Iteration 9/25 | Loss: 0.00126910
Iteration 10/25 | Loss: 0.00126910
Iteration 11/25 | Loss: 0.00126910
Iteration 12/25 | Loss: 0.00126910
Iteration 13/25 | Loss: 0.00126910
Iteration 14/25 | Loss: 0.00126910
Iteration 15/25 | Loss: 0.00126910
Iteration 16/25 | Loss: 0.00126910
Iteration 17/25 | Loss: 0.00126910
Iteration 18/25 | Loss: 0.00126910
Iteration 19/25 | Loss: 0.00126910
Iteration 20/25 | Loss: 0.00126910
Iteration 21/25 | Loss: 0.00126910
Iteration 22/25 | Loss: 0.00126910
Iteration 23/25 | Loss: 0.00126910
Iteration 24/25 | Loss: 0.00126910
Iteration 25/25 | Loss: 0.00126910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83985591
Iteration 2/25 | Loss: 0.00084608
Iteration 3/25 | Loss: 0.00084608
Iteration 4/25 | Loss: 0.00084608
Iteration 5/25 | Loss: 0.00084608
Iteration 6/25 | Loss: 0.00084608
Iteration 7/25 | Loss: 0.00084608
Iteration 8/25 | Loss: 0.00084608
Iteration 9/25 | Loss: 0.00084608
Iteration 10/25 | Loss: 0.00084608
Iteration 11/25 | Loss: 0.00084608
Iteration 12/25 | Loss: 0.00084608
Iteration 13/25 | Loss: 0.00084608
Iteration 14/25 | Loss: 0.00084608
Iteration 15/25 | Loss: 0.00084608
Iteration 16/25 | Loss: 0.00084608
Iteration 17/25 | Loss: 0.00084608
Iteration 18/25 | Loss: 0.00084608
Iteration 19/25 | Loss: 0.00084608
Iteration 20/25 | Loss: 0.00084608
Iteration 21/25 | Loss: 0.00084608
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008460751851089299, 0.0008460751851089299, 0.0008460751851089299, 0.0008460751851089299, 0.0008460751851089299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008460751851089299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084608
Iteration 2/1000 | Loss: 0.00006090
Iteration 3/1000 | Loss: 0.00004537
Iteration 4/1000 | Loss: 0.00004271
Iteration 5/1000 | Loss: 0.00004078
Iteration 6/1000 | Loss: 0.00003951
Iteration 7/1000 | Loss: 0.00003852
Iteration 8/1000 | Loss: 0.00003798
Iteration 9/1000 | Loss: 0.00003744
Iteration 10/1000 | Loss: 0.00003705
Iteration 11/1000 | Loss: 0.00003657
Iteration 12/1000 | Loss: 0.00003615
Iteration 13/1000 | Loss: 0.00003590
Iteration 14/1000 | Loss: 0.00003558
Iteration 15/1000 | Loss: 0.00003523
Iteration 16/1000 | Loss: 0.00003500
Iteration 17/1000 | Loss: 0.00003480
Iteration 18/1000 | Loss: 0.00003474
Iteration 19/1000 | Loss: 0.00003460
Iteration 20/1000 | Loss: 0.00003457
Iteration 21/1000 | Loss: 0.00003442
Iteration 22/1000 | Loss: 0.00003437
Iteration 23/1000 | Loss: 0.00003433
Iteration 24/1000 | Loss: 0.00003432
Iteration 25/1000 | Loss: 0.00003431
Iteration 26/1000 | Loss: 0.00003430
Iteration 27/1000 | Loss: 0.00003429
Iteration 28/1000 | Loss: 0.00003428
Iteration 29/1000 | Loss: 0.00003428
Iteration 30/1000 | Loss: 0.00003428
Iteration 31/1000 | Loss: 0.00003428
Iteration 32/1000 | Loss: 0.00003428
Iteration 33/1000 | Loss: 0.00003428
Iteration 34/1000 | Loss: 0.00003428
Iteration 35/1000 | Loss: 0.00003427
Iteration 36/1000 | Loss: 0.00003426
Iteration 37/1000 | Loss: 0.00003426
Iteration 38/1000 | Loss: 0.00003424
Iteration 39/1000 | Loss: 0.00003424
Iteration 40/1000 | Loss: 0.00003424
Iteration 41/1000 | Loss: 0.00003423
Iteration 42/1000 | Loss: 0.00003423
Iteration 43/1000 | Loss: 0.00003423
Iteration 44/1000 | Loss: 0.00003423
Iteration 45/1000 | Loss: 0.00003422
Iteration 46/1000 | Loss: 0.00003422
Iteration 47/1000 | Loss: 0.00003421
Iteration 48/1000 | Loss: 0.00003421
Iteration 49/1000 | Loss: 0.00003420
Iteration 50/1000 | Loss: 0.00003419
Iteration 51/1000 | Loss: 0.00003419
Iteration 52/1000 | Loss: 0.00003419
Iteration 53/1000 | Loss: 0.00003419
Iteration 54/1000 | Loss: 0.00003418
Iteration 55/1000 | Loss: 0.00003418
Iteration 56/1000 | Loss: 0.00003418
Iteration 57/1000 | Loss: 0.00003417
Iteration 58/1000 | Loss: 0.00003417
Iteration 59/1000 | Loss: 0.00003417
Iteration 60/1000 | Loss: 0.00003417
Iteration 61/1000 | Loss: 0.00003416
Iteration 62/1000 | Loss: 0.00003416
Iteration 63/1000 | Loss: 0.00003416
Iteration 64/1000 | Loss: 0.00003416
Iteration 65/1000 | Loss: 0.00003416
Iteration 66/1000 | Loss: 0.00003416
Iteration 67/1000 | Loss: 0.00003416
Iteration 68/1000 | Loss: 0.00003416
Iteration 69/1000 | Loss: 0.00003416
Iteration 70/1000 | Loss: 0.00003416
Iteration 71/1000 | Loss: 0.00003415
Iteration 72/1000 | Loss: 0.00003415
Iteration 73/1000 | Loss: 0.00003415
Iteration 74/1000 | Loss: 0.00003415
Iteration 75/1000 | Loss: 0.00003414
Iteration 76/1000 | Loss: 0.00003414
Iteration 77/1000 | Loss: 0.00003413
Iteration 78/1000 | Loss: 0.00003413
Iteration 79/1000 | Loss: 0.00003413
Iteration 80/1000 | Loss: 0.00003412
Iteration 81/1000 | Loss: 0.00003412
Iteration 82/1000 | Loss: 0.00003412
Iteration 83/1000 | Loss: 0.00003412
Iteration 84/1000 | Loss: 0.00003411
Iteration 85/1000 | Loss: 0.00003411
Iteration 86/1000 | Loss: 0.00003411
Iteration 87/1000 | Loss: 0.00003411
Iteration 88/1000 | Loss: 0.00003411
Iteration 89/1000 | Loss: 0.00003411
Iteration 90/1000 | Loss: 0.00003410
Iteration 91/1000 | Loss: 0.00003410
Iteration 92/1000 | Loss: 0.00003410
Iteration 93/1000 | Loss: 0.00003410
Iteration 94/1000 | Loss: 0.00003410
Iteration 95/1000 | Loss: 0.00003410
Iteration 96/1000 | Loss: 0.00003410
Iteration 97/1000 | Loss: 0.00003410
Iteration 98/1000 | Loss: 0.00003410
Iteration 99/1000 | Loss: 0.00003409
Iteration 100/1000 | Loss: 0.00003409
Iteration 101/1000 | Loss: 0.00003409
Iteration 102/1000 | Loss: 0.00003409
Iteration 103/1000 | Loss: 0.00003409
Iteration 104/1000 | Loss: 0.00003408
Iteration 105/1000 | Loss: 0.00003408
Iteration 106/1000 | Loss: 0.00003408
Iteration 107/1000 | Loss: 0.00003408
Iteration 108/1000 | Loss: 0.00003408
Iteration 109/1000 | Loss: 0.00003407
Iteration 110/1000 | Loss: 0.00003407
Iteration 111/1000 | Loss: 0.00003407
Iteration 112/1000 | Loss: 0.00003407
Iteration 113/1000 | Loss: 0.00003407
Iteration 114/1000 | Loss: 0.00003407
Iteration 115/1000 | Loss: 0.00003407
Iteration 116/1000 | Loss: 0.00003407
Iteration 117/1000 | Loss: 0.00003407
Iteration 118/1000 | Loss: 0.00003407
Iteration 119/1000 | Loss: 0.00003406
Iteration 120/1000 | Loss: 0.00003406
Iteration 121/1000 | Loss: 0.00003406
Iteration 122/1000 | Loss: 0.00003406
Iteration 123/1000 | Loss: 0.00003406
Iteration 124/1000 | Loss: 0.00003406
Iteration 125/1000 | Loss: 0.00003406
Iteration 126/1000 | Loss: 0.00003406
Iteration 127/1000 | Loss: 0.00003406
Iteration 128/1000 | Loss: 0.00003406
Iteration 129/1000 | Loss: 0.00003406
Iteration 130/1000 | Loss: 0.00003406
Iteration 131/1000 | Loss: 0.00003405
Iteration 132/1000 | Loss: 0.00003405
Iteration 133/1000 | Loss: 0.00003405
Iteration 134/1000 | Loss: 0.00003405
Iteration 135/1000 | Loss: 0.00003405
Iteration 136/1000 | Loss: 0.00003405
Iteration 137/1000 | Loss: 0.00003405
Iteration 138/1000 | Loss: 0.00003404
Iteration 139/1000 | Loss: 0.00003404
Iteration 140/1000 | Loss: 0.00003404
Iteration 141/1000 | Loss: 0.00003404
Iteration 142/1000 | Loss: 0.00003404
Iteration 143/1000 | Loss: 0.00003403
Iteration 144/1000 | Loss: 0.00003403
Iteration 145/1000 | Loss: 0.00003403
Iteration 146/1000 | Loss: 0.00003403
Iteration 147/1000 | Loss: 0.00003403
Iteration 148/1000 | Loss: 0.00003403
Iteration 149/1000 | Loss: 0.00003403
Iteration 150/1000 | Loss: 0.00003403
Iteration 151/1000 | Loss: 0.00003403
Iteration 152/1000 | Loss: 0.00003402
Iteration 153/1000 | Loss: 0.00003402
Iteration 154/1000 | Loss: 0.00003402
Iteration 155/1000 | Loss: 0.00003402
Iteration 156/1000 | Loss: 0.00003401
Iteration 157/1000 | Loss: 0.00003401
Iteration 158/1000 | Loss: 0.00003401
Iteration 159/1000 | Loss: 0.00003401
Iteration 160/1000 | Loss: 0.00003401
Iteration 161/1000 | Loss: 0.00003401
Iteration 162/1000 | Loss: 0.00003401
Iteration 163/1000 | Loss: 0.00003401
Iteration 164/1000 | Loss: 0.00003400
Iteration 165/1000 | Loss: 0.00003400
Iteration 166/1000 | Loss: 0.00003400
Iteration 167/1000 | Loss: 0.00003400
Iteration 168/1000 | Loss: 0.00003400
Iteration 169/1000 | Loss: 0.00003400
Iteration 170/1000 | Loss: 0.00003400
Iteration 171/1000 | Loss: 0.00003400
Iteration 172/1000 | Loss: 0.00003400
Iteration 173/1000 | Loss: 0.00003400
Iteration 174/1000 | Loss: 0.00003400
Iteration 175/1000 | Loss: 0.00003400
Iteration 176/1000 | Loss: 0.00003400
Iteration 177/1000 | Loss: 0.00003400
Iteration 178/1000 | Loss: 0.00003400
Iteration 179/1000 | Loss: 0.00003400
Iteration 180/1000 | Loss: 0.00003400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [3.399587149033323e-05, 3.399587149033323e-05, 3.399587149033323e-05, 3.399587149033323e-05, 3.399587149033323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.399587149033323e-05

Optimization complete. Final v2v error: 4.873402118682861 mm

Highest mean error: 5.370223522186279 mm for frame 97

Lowest mean error: 3.935654640197754 mm for frame 48

Saving results

Total time: 56.46150517463684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974092
Iteration 2/25 | Loss: 0.00169089
Iteration 3/25 | Loss: 0.00124343
Iteration 4/25 | Loss: 0.00120339
Iteration 5/25 | Loss: 0.00121817
Iteration 6/25 | Loss: 0.00115900
Iteration 7/25 | Loss: 0.00113234
Iteration 8/25 | Loss: 0.00113578
Iteration 9/25 | Loss: 0.00111157
Iteration 10/25 | Loss: 0.00110499
Iteration 11/25 | Loss: 0.00109870
Iteration 12/25 | Loss: 0.00110023
Iteration 13/25 | Loss: 0.00109439
Iteration 14/25 | Loss: 0.00109325
Iteration 15/25 | Loss: 0.00108612
Iteration 16/25 | Loss: 0.00108269
Iteration 17/25 | Loss: 0.00108057
Iteration 18/25 | Loss: 0.00108312
Iteration 19/25 | Loss: 0.00108082
Iteration 20/25 | Loss: 0.00107910
Iteration 21/25 | Loss: 0.00107704
Iteration 22/25 | Loss: 0.00108245
Iteration 23/25 | Loss: 0.00108243
Iteration 24/25 | Loss: 0.00108188
Iteration 25/25 | Loss: 0.00108119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48356688
Iteration 2/25 | Loss: 0.00098606
Iteration 3/25 | Loss: 0.00098606
Iteration 4/25 | Loss: 0.00098606
Iteration 5/25 | Loss: 0.00098605
Iteration 6/25 | Loss: 0.00098605
Iteration 7/25 | Loss: 0.00098605
Iteration 8/25 | Loss: 0.00098605
Iteration 9/25 | Loss: 0.00098605
Iteration 10/25 | Loss: 0.00098605
Iteration 11/25 | Loss: 0.00098605
Iteration 12/25 | Loss: 0.00098605
Iteration 13/25 | Loss: 0.00098605
Iteration 14/25 | Loss: 0.00098605
Iteration 15/25 | Loss: 0.00098605
Iteration 16/25 | Loss: 0.00098605
Iteration 17/25 | Loss: 0.00098605
Iteration 18/25 | Loss: 0.00098605
Iteration 19/25 | Loss: 0.00098605
Iteration 20/25 | Loss: 0.00098605
Iteration 21/25 | Loss: 0.00098605
Iteration 22/25 | Loss: 0.00098605
Iteration 23/25 | Loss: 0.00098605
Iteration 24/25 | Loss: 0.00098605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009860527934506536, 0.0009860527934506536, 0.0009860527934506536, 0.0009860527934506536, 0.0009860527934506536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009860527934506536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098605
Iteration 2/1000 | Loss: 0.00006013
Iteration 3/1000 | Loss: 0.00026946
Iteration 4/1000 | Loss: 0.00004634
Iteration 5/1000 | Loss: 0.00014287
Iteration 6/1000 | Loss: 0.00018391
Iteration 7/1000 | Loss: 0.00015962
Iteration 8/1000 | Loss: 0.00020355
Iteration 9/1000 | Loss: 0.00028842
Iteration 10/1000 | Loss: 0.00020270
Iteration 11/1000 | Loss: 0.00028097
Iteration 12/1000 | Loss: 0.00006480
Iteration 13/1000 | Loss: 0.00013602
Iteration 14/1000 | Loss: 0.00003496
Iteration 15/1000 | Loss: 0.00002940
Iteration 16/1000 | Loss: 0.00002543
Iteration 17/1000 | Loss: 0.00038618
Iteration 18/1000 | Loss: 0.00008625
Iteration 19/1000 | Loss: 0.00005344
Iteration 20/1000 | Loss: 0.00002470
Iteration 21/1000 | Loss: 0.00007820
Iteration 22/1000 | Loss: 0.00002178
Iteration 23/1000 | Loss: 0.00004628
Iteration 24/1000 | Loss: 0.00004286
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00001971
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00015948
Iteration 29/1000 | Loss: 0.00007148
Iteration 30/1000 | Loss: 0.00002971
Iteration 31/1000 | Loss: 0.00003594
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002083
Iteration 34/1000 | Loss: 0.00016389
Iteration 35/1000 | Loss: 0.00007681
Iteration 36/1000 | Loss: 0.00013333
Iteration 37/1000 | Loss: 0.00006309
Iteration 38/1000 | Loss: 0.00001589
Iteration 39/1000 | Loss: 0.00007730
Iteration 40/1000 | Loss: 0.00001663
Iteration 41/1000 | Loss: 0.00006827
Iteration 42/1000 | Loss: 0.00028365
Iteration 43/1000 | Loss: 0.00001422
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00004650
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00007376
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00010764
Iteration 53/1000 | Loss: 0.00001644
Iteration 54/1000 | Loss: 0.00001989
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00002206
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001172
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001171
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001169
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001169
Iteration 166/1000 | Loss: 0.00001169
Iteration 167/1000 | Loss: 0.00001169
Iteration 168/1000 | Loss: 0.00001169
Iteration 169/1000 | Loss: 0.00001169
Iteration 170/1000 | Loss: 0.00001169
Iteration 171/1000 | Loss: 0.00001169
Iteration 172/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.1687211554090027e-05, 1.1687211554090027e-05, 1.1687211554090027e-05, 1.1687211554090027e-05, 1.1687211554090027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1687211554090027e-05

Optimization complete. Final v2v error: 2.8899428844451904 mm

Highest mean error: 4.7946553230285645 mm for frame 58

Lowest mean error: 2.273712158203125 mm for frame 97

Saving results

Total time: 129.42736268043518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401046
Iteration 2/25 | Loss: 0.00119450
Iteration 3/25 | Loss: 0.00106895
Iteration 4/25 | Loss: 0.00105339
Iteration 5/25 | Loss: 0.00104999
Iteration 6/25 | Loss: 0.00104896
Iteration 7/25 | Loss: 0.00104896
Iteration 8/25 | Loss: 0.00104896
Iteration 9/25 | Loss: 0.00104896
Iteration 10/25 | Loss: 0.00104896
Iteration 11/25 | Loss: 0.00104896
Iteration 12/25 | Loss: 0.00104896
Iteration 13/25 | Loss: 0.00104896
Iteration 14/25 | Loss: 0.00104896
Iteration 15/25 | Loss: 0.00104896
Iteration 16/25 | Loss: 0.00104896
Iteration 17/25 | Loss: 0.00104896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010489554842934012, 0.0010489554842934012, 0.0010489554842934012, 0.0010489554842934012, 0.0010489554842934012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010489554842934012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36721671
Iteration 2/25 | Loss: 0.00058518
Iteration 3/25 | Loss: 0.00058518
Iteration 4/25 | Loss: 0.00058517
Iteration 5/25 | Loss: 0.00058517
Iteration 6/25 | Loss: 0.00058517
Iteration 7/25 | Loss: 0.00058517
Iteration 8/25 | Loss: 0.00058517
Iteration 9/25 | Loss: 0.00058517
Iteration 10/25 | Loss: 0.00058517
Iteration 11/25 | Loss: 0.00058517
Iteration 12/25 | Loss: 0.00058517
Iteration 13/25 | Loss: 0.00058517
Iteration 14/25 | Loss: 0.00058517
Iteration 15/25 | Loss: 0.00058517
Iteration 16/25 | Loss: 0.00058517
Iteration 17/25 | Loss: 0.00058517
Iteration 18/25 | Loss: 0.00058517
Iteration 19/25 | Loss: 0.00058517
Iteration 20/25 | Loss: 0.00058517
Iteration 21/25 | Loss: 0.00058517
Iteration 22/25 | Loss: 0.00058517
Iteration 23/25 | Loss: 0.00058517
Iteration 24/25 | Loss: 0.00058517
Iteration 25/25 | Loss: 0.00058517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058517
Iteration 2/1000 | Loss: 0.00002555
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001452
Iteration 6/1000 | Loss: 0.00001374
Iteration 7/1000 | Loss: 0.00001336
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001268
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001224
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001213
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001212
Iteration 56/1000 | Loss: 0.00001212
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001205
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001204
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001203
Iteration 78/1000 | Loss: 0.00001203
Iteration 79/1000 | Loss: 0.00001203
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001199
Iteration 110/1000 | Loss: 0.00001199
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001196
Iteration 141/1000 | Loss: 0.00001196
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001196
Iteration 147/1000 | Loss: 0.00001196
Iteration 148/1000 | Loss: 0.00001195
Iteration 149/1000 | Loss: 0.00001195
Iteration 150/1000 | Loss: 0.00001195
Iteration 151/1000 | Loss: 0.00001195
Iteration 152/1000 | Loss: 0.00001195
Iteration 153/1000 | Loss: 0.00001195
Iteration 154/1000 | Loss: 0.00001195
Iteration 155/1000 | Loss: 0.00001194
Iteration 156/1000 | Loss: 0.00001194
Iteration 157/1000 | Loss: 0.00001194
Iteration 158/1000 | Loss: 0.00001194
Iteration 159/1000 | Loss: 0.00001194
Iteration 160/1000 | Loss: 0.00001194
Iteration 161/1000 | Loss: 0.00001194
Iteration 162/1000 | Loss: 0.00001194
Iteration 163/1000 | Loss: 0.00001194
Iteration 164/1000 | Loss: 0.00001194
Iteration 165/1000 | Loss: 0.00001194
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001193
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Iteration 177/1000 | Loss: 0.00001192
Iteration 178/1000 | Loss: 0.00001192
Iteration 179/1000 | Loss: 0.00001191
Iteration 180/1000 | Loss: 0.00001191
Iteration 181/1000 | Loss: 0.00001191
Iteration 182/1000 | Loss: 0.00001191
Iteration 183/1000 | Loss: 0.00001191
Iteration 184/1000 | Loss: 0.00001191
Iteration 185/1000 | Loss: 0.00001191
Iteration 186/1000 | Loss: 0.00001191
Iteration 187/1000 | Loss: 0.00001191
Iteration 188/1000 | Loss: 0.00001191
Iteration 189/1000 | Loss: 0.00001191
Iteration 190/1000 | Loss: 0.00001191
Iteration 191/1000 | Loss: 0.00001191
Iteration 192/1000 | Loss: 0.00001191
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.1912918125744909e-05, 1.1912918125744909e-05, 1.1912918125744909e-05, 1.1912918125744909e-05, 1.1912918125744909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1912918125744909e-05

Optimization complete. Final v2v error: 2.8984391689300537 mm

Highest mean error: 3.350592613220215 mm for frame 86

Lowest mean error: 2.5658843517303467 mm for frame 27

Saving results

Total time: 36.86165452003479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017821
Iteration 2/25 | Loss: 0.00147034
Iteration 3/25 | Loss: 0.00124543
Iteration 4/25 | Loss: 0.00123846
Iteration 5/25 | Loss: 0.00117508
Iteration 6/25 | Loss: 0.00115770
Iteration 7/25 | Loss: 0.00114681
Iteration 8/25 | Loss: 0.00113885
Iteration 9/25 | Loss: 0.00112939
Iteration 10/25 | Loss: 0.00113011
Iteration 11/25 | Loss: 0.00112727
Iteration 12/25 | Loss: 0.00112101
Iteration 13/25 | Loss: 0.00111322
Iteration 14/25 | Loss: 0.00111033
Iteration 15/25 | Loss: 0.00111202
Iteration 16/25 | Loss: 0.00110924
Iteration 17/25 | Loss: 0.00111092
Iteration 18/25 | Loss: 0.00111073
Iteration 19/25 | Loss: 0.00111173
Iteration 20/25 | Loss: 0.00110838
Iteration 21/25 | Loss: 0.00110627
Iteration 22/25 | Loss: 0.00110356
Iteration 23/25 | Loss: 0.00110460
Iteration 24/25 | Loss: 0.00110423
Iteration 25/25 | Loss: 0.00110422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96714187
Iteration 2/25 | Loss: 0.00098426
Iteration 3/25 | Loss: 0.00096506
Iteration 4/25 | Loss: 0.00096506
Iteration 5/25 | Loss: 0.00096506
Iteration 6/25 | Loss: 0.00096506
Iteration 7/25 | Loss: 0.00096506
Iteration 8/25 | Loss: 0.00096506
Iteration 9/25 | Loss: 0.00096506
Iteration 10/25 | Loss: 0.00096506
Iteration 11/25 | Loss: 0.00096506
Iteration 12/25 | Loss: 0.00096506
Iteration 13/25 | Loss: 0.00096506
Iteration 14/25 | Loss: 0.00096506
Iteration 15/25 | Loss: 0.00096506
Iteration 16/25 | Loss: 0.00096506
Iteration 17/25 | Loss: 0.00096506
Iteration 18/25 | Loss: 0.00096506
Iteration 19/25 | Loss: 0.00096506
Iteration 20/25 | Loss: 0.00096506
Iteration 21/25 | Loss: 0.00096506
Iteration 22/25 | Loss: 0.00096506
Iteration 23/25 | Loss: 0.00096506
Iteration 24/25 | Loss: 0.00096506
Iteration 25/25 | Loss: 0.00096506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096506
Iteration 2/1000 | Loss: 0.00008437
Iteration 3/1000 | Loss: 0.00019224
Iteration 4/1000 | Loss: 0.00014091
Iteration 5/1000 | Loss: 0.00012838
Iteration 6/1000 | Loss: 0.00014738
Iteration 7/1000 | Loss: 0.00002533
Iteration 8/1000 | Loss: 0.00004359
Iteration 9/1000 | Loss: 0.00008466
Iteration 10/1000 | Loss: 0.00005430
Iteration 11/1000 | Loss: 0.00002168
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00005713
Iteration 14/1000 | Loss: 0.00009817
Iteration 15/1000 | Loss: 0.00004452
Iteration 16/1000 | Loss: 0.00005931
Iteration 17/1000 | Loss: 0.00003608
Iteration 18/1000 | Loss: 0.00004986
Iteration 19/1000 | Loss: 0.00006598
Iteration 20/1000 | Loss: 0.00007060
Iteration 21/1000 | Loss: 0.00009136
Iteration 22/1000 | Loss: 0.00007672
Iteration 23/1000 | Loss: 0.00006287
Iteration 24/1000 | Loss: 0.00007900
Iteration 25/1000 | Loss: 0.00007757
Iteration 26/1000 | Loss: 0.00009196
Iteration 27/1000 | Loss: 0.00007630
Iteration 28/1000 | Loss: 0.00018519
Iteration 29/1000 | Loss: 0.00012892
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001385
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001340
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001340
Iteration 119/1000 | Loss: 0.00001340
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001338
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001336
Iteration 129/1000 | Loss: 0.00001336
Iteration 130/1000 | Loss: 0.00001336
Iteration 131/1000 | Loss: 0.00001335
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001335
Iteration 135/1000 | Loss: 0.00001334
Iteration 136/1000 | Loss: 0.00001334
Iteration 137/1000 | Loss: 0.00001334
Iteration 138/1000 | Loss: 0.00001333
Iteration 139/1000 | Loss: 0.00001333
Iteration 140/1000 | Loss: 0.00001332
Iteration 141/1000 | Loss: 0.00001332
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001331
Iteration 144/1000 | Loss: 0.00001331
Iteration 145/1000 | Loss: 0.00001331
Iteration 146/1000 | Loss: 0.00001330
Iteration 147/1000 | Loss: 0.00001330
Iteration 148/1000 | Loss: 0.00001330
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001324
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001322
Iteration 166/1000 | Loss: 0.00001322
Iteration 167/1000 | Loss: 0.00001322
Iteration 168/1000 | Loss: 0.00001321
Iteration 169/1000 | Loss: 0.00001321
Iteration 170/1000 | Loss: 0.00001321
Iteration 171/1000 | Loss: 0.00001321
Iteration 172/1000 | Loss: 0.00001321
Iteration 173/1000 | Loss: 0.00001321
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001320
Iteration 176/1000 | Loss: 0.00001320
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001319
Iteration 179/1000 | Loss: 0.00001319
Iteration 180/1000 | Loss: 0.00001319
Iteration 181/1000 | Loss: 0.00001319
Iteration 182/1000 | Loss: 0.00001318
Iteration 183/1000 | Loss: 0.00001318
Iteration 184/1000 | Loss: 0.00002432
Iteration 185/1000 | Loss: 0.00001494
Iteration 186/1000 | Loss: 0.00001397
Iteration 187/1000 | Loss: 0.00001358
Iteration 188/1000 | Loss: 0.00001331
Iteration 189/1000 | Loss: 0.00001316
Iteration 190/1000 | Loss: 0.00001314
Iteration 191/1000 | Loss: 0.00001314
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001312
Iteration 194/1000 | Loss: 0.00001312
Iteration 195/1000 | Loss: 0.00001311
Iteration 196/1000 | Loss: 0.00001309
Iteration 197/1000 | Loss: 0.00001308
Iteration 198/1000 | Loss: 0.00001307
Iteration 199/1000 | Loss: 0.00001306
Iteration 200/1000 | Loss: 0.00001306
Iteration 201/1000 | Loss: 0.00001305
Iteration 202/1000 | Loss: 0.00001305
Iteration 203/1000 | Loss: 0.00001305
Iteration 204/1000 | Loss: 0.00001304
Iteration 205/1000 | Loss: 0.00001304
Iteration 206/1000 | Loss: 0.00001304
Iteration 207/1000 | Loss: 0.00001303
Iteration 208/1000 | Loss: 0.00001302
Iteration 209/1000 | Loss: 0.00001301
Iteration 210/1000 | Loss: 0.00001301
Iteration 211/1000 | Loss: 0.00001301
Iteration 212/1000 | Loss: 0.00001301
Iteration 213/1000 | Loss: 0.00001301
Iteration 214/1000 | Loss: 0.00001301
Iteration 215/1000 | Loss: 0.00001301
Iteration 216/1000 | Loss: 0.00001301
Iteration 217/1000 | Loss: 0.00001300
Iteration 218/1000 | Loss: 0.00001300
Iteration 219/1000 | Loss: 0.00001300
Iteration 220/1000 | Loss: 0.00001300
Iteration 221/1000 | Loss: 0.00001300
Iteration 222/1000 | Loss: 0.00001300
Iteration 223/1000 | Loss: 0.00001300
Iteration 224/1000 | Loss: 0.00001299
Iteration 225/1000 | Loss: 0.00001299
Iteration 226/1000 | Loss: 0.00001299
Iteration 227/1000 | Loss: 0.00001299
Iteration 228/1000 | Loss: 0.00001299
Iteration 229/1000 | Loss: 0.00001299
Iteration 230/1000 | Loss: 0.00001298
Iteration 231/1000 | Loss: 0.00001298
Iteration 232/1000 | Loss: 0.00001298
Iteration 233/1000 | Loss: 0.00001298
Iteration 234/1000 | Loss: 0.00001297
Iteration 235/1000 | Loss: 0.00001297
Iteration 236/1000 | Loss: 0.00001297
Iteration 237/1000 | Loss: 0.00001297
Iteration 238/1000 | Loss: 0.00001297
Iteration 239/1000 | Loss: 0.00001297
Iteration 240/1000 | Loss: 0.00001297
Iteration 241/1000 | Loss: 0.00001297
Iteration 242/1000 | Loss: 0.00001296
Iteration 243/1000 | Loss: 0.00001296
Iteration 244/1000 | Loss: 0.00001296
Iteration 245/1000 | Loss: 0.00001296
Iteration 246/1000 | Loss: 0.00001296
Iteration 247/1000 | Loss: 0.00001296
Iteration 248/1000 | Loss: 0.00001295
Iteration 249/1000 | Loss: 0.00001295
Iteration 250/1000 | Loss: 0.00001295
Iteration 251/1000 | Loss: 0.00001295
Iteration 252/1000 | Loss: 0.00001294
Iteration 253/1000 | Loss: 0.00001294
Iteration 254/1000 | Loss: 0.00001294
Iteration 255/1000 | Loss: 0.00001294
Iteration 256/1000 | Loss: 0.00001294
Iteration 257/1000 | Loss: 0.00001294
Iteration 258/1000 | Loss: 0.00001294
Iteration 259/1000 | Loss: 0.00001294
Iteration 260/1000 | Loss: 0.00001293
Iteration 261/1000 | Loss: 0.00001293
Iteration 262/1000 | Loss: 0.00001293
Iteration 263/1000 | Loss: 0.00001293
Iteration 264/1000 | Loss: 0.00001293
Iteration 265/1000 | Loss: 0.00001293
Iteration 266/1000 | Loss: 0.00001293
Iteration 267/1000 | Loss: 0.00001293
Iteration 268/1000 | Loss: 0.00001293
Iteration 269/1000 | Loss: 0.00001293
Iteration 270/1000 | Loss: 0.00001293
Iteration 271/1000 | Loss: 0.00001293
Iteration 272/1000 | Loss: 0.00001292
Iteration 273/1000 | Loss: 0.00001292
Iteration 274/1000 | Loss: 0.00001292
Iteration 275/1000 | Loss: 0.00001292
Iteration 276/1000 | Loss: 0.00001292
Iteration 277/1000 | Loss: 0.00001292
Iteration 278/1000 | Loss: 0.00001292
Iteration 279/1000 | Loss: 0.00001292
Iteration 280/1000 | Loss: 0.00001292
Iteration 281/1000 | Loss: 0.00001292
Iteration 282/1000 | Loss: 0.00001292
Iteration 283/1000 | Loss: 0.00001292
Iteration 284/1000 | Loss: 0.00001292
Iteration 285/1000 | Loss: 0.00001292
Iteration 286/1000 | Loss: 0.00001292
Iteration 287/1000 | Loss: 0.00001292
Iteration 288/1000 | Loss: 0.00001292
Iteration 289/1000 | Loss: 0.00001292
Iteration 290/1000 | Loss: 0.00001292
Iteration 291/1000 | Loss: 0.00001292
Iteration 292/1000 | Loss: 0.00001292
Iteration 293/1000 | Loss: 0.00001291
Iteration 294/1000 | Loss: 0.00001291
Iteration 295/1000 | Loss: 0.00001291
Iteration 296/1000 | Loss: 0.00001291
Iteration 297/1000 | Loss: 0.00001291
Iteration 298/1000 | Loss: 0.00001291
Iteration 299/1000 | Loss: 0.00001291
Iteration 300/1000 | Loss: 0.00001291
Iteration 301/1000 | Loss: 0.00001291
Iteration 302/1000 | Loss: 0.00001291
Iteration 303/1000 | Loss: 0.00001291
Iteration 304/1000 | Loss: 0.00001291
Iteration 305/1000 | Loss: 0.00001291
Iteration 306/1000 | Loss: 0.00001291
Iteration 307/1000 | Loss: 0.00001291
Iteration 308/1000 | Loss: 0.00001291
Iteration 309/1000 | Loss: 0.00001291
Iteration 310/1000 | Loss: 0.00001291
Iteration 311/1000 | Loss: 0.00001291
Iteration 312/1000 | Loss: 0.00001291
Iteration 313/1000 | Loss: 0.00001291
Iteration 314/1000 | Loss: 0.00001291
Iteration 315/1000 | Loss: 0.00001291
Iteration 316/1000 | Loss: 0.00001291
Iteration 317/1000 | Loss: 0.00001291
Iteration 318/1000 | Loss: 0.00001291
Iteration 319/1000 | Loss: 0.00001291
Iteration 320/1000 | Loss: 0.00001291
Iteration 321/1000 | Loss: 0.00001291
Iteration 322/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [1.2912886631966103e-05, 1.2912886631966103e-05, 1.2912886631966103e-05, 1.2912886631966103e-05, 1.2912886631966103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2912886631966103e-05

Optimization complete. Final v2v error: 2.846607208251953 mm

Highest mean error: 11.278603553771973 mm for frame 20

Lowest mean error: 2.492084503173828 mm for frame 73

Saving results

Total time: 146.39731168746948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059186
Iteration 2/25 | Loss: 0.00127084
Iteration 3/25 | Loss: 0.00109589
Iteration 4/25 | Loss: 0.00107069
Iteration 5/25 | Loss: 0.00106585
Iteration 6/25 | Loss: 0.00106585
Iteration 7/25 | Loss: 0.00106585
Iteration 8/25 | Loss: 0.00106585
Iteration 9/25 | Loss: 0.00106585
Iteration 10/25 | Loss: 0.00106585
Iteration 11/25 | Loss: 0.00106585
Iteration 12/25 | Loss: 0.00106585
Iteration 13/25 | Loss: 0.00106585
Iteration 14/25 | Loss: 0.00106585
Iteration 15/25 | Loss: 0.00106585
Iteration 16/25 | Loss: 0.00106585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010658490937203169, 0.0010658490937203169, 0.0010658490937203169, 0.0010658490937203169, 0.0010658490937203169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010658490937203169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13777924
Iteration 2/25 | Loss: 0.00065769
Iteration 3/25 | Loss: 0.00065769
Iteration 4/25 | Loss: 0.00065769
Iteration 5/25 | Loss: 0.00065769
Iteration 6/25 | Loss: 0.00065769
Iteration 7/25 | Loss: 0.00065769
Iteration 8/25 | Loss: 0.00065769
Iteration 9/25 | Loss: 0.00065769
Iteration 10/25 | Loss: 0.00065769
Iteration 11/25 | Loss: 0.00065769
Iteration 12/25 | Loss: 0.00065769
Iteration 13/25 | Loss: 0.00065769
Iteration 14/25 | Loss: 0.00065769
Iteration 15/25 | Loss: 0.00065769
Iteration 16/25 | Loss: 0.00065769
Iteration 17/25 | Loss: 0.00065769
Iteration 18/25 | Loss: 0.00065769
Iteration 19/25 | Loss: 0.00065769
Iteration 20/25 | Loss: 0.00065769
Iteration 21/25 | Loss: 0.00065769
Iteration 22/25 | Loss: 0.00065769
Iteration 23/25 | Loss: 0.00065769
Iteration 24/25 | Loss: 0.00065769
Iteration 25/25 | Loss: 0.00065769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065769
Iteration 2/1000 | Loss: 0.00002590
Iteration 3/1000 | Loss: 0.00001665
Iteration 4/1000 | Loss: 0.00001373
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001175
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001097
Iteration 9/1000 | Loss: 0.00001068
Iteration 10/1000 | Loss: 0.00001055
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001032
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001031
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001025
Iteration 17/1000 | Loss: 0.00001024
Iteration 18/1000 | Loss: 0.00001024
Iteration 19/1000 | Loss: 0.00001023
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001020
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001017
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001015
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00001013
Iteration 35/1000 | Loss: 0.00001013
Iteration 36/1000 | Loss: 0.00001013
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001010
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001008
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001007
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001006
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001003
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001002
Iteration 78/1000 | Loss: 0.00001002
Iteration 79/1000 | Loss: 0.00001002
Iteration 80/1000 | Loss: 0.00001002
Iteration 81/1000 | Loss: 0.00001002
Iteration 82/1000 | Loss: 0.00001002
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00000998
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000998
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Iteration 138/1000 | Loss: 0.00000996
Iteration 139/1000 | Loss: 0.00000996
Iteration 140/1000 | Loss: 0.00000996
Iteration 141/1000 | Loss: 0.00000996
Iteration 142/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [9.956802387023345e-06, 9.956802387023345e-06, 9.956802387023345e-06, 9.956802387023345e-06, 9.956802387023345e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.956802387023345e-06

Optimization complete. Final v2v error: 2.6847779750823975 mm

Highest mean error: 2.9916861057281494 mm for frame 203

Lowest mean error: 2.476757049560547 mm for frame 252

Saving results

Total time: 37.9865562915802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877342
Iteration 2/25 | Loss: 0.00124879
Iteration 3/25 | Loss: 0.00113803
Iteration 4/25 | Loss: 0.00111576
Iteration 5/25 | Loss: 0.00110791
Iteration 6/25 | Loss: 0.00110599
Iteration 7/25 | Loss: 0.00110573
Iteration 8/25 | Loss: 0.00110573
Iteration 9/25 | Loss: 0.00110573
Iteration 10/25 | Loss: 0.00110573
Iteration 11/25 | Loss: 0.00110573
Iteration 12/25 | Loss: 0.00110573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011057319352403283, 0.0011057319352403283, 0.0011057319352403283, 0.0011057319352403283, 0.0011057319352403283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011057319352403283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38687289
Iteration 2/25 | Loss: 0.00069452
Iteration 3/25 | Loss: 0.00069449
Iteration 4/25 | Loss: 0.00069449
Iteration 5/25 | Loss: 0.00069449
Iteration 6/25 | Loss: 0.00069449
Iteration 7/25 | Loss: 0.00069449
Iteration 8/25 | Loss: 0.00069449
Iteration 9/25 | Loss: 0.00069449
Iteration 10/25 | Loss: 0.00069449
Iteration 11/25 | Loss: 0.00069449
Iteration 12/25 | Loss: 0.00069449
Iteration 13/25 | Loss: 0.00069449
Iteration 14/25 | Loss: 0.00069449
Iteration 15/25 | Loss: 0.00069449
Iteration 16/25 | Loss: 0.00069449
Iteration 17/25 | Loss: 0.00069449
Iteration 18/25 | Loss: 0.00069449
Iteration 19/25 | Loss: 0.00069449
Iteration 20/25 | Loss: 0.00069449
Iteration 21/25 | Loss: 0.00069449
Iteration 22/25 | Loss: 0.00069449
Iteration 23/25 | Loss: 0.00069449
Iteration 24/25 | Loss: 0.00069449
Iteration 25/25 | Loss: 0.00069449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069449
Iteration 2/1000 | Loss: 0.00004458
Iteration 3/1000 | Loss: 0.00002865
Iteration 4/1000 | Loss: 0.00002350
Iteration 5/1000 | Loss: 0.00002146
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001884
Iteration 10/1000 | Loss: 0.00001850
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001823
Iteration 14/1000 | Loss: 0.00001819
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001804
Iteration 17/1000 | Loss: 0.00001803
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001791
Iteration 22/1000 | Loss: 0.00001787
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001780
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001777
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001773
Iteration 32/1000 | Loss: 0.00001773
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001772
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001770
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001770
Iteration 48/1000 | Loss: 0.00001770
Iteration 49/1000 | Loss: 0.00001770
Iteration 50/1000 | Loss: 0.00001770
Iteration 51/1000 | Loss: 0.00001770
Iteration 52/1000 | Loss: 0.00001770
Iteration 53/1000 | Loss: 0.00001770
Iteration 54/1000 | Loss: 0.00001770
Iteration 55/1000 | Loss: 0.00001770
Iteration 56/1000 | Loss: 0.00001770
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.7697699149721302e-05, 1.7697699149721302e-05, 1.7697699149721302e-05, 1.7697699149721302e-05, 1.7697699149721302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7697699149721302e-05

Optimization complete. Final v2v error: 3.498837471008301 mm

Highest mean error: 5.087849140167236 mm for frame 67

Lowest mean error: 2.890493392944336 mm for frame 98

Saving results

Total time: 32.44624137878418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412013
Iteration 2/25 | Loss: 0.00112712
Iteration 3/25 | Loss: 0.00105852
Iteration 4/25 | Loss: 0.00104815
Iteration 5/25 | Loss: 0.00104507
Iteration 6/25 | Loss: 0.00104494
Iteration 7/25 | Loss: 0.00104494
Iteration 8/25 | Loss: 0.00104494
Iteration 9/25 | Loss: 0.00104494
Iteration 10/25 | Loss: 0.00104494
Iteration 11/25 | Loss: 0.00104494
Iteration 12/25 | Loss: 0.00104494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010449367109686136, 0.0010449367109686136, 0.0010449367109686136, 0.0010449367109686136, 0.0010449367109686136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010449367109686136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19079828
Iteration 2/25 | Loss: 0.00060919
Iteration 3/25 | Loss: 0.00060919
Iteration 4/25 | Loss: 0.00060919
Iteration 5/25 | Loss: 0.00060919
Iteration 6/25 | Loss: 0.00060919
Iteration 7/25 | Loss: 0.00060919
Iteration 8/25 | Loss: 0.00060919
Iteration 9/25 | Loss: 0.00060919
Iteration 10/25 | Loss: 0.00060919
Iteration 11/25 | Loss: 0.00060919
Iteration 12/25 | Loss: 0.00060919
Iteration 13/25 | Loss: 0.00060919
Iteration 14/25 | Loss: 0.00060919
Iteration 15/25 | Loss: 0.00060919
Iteration 16/25 | Loss: 0.00060919
Iteration 17/25 | Loss: 0.00060919
Iteration 18/25 | Loss: 0.00060919
Iteration 19/25 | Loss: 0.00060919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006091900286264718, 0.0006091900286264718, 0.0006091900286264718, 0.0006091900286264718, 0.0006091900286264718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006091900286264718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060919
Iteration 2/1000 | Loss: 0.00001878
Iteration 3/1000 | Loss: 0.00001512
Iteration 4/1000 | Loss: 0.00001421
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001295
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001177
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001129
Iteration 50/1000 | Loss: 0.00001129
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001126
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001125
Iteration 59/1000 | Loss: 0.00001125
Iteration 60/1000 | Loss: 0.00001125
Iteration 61/1000 | Loss: 0.00001125
Iteration 62/1000 | Loss: 0.00001124
Iteration 63/1000 | Loss: 0.00001124
Iteration 64/1000 | Loss: 0.00001124
Iteration 65/1000 | Loss: 0.00001124
Iteration 66/1000 | Loss: 0.00001124
Iteration 67/1000 | Loss: 0.00001123
Iteration 68/1000 | Loss: 0.00001123
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001123
Iteration 72/1000 | Loss: 0.00001122
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001117
Iteration 91/1000 | Loss: 0.00001117
Iteration 92/1000 | Loss: 0.00001117
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001116
Iteration 95/1000 | Loss: 0.00001116
Iteration 96/1000 | Loss: 0.00001115
Iteration 97/1000 | Loss: 0.00001115
Iteration 98/1000 | Loss: 0.00001115
Iteration 99/1000 | Loss: 0.00001114
Iteration 100/1000 | Loss: 0.00001114
Iteration 101/1000 | Loss: 0.00001114
Iteration 102/1000 | Loss: 0.00001114
Iteration 103/1000 | Loss: 0.00001114
Iteration 104/1000 | Loss: 0.00001114
Iteration 105/1000 | Loss: 0.00001114
Iteration 106/1000 | Loss: 0.00001114
Iteration 107/1000 | Loss: 0.00001114
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001112
Iteration 116/1000 | Loss: 0.00001112
Iteration 117/1000 | Loss: 0.00001112
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001111
Iteration 122/1000 | Loss: 0.00001111
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001108
Iteration 143/1000 | Loss: 0.00001108
Iteration 144/1000 | Loss: 0.00001108
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001107
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1065633771067951e-05, 1.1065633771067951e-05, 1.1065633771067951e-05, 1.1065633771067951e-05, 1.1065633771067951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1065633771067951e-05

Optimization complete. Final v2v error: 2.867900848388672 mm

Highest mean error: 3.127558946609497 mm for frame 170

Lowest mean error: 2.6718337535858154 mm for frame 11

Saving results

Total time: 42.345420837402344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383031
Iteration 2/25 | Loss: 0.00115539
Iteration 3/25 | Loss: 0.00104295
Iteration 4/25 | Loss: 0.00103238
Iteration 5/25 | Loss: 0.00103021
Iteration 6/25 | Loss: 0.00102973
Iteration 7/25 | Loss: 0.00102973
Iteration 8/25 | Loss: 0.00102973
Iteration 9/25 | Loss: 0.00102973
Iteration 10/25 | Loss: 0.00102973
Iteration 11/25 | Loss: 0.00102973
Iteration 12/25 | Loss: 0.00102973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010297332191839814, 0.0010297332191839814, 0.0010297332191839814, 0.0010297332191839814, 0.0010297332191839814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010297332191839814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37415123
Iteration 2/25 | Loss: 0.00058648
Iteration 3/25 | Loss: 0.00058648
Iteration 4/25 | Loss: 0.00058648
Iteration 5/25 | Loss: 0.00058648
Iteration 6/25 | Loss: 0.00058648
Iteration 7/25 | Loss: 0.00058648
Iteration 8/25 | Loss: 0.00058648
Iteration 9/25 | Loss: 0.00058648
Iteration 10/25 | Loss: 0.00058648
Iteration 11/25 | Loss: 0.00058648
Iteration 12/25 | Loss: 0.00058648
Iteration 13/25 | Loss: 0.00058648
Iteration 14/25 | Loss: 0.00058648
Iteration 15/25 | Loss: 0.00058648
Iteration 16/25 | Loss: 0.00058648
Iteration 17/25 | Loss: 0.00058648
Iteration 18/25 | Loss: 0.00058648
Iteration 19/25 | Loss: 0.00058648
Iteration 20/25 | Loss: 0.00058648
Iteration 21/25 | Loss: 0.00058648
Iteration 22/25 | Loss: 0.00058648
Iteration 23/25 | Loss: 0.00058648
Iteration 24/25 | Loss: 0.00058648
Iteration 25/25 | Loss: 0.00058648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058648
Iteration 2/1000 | Loss: 0.00002725
Iteration 3/1000 | Loss: 0.00001609
Iteration 4/1000 | Loss: 0.00001319
Iteration 5/1000 | Loss: 0.00001207
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001060
Iteration 9/1000 | Loss: 0.00001039
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001016
Iteration 14/1000 | Loss: 0.00001013
Iteration 15/1000 | Loss: 0.00001012
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001010
Iteration 20/1000 | Loss: 0.00001009
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001002
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000996
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000995
Iteration 33/1000 | Loss: 0.00000995
Iteration 34/1000 | Loss: 0.00000994
Iteration 35/1000 | Loss: 0.00000994
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000992
Iteration 40/1000 | Loss: 0.00000992
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000990
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000983
Iteration 51/1000 | Loss: 0.00000983
Iteration 52/1000 | Loss: 0.00000983
Iteration 53/1000 | Loss: 0.00000983
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000982
Iteration 57/1000 | Loss: 0.00000982
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000980
Iteration 64/1000 | Loss: 0.00000980
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000979
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000978
Iteration 70/1000 | Loss: 0.00000978
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000976
Iteration 85/1000 | Loss: 0.00000976
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000974
Iteration 99/1000 | Loss: 0.00000974
Iteration 100/1000 | Loss: 0.00000974
Iteration 101/1000 | Loss: 0.00000974
Iteration 102/1000 | Loss: 0.00000974
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000973
Iteration 109/1000 | Loss: 0.00000973
Iteration 110/1000 | Loss: 0.00000972
Iteration 111/1000 | Loss: 0.00000972
Iteration 112/1000 | Loss: 0.00000972
Iteration 113/1000 | Loss: 0.00000971
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000970
Iteration 117/1000 | Loss: 0.00000970
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000969
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000969
Iteration 125/1000 | Loss: 0.00000968
Iteration 126/1000 | Loss: 0.00000968
Iteration 127/1000 | Loss: 0.00000968
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000967
Iteration 133/1000 | Loss: 0.00000967
Iteration 134/1000 | Loss: 0.00000967
Iteration 135/1000 | Loss: 0.00000966
Iteration 136/1000 | Loss: 0.00000966
Iteration 137/1000 | Loss: 0.00000966
Iteration 138/1000 | Loss: 0.00000966
Iteration 139/1000 | Loss: 0.00000966
Iteration 140/1000 | Loss: 0.00000965
Iteration 141/1000 | Loss: 0.00000965
Iteration 142/1000 | Loss: 0.00000965
Iteration 143/1000 | Loss: 0.00000964
Iteration 144/1000 | Loss: 0.00000964
Iteration 145/1000 | Loss: 0.00000963
Iteration 146/1000 | Loss: 0.00000963
Iteration 147/1000 | Loss: 0.00000963
Iteration 148/1000 | Loss: 0.00000963
Iteration 149/1000 | Loss: 0.00000963
Iteration 150/1000 | Loss: 0.00000963
Iteration 151/1000 | Loss: 0.00000962
Iteration 152/1000 | Loss: 0.00000962
Iteration 153/1000 | Loss: 0.00000962
Iteration 154/1000 | Loss: 0.00000962
Iteration 155/1000 | Loss: 0.00000961
Iteration 156/1000 | Loss: 0.00000961
Iteration 157/1000 | Loss: 0.00000961
Iteration 158/1000 | Loss: 0.00000961
Iteration 159/1000 | Loss: 0.00000960
Iteration 160/1000 | Loss: 0.00000960
Iteration 161/1000 | Loss: 0.00000960
Iteration 162/1000 | Loss: 0.00000959
Iteration 163/1000 | Loss: 0.00000959
Iteration 164/1000 | Loss: 0.00000959
Iteration 165/1000 | Loss: 0.00000959
Iteration 166/1000 | Loss: 0.00000959
Iteration 167/1000 | Loss: 0.00000959
Iteration 168/1000 | Loss: 0.00000959
Iteration 169/1000 | Loss: 0.00000958
Iteration 170/1000 | Loss: 0.00000958
Iteration 171/1000 | Loss: 0.00000958
Iteration 172/1000 | Loss: 0.00000958
Iteration 173/1000 | Loss: 0.00000957
Iteration 174/1000 | Loss: 0.00000957
Iteration 175/1000 | Loss: 0.00000957
Iteration 176/1000 | Loss: 0.00000957
Iteration 177/1000 | Loss: 0.00000956
Iteration 178/1000 | Loss: 0.00000956
Iteration 179/1000 | Loss: 0.00000956
Iteration 180/1000 | Loss: 0.00000956
Iteration 181/1000 | Loss: 0.00000956
Iteration 182/1000 | Loss: 0.00000956
Iteration 183/1000 | Loss: 0.00000955
Iteration 184/1000 | Loss: 0.00000955
Iteration 185/1000 | Loss: 0.00000955
Iteration 186/1000 | Loss: 0.00000955
Iteration 187/1000 | Loss: 0.00000955
Iteration 188/1000 | Loss: 0.00000955
Iteration 189/1000 | Loss: 0.00000955
Iteration 190/1000 | Loss: 0.00000954
Iteration 191/1000 | Loss: 0.00000954
Iteration 192/1000 | Loss: 0.00000954
Iteration 193/1000 | Loss: 0.00000954
Iteration 194/1000 | Loss: 0.00000953
Iteration 195/1000 | Loss: 0.00000953
Iteration 196/1000 | Loss: 0.00000953
Iteration 197/1000 | Loss: 0.00000953
Iteration 198/1000 | Loss: 0.00000953
Iteration 199/1000 | Loss: 0.00000953
Iteration 200/1000 | Loss: 0.00000953
Iteration 201/1000 | Loss: 0.00000952
Iteration 202/1000 | Loss: 0.00000952
Iteration 203/1000 | Loss: 0.00000952
Iteration 204/1000 | Loss: 0.00000952
Iteration 205/1000 | Loss: 0.00000952
Iteration 206/1000 | Loss: 0.00000951
Iteration 207/1000 | Loss: 0.00000951
Iteration 208/1000 | Loss: 0.00000951
Iteration 209/1000 | Loss: 0.00000951
Iteration 210/1000 | Loss: 0.00000951
Iteration 211/1000 | Loss: 0.00000951
Iteration 212/1000 | Loss: 0.00000951
Iteration 213/1000 | Loss: 0.00000951
Iteration 214/1000 | Loss: 0.00000951
Iteration 215/1000 | Loss: 0.00000951
Iteration 216/1000 | Loss: 0.00000951
Iteration 217/1000 | Loss: 0.00000951
Iteration 218/1000 | Loss: 0.00000951
Iteration 219/1000 | Loss: 0.00000951
Iteration 220/1000 | Loss: 0.00000951
Iteration 221/1000 | Loss: 0.00000951
Iteration 222/1000 | Loss: 0.00000951
Iteration 223/1000 | Loss: 0.00000951
Iteration 224/1000 | Loss: 0.00000951
Iteration 225/1000 | Loss: 0.00000951
Iteration 226/1000 | Loss: 0.00000951
Iteration 227/1000 | Loss: 0.00000951
Iteration 228/1000 | Loss: 0.00000951
Iteration 229/1000 | Loss: 0.00000951
Iteration 230/1000 | Loss: 0.00000951
Iteration 231/1000 | Loss: 0.00000951
Iteration 232/1000 | Loss: 0.00000951
Iteration 233/1000 | Loss: 0.00000951
Iteration 234/1000 | Loss: 0.00000951
Iteration 235/1000 | Loss: 0.00000951
Iteration 236/1000 | Loss: 0.00000951
Iteration 237/1000 | Loss: 0.00000951
Iteration 238/1000 | Loss: 0.00000951
Iteration 239/1000 | Loss: 0.00000951
Iteration 240/1000 | Loss: 0.00000951
Iteration 241/1000 | Loss: 0.00000951
Iteration 242/1000 | Loss: 0.00000951
Iteration 243/1000 | Loss: 0.00000951
Iteration 244/1000 | Loss: 0.00000951
Iteration 245/1000 | Loss: 0.00000951
Iteration 246/1000 | Loss: 0.00000951
Iteration 247/1000 | Loss: 0.00000951
Iteration 248/1000 | Loss: 0.00000951
Iteration 249/1000 | Loss: 0.00000951
Iteration 250/1000 | Loss: 0.00000951
Iteration 251/1000 | Loss: 0.00000951
Iteration 252/1000 | Loss: 0.00000951
Iteration 253/1000 | Loss: 0.00000951
Iteration 254/1000 | Loss: 0.00000951
Iteration 255/1000 | Loss: 0.00000951
Iteration 256/1000 | Loss: 0.00000951
Iteration 257/1000 | Loss: 0.00000951
Iteration 258/1000 | Loss: 0.00000951
Iteration 259/1000 | Loss: 0.00000951
Iteration 260/1000 | Loss: 0.00000951
Iteration 261/1000 | Loss: 0.00000951
Iteration 262/1000 | Loss: 0.00000951
Iteration 263/1000 | Loss: 0.00000951
Iteration 264/1000 | Loss: 0.00000951
Iteration 265/1000 | Loss: 0.00000951
Iteration 266/1000 | Loss: 0.00000951
Iteration 267/1000 | Loss: 0.00000951
Iteration 268/1000 | Loss: 0.00000951
Iteration 269/1000 | Loss: 0.00000951
Iteration 270/1000 | Loss: 0.00000951
Iteration 271/1000 | Loss: 0.00000951
Iteration 272/1000 | Loss: 0.00000951
Iteration 273/1000 | Loss: 0.00000951
Iteration 274/1000 | Loss: 0.00000951
Iteration 275/1000 | Loss: 0.00000951
Iteration 276/1000 | Loss: 0.00000951
Iteration 277/1000 | Loss: 0.00000951
Iteration 278/1000 | Loss: 0.00000951
Iteration 279/1000 | Loss: 0.00000951
Iteration 280/1000 | Loss: 0.00000951
Iteration 281/1000 | Loss: 0.00000951
Iteration 282/1000 | Loss: 0.00000951
Iteration 283/1000 | Loss: 0.00000951
Iteration 284/1000 | Loss: 0.00000951
Iteration 285/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [9.508787115919404e-06, 9.508787115919404e-06, 9.508787115919404e-06, 9.508787115919404e-06, 9.508787115919404e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.508787115919404e-06

Optimization complete. Final v2v error: 2.648635149002075 mm

Highest mean error: 2.90480637550354 mm for frame 42

Lowest mean error: 2.507331371307373 mm for frame 100

Saving results

Total time: 42.20771336555481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047591
Iteration 2/25 | Loss: 0.00354971
Iteration 3/25 | Loss: 0.00195446
Iteration 4/25 | Loss: 0.00185255
Iteration 5/25 | Loss: 0.00170493
Iteration 6/25 | Loss: 0.00162231
Iteration 7/25 | Loss: 0.00158595
Iteration 8/25 | Loss: 0.00159471
Iteration 9/25 | Loss: 0.00151519
Iteration 10/25 | Loss: 0.00152698
Iteration 11/25 | Loss: 0.00148443
Iteration 12/25 | Loss: 0.00142533
Iteration 13/25 | Loss: 0.00141373
Iteration 14/25 | Loss: 0.00140752
Iteration 15/25 | Loss: 0.00139669
Iteration 16/25 | Loss: 0.00139693
Iteration 17/25 | Loss: 0.00139359
Iteration 18/25 | Loss: 0.00139328
Iteration 19/25 | Loss: 0.00138866
Iteration 20/25 | Loss: 0.00138887
Iteration 21/25 | Loss: 0.00138523
Iteration 22/25 | Loss: 0.00138750
Iteration 23/25 | Loss: 0.00138368
Iteration 24/25 | Loss: 0.00137927
Iteration 25/25 | Loss: 0.00137889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34631538
Iteration 2/25 | Loss: 0.00541122
Iteration 3/25 | Loss: 0.00304879
Iteration 4/25 | Loss: 0.00304879
Iteration 5/25 | Loss: 0.00304879
Iteration 6/25 | Loss: 0.00304879
Iteration 7/25 | Loss: 0.00304878
Iteration 8/25 | Loss: 0.00304878
Iteration 9/25 | Loss: 0.00304878
Iteration 10/25 | Loss: 0.00304878
Iteration 11/25 | Loss: 0.00304878
Iteration 12/25 | Loss: 0.00304878
Iteration 13/25 | Loss: 0.00304878
Iteration 14/25 | Loss: 0.00304878
Iteration 15/25 | Loss: 0.00304878
Iteration 16/25 | Loss: 0.00304878
Iteration 17/25 | Loss: 0.00304878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0030487836338579655, 0.0030487836338579655, 0.0030487836338579655, 0.0030487836338579655, 0.0030487836338579655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030487836338579655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304878
Iteration 2/1000 | Loss: 0.00268672
Iteration 3/1000 | Loss: 0.00183055
Iteration 4/1000 | Loss: 0.00379622
Iteration 5/1000 | Loss: 0.00092617
Iteration 6/1000 | Loss: 0.00230971
Iteration 7/1000 | Loss: 0.00096279
Iteration 8/1000 | Loss: 0.00323557
Iteration 9/1000 | Loss: 0.00318438
Iteration 10/1000 | Loss: 0.00433759
Iteration 11/1000 | Loss: 0.00292905
Iteration 12/1000 | Loss: 0.00277779
Iteration 13/1000 | Loss: 0.00357119
Iteration 14/1000 | Loss: 0.00151238
Iteration 15/1000 | Loss: 0.00310577
Iteration 16/1000 | Loss: 0.00193970
Iteration 17/1000 | Loss: 0.00356788
Iteration 18/1000 | Loss: 0.00255107
Iteration 19/1000 | Loss: 0.00349834
Iteration 20/1000 | Loss: 0.00102884
Iteration 21/1000 | Loss: 0.00136397
Iteration 22/1000 | Loss: 0.00019649
Iteration 23/1000 | Loss: 0.00024779
Iteration 24/1000 | Loss: 0.00064873
Iteration 25/1000 | Loss: 0.00051569
Iteration 26/1000 | Loss: 0.00169415
Iteration 27/1000 | Loss: 0.00189005
Iteration 28/1000 | Loss: 0.00289931
Iteration 29/1000 | Loss: 0.00378105
Iteration 30/1000 | Loss: 0.00203138
Iteration 31/1000 | Loss: 0.00034409
Iteration 32/1000 | Loss: 0.00057088
Iteration 33/1000 | Loss: 0.00022577
Iteration 34/1000 | Loss: 0.00229580
Iteration 35/1000 | Loss: 0.00285110
Iteration 36/1000 | Loss: 0.00065422
Iteration 37/1000 | Loss: 0.00424937
Iteration 38/1000 | Loss: 0.00287464
Iteration 39/1000 | Loss: 0.00183846
Iteration 40/1000 | Loss: 0.00183765
Iteration 41/1000 | Loss: 0.00212184
Iteration 42/1000 | Loss: 0.00213289
Iteration 43/1000 | Loss: 0.00510355
Iteration 44/1000 | Loss: 0.00355561
Iteration 45/1000 | Loss: 0.00309717
Iteration 46/1000 | Loss: 0.00334819
Iteration 47/1000 | Loss: 0.00845713
Iteration 48/1000 | Loss: 0.00361159
Iteration 49/1000 | Loss: 0.00255500
Iteration 50/1000 | Loss: 0.00365647
Iteration 51/1000 | Loss: 0.00380398
Iteration 52/1000 | Loss: 0.00397325
Iteration 53/1000 | Loss: 0.00619979
Iteration 54/1000 | Loss: 0.00391660
Iteration 55/1000 | Loss: 0.00275566
Iteration 56/1000 | Loss: 0.00459569
Iteration 57/1000 | Loss: 0.00333554
Iteration 58/1000 | Loss: 0.00282812
Iteration 59/1000 | Loss: 0.00145366
Iteration 60/1000 | Loss: 0.00412972
Iteration 61/1000 | Loss: 0.00109462
Iteration 62/1000 | Loss: 0.00122585
Iteration 63/1000 | Loss: 0.00178074
Iteration 64/1000 | Loss: 0.00290543
Iteration 65/1000 | Loss: 0.00186082
Iteration 66/1000 | Loss: 0.00396178
Iteration 67/1000 | Loss: 0.00213243
Iteration 68/1000 | Loss: 0.00306161
Iteration 69/1000 | Loss: 0.00325258
Iteration 70/1000 | Loss: 0.00160510
Iteration 71/1000 | Loss: 0.00158282
Iteration 72/1000 | Loss: 0.00134278
Iteration 73/1000 | Loss: 0.00395255
Iteration 74/1000 | Loss: 0.00175908
Iteration 75/1000 | Loss: 0.00145411
Iteration 76/1000 | Loss: 0.00141785
Iteration 77/1000 | Loss: 0.00312963
Iteration 78/1000 | Loss: 0.00280117
Iteration 79/1000 | Loss: 0.00269088
Iteration 80/1000 | Loss: 0.00278802
Iteration 81/1000 | Loss: 0.00260279
Iteration 82/1000 | Loss: 0.00149795
Iteration 83/1000 | Loss: 0.00175985
Iteration 84/1000 | Loss: 0.00167844
Iteration 85/1000 | Loss: 0.00107995
Iteration 86/1000 | Loss: 0.00087637
Iteration 87/1000 | Loss: 0.00148220
Iteration 88/1000 | Loss: 0.00064871
Iteration 89/1000 | Loss: 0.00051681
Iteration 90/1000 | Loss: 0.00052705
Iteration 91/1000 | Loss: 0.00060152
Iteration 92/1000 | Loss: 0.00266031
Iteration 93/1000 | Loss: 0.00046097
Iteration 94/1000 | Loss: 0.00088275
Iteration 95/1000 | Loss: 0.00042831
Iteration 96/1000 | Loss: 0.00063156
Iteration 97/1000 | Loss: 0.00210213
Iteration 98/1000 | Loss: 0.00059295
Iteration 99/1000 | Loss: 0.00048414
Iteration 100/1000 | Loss: 0.00016015
Iteration 101/1000 | Loss: 0.00021800
Iteration 102/1000 | Loss: 0.00007008
Iteration 103/1000 | Loss: 0.00031442
Iteration 104/1000 | Loss: 0.00177076
Iteration 105/1000 | Loss: 0.00080758
Iteration 106/1000 | Loss: 0.00007332
Iteration 107/1000 | Loss: 0.00023128
Iteration 108/1000 | Loss: 0.00010918
Iteration 109/1000 | Loss: 0.00005021
Iteration 110/1000 | Loss: 0.00004150
Iteration 111/1000 | Loss: 0.00008457
Iteration 112/1000 | Loss: 0.00025216
Iteration 113/1000 | Loss: 0.00010276
Iteration 114/1000 | Loss: 0.00024984
Iteration 115/1000 | Loss: 0.00004900
Iteration 116/1000 | Loss: 0.00005646
Iteration 117/1000 | Loss: 0.00004962
Iteration 118/1000 | Loss: 0.00064771
Iteration 119/1000 | Loss: 0.00008447
Iteration 120/1000 | Loss: 0.00005418
Iteration 121/1000 | Loss: 0.00040296
Iteration 122/1000 | Loss: 0.00004398
Iteration 123/1000 | Loss: 0.00006096
Iteration 124/1000 | Loss: 0.00009074
Iteration 125/1000 | Loss: 0.00012328
Iteration 126/1000 | Loss: 0.00004218
Iteration 127/1000 | Loss: 0.00003232
Iteration 128/1000 | Loss: 0.00003985
Iteration 129/1000 | Loss: 0.00003497
Iteration 130/1000 | Loss: 0.00003803
Iteration 131/1000 | Loss: 0.00005545
Iteration 132/1000 | Loss: 0.00003262
Iteration 133/1000 | Loss: 0.00003784
Iteration 134/1000 | Loss: 0.00003301
Iteration 135/1000 | Loss: 0.00003821
Iteration 136/1000 | Loss: 0.00005784
Iteration 137/1000 | Loss: 0.00002574
Iteration 138/1000 | Loss: 0.00002168
Iteration 139/1000 | Loss: 0.00004546
Iteration 140/1000 | Loss: 0.00006261
Iteration 141/1000 | Loss: 0.00015447
Iteration 142/1000 | Loss: 0.00006085
Iteration 143/1000 | Loss: 0.00003595
Iteration 144/1000 | Loss: 0.00004747
Iteration 145/1000 | Loss: 0.00005526
Iteration 146/1000 | Loss: 0.00004763
Iteration 147/1000 | Loss: 0.00004758
Iteration 148/1000 | Loss: 0.00004504
Iteration 149/1000 | Loss: 0.00003778
Iteration 150/1000 | Loss: 0.00004656
Iteration 151/1000 | Loss: 0.00004105
Iteration 152/1000 | Loss: 0.00005720
Iteration 153/1000 | Loss: 0.00004201
Iteration 154/1000 | Loss: 0.00004764
Iteration 155/1000 | Loss: 0.00004589
Iteration 156/1000 | Loss: 0.00004685
Iteration 157/1000 | Loss: 0.00005142
Iteration 158/1000 | Loss: 0.00004624
Iteration 159/1000 | Loss: 0.00003833
Iteration 160/1000 | Loss: 0.00004380
Iteration 161/1000 | Loss: 0.00004440
Iteration 162/1000 | Loss: 0.00004297
Iteration 163/1000 | Loss: 0.00003887
Iteration 164/1000 | Loss: 0.00005438
Iteration 165/1000 | Loss: 0.00004264
Iteration 166/1000 | Loss: 0.00004731
Iteration 167/1000 | Loss: 0.00002511
Iteration 168/1000 | Loss: 0.00004145
Iteration 169/1000 | Loss: 0.00005562
Iteration 170/1000 | Loss: 0.00004001
Iteration 171/1000 | Loss: 0.00003947
Iteration 172/1000 | Loss: 0.00002919
Iteration 173/1000 | Loss: 0.00002827
Iteration 174/1000 | Loss: 0.00004514
Iteration 175/1000 | Loss: 0.00004837
Iteration 176/1000 | Loss: 0.00004053
Iteration 177/1000 | Loss: 0.00011096
Iteration 178/1000 | Loss: 0.00004876
Iteration 179/1000 | Loss: 0.00003905
Iteration 180/1000 | Loss: 0.00004622
Iteration 181/1000 | Loss: 0.00003945
Iteration 182/1000 | Loss: 0.00004078
Iteration 183/1000 | Loss: 0.00007543
Iteration 184/1000 | Loss: 0.00005416
Iteration 185/1000 | Loss: 0.00006277
Iteration 186/1000 | Loss: 0.00005868
Iteration 187/1000 | Loss: 0.00004816
Iteration 188/1000 | Loss: 0.00004274
Iteration 189/1000 | Loss: 0.00004035
Iteration 190/1000 | Loss: 0.00006742
Iteration 191/1000 | Loss: 0.00005160
Iteration 192/1000 | Loss: 0.00003652
Iteration 193/1000 | Loss: 0.00002507
Iteration 194/1000 | Loss: 0.00005958
Iteration 195/1000 | Loss: 0.00020407
Iteration 196/1000 | Loss: 0.00019970
Iteration 197/1000 | Loss: 0.00003522
Iteration 198/1000 | Loss: 0.00004056
Iteration 199/1000 | Loss: 0.00003488
Iteration 200/1000 | Loss: 0.00009059
Iteration 201/1000 | Loss: 0.00004856
Iteration 202/1000 | Loss: 0.00002560
Iteration 203/1000 | Loss: 0.00003718
Iteration 204/1000 | Loss: 0.00003975
Iteration 205/1000 | Loss: 0.00002214
Iteration 206/1000 | Loss: 0.00002135
Iteration 207/1000 | Loss: 0.00003834
Iteration 208/1000 | Loss: 0.00004308
Iteration 209/1000 | Loss: 0.00002181
Iteration 210/1000 | Loss: 0.00002883
Iteration 211/1000 | Loss: 0.00002006
Iteration 212/1000 | Loss: 0.00002665
Iteration 213/1000 | Loss: 0.00001957
Iteration 214/1000 | Loss: 0.00001636
Iteration 215/1000 | Loss: 0.00001990
Iteration 216/1000 | Loss: 0.00001541
Iteration 217/1000 | Loss: 0.00002502
Iteration 218/1000 | Loss: 0.00004426
Iteration 219/1000 | Loss: 0.00001501
Iteration 220/1000 | Loss: 0.00001797
Iteration 221/1000 | Loss: 0.00001477
Iteration 222/1000 | Loss: 0.00001462
Iteration 223/1000 | Loss: 0.00001460
Iteration 224/1000 | Loss: 0.00001455
Iteration 225/1000 | Loss: 0.00001453
Iteration 226/1000 | Loss: 0.00001452
Iteration 227/1000 | Loss: 0.00001452
Iteration 228/1000 | Loss: 0.00001450
Iteration 229/1000 | Loss: 0.00001450
Iteration 230/1000 | Loss: 0.00001449
Iteration 231/1000 | Loss: 0.00001448
Iteration 232/1000 | Loss: 0.00001447
Iteration 233/1000 | Loss: 0.00001446
Iteration 234/1000 | Loss: 0.00001445
Iteration 235/1000 | Loss: 0.00001441
Iteration 236/1000 | Loss: 0.00001441
Iteration 237/1000 | Loss: 0.00001440
Iteration 238/1000 | Loss: 0.00001440
Iteration 239/1000 | Loss: 0.00001440
Iteration 240/1000 | Loss: 0.00001439
Iteration 241/1000 | Loss: 0.00001439
Iteration 242/1000 | Loss: 0.00001439
Iteration 243/1000 | Loss: 0.00001439
Iteration 244/1000 | Loss: 0.00001438
Iteration 245/1000 | Loss: 0.00001438
Iteration 246/1000 | Loss: 0.00001437
Iteration 247/1000 | Loss: 0.00001437
Iteration 248/1000 | Loss: 0.00001436
Iteration 249/1000 | Loss: 0.00001436
Iteration 250/1000 | Loss: 0.00001436
Iteration 251/1000 | Loss: 0.00001436
Iteration 252/1000 | Loss: 0.00001436
Iteration 253/1000 | Loss: 0.00001435
Iteration 254/1000 | Loss: 0.00001435
Iteration 255/1000 | Loss: 0.00001435
Iteration 256/1000 | Loss: 0.00001435
Iteration 257/1000 | Loss: 0.00001434
Iteration 258/1000 | Loss: 0.00001434
Iteration 259/1000 | Loss: 0.00001434
Iteration 260/1000 | Loss: 0.00001433
Iteration 261/1000 | Loss: 0.00001433
Iteration 262/1000 | Loss: 0.00001432
Iteration 263/1000 | Loss: 0.00001432
Iteration 264/1000 | Loss: 0.00001431
Iteration 265/1000 | Loss: 0.00001431
Iteration 266/1000 | Loss: 0.00001431
Iteration 267/1000 | Loss: 0.00001431
Iteration 268/1000 | Loss: 0.00001430
Iteration 269/1000 | Loss: 0.00001430
Iteration 270/1000 | Loss: 0.00001430
Iteration 271/1000 | Loss: 0.00001429
Iteration 272/1000 | Loss: 0.00001429
Iteration 273/1000 | Loss: 0.00001429
Iteration 274/1000 | Loss: 0.00001429
Iteration 275/1000 | Loss: 0.00001428
Iteration 276/1000 | Loss: 0.00001428
Iteration 277/1000 | Loss: 0.00001428
Iteration 278/1000 | Loss: 0.00001427
Iteration 279/1000 | Loss: 0.00001427
Iteration 280/1000 | Loss: 0.00001427
Iteration 281/1000 | Loss: 0.00001427
Iteration 282/1000 | Loss: 0.00001427
Iteration 283/1000 | Loss: 0.00001427
Iteration 284/1000 | Loss: 0.00001427
Iteration 285/1000 | Loss: 0.00001426
Iteration 286/1000 | Loss: 0.00001426
Iteration 287/1000 | Loss: 0.00001426
Iteration 288/1000 | Loss: 0.00001426
Iteration 289/1000 | Loss: 0.00001426
Iteration 290/1000 | Loss: 0.00001426
Iteration 291/1000 | Loss: 0.00001426
Iteration 292/1000 | Loss: 0.00001426
Iteration 293/1000 | Loss: 0.00001426
Iteration 294/1000 | Loss: 0.00001426
Iteration 295/1000 | Loss: 0.00001425
Iteration 296/1000 | Loss: 0.00001425
Iteration 297/1000 | Loss: 0.00001425
Iteration 298/1000 | Loss: 0.00001425
Iteration 299/1000 | Loss: 0.00001425
Iteration 300/1000 | Loss: 0.00001425
Iteration 301/1000 | Loss: 0.00001425
Iteration 302/1000 | Loss: 0.00001425
Iteration 303/1000 | Loss: 0.00001425
Iteration 304/1000 | Loss: 0.00001425
Iteration 305/1000 | Loss: 0.00001425
Iteration 306/1000 | Loss: 0.00001425
Iteration 307/1000 | Loss: 0.00001425
Iteration 308/1000 | Loss: 0.00001425
Iteration 309/1000 | Loss: 0.00001425
Iteration 310/1000 | Loss: 0.00001424
Iteration 311/1000 | Loss: 0.00001424
Iteration 312/1000 | Loss: 0.00001424
Iteration 313/1000 | Loss: 0.00001424
Iteration 314/1000 | Loss: 0.00001424
Iteration 315/1000 | Loss: 0.00001424
Iteration 316/1000 | Loss: 0.00001424
Iteration 317/1000 | Loss: 0.00001424
Iteration 318/1000 | Loss: 0.00001424
Iteration 319/1000 | Loss: 0.00001424
Iteration 320/1000 | Loss: 0.00001423
Iteration 321/1000 | Loss: 0.00001423
Iteration 322/1000 | Loss: 0.00001423
Iteration 323/1000 | Loss: 0.00001422
Iteration 324/1000 | Loss: 0.00001422
Iteration 325/1000 | Loss: 0.00001422
Iteration 326/1000 | Loss: 0.00001422
Iteration 327/1000 | Loss: 0.00001421
Iteration 328/1000 | Loss: 0.00001421
Iteration 329/1000 | Loss: 0.00001421
Iteration 330/1000 | Loss: 0.00001420
Iteration 331/1000 | Loss: 0.00001420
Iteration 332/1000 | Loss: 0.00001420
Iteration 333/1000 | Loss: 0.00001420
Iteration 334/1000 | Loss: 0.00001420
Iteration 335/1000 | Loss: 0.00001420
Iteration 336/1000 | Loss: 0.00001420
Iteration 337/1000 | Loss: 0.00001420
Iteration 338/1000 | Loss: 0.00001420
Iteration 339/1000 | Loss: 0.00001420
Iteration 340/1000 | Loss: 0.00001420
Iteration 341/1000 | Loss: 0.00001419
Iteration 342/1000 | Loss: 0.00001419
Iteration 343/1000 | Loss: 0.00001419
Iteration 344/1000 | Loss: 0.00001419
Iteration 345/1000 | Loss: 0.00001419
Iteration 346/1000 | Loss: 0.00001419
Iteration 347/1000 | Loss: 0.00001419
Iteration 348/1000 | Loss: 0.00001419
Iteration 349/1000 | Loss: 0.00001419
Iteration 350/1000 | Loss: 0.00001419
Iteration 351/1000 | Loss: 0.00001419
Iteration 352/1000 | Loss: 0.00001419
Iteration 353/1000 | Loss: 0.00001419
Iteration 354/1000 | Loss: 0.00001419
Iteration 355/1000 | Loss: 0.00001419
Iteration 356/1000 | Loss: 0.00001419
Iteration 357/1000 | Loss: 0.00001418
Iteration 358/1000 | Loss: 0.00001418
Iteration 359/1000 | Loss: 0.00001418
Iteration 360/1000 | Loss: 0.00001418
Iteration 361/1000 | Loss: 0.00001418
Iteration 362/1000 | Loss: 0.00001418
Iteration 363/1000 | Loss: 0.00001418
Iteration 364/1000 | Loss: 0.00001418
Iteration 365/1000 | Loss: 0.00001418
Iteration 366/1000 | Loss: 0.00001418
Iteration 367/1000 | Loss: 0.00001418
Iteration 368/1000 | Loss: 0.00001418
Iteration 369/1000 | Loss: 0.00001418
Iteration 370/1000 | Loss: 0.00001418
Iteration 371/1000 | Loss: 0.00001418
Iteration 372/1000 | Loss: 0.00001418
Iteration 373/1000 | Loss: 0.00001418
Iteration 374/1000 | Loss: 0.00001418
Iteration 375/1000 | Loss: 0.00001418
Iteration 376/1000 | Loss: 0.00001418
Iteration 377/1000 | Loss: 0.00001418
Iteration 378/1000 | Loss: 0.00001418
Iteration 379/1000 | Loss: 0.00001418
Iteration 380/1000 | Loss: 0.00001418
Iteration 381/1000 | Loss: 0.00001418
Iteration 382/1000 | Loss: 0.00001418
Iteration 383/1000 | Loss: 0.00001418
Iteration 384/1000 | Loss: 0.00001418
Iteration 385/1000 | Loss: 0.00001418
Iteration 386/1000 | Loss: 0.00001418
Iteration 387/1000 | Loss: 0.00001418
Iteration 388/1000 | Loss: 0.00001418
Iteration 389/1000 | Loss: 0.00001418
Iteration 390/1000 | Loss: 0.00001418
Iteration 391/1000 | Loss: 0.00001418
Iteration 392/1000 | Loss: 0.00001418
Iteration 393/1000 | Loss: 0.00001418
Iteration 394/1000 | Loss: 0.00001418
Iteration 395/1000 | Loss: 0.00001418
Iteration 396/1000 | Loss: 0.00001418
Iteration 397/1000 | Loss: 0.00001418
Iteration 398/1000 | Loss: 0.00001418
Iteration 399/1000 | Loss: 0.00001418
Iteration 400/1000 | Loss: 0.00001418
Iteration 401/1000 | Loss: 0.00001418
Iteration 402/1000 | Loss: 0.00001418
Iteration 403/1000 | Loss: 0.00001418
Iteration 404/1000 | Loss: 0.00001418
Iteration 405/1000 | Loss: 0.00001418
Iteration 406/1000 | Loss: 0.00001418
Iteration 407/1000 | Loss: 0.00001418
Iteration 408/1000 | Loss: 0.00001418
Iteration 409/1000 | Loss: 0.00001418
Iteration 410/1000 | Loss: 0.00001418
Iteration 411/1000 | Loss: 0.00001418
Iteration 412/1000 | Loss: 0.00001418
Iteration 413/1000 | Loss: 0.00001418
Iteration 414/1000 | Loss: 0.00001418
Iteration 415/1000 | Loss: 0.00001418
Iteration 416/1000 | Loss: 0.00001418
Iteration 417/1000 | Loss: 0.00001418
Iteration 418/1000 | Loss: 0.00001418
Iteration 419/1000 | Loss: 0.00001418
Iteration 420/1000 | Loss: 0.00001418
Iteration 421/1000 | Loss: 0.00001418
Iteration 422/1000 | Loss: 0.00001418
Iteration 423/1000 | Loss: 0.00001418
Iteration 424/1000 | Loss: 0.00001418
Iteration 425/1000 | Loss: 0.00001418
Iteration 426/1000 | Loss: 0.00001418
Iteration 427/1000 | Loss: 0.00001418
Iteration 428/1000 | Loss: 0.00001418
Iteration 429/1000 | Loss: 0.00001418
Iteration 430/1000 | Loss: 0.00001418
Iteration 431/1000 | Loss: 0.00001418
Iteration 432/1000 | Loss: 0.00001418
Iteration 433/1000 | Loss: 0.00001418
Iteration 434/1000 | Loss: 0.00001418
Iteration 435/1000 | Loss: 0.00001418
Iteration 436/1000 | Loss: 0.00001418
Iteration 437/1000 | Loss: 0.00001418
Iteration 438/1000 | Loss: 0.00001418
Iteration 439/1000 | Loss: 0.00001418
Iteration 440/1000 | Loss: 0.00001418
Iteration 441/1000 | Loss: 0.00001418
Iteration 442/1000 | Loss: 0.00001418
Iteration 443/1000 | Loss: 0.00001418
Iteration 444/1000 | Loss: 0.00001418
Iteration 445/1000 | Loss: 0.00001418
Iteration 446/1000 | Loss: 0.00001418
Iteration 447/1000 | Loss: 0.00001418
Iteration 448/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 448. Stopping optimization.
Last 5 losses: [1.417839121131692e-05, 1.417839121131692e-05, 1.417839121131692e-05, 1.417839121131692e-05, 1.417839121131692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.417839121131692e-05

Optimization complete. Final v2v error: 3.052748918533325 mm

Highest mean error: 5.527864933013916 mm for frame 64

Lowest mean error: 2.5019052028656006 mm for frame 118

Saving results

Total time: 413.1019847393036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030300
Iteration 2/25 | Loss: 0.00184402
Iteration 3/25 | Loss: 0.00150848
Iteration 4/25 | Loss: 0.00146629
Iteration 5/25 | Loss: 0.00155886
Iteration 6/25 | Loss: 0.00145265
Iteration 7/25 | Loss: 0.00134066
Iteration 8/25 | Loss: 0.00126659
Iteration 9/25 | Loss: 0.00118844
Iteration 10/25 | Loss: 0.00118973
Iteration 11/25 | Loss: 0.00117139
Iteration 12/25 | Loss: 0.00116601
Iteration 13/25 | Loss: 0.00119666
Iteration 14/25 | Loss: 0.00115269
Iteration 15/25 | Loss: 0.00113909
Iteration 16/25 | Loss: 0.00113332
Iteration 17/25 | Loss: 0.00113226
Iteration 18/25 | Loss: 0.00113400
Iteration 19/25 | Loss: 0.00113332
Iteration 20/25 | Loss: 0.00113129
Iteration 21/25 | Loss: 0.00113398
Iteration 22/25 | Loss: 0.00111591
Iteration 23/25 | Loss: 0.00111469
Iteration 24/25 | Loss: 0.00111617
Iteration 25/25 | Loss: 0.00111206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43565023
Iteration 2/25 | Loss: 0.00643114
Iteration 3/25 | Loss: 0.00137022
Iteration 4/25 | Loss: 0.00137022
Iteration 5/25 | Loss: 0.00137022
Iteration 6/25 | Loss: 0.00137022
Iteration 7/25 | Loss: 0.00137021
Iteration 8/25 | Loss: 0.00137021
Iteration 9/25 | Loss: 0.00137021
Iteration 10/25 | Loss: 0.00137021
Iteration 11/25 | Loss: 0.00137021
Iteration 12/25 | Loss: 0.00137021
Iteration 13/25 | Loss: 0.00137021
Iteration 14/25 | Loss: 0.00137021
Iteration 15/25 | Loss: 0.00137021
Iteration 16/25 | Loss: 0.00137021
Iteration 17/25 | Loss: 0.00137021
Iteration 18/25 | Loss: 0.00137021
Iteration 19/25 | Loss: 0.00137021
Iteration 20/25 | Loss: 0.00137021
Iteration 21/25 | Loss: 0.00137021
Iteration 22/25 | Loss: 0.00137021
Iteration 23/25 | Loss: 0.00137021
Iteration 24/25 | Loss: 0.00137021
Iteration 25/25 | Loss: 0.00137021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137021
Iteration 2/1000 | Loss: 0.00013182
Iteration 3/1000 | Loss: 0.00007535
Iteration 4/1000 | Loss: 0.00007362
Iteration 5/1000 | Loss: 0.00005846
Iteration 6/1000 | Loss: 0.00005420
Iteration 7/1000 | Loss: 0.00005152
Iteration 8/1000 | Loss: 0.00004890
Iteration 9/1000 | Loss: 0.00035548
Iteration 10/1000 | Loss: 0.00005631
Iteration 11/1000 | Loss: 0.00004435
Iteration 12/1000 | Loss: 0.00004105
Iteration 13/1000 | Loss: 0.00003947
Iteration 14/1000 | Loss: 0.00003823
Iteration 15/1000 | Loss: 0.00003750
Iteration 16/1000 | Loss: 0.00003697
Iteration 17/1000 | Loss: 0.00003652
Iteration 18/1000 | Loss: 0.00003625
Iteration 19/1000 | Loss: 0.00003603
Iteration 20/1000 | Loss: 0.00003583
Iteration 21/1000 | Loss: 0.00003574
Iteration 22/1000 | Loss: 0.00003562
Iteration 23/1000 | Loss: 0.00003542
Iteration 24/1000 | Loss: 0.00003533
Iteration 25/1000 | Loss: 0.00003531
Iteration 26/1000 | Loss: 0.00003530
Iteration 27/1000 | Loss: 0.00003530
Iteration 28/1000 | Loss: 0.00003530
Iteration 29/1000 | Loss: 0.00003530
Iteration 30/1000 | Loss: 0.00003530
Iteration 31/1000 | Loss: 0.00003530
Iteration 32/1000 | Loss: 0.00003529
Iteration 33/1000 | Loss: 0.00003529
Iteration 34/1000 | Loss: 0.00003528
Iteration 35/1000 | Loss: 0.00003528
Iteration 36/1000 | Loss: 0.00003528
Iteration 37/1000 | Loss: 0.00003527
Iteration 38/1000 | Loss: 0.00003527
Iteration 39/1000 | Loss: 0.00003527
Iteration 40/1000 | Loss: 0.00003526
Iteration 41/1000 | Loss: 0.00003526
Iteration 42/1000 | Loss: 0.00003526
Iteration 43/1000 | Loss: 0.00003526
Iteration 44/1000 | Loss: 0.00003526
Iteration 45/1000 | Loss: 0.00003526
Iteration 46/1000 | Loss: 0.00003525
Iteration 47/1000 | Loss: 0.00003525
Iteration 48/1000 | Loss: 0.00003525
Iteration 49/1000 | Loss: 0.00003525
Iteration 50/1000 | Loss: 0.00003525
Iteration 51/1000 | Loss: 0.00003524
Iteration 52/1000 | Loss: 0.00003524
Iteration 53/1000 | Loss: 0.00003524
Iteration 54/1000 | Loss: 0.00003524
Iteration 55/1000 | Loss: 0.00003524
Iteration 56/1000 | Loss: 0.00003524
Iteration 57/1000 | Loss: 0.00003524
Iteration 58/1000 | Loss: 0.00003524
Iteration 59/1000 | Loss: 0.00003524
Iteration 60/1000 | Loss: 0.00003524
Iteration 61/1000 | Loss: 0.00003524
Iteration 62/1000 | Loss: 0.00003524
Iteration 63/1000 | Loss: 0.00003523
Iteration 64/1000 | Loss: 0.00003523
Iteration 65/1000 | Loss: 0.00003523
Iteration 66/1000 | Loss: 0.00003522
Iteration 67/1000 | Loss: 0.00003522
Iteration 68/1000 | Loss: 0.00003521
Iteration 69/1000 | Loss: 0.00003521
Iteration 70/1000 | Loss: 0.00003521
Iteration 71/1000 | Loss: 0.00003520
Iteration 72/1000 | Loss: 0.00003520
Iteration 73/1000 | Loss: 0.00003520
Iteration 74/1000 | Loss: 0.00003519
Iteration 75/1000 | Loss: 0.00003519
Iteration 76/1000 | Loss: 0.00003518
Iteration 77/1000 | Loss: 0.00003518
Iteration 78/1000 | Loss: 0.00003518
Iteration 79/1000 | Loss: 0.00003517
Iteration 80/1000 | Loss: 0.00003517
Iteration 81/1000 | Loss: 0.00003517
Iteration 82/1000 | Loss: 0.00003517
Iteration 83/1000 | Loss: 0.00003517
Iteration 84/1000 | Loss: 0.00003517
Iteration 85/1000 | Loss: 0.00003517
Iteration 86/1000 | Loss: 0.00003517
Iteration 87/1000 | Loss: 0.00003517
Iteration 88/1000 | Loss: 0.00003516
Iteration 89/1000 | Loss: 0.00003516
Iteration 90/1000 | Loss: 0.00003516
Iteration 91/1000 | Loss: 0.00003516
Iteration 92/1000 | Loss: 0.00003515
Iteration 93/1000 | Loss: 0.00003515
Iteration 94/1000 | Loss: 0.00003515
Iteration 95/1000 | Loss: 0.00003515
Iteration 96/1000 | Loss: 0.00003515
Iteration 97/1000 | Loss: 0.00003515
Iteration 98/1000 | Loss: 0.00003515
Iteration 99/1000 | Loss: 0.00003515
Iteration 100/1000 | Loss: 0.00003515
Iteration 101/1000 | Loss: 0.00003515
Iteration 102/1000 | Loss: 0.00003515
Iteration 103/1000 | Loss: 0.00003514
Iteration 104/1000 | Loss: 0.00003514
Iteration 105/1000 | Loss: 0.00003514
Iteration 106/1000 | Loss: 0.00003514
Iteration 107/1000 | Loss: 0.00003513
Iteration 108/1000 | Loss: 0.00003513
Iteration 109/1000 | Loss: 0.00003513
Iteration 110/1000 | Loss: 0.00003513
Iteration 111/1000 | Loss: 0.00003513
Iteration 112/1000 | Loss: 0.00003513
Iteration 113/1000 | Loss: 0.00003513
Iteration 114/1000 | Loss: 0.00003513
Iteration 115/1000 | Loss: 0.00003513
Iteration 116/1000 | Loss: 0.00003513
Iteration 117/1000 | Loss: 0.00003513
Iteration 118/1000 | Loss: 0.00003513
Iteration 119/1000 | Loss: 0.00003513
Iteration 120/1000 | Loss: 0.00003513
Iteration 121/1000 | Loss: 0.00003513
Iteration 122/1000 | Loss: 0.00003513
Iteration 123/1000 | Loss: 0.00003513
Iteration 124/1000 | Loss: 0.00003513
Iteration 125/1000 | Loss: 0.00003513
Iteration 126/1000 | Loss: 0.00003513
Iteration 127/1000 | Loss: 0.00003513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [3.512896364554763e-05, 3.512896364554763e-05, 3.512896364554763e-05, 3.512896364554763e-05, 3.512896364554763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.512896364554763e-05

Optimization complete. Final v2v error: 3.289060592651367 mm

Highest mean error: 12.308891296386719 mm for frame 97

Lowest mean error: 2.463777780532837 mm for frame 131

Saving results

Total time: 84.7532286643982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882515
Iteration 2/25 | Loss: 0.00353825
Iteration 3/25 | Loss: 0.00275054
Iteration 4/25 | Loss: 0.00234883
Iteration 5/25 | Loss: 0.00202420
Iteration 6/25 | Loss: 0.00182541
Iteration 7/25 | Loss: 0.00178219
Iteration 8/25 | Loss: 0.00178710
Iteration 9/25 | Loss: 0.00195695
Iteration 10/25 | Loss: 0.00124253
Iteration 11/25 | Loss: 0.00116090
Iteration 12/25 | Loss: 0.00114206
Iteration 13/25 | Loss: 0.00113821
Iteration 14/25 | Loss: 0.00113655
Iteration 15/25 | Loss: 0.00113622
Iteration 16/25 | Loss: 0.00113622
Iteration 17/25 | Loss: 0.00113622
Iteration 18/25 | Loss: 0.00113622
Iteration 19/25 | Loss: 0.00113622
Iteration 20/25 | Loss: 0.00113622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011362199438735843, 0.0011362199438735843, 0.0011362199438735843, 0.0011362199438735843, 0.0011362199438735843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011362199438735843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35336423
Iteration 2/25 | Loss: 0.00041067
Iteration 3/25 | Loss: 0.00041067
Iteration 4/25 | Loss: 0.00041067
Iteration 5/25 | Loss: 0.00041067
Iteration 6/25 | Loss: 0.00041067
Iteration 7/25 | Loss: 0.00041067
Iteration 8/25 | Loss: 0.00041067
Iteration 9/25 | Loss: 0.00041067
Iteration 10/25 | Loss: 0.00041067
Iteration 11/25 | Loss: 0.00041067
Iteration 12/25 | Loss: 0.00041067
Iteration 13/25 | Loss: 0.00041067
Iteration 14/25 | Loss: 0.00041067
Iteration 15/25 | Loss: 0.00041067
Iteration 16/25 | Loss: 0.00041067
Iteration 17/25 | Loss: 0.00041067
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00041066828998737037, 0.00041066828998737037, 0.00041066828998737037, 0.00041066828998737037, 0.00041066828998737037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041066828998737037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041067
Iteration 2/1000 | Loss: 0.00002322
Iteration 3/1000 | Loss: 0.00001784
Iteration 4/1000 | Loss: 0.00001644
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001553
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001488
Iteration 10/1000 | Loss: 0.00001466
Iteration 11/1000 | Loss: 0.00001461
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001443
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001434
Iteration 16/1000 | Loss: 0.00001427
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001407
Iteration 23/1000 | Loss: 0.00001406
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001400
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001395
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001393
Iteration 43/1000 | Loss: 0.00001393
Iteration 44/1000 | Loss: 0.00001392
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001390
Iteration 54/1000 | Loss: 0.00001390
Iteration 55/1000 | Loss: 0.00001390
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001383
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001381
Iteration 77/1000 | Loss: 0.00001381
Iteration 78/1000 | Loss: 0.00001381
Iteration 79/1000 | Loss: 0.00001381
Iteration 80/1000 | Loss: 0.00001381
Iteration 81/1000 | Loss: 0.00001380
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001378
Iteration 97/1000 | Loss: 0.00001378
Iteration 98/1000 | Loss: 0.00001378
Iteration 99/1000 | Loss: 0.00001378
Iteration 100/1000 | Loss: 0.00001378
Iteration 101/1000 | Loss: 0.00001378
Iteration 102/1000 | Loss: 0.00001378
Iteration 103/1000 | Loss: 0.00001378
Iteration 104/1000 | Loss: 0.00001378
Iteration 105/1000 | Loss: 0.00001378
Iteration 106/1000 | Loss: 0.00001378
Iteration 107/1000 | Loss: 0.00001378
Iteration 108/1000 | Loss: 0.00001378
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.378294200549135e-05, 1.378294200549135e-05, 1.378294200549135e-05, 1.378294200549135e-05, 1.378294200549135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.378294200549135e-05

Optimization complete. Final v2v error: 3.188490867614746 mm

Highest mean error: 3.2552788257598877 mm for frame 93

Lowest mean error: 3.0549280643463135 mm for frame 11

Saving results

Total time: 49.68132519721985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847715
Iteration 2/25 | Loss: 0.00126377
Iteration 3/25 | Loss: 0.00109132
Iteration 4/25 | Loss: 0.00107691
Iteration 5/25 | Loss: 0.00107307
Iteration 6/25 | Loss: 0.00107273
Iteration 7/25 | Loss: 0.00107273
Iteration 8/25 | Loss: 0.00107273
Iteration 9/25 | Loss: 0.00107273
Iteration 10/25 | Loss: 0.00107273
Iteration 11/25 | Loss: 0.00107273
Iteration 12/25 | Loss: 0.00107273
Iteration 13/25 | Loss: 0.00107273
Iteration 14/25 | Loss: 0.00107273
Iteration 15/25 | Loss: 0.00107273
Iteration 16/25 | Loss: 0.00107273
Iteration 17/25 | Loss: 0.00107273
Iteration 18/25 | Loss: 0.00107273
Iteration 19/25 | Loss: 0.00107273
Iteration 20/25 | Loss: 0.00107273
Iteration 21/25 | Loss: 0.00107273
Iteration 22/25 | Loss: 0.00107273
Iteration 23/25 | Loss: 0.00107273
Iteration 24/25 | Loss: 0.00107273
Iteration 25/25 | Loss: 0.00107273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32608867
Iteration 2/25 | Loss: 0.00079713
Iteration 3/25 | Loss: 0.00079713
Iteration 4/25 | Loss: 0.00079713
Iteration 5/25 | Loss: 0.00079713
Iteration 6/25 | Loss: 0.00079713
Iteration 7/25 | Loss: 0.00079713
Iteration 8/25 | Loss: 0.00079713
Iteration 9/25 | Loss: 0.00079713
Iteration 10/25 | Loss: 0.00079713
Iteration 11/25 | Loss: 0.00079713
Iteration 12/25 | Loss: 0.00079713
Iteration 13/25 | Loss: 0.00079713
Iteration 14/25 | Loss: 0.00079713
Iteration 15/25 | Loss: 0.00079713
Iteration 16/25 | Loss: 0.00079713
Iteration 17/25 | Loss: 0.00079713
Iteration 18/25 | Loss: 0.00079713
Iteration 19/25 | Loss: 0.00079713
Iteration 20/25 | Loss: 0.00079713
Iteration 21/25 | Loss: 0.00079713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000797125743702054, 0.000797125743702054, 0.000797125743702054, 0.000797125743702054, 0.000797125743702054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000797125743702054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079713
Iteration 2/1000 | Loss: 0.00002685
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001267
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001204
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001197
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001192
Iteration 28/1000 | Loss: 0.00001192
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001182
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001181
Iteration 71/1000 | Loss: 0.00001181
Iteration 72/1000 | Loss: 0.00001181
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001173
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001168
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001166
Iteration 129/1000 | Loss: 0.00001166
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001165
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001165
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001164
Iteration 140/1000 | Loss: 0.00001164
Iteration 141/1000 | Loss: 0.00001164
Iteration 142/1000 | Loss: 0.00001164
Iteration 143/1000 | Loss: 0.00001164
Iteration 144/1000 | Loss: 0.00001164
Iteration 145/1000 | Loss: 0.00001164
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001164
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001164
Iteration 150/1000 | Loss: 0.00001164
Iteration 151/1000 | Loss: 0.00001164
Iteration 152/1000 | Loss: 0.00001164
Iteration 153/1000 | Loss: 0.00001164
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001163
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001163
Iteration 182/1000 | Loss: 0.00001163
Iteration 183/1000 | Loss: 0.00001163
Iteration 184/1000 | Loss: 0.00001163
Iteration 185/1000 | Loss: 0.00001163
Iteration 186/1000 | Loss: 0.00001163
Iteration 187/1000 | Loss: 0.00001163
Iteration 188/1000 | Loss: 0.00001163
Iteration 189/1000 | Loss: 0.00001163
Iteration 190/1000 | Loss: 0.00001163
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Iteration 196/1000 | Loss: 0.00001163
Iteration 197/1000 | Loss: 0.00001163
Iteration 198/1000 | Loss: 0.00001163
Iteration 199/1000 | Loss: 0.00001163
Iteration 200/1000 | Loss: 0.00001163
Iteration 201/1000 | Loss: 0.00001163
Iteration 202/1000 | Loss: 0.00001163
Iteration 203/1000 | Loss: 0.00001163
Iteration 204/1000 | Loss: 0.00001163
Iteration 205/1000 | Loss: 0.00001163
Iteration 206/1000 | Loss: 0.00001163
Iteration 207/1000 | Loss: 0.00001163
Iteration 208/1000 | Loss: 0.00001163
Iteration 209/1000 | Loss: 0.00001163
Iteration 210/1000 | Loss: 0.00001163
Iteration 211/1000 | Loss: 0.00001163
Iteration 212/1000 | Loss: 0.00001163
Iteration 213/1000 | Loss: 0.00001163
Iteration 214/1000 | Loss: 0.00001163
Iteration 215/1000 | Loss: 0.00001163
Iteration 216/1000 | Loss: 0.00001163
Iteration 217/1000 | Loss: 0.00001163
Iteration 218/1000 | Loss: 0.00001163
Iteration 219/1000 | Loss: 0.00001163
Iteration 220/1000 | Loss: 0.00001163
Iteration 221/1000 | Loss: 0.00001163
Iteration 222/1000 | Loss: 0.00001163
Iteration 223/1000 | Loss: 0.00001163
Iteration 224/1000 | Loss: 0.00001163
Iteration 225/1000 | Loss: 0.00001163
Iteration 226/1000 | Loss: 0.00001163
Iteration 227/1000 | Loss: 0.00001163
Iteration 228/1000 | Loss: 0.00001163
Iteration 229/1000 | Loss: 0.00001163
Iteration 230/1000 | Loss: 0.00001163
Iteration 231/1000 | Loss: 0.00001163
Iteration 232/1000 | Loss: 0.00001163
Iteration 233/1000 | Loss: 0.00001163
Iteration 234/1000 | Loss: 0.00001163
Iteration 235/1000 | Loss: 0.00001163
Iteration 236/1000 | Loss: 0.00001163
Iteration 237/1000 | Loss: 0.00001163
Iteration 238/1000 | Loss: 0.00001163
Iteration 239/1000 | Loss: 0.00001163
Iteration 240/1000 | Loss: 0.00001163
Iteration 241/1000 | Loss: 0.00001163
Iteration 242/1000 | Loss: 0.00001163
Iteration 243/1000 | Loss: 0.00001163
Iteration 244/1000 | Loss: 0.00001163
Iteration 245/1000 | Loss: 0.00001163
Iteration 246/1000 | Loss: 0.00001163
Iteration 247/1000 | Loss: 0.00001163
Iteration 248/1000 | Loss: 0.00001163
Iteration 249/1000 | Loss: 0.00001163
Iteration 250/1000 | Loss: 0.00001163
Iteration 251/1000 | Loss: 0.00001163
Iteration 252/1000 | Loss: 0.00001163
Iteration 253/1000 | Loss: 0.00001163
Iteration 254/1000 | Loss: 0.00001163
Iteration 255/1000 | Loss: 0.00001163
Iteration 256/1000 | Loss: 0.00001163
Iteration 257/1000 | Loss: 0.00001163
Iteration 258/1000 | Loss: 0.00001163
Iteration 259/1000 | Loss: 0.00001163
Iteration 260/1000 | Loss: 0.00001163
Iteration 261/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.16268192869029e-05, 1.16268192869029e-05, 1.16268192869029e-05, 1.16268192869029e-05, 1.16268192869029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.16268192869029e-05

Optimization complete. Final v2v error: 2.7272632122039795 mm

Highest mean error: 3.6908645629882812 mm for frame 219

Lowest mean error: 2.282346725463867 mm for frame 152

Saving results

Total time: 42.87928891181946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042347
Iteration 2/25 | Loss: 0.00412733
Iteration 3/25 | Loss: 0.00257103
Iteration 4/25 | Loss: 0.00240240
Iteration 5/25 | Loss: 0.00205981
Iteration 6/25 | Loss: 0.00193229
Iteration 7/25 | Loss: 0.00191377
Iteration 8/25 | Loss: 0.00175814
Iteration 9/25 | Loss: 0.00166148
Iteration 10/25 | Loss: 0.00160785
Iteration 11/25 | Loss: 0.00152515
Iteration 12/25 | Loss: 0.00152187
Iteration 13/25 | Loss: 0.00148630
Iteration 14/25 | Loss: 0.00148285
Iteration 15/25 | Loss: 0.00147496
Iteration 16/25 | Loss: 0.00147416
Iteration 17/25 | Loss: 0.00148207
Iteration 18/25 | Loss: 0.00145517
Iteration 19/25 | Loss: 0.00146840
Iteration 20/25 | Loss: 0.00144770
Iteration 21/25 | Loss: 0.00144306
Iteration 22/25 | Loss: 0.00143549
Iteration 23/25 | Loss: 0.00142908
Iteration 24/25 | Loss: 0.00142589
Iteration 25/25 | Loss: 0.00143221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34294045
Iteration 2/25 | Loss: 0.00389167
Iteration 3/25 | Loss: 0.00389166
Iteration 4/25 | Loss: 0.00363766
Iteration 5/25 | Loss: 0.00363764
Iteration 6/25 | Loss: 0.00363764
Iteration 7/25 | Loss: 0.00363764
Iteration 8/25 | Loss: 0.00363764
Iteration 9/25 | Loss: 0.00363764
Iteration 10/25 | Loss: 0.00363764
Iteration 11/25 | Loss: 0.00363764
Iteration 12/25 | Loss: 0.00363764
Iteration 13/25 | Loss: 0.00363764
Iteration 14/25 | Loss: 0.00363764
Iteration 15/25 | Loss: 0.00363764
Iteration 16/25 | Loss: 0.00363764
Iteration 17/25 | Loss: 0.00363764
Iteration 18/25 | Loss: 0.00363764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003637637011706829, 0.003637637011706829, 0.003637637011706829, 0.003637637011706829, 0.003637637011706829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003637637011706829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00363764
Iteration 2/1000 | Loss: 0.00331439
Iteration 3/1000 | Loss: 0.00345597
Iteration 4/1000 | Loss: 0.00144248
Iteration 5/1000 | Loss: 0.00202674
Iteration 6/1000 | Loss: 0.00198410
Iteration 7/1000 | Loss: 0.00286897
Iteration 8/1000 | Loss: 0.00222208
Iteration 9/1000 | Loss: 0.00032669
Iteration 10/1000 | Loss: 0.00100961
Iteration 11/1000 | Loss: 0.00030837
Iteration 12/1000 | Loss: 0.00020164
Iteration 13/1000 | Loss: 0.00017864
Iteration 14/1000 | Loss: 0.00125842
Iteration 15/1000 | Loss: 0.00035682
Iteration 16/1000 | Loss: 0.00014912
Iteration 17/1000 | Loss: 0.00096987
Iteration 18/1000 | Loss: 0.00660641
Iteration 19/1000 | Loss: 0.00452611
Iteration 20/1000 | Loss: 0.00761422
Iteration 21/1000 | Loss: 0.00282161
Iteration 22/1000 | Loss: 0.00254422
Iteration 23/1000 | Loss: 0.00311348
Iteration 24/1000 | Loss: 0.00180269
Iteration 25/1000 | Loss: 0.00149245
Iteration 26/1000 | Loss: 0.00085075
Iteration 27/1000 | Loss: 0.00054924
Iteration 28/1000 | Loss: 0.00104667
Iteration 29/1000 | Loss: 0.00054283
Iteration 30/1000 | Loss: 0.00092118
Iteration 31/1000 | Loss: 0.00055794
Iteration 32/1000 | Loss: 0.00037341
Iteration 33/1000 | Loss: 0.00146740
Iteration 34/1000 | Loss: 0.00111793
Iteration 35/1000 | Loss: 0.00055814
Iteration 36/1000 | Loss: 0.00055029
Iteration 37/1000 | Loss: 0.00036882
Iteration 38/1000 | Loss: 0.00021213
Iteration 39/1000 | Loss: 0.00065287
Iteration 40/1000 | Loss: 0.00042545
Iteration 41/1000 | Loss: 0.00055969
Iteration 42/1000 | Loss: 0.00051381
Iteration 43/1000 | Loss: 0.00049492
Iteration 44/1000 | Loss: 0.00034241
Iteration 45/1000 | Loss: 0.00021491
Iteration 46/1000 | Loss: 0.00027241
Iteration 47/1000 | Loss: 0.00048524
Iteration 48/1000 | Loss: 0.00071556
Iteration 49/1000 | Loss: 0.00065370
Iteration 50/1000 | Loss: 0.00054717
Iteration 51/1000 | Loss: 0.00057173
Iteration 52/1000 | Loss: 0.00057589
Iteration 53/1000 | Loss: 0.00009560
Iteration 54/1000 | Loss: 0.00006755
Iteration 55/1000 | Loss: 0.00048026
Iteration 56/1000 | Loss: 0.00015025
Iteration 57/1000 | Loss: 0.00006308
Iteration 58/1000 | Loss: 0.00007819
Iteration 59/1000 | Loss: 0.00005182
Iteration 60/1000 | Loss: 0.00004816
Iteration 61/1000 | Loss: 0.00029370
Iteration 62/1000 | Loss: 0.00004704
Iteration 63/1000 | Loss: 0.00005819
Iteration 64/1000 | Loss: 0.00004049
Iteration 65/1000 | Loss: 0.00135642
Iteration 66/1000 | Loss: 0.00064541
Iteration 67/1000 | Loss: 0.00105837
Iteration 68/1000 | Loss: 0.00201135
Iteration 69/1000 | Loss: 0.00155492
Iteration 70/1000 | Loss: 0.00096324
Iteration 71/1000 | Loss: 0.00227428
Iteration 72/1000 | Loss: 0.00135464
Iteration 73/1000 | Loss: 0.00184817
Iteration 74/1000 | Loss: 0.00115092
Iteration 75/1000 | Loss: 0.00170662
Iteration 76/1000 | Loss: 0.00121161
Iteration 77/1000 | Loss: 0.00039574
Iteration 78/1000 | Loss: 0.00060975
Iteration 79/1000 | Loss: 0.00144832
Iteration 80/1000 | Loss: 0.00043950
Iteration 81/1000 | Loss: 0.00079744
Iteration 82/1000 | Loss: 0.00036751
Iteration 83/1000 | Loss: 0.00028271
Iteration 84/1000 | Loss: 0.00018690
Iteration 85/1000 | Loss: 0.00047006
Iteration 86/1000 | Loss: 0.00018924
Iteration 87/1000 | Loss: 0.00035950
Iteration 88/1000 | Loss: 0.00013101
Iteration 89/1000 | Loss: 0.00030208
Iteration 90/1000 | Loss: 0.00016753
Iteration 91/1000 | Loss: 0.00011857
Iteration 92/1000 | Loss: 0.00054339
Iteration 93/1000 | Loss: 0.00017444
Iteration 94/1000 | Loss: 0.00009331
Iteration 95/1000 | Loss: 0.00004822
Iteration 96/1000 | Loss: 0.00009643
Iteration 97/1000 | Loss: 0.00097784
Iteration 98/1000 | Loss: 0.00074813
Iteration 99/1000 | Loss: 0.00063482
Iteration 100/1000 | Loss: 0.00040268
Iteration 101/1000 | Loss: 0.00036459
Iteration 102/1000 | Loss: 0.00079732
Iteration 103/1000 | Loss: 0.00038558
Iteration 104/1000 | Loss: 0.00046303
Iteration 105/1000 | Loss: 0.00064567
Iteration 106/1000 | Loss: 0.00040178
Iteration 107/1000 | Loss: 0.00059164
Iteration 108/1000 | Loss: 0.00029105
Iteration 109/1000 | Loss: 0.00014702
Iteration 110/1000 | Loss: 0.00006647
Iteration 111/1000 | Loss: 0.00005874
Iteration 112/1000 | Loss: 0.00016862
Iteration 113/1000 | Loss: 0.00044577
Iteration 114/1000 | Loss: 0.00030662
Iteration 115/1000 | Loss: 0.00036062
Iteration 116/1000 | Loss: 0.00038817
Iteration 117/1000 | Loss: 0.00006126
Iteration 118/1000 | Loss: 0.00002860
Iteration 119/1000 | Loss: 0.00006155
Iteration 120/1000 | Loss: 0.00004804
Iteration 121/1000 | Loss: 0.00006695
Iteration 122/1000 | Loss: 0.00041275
Iteration 123/1000 | Loss: 0.00050225
Iteration 124/1000 | Loss: 0.00027575
Iteration 125/1000 | Loss: 0.00038856
Iteration 126/1000 | Loss: 0.00024150
Iteration 127/1000 | Loss: 0.00044305
Iteration 128/1000 | Loss: 0.00034784
Iteration 129/1000 | Loss: 0.00039646
Iteration 130/1000 | Loss: 0.00019897
Iteration 131/1000 | Loss: 0.00008724
Iteration 132/1000 | Loss: 0.00002751
Iteration 133/1000 | Loss: 0.00003565
Iteration 134/1000 | Loss: 0.00003705
Iteration 135/1000 | Loss: 0.00035261
Iteration 136/1000 | Loss: 0.00046149
Iteration 137/1000 | Loss: 0.00022140
Iteration 138/1000 | Loss: 0.00046215
Iteration 139/1000 | Loss: 0.00021040
Iteration 140/1000 | Loss: 0.00022445
Iteration 141/1000 | Loss: 0.00020681
Iteration 142/1000 | Loss: 0.00021970
Iteration 143/1000 | Loss: 0.00032283
Iteration 144/1000 | Loss: 0.00063351
Iteration 145/1000 | Loss: 0.00064989
Iteration 146/1000 | Loss: 0.00051817
Iteration 147/1000 | Loss: 0.00041581
Iteration 148/1000 | Loss: 0.00003319
Iteration 149/1000 | Loss: 0.00028809
Iteration 150/1000 | Loss: 0.00002549
Iteration 151/1000 | Loss: 0.00002373
Iteration 152/1000 | Loss: 0.00003791
Iteration 153/1000 | Loss: 0.00002220
Iteration 154/1000 | Loss: 0.00041391
Iteration 155/1000 | Loss: 0.00020837
Iteration 156/1000 | Loss: 0.00034599
Iteration 157/1000 | Loss: 0.00015466
Iteration 158/1000 | Loss: 0.00004817
Iteration 159/1000 | Loss: 0.00003855
Iteration 160/1000 | Loss: 0.00002116
Iteration 161/1000 | Loss: 0.00004403
Iteration 162/1000 | Loss: 0.00004288
Iteration 163/1000 | Loss: 0.00002055
Iteration 164/1000 | Loss: 0.00004896
Iteration 165/1000 | Loss: 0.00002023
Iteration 166/1000 | Loss: 0.00002012
Iteration 167/1000 | Loss: 0.00002009
Iteration 168/1000 | Loss: 0.00039976
Iteration 169/1000 | Loss: 0.00016250
Iteration 170/1000 | Loss: 0.00103489
Iteration 171/1000 | Loss: 0.00087237
Iteration 172/1000 | Loss: 0.00030427
Iteration 173/1000 | Loss: 0.00016040
Iteration 174/1000 | Loss: 0.00024771
Iteration 175/1000 | Loss: 0.00038918
Iteration 176/1000 | Loss: 0.00028947
Iteration 177/1000 | Loss: 0.00015961
Iteration 178/1000 | Loss: 0.00047550
Iteration 179/1000 | Loss: 0.00028614
Iteration 180/1000 | Loss: 0.00018807
Iteration 181/1000 | Loss: 0.00002661
Iteration 182/1000 | Loss: 0.00002229
Iteration 183/1000 | Loss: 0.00045505
Iteration 184/1000 | Loss: 0.00008929
Iteration 185/1000 | Loss: 0.00027300
Iteration 186/1000 | Loss: 0.00026272
Iteration 187/1000 | Loss: 0.00003552
Iteration 188/1000 | Loss: 0.00002189
Iteration 189/1000 | Loss: 0.00002626
Iteration 190/1000 | Loss: 0.00002017
Iteration 191/1000 | Loss: 0.00001995
Iteration 192/1000 | Loss: 0.00001993
Iteration 193/1000 | Loss: 0.00001986
Iteration 194/1000 | Loss: 0.00001963
Iteration 195/1000 | Loss: 0.00060379
Iteration 196/1000 | Loss: 0.00003105
Iteration 197/1000 | Loss: 0.00002235
Iteration 198/1000 | Loss: 0.00001940
Iteration 199/1000 | Loss: 0.00003108
Iteration 200/1000 | Loss: 0.00001763
Iteration 201/1000 | Loss: 0.00001703
Iteration 202/1000 | Loss: 0.00001674
Iteration 203/1000 | Loss: 0.00001664
Iteration 204/1000 | Loss: 0.00001663
Iteration 205/1000 | Loss: 0.00001659
Iteration 206/1000 | Loss: 0.00044444
Iteration 207/1000 | Loss: 0.00047323
Iteration 208/1000 | Loss: 0.00004064
Iteration 209/1000 | Loss: 0.00045882
Iteration 210/1000 | Loss: 0.00003661
Iteration 211/1000 | Loss: 0.00031387
Iteration 212/1000 | Loss: 0.00002850
Iteration 213/1000 | Loss: 0.00001988
Iteration 214/1000 | Loss: 0.00001821
Iteration 215/1000 | Loss: 0.00001764
Iteration 216/1000 | Loss: 0.00001733
Iteration 217/1000 | Loss: 0.00005185
Iteration 218/1000 | Loss: 0.00002460
Iteration 219/1000 | Loss: 0.00001690
Iteration 220/1000 | Loss: 0.00056298
Iteration 221/1000 | Loss: 0.00053611
Iteration 222/1000 | Loss: 0.00017090
Iteration 223/1000 | Loss: 0.00002311
Iteration 224/1000 | Loss: 0.00001936
Iteration 225/1000 | Loss: 0.00028524
Iteration 226/1000 | Loss: 0.00012131
Iteration 227/1000 | Loss: 0.00034780
Iteration 228/1000 | Loss: 0.00012284
Iteration 229/1000 | Loss: 0.00001772
Iteration 230/1000 | Loss: 0.00001690
Iteration 231/1000 | Loss: 0.00019858
Iteration 232/1000 | Loss: 0.00002462
Iteration 233/1000 | Loss: 0.00002278
Iteration 234/1000 | Loss: 0.00031306
Iteration 235/1000 | Loss: 0.00038231
Iteration 236/1000 | Loss: 0.00006376
Iteration 237/1000 | Loss: 0.00040251
Iteration 238/1000 | Loss: 0.00039798
Iteration 239/1000 | Loss: 0.00006705
Iteration 240/1000 | Loss: 0.00002171
Iteration 241/1000 | Loss: 0.00001920
Iteration 242/1000 | Loss: 0.00001820
Iteration 243/1000 | Loss: 0.00002030
Iteration 244/1000 | Loss: 0.00001700
Iteration 245/1000 | Loss: 0.00041592
Iteration 246/1000 | Loss: 0.00002702
Iteration 247/1000 | Loss: 0.00003852
Iteration 248/1000 | Loss: 0.00001994
Iteration 249/1000 | Loss: 0.00001609
Iteration 250/1000 | Loss: 0.00001528
Iteration 251/1000 | Loss: 0.00001476
Iteration 252/1000 | Loss: 0.00002368
Iteration 253/1000 | Loss: 0.00001565
Iteration 254/1000 | Loss: 0.00001420
Iteration 255/1000 | Loss: 0.00001416
Iteration 256/1000 | Loss: 0.00001413
Iteration 257/1000 | Loss: 0.00001413
Iteration 258/1000 | Loss: 0.00001413
Iteration 259/1000 | Loss: 0.00001413
Iteration 260/1000 | Loss: 0.00001413
Iteration 261/1000 | Loss: 0.00001413
Iteration 262/1000 | Loss: 0.00001413
Iteration 263/1000 | Loss: 0.00001412
Iteration 264/1000 | Loss: 0.00001412
Iteration 265/1000 | Loss: 0.00001412
Iteration 266/1000 | Loss: 0.00001412
Iteration 267/1000 | Loss: 0.00001412
Iteration 268/1000 | Loss: 0.00001412
Iteration 269/1000 | Loss: 0.00001412
Iteration 270/1000 | Loss: 0.00001412
Iteration 271/1000 | Loss: 0.00001412
Iteration 272/1000 | Loss: 0.00001412
Iteration 273/1000 | Loss: 0.00001412
Iteration 274/1000 | Loss: 0.00001411
Iteration 275/1000 | Loss: 0.00001411
Iteration 276/1000 | Loss: 0.00001410
Iteration 277/1000 | Loss: 0.00001410
Iteration 278/1000 | Loss: 0.00001410
Iteration 279/1000 | Loss: 0.00001410
Iteration 280/1000 | Loss: 0.00001410
Iteration 281/1000 | Loss: 0.00001410
Iteration 282/1000 | Loss: 0.00001410
Iteration 283/1000 | Loss: 0.00001409
Iteration 284/1000 | Loss: 0.00001409
Iteration 285/1000 | Loss: 0.00001409
Iteration 286/1000 | Loss: 0.00001408
Iteration 287/1000 | Loss: 0.00001407
Iteration 288/1000 | Loss: 0.00001407
Iteration 289/1000 | Loss: 0.00001407
Iteration 290/1000 | Loss: 0.00001406
Iteration 291/1000 | Loss: 0.00001406
Iteration 292/1000 | Loss: 0.00001406
Iteration 293/1000 | Loss: 0.00001405
Iteration 294/1000 | Loss: 0.00001405
Iteration 295/1000 | Loss: 0.00001405
Iteration 296/1000 | Loss: 0.00001405
Iteration 297/1000 | Loss: 0.00001404
Iteration 298/1000 | Loss: 0.00001404
Iteration 299/1000 | Loss: 0.00001404
Iteration 300/1000 | Loss: 0.00001404
Iteration 301/1000 | Loss: 0.00001404
Iteration 302/1000 | Loss: 0.00001404
Iteration 303/1000 | Loss: 0.00001404
Iteration 304/1000 | Loss: 0.00001404
Iteration 305/1000 | Loss: 0.00001403
Iteration 306/1000 | Loss: 0.00001403
Iteration 307/1000 | Loss: 0.00001403
Iteration 308/1000 | Loss: 0.00001403
Iteration 309/1000 | Loss: 0.00001403
Iteration 310/1000 | Loss: 0.00001403
Iteration 311/1000 | Loss: 0.00001402
Iteration 312/1000 | Loss: 0.00001402
Iteration 313/1000 | Loss: 0.00001402
Iteration 314/1000 | Loss: 0.00001402
Iteration 315/1000 | Loss: 0.00001402
Iteration 316/1000 | Loss: 0.00001402
Iteration 317/1000 | Loss: 0.00001401
Iteration 318/1000 | Loss: 0.00001401
Iteration 319/1000 | Loss: 0.00001401
Iteration 320/1000 | Loss: 0.00001401
Iteration 321/1000 | Loss: 0.00001400
Iteration 322/1000 | Loss: 0.00001400
Iteration 323/1000 | Loss: 0.00001399
Iteration 324/1000 | Loss: 0.00001399
Iteration 325/1000 | Loss: 0.00001399
Iteration 326/1000 | Loss: 0.00001399
Iteration 327/1000 | Loss: 0.00001399
Iteration 328/1000 | Loss: 0.00001399
Iteration 329/1000 | Loss: 0.00001398
Iteration 330/1000 | Loss: 0.00001398
Iteration 331/1000 | Loss: 0.00001398
Iteration 332/1000 | Loss: 0.00001398
Iteration 333/1000 | Loss: 0.00001398
Iteration 334/1000 | Loss: 0.00001398
Iteration 335/1000 | Loss: 0.00001397
Iteration 336/1000 | Loss: 0.00001397
Iteration 337/1000 | Loss: 0.00001397
Iteration 338/1000 | Loss: 0.00001397
Iteration 339/1000 | Loss: 0.00001397
Iteration 340/1000 | Loss: 0.00001397
Iteration 341/1000 | Loss: 0.00001397
Iteration 342/1000 | Loss: 0.00001397
Iteration 343/1000 | Loss: 0.00001396
Iteration 344/1000 | Loss: 0.00001396
Iteration 345/1000 | Loss: 0.00001396
Iteration 346/1000 | Loss: 0.00001396
Iteration 347/1000 | Loss: 0.00001396
Iteration 348/1000 | Loss: 0.00001396
Iteration 349/1000 | Loss: 0.00001396
Iteration 350/1000 | Loss: 0.00001396
Iteration 351/1000 | Loss: 0.00001396
Iteration 352/1000 | Loss: 0.00001396
Iteration 353/1000 | Loss: 0.00001396
Iteration 354/1000 | Loss: 0.00001396
Iteration 355/1000 | Loss: 0.00001396
Iteration 356/1000 | Loss: 0.00001396
Iteration 357/1000 | Loss: 0.00001396
Iteration 358/1000 | Loss: 0.00001396
Iteration 359/1000 | Loss: 0.00001396
Iteration 360/1000 | Loss: 0.00001396
Iteration 361/1000 | Loss: 0.00001395
Iteration 362/1000 | Loss: 0.00001395
Iteration 363/1000 | Loss: 0.00001395
Iteration 364/1000 | Loss: 0.00001395
Iteration 365/1000 | Loss: 0.00001395
Iteration 366/1000 | Loss: 0.00001395
Iteration 367/1000 | Loss: 0.00001395
Iteration 368/1000 | Loss: 0.00001395
Iteration 369/1000 | Loss: 0.00001395
Iteration 370/1000 | Loss: 0.00001395
Iteration 371/1000 | Loss: 0.00001395
Iteration 372/1000 | Loss: 0.00001395
Iteration 373/1000 | Loss: 0.00001395
Iteration 374/1000 | Loss: 0.00001395
Iteration 375/1000 | Loss: 0.00001395
Iteration 376/1000 | Loss: 0.00001395
Iteration 377/1000 | Loss: 0.00001395
Iteration 378/1000 | Loss: 0.00001395
Iteration 379/1000 | Loss: 0.00001395
Iteration 380/1000 | Loss: 0.00001395
Iteration 381/1000 | Loss: 0.00001395
Iteration 382/1000 | Loss: 0.00001395
Iteration 383/1000 | Loss: 0.00001394
Iteration 384/1000 | Loss: 0.00001394
Iteration 385/1000 | Loss: 0.00001394
Iteration 386/1000 | Loss: 0.00001394
Iteration 387/1000 | Loss: 0.00001394
Iteration 388/1000 | Loss: 0.00001394
Iteration 389/1000 | Loss: 0.00001394
Iteration 390/1000 | Loss: 0.00001394
Iteration 391/1000 | Loss: 0.00001394
Iteration 392/1000 | Loss: 0.00001394
Iteration 393/1000 | Loss: 0.00001394
Iteration 394/1000 | Loss: 0.00001394
Iteration 395/1000 | Loss: 0.00001394
Iteration 396/1000 | Loss: 0.00001394
Iteration 397/1000 | Loss: 0.00001394
Iteration 398/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [1.3938853953732178e-05, 1.3938853953732178e-05, 1.3938853953732178e-05, 1.3938853953732178e-05, 1.3938853953732178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3938853953732178e-05

Optimization complete. Final v2v error: 3.075387954711914 mm

Highest mean error: 4.907281398773193 mm for frame 67

Lowest mean error: 2.4977385997772217 mm for frame 138

Saving results

Total time: 453.9242630004883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00682157
Iteration 2/25 | Loss: 0.00127585
Iteration 3/25 | Loss: 0.00113781
Iteration 4/25 | Loss: 0.00106789
Iteration 5/25 | Loss: 0.00106033
Iteration 6/25 | Loss: 0.00105749
Iteration 7/25 | Loss: 0.00106181
Iteration 8/25 | Loss: 0.00106304
Iteration 9/25 | Loss: 0.00104938
Iteration 10/25 | Loss: 0.00104927
Iteration 11/25 | Loss: 0.00104927
Iteration 12/25 | Loss: 0.00104927
Iteration 13/25 | Loss: 0.00104927
Iteration 14/25 | Loss: 0.00104927
Iteration 15/25 | Loss: 0.00104927
Iteration 16/25 | Loss: 0.00104927
Iteration 17/25 | Loss: 0.00104927
Iteration 18/25 | Loss: 0.00104927
Iteration 19/25 | Loss: 0.00104927
Iteration 20/25 | Loss: 0.00104927
Iteration 21/25 | Loss: 0.00104927
Iteration 22/25 | Loss: 0.00104927
Iteration 23/25 | Loss: 0.00104927
Iteration 24/25 | Loss: 0.00104927
Iteration 25/25 | Loss: 0.00104926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64404869
Iteration 2/25 | Loss: 0.00075509
Iteration 3/25 | Loss: 0.00075509
Iteration 4/25 | Loss: 0.00075509
Iteration 5/25 | Loss: 0.00075509
Iteration 6/25 | Loss: 0.00075509
Iteration 7/25 | Loss: 0.00075509
Iteration 8/25 | Loss: 0.00075509
Iteration 9/25 | Loss: 0.00075509
Iteration 10/25 | Loss: 0.00075509
Iteration 11/25 | Loss: 0.00075508
Iteration 12/25 | Loss: 0.00075508
Iteration 13/25 | Loss: 0.00075508
Iteration 14/25 | Loss: 0.00075508
Iteration 15/25 | Loss: 0.00075508
Iteration 16/25 | Loss: 0.00075508
Iteration 17/25 | Loss: 0.00075508
Iteration 18/25 | Loss: 0.00075508
Iteration 19/25 | Loss: 0.00075508
Iteration 20/25 | Loss: 0.00075508
Iteration 21/25 | Loss: 0.00075508
Iteration 22/25 | Loss: 0.00075508
Iteration 23/25 | Loss: 0.00075508
Iteration 24/25 | Loss: 0.00075508
Iteration 25/25 | Loss: 0.00075508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075508
Iteration 2/1000 | Loss: 0.00001594
Iteration 3/1000 | Loss: 0.00001262
Iteration 4/1000 | Loss: 0.00001172
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001070
Iteration 7/1000 | Loss: 0.00001058
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00001014
Iteration 11/1000 | Loss: 0.00001004
Iteration 12/1000 | Loss: 0.00000994
Iteration 13/1000 | Loss: 0.00000991
Iteration 14/1000 | Loss: 0.00000991
Iteration 15/1000 | Loss: 0.00000991
Iteration 16/1000 | Loss: 0.00000991
Iteration 17/1000 | Loss: 0.00000991
Iteration 18/1000 | Loss: 0.00000991
Iteration 19/1000 | Loss: 0.00000991
Iteration 20/1000 | Loss: 0.00000991
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000989
Iteration 28/1000 | Loss: 0.00000987
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000986
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000984
Iteration 36/1000 | Loss: 0.00000984
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000983
Iteration 39/1000 | Loss: 0.00000983
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000980
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000979
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000973
Iteration 57/1000 | Loss: 0.00000972
Iteration 58/1000 | Loss: 0.00000972
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000972
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000971
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000970
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000967
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000964
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000962
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000962
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000961
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000960
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000960
Iteration 111/1000 | Loss: 0.00000960
Iteration 112/1000 | Loss: 0.00000960
Iteration 113/1000 | Loss: 0.00000960
Iteration 114/1000 | Loss: 0.00000960
Iteration 115/1000 | Loss: 0.00000960
Iteration 116/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [9.600767953088507e-06, 9.600767953088507e-06, 9.600767953088507e-06, 9.600767953088507e-06, 9.600767953088507e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.600767953088507e-06

Optimization complete. Final v2v error: 2.6465346813201904 mm

Highest mean error: 2.9961416721343994 mm for frame 159

Lowest mean error: 2.4137675762176514 mm for frame 40

Saving results

Total time: 47.169668674468994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968545
Iteration 2/25 | Loss: 0.00296091
Iteration 3/25 | Loss: 0.00203721
Iteration 4/25 | Loss: 0.00176305
Iteration 5/25 | Loss: 0.00168630
Iteration 6/25 | Loss: 0.00162378
Iteration 7/25 | Loss: 0.00155765
Iteration 8/25 | Loss: 0.00153877
Iteration 9/25 | Loss: 0.00150095
Iteration 10/25 | Loss: 0.00148629
Iteration 11/25 | Loss: 0.00144367
Iteration 12/25 | Loss: 0.00142713
Iteration 13/25 | Loss: 0.00140799
Iteration 14/25 | Loss: 0.00137131
Iteration 15/25 | Loss: 0.00135393
Iteration 16/25 | Loss: 0.00132696
Iteration 17/25 | Loss: 0.00131902
Iteration 18/25 | Loss: 0.00129387
Iteration 19/25 | Loss: 0.00128572
Iteration 20/25 | Loss: 0.00128592
Iteration 21/25 | Loss: 0.00127930
Iteration 22/25 | Loss: 0.00127263
Iteration 23/25 | Loss: 0.00127013
Iteration 24/25 | Loss: 0.00127189
Iteration 25/25 | Loss: 0.00127143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32693732
Iteration 2/25 | Loss: 0.00260186
Iteration 3/25 | Loss: 0.00260186
Iteration 4/25 | Loss: 0.00260186
Iteration 5/25 | Loss: 0.00236390
Iteration 6/25 | Loss: 0.00236000
Iteration 7/25 | Loss: 0.00232706
Iteration 8/25 | Loss: 0.00232706
Iteration 9/25 | Loss: 0.00232706
Iteration 10/25 | Loss: 0.00232706
Iteration 11/25 | Loss: 0.00232706
Iteration 12/25 | Loss: 0.00232706
Iteration 13/25 | Loss: 0.00232706
Iteration 14/25 | Loss: 0.00232706
Iteration 15/25 | Loss: 0.00232706
Iteration 16/25 | Loss: 0.00232706
Iteration 17/25 | Loss: 0.00232706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002327055437490344, 0.002327055437490344, 0.002327055437490344, 0.002327055437490344, 0.002327055437490344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002327055437490344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232706
Iteration 2/1000 | Loss: 0.00062687
Iteration 3/1000 | Loss: 0.00017381
Iteration 4/1000 | Loss: 0.00012365
Iteration 5/1000 | Loss: 0.00010731
Iteration 6/1000 | Loss: 0.00037488
Iteration 7/1000 | Loss: 0.00014793
Iteration 8/1000 | Loss: 0.00039971
Iteration 9/1000 | Loss: 0.00013260
Iteration 10/1000 | Loss: 0.00031020
Iteration 11/1000 | Loss: 0.00009715
Iteration 12/1000 | Loss: 0.00029269
Iteration 13/1000 | Loss: 0.00015140
Iteration 14/1000 | Loss: 0.00024648
Iteration 15/1000 | Loss: 0.00072260
Iteration 16/1000 | Loss: 0.00040521
Iteration 17/1000 | Loss: 0.00012794
Iteration 18/1000 | Loss: 0.00008829
Iteration 19/1000 | Loss: 0.00023847
Iteration 20/1000 | Loss: 0.00038516
Iteration 21/1000 | Loss: 0.00027197
Iteration 22/1000 | Loss: 0.00016378
Iteration 23/1000 | Loss: 0.00008492
Iteration 24/1000 | Loss: 0.00039525
Iteration 25/1000 | Loss: 0.00016210
Iteration 26/1000 | Loss: 0.00027412
Iteration 27/1000 | Loss: 0.00026574
Iteration 28/1000 | Loss: 0.00008368
Iteration 29/1000 | Loss: 0.00016283
Iteration 30/1000 | Loss: 0.00007883
Iteration 31/1000 | Loss: 0.00007751
Iteration 32/1000 | Loss: 0.00009043
Iteration 33/1000 | Loss: 0.00008023
Iteration 34/1000 | Loss: 0.00007542
Iteration 35/1000 | Loss: 0.00007460
Iteration 36/1000 | Loss: 0.00007387
Iteration 37/1000 | Loss: 0.00007338
Iteration 38/1000 | Loss: 0.00007308
Iteration 39/1000 | Loss: 0.00007287
Iteration 40/1000 | Loss: 0.00047443
Iteration 41/1000 | Loss: 0.00053202
Iteration 42/1000 | Loss: 0.00010147
Iteration 43/1000 | Loss: 0.00008174
Iteration 44/1000 | Loss: 0.00007499
Iteration 45/1000 | Loss: 0.00006925
Iteration 46/1000 | Loss: 0.00006442
Iteration 47/1000 | Loss: 0.00006104
Iteration 48/1000 | Loss: 0.00005990
Iteration 49/1000 | Loss: 0.00005895
Iteration 50/1000 | Loss: 0.00005824
Iteration 51/1000 | Loss: 0.00005769
Iteration 52/1000 | Loss: 0.00005728
Iteration 53/1000 | Loss: 0.00005689
Iteration 54/1000 | Loss: 0.00005664
Iteration 55/1000 | Loss: 0.00005639
Iteration 56/1000 | Loss: 0.00005618
Iteration 57/1000 | Loss: 0.00005608
Iteration 58/1000 | Loss: 0.00005592
Iteration 59/1000 | Loss: 0.00005585
Iteration 60/1000 | Loss: 0.00005571
Iteration 61/1000 | Loss: 0.00005565
Iteration 62/1000 | Loss: 0.00005551
Iteration 63/1000 | Loss: 0.00005550
Iteration 64/1000 | Loss: 0.00005549
Iteration 65/1000 | Loss: 0.00005547
Iteration 66/1000 | Loss: 0.00005545
Iteration 67/1000 | Loss: 0.00005538
Iteration 68/1000 | Loss: 0.00005538
Iteration 69/1000 | Loss: 0.00005537
Iteration 70/1000 | Loss: 0.00005531
Iteration 71/1000 | Loss: 0.00005528
Iteration 72/1000 | Loss: 0.00005528
Iteration 73/1000 | Loss: 0.00005527
Iteration 74/1000 | Loss: 0.00005526
Iteration 75/1000 | Loss: 0.00005526
Iteration 76/1000 | Loss: 0.00005525
Iteration 77/1000 | Loss: 0.00005525
Iteration 78/1000 | Loss: 0.00005524
Iteration 79/1000 | Loss: 0.00005524
Iteration 80/1000 | Loss: 0.00005524
Iteration 81/1000 | Loss: 0.00005523
Iteration 82/1000 | Loss: 0.00005521
Iteration 83/1000 | Loss: 0.00005517
Iteration 84/1000 | Loss: 0.00005517
Iteration 85/1000 | Loss: 0.00005517
Iteration 86/1000 | Loss: 0.00005517
Iteration 87/1000 | Loss: 0.00005517
Iteration 88/1000 | Loss: 0.00005516
Iteration 89/1000 | Loss: 0.00005516
Iteration 90/1000 | Loss: 0.00005514
Iteration 91/1000 | Loss: 0.00005514
Iteration 92/1000 | Loss: 0.00005513
Iteration 93/1000 | Loss: 0.00005513
Iteration 94/1000 | Loss: 0.00005513
Iteration 95/1000 | Loss: 0.00005512
Iteration 96/1000 | Loss: 0.00005512
Iteration 97/1000 | Loss: 0.00005511
Iteration 98/1000 | Loss: 0.00005511
Iteration 99/1000 | Loss: 0.00005510
Iteration 100/1000 | Loss: 0.00005510
Iteration 101/1000 | Loss: 0.00005509
Iteration 102/1000 | Loss: 0.00005509
Iteration 103/1000 | Loss: 0.00005509
Iteration 104/1000 | Loss: 0.00005509
Iteration 105/1000 | Loss: 0.00005508
Iteration 106/1000 | Loss: 0.00005508
Iteration 107/1000 | Loss: 0.00005508
Iteration 108/1000 | Loss: 0.00005508
Iteration 109/1000 | Loss: 0.00005508
Iteration 110/1000 | Loss: 0.00005508
Iteration 111/1000 | Loss: 0.00005508
Iteration 112/1000 | Loss: 0.00005508
Iteration 113/1000 | Loss: 0.00005508
Iteration 114/1000 | Loss: 0.00005508
Iteration 115/1000 | Loss: 0.00005508
Iteration 116/1000 | Loss: 0.00005507
Iteration 117/1000 | Loss: 0.00005507
Iteration 118/1000 | Loss: 0.00005507
Iteration 119/1000 | Loss: 0.00005507
Iteration 120/1000 | Loss: 0.00005507
Iteration 121/1000 | Loss: 0.00005507
Iteration 122/1000 | Loss: 0.00005507
Iteration 123/1000 | Loss: 0.00005507
Iteration 124/1000 | Loss: 0.00005507
Iteration 125/1000 | Loss: 0.00005507
Iteration 126/1000 | Loss: 0.00005506
Iteration 127/1000 | Loss: 0.00005506
Iteration 128/1000 | Loss: 0.00005506
Iteration 129/1000 | Loss: 0.00005505
Iteration 130/1000 | Loss: 0.00005505
Iteration 131/1000 | Loss: 0.00005505
Iteration 132/1000 | Loss: 0.00005505
Iteration 133/1000 | Loss: 0.00005504
Iteration 134/1000 | Loss: 0.00005504
Iteration 135/1000 | Loss: 0.00005504
Iteration 136/1000 | Loss: 0.00005504
Iteration 137/1000 | Loss: 0.00005504
Iteration 138/1000 | Loss: 0.00005503
Iteration 139/1000 | Loss: 0.00005503
Iteration 140/1000 | Loss: 0.00005503
Iteration 141/1000 | Loss: 0.00005503
Iteration 142/1000 | Loss: 0.00005503
Iteration 143/1000 | Loss: 0.00005502
Iteration 144/1000 | Loss: 0.00005502
Iteration 145/1000 | Loss: 0.00005502
Iteration 146/1000 | Loss: 0.00005502
Iteration 147/1000 | Loss: 0.00005502
Iteration 148/1000 | Loss: 0.00005502
Iteration 149/1000 | Loss: 0.00005502
Iteration 150/1000 | Loss: 0.00005502
Iteration 151/1000 | Loss: 0.00005502
Iteration 152/1000 | Loss: 0.00005502
Iteration 153/1000 | Loss: 0.00005502
Iteration 154/1000 | Loss: 0.00005502
Iteration 155/1000 | Loss: 0.00005502
Iteration 156/1000 | Loss: 0.00005502
Iteration 157/1000 | Loss: 0.00005502
Iteration 158/1000 | Loss: 0.00005502
Iteration 159/1000 | Loss: 0.00005502
Iteration 160/1000 | Loss: 0.00005501
Iteration 161/1000 | Loss: 0.00005501
Iteration 162/1000 | Loss: 0.00005501
Iteration 163/1000 | Loss: 0.00005501
Iteration 164/1000 | Loss: 0.00005501
Iteration 165/1000 | Loss: 0.00005501
Iteration 166/1000 | Loss: 0.00005501
Iteration 167/1000 | Loss: 0.00005501
Iteration 168/1000 | Loss: 0.00005501
Iteration 169/1000 | Loss: 0.00005501
Iteration 170/1000 | Loss: 0.00005501
Iteration 171/1000 | Loss: 0.00005501
Iteration 172/1000 | Loss: 0.00005501
Iteration 173/1000 | Loss: 0.00005501
Iteration 174/1000 | Loss: 0.00005500
Iteration 175/1000 | Loss: 0.00005500
Iteration 176/1000 | Loss: 0.00005500
Iteration 177/1000 | Loss: 0.00005500
Iteration 178/1000 | Loss: 0.00005500
Iteration 179/1000 | Loss: 0.00005500
Iteration 180/1000 | Loss: 0.00005500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [5.500452971318737e-05, 5.500452971318737e-05, 5.500452971318737e-05, 5.500452971318737e-05, 5.500452971318737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.500452971318737e-05

Optimization complete. Final v2v error: 4.136496067047119 mm

Highest mean error: 11.898553848266602 mm for frame 117

Lowest mean error: 2.9849483966827393 mm for frame 97

Saving results

Total time: 162.82839632034302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526635
Iteration 2/25 | Loss: 0.00132917
Iteration 3/25 | Loss: 0.00115711
Iteration 4/25 | Loss: 0.00111241
Iteration 5/25 | Loss: 0.00109133
Iteration 6/25 | Loss: 0.00108148
Iteration 7/25 | Loss: 0.00108016
Iteration 8/25 | Loss: 0.00108174
Iteration 9/25 | Loss: 0.00106804
Iteration 10/25 | Loss: 0.00106467
Iteration 11/25 | Loss: 0.00106169
Iteration 12/25 | Loss: 0.00106036
Iteration 13/25 | Loss: 0.00106029
Iteration 14/25 | Loss: 0.00105663
Iteration 15/25 | Loss: 0.00105476
Iteration 16/25 | Loss: 0.00105508
Iteration 17/25 | Loss: 0.00105512
Iteration 18/25 | Loss: 0.00105601
Iteration 19/25 | Loss: 0.00105442
Iteration 20/25 | Loss: 0.00105586
Iteration 21/25 | Loss: 0.00105444
Iteration 22/25 | Loss: 0.00105551
Iteration 23/25 | Loss: 0.00105486
Iteration 24/25 | Loss: 0.00105496
Iteration 25/25 | Loss: 0.00105565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43554342
Iteration 2/25 | Loss: 0.00078444
Iteration 3/25 | Loss: 0.00078444
Iteration 4/25 | Loss: 0.00078444
Iteration 5/25 | Loss: 0.00078444
Iteration 6/25 | Loss: 0.00078444
Iteration 7/25 | Loss: 0.00078443
Iteration 8/25 | Loss: 0.00078443
Iteration 9/25 | Loss: 0.00078443
Iteration 10/25 | Loss: 0.00078443
Iteration 11/25 | Loss: 0.00078443
Iteration 12/25 | Loss: 0.00078443
Iteration 13/25 | Loss: 0.00078443
Iteration 14/25 | Loss: 0.00078443
Iteration 15/25 | Loss: 0.00078443
Iteration 16/25 | Loss: 0.00078443
Iteration 17/25 | Loss: 0.00078443
Iteration 18/25 | Loss: 0.00078443
Iteration 19/25 | Loss: 0.00078443
Iteration 20/25 | Loss: 0.00078443
Iteration 21/25 | Loss: 0.00078443
Iteration 22/25 | Loss: 0.00078443
Iteration 23/25 | Loss: 0.00078443
Iteration 24/25 | Loss: 0.00078443
Iteration 25/25 | Loss: 0.00078443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078443
Iteration 2/1000 | Loss: 0.00002739
Iteration 3/1000 | Loss: 0.00003625
Iteration 4/1000 | Loss: 0.00001877
Iteration 5/1000 | Loss: 0.00001398
Iteration 6/1000 | Loss: 0.00002981
Iteration 7/1000 | Loss: 0.00002646
Iteration 8/1000 | Loss: 0.00002673
Iteration 9/1000 | Loss: 0.00002874
Iteration 10/1000 | Loss: 0.00002615
Iteration 11/1000 | Loss: 0.00004553
Iteration 12/1000 | Loss: 0.00002554
Iteration 13/1000 | Loss: 0.00003197
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00002806
Iteration 16/1000 | Loss: 0.00004049
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001081
Iteration 20/1000 | Loss: 0.00001041
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001012
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00000999
Iteration 25/1000 | Loss: 0.00000992
Iteration 26/1000 | Loss: 0.00000990
Iteration 27/1000 | Loss: 0.00000990
Iteration 28/1000 | Loss: 0.00000988
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000986
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000980
Iteration 37/1000 | Loss: 0.00000980
Iteration 38/1000 | Loss: 0.00000980
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000977
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000970
Iteration 58/1000 | Loss: 0.00000970
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000969
Iteration 61/1000 | Loss: 0.00000969
Iteration 62/1000 | Loss: 0.00000969
Iteration 63/1000 | Loss: 0.00000969
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000964
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000964
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000963
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000962
Iteration 95/1000 | Loss: 0.00000962
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000960
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000957
Iteration 119/1000 | Loss: 0.00000957
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000957
Iteration 124/1000 | Loss: 0.00000956
Iteration 125/1000 | Loss: 0.00000956
Iteration 126/1000 | Loss: 0.00000956
Iteration 127/1000 | Loss: 0.00000956
Iteration 128/1000 | Loss: 0.00000956
Iteration 129/1000 | Loss: 0.00000956
Iteration 130/1000 | Loss: 0.00000956
Iteration 131/1000 | Loss: 0.00000956
Iteration 132/1000 | Loss: 0.00000956
Iteration 133/1000 | Loss: 0.00000955
Iteration 134/1000 | Loss: 0.00000955
Iteration 135/1000 | Loss: 0.00000955
Iteration 136/1000 | Loss: 0.00000955
Iteration 137/1000 | Loss: 0.00000955
Iteration 138/1000 | Loss: 0.00000955
Iteration 139/1000 | Loss: 0.00000955
Iteration 140/1000 | Loss: 0.00000955
Iteration 141/1000 | Loss: 0.00000954
Iteration 142/1000 | Loss: 0.00000954
Iteration 143/1000 | Loss: 0.00000954
Iteration 144/1000 | Loss: 0.00000954
Iteration 145/1000 | Loss: 0.00000954
Iteration 146/1000 | Loss: 0.00000954
Iteration 147/1000 | Loss: 0.00000954
Iteration 148/1000 | Loss: 0.00000954
Iteration 149/1000 | Loss: 0.00000954
Iteration 150/1000 | Loss: 0.00000954
Iteration 151/1000 | Loss: 0.00000954
Iteration 152/1000 | Loss: 0.00000953
Iteration 153/1000 | Loss: 0.00000953
Iteration 154/1000 | Loss: 0.00000953
Iteration 155/1000 | Loss: 0.00000953
Iteration 156/1000 | Loss: 0.00000953
Iteration 157/1000 | Loss: 0.00000953
Iteration 158/1000 | Loss: 0.00000953
Iteration 159/1000 | Loss: 0.00000952
Iteration 160/1000 | Loss: 0.00000952
Iteration 161/1000 | Loss: 0.00000952
Iteration 162/1000 | Loss: 0.00000952
Iteration 163/1000 | Loss: 0.00000952
Iteration 164/1000 | Loss: 0.00000952
Iteration 165/1000 | Loss: 0.00000952
Iteration 166/1000 | Loss: 0.00000952
Iteration 167/1000 | Loss: 0.00000952
Iteration 168/1000 | Loss: 0.00000952
Iteration 169/1000 | Loss: 0.00000952
Iteration 170/1000 | Loss: 0.00000952
Iteration 171/1000 | Loss: 0.00000952
Iteration 172/1000 | Loss: 0.00000952
Iteration 173/1000 | Loss: 0.00000952
Iteration 174/1000 | Loss: 0.00000952
Iteration 175/1000 | Loss: 0.00000952
Iteration 176/1000 | Loss: 0.00000952
Iteration 177/1000 | Loss: 0.00000952
Iteration 178/1000 | Loss: 0.00000952
Iteration 179/1000 | Loss: 0.00000952
Iteration 180/1000 | Loss: 0.00000952
Iteration 181/1000 | Loss: 0.00000952
Iteration 182/1000 | Loss: 0.00000952
Iteration 183/1000 | Loss: 0.00000952
Iteration 184/1000 | Loss: 0.00000952
Iteration 185/1000 | Loss: 0.00000952
Iteration 186/1000 | Loss: 0.00000952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [9.52188202063553e-06, 9.52188202063553e-06, 9.52188202063553e-06, 9.52188202063553e-06, 9.52188202063553e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.52188202063553e-06

Optimization complete. Final v2v error: 2.639249086380005 mm

Highest mean error: 3.729679822921753 mm for frame 83

Lowest mean error: 2.3400521278381348 mm for frame 120

Saving results

Total time: 90.62708830833435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136260
Iteration 2/25 | Loss: 0.01136260
Iteration 3/25 | Loss: 0.01136260
Iteration 4/25 | Loss: 0.01136259
Iteration 5/25 | Loss: 0.01136259
Iteration 6/25 | Loss: 0.01136259
Iteration 7/25 | Loss: 0.01136259
Iteration 8/25 | Loss: 0.01136259
Iteration 9/25 | Loss: 0.01136259
Iteration 10/25 | Loss: 0.01136259
Iteration 11/25 | Loss: 0.01136259
Iteration 12/25 | Loss: 0.01136259
Iteration 13/25 | Loss: 0.01136259
Iteration 14/25 | Loss: 0.01136259
Iteration 15/25 | Loss: 0.01136259
Iteration 16/25 | Loss: 0.01136259
Iteration 17/25 | Loss: 0.01136259
Iteration 18/25 | Loss: 0.01136259
Iteration 19/25 | Loss: 0.01136259
Iteration 20/25 | Loss: 0.01136259
Iteration 21/25 | Loss: 0.01136259
Iteration 22/25 | Loss: 0.01136259
Iteration 23/25 | Loss: 0.01136259
Iteration 24/25 | Loss: 0.01136259
Iteration 25/25 | Loss: 0.01136258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.67644024
Iteration 2/25 | Loss: 0.18442115
Iteration 3/25 | Loss: 0.18383344
Iteration 4/25 | Loss: 0.18383344
Iteration 5/25 | Loss: 0.18383344
Iteration 6/25 | Loss: 0.18383344
Iteration 7/25 | Loss: 0.18383344
Iteration 8/25 | Loss: 0.18383344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.18383343517780304, 0.18383343517780304, 0.18383343517780304, 0.18383343517780304, 0.18383343517780304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18383343517780304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18383344
Iteration 2/1000 | Loss: 0.00543118
Iteration 3/1000 | Loss: 0.00163982
Iteration 4/1000 | Loss: 0.00047725
Iteration 5/1000 | Loss: 0.00035168
Iteration 6/1000 | Loss: 0.00024386
Iteration 7/1000 | Loss: 0.00007937
Iteration 8/1000 | Loss: 0.00007577
Iteration 9/1000 | Loss: 0.00014013
Iteration 10/1000 | Loss: 0.00006563
Iteration 11/1000 | Loss: 0.00004260
Iteration 12/1000 | Loss: 0.00010651
Iteration 13/1000 | Loss: 0.00004480
Iteration 14/1000 | Loss: 0.00003089
Iteration 15/1000 | Loss: 0.00005371
Iteration 16/1000 | Loss: 0.00002848
Iteration 17/1000 | Loss: 0.00005618
Iteration 18/1000 | Loss: 0.00013022
Iteration 19/1000 | Loss: 0.00002760
Iteration 20/1000 | Loss: 0.00005449
Iteration 21/1000 | Loss: 0.00009610
Iteration 22/1000 | Loss: 0.00005223
Iteration 23/1000 | Loss: 0.00012485
Iteration 24/1000 | Loss: 0.00003216
Iteration 25/1000 | Loss: 0.00002518
Iteration 26/1000 | Loss: 0.00002840
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00005182
Iteration 29/1000 | Loss: 0.00007400
Iteration 30/1000 | Loss: 0.00002428
Iteration 31/1000 | Loss: 0.00002653
Iteration 32/1000 | Loss: 0.00007438
Iteration 33/1000 | Loss: 0.00002455
Iteration 34/1000 | Loss: 0.00002233
Iteration 35/1000 | Loss: 0.00002205
Iteration 36/1000 | Loss: 0.00002185
Iteration 37/1000 | Loss: 0.00002474
Iteration 38/1000 | Loss: 0.00005329
Iteration 39/1000 | Loss: 0.00008181
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002102
Iteration 43/1000 | Loss: 0.00002099
Iteration 44/1000 | Loss: 0.00002094
Iteration 45/1000 | Loss: 0.00005929
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002413
Iteration 48/1000 | Loss: 0.00006996
Iteration 49/1000 | Loss: 0.00006032
Iteration 50/1000 | Loss: 0.00002070
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002055
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002217
Iteration 60/1000 | Loss: 0.00005934
Iteration 61/1000 | Loss: 0.00003831
Iteration 62/1000 | Loss: 0.00002199
Iteration 63/1000 | Loss: 0.00007993
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002203
Iteration 66/1000 | Loss: 0.00005377
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002048
Iteration 69/1000 | Loss: 0.00002047
Iteration 70/1000 | Loss: 0.00004446
Iteration 71/1000 | Loss: 0.00003657
Iteration 72/1000 | Loss: 0.00002180
Iteration 73/1000 | Loss: 0.00003896
Iteration 74/1000 | Loss: 0.00002230
Iteration 75/1000 | Loss: 0.00002084
Iteration 76/1000 | Loss: 0.00008323
Iteration 77/1000 | Loss: 0.00011504
Iteration 78/1000 | Loss: 0.00002383
Iteration 79/1000 | Loss: 0.00003576
Iteration 80/1000 | Loss: 0.00002033
Iteration 81/1000 | Loss: 0.00007343
Iteration 82/1000 | Loss: 0.00007343
Iteration 83/1000 | Loss: 0.00002430
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00003586
Iteration 86/1000 | Loss: 0.00002585
Iteration 87/1000 | Loss: 0.00003007
Iteration 88/1000 | Loss: 0.00002908
Iteration 89/1000 | Loss: 0.00002859
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002024
Iteration 92/1000 | Loss: 0.00002024
Iteration 93/1000 | Loss: 0.00002024
Iteration 94/1000 | Loss: 0.00002024
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002023
Iteration 100/1000 | Loss: 0.00002023
Iteration 101/1000 | Loss: 0.00002023
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002022
Iteration 106/1000 | Loss: 0.00002021
Iteration 107/1000 | Loss: 0.00002021
Iteration 108/1000 | Loss: 0.00002021
Iteration 109/1000 | Loss: 0.00002021
Iteration 110/1000 | Loss: 0.00002021
Iteration 111/1000 | Loss: 0.00002021
Iteration 112/1000 | Loss: 0.00002021
Iteration 113/1000 | Loss: 0.00002021
Iteration 114/1000 | Loss: 0.00002021
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002020
Iteration 118/1000 | Loss: 0.00002020
Iteration 119/1000 | Loss: 0.00002020
Iteration 120/1000 | Loss: 0.00002020
Iteration 121/1000 | Loss: 0.00002020
Iteration 122/1000 | Loss: 0.00002020
Iteration 123/1000 | Loss: 0.00002020
Iteration 124/1000 | Loss: 0.00002020
Iteration 125/1000 | Loss: 0.00002020
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002019
Iteration 128/1000 | Loss: 0.00002019
Iteration 129/1000 | Loss: 0.00002019
Iteration 130/1000 | Loss: 0.00002019
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002019
Iteration 133/1000 | Loss: 0.00002019
Iteration 134/1000 | Loss: 0.00002019
Iteration 135/1000 | Loss: 0.00002019
Iteration 136/1000 | Loss: 0.00002019
Iteration 137/1000 | Loss: 0.00002019
Iteration 138/1000 | Loss: 0.00002019
Iteration 139/1000 | Loss: 0.00002019
Iteration 140/1000 | Loss: 0.00002019
Iteration 141/1000 | Loss: 0.00002018
Iteration 142/1000 | Loss: 0.00002018
Iteration 143/1000 | Loss: 0.00002018
Iteration 144/1000 | Loss: 0.00002018
Iteration 145/1000 | Loss: 0.00002018
Iteration 146/1000 | Loss: 0.00002018
Iteration 147/1000 | Loss: 0.00002018
Iteration 148/1000 | Loss: 0.00002018
Iteration 149/1000 | Loss: 0.00002018
Iteration 150/1000 | Loss: 0.00002018
Iteration 151/1000 | Loss: 0.00002017
Iteration 152/1000 | Loss: 0.00002017
Iteration 153/1000 | Loss: 0.00002017
Iteration 154/1000 | Loss: 0.00002017
Iteration 155/1000 | Loss: 0.00002017
Iteration 156/1000 | Loss: 0.00002017
Iteration 157/1000 | Loss: 0.00002017
Iteration 158/1000 | Loss: 0.00002017
Iteration 159/1000 | Loss: 0.00002017
Iteration 160/1000 | Loss: 0.00002017
Iteration 161/1000 | Loss: 0.00002017
Iteration 162/1000 | Loss: 0.00002017
Iteration 163/1000 | Loss: 0.00002017
Iteration 164/1000 | Loss: 0.00002017
Iteration 165/1000 | Loss: 0.00002017
Iteration 166/1000 | Loss: 0.00002017
Iteration 167/1000 | Loss: 0.00002016
Iteration 168/1000 | Loss: 0.00002016
Iteration 169/1000 | Loss: 0.00002100
Iteration 170/1000 | Loss: 0.00002024
Iteration 171/1000 | Loss: 0.00007049
Iteration 172/1000 | Loss: 0.00002023
Iteration 173/1000 | Loss: 0.00002018
Iteration 174/1000 | Loss: 0.00002018
Iteration 175/1000 | Loss: 0.00002018
Iteration 176/1000 | Loss: 0.00002018
Iteration 177/1000 | Loss: 0.00002018
Iteration 178/1000 | Loss: 0.00002018
Iteration 179/1000 | Loss: 0.00002018
Iteration 180/1000 | Loss: 0.00002018
Iteration 181/1000 | Loss: 0.00002056
Iteration 182/1000 | Loss: 0.00002056
Iteration 183/1000 | Loss: 0.00002020
Iteration 184/1000 | Loss: 0.00002020
Iteration 185/1000 | Loss: 0.00002013
Iteration 186/1000 | Loss: 0.00002012
Iteration 187/1000 | Loss: 0.00002012
Iteration 188/1000 | Loss: 0.00002012
Iteration 189/1000 | Loss: 0.00002012
Iteration 190/1000 | Loss: 0.00002012
Iteration 191/1000 | Loss: 0.00002012
Iteration 192/1000 | Loss: 0.00002012
Iteration 193/1000 | Loss: 0.00002012
Iteration 194/1000 | Loss: 0.00002012
Iteration 195/1000 | Loss: 0.00002012
Iteration 196/1000 | Loss: 0.00002012
Iteration 197/1000 | Loss: 0.00002012
Iteration 198/1000 | Loss: 0.00002012
Iteration 199/1000 | Loss: 0.00002012
Iteration 200/1000 | Loss: 0.00002012
Iteration 201/1000 | Loss: 0.00002012
Iteration 202/1000 | Loss: 0.00002012
Iteration 203/1000 | Loss: 0.00002012
Iteration 204/1000 | Loss: 0.00002012
Iteration 205/1000 | Loss: 0.00002012
Iteration 206/1000 | Loss: 0.00002012
Iteration 207/1000 | Loss: 0.00002012
Iteration 208/1000 | Loss: 0.00002012
Iteration 209/1000 | Loss: 0.00002012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.0121475245105103e-05, 2.0121475245105103e-05, 2.0121475245105103e-05, 2.0121475245105103e-05, 2.0121475245105103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0121475245105103e-05

Optimization complete. Final v2v error: 3.4171383380889893 mm

Highest mean error: 20.82022476196289 mm for frame 51

Lowest mean error: 3.049891471862793 mm for frame 211

Saving results

Total time: 139.0808081626892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814600
Iteration 2/25 | Loss: 0.00136004
Iteration 3/25 | Loss: 0.00117123
Iteration 4/25 | Loss: 0.00115826
Iteration 5/25 | Loss: 0.00115713
Iteration 6/25 | Loss: 0.00115704
Iteration 7/25 | Loss: 0.00115704
Iteration 8/25 | Loss: 0.00115704
Iteration 9/25 | Loss: 0.00115704
Iteration 10/25 | Loss: 0.00115704
Iteration 11/25 | Loss: 0.00115704
Iteration 12/25 | Loss: 0.00115704
Iteration 13/25 | Loss: 0.00115704
Iteration 14/25 | Loss: 0.00115704
Iteration 15/25 | Loss: 0.00115704
Iteration 16/25 | Loss: 0.00115704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001157037797383964, 0.001157037797383964, 0.001157037797383964, 0.001157037797383964, 0.001157037797383964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157037797383964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27879643
Iteration 2/25 | Loss: 0.00063810
Iteration 3/25 | Loss: 0.00063809
Iteration 4/25 | Loss: 0.00063809
Iteration 5/25 | Loss: 0.00063809
Iteration 6/25 | Loss: 0.00063809
Iteration 7/25 | Loss: 0.00063808
Iteration 8/25 | Loss: 0.00063808
Iteration 9/25 | Loss: 0.00063808
Iteration 10/25 | Loss: 0.00063808
Iteration 11/25 | Loss: 0.00063808
Iteration 12/25 | Loss: 0.00063808
Iteration 13/25 | Loss: 0.00063808
Iteration 14/25 | Loss: 0.00063808
Iteration 15/25 | Loss: 0.00063808
Iteration 16/25 | Loss: 0.00063808
Iteration 17/25 | Loss: 0.00063808
Iteration 18/25 | Loss: 0.00063808
Iteration 19/25 | Loss: 0.00063808
Iteration 20/25 | Loss: 0.00063808
Iteration 21/25 | Loss: 0.00063808
Iteration 22/25 | Loss: 0.00063808
Iteration 23/25 | Loss: 0.00063808
Iteration 24/25 | Loss: 0.00063808
Iteration 25/25 | Loss: 0.00063808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063808
Iteration 2/1000 | Loss: 0.00003343
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001845
Iteration 6/1000 | Loss: 0.00001787
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001724
Iteration 9/1000 | Loss: 0.00001701
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001658
Iteration 19/1000 | Loss: 0.00001657
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001648
Iteration 23/1000 | Loss: 0.00001646
Iteration 24/1000 | Loss: 0.00001645
Iteration 25/1000 | Loss: 0.00001645
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001644
Iteration 28/1000 | Loss: 0.00001644
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001638
Iteration 51/1000 | Loss: 0.00001638
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001638
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001637
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001636
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001634
Iteration 78/1000 | Loss: 0.00001634
Iteration 79/1000 | Loss: 0.00001634
Iteration 80/1000 | Loss: 0.00001634
Iteration 81/1000 | Loss: 0.00001634
Iteration 82/1000 | Loss: 0.00001634
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001633
Iteration 86/1000 | Loss: 0.00001633
Iteration 87/1000 | Loss: 0.00001633
Iteration 88/1000 | Loss: 0.00001633
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001632
Iteration 91/1000 | Loss: 0.00001632
Iteration 92/1000 | Loss: 0.00001632
Iteration 93/1000 | Loss: 0.00001632
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001632
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001632
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001630
Iteration 109/1000 | Loss: 0.00001630
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001630
Iteration 112/1000 | Loss: 0.00001630
Iteration 113/1000 | Loss: 0.00001630
Iteration 114/1000 | Loss: 0.00001630
Iteration 115/1000 | Loss: 0.00001630
Iteration 116/1000 | Loss: 0.00001630
Iteration 117/1000 | Loss: 0.00001629
Iteration 118/1000 | Loss: 0.00001629
Iteration 119/1000 | Loss: 0.00001629
Iteration 120/1000 | Loss: 0.00001629
Iteration 121/1000 | Loss: 0.00001629
Iteration 122/1000 | Loss: 0.00001629
Iteration 123/1000 | Loss: 0.00001629
Iteration 124/1000 | Loss: 0.00001629
Iteration 125/1000 | Loss: 0.00001628
Iteration 126/1000 | Loss: 0.00001628
Iteration 127/1000 | Loss: 0.00001628
Iteration 128/1000 | Loss: 0.00001628
Iteration 129/1000 | Loss: 0.00001628
Iteration 130/1000 | Loss: 0.00001628
Iteration 131/1000 | Loss: 0.00001628
Iteration 132/1000 | Loss: 0.00001628
Iteration 133/1000 | Loss: 0.00001628
Iteration 134/1000 | Loss: 0.00001628
Iteration 135/1000 | Loss: 0.00001628
Iteration 136/1000 | Loss: 0.00001628
Iteration 137/1000 | Loss: 0.00001628
Iteration 138/1000 | Loss: 0.00001628
Iteration 139/1000 | Loss: 0.00001628
Iteration 140/1000 | Loss: 0.00001628
Iteration 141/1000 | Loss: 0.00001628
Iteration 142/1000 | Loss: 0.00001628
Iteration 143/1000 | Loss: 0.00001628
Iteration 144/1000 | Loss: 0.00001628
Iteration 145/1000 | Loss: 0.00001628
Iteration 146/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.6278994735330343e-05, 1.6278994735330343e-05, 1.6278994735330343e-05, 1.6278994735330343e-05, 1.6278994735330343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6278994735330343e-05

Optimization complete. Final v2v error: 3.393616199493408 mm

Highest mean error: 3.8275275230407715 mm for frame 61

Lowest mean error: 3.163381814956665 mm for frame 5

Saving results

Total time: 33.00065779685974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807341
Iteration 2/25 | Loss: 0.00124674
Iteration 3/25 | Loss: 0.00109333
Iteration 4/25 | Loss: 0.00106038
Iteration 5/25 | Loss: 0.00105487
Iteration 6/25 | Loss: 0.00105347
Iteration 7/25 | Loss: 0.00105337
Iteration 8/25 | Loss: 0.00105337
Iteration 9/25 | Loss: 0.00105337
Iteration 10/25 | Loss: 0.00105337
Iteration 11/25 | Loss: 0.00105337
Iteration 12/25 | Loss: 0.00105337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001053368323482573, 0.001053368323482573, 0.001053368323482573, 0.001053368323482573, 0.001053368323482573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001053368323482573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34986341
Iteration 2/25 | Loss: 0.00053113
Iteration 3/25 | Loss: 0.00053113
Iteration 4/25 | Loss: 0.00053112
Iteration 5/25 | Loss: 0.00053112
Iteration 6/25 | Loss: 0.00053112
Iteration 7/25 | Loss: 0.00053112
Iteration 8/25 | Loss: 0.00053112
Iteration 9/25 | Loss: 0.00053112
Iteration 10/25 | Loss: 0.00053112
Iteration 11/25 | Loss: 0.00053112
Iteration 12/25 | Loss: 0.00053112
Iteration 13/25 | Loss: 0.00053112
Iteration 14/25 | Loss: 0.00053112
Iteration 15/25 | Loss: 0.00053112
Iteration 16/25 | Loss: 0.00053112
Iteration 17/25 | Loss: 0.00053112
Iteration 18/25 | Loss: 0.00053112
Iteration 19/25 | Loss: 0.00053112
Iteration 20/25 | Loss: 0.00053112
Iteration 21/25 | Loss: 0.00053112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005311230779625475, 0.0005311230779625475, 0.0005311230779625475, 0.0005311230779625475, 0.0005311230779625475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005311230779625475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053112
Iteration 2/1000 | Loss: 0.00004004
Iteration 3/1000 | Loss: 0.00002724
Iteration 4/1000 | Loss: 0.00002270
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002065
Iteration 7/1000 | Loss: 0.00001982
Iteration 8/1000 | Loss: 0.00001938
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001823
Iteration 13/1000 | Loss: 0.00001820
Iteration 14/1000 | Loss: 0.00001817
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001805
Iteration 17/1000 | Loss: 0.00001793
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001789
Iteration 23/1000 | Loss: 0.00001788
Iteration 24/1000 | Loss: 0.00001788
Iteration 25/1000 | Loss: 0.00001788
Iteration 26/1000 | Loss: 0.00001787
Iteration 27/1000 | Loss: 0.00001787
Iteration 28/1000 | Loss: 0.00001786
Iteration 29/1000 | Loss: 0.00001786
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001785
Iteration 32/1000 | Loss: 0.00001785
Iteration 33/1000 | Loss: 0.00001784
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001773
Iteration 42/1000 | Loss: 0.00001772
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001768
Iteration 46/1000 | Loss: 0.00001767
Iteration 47/1000 | Loss: 0.00001767
Iteration 48/1000 | Loss: 0.00001766
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001766
Iteration 51/1000 | Loss: 0.00001765
Iteration 52/1000 | Loss: 0.00001765
Iteration 53/1000 | Loss: 0.00001765
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001764
Iteration 56/1000 | Loss: 0.00001763
Iteration 57/1000 | Loss: 0.00001763
Iteration 58/1000 | Loss: 0.00001763
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001761
Iteration 70/1000 | Loss: 0.00001761
Iteration 71/1000 | Loss: 0.00001761
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001759
Iteration 90/1000 | Loss: 0.00001759
Iteration 91/1000 | Loss: 0.00001759
Iteration 92/1000 | Loss: 0.00001759
Iteration 93/1000 | Loss: 0.00001759
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001758
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001758
Iteration 100/1000 | Loss: 0.00001758
Iteration 101/1000 | Loss: 0.00001758
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001757
Iteration 107/1000 | Loss: 0.00001757
Iteration 108/1000 | Loss: 0.00001757
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001756
Iteration 112/1000 | Loss: 0.00001756
Iteration 113/1000 | Loss: 0.00001756
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001755
Iteration 116/1000 | Loss: 0.00001755
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001755
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001755
Iteration 124/1000 | Loss: 0.00001755
Iteration 125/1000 | Loss: 0.00001755
Iteration 126/1000 | Loss: 0.00001754
Iteration 127/1000 | Loss: 0.00001754
Iteration 128/1000 | Loss: 0.00001754
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001754
Iteration 131/1000 | Loss: 0.00001754
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Iteration 138/1000 | Loss: 0.00001753
Iteration 139/1000 | Loss: 0.00001753
Iteration 140/1000 | Loss: 0.00001753
Iteration 141/1000 | Loss: 0.00001753
Iteration 142/1000 | Loss: 0.00001753
Iteration 143/1000 | Loss: 0.00001753
Iteration 144/1000 | Loss: 0.00001753
Iteration 145/1000 | Loss: 0.00001752
Iteration 146/1000 | Loss: 0.00001752
Iteration 147/1000 | Loss: 0.00001752
Iteration 148/1000 | Loss: 0.00001752
Iteration 149/1000 | Loss: 0.00001752
Iteration 150/1000 | Loss: 0.00001752
Iteration 151/1000 | Loss: 0.00001751
Iteration 152/1000 | Loss: 0.00001751
Iteration 153/1000 | Loss: 0.00001751
Iteration 154/1000 | Loss: 0.00001751
Iteration 155/1000 | Loss: 0.00001751
Iteration 156/1000 | Loss: 0.00001751
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001750
Iteration 161/1000 | Loss: 0.00001750
Iteration 162/1000 | Loss: 0.00001750
Iteration 163/1000 | Loss: 0.00001750
Iteration 164/1000 | Loss: 0.00001750
Iteration 165/1000 | Loss: 0.00001750
Iteration 166/1000 | Loss: 0.00001750
Iteration 167/1000 | Loss: 0.00001750
Iteration 168/1000 | Loss: 0.00001750
Iteration 169/1000 | Loss: 0.00001750
Iteration 170/1000 | Loss: 0.00001750
Iteration 171/1000 | Loss: 0.00001750
Iteration 172/1000 | Loss: 0.00001750
Iteration 173/1000 | Loss: 0.00001750
Iteration 174/1000 | Loss: 0.00001750
Iteration 175/1000 | Loss: 0.00001750
Iteration 176/1000 | Loss: 0.00001750
Iteration 177/1000 | Loss: 0.00001750
Iteration 178/1000 | Loss: 0.00001750
Iteration 179/1000 | Loss: 0.00001750
Iteration 180/1000 | Loss: 0.00001750
Iteration 181/1000 | Loss: 0.00001750
Iteration 182/1000 | Loss: 0.00001749
Iteration 183/1000 | Loss: 0.00001749
Iteration 184/1000 | Loss: 0.00001749
Iteration 185/1000 | Loss: 0.00001749
Iteration 186/1000 | Loss: 0.00001749
Iteration 187/1000 | Loss: 0.00001749
Iteration 188/1000 | Loss: 0.00001749
Iteration 189/1000 | Loss: 0.00001749
Iteration 190/1000 | Loss: 0.00001749
Iteration 191/1000 | Loss: 0.00001749
Iteration 192/1000 | Loss: 0.00001749
Iteration 193/1000 | Loss: 0.00001749
Iteration 194/1000 | Loss: 0.00001749
Iteration 195/1000 | Loss: 0.00001749
Iteration 196/1000 | Loss: 0.00001749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.7490458048996516e-05, 1.7490458048996516e-05, 1.7490458048996516e-05, 1.7490458048996516e-05, 1.7490458048996516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7490458048996516e-05

Optimization complete. Final v2v error: 3.574371099472046 mm

Highest mean error: 3.9632837772369385 mm for frame 25

Lowest mean error: 3.2841641902923584 mm for frame 139

Saving results

Total time: 41.43722939491272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996906
Iteration 2/25 | Loss: 0.00271546
Iteration 3/25 | Loss: 0.00189052
Iteration 4/25 | Loss: 0.00182111
Iteration 5/25 | Loss: 0.00164543
Iteration 6/25 | Loss: 0.00154875
Iteration 7/25 | Loss: 0.00139313
Iteration 8/25 | Loss: 0.00122449
Iteration 9/25 | Loss: 0.00119383
Iteration 10/25 | Loss: 0.00118161
Iteration 11/25 | Loss: 0.00118149
Iteration 12/25 | Loss: 0.00118334
Iteration 13/25 | Loss: 0.00117499
Iteration 14/25 | Loss: 0.00116738
Iteration 15/25 | Loss: 0.00116229
Iteration 16/25 | Loss: 0.00116027
Iteration 17/25 | Loss: 0.00115938
Iteration 18/25 | Loss: 0.00115910
Iteration 19/25 | Loss: 0.00115903
Iteration 20/25 | Loss: 0.00115902
Iteration 21/25 | Loss: 0.00115902
Iteration 22/25 | Loss: 0.00115902
Iteration 23/25 | Loss: 0.00115902
Iteration 24/25 | Loss: 0.00115902
Iteration 25/25 | Loss: 0.00115902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35656238
Iteration 2/25 | Loss: 0.00056257
Iteration 3/25 | Loss: 0.00056257
Iteration 4/25 | Loss: 0.00056257
Iteration 5/25 | Loss: 0.00056257
Iteration 6/25 | Loss: 0.00056257
Iteration 7/25 | Loss: 0.00056257
Iteration 8/25 | Loss: 0.00056257
Iteration 9/25 | Loss: 0.00056257
Iteration 10/25 | Loss: 0.00056257
Iteration 11/25 | Loss: 0.00056257
Iteration 12/25 | Loss: 0.00056257
Iteration 13/25 | Loss: 0.00056257
Iteration 14/25 | Loss: 0.00056257
Iteration 15/25 | Loss: 0.00056257
Iteration 16/25 | Loss: 0.00056257
Iteration 17/25 | Loss: 0.00056257
Iteration 18/25 | Loss: 0.00056257
Iteration 19/25 | Loss: 0.00056257
Iteration 20/25 | Loss: 0.00056257
Iteration 21/25 | Loss: 0.00056257
Iteration 22/25 | Loss: 0.00056257
Iteration 23/25 | Loss: 0.00056257
Iteration 24/25 | Loss: 0.00056257
Iteration 25/25 | Loss: 0.00056257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056257
Iteration 2/1000 | Loss: 0.00003354
Iteration 3/1000 | Loss: 0.00002415
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00002187
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002076
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002031
Iteration 11/1000 | Loss: 0.00002022
Iteration 12/1000 | Loss: 0.00002019
Iteration 13/1000 | Loss: 0.00002016
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00002016
Iteration 16/1000 | Loss: 0.00002015
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002015
Iteration 19/1000 | Loss: 0.00002010
Iteration 20/1000 | Loss: 0.00002009
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002008
Iteration 23/1000 | Loss: 0.00002008
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00002007
Iteration 26/1000 | Loss: 0.00002006
Iteration 27/1000 | Loss: 0.00002006
Iteration 28/1000 | Loss: 0.00002006
Iteration 29/1000 | Loss: 0.00002006
Iteration 30/1000 | Loss: 0.00002006
Iteration 31/1000 | Loss: 0.00002005
Iteration 32/1000 | Loss: 0.00002005
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00002005
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00002005
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00001998
Iteration 39/1000 | Loss: 0.00001998
Iteration 40/1000 | Loss: 0.00001998
Iteration 41/1000 | Loss: 0.00001998
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001997
Iteration 45/1000 | Loss: 0.00001997
Iteration 46/1000 | Loss: 0.00001997
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001996
Iteration 50/1000 | Loss: 0.00001996
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001995
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00001994
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001993
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001989
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001989
Iteration 84/1000 | Loss: 0.00001989
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001987
Iteration 99/1000 | Loss: 0.00001987
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001987
Iteration 102/1000 | Loss: 0.00001987
Iteration 103/1000 | Loss: 0.00001986
Iteration 104/1000 | Loss: 0.00001986
Iteration 105/1000 | Loss: 0.00001986
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001986
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001985
Iteration 111/1000 | Loss: 0.00001985
Iteration 112/1000 | Loss: 0.00001985
Iteration 113/1000 | Loss: 0.00001985
Iteration 114/1000 | Loss: 0.00001985
Iteration 115/1000 | Loss: 0.00001985
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001984
Iteration 120/1000 | Loss: 0.00001984
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001984
Iteration 129/1000 | Loss: 0.00001984
Iteration 130/1000 | Loss: 0.00001984
Iteration 131/1000 | Loss: 0.00001984
Iteration 132/1000 | Loss: 0.00001984
Iteration 133/1000 | Loss: 0.00001983
Iteration 134/1000 | Loss: 0.00001983
Iteration 135/1000 | Loss: 0.00001983
Iteration 136/1000 | Loss: 0.00001983
Iteration 137/1000 | Loss: 0.00001983
Iteration 138/1000 | Loss: 0.00001983
Iteration 139/1000 | Loss: 0.00001983
Iteration 140/1000 | Loss: 0.00001983
Iteration 141/1000 | Loss: 0.00001983
Iteration 142/1000 | Loss: 0.00001983
Iteration 143/1000 | Loss: 0.00001983
Iteration 144/1000 | Loss: 0.00001983
Iteration 145/1000 | Loss: 0.00001983
Iteration 146/1000 | Loss: 0.00001983
Iteration 147/1000 | Loss: 0.00001983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.983128095162101e-05, 1.983128095162101e-05, 1.983128095162101e-05, 1.983128095162101e-05, 1.983128095162101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.983128095162101e-05

Optimization complete. Final v2v error: 3.74747896194458 mm

Highest mean error: 4.05577278137207 mm for frame 238

Lowest mean error: 3.5574748516082764 mm for frame 94

Saving results

Total time: 64.63071084022522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835601
Iteration 2/25 | Loss: 0.00135502
Iteration 3/25 | Loss: 0.00120736
Iteration 4/25 | Loss: 0.00118652
Iteration 5/25 | Loss: 0.00118113
Iteration 6/25 | Loss: 0.00117973
Iteration 7/25 | Loss: 0.00117912
Iteration 8/25 | Loss: 0.00117912
Iteration 9/25 | Loss: 0.00117912
Iteration 10/25 | Loss: 0.00117912
Iteration 11/25 | Loss: 0.00117912
Iteration 12/25 | Loss: 0.00117912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011791186407208443, 0.0011791186407208443, 0.0011791186407208443, 0.0011791186407208443, 0.0011791186407208443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011791186407208443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33248806
Iteration 2/25 | Loss: 0.00058809
Iteration 3/25 | Loss: 0.00058801
Iteration 4/25 | Loss: 0.00058801
Iteration 5/25 | Loss: 0.00058801
Iteration 6/25 | Loss: 0.00058801
Iteration 7/25 | Loss: 0.00058800
Iteration 8/25 | Loss: 0.00058800
Iteration 9/25 | Loss: 0.00058800
Iteration 10/25 | Loss: 0.00058800
Iteration 11/25 | Loss: 0.00058800
Iteration 12/25 | Loss: 0.00058800
Iteration 13/25 | Loss: 0.00058800
Iteration 14/25 | Loss: 0.00058800
Iteration 15/25 | Loss: 0.00058800
Iteration 16/25 | Loss: 0.00058800
Iteration 17/25 | Loss: 0.00058800
Iteration 18/25 | Loss: 0.00058800
Iteration 19/25 | Loss: 0.00058800
Iteration 20/25 | Loss: 0.00058800
Iteration 21/25 | Loss: 0.00058800
Iteration 22/25 | Loss: 0.00058800
Iteration 23/25 | Loss: 0.00058800
Iteration 24/25 | Loss: 0.00058800
Iteration 25/25 | Loss: 0.00058800

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058800
Iteration 2/1000 | Loss: 0.00005055
Iteration 3/1000 | Loss: 0.00003500
Iteration 4/1000 | Loss: 0.00003179
Iteration 5/1000 | Loss: 0.00003059
Iteration 6/1000 | Loss: 0.00002965
Iteration 7/1000 | Loss: 0.00002901
Iteration 8/1000 | Loss: 0.00002849
Iteration 9/1000 | Loss: 0.00002813
Iteration 10/1000 | Loss: 0.00002784
Iteration 11/1000 | Loss: 0.00002764
Iteration 12/1000 | Loss: 0.00002747
Iteration 13/1000 | Loss: 0.00002745
Iteration 14/1000 | Loss: 0.00002743
Iteration 15/1000 | Loss: 0.00002741
Iteration 16/1000 | Loss: 0.00002729
Iteration 17/1000 | Loss: 0.00002727
Iteration 18/1000 | Loss: 0.00002715
Iteration 19/1000 | Loss: 0.00002714
Iteration 20/1000 | Loss: 0.00002712
Iteration 21/1000 | Loss: 0.00002712
Iteration 22/1000 | Loss: 0.00002711
Iteration 23/1000 | Loss: 0.00002711
Iteration 24/1000 | Loss: 0.00002711
Iteration 25/1000 | Loss: 0.00002707
Iteration 26/1000 | Loss: 0.00002704
Iteration 27/1000 | Loss: 0.00002704
Iteration 28/1000 | Loss: 0.00002703
Iteration 29/1000 | Loss: 0.00002703
Iteration 30/1000 | Loss: 0.00002703
Iteration 31/1000 | Loss: 0.00002703
Iteration 32/1000 | Loss: 0.00002703
Iteration 33/1000 | Loss: 0.00002703
Iteration 34/1000 | Loss: 0.00002703
Iteration 35/1000 | Loss: 0.00002702
Iteration 36/1000 | Loss: 0.00002702
Iteration 37/1000 | Loss: 0.00002702
Iteration 38/1000 | Loss: 0.00002702
Iteration 39/1000 | Loss: 0.00002702
Iteration 40/1000 | Loss: 0.00002702
Iteration 41/1000 | Loss: 0.00002702
Iteration 42/1000 | Loss: 0.00002702
Iteration 43/1000 | Loss: 0.00002702
Iteration 44/1000 | Loss: 0.00002701
Iteration 45/1000 | Loss: 0.00002701
Iteration 46/1000 | Loss: 0.00002701
Iteration 47/1000 | Loss: 0.00002701
Iteration 48/1000 | Loss: 0.00002701
Iteration 49/1000 | Loss: 0.00002701
Iteration 50/1000 | Loss: 0.00002700
Iteration 51/1000 | Loss: 0.00002700
Iteration 52/1000 | Loss: 0.00002700
Iteration 53/1000 | Loss: 0.00002700
Iteration 54/1000 | Loss: 0.00002700
Iteration 55/1000 | Loss: 0.00002700
Iteration 56/1000 | Loss: 0.00002700
Iteration 57/1000 | Loss: 0.00002700
Iteration 58/1000 | Loss: 0.00002700
Iteration 59/1000 | Loss: 0.00002700
Iteration 60/1000 | Loss: 0.00002699
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002698
Iteration 64/1000 | Loss: 0.00002698
Iteration 65/1000 | Loss: 0.00002698
Iteration 66/1000 | Loss: 0.00002698
Iteration 67/1000 | Loss: 0.00002698
Iteration 68/1000 | Loss: 0.00002698
Iteration 69/1000 | Loss: 0.00002698
Iteration 70/1000 | Loss: 0.00002698
Iteration 71/1000 | Loss: 0.00002698
Iteration 72/1000 | Loss: 0.00002698
Iteration 73/1000 | Loss: 0.00002698
Iteration 74/1000 | Loss: 0.00002698
Iteration 75/1000 | Loss: 0.00002698
Iteration 76/1000 | Loss: 0.00002698
Iteration 77/1000 | Loss: 0.00002698
Iteration 78/1000 | Loss: 0.00002698
Iteration 79/1000 | Loss: 0.00002698
Iteration 80/1000 | Loss: 0.00002698
Iteration 81/1000 | Loss: 0.00002698
Iteration 82/1000 | Loss: 0.00002698
Iteration 83/1000 | Loss: 0.00002698
Iteration 84/1000 | Loss: 0.00002698
Iteration 85/1000 | Loss: 0.00002698
Iteration 86/1000 | Loss: 0.00002698
Iteration 87/1000 | Loss: 0.00002698
Iteration 88/1000 | Loss: 0.00002698
Iteration 89/1000 | Loss: 0.00002698
Iteration 90/1000 | Loss: 0.00002698
Iteration 91/1000 | Loss: 0.00002698
Iteration 92/1000 | Loss: 0.00002698
Iteration 93/1000 | Loss: 0.00002698
Iteration 94/1000 | Loss: 0.00002698
Iteration 95/1000 | Loss: 0.00002698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.6977271772921085e-05, 2.6977271772921085e-05, 2.6977271772921085e-05, 2.6977271772921085e-05, 2.6977271772921085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6977271772921085e-05

Optimization complete. Final v2v error: 4.2432403564453125 mm

Highest mean error: 4.521909236907959 mm for frame 22

Lowest mean error: 3.893131971359253 mm for frame 120

Saving results

Total time: 36.15605688095093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540207
Iteration 2/25 | Loss: 0.00114886
Iteration 3/25 | Loss: 0.00106816
Iteration 4/25 | Loss: 0.00105650
Iteration 5/25 | Loss: 0.00105290
Iteration 6/25 | Loss: 0.00105221
Iteration 7/25 | Loss: 0.00105221
Iteration 8/25 | Loss: 0.00105221
Iteration 9/25 | Loss: 0.00105221
Iteration 10/25 | Loss: 0.00105221
Iteration 11/25 | Loss: 0.00105221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001052209408953786, 0.001052209408953786, 0.001052209408953786, 0.001052209408953786, 0.001052209408953786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001052209408953786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.44592381
Iteration 2/25 | Loss: 0.00065167
Iteration 3/25 | Loss: 0.00065166
Iteration 4/25 | Loss: 0.00065166
Iteration 5/25 | Loss: 0.00065166
Iteration 6/25 | Loss: 0.00065166
Iteration 7/25 | Loss: 0.00065166
Iteration 8/25 | Loss: 0.00065166
Iteration 9/25 | Loss: 0.00065166
Iteration 10/25 | Loss: 0.00065166
Iteration 11/25 | Loss: 0.00065166
Iteration 12/25 | Loss: 0.00065166
Iteration 13/25 | Loss: 0.00065166
Iteration 14/25 | Loss: 0.00065166
Iteration 15/25 | Loss: 0.00065166
Iteration 16/25 | Loss: 0.00065166
Iteration 17/25 | Loss: 0.00065166
Iteration 18/25 | Loss: 0.00065166
Iteration 19/25 | Loss: 0.00065166
Iteration 20/25 | Loss: 0.00065166
Iteration 21/25 | Loss: 0.00065166
Iteration 22/25 | Loss: 0.00065166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006516616558656096, 0.0006516616558656096, 0.0006516616558656096, 0.0006516616558656096, 0.0006516616558656096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006516616558656096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065166
Iteration 2/1000 | Loss: 0.00002158
Iteration 3/1000 | Loss: 0.00001471
Iteration 4/1000 | Loss: 0.00001349
Iteration 5/1000 | Loss: 0.00001287
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001202
Iteration 8/1000 | Loss: 0.00001177
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001108
Iteration 15/1000 | Loss: 0.00001105
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001096
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001089
Iteration 53/1000 | Loss: 0.00001089
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001083
Iteration 63/1000 | Loss: 0.00001083
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001079
Iteration 68/1000 | Loss: 0.00001079
Iteration 69/1000 | Loss: 0.00001078
Iteration 70/1000 | Loss: 0.00001078
Iteration 71/1000 | Loss: 0.00001077
Iteration 72/1000 | Loss: 0.00001077
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001076
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001072
Iteration 82/1000 | Loss: 0.00001072
Iteration 83/1000 | Loss: 0.00001072
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001069
Iteration 90/1000 | Loss: 0.00001069
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001066
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001065
Iteration 103/1000 | Loss: 0.00001065
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001064
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001061
Iteration 117/1000 | Loss: 0.00001061
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00001060
Iteration 120/1000 | Loss: 0.00001060
Iteration 121/1000 | Loss: 0.00001060
Iteration 122/1000 | Loss: 0.00001060
Iteration 123/1000 | Loss: 0.00001060
Iteration 124/1000 | Loss: 0.00001060
Iteration 125/1000 | Loss: 0.00001060
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001059
Iteration 128/1000 | Loss: 0.00001059
Iteration 129/1000 | Loss: 0.00001059
Iteration 130/1000 | Loss: 0.00001059
Iteration 131/1000 | Loss: 0.00001059
Iteration 132/1000 | Loss: 0.00001059
Iteration 133/1000 | Loss: 0.00001059
Iteration 134/1000 | Loss: 0.00001059
Iteration 135/1000 | Loss: 0.00001059
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001058
Iteration 138/1000 | Loss: 0.00001058
Iteration 139/1000 | Loss: 0.00001058
Iteration 140/1000 | Loss: 0.00001058
Iteration 141/1000 | Loss: 0.00001058
Iteration 142/1000 | Loss: 0.00001058
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001058
Iteration 145/1000 | Loss: 0.00001058
Iteration 146/1000 | Loss: 0.00001057
Iteration 147/1000 | Loss: 0.00001057
Iteration 148/1000 | Loss: 0.00001057
Iteration 149/1000 | Loss: 0.00001057
Iteration 150/1000 | Loss: 0.00001057
Iteration 151/1000 | Loss: 0.00001057
Iteration 152/1000 | Loss: 0.00001056
Iteration 153/1000 | Loss: 0.00001056
Iteration 154/1000 | Loss: 0.00001056
Iteration 155/1000 | Loss: 0.00001056
Iteration 156/1000 | Loss: 0.00001056
Iteration 157/1000 | Loss: 0.00001056
Iteration 158/1000 | Loss: 0.00001056
Iteration 159/1000 | Loss: 0.00001056
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001056
Iteration 164/1000 | Loss: 0.00001056
Iteration 165/1000 | Loss: 0.00001056
Iteration 166/1000 | Loss: 0.00001056
Iteration 167/1000 | Loss: 0.00001056
Iteration 168/1000 | Loss: 0.00001055
Iteration 169/1000 | Loss: 0.00001055
Iteration 170/1000 | Loss: 0.00001055
Iteration 171/1000 | Loss: 0.00001055
Iteration 172/1000 | Loss: 0.00001055
Iteration 173/1000 | Loss: 0.00001055
Iteration 174/1000 | Loss: 0.00001055
Iteration 175/1000 | Loss: 0.00001055
Iteration 176/1000 | Loss: 0.00001055
Iteration 177/1000 | Loss: 0.00001055
Iteration 178/1000 | Loss: 0.00001055
Iteration 179/1000 | Loss: 0.00001054
Iteration 180/1000 | Loss: 0.00001054
Iteration 181/1000 | Loss: 0.00001054
Iteration 182/1000 | Loss: 0.00001054
Iteration 183/1000 | Loss: 0.00001054
Iteration 184/1000 | Loss: 0.00001054
Iteration 185/1000 | Loss: 0.00001054
Iteration 186/1000 | Loss: 0.00001054
Iteration 187/1000 | Loss: 0.00001054
Iteration 188/1000 | Loss: 0.00001054
Iteration 189/1000 | Loss: 0.00001054
Iteration 190/1000 | Loss: 0.00001054
Iteration 191/1000 | Loss: 0.00001054
Iteration 192/1000 | Loss: 0.00001054
Iteration 193/1000 | Loss: 0.00001054
Iteration 194/1000 | Loss: 0.00001054
Iteration 195/1000 | Loss: 0.00001054
Iteration 196/1000 | Loss: 0.00001054
Iteration 197/1000 | Loss: 0.00001054
Iteration 198/1000 | Loss: 0.00001053
Iteration 199/1000 | Loss: 0.00001053
Iteration 200/1000 | Loss: 0.00001053
Iteration 201/1000 | Loss: 0.00001053
Iteration 202/1000 | Loss: 0.00001053
Iteration 203/1000 | Loss: 0.00001053
Iteration 204/1000 | Loss: 0.00001053
Iteration 205/1000 | Loss: 0.00001053
Iteration 206/1000 | Loss: 0.00001053
Iteration 207/1000 | Loss: 0.00001053
Iteration 208/1000 | Loss: 0.00001053
Iteration 209/1000 | Loss: 0.00001052
Iteration 210/1000 | Loss: 0.00001052
Iteration 211/1000 | Loss: 0.00001052
Iteration 212/1000 | Loss: 0.00001052
Iteration 213/1000 | Loss: 0.00001052
Iteration 214/1000 | Loss: 0.00001052
Iteration 215/1000 | Loss: 0.00001052
Iteration 216/1000 | Loss: 0.00001052
Iteration 217/1000 | Loss: 0.00001052
Iteration 218/1000 | Loss: 0.00001052
Iteration 219/1000 | Loss: 0.00001052
Iteration 220/1000 | Loss: 0.00001052
Iteration 221/1000 | Loss: 0.00001052
Iteration 222/1000 | Loss: 0.00001052
Iteration 223/1000 | Loss: 0.00001052
Iteration 224/1000 | Loss: 0.00001052
Iteration 225/1000 | Loss: 0.00001052
Iteration 226/1000 | Loss: 0.00001052
Iteration 227/1000 | Loss: 0.00001052
Iteration 228/1000 | Loss: 0.00001052
Iteration 229/1000 | Loss: 0.00001052
Iteration 230/1000 | Loss: 0.00001052
Iteration 231/1000 | Loss: 0.00001052
Iteration 232/1000 | Loss: 0.00001052
Iteration 233/1000 | Loss: 0.00001052
Iteration 234/1000 | Loss: 0.00001052
Iteration 235/1000 | Loss: 0.00001052
Iteration 236/1000 | Loss: 0.00001052
Iteration 237/1000 | Loss: 0.00001052
Iteration 238/1000 | Loss: 0.00001052
Iteration 239/1000 | Loss: 0.00001052
Iteration 240/1000 | Loss: 0.00001052
Iteration 241/1000 | Loss: 0.00001052
Iteration 242/1000 | Loss: 0.00001052
Iteration 243/1000 | Loss: 0.00001052
Iteration 244/1000 | Loss: 0.00001052
Iteration 245/1000 | Loss: 0.00001052
Iteration 246/1000 | Loss: 0.00001052
Iteration 247/1000 | Loss: 0.00001052
Iteration 248/1000 | Loss: 0.00001052
Iteration 249/1000 | Loss: 0.00001052
Iteration 250/1000 | Loss: 0.00001052
Iteration 251/1000 | Loss: 0.00001052
Iteration 252/1000 | Loss: 0.00001052
Iteration 253/1000 | Loss: 0.00001052
Iteration 254/1000 | Loss: 0.00001052
Iteration 255/1000 | Loss: 0.00001052
Iteration 256/1000 | Loss: 0.00001052
Iteration 257/1000 | Loss: 0.00001052
Iteration 258/1000 | Loss: 0.00001052
Iteration 259/1000 | Loss: 0.00001052
Iteration 260/1000 | Loss: 0.00001052
Iteration 261/1000 | Loss: 0.00001052
Iteration 262/1000 | Loss: 0.00001052
Iteration 263/1000 | Loss: 0.00001052
Iteration 264/1000 | Loss: 0.00001052
Iteration 265/1000 | Loss: 0.00001052
Iteration 266/1000 | Loss: 0.00001052
Iteration 267/1000 | Loss: 0.00001052
Iteration 268/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.0517560440348461e-05, 1.0517560440348461e-05, 1.0517560440348461e-05, 1.0517560440348461e-05, 1.0517560440348461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0517560440348461e-05

Optimization complete. Final v2v error: 2.7459545135498047 mm

Highest mean error: 3.435878276824951 mm for frame 93

Lowest mean error: 2.419545888900757 mm for frame 47

Saving results

Total time: 41.78245544433594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435830
Iteration 2/25 | Loss: 0.00135282
Iteration 3/25 | Loss: 0.00113287
Iteration 4/25 | Loss: 0.00109620
Iteration 5/25 | Loss: 0.00109157
Iteration 6/25 | Loss: 0.00109074
Iteration 7/25 | Loss: 0.00109074
Iteration 8/25 | Loss: 0.00109074
Iteration 9/25 | Loss: 0.00109074
Iteration 10/25 | Loss: 0.00109074
Iteration 11/25 | Loss: 0.00109074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010907364776358008, 0.0010907364776358008, 0.0010907364776358008, 0.0010907364776358008, 0.0010907364776358008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010907364776358008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37201834
Iteration 2/25 | Loss: 0.00062321
Iteration 3/25 | Loss: 0.00062320
Iteration 4/25 | Loss: 0.00062320
Iteration 5/25 | Loss: 0.00062320
Iteration 6/25 | Loss: 0.00062320
Iteration 7/25 | Loss: 0.00062320
Iteration 8/25 | Loss: 0.00062320
Iteration 9/25 | Loss: 0.00062320
Iteration 10/25 | Loss: 0.00062320
Iteration 11/25 | Loss: 0.00062320
Iteration 12/25 | Loss: 0.00062320
Iteration 13/25 | Loss: 0.00062320
Iteration 14/25 | Loss: 0.00062320
Iteration 15/25 | Loss: 0.00062320
Iteration 16/25 | Loss: 0.00062320
Iteration 17/25 | Loss: 0.00062320
Iteration 18/25 | Loss: 0.00062320
Iteration 19/25 | Loss: 0.00062320
Iteration 20/25 | Loss: 0.00062320
Iteration 21/25 | Loss: 0.00062320
Iteration 22/25 | Loss: 0.00062320
Iteration 23/25 | Loss: 0.00062320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006231997977010906, 0.0006231997977010906, 0.0006231997977010906, 0.0006231997977010906, 0.0006231997977010906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006231997977010906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062320
Iteration 2/1000 | Loss: 0.00003933
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001711
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001621
Iteration 9/1000 | Loss: 0.00001593
Iteration 10/1000 | Loss: 0.00001580
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001556
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001549
Iteration 18/1000 | Loss: 0.00001545
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001543
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001524
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001523
Iteration 38/1000 | Loss: 0.00001523
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001523
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001508
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001508
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001507
Iteration 91/1000 | Loss: 0.00001507
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001507
Iteration 94/1000 | Loss: 0.00001507
Iteration 95/1000 | Loss: 0.00001507
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001504
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001503
Iteration 113/1000 | Loss: 0.00001503
Iteration 114/1000 | Loss: 0.00001503
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001502
Iteration 123/1000 | Loss: 0.00001502
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001501
Iteration 132/1000 | Loss: 0.00001501
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001501
Iteration 139/1000 | Loss: 0.00001501
Iteration 140/1000 | Loss: 0.00001501
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001501
Iteration 143/1000 | Loss: 0.00001501
Iteration 144/1000 | Loss: 0.00001501
Iteration 145/1000 | Loss: 0.00001501
Iteration 146/1000 | Loss: 0.00001501
Iteration 147/1000 | Loss: 0.00001501
Iteration 148/1000 | Loss: 0.00001501
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001500
Iteration 151/1000 | Loss: 0.00001500
Iteration 152/1000 | Loss: 0.00001500
Iteration 153/1000 | Loss: 0.00001500
Iteration 154/1000 | Loss: 0.00001500
Iteration 155/1000 | Loss: 0.00001500
Iteration 156/1000 | Loss: 0.00001500
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001500
Iteration 163/1000 | Loss: 0.00001500
Iteration 164/1000 | Loss: 0.00001500
Iteration 165/1000 | Loss: 0.00001500
Iteration 166/1000 | Loss: 0.00001500
Iteration 167/1000 | Loss: 0.00001500
Iteration 168/1000 | Loss: 0.00001500
Iteration 169/1000 | Loss: 0.00001500
Iteration 170/1000 | Loss: 0.00001500
Iteration 171/1000 | Loss: 0.00001500
Iteration 172/1000 | Loss: 0.00001500
Iteration 173/1000 | Loss: 0.00001500
Iteration 174/1000 | Loss: 0.00001500
Iteration 175/1000 | Loss: 0.00001500
Iteration 176/1000 | Loss: 0.00001500
Iteration 177/1000 | Loss: 0.00001500
Iteration 178/1000 | Loss: 0.00001500
Iteration 179/1000 | Loss: 0.00001500
Iteration 180/1000 | Loss: 0.00001500
Iteration 181/1000 | Loss: 0.00001500
Iteration 182/1000 | Loss: 0.00001500
Iteration 183/1000 | Loss: 0.00001500
Iteration 184/1000 | Loss: 0.00001500
Iteration 185/1000 | Loss: 0.00001500
Iteration 186/1000 | Loss: 0.00001500
Iteration 187/1000 | Loss: 0.00001500
Iteration 188/1000 | Loss: 0.00001500
Iteration 189/1000 | Loss: 0.00001500
Iteration 190/1000 | Loss: 0.00001500
Iteration 191/1000 | Loss: 0.00001500
Iteration 192/1000 | Loss: 0.00001500
Iteration 193/1000 | Loss: 0.00001500
Iteration 194/1000 | Loss: 0.00001500
Iteration 195/1000 | Loss: 0.00001500
Iteration 196/1000 | Loss: 0.00001500
Iteration 197/1000 | Loss: 0.00001500
Iteration 198/1000 | Loss: 0.00001500
Iteration 199/1000 | Loss: 0.00001500
Iteration 200/1000 | Loss: 0.00001500
Iteration 201/1000 | Loss: 0.00001500
Iteration 202/1000 | Loss: 0.00001500
Iteration 203/1000 | Loss: 0.00001500
Iteration 204/1000 | Loss: 0.00001500
Iteration 205/1000 | Loss: 0.00001500
Iteration 206/1000 | Loss: 0.00001500
Iteration 207/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.5004314263933338e-05, 1.5004314263933338e-05, 1.5004314263933338e-05, 1.5004314263933338e-05, 1.5004314263933338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5004314263933338e-05

Optimization complete. Final v2v error: 3.2522778511047363 mm

Highest mean error: 4.4657487869262695 mm for frame 203

Lowest mean error: 2.777686357498169 mm for frame 97

Saving results

Total time: 43.99690580368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822422
Iteration 2/25 | Loss: 0.00151830
Iteration 3/25 | Loss: 0.00124584
Iteration 4/25 | Loss: 0.00122391
Iteration 5/25 | Loss: 0.00121970
Iteration 6/25 | Loss: 0.00121877
Iteration 7/25 | Loss: 0.00121877
Iteration 8/25 | Loss: 0.00121877
Iteration 9/25 | Loss: 0.00121877
Iteration 10/25 | Loss: 0.00121877
Iteration 11/25 | Loss: 0.00121877
Iteration 12/25 | Loss: 0.00121877
Iteration 13/25 | Loss: 0.00121877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012187707470729947, 0.0012187707470729947, 0.0012187707470729947, 0.0012187707470729947, 0.0012187707470729947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012187707470729947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29772186
Iteration 2/25 | Loss: 0.00058873
Iteration 3/25 | Loss: 0.00058867
Iteration 4/25 | Loss: 0.00058867
Iteration 5/25 | Loss: 0.00058867
Iteration 6/25 | Loss: 0.00058867
Iteration 7/25 | Loss: 0.00058867
Iteration 8/25 | Loss: 0.00058867
Iteration 9/25 | Loss: 0.00058867
Iteration 10/25 | Loss: 0.00058867
Iteration 11/25 | Loss: 0.00058867
Iteration 12/25 | Loss: 0.00058867
Iteration 13/25 | Loss: 0.00058867
Iteration 14/25 | Loss: 0.00058867
Iteration 15/25 | Loss: 0.00058867
Iteration 16/25 | Loss: 0.00058867
Iteration 17/25 | Loss: 0.00058867
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005886705475859344, 0.0005886705475859344, 0.0005886705475859344, 0.0005886705475859344, 0.0005886705475859344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005886705475859344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058867
Iteration 2/1000 | Loss: 0.00004257
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002421
Iteration 6/1000 | Loss: 0.00002308
Iteration 7/1000 | Loss: 0.00002256
Iteration 8/1000 | Loss: 0.00002195
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002119
Iteration 11/1000 | Loss: 0.00002082
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00002038
Iteration 15/1000 | Loss: 0.00002038
Iteration 16/1000 | Loss: 0.00002038
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002033
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002029
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002010
Iteration 28/1000 | Loss: 0.00002010
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00002007
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00002001
Iteration 36/1000 | Loss: 0.00002000
Iteration 37/1000 | Loss: 0.00001999
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001994
Iteration 42/1000 | Loss: 0.00001993
Iteration 43/1000 | Loss: 0.00001992
Iteration 44/1000 | Loss: 0.00001991
Iteration 45/1000 | Loss: 0.00001991
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001990
Iteration 48/1000 | Loss: 0.00001990
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001984
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001983
Iteration 75/1000 | Loss: 0.00001983
Iteration 76/1000 | Loss: 0.00001982
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001982
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001981
Iteration 82/1000 | Loss: 0.00001981
Iteration 83/1000 | Loss: 0.00001981
Iteration 84/1000 | Loss: 0.00001981
Iteration 85/1000 | Loss: 0.00001980
Iteration 86/1000 | Loss: 0.00001975
Iteration 87/1000 | Loss: 0.00001975
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001969
Iteration 97/1000 | Loss: 0.00001969
Iteration 98/1000 | Loss: 0.00001969
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001968
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001968
Iteration 104/1000 | Loss: 0.00001968
Iteration 105/1000 | Loss: 0.00001968
Iteration 106/1000 | Loss: 0.00001967
Iteration 107/1000 | Loss: 0.00001967
Iteration 108/1000 | Loss: 0.00001967
Iteration 109/1000 | Loss: 0.00001967
Iteration 110/1000 | Loss: 0.00001966
Iteration 111/1000 | Loss: 0.00001966
Iteration 112/1000 | Loss: 0.00001966
Iteration 113/1000 | Loss: 0.00001966
Iteration 114/1000 | Loss: 0.00001965
Iteration 115/1000 | Loss: 0.00001965
Iteration 116/1000 | Loss: 0.00001965
Iteration 117/1000 | Loss: 0.00001965
Iteration 118/1000 | Loss: 0.00001965
Iteration 119/1000 | Loss: 0.00001965
Iteration 120/1000 | Loss: 0.00001965
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00001965
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001964
Iteration 125/1000 | Loss: 0.00001964
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00001963
Iteration 131/1000 | Loss: 0.00001963
Iteration 132/1000 | Loss: 0.00001962
Iteration 133/1000 | Loss: 0.00001962
Iteration 134/1000 | Loss: 0.00001962
Iteration 135/1000 | Loss: 0.00001962
Iteration 136/1000 | Loss: 0.00001962
Iteration 137/1000 | Loss: 0.00001962
Iteration 138/1000 | Loss: 0.00001962
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001961
Iteration 141/1000 | Loss: 0.00001961
Iteration 142/1000 | Loss: 0.00001961
Iteration 143/1000 | Loss: 0.00001961
Iteration 144/1000 | Loss: 0.00001960
Iteration 145/1000 | Loss: 0.00001960
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001959
Iteration 148/1000 | Loss: 0.00001959
Iteration 149/1000 | Loss: 0.00001959
Iteration 150/1000 | Loss: 0.00001959
Iteration 151/1000 | Loss: 0.00001959
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001959
Iteration 155/1000 | Loss: 0.00001959
Iteration 156/1000 | Loss: 0.00001958
Iteration 157/1000 | Loss: 0.00001958
Iteration 158/1000 | Loss: 0.00001958
Iteration 159/1000 | Loss: 0.00001957
Iteration 160/1000 | Loss: 0.00001957
Iteration 161/1000 | Loss: 0.00001957
Iteration 162/1000 | Loss: 0.00001957
Iteration 163/1000 | Loss: 0.00001957
Iteration 164/1000 | Loss: 0.00001957
Iteration 165/1000 | Loss: 0.00001957
Iteration 166/1000 | Loss: 0.00001957
Iteration 167/1000 | Loss: 0.00001957
Iteration 168/1000 | Loss: 0.00001957
Iteration 169/1000 | Loss: 0.00001957
Iteration 170/1000 | Loss: 0.00001957
Iteration 171/1000 | Loss: 0.00001956
Iteration 172/1000 | Loss: 0.00001956
Iteration 173/1000 | Loss: 0.00001956
Iteration 174/1000 | Loss: 0.00001956
Iteration 175/1000 | Loss: 0.00001956
Iteration 176/1000 | Loss: 0.00001956
Iteration 177/1000 | Loss: 0.00001956
Iteration 178/1000 | Loss: 0.00001956
Iteration 179/1000 | Loss: 0.00001956
Iteration 180/1000 | Loss: 0.00001956
Iteration 181/1000 | Loss: 0.00001956
Iteration 182/1000 | Loss: 0.00001956
Iteration 183/1000 | Loss: 0.00001956
Iteration 184/1000 | Loss: 0.00001956
Iteration 185/1000 | Loss: 0.00001956
Iteration 186/1000 | Loss: 0.00001956
Iteration 187/1000 | Loss: 0.00001956
Iteration 188/1000 | Loss: 0.00001956
Iteration 189/1000 | Loss: 0.00001956
Iteration 190/1000 | Loss: 0.00001956
Iteration 191/1000 | Loss: 0.00001956
Iteration 192/1000 | Loss: 0.00001956
Iteration 193/1000 | Loss: 0.00001956
Iteration 194/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.955746665771585e-05, 1.955746665771585e-05, 1.955746665771585e-05, 1.955746665771585e-05, 1.955746665771585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.955746665771585e-05

Optimization complete. Final v2v error: 3.790821075439453 mm

Highest mean error: 4.136295795440674 mm for frame 137

Lowest mean error: 3.3816919326782227 mm for frame 0

Saving results

Total time: 42.976367712020874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698592
Iteration 2/25 | Loss: 0.00137850
Iteration 3/25 | Loss: 0.00117612
Iteration 4/25 | Loss: 0.00115858
Iteration 5/25 | Loss: 0.00115649
Iteration 6/25 | Loss: 0.00115649
Iteration 7/25 | Loss: 0.00115649
Iteration 8/25 | Loss: 0.00115649
Iteration 9/25 | Loss: 0.00115649
Iteration 10/25 | Loss: 0.00115649
Iteration 11/25 | Loss: 0.00115649
Iteration 12/25 | Loss: 0.00115649
Iteration 13/25 | Loss: 0.00115649
Iteration 14/25 | Loss: 0.00115649
Iteration 15/25 | Loss: 0.00115649
Iteration 16/25 | Loss: 0.00115649
Iteration 17/25 | Loss: 0.00115649
Iteration 18/25 | Loss: 0.00115649
Iteration 19/25 | Loss: 0.00115649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011564928572624922, 0.0011564928572624922, 0.0011564928572624922, 0.0011564928572624922, 0.0011564928572624922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011564928572624922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34934235
Iteration 2/25 | Loss: 0.00063727
Iteration 3/25 | Loss: 0.00063724
Iteration 4/25 | Loss: 0.00063724
Iteration 5/25 | Loss: 0.00063724
Iteration 6/25 | Loss: 0.00063724
Iteration 7/25 | Loss: 0.00063724
Iteration 8/25 | Loss: 0.00063724
Iteration 9/25 | Loss: 0.00063724
Iteration 10/25 | Loss: 0.00063724
Iteration 11/25 | Loss: 0.00063724
Iteration 12/25 | Loss: 0.00063724
Iteration 13/25 | Loss: 0.00063724
Iteration 14/25 | Loss: 0.00063724
Iteration 15/25 | Loss: 0.00063724
Iteration 16/25 | Loss: 0.00063724
Iteration 17/25 | Loss: 0.00063724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006372400093823671, 0.0006372400093823671, 0.0006372400093823671, 0.0006372400093823671, 0.0006372400093823671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006372400093823671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063724
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00002045
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00001783
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001697
Iteration 8/1000 | Loss: 0.00001669
Iteration 9/1000 | Loss: 0.00001647
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00001621
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001612
Iteration 18/1000 | Loss: 0.00001611
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001583
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001549
Iteration 24/1000 | Loss: 0.00001547
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001542
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001534
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00001525
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001521
Iteration 73/1000 | Loss: 0.00001521
Iteration 74/1000 | Loss: 0.00001521
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001520
Iteration 78/1000 | Loss: 0.00001520
Iteration 79/1000 | Loss: 0.00001520
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001519
Iteration 82/1000 | Loss: 0.00001519
Iteration 83/1000 | Loss: 0.00001519
Iteration 84/1000 | Loss: 0.00001519
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001518
Iteration 90/1000 | Loss: 0.00001518
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001517
Iteration 96/1000 | Loss: 0.00001517
Iteration 97/1000 | Loss: 0.00001517
Iteration 98/1000 | Loss: 0.00001517
Iteration 99/1000 | Loss: 0.00001517
Iteration 100/1000 | Loss: 0.00001516
Iteration 101/1000 | Loss: 0.00001516
Iteration 102/1000 | Loss: 0.00001516
Iteration 103/1000 | Loss: 0.00001516
Iteration 104/1000 | Loss: 0.00001516
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001516
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001516
Iteration 109/1000 | Loss: 0.00001516
Iteration 110/1000 | Loss: 0.00001516
Iteration 111/1000 | Loss: 0.00001516
Iteration 112/1000 | Loss: 0.00001515
Iteration 113/1000 | Loss: 0.00001515
Iteration 114/1000 | Loss: 0.00001515
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001515
Iteration 117/1000 | Loss: 0.00001515
Iteration 118/1000 | Loss: 0.00001515
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.5153479580476414e-05, 1.5153479580476414e-05, 1.5153479580476414e-05, 1.5153479580476414e-05, 1.5153479580476414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5153479580476414e-05

Optimization complete. Final v2v error: 3.30503249168396 mm

Highest mean error: 3.4969677925109863 mm for frame 91

Lowest mean error: 3.105337142944336 mm for frame 194

Saving results

Total time: 41.21503949165344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988999
Iteration 2/25 | Loss: 0.00988999
Iteration 3/25 | Loss: 0.00262050
Iteration 4/25 | Loss: 0.00180902
Iteration 5/25 | Loss: 0.00165891
Iteration 6/25 | Loss: 0.00198934
Iteration 7/25 | Loss: 0.00149266
Iteration 8/25 | Loss: 0.00133981
Iteration 9/25 | Loss: 0.00131538
Iteration 10/25 | Loss: 0.00130822
Iteration 11/25 | Loss: 0.00129968
Iteration 12/25 | Loss: 0.00128511
Iteration 13/25 | Loss: 0.00128353
Iteration 14/25 | Loss: 0.00128364
Iteration 15/25 | Loss: 0.00128182
Iteration 16/25 | Loss: 0.00128134
Iteration 17/25 | Loss: 0.00130335
Iteration 18/25 | Loss: 0.00129416
Iteration 19/25 | Loss: 0.00127611
Iteration 20/25 | Loss: 0.00127265
Iteration 21/25 | Loss: 0.00127104
Iteration 22/25 | Loss: 0.00127037
Iteration 23/25 | Loss: 0.00126990
Iteration 24/25 | Loss: 0.00127070
Iteration 25/25 | Loss: 0.00126993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33114886
Iteration 2/25 | Loss: 0.00086104
Iteration 3/25 | Loss: 0.00086104
Iteration 4/25 | Loss: 0.00086104
Iteration 5/25 | Loss: 0.00086104
Iteration 6/25 | Loss: 0.00086104
Iteration 7/25 | Loss: 0.00086104
Iteration 8/25 | Loss: 0.00086104
Iteration 9/25 | Loss: 0.00086103
Iteration 10/25 | Loss: 0.00086103
Iteration 11/25 | Loss: 0.00086103
Iteration 12/25 | Loss: 0.00086103
Iteration 13/25 | Loss: 0.00086103
Iteration 14/25 | Loss: 0.00086103
Iteration 15/25 | Loss: 0.00086103
Iteration 16/25 | Loss: 0.00086103
Iteration 17/25 | Loss: 0.00086103
Iteration 18/25 | Loss: 0.00086103
Iteration 19/25 | Loss: 0.00086103
Iteration 20/25 | Loss: 0.00086103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008610348450019956, 0.0008610348450019956, 0.0008610348450019956, 0.0008610348450019956, 0.0008610348450019956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008610348450019956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086103
Iteration 2/1000 | Loss: 0.00010017
Iteration 3/1000 | Loss: 0.00006948
Iteration 4/1000 | Loss: 0.00007193
Iteration 5/1000 | Loss: 0.00005759
Iteration 6/1000 | Loss: 0.00006991
Iteration 7/1000 | Loss: 0.00006545
Iteration 8/1000 | Loss: 0.00006718
Iteration 9/1000 | Loss: 0.00004976
Iteration 10/1000 | Loss: 0.00005501
Iteration 11/1000 | Loss: 0.00005374
Iteration 12/1000 | Loss: 0.00005433
Iteration 13/1000 | Loss: 0.00005817
Iteration 14/1000 | Loss: 0.00005870
Iteration 15/1000 | Loss: 0.00005866
Iteration 16/1000 | Loss: 0.00005699
Iteration 17/1000 | Loss: 0.00005635
Iteration 18/1000 | Loss: 0.00005625
Iteration 19/1000 | Loss: 0.00005639
Iteration 20/1000 | Loss: 0.00005875
Iteration 21/1000 | Loss: 0.00005530
Iteration 22/1000 | Loss: 0.00005612
Iteration 23/1000 | Loss: 0.00005424
Iteration 24/1000 | Loss: 0.00005723
Iteration 25/1000 | Loss: 0.00005175
Iteration 26/1000 | Loss: 0.00005303
Iteration 27/1000 | Loss: 0.00005556
Iteration 28/1000 | Loss: 0.00015272
Iteration 29/1000 | Loss: 0.00059957
Iteration 30/1000 | Loss: 0.00111631
Iteration 31/1000 | Loss: 0.00106738
Iteration 32/1000 | Loss: 0.00105935
Iteration 33/1000 | Loss: 0.00091669
Iteration 34/1000 | Loss: 0.00098749
Iteration 35/1000 | Loss: 0.00043661
Iteration 36/1000 | Loss: 0.00020591
Iteration 37/1000 | Loss: 0.00029103
Iteration 38/1000 | Loss: 0.00006484
Iteration 39/1000 | Loss: 0.00005662
Iteration 40/1000 | Loss: 0.00004666
Iteration 41/1000 | Loss: 0.00003848
Iteration 42/1000 | Loss: 0.00003433
Iteration 43/1000 | Loss: 0.00003109
Iteration 44/1000 | Loss: 0.00002859
Iteration 45/1000 | Loss: 0.00002700
Iteration 46/1000 | Loss: 0.00002584
Iteration 47/1000 | Loss: 0.00002466
Iteration 48/1000 | Loss: 0.00002392
Iteration 49/1000 | Loss: 0.00002322
Iteration 50/1000 | Loss: 0.00002287
Iteration 51/1000 | Loss: 0.00002265
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002246
Iteration 54/1000 | Loss: 0.00002243
Iteration 55/1000 | Loss: 0.00002242
Iteration 56/1000 | Loss: 0.00002239
Iteration 57/1000 | Loss: 0.00002236
Iteration 58/1000 | Loss: 0.00002235
Iteration 59/1000 | Loss: 0.00002235
Iteration 60/1000 | Loss: 0.00002234
Iteration 61/1000 | Loss: 0.00002233
Iteration 62/1000 | Loss: 0.00002232
Iteration 63/1000 | Loss: 0.00002231
Iteration 64/1000 | Loss: 0.00002230
Iteration 65/1000 | Loss: 0.00002230
Iteration 66/1000 | Loss: 0.00002230
Iteration 67/1000 | Loss: 0.00002230
Iteration 68/1000 | Loss: 0.00002230
Iteration 69/1000 | Loss: 0.00002230
Iteration 70/1000 | Loss: 0.00002229
Iteration 71/1000 | Loss: 0.00002229
Iteration 72/1000 | Loss: 0.00002229
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002227
Iteration 76/1000 | Loss: 0.00002226
Iteration 77/1000 | Loss: 0.00002226
Iteration 78/1000 | Loss: 0.00002226
Iteration 79/1000 | Loss: 0.00002225
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00002225
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002224
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002222
Iteration 89/1000 | Loss: 0.00002222
Iteration 90/1000 | Loss: 0.00002222
Iteration 91/1000 | Loss: 0.00002221
Iteration 92/1000 | Loss: 0.00002221
Iteration 93/1000 | Loss: 0.00002221
Iteration 94/1000 | Loss: 0.00002221
Iteration 95/1000 | Loss: 0.00002221
Iteration 96/1000 | Loss: 0.00002221
Iteration 97/1000 | Loss: 0.00002220
Iteration 98/1000 | Loss: 0.00002219
Iteration 99/1000 | Loss: 0.00002219
Iteration 100/1000 | Loss: 0.00002219
Iteration 101/1000 | Loss: 0.00002219
Iteration 102/1000 | Loss: 0.00002219
Iteration 103/1000 | Loss: 0.00002218
Iteration 104/1000 | Loss: 0.00002218
Iteration 105/1000 | Loss: 0.00002218
Iteration 106/1000 | Loss: 0.00002218
Iteration 107/1000 | Loss: 0.00002218
Iteration 108/1000 | Loss: 0.00002218
Iteration 109/1000 | Loss: 0.00002218
Iteration 110/1000 | Loss: 0.00002218
Iteration 111/1000 | Loss: 0.00002218
Iteration 112/1000 | Loss: 0.00002217
Iteration 113/1000 | Loss: 0.00002217
Iteration 114/1000 | Loss: 0.00002217
Iteration 115/1000 | Loss: 0.00002217
Iteration 116/1000 | Loss: 0.00002217
Iteration 117/1000 | Loss: 0.00002216
Iteration 118/1000 | Loss: 0.00002216
Iteration 119/1000 | Loss: 0.00002216
Iteration 120/1000 | Loss: 0.00002216
Iteration 121/1000 | Loss: 0.00002216
Iteration 122/1000 | Loss: 0.00002216
Iteration 123/1000 | Loss: 0.00002216
Iteration 124/1000 | Loss: 0.00002216
Iteration 125/1000 | Loss: 0.00002216
Iteration 126/1000 | Loss: 0.00002216
Iteration 127/1000 | Loss: 0.00002216
Iteration 128/1000 | Loss: 0.00002215
Iteration 129/1000 | Loss: 0.00002215
Iteration 130/1000 | Loss: 0.00002215
Iteration 131/1000 | Loss: 0.00002215
Iteration 132/1000 | Loss: 0.00002215
Iteration 133/1000 | Loss: 0.00002215
Iteration 134/1000 | Loss: 0.00002215
Iteration 135/1000 | Loss: 0.00002215
Iteration 136/1000 | Loss: 0.00002214
Iteration 137/1000 | Loss: 0.00002214
Iteration 138/1000 | Loss: 0.00002214
Iteration 139/1000 | Loss: 0.00002214
Iteration 140/1000 | Loss: 0.00002214
Iteration 141/1000 | Loss: 0.00002214
Iteration 142/1000 | Loss: 0.00002214
Iteration 143/1000 | Loss: 0.00002214
Iteration 144/1000 | Loss: 0.00002214
Iteration 145/1000 | Loss: 0.00002214
Iteration 146/1000 | Loss: 0.00002214
Iteration 147/1000 | Loss: 0.00002214
Iteration 148/1000 | Loss: 0.00002214
Iteration 149/1000 | Loss: 0.00002214
Iteration 150/1000 | Loss: 0.00002213
Iteration 151/1000 | Loss: 0.00002213
Iteration 152/1000 | Loss: 0.00002213
Iteration 153/1000 | Loss: 0.00002213
Iteration 154/1000 | Loss: 0.00002213
Iteration 155/1000 | Loss: 0.00002213
Iteration 156/1000 | Loss: 0.00002213
Iteration 157/1000 | Loss: 0.00002213
Iteration 158/1000 | Loss: 0.00002213
Iteration 159/1000 | Loss: 0.00002213
Iteration 160/1000 | Loss: 0.00002213
Iteration 161/1000 | Loss: 0.00002212
Iteration 162/1000 | Loss: 0.00002212
Iteration 163/1000 | Loss: 0.00002212
Iteration 164/1000 | Loss: 0.00002212
Iteration 165/1000 | Loss: 0.00002212
Iteration 166/1000 | Loss: 0.00002212
Iteration 167/1000 | Loss: 0.00002212
Iteration 168/1000 | Loss: 0.00002212
Iteration 169/1000 | Loss: 0.00002212
Iteration 170/1000 | Loss: 0.00002211
Iteration 171/1000 | Loss: 0.00002211
Iteration 172/1000 | Loss: 0.00002211
Iteration 173/1000 | Loss: 0.00002211
Iteration 174/1000 | Loss: 0.00002211
Iteration 175/1000 | Loss: 0.00002211
Iteration 176/1000 | Loss: 0.00002211
Iteration 177/1000 | Loss: 0.00002211
Iteration 178/1000 | Loss: 0.00002211
Iteration 179/1000 | Loss: 0.00002211
Iteration 180/1000 | Loss: 0.00002211
Iteration 181/1000 | Loss: 0.00002211
Iteration 182/1000 | Loss: 0.00002211
Iteration 183/1000 | Loss: 0.00002211
Iteration 184/1000 | Loss: 0.00002211
Iteration 185/1000 | Loss: 0.00002211
Iteration 186/1000 | Loss: 0.00002211
Iteration 187/1000 | Loss: 0.00002211
Iteration 188/1000 | Loss: 0.00002211
Iteration 189/1000 | Loss: 0.00002211
Iteration 190/1000 | Loss: 0.00002211
Iteration 191/1000 | Loss: 0.00002211
Iteration 192/1000 | Loss: 0.00002211
Iteration 193/1000 | Loss: 0.00002211
Iteration 194/1000 | Loss: 0.00002211
Iteration 195/1000 | Loss: 0.00002211
Iteration 196/1000 | Loss: 0.00002211
Iteration 197/1000 | Loss: 0.00002211
Iteration 198/1000 | Loss: 0.00002211
Iteration 199/1000 | Loss: 0.00002211
Iteration 200/1000 | Loss: 0.00002211
Iteration 201/1000 | Loss: 0.00002211
Iteration 202/1000 | Loss: 0.00002211
Iteration 203/1000 | Loss: 0.00002211
Iteration 204/1000 | Loss: 0.00002211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.2113967133918777e-05, 2.2113967133918777e-05, 2.2113967133918777e-05, 2.2113967133918777e-05, 2.2113967133918777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2113967133918777e-05

Optimization complete. Final v2v error: 3.8829896450042725 mm

Highest mean error: 6.435864448547363 mm for frame 22

Lowest mean error: 3.3416342735290527 mm for frame 32

Saving results

Total time: 143.8386631011963
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375927
Iteration 2/25 | Loss: 0.00149206
Iteration 3/25 | Loss: 0.00112806
Iteration 4/25 | Loss: 0.00107484
Iteration 5/25 | Loss: 0.00106479
Iteration 6/25 | Loss: 0.00106274
Iteration 7/25 | Loss: 0.00106220
Iteration 8/25 | Loss: 0.00106220
Iteration 9/25 | Loss: 0.00106220
Iteration 10/25 | Loss: 0.00106220
Iteration 11/25 | Loss: 0.00106220
Iteration 12/25 | Loss: 0.00106220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010621997062116861, 0.0010621997062116861, 0.0010621997062116861, 0.0010621997062116861, 0.0010621997062116861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010621997062116861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34370482
Iteration 2/25 | Loss: 0.00047111
Iteration 3/25 | Loss: 0.00047111
Iteration 4/25 | Loss: 0.00047111
Iteration 5/25 | Loss: 0.00047111
Iteration 6/25 | Loss: 0.00047111
Iteration 7/25 | Loss: 0.00047111
Iteration 8/25 | Loss: 0.00047111
Iteration 9/25 | Loss: 0.00047111
Iteration 10/25 | Loss: 0.00047111
Iteration 11/25 | Loss: 0.00047111
Iteration 12/25 | Loss: 0.00047111
Iteration 13/25 | Loss: 0.00047111
Iteration 14/25 | Loss: 0.00047111
Iteration 15/25 | Loss: 0.00047111
Iteration 16/25 | Loss: 0.00047111
Iteration 17/25 | Loss: 0.00047111
Iteration 18/25 | Loss: 0.00047111
Iteration 19/25 | Loss: 0.00047111
Iteration 20/25 | Loss: 0.00047111
Iteration 21/25 | Loss: 0.00047111
Iteration 22/25 | Loss: 0.00047111
Iteration 23/25 | Loss: 0.00047111
Iteration 24/25 | Loss: 0.00047111
Iteration 25/25 | Loss: 0.00047111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047111
Iteration 2/1000 | Loss: 0.00004893
Iteration 3/1000 | Loss: 0.00002360
Iteration 4/1000 | Loss: 0.00001813
Iteration 5/1000 | Loss: 0.00001571
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001197
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001191
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001190
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001184
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001181
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.179149876406882e-05, 1.179149876406882e-05, 1.179149876406882e-05, 1.179149876406882e-05, 1.179149876406882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.179149876406882e-05

Optimization complete. Final v2v error: 2.90954852104187 mm

Highest mean error: 3.124567985534668 mm for frame 103

Lowest mean error: 2.733290433883667 mm for frame 25

Saving results

Total time: 34.731515884399414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399278
Iteration 2/25 | Loss: 0.00123434
Iteration 3/25 | Loss: 0.00107170
Iteration 4/25 | Loss: 0.00105805
Iteration 5/25 | Loss: 0.00105608
Iteration 6/25 | Loss: 0.00105560
Iteration 7/25 | Loss: 0.00105560
Iteration 8/25 | Loss: 0.00105560
Iteration 9/25 | Loss: 0.00105560
Iteration 10/25 | Loss: 0.00105560
Iteration 11/25 | Loss: 0.00105560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010556037304922938, 0.0010556037304922938, 0.0010556037304922938, 0.0010556037304922938, 0.0010556037304922938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010556037304922938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37012076
Iteration 2/25 | Loss: 0.00051660
Iteration 3/25 | Loss: 0.00051659
Iteration 4/25 | Loss: 0.00051659
Iteration 5/25 | Loss: 0.00051659
Iteration 6/25 | Loss: 0.00051659
Iteration 7/25 | Loss: 0.00051659
Iteration 8/25 | Loss: 0.00051659
Iteration 9/25 | Loss: 0.00051659
Iteration 10/25 | Loss: 0.00051659
Iteration 11/25 | Loss: 0.00051659
Iteration 12/25 | Loss: 0.00051659
Iteration 13/25 | Loss: 0.00051659
Iteration 14/25 | Loss: 0.00051659
Iteration 15/25 | Loss: 0.00051659
Iteration 16/25 | Loss: 0.00051659
Iteration 17/25 | Loss: 0.00051659
Iteration 18/25 | Loss: 0.00051659
Iteration 19/25 | Loss: 0.00051659
Iteration 20/25 | Loss: 0.00051659
Iteration 21/25 | Loss: 0.00051659
Iteration 22/25 | Loss: 0.00051659
Iteration 23/25 | Loss: 0.00051659
Iteration 24/25 | Loss: 0.00051659
Iteration 25/25 | Loss: 0.00051659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051659
Iteration 2/1000 | Loss: 0.00002234
Iteration 3/1000 | Loss: 0.00001550
Iteration 4/1000 | Loss: 0.00001404
Iteration 5/1000 | Loss: 0.00001314
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001220
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001131
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001130
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001111
Iteration 26/1000 | Loss: 0.00001111
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001108
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001105
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001104
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001093
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001091
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001091
Iteration 73/1000 | Loss: 0.00001090
Iteration 74/1000 | Loss: 0.00001090
Iteration 75/1000 | Loss: 0.00001090
Iteration 76/1000 | Loss: 0.00001089
Iteration 77/1000 | Loss: 0.00001089
Iteration 78/1000 | Loss: 0.00001089
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001087
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001086
Iteration 84/1000 | Loss: 0.00001086
Iteration 85/1000 | Loss: 0.00001086
Iteration 86/1000 | Loss: 0.00001086
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001086
Iteration 89/1000 | Loss: 0.00001086
Iteration 90/1000 | Loss: 0.00001086
Iteration 91/1000 | Loss: 0.00001086
Iteration 92/1000 | Loss: 0.00001084
Iteration 93/1000 | Loss: 0.00001084
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001082
Iteration 102/1000 | Loss: 0.00001082
Iteration 103/1000 | Loss: 0.00001082
Iteration 104/1000 | Loss: 0.00001082
Iteration 105/1000 | Loss: 0.00001081
Iteration 106/1000 | Loss: 0.00001081
Iteration 107/1000 | Loss: 0.00001081
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001079
Iteration 110/1000 | Loss: 0.00001079
Iteration 111/1000 | Loss: 0.00001079
Iteration 112/1000 | Loss: 0.00001079
Iteration 113/1000 | Loss: 0.00001079
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001078
Iteration 116/1000 | Loss: 0.00001078
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001078
Iteration 119/1000 | Loss: 0.00001078
Iteration 120/1000 | Loss: 0.00001078
Iteration 121/1000 | Loss: 0.00001078
Iteration 122/1000 | Loss: 0.00001078
Iteration 123/1000 | Loss: 0.00001077
Iteration 124/1000 | Loss: 0.00001077
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001073
Iteration 148/1000 | Loss: 0.00001073
Iteration 149/1000 | Loss: 0.00001073
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001072
Iteration 154/1000 | Loss: 0.00001072
Iteration 155/1000 | Loss: 0.00001072
Iteration 156/1000 | Loss: 0.00001072
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001071
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001071
Iteration 168/1000 | Loss: 0.00001071
Iteration 169/1000 | Loss: 0.00001071
Iteration 170/1000 | Loss: 0.00001071
Iteration 171/1000 | Loss: 0.00001071
Iteration 172/1000 | Loss: 0.00001071
Iteration 173/1000 | Loss: 0.00001071
Iteration 174/1000 | Loss: 0.00001071
Iteration 175/1000 | Loss: 0.00001071
Iteration 176/1000 | Loss: 0.00001071
Iteration 177/1000 | Loss: 0.00001071
Iteration 178/1000 | Loss: 0.00001071
Iteration 179/1000 | Loss: 0.00001071
Iteration 180/1000 | Loss: 0.00001071
Iteration 181/1000 | Loss: 0.00001071
Iteration 182/1000 | Loss: 0.00001071
Iteration 183/1000 | Loss: 0.00001071
Iteration 184/1000 | Loss: 0.00001071
Iteration 185/1000 | Loss: 0.00001071
Iteration 186/1000 | Loss: 0.00001071
Iteration 187/1000 | Loss: 0.00001071
Iteration 188/1000 | Loss: 0.00001071
Iteration 189/1000 | Loss: 0.00001071
Iteration 190/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.0706753528211266e-05, 1.0706753528211266e-05, 1.0706753528211266e-05, 1.0706753528211266e-05, 1.0706753528211266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0706753528211266e-05

Optimization complete. Final v2v error: 2.8104710578918457 mm

Highest mean error: 2.9047482013702393 mm for frame 26

Lowest mean error: 2.718745708465576 mm for frame 130

Saving results

Total time: 38.136377573013306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389468
Iteration 2/25 | Loss: 0.00112436
Iteration 3/25 | Loss: 0.00105690
Iteration 4/25 | Loss: 0.00105085
Iteration 5/25 | Loss: 0.00104786
Iteration 6/25 | Loss: 0.00104740
Iteration 7/25 | Loss: 0.00104740
Iteration 8/25 | Loss: 0.00104740
Iteration 9/25 | Loss: 0.00104740
Iteration 10/25 | Loss: 0.00104740
Iteration 11/25 | Loss: 0.00104740
Iteration 12/25 | Loss: 0.00104740
Iteration 13/25 | Loss: 0.00104740
Iteration 14/25 | Loss: 0.00104740
Iteration 15/25 | Loss: 0.00104740
Iteration 16/25 | Loss: 0.00104740
Iteration 17/25 | Loss: 0.00104740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00104740250390023, 0.00104740250390023, 0.00104740250390023, 0.00104740250390023, 0.00104740250390023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00104740250390023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62374198
Iteration 2/25 | Loss: 0.00056037
Iteration 3/25 | Loss: 0.00056037
Iteration 4/25 | Loss: 0.00056037
Iteration 5/25 | Loss: 0.00056037
Iteration 6/25 | Loss: 0.00056037
Iteration 7/25 | Loss: 0.00056037
Iteration 8/25 | Loss: 0.00056037
Iteration 9/25 | Loss: 0.00056037
Iteration 10/25 | Loss: 0.00056037
Iteration 11/25 | Loss: 0.00056037
Iteration 12/25 | Loss: 0.00056037
Iteration 13/25 | Loss: 0.00056037
Iteration 14/25 | Loss: 0.00056037
Iteration 15/25 | Loss: 0.00056037
Iteration 16/25 | Loss: 0.00056037
Iteration 17/25 | Loss: 0.00056037
Iteration 18/25 | Loss: 0.00056037
Iteration 19/25 | Loss: 0.00056037
Iteration 20/25 | Loss: 0.00056037
Iteration 21/25 | Loss: 0.00056037
Iteration 22/25 | Loss: 0.00056037
Iteration 23/25 | Loss: 0.00056037
Iteration 24/25 | Loss: 0.00056037
Iteration 25/25 | Loss: 0.00056037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056037
Iteration 2/1000 | Loss: 0.00002624
Iteration 3/1000 | Loss: 0.00001643
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001174
Iteration 6/1000 | Loss: 0.00001110
Iteration 7/1000 | Loss: 0.00001062
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001016
Iteration 10/1000 | Loss: 0.00000990
Iteration 11/1000 | Loss: 0.00000969
Iteration 12/1000 | Loss: 0.00000966
Iteration 13/1000 | Loss: 0.00000965
Iteration 14/1000 | Loss: 0.00000963
Iteration 15/1000 | Loss: 0.00000963
Iteration 16/1000 | Loss: 0.00000962
Iteration 17/1000 | Loss: 0.00000961
Iteration 18/1000 | Loss: 0.00000959
Iteration 19/1000 | Loss: 0.00000954
Iteration 20/1000 | Loss: 0.00000949
Iteration 21/1000 | Loss: 0.00000947
Iteration 22/1000 | Loss: 0.00000945
Iteration 23/1000 | Loss: 0.00000942
Iteration 24/1000 | Loss: 0.00000942
Iteration 25/1000 | Loss: 0.00000940
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000938
Iteration 29/1000 | Loss: 0.00000937
Iteration 30/1000 | Loss: 0.00000933
Iteration 31/1000 | Loss: 0.00000933
Iteration 32/1000 | Loss: 0.00000933
Iteration 33/1000 | Loss: 0.00000932
Iteration 34/1000 | Loss: 0.00000932
Iteration 35/1000 | Loss: 0.00000931
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000930
Iteration 38/1000 | Loss: 0.00000930
Iteration 39/1000 | Loss: 0.00000930
Iteration 40/1000 | Loss: 0.00000929
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000928
Iteration 44/1000 | Loss: 0.00000928
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000927
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000927
Iteration 51/1000 | Loss: 0.00000927
Iteration 52/1000 | Loss: 0.00000927
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000926
Iteration 56/1000 | Loss: 0.00000925
Iteration 57/1000 | Loss: 0.00000924
Iteration 58/1000 | Loss: 0.00000924
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000923
Iteration 61/1000 | Loss: 0.00000923
Iteration 62/1000 | Loss: 0.00000923
Iteration 63/1000 | Loss: 0.00000923
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000922
Iteration 66/1000 | Loss: 0.00000922
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000922
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000922
Iteration 74/1000 | Loss: 0.00000922
Iteration 75/1000 | Loss: 0.00000922
Iteration 76/1000 | Loss: 0.00000921
Iteration 77/1000 | Loss: 0.00000921
Iteration 78/1000 | Loss: 0.00000920
Iteration 79/1000 | Loss: 0.00000920
Iteration 80/1000 | Loss: 0.00000920
Iteration 81/1000 | Loss: 0.00000920
Iteration 82/1000 | Loss: 0.00000919
Iteration 83/1000 | Loss: 0.00000918
Iteration 84/1000 | Loss: 0.00000918
Iteration 85/1000 | Loss: 0.00000918
Iteration 86/1000 | Loss: 0.00000918
Iteration 87/1000 | Loss: 0.00000918
Iteration 88/1000 | Loss: 0.00000918
Iteration 89/1000 | Loss: 0.00000918
Iteration 90/1000 | Loss: 0.00000918
Iteration 91/1000 | Loss: 0.00000918
Iteration 92/1000 | Loss: 0.00000918
Iteration 93/1000 | Loss: 0.00000918
Iteration 94/1000 | Loss: 0.00000918
Iteration 95/1000 | Loss: 0.00000918
Iteration 96/1000 | Loss: 0.00000918
Iteration 97/1000 | Loss: 0.00000917
Iteration 98/1000 | Loss: 0.00000917
Iteration 99/1000 | Loss: 0.00000917
Iteration 100/1000 | Loss: 0.00000917
Iteration 101/1000 | Loss: 0.00000917
Iteration 102/1000 | Loss: 0.00000917
Iteration 103/1000 | Loss: 0.00000916
Iteration 104/1000 | Loss: 0.00000916
Iteration 105/1000 | Loss: 0.00000916
Iteration 106/1000 | Loss: 0.00000916
Iteration 107/1000 | Loss: 0.00000916
Iteration 108/1000 | Loss: 0.00000916
Iteration 109/1000 | Loss: 0.00000916
Iteration 110/1000 | Loss: 0.00000916
Iteration 111/1000 | Loss: 0.00000916
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000915
Iteration 114/1000 | Loss: 0.00000915
Iteration 115/1000 | Loss: 0.00000915
Iteration 116/1000 | Loss: 0.00000915
Iteration 117/1000 | Loss: 0.00000915
Iteration 118/1000 | Loss: 0.00000915
Iteration 119/1000 | Loss: 0.00000915
Iteration 120/1000 | Loss: 0.00000915
Iteration 121/1000 | Loss: 0.00000915
Iteration 122/1000 | Loss: 0.00000915
Iteration 123/1000 | Loss: 0.00000915
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000914
Iteration 127/1000 | Loss: 0.00000914
Iteration 128/1000 | Loss: 0.00000914
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000914
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000913
Iteration 136/1000 | Loss: 0.00000913
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000913
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000913
Iteration 146/1000 | Loss: 0.00000913
Iteration 147/1000 | Loss: 0.00000912
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000912
Iteration 153/1000 | Loss: 0.00000912
Iteration 154/1000 | Loss: 0.00000912
Iteration 155/1000 | Loss: 0.00000912
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000911
Iteration 165/1000 | Loss: 0.00000911
Iteration 166/1000 | Loss: 0.00000911
Iteration 167/1000 | Loss: 0.00000911
Iteration 168/1000 | Loss: 0.00000911
Iteration 169/1000 | Loss: 0.00000911
Iteration 170/1000 | Loss: 0.00000911
Iteration 171/1000 | Loss: 0.00000911
Iteration 172/1000 | Loss: 0.00000911
Iteration 173/1000 | Loss: 0.00000911
Iteration 174/1000 | Loss: 0.00000911
Iteration 175/1000 | Loss: 0.00000911
Iteration 176/1000 | Loss: 0.00000911
Iteration 177/1000 | Loss: 0.00000911
Iteration 178/1000 | Loss: 0.00000911
Iteration 179/1000 | Loss: 0.00000911
Iteration 180/1000 | Loss: 0.00000911
Iteration 181/1000 | Loss: 0.00000911
Iteration 182/1000 | Loss: 0.00000911
Iteration 183/1000 | Loss: 0.00000911
Iteration 184/1000 | Loss: 0.00000911
Iteration 185/1000 | Loss: 0.00000911
Iteration 186/1000 | Loss: 0.00000911
Iteration 187/1000 | Loss: 0.00000911
Iteration 188/1000 | Loss: 0.00000911
Iteration 189/1000 | Loss: 0.00000911
Iteration 190/1000 | Loss: 0.00000911
Iteration 191/1000 | Loss: 0.00000911
Iteration 192/1000 | Loss: 0.00000911
Iteration 193/1000 | Loss: 0.00000911
Iteration 194/1000 | Loss: 0.00000911
Iteration 195/1000 | Loss: 0.00000911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [9.108013728109654e-06, 9.108013728109654e-06, 9.108013728109654e-06, 9.108013728109654e-06, 9.108013728109654e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.108013728109654e-06

Optimization complete. Final v2v error: 2.5762393474578857 mm

Highest mean error: 2.9577956199645996 mm for frame 118

Lowest mean error: 2.2815654277801514 mm for frame 260

Saving results

Total time: 42.80441212654114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997924
Iteration 2/25 | Loss: 0.00208600
Iteration 3/25 | Loss: 0.00155215
Iteration 4/25 | Loss: 0.00138006
Iteration 5/25 | Loss: 0.00146462
Iteration 6/25 | Loss: 0.00127613
Iteration 7/25 | Loss: 0.00119853
Iteration 8/25 | Loss: 0.00117888
Iteration 9/25 | Loss: 0.00116854
Iteration 10/25 | Loss: 0.00116044
Iteration 11/25 | Loss: 0.00115341
Iteration 12/25 | Loss: 0.00114334
Iteration 13/25 | Loss: 0.00113784
Iteration 14/25 | Loss: 0.00113032
Iteration 15/25 | Loss: 0.00112862
Iteration 16/25 | Loss: 0.00112793
Iteration 17/25 | Loss: 0.00112761
Iteration 18/25 | Loss: 0.00112745
Iteration 19/25 | Loss: 0.00112732
Iteration 20/25 | Loss: 0.00112721
Iteration 21/25 | Loss: 0.00112711
Iteration 22/25 | Loss: 0.00113071
Iteration 23/25 | Loss: 0.00113025
Iteration 24/25 | Loss: 0.00112773
Iteration 25/25 | Loss: 0.00112935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42065680
Iteration 2/25 | Loss: 0.00118141
Iteration 3/25 | Loss: 0.00077438
Iteration 4/25 | Loss: 0.00077425
Iteration 5/25 | Loss: 0.00077424
Iteration 6/25 | Loss: 0.00077424
Iteration 7/25 | Loss: 0.00077424
Iteration 8/25 | Loss: 0.00077424
Iteration 9/25 | Loss: 0.00077424
Iteration 10/25 | Loss: 0.00077424
Iteration 11/25 | Loss: 0.00077424
Iteration 12/25 | Loss: 0.00077424
Iteration 13/25 | Loss: 0.00077424
Iteration 14/25 | Loss: 0.00077424
Iteration 15/25 | Loss: 0.00077424
Iteration 16/25 | Loss: 0.00077424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007742425077594817, 0.0007742425077594817, 0.0007742425077594817, 0.0007742425077594817, 0.0007742425077594817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007742425077594817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077424
Iteration 2/1000 | Loss: 0.00046688
Iteration 3/1000 | Loss: 0.00005515
Iteration 4/1000 | Loss: 0.00002789
Iteration 5/1000 | Loss: 0.00002451
Iteration 6/1000 | Loss: 0.00005033
Iteration 7/1000 | Loss: 0.00016558
Iteration 8/1000 | Loss: 0.00031113
Iteration 9/1000 | Loss: 0.00021536
Iteration 10/1000 | Loss: 0.00029928
Iteration 11/1000 | Loss: 0.00007264
Iteration 12/1000 | Loss: 0.00007131
Iteration 13/1000 | Loss: 0.00003542
Iteration 14/1000 | Loss: 0.00005116
Iteration 15/1000 | Loss: 0.00024980
Iteration 16/1000 | Loss: 0.00003560
Iteration 17/1000 | Loss: 0.00003541
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002892
Iteration 20/1000 | Loss: 0.00004481
Iteration 21/1000 | Loss: 0.00001569
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00004220
Iteration 24/1000 | Loss: 0.00017096
Iteration 25/1000 | Loss: 0.00002282
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001441
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00008712
Iteration 35/1000 | Loss: 0.00019493
Iteration 36/1000 | Loss: 0.00001689
Iteration 37/1000 | Loss: 0.00007619
Iteration 38/1000 | Loss: 0.00002586
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001403
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001391
Iteration 47/1000 | Loss: 0.00003682
Iteration 48/1000 | Loss: 0.00005216
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00002045
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001375
Iteration 83/1000 | Loss: 0.00001375
Iteration 84/1000 | Loss: 0.00001375
Iteration 85/1000 | Loss: 0.00001375
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001374
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001372
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001371
Iteration 126/1000 | Loss: 0.00001371
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Iteration 133/1000 | Loss: 0.00001370
Iteration 134/1000 | Loss: 0.00001370
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001368
Iteration 151/1000 | Loss: 0.00001368
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001368
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001367
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001367
Iteration 169/1000 | Loss: 0.00001367
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001367
Iteration 172/1000 | Loss: 0.00001367
Iteration 173/1000 | Loss: 0.00001367
Iteration 174/1000 | Loss: 0.00001367
Iteration 175/1000 | Loss: 0.00001367
Iteration 176/1000 | Loss: 0.00001367
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Iteration 179/1000 | Loss: 0.00001366
Iteration 180/1000 | Loss: 0.00001366
Iteration 181/1000 | Loss: 0.00001366
Iteration 182/1000 | Loss: 0.00001366
Iteration 183/1000 | Loss: 0.00001366
Iteration 184/1000 | Loss: 0.00001366
Iteration 185/1000 | Loss: 0.00001366
Iteration 186/1000 | Loss: 0.00001366
Iteration 187/1000 | Loss: 0.00001366
Iteration 188/1000 | Loss: 0.00001366
Iteration 189/1000 | Loss: 0.00001366
Iteration 190/1000 | Loss: 0.00001366
Iteration 191/1000 | Loss: 0.00001365
Iteration 192/1000 | Loss: 0.00001365
Iteration 193/1000 | Loss: 0.00001365
Iteration 194/1000 | Loss: 0.00001365
Iteration 195/1000 | Loss: 0.00001365
Iteration 196/1000 | Loss: 0.00001365
Iteration 197/1000 | Loss: 0.00001365
Iteration 198/1000 | Loss: 0.00001365
Iteration 199/1000 | Loss: 0.00001365
Iteration 200/1000 | Loss: 0.00001365
Iteration 201/1000 | Loss: 0.00001364
Iteration 202/1000 | Loss: 0.00001364
Iteration 203/1000 | Loss: 0.00001364
Iteration 204/1000 | Loss: 0.00001364
Iteration 205/1000 | Loss: 0.00001364
Iteration 206/1000 | Loss: 0.00001364
Iteration 207/1000 | Loss: 0.00001364
Iteration 208/1000 | Loss: 0.00001364
Iteration 209/1000 | Loss: 0.00001364
Iteration 210/1000 | Loss: 0.00001364
Iteration 211/1000 | Loss: 0.00001364
Iteration 212/1000 | Loss: 0.00001364
Iteration 213/1000 | Loss: 0.00001364
Iteration 214/1000 | Loss: 0.00001364
Iteration 215/1000 | Loss: 0.00001364
Iteration 216/1000 | Loss: 0.00001364
Iteration 217/1000 | Loss: 0.00001364
Iteration 218/1000 | Loss: 0.00001364
Iteration 219/1000 | Loss: 0.00001364
Iteration 220/1000 | Loss: 0.00001364
Iteration 221/1000 | Loss: 0.00001364
Iteration 222/1000 | Loss: 0.00001364
Iteration 223/1000 | Loss: 0.00001364
Iteration 224/1000 | Loss: 0.00001364
Iteration 225/1000 | Loss: 0.00001364
Iteration 226/1000 | Loss: 0.00001364
Iteration 227/1000 | Loss: 0.00001364
Iteration 228/1000 | Loss: 0.00001364
Iteration 229/1000 | Loss: 0.00001364
Iteration 230/1000 | Loss: 0.00001364
Iteration 231/1000 | Loss: 0.00001364
Iteration 232/1000 | Loss: 0.00001364
Iteration 233/1000 | Loss: 0.00001364
Iteration 234/1000 | Loss: 0.00001364
Iteration 235/1000 | Loss: 0.00001364
Iteration 236/1000 | Loss: 0.00001364
Iteration 237/1000 | Loss: 0.00001364
Iteration 238/1000 | Loss: 0.00001364
Iteration 239/1000 | Loss: 0.00001364
Iteration 240/1000 | Loss: 0.00001364
Iteration 241/1000 | Loss: 0.00001364
Iteration 242/1000 | Loss: 0.00001364
Iteration 243/1000 | Loss: 0.00001364
Iteration 244/1000 | Loss: 0.00001364
Iteration 245/1000 | Loss: 0.00001364
Iteration 246/1000 | Loss: 0.00001364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.363634964945959e-05, 1.363634964945959e-05, 1.363634964945959e-05, 1.363634964945959e-05, 1.363634964945959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.363634964945959e-05

Optimization complete. Final v2v error: 3.0141406059265137 mm

Highest mean error: 4.913259506225586 mm for frame 34

Lowest mean error: 2.5315380096435547 mm for frame 147

Saving results

Total time: 113.74724340438843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786842
Iteration 2/25 | Loss: 0.00163536
Iteration 3/25 | Loss: 0.00127736
Iteration 4/25 | Loss: 0.00125641
Iteration 5/25 | Loss: 0.00129123
Iteration 6/25 | Loss: 0.00124822
Iteration 7/25 | Loss: 0.00121214
Iteration 8/25 | Loss: 0.00116591
Iteration 9/25 | Loss: 0.00115966
Iteration 10/25 | Loss: 0.00117472
Iteration 11/25 | Loss: 0.00114370
Iteration 12/25 | Loss: 0.00113714
Iteration 13/25 | Loss: 0.00113267
Iteration 14/25 | Loss: 0.00112884
Iteration 15/25 | Loss: 0.00112572
Iteration 16/25 | Loss: 0.00113095
Iteration 17/25 | Loss: 0.00112715
Iteration 18/25 | Loss: 0.00112462
Iteration 19/25 | Loss: 0.00112219
Iteration 20/25 | Loss: 0.00112185
Iteration 21/25 | Loss: 0.00111947
Iteration 22/25 | Loss: 0.00112201
Iteration 23/25 | Loss: 0.00111539
Iteration 24/25 | Loss: 0.00111322
Iteration 25/25 | Loss: 0.00111242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23738766
Iteration 2/25 | Loss: 0.00085197
Iteration 3/25 | Loss: 0.00085197
Iteration 4/25 | Loss: 0.00085197
Iteration 5/25 | Loss: 0.00085197
Iteration 6/25 | Loss: 0.00085197
Iteration 7/25 | Loss: 0.00085197
Iteration 8/25 | Loss: 0.00085197
Iteration 9/25 | Loss: 0.00085197
Iteration 10/25 | Loss: 0.00085197
Iteration 11/25 | Loss: 0.00085197
Iteration 12/25 | Loss: 0.00085197
Iteration 13/25 | Loss: 0.00085197
Iteration 14/25 | Loss: 0.00085197
Iteration 15/25 | Loss: 0.00085197
Iteration 16/25 | Loss: 0.00085197
Iteration 17/25 | Loss: 0.00085197
Iteration 18/25 | Loss: 0.00085197
Iteration 19/25 | Loss: 0.00085197
Iteration 20/25 | Loss: 0.00085197
Iteration 21/25 | Loss: 0.00085197
Iteration 22/25 | Loss: 0.00085197
Iteration 23/25 | Loss: 0.00085197
Iteration 24/25 | Loss: 0.00085197
Iteration 25/25 | Loss: 0.00085197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085197
Iteration 2/1000 | Loss: 0.00004148
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002423
Iteration 5/1000 | Loss: 0.00002250
Iteration 6/1000 | Loss: 0.00021123
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00001878
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001651
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001599
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001588
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001578
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001557
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001553
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001546
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001544
Iteration 54/1000 | Loss: 0.00001544
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001541
Iteration 72/1000 | Loss: 0.00001541
Iteration 73/1000 | Loss: 0.00001541
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001539
Iteration 81/1000 | Loss: 0.00001539
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001539
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001534
Iteration 100/1000 | Loss: 0.00001534
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00001533
Iteration 103/1000 | Loss: 0.00001533
Iteration 104/1000 | Loss: 0.00001533
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001532
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001531
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001530
Iteration 113/1000 | Loss: 0.00001530
Iteration 114/1000 | Loss: 0.00001530
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001528
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001528
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001527
Iteration 132/1000 | Loss: 0.00001527
Iteration 133/1000 | Loss: 0.00001527
Iteration 134/1000 | Loss: 0.00001527
Iteration 135/1000 | Loss: 0.00001527
Iteration 136/1000 | Loss: 0.00001527
Iteration 137/1000 | Loss: 0.00001527
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001526
Iteration 140/1000 | Loss: 0.00001526
Iteration 141/1000 | Loss: 0.00001526
Iteration 142/1000 | Loss: 0.00001525
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001524
Iteration 150/1000 | Loss: 0.00001524
Iteration 151/1000 | Loss: 0.00001524
Iteration 152/1000 | Loss: 0.00001523
Iteration 153/1000 | Loss: 0.00001523
Iteration 154/1000 | Loss: 0.00001523
Iteration 155/1000 | Loss: 0.00001522
Iteration 156/1000 | Loss: 0.00001522
Iteration 157/1000 | Loss: 0.00001522
Iteration 158/1000 | Loss: 0.00001522
Iteration 159/1000 | Loss: 0.00001522
Iteration 160/1000 | Loss: 0.00001522
Iteration 161/1000 | Loss: 0.00001522
Iteration 162/1000 | Loss: 0.00001522
Iteration 163/1000 | Loss: 0.00001521
Iteration 164/1000 | Loss: 0.00001521
Iteration 165/1000 | Loss: 0.00001521
Iteration 166/1000 | Loss: 0.00001521
Iteration 167/1000 | Loss: 0.00001521
Iteration 168/1000 | Loss: 0.00001521
Iteration 169/1000 | Loss: 0.00001521
Iteration 170/1000 | Loss: 0.00001521
Iteration 171/1000 | Loss: 0.00001521
Iteration 172/1000 | Loss: 0.00001521
Iteration 173/1000 | Loss: 0.00001521
Iteration 174/1000 | Loss: 0.00001520
Iteration 175/1000 | Loss: 0.00001520
Iteration 176/1000 | Loss: 0.00001520
Iteration 177/1000 | Loss: 0.00001520
Iteration 178/1000 | Loss: 0.00001520
Iteration 179/1000 | Loss: 0.00001520
Iteration 180/1000 | Loss: 0.00001520
Iteration 181/1000 | Loss: 0.00001520
Iteration 182/1000 | Loss: 0.00001520
Iteration 183/1000 | Loss: 0.00001520
Iteration 184/1000 | Loss: 0.00001520
Iteration 185/1000 | Loss: 0.00001520
Iteration 186/1000 | Loss: 0.00001520
Iteration 187/1000 | Loss: 0.00001519
Iteration 188/1000 | Loss: 0.00001519
Iteration 189/1000 | Loss: 0.00001519
Iteration 190/1000 | Loss: 0.00001519
Iteration 191/1000 | Loss: 0.00001519
Iteration 192/1000 | Loss: 0.00001519
Iteration 193/1000 | Loss: 0.00001519
Iteration 194/1000 | Loss: 0.00001519
Iteration 195/1000 | Loss: 0.00001519
Iteration 196/1000 | Loss: 0.00001519
Iteration 197/1000 | Loss: 0.00001519
Iteration 198/1000 | Loss: 0.00001519
Iteration 199/1000 | Loss: 0.00001519
Iteration 200/1000 | Loss: 0.00001519
Iteration 201/1000 | Loss: 0.00001519
Iteration 202/1000 | Loss: 0.00001519
Iteration 203/1000 | Loss: 0.00001519
Iteration 204/1000 | Loss: 0.00001519
Iteration 205/1000 | Loss: 0.00001519
Iteration 206/1000 | Loss: 0.00001519
Iteration 207/1000 | Loss: 0.00001518
Iteration 208/1000 | Loss: 0.00001518
Iteration 209/1000 | Loss: 0.00001518
Iteration 210/1000 | Loss: 0.00001518
Iteration 211/1000 | Loss: 0.00001518
Iteration 212/1000 | Loss: 0.00001518
Iteration 213/1000 | Loss: 0.00001518
Iteration 214/1000 | Loss: 0.00001518
Iteration 215/1000 | Loss: 0.00001518
Iteration 216/1000 | Loss: 0.00001518
Iteration 217/1000 | Loss: 0.00001518
Iteration 218/1000 | Loss: 0.00001518
Iteration 219/1000 | Loss: 0.00001518
Iteration 220/1000 | Loss: 0.00001518
Iteration 221/1000 | Loss: 0.00001518
Iteration 222/1000 | Loss: 0.00001518
Iteration 223/1000 | Loss: 0.00001518
Iteration 224/1000 | Loss: 0.00001518
Iteration 225/1000 | Loss: 0.00001518
Iteration 226/1000 | Loss: 0.00001518
Iteration 227/1000 | Loss: 0.00001518
Iteration 228/1000 | Loss: 0.00001518
Iteration 229/1000 | Loss: 0.00001518
Iteration 230/1000 | Loss: 0.00001518
Iteration 231/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.5176858141785488e-05, 1.5176858141785488e-05, 1.5176858141785488e-05, 1.5176858141785488e-05, 1.5176858141785488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5176858141785488e-05

Optimization complete. Final v2v error: 3.1860270500183105 mm

Highest mean error: 4.788486480712891 mm for frame 103

Lowest mean error: 2.376993179321289 mm for frame 177

Saving results

Total time: 93.27757740020752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864540
Iteration 2/25 | Loss: 0.00233797
Iteration 3/25 | Loss: 0.00178143
Iteration 4/25 | Loss: 0.00144850
Iteration 5/25 | Loss: 0.00135035
Iteration 6/25 | Loss: 0.00134757
Iteration 7/25 | Loss: 0.00135332
Iteration 8/25 | Loss: 0.00134543
Iteration 9/25 | Loss: 0.00131618
Iteration 10/25 | Loss: 0.00130884
Iteration 11/25 | Loss: 0.00130367
Iteration 12/25 | Loss: 0.00130229
Iteration 13/25 | Loss: 0.00129826
Iteration 14/25 | Loss: 0.00129867
Iteration 15/25 | Loss: 0.00129862
Iteration 16/25 | Loss: 0.00130042
Iteration 17/25 | Loss: 0.00129953
Iteration 18/25 | Loss: 0.00129873
Iteration 19/25 | Loss: 0.00129668
Iteration 20/25 | Loss: 0.00129872
Iteration 21/25 | Loss: 0.00129798
Iteration 22/25 | Loss: 0.00129640
Iteration 23/25 | Loss: 0.00129896
Iteration 24/25 | Loss: 0.00129993
Iteration 25/25 | Loss: 0.00129949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15116501
Iteration 2/25 | Loss: 0.00066039
Iteration 3/25 | Loss: 0.00066038
Iteration 4/25 | Loss: 0.00066038
Iteration 5/25 | Loss: 0.00066038
Iteration 6/25 | Loss: 0.00066038
Iteration 7/25 | Loss: 0.00066038
Iteration 8/25 | Loss: 0.00066038
Iteration 9/25 | Loss: 0.00066038
Iteration 10/25 | Loss: 0.00066038
Iteration 11/25 | Loss: 0.00066038
Iteration 12/25 | Loss: 0.00066038
Iteration 13/25 | Loss: 0.00066038
Iteration 14/25 | Loss: 0.00066038
Iteration 15/25 | Loss: 0.00066038
Iteration 16/25 | Loss: 0.00066038
Iteration 17/25 | Loss: 0.00066038
Iteration 18/25 | Loss: 0.00066038
Iteration 19/25 | Loss: 0.00066038
Iteration 20/25 | Loss: 0.00066038
Iteration 21/25 | Loss: 0.00066038
Iteration 22/25 | Loss: 0.00066038
Iteration 23/25 | Loss: 0.00066038
Iteration 24/25 | Loss: 0.00066038
Iteration 25/25 | Loss: 0.00066038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066038
Iteration 2/1000 | Loss: 0.00014309
Iteration 3/1000 | Loss: 0.00012712
Iteration 4/1000 | Loss: 0.00013852
Iteration 5/1000 | Loss: 0.00017517
Iteration 6/1000 | Loss: 0.00016488
Iteration 7/1000 | Loss: 0.00022371
Iteration 8/1000 | Loss: 0.00022255
Iteration 9/1000 | Loss: 0.00024096
Iteration 10/1000 | Loss: 0.00022594
Iteration 11/1000 | Loss: 0.00024748
Iteration 12/1000 | Loss: 0.00024320
Iteration 13/1000 | Loss: 0.00024627
Iteration 14/1000 | Loss: 0.00023004
Iteration 15/1000 | Loss: 0.00019794
Iteration 16/1000 | Loss: 0.00024277
Iteration 17/1000 | Loss: 0.00016365
Iteration 18/1000 | Loss: 0.00011089
Iteration 19/1000 | Loss: 0.00016428
Iteration 20/1000 | Loss: 0.00015035
Iteration 21/1000 | Loss: 0.00015851
Iteration 22/1000 | Loss: 0.00017063
Iteration 23/1000 | Loss: 0.00017157
Iteration 24/1000 | Loss: 0.00021650
Iteration 25/1000 | Loss: 0.00021004
Iteration 26/1000 | Loss: 0.00017449
Iteration 27/1000 | Loss: 0.00011378
Iteration 28/1000 | Loss: 0.00010715
Iteration 29/1000 | Loss: 0.00018724
Iteration 30/1000 | Loss: 0.00014609
Iteration 31/1000 | Loss: 0.00010861
Iteration 32/1000 | Loss: 0.00019874
Iteration 33/1000 | Loss: 0.00013656
Iteration 34/1000 | Loss: 0.00019811
Iteration 35/1000 | Loss: 0.00014921
Iteration 36/1000 | Loss: 0.00019533
Iteration 37/1000 | Loss: 0.00010038
Iteration 38/1000 | Loss: 0.00007550
Iteration 39/1000 | Loss: 0.00010033
Iteration 40/1000 | Loss: 0.00012269
Iteration 41/1000 | Loss: 0.00010332
Iteration 42/1000 | Loss: 0.00013403
Iteration 43/1000 | Loss: 0.00008850
Iteration 44/1000 | Loss: 0.00019103
Iteration 45/1000 | Loss: 0.00010027
Iteration 46/1000 | Loss: 0.00008141
Iteration 47/1000 | Loss: 0.00010810
Iteration 48/1000 | Loss: 0.00014689
Iteration 49/1000 | Loss: 0.00012749
Iteration 50/1000 | Loss: 0.00013148
Iteration 51/1000 | Loss: 0.00012177
Iteration 52/1000 | Loss: 0.00012383
Iteration 53/1000 | Loss: 0.00013200
Iteration 54/1000 | Loss: 0.00018875
Iteration 55/1000 | Loss: 0.00013341
Iteration 56/1000 | Loss: 0.00019543
Iteration 57/1000 | Loss: 0.00015095
Iteration 58/1000 | Loss: 0.00017031
Iteration 59/1000 | Loss: 0.00014502
Iteration 60/1000 | Loss: 0.00017314
Iteration 61/1000 | Loss: 0.00011959
Iteration 62/1000 | Loss: 0.00010766
Iteration 63/1000 | Loss: 0.00010535
Iteration 64/1000 | Loss: 0.00012500
Iteration 65/1000 | Loss: 0.00014031
Iteration 66/1000 | Loss: 0.00014789
Iteration 67/1000 | Loss: 0.00013014
Iteration 68/1000 | Loss: 0.00011861
Iteration 69/1000 | Loss: 0.00010366
Iteration 70/1000 | Loss: 0.00007248
Iteration 71/1000 | Loss: 0.00006572
Iteration 72/1000 | Loss: 0.00006235
Iteration 73/1000 | Loss: 0.00005057
Iteration 74/1000 | Loss: 0.00004759
Iteration 75/1000 | Loss: 0.00005461
Iteration 76/1000 | Loss: 0.00005861
Iteration 77/1000 | Loss: 0.00006155
Iteration 78/1000 | Loss: 0.00004145
Iteration 79/1000 | Loss: 0.00005145
Iteration 80/1000 | Loss: 0.00004736
Iteration 81/1000 | Loss: 0.00005251
Iteration 82/1000 | Loss: 0.00003896
Iteration 83/1000 | Loss: 0.00004506
Iteration 84/1000 | Loss: 0.00003807
Iteration 85/1000 | Loss: 0.00003673
Iteration 86/1000 | Loss: 0.00004682
Iteration 87/1000 | Loss: 0.00008363
Iteration 88/1000 | Loss: 0.00005687
Iteration 89/1000 | Loss: 0.00004316
Iteration 90/1000 | Loss: 0.00004039
Iteration 91/1000 | Loss: 0.00003638
Iteration 92/1000 | Loss: 0.00003484
Iteration 93/1000 | Loss: 0.00003673
Iteration 94/1000 | Loss: 0.00003046
Iteration 95/1000 | Loss: 0.00003748
Iteration 96/1000 | Loss: 0.00003845
Iteration 97/1000 | Loss: 0.00004023
Iteration 98/1000 | Loss: 0.00004182
Iteration 99/1000 | Loss: 0.00003598
Iteration 100/1000 | Loss: 0.00003379
Iteration 101/1000 | Loss: 0.00002487
Iteration 102/1000 | Loss: 0.00002405
Iteration 103/1000 | Loss: 0.00002849
Iteration 104/1000 | Loss: 0.00003549
Iteration 105/1000 | Loss: 0.00003632
Iteration 106/1000 | Loss: 0.00003627
Iteration 107/1000 | Loss: 0.00004714
Iteration 108/1000 | Loss: 0.00004428
Iteration 109/1000 | Loss: 0.00005220
Iteration 110/1000 | Loss: 0.00005605
Iteration 111/1000 | Loss: 0.00004156
Iteration 112/1000 | Loss: 0.00003986
Iteration 113/1000 | Loss: 0.00004475
Iteration 114/1000 | Loss: 0.00004703
Iteration 115/1000 | Loss: 0.00003957
Iteration 116/1000 | Loss: 0.00004264
Iteration 117/1000 | Loss: 0.00004217
Iteration 118/1000 | Loss: 0.00004468
Iteration 119/1000 | Loss: 0.00004044
Iteration 120/1000 | Loss: 0.00004126
Iteration 121/1000 | Loss: 0.00004364
Iteration 122/1000 | Loss: 0.00003801
Iteration 123/1000 | Loss: 0.00003190
Iteration 124/1000 | Loss: 0.00003062
Iteration 125/1000 | Loss: 0.00002762
Iteration 126/1000 | Loss: 0.00002441
Iteration 127/1000 | Loss: 0.00002719
Iteration 128/1000 | Loss: 0.00002559
Iteration 129/1000 | Loss: 0.00002633
Iteration 130/1000 | Loss: 0.00003291
Iteration 131/1000 | Loss: 0.00002322
Iteration 132/1000 | Loss: 0.00002675
Iteration 133/1000 | Loss: 0.00002788
Iteration 134/1000 | Loss: 0.00003219
Iteration 135/1000 | Loss: 0.00003349
Iteration 136/1000 | Loss: 0.00002806
Iteration 137/1000 | Loss: 0.00002641
Iteration 138/1000 | Loss: 0.00002890
Iteration 139/1000 | Loss: 0.00003365
Iteration 140/1000 | Loss: 0.00002925
Iteration 141/1000 | Loss: 0.00002886
Iteration 142/1000 | Loss: 0.00002437
Iteration 143/1000 | Loss: 0.00003071
Iteration 144/1000 | Loss: 0.00002430
Iteration 145/1000 | Loss: 0.00002903
Iteration 146/1000 | Loss: 0.00002393
Iteration 147/1000 | Loss: 0.00002938
Iteration 148/1000 | Loss: 0.00002337
Iteration 149/1000 | Loss: 0.00002877
Iteration 150/1000 | Loss: 0.00002543
Iteration 151/1000 | Loss: 0.00002759
Iteration 152/1000 | Loss: 0.00002562
Iteration 153/1000 | Loss: 0.00002917
Iteration 154/1000 | Loss: 0.00002824
Iteration 155/1000 | Loss: 0.00002690
Iteration 156/1000 | Loss: 0.00003123
Iteration 157/1000 | Loss: 0.00002897
Iteration 158/1000 | Loss: 0.00002348
Iteration 159/1000 | Loss: 0.00002180
Iteration 160/1000 | Loss: 0.00003056
Iteration 161/1000 | Loss: 0.00002880
Iteration 162/1000 | Loss: 0.00002854
Iteration 163/1000 | Loss: 0.00003186
Iteration 164/1000 | Loss: 0.00003427
Iteration 165/1000 | Loss: 0.00003101
Iteration 166/1000 | Loss: 0.00002365
Iteration 167/1000 | Loss: 0.00002705
Iteration 168/1000 | Loss: 0.00002382
Iteration 169/1000 | Loss: 0.00002316
Iteration 170/1000 | Loss: 0.00002234
Iteration 171/1000 | Loss: 0.00002777
Iteration 172/1000 | Loss: 0.00002671
Iteration 173/1000 | Loss: 0.00002764
Iteration 174/1000 | Loss: 0.00002651
Iteration 175/1000 | Loss: 0.00002251
Iteration 176/1000 | Loss: 0.00002190
Iteration 177/1000 | Loss: 0.00002169
Iteration 178/1000 | Loss: 0.00002168
Iteration 179/1000 | Loss: 0.00002168
Iteration 180/1000 | Loss: 0.00002167
Iteration 181/1000 | Loss: 0.00002165
Iteration 182/1000 | Loss: 0.00002161
Iteration 183/1000 | Loss: 0.00002157
Iteration 184/1000 | Loss: 0.00002153
Iteration 185/1000 | Loss: 0.00002153
Iteration 186/1000 | Loss: 0.00002151
Iteration 187/1000 | Loss: 0.00002151
Iteration 188/1000 | Loss: 0.00009810
Iteration 189/1000 | Loss: 0.00002623
Iteration 190/1000 | Loss: 0.00002487
Iteration 191/1000 | Loss: 0.00002392
Iteration 192/1000 | Loss: 0.00002330
Iteration 193/1000 | Loss: 0.00002288
Iteration 194/1000 | Loss: 0.00002219
Iteration 195/1000 | Loss: 0.00002186
Iteration 196/1000 | Loss: 0.00004029
Iteration 197/1000 | Loss: 0.00002941
Iteration 198/1000 | Loss: 0.00004023
Iteration 199/1000 | Loss: 0.00002924
Iteration 200/1000 | Loss: 0.00003989
Iteration 201/1000 | Loss: 0.00002880
Iteration 202/1000 | Loss: 0.00003944
Iteration 203/1000 | Loss: 0.00002286
Iteration 204/1000 | Loss: 0.00002223
Iteration 205/1000 | Loss: 0.00002191
Iteration 206/1000 | Loss: 0.00002184
Iteration 207/1000 | Loss: 0.00002166
Iteration 208/1000 | Loss: 0.00002154
Iteration 209/1000 | Loss: 0.00002154
Iteration 210/1000 | Loss: 0.00002153
Iteration 211/1000 | Loss: 0.00002153
Iteration 212/1000 | Loss: 0.00002153
Iteration 213/1000 | Loss: 0.00002152
Iteration 214/1000 | Loss: 0.00002148
Iteration 215/1000 | Loss: 0.00002121
Iteration 216/1000 | Loss: 0.00002094
Iteration 217/1000 | Loss: 0.00002069
Iteration 218/1000 | Loss: 0.00002040
Iteration 219/1000 | Loss: 0.00002034
Iteration 220/1000 | Loss: 0.00002012
Iteration 221/1000 | Loss: 0.00002001
Iteration 222/1000 | Loss: 0.00001999
Iteration 223/1000 | Loss: 0.00001997
Iteration 224/1000 | Loss: 0.00001997
Iteration 225/1000 | Loss: 0.00001997
Iteration 226/1000 | Loss: 0.00001996
Iteration 227/1000 | Loss: 0.00001996
Iteration 228/1000 | Loss: 0.00001996
Iteration 229/1000 | Loss: 0.00001996
Iteration 230/1000 | Loss: 0.00001996
Iteration 231/1000 | Loss: 0.00001996
Iteration 232/1000 | Loss: 0.00001995
Iteration 233/1000 | Loss: 0.00001995
Iteration 234/1000 | Loss: 0.00001995
Iteration 235/1000 | Loss: 0.00001995
Iteration 236/1000 | Loss: 0.00001995
Iteration 237/1000 | Loss: 0.00001995
Iteration 238/1000 | Loss: 0.00001994
Iteration 239/1000 | Loss: 0.00001994
Iteration 240/1000 | Loss: 0.00001994
Iteration 241/1000 | Loss: 0.00001994
Iteration 242/1000 | Loss: 0.00001994
Iteration 243/1000 | Loss: 0.00001994
Iteration 244/1000 | Loss: 0.00001994
Iteration 245/1000 | Loss: 0.00001994
Iteration 246/1000 | Loss: 0.00001994
Iteration 247/1000 | Loss: 0.00001993
Iteration 248/1000 | Loss: 0.00001993
Iteration 249/1000 | Loss: 0.00001993
Iteration 250/1000 | Loss: 0.00001993
Iteration 251/1000 | Loss: 0.00001993
Iteration 252/1000 | Loss: 0.00001992
Iteration 253/1000 | Loss: 0.00001992
Iteration 254/1000 | Loss: 0.00001992
Iteration 255/1000 | Loss: 0.00001992
Iteration 256/1000 | Loss: 0.00001992
Iteration 257/1000 | Loss: 0.00001991
Iteration 258/1000 | Loss: 0.00001991
Iteration 259/1000 | Loss: 0.00001991
Iteration 260/1000 | Loss: 0.00001991
Iteration 261/1000 | Loss: 0.00001991
Iteration 262/1000 | Loss: 0.00001991
Iteration 263/1000 | Loss: 0.00001991
Iteration 264/1000 | Loss: 0.00001990
Iteration 265/1000 | Loss: 0.00001990
Iteration 266/1000 | Loss: 0.00001990
Iteration 267/1000 | Loss: 0.00001990
Iteration 268/1000 | Loss: 0.00001990
Iteration 269/1000 | Loss: 0.00001990
Iteration 270/1000 | Loss: 0.00001990
Iteration 271/1000 | Loss: 0.00001990
Iteration 272/1000 | Loss: 0.00001989
Iteration 273/1000 | Loss: 0.00001989
Iteration 274/1000 | Loss: 0.00001988
Iteration 275/1000 | Loss: 0.00001988
Iteration 276/1000 | Loss: 0.00001988
Iteration 277/1000 | Loss: 0.00001988
Iteration 278/1000 | Loss: 0.00001987
Iteration 279/1000 | Loss: 0.00001986
Iteration 280/1000 | Loss: 0.00001986
Iteration 281/1000 | Loss: 0.00001985
Iteration 282/1000 | Loss: 0.00001984
Iteration 283/1000 | Loss: 0.00001984
Iteration 284/1000 | Loss: 0.00001983
Iteration 285/1000 | Loss: 0.00001983
Iteration 286/1000 | Loss: 0.00001983
Iteration 287/1000 | Loss: 0.00001983
Iteration 288/1000 | Loss: 0.00001983
Iteration 289/1000 | Loss: 0.00001982
Iteration 290/1000 | Loss: 0.00001982
Iteration 291/1000 | Loss: 0.00001982
Iteration 292/1000 | Loss: 0.00001982
Iteration 293/1000 | Loss: 0.00001982
Iteration 294/1000 | Loss: 0.00001982
Iteration 295/1000 | Loss: 0.00001982
Iteration 296/1000 | Loss: 0.00001982
Iteration 297/1000 | Loss: 0.00001982
Iteration 298/1000 | Loss: 0.00001982
Iteration 299/1000 | Loss: 0.00001982
Iteration 300/1000 | Loss: 0.00001982
Iteration 301/1000 | Loss: 0.00001982
Iteration 302/1000 | Loss: 0.00001982
Iteration 303/1000 | Loss: 0.00001982
Iteration 304/1000 | Loss: 0.00001982
Iteration 305/1000 | Loss: 0.00001982
Iteration 306/1000 | Loss: 0.00001982
Iteration 307/1000 | Loss: 0.00001982
Iteration 308/1000 | Loss: 0.00001981
Iteration 309/1000 | Loss: 0.00001981
Iteration 310/1000 | Loss: 0.00001981
Iteration 311/1000 | Loss: 0.00001981
Iteration 312/1000 | Loss: 0.00001981
Iteration 313/1000 | Loss: 0.00001981
Iteration 314/1000 | Loss: 0.00001981
Iteration 315/1000 | Loss: 0.00001981
Iteration 316/1000 | Loss: 0.00001980
Iteration 317/1000 | Loss: 0.00001980
Iteration 318/1000 | Loss: 0.00001980
Iteration 319/1000 | Loss: 0.00001980
Iteration 320/1000 | Loss: 0.00001980
Iteration 321/1000 | Loss: 0.00001980
Iteration 322/1000 | Loss: 0.00001980
Iteration 323/1000 | Loss: 0.00001980
Iteration 324/1000 | Loss: 0.00001980
Iteration 325/1000 | Loss: 0.00001980
Iteration 326/1000 | Loss: 0.00001980
Iteration 327/1000 | Loss: 0.00001980
Iteration 328/1000 | Loss: 0.00001980
Iteration 329/1000 | Loss: 0.00001980
Iteration 330/1000 | Loss: 0.00001980
Iteration 331/1000 | Loss: 0.00001980
Iteration 332/1000 | Loss: 0.00001980
Iteration 333/1000 | Loss: 0.00001979
Iteration 334/1000 | Loss: 0.00001979
Iteration 335/1000 | Loss: 0.00001979
Iteration 336/1000 | Loss: 0.00001979
Iteration 337/1000 | Loss: 0.00001979
Iteration 338/1000 | Loss: 0.00001979
Iteration 339/1000 | Loss: 0.00001979
Iteration 340/1000 | Loss: 0.00001979
Iteration 341/1000 | Loss: 0.00001979
Iteration 342/1000 | Loss: 0.00001979
Iteration 343/1000 | Loss: 0.00001979
Iteration 344/1000 | Loss: 0.00001979
Iteration 345/1000 | Loss: 0.00001979
Iteration 346/1000 | Loss: 0.00001979
Iteration 347/1000 | Loss: 0.00001979
Iteration 348/1000 | Loss: 0.00001979
Iteration 349/1000 | Loss: 0.00001979
Iteration 350/1000 | Loss: 0.00001978
Iteration 351/1000 | Loss: 0.00001978
Iteration 352/1000 | Loss: 0.00001978
Iteration 353/1000 | Loss: 0.00001978
Iteration 354/1000 | Loss: 0.00001978
Iteration 355/1000 | Loss: 0.00001978
Iteration 356/1000 | Loss: 0.00001978
Iteration 357/1000 | Loss: 0.00001978
Iteration 358/1000 | Loss: 0.00001978
Iteration 359/1000 | Loss: 0.00001978
Iteration 360/1000 | Loss: 0.00001978
Iteration 361/1000 | Loss: 0.00001978
Iteration 362/1000 | Loss: 0.00001978
Iteration 363/1000 | Loss: 0.00001978
Iteration 364/1000 | Loss: 0.00001977
Iteration 365/1000 | Loss: 0.00001977
Iteration 366/1000 | Loss: 0.00001977
Iteration 367/1000 | Loss: 0.00001977
Iteration 368/1000 | Loss: 0.00001977
Iteration 369/1000 | Loss: 0.00001977
Iteration 370/1000 | Loss: 0.00001977
Iteration 371/1000 | Loss: 0.00001977
Iteration 372/1000 | Loss: 0.00001977
Iteration 373/1000 | Loss: 0.00001977
Iteration 374/1000 | Loss: 0.00001977
Iteration 375/1000 | Loss: 0.00001977
Iteration 376/1000 | Loss: 0.00001977
Iteration 377/1000 | Loss: 0.00001977
Iteration 378/1000 | Loss: 0.00001977
Iteration 379/1000 | Loss: 0.00001977
Iteration 380/1000 | Loss: 0.00001977
Iteration 381/1000 | Loss: 0.00001977
Iteration 382/1000 | Loss: 0.00001977
Iteration 383/1000 | Loss: 0.00001977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 383. Stopping optimization.
Last 5 losses: [1.9770364815485664e-05, 1.9770364815485664e-05, 1.9770364815485664e-05, 1.9770364815485664e-05, 1.9770364815485664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9770364815485664e-05

Optimization complete. Final v2v error: 3.675419569015503 mm

Highest mean error: 4.379983901977539 mm for frame 19

Lowest mean error: 3.496873617172241 mm for frame 135

Saving results

Total time: 386.71392154693604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830432
Iteration 2/25 | Loss: 0.00203340
Iteration 3/25 | Loss: 0.00141108
Iteration 4/25 | Loss: 0.00132202
Iteration 5/25 | Loss: 0.00131511
Iteration 6/25 | Loss: 0.00130875
Iteration 7/25 | Loss: 0.00130713
Iteration 8/25 | Loss: 0.00130500
Iteration 9/25 | Loss: 0.00130440
Iteration 10/25 | Loss: 0.00130410
Iteration 11/25 | Loss: 0.00130327
Iteration 12/25 | Loss: 0.00130107
Iteration 13/25 | Loss: 0.00130037
Iteration 14/25 | Loss: 0.00130005
Iteration 15/25 | Loss: 0.00129997
Iteration 16/25 | Loss: 0.00129997
Iteration 17/25 | Loss: 0.00129997
Iteration 18/25 | Loss: 0.00129997
Iteration 19/25 | Loss: 0.00129997
Iteration 20/25 | Loss: 0.00129997
Iteration 21/25 | Loss: 0.00129997
Iteration 22/25 | Loss: 0.00129997
Iteration 23/25 | Loss: 0.00129997
Iteration 24/25 | Loss: 0.00129997
Iteration 25/25 | Loss: 0.00129997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21841991
Iteration 2/25 | Loss: 0.00071166
Iteration 3/25 | Loss: 0.00071162
Iteration 4/25 | Loss: 0.00071162
Iteration 5/25 | Loss: 0.00071162
Iteration 6/25 | Loss: 0.00071162
Iteration 7/25 | Loss: 0.00071162
Iteration 8/25 | Loss: 0.00071162
Iteration 9/25 | Loss: 0.00071162
Iteration 10/25 | Loss: 0.00071162
Iteration 11/25 | Loss: 0.00071162
Iteration 12/25 | Loss: 0.00071162
Iteration 13/25 | Loss: 0.00071162
Iteration 14/25 | Loss: 0.00071162
Iteration 15/25 | Loss: 0.00071162
Iteration 16/25 | Loss: 0.00071162
Iteration 17/25 | Loss: 0.00071162
Iteration 18/25 | Loss: 0.00071162
Iteration 19/25 | Loss: 0.00071162
Iteration 20/25 | Loss: 0.00071162
Iteration 21/25 | Loss: 0.00071162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007116211927495897, 0.0007116211927495897, 0.0007116211927495897, 0.0007116211927495897, 0.0007116211927495897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007116211927495897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071162
Iteration 2/1000 | Loss: 0.00004742
Iteration 3/1000 | Loss: 0.00003103
Iteration 4/1000 | Loss: 0.00002805
Iteration 5/1000 | Loss: 0.00002717
Iteration 6/1000 | Loss: 0.00002634
Iteration 7/1000 | Loss: 0.00002594
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002559
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002552
Iteration 12/1000 | Loss: 0.00002550
Iteration 13/1000 | Loss: 0.00002549
Iteration 14/1000 | Loss: 0.00002549
Iteration 15/1000 | Loss: 0.00002539
Iteration 16/1000 | Loss: 0.00002531
Iteration 17/1000 | Loss: 0.00002529
Iteration 18/1000 | Loss: 0.00002528
Iteration 19/1000 | Loss: 0.00002528
Iteration 20/1000 | Loss: 0.00002527
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002527
Iteration 23/1000 | Loss: 0.00002526
Iteration 24/1000 | Loss: 0.00002523
Iteration 25/1000 | Loss: 0.00002522
Iteration 26/1000 | Loss: 0.00002521
Iteration 27/1000 | Loss: 0.00002521
Iteration 28/1000 | Loss: 0.00002519
Iteration 29/1000 | Loss: 0.00002519
Iteration 30/1000 | Loss: 0.00002517
Iteration 31/1000 | Loss: 0.00002516
Iteration 32/1000 | Loss: 0.00002516
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002516
Iteration 35/1000 | Loss: 0.00002516
Iteration 36/1000 | Loss: 0.00002515
Iteration 37/1000 | Loss: 0.00002514
Iteration 38/1000 | Loss: 0.00002514
Iteration 39/1000 | Loss: 0.00002513
Iteration 40/1000 | Loss: 0.00002513
Iteration 41/1000 | Loss: 0.00002513
Iteration 42/1000 | Loss: 0.00002512
Iteration 43/1000 | Loss: 0.00002512
Iteration 44/1000 | Loss: 0.00002512
Iteration 45/1000 | Loss: 0.00002511
Iteration 46/1000 | Loss: 0.00002511
Iteration 47/1000 | Loss: 0.00002511
Iteration 48/1000 | Loss: 0.00002510
Iteration 49/1000 | Loss: 0.00002510
Iteration 50/1000 | Loss: 0.00002510
Iteration 51/1000 | Loss: 0.00002506
Iteration 52/1000 | Loss: 0.00002504
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002503
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00002502
Iteration 60/1000 | Loss: 0.00002502
Iteration 61/1000 | Loss: 0.00002502
Iteration 62/1000 | Loss: 0.00002502
Iteration 63/1000 | Loss: 0.00002502
Iteration 64/1000 | Loss: 0.00002501
Iteration 65/1000 | Loss: 0.00002501
Iteration 66/1000 | Loss: 0.00002501
Iteration 67/1000 | Loss: 0.00002500
Iteration 68/1000 | Loss: 0.00002500
Iteration 69/1000 | Loss: 0.00002500
Iteration 70/1000 | Loss: 0.00002500
Iteration 71/1000 | Loss: 0.00002500
Iteration 72/1000 | Loss: 0.00002500
Iteration 73/1000 | Loss: 0.00002500
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002499
Iteration 78/1000 | Loss: 0.00002499
Iteration 79/1000 | Loss: 0.00002499
Iteration 80/1000 | Loss: 0.00002499
Iteration 81/1000 | Loss: 0.00002499
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002498
Iteration 85/1000 | Loss: 0.00002498
Iteration 86/1000 | Loss: 0.00002498
Iteration 87/1000 | Loss: 0.00002498
Iteration 88/1000 | Loss: 0.00002498
Iteration 89/1000 | Loss: 0.00002498
Iteration 90/1000 | Loss: 0.00002498
Iteration 91/1000 | Loss: 0.00002498
Iteration 92/1000 | Loss: 0.00002498
Iteration 93/1000 | Loss: 0.00002498
Iteration 94/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.4981487513286993e-05, 2.4981487513286993e-05, 2.4981487513286993e-05, 2.4981487513286993e-05, 2.4981487513286993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4981487513286993e-05

Optimization complete. Final v2v error: 4.058380603790283 mm

Highest mean error: 4.269684791564941 mm for frame 171

Lowest mean error: 3.904679536819458 mm for frame 105

Saving results

Total time: 51.88643789291382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792994
Iteration 2/25 | Loss: 0.00186407
Iteration 3/25 | Loss: 0.00126084
Iteration 4/25 | Loss: 0.00121295
Iteration 5/25 | Loss: 0.00118394
Iteration 6/25 | Loss: 0.00120119
Iteration 7/25 | Loss: 0.00117763
Iteration 8/25 | Loss: 0.00116976
Iteration 9/25 | Loss: 0.00117584
Iteration 10/25 | Loss: 0.00116950
Iteration 11/25 | Loss: 0.00116896
Iteration 12/25 | Loss: 0.00116895
Iteration 13/25 | Loss: 0.00116895
Iteration 14/25 | Loss: 0.00116895
Iteration 15/25 | Loss: 0.00116894
Iteration 16/25 | Loss: 0.00116894
Iteration 17/25 | Loss: 0.00116894
Iteration 18/25 | Loss: 0.00116894
Iteration 19/25 | Loss: 0.00116894
Iteration 20/25 | Loss: 0.00116894
Iteration 21/25 | Loss: 0.00116894
Iteration 22/25 | Loss: 0.00116894
Iteration 23/25 | Loss: 0.00116894
Iteration 24/25 | Loss: 0.00116894
Iteration 25/25 | Loss: 0.00116894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71925616
Iteration 2/25 | Loss: 0.00055140
Iteration 3/25 | Loss: 0.00055137
Iteration 4/25 | Loss: 0.00055137
Iteration 5/25 | Loss: 0.00055137
Iteration 6/25 | Loss: 0.00055137
Iteration 7/25 | Loss: 0.00055137
Iteration 8/25 | Loss: 0.00055137
Iteration 9/25 | Loss: 0.00055137
Iteration 10/25 | Loss: 0.00055137
Iteration 11/25 | Loss: 0.00055137
Iteration 12/25 | Loss: 0.00055137
Iteration 13/25 | Loss: 0.00055137
Iteration 14/25 | Loss: 0.00055137
Iteration 15/25 | Loss: 0.00055137
Iteration 16/25 | Loss: 0.00055137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005513678770512342, 0.0005513678770512342, 0.0005513678770512342, 0.0005513678770512342, 0.0005513678770512342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005513678770512342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055137
Iteration 2/1000 | Loss: 0.00003051
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00011452
Iteration 6/1000 | Loss: 0.00036078
Iteration 7/1000 | Loss: 0.00004393
Iteration 8/1000 | Loss: 0.00002611
Iteration 9/1000 | Loss: 0.00009501
Iteration 10/1000 | Loss: 0.00020565
Iteration 11/1000 | Loss: 0.00004846
Iteration 12/1000 | Loss: 0.00001790
Iteration 13/1000 | Loss: 0.00001756
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00009228
Iteration 21/1000 | Loss: 0.00001694
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001669
Iteration 26/1000 | Loss: 0.00001668
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001668
Iteration 29/1000 | Loss: 0.00001668
Iteration 30/1000 | Loss: 0.00001668
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001668
Iteration 33/1000 | Loss: 0.00001668
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001668
Iteration 36/1000 | Loss: 0.00001668
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001667
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001661
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001658
Iteration 65/1000 | Loss: 0.00001658
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001657
Iteration 68/1000 | Loss: 0.00001657
Iteration 69/1000 | Loss: 0.00001657
Iteration 70/1000 | Loss: 0.00001657
Iteration 71/1000 | Loss: 0.00001657
Iteration 72/1000 | Loss: 0.00001657
Iteration 73/1000 | Loss: 0.00001657
Iteration 74/1000 | Loss: 0.00001657
Iteration 75/1000 | Loss: 0.00001657
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001656
Iteration 78/1000 | Loss: 0.00001656
Iteration 79/1000 | Loss: 0.00001656
Iteration 80/1000 | Loss: 0.00001656
Iteration 81/1000 | Loss: 0.00001655
Iteration 82/1000 | Loss: 0.00001655
Iteration 83/1000 | Loss: 0.00001655
Iteration 84/1000 | Loss: 0.00001655
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001654
Iteration 94/1000 | Loss: 0.00001654
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001653
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001652
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001651
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.648619945626706e-05, 1.648619945626706e-05, 1.648619945626706e-05, 1.648619945626706e-05, 1.648619945626706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.648619945626706e-05

Optimization complete. Final v2v error: 3.392456531524658 mm

Highest mean error: 3.787921667098999 mm for frame 134

Lowest mean error: 2.883981466293335 mm for frame 178

Saving results

Total time: 54.39028310775757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770860
Iteration 2/25 | Loss: 0.00159505
Iteration 3/25 | Loss: 0.00126334
Iteration 4/25 | Loss: 0.00123350
Iteration 5/25 | Loss: 0.00122494
Iteration 6/25 | Loss: 0.00122473
Iteration 7/25 | Loss: 0.00121979
Iteration 8/25 | Loss: 0.00121778
Iteration 9/25 | Loss: 0.00121695
Iteration 10/25 | Loss: 0.00121235
Iteration 11/25 | Loss: 0.00121741
Iteration 12/25 | Loss: 0.00121469
Iteration 13/25 | Loss: 0.00121333
Iteration 14/25 | Loss: 0.00121080
Iteration 15/25 | Loss: 0.00121034
Iteration 16/25 | Loss: 0.00121025
Iteration 17/25 | Loss: 0.00121025
Iteration 18/25 | Loss: 0.00121025
Iteration 19/25 | Loss: 0.00121025
Iteration 20/25 | Loss: 0.00121025
Iteration 21/25 | Loss: 0.00121025
Iteration 22/25 | Loss: 0.00121025
Iteration 23/25 | Loss: 0.00121025
Iteration 24/25 | Loss: 0.00121025
Iteration 25/25 | Loss: 0.00121025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37852740
Iteration 2/25 | Loss: 0.00065821
Iteration 3/25 | Loss: 0.00065818
Iteration 4/25 | Loss: 0.00065818
Iteration 5/25 | Loss: 0.00065818
Iteration 6/25 | Loss: 0.00065818
Iteration 7/25 | Loss: 0.00065818
Iteration 8/25 | Loss: 0.00065818
Iteration 9/25 | Loss: 0.00065818
Iteration 10/25 | Loss: 0.00065818
Iteration 11/25 | Loss: 0.00065818
Iteration 12/25 | Loss: 0.00065818
Iteration 13/25 | Loss: 0.00065818
Iteration 14/25 | Loss: 0.00065818
Iteration 15/25 | Loss: 0.00065818
Iteration 16/25 | Loss: 0.00065818
Iteration 17/25 | Loss: 0.00065818
Iteration 18/25 | Loss: 0.00065818
Iteration 19/25 | Loss: 0.00065818
Iteration 20/25 | Loss: 0.00065818
Iteration 21/25 | Loss: 0.00065818
Iteration 22/25 | Loss: 0.00065818
Iteration 23/25 | Loss: 0.00065818
Iteration 24/25 | Loss: 0.00065818
Iteration 25/25 | Loss: 0.00065818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065818
Iteration 2/1000 | Loss: 0.00003523
Iteration 3/1000 | Loss: 0.00002507
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002101
Iteration 7/1000 | Loss: 0.00002047
Iteration 8/1000 | Loss: 0.00002012
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001975
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001971
Iteration 16/1000 | Loss: 0.00001963
Iteration 17/1000 | Loss: 0.00001963
Iteration 18/1000 | Loss: 0.00001953
Iteration 19/1000 | Loss: 0.00001951
Iteration 20/1000 | Loss: 0.00001950
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001946
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001937
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001937
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001937
Iteration 43/1000 | Loss: 0.00001936
Iteration 44/1000 | Loss: 0.00001935
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001934
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001933
Iteration 49/1000 | Loss: 0.00001933
Iteration 50/1000 | Loss: 0.00001933
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001932
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001932
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001932
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001927
Iteration 99/1000 | Loss: 0.00001927
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001926
Iteration 113/1000 | Loss: 0.00001926
Iteration 114/1000 | Loss: 0.00001926
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001925
Iteration 117/1000 | Loss: 0.00001925
Iteration 118/1000 | Loss: 0.00001925
Iteration 119/1000 | Loss: 0.00001925
Iteration 120/1000 | Loss: 0.00001925
Iteration 121/1000 | Loss: 0.00001925
Iteration 122/1000 | Loss: 0.00001924
Iteration 123/1000 | Loss: 0.00001924
Iteration 124/1000 | Loss: 0.00001924
Iteration 125/1000 | Loss: 0.00001924
Iteration 126/1000 | Loss: 0.00001924
Iteration 127/1000 | Loss: 0.00001924
Iteration 128/1000 | Loss: 0.00001924
Iteration 129/1000 | Loss: 0.00001924
Iteration 130/1000 | Loss: 0.00001924
Iteration 131/1000 | Loss: 0.00001924
Iteration 132/1000 | Loss: 0.00001924
Iteration 133/1000 | Loss: 0.00001924
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001923
Iteration 136/1000 | Loss: 0.00001923
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001923
Iteration 139/1000 | Loss: 0.00001923
Iteration 140/1000 | Loss: 0.00001923
Iteration 141/1000 | Loss: 0.00001923
Iteration 142/1000 | Loss: 0.00001923
Iteration 143/1000 | Loss: 0.00001923
Iteration 144/1000 | Loss: 0.00001923
Iteration 145/1000 | Loss: 0.00001923
Iteration 146/1000 | Loss: 0.00001923
Iteration 147/1000 | Loss: 0.00001923
Iteration 148/1000 | Loss: 0.00001923
Iteration 149/1000 | Loss: 0.00001923
Iteration 150/1000 | Loss: 0.00001923
Iteration 151/1000 | Loss: 0.00001923
Iteration 152/1000 | Loss: 0.00001923
Iteration 153/1000 | Loss: 0.00001923
Iteration 154/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.9233884813729674e-05, 1.9233884813729674e-05, 1.9233884813729674e-05, 1.9233884813729674e-05, 1.9233884813729674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9233884813729674e-05

Optimization complete. Final v2v error: 3.5749549865722656 mm

Highest mean error: 5.523439884185791 mm for frame 65

Lowest mean error: 3.148991346359253 mm for frame 193

Saving results

Total time: 59.26221585273743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383487
Iteration 2/25 | Loss: 0.00116238
Iteration 3/25 | Loss: 0.00107587
Iteration 4/25 | Loss: 0.00105741
Iteration 5/25 | Loss: 0.00105014
Iteration 6/25 | Loss: 0.00104823
Iteration 7/25 | Loss: 0.00104768
Iteration 8/25 | Loss: 0.00104763
Iteration 9/25 | Loss: 0.00104763
Iteration 10/25 | Loss: 0.00104763
Iteration 11/25 | Loss: 0.00104763
Iteration 12/25 | Loss: 0.00104763
Iteration 13/25 | Loss: 0.00104763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010476340539753437, 0.0010476340539753437, 0.0010476340539753437, 0.0010476340539753437, 0.0010476340539753437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010476340539753437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44898379
Iteration 2/25 | Loss: 0.00080373
Iteration 3/25 | Loss: 0.00080372
Iteration 4/25 | Loss: 0.00080372
Iteration 5/25 | Loss: 0.00080372
Iteration 6/25 | Loss: 0.00080372
Iteration 7/25 | Loss: 0.00080372
Iteration 8/25 | Loss: 0.00080372
Iteration 9/25 | Loss: 0.00080372
Iteration 10/25 | Loss: 0.00080372
Iteration 11/25 | Loss: 0.00080372
Iteration 12/25 | Loss: 0.00080372
Iteration 13/25 | Loss: 0.00080372
Iteration 14/25 | Loss: 0.00080372
Iteration 15/25 | Loss: 0.00080372
Iteration 16/25 | Loss: 0.00080372
Iteration 17/25 | Loss: 0.00080372
Iteration 18/25 | Loss: 0.00080372
Iteration 19/25 | Loss: 0.00080372
Iteration 20/25 | Loss: 0.00080372
Iteration 21/25 | Loss: 0.00080372
Iteration 22/25 | Loss: 0.00080372
Iteration 23/25 | Loss: 0.00080372
Iteration 24/25 | Loss: 0.00080372
Iteration 25/25 | Loss: 0.00080372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080372
Iteration 2/1000 | Loss: 0.00005536
Iteration 3/1000 | Loss: 0.00003343
Iteration 4/1000 | Loss: 0.00002630
Iteration 5/1000 | Loss: 0.00002350
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002003
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001851
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001774
Iteration 16/1000 | Loss: 0.00001772
Iteration 17/1000 | Loss: 0.00001771
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001753
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001737
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001736
Iteration 38/1000 | Loss: 0.00001736
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001733
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001732
Iteration 54/1000 | Loss: 0.00001732
Iteration 55/1000 | Loss: 0.00001731
Iteration 56/1000 | Loss: 0.00001731
Iteration 57/1000 | Loss: 0.00001731
Iteration 58/1000 | Loss: 0.00001731
Iteration 59/1000 | Loss: 0.00001731
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001730
Iteration 64/1000 | Loss: 0.00001730
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00001729
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001728
Iteration 88/1000 | Loss: 0.00001728
Iteration 89/1000 | Loss: 0.00001728
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001728
Iteration 95/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.7284641216974705e-05, 1.7284641216974705e-05, 1.7284641216974705e-05, 1.7284641216974705e-05, 1.7284641216974705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7284641216974705e-05

Optimization complete. Final v2v error: 3.3550477027893066 mm

Highest mean error: 4.768889904022217 mm for frame 105

Lowest mean error: 2.2738611698150635 mm for frame 64

Saving results

Total time: 38.211487770080566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406719
Iteration 2/25 | Loss: 0.00122030
Iteration 3/25 | Loss: 0.00111594
Iteration 4/25 | Loss: 0.00109942
Iteration 5/25 | Loss: 0.00109456
Iteration 6/25 | Loss: 0.00109369
Iteration 7/25 | Loss: 0.00109360
Iteration 8/25 | Loss: 0.00109360
Iteration 9/25 | Loss: 0.00109360
Iteration 10/25 | Loss: 0.00109360
Iteration 11/25 | Loss: 0.00109360
Iteration 12/25 | Loss: 0.00109360
Iteration 13/25 | Loss: 0.00109360
Iteration 14/25 | Loss: 0.00109360
Iteration 15/25 | Loss: 0.00109360
Iteration 16/25 | Loss: 0.00109360
Iteration 17/25 | Loss: 0.00109360
Iteration 18/25 | Loss: 0.00109360
Iteration 19/25 | Loss: 0.00109360
Iteration 20/25 | Loss: 0.00109360
Iteration 21/25 | Loss: 0.00109360
Iteration 22/25 | Loss: 0.00109360
Iteration 23/25 | Loss: 0.00109360
Iteration 24/25 | Loss: 0.00109360
Iteration 25/25 | Loss: 0.00109360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02016568
Iteration 2/25 | Loss: 0.00066481
Iteration 3/25 | Loss: 0.00066481
Iteration 4/25 | Loss: 0.00066481
Iteration 5/25 | Loss: 0.00066481
Iteration 6/25 | Loss: 0.00066481
Iteration 7/25 | Loss: 0.00066481
Iteration 8/25 | Loss: 0.00066481
Iteration 9/25 | Loss: 0.00066481
Iteration 10/25 | Loss: 0.00066481
Iteration 11/25 | Loss: 0.00066481
Iteration 12/25 | Loss: 0.00066481
Iteration 13/25 | Loss: 0.00066481
Iteration 14/25 | Loss: 0.00066481
Iteration 15/25 | Loss: 0.00066481
Iteration 16/25 | Loss: 0.00066481
Iteration 17/25 | Loss: 0.00066481
Iteration 18/25 | Loss: 0.00066481
Iteration 19/25 | Loss: 0.00066481
Iteration 20/25 | Loss: 0.00066481
Iteration 21/25 | Loss: 0.00066481
Iteration 22/25 | Loss: 0.00066481
Iteration 23/25 | Loss: 0.00066481
Iteration 24/25 | Loss: 0.00066481
Iteration 25/25 | Loss: 0.00066481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066481
Iteration 2/1000 | Loss: 0.00002948
Iteration 3/1000 | Loss: 0.00002263
Iteration 4/1000 | Loss: 0.00001977
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001769
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001652
Iteration 12/1000 | Loss: 0.00001643
Iteration 13/1000 | Loss: 0.00001640
Iteration 14/1000 | Loss: 0.00001639
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001631
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001626
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001624
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001620
Iteration 34/1000 | Loss: 0.00001620
Iteration 35/1000 | Loss: 0.00001620
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001620
Iteration 39/1000 | Loss: 0.00001619
Iteration 40/1000 | Loss: 0.00001618
Iteration 41/1000 | Loss: 0.00001617
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001615
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001609
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001607
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001603
Iteration 60/1000 | Loss: 0.00001602
Iteration 61/1000 | Loss: 0.00001602
Iteration 62/1000 | Loss: 0.00001602
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001601
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001599
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001599
Iteration 74/1000 | Loss: 0.00001599
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001599
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001598
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001597
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001596
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001593
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001592
Iteration 110/1000 | Loss: 0.00001592
Iteration 111/1000 | Loss: 0.00001592
Iteration 112/1000 | Loss: 0.00001592
Iteration 113/1000 | Loss: 0.00001592
Iteration 114/1000 | Loss: 0.00001592
Iteration 115/1000 | Loss: 0.00001592
Iteration 116/1000 | Loss: 0.00001592
Iteration 117/1000 | Loss: 0.00001592
Iteration 118/1000 | Loss: 0.00001592
Iteration 119/1000 | Loss: 0.00001591
Iteration 120/1000 | Loss: 0.00001591
Iteration 121/1000 | Loss: 0.00001591
Iteration 122/1000 | Loss: 0.00001591
Iteration 123/1000 | Loss: 0.00001591
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001590
Iteration 127/1000 | Loss: 0.00001589
Iteration 128/1000 | Loss: 0.00001589
Iteration 129/1000 | Loss: 0.00001589
Iteration 130/1000 | Loss: 0.00001589
Iteration 131/1000 | Loss: 0.00001589
Iteration 132/1000 | Loss: 0.00001589
Iteration 133/1000 | Loss: 0.00001589
Iteration 134/1000 | Loss: 0.00001589
Iteration 135/1000 | Loss: 0.00001589
Iteration 136/1000 | Loss: 0.00001589
Iteration 137/1000 | Loss: 0.00001589
Iteration 138/1000 | Loss: 0.00001589
Iteration 139/1000 | Loss: 0.00001589
Iteration 140/1000 | Loss: 0.00001589
Iteration 141/1000 | Loss: 0.00001589
Iteration 142/1000 | Loss: 0.00001589
Iteration 143/1000 | Loss: 0.00001589
Iteration 144/1000 | Loss: 0.00001589
Iteration 145/1000 | Loss: 0.00001589
Iteration 146/1000 | Loss: 0.00001589
Iteration 147/1000 | Loss: 0.00001589
Iteration 148/1000 | Loss: 0.00001589
Iteration 149/1000 | Loss: 0.00001589
Iteration 150/1000 | Loss: 0.00001589
Iteration 151/1000 | Loss: 0.00001589
Iteration 152/1000 | Loss: 0.00001589
Iteration 153/1000 | Loss: 0.00001589
Iteration 154/1000 | Loss: 0.00001589
Iteration 155/1000 | Loss: 0.00001589
Iteration 156/1000 | Loss: 0.00001589
Iteration 157/1000 | Loss: 0.00001589
Iteration 158/1000 | Loss: 0.00001589
Iteration 159/1000 | Loss: 0.00001589
Iteration 160/1000 | Loss: 0.00001589
Iteration 161/1000 | Loss: 0.00001589
Iteration 162/1000 | Loss: 0.00001589
Iteration 163/1000 | Loss: 0.00001589
Iteration 164/1000 | Loss: 0.00001589
Iteration 165/1000 | Loss: 0.00001589
Iteration 166/1000 | Loss: 0.00001589
Iteration 167/1000 | Loss: 0.00001589
Iteration 168/1000 | Loss: 0.00001589
Iteration 169/1000 | Loss: 0.00001589
Iteration 170/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.5885756511124782e-05, 1.5885756511124782e-05, 1.5885756511124782e-05, 1.5885756511124782e-05, 1.5885756511124782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5885756511124782e-05

Optimization complete. Final v2v error: 3.351499319076538 mm

Highest mean error: 3.935636043548584 mm for frame 29

Lowest mean error: 3.0585100650787354 mm for frame 60

Saving results

Total time: 38.10653328895569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398475
Iteration 2/25 | Loss: 0.00110430
Iteration 3/25 | Loss: 0.00103277
Iteration 4/25 | Loss: 0.00102236
Iteration 5/25 | Loss: 0.00101946
Iteration 6/25 | Loss: 0.00101946
Iteration 7/25 | Loss: 0.00101946
Iteration 8/25 | Loss: 0.00101946
Iteration 9/25 | Loss: 0.00101946
Iteration 10/25 | Loss: 0.00101946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00101945991627872, 0.00101945991627872, 0.00101945991627872, 0.00101945991627872, 0.00101945991627872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00101945991627872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98868608
Iteration 2/25 | Loss: 0.00062163
Iteration 3/25 | Loss: 0.00062163
Iteration 4/25 | Loss: 0.00062163
Iteration 5/25 | Loss: 0.00062163
Iteration 6/25 | Loss: 0.00062163
Iteration 7/25 | Loss: 0.00062163
Iteration 8/25 | Loss: 0.00062163
Iteration 9/25 | Loss: 0.00062163
Iteration 10/25 | Loss: 0.00062163
Iteration 11/25 | Loss: 0.00062163
Iteration 12/25 | Loss: 0.00062162
Iteration 13/25 | Loss: 0.00062162
Iteration 14/25 | Loss: 0.00062162
Iteration 15/25 | Loss: 0.00062162
Iteration 16/25 | Loss: 0.00062162
Iteration 17/25 | Loss: 0.00062162
Iteration 18/25 | Loss: 0.00062162
Iteration 19/25 | Loss: 0.00062162
Iteration 20/25 | Loss: 0.00062162
Iteration 21/25 | Loss: 0.00062162
Iteration 22/25 | Loss: 0.00062162
Iteration 23/25 | Loss: 0.00062162
Iteration 24/25 | Loss: 0.00062162
Iteration 25/25 | Loss: 0.00062162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062162
Iteration 2/1000 | Loss: 0.00001608
Iteration 3/1000 | Loss: 0.00001222
Iteration 4/1000 | Loss: 0.00001127
Iteration 5/1000 | Loss: 0.00001084
Iteration 6/1000 | Loss: 0.00001043
Iteration 7/1000 | Loss: 0.00001016
Iteration 8/1000 | Loss: 0.00000993
Iteration 9/1000 | Loss: 0.00000976
Iteration 10/1000 | Loss: 0.00000964
Iteration 11/1000 | Loss: 0.00000960
Iteration 12/1000 | Loss: 0.00000959
Iteration 13/1000 | Loss: 0.00000959
Iteration 14/1000 | Loss: 0.00000958
Iteration 15/1000 | Loss: 0.00000954
Iteration 16/1000 | Loss: 0.00000948
Iteration 17/1000 | Loss: 0.00000945
Iteration 18/1000 | Loss: 0.00000944
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000942
Iteration 21/1000 | Loss: 0.00000942
Iteration 22/1000 | Loss: 0.00000941
Iteration 23/1000 | Loss: 0.00000941
Iteration 24/1000 | Loss: 0.00000941
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000941
Iteration 27/1000 | Loss: 0.00000941
Iteration 28/1000 | Loss: 0.00000940
Iteration 29/1000 | Loss: 0.00000940
Iteration 30/1000 | Loss: 0.00000938
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000935
Iteration 34/1000 | Loss: 0.00000935
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000933
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000930
Iteration 43/1000 | Loss: 0.00000928
Iteration 44/1000 | Loss: 0.00000927
Iteration 45/1000 | Loss: 0.00000927
Iteration 46/1000 | Loss: 0.00000926
Iteration 47/1000 | Loss: 0.00000925
Iteration 48/1000 | Loss: 0.00000925
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000922
Iteration 51/1000 | Loss: 0.00000921
Iteration 52/1000 | Loss: 0.00000921
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000919
Iteration 55/1000 | Loss: 0.00000918
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000917
Iteration 58/1000 | Loss: 0.00000917
Iteration 59/1000 | Loss: 0.00000916
Iteration 60/1000 | Loss: 0.00000916
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000915
Iteration 63/1000 | Loss: 0.00000915
Iteration 64/1000 | Loss: 0.00000915
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000913
Iteration 73/1000 | Loss: 0.00000913
Iteration 74/1000 | Loss: 0.00000913
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000912
Iteration 79/1000 | Loss: 0.00000912
Iteration 80/1000 | Loss: 0.00000912
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000911
Iteration 83/1000 | Loss: 0.00000911
Iteration 84/1000 | Loss: 0.00000911
Iteration 85/1000 | Loss: 0.00000911
Iteration 86/1000 | Loss: 0.00000911
Iteration 87/1000 | Loss: 0.00000911
Iteration 88/1000 | Loss: 0.00000911
Iteration 89/1000 | Loss: 0.00000911
Iteration 90/1000 | Loss: 0.00000910
Iteration 91/1000 | Loss: 0.00000910
Iteration 92/1000 | Loss: 0.00000910
Iteration 93/1000 | Loss: 0.00000910
Iteration 94/1000 | Loss: 0.00000910
Iteration 95/1000 | Loss: 0.00000910
Iteration 96/1000 | Loss: 0.00000909
Iteration 97/1000 | Loss: 0.00000909
Iteration 98/1000 | Loss: 0.00000908
Iteration 99/1000 | Loss: 0.00000908
Iteration 100/1000 | Loss: 0.00000907
Iteration 101/1000 | Loss: 0.00000907
Iteration 102/1000 | Loss: 0.00000907
Iteration 103/1000 | Loss: 0.00000907
Iteration 104/1000 | Loss: 0.00000906
Iteration 105/1000 | Loss: 0.00000906
Iteration 106/1000 | Loss: 0.00000906
Iteration 107/1000 | Loss: 0.00000905
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000903
Iteration 110/1000 | Loss: 0.00000903
Iteration 111/1000 | Loss: 0.00000903
Iteration 112/1000 | Loss: 0.00000903
Iteration 113/1000 | Loss: 0.00000902
Iteration 114/1000 | Loss: 0.00000902
Iteration 115/1000 | Loss: 0.00000902
Iteration 116/1000 | Loss: 0.00000902
Iteration 117/1000 | Loss: 0.00000902
Iteration 118/1000 | Loss: 0.00000901
Iteration 119/1000 | Loss: 0.00000901
Iteration 120/1000 | Loss: 0.00000901
Iteration 121/1000 | Loss: 0.00000900
Iteration 122/1000 | Loss: 0.00000900
Iteration 123/1000 | Loss: 0.00000900
Iteration 124/1000 | Loss: 0.00000900
Iteration 125/1000 | Loss: 0.00000900
Iteration 126/1000 | Loss: 0.00000900
Iteration 127/1000 | Loss: 0.00000899
Iteration 128/1000 | Loss: 0.00000899
Iteration 129/1000 | Loss: 0.00000899
Iteration 130/1000 | Loss: 0.00000899
Iteration 131/1000 | Loss: 0.00000899
Iteration 132/1000 | Loss: 0.00000899
Iteration 133/1000 | Loss: 0.00000899
Iteration 134/1000 | Loss: 0.00000899
Iteration 135/1000 | Loss: 0.00000899
Iteration 136/1000 | Loss: 0.00000898
Iteration 137/1000 | Loss: 0.00000898
Iteration 138/1000 | Loss: 0.00000898
Iteration 139/1000 | Loss: 0.00000897
Iteration 140/1000 | Loss: 0.00000897
Iteration 141/1000 | Loss: 0.00000897
Iteration 142/1000 | Loss: 0.00000897
Iteration 143/1000 | Loss: 0.00000896
Iteration 144/1000 | Loss: 0.00000896
Iteration 145/1000 | Loss: 0.00000896
Iteration 146/1000 | Loss: 0.00000896
Iteration 147/1000 | Loss: 0.00000896
Iteration 148/1000 | Loss: 0.00000896
Iteration 149/1000 | Loss: 0.00000896
Iteration 150/1000 | Loss: 0.00000896
Iteration 151/1000 | Loss: 0.00000896
Iteration 152/1000 | Loss: 0.00000896
Iteration 153/1000 | Loss: 0.00000896
Iteration 154/1000 | Loss: 0.00000896
Iteration 155/1000 | Loss: 0.00000896
Iteration 156/1000 | Loss: 0.00000896
Iteration 157/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [8.960025297710672e-06, 8.960025297710672e-06, 8.960025297710672e-06, 8.960025297710672e-06, 8.960025297710672e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.960025297710672e-06

Optimization complete. Final v2v error: 2.588711977005005 mm

Highest mean error: 2.8584389686584473 mm for frame 139

Lowest mean error: 2.4679641723632812 mm for frame 202

Saving results

Total time: 40.74339461326599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868828
Iteration 2/25 | Loss: 0.00174369
Iteration 3/25 | Loss: 0.00137864
Iteration 4/25 | Loss: 0.00129780
Iteration 5/25 | Loss: 0.00134561
Iteration 6/25 | Loss: 0.00128114
Iteration 7/25 | Loss: 0.00120168
Iteration 8/25 | Loss: 0.00117499
Iteration 9/25 | Loss: 0.00118255
Iteration 10/25 | Loss: 0.00118414
Iteration 11/25 | Loss: 0.00116499
Iteration 12/25 | Loss: 0.00116113
Iteration 13/25 | Loss: 0.00115250
Iteration 14/25 | Loss: 0.00114648
Iteration 15/25 | Loss: 0.00114431
Iteration 16/25 | Loss: 0.00114385
Iteration 17/25 | Loss: 0.00114373
Iteration 18/25 | Loss: 0.00114373
Iteration 19/25 | Loss: 0.00114373
Iteration 20/25 | Loss: 0.00114373
Iteration 21/25 | Loss: 0.00114373
Iteration 22/25 | Loss: 0.00114372
Iteration 23/25 | Loss: 0.00114372
Iteration 24/25 | Loss: 0.00114372
Iteration 25/25 | Loss: 0.00114372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.54879665
Iteration 2/25 | Loss: 0.00080770
Iteration 3/25 | Loss: 0.00080770
Iteration 4/25 | Loss: 0.00080770
Iteration 5/25 | Loss: 0.00080770
Iteration 6/25 | Loss: 0.00080770
Iteration 7/25 | Loss: 0.00080770
Iteration 8/25 | Loss: 0.00080770
Iteration 9/25 | Loss: 0.00080770
Iteration 10/25 | Loss: 0.00080770
Iteration 11/25 | Loss: 0.00080770
Iteration 12/25 | Loss: 0.00080769
Iteration 13/25 | Loss: 0.00080769
Iteration 14/25 | Loss: 0.00080769
Iteration 15/25 | Loss: 0.00080769
Iteration 16/25 | Loss: 0.00080769
Iteration 17/25 | Loss: 0.00080769
Iteration 18/25 | Loss: 0.00080769
Iteration 19/25 | Loss: 0.00080769
Iteration 20/25 | Loss: 0.00080769
Iteration 21/25 | Loss: 0.00080769
Iteration 22/25 | Loss: 0.00080769
Iteration 23/25 | Loss: 0.00080769
Iteration 24/25 | Loss: 0.00080769
Iteration 25/25 | Loss: 0.00080769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080769
Iteration 2/1000 | Loss: 0.00005765
Iteration 3/1000 | Loss: 0.00017163
Iteration 4/1000 | Loss: 0.00002500
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002048
Iteration 7/1000 | Loss: 0.00001925
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001717
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00026898
Iteration 17/1000 | Loss: 0.00004784
Iteration 18/1000 | Loss: 0.00005928
Iteration 19/1000 | Loss: 0.00003759
Iteration 20/1000 | Loss: 0.00001941
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001771
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001628
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001614
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001605
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00001604
Iteration 41/1000 | Loss: 0.00001604
Iteration 42/1000 | Loss: 0.00001603
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001602
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001598
Iteration 55/1000 | Loss: 0.00001596
Iteration 56/1000 | Loss: 0.00001596
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001596
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001596
Iteration 65/1000 | Loss: 0.00001595
Iteration 66/1000 | Loss: 0.00001595
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001595
Iteration 70/1000 | Loss: 0.00001594
Iteration 71/1000 | Loss: 0.00001594
Iteration 72/1000 | Loss: 0.00001594
Iteration 73/1000 | Loss: 0.00001594
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00001593
Iteration 77/1000 | Loss: 0.00001593
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001592
Iteration 84/1000 | Loss: 0.00001592
Iteration 85/1000 | Loss: 0.00001592
Iteration 86/1000 | Loss: 0.00001592
Iteration 87/1000 | Loss: 0.00001592
Iteration 88/1000 | Loss: 0.00001592
Iteration 89/1000 | Loss: 0.00001592
Iteration 90/1000 | Loss: 0.00001592
Iteration 91/1000 | Loss: 0.00001592
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001592
Iteration 95/1000 | Loss: 0.00001592
Iteration 96/1000 | Loss: 0.00001592
Iteration 97/1000 | Loss: 0.00001592
Iteration 98/1000 | Loss: 0.00001592
Iteration 99/1000 | Loss: 0.00001592
Iteration 100/1000 | Loss: 0.00001592
Iteration 101/1000 | Loss: 0.00001591
Iteration 102/1000 | Loss: 0.00001591
Iteration 103/1000 | Loss: 0.00001591
Iteration 104/1000 | Loss: 0.00001591
Iteration 105/1000 | Loss: 0.00001591
Iteration 106/1000 | Loss: 0.00001591
Iteration 107/1000 | Loss: 0.00001591
Iteration 108/1000 | Loss: 0.00001591
Iteration 109/1000 | Loss: 0.00001591
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001591
Iteration 113/1000 | Loss: 0.00001591
Iteration 114/1000 | Loss: 0.00001591
Iteration 115/1000 | Loss: 0.00001591
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001590
Iteration 119/1000 | Loss: 0.00001590
Iteration 120/1000 | Loss: 0.00001590
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001590
Iteration 123/1000 | Loss: 0.00001590
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001590
Iteration 127/1000 | Loss: 0.00001590
Iteration 128/1000 | Loss: 0.00001590
Iteration 129/1000 | Loss: 0.00001590
Iteration 130/1000 | Loss: 0.00001590
Iteration 131/1000 | Loss: 0.00001590
Iteration 132/1000 | Loss: 0.00001590
Iteration 133/1000 | Loss: 0.00001590
Iteration 134/1000 | Loss: 0.00001590
Iteration 135/1000 | Loss: 0.00001590
Iteration 136/1000 | Loss: 0.00001590
Iteration 137/1000 | Loss: 0.00001589
Iteration 138/1000 | Loss: 0.00001589
Iteration 139/1000 | Loss: 0.00001589
Iteration 140/1000 | Loss: 0.00001589
Iteration 141/1000 | Loss: 0.00001589
Iteration 142/1000 | Loss: 0.00001589
Iteration 143/1000 | Loss: 0.00001589
Iteration 144/1000 | Loss: 0.00001589
Iteration 145/1000 | Loss: 0.00001589
Iteration 146/1000 | Loss: 0.00001589
Iteration 147/1000 | Loss: 0.00001589
Iteration 148/1000 | Loss: 0.00001589
Iteration 149/1000 | Loss: 0.00001589
Iteration 150/1000 | Loss: 0.00001589
Iteration 151/1000 | Loss: 0.00001588
Iteration 152/1000 | Loss: 0.00001588
Iteration 153/1000 | Loss: 0.00001588
Iteration 154/1000 | Loss: 0.00001588
Iteration 155/1000 | Loss: 0.00001588
Iteration 156/1000 | Loss: 0.00001588
Iteration 157/1000 | Loss: 0.00001588
Iteration 158/1000 | Loss: 0.00001588
Iteration 159/1000 | Loss: 0.00001588
Iteration 160/1000 | Loss: 0.00001588
Iteration 161/1000 | Loss: 0.00001588
Iteration 162/1000 | Loss: 0.00001588
Iteration 163/1000 | Loss: 0.00001588
Iteration 164/1000 | Loss: 0.00001588
Iteration 165/1000 | Loss: 0.00001588
Iteration 166/1000 | Loss: 0.00001588
Iteration 167/1000 | Loss: 0.00001588
Iteration 168/1000 | Loss: 0.00001588
Iteration 169/1000 | Loss: 0.00001587
Iteration 170/1000 | Loss: 0.00001587
Iteration 171/1000 | Loss: 0.00001587
Iteration 172/1000 | Loss: 0.00001587
Iteration 173/1000 | Loss: 0.00001587
Iteration 174/1000 | Loss: 0.00001587
Iteration 175/1000 | Loss: 0.00001587
Iteration 176/1000 | Loss: 0.00001587
Iteration 177/1000 | Loss: 0.00001587
Iteration 178/1000 | Loss: 0.00001587
Iteration 179/1000 | Loss: 0.00001587
Iteration 180/1000 | Loss: 0.00001587
Iteration 181/1000 | Loss: 0.00001586
Iteration 182/1000 | Loss: 0.00001586
Iteration 183/1000 | Loss: 0.00001586
Iteration 184/1000 | Loss: 0.00001586
Iteration 185/1000 | Loss: 0.00001586
Iteration 186/1000 | Loss: 0.00001586
Iteration 187/1000 | Loss: 0.00001585
Iteration 188/1000 | Loss: 0.00001585
Iteration 189/1000 | Loss: 0.00001585
Iteration 190/1000 | Loss: 0.00001585
Iteration 191/1000 | Loss: 0.00001585
Iteration 192/1000 | Loss: 0.00001585
Iteration 193/1000 | Loss: 0.00001585
Iteration 194/1000 | Loss: 0.00001585
Iteration 195/1000 | Loss: 0.00001585
Iteration 196/1000 | Loss: 0.00001585
Iteration 197/1000 | Loss: 0.00001584
Iteration 198/1000 | Loss: 0.00001584
Iteration 199/1000 | Loss: 0.00001584
Iteration 200/1000 | Loss: 0.00001584
Iteration 201/1000 | Loss: 0.00001584
Iteration 202/1000 | Loss: 0.00001584
Iteration 203/1000 | Loss: 0.00001584
Iteration 204/1000 | Loss: 0.00001584
Iteration 205/1000 | Loss: 0.00001584
Iteration 206/1000 | Loss: 0.00001583
Iteration 207/1000 | Loss: 0.00001583
Iteration 208/1000 | Loss: 0.00001583
Iteration 209/1000 | Loss: 0.00001583
Iteration 210/1000 | Loss: 0.00001583
Iteration 211/1000 | Loss: 0.00001583
Iteration 212/1000 | Loss: 0.00001583
Iteration 213/1000 | Loss: 0.00001583
Iteration 214/1000 | Loss: 0.00001583
Iteration 215/1000 | Loss: 0.00001583
Iteration 216/1000 | Loss: 0.00001583
Iteration 217/1000 | Loss: 0.00001582
Iteration 218/1000 | Loss: 0.00001582
Iteration 219/1000 | Loss: 0.00001582
Iteration 220/1000 | Loss: 0.00001582
Iteration 221/1000 | Loss: 0.00001582
Iteration 222/1000 | Loss: 0.00001582
Iteration 223/1000 | Loss: 0.00001582
Iteration 224/1000 | Loss: 0.00001582
Iteration 225/1000 | Loss: 0.00001582
Iteration 226/1000 | Loss: 0.00001582
Iteration 227/1000 | Loss: 0.00001582
Iteration 228/1000 | Loss: 0.00001581
Iteration 229/1000 | Loss: 0.00001581
Iteration 230/1000 | Loss: 0.00001581
Iteration 231/1000 | Loss: 0.00001581
Iteration 232/1000 | Loss: 0.00001581
Iteration 233/1000 | Loss: 0.00001581
Iteration 234/1000 | Loss: 0.00001581
Iteration 235/1000 | Loss: 0.00001581
Iteration 236/1000 | Loss: 0.00001581
Iteration 237/1000 | Loss: 0.00001581
Iteration 238/1000 | Loss: 0.00001581
Iteration 239/1000 | Loss: 0.00001581
Iteration 240/1000 | Loss: 0.00001581
Iteration 241/1000 | Loss: 0.00001581
Iteration 242/1000 | Loss: 0.00001581
Iteration 243/1000 | Loss: 0.00001581
Iteration 244/1000 | Loss: 0.00001581
Iteration 245/1000 | Loss: 0.00001581
Iteration 246/1000 | Loss: 0.00001581
Iteration 247/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.5808331227162853e-05, 1.5808331227162853e-05, 1.5808331227162853e-05, 1.5808331227162853e-05, 1.5808331227162853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5808331227162853e-05

Optimization complete. Final v2v error: 3.263094902038574 mm

Highest mean error: 5.205251216888428 mm for frame 95

Lowest mean error: 2.8183090686798096 mm for frame 52

Saving results

Total time: 80.4034218788147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401484
Iteration 2/25 | Loss: 0.00123126
Iteration 3/25 | Loss: 0.00108021
Iteration 4/25 | Loss: 0.00106626
Iteration 5/25 | Loss: 0.00106436
Iteration 6/25 | Loss: 0.00106373
Iteration 7/25 | Loss: 0.00106370
Iteration 8/25 | Loss: 0.00106370
Iteration 9/25 | Loss: 0.00106370
Iteration 10/25 | Loss: 0.00106370
Iteration 11/25 | Loss: 0.00106370
Iteration 12/25 | Loss: 0.00106370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010636965744197369, 0.0010636965744197369, 0.0010636965744197369, 0.0010636965744197369, 0.0010636965744197369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010636965744197369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37057054
Iteration 2/25 | Loss: 0.00052882
Iteration 3/25 | Loss: 0.00052882
Iteration 4/25 | Loss: 0.00052882
Iteration 5/25 | Loss: 0.00052882
Iteration 6/25 | Loss: 0.00052882
Iteration 7/25 | Loss: 0.00052882
Iteration 8/25 | Loss: 0.00052882
Iteration 9/25 | Loss: 0.00052882
Iteration 10/25 | Loss: 0.00052882
Iteration 11/25 | Loss: 0.00052882
Iteration 12/25 | Loss: 0.00052882
Iteration 13/25 | Loss: 0.00052882
Iteration 14/25 | Loss: 0.00052882
Iteration 15/25 | Loss: 0.00052882
Iteration 16/25 | Loss: 0.00052882
Iteration 17/25 | Loss: 0.00052882
Iteration 18/25 | Loss: 0.00052882
Iteration 19/25 | Loss: 0.00052882
Iteration 20/25 | Loss: 0.00052882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005288163665682077, 0.0005288163665682077, 0.0005288163665682077, 0.0005288163665682077, 0.0005288163665682077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005288163665682077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052882
Iteration 2/1000 | Loss: 0.00002258
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001431
Iteration 5/1000 | Loss: 0.00001336
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001188
Iteration 10/1000 | Loss: 0.00001168
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001156
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001151
Iteration 18/1000 | Loss: 0.00001151
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001147
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001145
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001145
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001144
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001142
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001136
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001131
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001127
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001124
Iteration 67/1000 | Loss: 0.00001124
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001121
Iteration 93/1000 | Loss: 0.00001121
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001114
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001108
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001106
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001103
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001101
Iteration 140/1000 | Loss: 0.00001101
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001099
Iteration 147/1000 | Loss: 0.00001099
Iteration 148/1000 | Loss: 0.00001099
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001098
Iteration 155/1000 | Loss: 0.00001098
Iteration 156/1000 | Loss: 0.00001098
Iteration 157/1000 | Loss: 0.00001098
Iteration 158/1000 | Loss: 0.00001098
Iteration 159/1000 | Loss: 0.00001098
Iteration 160/1000 | Loss: 0.00001098
Iteration 161/1000 | Loss: 0.00001098
Iteration 162/1000 | Loss: 0.00001098
Iteration 163/1000 | Loss: 0.00001098
Iteration 164/1000 | Loss: 0.00001098
Iteration 165/1000 | Loss: 0.00001098
Iteration 166/1000 | Loss: 0.00001098
Iteration 167/1000 | Loss: 0.00001098
Iteration 168/1000 | Loss: 0.00001097
Iteration 169/1000 | Loss: 0.00001097
Iteration 170/1000 | Loss: 0.00001097
Iteration 171/1000 | Loss: 0.00001097
Iteration 172/1000 | Loss: 0.00001097
Iteration 173/1000 | Loss: 0.00001097
Iteration 174/1000 | Loss: 0.00001097
Iteration 175/1000 | Loss: 0.00001097
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001097
Iteration 178/1000 | Loss: 0.00001097
Iteration 179/1000 | Loss: 0.00001097
Iteration 180/1000 | Loss: 0.00001097
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001096
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001096
Iteration 194/1000 | Loss: 0.00001096
Iteration 195/1000 | Loss: 0.00001096
Iteration 196/1000 | Loss: 0.00001096
Iteration 197/1000 | Loss: 0.00001096
Iteration 198/1000 | Loss: 0.00001096
Iteration 199/1000 | Loss: 0.00001096
Iteration 200/1000 | Loss: 0.00001096
Iteration 201/1000 | Loss: 0.00001096
Iteration 202/1000 | Loss: 0.00001096
Iteration 203/1000 | Loss: 0.00001096
Iteration 204/1000 | Loss: 0.00001096
Iteration 205/1000 | Loss: 0.00001096
Iteration 206/1000 | Loss: 0.00001096
Iteration 207/1000 | Loss: 0.00001096
Iteration 208/1000 | Loss: 0.00001096
Iteration 209/1000 | Loss: 0.00001096
Iteration 210/1000 | Loss: 0.00001096
Iteration 211/1000 | Loss: 0.00001096
Iteration 212/1000 | Loss: 0.00001096
Iteration 213/1000 | Loss: 0.00001096
Iteration 214/1000 | Loss: 0.00001096
Iteration 215/1000 | Loss: 0.00001096
Iteration 216/1000 | Loss: 0.00001096
Iteration 217/1000 | Loss: 0.00001096
Iteration 218/1000 | Loss: 0.00001096
Iteration 219/1000 | Loss: 0.00001096
Iteration 220/1000 | Loss: 0.00001096
Iteration 221/1000 | Loss: 0.00001096
Iteration 222/1000 | Loss: 0.00001096
Iteration 223/1000 | Loss: 0.00001096
Iteration 224/1000 | Loss: 0.00001096
Iteration 225/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.0963667591568083e-05, 1.0963667591568083e-05, 1.0963667591568083e-05, 1.0963667591568083e-05, 1.0963667591568083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0963667591568083e-05

Optimization complete. Final v2v error: 2.8475630283355713 mm

Highest mean error: 2.9512369632720947 mm for frame 23

Lowest mean error: 2.7480573654174805 mm for frame 12

Saving results

Total time: 38.58954334259033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049701
Iteration 2/25 | Loss: 0.01049701
Iteration 3/25 | Loss: 0.01049701
Iteration 4/25 | Loss: 0.00253961
Iteration 5/25 | Loss: 0.00188195
Iteration 6/25 | Loss: 0.00156352
Iteration 7/25 | Loss: 0.00160286
Iteration 8/25 | Loss: 0.00145635
Iteration 9/25 | Loss: 0.00142916
Iteration 10/25 | Loss: 0.00138183
Iteration 11/25 | Loss: 0.00134891
Iteration 12/25 | Loss: 0.00132368
Iteration 13/25 | Loss: 0.00130686
Iteration 14/25 | Loss: 0.00129459
Iteration 15/25 | Loss: 0.00129638
Iteration 16/25 | Loss: 0.00128529
Iteration 17/25 | Loss: 0.00127146
Iteration 18/25 | Loss: 0.00126350
Iteration 19/25 | Loss: 0.00126050
Iteration 20/25 | Loss: 0.00124355
Iteration 21/25 | Loss: 0.00124305
Iteration 22/25 | Loss: 0.00124356
Iteration 23/25 | Loss: 0.00123678
Iteration 24/25 | Loss: 0.00123594
Iteration 25/25 | Loss: 0.00122874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41034961
Iteration 2/25 | Loss: 0.00226445
Iteration 3/25 | Loss: 0.00218711
Iteration 4/25 | Loss: 0.00218711
Iteration 5/25 | Loss: 0.00218711
Iteration 6/25 | Loss: 0.00218711
Iteration 7/25 | Loss: 0.00218711
Iteration 8/25 | Loss: 0.00218711
Iteration 9/25 | Loss: 0.00218711
Iteration 10/25 | Loss: 0.00218711
Iteration 11/25 | Loss: 0.00218711
Iteration 12/25 | Loss: 0.00218711
Iteration 13/25 | Loss: 0.00218711
Iteration 14/25 | Loss: 0.00218711
Iteration 15/25 | Loss: 0.00218711
Iteration 16/25 | Loss: 0.00218711
Iteration 17/25 | Loss: 0.00218711
Iteration 18/25 | Loss: 0.00218711
Iteration 19/25 | Loss: 0.00218711
Iteration 20/25 | Loss: 0.00218711
Iteration 21/25 | Loss: 0.00218711
Iteration 22/25 | Loss: 0.00218711
Iteration 23/25 | Loss: 0.00218711
Iteration 24/25 | Loss: 0.00218711
Iteration 25/25 | Loss: 0.00218711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218711
Iteration 2/1000 | Loss: 0.00381544
Iteration 3/1000 | Loss: 0.00639639
Iteration 4/1000 | Loss: 0.00595699
Iteration 5/1000 | Loss: 0.00441016
Iteration 6/1000 | Loss: 0.00630782
Iteration 7/1000 | Loss: 0.00451308
Iteration 8/1000 | Loss: 0.00864467
Iteration 9/1000 | Loss: 0.00650878
Iteration 10/1000 | Loss: 0.00584723
Iteration 11/1000 | Loss: 0.00661404
Iteration 12/1000 | Loss: 0.00524006
Iteration 13/1000 | Loss: 0.00619129
Iteration 14/1000 | Loss: 0.00567509
Iteration 15/1000 | Loss: 0.00678589
Iteration 16/1000 | Loss: 0.00670131
Iteration 17/1000 | Loss: 0.00735298
Iteration 18/1000 | Loss: 0.00721469
Iteration 19/1000 | Loss: 0.00798932
Iteration 20/1000 | Loss: 0.00796152
Iteration 21/1000 | Loss: 0.00760668
Iteration 22/1000 | Loss: 0.00728521
Iteration 23/1000 | Loss: 0.00735875
Iteration 24/1000 | Loss: 0.00774678
Iteration 25/1000 | Loss: 0.00731140
Iteration 26/1000 | Loss: 0.00851627
Iteration 27/1000 | Loss: 0.00774958
Iteration 28/1000 | Loss: 0.00857424
Iteration 29/1000 | Loss: 0.00834441
Iteration 30/1000 | Loss: 0.00494886
Iteration 31/1000 | Loss: 0.00495452
Iteration 32/1000 | Loss: 0.00593515
Iteration 33/1000 | Loss: 0.00652278
Iteration 34/1000 | Loss: 0.00653972
Iteration 35/1000 | Loss: 0.00701850
Iteration 36/1000 | Loss: 0.00750730
Iteration 37/1000 | Loss: 0.00747376
Iteration 38/1000 | Loss: 0.00719662
Iteration 39/1000 | Loss: 0.00848098
Iteration 40/1000 | Loss: 0.00696911
Iteration 41/1000 | Loss: 0.00638664
Iteration 42/1000 | Loss: 0.00649194
Iteration 43/1000 | Loss: 0.00690912
Iteration 44/1000 | Loss: 0.00605934
Iteration 45/1000 | Loss: 0.00795659
Iteration 46/1000 | Loss: 0.00802584
Iteration 47/1000 | Loss: 0.00640108
Iteration 48/1000 | Loss: 0.00699912
Iteration 49/1000 | Loss: 0.00779636
Iteration 50/1000 | Loss: 0.00669904
Iteration 51/1000 | Loss: 0.00595852
Iteration 52/1000 | Loss: 0.00826927
Iteration 53/1000 | Loss: 0.00644150
Iteration 54/1000 | Loss: 0.00551466
Iteration 55/1000 | Loss: 0.00616593
Iteration 56/1000 | Loss: 0.00602884
Iteration 57/1000 | Loss: 0.00812409
Iteration 58/1000 | Loss: 0.00699878
Iteration 59/1000 | Loss: 0.00663948
Iteration 60/1000 | Loss: 0.00622865
Iteration 61/1000 | Loss: 0.00687661
Iteration 62/1000 | Loss: 0.00633613
Iteration 63/1000 | Loss: 0.00833041
Iteration 64/1000 | Loss: 0.00654674
Iteration 65/1000 | Loss: 0.00583250
Iteration 66/1000 | Loss: 0.00625221
Iteration 67/1000 | Loss: 0.00660537
Iteration 68/1000 | Loss: 0.00642816
Iteration 69/1000 | Loss: 0.01096156
Iteration 70/1000 | Loss: 0.01278973
Iteration 71/1000 | Loss: 0.00836362
Iteration 72/1000 | Loss: 0.00637972
Iteration 73/1000 | Loss: 0.00803253
Iteration 74/1000 | Loss: 0.00731986
Iteration 75/1000 | Loss: 0.00706163
Iteration 76/1000 | Loss: 0.00596751
Iteration 77/1000 | Loss: 0.00487199
Iteration 78/1000 | Loss: 0.00608142
Iteration 79/1000 | Loss: 0.00709669
Iteration 80/1000 | Loss: 0.00673010
Iteration 81/1000 | Loss: 0.00695673
Iteration 82/1000 | Loss: 0.00704414
Iteration 83/1000 | Loss: 0.00989283
Iteration 84/1000 | Loss: 0.00680179
Iteration 85/1000 | Loss: 0.00630602
Iteration 86/1000 | Loss: 0.00661653
Iteration 87/1000 | Loss: 0.00618742
Iteration 88/1000 | Loss: 0.00617486
Iteration 89/1000 | Loss: 0.00634671
Iteration 90/1000 | Loss: 0.00685942
Iteration 91/1000 | Loss: 0.00632256
Iteration 92/1000 | Loss: 0.00843235
Iteration 93/1000 | Loss: 0.00878641
Iteration 94/1000 | Loss: 0.00595396
Iteration 95/1000 | Loss: 0.00604255
Iteration 96/1000 | Loss: 0.00543742
Iteration 97/1000 | Loss: 0.00771769
Iteration 98/1000 | Loss: 0.00553076
Iteration 99/1000 | Loss: 0.00717587
Iteration 100/1000 | Loss: 0.00555495
Iteration 101/1000 | Loss: 0.00656571
Iteration 102/1000 | Loss: 0.00627579
Iteration 103/1000 | Loss: 0.00638320
Iteration 104/1000 | Loss: 0.00898710
Iteration 105/1000 | Loss: 0.00702723
Iteration 106/1000 | Loss: 0.01049005
Iteration 107/1000 | Loss: 0.00565133
Iteration 108/1000 | Loss: 0.00572757
Iteration 109/1000 | Loss: 0.00572289
Iteration 110/1000 | Loss: 0.00583868
Iteration 111/1000 | Loss: 0.00523453
Iteration 112/1000 | Loss: 0.00810111
Iteration 113/1000 | Loss: 0.00724872
Iteration 114/1000 | Loss: 0.00807857
Iteration 115/1000 | Loss: 0.00536514
Iteration 116/1000 | Loss: 0.00549394
Iteration 117/1000 | Loss: 0.00744647
Iteration 118/1000 | Loss: 0.00561320
Iteration 119/1000 | Loss: 0.00687476
Iteration 120/1000 | Loss: 0.00568923
Iteration 121/1000 | Loss: 0.00614074
Iteration 122/1000 | Loss: 0.00723174
Iteration 123/1000 | Loss: 0.00468698
Iteration 124/1000 | Loss: 0.00736751
Iteration 125/1000 | Loss: 0.00806903
Iteration 126/1000 | Loss: 0.00567612
Iteration 127/1000 | Loss: 0.00489695
Iteration 128/1000 | Loss: 0.00505468
Iteration 129/1000 | Loss: 0.00628670
Iteration 130/1000 | Loss: 0.00505668
Iteration 131/1000 | Loss: 0.00424408
Iteration 132/1000 | Loss: 0.00453064
Iteration 133/1000 | Loss: 0.00550517
Iteration 134/1000 | Loss: 0.00563869
Iteration 135/1000 | Loss: 0.00757136
Iteration 136/1000 | Loss: 0.00591980
Iteration 137/1000 | Loss: 0.00572624
Iteration 138/1000 | Loss: 0.00580128
Iteration 139/1000 | Loss: 0.00578029
Iteration 140/1000 | Loss: 0.00467699
Iteration 141/1000 | Loss: 0.00699321
Iteration 142/1000 | Loss: 0.00812593
Iteration 143/1000 | Loss: 0.00907514
Iteration 144/1000 | Loss: 0.00700288
Iteration 145/1000 | Loss: 0.00911326
Iteration 146/1000 | Loss: 0.00788048
Iteration 147/1000 | Loss: 0.00489297
Iteration 148/1000 | Loss: 0.00570849
Iteration 149/1000 | Loss: 0.00411543
Iteration 150/1000 | Loss: 0.00467639
Iteration 151/1000 | Loss: 0.00554448
Iteration 152/1000 | Loss: 0.00457071
Iteration 153/1000 | Loss: 0.00550423
Iteration 154/1000 | Loss: 0.00592293
Iteration 155/1000 | Loss: 0.00483684
Iteration 156/1000 | Loss: 0.00482804
Iteration 157/1000 | Loss: 0.00521688
Iteration 158/1000 | Loss: 0.00537690
Iteration 159/1000 | Loss: 0.00521101
Iteration 160/1000 | Loss: 0.00497317
Iteration 161/1000 | Loss: 0.00585792
Iteration 162/1000 | Loss: 0.00483704
Iteration 163/1000 | Loss: 0.00592678
Iteration 164/1000 | Loss: 0.00493012
Iteration 165/1000 | Loss: 0.00508788
Iteration 166/1000 | Loss: 0.00552700
Iteration 167/1000 | Loss: 0.00644191
Iteration 168/1000 | Loss: 0.00540075
Iteration 169/1000 | Loss: 0.00532975
Iteration 170/1000 | Loss: 0.00497179
Iteration 171/1000 | Loss: 0.00598846
Iteration 172/1000 | Loss: 0.00500485
Iteration 173/1000 | Loss: 0.00561945
Iteration 174/1000 | Loss: 0.00629036
Iteration 175/1000 | Loss: 0.00621255
Iteration 176/1000 | Loss: 0.00475794
Iteration 177/1000 | Loss: 0.00456603
Iteration 178/1000 | Loss: 0.00560130
Iteration 179/1000 | Loss: 0.00643614
Iteration 180/1000 | Loss: 0.00449087
Iteration 181/1000 | Loss: 0.00515150
Iteration 182/1000 | Loss: 0.00724931
Iteration 183/1000 | Loss: 0.00571105
Iteration 184/1000 | Loss: 0.00432217
Iteration 185/1000 | Loss: 0.00468783
Iteration 186/1000 | Loss: 0.00461713
Iteration 187/1000 | Loss: 0.00492446
Iteration 188/1000 | Loss: 0.00518577
Iteration 189/1000 | Loss: 0.00423110
Iteration 190/1000 | Loss: 0.00495901
Iteration 191/1000 | Loss: 0.00558362
Iteration 192/1000 | Loss: 0.00540557
Iteration 193/1000 | Loss: 0.00486228
Iteration 194/1000 | Loss: 0.00445462
Iteration 195/1000 | Loss: 0.00489084
Iteration 196/1000 | Loss: 0.00501234
Iteration 197/1000 | Loss: 0.00516360
Iteration 198/1000 | Loss: 0.00525132
Iteration 199/1000 | Loss: 0.00498231
Iteration 200/1000 | Loss: 0.00447185
Iteration 201/1000 | Loss: 0.00468250
Iteration 202/1000 | Loss: 0.00724983
Iteration 203/1000 | Loss: 0.00467809
Iteration 204/1000 | Loss: 0.00432206
Iteration 205/1000 | Loss: 0.00564344
Iteration 206/1000 | Loss: 0.00759108
Iteration 207/1000 | Loss: 0.00474524
Iteration 208/1000 | Loss: 0.00385797
Iteration 209/1000 | Loss: 0.00441099
Iteration 210/1000 | Loss: 0.00504413
Iteration 211/1000 | Loss: 0.00399072
Iteration 212/1000 | Loss: 0.00352721
Iteration 213/1000 | Loss: 0.00320675
Iteration 214/1000 | Loss: 0.00440771
Iteration 215/1000 | Loss: 0.00425215
Iteration 216/1000 | Loss: 0.00394622
Iteration 217/1000 | Loss: 0.00349680
Iteration 218/1000 | Loss: 0.00309932
Iteration 219/1000 | Loss: 0.00338854
Iteration 220/1000 | Loss: 0.00323194
Iteration 221/1000 | Loss: 0.00440663
Iteration 222/1000 | Loss: 0.00440066
Iteration 223/1000 | Loss: 0.00478943
Iteration 224/1000 | Loss: 0.00411579
Iteration 225/1000 | Loss: 0.00583779
Iteration 226/1000 | Loss: 0.00670097
Iteration 227/1000 | Loss: 0.00556869
Iteration 228/1000 | Loss: 0.00608671
Iteration 229/1000 | Loss: 0.00532345
Iteration 230/1000 | Loss: 0.00534000
Iteration 231/1000 | Loss: 0.00534844
Iteration 232/1000 | Loss: 0.00628502
Iteration 233/1000 | Loss: 0.00391554
Iteration 234/1000 | Loss: 0.00364487
Iteration 235/1000 | Loss: 0.00323476
Iteration 236/1000 | Loss: 0.00353488
Iteration 237/1000 | Loss: 0.00389384
Iteration 238/1000 | Loss: 0.00300664
Iteration 239/1000 | Loss: 0.00299552
Iteration 240/1000 | Loss: 0.00299992
Iteration 241/1000 | Loss: 0.00501297
Iteration 242/1000 | Loss: 0.00578631
Iteration 243/1000 | Loss: 0.00482108
Iteration 244/1000 | Loss: 0.00308752
Iteration 245/1000 | Loss: 0.00345077
Iteration 246/1000 | Loss: 0.00304138
Iteration 247/1000 | Loss: 0.00292384
Iteration 248/1000 | Loss: 0.00285464
Iteration 249/1000 | Loss: 0.00299821
Iteration 250/1000 | Loss: 0.00344974
Iteration 251/1000 | Loss: 0.00326727
Iteration 252/1000 | Loss: 0.00397622
Iteration 253/1000 | Loss: 0.00322786
Iteration 254/1000 | Loss: 0.00337299
Iteration 255/1000 | Loss: 0.00404547
Iteration 256/1000 | Loss: 0.00235009
Iteration 257/1000 | Loss: 0.00265360
Iteration 258/1000 | Loss: 0.00317921
Iteration 259/1000 | Loss: 0.00293317
Iteration 260/1000 | Loss: 0.00410602
Iteration 261/1000 | Loss: 0.00270238
Iteration 262/1000 | Loss: 0.00380647
Iteration 263/1000 | Loss: 0.00324558
Iteration 264/1000 | Loss: 0.00328168
Iteration 265/1000 | Loss: 0.00239981
Iteration 266/1000 | Loss: 0.00248209
Iteration 267/1000 | Loss: 0.00273428
Iteration 268/1000 | Loss: 0.00401549
Iteration 269/1000 | Loss: 0.00331244
Iteration 270/1000 | Loss: 0.00322613
Iteration 271/1000 | Loss: 0.00294575
Iteration 272/1000 | Loss: 0.00282562
Iteration 273/1000 | Loss: 0.00373787
Iteration 274/1000 | Loss: 0.00286880
Iteration 275/1000 | Loss: 0.00240756
Iteration 276/1000 | Loss: 0.00251378
Iteration 277/1000 | Loss: 0.00288501
Iteration 278/1000 | Loss: 0.00322927
Iteration 279/1000 | Loss: 0.00255027
Iteration 280/1000 | Loss: 0.00209441
Iteration 281/1000 | Loss: 0.00291713
Iteration 282/1000 | Loss: 0.00276738
Iteration 283/1000 | Loss: 0.00287445
Iteration 284/1000 | Loss: 0.00395271
Iteration 285/1000 | Loss: 0.00277778
Iteration 286/1000 | Loss: 0.00354751
Iteration 287/1000 | Loss: 0.00297731
Iteration 288/1000 | Loss: 0.00357050
Iteration 289/1000 | Loss: 0.00339049
Iteration 290/1000 | Loss: 0.00305753
Iteration 291/1000 | Loss: 0.00304074
Iteration 292/1000 | Loss: 0.00319107
Iteration 293/1000 | Loss: 0.00321705
Iteration 294/1000 | Loss: 0.00298185
Iteration 295/1000 | Loss: 0.00296593
Iteration 296/1000 | Loss: 0.00239813
Iteration 297/1000 | Loss: 0.00301295
Iteration 298/1000 | Loss: 0.00307456
Iteration 299/1000 | Loss: 0.00378506
Iteration 300/1000 | Loss: 0.00222690
Iteration 301/1000 | Loss: 0.00245904
Iteration 302/1000 | Loss: 0.00280279
Iteration 303/1000 | Loss: 0.00315867
Iteration 304/1000 | Loss: 0.00302512
Iteration 305/1000 | Loss: 0.00290019
Iteration 306/1000 | Loss: 0.00372944
Iteration 307/1000 | Loss: 0.00421453
Iteration 308/1000 | Loss: 0.00326052
Iteration 309/1000 | Loss: 0.00337927
Iteration 310/1000 | Loss: 0.00304175
Iteration 311/1000 | Loss: 0.00311846
Iteration 312/1000 | Loss: 0.00290479
Iteration 313/1000 | Loss: 0.00304462
Iteration 314/1000 | Loss: 0.00299657
Iteration 315/1000 | Loss: 0.00299977
Iteration 316/1000 | Loss: 0.00449323
Iteration 317/1000 | Loss: 0.00306064
Iteration 318/1000 | Loss: 0.00297797
Iteration 319/1000 | Loss: 0.00300746
Iteration 320/1000 | Loss: 0.00363321
Iteration 321/1000 | Loss: 0.00302170
Iteration 322/1000 | Loss: 0.00298665
Iteration 323/1000 | Loss: 0.00298047
Iteration 324/1000 | Loss: 0.00411646
Iteration 325/1000 | Loss: 0.00428416
Iteration 326/1000 | Loss: 0.00383383
Iteration 327/1000 | Loss: 0.00474149
Iteration 328/1000 | Loss: 0.00347832
Iteration 329/1000 | Loss: 0.00272769
Iteration 330/1000 | Loss: 0.00163706
Iteration 331/1000 | Loss: 0.00180892
Iteration 332/1000 | Loss: 0.00241833
Iteration 333/1000 | Loss: 0.00273820
Iteration 334/1000 | Loss: 0.00297033
Iteration 335/1000 | Loss: 0.00239567
Iteration 336/1000 | Loss: 0.00264825
Iteration 337/1000 | Loss: 0.00327136
Iteration 338/1000 | Loss: 0.00261157
Iteration 339/1000 | Loss: 0.00250110
Iteration 340/1000 | Loss: 0.00276202
Iteration 341/1000 | Loss: 0.00252605
Iteration 342/1000 | Loss: 0.00275071
Iteration 343/1000 | Loss: 0.00231139
Iteration 344/1000 | Loss: 0.00258266
Iteration 345/1000 | Loss: 0.00336841
Iteration 346/1000 | Loss: 0.00345522
Iteration 347/1000 | Loss: 0.00316865
Iteration 348/1000 | Loss: 0.00316478
Iteration 349/1000 | Loss: 0.00247835
Iteration 350/1000 | Loss: 0.00285645
Iteration 351/1000 | Loss: 0.00350776
Iteration 352/1000 | Loss: 0.00365407
Iteration 353/1000 | Loss: 0.00506665
Iteration 354/1000 | Loss: 0.00334247
Iteration 355/1000 | Loss: 0.00471714
Iteration 356/1000 | Loss: 0.00241794
Iteration 357/1000 | Loss: 0.00221714
Iteration 358/1000 | Loss: 0.00210063
Iteration 359/1000 | Loss: 0.00206015
Iteration 360/1000 | Loss: 0.00255002
Iteration 361/1000 | Loss: 0.00252026
Iteration 362/1000 | Loss: 0.00243356
Iteration 363/1000 | Loss: 0.00253974
Iteration 364/1000 | Loss: 0.00300960
Iteration 365/1000 | Loss: 0.00211568
Iteration 366/1000 | Loss: 0.00197872
Iteration 367/1000 | Loss: 0.00205556
Iteration 368/1000 | Loss: 0.00230547
Iteration 369/1000 | Loss: 0.00220951
Iteration 370/1000 | Loss: 0.00231531
Iteration 371/1000 | Loss: 0.00225505
Iteration 372/1000 | Loss: 0.00237252
Iteration 373/1000 | Loss: 0.00207560
Iteration 374/1000 | Loss: 0.00259605
Iteration 375/1000 | Loss: 0.00357987
Iteration 376/1000 | Loss: 0.00412586
Iteration 377/1000 | Loss: 0.00243946
Iteration 378/1000 | Loss: 0.00263263
Iteration 379/1000 | Loss: 0.00292977
Iteration 380/1000 | Loss: 0.00365162
Iteration 381/1000 | Loss: 0.00469589
Iteration 382/1000 | Loss: 0.00286207
Iteration 383/1000 | Loss: 0.00314136
Iteration 384/1000 | Loss: 0.00288082
Iteration 385/1000 | Loss: 0.00247709
Iteration 386/1000 | Loss: 0.00260510
Iteration 387/1000 | Loss: 0.00325372
Iteration 388/1000 | Loss: 0.00293629
Iteration 389/1000 | Loss: 0.00340252
Iteration 390/1000 | Loss: 0.00306149
Iteration 391/1000 | Loss: 0.00315825
Iteration 392/1000 | Loss: 0.00261200
Iteration 393/1000 | Loss: 0.00407087
Iteration 394/1000 | Loss: 0.00333022
Iteration 395/1000 | Loss: 0.00251299
Iteration 396/1000 | Loss: 0.00197483
Iteration 397/1000 | Loss: 0.00196276
Iteration 398/1000 | Loss: 0.00292849
Iteration 399/1000 | Loss: 0.00250433
Iteration 400/1000 | Loss: 0.00171717
Iteration 401/1000 | Loss: 0.00184030
Iteration 402/1000 | Loss: 0.00259656
Iteration 403/1000 | Loss: 0.00220129
Iteration 404/1000 | Loss: 0.00185102
Iteration 405/1000 | Loss: 0.00257218
Iteration 406/1000 | Loss: 0.00195874
Iteration 407/1000 | Loss: 0.00212222
Iteration 408/1000 | Loss: 0.00199448
Iteration 409/1000 | Loss: 0.00235061
Iteration 410/1000 | Loss: 0.00225170
Iteration 411/1000 | Loss: 0.00205324
Iteration 412/1000 | Loss: 0.00216435
Iteration 413/1000 | Loss: 0.00239422
Iteration 414/1000 | Loss: 0.00207614
Iteration 415/1000 | Loss: 0.00216873
Iteration 416/1000 | Loss: 0.00203359
Iteration 417/1000 | Loss: 0.00190031
Iteration 418/1000 | Loss: 0.00326282
Iteration 419/1000 | Loss: 0.00364200
Iteration 420/1000 | Loss: 0.00248901
Iteration 421/1000 | Loss: 0.00182987
Iteration 422/1000 | Loss: 0.00230477
Iteration 423/1000 | Loss: 0.00331048
Iteration 424/1000 | Loss: 0.00455142
Iteration 425/1000 | Loss: 0.00535275
Iteration 426/1000 | Loss: 0.00609817
Iteration 427/1000 | Loss: 0.00248091
Iteration 428/1000 | Loss: 0.00202824
Iteration 429/1000 | Loss: 0.00519582
Iteration 430/1000 | Loss: 0.00324311
Iteration 431/1000 | Loss: 0.00356492
Iteration 432/1000 | Loss: 0.00217086
Iteration 433/1000 | Loss: 0.00288204
Iteration 434/1000 | Loss: 0.00252462
Iteration 435/1000 | Loss: 0.00222026
Iteration 436/1000 | Loss: 0.00245855
Iteration 437/1000 | Loss: 0.00163903
Iteration 438/1000 | Loss: 0.00243314
Iteration 439/1000 | Loss: 0.00215420
Iteration 440/1000 | Loss: 0.00249090
Iteration 441/1000 | Loss: 0.00198042
Iteration 442/1000 | Loss: 0.00222042
Iteration 443/1000 | Loss: 0.00220158
Iteration 444/1000 | Loss: 0.00207194
Iteration 445/1000 | Loss: 0.00225832
Iteration 446/1000 | Loss: 0.00226773
Iteration 447/1000 | Loss: 0.00306056
Iteration 448/1000 | Loss: 0.00241260
Iteration 449/1000 | Loss: 0.00296226
Iteration 450/1000 | Loss: 0.00230672
Iteration 451/1000 | Loss: 0.00241720
Iteration 452/1000 | Loss: 0.00199194
Iteration 453/1000 | Loss: 0.00209752
Iteration 454/1000 | Loss: 0.00262073
Iteration 455/1000 | Loss: 0.00295222
Iteration 456/1000 | Loss: 0.00221718
Iteration 457/1000 | Loss: 0.00251013
Iteration 458/1000 | Loss: 0.00267572
Iteration 459/1000 | Loss: 0.00193711
Iteration 460/1000 | Loss: 0.00223435
Iteration 461/1000 | Loss: 0.00212209
Iteration 462/1000 | Loss: 0.00232011
Iteration 463/1000 | Loss: 0.00209541
Iteration 464/1000 | Loss: 0.00235737
Iteration 465/1000 | Loss: 0.00285198
Iteration 466/1000 | Loss: 0.00230147
Iteration 467/1000 | Loss: 0.00227624
Iteration 468/1000 | Loss: 0.00241648
Iteration 469/1000 | Loss: 0.00205544
Iteration 470/1000 | Loss: 0.00237590
Iteration 471/1000 | Loss: 0.00198665
Iteration 472/1000 | Loss: 0.00214015
Iteration 473/1000 | Loss: 0.00233673
Iteration 474/1000 | Loss: 0.00298382
Iteration 475/1000 | Loss: 0.00280705
Iteration 476/1000 | Loss: 0.00277400
Iteration 477/1000 | Loss: 0.00158212
Iteration 478/1000 | Loss: 0.00184130
Iteration 479/1000 | Loss: 0.00246251
Iteration 480/1000 | Loss: 0.00225538
Iteration 481/1000 | Loss: 0.00266286
Iteration 482/1000 | Loss: 0.00219660
Iteration 483/1000 | Loss: 0.00239609
Iteration 484/1000 | Loss: 0.00207917
Iteration 485/1000 | Loss: 0.00303283
Iteration 486/1000 | Loss: 0.00176767
Iteration 487/1000 | Loss: 0.00424997
Iteration 488/1000 | Loss: 0.00244561
Iteration 489/1000 | Loss: 0.00277521
Iteration 490/1000 | Loss: 0.00298975
Iteration 491/1000 | Loss: 0.00353710
Iteration 492/1000 | Loss: 0.00270491
Iteration 493/1000 | Loss: 0.00260607
Iteration 494/1000 | Loss: 0.00317656
Iteration 495/1000 | Loss: 0.00260225
Iteration 496/1000 | Loss: 0.00366157
Iteration 497/1000 | Loss: 0.00300858
Iteration 498/1000 | Loss: 0.00257522
Iteration 499/1000 | Loss: 0.00270443
Iteration 500/1000 | Loss: 0.00223962
Iteration 501/1000 | Loss: 0.00276114
Iteration 502/1000 | Loss: 0.00296211
Iteration 503/1000 | Loss: 0.00210894
Iteration 504/1000 | Loss: 0.00259500
Iteration 505/1000 | Loss: 0.00336816
Iteration 506/1000 | Loss: 0.00187368
Iteration 507/1000 | Loss: 0.00484217
Iteration 508/1000 | Loss: 0.00251567
Iteration 509/1000 | Loss: 0.00291113
Iteration 510/1000 | Loss: 0.00309673
Iteration 511/1000 | Loss: 0.00239104
Iteration 512/1000 | Loss: 0.00336697
Iteration 513/1000 | Loss: 0.00231174
Iteration 514/1000 | Loss: 0.00323662
Iteration 515/1000 | Loss: 0.00237691
Iteration 516/1000 | Loss: 0.00230917
Iteration 517/1000 | Loss: 0.00143158
Iteration 518/1000 | Loss: 0.00220388
Iteration 519/1000 | Loss: 0.00214262
Iteration 520/1000 | Loss: 0.00397676
Iteration 521/1000 | Loss: 0.00266705
Iteration 522/1000 | Loss: 0.00224578
Iteration 523/1000 | Loss: 0.00240064
Iteration 524/1000 | Loss: 0.00251386
Iteration 525/1000 | Loss: 0.00400875
Iteration 526/1000 | Loss: 0.00284818
Iteration 527/1000 | Loss: 0.00205793
Iteration 528/1000 | Loss: 0.00157959
Iteration 529/1000 | Loss: 0.00220092
Iteration 530/1000 | Loss: 0.00235456
Iteration 531/1000 | Loss: 0.00199253
Iteration 532/1000 | Loss: 0.00212873
Iteration 533/1000 | Loss: 0.00249548
Iteration 534/1000 | Loss: 0.00236842
Iteration 535/1000 | Loss: 0.00273497
Iteration 536/1000 | Loss: 0.00243967
Iteration 537/1000 | Loss: 0.00294746
Iteration 538/1000 | Loss: 0.00328192
Iteration 539/1000 | Loss: 0.00245523
Iteration 540/1000 | Loss: 0.00198931
Iteration 541/1000 | Loss: 0.00228929
Iteration 542/1000 | Loss: 0.00271965
Iteration 543/1000 | Loss: 0.00222232
Iteration 544/1000 | Loss: 0.00257867
Iteration 545/1000 | Loss: 0.00247924
Iteration 546/1000 | Loss: 0.00396234
Iteration 547/1000 | Loss: 0.00182672
Iteration 548/1000 | Loss: 0.00142845
Iteration 549/1000 | Loss: 0.00157421
Iteration 550/1000 | Loss: 0.00197883
Iteration 551/1000 | Loss: 0.00188142
Iteration 552/1000 | Loss: 0.00317655
Iteration 553/1000 | Loss: 0.00187172
Iteration 554/1000 | Loss: 0.00206359
Iteration 555/1000 | Loss: 0.00182331
Iteration 556/1000 | Loss: 0.00163490
Iteration 557/1000 | Loss: 0.00192264
Iteration 558/1000 | Loss: 0.00193714
Iteration 559/1000 | Loss: 0.00182731
Iteration 560/1000 | Loss: 0.00236806
Iteration 561/1000 | Loss: 0.00188908
Iteration 562/1000 | Loss: 0.00209930
Iteration 563/1000 | Loss: 0.00353805
Iteration 564/1000 | Loss: 0.00135388
Iteration 565/1000 | Loss: 0.00164790
Iteration 566/1000 | Loss: 0.00157324
Iteration 567/1000 | Loss: 0.00197193
Iteration 568/1000 | Loss: 0.00334108
Iteration 569/1000 | Loss: 0.00201436
Iteration 570/1000 | Loss: 0.00160613
Iteration 571/1000 | Loss: 0.00191588
Iteration 572/1000 | Loss: 0.00135063
Iteration 573/1000 | Loss: 0.00179786
Iteration 574/1000 | Loss: 0.00203591
Iteration 575/1000 | Loss: 0.00204529
Iteration 576/1000 | Loss: 0.00174590
Iteration 577/1000 | Loss: 0.00174560
Iteration 578/1000 | Loss: 0.00167255
Iteration 579/1000 | Loss: 0.00154841
Iteration 580/1000 | Loss: 0.00326513
Iteration 581/1000 | Loss: 0.00244185
Iteration 582/1000 | Loss: 0.00148641
Iteration 583/1000 | Loss: 0.00118374
Iteration 584/1000 | Loss: 0.00149417
Iteration 585/1000 | Loss: 0.00163020
Iteration 586/1000 | Loss: 0.00167760
Iteration 587/1000 | Loss: 0.00183237
Iteration 588/1000 | Loss: 0.00164928
Iteration 589/1000 | Loss: 0.00226048
Iteration 590/1000 | Loss: 0.00199206
Iteration 591/1000 | Loss: 0.00281206
Iteration 592/1000 | Loss: 0.00470630
Iteration 593/1000 | Loss: 0.00203014
Iteration 594/1000 | Loss: 0.00188712
Iteration 595/1000 | Loss: 0.00156748
Iteration 596/1000 | Loss: 0.00211194
Iteration 597/1000 | Loss: 0.00204908
Iteration 598/1000 | Loss: 0.00127535
Iteration 599/1000 | Loss: 0.00250633
Iteration 600/1000 | Loss: 0.00262408
Iteration 601/1000 | Loss: 0.00185165
Iteration 602/1000 | Loss: 0.00196424
Iteration 603/1000 | Loss: 0.00197387
Iteration 604/1000 | Loss: 0.00177573
Iteration 605/1000 | Loss: 0.00295342
Iteration 606/1000 | Loss: 0.00161323
Iteration 607/1000 | Loss: 0.00153636
Iteration 608/1000 | Loss: 0.00144795
Iteration 609/1000 | Loss: 0.00207299
Iteration 610/1000 | Loss: 0.00218329
Iteration 611/1000 | Loss: 0.00177775
Iteration 612/1000 | Loss: 0.00189257
Iteration 613/1000 | Loss: 0.00143163
Iteration 614/1000 | Loss: 0.00135606
Iteration 615/1000 | Loss: 0.00149872
Iteration 616/1000 | Loss: 0.00214809
Iteration 617/1000 | Loss: 0.00172337
Iteration 618/1000 | Loss: 0.00218413
Iteration 619/1000 | Loss: 0.00201384
Iteration 620/1000 | Loss: 0.00159023
Iteration 621/1000 | Loss: 0.00140191
Iteration 622/1000 | Loss: 0.00148188
Iteration 623/1000 | Loss: 0.00165101
Iteration 624/1000 | Loss: 0.00177260
Iteration 625/1000 | Loss: 0.00198935
Iteration 626/1000 | Loss: 0.00166788
Iteration 627/1000 | Loss: 0.00196976
Iteration 628/1000 | Loss: 0.00164125
Iteration 629/1000 | Loss: 0.00175344
Iteration 630/1000 | Loss: 0.00217391
Iteration 631/1000 | Loss: 0.00178945
Iteration 632/1000 | Loss: 0.00188127
Iteration 633/1000 | Loss: 0.00195431
Iteration 634/1000 | Loss: 0.00167556
Iteration 635/1000 | Loss: 0.00171036
Iteration 636/1000 | Loss: 0.00239421
Iteration 637/1000 | Loss: 0.00274623
Iteration 638/1000 | Loss: 0.00136165
Iteration 639/1000 | Loss: 0.00151165
Iteration 640/1000 | Loss: 0.00167548
Iteration 641/1000 | Loss: 0.00136192
Iteration 642/1000 | Loss: 0.00186965
Iteration 643/1000 | Loss: 0.00171335
Iteration 644/1000 | Loss: 0.00167432
Iteration 645/1000 | Loss: 0.00184474
Iteration 646/1000 | Loss: 0.00170162
Iteration 647/1000 | Loss: 0.00213910
Iteration 648/1000 | Loss: 0.00234171
Iteration 649/1000 | Loss: 0.00193869
Iteration 650/1000 | Loss: 0.00162273
Iteration 651/1000 | Loss: 0.00200249
Iteration 652/1000 | Loss: 0.00290470
Iteration 653/1000 | Loss: 0.00451241
Iteration 654/1000 | Loss: 0.00180727
Iteration 655/1000 | Loss: 0.00191620
Iteration 656/1000 | Loss: 0.00103695
Iteration 657/1000 | Loss: 0.00125826
Iteration 658/1000 | Loss: 0.00105381
Iteration 659/1000 | Loss: 0.00117538
Iteration 660/1000 | Loss: 0.00131419
Iteration 661/1000 | Loss: 0.00179822
Iteration 662/1000 | Loss: 0.00123913
Iteration 663/1000 | Loss: 0.00130137
Iteration 664/1000 | Loss: 0.00137586
Iteration 665/1000 | Loss: 0.00143399
Iteration 666/1000 | Loss: 0.00156042
Iteration 667/1000 | Loss: 0.00154305
Iteration 668/1000 | Loss: 0.00176486
Iteration 669/1000 | Loss: 0.00152256
Iteration 670/1000 | Loss: 0.00201892
Iteration 671/1000 | Loss: 0.00153412
Iteration 672/1000 | Loss: 0.00165606
Iteration 673/1000 | Loss: 0.00232098
Iteration 674/1000 | Loss: 0.00113457
Iteration 675/1000 | Loss: 0.00147317
Iteration 676/1000 | Loss: 0.00157905
Iteration 677/1000 | Loss: 0.00146340
Iteration 678/1000 | Loss: 0.00148019
Iteration 679/1000 | Loss: 0.00146032
Iteration 680/1000 | Loss: 0.00133216
Iteration 681/1000 | Loss: 0.00156909
Iteration 682/1000 | Loss: 0.00165441
Iteration 683/1000 | Loss: 0.00161807
Iteration 684/1000 | Loss: 0.00149404
Iteration 685/1000 | Loss: 0.00139261
Iteration 686/1000 | Loss: 0.00120460
Iteration 687/1000 | Loss: 0.00135368
Iteration 688/1000 | Loss: 0.00156170
Iteration 689/1000 | Loss: 0.00155744
Iteration 690/1000 | Loss: 0.00181955
Iteration 691/1000 | Loss: 0.00134420
Iteration 692/1000 | Loss: 0.00119931
Iteration 693/1000 | Loss: 0.00137886
Iteration 694/1000 | Loss: 0.00178246
Iteration 695/1000 | Loss: 0.00093449
Iteration 696/1000 | Loss: 0.00108427
Iteration 697/1000 | Loss: 0.00139468
Iteration 698/1000 | Loss: 0.00149950
Iteration 699/1000 | Loss: 0.00200230
Iteration 700/1000 | Loss: 0.00134691
Iteration 701/1000 | Loss: 0.00104081
Iteration 702/1000 | Loss: 0.00130398
Iteration 703/1000 | Loss: 0.00149613
Iteration 704/1000 | Loss: 0.00140616
Iteration 705/1000 | Loss: 0.00179103
Iteration 706/1000 | Loss: 0.00171245
Iteration 707/1000 | Loss: 0.00081759
Iteration 708/1000 | Loss: 0.00080406
Iteration 709/1000 | Loss: 0.00083347
Iteration 710/1000 | Loss: 0.00088175
Iteration 711/1000 | Loss: 0.00102973
Iteration 712/1000 | Loss: 0.00103604
Iteration 713/1000 | Loss: 0.00090951
Iteration 714/1000 | Loss: 0.00076924
Iteration 715/1000 | Loss: 0.00084971
Iteration 716/1000 | Loss: 0.00106927
Iteration 717/1000 | Loss: 0.00108363
Iteration 718/1000 | Loss: 0.00093546
Iteration 719/1000 | Loss: 0.00104255
Iteration 720/1000 | Loss: 0.00098837
Iteration 721/1000 | Loss: 0.00128339
Iteration 722/1000 | Loss: 0.00086292
Iteration 723/1000 | Loss: 0.00117618
Iteration 724/1000 | Loss: 0.00106493
Iteration 725/1000 | Loss: 0.00097158
Iteration 726/1000 | Loss: 0.00097276
Iteration 727/1000 | Loss: 0.00090743
Iteration 728/1000 | Loss: 0.00064708
Iteration 729/1000 | Loss: 0.00061231
Iteration 730/1000 | Loss: 0.00081675
Iteration 731/1000 | Loss: 0.00136223
Iteration 732/1000 | Loss: 0.00160906
Iteration 733/1000 | Loss: 0.00091577
Iteration 734/1000 | Loss: 0.00083678
Iteration 735/1000 | Loss: 0.00073669
Iteration 736/1000 | Loss: 0.00163626
Iteration 737/1000 | Loss: 0.00108435
Iteration 738/1000 | Loss: 0.00144097
Iteration 739/1000 | Loss: 0.00085830
Iteration 740/1000 | Loss: 0.00076003
Iteration 741/1000 | Loss: 0.00126711
Iteration 742/1000 | Loss: 0.00105040
Iteration 743/1000 | Loss: 0.00102248
Iteration 744/1000 | Loss: 0.00067529
Iteration 745/1000 | Loss: 0.00094734
Iteration 746/1000 | Loss: 0.00085127
Iteration 747/1000 | Loss: 0.00097370
Iteration 748/1000 | Loss: 0.00097402
Iteration 749/1000 | Loss: 0.00094144
Iteration 750/1000 | Loss: 0.00157160
Iteration 751/1000 | Loss: 0.00141707
Iteration 752/1000 | Loss: 0.00092492
Iteration 753/1000 | Loss: 0.00100664
Iteration 754/1000 | Loss: 0.00130498
Iteration 755/1000 | Loss: 0.00137381
Iteration 756/1000 | Loss: 0.00104895
Iteration 757/1000 | Loss: 0.00095224
Iteration 758/1000 | Loss: 0.00082100
Iteration 759/1000 | Loss: 0.00114364
Iteration 760/1000 | Loss: 0.00096475
Iteration 761/1000 | Loss: 0.00118923
Iteration 762/1000 | Loss: 0.00113928
Iteration 763/1000 | Loss: 0.00100600
Iteration 764/1000 | Loss: 0.00116931
Iteration 765/1000 | Loss: 0.00104669
Iteration 766/1000 | Loss: 0.00143994
Iteration 767/1000 | Loss: 0.00259949
Iteration 768/1000 | Loss: 0.00141070
Iteration 769/1000 | Loss: 0.00105473
Iteration 770/1000 | Loss: 0.00073510
Iteration 771/1000 | Loss: 0.00037162
Iteration 772/1000 | Loss: 0.00031785
Iteration 773/1000 | Loss: 0.00042427
Iteration 774/1000 | Loss: 0.00029423
Iteration 775/1000 | Loss: 0.00036434
Iteration 776/1000 | Loss: 0.00120029
Iteration 777/1000 | Loss: 0.00142977
Iteration 778/1000 | Loss: 0.00051380
Iteration 779/1000 | Loss: 0.00087333
Iteration 780/1000 | Loss: 0.00085949
Iteration 781/1000 | Loss: 0.00071480
Iteration 782/1000 | Loss: 0.00005885
Iteration 783/1000 | Loss: 0.00006679
Iteration 784/1000 | Loss: 0.00003255
Iteration 785/1000 | Loss: 0.00003741
Iteration 786/1000 | Loss: 0.00003467
Iteration 787/1000 | Loss: 0.00003541
Iteration 788/1000 | Loss: 0.00002719
Iteration 789/1000 | Loss: 0.00002956
Iteration 790/1000 | Loss: 0.00002630
Iteration 791/1000 | Loss: 0.00006093
Iteration 792/1000 | Loss: 0.00009161
Iteration 793/1000 | Loss: 0.00003772
Iteration 794/1000 | Loss: 0.00003223
Iteration 795/1000 | Loss: 0.00001725
Iteration 796/1000 | Loss: 0.00002814
Iteration 797/1000 | Loss: 0.00003375
Iteration 798/1000 | Loss: 0.00039383
Iteration 799/1000 | Loss: 0.00018157
Iteration 800/1000 | Loss: 0.00003958
Iteration 801/1000 | Loss: 0.00002219
Iteration 802/1000 | Loss: 0.00032638
Iteration 803/1000 | Loss: 0.00010323
Iteration 804/1000 | Loss: 0.00060823
Iteration 805/1000 | Loss: 0.00013968
Iteration 806/1000 | Loss: 0.00034966
Iteration 807/1000 | Loss: 0.00014079
Iteration 808/1000 | Loss: 0.00001783
Iteration 809/1000 | Loss: 0.00007071
Iteration 810/1000 | Loss: 0.00003448
Iteration 811/1000 | Loss: 0.00007137
Iteration 812/1000 | Loss: 0.00002869
Iteration 813/1000 | Loss: 0.00003080
Iteration 814/1000 | Loss: 0.00001282
Iteration 815/1000 | Loss: 0.00001672
Iteration 816/1000 | Loss: 0.00002310
Iteration 817/1000 | Loss: 0.00002199
Iteration 818/1000 | Loss: 0.00002406
Iteration 819/1000 | Loss: 0.00002208
Iteration 820/1000 | Loss: 0.00002278
Iteration 821/1000 | Loss: 0.00002016
Iteration 822/1000 | Loss: 0.00003136
Iteration 823/1000 | Loss: 0.00002084
Iteration 824/1000 | Loss: 0.00003344
Iteration 825/1000 | Loss: 0.00002031
Iteration 826/1000 | Loss: 0.00001992
Iteration 827/1000 | Loss: 0.00002821
Iteration 828/1000 | Loss: 0.00002439
Iteration 829/1000 | Loss: 0.00002766
Iteration 830/1000 | Loss: 0.00001800
Iteration 831/1000 | Loss: 0.00003145
Iteration 832/1000 | Loss: 0.00001215
Iteration 833/1000 | Loss: 0.00002589
Iteration 834/1000 | Loss: 0.00003781
Iteration 835/1000 | Loss: 0.00003449
Iteration 836/1000 | Loss: 0.00001613
Iteration 837/1000 | Loss: 0.00000918
Iteration 838/1000 | Loss: 0.00000820
Iteration 839/1000 | Loss: 0.00000758
Iteration 840/1000 | Loss: 0.00000731
Iteration 841/1000 | Loss: 0.00000731
Iteration 842/1000 | Loss: 0.00000725
Iteration 843/1000 | Loss: 0.00000709
Iteration 844/1000 | Loss: 0.00000708
Iteration 845/1000 | Loss: 0.00000705
Iteration 846/1000 | Loss: 0.00000704
Iteration 847/1000 | Loss: 0.00000703
Iteration 848/1000 | Loss: 0.00000703
Iteration 849/1000 | Loss: 0.00000702
Iteration 850/1000 | Loss: 0.00000695
Iteration 851/1000 | Loss: 0.00000695
Iteration 852/1000 | Loss: 0.00000695
Iteration 853/1000 | Loss: 0.00000695
Iteration 854/1000 | Loss: 0.00000695
Iteration 855/1000 | Loss: 0.00000695
Iteration 856/1000 | Loss: 0.00000694
Iteration 857/1000 | Loss: 0.00000694
Iteration 858/1000 | Loss: 0.00000694
Iteration 859/1000 | Loss: 0.00000694
Iteration 860/1000 | Loss: 0.00000693
Iteration 861/1000 | Loss: 0.00000692
Iteration 862/1000 | Loss: 0.00000691
Iteration 863/1000 | Loss: 0.00000691
Iteration 864/1000 | Loss: 0.00000691
Iteration 865/1000 | Loss: 0.00000690
Iteration 866/1000 | Loss: 0.00000689
Iteration 867/1000 | Loss: 0.00000689
Iteration 868/1000 | Loss: 0.00000689
Iteration 869/1000 | Loss: 0.00000689
Iteration 870/1000 | Loss: 0.00000689
Iteration 871/1000 | Loss: 0.00000689
Iteration 872/1000 | Loss: 0.00000689
Iteration 873/1000 | Loss: 0.00000688
Iteration 874/1000 | Loss: 0.00000688
Iteration 875/1000 | Loss: 0.00000688
Iteration 876/1000 | Loss: 0.00000688
Iteration 877/1000 | Loss: 0.00000687
Iteration 878/1000 | Loss: 0.00000687
Iteration 879/1000 | Loss: 0.00000687
Iteration 880/1000 | Loss: 0.00000687
Iteration 881/1000 | Loss: 0.00000687
Iteration 882/1000 | Loss: 0.00000687
Iteration 883/1000 | Loss: 0.00000687
Iteration 884/1000 | Loss: 0.00000686
Iteration 885/1000 | Loss: 0.00000686
Iteration 886/1000 | Loss: 0.00000686
Iteration 887/1000 | Loss: 0.00000686
Iteration 888/1000 | Loss: 0.00000686
Iteration 889/1000 | Loss: 0.00000686
Iteration 890/1000 | Loss: 0.00000686
Iteration 891/1000 | Loss: 0.00000686
Iteration 892/1000 | Loss: 0.00000686
Iteration 893/1000 | Loss: 0.00000686
Iteration 894/1000 | Loss: 0.00000686
Iteration 895/1000 | Loss: 0.00000685
Iteration 896/1000 | Loss: 0.00000684
Iteration 897/1000 | Loss: 0.00000684
Iteration 898/1000 | Loss: 0.00000683
Iteration 899/1000 | Loss: 0.00000683
Iteration 900/1000 | Loss: 0.00000682
Iteration 901/1000 | Loss: 0.00000682
Iteration 902/1000 | Loss: 0.00000682
Iteration 903/1000 | Loss: 0.00000681
Iteration 904/1000 | Loss: 0.00000681
Iteration 905/1000 | Loss: 0.00000681
Iteration 906/1000 | Loss: 0.00000681
Iteration 907/1000 | Loss: 0.00000680
Iteration 908/1000 | Loss: 0.00000679
Iteration 909/1000 | Loss: 0.00000679
Iteration 910/1000 | Loss: 0.00000678
Iteration 911/1000 | Loss: 0.00000678
Iteration 912/1000 | Loss: 0.00000677
Iteration 913/1000 | Loss: 0.00000677
Iteration 914/1000 | Loss: 0.00000677
Iteration 915/1000 | Loss: 0.00000677
Iteration 916/1000 | Loss: 0.00000677
Iteration 917/1000 | Loss: 0.00000677
Iteration 918/1000 | Loss: 0.00000677
Iteration 919/1000 | Loss: 0.00000676
Iteration 920/1000 | Loss: 0.00000676
Iteration 921/1000 | Loss: 0.00000676
Iteration 922/1000 | Loss: 0.00000676
Iteration 923/1000 | Loss: 0.00000676
Iteration 924/1000 | Loss: 0.00000676
Iteration 925/1000 | Loss: 0.00000676
Iteration 926/1000 | Loss: 0.00000676
Iteration 927/1000 | Loss: 0.00000676
Iteration 928/1000 | Loss: 0.00000676
Iteration 929/1000 | Loss: 0.00000676
Iteration 930/1000 | Loss: 0.00000676
Iteration 931/1000 | Loss: 0.00000676
Iteration 932/1000 | Loss: 0.00000676
Iteration 933/1000 | Loss: 0.00000676
Iteration 934/1000 | Loss: 0.00000676
Iteration 935/1000 | Loss: 0.00000676
Iteration 936/1000 | Loss: 0.00000676
Iteration 937/1000 | Loss: 0.00000676
Iteration 938/1000 | Loss: 0.00000676
Iteration 939/1000 | Loss: 0.00000676
Iteration 940/1000 | Loss: 0.00000675
Iteration 941/1000 | Loss: 0.00000675
Iteration 942/1000 | Loss: 0.00000675
Iteration 943/1000 | Loss: 0.00000675
Iteration 944/1000 | Loss: 0.00000675
Iteration 945/1000 | Loss: 0.00000675
Iteration 946/1000 | Loss: 0.00000674
Iteration 947/1000 | Loss: 0.00000674
Iteration 948/1000 | Loss: 0.00000674
Iteration 949/1000 | Loss: 0.00000674
Iteration 950/1000 | Loss: 0.00000674
Iteration 951/1000 | Loss: 0.00000674
Iteration 952/1000 | Loss: 0.00000674
Iteration 953/1000 | Loss: 0.00000674
Iteration 954/1000 | Loss: 0.00000674
Iteration 955/1000 | Loss: 0.00000674
Iteration 956/1000 | Loss: 0.00000674
Iteration 957/1000 | Loss: 0.00000674
Iteration 958/1000 | Loss: 0.00000674
Iteration 959/1000 | Loss: 0.00000674
Iteration 960/1000 | Loss: 0.00000674
Iteration 961/1000 | Loss: 0.00000674
Iteration 962/1000 | Loss: 0.00000673
Iteration 963/1000 | Loss: 0.00000673
Iteration 964/1000 | Loss: 0.00000673
Iteration 965/1000 | Loss: 0.00000673
Iteration 966/1000 | Loss: 0.00000673
Iteration 967/1000 | Loss: 0.00000673
Iteration 968/1000 | Loss: 0.00000673
Iteration 969/1000 | Loss: 0.00000673
Iteration 970/1000 | Loss: 0.00000673
Iteration 971/1000 | Loss: 0.00000673
Iteration 972/1000 | Loss: 0.00000673
Iteration 973/1000 | Loss: 0.00000673
Iteration 974/1000 | Loss: 0.00000673
Iteration 975/1000 | Loss: 0.00000673
Iteration 976/1000 | Loss: 0.00000673
Iteration 977/1000 | Loss: 0.00000673
Iteration 978/1000 | Loss: 0.00000673
Iteration 979/1000 | Loss: 0.00000673
Iteration 980/1000 | Loss: 0.00000672
Iteration 981/1000 | Loss: 0.00000672
Iteration 982/1000 | Loss: 0.00000672
Iteration 983/1000 | Loss: 0.00000672
Iteration 984/1000 | Loss: 0.00000672
Iteration 985/1000 | Loss: 0.00000672
Iteration 986/1000 | Loss: 0.00000672
Iteration 987/1000 | Loss: 0.00000672
Iteration 988/1000 | Loss: 0.00000672
Iteration 989/1000 | Loss: 0.00000672
Iteration 990/1000 | Loss: 0.00000672
Iteration 991/1000 | Loss: 0.00000672
Iteration 992/1000 | Loss: 0.00000672
Iteration 993/1000 | Loss: 0.00000672
Iteration 994/1000 | Loss: 0.00000672
Iteration 995/1000 | Loss: 0.00000672
Iteration 996/1000 | Loss: 0.00000672
Iteration 997/1000 | Loss: 0.00000672
Iteration 998/1000 | Loss: 0.00000672
Iteration 999/1000 | Loss: 0.00000672
Iteration 1000/1000 | Loss: 0.00000672

Optimization complete. Final v2v error: 2.245504379272461 mm

Highest mean error: 2.984933376312256 mm for frame 104

Lowest mean error: 2.1485393047332764 mm for frame 6

Saving results

Total time: 1289.8991079330444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807589
Iteration 2/25 | Loss: 0.00201756
Iteration 3/25 | Loss: 0.00148705
Iteration 4/25 | Loss: 0.00133693
Iteration 5/25 | Loss: 0.00130801
Iteration 6/25 | Loss: 0.00122755
Iteration 7/25 | Loss: 0.00122001
Iteration 8/25 | Loss: 0.00120625
Iteration 9/25 | Loss: 0.00120504
Iteration 10/25 | Loss: 0.00120483
Iteration 11/25 | Loss: 0.00120476
Iteration 12/25 | Loss: 0.00120476
Iteration 13/25 | Loss: 0.00120476
Iteration 14/25 | Loss: 0.00120476
Iteration 15/25 | Loss: 0.00120476
Iteration 16/25 | Loss: 0.00120476
Iteration 17/25 | Loss: 0.00120476
Iteration 18/25 | Loss: 0.00120475
Iteration 19/25 | Loss: 0.00120475
Iteration 20/25 | Loss: 0.00120475
Iteration 21/25 | Loss: 0.00120475
Iteration 22/25 | Loss: 0.00120475
Iteration 23/25 | Loss: 0.00120475
Iteration 24/25 | Loss: 0.00120475
Iteration 25/25 | Loss: 0.00120475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31564534
Iteration 2/25 | Loss: 0.00055428
Iteration 3/25 | Loss: 0.00055426
Iteration 4/25 | Loss: 0.00055426
Iteration 5/25 | Loss: 0.00055426
Iteration 6/25 | Loss: 0.00055426
Iteration 7/25 | Loss: 0.00055426
Iteration 8/25 | Loss: 0.00055426
Iteration 9/25 | Loss: 0.00055426
Iteration 10/25 | Loss: 0.00055426
Iteration 11/25 | Loss: 0.00055425
Iteration 12/25 | Loss: 0.00055425
Iteration 13/25 | Loss: 0.00055425
Iteration 14/25 | Loss: 0.00055425
Iteration 15/25 | Loss: 0.00055425
Iteration 16/25 | Loss: 0.00055425
Iteration 17/25 | Loss: 0.00055425
Iteration 18/25 | Loss: 0.00055425
Iteration 19/25 | Loss: 0.00055425
Iteration 20/25 | Loss: 0.00055425
Iteration 21/25 | Loss: 0.00055425
Iteration 22/25 | Loss: 0.00055425
Iteration 23/25 | Loss: 0.00055425
Iteration 24/25 | Loss: 0.00055425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005542546277865767, 0.0005542546277865767, 0.0005542546277865767, 0.0005542546277865767, 0.0005542546277865767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005542546277865767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055425
Iteration 2/1000 | Loss: 0.00003625
Iteration 3/1000 | Loss: 0.00002122
Iteration 4/1000 | Loss: 0.00001836
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001696
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001650
Iteration 9/1000 | Loss: 0.00001630
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001623
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001622
Iteration 14/1000 | Loss: 0.00001618
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001616
Iteration 19/1000 | Loss: 0.00001613
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001612
Iteration 22/1000 | Loss: 0.00001612
Iteration 23/1000 | Loss: 0.00001612
Iteration 24/1000 | Loss: 0.00001612
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001612
Iteration 30/1000 | Loss: 0.00001611
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001610
Iteration 34/1000 | Loss: 0.00001610
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001607
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00001598
Iteration 47/1000 | Loss: 0.00001597
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001594
Iteration 53/1000 | Loss: 0.00001594
Iteration 54/1000 | Loss: 0.00001594
Iteration 55/1000 | Loss: 0.00001594
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001586
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001584
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001583
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001581
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001578
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001577
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001577
Iteration 106/1000 | Loss: 0.00001577
Iteration 107/1000 | Loss: 0.00001576
Iteration 108/1000 | Loss: 0.00001576
Iteration 109/1000 | Loss: 0.00001576
Iteration 110/1000 | Loss: 0.00001576
Iteration 111/1000 | Loss: 0.00001576
Iteration 112/1000 | Loss: 0.00001576
Iteration 113/1000 | Loss: 0.00001576
Iteration 114/1000 | Loss: 0.00001576
Iteration 115/1000 | Loss: 0.00001576
Iteration 116/1000 | Loss: 0.00001576
Iteration 117/1000 | Loss: 0.00001576
Iteration 118/1000 | Loss: 0.00001576
Iteration 119/1000 | Loss: 0.00001576
Iteration 120/1000 | Loss: 0.00001576
Iteration 121/1000 | Loss: 0.00001576
Iteration 122/1000 | Loss: 0.00001576
Iteration 123/1000 | Loss: 0.00001576
Iteration 124/1000 | Loss: 0.00001576
Iteration 125/1000 | Loss: 0.00001576
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001576
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001576
Iteration 131/1000 | Loss: 0.00001576
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001576
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001576
Iteration 141/1000 | Loss: 0.00001576
Iteration 142/1000 | Loss: 0.00001576
Iteration 143/1000 | Loss: 0.00001576
Iteration 144/1000 | Loss: 0.00001576
Iteration 145/1000 | Loss: 0.00001576
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001576
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.576067734276876e-05, 1.576067734276876e-05, 1.576067734276876e-05, 1.576067734276876e-05, 1.576067734276876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.576067734276876e-05

Optimization complete. Final v2v error: 3.3022663593292236 mm

Highest mean error: 3.6390316486358643 mm for frame 7

Lowest mean error: 3.1730728149414062 mm for frame 88

Saving results

Total time: 40.74862742424011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887427
Iteration 2/25 | Loss: 0.00140912
Iteration 3/25 | Loss: 0.00119188
Iteration 4/25 | Loss: 0.00117666
Iteration 5/25 | Loss: 0.00116998
Iteration 6/25 | Loss: 0.00117054
Iteration 7/25 | Loss: 0.00116494
Iteration 8/25 | Loss: 0.00116298
Iteration 9/25 | Loss: 0.00116189
Iteration 10/25 | Loss: 0.00116167
Iteration 11/25 | Loss: 0.00116151
Iteration 12/25 | Loss: 0.00116579
Iteration 13/25 | Loss: 0.00116719
Iteration 14/25 | Loss: 0.00116576
Iteration 15/25 | Loss: 0.00116405
Iteration 16/25 | Loss: 0.00116505
Iteration 17/25 | Loss: 0.00116346
Iteration 18/25 | Loss: 0.00116511
Iteration 19/25 | Loss: 0.00116389
Iteration 20/25 | Loss: 0.00116205
Iteration 21/25 | Loss: 0.00116171
Iteration 22/25 | Loss: 0.00116153
Iteration 23/25 | Loss: 0.00116144
Iteration 24/25 | Loss: 0.00116134
Iteration 25/25 | Loss: 0.00116131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.37072945
Iteration 2/25 | Loss: 0.00060445
Iteration 3/25 | Loss: 0.00060432
Iteration 4/25 | Loss: 0.00060432
Iteration 5/25 | Loss: 0.00060432
Iteration 6/25 | Loss: 0.00060432
Iteration 7/25 | Loss: 0.00060432
Iteration 8/25 | Loss: 0.00060432
Iteration 9/25 | Loss: 0.00060432
Iteration 10/25 | Loss: 0.00060432
Iteration 11/25 | Loss: 0.00060432
Iteration 12/25 | Loss: 0.00060432
Iteration 13/25 | Loss: 0.00060432
Iteration 14/25 | Loss: 0.00060432
Iteration 15/25 | Loss: 0.00060432
Iteration 16/25 | Loss: 0.00060432
Iteration 17/25 | Loss: 0.00060432
Iteration 18/25 | Loss: 0.00060432
Iteration 19/25 | Loss: 0.00060432
Iteration 20/25 | Loss: 0.00060432
Iteration 21/25 | Loss: 0.00060432
Iteration 22/25 | Loss: 0.00060432
Iteration 23/25 | Loss: 0.00060432
Iteration 24/25 | Loss: 0.00060432
Iteration 25/25 | Loss: 0.00060432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060432
Iteration 2/1000 | Loss: 0.00003848
Iteration 3/1000 | Loss: 0.00002807
Iteration 4/1000 | Loss: 0.00002567
Iteration 5/1000 | Loss: 0.00002427
Iteration 6/1000 | Loss: 0.00002348
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002193
Iteration 10/1000 | Loss: 0.00002167
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002128
Iteration 19/1000 | Loss: 0.00002127
Iteration 20/1000 | Loss: 0.00002127
Iteration 21/1000 | Loss: 0.00002126
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002125
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002122
Iteration 26/1000 | Loss: 0.00002121
Iteration 27/1000 | Loss: 0.00002120
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002116
Iteration 33/1000 | Loss: 0.00002116
Iteration 34/1000 | Loss: 0.00002115
Iteration 35/1000 | Loss: 0.00002115
Iteration 36/1000 | Loss: 0.00002115
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002112
Iteration 48/1000 | Loss: 0.00002111
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002107
Iteration 58/1000 | Loss: 0.00002107
Iteration 59/1000 | Loss: 0.00002106
Iteration 60/1000 | Loss: 0.00002106
Iteration 61/1000 | Loss: 0.00002105
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002105
Iteration 65/1000 | Loss: 0.00002105
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002105
Iteration 68/1000 | Loss: 0.00002105
Iteration 69/1000 | Loss: 0.00002105
Iteration 70/1000 | Loss: 0.00002105
Iteration 71/1000 | Loss: 0.00002104
Iteration 72/1000 | Loss: 0.00002104
Iteration 73/1000 | Loss: 0.00002104
Iteration 74/1000 | Loss: 0.00002104
Iteration 75/1000 | Loss: 0.00002103
Iteration 76/1000 | Loss: 0.00002103
Iteration 77/1000 | Loss: 0.00002103
Iteration 78/1000 | Loss: 0.00002102
Iteration 79/1000 | Loss: 0.00002102
Iteration 80/1000 | Loss: 0.00002102
Iteration 81/1000 | Loss: 0.00002101
Iteration 82/1000 | Loss: 0.00002101
Iteration 83/1000 | Loss: 0.00002101
Iteration 84/1000 | Loss: 0.00002101
Iteration 85/1000 | Loss: 0.00002100
Iteration 86/1000 | Loss: 0.00002100
Iteration 87/1000 | Loss: 0.00002100
Iteration 88/1000 | Loss: 0.00002100
Iteration 89/1000 | Loss: 0.00002100
Iteration 90/1000 | Loss: 0.00002099
Iteration 91/1000 | Loss: 0.00002099
Iteration 92/1000 | Loss: 0.00002099
Iteration 93/1000 | Loss: 0.00002098
Iteration 94/1000 | Loss: 0.00002098
Iteration 95/1000 | Loss: 0.00002098
Iteration 96/1000 | Loss: 0.00002098
Iteration 97/1000 | Loss: 0.00002097
Iteration 98/1000 | Loss: 0.00002097
Iteration 99/1000 | Loss: 0.00002097
Iteration 100/1000 | Loss: 0.00002097
Iteration 101/1000 | Loss: 0.00002097
Iteration 102/1000 | Loss: 0.00002097
Iteration 103/1000 | Loss: 0.00002096
Iteration 104/1000 | Loss: 0.00002096
Iteration 105/1000 | Loss: 0.00002096
Iteration 106/1000 | Loss: 0.00002096
Iteration 107/1000 | Loss: 0.00002096
Iteration 108/1000 | Loss: 0.00002096
Iteration 109/1000 | Loss: 0.00002096
Iteration 110/1000 | Loss: 0.00002096
Iteration 111/1000 | Loss: 0.00002095
Iteration 112/1000 | Loss: 0.00002095
Iteration 113/1000 | Loss: 0.00002095
Iteration 114/1000 | Loss: 0.00002095
Iteration 115/1000 | Loss: 0.00002095
Iteration 116/1000 | Loss: 0.00002095
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002095
Iteration 120/1000 | Loss: 0.00002095
Iteration 121/1000 | Loss: 0.00002095
Iteration 122/1000 | Loss: 0.00002094
Iteration 123/1000 | Loss: 0.00002094
Iteration 124/1000 | Loss: 0.00002094
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002094
Iteration 127/1000 | Loss: 0.00002094
Iteration 128/1000 | Loss: 0.00002094
Iteration 129/1000 | Loss: 0.00002094
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002093
Iteration 133/1000 | Loss: 0.00002093
Iteration 134/1000 | Loss: 0.00002093
Iteration 135/1000 | Loss: 0.00002093
Iteration 136/1000 | Loss: 0.00002093
Iteration 137/1000 | Loss: 0.00002093
Iteration 138/1000 | Loss: 0.00002093
Iteration 139/1000 | Loss: 0.00002093
Iteration 140/1000 | Loss: 0.00002092
Iteration 141/1000 | Loss: 0.00002092
Iteration 142/1000 | Loss: 0.00002092
Iteration 143/1000 | Loss: 0.00002092
Iteration 144/1000 | Loss: 0.00002092
Iteration 145/1000 | Loss: 0.00002092
Iteration 146/1000 | Loss: 0.00002092
Iteration 147/1000 | Loss: 0.00002092
Iteration 148/1000 | Loss: 0.00002092
Iteration 149/1000 | Loss: 0.00002092
Iteration 150/1000 | Loss: 0.00002091
Iteration 151/1000 | Loss: 0.00018507
Iteration 152/1000 | Loss: 0.00002541
Iteration 153/1000 | Loss: 0.00002393
Iteration 154/1000 | Loss: 0.00002299
Iteration 155/1000 | Loss: 0.00002221
Iteration 156/1000 | Loss: 0.00002188
Iteration 157/1000 | Loss: 0.00002161
Iteration 158/1000 | Loss: 0.00002142
Iteration 159/1000 | Loss: 0.00002133
Iteration 160/1000 | Loss: 0.00002132
Iteration 161/1000 | Loss: 0.00002128
Iteration 162/1000 | Loss: 0.00002127
Iteration 163/1000 | Loss: 0.00002120
Iteration 164/1000 | Loss: 0.00002118
Iteration 165/1000 | Loss: 0.00002117
Iteration 166/1000 | Loss: 0.00002116
Iteration 167/1000 | Loss: 0.00002115
Iteration 168/1000 | Loss: 0.00002114
Iteration 169/1000 | Loss: 0.00002114
Iteration 170/1000 | Loss: 0.00002114
Iteration 171/1000 | Loss: 0.00002113
Iteration 172/1000 | Loss: 0.00002113
Iteration 173/1000 | Loss: 0.00002113
Iteration 174/1000 | Loss: 0.00002113
Iteration 175/1000 | Loss: 0.00002112
Iteration 176/1000 | Loss: 0.00002112
Iteration 177/1000 | Loss: 0.00002112
Iteration 178/1000 | Loss: 0.00002112
Iteration 179/1000 | Loss: 0.00002112
Iteration 180/1000 | Loss: 0.00002111
Iteration 181/1000 | Loss: 0.00002111
Iteration 182/1000 | Loss: 0.00002111
Iteration 183/1000 | Loss: 0.00002111
Iteration 184/1000 | Loss: 0.00002111
Iteration 185/1000 | Loss: 0.00002111
Iteration 186/1000 | Loss: 0.00002111
Iteration 187/1000 | Loss: 0.00002110
Iteration 188/1000 | Loss: 0.00002110
Iteration 189/1000 | Loss: 0.00002110
Iteration 190/1000 | Loss: 0.00002110
Iteration 191/1000 | Loss: 0.00002110
Iteration 192/1000 | Loss: 0.00002110
Iteration 193/1000 | Loss: 0.00002110
Iteration 194/1000 | Loss: 0.00002110
Iteration 195/1000 | Loss: 0.00002110
Iteration 196/1000 | Loss: 0.00002110
Iteration 197/1000 | Loss: 0.00002110
Iteration 198/1000 | Loss: 0.00002109
Iteration 199/1000 | Loss: 0.00002109
Iteration 200/1000 | Loss: 0.00002109
Iteration 201/1000 | Loss: 0.00002109
Iteration 202/1000 | Loss: 0.00002109
Iteration 203/1000 | Loss: 0.00002109
Iteration 204/1000 | Loss: 0.00002109
Iteration 205/1000 | Loss: 0.00002109
Iteration 206/1000 | Loss: 0.00002108
Iteration 207/1000 | Loss: 0.00002108
Iteration 208/1000 | Loss: 0.00002108
Iteration 209/1000 | Loss: 0.00002108
Iteration 210/1000 | Loss: 0.00002108
Iteration 211/1000 | Loss: 0.00002108
Iteration 212/1000 | Loss: 0.00002108
Iteration 213/1000 | Loss: 0.00002108
Iteration 214/1000 | Loss: 0.00002107
Iteration 215/1000 | Loss: 0.00002107
Iteration 216/1000 | Loss: 0.00002107
Iteration 217/1000 | Loss: 0.00002107
Iteration 218/1000 | Loss: 0.00002107
Iteration 219/1000 | Loss: 0.00002107
Iteration 220/1000 | Loss: 0.00002107
Iteration 221/1000 | Loss: 0.00002107
Iteration 222/1000 | Loss: 0.00002107
Iteration 223/1000 | Loss: 0.00002107
Iteration 224/1000 | Loss: 0.00002107
Iteration 225/1000 | Loss: 0.00002107
Iteration 226/1000 | Loss: 0.00002107
Iteration 227/1000 | Loss: 0.00002106
Iteration 228/1000 | Loss: 0.00002106
Iteration 229/1000 | Loss: 0.00002106
Iteration 230/1000 | Loss: 0.00002106
Iteration 231/1000 | Loss: 0.00002106
Iteration 232/1000 | Loss: 0.00002106
Iteration 233/1000 | Loss: 0.00002106
Iteration 234/1000 | Loss: 0.00002106
Iteration 235/1000 | Loss: 0.00002106
Iteration 236/1000 | Loss: 0.00002106
Iteration 237/1000 | Loss: 0.00002106
Iteration 238/1000 | Loss: 0.00002106
Iteration 239/1000 | Loss: 0.00002106
Iteration 240/1000 | Loss: 0.00002105
Iteration 241/1000 | Loss: 0.00002105
Iteration 242/1000 | Loss: 0.00002105
Iteration 243/1000 | Loss: 0.00002105
Iteration 244/1000 | Loss: 0.00002105
Iteration 245/1000 | Loss: 0.00002105
Iteration 246/1000 | Loss: 0.00002105
Iteration 247/1000 | Loss: 0.00002105
Iteration 248/1000 | Loss: 0.00002105
Iteration 249/1000 | Loss: 0.00002104
Iteration 250/1000 | Loss: 0.00002104
Iteration 251/1000 | Loss: 0.00002104
Iteration 252/1000 | Loss: 0.00002104
Iteration 253/1000 | Loss: 0.00002104
Iteration 254/1000 | Loss: 0.00002104
Iteration 255/1000 | Loss: 0.00002104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.1044190361863002e-05, 2.1044190361863002e-05, 2.1044190361863002e-05, 2.1044190361863002e-05, 2.1044190361863002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1044190361863002e-05

Optimization complete. Final v2v error: 3.7651920318603516 mm

Highest mean error: 6.6450419425964355 mm for frame 112

Lowest mean error: 3.123197555541992 mm for frame 206

Saving results

Total time: 100.69139361381531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996333
Iteration 2/25 | Loss: 0.00131504
Iteration 3/25 | Loss: 0.00116197
Iteration 4/25 | Loss: 0.00112240
Iteration 5/25 | Loss: 0.00111697
Iteration 6/25 | Loss: 0.00111567
Iteration 7/25 | Loss: 0.00111531
Iteration 8/25 | Loss: 0.00111531
Iteration 9/25 | Loss: 0.00111531
Iteration 10/25 | Loss: 0.00111531
Iteration 11/25 | Loss: 0.00111531
Iteration 12/25 | Loss: 0.00111531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011153052328154445, 0.0011153052328154445, 0.0011153052328154445, 0.0011153052328154445, 0.0011153052328154445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011153052328154445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.10672474
Iteration 2/25 | Loss: 0.00085611
Iteration 3/25 | Loss: 0.00083547
Iteration 4/25 | Loss: 0.00083547
Iteration 5/25 | Loss: 0.00083547
Iteration 6/25 | Loss: 0.00083547
Iteration 7/25 | Loss: 0.00083547
Iteration 8/25 | Loss: 0.00083546
Iteration 9/25 | Loss: 0.00083546
Iteration 10/25 | Loss: 0.00083546
Iteration 11/25 | Loss: 0.00083546
Iteration 12/25 | Loss: 0.00083546
Iteration 13/25 | Loss: 0.00083546
Iteration 14/25 | Loss: 0.00083546
Iteration 15/25 | Loss: 0.00083546
Iteration 16/25 | Loss: 0.00083546
Iteration 17/25 | Loss: 0.00083546
Iteration 18/25 | Loss: 0.00083546
Iteration 19/25 | Loss: 0.00083546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000835463753901422, 0.000835463753901422, 0.000835463753901422, 0.000835463753901422, 0.000835463753901422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000835463753901422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083546
Iteration 2/1000 | Loss: 0.00005475
Iteration 3/1000 | Loss: 0.00001758
Iteration 4/1000 | Loss: 0.00004864
Iteration 5/1000 | Loss: 0.00003290
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001206
Iteration 33/1000 | Loss: 0.00001206
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001186
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001186
Iteration 141/1000 | Loss: 0.00001186
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001185
Iteration 145/1000 | Loss: 0.00001185
Iteration 146/1000 | Loss: 0.00001185
Iteration 147/1000 | Loss: 0.00001185
Iteration 148/1000 | Loss: 0.00001185
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001184
Iteration 151/1000 | Loss: 0.00001184
Iteration 152/1000 | Loss: 0.00001184
Iteration 153/1000 | Loss: 0.00001184
Iteration 154/1000 | Loss: 0.00001184
Iteration 155/1000 | Loss: 0.00001184
Iteration 156/1000 | Loss: 0.00001184
Iteration 157/1000 | Loss: 0.00001184
Iteration 158/1000 | Loss: 0.00001184
Iteration 159/1000 | Loss: 0.00001184
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001182
Iteration 171/1000 | Loss: 0.00001182
Iteration 172/1000 | Loss: 0.00001182
Iteration 173/1000 | Loss: 0.00001182
Iteration 174/1000 | Loss: 0.00001182
Iteration 175/1000 | Loss: 0.00001182
Iteration 176/1000 | Loss: 0.00001182
Iteration 177/1000 | Loss: 0.00001182
Iteration 178/1000 | Loss: 0.00001182
Iteration 179/1000 | Loss: 0.00001182
Iteration 180/1000 | Loss: 0.00001182
Iteration 181/1000 | Loss: 0.00001182
Iteration 182/1000 | Loss: 0.00001182
Iteration 183/1000 | Loss: 0.00001182
Iteration 184/1000 | Loss: 0.00001181
Iteration 185/1000 | Loss: 0.00001181
Iteration 186/1000 | Loss: 0.00001181
Iteration 187/1000 | Loss: 0.00001181
Iteration 188/1000 | Loss: 0.00001181
Iteration 189/1000 | Loss: 0.00001181
Iteration 190/1000 | Loss: 0.00001181
Iteration 191/1000 | Loss: 0.00001181
Iteration 192/1000 | Loss: 0.00001181
Iteration 193/1000 | Loss: 0.00001181
Iteration 194/1000 | Loss: 0.00001181
Iteration 195/1000 | Loss: 0.00001180
Iteration 196/1000 | Loss: 0.00001180
Iteration 197/1000 | Loss: 0.00001180
Iteration 198/1000 | Loss: 0.00001180
Iteration 199/1000 | Loss: 0.00001180
Iteration 200/1000 | Loss: 0.00001180
Iteration 201/1000 | Loss: 0.00001180
Iteration 202/1000 | Loss: 0.00001180
Iteration 203/1000 | Loss: 0.00001180
Iteration 204/1000 | Loss: 0.00001180
Iteration 205/1000 | Loss: 0.00001180
Iteration 206/1000 | Loss: 0.00001180
Iteration 207/1000 | Loss: 0.00001179
Iteration 208/1000 | Loss: 0.00001179
Iteration 209/1000 | Loss: 0.00001179
Iteration 210/1000 | Loss: 0.00001179
Iteration 211/1000 | Loss: 0.00001179
Iteration 212/1000 | Loss: 0.00001179
Iteration 213/1000 | Loss: 0.00001178
Iteration 214/1000 | Loss: 0.00001178
Iteration 215/1000 | Loss: 0.00001178
Iteration 216/1000 | Loss: 0.00001178
Iteration 217/1000 | Loss: 0.00001178
Iteration 218/1000 | Loss: 0.00001178
Iteration 219/1000 | Loss: 0.00001178
Iteration 220/1000 | Loss: 0.00001178
Iteration 221/1000 | Loss: 0.00001178
Iteration 222/1000 | Loss: 0.00001178
Iteration 223/1000 | Loss: 0.00001178
Iteration 224/1000 | Loss: 0.00001178
Iteration 225/1000 | Loss: 0.00001177
Iteration 226/1000 | Loss: 0.00001177
Iteration 227/1000 | Loss: 0.00001177
Iteration 228/1000 | Loss: 0.00001177
Iteration 229/1000 | Loss: 0.00001177
Iteration 230/1000 | Loss: 0.00001177
Iteration 231/1000 | Loss: 0.00001177
Iteration 232/1000 | Loss: 0.00001177
Iteration 233/1000 | Loss: 0.00001177
Iteration 234/1000 | Loss: 0.00001177
Iteration 235/1000 | Loss: 0.00001176
Iteration 236/1000 | Loss: 0.00001176
Iteration 237/1000 | Loss: 0.00001176
Iteration 238/1000 | Loss: 0.00001176
Iteration 239/1000 | Loss: 0.00001176
Iteration 240/1000 | Loss: 0.00001175
Iteration 241/1000 | Loss: 0.00001175
Iteration 242/1000 | Loss: 0.00001175
Iteration 243/1000 | Loss: 0.00001175
Iteration 244/1000 | Loss: 0.00001174
Iteration 245/1000 | Loss: 0.00001174
Iteration 246/1000 | Loss: 0.00001174
Iteration 247/1000 | Loss: 0.00001174
Iteration 248/1000 | Loss: 0.00001174
Iteration 249/1000 | Loss: 0.00001174
Iteration 250/1000 | Loss: 0.00001174
Iteration 251/1000 | Loss: 0.00001174
Iteration 252/1000 | Loss: 0.00001174
Iteration 253/1000 | Loss: 0.00001174
Iteration 254/1000 | Loss: 0.00001174
Iteration 255/1000 | Loss: 0.00001174
Iteration 256/1000 | Loss: 0.00001174
Iteration 257/1000 | Loss: 0.00001174
Iteration 258/1000 | Loss: 0.00001174
Iteration 259/1000 | Loss: 0.00001174
Iteration 260/1000 | Loss: 0.00001174
Iteration 261/1000 | Loss: 0.00001174
Iteration 262/1000 | Loss: 0.00001174
Iteration 263/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.1740647096303292e-05, 1.1740647096303292e-05, 1.1740647096303292e-05, 1.1740647096303292e-05, 1.1740647096303292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1740647096303292e-05

Optimization complete. Final v2v error: 2.870882749557495 mm

Highest mean error: 3.3424148559570312 mm for frame 172

Lowest mean error: 2.5558714866638184 mm for frame 138

Saving results

Total time: 49.4617817401886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346471
Iteration 2/25 | Loss: 0.00117209
Iteration 3/25 | Loss: 0.00107602
Iteration 4/25 | Loss: 0.00105653
Iteration 5/25 | Loss: 0.00104842
Iteration 6/25 | Loss: 0.00104680
Iteration 7/25 | Loss: 0.00104680
Iteration 8/25 | Loss: 0.00104680
Iteration 9/25 | Loss: 0.00104680
Iteration 10/25 | Loss: 0.00104680
Iteration 11/25 | Loss: 0.00104680
Iteration 12/25 | Loss: 0.00104680
Iteration 13/25 | Loss: 0.00104680
Iteration 14/25 | Loss: 0.00104680
Iteration 15/25 | Loss: 0.00104680
Iteration 16/25 | Loss: 0.00104680
Iteration 17/25 | Loss: 0.00104680
Iteration 18/25 | Loss: 0.00104680
Iteration 19/25 | Loss: 0.00104680
Iteration 20/25 | Loss: 0.00104680
Iteration 21/25 | Loss: 0.00104680
Iteration 22/25 | Loss: 0.00104680
Iteration 23/25 | Loss: 0.00104680
Iteration 24/25 | Loss: 0.00104680
Iteration 25/25 | Loss: 0.00104680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33286250
Iteration 2/25 | Loss: 0.00075941
Iteration 3/25 | Loss: 0.00075940
Iteration 4/25 | Loss: 0.00075940
Iteration 5/25 | Loss: 0.00075940
Iteration 6/25 | Loss: 0.00075940
Iteration 7/25 | Loss: 0.00075940
Iteration 8/25 | Loss: 0.00075940
Iteration 9/25 | Loss: 0.00075940
Iteration 10/25 | Loss: 0.00075940
Iteration 11/25 | Loss: 0.00075940
Iteration 12/25 | Loss: 0.00075940
Iteration 13/25 | Loss: 0.00075940
Iteration 14/25 | Loss: 0.00075940
Iteration 15/25 | Loss: 0.00075940
Iteration 16/25 | Loss: 0.00075940
Iteration 17/25 | Loss: 0.00075940
Iteration 18/25 | Loss: 0.00075940
Iteration 19/25 | Loss: 0.00075940
Iteration 20/25 | Loss: 0.00075940
Iteration 21/25 | Loss: 0.00075940
Iteration 22/25 | Loss: 0.00075940
Iteration 23/25 | Loss: 0.00075940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007593967602588236, 0.0007593967602588236, 0.0007593967602588236, 0.0007593967602588236, 0.0007593967602588236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007593967602588236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075940
Iteration 2/1000 | Loss: 0.00004537
Iteration 3/1000 | Loss: 0.00003006
Iteration 4/1000 | Loss: 0.00002374
Iteration 5/1000 | Loss: 0.00002136
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001734
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001604
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001589
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001586
Iteration 37/1000 | Loss: 0.00001585
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001584
Iteration 40/1000 | Loss: 0.00001584
Iteration 41/1000 | Loss: 0.00001583
Iteration 42/1000 | Loss: 0.00001582
Iteration 43/1000 | Loss: 0.00001582
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001579
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001578
Iteration 48/1000 | Loss: 0.00001578
Iteration 49/1000 | Loss: 0.00001578
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001576
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001573
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001570
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001569
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001569
Iteration 90/1000 | Loss: 0.00001569
Iteration 91/1000 | Loss: 0.00001569
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001568
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001568
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001567
Iteration 101/1000 | Loss: 0.00001567
Iteration 102/1000 | Loss: 0.00001567
Iteration 103/1000 | Loss: 0.00001567
Iteration 104/1000 | Loss: 0.00001566
Iteration 105/1000 | Loss: 0.00001566
Iteration 106/1000 | Loss: 0.00001566
Iteration 107/1000 | Loss: 0.00001566
Iteration 108/1000 | Loss: 0.00001565
Iteration 109/1000 | Loss: 0.00001565
Iteration 110/1000 | Loss: 0.00001565
Iteration 111/1000 | Loss: 0.00001565
Iteration 112/1000 | Loss: 0.00001564
Iteration 113/1000 | Loss: 0.00001564
Iteration 114/1000 | Loss: 0.00001564
Iteration 115/1000 | Loss: 0.00001564
Iteration 116/1000 | Loss: 0.00001563
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001562
Iteration 122/1000 | Loss: 0.00001562
Iteration 123/1000 | Loss: 0.00001562
Iteration 124/1000 | Loss: 0.00001562
Iteration 125/1000 | Loss: 0.00001562
Iteration 126/1000 | Loss: 0.00001562
Iteration 127/1000 | Loss: 0.00001562
Iteration 128/1000 | Loss: 0.00001562
Iteration 129/1000 | Loss: 0.00001561
Iteration 130/1000 | Loss: 0.00001561
Iteration 131/1000 | Loss: 0.00001561
Iteration 132/1000 | Loss: 0.00001561
Iteration 133/1000 | Loss: 0.00001561
Iteration 134/1000 | Loss: 0.00001561
Iteration 135/1000 | Loss: 0.00001561
Iteration 136/1000 | Loss: 0.00001561
Iteration 137/1000 | Loss: 0.00001561
Iteration 138/1000 | Loss: 0.00001561
Iteration 139/1000 | Loss: 0.00001560
Iteration 140/1000 | Loss: 0.00001560
Iteration 141/1000 | Loss: 0.00001560
Iteration 142/1000 | Loss: 0.00001560
Iteration 143/1000 | Loss: 0.00001560
Iteration 144/1000 | Loss: 0.00001560
Iteration 145/1000 | Loss: 0.00001560
Iteration 146/1000 | Loss: 0.00001560
Iteration 147/1000 | Loss: 0.00001560
Iteration 148/1000 | Loss: 0.00001560
Iteration 149/1000 | Loss: 0.00001560
Iteration 150/1000 | Loss: 0.00001560
Iteration 151/1000 | Loss: 0.00001560
Iteration 152/1000 | Loss: 0.00001560
Iteration 153/1000 | Loss: 0.00001559
Iteration 154/1000 | Loss: 0.00001559
Iteration 155/1000 | Loss: 0.00001559
Iteration 156/1000 | Loss: 0.00001559
Iteration 157/1000 | Loss: 0.00001559
Iteration 158/1000 | Loss: 0.00001559
Iteration 159/1000 | Loss: 0.00001559
Iteration 160/1000 | Loss: 0.00001559
Iteration 161/1000 | Loss: 0.00001559
Iteration 162/1000 | Loss: 0.00001559
Iteration 163/1000 | Loss: 0.00001559
Iteration 164/1000 | Loss: 0.00001559
Iteration 165/1000 | Loss: 0.00001559
Iteration 166/1000 | Loss: 0.00001559
Iteration 167/1000 | Loss: 0.00001559
Iteration 168/1000 | Loss: 0.00001559
Iteration 169/1000 | Loss: 0.00001559
Iteration 170/1000 | Loss: 0.00001559
Iteration 171/1000 | Loss: 0.00001558
Iteration 172/1000 | Loss: 0.00001558
Iteration 173/1000 | Loss: 0.00001558
Iteration 174/1000 | Loss: 0.00001558
Iteration 175/1000 | Loss: 0.00001558
Iteration 176/1000 | Loss: 0.00001558
Iteration 177/1000 | Loss: 0.00001558
Iteration 178/1000 | Loss: 0.00001558
Iteration 179/1000 | Loss: 0.00001558
Iteration 180/1000 | Loss: 0.00001558
Iteration 181/1000 | Loss: 0.00001558
Iteration 182/1000 | Loss: 0.00001558
Iteration 183/1000 | Loss: 0.00001558
Iteration 184/1000 | Loss: 0.00001558
Iteration 185/1000 | Loss: 0.00001558
Iteration 186/1000 | Loss: 0.00001558
Iteration 187/1000 | Loss: 0.00001558
Iteration 188/1000 | Loss: 0.00001558
Iteration 189/1000 | Loss: 0.00001557
Iteration 190/1000 | Loss: 0.00001557
Iteration 191/1000 | Loss: 0.00001557
Iteration 192/1000 | Loss: 0.00001557
Iteration 193/1000 | Loss: 0.00001557
Iteration 194/1000 | Loss: 0.00001557
Iteration 195/1000 | Loss: 0.00001557
Iteration 196/1000 | Loss: 0.00001557
Iteration 197/1000 | Loss: 0.00001557
Iteration 198/1000 | Loss: 0.00001557
Iteration 199/1000 | Loss: 0.00001556
Iteration 200/1000 | Loss: 0.00001556
Iteration 201/1000 | Loss: 0.00001556
Iteration 202/1000 | Loss: 0.00001556
Iteration 203/1000 | Loss: 0.00001556
Iteration 204/1000 | Loss: 0.00001556
Iteration 205/1000 | Loss: 0.00001556
Iteration 206/1000 | Loss: 0.00001556
Iteration 207/1000 | Loss: 0.00001556
Iteration 208/1000 | Loss: 0.00001556
Iteration 209/1000 | Loss: 0.00001556
Iteration 210/1000 | Loss: 0.00001556
Iteration 211/1000 | Loss: 0.00001556
Iteration 212/1000 | Loss: 0.00001556
Iteration 213/1000 | Loss: 0.00001556
Iteration 214/1000 | Loss: 0.00001556
Iteration 215/1000 | Loss: 0.00001556
Iteration 216/1000 | Loss: 0.00001556
Iteration 217/1000 | Loss: 0.00001556
Iteration 218/1000 | Loss: 0.00001556
Iteration 219/1000 | Loss: 0.00001556
Iteration 220/1000 | Loss: 0.00001556
Iteration 221/1000 | Loss: 0.00001556
Iteration 222/1000 | Loss: 0.00001556
Iteration 223/1000 | Loss: 0.00001556
Iteration 224/1000 | Loss: 0.00001556
Iteration 225/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.555645758344326e-05, 1.555645758344326e-05, 1.555645758344326e-05, 1.555645758344326e-05, 1.555645758344326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555645758344326e-05

Optimization complete. Final v2v error: 3.260960102081299 mm

Highest mean error: 4.0395402908325195 mm for frame 160

Lowest mean error: 2.708786964416504 mm for frame 196

Saving results

Total time: 51.22508215904236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820367
Iteration 2/25 | Loss: 0.00117575
Iteration 3/25 | Loss: 0.00104879
Iteration 4/25 | Loss: 0.00103231
Iteration 5/25 | Loss: 0.00102883
Iteration 6/25 | Loss: 0.00102883
Iteration 7/25 | Loss: 0.00102883
Iteration 8/25 | Loss: 0.00102883
Iteration 9/25 | Loss: 0.00102883
Iteration 10/25 | Loss: 0.00102883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001028832863084972, 0.001028832863084972, 0.001028832863084972, 0.001028832863084972, 0.001028832863084972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001028832863084972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37507010
Iteration 2/25 | Loss: 0.00061047
Iteration 3/25 | Loss: 0.00061046
Iteration 4/25 | Loss: 0.00061046
Iteration 5/25 | Loss: 0.00061046
Iteration 6/25 | Loss: 0.00061046
Iteration 7/25 | Loss: 0.00061046
Iteration 8/25 | Loss: 0.00061046
Iteration 9/25 | Loss: 0.00061046
Iteration 10/25 | Loss: 0.00061046
Iteration 11/25 | Loss: 0.00061046
Iteration 12/25 | Loss: 0.00061046
Iteration 13/25 | Loss: 0.00061046
Iteration 14/25 | Loss: 0.00061046
Iteration 15/25 | Loss: 0.00061046
Iteration 16/25 | Loss: 0.00061046
Iteration 17/25 | Loss: 0.00061046
Iteration 18/25 | Loss: 0.00061046
Iteration 19/25 | Loss: 0.00061046
Iteration 20/25 | Loss: 0.00061046
Iteration 21/25 | Loss: 0.00061046
Iteration 22/25 | Loss: 0.00061046
Iteration 23/25 | Loss: 0.00061046
Iteration 24/25 | Loss: 0.00061046
Iteration 25/25 | Loss: 0.00061046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061046
Iteration 2/1000 | Loss: 0.00001976
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001146
Iteration 5/1000 | Loss: 0.00001068
Iteration 6/1000 | Loss: 0.00001011
Iteration 7/1000 | Loss: 0.00000976
Iteration 8/1000 | Loss: 0.00000946
Iteration 9/1000 | Loss: 0.00000923
Iteration 10/1000 | Loss: 0.00000921
Iteration 11/1000 | Loss: 0.00000920
Iteration 12/1000 | Loss: 0.00000919
Iteration 13/1000 | Loss: 0.00000919
Iteration 14/1000 | Loss: 0.00000918
Iteration 15/1000 | Loss: 0.00000918
Iteration 16/1000 | Loss: 0.00000917
Iteration 17/1000 | Loss: 0.00000917
Iteration 18/1000 | Loss: 0.00000907
Iteration 19/1000 | Loss: 0.00000907
Iteration 20/1000 | Loss: 0.00000905
Iteration 21/1000 | Loss: 0.00000904
Iteration 22/1000 | Loss: 0.00000903
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000902
Iteration 25/1000 | Loss: 0.00000902
Iteration 26/1000 | Loss: 0.00000901
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000901
Iteration 29/1000 | Loss: 0.00000901
Iteration 30/1000 | Loss: 0.00000900
Iteration 31/1000 | Loss: 0.00000900
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000899
Iteration 34/1000 | Loss: 0.00000899
Iteration 35/1000 | Loss: 0.00000899
Iteration 36/1000 | Loss: 0.00000898
Iteration 37/1000 | Loss: 0.00000898
Iteration 38/1000 | Loss: 0.00000898
Iteration 39/1000 | Loss: 0.00000898
Iteration 40/1000 | Loss: 0.00000898
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000898
Iteration 43/1000 | Loss: 0.00000897
Iteration 44/1000 | Loss: 0.00000897
Iteration 45/1000 | Loss: 0.00000897
Iteration 46/1000 | Loss: 0.00000897
Iteration 47/1000 | Loss: 0.00000896
Iteration 48/1000 | Loss: 0.00000896
Iteration 49/1000 | Loss: 0.00000896
Iteration 50/1000 | Loss: 0.00000896
Iteration 51/1000 | Loss: 0.00000895
Iteration 52/1000 | Loss: 0.00000895
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000894
Iteration 55/1000 | Loss: 0.00000892
Iteration 56/1000 | Loss: 0.00000892
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000890
Iteration 61/1000 | Loss: 0.00000890
Iteration 62/1000 | Loss: 0.00000890
Iteration 63/1000 | Loss: 0.00000890
Iteration 64/1000 | Loss: 0.00000890
Iteration 65/1000 | Loss: 0.00000889
Iteration 66/1000 | Loss: 0.00000889
Iteration 67/1000 | Loss: 0.00000888
Iteration 68/1000 | Loss: 0.00000888
Iteration 69/1000 | Loss: 0.00000888
Iteration 70/1000 | Loss: 0.00000887
Iteration 71/1000 | Loss: 0.00000887
Iteration 72/1000 | Loss: 0.00000885
Iteration 73/1000 | Loss: 0.00000885
Iteration 74/1000 | Loss: 0.00000884
Iteration 75/1000 | Loss: 0.00000884
Iteration 76/1000 | Loss: 0.00000884
Iteration 77/1000 | Loss: 0.00000883
Iteration 78/1000 | Loss: 0.00000883
Iteration 79/1000 | Loss: 0.00000882
Iteration 80/1000 | Loss: 0.00000882
Iteration 81/1000 | Loss: 0.00000881
Iteration 82/1000 | Loss: 0.00000881
Iteration 83/1000 | Loss: 0.00000881
Iteration 84/1000 | Loss: 0.00000880
Iteration 85/1000 | Loss: 0.00000879
Iteration 86/1000 | Loss: 0.00000879
Iteration 87/1000 | Loss: 0.00000878
Iteration 88/1000 | Loss: 0.00000877
Iteration 89/1000 | Loss: 0.00000877
Iteration 90/1000 | Loss: 0.00000876
Iteration 91/1000 | Loss: 0.00000875
Iteration 92/1000 | Loss: 0.00000875
Iteration 93/1000 | Loss: 0.00000875
Iteration 94/1000 | Loss: 0.00000875
Iteration 95/1000 | Loss: 0.00000875
Iteration 96/1000 | Loss: 0.00000874
Iteration 97/1000 | Loss: 0.00000874
Iteration 98/1000 | Loss: 0.00000874
Iteration 99/1000 | Loss: 0.00000874
Iteration 100/1000 | Loss: 0.00000874
Iteration 101/1000 | Loss: 0.00000874
Iteration 102/1000 | Loss: 0.00000874
Iteration 103/1000 | Loss: 0.00000874
Iteration 104/1000 | Loss: 0.00000874
Iteration 105/1000 | Loss: 0.00000874
Iteration 106/1000 | Loss: 0.00000874
Iteration 107/1000 | Loss: 0.00000873
Iteration 108/1000 | Loss: 0.00000873
Iteration 109/1000 | Loss: 0.00000873
Iteration 110/1000 | Loss: 0.00000873
Iteration 111/1000 | Loss: 0.00000872
Iteration 112/1000 | Loss: 0.00000872
Iteration 113/1000 | Loss: 0.00000871
Iteration 114/1000 | Loss: 0.00000871
Iteration 115/1000 | Loss: 0.00000871
Iteration 116/1000 | Loss: 0.00000871
Iteration 117/1000 | Loss: 0.00000871
Iteration 118/1000 | Loss: 0.00000871
Iteration 119/1000 | Loss: 0.00000871
Iteration 120/1000 | Loss: 0.00000871
Iteration 121/1000 | Loss: 0.00000871
Iteration 122/1000 | Loss: 0.00000870
Iteration 123/1000 | Loss: 0.00000870
Iteration 124/1000 | Loss: 0.00000870
Iteration 125/1000 | Loss: 0.00000870
Iteration 126/1000 | Loss: 0.00000870
Iteration 127/1000 | Loss: 0.00000870
Iteration 128/1000 | Loss: 0.00000870
Iteration 129/1000 | Loss: 0.00000870
Iteration 130/1000 | Loss: 0.00000869
Iteration 131/1000 | Loss: 0.00000869
Iteration 132/1000 | Loss: 0.00000869
Iteration 133/1000 | Loss: 0.00000869
Iteration 134/1000 | Loss: 0.00000869
Iteration 135/1000 | Loss: 0.00000867
Iteration 136/1000 | Loss: 0.00000867
Iteration 137/1000 | Loss: 0.00000867
Iteration 138/1000 | Loss: 0.00000867
Iteration 139/1000 | Loss: 0.00000867
Iteration 140/1000 | Loss: 0.00000867
Iteration 141/1000 | Loss: 0.00000867
Iteration 142/1000 | Loss: 0.00000867
Iteration 143/1000 | Loss: 0.00000866
Iteration 144/1000 | Loss: 0.00000866
Iteration 145/1000 | Loss: 0.00000866
Iteration 146/1000 | Loss: 0.00000866
Iteration 147/1000 | Loss: 0.00000866
Iteration 148/1000 | Loss: 0.00000865
Iteration 149/1000 | Loss: 0.00000865
Iteration 150/1000 | Loss: 0.00000864
Iteration 151/1000 | Loss: 0.00000864
Iteration 152/1000 | Loss: 0.00000864
Iteration 153/1000 | Loss: 0.00000864
Iteration 154/1000 | Loss: 0.00000863
Iteration 155/1000 | Loss: 0.00000863
Iteration 156/1000 | Loss: 0.00000863
Iteration 157/1000 | Loss: 0.00000863
Iteration 158/1000 | Loss: 0.00000863
Iteration 159/1000 | Loss: 0.00000863
Iteration 160/1000 | Loss: 0.00000862
Iteration 161/1000 | Loss: 0.00000862
Iteration 162/1000 | Loss: 0.00000862
Iteration 163/1000 | Loss: 0.00000862
Iteration 164/1000 | Loss: 0.00000861
Iteration 165/1000 | Loss: 0.00000861
Iteration 166/1000 | Loss: 0.00000861
Iteration 167/1000 | Loss: 0.00000861
Iteration 168/1000 | Loss: 0.00000861
Iteration 169/1000 | Loss: 0.00000861
Iteration 170/1000 | Loss: 0.00000861
Iteration 171/1000 | Loss: 0.00000861
Iteration 172/1000 | Loss: 0.00000861
Iteration 173/1000 | Loss: 0.00000861
Iteration 174/1000 | Loss: 0.00000861
Iteration 175/1000 | Loss: 0.00000861
Iteration 176/1000 | Loss: 0.00000861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [8.61234002513811e-06, 8.61234002513811e-06, 8.61234002513811e-06, 8.61234002513811e-06, 8.61234002513811e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.61234002513811e-06

Optimization complete. Final v2v error: 2.5202929973602295 mm

Highest mean error: 2.6911723613739014 mm for frame 32

Lowest mean error: 2.3585751056671143 mm for frame 250

Saving results

Total time: 40.86340141296387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393730
Iteration 2/25 | Loss: 0.00113279
Iteration 3/25 | Loss: 0.00105115
Iteration 4/25 | Loss: 0.00103761
Iteration 5/25 | Loss: 0.00103331
Iteration 6/25 | Loss: 0.00103275
Iteration 7/25 | Loss: 0.00103275
Iteration 8/25 | Loss: 0.00103275
Iteration 9/25 | Loss: 0.00103275
Iteration 10/25 | Loss: 0.00103275
Iteration 11/25 | Loss: 0.00103275
Iteration 12/25 | Loss: 0.00103275
Iteration 13/25 | Loss: 0.00103275
Iteration 14/25 | Loss: 0.00103275
Iteration 15/25 | Loss: 0.00103275
Iteration 16/25 | Loss: 0.00103275
Iteration 17/25 | Loss: 0.00103275
Iteration 18/25 | Loss: 0.00103275
Iteration 19/25 | Loss: 0.00103275
Iteration 20/25 | Loss: 0.00103275
Iteration 21/25 | Loss: 0.00103275
Iteration 22/25 | Loss: 0.00103275
Iteration 23/25 | Loss: 0.00103275
Iteration 24/25 | Loss: 0.00103275
Iteration 25/25 | Loss: 0.00103275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39741254
Iteration 2/25 | Loss: 0.00067897
Iteration 3/25 | Loss: 0.00067897
Iteration 4/25 | Loss: 0.00067897
Iteration 5/25 | Loss: 0.00067897
Iteration 6/25 | Loss: 0.00067897
Iteration 7/25 | Loss: 0.00067897
Iteration 8/25 | Loss: 0.00067897
Iteration 9/25 | Loss: 0.00067897
Iteration 10/25 | Loss: 0.00067897
Iteration 11/25 | Loss: 0.00067897
Iteration 12/25 | Loss: 0.00067897
Iteration 13/25 | Loss: 0.00067897
Iteration 14/25 | Loss: 0.00067897
Iteration 15/25 | Loss: 0.00067897
Iteration 16/25 | Loss: 0.00067897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006789654726162553, 0.0006789654726162553, 0.0006789654726162553, 0.0006789654726162553, 0.0006789654726162553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006789654726162553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067897
Iteration 2/1000 | Loss: 0.00001428
Iteration 3/1000 | Loss: 0.00001116
Iteration 4/1000 | Loss: 0.00001033
Iteration 5/1000 | Loss: 0.00000986
Iteration 6/1000 | Loss: 0.00000968
Iteration 7/1000 | Loss: 0.00000961
Iteration 8/1000 | Loss: 0.00000961
Iteration 9/1000 | Loss: 0.00000940
Iteration 10/1000 | Loss: 0.00000940
Iteration 11/1000 | Loss: 0.00000920
Iteration 12/1000 | Loss: 0.00000906
Iteration 13/1000 | Loss: 0.00000906
Iteration 14/1000 | Loss: 0.00000905
Iteration 15/1000 | Loss: 0.00000900
Iteration 16/1000 | Loss: 0.00000898
Iteration 17/1000 | Loss: 0.00000895
Iteration 18/1000 | Loss: 0.00000894
Iteration 19/1000 | Loss: 0.00000894
Iteration 20/1000 | Loss: 0.00000893
Iteration 21/1000 | Loss: 0.00000888
Iteration 22/1000 | Loss: 0.00000887
Iteration 23/1000 | Loss: 0.00000886
Iteration 24/1000 | Loss: 0.00000886
Iteration 25/1000 | Loss: 0.00000880
Iteration 26/1000 | Loss: 0.00000880
Iteration 27/1000 | Loss: 0.00000880
Iteration 28/1000 | Loss: 0.00000880
Iteration 29/1000 | Loss: 0.00000880
Iteration 30/1000 | Loss: 0.00000880
Iteration 31/1000 | Loss: 0.00000878
Iteration 32/1000 | Loss: 0.00000878
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000876
Iteration 35/1000 | Loss: 0.00000876
Iteration 36/1000 | Loss: 0.00000875
Iteration 37/1000 | Loss: 0.00000875
Iteration 38/1000 | Loss: 0.00000875
Iteration 39/1000 | Loss: 0.00000875
Iteration 40/1000 | Loss: 0.00000874
Iteration 41/1000 | Loss: 0.00000874
Iteration 42/1000 | Loss: 0.00000874
Iteration 43/1000 | Loss: 0.00000873
Iteration 44/1000 | Loss: 0.00000873
Iteration 45/1000 | Loss: 0.00000873
Iteration 46/1000 | Loss: 0.00000873
Iteration 47/1000 | Loss: 0.00000872
Iteration 48/1000 | Loss: 0.00000872
Iteration 49/1000 | Loss: 0.00000872
Iteration 50/1000 | Loss: 0.00000871
Iteration 51/1000 | Loss: 0.00000871
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000870
Iteration 54/1000 | Loss: 0.00000869
Iteration 55/1000 | Loss: 0.00000869
Iteration 56/1000 | Loss: 0.00000868
Iteration 57/1000 | Loss: 0.00000868
Iteration 58/1000 | Loss: 0.00000867
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000866
Iteration 61/1000 | Loss: 0.00000866
Iteration 62/1000 | Loss: 0.00000865
Iteration 63/1000 | Loss: 0.00000864
Iteration 64/1000 | Loss: 0.00000864
Iteration 65/1000 | Loss: 0.00000863
Iteration 66/1000 | Loss: 0.00000861
Iteration 67/1000 | Loss: 0.00000860
Iteration 68/1000 | Loss: 0.00000859
Iteration 69/1000 | Loss: 0.00000858
Iteration 70/1000 | Loss: 0.00000858
Iteration 71/1000 | Loss: 0.00000858
Iteration 72/1000 | Loss: 0.00000858
Iteration 73/1000 | Loss: 0.00000857
Iteration 74/1000 | Loss: 0.00000857
Iteration 75/1000 | Loss: 0.00000857
Iteration 76/1000 | Loss: 0.00000857
Iteration 77/1000 | Loss: 0.00000857
Iteration 78/1000 | Loss: 0.00000856
Iteration 79/1000 | Loss: 0.00000856
Iteration 80/1000 | Loss: 0.00000856
Iteration 81/1000 | Loss: 0.00000855
Iteration 82/1000 | Loss: 0.00000855
Iteration 83/1000 | Loss: 0.00000855
Iteration 84/1000 | Loss: 0.00000855
Iteration 85/1000 | Loss: 0.00000855
Iteration 86/1000 | Loss: 0.00000855
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000854
Iteration 89/1000 | Loss: 0.00000854
Iteration 90/1000 | Loss: 0.00000854
Iteration 91/1000 | Loss: 0.00000854
Iteration 92/1000 | Loss: 0.00000854
Iteration 93/1000 | Loss: 0.00000854
Iteration 94/1000 | Loss: 0.00000854
Iteration 95/1000 | Loss: 0.00000854
Iteration 96/1000 | Loss: 0.00000854
Iteration 97/1000 | Loss: 0.00000853
Iteration 98/1000 | Loss: 0.00000853
Iteration 99/1000 | Loss: 0.00000853
Iteration 100/1000 | Loss: 0.00000853
Iteration 101/1000 | Loss: 0.00000853
Iteration 102/1000 | Loss: 0.00000853
Iteration 103/1000 | Loss: 0.00000853
Iteration 104/1000 | Loss: 0.00000853
Iteration 105/1000 | Loss: 0.00000853
Iteration 106/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [8.53183610161068e-06, 8.53183610161068e-06, 8.53183610161068e-06, 8.53183610161068e-06, 8.53183610161068e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.53183610161068e-06

Optimization complete. Final v2v error: 2.5227696895599365 mm

Highest mean error: 2.853574514389038 mm for frame 100

Lowest mean error: 2.3480823040008545 mm for frame 11

Saving results

Total time: 34.20643973350525
