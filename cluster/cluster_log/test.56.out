Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=56, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3136-3191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794654
Iteration 2/25 | Loss: 0.00140287
Iteration 3/25 | Loss: 0.00129950
Iteration 4/25 | Loss: 0.00130292
Iteration 5/25 | Loss: 0.00127886
Iteration 6/25 | Loss: 0.00127629
Iteration 7/25 | Loss: 0.00127405
Iteration 8/25 | Loss: 0.00127243
Iteration 9/25 | Loss: 0.00127151
Iteration 10/25 | Loss: 0.00127404
Iteration 11/25 | Loss: 0.00126991
Iteration 12/25 | Loss: 0.00126871
Iteration 13/25 | Loss: 0.00126832
Iteration 14/25 | Loss: 0.00126827
Iteration 15/25 | Loss: 0.00126827
Iteration 16/25 | Loss: 0.00126826
Iteration 17/25 | Loss: 0.00126826
Iteration 18/25 | Loss: 0.00126826
Iteration 19/25 | Loss: 0.00126826
Iteration 20/25 | Loss: 0.00126826
Iteration 21/25 | Loss: 0.00126826
Iteration 22/25 | Loss: 0.00126826
Iteration 23/25 | Loss: 0.00126826
Iteration 24/25 | Loss: 0.00126826
Iteration 25/25 | Loss: 0.00126825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43699491
Iteration 2/25 | Loss: 0.00094082
Iteration 3/25 | Loss: 0.00094073
Iteration 4/25 | Loss: 0.00094073
Iteration 5/25 | Loss: 0.00094073
Iteration 6/25 | Loss: 0.00094073
Iteration 7/25 | Loss: 0.00094073
Iteration 8/25 | Loss: 0.00094073
Iteration 9/25 | Loss: 0.00094073
Iteration 10/25 | Loss: 0.00094073
Iteration 11/25 | Loss: 0.00094073
Iteration 12/25 | Loss: 0.00094073
Iteration 13/25 | Loss: 0.00094073
Iteration 14/25 | Loss: 0.00094073
Iteration 15/25 | Loss: 0.00094073
Iteration 16/25 | Loss: 0.00094073
Iteration 17/25 | Loss: 0.00094073
Iteration 18/25 | Loss: 0.00094073
Iteration 19/25 | Loss: 0.00094073
Iteration 20/25 | Loss: 0.00094073
Iteration 21/25 | Loss: 0.00094073
Iteration 22/25 | Loss: 0.00094073
Iteration 23/25 | Loss: 0.00094073
Iteration 24/25 | Loss: 0.00094073
Iteration 25/25 | Loss: 0.00094073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000940729514695704, 0.000940729514695704, 0.000940729514695704, 0.000940729514695704, 0.000940729514695704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000940729514695704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094073
Iteration 2/1000 | Loss: 0.00003922
Iteration 3/1000 | Loss: 0.00002338
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001754
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001590
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001574
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001573
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001567
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001566
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001564
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001562
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001560
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001550
Iteration 102/1000 | Loss: 0.00001550
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00001548
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001546
Iteration 118/1000 | Loss: 0.00001546
Iteration 119/1000 | Loss: 0.00001546
Iteration 120/1000 | Loss: 0.00001545
Iteration 121/1000 | Loss: 0.00001545
Iteration 122/1000 | Loss: 0.00001545
Iteration 123/1000 | Loss: 0.00001544
Iteration 124/1000 | Loss: 0.00001544
Iteration 125/1000 | Loss: 0.00001544
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001543
Iteration 132/1000 | Loss: 0.00001543
Iteration 133/1000 | Loss: 0.00001542
Iteration 134/1000 | Loss: 0.00001542
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Iteration 140/1000 | Loss: 0.00001540
Iteration 141/1000 | Loss: 0.00001540
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001539
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001539
Iteration 146/1000 | Loss: 0.00001538
Iteration 147/1000 | Loss: 0.00001538
Iteration 148/1000 | Loss: 0.00001538
Iteration 149/1000 | Loss: 0.00001538
Iteration 150/1000 | Loss: 0.00001538
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Iteration 164/1000 | Loss: 0.00001537
Iteration 165/1000 | Loss: 0.00001537
Iteration 166/1000 | Loss: 0.00001537
Iteration 167/1000 | Loss: 0.00001537
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001537
Iteration 172/1000 | Loss: 0.00001537
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Iteration 178/1000 | Loss: 0.00001537
Iteration 179/1000 | Loss: 0.00001537
Iteration 180/1000 | Loss: 0.00001537
Iteration 181/1000 | Loss: 0.00001537
Iteration 182/1000 | Loss: 0.00001537
Iteration 183/1000 | Loss: 0.00001537
Iteration 184/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.536809759272728e-05, 1.536809759272728e-05, 1.536809759272728e-05, 1.536809759272728e-05, 1.536809759272728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536809759272728e-05

Optimization complete. Final v2v error: 3.275972604751587 mm

Highest mean error: 4.576524257659912 mm for frame 222

Lowest mean error: 2.8691775798797607 mm for frame 119

Saving results

Total time: 61.1201593875885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994325
Iteration 2/25 | Loss: 0.00289329
Iteration 3/25 | Loss: 0.00225892
Iteration 4/25 | Loss: 0.00201453
Iteration 5/25 | Loss: 0.00194315
Iteration 6/25 | Loss: 0.00176831
Iteration 7/25 | Loss: 0.00168305
Iteration 8/25 | Loss: 0.00165522
Iteration 9/25 | Loss: 0.00158951
Iteration 10/25 | Loss: 0.00158860
Iteration 11/25 | Loss: 0.00157400
Iteration 12/25 | Loss: 0.00156771
Iteration 13/25 | Loss: 0.00157103
Iteration 14/25 | Loss: 0.00155684
Iteration 15/25 | Loss: 0.00156547
Iteration 16/25 | Loss: 0.00154898
Iteration 17/25 | Loss: 0.00155411
Iteration 18/25 | Loss: 0.00154375
Iteration 19/25 | Loss: 0.00154490
Iteration 20/25 | Loss: 0.00154742
Iteration 21/25 | Loss: 0.00154572
Iteration 22/25 | Loss: 0.00154857
Iteration 23/25 | Loss: 0.00151788
Iteration 24/25 | Loss: 0.00153578
Iteration 25/25 | Loss: 0.00154578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61197722
Iteration 2/25 | Loss: 0.00539251
Iteration 3/25 | Loss: 0.00363846
Iteration 4/25 | Loss: 0.00236982
Iteration 5/25 | Loss: 0.00236978
Iteration 6/25 | Loss: 0.00236978
Iteration 7/25 | Loss: 0.00236978
Iteration 8/25 | Loss: 0.00236978
Iteration 9/25 | Loss: 0.00236978
Iteration 10/25 | Loss: 0.00236977
Iteration 11/25 | Loss: 0.00236977
Iteration 12/25 | Loss: 0.00236977
Iteration 13/25 | Loss: 0.00236977
Iteration 14/25 | Loss: 0.00236977
Iteration 15/25 | Loss: 0.00236977
Iteration 16/25 | Loss: 0.00236977
Iteration 17/25 | Loss: 0.00236977
Iteration 18/25 | Loss: 0.00236977
Iteration 19/25 | Loss: 0.00236977
Iteration 20/25 | Loss: 0.00236977
Iteration 21/25 | Loss: 0.00236977
Iteration 22/25 | Loss: 0.00236977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002369773807004094, 0.002369773807004094, 0.002369773807004094, 0.002369773807004094, 0.002369773807004094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002369773807004094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236977
Iteration 2/1000 | Loss: 0.00214336
Iteration 3/1000 | Loss: 0.00220788
Iteration 4/1000 | Loss: 0.00154829
Iteration 5/1000 | Loss: 0.00103131
Iteration 6/1000 | Loss: 0.00076132
Iteration 7/1000 | Loss: 0.00053784
Iteration 8/1000 | Loss: 0.00057970
Iteration 9/1000 | Loss: 0.00104764
Iteration 10/1000 | Loss: 0.00049821
Iteration 11/1000 | Loss: 0.00132625
Iteration 12/1000 | Loss: 0.00049830
Iteration 13/1000 | Loss: 0.00051371
Iteration 14/1000 | Loss: 0.00040960
Iteration 15/1000 | Loss: 0.00046715
Iteration 16/1000 | Loss: 0.00029137
Iteration 17/1000 | Loss: 0.00021842
Iteration 18/1000 | Loss: 0.00050422
Iteration 19/1000 | Loss: 0.00030460
Iteration 20/1000 | Loss: 0.00021647
Iteration 21/1000 | Loss: 0.00021322
Iteration 22/1000 | Loss: 0.00031453
Iteration 23/1000 | Loss: 0.00041845
Iteration 24/1000 | Loss: 0.00035879
Iteration 25/1000 | Loss: 0.00077824
Iteration 26/1000 | Loss: 0.00059975
Iteration 27/1000 | Loss: 0.00046167
Iteration 28/1000 | Loss: 0.00031826
Iteration 29/1000 | Loss: 0.00038149
Iteration 30/1000 | Loss: 0.00036705
Iteration 31/1000 | Loss: 0.00058549
Iteration 32/1000 | Loss: 0.00017643
Iteration 33/1000 | Loss: 0.00020481
Iteration 34/1000 | Loss: 0.00016570
Iteration 35/1000 | Loss: 0.00020761
Iteration 36/1000 | Loss: 0.00059573
Iteration 37/1000 | Loss: 0.00117876
Iteration 38/1000 | Loss: 0.00026153
Iteration 39/1000 | Loss: 0.00012559
Iteration 40/1000 | Loss: 0.00024934
Iteration 41/1000 | Loss: 0.00015402
Iteration 42/1000 | Loss: 0.00036453
Iteration 43/1000 | Loss: 0.00075969
Iteration 44/1000 | Loss: 0.00025793
Iteration 45/1000 | Loss: 0.00036721
Iteration 46/1000 | Loss: 0.00046479
Iteration 47/1000 | Loss: 0.00068052
Iteration 48/1000 | Loss: 0.00060817
Iteration 49/1000 | Loss: 0.00036141
Iteration 50/1000 | Loss: 0.00113273
Iteration 51/1000 | Loss: 0.00043421
Iteration 52/1000 | Loss: 0.00028440
Iteration 53/1000 | Loss: 0.00030348
Iteration 54/1000 | Loss: 0.00020974
Iteration 55/1000 | Loss: 0.00051667
Iteration 56/1000 | Loss: 0.00033593
Iteration 57/1000 | Loss: 0.00018606
Iteration 58/1000 | Loss: 0.00025019
Iteration 59/1000 | Loss: 0.00040749
Iteration 60/1000 | Loss: 0.00019602
Iteration 61/1000 | Loss: 0.00030319
Iteration 62/1000 | Loss: 0.00024834
Iteration 63/1000 | Loss: 0.00023746
Iteration 64/1000 | Loss: 0.00010581
Iteration 65/1000 | Loss: 0.00010162
Iteration 66/1000 | Loss: 0.00031987
Iteration 67/1000 | Loss: 0.00011164
Iteration 68/1000 | Loss: 0.00034106
Iteration 69/1000 | Loss: 0.00024042
Iteration 70/1000 | Loss: 0.00009988
Iteration 71/1000 | Loss: 0.00009564
Iteration 72/1000 | Loss: 0.00065228
Iteration 73/1000 | Loss: 0.00032431
Iteration 74/1000 | Loss: 0.00088005
Iteration 75/1000 | Loss: 0.00013855
Iteration 76/1000 | Loss: 0.00017906
Iteration 77/1000 | Loss: 0.00009415
Iteration 78/1000 | Loss: 0.00008984
Iteration 79/1000 | Loss: 0.00039789
Iteration 80/1000 | Loss: 0.00048700
Iteration 81/1000 | Loss: 0.00017776
Iteration 82/1000 | Loss: 0.00086859
Iteration 83/1000 | Loss: 0.00101779
Iteration 84/1000 | Loss: 0.00072213
Iteration 85/1000 | Loss: 0.00042073
Iteration 86/1000 | Loss: 0.00027384
Iteration 87/1000 | Loss: 0.00017362
Iteration 88/1000 | Loss: 0.00025533
Iteration 89/1000 | Loss: 0.00081438
Iteration 90/1000 | Loss: 0.00029203
Iteration 91/1000 | Loss: 0.00027922
Iteration 92/1000 | Loss: 0.00020269
Iteration 93/1000 | Loss: 0.00010364
Iteration 94/1000 | Loss: 0.00007103
Iteration 95/1000 | Loss: 0.00009382
Iteration 96/1000 | Loss: 0.00006312
Iteration 97/1000 | Loss: 0.00005623
Iteration 98/1000 | Loss: 0.00005342
Iteration 99/1000 | Loss: 0.00033802
Iteration 100/1000 | Loss: 0.00016735
Iteration 101/1000 | Loss: 0.00009363
Iteration 102/1000 | Loss: 0.00005411
Iteration 103/1000 | Loss: 0.00005042
Iteration 104/1000 | Loss: 0.00013875
Iteration 105/1000 | Loss: 0.00016472
Iteration 106/1000 | Loss: 0.00008050
Iteration 107/1000 | Loss: 0.00004696
Iteration 108/1000 | Loss: 0.00014511
Iteration 109/1000 | Loss: 0.00008890
Iteration 110/1000 | Loss: 0.00004622
Iteration 111/1000 | Loss: 0.00014482
Iteration 112/1000 | Loss: 0.00012447
Iteration 113/1000 | Loss: 0.00004626
Iteration 114/1000 | Loss: 0.00008046
Iteration 115/1000 | Loss: 0.00004536
Iteration 116/1000 | Loss: 0.00004853
Iteration 117/1000 | Loss: 0.00006685
Iteration 118/1000 | Loss: 0.00004303
Iteration 119/1000 | Loss: 0.00004767
Iteration 120/1000 | Loss: 0.00007902
Iteration 121/1000 | Loss: 0.00006392
Iteration 122/1000 | Loss: 0.00004601
Iteration 123/1000 | Loss: 0.00005845
Iteration 124/1000 | Loss: 0.00004172
Iteration 125/1000 | Loss: 0.00004496
Iteration 126/1000 | Loss: 0.00004101
Iteration 127/1000 | Loss: 0.00016378
Iteration 128/1000 | Loss: 0.00004708
Iteration 129/1000 | Loss: 0.00004413
Iteration 130/1000 | Loss: 0.00004027
Iteration 131/1000 | Loss: 0.00014482
Iteration 132/1000 | Loss: 0.00021826
Iteration 133/1000 | Loss: 0.00008886
Iteration 134/1000 | Loss: 0.00006182
Iteration 135/1000 | Loss: 0.00003927
Iteration 136/1000 | Loss: 0.00003906
Iteration 137/1000 | Loss: 0.00011364
Iteration 138/1000 | Loss: 0.00014038
Iteration 139/1000 | Loss: 0.00005807
Iteration 140/1000 | Loss: 0.00008520
Iteration 141/1000 | Loss: 0.00009903
Iteration 142/1000 | Loss: 0.00018058
Iteration 143/1000 | Loss: 0.00004697
Iteration 144/1000 | Loss: 0.00003769
Iteration 145/1000 | Loss: 0.00014247
Iteration 146/1000 | Loss: 0.00007211
Iteration 147/1000 | Loss: 0.00003919
Iteration 148/1000 | Loss: 0.00003690
Iteration 149/1000 | Loss: 0.00007665
Iteration 150/1000 | Loss: 0.00003944
Iteration 151/1000 | Loss: 0.00006602
Iteration 152/1000 | Loss: 0.00003900
Iteration 153/1000 | Loss: 0.00004136
Iteration 154/1000 | Loss: 0.00003676
Iteration 155/1000 | Loss: 0.00003653
Iteration 156/1000 | Loss: 0.00003653
Iteration 157/1000 | Loss: 0.00003653
Iteration 158/1000 | Loss: 0.00003652
Iteration 159/1000 | Loss: 0.00003652
Iteration 160/1000 | Loss: 0.00003651
Iteration 161/1000 | Loss: 0.00012057
Iteration 162/1000 | Loss: 0.00012057
Iteration 163/1000 | Loss: 0.00136632
Iteration 164/1000 | Loss: 0.00018380
Iteration 165/1000 | Loss: 0.00014996
Iteration 166/1000 | Loss: 0.00008405
Iteration 167/1000 | Loss: 0.00039017
Iteration 168/1000 | Loss: 0.00008644
Iteration 169/1000 | Loss: 0.00003692
Iteration 170/1000 | Loss: 0.00003647
Iteration 171/1000 | Loss: 0.00018045
Iteration 172/1000 | Loss: 0.00017706
Iteration 173/1000 | Loss: 0.00003822
Iteration 174/1000 | Loss: 0.00012993
Iteration 175/1000 | Loss: 0.00034870
Iteration 176/1000 | Loss: 0.00008183
Iteration 177/1000 | Loss: 0.00007768
Iteration 178/1000 | Loss: 0.00007857
Iteration 179/1000 | Loss: 0.00003514
Iteration 180/1000 | Loss: 0.00003507
Iteration 181/1000 | Loss: 0.00005990
Iteration 182/1000 | Loss: 0.00009823
Iteration 183/1000 | Loss: 0.00003574
Iteration 184/1000 | Loss: 0.00006202
Iteration 185/1000 | Loss: 0.00010403
Iteration 186/1000 | Loss: 0.00004170
Iteration 187/1000 | Loss: 0.00004634
Iteration 188/1000 | Loss: 0.00016916
Iteration 189/1000 | Loss: 0.00005963
Iteration 190/1000 | Loss: 0.00003929
Iteration 191/1000 | Loss: 0.00005316
Iteration 192/1000 | Loss: 0.00003873
Iteration 193/1000 | Loss: 0.00003429
Iteration 194/1000 | Loss: 0.00003429
Iteration 195/1000 | Loss: 0.00003429
Iteration 196/1000 | Loss: 0.00003429
Iteration 197/1000 | Loss: 0.00003429
Iteration 198/1000 | Loss: 0.00003429
Iteration 199/1000 | Loss: 0.00003429
Iteration 200/1000 | Loss: 0.00003429
Iteration 201/1000 | Loss: 0.00003694
Iteration 202/1000 | Loss: 0.00005554
Iteration 203/1000 | Loss: 0.00003427
Iteration 204/1000 | Loss: 0.00003424
Iteration 205/1000 | Loss: 0.00003424
Iteration 206/1000 | Loss: 0.00003423
Iteration 207/1000 | Loss: 0.00003422
Iteration 208/1000 | Loss: 0.00003422
Iteration 209/1000 | Loss: 0.00003421
Iteration 210/1000 | Loss: 0.00003420
Iteration 211/1000 | Loss: 0.00007475
Iteration 212/1000 | Loss: 0.00003642
Iteration 213/1000 | Loss: 0.00003999
Iteration 214/1000 | Loss: 0.00005074
Iteration 215/1000 | Loss: 0.00003521
Iteration 216/1000 | Loss: 0.00004110
Iteration 217/1000 | Loss: 0.00003430
Iteration 218/1000 | Loss: 0.00003430
Iteration 219/1000 | Loss: 0.00003427
Iteration 220/1000 | Loss: 0.00003960
Iteration 221/1000 | Loss: 0.00004981
Iteration 222/1000 | Loss: 0.00003628
Iteration 223/1000 | Loss: 0.00004396
Iteration 224/1000 | Loss: 0.00003412
Iteration 225/1000 | Loss: 0.00003996
Iteration 226/1000 | Loss: 0.00003585
Iteration 227/1000 | Loss: 0.00003421
Iteration 228/1000 | Loss: 0.00003404
Iteration 229/1000 | Loss: 0.00003404
Iteration 230/1000 | Loss: 0.00003404
Iteration 231/1000 | Loss: 0.00003403
Iteration 232/1000 | Loss: 0.00003403
Iteration 233/1000 | Loss: 0.00003403
Iteration 234/1000 | Loss: 0.00003403
Iteration 235/1000 | Loss: 0.00003403
Iteration 236/1000 | Loss: 0.00003403
Iteration 237/1000 | Loss: 0.00003403
Iteration 238/1000 | Loss: 0.00003403
Iteration 239/1000 | Loss: 0.00003403
Iteration 240/1000 | Loss: 0.00003403
Iteration 241/1000 | Loss: 0.00003403
Iteration 242/1000 | Loss: 0.00003403
Iteration 243/1000 | Loss: 0.00003403
Iteration 244/1000 | Loss: 0.00003403
Iteration 245/1000 | Loss: 0.00003403
Iteration 246/1000 | Loss: 0.00003403
Iteration 247/1000 | Loss: 0.00003403
Iteration 248/1000 | Loss: 0.00003403
Iteration 249/1000 | Loss: 0.00003403
Iteration 250/1000 | Loss: 0.00003403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [3.402843503863551e-05, 3.402843503863551e-05, 3.402843503863551e-05, 3.402843503863551e-05, 3.402843503863551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.402843503863551e-05

Optimization complete. Final v2v error: 3.8421578407287598 mm

Highest mean error: 10.87058162689209 mm for frame 46

Lowest mean error: 2.916666030883789 mm for frame 22

Saving results

Total time: 370.7176322937012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966537
Iteration 2/25 | Loss: 0.00185741
Iteration 3/25 | Loss: 0.00162107
Iteration 4/25 | Loss: 0.00158185
Iteration 5/25 | Loss: 0.00154010
Iteration 6/25 | Loss: 0.00152640
Iteration 7/25 | Loss: 0.00151064
Iteration 8/25 | Loss: 0.00144666
Iteration 9/25 | Loss: 0.00145756
Iteration 10/25 | Loss: 0.00145039
Iteration 11/25 | Loss: 0.00137630
Iteration 12/25 | Loss: 0.00138071
Iteration 13/25 | Loss: 0.00137438
Iteration 14/25 | Loss: 0.00137051
Iteration 15/25 | Loss: 0.00136282
Iteration 16/25 | Loss: 0.00136101
Iteration 17/25 | Loss: 0.00136285
Iteration 18/25 | Loss: 0.00136207
Iteration 19/25 | Loss: 0.00136702
Iteration 20/25 | Loss: 0.00136455
Iteration 21/25 | Loss: 0.00136724
Iteration 22/25 | Loss: 0.00136452
Iteration 23/25 | Loss: 0.00136049
Iteration 24/25 | Loss: 0.00135786
Iteration 25/25 | Loss: 0.00136230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04442573
Iteration 2/25 | Loss: 0.00114449
Iteration 3/25 | Loss: 0.00114448
Iteration 4/25 | Loss: 0.00114448
Iteration 5/25 | Loss: 0.00114448
Iteration 6/25 | Loss: 0.00114448
Iteration 7/25 | Loss: 0.00114447
Iteration 8/25 | Loss: 0.00114447
Iteration 9/25 | Loss: 0.00114447
Iteration 10/25 | Loss: 0.00114447
Iteration 11/25 | Loss: 0.00114447
Iteration 12/25 | Loss: 0.00114447
Iteration 13/25 | Loss: 0.00114447
Iteration 14/25 | Loss: 0.00114447
Iteration 15/25 | Loss: 0.00114447
Iteration 16/25 | Loss: 0.00114447
Iteration 17/25 | Loss: 0.00114447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001144473790191114, 0.001144473790191114, 0.001144473790191114, 0.001144473790191114, 0.001144473790191114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001144473790191114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114447
Iteration 2/1000 | Loss: 0.00136963
Iteration 3/1000 | Loss: 0.00110400
Iteration 4/1000 | Loss: 0.00008247
Iteration 5/1000 | Loss: 0.00041240
Iteration 6/1000 | Loss: 0.00045029
Iteration 7/1000 | Loss: 0.00005854
Iteration 8/1000 | Loss: 0.00005164
Iteration 9/1000 | Loss: 0.00004912
Iteration 10/1000 | Loss: 0.00038975
Iteration 11/1000 | Loss: 0.00041977
Iteration 12/1000 | Loss: 0.00037137
Iteration 13/1000 | Loss: 0.00010074
Iteration 14/1000 | Loss: 0.00092054
Iteration 15/1000 | Loss: 0.00067873
Iteration 16/1000 | Loss: 0.00045289
Iteration 17/1000 | Loss: 0.00033228
Iteration 18/1000 | Loss: 0.00007881
Iteration 19/1000 | Loss: 0.00031611
Iteration 20/1000 | Loss: 0.00055714
Iteration 21/1000 | Loss: 0.00066404
Iteration 22/1000 | Loss: 0.00094026
Iteration 23/1000 | Loss: 0.00038847
Iteration 24/1000 | Loss: 0.00058260
Iteration 25/1000 | Loss: 0.00045026
Iteration 26/1000 | Loss: 0.00034210
Iteration 27/1000 | Loss: 0.00021885
Iteration 28/1000 | Loss: 0.00033424
Iteration 29/1000 | Loss: 0.00026633
Iteration 30/1000 | Loss: 0.00016211
Iteration 31/1000 | Loss: 0.00006124
Iteration 32/1000 | Loss: 0.00005313
Iteration 33/1000 | Loss: 0.00004988
Iteration 34/1000 | Loss: 0.00130944
Iteration 35/1000 | Loss: 0.00120288
Iteration 36/1000 | Loss: 0.00116588
Iteration 37/1000 | Loss: 0.00137846
Iteration 38/1000 | Loss: 0.00005764
Iteration 39/1000 | Loss: 0.00005206
Iteration 40/1000 | Loss: 0.00035009
Iteration 41/1000 | Loss: 0.00028006
Iteration 42/1000 | Loss: 0.00013776
Iteration 43/1000 | Loss: 0.00012650
Iteration 44/1000 | Loss: 0.00004999
Iteration 45/1000 | Loss: 0.00011234
Iteration 46/1000 | Loss: 0.00005111
Iteration 47/1000 | Loss: 0.00010067
Iteration 48/1000 | Loss: 0.00037937
Iteration 49/1000 | Loss: 0.00023618
Iteration 50/1000 | Loss: 0.00019499
Iteration 51/1000 | Loss: 0.00004601
Iteration 52/1000 | Loss: 0.00004277
Iteration 53/1000 | Loss: 0.00050874
Iteration 54/1000 | Loss: 0.00121263
Iteration 55/1000 | Loss: 0.00081365
Iteration 56/1000 | Loss: 0.00005200
Iteration 57/1000 | Loss: 0.00014940
Iteration 58/1000 | Loss: 0.00008378
Iteration 59/1000 | Loss: 0.00006937
Iteration 60/1000 | Loss: 0.00004035
Iteration 61/1000 | Loss: 0.00003899
Iteration 62/1000 | Loss: 0.00052748
Iteration 63/1000 | Loss: 0.00039373
Iteration 64/1000 | Loss: 0.00004307
Iteration 65/1000 | Loss: 0.00003732
Iteration 66/1000 | Loss: 0.00003517
Iteration 67/1000 | Loss: 0.00027001
Iteration 68/1000 | Loss: 0.00016653
Iteration 69/1000 | Loss: 0.00073307
Iteration 70/1000 | Loss: 0.00100101
Iteration 71/1000 | Loss: 0.00008785
Iteration 72/1000 | Loss: 0.00004430
Iteration 73/1000 | Loss: 0.00003833
Iteration 74/1000 | Loss: 0.00003515
Iteration 75/1000 | Loss: 0.00057387
Iteration 76/1000 | Loss: 0.00037502
Iteration 77/1000 | Loss: 0.00052190
Iteration 78/1000 | Loss: 0.00046062
Iteration 79/1000 | Loss: 0.00035792
Iteration 80/1000 | Loss: 0.00087994
Iteration 81/1000 | Loss: 0.00042207
Iteration 82/1000 | Loss: 0.00005797
Iteration 83/1000 | Loss: 0.00004336
Iteration 84/1000 | Loss: 0.00003599
Iteration 85/1000 | Loss: 0.00048315
Iteration 86/1000 | Loss: 0.00058602
Iteration 87/1000 | Loss: 0.00014807
Iteration 88/1000 | Loss: 0.00090736
Iteration 89/1000 | Loss: 0.00005555
Iteration 90/1000 | Loss: 0.00003997
Iteration 91/1000 | Loss: 0.00003365
Iteration 92/1000 | Loss: 0.00003081
Iteration 93/1000 | Loss: 0.00002865
Iteration 94/1000 | Loss: 0.00002750
Iteration 95/1000 | Loss: 0.00002672
Iteration 96/1000 | Loss: 0.00002586
Iteration 97/1000 | Loss: 0.00002524
Iteration 98/1000 | Loss: 0.00002482
Iteration 99/1000 | Loss: 0.00103999
Iteration 100/1000 | Loss: 0.00004302
Iteration 101/1000 | Loss: 0.00002783
Iteration 102/1000 | Loss: 0.00002407
Iteration 103/1000 | Loss: 0.00002298
Iteration 104/1000 | Loss: 0.00002214
Iteration 105/1000 | Loss: 0.00002168
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002079
Iteration 108/1000 | Loss: 0.00002055
Iteration 109/1000 | Loss: 0.00002052
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002043
Iteration 112/1000 | Loss: 0.00002039
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002026
Iteration 115/1000 | Loss: 0.00002025
Iteration 116/1000 | Loss: 0.00002023
Iteration 117/1000 | Loss: 0.00002023
Iteration 118/1000 | Loss: 0.00002023
Iteration 119/1000 | Loss: 0.00002023
Iteration 120/1000 | Loss: 0.00002023
Iteration 121/1000 | Loss: 0.00002023
Iteration 122/1000 | Loss: 0.00002023
Iteration 123/1000 | Loss: 0.00002023
Iteration 124/1000 | Loss: 0.00002023
Iteration 125/1000 | Loss: 0.00002023
Iteration 126/1000 | Loss: 0.00002022
Iteration 127/1000 | Loss: 0.00002022
Iteration 128/1000 | Loss: 0.00002022
Iteration 129/1000 | Loss: 0.00002022
Iteration 130/1000 | Loss: 0.00002022
Iteration 131/1000 | Loss: 0.00002022
Iteration 132/1000 | Loss: 0.00002022
Iteration 133/1000 | Loss: 0.00002022
Iteration 134/1000 | Loss: 0.00002021
Iteration 135/1000 | Loss: 0.00002021
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002021
Iteration 138/1000 | Loss: 0.00002021
Iteration 139/1000 | Loss: 0.00002021
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002020
Iteration 142/1000 | Loss: 0.00002020
Iteration 143/1000 | Loss: 0.00002020
Iteration 144/1000 | Loss: 0.00002020
Iteration 145/1000 | Loss: 0.00002019
Iteration 146/1000 | Loss: 0.00002019
Iteration 147/1000 | Loss: 0.00002019
Iteration 148/1000 | Loss: 0.00002019
Iteration 149/1000 | Loss: 0.00002019
Iteration 150/1000 | Loss: 0.00002018
Iteration 151/1000 | Loss: 0.00002018
Iteration 152/1000 | Loss: 0.00002018
Iteration 153/1000 | Loss: 0.00002018
Iteration 154/1000 | Loss: 0.00002018
Iteration 155/1000 | Loss: 0.00002018
Iteration 156/1000 | Loss: 0.00002018
Iteration 157/1000 | Loss: 0.00002018
Iteration 158/1000 | Loss: 0.00002018
Iteration 159/1000 | Loss: 0.00002017
Iteration 160/1000 | Loss: 0.00002017
Iteration 161/1000 | Loss: 0.00002017
Iteration 162/1000 | Loss: 0.00002017
Iteration 163/1000 | Loss: 0.00002017
Iteration 164/1000 | Loss: 0.00002017
Iteration 165/1000 | Loss: 0.00002017
Iteration 166/1000 | Loss: 0.00002017
Iteration 167/1000 | Loss: 0.00002016
Iteration 168/1000 | Loss: 0.00002016
Iteration 169/1000 | Loss: 0.00002016
Iteration 170/1000 | Loss: 0.00002016
Iteration 171/1000 | Loss: 0.00002015
Iteration 172/1000 | Loss: 0.00002015
Iteration 173/1000 | Loss: 0.00002014
Iteration 174/1000 | Loss: 0.00002014
Iteration 175/1000 | Loss: 0.00002014
Iteration 176/1000 | Loss: 0.00002014
Iteration 177/1000 | Loss: 0.00002014
Iteration 178/1000 | Loss: 0.00002014
Iteration 179/1000 | Loss: 0.00002014
Iteration 180/1000 | Loss: 0.00002014
Iteration 181/1000 | Loss: 0.00002014
Iteration 182/1000 | Loss: 0.00002013
Iteration 183/1000 | Loss: 0.00002013
Iteration 184/1000 | Loss: 0.00002013
Iteration 185/1000 | Loss: 0.00002013
Iteration 186/1000 | Loss: 0.00002013
Iteration 187/1000 | Loss: 0.00002013
Iteration 188/1000 | Loss: 0.00002013
Iteration 189/1000 | Loss: 0.00002012
Iteration 190/1000 | Loss: 0.00002012
Iteration 191/1000 | Loss: 0.00002012
Iteration 192/1000 | Loss: 0.00002012
Iteration 193/1000 | Loss: 0.00002012
Iteration 194/1000 | Loss: 0.00002012
Iteration 195/1000 | Loss: 0.00002012
Iteration 196/1000 | Loss: 0.00002012
Iteration 197/1000 | Loss: 0.00002012
Iteration 198/1000 | Loss: 0.00002012
Iteration 199/1000 | Loss: 0.00002012
Iteration 200/1000 | Loss: 0.00002011
Iteration 201/1000 | Loss: 0.00002011
Iteration 202/1000 | Loss: 0.00002011
Iteration 203/1000 | Loss: 0.00002011
Iteration 204/1000 | Loss: 0.00002011
Iteration 205/1000 | Loss: 0.00002011
Iteration 206/1000 | Loss: 0.00002011
Iteration 207/1000 | Loss: 0.00002011
Iteration 208/1000 | Loss: 0.00002011
Iteration 209/1000 | Loss: 0.00002011
Iteration 210/1000 | Loss: 0.00002011
Iteration 211/1000 | Loss: 0.00002011
Iteration 212/1000 | Loss: 0.00002011
Iteration 213/1000 | Loss: 0.00002011
Iteration 214/1000 | Loss: 0.00002011
Iteration 215/1000 | Loss: 0.00002011
Iteration 216/1000 | Loss: 0.00002011
Iteration 217/1000 | Loss: 0.00002011
Iteration 218/1000 | Loss: 0.00002011
Iteration 219/1000 | Loss: 0.00002011
Iteration 220/1000 | Loss: 0.00002011
Iteration 221/1000 | Loss: 0.00002011
Iteration 222/1000 | Loss: 0.00002011
Iteration 223/1000 | Loss: 0.00002011
Iteration 224/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.011189098993782e-05, 2.011189098993782e-05, 2.011189098993782e-05, 2.011189098993782e-05, 2.011189098993782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.011189098993782e-05

Optimization complete. Final v2v error: 3.5419511795043945 mm

Highest mean error: 11.974281311035156 mm for frame 88

Lowest mean error: 3.0210206508636475 mm for frame 37

Saving results

Total time: 201.79725337028503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407039
Iteration 2/25 | Loss: 0.00136958
Iteration 3/25 | Loss: 0.00129439
Iteration 4/25 | Loss: 0.00128822
Iteration 5/25 | Loss: 0.00128647
Iteration 6/25 | Loss: 0.00128592
Iteration 7/25 | Loss: 0.00128592
Iteration 8/25 | Loss: 0.00128592
Iteration 9/25 | Loss: 0.00128592
Iteration 10/25 | Loss: 0.00128592
Iteration 11/25 | Loss: 0.00128592
Iteration 12/25 | Loss: 0.00128592
Iteration 13/25 | Loss: 0.00128592
Iteration 14/25 | Loss: 0.00128592
Iteration 15/25 | Loss: 0.00128592
Iteration 16/25 | Loss: 0.00128592
Iteration 17/25 | Loss: 0.00128592
Iteration 18/25 | Loss: 0.00128592
Iteration 19/25 | Loss: 0.00128592
Iteration 20/25 | Loss: 0.00128592
Iteration 21/25 | Loss: 0.00128592
Iteration 22/25 | Loss: 0.00128592
Iteration 23/25 | Loss: 0.00128592
Iteration 24/25 | Loss: 0.00128592
Iteration 25/25 | Loss: 0.00128592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43244982
Iteration 2/25 | Loss: 0.00086371
Iteration 3/25 | Loss: 0.00086371
Iteration 4/25 | Loss: 0.00086371
Iteration 5/25 | Loss: 0.00086371
Iteration 6/25 | Loss: 0.00086371
Iteration 7/25 | Loss: 0.00086371
Iteration 8/25 | Loss: 0.00086371
Iteration 9/25 | Loss: 0.00086371
Iteration 10/25 | Loss: 0.00086371
Iteration 11/25 | Loss: 0.00086371
Iteration 12/25 | Loss: 0.00086371
Iteration 13/25 | Loss: 0.00086371
Iteration 14/25 | Loss: 0.00086371
Iteration 15/25 | Loss: 0.00086371
Iteration 16/25 | Loss: 0.00086371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008637059945613146, 0.0008637059945613146, 0.0008637059945613146, 0.0008637059945613146, 0.0008637059945613146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008637059945613146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086371
Iteration 2/1000 | Loss: 0.00003885
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00002007
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001750
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001711
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001708
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001707
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001703
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001700
Iteration 34/1000 | Loss: 0.00001700
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001698
Iteration 38/1000 | Loss: 0.00001698
Iteration 39/1000 | Loss: 0.00001698
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001697
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001694
Iteration 47/1000 | Loss: 0.00001694
Iteration 48/1000 | Loss: 0.00001693
Iteration 49/1000 | Loss: 0.00001693
Iteration 50/1000 | Loss: 0.00001693
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001693
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001692
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001690
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001689
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001686
Iteration 78/1000 | Loss: 0.00001686
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001683
Iteration 88/1000 | Loss: 0.00001683
Iteration 89/1000 | Loss: 0.00001683
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001682
Iteration 92/1000 | Loss: 0.00001682
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001679
Iteration 100/1000 | Loss: 0.00001679
Iteration 101/1000 | Loss: 0.00001679
Iteration 102/1000 | Loss: 0.00001679
Iteration 103/1000 | Loss: 0.00001679
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001678
Iteration 107/1000 | Loss: 0.00001678
Iteration 108/1000 | Loss: 0.00001678
Iteration 109/1000 | Loss: 0.00001678
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001676
Iteration 114/1000 | Loss: 0.00001676
Iteration 115/1000 | Loss: 0.00001675
Iteration 116/1000 | Loss: 0.00001675
Iteration 117/1000 | Loss: 0.00001675
Iteration 118/1000 | Loss: 0.00001675
Iteration 119/1000 | Loss: 0.00001675
Iteration 120/1000 | Loss: 0.00001675
Iteration 121/1000 | Loss: 0.00001674
Iteration 122/1000 | Loss: 0.00001674
Iteration 123/1000 | Loss: 0.00001674
Iteration 124/1000 | Loss: 0.00001673
Iteration 125/1000 | Loss: 0.00001673
Iteration 126/1000 | Loss: 0.00001673
Iteration 127/1000 | Loss: 0.00001673
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001671
Iteration 132/1000 | Loss: 0.00001671
Iteration 133/1000 | Loss: 0.00001671
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001669
Iteration 139/1000 | Loss: 0.00001669
Iteration 140/1000 | Loss: 0.00001669
Iteration 141/1000 | Loss: 0.00001669
Iteration 142/1000 | Loss: 0.00001669
Iteration 143/1000 | Loss: 0.00001669
Iteration 144/1000 | Loss: 0.00001669
Iteration 145/1000 | Loss: 0.00001669
Iteration 146/1000 | Loss: 0.00001669
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001669
Iteration 152/1000 | Loss: 0.00001669
Iteration 153/1000 | Loss: 0.00001669
Iteration 154/1000 | Loss: 0.00001669
Iteration 155/1000 | Loss: 0.00001669
Iteration 156/1000 | Loss: 0.00001669
Iteration 157/1000 | Loss: 0.00001669
Iteration 158/1000 | Loss: 0.00001669
Iteration 159/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6687165043549612e-05, 1.6687165043549612e-05, 1.6687165043549612e-05, 1.6687165043549612e-05, 1.6687165043549612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6687165043549612e-05

Optimization complete. Final v2v error: 3.4071860313415527 mm

Highest mean error: 3.6208908557891846 mm for frame 43

Lowest mean error: 3.062861442565918 mm for frame 65

Saving results

Total time: 37.97656559944153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383703
Iteration 2/25 | Loss: 0.00133033
Iteration 3/25 | Loss: 0.00125620
Iteration 4/25 | Loss: 0.00124698
Iteration 5/25 | Loss: 0.00124439
Iteration 6/25 | Loss: 0.00124404
Iteration 7/25 | Loss: 0.00124404
Iteration 8/25 | Loss: 0.00124404
Iteration 9/25 | Loss: 0.00124404
Iteration 10/25 | Loss: 0.00124404
Iteration 11/25 | Loss: 0.00124404
Iteration 12/25 | Loss: 0.00124404
Iteration 13/25 | Loss: 0.00124404
Iteration 14/25 | Loss: 0.00124404
Iteration 15/25 | Loss: 0.00124404
Iteration 16/25 | Loss: 0.00124404
Iteration 17/25 | Loss: 0.00124404
Iteration 18/25 | Loss: 0.00124404
Iteration 19/25 | Loss: 0.00124404
Iteration 20/25 | Loss: 0.00124404
Iteration 21/25 | Loss: 0.00124404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012440442806109786, 0.0012440442806109786, 0.0012440442806109786, 0.0012440442806109786, 0.0012440442806109786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012440442806109786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93494368
Iteration 2/25 | Loss: 0.00087502
Iteration 3/25 | Loss: 0.00087502
Iteration 4/25 | Loss: 0.00087502
Iteration 5/25 | Loss: 0.00087501
Iteration 6/25 | Loss: 0.00087501
Iteration 7/25 | Loss: 0.00087501
Iteration 8/25 | Loss: 0.00087501
Iteration 9/25 | Loss: 0.00087501
Iteration 10/25 | Loss: 0.00087501
Iteration 11/25 | Loss: 0.00087501
Iteration 12/25 | Loss: 0.00087501
Iteration 13/25 | Loss: 0.00087501
Iteration 14/25 | Loss: 0.00087501
Iteration 15/25 | Loss: 0.00087501
Iteration 16/25 | Loss: 0.00087501
Iteration 17/25 | Loss: 0.00087501
Iteration 18/25 | Loss: 0.00087501
Iteration 19/25 | Loss: 0.00087501
Iteration 20/25 | Loss: 0.00087501
Iteration 21/25 | Loss: 0.00087501
Iteration 22/25 | Loss: 0.00087501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000875012599863112, 0.000875012599863112, 0.000875012599863112, 0.000875012599863112, 0.000875012599863112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000875012599863112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087501
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001646
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001167
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001141
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001138
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001136
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001125
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001118
Iteration 44/1000 | Loss: 0.00001118
Iteration 45/1000 | Loss: 0.00001117
Iteration 46/1000 | Loss: 0.00001117
Iteration 47/1000 | Loss: 0.00001117
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001116
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001114
Iteration 53/1000 | Loss: 0.00001114
Iteration 54/1000 | Loss: 0.00001114
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001113
Iteration 64/1000 | Loss: 0.00001113
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001108
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001108
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001108
Iteration 88/1000 | Loss: 0.00001108
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001106
Iteration 93/1000 | Loss: 0.00001106
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001104
Iteration 96/1000 | Loss: 0.00001104
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001103
Iteration 100/1000 | Loss: 0.00001103
Iteration 101/1000 | Loss: 0.00001103
Iteration 102/1000 | Loss: 0.00001103
Iteration 103/1000 | Loss: 0.00001102
Iteration 104/1000 | Loss: 0.00001102
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001101
Iteration 108/1000 | Loss: 0.00001101
Iteration 109/1000 | Loss: 0.00001101
Iteration 110/1000 | Loss: 0.00001101
Iteration 111/1000 | Loss: 0.00001101
Iteration 112/1000 | Loss: 0.00001101
Iteration 113/1000 | Loss: 0.00001100
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001095
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001095
Iteration 136/1000 | Loss: 0.00001095
Iteration 137/1000 | Loss: 0.00001094
Iteration 138/1000 | Loss: 0.00001094
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001094
Iteration 141/1000 | Loss: 0.00001094
Iteration 142/1000 | Loss: 0.00001093
Iteration 143/1000 | Loss: 0.00001093
Iteration 144/1000 | Loss: 0.00001093
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001092
Iteration 151/1000 | Loss: 0.00001092
Iteration 152/1000 | Loss: 0.00001092
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001091
Iteration 162/1000 | Loss: 0.00001091
Iteration 163/1000 | Loss: 0.00001091
Iteration 164/1000 | Loss: 0.00001090
Iteration 165/1000 | Loss: 0.00001090
Iteration 166/1000 | Loss: 0.00001090
Iteration 167/1000 | Loss: 0.00001090
Iteration 168/1000 | Loss: 0.00001090
Iteration 169/1000 | Loss: 0.00001090
Iteration 170/1000 | Loss: 0.00001090
Iteration 171/1000 | Loss: 0.00001090
Iteration 172/1000 | Loss: 0.00001090
Iteration 173/1000 | Loss: 0.00001090
Iteration 174/1000 | Loss: 0.00001090
Iteration 175/1000 | Loss: 0.00001090
Iteration 176/1000 | Loss: 0.00001089
Iteration 177/1000 | Loss: 0.00001089
Iteration 178/1000 | Loss: 0.00001089
Iteration 179/1000 | Loss: 0.00001089
Iteration 180/1000 | Loss: 0.00001089
Iteration 181/1000 | Loss: 0.00001089
Iteration 182/1000 | Loss: 0.00001089
Iteration 183/1000 | Loss: 0.00001089
Iteration 184/1000 | Loss: 0.00001089
Iteration 185/1000 | Loss: 0.00001089
Iteration 186/1000 | Loss: 0.00001088
Iteration 187/1000 | Loss: 0.00001088
Iteration 188/1000 | Loss: 0.00001088
Iteration 189/1000 | Loss: 0.00001088
Iteration 190/1000 | Loss: 0.00001088
Iteration 191/1000 | Loss: 0.00001088
Iteration 192/1000 | Loss: 0.00001088
Iteration 193/1000 | Loss: 0.00001088
Iteration 194/1000 | Loss: 0.00001088
Iteration 195/1000 | Loss: 0.00001088
Iteration 196/1000 | Loss: 0.00001088
Iteration 197/1000 | Loss: 0.00001088
Iteration 198/1000 | Loss: 0.00001088
Iteration 199/1000 | Loss: 0.00001088
Iteration 200/1000 | Loss: 0.00001088
Iteration 201/1000 | Loss: 0.00001088
Iteration 202/1000 | Loss: 0.00001088
Iteration 203/1000 | Loss: 0.00001088
Iteration 204/1000 | Loss: 0.00001088
Iteration 205/1000 | Loss: 0.00001088
Iteration 206/1000 | Loss: 0.00001087
Iteration 207/1000 | Loss: 0.00001087
Iteration 208/1000 | Loss: 0.00001087
Iteration 209/1000 | Loss: 0.00001087
Iteration 210/1000 | Loss: 0.00001087
Iteration 211/1000 | Loss: 0.00001087
Iteration 212/1000 | Loss: 0.00001087
Iteration 213/1000 | Loss: 0.00001087
Iteration 214/1000 | Loss: 0.00001087
Iteration 215/1000 | Loss: 0.00001087
Iteration 216/1000 | Loss: 0.00001087
Iteration 217/1000 | Loss: 0.00001087
Iteration 218/1000 | Loss: 0.00001087
Iteration 219/1000 | Loss: 0.00001087
Iteration 220/1000 | Loss: 0.00001087
Iteration 221/1000 | Loss: 0.00001087
Iteration 222/1000 | Loss: 0.00001086
Iteration 223/1000 | Loss: 0.00001086
Iteration 224/1000 | Loss: 0.00001086
Iteration 225/1000 | Loss: 0.00001086
Iteration 226/1000 | Loss: 0.00001086
Iteration 227/1000 | Loss: 0.00001086
Iteration 228/1000 | Loss: 0.00001086
Iteration 229/1000 | Loss: 0.00001086
Iteration 230/1000 | Loss: 0.00001086
Iteration 231/1000 | Loss: 0.00001086
Iteration 232/1000 | Loss: 0.00001086
Iteration 233/1000 | Loss: 0.00001086
Iteration 234/1000 | Loss: 0.00001086
Iteration 235/1000 | Loss: 0.00001086
Iteration 236/1000 | Loss: 0.00001086
Iteration 237/1000 | Loss: 0.00001086
Iteration 238/1000 | Loss: 0.00001086
Iteration 239/1000 | Loss: 0.00001085
Iteration 240/1000 | Loss: 0.00001085
Iteration 241/1000 | Loss: 0.00001085
Iteration 242/1000 | Loss: 0.00001085
Iteration 243/1000 | Loss: 0.00001085
Iteration 244/1000 | Loss: 0.00001084
Iteration 245/1000 | Loss: 0.00001084
Iteration 246/1000 | Loss: 0.00001084
Iteration 247/1000 | Loss: 0.00001084
Iteration 248/1000 | Loss: 0.00001084
Iteration 249/1000 | Loss: 0.00001084
Iteration 250/1000 | Loss: 0.00001084
Iteration 251/1000 | Loss: 0.00001083
Iteration 252/1000 | Loss: 0.00001083
Iteration 253/1000 | Loss: 0.00001083
Iteration 254/1000 | Loss: 0.00001083
Iteration 255/1000 | Loss: 0.00001083
Iteration 256/1000 | Loss: 0.00001083
Iteration 257/1000 | Loss: 0.00001083
Iteration 258/1000 | Loss: 0.00001083
Iteration 259/1000 | Loss: 0.00001083
Iteration 260/1000 | Loss: 0.00001083
Iteration 261/1000 | Loss: 0.00001083
Iteration 262/1000 | Loss: 0.00001083
Iteration 263/1000 | Loss: 0.00001083
Iteration 264/1000 | Loss: 0.00001083
Iteration 265/1000 | Loss: 0.00001083
Iteration 266/1000 | Loss: 0.00001083
Iteration 267/1000 | Loss: 0.00001083
Iteration 268/1000 | Loss: 0.00001083
Iteration 269/1000 | Loss: 0.00001083
Iteration 270/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.0829191523953341e-05, 1.0829191523953341e-05, 1.0829191523953341e-05, 1.0829191523953341e-05, 1.0829191523953341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0829191523953341e-05

Optimization complete. Final v2v error: 2.8129096031188965 mm

Highest mean error: 3.4160706996917725 mm for frame 77

Lowest mean error: 2.6567909717559814 mm for frame 166

Saving results

Total time: 42.3199987411499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781824
Iteration 2/25 | Loss: 0.00157999
Iteration 3/25 | Loss: 0.00132891
Iteration 4/25 | Loss: 0.00131334
Iteration 5/25 | Loss: 0.00131100
Iteration 6/25 | Loss: 0.00131100
Iteration 7/25 | Loss: 0.00131100
Iteration 8/25 | Loss: 0.00131100
Iteration 9/25 | Loss: 0.00131100
Iteration 10/25 | Loss: 0.00131100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013109996216371655, 0.0013109996216371655, 0.0013109996216371655, 0.0013109996216371655, 0.0013109996216371655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013109996216371655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45194757
Iteration 2/25 | Loss: 0.00072956
Iteration 3/25 | Loss: 0.00072955
Iteration 4/25 | Loss: 0.00072955
Iteration 5/25 | Loss: 0.00072955
Iteration 6/25 | Loss: 0.00072955
Iteration 7/25 | Loss: 0.00072955
Iteration 8/25 | Loss: 0.00072955
Iteration 9/25 | Loss: 0.00072955
Iteration 10/25 | Loss: 0.00072955
Iteration 11/25 | Loss: 0.00072955
Iteration 12/25 | Loss: 0.00072955
Iteration 13/25 | Loss: 0.00072955
Iteration 14/25 | Loss: 0.00072955
Iteration 15/25 | Loss: 0.00072955
Iteration 16/25 | Loss: 0.00072955
Iteration 17/25 | Loss: 0.00072955
Iteration 18/25 | Loss: 0.00072955
Iteration 19/25 | Loss: 0.00072955
Iteration 20/25 | Loss: 0.00072955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007295503164641559, 0.0007295503164641559, 0.0007295503164641559, 0.0007295503164641559, 0.0007295503164641559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007295503164641559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072955
Iteration 2/1000 | Loss: 0.00003740
Iteration 3/1000 | Loss: 0.00002564
Iteration 4/1000 | Loss: 0.00002319
Iteration 5/1000 | Loss: 0.00002179
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002000
Iteration 8/1000 | Loss: 0.00001945
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001853
Iteration 11/1000 | Loss: 0.00001843
Iteration 12/1000 | Loss: 0.00001834
Iteration 13/1000 | Loss: 0.00001818
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001805
Iteration 16/1000 | Loss: 0.00001803
Iteration 17/1000 | Loss: 0.00001803
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001793
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001788
Iteration 33/1000 | Loss: 0.00001788
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001785
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001768
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001765
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001764
Iteration 105/1000 | Loss: 0.00001764
Iteration 106/1000 | Loss: 0.00001764
Iteration 107/1000 | Loss: 0.00001764
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001763
Iteration 113/1000 | Loss: 0.00001763
Iteration 114/1000 | Loss: 0.00001763
Iteration 115/1000 | Loss: 0.00001763
Iteration 116/1000 | Loss: 0.00001763
Iteration 117/1000 | Loss: 0.00001763
Iteration 118/1000 | Loss: 0.00001763
Iteration 119/1000 | Loss: 0.00001763
Iteration 120/1000 | Loss: 0.00001763
Iteration 121/1000 | Loss: 0.00001763
Iteration 122/1000 | Loss: 0.00001763
Iteration 123/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.7628774003242142e-05, 1.7628774003242142e-05, 1.7628774003242142e-05, 1.7628774003242142e-05, 1.7628774003242142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7628774003242142e-05

Optimization complete. Final v2v error: 3.551823854446411 mm

Highest mean error: 3.8403983116149902 mm for frame 168

Lowest mean error: 3.231618642807007 mm for frame 221

Saving results

Total time: 39.34032964706421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782213
Iteration 2/25 | Loss: 0.00140224
Iteration 3/25 | Loss: 0.00130803
Iteration 4/25 | Loss: 0.00129229
Iteration 5/25 | Loss: 0.00128798
Iteration 6/25 | Loss: 0.00128701
Iteration 7/25 | Loss: 0.00128701
Iteration 8/25 | Loss: 0.00128701
Iteration 9/25 | Loss: 0.00128701
Iteration 10/25 | Loss: 0.00128701
Iteration 11/25 | Loss: 0.00128701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001287014689296484, 0.001287014689296484, 0.001287014689296484, 0.001287014689296484, 0.001287014689296484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001287014689296484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.22901249
Iteration 2/25 | Loss: 0.00096286
Iteration 3/25 | Loss: 0.00096286
Iteration 4/25 | Loss: 0.00096286
Iteration 5/25 | Loss: 0.00096286
Iteration 6/25 | Loss: 0.00096286
Iteration 7/25 | Loss: 0.00096286
Iteration 8/25 | Loss: 0.00096286
Iteration 9/25 | Loss: 0.00096286
Iteration 10/25 | Loss: 0.00096286
Iteration 11/25 | Loss: 0.00096286
Iteration 12/25 | Loss: 0.00096286
Iteration 13/25 | Loss: 0.00096286
Iteration 14/25 | Loss: 0.00096286
Iteration 15/25 | Loss: 0.00096286
Iteration 16/25 | Loss: 0.00096286
Iteration 17/25 | Loss: 0.00096286
Iteration 18/25 | Loss: 0.00096286
Iteration 19/25 | Loss: 0.00096286
Iteration 20/25 | Loss: 0.00096286
Iteration 21/25 | Loss: 0.00096286
Iteration 22/25 | Loss: 0.00096286
Iteration 23/25 | Loss: 0.00096286
Iteration 24/25 | Loss: 0.00096286
Iteration 25/25 | Loss: 0.00096286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096286
Iteration 2/1000 | Loss: 0.00006971
Iteration 3/1000 | Loss: 0.00004291
Iteration 4/1000 | Loss: 0.00003374
Iteration 5/1000 | Loss: 0.00002899
Iteration 6/1000 | Loss: 0.00002643
Iteration 7/1000 | Loss: 0.00002458
Iteration 8/1000 | Loss: 0.00002344
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002199
Iteration 11/1000 | Loss: 0.00002166
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002089
Iteration 15/1000 | Loss: 0.00002088
Iteration 16/1000 | Loss: 0.00002071
Iteration 17/1000 | Loss: 0.00002061
Iteration 18/1000 | Loss: 0.00002057
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002054
Iteration 23/1000 | Loss: 0.00002054
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002051
Iteration 29/1000 | Loss: 0.00002050
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002046
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002045
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002044
Iteration 43/1000 | Loss: 0.00002043
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002042
Iteration 49/1000 | Loss: 0.00002042
Iteration 50/1000 | Loss: 0.00002042
Iteration 51/1000 | Loss: 0.00002041
Iteration 52/1000 | Loss: 0.00002041
Iteration 53/1000 | Loss: 0.00002041
Iteration 54/1000 | Loss: 0.00002041
Iteration 55/1000 | Loss: 0.00002040
Iteration 56/1000 | Loss: 0.00002040
Iteration 57/1000 | Loss: 0.00002040
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00002039
Iteration 60/1000 | Loss: 0.00002039
Iteration 61/1000 | Loss: 0.00002038
Iteration 62/1000 | Loss: 0.00002038
Iteration 63/1000 | Loss: 0.00002038
Iteration 64/1000 | Loss: 0.00002038
Iteration 65/1000 | Loss: 0.00002037
Iteration 66/1000 | Loss: 0.00002037
Iteration 67/1000 | Loss: 0.00002037
Iteration 68/1000 | Loss: 0.00002037
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002036
Iteration 71/1000 | Loss: 0.00002036
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00002035
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002034
Iteration 86/1000 | Loss: 0.00002034
Iteration 87/1000 | Loss: 0.00002033
Iteration 88/1000 | Loss: 0.00002033
Iteration 89/1000 | Loss: 0.00002033
Iteration 90/1000 | Loss: 0.00002033
Iteration 91/1000 | Loss: 0.00002033
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002033
Iteration 94/1000 | Loss: 0.00002033
Iteration 95/1000 | Loss: 0.00002032
Iteration 96/1000 | Loss: 0.00002032
Iteration 97/1000 | Loss: 0.00002032
Iteration 98/1000 | Loss: 0.00002032
Iteration 99/1000 | Loss: 0.00002032
Iteration 100/1000 | Loss: 0.00002032
Iteration 101/1000 | Loss: 0.00002032
Iteration 102/1000 | Loss: 0.00002031
Iteration 103/1000 | Loss: 0.00002031
Iteration 104/1000 | Loss: 0.00002031
Iteration 105/1000 | Loss: 0.00002031
Iteration 106/1000 | Loss: 0.00002031
Iteration 107/1000 | Loss: 0.00002031
Iteration 108/1000 | Loss: 0.00002031
Iteration 109/1000 | Loss: 0.00002031
Iteration 110/1000 | Loss: 0.00002031
Iteration 111/1000 | Loss: 0.00002030
Iteration 112/1000 | Loss: 0.00002030
Iteration 113/1000 | Loss: 0.00002030
Iteration 114/1000 | Loss: 0.00002030
Iteration 115/1000 | Loss: 0.00002030
Iteration 116/1000 | Loss: 0.00002030
Iteration 117/1000 | Loss: 0.00002030
Iteration 118/1000 | Loss: 0.00002029
Iteration 119/1000 | Loss: 0.00002029
Iteration 120/1000 | Loss: 0.00002029
Iteration 121/1000 | Loss: 0.00002029
Iteration 122/1000 | Loss: 0.00002029
Iteration 123/1000 | Loss: 0.00002029
Iteration 124/1000 | Loss: 0.00002028
Iteration 125/1000 | Loss: 0.00002028
Iteration 126/1000 | Loss: 0.00002028
Iteration 127/1000 | Loss: 0.00002028
Iteration 128/1000 | Loss: 0.00002028
Iteration 129/1000 | Loss: 0.00002028
Iteration 130/1000 | Loss: 0.00002028
Iteration 131/1000 | Loss: 0.00002027
Iteration 132/1000 | Loss: 0.00002027
Iteration 133/1000 | Loss: 0.00002027
Iteration 134/1000 | Loss: 0.00002027
Iteration 135/1000 | Loss: 0.00002027
Iteration 136/1000 | Loss: 0.00002027
Iteration 137/1000 | Loss: 0.00002027
Iteration 138/1000 | Loss: 0.00002026
Iteration 139/1000 | Loss: 0.00002026
Iteration 140/1000 | Loss: 0.00002026
Iteration 141/1000 | Loss: 0.00002026
Iteration 142/1000 | Loss: 0.00002026
Iteration 143/1000 | Loss: 0.00002026
Iteration 144/1000 | Loss: 0.00002026
Iteration 145/1000 | Loss: 0.00002026
Iteration 146/1000 | Loss: 0.00002026
Iteration 147/1000 | Loss: 0.00002026
Iteration 148/1000 | Loss: 0.00002026
Iteration 149/1000 | Loss: 0.00002026
Iteration 150/1000 | Loss: 0.00002026
Iteration 151/1000 | Loss: 0.00002026
Iteration 152/1000 | Loss: 0.00002026
Iteration 153/1000 | Loss: 0.00002026
Iteration 154/1000 | Loss: 0.00002026
Iteration 155/1000 | Loss: 0.00002026
Iteration 156/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.025578032771591e-05, 2.025578032771591e-05, 2.025578032771591e-05, 2.025578032771591e-05, 2.025578032771591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.025578032771591e-05

Optimization complete. Final v2v error: 3.768348455429077 mm

Highest mean error: 5.333806037902832 mm for frame 55

Lowest mean error: 3.050947666168213 mm for frame 39

Saving results

Total time: 40.70680904388428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858582
Iteration 2/25 | Loss: 0.00189843
Iteration 3/25 | Loss: 0.00156872
Iteration 4/25 | Loss: 0.00150009
Iteration 5/25 | Loss: 0.00153930
Iteration 6/25 | Loss: 0.00145343
Iteration 7/25 | Loss: 0.00140262
Iteration 8/25 | Loss: 0.00137804
Iteration 9/25 | Loss: 0.00137756
Iteration 10/25 | Loss: 0.00138324
Iteration 11/25 | Loss: 0.00137554
Iteration 12/25 | Loss: 0.00137778
Iteration 13/25 | Loss: 0.00135529
Iteration 14/25 | Loss: 0.00134873
Iteration 15/25 | Loss: 0.00134757
Iteration 16/25 | Loss: 0.00135183
Iteration 17/25 | Loss: 0.00135182
Iteration 18/25 | Loss: 0.00135168
Iteration 19/25 | Loss: 0.00135122
Iteration 20/25 | Loss: 0.00134752
Iteration 21/25 | Loss: 0.00134599
Iteration 22/25 | Loss: 0.00134573
Iteration 23/25 | Loss: 0.00134565
Iteration 24/25 | Loss: 0.00134565
Iteration 25/25 | Loss: 0.00134565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58999777
Iteration 2/25 | Loss: 0.00091999
Iteration 3/25 | Loss: 0.00091999
Iteration 4/25 | Loss: 0.00091999
Iteration 5/25 | Loss: 0.00091999
Iteration 6/25 | Loss: 0.00091999
Iteration 7/25 | Loss: 0.00091999
Iteration 8/25 | Loss: 0.00091999
Iteration 9/25 | Loss: 0.00091998
Iteration 10/25 | Loss: 0.00091998
Iteration 11/25 | Loss: 0.00091998
Iteration 12/25 | Loss: 0.00091998
Iteration 13/25 | Loss: 0.00091998
Iteration 14/25 | Loss: 0.00091998
Iteration 15/25 | Loss: 0.00091998
Iteration 16/25 | Loss: 0.00091998
Iteration 17/25 | Loss: 0.00091998
Iteration 18/25 | Loss: 0.00091998
Iteration 19/25 | Loss: 0.00091998
Iteration 20/25 | Loss: 0.00091998
Iteration 21/25 | Loss: 0.00091998
Iteration 22/25 | Loss: 0.00091998
Iteration 23/25 | Loss: 0.00091998
Iteration 24/25 | Loss: 0.00091998
Iteration 25/25 | Loss: 0.00091998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091998
Iteration 2/1000 | Loss: 0.00004266
Iteration 3/1000 | Loss: 0.00017020
Iteration 4/1000 | Loss: 0.00002490
Iteration 5/1000 | Loss: 0.00002342
Iteration 6/1000 | Loss: 0.00002233
Iteration 7/1000 | Loss: 0.00002181
Iteration 8/1000 | Loss: 0.00002123
Iteration 9/1000 | Loss: 0.00002092
Iteration 10/1000 | Loss: 0.00002061
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002007
Iteration 14/1000 | Loss: 0.00002006
Iteration 15/1000 | Loss: 0.00002005
Iteration 16/1000 | Loss: 0.00002001
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001997
Iteration 19/1000 | Loss: 0.00001997
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001987
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001985
Iteration 25/1000 | Loss: 0.00001985
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001984
Iteration 28/1000 | Loss: 0.00001983
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00001981
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001978
Iteration 40/1000 | Loss: 0.00001977
Iteration 41/1000 | Loss: 0.00001977
Iteration 42/1000 | Loss: 0.00001976
Iteration 43/1000 | Loss: 0.00001976
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001974
Iteration 46/1000 | Loss: 0.00001974
Iteration 47/1000 | Loss: 0.00001974
Iteration 48/1000 | Loss: 0.00001973
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001970
Iteration 58/1000 | Loss: 0.00001970
Iteration 59/1000 | Loss: 0.00001970
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001969
Iteration 62/1000 | Loss: 0.00001969
Iteration 63/1000 | Loss: 0.00001968
Iteration 64/1000 | Loss: 0.00001968
Iteration 65/1000 | Loss: 0.00001968
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001966
Iteration 70/1000 | Loss: 0.00001966
Iteration 71/1000 | Loss: 0.00001966
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001964
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001959
Iteration 103/1000 | Loss: 0.00001959
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001959
Iteration 107/1000 | Loss: 0.00001959
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001958
Iteration 112/1000 | Loss: 0.00001958
Iteration 113/1000 | Loss: 0.00001958
Iteration 114/1000 | Loss: 0.00001958
Iteration 115/1000 | Loss: 0.00001958
Iteration 116/1000 | Loss: 0.00001958
Iteration 117/1000 | Loss: 0.00001957
Iteration 118/1000 | Loss: 0.00001957
Iteration 119/1000 | Loss: 0.00001957
Iteration 120/1000 | Loss: 0.00001957
Iteration 121/1000 | Loss: 0.00001957
Iteration 122/1000 | Loss: 0.00001957
Iteration 123/1000 | Loss: 0.00001957
Iteration 124/1000 | Loss: 0.00001957
Iteration 125/1000 | Loss: 0.00001956
Iteration 126/1000 | Loss: 0.00001956
Iteration 127/1000 | Loss: 0.00001956
Iteration 128/1000 | Loss: 0.00001956
Iteration 129/1000 | Loss: 0.00001956
Iteration 130/1000 | Loss: 0.00001955
Iteration 131/1000 | Loss: 0.00001955
Iteration 132/1000 | Loss: 0.00001955
Iteration 133/1000 | Loss: 0.00001955
Iteration 134/1000 | Loss: 0.00001954
Iteration 135/1000 | Loss: 0.00001954
Iteration 136/1000 | Loss: 0.00001954
Iteration 137/1000 | Loss: 0.00001953
Iteration 138/1000 | Loss: 0.00001953
Iteration 139/1000 | Loss: 0.00001953
Iteration 140/1000 | Loss: 0.00001953
Iteration 141/1000 | Loss: 0.00001953
Iteration 142/1000 | Loss: 0.00001953
Iteration 143/1000 | Loss: 0.00001953
Iteration 144/1000 | Loss: 0.00001953
Iteration 145/1000 | Loss: 0.00001952
Iteration 146/1000 | Loss: 0.00001952
Iteration 147/1000 | Loss: 0.00001951
Iteration 148/1000 | Loss: 0.00001951
Iteration 149/1000 | Loss: 0.00001951
Iteration 150/1000 | Loss: 0.00001951
Iteration 151/1000 | Loss: 0.00001951
Iteration 152/1000 | Loss: 0.00001951
Iteration 153/1000 | Loss: 0.00001951
Iteration 154/1000 | Loss: 0.00001951
Iteration 155/1000 | Loss: 0.00001951
Iteration 156/1000 | Loss: 0.00001951
Iteration 157/1000 | Loss: 0.00001950
Iteration 158/1000 | Loss: 0.00001950
Iteration 159/1000 | Loss: 0.00001950
Iteration 160/1000 | Loss: 0.00001950
Iteration 161/1000 | Loss: 0.00001949
Iteration 162/1000 | Loss: 0.00001949
Iteration 163/1000 | Loss: 0.00001949
Iteration 164/1000 | Loss: 0.00001949
Iteration 165/1000 | Loss: 0.00001949
Iteration 166/1000 | Loss: 0.00001949
Iteration 167/1000 | Loss: 0.00001949
Iteration 168/1000 | Loss: 0.00001949
Iteration 169/1000 | Loss: 0.00001948
Iteration 170/1000 | Loss: 0.00001948
Iteration 171/1000 | Loss: 0.00001948
Iteration 172/1000 | Loss: 0.00001948
Iteration 173/1000 | Loss: 0.00001948
Iteration 174/1000 | Loss: 0.00001948
Iteration 175/1000 | Loss: 0.00001948
Iteration 176/1000 | Loss: 0.00001948
Iteration 177/1000 | Loss: 0.00001948
Iteration 178/1000 | Loss: 0.00001948
Iteration 179/1000 | Loss: 0.00001948
Iteration 180/1000 | Loss: 0.00001948
Iteration 181/1000 | Loss: 0.00001948
Iteration 182/1000 | Loss: 0.00001947
Iteration 183/1000 | Loss: 0.00001947
Iteration 184/1000 | Loss: 0.00001947
Iteration 185/1000 | Loss: 0.00001947
Iteration 186/1000 | Loss: 0.00001947
Iteration 187/1000 | Loss: 0.00001947
Iteration 188/1000 | Loss: 0.00001947
Iteration 189/1000 | Loss: 0.00001947
Iteration 190/1000 | Loss: 0.00001947
Iteration 191/1000 | Loss: 0.00001947
Iteration 192/1000 | Loss: 0.00001947
Iteration 193/1000 | Loss: 0.00001947
Iteration 194/1000 | Loss: 0.00001947
Iteration 195/1000 | Loss: 0.00001947
Iteration 196/1000 | Loss: 0.00001947
Iteration 197/1000 | Loss: 0.00001947
Iteration 198/1000 | Loss: 0.00001947
Iteration 199/1000 | Loss: 0.00001947
Iteration 200/1000 | Loss: 0.00001947
Iteration 201/1000 | Loss: 0.00001947
Iteration 202/1000 | Loss: 0.00001947
Iteration 203/1000 | Loss: 0.00001947
Iteration 204/1000 | Loss: 0.00001947
Iteration 205/1000 | Loss: 0.00001947
Iteration 206/1000 | Loss: 0.00001947
Iteration 207/1000 | Loss: 0.00001947
Iteration 208/1000 | Loss: 0.00001947
Iteration 209/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.9471533960313536e-05, 1.9471533960313536e-05, 1.9471533960313536e-05, 1.9471533960313536e-05, 1.9471533960313536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9471533960313536e-05

Optimization complete. Final v2v error: 3.6401891708374023 mm

Highest mean error: 5.574454307556152 mm for frame 95

Lowest mean error: 3.222562551498413 mm for frame 52

Saving results

Total time: 73.25025153160095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444203
Iteration 2/25 | Loss: 0.00135755
Iteration 3/25 | Loss: 0.00128470
Iteration 4/25 | Loss: 0.00127166
Iteration 5/25 | Loss: 0.00126886
Iteration 6/25 | Loss: 0.00126823
Iteration 7/25 | Loss: 0.00126823
Iteration 8/25 | Loss: 0.00126823
Iteration 9/25 | Loss: 0.00126823
Iteration 10/25 | Loss: 0.00126823
Iteration 11/25 | Loss: 0.00126823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012682256055995822, 0.0012682256055995822, 0.0012682256055995822, 0.0012682256055995822, 0.0012682256055995822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012682256055995822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50057447
Iteration 2/25 | Loss: 0.00085013
Iteration 3/25 | Loss: 0.00085013
Iteration 4/25 | Loss: 0.00085013
Iteration 5/25 | Loss: 0.00085013
Iteration 6/25 | Loss: 0.00085013
Iteration 7/25 | Loss: 0.00085013
Iteration 8/25 | Loss: 0.00085013
Iteration 9/25 | Loss: 0.00085013
Iteration 10/25 | Loss: 0.00085013
Iteration 11/25 | Loss: 0.00085013
Iteration 12/25 | Loss: 0.00085013
Iteration 13/25 | Loss: 0.00085013
Iteration 14/25 | Loss: 0.00085013
Iteration 15/25 | Loss: 0.00085013
Iteration 16/25 | Loss: 0.00085013
Iteration 17/25 | Loss: 0.00085013
Iteration 18/25 | Loss: 0.00085013
Iteration 19/25 | Loss: 0.00085013
Iteration 20/25 | Loss: 0.00085013
Iteration 21/25 | Loss: 0.00085013
Iteration 22/25 | Loss: 0.00085013
Iteration 23/25 | Loss: 0.00085013
Iteration 24/25 | Loss: 0.00085013
Iteration 25/25 | Loss: 0.00085013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085013
Iteration 2/1000 | Loss: 0.00003189
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00001912
Iteration 5/1000 | Loss: 0.00001822
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001728
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001680
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001638
Iteration 15/1000 | Loss: 0.00001628
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001601
Iteration 36/1000 | Loss: 0.00001601
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001600
Iteration 39/1000 | Loss: 0.00001600
Iteration 40/1000 | Loss: 0.00001599
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001597
Iteration 46/1000 | Loss: 0.00001596
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001594
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001594
Iteration 52/1000 | Loss: 0.00001593
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001585
Iteration 65/1000 | Loss: 0.00001584
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001580
Iteration 72/1000 | Loss: 0.00001580
Iteration 73/1000 | Loss: 0.00001580
Iteration 74/1000 | Loss: 0.00001579
Iteration 75/1000 | Loss: 0.00001579
Iteration 76/1000 | Loss: 0.00001579
Iteration 77/1000 | Loss: 0.00001578
Iteration 78/1000 | Loss: 0.00001578
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001576
Iteration 83/1000 | Loss: 0.00001576
Iteration 84/1000 | Loss: 0.00001575
Iteration 85/1000 | Loss: 0.00001575
Iteration 86/1000 | Loss: 0.00001575
Iteration 87/1000 | Loss: 0.00001575
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001574
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001574
Iteration 92/1000 | Loss: 0.00001573
Iteration 93/1000 | Loss: 0.00001573
Iteration 94/1000 | Loss: 0.00001573
Iteration 95/1000 | Loss: 0.00001573
Iteration 96/1000 | Loss: 0.00001573
Iteration 97/1000 | Loss: 0.00001573
Iteration 98/1000 | Loss: 0.00001573
Iteration 99/1000 | Loss: 0.00001573
Iteration 100/1000 | Loss: 0.00001572
Iteration 101/1000 | Loss: 0.00001572
Iteration 102/1000 | Loss: 0.00001572
Iteration 103/1000 | Loss: 0.00001572
Iteration 104/1000 | Loss: 0.00001572
Iteration 105/1000 | Loss: 0.00001572
Iteration 106/1000 | Loss: 0.00001572
Iteration 107/1000 | Loss: 0.00001572
Iteration 108/1000 | Loss: 0.00001572
Iteration 109/1000 | Loss: 0.00001572
Iteration 110/1000 | Loss: 0.00001571
Iteration 111/1000 | Loss: 0.00001571
Iteration 112/1000 | Loss: 0.00001571
Iteration 113/1000 | Loss: 0.00001571
Iteration 114/1000 | Loss: 0.00001571
Iteration 115/1000 | Loss: 0.00001571
Iteration 116/1000 | Loss: 0.00001571
Iteration 117/1000 | Loss: 0.00001570
Iteration 118/1000 | Loss: 0.00001570
Iteration 119/1000 | Loss: 0.00001570
Iteration 120/1000 | Loss: 0.00001570
Iteration 121/1000 | Loss: 0.00001570
Iteration 122/1000 | Loss: 0.00001570
Iteration 123/1000 | Loss: 0.00001570
Iteration 124/1000 | Loss: 0.00001570
Iteration 125/1000 | Loss: 0.00001570
Iteration 126/1000 | Loss: 0.00001570
Iteration 127/1000 | Loss: 0.00001570
Iteration 128/1000 | Loss: 0.00001570
Iteration 129/1000 | Loss: 0.00001570
Iteration 130/1000 | Loss: 0.00001570
Iteration 131/1000 | Loss: 0.00001570
Iteration 132/1000 | Loss: 0.00001570
Iteration 133/1000 | Loss: 0.00001570
Iteration 134/1000 | Loss: 0.00001570
Iteration 135/1000 | Loss: 0.00001570
Iteration 136/1000 | Loss: 0.00001570
Iteration 137/1000 | Loss: 0.00001570
Iteration 138/1000 | Loss: 0.00001570
Iteration 139/1000 | Loss: 0.00001570
Iteration 140/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.56962159962859e-05, 1.56962159962859e-05, 1.56962159962859e-05, 1.56962159962859e-05, 1.56962159962859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.56962159962859e-05

Optimization complete. Final v2v error: 3.3675060272216797 mm

Highest mean error: 3.879249095916748 mm for frame 78

Lowest mean error: 3.226151466369629 mm for frame 32

Saving results

Total time: 36.69308662414551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801823
Iteration 2/25 | Loss: 0.00133044
Iteration 3/25 | Loss: 0.00127490
Iteration 4/25 | Loss: 0.00126818
Iteration 5/25 | Loss: 0.00126605
Iteration 6/25 | Loss: 0.00126600
Iteration 7/25 | Loss: 0.00126600
Iteration 8/25 | Loss: 0.00126600
Iteration 9/25 | Loss: 0.00126600
Iteration 10/25 | Loss: 0.00126600
Iteration 11/25 | Loss: 0.00126600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012659996282309294, 0.0012659996282309294, 0.0012659996282309294, 0.0012659996282309294, 0.0012659996282309294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659996282309294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.18807507
Iteration 2/25 | Loss: 0.00083276
Iteration 3/25 | Loss: 0.00083275
Iteration 4/25 | Loss: 0.00083274
Iteration 5/25 | Loss: 0.00083274
Iteration 6/25 | Loss: 0.00083274
Iteration 7/25 | Loss: 0.00083274
Iteration 8/25 | Loss: 0.00083274
Iteration 9/25 | Loss: 0.00083274
Iteration 10/25 | Loss: 0.00083274
Iteration 11/25 | Loss: 0.00083274
Iteration 12/25 | Loss: 0.00083274
Iteration 13/25 | Loss: 0.00083274
Iteration 14/25 | Loss: 0.00083274
Iteration 15/25 | Loss: 0.00083274
Iteration 16/25 | Loss: 0.00083274
Iteration 17/25 | Loss: 0.00083274
Iteration 18/25 | Loss: 0.00083274
Iteration 19/25 | Loss: 0.00083274
Iteration 20/25 | Loss: 0.00083274
Iteration 21/25 | Loss: 0.00083274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008327420800924301, 0.0008327420800924301, 0.0008327420800924301, 0.0008327420800924301, 0.0008327420800924301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008327420800924301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083274
Iteration 2/1000 | Loss: 0.00003429
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00001882
Iteration 5/1000 | Loss: 0.00001769
Iteration 6/1000 | Loss: 0.00001693
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001577
Iteration 10/1000 | Loss: 0.00001559
Iteration 11/1000 | Loss: 0.00001558
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001502
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001494
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001492
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001484
Iteration 55/1000 | Loss: 0.00001484
Iteration 56/1000 | Loss: 0.00001484
Iteration 57/1000 | Loss: 0.00001484
Iteration 58/1000 | Loss: 0.00001484
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001483
Iteration 62/1000 | Loss: 0.00001483
Iteration 63/1000 | Loss: 0.00001483
Iteration 64/1000 | Loss: 0.00001482
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001476
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001472
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001471
Iteration 104/1000 | Loss: 0.00001471
Iteration 105/1000 | Loss: 0.00001471
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001469
Iteration 114/1000 | Loss: 0.00001469
Iteration 115/1000 | Loss: 0.00001469
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001466
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001466
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001464
Iteration 134/1000 | Loss: 0.00001464
Iteration 135/1000 | Loss: 0.00001464
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001461
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001460
Iteration 157/1000 | Loss: 0.00001460
Iteration 158/1000 | Loss: 0.00001459
Iteration 159/1000 | Loss: 0.00001459
Iteration 160/1000 | Loss: 0.00001459
Iteration 161/1000 | Loss: 0.00001459
Iteration 162/1000 | Loss: 0.00001459
Iteration 163/1000 | Loss: 0.00001459
Iteration 164/1000 | Loss: 0.00001459
Iteration 165/1000 | Loss: 0.00001459
Iteration 166/1000 | Loss: 0.00001459
Iteration 167/1000 | Loss: 0.00001458
Iteration 168/1000 | Loss: 0.00001458
Iteration 169/1000 | Loss: 0.00001458
Iteration 170/1000 | Loss: 0.00001458
Iteration 171/1000 | Loss: 0.00001458
Iteration 172/1000 | Loss: 0.00001457
Iteration 173/1000 | Loss: 0.00001457
Iteration 174/1000 | Loss: 0.00001457
Iteration 175/1000 | Loss: 0.00001457
Iteration 176/1000 | Loss: 0.00001457
Iteration 177/1000 | Loss: 0.00001457
Iteration 178/1000 | Loss: 0.00001457
Iteration 179/1000 | Loss: 0.00001457
Iteration 180/1000 | Loss: 0.00001457
Iteration 181/1000 | Loss: 0.00001457
Iteration 182/1000 | Loss: 0.00001457
Iteration 183/1000 | Loss: 0.00001457
Iteration 184/1000 | Loss: 0.00001457
Iteration 185/1000 | Loss: 0.00001457
Iteration 186/1000 | Loss: 0.00001456
Iteration 187/1000 | Loss: 0.00001456
Iteration 188/1000 | Loss: 0.00001456
Iteration 189/1000 | Loss: 0.00001456
Iteration 190/1000 | Loss: 0.00001456
Iteration 191/1000 | Loss: 0.00001456
Iteration 192/1000 | Loss: 0.00001456
Iteration 193/1000 | Loss: 0.00001456
Iteration 194/1000 | Loss: 0.00001456
Iteration 195/1000 | Loss: 0.00001456
Iteration 196/1000 | Loss: 0.00001456
Iteration 197/1000 | Loss: 0.00001456
Iteration 198/1000 | Loss: 0.00001456
Iteration 199/1000 | Loss: 0.00001456
Iteration 200/1000 | Loss: 0.00001456
Iteration 201/1000 | Loss: 0.00001456
Iteration 202/1000 | Loss: 0.00001455
Iteration 203/1000 | Loss: 0.00001455
Iteration 204/1000 | Loss: 0.00001455
Iteration 205/1000 | Loss: 0.00001455
Iteration 206/1000 | Loss: 0.00001455
Iteration 207/1000 | Loss: 0.00001455
Iteration 208/1000 | Loss: 0.00001455
Iteration 209/1000 | Loss: 0.00001455
Iteration 210/1000 | Loss: 0.00001455
Iteration 211/1000 | Loss: 0.00001455
Iteration 212/1000 | Loss: 0.00001455
Iteration 213/1000 | Loss: 0.00001455
Iteration 214/1000 | Loss: 0.00001455
Iteration 215/1000 | Loss: 0.00001455
Iteration 216/1000 | Loss: 0.00001455
Iteration 217/1000 | Loss: 0.00001455
Iteration 218/1000 | Loss: 0.00001455
Iteration 219/1000 | Loss: 0.00001455
Iteration 220/1000 | Loss: 0.00001455
Iteration 221/1000 | Loss: 0.00001455
Iteration 222/1000 | Loss: 0.00001455
Iteration 223/1000 | Loss: 0.00001455
Iteration 224/1000 | Loss: 0.00001455
Iteration 225/1000 | Loss: 0.00001455
Iteration 226/1000 | Loss: 0.00001455
Iteration 227/1000 | Loss: 0.00001455
Iteration 228/1000 | Loss: 0.00001455
Iteration 229/1000 | Loss: 0.00001455
Iteration 230/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.4551614185620565e-05, 1.4551614185620565e-05, 1.4551614185620565e-05, 1.4551614185620565e-05, 1.4551614185620565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4551614185620565e-05

Optimization complete. Final v2v error: 3.2332189083099365 mm

Highest mean error: 3.4201536178588867 mm for frame 22

Lowest mean error: 3.0187313556671143 mm for frame 96

Saving results

Total time: 40.009236574172974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516115
Iteration 2/25 | Loss: 0.00133800
Iteration 3/25 | Loss: 0.00126911
Iteration 4/25 | Loss: 0.00125829
Iteration 5/25 | Loss: 0.00125564
Iteration 6/25 | Loss: 0.00125564
Iteration 7/25 | Loss: 0.00125564
Iteration 8/25 | Loss: 0.00125564
Iteration 9/25 | Loss: 0.00125564
Iteration 10/25 | Loss: 0.00125564
Iteration 11/25 | Loss: 0.00125564
Iteration 12/25 | Loss: 0.00125564
Iteration 13/25 | Loss: 0.00125564
Iteration 14/25 | Loss: 0.00125564
Iteration 15/25 | Loss: 0.00125564
Iteration 16/25 | Loss: 0.00125564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001255635404959321, 0.001255635404959321, 0.001255635404959321, 0.001255635404959321, 0.001255635404959321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001255635404959321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45781803
Iteration 2/25 | Loss: 0.00080673
Iteration 3/25 | Loss: 0.00080673
Iteration 4/25 | Loss: 0.00080672
Iteration 5/25 | Loss: 0.00080672
Iteration 6/25 | Loss: 0.00080672
Iteration 7/25 | Loss: 0.00080672
Iteration 8/25 | Loss: 0.00080672
Iteration 9/25 | Loss: 0.00080672
Iteration 10/25 | Loss: 0.00080672
Iteration 11/25 | Loss: 0.00080672
Iteration 12/25 | Loss: 0.00080672
Iteration 13/25 | Loss: 0.00080672
Iteration 14/25 | Loss: 0.00080672
Iteration 15/25 | Loss: 0.00080672
Iteration 16/25 | Loss: 0.00080672
Iteration 17/25 | Loss: 0.00080672
Iteration 18/25 | Loss: 0.00080672
Iteration 19/25 | Loss: 0.00080672
Iteration 20/25 | Loss: 0.00080672
Iteration 21/25 | Loss: 0.00080672
Iteration 22/25 | Loss: 0.00080672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008067215676419437, 0.0008067215676419437, 0.0008067215676419437, 0.0008067215676419437, 0.0008067215676419437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008067215676419437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080672
Iteration 2/1000 | Loss: 0.00002311
Iteration 3/1000 | Loss: 0.00001859
Iteration 4/1000 | Loss: 0.00001712
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001550
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001475
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001446
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001413
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001387
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001382
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001373
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001347
Iteration 56/1000 | Loss: 0.00001347
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001342
Iteration 81/1000 | Loss: 0.00001342
Iteration 82/1000 | Loss: 0.00001342
Iteration 83/1000 | Loss: 0.00001342
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001342
Iteration 86/1000 | Loss: 0.00001342
Iteration 87/1000 | Loss: 0.00001342
Iteration 88/1000 | Loss: 0.00001342
Iteration 89/1000 | Loss: 0.00001342
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.3419516108115204e-05, 1.3419516108115204e-05, 1.3419516108115204e-05, 1.3419516108115204e-05, 1.3419516108115204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3419516108115204e-05

Optimization complete. Final v2v error: 3.1633262634277344 mm

Highest mean error: 3.434791326522827 mm for frame 232

Lowest mean error: 2.919053316116333 mm for frame 4

Saving results

Total time: 40.39706778526306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583369
Iteration 2/25 | Loss: 0.00151667
Iteration 3/25 | Loss: 0.00138086
Iteration 4/25 | Loss: 0.00134374
Iteration 5/25 | Loss: 0.00133594
Iteration 6/25 | Loss: 0.00133452
Iteration 7/25 | Loss: 0.00133431
Iteration 8/25 | Loss: 0.00133431
Iteration 9/25 | Loss: 0.00133431
Iteration 10/25 | Loss: 0.00133431
Iteration 11/25 | Loss: 0.00133431
Iteration 12/25 | Loss: 0.00133431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00133430864661932, 0.00133430864661932, 0.00133430864661932, 0.00133430864661932, 0.00133430864661932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133430864661932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39101243
Iteration 2/25 | Loss: 0.00084311
Iteration 3/25 | Loss: 0.00084306
Iteration 4/25 | Loss: 0.00084306
Iteration 5/25 | Loss: 0.00084306
Iteration 6/25 | Loss: 0.00084306
Iteration 7/25 | Loss: 0.00084306
Iteration 8/25 | Loss: 0.00084306
Iteration 9/25 | Loss: 0.00084306
Iteration 10/25 | Loss: 0.00084306
Iteration 11/25 | Loss: 0.00084306
Iteration 12/25 | Loss: 0.00084306
Iteration 13/25 | Loss: 0.00084306
Iteration 14/25 | Loss: 0.00084306
Iteration 15/25 | Loss: 0.00084306
Iteration 16/25 | Loss: 0.00084306
Iteration 17/25 | Loss: 0.00084306
Iteration 18/25 | Loss: 0.00084306
Iteration 19/25 | Loss: 0.00084306
Iteration 20/25 | Loss: 0.00084306
Iteration 21/25 | Loss: 0.00084306
Iteration 22/25 | Loss: 0.00084306
Iteration 23/25 | Loss: 0.00084306
Iteration 24/25 | Loss: 0.00084306
Iteration 25/25 | Loss: 0.00084306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084306
Iteration 2/1000 | Loss: 0.00007408
Iteration 3/1000 | Loss: 0.00005184
Iteration 4/1000 | Loss: 0.00004376
Iteration 5/1000 | Loss: 0.00004120
Iteration 6/1000 | Loss: 0.00003994
Iteration 7/1000 | Loss: 0.00003906
Iteration 8/1000 | Loss: 0.00003860
Iteration 9/1000 | Loss: 0.00003805
Iteration 10/1000 | Loss: 0.00003762
Iteration 11/1000 | Loss: 0.00003732
Iteration 12/1000 | Loss: 0.00003706
Iteration 13/1000 | Loss: 0.00003685
Iteration 14/1000 | Loss: 0.00003685
Iteration 15/1000 | Loss: 0.00003674
Iteration 16/1000 | Loss: 0.00003661
Iteration 17/1000 | Loss: 0.00003660
Iteration 18/1000 | Loss: 0.00003658
Iteration 19/1000 | Loss: 0.00003657
Iteration 20/1000 | Loss: 0.00003652
Iteration 21/1000 | Loss: 0.00003651
Iteration 22/1000 | Loss: 0.00003650
Iteration 23/1000 | Loss: 0.00003649
Iteration 24/1000 | Loss: 0.00003647
Iteration 25/1000 | Loss: 0.00003644
Iteration 26/1000 | Loss: 0.00003643
Iteration 27/1000 | Loss: 0.00003642
Iteration 28/1000 | Loss: 0.00003642
Iteration 29/1000 | Loss: 0.00003639
Iteration 30/1000 | Loss: 0.00003638
Iteration 31/1000 | Loss: 0.00003638
Iteration 32/1000 | Loss: 0.00003637
Iteration 33/1000 | Loss: 0.00003637
Iteration 34/1000 | Loss: 0.00003637
Iteration 35/1000 | Loss: 0.00003636
Iteration 36/1000 | Loss: 0.00003636
Iteration 37/1000 | Loss: 0.00003636
Iteration 38/1000 | Loss: 0.00003636
Iteration 39/1000 | Loss: 0.00003636
Iteration 40/1000 | Loss: 0.00003635
Iteration 41/1000 | Loss: 0.00003635
Iteration 42/1000 | Loss: 0.00003635
Iteration 43/1000 | Loss: 0.00003635
Iteration 44/1000 | Loss: 0.00003635
Iteration 45/1000 | Loss: 0.00003635
Iteration 46/1000 | Loss: 0.00003635
Iteration 47/1000 | Loss: 0.00003635
Iteration 48/1000 | Loss: 0.00003634
Iteration 49/1000 | Loss: 0.00003634
Iteration 50/1000 | Loss: 0.00003634
Iteration 51/1000 | Loss: 0.00003634
Iteration 52/1000 | Loss: 0.00003633
Iteration 53/1000 | Loss: 0.00003633
Iteration 54/1000 | Loss: 0.00003633
Iteration 55/1000 | Loss: 0.00003633
Iteration 56/1000 | Loss: 0.00003632
Iteration 57/1000 | Loss: 0.00003632
Iteration 58/1000 | Loss: 0.00003632
Iteration 59/1000 | Loss: 0.00003632
Iteration 60/1000 | Loss: 0.00003632
Iteration 61/1000 | Loss: 0.00003631
Iteration 62/1000 | Loss: 0.00003631
Iteration 63/1000 | Loss: 0.00003631
Iteration 64/1000 | Loss: 0.00003631
Iteration 65/1000 | Loss: 0.00003631
Iteration 66/1000 | Loss: 0.00003631
Iteration 67/1000 | Loss: 0.00003631
Iteration 68/1000 | Loss: 0.00003631
Iteration 69/1000 | Loss: 0.00003631
Iteration 70/1000 | Loss: 0.00003631
Iteration 71/1000 | Loss: 0.00003631
Iteration 72/1000 | Loss: 0.00003630
Iteration 73/1000 | Loss: 0.00003630
Iteration 74/1000 | Loss: 0.00003630
Iteration 75/1000 | Loss: 0.00003630
Iteration 76/1000 | Loss: 0.00003630
Iteration 77/1000 | Loss: 0.00003630
Iteration 78/1000 | Loss: 0.00003629
Iteration 79/1000 | Loss: 0.00003629
Iteration 80/1000 | Loss: 0.00003629
Iteration 81/1000 | Loss: 0.00003628
Iteration 82/1000 | Loss: 0.00003628
Iteration 83/1000 | Loss: 0.00003627
Iteration 84/1000 | Loss: 0.00003627
Iteration 85/1000 | Loss: 0.00003627
Iteration 86/1000 | Loss: 0.00003626
Iteration 87/1000 | Loss: 0.00003626
Iteration 88/1000 | Loss: 0.00003626
Iteration 89/1000 | Loss: 0.00003625
Iteration 90/1000 | Loss: 0.00003625
Iteration 91/1000 | Loss: 0.00003625
Iteration 92/1000 | Loss: 0.00003625
Iteration 93/1000 | Loss: 0.00003624
Iteration 94/1000 | Loss: 0.00003624
Iteration 95/1000 | Loss: 0.00003624
Iteration 96/1000 | Loss: 0.00003624
Iteration 97/1000 | Loss: 0.00003624
Iteration 98/1000 | Loss: 0.00003623
Iteration 99/1000 | Loss: 0.00003623
Iteration 100/1000 | Loss: 0.00003623
Iteration 101/1000 | Loss: 0.00003623
Iteration 102/1000 | Loss: 0.00003622
Iteration 103/1000 | Loss: 0.00003622
Iteration 104/1000 | Loss: 0.00003622
Iteration 105/1000 | Loss: 0.00003622
Iteration 106/1000 | Loss: 0.00003622
Iteration 107/1000 | Loss: 0.00003622
Iteration 108/1000 | Loss: 0.00003622
Iteration 109/1000 | Loss: 0.00003622
Iteration 110/1000 | Loss: 0.00003622
Iteration 111/1000 | Loss: 0.00003622
Iteration 112/1000 | Loss: 0.00003621
Iteration 113/1000 | Loss: 0.00003621
Iteration 114/1000 | Loss: 0.00003621
Iteration 115/1000 | Loss: 0.00003621
Iteration 116/1000 | Loss: 0.00003621
Iteration 117/1000 | Loss: 0.00003621
Iteration 118/1000 | Loss: 0.00003621
Iteration 119/1000 | Loss: 0.00003621
Iteration 120/1000 | Loss: 0.00003620
Iteration 121/1000 | Loss: 0.00003620
Iteration 122/1000 | Loss: 0.00003620
Iteration 123/1000 | Loss: 0.00003620
Iteration 124/1000 | Loss: 0.00003620
Iteration 125/1000 | Loss: 0.00003620
Iteration 126/1000 | Loss: 0.00003620
Iteration 127/1000 | Loss: 0.00003620
Iteration 128/1000 | Loss: 0.00003620
Iteration 129/1000 | Loss: 0.00003619
Iteration 130/1000 | Loss: 0.00003619
Iteration 131/1000 | Loss: 0.00003619
Iteration 132/1000 | Loss: 0.00003619
Iteration 133/1000 | Loss: 0.00003619
Iteration 134/1000 | Loss: 0.00003619
Iteration 135/1000 | Loss: 0.00003619
Iteration 136/1000 | Loss: 0.00003619
Iteration 137/1000 | Loss: 0.00003619
Iteration 138/1000 | Loss: 0.00003619
Iteration 139/1000 | Loss: 0.00003619
Iteration 140/1000 | Loss: 0.00003619
Iteration 141/1000 | Loss: 0.00003618
Iteration 142/1000 | Loss: 0.00003618
Iteration 143/1000 | Loss: 0.00003618
Iteration 144/1000 | Loss: 0.00003618
Iteration 145/1000 | Loss: 0.00003618
Iteration 146/1000 | Loss: 0.00003618
Iteration 147/1000 | Loss: 0.00003618
Iteration 148/1000 | Loss: 0.00003618
Iteration 149/1000 | Loss: 0.00003618
Iteration 150/1000 | Loss: 0.00003618
Iteration 151/1000 | Loss: 0.00003617
Iteration 152/1000 | Loss: 0.00003617
Iteration 153/1000 | Loss: 0.00003617
Iteration 154/1000 | Loss: 0.00003617
Iteration 155/1000 | Loss: 0.00003617
Iteration 156/1000 | Loss: 0.00003617
Iteration 157/1000 | Loss: 0.00003617
Iteration 158/1000 | Loss: 0.00003617
Iteration 159/1000 | Loss: 0.00003617
Iteration 160/1000 | Loss: 0.00003617
Iteration 161/1000 | Loss: 0.00003616
Iteration 162/1000 | Loss: 0.00003616
Iteration 163/1000 | Loss: 0.00003616
Iteration 164/1000 | Loss: 0.00003616
Iteration 165/1000 | Loss: 0.00003616
Iteration 166/1000 | Loss: 0.00003616
Iteration 167/1000 | Loss: 0.00003616
Iteration 168/1000 | Loss: 0.00003616
Iteration 169/1000 | Loss: 0.00003616
Iteration 170/1000 | Loss: 0.00003616
Iteration 171/1000 | Loss: 0.00003616
Iteration 172/1000 | Loss: 0.00003616
Iteration 173/1000 | Loss: 0.00003616
Iteration 174/1000 | Loss: 0.00003616
Iteration 175/1000 | Loss: 0.00003616
Iteration 176/1000 | Loss: 0.00003616
Iteration 177/1000 | Loss: 0.00003616
Iteration 178/1000 | Loss: 0.00003615
Iteration 179/1000 | Loss: 0.00003615
Iteration 180/1000 | Loss: 0.00003615
Iteration 181/1000 | Loss: 0.00003615
Iteration 182/1000 | Loss: 0.00003615
Iteration 183/1000 | Loss: 0.00003615
Iteration 184/1000 | Loss: 0.00003615
Iteration 185/1000 | Loss: 0.00003615
Iteration 186/1000 | Loss: 0.00003615
Iteration 187/1000 | Loss: 0.00003615
Iteration 188/1000 | Loss: 0.00003615
Iteration 189/1000 | Loss: 0.00003615
Iteration 190/1000 | Loss: 0.00003615
Iteration 191/1000 | Loss: 0.00003615
Iteration 192/1000 | Loss: 0.00003615
Iteration 193/1000 | Loss: 0.00003615
Iteration 194/1000 | Loss: 0.00003615
Iteration 195/1000 | Loss: 0.00003615
Iteration 196/1000 | Loss: 0.00003615
Iteration 197/1000 | Loss: 0.00003615
Iteration 198/1000 | Loss: 0.00003615
Iteration 199/1000 | Loss: 0.00003615
Iteration 200/1000 | Loss: 0.00003615
Iteration 201/1000 | Loss: 0.00003615
Iteration 202/1000 | Loss: 0.00003615
Iteration 203/1000 | Loss: 0.00003615
Iteration 204/1000 | Loss: 0.00003615
Iteration 205/1000 | Loss: 0.00003615
Iteration 206/1000 | Loss: 0.00003615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.6150628147879615e-05, 3.6150628147879615e-05, 3.6150628147879615e-05, 3.6150628147879615e-05, 3.6150628147879615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6150628147879615e-05

Optimization complete. Final v2v error: 4.9987263679504395 mm

Highest mean error: 5.658562660217285 mm for frame 121

Lowest mean error: 3.900932550430298 mm for frame 17

Saving results

Total time: 42.39887762069702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769159
Iteration 2/25 | Loss: 0.00169286
Iteration 3/25 | Loss: 0.00150093
Iteration 4/25 | Loss: 0.00140874
Iteration 5/25 | Loss: 0.00138962
Iteration 6/25 | Loss: 0.00138461
Iteration 7/25 | Loss: 0.00138689
Iteration 8/25 | Loss: 0.00138363
Iteration 9/25 | Loss: 0.00138610
Iteration 10/25 | Loss: 0.00138346
Iteration 11/25 | Loss: 0.00138345
Iteration 12/25 | Loss: 0.00138345
Iteration 13/25 | Loss: 0.00138345
Iteration 14/25 | Loss: 0.00138345
Iteration 15/25 | Loss: 0.00138345
Iteration 16/25 | Loss: 0.00138345
Iteration 17/25 | Loss: 0.00138345
Iteration 18/25 | Loss: 0.00138344
Iteration 19/25 | Loss: 0.00138344
Iteration 20/25 | Loss: 0.00138344
Iteration 21/25 | Loss: 0.00138344
Iteration 22/25 | Loss: 0.00138344
Iteration 23/25 | Loss: 0.00138344
Iteration 24/25 | Loss: 0.00138344
Iteration 25/25 | Loss: 0.00138344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14249873
Iteration 2/25 | Loss: 0.00114723
Iteration 3/25 | Loss: 0.00112815
Iteration 4/25 | Loss: 0.00112815
Iteration 5/25 | Loss: 0.00112815
Iteration 6/25 | Loss: 0.00112815
Iteration 7/25 | Loss: 0.00112815
Iteration 8/25 | Loss: 0.00112815
Iteration 9/25 | Loss: 0.00112815
Iteration 10/25 | Loss: 0.00112815
Iteration 11/25 | Loss: 0.00112815
Iteration 12/25 | Loss: 0.00112815
Iteration 13/25 | Loss: 0.00112815
Iteration 14/25 | Loss: 0.00112815
Iteration 15/25 | Loss: 0.00112815
Iteration 16/25 | Loss: 0.00112815
Iteration 17/25 | Loss: 0.00112815
Iteration 18/25 | Loss: 0.00112815
Iteration 19/25 | Loss: 0.00112815
Iteration 20/25 | Loss: 0.00112815
Iteration 21/25 | Loss: 0.00112815
Iteration 22/25 | Loss: 0.00112815
Iteration 23/25 | Loss: 0.00112815
Iteration 24/25 | Loss: 0.00112815
Iteration 25/25 | Loss: 0.00112815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112815
Iteration 2/1000 | Loss: 0.00005719
Iteration 3/1000 | Loss: 0.00005408
Iteration 4/1000 | Loss: 0.00003153
Iteration 5/1000 | Loss: 0.00003000
Iteration 6/1000 | Loss: 0.00010007
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002802
Iteration 9/1000 | Loss: 0.00002748
Iteration 10/1000 | Loss: 0.00007323
Iteration 11/1000 | Loss: 0.00009004
Iteration 12/1000 | Loss: 0.00002667
Iteration 13/1000 | Loss: 0.00004030
Iteration 14/1000 | Loss: 0.00002869
Iteration 15/1000 | Loss: 0.00002757
Iteration 16/1000 | Loss: 0.00002652
Iteration 17/1000 | Loss: 0.00005166
Iteration 18/1000 | Loss: 0.00002646
Iteration 19/1000 | Loss: 0.00002616
Iteration 20/1000 | Loss: 0.00002592
Iteration 21/1000 | Loss: 0.00002570
Iteration 22/1000 | Loss: 0.00002562
Iteration 23/1000 | Loss: 0.00002555
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00006097
Iteration 26/1000 | Loss: 0.00006097
Iteration 27/1000 | Loss: 0.00002894
Iteration 28/1000 | Loss: 0.00002737
Iteration 29/1000 | Loss: 0.00004192
Iteration 30/1000 | Loss: 0.00002544
Iteration 31/1000 | Loss: 0.00002539
Iteration 32/1000 | Loss: 0.00003237
Iteration 33/1000 | Loss: 0.00002539
Iteration 34/1000 | Loss: 0.00002538
Iteration 35/1000 | Loss: 0.00002537
Iteration 36/1000 | Loss: 0.00002537
Iteration 37/1000 | Loss: 0.00002537
Iteration 38/1000 | Loss: 0.00002536
Iteration 39/1000 | Loss: 0.00002535
Iteration 40/1000 | Loss: 0.00002533
Iteration 41/1000 | Loss: 0.00002533
Iteration 42/1000 | Loss: 0.00002533
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002533
Iteration 45/1000 | Loss: 0.00002533
Iteration 46/1000 | Loss: 0.00002533
Iteration 47/1000 | Loss: 0.00002533
Iteration 48/1000 | Loss: 0.00002532
Iteration 49/1000 | Loss: 0.00002532
Iteration 50/1000 | Loss: 0.00002531
Iteration 51/1000 | Loss: 0.00003759
Iteration 52/1000 | Loss: 0.00002632
Iteration 53/1000 | Loss: 0.00002550
Iteration 54/1000 | Loss: 0.00003875
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00002528
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002528
Iteration 60/1000 | Loss: 0.00002528
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002527
Iteration 63/1000 | Loss: 0.00003009
Iteration 64/1000 | Loss: 0.00005297
Iteration 65/1000 | Loss: 0.00002534
Iteration 66/1000 | Loss: 0.00003195
Iteration 67/1000 | Loss: 0.00002528
Iteration 68/1000 | Loss: 0.00002528
Iteration 69/1000 | Loss: 0.00002528
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002528
Iteration 74/1000 | Loss: 0.00002528
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002527
Iteration 77/1000 | Loss: 0.00002527
Iteration 78/1000 | Loss: 0.00002527
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002526
Iteration 82/1000 | Loss: 0.00002526
Iteration 83/1000 | Loss: 0.00002526
Iteration 84/1000 | Loss: 0.00002526
Iteration 85/1000 | Loss: 0.00002526
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002525
Iteration 90/1000 | Loss: 0.00002525
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002525
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002525
Iteration 98/1000 | Loss: 0.00002525
Iteration 99/1000 | Loss: 0.00002525
Iteration 100/1000 | Loss: 0.00002525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.525041600165423e-05, 2.525041600165423e-05, 2.525041600165423e-05, 2.525041600165423e-05, 2.525041600165423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.525041600165423e-05

Optimization complete. Final v2v error: 4.1948699951171875 mm

Highest mean error: 4.824311256408691 mm for frame 119

Lowest mean error: 3.3279483318328857 mm for frame 193

Saving results

Total time: 80.05793786048889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990630
Iteration 2/25 | Loss: 0.00281904
Iteration 3/25 | Loss: 0.00226176
Iteration 4/25 | Loss: 0.00211284
Iteration 5/25 | Loss: 0.00193891
Iteration 6/25 | Loss: 0.00172886
Iteration 7/25 | Loss: 0.00165299
Iteration 8/25 | Loss: 0.00162817
Iteration 9/25 | Loss: 0.00154671
Iteration 10/25 | Loss: 0.00152442
Iteration 11/25 | Loss: 0.00148373
Iteration 12/25 | Loss: 0.00147528
Iteration 13/25 | Loss: 0.00147700
Iteration 14/25 | Loss: 0.00147437
Iteration 15/25 | Loss: 0.00147177
Iteration 16/25 | Loss: 0.00147162
Iteration 17/25 | Loss: 0.00147019
Iteration 18/25 | Loss: 0.00146195
Iteration 19/25 | Loss: 0.00146072
Iteration 20/25 | Loss: 0.00146041
Iteration 21/25 | Loss: 0.00146038
Iteration 22/25 | Loss: 0.00146038
Iteration 23/25 | Loss: 0.00146037
Iteration 24/25 | Loss: 0.00146037
Iteration 25/25 | Loss: 0.00146037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36293328
Iteration 2/25 | Loss: 0.00090353
Iteration 3/25 | Loss: 0.00090352
Iteration 4/25 | Loss: 0.00090352
Iteration 5/25 | Loss: 0.00090352
Iteration 6/25 | Loss: 0.00090352
Iteration 7/25 | Loss: 0.00090352
Iteration 8/25 | Loss: 0.00090352
Iteration 9/25 | Loss: 0.00090352
Iteration 10/25 | Loss: 0.00090352
Iteration 11/25 | Loss: 0.00090352
Iteration 12/25 | Loss: 0.00090352
Iteration 13/25 | Loss: 0.00090352
Iteration 14/25 | Loss: 0.00090352
Iteration 15/25 | Loss: 0.00090352
Iteration 16/25 | Loss: 0.00090352
Iteration 17/25 | Loss: 0.00090352
Iteration 18/25 | Loss: 0.00090352
Iteration 19/25 | Loss: 0.00090352
Iteration 20/25 | Loss: 0.00090352
Iteration 21/25 | Loss: 0.00090352
Iteration 22/25 | Loss: 0.00090352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009035224211402237, 0.0009035224211402237, 0.0009035224211402237, 0.0009035224211402237, 0.0009035224211402237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009035224211402237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090352
Iteration 2/1000 | Loss: 0.00005929
Iteration 3/1000 | Loss: 0.00004496
Iteration 4/1000 | Loss: 0.00004094
Iteration 5/1000 | Loss: 0.00003947
Iteration 6/1000 | Loss: 0.00003813
Iteration 7/1000 | Loss: 0.00003738
Iteration 8/1000 | Loss: 0.00012696
Iteration 9/1000 | Loss: 0.00005777
Iteration 10/1000 | Loss: 0.00003732
Iteration 11/1000 | Loss: 0.00003507
Iteration 12/1000 | Loss: 0.00003391
Iteration 13/1000 | Loss: 0.00003305
Iteration 14/1000 | Loss: 0.00003261
Iteration 15/1000 | Loss: 0.00003226
Iteration 16/1000 | Loss: 0.00003198
Iteration 17/1000 | Loss: 0.00003175
Iteration 18/1000 | Loss: 0.00003155
Iteration 19/1000 | Loss: 0.00003154
Iteration 20/1000 | Loss: 0.00003152
Iteration 21/1000 | Loss: 0.00003144
Iteration 22/1000 | Loss: 0.00003141
Iteration 23/1000 | Loss: 0.00003137
Iteration 24/1000 | Loss: 0.00003131
Iteration 25/1000 | Loss: 0.00003130
Iteration 26/1000 | Loss: 0.00003129
Iteration 27/1000 | Loss: 0.00003129
Iteration 28/1000 | Loss: 0.00003128
Iteration 29/1000 | Loss: 0.00003125
Iteration 30/1000 | Loss: 0.00003125
Iteration 31/1000 | Loss: 0.00003120
Iteration 32/1000 | Loss: 0.00003120
Iteration 33/1000 | Loss: 0.00003119
Iteration 34/1000 | Loss: 0.00003119
Iteration 35/1000 | Loss: 0.00003119
Iteration 36/1000 | Loss: 0.00003118
Iteration 37/1000 | Loss: 0.00003118
Iteration 38/1000 | Loss: 0.00003117
Iteration 39/1000 | Loss: 0.00003117
Iteration 40/1000 | Loss: 0.00003116
Iteration 41/1000 | Loss: 0.00003116
Iteration 42/1000 | Loss: 0.00003116
Iteration 43/1000 | Loss: 0.00003115
Iteration 44/1000 | Loss: 0.00003115
Iteration 45/1000 | Loss: 0.00003115
Iteration 46/1000 | Loss: 0.00003115
Iteration 47/1000 | Loss: 0.00003115
Iteration 48/1000 | Loss: 0.00003114
Iteration 49/1000 | Loss: 0.00003114
Iteration 50/1000 | Loss: 0.00003114
Iteration 51/1000 | Loss: 0.00003114
Iteration 52/1000 | Loss: 0.00003114
Iteration 53/1000 | Loss: 0.00003113
Iteration 54/1000 | Loss: 0.00003112
Iteration 55/1000 | Loss: 0.00003112
Iteration 56/1000 | Loss: 0.00003112
Iteration 57/1000 | Loss: 0.00003112
Iteration 58/1000 | Loss: 0.00003112
Iteration 59/1000 | Loss: 0.00003112
Iteration 60/1000 | Loss: 0.00003112
Iteration 61/1000 | Loss: 0.00003112
Iteration 62/1000 | Loss: 0.00003112
Iteration 63/1000 | Loss: 0.00003112
Iteration 64/1000 | Loss: 0.00003112
Iteration 65/1000 | Loss: 0.00003112
Iteration 66/1000 | Loss: 0.00003112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [3.1123719963943586e-05, 3.1123719963943586e-05, 3.1123719963943586e-05, 3.1123719963943586e-05, 3.1123719963943586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1123719963943586e-05

Optimization complete. Final v2v error: 4.766901016235352 mm

Highest mean error: 5.6247944831848145 mm for frame 238

Lowest mean error: 4.533698558807373 mm for frame 48

Saving results

Total time: 74.6271493434906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052248
Iteration 2/25 | Loss: 0.00175350
Iteration 3/25 | Loss: 0.00141142
Iteration 4/25 | Loss: 0.00137171
Iteration 5/25 | Loss: 0.00138392
Iteration 6/25 | Loss: 0.00140134
Iteration 7/25 | Loss: 0.00138387
Iteration 8/25 | Loss: 0.00136976
Iteration 9/25 | Loss: 0.00137109
Iteration 10/25 | Loss: 0.00137390
Iteration 11/25 | Loss: 0.00137781
Iteration 12/25 | Loss: 0.00137824
Iteration 13/25 | Loss: 0.00137437
Iteration 14/25 | Loss: 0.00137471
Iteration 15/25 | Loss: 0.00137598
Iteration 16/25 | Loss: 0.00136946
Iteration 17/25 | Loss: 0.00137019
Iteration 18/25 | Loss: 0.00136178
Iteration 19/25 | Loss: 0.00135214
Iteration 20/25 | Loss: 0.00134688
Iteration 21/25 | Loss: 0.00134407
Iteration 22/25 | Loss: 0.00134838
Iteration 23/25 | Loss: 0.00135053
Iteration 24/25 | Loss: 0.00134620
Iteration 25/25 | Loss: 0.00133941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44116378
Iteration 2/25 | Loss: 0.00105577
Iteration 3/25 | Loss: 0.00105576
Iteration 4/25 | Loss: 0.00105576
Iteration 5/25 | Loss: 0.00105576
Iteration 6/25 | Loss: 0.00105576
Iteration 7/25 | Loss: 0.00105576
Iteration 8/25 | Loss: 0.00105576
Iteration 9/25 | Loss: 0.00105576
Iteration 10/25 | Loss: 0.00105575
Iteration 11/25 | Loss: 0.00105575
Iteration 12/25 | Loss: 0.00105575
Iteration 13/25 | Loss: 0.00105575
Iteration 14/25 | Loss: 0.00105575
Iteration 15/25 | Loss: 0.00105575
Iteration 16/25 | Loss: 0.00105575
Iteration 17/25 | Loss: 0.00105575
Iteration 18/25 | Loss: 0.00105575
Iteration 19/25 | Loss: 0.00105575
Iteration 20/25 | Loss: 0.00105575
Iteration 21/25 | Loss: 0.00105575
Iteration 22/25 | Loss: 0.00105575
Iteration 23/25 | Loss: 0.00105575
Iteration 24/25 | Loss: 0.00105575
Iteration 25/25 | Loss: 0.00105575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105575
Iteration 2/1000 | Loss: 0.00003473
Iteration 3/1000 | Loss: 0.00091510
Iteration 4/1000 | Loss: 0.00018397
Iteration 5/1000 | Loss: 0.00048877
Iteration 6/1000 | Loss: 0.00021395
Iteration 7/1000 | Loss: 0.00026016
Iteration 8/1000 | Loss: 0.00082517
Iteration 9/1000 | Loss: 0.00130669
Iteration 10/1000 | Loss: 0.00003274
Iteration 11/1000 | Loss: 0.00027065
Iteration 12/1000 | Loss: 0.00002682
Iteration 13/1000 | Loss: 0.00002466
Iteration 14/1000 | Loss: 0.00002356
Iteration 15/1000 | Loss: 0.00002235
Iteration 16/1000 | Loss: 0.00023517
Iteration 17/1000 | Loss: 0.00013831
Iteration 18/1000 | Loss: 0.00002721
Iteration 19/1000 | Loss: 0.00002181
Iteration 20/1000 | Loss: 0.00046982
Iteration 21/1000 | Loss: 0.00019738
Iteration 22/1000 | Loss: 0.00025136
Iteration 23/1000 | Loss: 0.00010190
Iteration 24/1000 | Loss: 0.00024721
Iteration 25/1000 | Loss: 0.00011323
Iteration 26/1000 | Loss: 0.00053082
Iteration 27/1000 | Loss: 0.00015616
Iteration 28/1000 | Loss: 0.00028438
Iteration 29/1000 | Loss: 0.00022101
Iteration 30/1000 | Loss: 0.00006657
Iteration 31/1000 | Loss: 0.00023493
Iteration 32/1000 | Loss: 0.00025654
Iteration 33/1000 | Loss: 0.00023131
Iteration 34/1000 | Loss: 0.00022504
Iteration 35/1000 | Loss: 0.00028892
Iteration 36/1000 | Loss: 0.00036748
Iteration 37/1000 | Loss: 0.00035226
Iteration 38/1000 | Loss: 0.00028404
Iteration 39/1000 | Loss: 0.00024027
Iteration 40/1000 | Loss: 0.00011306
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00026530
Iteration 43/1000 | Loss: 0.00028396
Iteration 44/1000 | Loss: 0.00017309
Iteration 45/1000 | Loss: 0.00027298
Iteration 46/1000 | Loss: 0.00078616
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00023354
Iteration 51/1000 | Loss: 0.00002625
Iteration 52/1000 | Loss: 0.00002349
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002063
Iteration 56/1000 | Loss: 0.00031570
Iteration 57/1000 | Loss: 0.00011340
Iteration 58/1000 | Loss: 0.00011010
Iteration 59/1000 | Loss: 0.00027386
Iteration 60/1000 | Loss: 0.00011178
Iteration 61/1000 | Loss: 0.00010035
Iteration 62/1000 | Loss: 0.00062581
Iteration 63/1000 | Loss: 0.00029528
Iteration 64/1000 | Loss: 0.00016025
Iteration 65/1000 | Loss: 0.00044162
Iteration 66/1000 | Loss: 0.00031368
Iteration 67/1000 | Loss: 0.00020307
Iteration 68/1000 | Loss: 0.00002463
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00034127
Iteration 71/1000 | Loss: 0.00017520
Iteration 72/1000 | Loss: 0.00010923
Iteration 73/1000 | Loss: 0.00002066
Iteration 74/1000 | Loss: 0.00022631
Iteration 75/1000 | Loss: 0.00002179
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001870
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001749
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001744
Iteration 96/1000 | Loss: 0.00001744
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001743
Iteration 100/1000 | Loss: 0.00001743
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001741
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001740
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001736
Iteration 128/1000 | Loss: 0.00001735
Iteration 129/1000 | Loss: 0.00001735
Iteration 130/1000 | Loss: 0.00001735
Iteration 131/1000 | Loss: 0.00001735
Iteration 132/1000 | Loss: 0.00001735
Iteration 133/1000 | Loss: 0.00001735
Iteration 134/1000 | Loss: 0.00001735
Iteration 135/1000 | Loss: 0.00001735
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001733
Iteration 140/1000 | Loss: 0.00001733
Iteration 141/1000 | Loss: 0.00001733
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Iteration 145/1000 | Loss: 0.00001732
Iteration 146/1000 | Loss: 0.00001732
Iteration 147/1000 | Loss: 0.00001732
Iteration 148/1000 | Loss: 0.00001732
Iteration 149/1000 | Loss: 0.00001732
Iteration 150/1000 | Loss: 0.00001731
Iteration 151/1000 | Loss: 0.00001731
Iteration 152/1000 | Loss: 0.00001731
Iteration 153/1000 | Loss: 0.00001731
Iteration 154/1000 | Loss: 0.00001731
Iteration 155/1000 | Loss: 0.00001730
Iteration 156/1000 | Loss: 0.00001730
Iteration 157/1000 | Loss: 0.00001730
Iteration 158/1000 | Loss: 0.00001730
Iteration 159/1000 | Loss: 0.00001730
Iteration 160/1000 | Loss: 0.00001730
Iteration 161/1000 | Loss: 0.00001730
Iteration 162/1000 | Loss: 0.00001730
Iteration 163/1000 | Loss: 0.00001729
Iteration 164/1000 | Loss: 0.00001729
Iteration 165/1000 | Loss: 0.00001729
Iteration 166/1000 | Loss: 0.00001729
Iteration 167/1000 | Loss: 0.00001729
Iteration 168/1000 | Loss: 0.00001729
Iteration 169/1000 | Loss: 0.00001729
Iteration 170/1000 | Loss: 0.00001729
Iteration 171/1000 | Loss: 0.00001729
Iteration 172/1000 | Loss: 0.00001729
Iteration 173/1000 | Loss: 0.00001729
Iteration 174/1000 | Loss: 0.00001729
Iteration 175/1000 | Loss: 0.00001729
Iteration 176/1000 | Loss: 0.00001729
Iteration 177/1000 | Loss: 0.00001729
Iteration 178/1000 | Loss: 0.00001729
Iteration 179/1000 | Loss: 0.00001729
Iteration 180/1000 | Loss: 0.00001729
Iteration 181/1000 | Loss: 0.00001729
Iteration 182/1000 | Loss: 0.00001729
Iteration 183/1000 | Loss: 0.00001729
Iteration 184/1000 | Loss: 0.00001729
Iteration 185/1000 | Loss: 0.00001729
Iteration 186/1000 | Loss: 0.00001729
Iteration 187/1000 | Loss: 0.00001729
Iteration 188/1000 | Loss: 0.00001729
Iteration 189/1000 | Loss: 0.00001729
Iteration 190/1000 | Loss: 0.00001729
Iteration 191/1000 | Loss: 0.00001729
Iteration 192/1000 | Loss: 0.00001729
Iteration 193/1000 | Loss: 0.00001729
Iteration 194/1000 | Loss: 0.00001729
Iteration 195/1000 | Loss: 0.00001729
Iteration 196/1000 | Loss: 0.00001729
Iteration 197/1000 | Loss: 0.00001729
Iteration 198/1000 | Loss: 0.00001729
Iteration 199/1000 | Loss: 0.00001729
Iteration 200/1000 | Loss: 0.00001729
Iteration 201/1000 | Loss: 0.00001729
Iteration 202/1000 | Loss: 0.00001729
Iteration 203/1000 | Loss: 0.00001729
Iteration 204/1000 | Loss: 0.00001729
Iteration 205/1000 | Loss: 0.00001729
Iteration 206/1000 | Loss: 0.00001729
Iteration 207/1000 | Loss: 0.00001729
Iteration 208/1000 | Loss: 0.00001729
Iteration 209/1000 | Loss: 0.00001729
Iteration 210/1000 | Loss: 0.00001729
Iteration 211/1000 | Loss: 0.00001729
Iteration 212/1000 | Loss: 0.00001729
Iteration 213/1000 | Loss: 0.00001729
Iteration 214/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.729310861264821e-05, 1.729310861264821e-05, 1.729310861264821e-05, 1.729310861264821e-05, 1.729310861264821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.729310861264821e-05

Optimization complete. Final v2v error: 3.397829294204712 mm

Highest mean error: 6.905763626098633 mm for frame 10

Lowest mean error: 2.969435691833496 mm for frame 74

Saving results

Total time: 163.61942791938782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023405
Iteration 2/25 | Loss: 0.00212480
Iteration 3/25 | Loss: 0.00163291
Iteration 4/25 | Loss: 0.00160456
Iteration 5/25 | Loss: 0.00160154
Iteration 6/25 | Loss: 0.00160435
Iteration 7/25 | Loss: 0.00159330
Iteration 8/25 | Loss: 0.00158376
Iteration 9/25 | Loss: 0.00158068
Iteration 10/25 | Loss: 0.00157885
Iteration 11/25 | Loss: 0.00157843
Iteration 12/25 | Loss: 0.00157827
Iteration 13/25 | Loss: 0.00157821
Iteration 14/25 | Loss: 0.00157821
Iteration 15/25 | Loss: 0.00157820
Iteration 16/25 | Loss: 0.00157820
Iteration 17/25 | Loss: 0.00157820
Iteration 18/25 | Loss: 0.00157819
Iteration 19/25 | Loss: 0.00157819
Iteration 20/25 | Loss: 0.00157819
Iteration 21/25 | Loss: 0.00157819
Iteration 22/25 | Loss: 0.00157819
Iteration 23/25 | Loss: 0.00157819
Iteration 24/25 | Loss: 0.00157819
Iteration 25/25 | Loss: 0.00157819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67365426
Iteration 2/25 | Loss: 0.00102095
Iteration 3/25 | Loss: 0.00102092
Iteration 4/25 | Loss: 0.00102092
Iteration 5/25 | Loss: 0.00102092
Iteration 6/25 | Loss: 0.00102092
Iteration 7/25 | Loss: 0.00102092
Iteration 8/25 | Loss: 0.00102092
Iteration 9/25 | Loss: 0.00102092
Iteration 10/25 | Loss: 0.00102092
Iteration 11/25 | Loss: 0.00102092
Iteration 12/25 | Loss: 0.00102092
Iteration 13/25 | Loss: 0.00102092
Iteration 14/25 | Loss: 0.00102092
Iteration 15/25 | Loss: 0.00102092
Iteration 16/25 | Loss: 0.00102092
Iteration 17/25 | Loss: 0.00102092
Iteration 18/25 | Loss: 0.00102092
Iteration 19/25 | Loss: 0.00102092
Iteration 20/25 | Loss: 0.00102092
Iteration 21/25 | Loss: 0.00102092
Iteration 22/25 | Loss: 0.00102092
Iteration 23/25 | Loss: 0.00102092
Iteration 24/25 | Loss: 0.00102092
Iteration 25/25 | Loss: 0.00102092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010209211613982916, 0.0010209211613982916, 0.0010209211613982916, 0.0010209211613982916, 0.0010209211613982916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010209211613982916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102092
Iteration 2/1000 | Loss: 0.00014424
Iteration 3/1000 | Loss: 0.00007353
Iteration 4/1000 | Loss: 0.00005762
Iteration 5/1000 | Loss: 0.00005408
Iteration 6/1000 | Loss: 0.00005240
Iteration 7/1000 | Loss: 0.00005153
Iteration 8/1000 | Loss: 0.00005067
Iteration 9/1000 | Loss: 0.00004982
Iteration 10/1000 | Loss: 0.00004910
Iteration 11/1000 | Loss: 0.00004866
Iteration 12/1000 | Loss: 0.00004824
Iteration 13/1000 | Loss: 0.00004779
Iteration 14/1000 | Loss: 0.00004746
Iteration 15/1000 | Loss: 0.00004718
Iteration 16/1000 | Loss: 0.00004691
Iteration 17/1000 | Loss: 0.00004668
Iteration 18/1000 | Loss: 0.00004648
Iteration 19/1000 | Loss: 0.00004634
Iteration 20/1000 | Loss: 0.00004628
Iteration 21/1000 | Loss: 0.00004627
Iteration 22/1000 | Loss: 0.00004627
Iteration 23/1000 | Loss: 0.00004627
Iteration 24/1000 | Loss: 0.00004625
Iteration 25/1000 | Loss: 0.00004624
Iteration 26/1000 | Loss: 0.00004624
Iteration 27/1000 | Loss: 0.00004622
Iteration 28/1000 | Loss: 0.00004622
Iteration 29/1000 | Loss: 0.00004621
Iteration 30/1000 | Loss: 0.00004619
Iteration 31/1000 | Loss: 0.00004618
Iteration 32/1000 | Loss: 0.00004618
Iteration 33/1000 | Loss: 0.00004618
Iteration 34/1000 | Loss: 0.00004617
Iteration 35/1000 | Loss: 0.00004616
Iteration 36/1000 | Loss: 0.00004616
Iteration 37/1000 | Loss: 0.00004615
Iteration 38/1000 | Loss: 0.00004615
Iteration 39/1000 | Loss: 0.00004615
Iteration 40/1000 | Loss: 0.00004615
Iteration 41/1000 | Loss: 0.00004614
Iteration 42/1000 | Loss: 0.00004614
Iteration 43/1000 | Loss: 0.00004614
Iteration 44/1000 | Loss: 0.00004614
Iteration 45/1000 | Loss: 0.00004613
Iteration 46/1000 | Loss: 0.00004613
Iteration 47/1000 | Loss: 0.00004613
Iteration 48/1000 | Loss: 0.00004613
Iteration 49/1000 | Loss: 0.00004613
Iteration 50/1000 | Loss: 0.00004613
Iteration 51/1000 | Loss: 0.00004613
Iteration 52/1000 | Loss: 0.00004612
Iteration 53/1000 | Loss: 0.00004612
Iteration 54/1000 | Loss: 0.00004611
Iteration 55/1000 | Loss: 0.00004611
Iteration 56/1000 | Loss: 0.00004611
Iteration 57/1000 | Loss: 0.00004611
Iteration 58/1000 | Loss: 0.00004611
Iteration 59/1000 | Loss: 0.00004611
Iteration 60/1000 | Loss: 0.00004611
Iteration 61/1000 | Loss: 0.00004611
Iteration 62/1000 | Loss: 0.00004611
Iteration 63/1000 | Loss: 0.00004611
Iteration 64/1000 | Loss: 0.00004611
Iteration 65/1000 | Loss: 0.00004610
Iteration 66/1000 | Loss: 0.00004610
Iteration 67/1000 | Loss: 0.00004608
Iteration 68/1000 | Loss: 0.00004608
Iteration 69/1000 | Loss: 0.00004607
Iteration 70/1000 | Loss: 0.00004607
Iteration 71/1000 | Loss: 0.00004607
Iteration 72/1000 | Loss: 0.00004607
Iteration 73/1000 | Loss: 0.00004606
Iteration 74/1000 | Loss: 0.00004606
Iteration 75/1000 | Loss: 0.00004605
Iteration 76/1000 | Loss: 0.00004605
Iteration 77/1000 | Loss: 0.00004605
Iteration 78/1000 | Loss: 0.00004605
Iteration 79/1000 | Loss: 0.00004605
Iteration 80/1000 | Loss: 0.00004605
Iteration 81/1000 | Loss: 0.00004604
Iteration 82/1000 | Loss: 0.00004604
Iteration 83/1000 | Loss: 0.00004604
Iteration 84/1000 | Loss: 0.00004603
Iteration 85/1000 | Loss: 0.00004603
Iteration 86/1000 | Loss: 0.00004603
Iteration 87/1000 | Loss: 0.00004603
Iteration 88/1000 | Loss: 0.00004602
Iteration 89/1000 | Loss: 0.00004602
Iteration 90/1000 | Loss: 0.00004602
Iteration 91/1000 | Loss: 0.00004602
Iteration 92/1000 | Loss: 0.00004601
Iteration 93/1000 | Loss: 0.00004601
Iteration 94/1000 | Loss: 0.00004601
Iteration 95/1000 | Loss: 0.00004601
Iteration 96/1000 | Loss: 0.00004601
Iteration 97/1000 | Loss: 0.00004601
Iteration 98/1000 | Loss: 0.00004600
Iteration 99/1000 | Loss: 0.00004600
Iteration 100/1000 | Loss: 0.00004600
Iteration 101/1000 | Loss: 0.00004600
Iteration 102/1000 | Loss: 0.00004600
Iteration 103/1000 | Loss: 0.00004600
Iteration 104/1000 | Loss: 0.00004600
Iteration 105/1000 | Loss: 0.00004600
Iteration 106/1000 | Loss: 0.00004600
Iteration 107/1000 | Loss: 0.00004600
Iteration 108/1000 | Loss: 0.00004600
Iteration 109/1000 | Loss: 0.00004600
Iteration 110/1000 | Loss: 0.00004599
Iteration 111/1000 | Loss: 0.00004599
Iteration 112/1000 | Loss: 0.00004599
Iteration 113/1000 | Loss: 0.00004598
Iteration 114/1000 | Loss: 0.00004598
Iteration 115/1000 | Loss: 0.00004598
Iteration 116/1000 | Loss: 0.00004598
Iteration 117/1000 | Loss: 0.00004598
Iteration 118/1000 | Loss: 0.00004597
Iteration 119/1000 | Loss: 0.00004597
Iteration 120/1000 | Loss: 0.00004597
Iteration 121/1000 | Loss: 0.00004597
Iteration 122/1000 | Loss: 0.00004597
Iteration 123/1000 | Loss: 0.00004596
Iteration 124/1000 | Loss: 0.00004596
Iteration 125/1000 | Loss: 0.00004596
Iteration 126/1000 | Loss: 0.00004596
Iteration 127/1000 | Loss: 0.00004596
Iteration 128/1000 | Loss: 0.00004596
Iteration 129/1000 | Loss: 0.00004596
Iteration 130/1000 | Loss: 0.00004596
Iteration 131/1000 | Loss: 0.00004596
Iteration 132/1000 | Loss: 0.00004595
Iteration 133/1000 | Loss: 0.00004595
Iteration 134/1000 | Loss: 0.00004595
Iteration 135/1000 | Loss: 0.00004595
Iteration 136/1000 | Loss: 0.00004594
Iteration 137/1000 | Loss: 0.00004594
Iteration 138/1000 | Loss: 0.00004594
Iteration 139/1000 | Loss: 0.00004594
Iteration 140/1000 | Loss: 0.00004594
Iteration 141/1000 | Loss: 0.00004594
Iteration 142/1000 | Loss: 0.00004593
Iteration 143/1000 | Loss: 0.00004593
Iteration 144/1000 | Loss: 0.00004593
Iteration 145/1000 | Loss: 0.00004593
Iteration 146/1000 | Loss: 0.00004593
Iteration 147/1000 | Loss: 0.00004593
Iteration 148/1000 | Loss: 0.00004593
Iteration 149/1000 | Loss: 0.00004593
Iteration 150/1000 | Loss: 0.00004593
Iteration 151/1000 | Loss: 0.00004593
Iteration 152/1000 | Loss: 0.00004593
Iteration 153/1000 | Loss: 0.00004592
Iteration 154/1000 | Loss: 0.00004592
Iteration 155/1000 | Loss: 0.00004592
Iteration 156/1000 | Loss: 0.00004592
Iteration 157/1000 | Loss: 0.00004592
Iteration 158/1000 | Loss: 0.00004592
Iteration 159/1000 | Loss: 0.00004592
Iteration 160/1000 | Loss: 0.00004592
Iteration 161/1000 | Loss: 0.00004592
Iteration 162/1000 | Loss: 0.00004592
Iteration 163/1000 | Loss: 0.00004592
Iteration 164/1000 | Loss: 0.00004592
Iteration 165/1000 | Loss: 0.00004592
Iteration 166/1000 | Loss: 0.00004592
Iteration 167/1000 | Loss: 0.00004592
Iteration 168/1000 | Loss: 0.00004592
Iteration 169/1000 | Loss: 0.00004592
Iteration 170/1000 | Loss: 0.00004592
Iteration 171/1000 | Loss: 0.00004592
Iteration 172/1000 | Loss: 0.00004592
Iteration 173/1000 | Loss: 0.00004592
Iteration 174/1000 | Loss: 0.00004592
Iteration 175/1000 | Loss: 0.00004592
Iteration 176/1000 | Loss: 0.00004592
Iteration 177/1000 | Loss: 0.00004592
Iteration 178/1000 | Loss: 0.00004592
Iteration 179/1000 | Loss: 0.00004592
Iteration 180/1000 | Loss: 0.00004592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [4.592226832755841e-05, 4.592226832755841e-05, 4.592226832755841e-05, 4.592226832755841e-05, 4.592226832755841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.592226832755841e-05

Optimization complete. Final v2v error: 5.176641464233398 mm

Highest mean error: 5.743460178375244 mm for frame 64

Lowest mean error: 3.918196439743042 mm for frame 19

Saving results

Total time: 58.724751234054565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780153
Iteration 2/25 | Loss: 0.00158172
Iteration 3/25 | Loss: 0.00140699
Iteration 4/25 | Loss: 0.00133511
Iteration 5/25 | Loss: 0.00132978
Iteration 6/25 | Loss: 0.00130929
Iteration 7/25 | Loss: 0.00130833
Iteration 8/25 | Loss: 0.00130804
Iteration 9/25 | Loss: 0.00130784
Iteration 10/25 | Loss: 0.00130856
Iteration 11/25 | Loss: 0.00131021
Iteration 12/25 | Loss: 0.00130832
Iteration 13/25 | Loss: 0.00130719
Iteration 14/25 | Loss: 0.00130641
Iteration 15/25 | Loss: 0.00130580
Iteration 16/25 | Loss: 0.00130526
Iteration 17/25 | Loss: 0.00130510
Iteration 18/25 | Loss: 0.00130506
Iteration 19/25 | Loss: 0.00130506
Iteration 20/25 | Loss: 0.00130506
Iteration 21/25 | Loss: 0.00130506
Iteration 22/25 | Loss: 0.00130505
Iteration 23/25 | Loss: 0.00130505
Iteration 24/25 | Loss: 0.00130505
Iteration 25/25 | Loss: 0.00130505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.21484089
Iteration 2/25 | Loss: 0.00093508
Iteration 3/25 | Loss: 0.00090162
Iteration 4/25 | Loss: 0.00090162
Iteration 5/25 | Loss: 0.00090161
Iteration 6/25 | Loss: 0.00090161
Iteration 7/25 | Loss: 0.00090161
Iteration 8/25 | Loss: 0.00090161
Iteration 9/25 | Loss: 0.00090161
Iteration 10/25 | Loss: 0.00090161
Iteration 11/25 | Loss: 0.00090161
Iteration 12/25 | Loss: 0.00090161
Iteration 13/25 | Loss: 0.00090161
Iteration 14/25 | Loss: 0.00090161
Iteration 15/25 | Loss: 0.00090161
Iteration 16/25 | Loss: 0.00090161
Iteration 17/25 | Loss: 0.00090161
Iteration 18/25 | Loss: 0.00090161
Iteration 19/25 | Loss: 0.00090161
Iteration 20/25 | Loss: 0.00090161
Iteration 21/25 | Loss: 0.00090161
Iteration 22/25 | Loss: 0.00090161
Iteration 23/25 | Loss: 0.00090161
Iteration 24/25 | Loss: 0.00090161
Iteration 25/25 | Loss: 0.00090161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090161
Iteration 2/1000 | Loss: 0.00007147
Iteration 3/1000 | Loss: 0.00002386
Iteration 4/1000 | Loss: 0.00005690
Iteration 5/1000 | Loss: 0.00007765
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001971
Iteration 8/1000 | Loss: 0.00001902
Iteration 9/1000 | Loss: 0.00008016
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001802
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001780
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001776
Iteration 25/1000 | Loss: 0.00001775
Iteration 26/1000 | Loss: 0.00001775
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001773
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001770
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001767
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001760
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001760
Iteration 51/1000 | Loss: 0.00001760
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001758
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001750
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001749
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001748
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001747
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001744
Iteration 121/1000 | Loss: 0.00001744
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001743
Iteration 131/1000 | Loss: 0.00001743
Iteration 132/1000 | Loss: 0.00001743
Iteration 133/1000 | Loss: 0.00001743
Iteration 134/1000 | Loss: 0.00001743
Iteration 135/1000 | Loss: 0.00001743
Iteration 136/1000 | Loss: 0.00001743
Iteration 137/1000 | Loss: 0.00001742
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001742
Iteration 140/1000 | Loss: 0.00001742
Iteration 141/1000 | Loss: 0.00001742
Iteration 142/1000 | Loss: 0.00001741
Iteration 143/1000 | Loss: 0.00001741
Iteration 144/1000 | Loss: 0.00001741
Iteration 145/1000 | Loss: 0.00001741
Iteration 146/1000 | Loss: 0.00001741
Iteration 147/1000 | Loss: 0.00001740
Iteration 148/1000 | Loss: 0.00001740
Iteration 149/1000 | Loss: 0.00001740
Iteration 150/1000 | Loss: 0.00001740
Iteration 151/1000 | Loss: 0.00001740
Iteration 152/1000 | Loss: 0.00001740
Iteration 153/1000 | Loss: 0.00001740
Iteration 154/1000 | Loss: 0.00001740
Iteration 155/1000 | Loss: 0.00001740
Iteration 156/1000 | Loss: 0.00001740
Iteration 157/1000 | Loss: 0.00001740
Iteration 158/1000 | Loss: 0.00001740
Iteration 159/1000 | Loss: 0.00001740
Iteration 160/1000 | Loss: 0.00001740
Iteration 161/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7395868781022727e-05, 1.7395868781022727e-05, 1.7395868781022727e-05, 1.7395868781022727e-05, 1.7395868781022727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7395868781022727e-05

Optimization complete. Final v2v error: 3.512387275695801 mm

Highest mean error: 3.842216730117798 mm for frame 141

Lowest mean error: 3.1334939002990723 mm for frame 203

Saving results

Total time: 72.13416481018066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396793
Iteration 2/25 | Loss: 0.00136132
Iteration 3/25 | Loss: 0.00126082
Iteration 4/25 | Loss: 0.00124828
Iteration 5/25 | Loss: 0.00124607
Iteration 6/25 | Loss: 0.00124572
Iteration 7/25 | Loss: 0.00124572
Iteration 8/25 | Loss: 0.00124572
Iteration 9/25 | Loss: 0.00124572
Iteration 10/25 | Loss: 0.00124572
Iteration 11/25 | Loss: 0.00124572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001245721592567861, 0.001245721592567861, 0.001245721592567861, 0.001245721592567861, 0.001245721592567861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001245721592567861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41133952
Iteration 2/25 | Loss: 0.00078170
Iteration 3/25 | Loss: 0.00078169
Iteration 4/25 | Loss: 0.00078169
Iteration 5/25 | Loss: 0.00078169
Iteration 6/25 | Loss: 0.00078169
Iteration 7/25 | Loss: 0.00078169
Iteration 8/25 | Loss: 0.00078169
Iteration 9/25 | Loss: 0.00078169
Iteration 10/25 | Loss: 0.00078169
Iteration 11/25 | Loss: 0.00078169
Iteration 12/25 | Loss: 0.00078169
Iteration 13/25 | Loss: 0.00078169
Iteration 14/25 | Loss: 0.00078169
Iteration 15/25 | Loss: 0.00078169
Iteration 16/25 | Loss: 0.00078169
Iteration 17/25 | Loss: 0.00078169
Iteration 18/25 | Loss: 0.00078169
Iteration 19/25 | Loss: 0.00078169
Iteration 20/25 | Loss: 0.00078169
Iteration 21/25 | Loss: 0.00078169
Iteration 22/25 | Loss: 0.00078169
Iteration 23/25 | Loss: 0.00078169
Iteration 24/25 | Loss: 0.00078169
Iteration 25/25 | Loss: 0.00078169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078169
Iteration 2/1000 | Loss: 0.00005268
Iteration 3/1000 | Loss: 0.00003772
Iteration 4/1000 | Loss: 0.00002791
Iteration 5/1000 | Loss: 0.00002454
Iteration 6/1000 | Loss: 0.00002265
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00001992
Iteration 9/1000 | Loss: 0.00001922
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001738
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001729
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001722
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001720
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001707
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001706
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001702
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001701
Iteration 113/1000 | Loss: 0.00001701
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001700
Iteration 117/1000 | Loss: 0.00001700
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001700
Iteration 128/1000 | Loss: 0.00001700
Iteration 129/1000 | Loss: 0.00001700
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001698
Iteration 138/1000 | Loss: 0.00001698
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001698
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001697
Iteration 154/1000 | Loss: 0.00001697
Iteration 155/1000 | Loss: 0.00001697
Iteration 156/1000 | Loss: 0.00001697
Iteration 157/1000 | Loss: 0.00001697
Iteration 158/1000 | Loss: 0.00001697
Iteration 159/1000 | Loss: 0.00001697
Iteration 160/1000 | Loss: 0.00001697
Iteration 161/1000 | Loss: 0.00001697
Iteration 162/1000 | Loss: 0.00001697
Iteration 163/1000 | Loss: 0.00001697
Iteration 164/1000 | Loss: 0.00001697
Iteration 165/1000 | Loss: 0.00001696
Iteration 166/1000 | Loss: 0.00001696
Iteration 167/1000 | Loss: 0.00001696
Iteration 168/1000 | Loss: 0.00001696
Iteration 169/1000 | Loss: 0.00001696
Iteration 170/1000 | Loss: 0.00001696
Iteration 171/1000 | Loss: 0.00001696
Iteration 172/1000 | Loss: 0.00001696
Iteration 173/1000 | Loss: 0.00001696
Iteration 174/1000 | Loss: 0.00001696
Iteration 175/1000 | Loss: 0.00001695
Iteration 176/1000 | Loss: 0.00001695
Iteration 177/1000 | Loss: 0.00001695
Iteration 178/1000 | Loss: 0.00001695
Iteration 179/1000 | Loss: 0.00001695
Iteration 180/1000 | Loss: 0.00001695
Iteration 181/1000 | Loss: 0.00001694
Iteration 182/1000 | Loss: 0.00001694
Iteration 183/1000 | Loss: 0.00001694
Iteration 184/1000 | Loss: 0.00001694
Iteration 185/1000 | Loss: 0.00001694
Iteration 186/1000 | Loss: 0.00001694
Iteration 187/1000 | Loss: 0.00001694
Iteration 188/1000 | Loss: 0.00001694
Iteration 189/1000 | Loss: 0.00001694
Iteration 190/1000 | Loss: 0.00001694
Iteration 191/1000 | Loss: 0.00001694
Iteration 192/1000 | Loss: 0.00001694
Iteration 193/1000 | Loss: 0.00001694
Iteration 194/1000 | Loss: 0.00001694
Iteration 195/1000 | Loss: 0.00001694
Iteration 196/1000 | Loss: 0.00001694
Iteration 197/1000 | Loss: 0.00001694
Iteration 198/1000 | Loss: 0.00001693
Iteration 199/1000 | Loss: 0.00001693
Iteration 200/1000 | Loss: 0.00001693
Iteration 201/1000 | Loss: 0.00001693
Iteration 202/1000 | Loss: 0.00001693
Iteration 203/1000 | Loss: 0.00001693
Iteration 204/1000 | Loss: 0.00001693
Iteration 205/1000 | Loss: 0.00001693
Iteration 206/1000 | Loss: 0.00001693
Iteration 207/1000 | Loss: 0.00001693
Iteration 208/1000 | Loss: 0.00001693
Iteration 209/1000 | Loss: 0.00001693
Iteration 210/1000 | Loss: 0.00001693
Iteration 211/1000 | Loss: 0.00001693
Iteration 212/1000 | Loss: 0.00001692
Iteration 213/1000 | Loss: 0.00001692
Iteration 214/1000 | Loss: 0.00001692
Iteration 215/1000 | Loss: 0.00001692
Iteration 216/1000 | Loss: 0.00001692
Iteration 217/1000 | Loss: 0.00001692
Iteration 218/1000 | Loss: 0.00001692
Iteration 219/1000 | Loss: 0.00001692
Iteration 220/1000 | Loss: 0.00001692
Iteration 221/1000 | Loss: 0.00001692
Iteration 222/1000 | Loss: 0.00001692
Iteration 223/1000 | Loss: 0.00001692
Iteration 224/1000 | Loss: 0.00001691
Iteration 225/1000 | Loss: 0.00001691
Iteration 226/1000 | Loss: 0.00001691
Iteration 227/1000 | Loss: 0.00001691
Iteration 228/1000 | Loss: 0.00001691
Iteration 229/1000 | Loss: 0.00001691
Iteration 230/1000 | Loss: 0.00001691
Iteration 231/1000 | Loss: 0.00001691
Iteration 232/1000 | Loss: 0.00001691
Iteration 233/1000 | Loss: 0.00001691
Iteration 234/1000 | Loss: 0.00001691
Iteration 235/1000 | Loss: 0.00001691
Iteration 236/1000 | Loss: 0.00001691
Iteration 237/1000 | Loss: 0.00001691
Iteration 238/1000 | Loss: 0.00001691
Iteration 239/1000 | Loss: 0.00001691
Iteration 240/1000 | Loss: 0.00001691
Iteration 241/1000 | Loss: 0.00001691
Iteration 242/1000 | Loss: 0.00001691
Iteration 243/1000 | Loss: 0.00001691
Iteration 244/1000 | Loss: 0.00001690
Iteration 245/1000 | Loss: 0.00001690
Iteration 246/1000 | Loss: 0.00001690
Iteration 247/1000 | Loss: 0.00001690
Iteration 248/1000 | Loss: 0.00001690
Iteration 249/1000 | Loss: 0.00001690
Iteration 250/1000 | Loss: 0.00001690
Iteration 251/1000 | Loss: 0.00001690
Iteration 252/1000 | Loss: 0.00001690
Iteration 253/1000 | Loss: 0.00001690
Iteration 254/1000 | Loss: 0.00001690
Iteration 255/1000 | Loss: 0.00001690
Iteration 256/1000 | Loss: 0.00001690
Iteration 257/1000 | Loss: 0.00001690
Iteration 258/1000 | Loss: 0.00001690
Iteration 259/1000 | Loss: 0.00001690
Iteration 260/1000 | Loss: 0.00001690
Iteration 261/1000 | Loss: 0.00001690
Iteration 262/1000 | Loss: 0.00001690
Iteration 263/1000 | Loss: 0.00001690
Iteration 264/1000 | Loss: 0.00001690
Iteration 265/1000 | Loss: 0.00001690
Iteration 266/1000 | Loss: 0.00001690
Iteration 267/1000 | Loss: 0.00001690
Iteration 268/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.690418503130786e-05, 1.690418503130786e-05, 1.690418503130786e-05, 1.690418503130786e-05, 1.690418503130786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.690418503130786e-05

Optimization complete. Final v2v error: 3.476694107055664 mm

Highest mean error: 4.3421630859375 mm for frame 23

Lowest mean error: 2.9507594108581543 mm for frame 12

Saving results

Total time: 46.431578397750854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780555
Iteration 2/25 | Loss: 0.00142217
Iteration 3/25 | Loss: 0.00127636
Iteration 4/25 | Loss: 0.00125871
Iteration 5/25 | Loss: 0.00125493
Iteration 6/25 | Loss: 0.00125467
Iteration 7/25 | Loss: 0.00125467
Iteration 8/25 | Loss: 0.00125467
Iteration 9/25 | Loss: 0.00125467
Iteration 10/25 | Loss: 0.00125467
Iteration 11/25 | Loss: 0.00125467
Iteration 12/25 | Loss: 0.00125467
Iteration 13/25 | Loss: 0.00125467
Iteration 14/25 | Loss: 0.00125467
Iteration 15/25 | Loss: 0.00125467
Iteration 16/25 | Loss: 0.00125467
Iteration 17/25 | Loss: 0.00125467
Iteration 18/25 | Loss: 0.00125467
Iteration 19/25 | Loss: 0.00125467
Iteration 20/25 | Loss: 0.00125467
Iteration 21/25 | Loss: 0.00125467
Iteration 22/25 | Loss: 0.00125467
Iteration 23/25 | Loss: 0.00125467
Iteration 24/25 | Loss: 0.00125467
Iteration 25/25 | Loss: 0.00125467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42170739
Iteration 2/25 | Loss: 0.00081673
Iteration 3/25 | Loss: 0.00081673
Iteration 4/25 | Loss: 0.00081673
Iteration 5/25 | Loss: 0.00081673
Iteration 6/25 | Loss: 0.00081673
Iteration 7/25 | Loss: 0.00081673
Iteration 8/25 | Loss: 0.00081673
Iteration 9/25 | Loss: 0.00081673
Iteration 10/25 | Loss: 0.00081673
Iteration 11/25 | Loss: 0.00081673
Iteration 12/25 | Loss: 0.00081673
Iteration 13/25 | Loss: 0.00081673
Iteration 14/25 | Loss: 0.00081673
Iteration 15/25 | Loss: 0.00081673
Iteration 16/25 | Loss: 0.00081673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008167261839844286, 0.0008167261839844286, 0.0008167261839844286, 0.0008167261839844286, 0.0008167261839844286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008167261839844286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081673
Iteration 2/1000 | Loss: 0.00003001
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001383
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001323
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001290
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001286
Iteration 20/1000 | Loss: 0.00001286
Iteration 21/1000 | Loss: 0.00001284
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001283
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001274
Iteration 47/1000 | Loss: 0.00001274
Iteration 48/1000 | Loss: 0.00001274
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001273
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.272734152735211e-05, 1.272734152735211e-05, 1.272734152735211e-05, 1.272734152735211e-05, 1.272734152735211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.272734152735211e-05

Optimization complete. Final v2v error: 3.0530495643615723 mm

Highest mean error: 3.282222032546997 mm for frame 84

Lowest mean error: 2.876922607421875 mm for frame 3

Saving results

Total time: 32.31494402885437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438774
Iteration 2/25 | Loss: 0.00135262
Iteration 3/25 | Loss: 0.00129434
Iteration 4/25 | Loss: 0.00128560
Iteration 5/25 | Loss: 0.00128316
Iteration 6/25 | Loss: 0.00128316
Iteration 7/25 | Loss: 0.00128316
Iteration 8/25 | Loss: 0.00128316
Iteration 9/25 | Loss: 0.00128316
Iteration 10/25 | Loss: 0.00128316
Iteration 11/25 | Loss: 0.00128316
Iteration 12/25 | Loss: 0.00128316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012831630883738399, 0.0012831630883738399, 0.0012831630883738399, 0.0012831630883738399, 0.0012831630883738399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012831630883738399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46331084
Iteration 2/25 | Loss: 0.00076399
Iteration 3/25 | Loss: 0.00076398
Iteration 4/25 | Loss: 0.00076398
Iteration 5/25 | Loss: 0.00076398
Iteration 6/25 | Loss: 0.00076398
Iteration 7/25 | Loss: 0.00076398
Iteration 8/25 | Loss: 0.00076398
Iteration 9/25 | Loss: 0.00076398
Iteration 10/25 | Loss: 0.00076398
Iteration 11/25 | Loss: 0.00076398
Iteration 12/25 | Loss: 0.00076398
Iteration 13/25 | Loss: 0.00076398
Iteration 14/25 | Loss: 0.00076398
Iteration 15/25 | Loss: 0.00076398
Iteration 16/25 | Loss: 0.00076398
Iteration 17/25 | Loss: 0.00076398
Iteration 18/25 | Loss: 0.00076398
Iteration 19/25 | Loss: 0.00076398
Iteration 20/25 | Loss: 0.00076398
Iteration 21/25 | Loss: 0.00076398
Iteration 22/25 | Loss: 0.00076398
Iteration 23/25 | Loss: 0.00076398
Iteration 24/25 | Loss: 0.00076398
Iteration 25/25 | Loss: 0.00076398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076398
Iteration 2/1000 | Loss: 0.00003200
Iteration 3/1000 | Loss: 0.00002474
Iteration 4/1000 | Loss: 0.00002222
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00002004
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001899
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001838
Iteration 11/1000 | Loss: 0.00001819
Iteration 12/1000 | Loss: 0.00001810
Iteration 13/1000 | Loss: 0.00001809
Iteration 14/1000 | Loss: 0.00001808
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001806
Iteration 17/1000 | Loss: 0.00001803
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001729
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001728
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001727
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001717
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001717
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001716
Iteration 80/1000 | Loss: 0.00001716
Iteration 81/1000 | Loss: 0.00001716
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001711
Iteration 98/1000 | Loss: 0.00001711
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001710
Iteration 101/1000 | Loss: 0.00001710
Iteration 102/1000 | Loss: 0.00001710
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001707
Iteration 132/1000 | Loss: 0.00001707
Iteration 133/1000 | Loss: 0.00001707
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001707
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.7071231923182495e-05, 1.7071231923182495e-05, 1.7071231923182495e-05, 1.7071231923182495e-05, 1.7071231923182495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7071231923182495e-05

Optimization complete. Final v2v error: 3.5120813846588135 mm

Highest mean error: 3.847132682800293 mm for frame 70

Lowest mean error: 3.2238028049468994 mm for frame 85

Saving results

Total time: 44.97232437133789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514005
Iteration 2/25 | Loss: 0.00150898
Iteration 3/25 | Loss: 0.00137117
Iteration 4/25 | Loss: 0.00135912
Iteration 5/25 | Loss: 0.00135582
Iteration 6/25 | Loss: 0.00135582
Iteration 7/25 | Loss: 0.00135582
Iteration 8/25 | Loss: 0.00135582
Iteration 9/25 | Loss: 0.00135582
Iteration 10/25 | Loss: 0.00135582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013558224309235811, 0.0013558224309235811, 0.0013558224309235811, 0.0013558224309235811, 0.0013558224309235811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013558224309235811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43706572
Iteration 2/25 | Loss: 0.00104376
Iteration 3/25 | Loss: 0.00104374
Iteration 4/25 | Loss: 0.00104374
Iteration 5/25 | Loss: 0.00104374
Iteration 6/25 | Loss: 0.00104374
Iteration 7/25 | Loss: 0.00104374
Iteration 8/25 | Loss: 0.00104374
Iteration 9/25 | Loss: 0.00104374
Iteration 10/25 | Loss: 0.00104374
Iteration 11/25 | Loss: 0.00104374
Iteration 12/25 | Loss: 0.00104374
Iteration 13/25 | Loss: 0.00104374
Iteration 14/25 | Loss: 0.00104374
Iteration 15/25 | Loss: 0.00104374
Iteration 16/25 | Loss: 0.00104374
Iteration 17/25 | Loss: 0.00104374
Iteration 18/25 | Loss: 0.00104374
Iteration 19/25 | Loss: 0.00104374
Iteration 20/25 | Loss: 0.00104374
Iteration 21/25 | Loss: 0.00104374
Iteration 22/25 | Loss: 0.00104374
Iteration 23/25 | Loss: 0.00104374
Iteration 24/25 | Loss: 0.00104374
Iteration 25/25 | Loss: 0.00104374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104374
Iteration 2/1000 | Loss: 0.00004491
Iteration 3/1000 | Loss: 0.00002921
Iteration 4/1000 | Loss: 0.00002581
Iteration 5/1000 | Loss: 0.00002440
Iteration 6/1000 | Loss: 0.00002325
Iteration 7/1000 | Loss: 0.00002247
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002141
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002062
Iteration 12/1000 | Loss: 0.00002039
Iteration 13/1000 | Loss: 0.00002021
Iteration 14/1000 | Loss: 0.00002011
Iteration 15/1000 | Loss: 0.00001995
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001987
Iteration 20/1000 | Loss: 0.00001986
Iteration 21/1000 | Loss: 0.00001983
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001981
Iteration 26/1000 | Loss: 0.00001981
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001980
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001979
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001977
Iteration 36/1000 | Loss: 0.00001977
Iteration 37/1000 | Loss: 0.00001976
Iteration 38/1000 | Loss: 0.00001976
Iteration 39/1000 | Loss: 0.00001976
Iteration 40/1000 | Loss: 0.00001975
Iteration 41/1000 | Loss: 0.00001975
Iteration 42/1000 | Loss: 0.00001975
Iteration 43/1000 | Loss: 0.00001975
Iteration 44/1000 | Loss: 0.00001974
Iteration 45/1000 | Loss: 0.00001974
Iteration 46/1000 | Loss: 0.00001973
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001972
Iteration 50/1000 | Loss: 0.00001970
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001968
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001965
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001965
Iteration 60/1000 | Loss: 0.00001964
Iteration 61/1000 | Loss: 0.00001964
Iteration 62/1000 | Loss: 0.00001964
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00001963
Iteration 65/1000 | Loss: 0.00001963
Iteration 66/1000 | Loss: 0.00001963
Iteration 67/1000 | Loss: 0.00001963
Iteration 68/1000 | Loss: 0.00001962
Iteration 69/1000 | Loss: 0.00001962
Iteration 70/1000 | Loss: 0.00001962
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001959
Iteration 82/1000 | Loss: 0.00001959
Iteration 83/1000 | Loss: 0.00001959
Iteration 84/1000 | Loss: 0.00001959
Iteration 85/1000 | Loss: 0.00001959
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001958
Iteration 88/1000 | Loss: 0.00001958
Iteration 89/1000 | Loss: 0.00001958
Iteration 90/1000 | Loss: 0.00001958
Iteration 91/1000 | Loss: 0.00001958
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001957
Iteration 94/1000 | Loss: 0.00001957
Iteration 95/1000 | Loss: 0.00001957
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001956
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001956
Iteration 101/1000 | Loss: 0.00001956
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00001955
Iteration 104/1000 | Loss: 0.00001955
Iteration 105/1000 | Loss: 0.00001955
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001952
Iteration 117/1000 | Loss: 0.00001952
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001951
Iteration 121/1000 | Loss: 0.00001951
Iteration 122/1000 | Loss: 0.00001951
Iteration 123/1000 | Loss: 0.00001951
Iteration 124/1000 | Loss: 0.00001951
Iteration 125/1000 | Loss: 0.00001950
Iteration 126/1000 | Loss: 0.00001950
Iteration 127/1000 | Loss: 0.00001950
Iteration 128/1000 | Loss: 0.00001950
Iteration 129/1000 | Loss: 0.00001950
Iteration 130/1000 | Loss: 0.00001950
Iteration 131/1000 | Loss: 0.00001949
Iteration 132/1000 | Loss: 0.00001949
Iteration 133/1000 | Loss: 0.00001949
Iteration 134/1000 | Loss: 0.00001949
Iteration 135/1000 | Loss: 0.00001949
Iteration 136/1000 | Loss: 0.00001948
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001948
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001948
Iteration 141/1000 | Loss: 0.00001948
Iteration 142/1000 | Loss: 0.00001947
Iteration 143/1000 | Loss: 0.00001947
Iteration 144/1000 | Loss: 0.00001947
Iteration 145/1000 | Loss: 0.00001947
Iteration 146/1000 | Loss: 0.00001947
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001946
Iteration 151/1000 | Loss: 0.00001946
Iteration 152/1000 | Loss: 0.00001946
Iteration 153/1000 | Loss: 0.00001946
Iteration 154/1000 | Loss: 0.00001946
Iteration 155/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.9462171621853486e-05, 1.9462171621853486e-05, 1.9462171621853486e-05, 1.9462171621853486e-05, 1.9462171621853486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9462171621853486e-05

Optimization complete. Final v2v error: 3.6502370834350586 mm

Highest mean error: 4.392729759216309 mm for frame 209

Lowest mean error: 3.169379472732544 mm for frame 59

Saving results

Total time: 45.618860721588135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046805
Iteration 2/25 | Loss: 0.01046805
Iteration 3/25 | Loss: 0.01046805
Iteration 4/25 | Loss: 0.01046805
Iteration 5/25 | Loss: 0.01046805
Iteration 6/25 | Loss: 0.01046804
Iteration 7/25 | Loss: 0.01046804
Iteration 8/25 | Loss: 0.01046804
Iteration 9/25 | Loss: 0.01046804
Iteration 10/25 | Loss: 0.01046804
Iteration 11/25 | Loss: 0.01046804
Iteration 12/25 | Loss: 0.01046804
Iteration 13/25 | Loss: 0.01046804
Iteration 14/25 | Loss: 0.01046804
Iteration 15/25 | Loss: 0.01046804
Iteration 16/25 | Loss: 0.01046804
Iteration 17/25 | Loss: 0.01046804
Iteration 18/25 | Loss: 0.01046804
Iteration 19/25 | Loss: 0.01046804
Iteration 20/25 | Loss: 0.01046803
Iteration 21/25 | Loss: 0.01046803
Iteration 22/25 | Loss: 0.01046803
Iteration 23/25 | Loss: 0.01046803
Iteration 24/25 | Loss: 0.01046803
Iteration 25/25 | Loss: 0.01046803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66800869
Iteration 2/25 | Loss: 0.08269629
Iteration 3/25 | Loss: 0.08269506
Iteration 4/25 | Loss: 0.08269503
Iteration 5/25 | Loss: 0.08269503
Iteration 6/25 | Loss: 0.08269503
Iteration 7/25 | Loss: 0.08269503
Iteration 8/25 | Loss: 0.08269503
Iteration 9/25 | Loss: 0.08269503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.08269502967596054, 0.08269502967596054, 0.08269502967596054, 0.08269502967596054, 0.08269502967596054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08269502967596054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08269503
Iteration 2/1000 | Loss: 0.00044794
Iteration 3/1000 | Loss: 0.00011482
Iteration 4/1000 | Loss: 0.00005908
Iteration 5/1000 | Loss: 0.00003864
Iteration 6/1000 | Loss: 0.00003140
Iteration 7/1000 | Loss: 0.00002820
Iteration 8/1000 | Loss: 0.00002449
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002080
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001863
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001731
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001564
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001307
Iteration 29/1000 | Loss: 0.00001307
Iteration 30/1000 | Loss: 0.00001306
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001281
Iteration 36/1000 | Loss: 0.00001281
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001275
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001269
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001264
Iteration 57/1000 | Loss: 0.00001264
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001262
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001252
Iteration 112/1000 | Loss: 0.00001252
Iteration 113/1000 | Loss: 0.00001252
Iteration 114/1000 | Loss: 0.00001252
Iteration 115/1000 | Loss: 0.00001252
Iteration 116/1000 | Loss: 0.00001252
Iteration 117/1000 | Loss: 0.00001252
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001251
Iteration 131/1000 | Loss: 0.00001251
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001250
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001249
Iteration 153/1000 | Loss: 0.00001249
Iteration 154/1000 | Loss: 0.00001249
Iteration 155/1000 | Loss: 0.00001249
Iteration 156/1000 | Loss: 0.00001249
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001248
Iteration 160/1000 | Loss: 0.00001248
Iteration 161/1000 | Loss: 0.00001248
Iteration 162/1000 | Loss: 0.00001248
Iteration 163/1000 | Loss: 0.00001248
Iteration 164/1000 | Loss: 0.00001248
Iteration 165/1000 | Loss: 0.00001248
Iteration 166/1000 | Loss: 0.00001248
Iteration 167/1000 | Loss: 0.00001248
Iteration 168/1000 | Loss: 0.00001248
Iteration 169/1000 | Loss: 0.00001248
Iteration 170/1000 | Loss: 0.00001248
Iteration 171/1000 | Loss: 0.00001248
Iteration 172/1000 | Loss: 0.00001248
Iteration 173/1000 | Loss: 0.00001248
Iteration 174/1000 | Loss: 0.00001247
Iteration 175/1000 | Loss: 0.00001247
Iteration 176/1000 | Loss: 0.00001247
Iteration 177/1000 | Loss: 0.00001247
Iteration 178/1000 | Loss: 0.00001247
Iteration 179/1000 | Loss: 0.00001247
Iteration 180/1000 | Loss: 0.00001247
Iteration 181/1000 | Loss: 0.00001247
Iteration 182/1000 | Loss: 0.00001247
Iteration 183/1000 | Loss: 0.00001247
Iteration 184/1000 | Loss: 0.00001247
Iteration 185/1000 | Loss: 0.00001247
Iteration 186/1000 | Loss: 0.00001247
Iteration 187/1000 | Loss: 0.00001247
Iteration 188/1000 | Loss: 0.00001247
Iteration 189/1000 | Loss: 0.00001247
Iteration 190/1000 | Loss: 0.00001247
Iteration 191/1000 | Loss: 0.00001247
Iteration 192/1000 | Loss: 0.00001247
Iteration 193/1000 | Loss: 0.00001247
Iteration 194/1000 | Loss: 0.00001247
Iteration 195/1000 | Loss: 0.00001247
Iteration 196/1000 | Loss: 0.00001247
Iteration 197/1000 | Loss: 0.00001247
Iteration 198/1000 | Loss: 0.00001247
Iteration 199/1000 | Loss: 0.00001247
Iteration 200/1000 | Loss: 0.00001247
Iteration 201/1000 | Loss: 0.00001247
Iteration 202/1000 | Loss: 0.00001247
Iteration 203/1000 | Loss: 0.00001247
Iteration 204/1000 | Loss: 0.00001247
Iteration 205/1000 | Loss: 0.00001247
Iteration 206/1000 | Loss: 0.00001247
Iteration 207/1000 | Loss: 0.00001247
Iteration 208/1000 | Loss: 0.00001247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.246516239916673e-05, 1.246516239916673e-05, 1.246516239916673e-05, 1.246516239916673e-05, 1.246516239916673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.246516239916673e-05

Optimization complete. Final v2v error: 3.016418695449829 mm

Highest mean error: 3.180385112762451 mm for frame 158

Lowest mean error: 2.7533388137817383 mm for frame 188

Saving results

Total time: 55.71060299873352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976376
Iteration 2/25 | Loss: 0.00976376
Iteration 3/25 | Loss: 0.00228901
Iteration 4/25 | Loss: 0.00159450
Iteration 5/25 | Loss: 0.00149937
Iteration 6/25 | Loss: 0.00145298
Iteration 7/25 | Loss: 0.00142090
Iteration 8/25 | Loss: 0.00139940
Iteration 9/25 | Loss: 0.00139411
Iteration 10/25 | Loss: 0.00139250
Iteration 11/25 | Loss: 0.00138706
Iteration 12/25 | Loss: 0.00139594
Iteration 13/25 | Loss: 0.00137937
Iteration 14/25 | Loss: 0.00137344
Iteration 15/25 | Loss: 0.00137556
Iteration 16/25 | Loss: 0.00137711
Iteration 17/25 | Loss: 0.00136854
Iteration 18/25 | Loss: 0.00136397
Iteration 19/25 | Loss: 0.00136846
Iteration 20/25 | Loss: 0.00136550
Iteration 21/25 | Loss: 0.00136189
Iteration 22/25 | Loss: 0.00135870
Iteration 23/25 | Loss: 0.00135734
Iteration 24/25 | Loss: 0.00135698
Iteration 25/25 | Loss: 0.00135686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33787310
Iteration 2/25 | Loss: 0.00137530
Iteration 3/25 | Loss: 0.00104022
Iteration 4/25 | Loss: 0.00104019
Iteration 5/25 | Loss: 0.00104019
Iteration 6/25 | Loss: 0.00104018
Iteration 7/25 | Loss: 0.00104018
Iteration 8/25 | Loss: 0.00104018
Iteration 9/25 | Loss: 0.00104018
Iteration 10/25 | Loss: 0.00104018
Iteration 11/25 | Loss: 0.00104018
Iteration 12/25 | Loss: 0.00104018
Iteration 13/25 | Loss: 0.00104018
Iteration 14/25 | Loss: 0.00104018
Iteration 15/25 | Loss: 0.00104018
Iteration 16/25 | Loss: 0.00104018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010401837062090635, 0.0010401837062090635, 0.0010401837062090635, 0.0010401837062090635, 0.0010401837062090635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010401837062090635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104018
Iteration 2/1000 | Loss: 0.00036086
Iteration 3/1000 | Loss: 0.00077020
Iteration 4/1000 | Loss: 0.00127157
Iteration 5/1000 | Loss: 0.00011616
Iteration 6/1000 | Loss: 0.00075030
Iteration 7/1000 | Loss: 0.00036526
Iteration 8/1000 | Loss: 0.00006940
Iteration 9/1000 | Loss: 0.00006752
Iteration 10/1000 | Loss: 0.00003617
Iteration 11/1000 | Loss: 0.00007208
Iteration 12/1000 | Loss: 0.00003851
Iteration 13/1000 | Loss: 0.00003023
Iteration 14/1000 | Loss: 0.00026229
Iteration 15/1000 | Loss: 0.00002645
Iteration 16/1000 | Loss: 0.00006999
Iteration 17/1000 | Loss: 0.00018319
Iteration 18/1000 | Loss: 0.00004935
Iteration 19/1000 | Loss: 0.00021226
Iteration 20/1000 | Loss: 0.00002499
Iteration 21/1000 | Loss: 0.00002339
Iteration 22/1000 | Loss: 0.00002227
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002098
Iteration 25/1000 | Loss: 0.00002065
Iteration 26/1000 | Loss: 0.00014247
Iteration 27/1000 | Loss: 0.00002039
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001994
Iteration 32/1000 | Loss: 0.00001994
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001991
Iteration 35/1000 | Loss: 0.00001991
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001985
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001983
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001982
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001979
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001974
Iteration 53/1000 | Loss: 0.00001974
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001973
Iteration 56/1000 | Loss: 0.00001973
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001973
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001973
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001973
Iteration 67/1000 | Loss: 0.00001973
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001971
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001971
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001970
Iteration 81/1000 | Loss: 0.00001970
Iteration 82/1000 | Loss: 0.00001970
Iteration 83/1000 | Loss: 0.00001970
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001969
Iteration 88/1000 | Loss: 0.00001969
Iteration 89/1000 | Loss: 0.00001969
Iteration 90/1000 | Loss: 0.00001969
Iteration 91/1000 | Loss: 0.00001969
Iteration 92/1000 | Loss: 0.00001968
Iteration 93/1000 | Loss: 0.00001968
Iteration 94/1000 | Loss: 0.00001968
Iteration 95/1000 | Loss: 0.00001968
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001968
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001968
Iteration 104/1000 | Loss: 0.00001968
Iteration 105/1000 | Loss: 0.00001968
Iteration 106/1000 | Loss: 0.00001968
Iteration 107/1000 | Loss: 0.00001968
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001967
Iteration 115/1000 | Loss: 0.00001967
Iteration 116/1000 | Loss: 0.00001967
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001967
Iteration 119/1000 | Loss: 0.00001967
Iteration 120/1000 | Loss: 0.00001967
Iteration 121/1000 | Loss: 0.00001967
Iteration 122/1000 | Loss: 0.00001967
Iteration 123/1000 | Loss: 0.00001966
Iteration 124/1000 | Loss: 0.00001966
Iteration 125/1000 | Loss: 0.00001966
Iteration 126/1000 | Loss: 0.00001966
Iteration 127/1000 | Loss: 0.00001966
Iteration 128/1000 | Loss: 0.00001966
Iteration 129/1000 | Loss: 0.00001966
Iteration 130/1000 | Loss: 0.00001966
Iteration 131/1000 | Loss: 0.00001966
Iteration 132/1000 | Loss: 0.00001966
Iteration 133/1000 | Loss: 0.00001965
Iteration 134/1000 | Loss: 0.00001965
Iteration 135/1000 | Loss: 0.00001965
Iteration 136/1000 | Loss: 0.00001965
Iteration 137/1000 | Loss: 0.00001965
Iteration 138/1000 | Loss: 0.00001965
Iteration 139/1000 | Loss: 0.00001965
Iteration 140/1000 | Loss: 0.00001965
Iteration 141/1000 | Loss: 0.00001964
Iteration 142/1000 | Loss: 0.00001964
Iteration 143/1000 | Loss: 0.00001964
Iteration 144/1000 | Loss: 0.00001964
Iteration 145/1000 | Loss: 0.00001964
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001963
Iteration 148/1000 | Loss: 0.00001963
Iteration 149/1000 | Loss: 0.00001963
Iteration 150/1000 | Loss: 0.00001963
Iteration 151/1000 | Loss: 0.00001963
Iteration 152/1000 | Loss: 0.00001963
Iteration 153/1000 | Loss: 0.00001963
Iteration 154/1000 | Loss: 0.00001963
Iteration 155/1000 | Loss: 0.00001962
Iteration 156/1000 | Loss: 0.00001962
Iteration 157/1000 | Loss: 0.00001962
Iteration 158/1000 | Loss: 0.00001962
Iteration 159/1000 | Loss: 0.00001962
Iteration 160/1000 | Loss: 0.00001962
Iteration 161/1000 | Loss: 0.00001962
Iteration 162/1000 | Loss: 0.00001962
Iteration 163/1000 | Loss: 0.00001962
Iteration 164/1000 | Loss: 0.00001962
Iteration 165/1000 | Loss: 0.00001962
Iteration 166/1000 | Loss: 0.00001962
Iteration 167/1000 | Loss: 0.00001962
Iteration 168/1000 | Loss: 0.00001962
Iteration 169/1000 | Loss: 0.00001961
Iteration 170/1000 | Loss: 0.00001961
Iteration 171/1000 | Loss: 0.00001961
Iteration 172/1000 | Loss: 0.00001961
Iteration 173/1000 | Loss: 0.00001961
Iteration 174/1000 | Loss: 0.00001961
Iteration 175/1000 | Loss: 0.00001961
Iteration 176/1000 | Loss: 0.00001961
Iteration 177/1000 | Loss: 0.00001960
Iteration 178/1000 | Loss: 0.00001960
Iteration 179/1000 | Loss: 0.00001960
Iteration 180/1000 | Loss: 0.00001960
Iteration 181/1000 | Loss: 0.00001960
Iteration 182/1000 | Loss: 0.00001960
Iteration 183/1000 | Loss: 0.00001960
Iteration 184/1000 | Loss: 0.00001960
Iteration 185/1000 | Loss: 0.00001960
Iteration 186/1000 | Loss: 0.00001960
Iteration 187/1000 | Loss: 0.00001960
Iteration 188/1000 | Loss: 0.00001960
Iteration 189/1000 | Loss: 0.00001960
Iteration 190/1000 | Loss: 0.00001960
Iteration 191/1000 | Loss: 0.00001960
Iteration 192/1000 | Loss: 0.00001960
Iteration 193/1000 | Loss: 0.00001960
Iteration 194/1000 | Loss: 0.00001960
Iteration 195/1000 | Loss: 0.00001960
Iteration 196/1000 | Loss: 0.00001960
Iteration 197/1000 | Loss: 0.00001960
Iteration 198/1000 | Loss: 0.00001960
Iteration 199/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.959844485099893e-05, 1.959844485099893e-05, 1.959844485099893e-05, 1.959844485099893e-05, 1.959844485099893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.959844485099893e-05

Optimization complete. Final v2v error: 3.7173776626586914 mm

Highest mean error: 4.301562786102295 mm for frame 134

Lowest mean error: 3.1978566646575928 mm for frame 17

Saving results

Total time: 112.13718271255493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521966
Iteration 2/25 | Loss: 0.00137274
Iteration 3/25 | Loss: 0.00130012
Iteration 4/25 | Loss: 0.00129082
Iteration 5/25 | Loss: 0.00128943
Iteration 6/25 | Loss: 0.00128943
Iteration 7/25 | Loss: 0.00128943
Iteration 8/25 | Loss: 0.00128943
Iteration 9/25 | Loss: 0.00128943
Iteration 10/25 | Loss: 0.00128943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001289432984776795, 0.001289432984776795, 0.001289432984776795, 0.001289432984776795, 0.001289432984776795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001289432984776795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83620238
Iteration 2/25 | Loss: 0.00083636
Iteration 3/25 | Loss: 0.00083635
Iteration 4/25 | Loss: 0.00083635
Iteration 5/25 | Loss: 0.00083635
Iteration 6/25 | Loss: 0.00083635
Iteration 7/25 | Loss: 0.00083635
Iteration 8/25 | Loss: 0.00083635
Iteration 9/25 | Loss: 0.00083635
Iteration 10/25 | Loss: 0.00083635
Iteration 11/25 | Loss: 0.00083635
Iteration 12/25 | Loss: 0.00083635
Iteration 13/25 | Loss: 0.00083635
Iteration 14/25 | Loss: 0.00083635
Iteration 15/25 | Loss: 0.00083635
Iteration 16/25 | Loss: 0.00083635
Iteration 17/25 | Loss: 0.00083635
Iteration 18/25 | Loss: 0.00083635
Iteration 19/25 | Loss: 0.00083635
Iteration 20/25 | Loss: 0.00083635
Iteration 21/25 | Loss: 0.00083635
Iteration 22/25 | Loss: 0.00083635
Iteration 23/25 | Loss: 0.00083635
Iteration 24/25 | Loss: 0.00083635
Iteration 25/25 | Loss: 0.00083635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083635
Iteration 2/1000 | Loss: 0.00003419
Iteration 3/1000 | Loss: 0.00002347
Iteration 4/1000 | Loss: 0.00002137
Iteration 5/1000 | Loss: 0.00001983
Iteration 6/1000 | Loss: 0.00001875
Iteration 7/1000 | Loss: 0.00001811
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001638
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001633
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001620
Iteration 21/1000 | Loss: 0.00001619
Iteration 22/1000 | Loss: 0.00001619
Iteration 23/1000 | Loss: 0.00001619
Iteration 24/1000 | Loss: 0.00001619
Iteration 25/1000 | Loss: 0.00001618
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001615
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001609
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001609
Iteration 59/1000 | Loss: 0.00001609
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001607
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001606
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001605
Iteration 90/1000 | Loss: 0.00001605
Iteration 91/1000 | Loss: 0.00001605
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001604
Iteration 95/1000 | Loss: 0.00001604
Iteration 96/1000 | Loss: 0.00001604
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001603
Iteration 102/1000 | Loss: 0.00001603
Iteration 103/1000 | Loss: 0.00001603
Iteration 104/1000 | Loss: 0.00001603
Iteration 105/1000 | Loss: 0.00001603
Iteration 106/1000 | Loss: 0.00001603
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001602
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001601
Iteration 114/1000 | Loss: 0.00001601
Iteration 115/1000 | Loss: 0.00001601
Iteration 116/1000 | Loss: 0.00001600
Iteration 117/1000 | Loss: 0.00001600
Iteration 118/1000 | Loss: 0.00001600
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001599
Iteration 124/1000 | Loss: 0.00001599
Iteration 125/1000 | Loss: 0.00001599
Iteration 126/1000 | Loss: 0.00001599
Iteration 127/1000 | Loss: 0.00001599
Iteration 128/1000 | Loss: 0.00001599
Iteration 129/1000 | Loss: 0.00001599
Iteration 130/1000 | Loss: 0.00001599
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001596
Iteration 139/1000 | Loss: 0.00001596
Iteration 140/1000 | Loss: 0.00001596
Iteration 141/1000 | Loss: 0.00001596
Iteration 142/1000 | Loss: 0.00001595
Iteration 143/1000 | Loss: 0.00001595
Iteration 144/1000 | Loss: 0.00001595
Iteration 145/1000 | Loss: 0.00001594
Iteration 146/1000 | Loss: 0.00001594
Iteration 147/1000 | Loss: 0.00001594
Iteration 148/1000 | Loss: 0.00001594
Iteration 149/1000 | Loss: 0.00001594
Iteration 150/1000 | Loss: 0.00001594
Iteration 151/1000 | Loss: 0.00001594
Iteration 152/1000 | Loss: 0.00001593
Iteration 153/1000 | Loss: 0.00001593
Iteration 154/1000 | Loss: 0.00001593
Iteration 155/1000 | Loss: 0.00001593
Iteration 156/1000 | Loss: 0.00001593
Iteration 157/1000 | Loss: 0.00001593
Iteration 158/1000 | Loss: 0.00001593
Iteration 159/1000 | Loss: 0.00001593
Iteration 160/1000 | Loss: 0.00001593
Iteration 161/1000 | Loss: 0.00001593
Iteration 162/1000 | Loss: 0.00001593
Iteration 163/1000 | Loss: 0.00001593
Iteration 164/1000 | Loss: 0.00001593
Iteration 165/1000 | Loss: 0.00001593
Iteration 166/1000 | Loss: 0.00001593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.5934221664792858e-05, 1.5934221664792858e-05, 1.5934221664792858e-05, 1.5934221664792858e-05, 1.5934221664792858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5934221664792858e-05

Optimization complete. Final v2v error: 3.3773584365844727 mm

Highest mean error: 3.6872947216033936 mm for frame 125

Lowest mean error: 3.146669626235962 mm for frame 255

Saving results

Total time: 42.48120474815369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870547
Iteration 2/25 | Loss: 0.00146108
Iteration 3/25 | Loss: 0.00130990
Iteration 4/25 | Loss: 0.00129656
Iteration 5/25 | Loss: 0.00129267
Iteration 6/25 | Loss: 0.00129207
Iteration 7/25 | Loss: 0.00129207
Iteration 8/25 | Loss: 0.00129207
Iteration 9/25 | Loss: 0.00129207
Iteration 10/25 | Loss: 0.00129207
Iteration 11/25 | Loss: 0.00129207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00129206501878798, 0.00129206501878798, 0.00129206501878798, 0.00129206501878798, 0.00129206501878798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00129206501878798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50578582
Iteration 2/25 | Loss: 0.00088661
Iteration 3/25 | Loss: 0.00088661
Iteration 4/25 | Loss: 0.00088661
Iteration 5/25 | Loss: 0.00088661
Iteration 6/25 | Loss: 0.00088661
Iteration 7/25 | Loss: 0.00088661
Iteration 8/25 | Loss: 0.00088661
Iteration 9/25 | Loss: 0.00088661
Iteration 10/25 | Loss: 0.00088661
Iteration 11/25 | Loss: 0.00088661
Iteration 12/25 | Loss: 0.00088661
Iteration 13/25 | Loss: 0.00088661
Iteration 14/25 | Loss: 0.00088661
Iteration 15/25 | Loss: 0.00088661
Iteration 16/25 | Loss: 0.00088661
Iteration 17/25 | Loss: 0.00088661
Iteration 18/25 | Loss: 0.00088661
Iteration 19/25 | Loss: 0.00088661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008866104180924594, 0.0008866104180924594, 0.0008866104180924594, 0.0008866104180924594, 0.0008866104180924594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008866104180924594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088661
Iteration 2/1000 | Loss: 0.00003635
Iteration 3/1000 | Loss: 0.00002517
Iteration 4/1000 | Loss: 0.00002243
Iteration 5/1000 | Loss: 0.00002121
Iteration 6/1000 | Loss: 0.00002056
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001953
Iteration 9/1000 | Loss: 0.00001923
Iteration 10/1000 | Loss: 0.00001890
Iteration 11/1000 | Loss: 0.00001863
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001827
Iteration 14/1000 | Loss: 0.00001820
Iteration 15/1000 | Loss: 0.00001815
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001813
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001812
Iteration 22/1000 | Loss: 0.00001811
Iteration 23/1000 | Loss: 0.00001810
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001805
Iteration 26/1000 | Loss: 0.00001804
Iteration 27/1000 | Loss: 0.00001803
Iteration 28/1000 | Loss: 0.00001803
Iteration 29/1000 | Loss: 0.00001802
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001800
Iteration 33/1000 | Loss: 0.00001800
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001798
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001797
Iteration 40/1000 | Loss: 0.00001797
Iteration 41/1000 | Loss: 0.00001795
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00001794
Iteration 44/1000 | Loss: 0.00001791
Iteration 45/1000 | Loss: 0.00001791
Iteration 46/1000 | Loss: 0.00001789
Iteration 47/1000 | Loss: 0.00001788
Iteration 48/1000 | Loss: 0.00001788
Iteration 49/1000 | Loss: 0.00001786
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001785
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001770
Iteration 111/1000 | Loss: 0.00001770
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001769
Iteration 120/1000 | Loss: 0.00001769
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001769
Iteration 126/1000 | Loss: 0.00001769
Iteration 127/1000 | Loss: 0.00001769
Iteration 128/1000 | Loss: 0.00001769
Iteration 129/1000 | Loss: 0.00001769
Iteration 130/1000 | Loss: 0.00001769
Iteration 131/1000 | Loss: 0.00001769
Iteration 132/1000 | Loss: 0.00001769
Iteration 133/1000 | Loss: 0.00001769
Iteration 134/1000 | Loss: 0.00001769
Iteration 135/1000 | Loss: 0.00001769
Iteration 136/1000 | Loss: 0.00001769
Iteration 137/1000 | Loss: 0.00001768
Iteration 138/1000 | Loss: 0.00001768
Iteration 139/1000 | Loss: 0.00001768
Iteration 140/1000 | Loss: 0.00001768
Iteration 141/1000 | Loss: 0.00001768
Iteration 142/1000 | Loss: 0.00001768
Iteration 143/1000 | Loss: 0.00001768
Iteration 144/1000 | Loss: 0.00001768
Iteration 145/1000 | Loss: 0.00001768
Iteration 146/1000 | Loss: 0.00001768
Iteration 147/1000 | Loss: 0.00001768
Iteration 148/1000 | Loss: 0.00001767
Iteration 149/1000 | Loss: 0.00001767
Iteration 150/1000 | Loss: 0.00001767
Iteration 151/1000 | Loss: 0.00001767
Iteration 152/1000 | Loss: 0.00001767
Iteration 153/1000 | Loss: 0.00001767
Iteration 154/1000 | Loss: 0.00001767
Iteration 155/1000 | Loss: 0.00001767
Iteration 156/1000 | Loss: 0.00001767
Iteration 157/1000 | Loss: 0.00001767
Iteration 158/1000 | Loss: 0.00001767
Iteration 159/1000 | Loss: 0.00001767
Iteration 160/1000 | Loss: 0.00001767
Iteration 161/1000 | Loss: 0.00001767
Iteration 162/1000 | Loss: 0.00001767
Iteration 163/1000 | Loss: 0.00001767
Iteration 164/1000 | Loss: 0.00001767
Iteration 165/1000 | Loss: 0.00001767
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001766
Iteration 168/1000 | Loss: 0.00001766
Iteration 169/1000 | Loss: 0.00001766
Iteration 170/1000 | Loss: 0.00001766
Iteration 171/1000 | Loss: 0.00001766
Iteration 172/1000 | Loss: 0.00001766
Iteration 173/1000 | Loss: 0.00001766
Iteration 174/1000 | Loss: 0.00001766
Iteration 175/1000 | Loss: 0.00001766
Iteration 176/1000 | Loss: 0.00001766
Iteration 177/1000 | Loss: 0.00001766
Iteration 178/1000 | Loss: 0.00001766
Iteration 179/1000 | Loss: 0.00001766
Iteration 180/1000 | Loss: 0.00001766
Iteration 181/1000 | Loss: 0.00001766
Iteration 182/1000 | Loss: 0.00001766
Iteration 183/1000 | Loss: 0.00001766
Iteration 184/1000 | Loss: 0.00001766
Iteration 185/1000 | Loss: 0.00001766
Iteration 186/1000 | Loss: 0.00001766
Iteration 187/1000 | Loss: 0.00001766
Iteration 188/1000 | Loss: 0.00001766
Iteration 189/1000 | Loss: 0.00001766
Iteration 190/1000 | Loss: 0.00001766
Iteration 191/1000 | Loss: 0.00001766
Iteration 192/1000 | Loss: 0.00001766
Iteration 193/1000 | Loss: 0.00001766
Iteration 194/1000 | Loss: 0.00001766
Iteration 195/1000 | Loss: 0.00001766
Iteration 196/1000 | Loss: 0.00001766
Iteration 197/1000 | Loss: 0.00001766
Iteration 198/1000 | Loss: 0.00001766
Iteration 199/1000 | Loss: 0.00001766
Iteration 200/1000 | Loss: 0.00001766
Iteration 201/1000 | Loss: 0.00001766
Iteration 202/1000 | Loss: 0.00001766
Iteration 203/1000 | Loss: 0.00001766
Iteration 204/1000 | Loss: 0.00001766
Iteration 205/1000 | Loss: 0.00001766
Iteration 206/1000 | Loss: 0.00001766
Iteration 207/1000 | Loss: 0.00001766
Iteration 208/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.7658319848123938e-05, 1.7658319848123938e-05, 1.7658319848123938e-05, 1.7658319848123938e-05, 1.7658319848123938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7658319848123938e-05

Optimization complete. Final v2v error: 3.4695019721984863 mm

Highest mean error: 5.0368547439575195 mm for frame 55

Lowest mean error: 3.006282329559326 mm for frame 6

Saving results

Total time: 39.772223711013794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957157
Iteration 2/25 | Loss: 0.00239983
Iteration 3/25 | Loss: 0.00186572
Iteration 4/25 | Loss: 0.00184241
Iteration 5/25 | Loss: 0.00182633
Iteration 6/25 | Loss: 0.00185119
Iteration 7/25 | Loss: 0.00162361
Iteration 8/25 | Loss: 0.00158902
Iteration 9/25 | Loss: 0.00152772
Iteration 10/25 | Loss: 0.00144291
Iteration 11/25 | Loss: 0.00145395
Iteration 12/25 | Loss: 0.00145465
Iteration 13/25 | Loss: 0.00145292
Iteration 14/25 | Loss: 0.00146136
Iteration 15/25 | Loss: 0.00146713
Iteration 16/25 | Loss: 0.00146699
Iteration 17/25 | Loss: 0.00142934
Iteration 18/25 | Loss: 0.00145586
Iteration 19/25 | Loss: 0.00145822
Iteration 20/25 | Loss: 0.00144296
Iteration 21/25 | Loss: 0.00144003
Iteration 22/25 | Loss: 0.00142591
Iteration 23/25 | Loss: 0.00142943
Iteration 24/25 | Loss: 0.00142859
Iteration 25/25 | Loss: 0.00142886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45832169
Iteration 2/25 | Loss: 0.00441434
Iteration 3/25 | Loss: 0.00264736
Iteration 4/25 | Loss: 0.00509519
Iteration 5/25 | Loss: 0.00311898
Iteration 6/25 | Loss: 0.00113592
Iteration 7/25 | Loss: 0.00113592
Iteration 8/25 | Loss: 0.00113591
Iteration 9/25 | Loss: 0.00113591
Iteration 10/25 | Loss: 0.00113591
Iteration 11/25 | Loss: 0.00113591
Iteration 12/25 | Loss: 0.00113591
Iteration 13/25 | Loss: 0.00113591
Iteration 14/25 | Loss: 0.00113591
Iteration 15/25 | Loss: 0.00113591
Iteration 16/25 | Loss: 0.00113591
Iteration 17/25 | Loss: 0.00113591
Iteration 18/25 | Loss: 0.00113591
Iteration 19/25 | Loss: 0.00113591
Iteration 20/25 | Loss: 0.00113591
Iteration 21/25 | Loss: 0.00113591
Iteration 22/25 | Loss: 0.00113591
Iteration 23/25 | Loss: 0.00113591
Iteration 24/25 | Loss: 0.00113591
Iteration 25/25 | Loss: 0.00113591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113591
Iteration 2/1000 | Loss: 0.00044635
Iteration 3/1000 | Loss: 0.00039909
Iteration 4/1000 | Loss: 0.00051594
Iteration 5/1000 | Loss: 0.00098404
Iteration 6/1000 | Loss: 0.00055443
Iteration 7/1000 | Loss: 0.00054247
Iteration 8/1000 | Loss: 0.00050008
Iteration 9/1000 | Loss: 0.00049246
Iteration 10/1000 | Loss: 0.00043902
Iteration 11/1000 | Loss: 0.00034803
Iteration 12/1000 | Loss: 0.00036227
Iteration 13/1000 | Loss: 0.00032870
Iteration 14/1000 | Loss: 0.00038236
Iteration 15/1000 | Loss: 0.00087324
Iteration 16/1000 | Loss: 0.00015912
Iteration 17/1000 | Loss: 0.00082204
Iteration 18/1000 | Loss: 0.00030455
Iteration 19/1000 | Loss: 0.00022910
Iteration 20/1000 | Loss: 0.00056309
Iteration 21/1000 | Loss: 0.00033319
Iteration 22/1000 | Loss: 0.00013378
Iteration 23/1000 | Loss: 0.00010828
Iteration 24/1000 | Loss: 0.00004884
Iteration 25/1000 | Loss: 0.00058150
Iteration 26/1000 | Loss: 0.00034053
Iteration 27/1000 | Loss: 0.00028404
Iteration 28/1000 | Loss: 0.00053195
Iteration 29/1000 | Loss: 0.00023194
Iteration 30/1000 | Loss: 0.00023673
Iteration 31/1000 | Loss: 0.00063002
Iteration 32/1000 | Loss: 0.00005971
Iteration 33/1000 | Loss: 0.00043425
Iteration 34/1000 | Loss: 0.00116209
Iteration 35/1000 | Loss: 0.00026505
Iteration 36/1000 | Loss: 0.00012235
Iteration 37/1000 | Loss: 0.00047187
Iteration 38/1000 | Loss: 0.00053569
Iteration 39/1000 | Loss: 0.00013842
Iteration 40/1000 | Loss: 0.00012497
Iteration 41/1000 | Loss: 0.00011651
Iteration 42/1000 | Loss: 0.00016632
Iteration 43/1000 | Loss: 0.00005336
Iteration 44/1000 | Loss: 0.00004558
Iteration 45/1000 | Loss: 0.00004207
Iteration 46/1000 | Loss: 0.00003942
Iteration 47/1000 | Loss: 0.00003867
Iteration 48/1000 | Loss: 0.00003500
Iteration 49/1000 | Loss: 0.00003493
Iteration 50/1000 | Loss: 0.00006159
Iteration 51/1000 | Loss: 0.00003146
Iteration 52/1000 | Loss: 0.00050738
Iteration 53/1000 | Loss: 0.00003496
Iteration 54/1000 | Loss: 0.00002907
Iteration 55/1000 | Loss: 0.00002703
Iteration 56/1000 | Loss: 0.00002808
Iteration 57/1000 | Loss: 0.00002803
Iteration 58/1000 | Loss: 0.00023888
Iteration 59/1000 | Loss: 0.00003908
Iteration 60/1000 | Loss: 0.00003026
Iteration 61/1000 | Loss: 0.00002640
Iteration 62/1000 | Loss: 0.00002579
Iteration 63/1000 | Loss: 0.00002526
Iteration 64/1000 | Loss: 0.00002579
Iteration 65/1000 | Loss: 0.00002452
Iteration 66/1000 | Loss: 0.00010226
Iteration 67/1000 | Loss: 0.00002676
Iteration 68/1000 | Loss: 0.00002291
Iteration 69/1000 | Loss: 0.00002223
Iteration 70/1000 | Loss: 0.00002192
Iteration 71/1000 | Loss: 0.00002157
Iteration 72/1000 | Loss: 0.00002121
Iteration 73/1000 | Loss: 0.00002192
Iteration 74/1000 | Loss: 0.00002191
Iteration 75/1000 | Loss: 0.00002162
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002062
Iteration 78/1000 | Loss: 0.00002039
Iteration 79/1000 | Loss: 0.00002019
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002005
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001976
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001969
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001964
Iteration 88/1000 | Loss: 0.00001964
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001959
Iteration 97/1000 | Loss: 0.00001958
Iteration 98/1000 | Loss: 0.00001956
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001955
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001953
Iteration 104/1000 | Loss: 0.00001952
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00001952
Iteration 107/1000 | Loss: 0.00001952
Iteration 108/1000 | Loss: 0.00001952
Iteration 109/1000 | Loss: 0.00001952
Iteration 110/1000 | Loss: 0.00001952
Iteration 111/1000 | Loss: 0.00001952
Iteration 112/1000 | Loss: 0.00001952
Iteration 113/1000 | Loss: 0.00001952
Iteration 114/1000 | Loss: 0.00001951
Iteration 115/1000 | Loss: 0.00001950
Iteration 116/1000 | Loss: 0.00001950
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001949
Iteration 119/1000 | Loss: 0.00001948
Iteration 120/1000 | Loss: 0.00001948
Iteration 121/1000 | Loss: 0.00001947
Iteration 122/1000 | Loss: 0.00001947
Iteration 123/1000 | Loss: 0.00001946
Iteration 124/1000 | Loss: 0.00001946
Iteration 125/1000 | Loss: 0.00001946
Iteration 126/1000 | Loss: 0.00001946
Iteration 127/1000 | Loss: 0.00001945
Iteration 128/1000 | Loss: 0.00001945
Iteration 129/1000 | Loss: 0.00001945
Iteration 130/1000 | Loss: 0.00001945
Iteration 131/1000 | Loss: 0.00001944
Iteration 132/1000 | Loss: 0.00001944
Iteration 133/1000 | Loss: 0.00001944
Iteration 134/1000 | Loss: 0.00001944
Iteration 135/1000 | Loss: 0.00001944
Iteration 136/1000 | Loss: 0.00001944
Iteration 137/1000 | Loss: 0.00001944
Iteration 138/1000 | Loss: 0.00001943
Iteration 139/1000 | Loss: 0.00001943
Iteration 140/1000 | Loss: 0.00001943
Iteration 141/1000 | Loss: 0.00001943
Iteration 142/1000 | Loss: 0.00001943
Iteration 143/1000 | Loss: 0.00001942
Iteration 144/1000 | Loss: 0.00001942
Iteration 145/1000 | Loss: 0.00001942
Iteration 146/1000 | Loss: 0.00001942
Iteration 147/1000 | Loss: 0.00001942
Iteration 148/1000 | Loss: 0.00001942
Iteration 149/1000 | Loss: 0.00001942
Iteration 150/1000 | Loss: 0.00001942
Iteration 151/1000 | Loss: 0.00001942
Iteration 152/1000 | Loss: 0.00001942
Iteration 153/1000 | Loss: 0.00001942
Iteration 154/1000 | Loss: 0.00001941
Iteration 155/1000 | Loss: 0.00001941
Iteration 156/1000 | Loss: 0.00001941
Iteration 157/1000 | Loss: 0.00001941
Iteration 158/1000 | Loss: 0.00001941
Iteration 159/1000 | Loss: 0.00001941
Iteration 160/1000 | Loss: 0.00001941
Iteration 161/1000 | Loss: 0.00001941
Iteration 162/1000 | Loss: 0.00001941
Iteration 163/1000 | Loss: 0.00001941
Iteration 164/1000 | Loss: 0.00001941
Iteration 165/1000 | Loss: 0.00001941
Iteration 166/1000 | Loss: 0.00001941
Iteration 167/1000 | Loss: 0.00001940
Iteration 168/1000 | Loss: 0.00001940
Iteration 169/1000 | Loss: 0.00001940
Iteration 170/1000 | Loss: 0.00001940
Iteration 171/1000 | Loss: 0.00001940
Iteration 172/1000 | Loss: 0.00001940
Iteration 173/1000 | Loss: 0.00001940
Iteration 174/1000 | Loss: 0.00001940
Iteration 175/1000 | Loss: 0.00001940
Iteration 176/1000 | Loss: 0.00001940
Iteration 177/1000 | Loss: 0.00001940
Iteration 178/1000 | Loss: 0.00001940
Iteration 179/1000 | Loss: 0.00001940
Iteration 180/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.9403172700549476e-05, 1.9403172700549476e-05, 1.9403172700549476e-05, 1.9403172700549476e-05, 1.9403172700549476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9403172700549476e-05

Optimization complete. Final v2v error: 3.7042181491851807 mm

Highest mean error: 5.565011024475098 mm for frame 96

Lowest mean error: 3.327988386154175 mm for frame 73

Saving results

Total time: 170.93945789337158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767332
Iteration 2/25 | Loss: 0.00202608
Iteration 3/25 | Loss: 0.00149800
Iteration 4/25 | Loss: 0.00145825
Iteration 5/25 | Loss: 0.00147001
Iteration 6/25 | Loss: 0.00145957
Iteration 7/25 | Loss: 0.00138558
Iteration 8/25 | Loss: 0.00137965
Iteration 9/25 | Loss: 0.00137375
Iteration 10/25 | Loss: 0.00137656
Iteration 11/25 | Loss: 0.00137208
Iteration 12/25 | Loss: 0.00137138
Iteration 13/25 | Loss: 0.00137126
Iteration 14/25 | Loss: 0.00137126
Iteration 15/25 | Loss: 0.00137125
Iteration 16/25 | Loss: 0.00137125
Iteration 17/25 | Loss: 0.00137125
Iteration 18/25 | Loss: 0.00137125
Iteration 19/25 | Loss: 0.00137124
Iteration 20/25 | Loss: 0.00137124
Iteration 21/25 | Loss: 0.00137124
Iteration 22/25 | Loss: 0.00137124
Iteration 23/25 | Loss: 0.00137124
Iteration 24/25 | Loss: 0.00137124
Iteration 25/25 | Loss: 0.00137124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60458803
Iteration 2/25 | Loss: 0.00076431
Iteration 3/25 | Loss: 0.00076425
Iteration 4/25 | Loss: 0.00076425
Iteration 5/25 | Loss: 0.00076425
Iteration 6/25 | Loss: 0.00076425
Iteration 7/25 | Loss: 0.00076425
Iteration 8/25 | Loss: 0.00076425
Iteration 9/25 | Loss: 0.00076425
Iteration 10/25 | Loss: 0.00076425
Iteration 11/25 | Loss: 0.00076425
Iteration 12/25 | Loss: 0.00076425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007642493001185358, 0.0007642493001185358, 0.0007642493001185358, 0.0007642493001185358, 0.0007642493001185358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007642493001185358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076425
Iteration 2/1000 | Loss: 0.00003321
Iteration 3/1000 | Loss: 0.00002700
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00002402
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002274
Iteration 8/1000 | Loss: 0.00011807
Iteration 9/1000 | Loss: 0.00002223
Iteration 10/1000 | Loss: 0.00002217
Iteration 11/1000 | Loss: 0.00002212
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002205
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002200
Iteration 17/1000 | Loss: 0.00002186
Iteration 18/1000 | Loss: 0.00002185
Iteration 19/1000 | Loss: 0.00002179
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002174
Iteration 22/1000 | Loss: 0.00002174
Iteration 23/1000 | Loss: 0.00002173
Iteration 24/1000 | Loss: 0.00002173
Iteration 25/1000 | Loss: 0.00002173
Iteration 26/1000 | Loss: 0.00002169
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002167
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002165
Iteration 32/1000 | Loss: 0.00002165
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002164
Iteration 37/1000 | Loss: 0.00002164
Iteration 38/1000 | Loss: 0.00002164
Iteration 39/1000 | Loss: 0.00002164
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00002164
Iteration 43/1000 | Loss: 0.00012549
Iteration 44/1000 | Loss: 0.00003307
Iteration 45/1000 | Loss: 0.00002301
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002161
Iteration 48/1000 | Loss: 0.00002157
Iteration 49/1000 | Loss: 0.00002156
Iteration 50/1000 | Loss: 0.00002156
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002156
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002156
Iteration 59/1000 | Loss: 0.00002156
Iteration 60/1000 | Loss: 0.00002156
Iteration 61/1000 | Loss: 0.00002155
Iteration 62/1000 | Loss: 0.00002155
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002154
Iteration 65/1000 | Loss: 0.00002154
Iteration 66/1000 | Loss: 0.00002154
Iteration 67/1000 | Loss: 0.00002154
Iteration 68/1000 | Loss: 0.00002154
Iteration 69/1000 | Loss: 0.00002153
Iteration 70/1000 | Loss: 0.00002153
Iteration 71/1000 | Loss: 0.00002153
Iteration 72/1000 | Loss: 0.00002153
Iteration 73/1000 | Loss: 0.00002153
Iteration 74/1000 | Loss: 0.00002153
Iteration 75/1000 | Loss: 0.00002152
Iteration 76/1000 | Loss: 0.00002152
Iteration 77/1000 | Loss: 0.00002152
Iteration 78/1000 | Loss: 0.00002151
Iteration 79/1000 | Loss: 0.00002151
Iteration 80/1000 | Loss: 0.00002151
Iteration 81/1000 | Loss: 0.00002151
Iteration 82/1000 | Loss: 0.00002151
Iteration 83/1000 | Loss: 0.00002150
Iteration 84/1000 | Loss: 0.00002150
Iteration 85/1000 | Loss: 0.00002150
Iteration 86/1000 | Loss: 0.00002150
Iteration 87/1000 | Loss: 0.00002149
Iteration 88/1000 | Loss: 0.00002149
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002148
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002148
Iteration 96/1000 | Loss: 0.00002148
Iteration 97/1000 | Loss: 0.00002148
Iteration 98/1000 | Loss: 0.00002148
Iteration 99/1000 | Loss: 0.00002147
Iteration 100/1000 | Loss: 0.00002147
Iteration 101/1000 | Loss: 0.00002146
Iteration 102/1000 | Loss: 0.00002146
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002146
Iteration 105/1000 | Loss: 0.00002146
Iteration 106/1000 | Loss: 0.00002146
Iteration 107/1000 | Loss: 0.00002146
Iteration 108/1000 | Loss: 0.00002146
Iteration 109/1000 | Loss: 0.00002146
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002145
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Iteration 116/1000 | Loss: 0.00002144
Iteration 117/1000 | Loss: 0.00002144
Iteration 118/1000 | Loss: 0.00002144
Iteration 119/1000 | Loss: 0.00002144
Iteration 120/1000 | Loss: 0.00002144
Iteration 121/1000 | Loss: 0.00002143
Iteration 122/1000 | Loss: 0.00002143
Iteration 123/1000 | Loss: 0.00002143
Iteration 124/1000 | Loss: 0.00002143
Iteration 125/1000 | Loss: 0.00002143
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002143
Iteration 135/1000 | Loss: 0.00002143
Iteration 136/1000 | Loss: 0.00002143
Iteration 137/1000 | Loss: 0.00002142
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002141
Iteration 145/1000 | Loss: 0.00002141
Iteration 146/1000 | Loss: 0.00002141
Iteration 147/1000 | Loss: 0.00002141
Iteration 148/1000 | Loss: 0.00002141
Iteration 149/1000 | Loss: 0.00002140
Iteration 150/1000 | Loss: 0.00002140
Iteration 151/1000 | Loss: 0.00002140
Iteration 152/1000 | Loss: 0.00002140
Iteration 153/1000 | Loss: 0.00002139
Iteration 154/1000 | Loss: 0.00002139
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002139
Iteration 157/1000 | Loss: 0.00002139
Iteration 158/1000 | Loss: 0.00002139
Iteration 159/1000 | Loss: 0.00002139
Iteration 160/1000 | Loss: 0.00002138
Iteration 161/1000 | Loss: 0.00013696
Iteration 162/1000 | Loss: 0.00002429
Iteration 163/1000 | Loss: 0.00002160
Iteration 164/1000 | Loss: 0.00003384
Iteration 165/1000 | Loss: 0.00002151
Iteration 166/1000 | Loss: 0.00002146
Iteration 167/1000 | Loss: 0.00002146
Iteration 168/1000 | Loss: 0.00002146
Iteration 169/1000 | Loss: 0.00002142
Iteration 170/1000 | Loss: 0.00002139
Iteration 171/1000 | Loss: 0.00002137
Iteration 172/1000 | Loss: 0.00002136
Iteration 173/1000 | Loss: 0.00002135
Iteration 174/1000 | Loss: 0.00002135
Iteration 175/1000 | Loss: 0.00002135
Iteration 176/1000 | Loss: 0.00002134
Iteration 177/1000 | Loss: 0.00002134
Iteration 178/1000 | Loss: 0.00002134
Iteration 179/1000 | Loss: 0.00002134
Iteration 180/1000 | Loss: 0.00002134
Iteration 181/1000 | Loss: 0.00002134
Iteration 182/1000 | Loss: 0.00002134
Iteration 183/1000 | Loss: 0.00002134
Iteration 184/1000 | Loss: 0.00002134
Iteration 185/1000 | Loss: 0.00002134
Iteration 186/1000 | Loss: 0.00002134
Iteration 187/1000 | Loss: 0.00002133
Iteration 188/1000 | Loss: 0.00002133
Iteration 189/1000 | Loss: 0.00002133
Iteration 190/1000 | Loss: 0.00002133
Iteration 191/1000 | Loss: 0.00002133
Iteration 192/1000 | Loss: 0.00002133
Iteration 193/1000 | Loss: 0.00002133
Iteration 194/1000 | Loss: 0.00002133
Iteration 195/1000 | Loss: 0.00002133
Iteration 196/1000 | Loss: 0.00002133
Iteration 197/1000 | Loss: 0.00002133
Iteration 198/1000 | Loss: 0.00002133
Iteration 199/1000 | Loss: 0.00002133
Iteration 200/1000 | Loss: 0.00002133
Iteration 201/1000 | Loss: 0.00002133
Iteration 202/1000 | Loss: 0.00002133
Iteration 203/1000 | Loss: 0.00002133
Iteration 204/1000 | Loss: 0.00002133
Iteration 205/1000 | Loss: 0.00002133
Iteration 206/1000 | Loss: 0.00002133
Iteration 207/1000 | Loss: 0.00002133
Iteration 208/1000 | Loss: 0.00002133
Iteration 209/1000 | Loss: 0.00002133
Iteration 210/1000 | Loss: 0.00002133
Iteration 211/1000 | Loss: 0.00002133
Iteration 212/1000 | Loss: 0.00002133
Iteration 213/1000 | Loss: 0.00002133
Iteration 214/1000 | Loss: 0.00002133
Iteration 215/1000 | Loss: 0.00002133
Iteration 216/1000 | Loss: 0.00002133
Iteration 217/1000 | Loss: 0.00002133
Iteration 218/1000 | Loss: 0.00002133
Iteration 219/1000 | Loss: 0.00002133
Iteration 220/1000 | Loss: 0.00002133
Iteration 221/1000 | Loss: 0.00002133
Iteration 222/1000 | Loss: 0.00002133
Iteration 223/1000 | Loss: 0.00002133
Iteration 224/1000 | Loss: 0.00002133
Iteration 225/1000 | Loss: 0.00002133
Iteration 226/1000 | Loss: 0.00002133
Iteration 227/1000 | Loss: 0.00002133
Iteration 228/1000 | Loss: 0.00002133
Iteration 229/1000 | Loss: 0.00002133
Iteration 230/1000 | Loss: 0.00002133
Iteration 231/1000 | Loss: 0.00002133
Iteration 232/1000 | Loss: 0.00002133
Iteration 233/1000 | Loss: 0.00002133
Iteration 234/1000 | Loss: 0.00002133
Iteration 235/1000 | Loss: 0.00002133
Iteration 236/1000 | Loss: 0.00002133
Iteration 237/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.1334324628696777e-05, 2.1334324628696777e-05, 2.1334324628696777e-05, 2.1334324628696777e-05, 2.1334324628696777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1334324628696777e-05

Optimization complete. Final v2v error: 3.8287951946258545 mm

Highest mean error: 4.385309219360352 mm for frame 8

Lowest mean error: 3.5488662719726562 mm for frame 156

Saving results

Total time: 74.13851714134216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403288
Iteration 2/25 | Loss: 0.00131484
Iteration 3/25 | Loss: 0.00124687
Iteration 4/25 | Loss: 0.00123735
Iteration 5/25 | Loss: 0.00123423
Iteration 6/25 | Loss: 0.00123410
Iteration 7/25 | Loss: 0.00123410
Iteration 8/25 | Loss: 0.00123410
Iteration 9/25 | Loss: 0.00123410
Iteration 10/25 | Loss: 0.00123410
Iteration 11/25 | Loss: 0.00123410
Iteration 12/25 | Loss: 0.00123410
Iteration 13/25 | Loss: 0.00123410
Iteration 14/25 | Loss: 0.00123410
Iteration 15/25 | Loss: 0.00123410
Iteration 16/25 | Loss: 0.00123410
Iteration 17/25 | Loss: 0.00123410
Iteration 18/25 | Loss: 0.00123410
Iteration 19/25 | Loss: 0.00123410
Iteration 20/25 | Loss: 0.00123410
Iteration 21/25 | Loss: 0.00123410
Iteration 22/25 | Loss: 0.00123410
Iteration 23/25 | Loss: 0.00123410
Iteration 24/25 | Loss: 0.00123410
Iteration 25/25 | Loss: 0.00123410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67035723
Iteration 2/25 | Loss: 0.00082784
Iteration 3/25 | Loss: 0.00082783
Iteration 4/25 | Loss: 0.00082783
Iteration 5/25 | Loss: 0.00082783
Iteration 6/25 | Loss: 0.00082783
Iteration 7/25 | Loss: 0.00082783
Iteration 8/25 | Loss: 0.00082783
Iteration 9/25 | Loss: 0.00082783
Iteration 10/25 | Loss: 0.00082783
Iteration 11/25 | Loss: 0.00082783
Iteration 12/25 | Loss: 0.00082783
Iteration 13/25 | Loss: 0.00082783
Iteration 14/25 | Loss: 0.00082783
Iteration 15/25 | Loss: 0.00082783
Iteration 16/25 | Loss: 0.00082783
Iteration 17/25 | Loss: 0.00082783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008278300520032644, 0.0008278300520032644, 0.0008278300520032644, 0.0008278300520032644, 0.0008278300520032644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008278300520032644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082783
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001423
Iteration 7/1000 | Loss: 0.00001379
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001328
Iteration 10/1000 | Loss: 0.00001309
Iteration 11/1000 | Loss: 0.00001298
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001282
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001229
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001226
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001220
Iteration 113/1000 | Loss: 0.00001220
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001219
Iteration 139/1000 | Loss: 0.00001219
Iteration 140/1000 | Loss: 0.00001219
Iteration 141/1000 | Loss: 0.00001219
Iteration 142/1000 | Loss: 0.00001219
Iteration 143/1000 | Loss: 0.00001219
Iteration 144/1000 | Loss: 0.00001219
Iteration 145/1000 | Loss: 0.00001219
Iteration 146/1000 | Loss: 0.00001219
Iteration 147/1000 | Loss: 0.00001219
Iteration 148/1000 | Loss: 0.00001219
Iteration 149/1000 | Loss: 0.00001219
Iteration 150/1000 | Loss: 0.00001219
Iteration 151/1000 | Loss: 0.00001219
Iteration 152/1000 | Loss: 0.00001219
Iteration 153/1000 | Loss: 0.00001219
Iteration 154/1000 | Loss: 0.00001219
Iteration 155/1000 | Loss: 0.00001219
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001219
Iteration 161/1000 | Loss: 0.00001219
Iteration 162/1000 | Loss: 0.00001219
Iteration 163/1000 | Loss: 0.00001219
Iteration 164/1000 | Loss: 0.00001219
Iteration 165/1000 | Loss: 0.00001219
Iteration 166/1000 | Loss: 0.00001219
Iteration 167/1000 | Loss: 0.00001219
Iteration 168/1000 | Loss: 0.00001219
Iteration 169/1000 | Loss: 0.00001219
Iteration 170/1000 | Loss: 0.00001219
Iteration 171/1000 | Loss: 0.00001219
Iteration 172/1000 | Loss: 0.00001219
Iteration 173/1000 | Loss: 0.00001219
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Iteration 181/1000 | Loss: 0.00001219
Iteration 182/1000 | Loss: 0.00001219
Iteration 183/1000 | Loss: 0.00001219
Iteration 184/1000 | Loss: 0.00001219
Iteration 185/1000 | Loss: 0.00001219
Iteration 186/1000 | Loss: 0.00001219
Iteration 187/1000 | Loss: 0.00001219
Iteration 188/1000 | Loss: 0.00001219
Iteration 189/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.219323712575715e-05, 1.219323712575715e-05, 1.219323712575715e-05, 1.219323712575715e-05, 1.219323712575715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.219323712575715e-05

Optimization complete. Final v2v error: 2.988844394683838 mm

Highest mean error: 3.348616600036621 mm for frame 135

Lowest mean error: 2.7991316318511963 mm for frame 166

Saving results

Total time: 38.71170210838318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584572
Iteration 2/25 | Loss: 0.00160037
Iteration 3/25 | Loss: 0.00136894
Iteration 4/25 | Loss: 0.00134044
Iteration 5/25 | Loss: 0.00133414
Iteration 6/25 | Loss: 0.00133200
Iteration 7/25 | Loss: 0.00133142
Iteration 8/25 | Loss: 0.00133112
Iteration 9/25 | Loss: 0.00133095
Iteration 10/25 | Loss: 0.00133082
Iteration 11/25 | Loss: 0.00133066
Iteration 12/25 | Loss: 0.00133030
Iteration 13/25 | Loss: 0.00132991
Iteration 14/25 | Loss: 0.00132970
Iteration 15/25 | Loss: 0.00132961
Iteration 16/25 | Loss: 0.00132961
Iteration 17/25 | Loss: 0.00132961
Iteration 18/25 | Loss: 0.00132960
Iteration 19/25 | Loss: 0.00132960
Iteration 20/25 | Loss: 0.00132960
Iteration 21/25 | Loss: 0.00132960
Iteration 22/25 | Loss: 0.00132960
Iteration 23/25 | Loss: 0.00132960
Iteration 24/25 | Loss: 0.00132960
Iteration 25/25 | Loss: 0.00132960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10138988
Iteration 2/25 | Loss: 0.00092221
Iteration 3/25 | Loss: 0.00092218
Iteration 4/25 | Loss: 0.00092218
Iteration 5/25 | Loss: 0.00092218
Iteration 6/25 | Loss: 0.00092218
Iteration 7/25 | Loss: 0.00092218
Iteration 8/25 | Loss: 0.00092218
Iteration 9/25 | Loss: 0.00092218
Iteration 10/25 | Loss: 0.00092218
Iteration 11/25 | Loss: 0.00092218
Iteration 12/25 | Loss: 0.00092217
Iteration 13/25 | Loss: 0.00092217
Iteration 14/25 | Loss: 0.00092217
Iteration 15/25 | Loss: 0.00092217
Iteration 16/25 | Loss: 0.00092217
Iteration 17/25 | Loss: 0.00092217
Iteration 18/25 | Loss: 0.00092217
Iteration 19/25 | Loss: 0.00092217
Iteration 20/25 | Loss: 0.00092217
Iteration 21/25 | Loss: 0.00092217
Iteration 22/25 | Loss: 0.00092217
Iteration 23/25 | Loss: 0.00092217
Iteration 24/25 | Loss: 0.00092217
Iteration 25/25 | Loss: 0.00092217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092217
Iteration 2/1000 | Loss: 0.00004136
Iteration 3/1000 | Loss: 0.00002689
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001789
Iteration 9/1000 | Loss: 0.00001753
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001646
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001636
Iteration 37/1000 | Loss: 0.00001636
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001634
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001633
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001632
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001621
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001620
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001620
Iteration 119/1000 | Loss: 0.00001620
Iteration 120/1000 | Loss: 0.00001620
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00001620
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.620173679839354e-05, 1.620173679839354e-05, 1.620173679839354e-05, 1.620173679839354e-05, 1.620173679839354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.620173679839354e-05

Optimization complete. Final v2v error: 3.391065835952759 mm

Highest mean error: 5.552740097045898 mm for frame 92

Lowest mean error: 3.0694737434387207 mm for frame 0

Saving results

Total time: 52.35782837867737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404189
Iteration 2/25 | Loss: 0.00132144
Iteration 3/25 | Loss: 0.00124848
Iteration 4/25 | Loss: 0.00124043
Iteration 5/25 | Loss: 0.00123797
Iteration 6/25 | Loss: 0.00123781
Iteration 7/25 | Loss: 0.00123781
Iteration 8/25 | Loss: 0.00123781
Iteration 9/25 | Loss: 0.00123781
Iteration 10/25 | Loss: 0.00123781
Iteration 11/25 | Loss: 0.00123781
Iteration 12/25 | Loss: 0.00123781
Iteration 13/25 | Loss: 0.00123781
Iteration 14/25 | Loss: 0.00123781
Iteration 15/25 | Loss: 0.00123781
Iteration 16/25 | Loss: 0.00123781
Iteration 17/25 | Loss: 0.00123781
Iteration 18/25 | Loss: 0.00123781
Iteration 19/25 | Loss: 0.00123781
Iteration 20/25 | Loss: 0.00123781
Iteration 21/25 | Loss: 0.00123781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012378089595586061, 0.0012378089595586061, 0.0012378089595586061, 0.0012378089595586061, 0.0012378089595586061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012378089595586061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43183327
Iteration 2/25 | Loss: 0.00087652
Iteration 3/25 | Loss: 0.00087651
Iteration 4/25 | Loss: 0.00087651
Iteration 5/25 | Loss: 0.00087651
Iteration 6/25 | Loss: 0.00087651
Iteration 7/25 | Loss: 0.00087651
Iteration 8/25 | Loss: 0.00087651
Iteration 9/25 | Loss: 0.00087651
Iteration 10/25 | Loss: 0.00087651
Iteration 11/25 | Loss: 0.00087651
Iteration 12/25 | Loss: 0.00087651
Iteration 13/25 | Loss: 0.00087651
Iteration 14/25 | Loss: 0.00087651
Iteration 15/25 | Loss: 0.00087651
Iteration 16/25 | Loss: 0.00087651
Iteration 17/25 | Loss: 0.00087651
Iteration 18/25 | Loss: 0.00087651
Iteration 19/25 | Loss: 0.00087651
Iteration 20/25 | Loss: 0.00087651
Iteration 21/25 | Loss: 0.00087651
Iteration 22/25 | Loss: 0.00087651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008765091770328581, 0.0008765091770328581, 0.0008765091770328581, 0.0008765091770328581, 0.0008765091770328581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008765091770328581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087651
Iteration 2/1000 | Loss: 0.00002719
Iteration 3/1000 | Loss: 0.00001708
Iteration 4/1000 | Loss: 0.00001497
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001241
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001168
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001143
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001139
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001138
Iteration 49/1000 | Loss: 0.00001138
Iteration 50/1000 | Loss: 0.00001138
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001137
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001136
Iteration 55/1000 | Loss: 0.00001135
Iteration 56/1000 | Loss: 0.00001135
Iteration 57/1000 | Loss: 0.00001134
Iteration 58/1000 | Loss: 0.00001134
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001133
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001132
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001127
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001126
Iteration 79/1000 | Loss: 0.00001125
Iteration 80/1000 | Loss: 0.00001125
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001125
Iteration 83/1000 | Loss: 0.00001125
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001122
Iteration 97/1000 | Loss: 0.00001122
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001121
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001120
Iteration 102/1000 | Loss: 0.00001120
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001116
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001116
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001114
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001113
Iteration 138/1000 | Loss: 0.00001113
Iteration 139/1000 | Loss: 0.00001113
Iteration 140/1000 | Loss: 0.00001113
Iteration 141/1000 | Loss: 0.00001112
Iteration 142/1000 | Loss: 0.00001112
Iteration 143/1000 | Loss: 0.00001112
Iteration 144/1000 | Loss: 0.00001112
Iteration 145/1000 | Loss: 0.00001111
Iteration 146/1000 | Loss: 0.00001111
Iteration 147/1000 | Loss: 0.00001111
Iteration 148/1000 | Loss: 0.00001111
Iteration 149/1000 | Loss: 0.00001110
Iteration 150/1000 | Loss: 0.00001110
Iteration 151/1000 | Loss: 0.00001110
Iteration 152/1000 | Loss: 0.00001110
Iteration 153/1000 | Loss: 0.00001110
Iteration 154/1000 | Loss: 0.00001110
Iteration 155/1000 | Loss: 0.00001110
Iteration 156/1000 | Loss: 0.00001110
Iteration 157/1000 | Loss: 0.00001110
Iteration 158/1000 | Loss: 0.00001109
Iteration 159/1000 | Loss: 0.00001109
Iteration 160/1000 | Loss: 0.00001109
Iteration 161/1000 | Loss: 0.00001109
Iteration 162/1000 | Loss: 0.00001109
Iteration 163/1000 | Loss: 0.00001109
Iteration 164/1000 | Loss: 0.00001109
Iteration 165/1000 | Loss: 0.00001109
Iteration 166/1000 | Loss: 0.00001109
Iteration 167/1000 | Loss: 0.00001109
Iteration 168/1000 | Loss: 0.00001109
Iteration 169/1000 | Loss: 0.00001109
Iteration 170/1000 | Loss: 0.00001109
Iteration 171/1000 | Loss: 0.00001109
Iteration 172/1000 | Loss: 0.00001109
Iteration 173/1000 | Loss: 0.00001109
Iteration 174/1000 | Loss: 0.00001109
Iteration 175/1000 | Loss: 0.00001109
Iteration 176/1000 | Loss: 0.00001108
Iteration 177/1000 | Loss: 0.00001108
Iteration 178/1000 | Loss: 0.00001108
Iteration 179/1000 | Loss: 0.00001108
Iteration 180/1000 | Loss: 0.00001108
Iteration 181/1000 | Loss: 0.00001108
Iteration 182/1000 | Loss: 0.00001108
Iteration 183/1000 | Loss: 0.00001108
Iteration 184/1000 | Loss: 0.00001108
Iteration 185/1000 | Loss: 0.00001108
Iteration 186/1000 | Loss: 0.00001108
Iteration 187/1000 | Loss: 0.00001108
Iteration 188/1000 | Loss: 0.00001107
Iteration 189/1000 | Loss: 0.00001107
Iteration 190/1000 | Loss: 0.00001107
Iteration 191/1000 | Loss: 0.00001107
Iteration 192/1000 | Loss: 0.00001107
Iteration 193/1000 | Loss: 0.00001107
Iteration 194/1000 | Loss: 0.00001107
Iteration 195/1000 | Loss: 0.00001107
Iteration 196/1000 | Loss: 0.00001107
Iteration 197/1000 | Loss: 0.00001107
Iteration 198/1000 | Loss: 0.00001107
Iteration 199/1000 | Loss: 0.00001107
Iteration 200/1000 | Loss: 0.00001107
Iteration 201/1000 | Loss: 0.00001107
Iteration 202/1000 | Loss: 0.00001107
Iteration 203/1000 | Loss: 0.00001107
Iteration 204/1000 | Loss: 0.00001107
Iteration 205/1000 | Loss: 0.00001107
Iteration 206/1000 | Loss: 0.00001107
Iteration 207/1000 | Loss: 0.00001107
Iteration 208/1000 | Loss: 0.00001107
Iteration 209/1000 | Loss: 0.00001106
Iteration 210/1000 | Loss: 0.00001106
Iteration 211/1000 | Loss: 0.00001106
Iteration 212/1000 | Loss: 0.00001106
Iteration 213/1000 | Loss: 0.00001106
Iteration 214/1000 | Loss: 0.00001106
Iteration 215/1000 | Loss: 0.00001106
Iteration 216/1000 | Loss: 0.00001106
Iteration 217/1000 | Loss: 0.00001106
Iteration 218/1000 | Loss: 0.00001105
Iteration 219/1000 | Loss: 0.00001105
Iteration 220/1000 | Loss: 0.00001105
Iteration 221/1000 | Loss: 0.00001105
Iteration 222/1000 | Loss: 0.00001105
Iteration 223/1000 | Loss: 0.00001105
Iteration 224/1000 | Loss: 0.00001105
Iteration 225/1000 | Loss: 0.00001105
Iteration 226/1000 | Loss: 0.00001104
Iteration 227/1000 | Loss: 0.00001104
Iteration 228/1000 | Loss: 0.00001104
Iteration 229/1000 | Loss: 0.00001104
Iteration 230/1000 | Loss: 0.00001104
Iteration 231/1000 | Loss: 0.00001104
Iteration 232/1000 | Loss: 0.00001103
Iteration 233/1000 | Loss: 0.00001103
Iteration 234/1000 | Loss: 0.00001103
Iteration 235/1000 | Loss: 0.00001103
Iteration 236/1000 | Loss: 0.00001103
Iteration 237/1000 | Loss: 0.00001103
Iteration 238/1000 | Loss: 0.00001103
Iteration 239/1000 | Loss: 0.00001102
Iteration 240/1000 | Loss: 0.00001102
Iteration 241/1000 | Loss: 0.00001102
Iteration 242/1000 | Loss: 0.00001102
Iteration 243/1000 | Loss: 0.00001102
Iteration 244/1000 | Loss: 0.00001102
Iteration 245/1000 | Loss: 0.00001102
Iteration 246/1000 | Loss: 0.00001101
Iteration 247/1000 | Loss: 0.00001101
Iteration 248/1000 | Loss: 0.00001101
Iteration 249/1000 | Loss: 0.00001101
Iteration 250/1000 | Loss: 0.00001101
Iteration 251/1000 | Loss: 0.00001101
Iteration 252/1000 | Loss: 0.00001101
Iteration 253/1000 | Loss: 0.00001101
Iteration 254/1000 | Loss: 0.00001101
Iteration 255/1000 | Loss: 0.00001101
Iteration 256/1000 | Loss: 0.00001101
Iteration 257/1000 | Loss: 0.00001101
Iteration 258/1000 | Loss: 0.00001101
Iteration 259/1000 | Loss: 0.00001101
Iteration 260/1000 | Loss: 0.00001101
Iteration 261/1000 | Loss: 0.00001101
Iteration 262/1000 | Loss: 0.00001101
Iteration 263/1000 | Loss: 0.00001101
Iteration 264/1000 | Loss: 0.00001101
Iteration 265/1000 | Loss: 0.00001101
Iteration 266/1000 | Loss: 0.00001100
Iteration 267/1000 | Loss: 0.00001100
Iteration 268/1000 | Loss: 0.00001100
Iteration 269/1000 | Loss: 0.00001100
Iteration 270/1000 | Loss: 0.00001100
Iteration 271/1000 | Loss: 0.00001100
Iteration 272/1000 | Loss: 0.00001100
Iteration 273/1000 | Loss: 0.00001100
Iteration 274/1000 | Loss: 0.00001100
Iteration 275/1000 | Loss: 0.00001100
Iteration 276/1000 | Loss: 0.00001100
Iteration 277/1000 | Loss: 0.00001100
Iteration 278/1000 | Loss: 0.00001100
Iteration 279/1000 | Loss: 0.00001100
Iteration 280/1000 | Loss: 0.00001100
Iteration 281/1000 | Loss: 0.00001100
Iteration 282/1000 | Loss: 0.00001100
Iteration 283/1000 | Loss: 0.00001100
Iteration 284/1000 | Loss: 0.00001100
Iteration 285/1000 | Loss: 0.00001100
Iteration 286/1000 | Loss: 0.00001100
Iteration 287/1000 | Loss: 0.00001100
Iteration 288/1000 | Loss: 0.00001099
Iteration 289/1000 | Loss: 0.00001099
Iteration 290/1000 | Loss: 0.00001099
Iteration 291/1000 | Loss: 0.00001099
Iteration 292/1000 | Loss: 0.00001099
Iteration 293/1000 | Loss: 0.00001099
Iteration 294/1000 | Loss: 0.00001099
Iteration 295/1000 | Loss: 0.00001099
Iteration 296/1000 | Loss: 0.00001099
Iteration 297/1000 | Loss: 0.00001099
Iteration 298/1000 | Loss: 0.00001099
Iteration 299/1000 | Loss: 0.00001099
Iteration 300/1000 | Loss: 0.00001099
Iteration 301/1000 | Loss: 0.00001099
Iteration 302/1000 | Loss: 0.00001099
Iteration 303/1000 | Loss: 0.00001099
Iteration 304/1000 | Loss: 0.00001099
Iteration 305/1000 | Loss: 0.00001099
Iteration 306/1000 | Loss: 0.00001099
Iteration 307/1000 | Loss: 0.00001099
Iteration 308/1000 | Loss: 0.00001099
Iteration 309/1000 | Loss: 0.00001099
Iteration 310/1000 | Loss: 0.00001099
Iteration 311/1000 | Loss: 0.00001099
Iteration 312/1000 | Loss: 0.00001099
Iteration 313/1000 | Loss: 0.00001099
Iteration 314/1000 | Loss: 0.00001099
Iteration 315/1000 | Loss: 0.00001099
Iteration 316/1000 | Loss: 0.00001099
Iteration 317/1000 | Loss: 0.00001099
Iteration 318/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [1.0994841431966051e-05, 1.0994841431966051e-05, 1.0994841431966051e-05, 1.0994841431966051e-05, 1.0994841431966051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0994841431966051e-05

Optimization complete. Final v2v error: 2.831770658493042 mm

Highest mean error: 3.6608657836914062 mm for frame 71

Lowest mean error: 2.6857404708862305 mm for frame 104

Saving results

Total time: 45.17399597167969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986138
Iteration 2/25 | Loss: 0.00174817
Iteration 3/25 | Loss: 0.00149301
Iteration 4/25 | Loss: 0.00147089
Iteration 5/25 | Loss: 0.00146580
Iteration 6/25 | Loss: 0.00146569
Iteration 7/25 | Loss: 0.00146569
Iteration 8/25 | Loss: 0.00146569
Iteration 9/25 | Loss: 0.00146569
Iteration 10/25 | Loss: 0.00146569
Iteration 11/25 | Loss: 0.00146569
Iteration 12/25 | Loss: 0.00146569
Iteration 13/25 | Loss: 0.00146569
Iteration 14/25 | Loss: 0.00146569
Iteration 15/25 | Loss: 0.00146569
Iteration 16/25 | Loss: 0.00146569
Iteration 17/25 | Loss: 0.00146569
Iteration 18/25 | Loss: 0.00146569
Iteration 19/25 | Loss: 0.00146569
Iteration 20/25 | Loss: 0.00146569
Iteration 21/25 | Loss: 0.00146569
Iteration 22/25 | Loss: 0.00146569
Iteration 23/25 | Loss: 0.00146569
Iteration 24/25 | Loss: 0.00146569
Iteration 25/25 | Loss: 0.00146569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.74892521
Iteration 2/25 | Loss: 0.00083076
Iteration 3/25 | Loss: 0.00083076
Iteration 4/25 | Loss: 0.00083076
Iteration 5/25 | Loss: 0.00083076
Iteration 6/25 | Loss: 0.00083076
Iteration 7/25 | Loss: 0.00083076
Iteration 8/25 | Loss: 0.00083076
Iteration 9/25 | Loss: 0.00083076
Iteration 10/25 | Loss: 0.00083076
Iteration 11/25 | Loss: 0.00083076
Iteration 12/25 | Loss: 0.00083076
Iteration 13/25 | Loss: 0.00083076
Iteration 14/25 | Loss: 0.00083076
Iteration 15/25 | Loss: 0.00083076
Iteration 16/25 | Loss: 0.00083076
Iteration 17/25 | Loss: 0.00083076
Iteration 18/25 | Loss: 0.00083076
Iteration 19/25 | Loss: 0.00083076
Iteration 20/25 | Loss: 0.00083076
Iteration 21/25 | Loss: 0.00083076
Iteration 22/25 | Loss: 0.00083076
Iteration 23/25 | Loss: 0.00083076
Iteration 24/25 | Loss: 0.00083076
Iteration 25/25 | Loss: 0.00083076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083076
Iteration 2/1000 | Loss: 0.00008294
Iteration 3/1000 | Loss: 0.00005227
Iteration 4/1000 | Loss: 0.00004139
Iteration 5/1000 | Loss: 0.00003950
Iteration 6/1000 | Loss: 0.00003845
Iteration 7/1000 | Loss: 0.00003716
Iteration 8/1000 | Loss: 0.00003645
Iteration 9/1000 | Loss: 0.00003588
Iteration 10/1000 | Loss: 0.00003544
Iteration 11/1000 | Loss: 0.00003508
Iteration 12/1000 | Loss: 0.00003479
Iteration 13/1000 | Loss: 0.00003459
Iteration 14/1000 | Loss: 0.00003439
Iteration 15/1000 | Loss: 0.00003431
Iteration 16/1000 | Loss: 0.00003429
Iteration 17/1000 | Loss: 0.00003419
Iteration 18/1000 | Loss: 0.00003408
Iteration 19/1000 | Loss: 0.00003408
Iteration 20/1000 | Loss: 0.00003404
Iteration 21/1000 | Loss: 0.00003396
Iteration 22/1000 | Loss: 0.00003395
Iteration 23/1000 | Loss: 0.00003385
Iteration 24/1000 | Loss: 0.00003385
Iteration 25/1000 | Loss: 0.00003385
Iteration 26/1000 | Loss: 0.00003385
Iteration 27/1000 | Loss: 0.00003385
Iteration 28/1000 | Loss: 0.00003381
Iteration 29/1000 | Loss: 0.00003381
Iteration 30/1000 | Loss: 0.00003379
Iteration 31/1000 | Loss: 0.00003377
Iteration 32/1000 | Loss: 0.00003376
Iteration 33/1000 | Loss: 0.00003375
Iteration 34/1000 | Loss: 0.00003375
Iteration 35/1000 | Loss: 0.00003374
Iteration 36/1000 | Loss: 0.00003373
Iteration 37/1000 | Loss: 0.00003373
Iteration 38/1000 | Loss: 0.00003369
Iteration 39/1000 | Loss: 0.00003369
Iteration 40/1000 | Loss: 0.00003366
Iteration 41/1000 | Loss: 0.00003365
Iteration 42/1000 | Loss: 0.00003365
Iteration 43/1000 | Loss: 0.00003365
Iteration 44/1000 | Loss: 0.00003365
Iteration 45/1000 | Loss: 0.00003365
Iteration 46/1000 | Loss: 0.00003365
Iteration 47/1000 | Loss: 0.00003364
Iteration 48/1000 | Loss: 0.00003364
Iteration 49/1000 | Loss: 0.00003364
Iteration 50/1000 | Loss: 0.00003364
Iteration 51/1000 | Loss: 0.00003364
Iteration 52/1000 | Loss: 0.00003364
Iteration 53/1000 | Loss: 0.00003364
Iteration 54/1000 | Loss: 0.00003364
Iteration 55/1000 | Loss: 0.00003364
Iteration 56/1000 | Loss: 0.00003364
Iteration 57/1000 | Loss: 0.00003364
Iteration 58/1000 | Loss: 0.00003364
Iteration 59/1000 | Loss: 0.00003362
Iteration 60/1000 | Loss: 0.00003362
Iteration 61/1000 | Loss: 0.00003362
Iteration 62/1000 | Loss: 0.00003362
Iteration 63/1000 | Loss: 0.00003362
Iteration 64/1000 | Loss: 0.00003362
Iteration 65/1000 | Loss: 0.00003362
Iteration 66/1000 | Loss: 0.00003361
Iteration 67/1000 | Loss: 0.00003361
Iteration 68/1000 | Loss: 0.00003361
Iteration 69/1000 | Loss: 0.00003361
Iteration 70/1000 | Loss: 0.00003360
Iteration 71/1000 | Loss: 0.00003360
Iteration 72/1000 | Loss: 0.00003359
Iteration 73/1000 | Loss: 0.00003359
Iteration 74/1000 | Loss: 0.00003359
Iteration 75/1000 | Loss: 0.00003359
Iteration 76/1000 | Loss: 0.00003359
Iteration 77/1000 | Loss: 0.00003358
Iteration 78/1000 | Loss: 0.00003358
Iteration 79/1000 | Loss: 0.00003358
Iteration 80/1000 | Loss: 0.00003358
Iteration 81/1000 | Loss: 0.00003357
Iteration 82/1000 | Loss: 0.00003357
Iteration 83/1000 | Loss: 0.00003357
Iteration 84/1000 | Loss: 0.00003357
Iteration 85/1000 | Loss: 0.00003357
Iteration 86/1000 | Loss: 0.00003356
Iteration 87/1000 | Loss: 0.00003356
Iteration 88/1000 | Loss: 0.00003356
Iteration 89/1000 | Loss: 0.00003356
Iteration 90/1000 | Loss: 0.00003356
Iteration 91/1000 | Loss: 0.00003356
Iteration 92/1000 | Loss: 0.00003356
Iteration 93/1000 | Loss: 0.00003356
Iteration 94/1000 | Loss: 0.00003355
Iteration 95/1000 | Loss: 0.00003355
Iteration 96/1000 | Loss: 0.00003355
Iteration 97/1000 | Loss: 0.00003355
Iteration 98/1000 | Loss: 0.00003355
Iteration 99/1000 | Loss: 0.00003355
Iteration 100/1000 | Loss: 0.00003354
Iteration 101/1000 | Loss: 0.00003354
Iteration 102/1000 | Loss: 0.00003354
Iteration 103/1000 | Loss: 0.00003354
Iteration 104/1000 | Loss: 0.00003354
Iteration 105/1000 | Loss: 0.00003353
Iteration 106/1000 | Loss: 0.00003353
Iteration 107/1000 | Loss: 0.00003353
Iteration 108/1000 | Loss: 0.00003353
Iteration 109/1000 | Loss: 0.00003353
Iteration 110/1000 | Loss: 0.00003353
Iteration 111/1000 | Loss: 0.00003353
Iteration 112/1000 | Loss: 0.00003353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [3.353312786202878e-05, 3.353312786202878e-05, 3.353312786202878e-05, 3.353312786202878e-05, 3.353312786202878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.353312786202878e-05

Optimization complete. Final v2v error: 4.760704040527344 mm

Highest mean error: 5.316819667816162 mm for frame 81

Lowest mean error: 4.184652805328369 mm for frame 157

Saving results

Total time: 47.114423751831055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034485
Iteration 2/25 | Loss: 0.01034485
Iteration 3/25 | Loss: 0.01034485
Iteration 4/25 | Loss: 0.01034485
Iteration 5/25 | Loss: 0.01034485
Iteration 6/25 | Loss: 0.01034484
Iteration 7/25 | Loss: 0.01034484
Iteration 8/25 | Loss: 0.01034484
Iteration 9/25 | Loss: 0.01034484
Iteration 10/25 | Loss: 0.01034484
Iteration 11/25 | Loss: 0.01034484
Iteration 12/25 | Loss: 0.01034484
Iteration 13/25 | Loss: 0.01034484
Iteration 14/25 | Loss: 0.01034483
Iteration 15/25 | Loss: 0.01034483
Iteration 16/25 | Loss: 0.01034483
Iteration 17/25 | Loss: 0.01034483
Iteration 18/25 | Loss: 0.01034483
Iteration 19/25 | Loss: 0.01034483
Iteration 20/25 | Loss: 0.01034483
Iteration 21/25 | Loss: 0.01034483
Iteration 22/25 | Loss: 0.01034483
Iteration 23/25 | Loss: 0.01034483
Iteration 24/25 | Loss: 0.01034482
Iteration 25/25 | Loss: 0.01034482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53746521
Iteration 2/25 | Loss: 0.13386402
Iteration 3/25 | Loss: 0.13093027
Iteration 4/25 | Loss: 0.12006775
Iteration 5/25 | Loss: 0.12006775
Iteration 6/25 | Loss: 0.12006775
Iteration 7/25 | Loss: 0.12006775
Iteration 8/25 | Loss: 0.12006775
Iteration 9/25 | Loss: 0.12006775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.12006774544715881, 0.12006774544715881, 0.12006774544715881, 0.12006774544715881, 0.12006774544715881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12006774544715881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12006775
Iteration 2/1000 | Loss: 0.00165914
Iteration 3/1000 | Loss: 0.00073676
Iteration 4/1000 | Loss: 0.00012897
Iteration 5/1000 | Loss: 0.00006472
Iteration 6/1000 | Loss: 0.00003828
Iteration 7/1000 | Loss: 0.00003008
Iteration 8/1000 | Loss: 0.00002518
Iteration 9/1000 | Loss: 0.00002149
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001746
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001437
Iteration 16/1000 | Loss: 0.00001393
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001277
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001177
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001175
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001168
Iteration 40/1000 | Loss: 0.00001167
Iteration 41/1000 | Loss: 0.00001167
Iteration 42/1000 | Loss: 0.00001166
Iteration 43/1000 | Loss: 0.00001165
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001160
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001150
Iteration 55/1000 | Loss: 0.00001149
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001147
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001147
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001144
Iteration 71/1000 | Loss: 0.00001144
Iteration 72/1000 | Loss: 0.00001144
Iteration 73/1000 | Loss: 0.00001144
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001143
Iteration 77/1000 | Loss: 0.00001143
Iteration 78/1000 | Loss: 0.00001143
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001143
Iteration 81/1000 | Loss: 0.00001143
Iteration 82/1000 | Loss: 0.00001143
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001143
Iteration 97/1000 | Loss: 0.00001143
Iteration 98/1000 | Loss: 0.00001143
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.1431440725573339e-05, 1.1431440725573339e-05, 1.1431440725573339e-05, 1.1431440725573339e-05, 1.1431440725573339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1431440725573339e-05

Optimization complete. Final v2v error: 2.9106838703155518 mm

Highest mean error: 3.2519845962524414 mm for frame 79

Lowest mean error: 2.7194151878356934 mm for frame 170

Saving results

Total time: 56.15822076797485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010840
Iteration 2/25 | Loss: 0.00245177
Iteration 3/25 | Loss: 0.00180160
Iteration 4/25 | Loss: 0.00176210
Iteration 5/25 | Loss: 0.00170381
Iteration 6/25 | Loss: 0.00165348
Iteration 7/25 | Loss: 0.00161153
Iteration 8/25 | Loss: 0.00160515
Iteration 9/25 | Loss: 0.00161863
Iteration 10/25 | Loss: 0.00165006
Iteration 11/25 | Loss: 0.00163495
Iteration 12/25 | Loss: 0.00157550
Iteration 13/25 | Loss: 0.00153781
Iteration 14/25 | Loss: 0.00149412
Iteration 15/25 | Loss: 0.00147715
Iteration 16/25 | Loss: 0.00146447
Iteration 17/25 | Loss: 0.00145215
Iteration 18/25 | Loss: 0.00143957
Iteration 19/25 | Loss: 0.00143066
Iteration 20/25 | Loss: 0.00142215
Iteration 21/25 | Loss: 0.00141990
Iteration 22/25 | Loss: 0.00141777
Iteration 23/25 | Loss: 0.00141207
Iteration 24/25 | Loss: 0.00141181
Iteration 25/25 | Loss: 0.00141135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45993936
Iteration 2/25 | Loss: 0.00160188
Iteration 3/25 | Loss: 0.00160188
Iteration 4/25 | Loss: 0.00160188
Iteration 5/25 | Loss: 0.00160188
Iteration 6/25 | Loss: 0.00160188
Iteration 7/25 | Loss: 0.00160188
Iteration 8/25 | Loss: 0.00160188
Iteration 9/25 | Loss: 0.00160188
Iteration 10/25 | Loss: 0.00160188
Iteration 11/25 | Loss: 0.00160188
Iteration 12/25 | Loss: 0.00160188
Iteration 13/25 | Loss: 0.00160188
Iteration 14/25 | Loss: 0.00160188
Iteration 15/25 | Loss: 0.00160188
Iteration 16/25 | Loss: 0.00160188
Iteration 17/25 | Loss: 0.00160188
Iteration 18/25 | Loss: 0.00160188
Iteration 19/25 | Loss: 0.00160188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016018759924918413, 0.0016018759924918413, 0.0016018759924918413, 0.0016018759924918413, 0.0016018759924918413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016018759924918413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160188
Iteration 2/1000 | Loss: 0.00014966
Iteration 3/1000 | Loss: 0.00011871
Iteration 4/1000 | Loss: 0.00011107
Iteration 5/1000 | Loss: 0.00010616
Iteration 6/1000 | Loss: 0.00008996
Iteration 7/1000 | Loss: 0.00045336
Iteration 8/1000 | Loss: 0.00009071
Iteration 9/1000 | Loss: 0.00008130
Iteration 10/1000 | Loss: 0.00008832
Iteration 11/1000 | Loss: 0.00009493
Iteration 12/1000 | Loss: 0.00007943
Iteration 13/1000 | Loss: 0.00008516
Iteration 14/1000 | Loss: 0.00008899
Iteration 15/1000 | Loss: 0.00029804
Iteration 16/1000 | Loss: 0.00707167
Iteration 17/1000 | Loss: 0.00349454
Iteration 18/1000 | Loss: 0.00139589
Iteration 19/1000 | Loss: 0.00018006
Iteration 20/1000 | Loss: 0.00011560
Iteration 21/1000 | Loss: 0.00007705
Iteration 22/1000 | Loss: 0.00078357
Iteration 23/1000 | Loss: 0.00004906
Iteration 24/1000 | Loss: 0.00005725
Iteration 25/1000 | Loss: 0.00003770
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00005065
Iteration 28/1000 | Loss: 0.00002893
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00001886
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001521
Iteration 36/1000 | Loss: 0.00001505
Iteration 37/1000 | Loss: 0.00007573
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001467
Iteration 43/1000 | Loss: 0.00001467
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00005085
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00005539
Iteration 54/1000 | Loss: 0.00005427
Iteration 55/1000 | Loss: 0.00004686
Iteration 56/1000 | Loss: 0.00003211
Iteration 57/1000 | Loss: 0.00004871
Iteration 58/1000 | Loss: 0.00002533
Iteration 59/1000 | Loss: 0.00005803
Iteration 60/1000 | Loss: 0.00002598
Iteration 61/1000 | Loss: 0.00003392
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001439
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001439
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001438
Iteration 95/1000 | Loss: 0.00001438
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001437
Iteration 99/1000 | Loss: 0.00001437
Iteration 100/1000 | Loss: 0.00001437
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001437
Iteration 118/1000 | Loss: 0.00001437
Iteration 119/1000 | Loss: 0.00001437
Iteration 120/1000 | Loss: 0.00001437
Iteration 121/1000 | Loss: 0.00001437
Iteration 122/1000 | Loss: 0.00001436
Iteration 123/1000 | Loss: 0.00001436
Iteration 124/1000 | Loss: 0.00001436
Iteration 125/1000 | Loss: 0.00001436
Iteration 126/1000 | Loss: 0.00001436
Iteration 127/1000 | Loss: 0.00001436
Iteration 128/1000 | Loss: 0.00001436
Iteration 129/1000 | Loss: 0.00001436
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001436
Iteration 132/1000 | Loss: 0.00001435
Iteration 133/1000 | Loss: 0.00001435
Iteration 134/1000 | Loss: 0.00001435
Iteration 135/1000 | Loss: 0.00001435
Iteration 136/1000 | Loss: 0.00001605
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4352398466144223e-05, 1.4352398466144223e-05, 1.4352398466144223e-05, 1.4352398466144223e-05, 1.4352398466144223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4352398466144223e-05

Optimization complete. Final v2v error: 3.205888271331787 mm

Highest mean error: 4.124765396118164 mm for frame 110

Lowest mean error: 2.75746488571167 mm for frame 31

Saving results

Total time: 130.31827688217163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007245
Iteration 2/25 | Loss: 0.00211144
Iteration 3/25 | Loss: 0.00170972
Iteration 4/25 | Loss: 0.00171223
Iteration 5/25 | Loss: 0.00161725
Iteration 6/25 | Loss: 0.00156111
Iteration 7/25 | Loss: 0.00151328
Iteration 8/25 | Loss: 0.00142563
Iteration 9/25 | Loss: 0.00141484
Iteration 10/25 | Loss: 0.00144388
Iteration 11/25 | Loss: 0.00141868
Iteration 12/25 | Loss: 0.00141868
Iteration 13/25 | Loss: 0.00141612
Iteration 14/25 | Loss: 0.00138453
Iteration 15/25 | Loss: 0.00137227
Iteration 16/25 | Loss: 0.00135904
Iteration 17/25 | Loss: 0.00135334
Iteration 18/25 | Loss: 0.00134310
Iteration 19/25 | Loss: 0.00134255
Iteration 20/25 | Loss: 0.00133382
Iteration 21/25 | Loss: 0.00132513
Iteration 22/25 | Loss: 0.00132110
Iteration 23/25 | Loss: 0.00132494
Iteration 24/25 | Loss: 0.00132984
Iteration 25/25 | Loss: 0.00132366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50231051
Iteration 2/25 | Loss: 0.00169060
Iteration 3/25 | Loss: 0.00169060
Iteration 4/25 | Loss: 0.00169060
Iteration 5/25 | Loss: 0.00169060
Iteration 6/25 | Loss: 0.00169060
Iteration 7/25 | Loss: 0.00169060
Iteration 8/25 | Loss: 0.00169060
Iteration 9/25 | Loss: 0.00169060
Iteration 10/25 | Loss: 0.00169060
Iteration 11/25 | Loss: 0.00169060
Iteration 12/25 | Loss: 0.00169060
Iteration 13/25 | Loss: 0.00169060
Iteration 14/25 | Loss: 0.00169060
Iteration 15/25 | Loss: 0.00169060
Iteration 16/25 | Loss: 0.00169060
Iteration 17/25 | Loss: 0.00169060
Iteration 18/25 | Loss: 0.00169060
Iteration 19/25 | Loss: 0.00169060
Iteration 20/25 | Loss: 0.00169060
Iteration 21/25 | Loss: 0.00169060
Iteration 22/25 | Loss: 0.00169060
Iteration 23/25 | Loss: 0.00169060
Iteration 24/25 | Loss: 0.00169060
Iteration 25/25 | Loss: 0.00169060

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169060
Iteration 2/1000 | Loss: 0.00019925
Iteration 3/1000 | Loss: 0.00021337
Iteration 4/1000 | Loss: 0.00006605
Iteration 5/1000 | Loss: 0.00286894
Iteration 6/1000 | Loss: 0.00028446
Iteration 7/1000 | Loss: 0.00025929
Iteration 8/1000 | Loss: 0.00026280
Iteration 9/1000 | Loss: 0.00033656
Iteration 10/1000 | Loss: 0.00016718
Iteration 11/1000 | Loss: 0.00027532
Iteration 12/1000 | Loss: 0.00051229
Iteration 13/1000 | Loss: 0.00043372
Iteration 14/1000 | Loss: 0.00036757
Iteration 15/1000 | Loss: 0.00030220
Iteration 16/1000 | Loss: 0.00011369
Iteration 17/1000 | Loss: 0.00012948
Iteration 18/1000 | Loss: 0.00018559
Iteration 19/1000 | Loss: 0.00006419
Iteration 20/1000 | Loss: 0.00015055
Iteration 21/1000 | Loss: 0.00015652
Iteration 22/1000 | Loss: 0.00012944
Iteration 23/1000 | Loss: 0.00018890
Iteration 24/1000 | Loss: 0.00019621
Iteration 25/1000 | Loss: 0.00017541
Iteration 26/1000 | Loss: 0.00017780
Iteration 27/1000 | Loss: 0.00015105
Iteration 28/1000 | Loss: 0.00021774
Iteration 29/1000 | Loss: 0.00014008
Iteration 30/1000 | Loss: 0.00014494
Iteration 31/1000 | Loss: 0.00024327
Iteration 32/1000 | Loss: 0.00020842
Iteration 33/1000 | Loss: 0.00015628
Iteration 34/1000 | Loss: 0.00014246
Iteration 35/1000 | Loss: 0.00012876
Iteration 36/1000 | Loss: 0.00040440
Iteration 37/1000 | Loss: 0.00046908
Iteration 38/1000 | Loss: 0.00031745
Iteration 39/1000 | Loss: 0.00028131
Iteration 40/1000 | Loss: 0.00011717
Iteration 41/1000 | Loss: 0.00013023
Iteration 42/1000 | Loss: 0.00011316
Iteration 43/1000 | Loss: 0.00018730
Iteration 44/1000 | Loss: 0.00020571
Iteration 45/1000 | Loss: 0.00019136
Iteration 46/1000 | Loss: 0.00031223
Iteration 47/1000 | Loss: 0.00019293
Iteration 48/1000 | Loss: 0.00025814
Iteration 49/1000 | Loss: 0.00021781
Iteration 50/1000 | Loss: 0.00031708
Iteration 51/1000 | Loss: 0.00021653
Iteration 52/1000 | Loss: 0.00031675
Iteration 53/1000 | Loss: 0.00021294
Iteration 54/1000 | Loss: 0.00028791
Iteration 55/1000 | Loss: 0.00015755
Iteration 56/1000 | Loss: 0.00037400
Iteration 57/1000 | Loss: 0.00017471
Iteration 58/1000 | Loss: 0.00054783
Iteration 59/1000 | Loss: 0.00108201
Iteration 60/1000 | Loss: 0.00008613
Iteration 61/1000 | Loss: 0.00028808
Iteration 62/1000 | Loss: 0.00014768
Iteration 63/1000 | Loss: 0.00003936
Iteration 64/1000 | Loss: 0.00003447
Iteration 65/1000 | Loss: 0.00005148
Iteration 66/1000 | Loss: 0.00017743
Iteration 67/1000 | Loss: 0.00009494
Iteration 68/1000 | Loss: 0.00018338
Iteration 69/1000 | Loss: 0.00018884
Iteration 70/1000 | Loss: 0.00015704
Iteration 71/1000 | Loss: 0.00017245
Iteration 72/1000 | Loss: 0.00003838
Iteration 73/1000 | Loss: 0.00003557
Iteration 74/1000 | Loss: 0.00003817
Iteration 75/1000 | Loss: 0.00003252
Iteration 76/1000 | Loss: 0.00014268
Iteration 77/1000 | Loss: 0.00019559
Iteration 78/1000 | Loss: 0.00020139
Iteration 79/1000 | Loss: 0.00016104
Iteration 80/1000 | Loss: 0.00014741
Iteration 81/1000 | Loss: 0.00017362
Iteration 82/1000 | Loss: 0.00019060
Iteration 83/1000 | Loss: 0.00021547
Iteration 84/1000 | Loss: 0.00018048
Iteration 85/1000 | Loss: 0.00015589
Iteration 86/1000 | Loss: 0.00015839
Iteration 87/1000 | Loss: 0.00016396
Iteration 88/1000 | Loss: 0.00013879
Iteration 89/1000 | Loss: 0.00016410
Iteration 90/1000 | Loss: 0.00017211
Iteration 91/1000 | Loss: 0.00011472
Iteration 92/1000 | Loss: 0.00003773
Iteration 93/1000 | Loss: 0.00025573
Iteration 94/1000 | Loss: 0.00026056
Iteration 95/1000 | Loss: 0.00017907
Iteration 96/1000 | Loss: 0.00019507
Iteration 97/1000 | Loss: 0.00016741
Iteration 98/1000 | Loss: 0.00014125
Iteration 99/1000 | Loss: 0.00013192
Iteration 100/1000 | Loss: 0.00006912
Iteration 101/1000 | Loss: 0.00016021
Iteration 102/1000 | Loss: 0.00004487
Iteration 103/1000 | Loss: 0.00002978
Iteration 104/1000 | Loss: 0.00002925
Iteration 105/1000 | Loss: 0.00025840
Iteration 106/1000 | Loss: 0.00003121
Iteration 107/1000 | Loss: 0.00003653
Iteration 108/1000 | Loss: 0.00002770
Iteration 109/1000 | Loss: 0.00046367
Iteration 110/1000 | Loss: 0.00022691
Iteration 111/1000 | Loss: 0.00024207
Iteration 112/1000 | Loss: 0.00003266
Iteration 113/1000 | Loss: 0.00003111
Iteration 114/1000 | Loss: 0.00002618
Iteration 115/1000 | Loss: 0.00002443
Iteration 116/1000 | Loss: 0.00002850
Iteration 117/1000 | Loss: 0.00011243
Iteration 118/1000 | Loss: 0.00002400
Iteration 119/1000 | Loss: 0.00002324
Iteration 120/1000 | Loss: 0.00003553
Iteration 121/1000 | Loss: 0.00002656
Iteration 122/1000 | Loss: 0.00056440
Iteration 123/1000 | Loss: 0.00009326
Iteration 124/1000 | Loss: 0.00005707
Iteration 125/1000 | Loss: 0.00009961
Iteration 126/1000 | Loss: 0.00008193
Iteration 127/1000 | Loss: 0.00016749
Iteration 128/1000 | Loss: 0.00008379
Iteration 129/1000 | Loss: 0.00022190
Iteration 130/1000 | Loss: 0.00014812
Iteration 131/1000 | Loss: 0.00024727
Iteration 132/1000 | Loss: 0.00012193
Iteration 133/1000 | Loss: 0.00006768
Iteration 134/1000 | Loss: 0.00009330
Iteration 135/1000 | Loss: 0.00011958
Iteration 136/1000 | Loss: 0.00017955
Iteration 137/1000 | Loss: 0.00016243
Iteration 138/1000 | Loss: 0.00019944
Iteration 139/1000 | Loss: 0.00018451
Iteration 140/1000 | Loss: 0.00025587
Iteration 141/1000 | Loss: 0.00013258
Iteration 142/1000 | Loss: 0.00017648
Iteration 143/1000 | Loss: 0.00011345
Iteration 144/1000 | Loss: 0.00006051
Iteration 145/1000 | Loss: 0.00021564
Iteration 146/1000 | Loss: 0.00021558
Iteration 147/1000 | Loss: 0.00025601
Iteration 148/1000 | Loss: 0.00021329
Iteration 149/1000 | Loss: 0.00022721
Iteration 150/1000 | Loss: 0.00021767
Iteration 151/1000 | Loss: 0.00022440
Iteration 152/1000 | Loss: 0.00015318
Iteration 153/1000 | Loss: 0.00027357
Iteration 154/1000 | Loss: 0.00004037
Iteration 155/1000 | Loss: 0.00009686
Iteration 156/1000 | Loss: 0.00002846
Iteration 157/1000 | Loss: 0.00004808
Iteration 158/1000 | Loss: 0.00006449
Iteration 159/1000 | Loss: 0.00002404
Iteration 160/1000 | Loss: 0.00003695
Iteration 161/1000 | Loss: 0.00002127
Iteration 162/1000 | Loss: 0.00004466
Iteration 163/1000 | Loss: 0.00002056
Iteration 164/1000 | Loss: 0.00002019
Iteration 165/1000 | Loss: 0.00004728
Iteration 166/1000 | Loss: 0.00005606
Iteration 167/1000 | Loss: 0.00005378
Iteration 168/1000 | Loss: 0.00001969
Iteration 169/1000 | Loss: 0.00002946
Iteration 170/1000 | Loss: 0.00001975
Iteration 171/1000 | Loss: 0.00001975
Iteration 172/1000 | Loss: 0.00001964
Iteration 173/1000 | Loss: 0.00001964
Iteration 174/1000 | Loss: 0.00001952
Iteration 175/1000 | Loss: 0.00001952
Iteration 176/1000 | Loss: 0.00001952
Iteration 177/1000 | Loss: 0.00001952
Iteration 178/1000 | Loss: 0.00001952
Iteration 179/1000 | Loss: 0.00001952
Iteration 180/1000 | Loss: 0.00001952
Iteration 181/1000 | Loss: 0.00001952
Iteration 182/1000 | Loss: 0.00001952
Iteration 183/1000 | Loss: 0.00001952
Iteration 184/1000 | Loss: 0.00001952
Iteration 185/1000 | Loss: 0.00002785
Iteration 186/1000 | Loss: 0.00002785
Iteration 187/1000 | Loss: 0.00002785
Iteration 188/1000 | Loss: 0.00002785
Iteration 189/1000 | Loss: 0.00002388
Iteration 190/1000 | Loss: 0.00004402
Iteration 191/1000 | Loss: 0.00002026
Iteration 192/1000 | Loss: 0.00002060
Iteration 193/1000 | Loss: 0.00001940
Iteration 194/1000 | Loss: 0.00001940
Iteration 195/1000 | Loss: 0.00001939
Iteration 196/1000 | Loss: 0.00001938
Iteration 197/1000 | Loss: 0.00001938
Iteration 198/1000 | Loss: 0.00001937
Iteration 199/1000 | Loss: 0.00002293
Iteration 200/1000 | Loss: 0.00001932
Iteration 201/1000 | Loss: 0.00002785
Iteration 202/1000 | Loss: 0.00001982
Iteration 203/1000 | Loss: 0.00001981
Iteration 204/1000 | Loss: 0.00001920
Iteration 205/1000 | Loss: 0.00001919
Iteration 206/1000 | Loss: 0.00001919
Iteration 207/1000 | Loss: 0.00001918
Iteration 208/1000 | Loss: 0.00001918
Iteration 209/1000 | Loss: 0.00001918
Iteration 210/1000 | Loss: 0.00001918
Iteration 211/1000 | Loss: 0.00001918
Iteration 212/1000 | Loss: 0.00001918
Iteration 213/1000 | Loss: 0.00001918
Iteration 214/1000 | Loss: 0.00001918
Iteration 215/1000 | Loss: 0.00001918
Iteration 216/1000 | Loss: 0.00001918
Iteration 217/1000 | Loss: 0.00001918
Iteration 218/1000 | Loss: 0.00001918
Iteration 219/1000 | Loss: 0.00001918
Iteration 220/1000 | Loss: 0.00001918
Iteration 221/1000 | Loss: 0.00001918
Iteration 222/1000 | Loss: 0.00001918
Iteration 223/1000 | Loss: 0.00001918
Iteration 224/1000 | Loss: 0.00001918
Iteration 225/1000 | Loss: 0.00001917
Iteration 226/1000 | Loss: 0.00001917
Iteration 227/1000 | Loss: 0.00001917
Iteration 228/1000 | Loss: 0.00001917
Iteration 229/1000 | Loss: 0.00001917
Iteration 230/1000 | Loss: 0.00001917
Iteration 231/1000 | Loss: 0.00001917
Iteration 232/1000 | Loss: 0.00001917
Iteration 233/1000 | Loss: 0.00001917
Iteration 234/1000 | Loss: 0.00001917
Iteration 235/1000 | Loss: 0.00001917
Iteration 236/1000 | Loss: 0.00001917
Iteration 237/1000 | Loss: 0.00001917
Iteration 238/1000 | Loss: 0.00001917
Iteration 239/1000 | Loss: 0.00001917
Iteration 240/1000 | Loss: 0.00001917
Iteration 241/1000 | Loss: 0.00001917
Iteration 242/1000 | Loss: 0.00001916
Iteration 243/1000 | Loss: 0.00001916
Iteration 244/1000 | Loss: 0.00001916
Iteration 245/1000 | Loss: 0.00001916
Iteration 246/1000 | Loss: 0.00001916
Iteration 247/1000 | Loss: 0.00001916
Iteration 248/1000 | Loss: 0.00001916
Iteration 249/1000 | Loss: 0.00001915
Iteration 250/1000 | Loss: 0.00001915
Iteration 251/1000 | Loss: 0.00001915
Iteration 252/1000 | Loss: 0.00001915
Iteration 253/1000 | Loss: 0.00001915
Iteration 254/1000 | Loss: 0.00001914
Iteration 255/1000 | Loss: 0.00001914
Iteration 256/1000 | Loss: 0.00001914
Iteration 257/1000 | Loss: 0.00001914
Iteration 258/1000 | Loss: 0.00001914
Iteration 259/1000 | Loss: 0.00001914
Iteration 260/1000 | Loss: 0.00001914
Iteration 261/1000 | Loss: 0.00001914
Iteration 262/1000 | Loss: 0.00001914
Iteration 263/1000 | Loss: 0.00001914
Iteration 264/1000 | Loss: 0.00001914
Iteration 265/1000 | Loss: 0.00001914
Iteration 266/1000 | Loss: 0.00001914
Iteration 267/1000 | Loss: 0.00001914
Iteration 268/1000 | Loss: 0.00001914
Iteration 269/1000 | Loss: 0.00001914
Iteration 270/1000 | Loss: 0.00001914
Iteration 271/1000 | Loss: 0.00001914
Iteration 272/1000 | Loss: 0.00001914
Iteration 273/1000 | Loss: 0.00001914
Iteration 274/1000 | Loss: 0.00001914
Iteration 275/1000 | Loss: 0.00001914
Iteration 276/1000 | Loss: 0.00001914
Iteration 277/1000 | Loss: 0.00001914
Iteration 278/1000 | Loss: 0.00001914
Iteration 279/1000 | Loss: 0.00001914
Iteration 280/1000 | Loss: 0.00001914
Iteration 281/1000 | Loss: 0.00001914
Iteration 282/1000 | Loss: 0.00001914
Iteration 283/1000 | Loss: 0.00001914
Iteration 284/1000 | Loss: 0.00001914
Iteration 285/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.9138033167109825e-05, 1.9138033167109825e-05, 1.9138033167109825e-05, 1.9138033167109825e-05, 1.9138033167109825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9138033167109825e-05

Optimization complete. Final v2v error: 3.3050053119659424 mm

Highest mean error: 11.973701477050781 mm for frame 97

Lowest mean error: 2.8392670154571533 mm for frame 44

Saving results

Total time: 294.483535528183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466035
Iteration 2/25 | Loss: 0.00131243
Iteration 3/25 | Loss: 0.00127221
Iteration 4/25 | Loss: 0.00126758
Iteration 5/25 | Loss: 0.00126647
Iteration 6/25 | Loss: 0.00126647
Iteration 7/25 | Loss: 0.00126647
Iteration 8/25 | Loss: 0.00126647
Iteration 9/25 | Loss: 0.00126647
Iteration 10/25 | Loss: 0.00126647
Iteration 11/25 | Loss: 0.00126647
Iteration 12/25 | Loss: 0.00126647
Iteration 13/25 | Loss: 0.00126647
Iteration 14/25 | Loss: 0.00126647
Iteration 15/25 | Loss: 0.00126647
Iteration 16/25 | Loss: 0.00126647
Iteration 17/25 | Loss: 0.00126647
Iteration 18/25 | Loss: 0.00126647
Iteration 19/25 | Loss: 0.00126647
Iteration 20/25 | Loss: 0.00126647
Iteration 21/25 | Loss: 0.00126647
Iteration 22/25 | Loss: 0.00126647
Iteration 23/25 | Loss: 0.00126647
Iteration 24/25 | Loss: 0.00126647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001266465988010168, 0.001266465988010168, 0.001266465988010168, 0.001266465988010168, 0.001266465988010168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001266465988010168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38759387
Iteration 2/25 | Loss: 0.00078089
Iteration 3/25 | Loss: 0.00078089
Iteration 4/25 | Loss: 0.00078089
Iteration 5/25 | Loss: 0.00078089
Iteration 6/25 | Loss: 0.00078089
Iteration 7/25 | Loss: 0.00078089
Iteration 8/25 | Loss: 0.00078089
Iteration 9/25 | Loss: 0.00078089
Iteration 10/25 | Loss: 0.00078089
Iteration 11/25 | Loss: 0.00078089
Iteration 12/25 | Loss: 0.00078089
Iteration 13/25 | Loss: 0.00078089
Iteration 14/25 | Loss: 0.00078089
Iteration 15/25 | Loss: 0.00078089
Iteration 16/25 | Loss: 0.00078089
Iteration 17/25 | Loss: 0.00078089
Iteration 18/25 | Loss: 0.00078089
Iteration 19/25 | Loss: 0.00078089
Iteration 20/25 | Loss: 0.00078089
Iteration 21/25 | Loss: 0.00078088
Iteration 22/25 | Loss: 0.00078089
Iteration 23/25 | Loss: 0.00078089
Iteration 24/25 | Loss: 0.00078088
Iteration 25/25 | Loss: 0.00078088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078088
Iteration 2/1000 | Loss: 0.00003147
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001738
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001612
Iteration 8/1000 | Loss: 0.00001606
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001575
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001539
Iteration 14/1000 | Loss: 0.00001538
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001524
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001486
Iteration 33/1000 | Loss: 0.00001486
Iteration 34/1000 | Loss: 0.00001485
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001480
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001478
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001476
Iteration 48/1000 | Loss: 0.00001476
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001467
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001453
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001441
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001441
Iteration 89/1000 | Loss: 0.00001441
Iteration 90/1000 | Loss: 0.00001440
Iteration 91/1000 | Loss: 0.00001440
Iteration 92/1000 | Loss: 0.00001440
Iteration 93/1000 | Loss: 0.00001439
Iteration 94/1000 | Loss: 0.00001439
Iteration 95/1000 | Loss: 0.00001439
Iteration 96/1000 | Loss: 0.00001439
Iteration 97/1000 | Loss: 0.00001439
Iteration 98/1000 | Loss: 0.00001439
Iteration 99/1000 | Loss: 0.00001439
Iteration 100/1000 | Loss: 0.00001439
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00001438
Iteration 103/1000 | Loss: 0.00001438
Iteration 104/1000 | Loss: 0.00001438
Iteration 105/1000 | Loss: 0.00001438
Iteration 106/1000 | Loss: 0.00001438
Iteration 107/1000 | Loss: 0.00001438
Iteration 108/1000 | Loss: 0.00001438
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001437
Iteration 111/1000 | Loss: 0.00001436
Iteration 112/1000 | Loss: 0.00001436
Iteration 113/1000 | Loss: 0.00001436
Iteration 114/1000 | Loss: 0.00001436
Iteration 115/1000 | Loss: 0.00001435
Iteration 116/1000 | Loss: 0.00001435
Iteration 117/1000 | Loss: 0.00001435
Iteration 118/1000 | Loss: 0.00001435
Iteration 119/1000 | Loss: 0.00001435
Iteration 120/1000 | Loss: 0.00001435
Iteration 121/1000 | Loss: 0.00001435
Iteration 122/1000 | Loss: 0.00001435
Iteration 123/1000 | Loss: 0.00001435
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001434
Iteration 127/1000 | Loss: 0.00001434
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001434
Iteration 132/1000 | Loss: 0.00001434
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001434
Iteration 135/1000 | Loss: 0.00001434
Iteration 136/1000 | Loss: 0.00001434
Iteration 137/1000 | Loss: 0.00001434
Iteration 138/1000 | Loss: 0.00001434
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001433
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001433
Iteration 149/1000 | Loss: 0.00001433
Iteration 150/1000 | Loss: 0.00001433
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001432
Iteration 153/1000 | Loss: 0.00001432
Iteration 154/1000 | Loss: 0.00001432
Iteration 155/1000 | Loss: 0.00001432
Iteration 156/1000 | Loss: 0.00001432
Iteration 157/1000 | Loss: 0.00001432
Iteration 158/1000 | Loss: 0.00001432
Iteration 159/1000 | Loss: 0.00001432
Iteration 160/1000 | Loss: 0.00001431
Iteration 161/1000 | Loss: 0.00001431
Iteration 162/1000 | Loss: 0.00001431
Iteration 163/1000 | Loss: 0.00001431
Iteration 164/1000 | Loss: 0.00001431
Iteration 165/1000 | Loss: 0.00001431
Iteration 166/1000 | Loss: 0.00001431
Iteration 167/1000 | Loss: 0.00001431
Iteration 168/1000 | Loss: 0.00001431
Iteration 169/1000 | Loss: 0.00001431
Iteration 170/1000 | Loss: 0.00001431
Iteration 171/1000 | Loss: 0.00001431
Iteration 172/1000 | Loss: 0.00001431
Iteration 173/1000 | Loss: 0.00001431
Iteration 174/1000 | Loss: 0.00001431
Iteration 175/1000 | Loss: 0.00001431
Iteration 176/1000 | Loss: 0.00001431
Iteration 177/1000 | Loss: 0.00001430
Iteration 178/1000 | Loss: 0.00001430
Iteration 179/1000 | Loss: 0.00001430
Iteration 180/1000 | Loss: 0.00001430
Iteration 181/1000 | Loss: 0.00001430
Iteration 182/1000 | Loss: 0.00001430
Iteration 183/1000 | Loss: 0.00001430
Iteration 184/1000 | Loss: 0.00001430
Iteration 185/1000 | Loss: 0.00001430
Iteration 186/1000 | Loss: 0.00001430
Iteration 187/1000 | Loss: 0.00001430
Iteration 188/1000 | Loss: 0.00001430
Iteration 189/1000 | Loss: 0.00001430
Iteration 190/1000 | Loss: 0.00001430
Iteration 191/1000 | Loss: 0.00001430
Iteration 192/1000 | Loss: 0.00001429
Iteration 193/1000 | Loss: 0.00001429
Iteration 194/1000 | Loss: 0.00001429
Iteration 195/1000 | Loss: 0.00001429
Iteration 196/1000 | Loss: 0.00001429
Iteration 197/1000 | Loss: 0.00001429
Iteration 198/1000 | Loss: 0.00001429
Iteration 199/1000 | Loss: 0.00001429
Iteration 200/1000 | Loss: 0.00001429
Iteration 201/1000 | Loss: 0.00001429
Iteration 202/1000 | Loss: 0.00001429
Iteration 203/1000 | Loss: 0.00001429
Iteration 204/1000 | Loss: 0.00001429
Iteration 205/1000 | Loss: 0.00001429
Iteration 206/1000 | Loss: 0.00001429
Iteration 207/1000 | Loss: 0.00001428
Iteration 208/1000 | Loss: 0.00001428
Iteration 209/1000 | Loss: 0.00001428
Iteration 210/1000 | Loss: 0.00001428
Iteration 211/1000 | Loss: 0.00001428
Iteration 212/1000 | Loss: 0.00001428
Iteration 213/1000 | Loss: 0.00001428
Iteration 214/1000 | Loss: 0.00001428
Iteration 215/1000 | Loss: 0.00001428
Iteration 216/1000 | Loss: 0.00001427
Iteration 217/1000 | Loss: 0.00001427
Iteration 218/1000 | Loss: 0.00001427
Iteration 219/1000 | Loss: 0.00001427
Iteration 220/1000 | Loss: 0.00001427
Iteration 221/1000 | Loss: 0.00001427
Iteration 222/1000 | Loss: 0.00001427
Iteration 223/1000 | Loss: 0.00001427
Iteration 224/1000 | Loss: 0.00001427
Iteration 225/1000 | Loss: 0.00001427
Iteration 226/1000 | Loss: 0.00001427
Iteration 227/1000 | Loss: 0.00001427
Iteration 228/1000 | Loss: 0.00001427
Iteration 229/1000 | Loss: 0.00001427
Iteration 230/1000 | Loss: 0.00001427
Iteration 231/1000 | Loss: 0.00001427
Iteration 232/1000 | Loss: 0.00001427
Iteration 233/1000 | Loss: 0.00001427
Iteration 234/1000 | Loss: 0.00001427
Iteration 235/1000 | Loss: 0.00001427
Iteration 236/1000 | Loss: 0.00001427
Iteration 237/1000 | Loss: 0.00001427
Iteration 238/1000 | Loss: 0.00001427
Iteration 239/1000 | Loss: 0.00001427
Iteration 240/1000 | Loss: 0.00001427
Iteration 241/1000 | Loss: 0.00001427
Iteration 242/1000 | Loss: 0.00001427
Iteration 243/1000 | Loss: 0.00001427
Iteration 244/1000 | Loss: 0.00001427
Iteration 245/1000 | Loss: 0.00001427
Iteration 246/1000 | Loss: 0.00001427
Iteration 247/1000 | Loss: 0.00001427
Iteration 248/1000 | Loss: 0.00001427
Iteration 249/1000 | Loss: 0.00001427
Iteration 250/1000 | Loss: 0.00001427
Iteration 251/1000 | Loss: 0.00001427
Iteration 252/1000 | Loss: 0.00001427
Iteration 253/1000 | Loss: 0.00001427
Iteration 254/1000 | Loss: 0.00001427
Iteration 255/1000 | Loss: 0.00001427
Iteration 256/1000 | Loss: 0.00001427
Iteration 257/1000 | Loss: 0.00001427
Iteration 258/1000 | Loss: 0.00001427
Iteration 259/1000 | Loss: 0.00001427
Iteration 260/1000 | Loss: 0.00001427
Iteration 261/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.4272380212787539e-05, 1.4272380212787539e-05, 1.4272380212787539e-05, 1.4272380212787539e-05, 1.4272380212787539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4272380212787539e-05

Optimization complete. Final v2v error: 3.2252111434936523 mm

Highest mean error: 3.38856840133667 mm for frame 23

Lowest mean error: 3.092566967010498 mm for frame 63

Saving results

Total time: 40.6142852306366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597441
Iteration 2/25 | Loss: 0.00138841
Iteration 3/25 | Loss: 0.00129592
Iteration 4/25 | Loss: 0.00128048
Iteration 5/25 | Loss: 0.00127358
Iteration 6/25 | Loss: 0.00127219
Iteration 7/25 | Loss: 0.00127219
Iteration 8/25 | Loss: 0.00127219
Iteration 9/25 | Loss: 0.00127219
Iteration 10/25 | Loss: 0.00127219
Iteration 11/25 | Loss: 0.00127219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001272189780138433, 0.001272189780138433, 0.001272189780138433, 0.001272189780138433, 0.001272189780138433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001272189780138433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.53305125
Iteration 2/25 | Loss: 0.00101621
Iteration 3/25 | Loss: 0.00101621
Iteration 4/25 | Loss: 0.00101620
Iteration 5/25 | Loss: 0.00101620
Iteration 6/25 | Loss: 0.00101620
Iteration 7/25 | Loss: 0.00101620
Iteration 8/25 | Loss: 0.00101620
Iteration 9/25 | Loss: 0.00101620
Iteration 10/25 | Loss: 0.00101620
Iteration 11/25 | Loss: 0.00101620
Iteration 12/25 | Loss: 0.00101620
Iteration 13/25 | Loss: 0.00101620
Iteration 14/25 | Loss: 0.00101620
Iteration 15/25 | Loss: 0.00101620
Iteration 16/25 | Loss: 0.00101620
Iteration 17/25 | Loss: 0.00101620
Iteration 18/25 | Loss: 0.00101620
Iteration 19/25 | Loss: 0.00101620
Iteration 20/25 | Loss: 0.00101620
Iteration 21/25 | Loss: 0.00101620
Iteration 22/25 | Loss: 0.00101620
Iteration 23/25 | Loss: 0.00101620
Iteration 24/25 | Loss: 0.00101620
Iteration 25/25 | Loss: 0.00101620

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101620
Iteration 2/1000 | Loss: 0.00002563
Iteration 3/1000 | Loss: 0.00001761
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00001506
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001397
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001367
Iteration 32/1000 | Loss: 0.00001367
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001358
Iteration 41/1000 | Loss: 0.00001358
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001356
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001341
Iteration 109/1000 | Loss: 0.00001341
Iteration 110/1000 | Loss: 0.00001340
Iteration 111/1000 | Loss: 0.00001340
Iteration 112/1000 | Loss: 0.00001339
Iteration 113/1000 | Loss: 0.00001339
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001339
Iteration 116/1000 | Loss: 0.00001339
Iteration 117/1000 | Loss: 0.00001339
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001338
Iteration 127/1000 | Loss: 0.00001338
Iteration 128/1000 | Loss: 0.00001338
Iteration 129/1000 | Loss: 0.00001338
Iteration 130/1000 | Loss: 0.00001338
Iteration 131/1000 | Loss: 0.00001338
Iteration 132/1000 | Loss: 0.00001337
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00001337
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001335
Iteration 147/1000 | Loss: 0.00001335
Iteration 148/1000 | Loss: 0.00001335
Iteration 149/1000 | Loss: 0.00001335
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001335
Iteration 152/1000 | Loss: 0.00001334
Iteration 153/1000 | Loss: 0.00001334
Iteration 154/1000 | Loss: 0.00001334
Iteration 155/1000 | Loss: 0.00001334
Iteration 156/1000 | Loss: 0.00001334
Iteration 157/1000 | Loss: 0.00001334
Iteration 158/1000 | Loss: 0.00001334
Iteration 159/1000 | Loss: 0.00001334
Iteration 160/1000 | Loss: 0.00001334
Iteration 161/1000 | Loss: 0.00001334
Iteration 162/1000 | Loss: 0.00001334
Iteration 163/1000 | Loss: 0.00001334
Iteration 164/1000 | Loss: 0.00001334
Iteration 165/1000 | Loss: 0.00001334
Iteration 166/1000 | Loss: 0.00001334
Iteration 167/1000 | Loss: 0.00001334
Iteration 168/1000 | Loss: 0.00001334
Iteration 169/1000 | Loss: 0.00001334
Iteration 170/1000 | Loss: 0.00001334
Iteration 171/1000 | Loss: 0.00001334
Iteration 172/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.3340988516574726e-05, 1.3340988516574726e-05, 1.3340988516574726e-05, 1.3340988516574726e-05, 1.3340988516574726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3340988516574726e-05

Optimization complete. Final v2v error: 3.1325371265411377 mm

Highest mean error: 3.6195192337036133 mm for frame 87

Lowest mean error: 2.909644603729248 mm for frame 11

Saving results

Total time: 36.93793201446533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967446
Iteration 2/25 | Loss: 0.00209406
Iteration 3/25 | Loss: 0.00180055
Iteration 4/25 | Loss: 0.00177094
Iteration 5/25 | Loss: 0.00177586
Iteration 6/25 | Loss: 0.00174529
Iteration 7/25 | Loss: 0.00170746
Iteration 8/25 | Loss: 0.00166146
Iteration 9/25 | Loss: 0.00163119
Iteration 10/25 | Loss: 0.00162582
Iteration 11/25 | Loss: 0.00162276
Iteration 12/25 | Loss: 0.00161764
Iteration 13/25 | Loss: 0.00161463
Iteration 14/25 | Loss: 0.00161412
Iteration 15/25 | Loss: 0.00161405
Iteration 16/25 | Loss: 0.00161404
Iteration 17/25 | Loss: 0.00161404
Iteration 18/25 | Loss: 0.00161404
Iteration 19/25 | Loss: 0.00161404
Iteration 20/25 | Loss: 0.00161404
Iteration 21/25 | Loss: 0.00161404
Iteration 22/25 | Loss: 0.00161404
Iteration 23/25 | Loss: 0.00161404
Iteration 24/25 | Loss: 0.00161403
Iteration 25/25 | Loss: 0.00161403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.19327860
Iteration 2/25 | Loss: 0.00325098
Iteration 3/25 | Loss: 0.00325098
Iteration 4/25 | Loss: 0.00325098
Iteration 5/25 | Loss: 0.00325098
Iteration 6/25 | Loss: 0.00325098
Iteration 7/25 | Loss: 0.00325098
Iteration 8/25 | Loss: 0.00325098
Iteration 9/25 | Loss: 0.00325098
Iteration 10/25 | Loss: 0.00325098
Iteration 11/25 | Loss: 0.00325098
Iteration 12/25 | Loss: 0.00325098
Iteration 13/25 | Loss: 0.00325098
Iteration 14/25 | Loss: 0.00325098
Iteration 15/25 | Loss: 0.00325098
Iteration 16/25 | Loss: 0.00325098
Iteration 17/25 | Loss: 0.00325098
Iteration 18/25 | Loss: 0.00325098
Iteration 19/25 | Loss: 0.00325098
Iteration 20/25 | Loss: 0.00325098
Iteration 21/25 | Loss: 0.00325098
Iteration 22/25 | Loss: 0.00325098
Iteration 23/25 | Loss: 0.00325098
Iteration 24/25 | Loss: 0.00325098
Iteration 25/25 | Loss: 0.00325098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325098
Iteration 2/1000 | Loss: 0.00034045
Iteration 3/1000 | Loss: 0.00017663
Iteration 4/1000 | Loss: 0.00016584
Iteration 5/1000 | Loss: 0.00020395
Iteration 6/1000 | Loss: 0.00019769
Iteration 7/1000 | Loss: 0.00011592
Iteration 8/1000 | Loss: 0.00019128
Iteration 9/1000 | Loss: 0.00010475
Iteration 10/1000 | Loss: 0.00009731
Iteration 11/1000 | Loss: 0.00026161
Iteration 12/1000 | Loss: 0.00009630
Iteration 13/1000 | Loss: 0.00008888
Iteration 14/1000 | Loss: 0.00008467
Iteration 15/1000 | Loss: 0.00008289
Iteration 16/1000 | Loss: 0.00008161
Iteration 17/1000 | Loss: 0.00019394
Iteration 18/1000 | Loss: 0.00008315
Iteration 19/1000 | Loss: 0.00007957
Iteration 20/1000 | Loss: 0.00007780
Iteration 21/1000 | Loss: 0.00023258
Iteration 22/1000 | Loss: 0.00008208
Iteration 23/1000 | Loss: 0.00007623
Iteration 24/1000 | Loss: 0.00007432
Iteration 25/1000 | Loss: 0.00007334
Iteration 26/1000 | Loss: 0.00007261
Iteration 27/1000 | Loss: 0.00021771
Iteration 28/1000 | Loss: 0.00007834
Iteration 29/1000 | Loss: 0.00007200
Iteration 30/1000 | Loss: 0.00007045
Iteration 31/1000 | Loss: 0.00006911
Iteration 32/1000 | Loss: 0.00006848
Iteration 33/1000 | Loss: 0.00006788
Iteration 34/1000 | Loss: 0.00006749
Iteration 35/1000 | Loss: 0.00026316
Iteration 36/1000 | Loss: 0.00064259
Iteration 37/1000 | Loss: 0.00006940
Iteration 38/1000 | Loss: 0.00006302
Iteration 39/1000 | Loss: 0.00005892
Iteration 40/1000 | Loss: 0.00005698
Iteration 41/1000 | Loss: 0.00005586
Iteration 42/1000 | Loss: 0.00005497
Iteration 43/1000 | Loss: 0.00005462
Iteration 44/1000 | Loss: 0.00025432
Iteration 45/1000 | Loss: 0.00006491
Iteration 46/1000 | Loss: 0.00005471
Iteration 47/1000 | Loss: 0.00005235
Iteration 48/1000 | Loss: 0.00005092
Iteration 49/1000 | Loss: 0.00005020
Iteration 50/1000 | Loss: 0.00004979
Iteration 51/1000 | Loss: 0.00004962
Iteration 52/1000 | Loss: 0.00004947
Iteration 53/1000 | Loss: 0.00004944
Iteration 54/1000 | Loss: 0.00004942
Iteration 55/1000 | Loss: 0.00004941
Iteration 56/1000 | Loss: 0.00004941
Iteration 57/1000 | Loss: 0.00004941
Iteration 58/1000 | Loss: 0.00004940
Iteration 59/1000 | Loss: 0.00004939
Iteration 60/1000 | Loss: 0.00004939
Iteration 61/1000 | Loss: 0.00004939
Iteration 62/1000 | Loss: 0.00004939
Iteration 63/1000 | Loss: 0.00004939
Iteration 64/1000 | Loss: 0.00004939
Iteration 65/1000 | Loss: 0.00004939
Iteration 66/1000 | Loss: 0.00004938
Iteration 67/1000 | Loss: 0.00004938
Iteration 68/1000 | Loss: 0.00004937
Iteration 69/1000 | Loss: 0.00004936
Iteration 70/1000 | Loss: 0.00004936
Iteration 71/1000 | Loss: 0.00004936
Iteration 72/1000 | Loss: 0.00004936
Iteration 73/1000 | Loss: 0.00004936
Iteration 74/1000 | Loss: 0.00004935
Iteration 75/1000 | Loss: 0.00004935
Iteration 76/1000 | Loss: 0.00004934
Iteration 77/1000 | Loss: 0.00004934
Iteration 78/1000 | Loss: 0.00004934
Iteration 79/1000 | Loss: 0.00004933
Iteration 80/1000 | Loss: 0.00004933
Iteration 81/1000 | Loss: 0.00004932
Iteration 82/1000 | Loss: 0.00004930
Iteration 83/1000 | Loss: 0.00004929
Iteration 84/1000 | Loss: 0.00004929
Iteration 85/1000 | Loss: 0.00004928
Iteration 86/1000 | Loss: 0.00004928
Iteration 87/1000 | Loss: 0.00004928
Iteration 88/1000 | Loss: 0.00004927
Iteration 89/1000 | Loss: 0.00004927
Iteration 90/1000 | Loss: 0.00004927
Iteration 91/1000 | Loss: 0.00004926
Iteration 92/1000 | Loss: 0.00004926
Iteration 93/1000 | Loss: 0.00004926
Iteration 94/1000 | Loss: 0.00004926
Iteration 95/1000 | Loss: 0.00004926
Iteration 96/1000 | Loss: 0.00004926
Iteration 97/1000 | Loss: 0.00004926
Iteration 98/1000 | Loss: 0.00004926
Iteration 99/1000 | Loss: 0.00004926
Iteration 100/1000 | Loss: 0.00004926
Iteration 101/1000 | Loss: 0.00004926
Iteration 102/1000 | Loss: 0.00004926
Iteration 103/1000 | Loss: 0.00004925
Iteration 104/1000 | Loss: 0.00004925
Iteration 105/1000 | Loss: 0.00004924
Iteration 106/1000 | Loss: 0.00004924
Iteration 107/1000 | Loss: 0.00004923
Iteration 108/1000 | Loss: 0.00004923
Iteration 109/1000 | Loss: 0.00004923
Iteration 110/1000 | Loss: 0.00004922
Iteration 111/1000 | Loss: 0.00004922
Iteration 112/1000 | Loss: 0.00004921
Iteration 113/1000 | Loss: 0.00004921
Iteration 114/1000 | Loss: 0.00004919
Iteration 115/1000 | Loss: 0.00004919
Iteration 116/1000 | Loss: 0.00004919
Iteration 117/1000 | Loss: 0.00004919
Iteration 118/1000 | Loss: 0.00004919
Iteration 119/1000 | Loss: 0.00004919
Iteration 120/1000 | Loss: 0.00004918
Iteration 121/1000 | Loss: 0.00004918
Iteration 122/1000 | Loss: 0.00004918
Iteration 123/1000 | Loss: 0.00004918
Iteration 124/1000 | Loss: 0.00004918
Iteration 125/1000 | Loss: 0.00004918
Iteration 126/1000 | Loss: 0.00004918
Iteration 127/1000 | Loss: 0.00004918
Iteration 128/1000 | Loss: 0.00004917
Iteration 129/1000 | Loss: 0.00004917
Iteration 130/1000 | Loss: 0.00004917
Iteration 131/1000 | Loss: 0.00004916
Iteration 132/1000 | Loss: 0.00004916
Iteration 133/1000 | Loss: 0.00004916
Iteration 134/1000 | Loss: 0.00004916
Iteration 135/1000 | Loss: 0.00004916
Iteration 136/1000 | Loss: 0.00004916
Iteration 137/1000 | Loss: 0.00004915
Iteration 138/1000 | Loss: 0.00004915
Iteration 139/1000 | Loss: 0.00004914
Iteration 140/1000 | Loss: 0.00004914
Iteration 141/1000 | Loss: 0.00004914
Iteration 142/1000 | Loss: 0.00004912
Iteration 143/1000 | Loss: 0.00004912
Iteration 144/1000 | Loss: 0.00004912
Iteration 145/1000 | Loss: 0.00004912
Iteration 146/1000 | Loss: 0.00004911
Iteration 147/1000 | Loss: 0.00004911
Iteration 148/1000 | Loss: 0.00004911
Iteration 149/1000 | Loss: 0.00004911
Iteration 150/1000 | Loss: 0.00004911
Iteration 151/1000 | Loss: 0.00004911
Iteration 152/1000 | Loss: 0.00004909
Iteration 153/1000 | Loss: 0.00004908
Iteration 154/1000 | Loss: 0.00004908
Iteration 155/1000 | Loss: 0.00004908
Iteration 156/1000 | Loss: 0.00004908
Iteration 157/1000 | Loss: 0.00004907
Iteration 158/1000 | Loss: 0.00004907
Iteration 159/1000 | Loss: 0.00004907
Iteration 160/1000 | Loss: 0.00004907
Iteration 161/1000 | Loss: 0.00004906
Iteration 162/1000 | Loss: 0.00004906
Iteration 163/1000 | Loss: 0.00004906
Iteration 164/1000 | Loss: 0.00004906
Iteration 165/1000 | Loss: 0.00004906
Iteration 166/1000 | Loss: 0.00004906
Iteration 167/1000 | Loss: 0.00004906
Iteration 168/1000 | Loss: 0.00004906
Iteration 169/1000 | Loss: 0.00004906
Iteration 170/1000 | Loss: 0.00004906
Iteration 171/1000 | Loss: 0.00004905
Iteration 172/1000 | Loss: 0.00004905
Iteration 173/1000 | Loss: 0.00004905
Iteration 174/1000 | Loss: 0.00004905
Iteration 175/1000 | Loss: 0.00004905
Iteration 176/1000 | Loss: 0.00004905
Iteration 177/1000 | Loss: 0.00004905
Iteration 178/1000 | Loss: 0.00004905
Iteration 179/1000 | Loss: 0.00004905
Iteration 180/1000 | Loss: 0.00004905
Iteration 181/1000 | Loss: 0.00004904
Iteration 182/1000 | Loss: 0.00004904
Iteration 183/1000 | Loss: 0.00004904
Iteration 184/1000 | Loss: 0.00004904
Iteration 185/1000 | Loss: 0.00004904
Iteration 186/1000 | Loss: 0.00004904
Iteration 187/1000 | Loss: 0.00004904
Iteration 188/1000 | Loss: 0.00004904
Iteration 189/1000 | Loss: 0.00004904
Iteration 190/1000 | Loss: 0.00004903
Iteration 191/1000 | Loss: 0.00004903
Iteration 192/1000 | Loss: 0.00004903
Iteration 193/1000 | Loss: 0.00004903
Iteration 194/1000 | Loss: 0.00004902
Iteration 195/1000 | Loss: 0.00004902
Iteration 196/1000 | Loss: 0.00004902
Iteration 197/1000 | Loss: 0.00004902
Iteration 198/1000 | Loss: 0.00004902
Iteration 199/1000 | Loss: 0.00004902
Iteration 200/1000 | Loss: 0.00004902
Iteration 201/1000 | Loss: 0.00004901
Iteration 202/1000 | Loss: 0.00004901
Iteration 203/1000 | Loss: 0.00004901
Iteration 204/1000 | Loss: 0.00004901
Iteration 205/1000 | Loss: 0.00004901
Iteration 206/1000 | Loss: 0.00004901
Iteration 207/1000 | Loss: 0.00004901
Iteration 208/1000 | Loss: 0.00004901
Iteration 209/1000 | Loss: 0.00004901
Iteration 210/1000 | Loss: 0.00004901
Iteration 211/1000 | Loss: 0.00004901
Iteration 212/1000 | Loss: 0.00004901
Iteration 213/1000 | Loss: 0.00004901
Iteration 214/1000 | Loss: 0.00004901
Iteration 215/1000 | Loss: 0.00004900
Iteration 216/1000 | Loss: 0.00004900
Iteration 217/1000 | Loss: 0.00004900
Iteration 218/1000 | Loss: 0.00004900
Iteration 219/1000 | Loss: 0.00004900
Iteration 220/1000 | Loss: 0.00004900
Iteration 221/1000 | Loss: 0.00004900
Iteration 222/1000 | Loss: 0.00004900
Iteration 223/1000 | Loss: 0.00004900
Iteration 224/1000 | Loss: 0.00004900
Iteration 225/1000 | Loss: 0.00004900
Iteration 226/1000 | Loss: 0.00004900
Iteration 227/1000 | Loss: 0.00004900
Iteration 228/1000 | Loss: 0.00004900
Iteration 229/1000 | Loss: 0.00004899
Iteration 230/1000 | Loss: 0.00004899
Iteration 231/1000 | Loss: 0.00004899
Iteration 232/1000 | Loss: 0.00004899
Iteration 233/1000 | Loss: 0.00004899
Iteration 234/1000 | Loss: 0.00004899
Iteration 235/1000 | Loss: 0.00004898
Iteration 236/1000 | Loss: 0.00004898
Iteration 237/1000 | Loss: 0.00004898
Iteration 238/1000 | Loss: 0.00004898
Iteration 239/1000 | Loss: 0.00004898
Iteration 240/1000 | Loss: 0.00004898
Iteration 241/1000 | Loss: 0.00004897
Iteration 242/1000 | Loss: 0.00004897
Iteration 243/1000 | Loss: 0.00004897
Iteration 244/1000 | Loss: 0.00004897
Iteration 245/1000 | Loss: 0.00004897
Iteration 246/1000 | Loss: 0.00004897
Iteration 247/1000 | Loss: 0.00004897
Iteration 248/1000 | Loss: 0.00004896
Iteration 249/1000 | Loss: 0.00004896
Iteration 250/1000 | Loss: 0.00004896
Iteration 251/1000 | Loss: 0.00004896
Iteration 252/1000 | Loss: 0.00004896
Iteration 253/1000 | Loss: 0.00004896
Iteration 254/1000 | Loss: 0.00004896
Iteration 255/1000 | Loss: 0.00004896
Iteration 256/1000 | Loss: 0.00004896
Iteration 257/1000 | Loss: 0.00004896
Iteration 258/1000 | Loss: 0.00004896
Iteration 259/1000 | Loss: 0.00004896
Iteration 260/1000 | Loss: 0.00004896
Iteration 261/1000 | Loss: 0.00004895
Iteration 262/1000 | Loss: 0.00004895
Iteration 263/1000 | Loss: 0.00004895
Iteration 264/1000 | Loss: 0.00004895
Iteration 265/1000 | Loss: 0.00004895
Iteration 266/1000 | Loss: 0.00004895
Iteration 267/1000 | Loss: 0.00004895
Iteration 268/1000 | Loss: 0.00004895
Iteration 269/1000 | Loss: 0.00004895
Iteration 270/1000 | Loss: 0.00004894
Iteration 271/1000 | Loss: 0.00004894
Iteration 272/1000 | Loss: 0.00004894
Iteration 273/1000 | Loss: 0.00004894
Iteration 274/1000 | Loss: 0.00004894
Iteration 275/1000 | Loss: 0.00004894
Iteration 276/1000 | Loss: 0.00004894
Iteration 277/1000 | Loss: 0.00004893
Iteration 278/1000 | Loss: 0.00004893
Iteration 279/1000 | Loss: 0.00004893
Iteration 280/1000 | Loss: 0.00004892
Iteration 281/1000 | Loss: 0.00004892
Iteration 282/1000 | Loss: 0.00004892
Iteration 283/1000 | Loss: 0.00004892
Iteration 284/1000 | Loss: 0.00004892
Iteration 285/1000 | Loss: 0.00004891
Iteration 286/1000 | Loss: 0.00004891
Iteration 287/1000 | Loss: 0.00004891
Iteration 288/1000 | Loss: 0.00004891
Iteration 289/1000 | Loss: 0.00004891
Iteration 290/1000 | Loss: 0.00004891
Iteration 291/1000 | Loss: 0.00004891
Iteration 292/1000 | Loss: 0.00004891
Iteration 293/1000 | Loss: 0.00004890
Iteration 294/1000 | Loss: 0.00004890
Iteration 295/1000 | Loss: 0.00004890
Iteration 296/1000 | Loss: 0.00004890
Iteration 297/1000 | Loss: 0.00004890
Iteration 298/1000 | Loss: 0.00004890
Iteration 299/1000 | Loss: 0.00004890
Iteration 300/1000 | Loss: 0.00004890
Iteration 301/1000 | Loss: 0.00004890
Iteration 302/1000 | Loss: 0.00004890
Iteration 303/1000 | Loss: 0.00004890
Iteration 304/1000 | Loss: 0.00004890
Iteration 305/1000 | Loss: 0.00004890
Iteration 306/1000 | Loss: 0.00004890
Iteration 307/1000 | Loss: 0.00004890
Iteration 308/1000 | Loss: 0.00004890
Iteration 309/1000 | Loss: 0.00004890
Iteration 310/1000 | Loss: 0.00004890
Iteration 311/1000 | Loss: 0.00004890
Iteration 312/1000 | Loss: 0.00004890
Iteration 313/1000 | Loss: 0.00004890
Iteration 314/1000 | Loss: 0.00004890
Iteration 315/1000 | Loss: 0.00004890
Iteration 316/1000 | Loss: 0.00004890
Iteration 317/1000 | Loss: 0.00004890
Iteration 318/1000 | Loss: 0.00004890
Iteration 319/1000 | Loss: 0.00004890
Iteration 320/1000 | Loss: 0.00004890
Iteration 321/1000 | Loss: 0.00004890
Iteration 322/1000 | Loss: 0.00004890
Iteration 323/1000 | Loss: 0.00004890
Iteration 324/1000 | Loss: 0.00004890
Iteration 325/1000 | Loss: 0.00004890
Iteration 326/1000 | Loss: 0.00004890
Iteration 327/1000 | Loss: 0.00004890
Iteration 328/1000 | Loss: 0.00004890
Iteration 329/1000 | Loss: 0.00004890
Iteration 330/1000 | Loss: 0.00004890
Iteration 331/1000 | Loss: 0.00004890
Iteration 332/1000 | Loss: 0.00004890
Iteration 333/1000 | Loss: 0.00004890
Iteration 334/1000 | Loss: 0.00004890
Iteration 335/1000 | Loss: 0.00004890
Iteration 336/1000 | Loss: 0.00004890
Iteration 337/1000 | Loss: 0.00004890
Iteration 338/1000 | Loss: 0.00004890
Iteration 339/1000 | Loss: 0.00004890
Iteration 340/1000 | Loss: 0.00004890
Iteration 341/1000 | Loss: 0.00004890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [4.889558476861566e-05, 4.889558476861566e-05, 4.889558476861566e-05, 4.889558476861566e-05, 4.889558476861566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.889558476861566e-05

Optimization complete. Final v2v error: 5.057786464691162 mm

Highest mean error: 12.851025581359863 mm for frame 23

Lowest mean error: 4.076300621032715 mm for frame 83

Saving results

Total time: 114.43019700050354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819843
Iteration 2/25 | Loss: 0.00171776
Iteration 3/25 | Loss: 0.00143294
Iteration 4/25 | Loss: 0.00139217
Iteration 5/25 | Loss: 0.00139004
Iteration 6/25 | Loss: 0.00138219
Iteration 7/25 | Loss: 0.00137396
Iteration 8/25 | Loss: 0.00137086
Iteration 9/25 | Loss: 0.00137010
Iteration 10/25 | Loss: 0.00136990
Iteration 11/25 | Loss: 0.00136983
Iteration 12/25 | Loss: 0.00136982
Iteration 13/25 | Loss: 0.00136982
Iteration 14/25 | Loss: 0.00136982
Iteration 15/25 | Loss: 0.00136982
Iteration 16/25 | Loss: 0.00136981
Iteration 17/25 | Loss: 0.00136981
Iteration 18/25 | Loss: 0.00136981
Iteration 19/25 | Loss: 0.00136981
Iteration 20/25 | Loss: 0.00136981
Iteration 21/25 | Loss: 0.00136981
Iteration 22/25 | Loss: 0.00136981
Iteration 23/25 | Loss: 0.00136981
Iteration 24/25 | Loss: 0.00136981
Iteration 25/25 | Loss: 0.00136981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44187522
Iteration 2/25 | Loss: 0.00092522
Iteration 3/25 | Loss: 0.00092521
Iteration 4/25 | Loss: 0.00092521
Iteration 5/25 | Loss: 0.00092521
Iteration 6/25 | Loss: 0.00092521
Iteration 7/25 | Loss: 0.00092521
Iteration 8/25 | Loss: 0.00092521
Iteration 9/25 | Loss: 0.00092521
Iteration 10/25 | Loss: 0.00092521
Iteration 11/25 | Loss: 0.00092521
Iteration 12/25 | Loss: 0.00092521
Iteration 13/25 | Loss: 0.00092521
Iteration 14/25 | Loss: 0.00092521
Iteration 15/25 | Loss: 0.00092521
Iteration 16/25 | Loss: 0.00092521
Iteration 17/25 | Loss: 0.00092521
Iteration 18/25 | Loss: 0.00092521
Iteration 19/25 | Loss: 0.00092521
Iteration 20/25 | Loss: 0.00092521
Iteration 21/25 | Loss: 0.00092521
Iteration 22/25 | Loss: 0.00092521
Iteration 23/25 | Loss: 0.00092521
Iteration 24/25 | Loss: 0.00092521
Iteration 25/25 | Loss: 0.00092521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092521
Iteration 2/1000 | Loss: 0.00004729
Iteration 3/1000 | Loss: 0.00003309
Iteration 4/1000 | Loss: 0.00002979
Iteration 5/1000 | Loss: 0.00002807
Iteration 6/1000 | Loss: 0.00002703
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002528
Iteration 9/1000 | Loss: 0.00002479
Iteration 10/1000 | Loss: 0.00002449
Iteration 11/1000 | Loss: 0.00002413
Iteration 12/1000 | Loss: 0.00002388
Iteration 13/1000 | Loss: 0.00002371
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002335
Iteration 16/1000 | Loss: 0.00002335
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002327
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002316
Iteration 23/1000 | Loss: 0.00002314
Iteration 24/1000 | Loss: 0.00002311
Iteration 25/1000 | Loss: 0.00002310
Iteration 26/1000 | Loss: 0.00002306
Iteration 27/1000 | Loss: 0.00002306
Iteration 28/1000 | Loss: 0.00002306
Iteration 29/1000 | Loss: 0.00002305
Iteration 30/1000 | Loss: 0.00002305
Iteration 31/1000 | Loss: 0.00002302
Iteration 32/1000 | Loss: 0.00002301
Iteration 33/1000 | Loss: 0.00002301
Iteration 34/1000 | Loss: 0.00002300
Iteration 35/1000 | Loss: 0.00002299
Iteration 36/1000 | Loss: 0.00002298
Iteration 37/1000 | Loss: 0.00002298
Iteration 38/1000 | Loss: 0.00002298
Iteration 39/1000 | Loss: 0.00002298
Iteration 40/1000 | Loss: 0.00002298
Iteration 41/1000 | Loss: 0.00002298
Iteration 42/1000 | Loss: 0.00002297
Iteration 43/1000 | Loss: 0.00002297
Iteration 44/1000 | Loss: 0.00002297
Iteration 45/1000 | Loss: 0.00002295
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002295
Iteration 48/1000 | Loss: 0.00002294
Iteration 49/1000 | Loss: 0.00002294
Iteration 50/1000 | Loss: 0.00002294
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002293
Iteration 53/1000 | Loss: 0.00002293
Iteration 54/1000 | Loss: 0.00002292
Iteration 55/1000 | Loss: 0.00002292
Iteration 56/1000 | Loss: 0.00002292
Iteration 57/1000 | Loss: 0.00002292
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002290
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002289
Iteration 66/1000 | Loss: 0.00002289
Iteration 67/1000 | Loss: 0.00002289
Iteration 68/1000 | Loss: 0.00002289
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002288
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002287
Iteration 74/1000 | Loss: 0.00002287
Iteration 75/1000 | Loss: 0.00002287
Iteration 76/1000 | Loss: 0.00002287
Iteration 77/1000 | Loss: 0.00002287
Iteration 78/1000 | Loss: 0.00002287
Iteration 79/1000 | Loss: 0.00002286
Iteration 80/1000 | Loss: 0.00002286
Iteration 81/1000 | Loss: 0.00002286
Iteration 82/1000 | Loss: 0.00002286
Iteration 83/1000 | Loss: 0.00002285
Iteration 84/1000 | Loss: 0.00002285
Iteration 85/1000 | Loss: 0.00002285
Iteration 86/1000 | Loss: 0.00002285
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002285
Iteration 89/1000 | Loss: 0.00002285
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002284
Iteration 98/1000 | Loss: 0.00002284
Iteration 99/1000 | Loss: 0.00002284
Iteration 100/1000 | Loss: 0.00002284
Iteration 101/1000 | Loss: 0.00002284
Iteration 102/1000 | Loss: 0.00002284
Iteration 103/1000 | Loss: 0.00002283
Iteration 104/1000 | Loss: 0.00002283
Iteration 105/1000 | Loss: 0.00002283
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002282
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002281
Iteration 116/1000 | Loss: 0.00002281
Iteration 117/1000 | Loss: 0.00002281
Iteration 118/1000 | Loss: 0.00002281
Iteration 119/1000 | Loss: 0.00002280
Iteration 120/1000 | Loss: 0.00002280
Iteration 121/1000 | Loss: 0.00002280
Iteration 122/1000 | Loss: 0.00002280
Iteration 123/1000 | Loss: 0.00002280
Iteration 124/1000 | Loss: 0.00002280
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.2799367798143066e-05, 2.2799367798143066e-05, 2.2799367798143066e-05, 2.2799367798143066e-05, 2.2799367798143066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2799367798143066e-05

Optimization complete. Final v2v error: 3.955146312713623 mm

Highest mean error: 5.138442039489746 mm for frame 57

Lowest mean error: 3.1649274826049805 mm for frame 75

Saving results

Total time: 50.7472140789032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815932
Iteration 2/25 | Loss: 0.00143154
Iteration 3/25 | Loss: 0.00132533
Iteration 4/25 | Loss: 0.00131068
Iteration 5/25 | Loss: 0.00130612
Iteration 6/25 | Loss: 0.00130562
Iteration 7/25 | Loss: 0.00130562
Iteration 8/25 | Loss: 0.00130562
Iteration 9/25 | Loss: 0.00130562
Iteration 10/25 | Loss: 0.00130562
Iteration 11/25 | Loss: 0.00130562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013056200696155429, 0.0013056200696155429, 0.0013056200696155429, 0.0013056200696155429, 0.0013056200696155429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013056200696155429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68811321
Iteration 2/25 | Loss: 0.00084560
Iteration 3/25 | Loss: 0.00084559
Iteration 4/25 | Loss: 0.00084559
Iteration 5/25 | Loss: 0.00084559
Iteration 6/25 | Loss: 0.00084559
Iteration 7/25 | Loss: 0.00084559
Iteration 8/25 | Loss: 0.00084559
Iteration 9/25 | Loss: 0.00084559
Iteration 10/25 | Loss: 0.00084559
Iteration 11/25 | Loss: 0.00084559
Iteration 12/25 | Loss: 0.00084559
Iteration 13/25 | Loss: 0.00084559
Iteration 14/25 | Loss: 0.00084559
Iteration 15/25 | Loss: 0.00084559
Iteration 16/25 | Loss: 0.00084559
Iteration 17/25 | Loss: 0.00084559
Iteration 18/25 | Loss: 0.00084559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008455907227471471, 0.0008455907227471471, 0.0008455907227471471, 0.0008455907227471471, 0.0008455907227471471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008455907227471471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084559
Iteration 2/1000 | Loss: 0.00004355
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002236
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00001998
Iteration 9/1000 | Loss: 0.00001965
Iteration 10/1000 | Loss: 0.00001932
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00001889
Iteration 13/1000 | Loss: 0.00001872
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001843
Iteration 17/1000 | Loss: 0.00001841
Iteration 18/1000 | Loss: 0.00001837
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001821
Iteration 24/1000 | Loss: 0.00001821
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001814
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001810
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001807
Iteration 48/1000 | Loss: 0.00001806
Iteration 49/1000 | Loss: 0.00001806
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001804
Iteration 56/1000 | Loss: 0.00001804
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001803
Iteration 59/1000 | Loss: 0.00001803
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001802
Iteration 62/1000 | Loss: 0.00001802
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001801
Iteration 65/1000 | Loss: 0.00001801
Iteration 66/1000 | Loss: 0.00001800
Iteration 67/1000 | Loss: 0.00001800
Iteration 68/1000 | Loss: 0.00001800
Iteration 69/1000 | Loss: 0.00001800
Iteration 70/1000 | Loss: 0.00001800
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001799
Iteration 73/1000 | Loss: 0.00001799
Iteration 74/1000 | Loss: 0.00001799
Iteration 75/1000 | Loss: 0.00001799
Iteration 76/1000 | Loss: 0.00001798
Iteration 77/1000 | Loss: 0.00001798
Iteration 78/1000 | Loss: 0.00001797
Iteration 79/1000 | Loss: 0.00001797
Iteration 80/1000 | Loss: 0.00001797
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001795
Iteration 87/1000 | Loss: 0.00001795
Iteration 88/1000 | Loss: 0.00001795
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00001795
Iteration 91/1000 | Loss: 0.00001794
Iteration 92/1000 | Loss: 0.00001794
Iteration 93/1000 | Loss: 0.00001794
Iteration 94/1000 | Loss: 0.00001793
Iteration 95/1000 | Loss: 0.00001793
Iteration 96/1000 | Loss: 0.00001793
Iteration 97/1000 | Loss: 0.00001793
Iteration 98/1000 | Loss: 0.00001793
Iteration 99/1000 | Loss: 0.00001792
Iteration 100/1000 | Loss: 0.00001792
Iteration 101/1000 | Loss: 0.00001792
Iteration 102/1000 | Loss: 0.00001792
Iteration 103/1000 | Loss: 0.00001792
Iteration 104/1000 | Loss: 0.00001792
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001791
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001791
Iteration 111/1000 | Loss: 0.00001791
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001791
Iteration 114/1000 | Loss: 0.00001790
Iteration 115/1000 | Loss: 0.00001790
Iteration 116/1000 | Loss: 0.00001790
Iteration 117/1000 | Loss: 0.00001790
Iteration 118/1000 | Loss: 0.00001789
Iteration 119/1000 | Loss: 0.00001789
Iteration 120/1000 | Loss: 0.00001789
Iteration 121/1000 | Loss: 0.00001789
Iteration 122/1000 | Loss: 0.00001788
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001788
Iteration 125/1000 | Loss: 0.00001788
Iteration 126/1000 | Loss: 0.00001788
Iteration 127/1000 | Loss: 0.00001788
Iteration 128/1000 | Loss: 0.00001788
Iteration 129/1000 | Loss: 0.00001788
Iteration 130/1000 | Loss: 0.00001788
Iteration 131/1000 | Loss: 0.00001788
Iteration 132/1000 | Loss: 0.00001788
Iteration 133/1000 | Loss: 0.00001788
Iteration 134/1000 | Loss: 0.00001788
Iteration 135/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.7878621292766184e-05, 1.7878621292766184e-05, 1.7878621292766184e-05, 1.7878621292766184e-05, 1.7878621292766184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7878621292766184e-05

Optimization complete. Final v2v error: 3.5449514389038086 mm

Highest mean error: 4.679562568664551 mm for frame 70

Lowest mean error: 2.9768710136413574 mm for frame 10

Saving results

Total time: 40.79203963279724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079799
Iteration 2/25 | Loss: 0.00171395
Iteration 3/25 | Loss: 0.00146573
Iteration 4/25 | Loss: 0.00144393
Iteration 5/25 | Loss: 0.00144213
Iteration 6/25 | Loss: 0.00144209
Iteration 7/25 | Loss: 0.00144209
Iteration 8/25 | Loss: 0.00144209
Iteration 9/25 | Loss: 0.00144209
Iteration 10/25 | Loss: 0.00144209
Iteration 11/25 | Loss: 0.00144209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014420909574255347, 0.0014420909574255347, 0.0014420909574255347, 0.0014420909574255347, 0.0014420909574255347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014420909574255347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17634249
Iteration 2/25 | Loss: 0.00187643
Iteration 3/25 | Loss: 0.00187643
Iteration 4/25 | Loss: 0.00187643
Iteration 5/25 | Loss: 0.00187643
Iteration 6/25 | Loss: 0.00187643
Iteration 7/25 | Loss: 0.00187642
Iteration 8/25 | Loss: 0.00187642
Iteration 9/25 | Loss: 0.00187642
Iteration 10/25 | Loss: 0.00187642
Iteration 11/25 | Loss: 0.00187642
Iteration 12/25 | Loss: 0.00187642
Iteration 13/25 | Loss: 0.00187642
Iteration 14/25 | Loss: 0.00187642
Iteration 15/25 | Loss: 0.00187642
Iteration 16/25 | Loss: 0.00187642
Iteration 17/25 | Loss: 0.00187642
Iteration 18/25 | Loss: 0.00187642
Iteration 19/25 | Loss: 0.00187642
Iteration 20/25 | Loss: 0.00187642
Iteration 21/25 | Loss: 0.00187642
Iteration 22/25 | Loss: 0.00187642
Iteration 23/25 | Loss: 0.00187642
Iteration 24/25 | Loss: 0.00187642
Iteration 25/25 | Loss: 0.00187642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187642
Iteration 2/1000 | Loss: 0.00004760
Iteration 3/1000 | Loss: 0.00003259
Iteration 4/1000 | Loss: 0.00002954
Iteration 5/1000 | Loss: 0.00002805
Iteration 6/1000 | Loss: 0.00002723
Iteration 7/1000 | Loss: 0.00002617
Iteration 8/1000 | Loss: 0.00002571
Iteration 9/1000 | Loss: 0.00002529
Iteration 10/1000 | Loss: 0.00002504
Iteration 11/1000 | Loss: 0.00002479
Iteration 12/1000 | Loss: 0.00002471
Iteration 13/1000 | Loss: 0.00002468
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002467
Iteration 16/1000 | Loss: 0.00002465
Iteration 17/1000 | Loss: 0.00002465
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00002459
Iteration 20/1000 | Loss: 0.00002453
Iteration 21/1000 | Loss: 0.00002453
Iteration 22/1000 | Loss: 0.00002452
Iteration 23/1000 | Loss: 0.00002452
Iteration 24/1000 | Loss: 0.00002449
Iteration 25/1000 | Loss: 0.00002449
Iteration 26/1000 | Loss: 0.00002442
Iteration 27/1000 | Loss: 0.00002442
Iteration 28/1000 | Loss: 0.00002442
Iteration 29/1000 | Loss: 0.00002442
Iteration 30/1000 | Loss: 0.00002442
Iteration 31/1000 | Loss: 0.00002442
Iteration 32/1000 | Loss: 0.00002442
Iteration 33/1000 | Loss: 0.00002442
Iteration 34/1000 | Loss: 0.00002442
Iteration 35/1000 | Loss: 0.00002442
Iteration 36/1000 | Loss: 0.00002442
Iteration 37/1000 | Loss: 0.00002442
Iteration 38/1000 | Loss: 0.00002442
Iteration 39/1000 | Loss: 0.00002442
Iteration 40/1000 | Loss: 0.00002442
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002442
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00002439
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002437
Iteration 48/1000 | Loss: 0.00002437
Iteration 49/1000 | Loss: 0.00002437
Iteration 50/1000 | Loss: 0.00002437
Iteration 51/1000 | Loss: 0.00002436
Iteration 52/1000 | Loss: 0.00002435
Iteration 53/1000 | Loss: 0.00002435
Iteration 54/1000 | Loss: 0.00002435
Iteration 55/1000 | Loss: 0.00002435
Iteration 56/1000 | Loss: 0.00002434
Iteration 57/1000 | Loss: 0.00002434
Iteration 58/1000 | Loss: 0.00002433
Iteration 59/1000 | Loss: 0.00002433
Iteration 60/1000 | Loss: 0.00002433
Iteration 61/1000 | Loss: 0.00002433
Iteration 62/1000 | Loss: 0.00002433
Iteration 63/1000 | Loss: 0.00002433
Iteration 64/1000 | Loss: 0.00002433
Iteration 65/1000 | Loss: 0.00002433
Iteration 66/1000 | Loss: 0.00002433
Iteration 67/1000 | Loss: 0.00002432
Iteration 68/1000 | Loss: 0.00002432
Iteration 69/1000 | Loss: 0.00002432
Iteration 70/1000 | Loss: 0.00002431
Iteration 71/1000 | Loss: 0.00002431
Iteration 72/1000 | Loss: 0.00002431
Iteration 73/1000 | Loss: 0.00002430
Iteration 74/1000 | Loss: 0.00002430
Iteration 75/1000 | Loss: 0.00002430
Iteration 76/1000 | Loss: 0.00002429
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002429
Iteration 79/1000 | Loss: 0.00002429
Iteration 80/1000 | Loss: 0.00002429
Iteration 81/1000 | Loss: 0.00002429
Iteration 82/1000 | Loss: 0.00002429
Iteration 83/1000 | Loss: 0.00002429
Iteration 84/1000 | Loss: 0.00002429
Iteration 85/1000 | Loss: 0.00002429
Iteration 86/1000 | Loss: 0.00002429
Iteration 87/1000 | Loss: 0.00002429
Iteration 88/1000 | Loss: 0.00002429
Iteration 89/1000 | Loss: 0.00002429
Iteration 90/1000 | Loss: 0.00002428
Iteration 91/1000 | Loss: 0.00002428
Iteration 92/1000 | Loss: 0.00002428
Iteration 93/1000 | Loss: 0.00002428
Iteration 94/1000 | Loss: 0.00002428
Iteration 95/1000 | Loss: 0.00002428
Iteration 96/1000 | Loss: 0.00002427
Iteration 97/1000 | Loss: 0.00002427
Iteration 98/1000 | Loss: 0.00002427
Iteration 99/1000 | Loss: 0.00002427
Iteration 100/1000 | Loss: 0.00002427
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002427
Iteration 104/1000 | Loss: 0.00002427
Iteration 105/1000 | Loss: 0.00002427
Iteration 106/1000 | Loss: 0.00002427
Iteration 107/1000 | Loss: 0.00002427
Iteration 108/1000 | Loss: 0.00002427
Iteration 109/1000 | Loss: 0.00002427
Iteration 110/1000 | Loss: 0.00002427
Iteration 111/1000 | Loss: 0.00002427
Iteration 112/1000 | Loss: 0.00002427
Iteration 113/1000 | Loss: 0.00002427
Iteration 114/1000 | Loss: 0.00002427
Iteration 115/1000 | Loss: 0.00002427
Iteration 116/1000 | Loss: 0.00002427
Iteration 117/1000 | Loss: 0.00002427
Iteration 118/1000 | Loss: 0.00002427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.4271304937428795e-05, 2.4271304937428795e-05, 2.4271304937428795e-05, 2.4271304937428795e-05, 2.4271304937428795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4271304937428795e-05

Optimization complete. Final v2v error: 4.224697113037109 mm

Highest mean error: 4.458927154541016 mm for frame 23

Lowest mean error: 3.613407611846924 mm for frame 2

Saving results

Total time: 31.340927362442017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622802
Iteration 2/25 | Loss: 0.00160124
Iteration 3/25 | Loss: 0.00134068
Iteration 4/25 | Loss: 0.00127595
Iteration 5/25 | Loss: 0.00126548
Iteration 6/25 | Loss: 0.00126263
Iteration 7/25 | Loss: 0.00125445
Iteration 8/25 | Loss: 0.00124885
Iteration 9/25 | Loss: 0.00124626
Iteration 10/25 | Loss: 0.00124461
Iteration 11/25 | Loss: 0.00124510
Iteration 12/25 | Loss: 0.00124466
Iteration 13/25 | Loss: 0.00124456
Iteration 14/25 | Loss: 0.00124461
Iteration 15/25 | Loss: 0.00124458
Iteration 16/25 | Loss: 0.00124459
Iteration 17/25 | Loss: 0.00124457
Iteration 18/25 | Loss: 0.00124466
Iteration 19/25 | Loss: 0.00124453
Iteration 20/25 | Loss: 0.00124462
Iteration 21/25 | Loss: 0.00124451
Iteration 22/25 | Loss: 0.00124465
Iteration 23/25 | Loss: 0.00124437
Iteration 24/25 | Loss: 0.00124459
Iteration 25/25 | Loss: 0.00124433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.50564766
Iteration 2/25 | Loss: 0.00209788
Iteration 3/25 | Loss: 0.00209785
Iteration 4/25 | Loss: 0.00209785
Iteration 5/25 | Loss: 0.00209784
Iteration 6/25 | Loss: 0.00209784
Iteration 7/25 | Loss: 0.00209784
Iteration 8/25 | Loss: 0.00209784
Iteration 9/25 | Loss: 0.00209784
Iteration 10/25 | Loss: 0.00209784
Iteration 11/25 | Loss: 0.00209784
Iteration 12/25 | Loss: 0.00209784
Iteration 13/25 | Loss: 0.00209784
Iteration 14/25 | Loss: 0.00209784
Iteration 15/25 | Loss: 0.00209784
Iteration 16/25 | Loss: 0.00209784
Iteration 17/25 | Loss: 0.00209784
Iteration 18/25 | Loss: 0.00209784
Iteration 19/25 | Loss: 0.00209784
Iteration 20/25 | Loss: 0.00209784
Iteration 21/25 | Loss: 0.00209784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002097842050716281, 0.002097842050716281, 0.002097842050716281, 0.002097842050716281, 0.002097842050716281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002097842050716281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209784
Iteration 2/1000 | Loss: 0.00003606
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002071
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001979
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001782
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001720
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001709
Iteration 37/1000 | Loss: 0.00001709
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001707
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001706
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001700
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001699
Iteration 97/1000 | Loss: 0.00001699
Iteration 98/1000 | Loss: 0.00001699
Iteration 99/1000 | Loss: 0.00001699
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001699
Iteration 104/1000 | Loss: 0.00001699
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.6992707969620824e-05, 1.6992707969620824e-05, 1.6992707969620824e-05, 1.6992707969620824e-05, 1.6992707969620824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6992707969620824e-05

Optimization complete. Final v2v error: 3.4497556686401367 mm

Highest mean error: 9.70788288116455 mm for frame 199

Lowest mean error: 2.9657630920410156 mm for frame 227

Saving results

Total time: 82.45238661766052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051457
Iteration 2/25 | Loss: 0.00176600
Iteration 3/25 | Loss: 0.00152466
Iteration 4/25 | Loss: 0.00150926
Iteration 5/25 | Loss: 0.00150497
Iteration 6/25 | Loss: 0.00150285
Iteration 7/25 | Loss: 0.00150243
Iteration 8/25 | Loss: 0.00150243
Iteration 9/25 | Loss: 0.00150243
Iteration 10/25 | Loss: 0.00150243
Iteration 11/25 | Loss: 0.00150243
Iteration 12/25 | Loss: 0.00150243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015024268068373203, 0.0015024268068373203, 0.0015024268068373203, 0.0015024268068373203, 0.0015024268068373203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015024268068373203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15539646
Iteration 2/25 | Loss: 0.00228412
Iteration 3/25 | Loss: 0.00228412
Iteration 4/25 | Loss: 0.00228412
Iteration 5/25 | Loss: 0.00228412
Iteration 6/25 | Loss: 0.00228411
Iteration 7/25 | Loss: 0.00228411
Iteration 8/25 | Loss: 0.00228411
Iteration 9/25 | Loss: 0.00228411
Iteration 10/25 | Loss: 0.00228411
Iteration 11/25 | Loss: 0.00228411
Iteration 12/25 | Loss: 0.00228411
Iteration 13/25 | Loss: 0.00228411
Iteration 14/25 | Loss: 0.00228411
Iteration 15/25 | Loss: 0.00228411
Iteration 16/25 | Loss: 0.00228411
Iteration 17/25 | Loss: 0.00228411
Iteration 18/25 | Loss: 0.00228411
Iteration 19/25 | Loss: 0.00228411
Iteration 20/25 | Loss: 0.00228411
Iteration 21/25 | Loss: 0.00228411
Iteration 22/25 | Loss: 0.00228411
Iteration 23/25 | Loss: 0.00228411
Iteration 24/25 | Loss: 0.00228411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002284113084897399, 0.002284113084897399, 0.002284113084897399, 0.002284113084897399, 0.002284113084897399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002284113084897399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228411
Iteration 2/1000 | Loss: 0.00006651
Iteration 3/1000 | Loss: 0.00004544
Iteration 4/1000 | Loss: 0.00003846
Iteration 5/1000 | Loss: 0.00003622
Iteration 6/1000 | Loss: 0.00003455
Iteration 7/1000 | Loss: 0.00003361
Iteration 8/1000 | Loss: 0.00003251
Iteration 9/1000 | Loss: 0.00003180
Iteration 10/1000 | Loss: 0.00003142
Iteration 11/1000 | Loss: 0.00003119
Iteration 12/1000 | Loss: 0.00003111
Iteration 13/1000 | Loss: 0.00003110
Iteration 14/1000 | Loss: 0.00003110
Iteration 15/1000 | Loss: 0.00003109
Iteration 16/1000 | Loss: 0.00003092
Iteration 17/1000 | Loss: 0.00003083
Iteration 18/1000 | Loss: 0.00003083
Iteration 19/1000 | Loss: 0.00003082
Iteration 20/1000 | Loss: 0.00003081
Iteration 21/1000 | Loss: 0.00003079
Iteration 22/1000 | Loss: 0.00003079
Iteration 23/1000 | Loss: 0.00003079
Iteration 24/1000 | Loss: 0.00003079
Iteration 25/1000 | Loss: 0.00003078
Iteration 26/1000 | Loss: 0.00003078
Iteration 27/1000 | Loss: 0.00003078
Iteration 28/1000 | Loss: 0.00003078
Iteration 29/1000 | Loss: 0.00003078
Iteration 30/1000 | Loss: 0.00003078
Iteration 31/1000 | Loss: 0.00003077
Iteration 32/1000 | Loss: 0.00003077
Iteration 33/1000 | Loss: 0.00003077
Iteration 34/1000 | Loss: 0.00003077
Iteration 35/1000 | Loss: 0.00003077
Iteration 36/1000 | Loss: 0.00003076
Iteration 37/1000 | Loss: 0.00003076
Iteration 38/1000 | Loss: 0.00003075
Iteration 39/1000 | Loss: 0.00003075
Iteration 40/1000 | Loss: 0.00003074
Iteration 41/1000 | Loss: 0.00003074
Iteration 42/1000 | Loss: 0.00003074
Iteration 43/1000 | Loss: 0.00003073
Iteration 44/1000 | Loss: 0.00003073
Iteration 45/1000 | Loss: 0.00003073
Iteration 46/1000 | Loss: 0.00003073
Iteration 47/1000 | Loss: 0.00003073
Iteration 48/1000 | Loss: 0.00003073
Iteration 49/1000 | Loss: 0.00003073
Iteration 50/1000 | Loss: 0.00003073
Iteration 51/1000 | Loss: 0.00003072
Iteration 52/1000 | Loss: 0.00003072
Iteration 53/1000 | Loss: 0.00003072
Iteration 54/1000 | Loss: 0.00003072
Iteration 55/1000 | Loss: 0.00003072
Iteration 56/1000 | Loss: 0.00003071
Iteration 57/1000 | Loss: 0.00003071
Iteration 58/1000 | Loss: 0.00003071
Iteration 59/1000 | Loss: 0.00003071
Iteration 60/1000 | Loss: 0.00003071
Iteration 61/1000 | Loss: 0.00003071
Iteration 62/1000 | Loss: 0.00003071
Iteration 63/1000 | Loss: 0.00003070
Iteration 64/1000 | Loss: 0.00003070
Iteration 65/1000 | Loss: 0.00003070
Iteration 66/1000 | Loss: 0.00003070
Iteration 67/1000 | Loss: 0.00003070
Iteration 68/1000 | Loss: 0.00003070
Iteration 69/1000 | Loss: 0.00003070
Iteration 70/1000 | Loss: 0.00003070
Iteration 71/1000 | Loss: 0.00003070
Iteration 72/1000 | Loss: 0.00003070
Iteration 73/1000 | Loss: 0.00003070
Iteration 74/1000 | Loss: 0.00003070
Iteration 75/1000 | Loss: 0.00003069
Iteration 76/1000 | Loss: 0.00003069
Iteration 77/1000 | Loss: 0.00003069
Iteration 78/1000 | Loss: 0.00003069
Iteration 79/1000 | Loss: 0.00003069
Iteration 80/1000 | Loss: 0.00003069
Iteration 81/1000 | Loss: 0.00003069
Iteration 82/1000 | Loss: 0.00003069
Iteration 83/1000 | Loss: 0.00003068
Iteration 84/1000 | Loss: 0.00003068
Iteration 85/1000 | Loss: 0.00003068
Iteration 86/1000 | Loss: 0.00003068
Iteration 87/1000 | Loss: 0.00003068
Iteration 88/1000 | Loss: 0.00003067
Iteration 89/1000 | Loss: 0.00003067
Iteration 90/1000 | Loss: 0.00003066
Iteration 91/1000 | Loss: 0.00003066
Iteration 92/1000 | Loss: 0.00003066
Iteration 93/1000 | Loss: 0.00003065
Iteration 94/1000 | Loss: 0.00003065
Iteration 95/1000 | Loss: 0.00003065
Iteration 96/1000 | Loss: 0.00003065
Iteration 97/1000 | Loss: 0.00003065
Iteration 98/1000 | Loss: 0.00003065
Iteration 99/1000 | Loss: 0.00003065
Iteration 100/1000 | Loss: 0.00003065
Iteration 101/1000 | Loss: 0.00003065
Iteration 102/1000 | Loss: 0.00003065
Iteration 103/1000 | Loss: 0.00003064
Iteration 104/1000 | Loss: 0.00003064
Iteration 105/1000 | Loss: 0.00003064
Iteration 106/1000 | Loss: 0.00003064
Iteration 107/1000 | Loss: 0.00003064
Iteration 108/1000 | Loss: 0.00003063
Iteration 109/1000 | Loss: 0.00003063
Iteration 110/1000 | Loss: 0.00003063
Iteration 111/1000 | Loss: 0.00003063
Iteration 112/1000 | Loss: 0.00003062
Iteration 113/1000 | Loss: 0.00003062
Iteration 114/1000 | Loss: 0.00003062
Iteration 115/1000 | Loss: 0.00003062
Iteration 116/1000 | Loss: 0.00003062
Iteration 117/1000 | Loss: 0.00003062
Iteration 118/1000 | Loss: 0.00003062
Iteration 119/1000 | Loss: 0.00003062
Iteration 120/1000 | Loss: 0.00003062
Iteration 121/1000 | Loss: 0.00003062
Iteration 122/1000 | Loss: 0.00003062
Iteration 123/1000 | Loss: 0.00003062
Iteration 124/1000 | Loss: 0.00003062
Iteration 125/1000 | Loss: 0.00003062
Iteration 126/1000 | Loss: 0.00003062
Iteration 127/1000 | Loss: 0.00003062
Iteration 128/1000 | Loss: 0.00003062
Iteration 129/1000 | Loss: 0.00003062
Iteration 130/1000 | Loss: 0.00003062
Iteration 131/1000 | Loss: 0.00003062
Iteration 132/1000 | Loss: 0.00003062
Iteration 133/1000 | Loss: 0.00003062
Iteration 134/1000 | Loss: 0.00003062
Iteration 135/1000 | Loss: 0.00003062
Iteration 136/1000 | Loss: 0.00003062
Iteration 137/1000 | Loss: 0.00003062
Iteration 138/1000 | Loss: 0.00003062
Iteration 139/1000 | Loss: 0.00003062
Iteration 140/1000 | Loss: 0.00003062
Iteration 141/1000 | Loss: 0.00003062
Iteration 142/1000 | Loss: 0.00003062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.061873576370999e-05, 3.061873576370999e-05, 3.061873576370999e-05, 3.061873576370999e-05, 3.061873576370999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.061873576370999e-05

Optimization complete. Final v2v error: 4.7117767333984375 mm

Highest mean error: 4.9843926429748535 mm for frame 15

Lowest mean error: 4.479153156280518 mm for frame 0

Saving results

Total time: 35.33023118972778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962553
Iteration 2/25 | Loss: 0.00305755
Iteration 3/25 | Loss: 0.00209937
Iteration 4/25 | Loss: 0.00185620
Iteration 5/25 | Loss: 0.00180046
Iteration 6/25 | Loss: 0.00181019
Iteration 7/25 | Loss: 0.00164643
Iteration 8/25 | Loss: 0.00162971
Iteration 9/25 | Loss: 0.00161736
Iteration 10/25 | Loss: 0.00161294
Iteration 11/25 | Loss: 0.00160851
Iteration 12/25 | Loss: 0.00160675
Iteration 13/25 | Loss: 0.00160417
Iteration 14/25 | Loss: 0.00160382
Iteration 15/25 | Loss: 0.00160156
Iteration 16/25 | Loss: 0.00160223
Iteration 17/25 | Loss: 0.00159977
Iteration 18/25 | Loss: 0.00160061
Iteration 19/25 | Loss: 0.00159863
Iteration 20/25 | Loss: 0.00160073
Iteration 21/25 | Loss: 0.00159790
Iteration 22/25 | Loss: 0.00159938
Iteration 23/25 | Loss: 0.00159829
Iteration 24/25 | Loss: 0.00159762
Iteration 25/25 | Loss: 0.00159731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.99840450
Iteration 2/25 | Loss: 0.00911473
Iteration 3/25 | Loss: 0.00403533
Iteration 4/25 | Loss: 0.00383152
Iteration 5/25 | Loss: 0.00383152
Iteration 6/25 | Loss: 0.00383152
Iteration 7/25 | Loss: 0.00383152
Iteration 8/25 | Loss: 0.00383152
Iteration 9/25 | Loss: 0.00383152
Iteration 10/25 | Loss: 0.00383152
Iteration 11/25 | Loss: 0.00383152
Iteration 12/25 | Loss: 0.00383152
Iteration 13/25 | Loss: 0.00383152
Iteration 14/25 | Loss: 0.00383152
Iteration 15/25 | Loss: 0.00383152
Iteration 16/25 | Loss: 0.00383152
Iteration 17/25 | Loss: 0.00383152
Iteration 18/25 | Loss: 0.00383152
Iteration 19/25 | Loss: 0.00383152
Iteration 20/25 | Loss: 0.00383152
Iteration 21/25 | Loss: 0.00383152
Iteration 22/25 | Loss: 0.00383152
Iteration 23/25 | Loss: 0.00383152
Iteration 24/25 | Loss: 0.00383152
Iteration 25/25 | Loss: 0.00383152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00383152
Iteration 2/1000 | Loss: 0.00739176
Iteration 3/1000 | Loss: 0.00674580
Iteration 4/1000 | Loss: 0.00318192
Iteration 5/1000 | Loss: 0.00202925
Iteration 6/1000 | Loss: 0.00259460
Iteration 7/1000 | Loss: 0.00402963
Iteration 8/1000 | Loss: 0.00136300
Iteration 9/1000 | Loss: 0.00560334
Iteration 10/1000 | Loss: 0.00125720
Iteration 11/1000 | Loss: 0.00133929
Iteration 12/1000 | Loss: 0.00229512
Iteration 13/1000 | Loss: 0.00180320
Iteration 14/1000 | Loss: 0.00175737
Iteration 15/1000 | Loss: 0.00085184
Iteration 16/1000 | Loss: 0.00080668
Iteration 17/1000 | Loss: 0.00051835
Iteration 18/1000 | Loss: 0.00053605
Iteration 19/1000 | Loss: 0.00020743
Iteration 20/1000 | Loss: 0.00027042
Iteration 21/1000 | Loss: 0.00015083
Iteration 22/1000 | Loss: 0.00072289
Iteration 23/1000 | Loss: 0.00016999
Iteration 24/1000 | Loss: 0.00067313
Iteration 25/1000 | Loss: 0.00096071
Iteration 26/1000 | Loss: 0.00034954
Iteration 27/1000 | Loss: 0.00029701
Iteration 28/1000 | Loss: 0.00032708
Iteration 29/1000 | Loss: 0.00024786
Iteration 30/1000 | Loss: 0.00010843
Iteration 31/1000 | Loss: 0.00074286
Iteration 32/1000 | Loss: 0.00050535
Iteration 33/1000 | Loss: 0.00033042
Iteration 34/1000 | Loss: 0.00018583
Iteration 35/1000 | Loss: 0.00047078
Iteration 36/1000 | Loss: 0.00075348
Iteration 37/1000 | Loss: 0.00018015
Iteration 38/1000 | Loss: 0.00017562
Iteration 39/1000 | Loss: 0.00014309
Iteration 40/1000 | Loss: 0.00007359
Iteration 41/1000 | Loss: 0.00012777
Iteration 42/1000 | Loss: 0.00011507
Iteration 43/1000 | Loss: 0.00005365
Iteration 44/1000 | Loss: 0.00042116
Iteration 45/1000 | Loss: 0.00036733
Iteration 46/1000 | Loss: 0.00040808
Iteration 47/1000 | Loss: 0.00017197
Iteration 48/1000 | Loss: 0.00012438
Iteration 49/1000 | Loss: 0.00013256
Iteration 50/1000 | Loss: 0.00060102
Iteration 51/1000 | Loss: 0.00029413
Iteration 52/1000 | Loss: 0.00074849
Iteration 53/1000 | Loss: 0.00050803
Iteration 54/1000 | Loss: 0.00026303
Iteration 55/1000 | Loss: 0.00032807
Iteration 56/1000 | Loss: 0.00013823
Iteration 57/1000 | Loss: 0.00007465
Iteration 58/1000 | Loss: 0.00031962
Iteration 59/1000 | Loss: 0.00024387
Iteration 60/1000 | Loss: 0.00055549
Iteration 61/1000 | Loss: 0.00026147
Iteration 62/1000 | Loss: 0.00028220
Iteration 63/1000 | Loss: 0.00023095
Iteration 64/1000 | Loss: 0.00011986
Iteration 65/1000 | Loss: 0.00004846
Iteration 66/1000 | Loss: 0.00004617
Iteration 67/1000 | Loss: 0.00055757
Iteration 68/1000 | Loss: 0.00033318
Iteration 69/1000 | Loss: 0.00045155
Iteration 70/1000 | Loss: 0.00030500
Iteration 71/1000 | Loss: 0.00039757
Iteration 72/1000 | Loss: 0.00013742
Iteration 73/1000 | Loss: 0.00005121
Iteration 74/1000 | Loss: 0.00004183
Iteration 75/1000 | Loss: 0.00070852
Iteration 76/1000 | Loss: 0.00051035
Iteration 77/1000 | Loss: 0.00039289
Iteration 78/1000 | Loss: 0.00019347
Iteration 79/1000 | Loss: 0.00010836
Iteration 80/1000 | Loss: 0.00004632
Iteration 81/1000 | Loss: 0.00004182
Iteration 82/1000 | Loss: 0.00004915
Iteration 83/1000 | Loss: 0.00003681
Iteration 84/1000 | Loss: 0.00003583
Iteration 85/1000 | Loss: 0.00004063
Iteration 86/1000 | Loss: 0.00003349
Iteration 87/1000 | Loss: 0.00003304
Iteration 88/1000 | Loss: 0.00014501
Iteration 89/1000 | Loss: 0.00028554
Iteration 90/1000 | Loss: 0.00040211
Iteration 91/1000 | Loss: 0.00029939
Iteration 92/1000 | Loss: 0.00035842
Iteration 93/1000 | Loss: 0.00030038
Iteration 94/1000 | Loss: 0.00024680
Iteration 95/1000 | Loss: 0.00003901
Iteration 96/1000 | Loss: 0.00036968
Iteration 97/1000 | Loss: 0.00064281
Iteration 98/1000 | Loss: 0.00022655
Iteration 99/1000 | Loss: 0.00047418
Iteration 100/1000 | Loss: 0.00004337
Iteration 101/1000 | Loss: 0.00003961
Iteration 102/1000 | Loss: 0.00014776
Iteration 103/1000 | Loss: 0.00006242
Iteration 104/1000 | Loss: 0.00003886
Iteration 105/1000 | Loss: 0.00003583
Iteration 106/1000 | Loss: 0.00015229
Iteration 107/1000 | Loss: 0.00004308
Iteration 108/1000 | Loss: 0.00003542
Iteration 109/1000 | Loss: 0.00003423
Iteration 110/1000 | Loss: 0.00057229
Iteration 111/1000 | Loss: 0.00019555
Iteration 112/1000 | Loss: 0.00021698
Iteration 113/1000 | Loss: 0.00007300
Iteration 114/1000 | Loss: 0.00003989
Iteration 115/1000 | Loss: 0.00015811
Iteration 116/1000 | Loss: 0.00003755
Iteration 117/1000 | Loss: 0.00003609
Iteration 118/1000 | Loss: 0.00012918
Iteration 119/1000 | Loss: 0.00004035
Iteration 120/1000 | Loss: 0.00003433
Iteration 121/1000 | Loss: 0.00003233
Iteration 122/1000 | Loss: 0.00003008
Iteration 123/1000 | Loss: 0.00002963
Iteration 124/1000 | Loss: 0.00002910
Iteration 125/1000 | Loss: 0.00002883
Iteration 126/1000 | Loss: 0.00002868
Iteration 127/1000 | Loss: 0.00002865
Iteration 128/1000 | Loss: 0.00002865
Iteration 129/1000 | Loss: 0.00002864
Iteration 130/1000 | Loss: 0.00002861
Iteration 131/1000 | Loss: 0.00002901
Iteration 132/1000 | Loss: 0.00002901
Iteration 133/1000 | Loss: 0.00002868
Iteration 134/1000 | Loss: 0.00002854
Iteration 135/1000 | Loss: 0.00002853
Iteration 136/1000 | Loss: 0.00002853
Iteration 137/1000 | Loss: 0.00002852
Iteration 138/1000 | Loss: 0.00002852
Iteration 139/1000 | Loss: 0.00002852
Iteration 140/1000 | Loss: 0.00002852
Iteration 141/1000 | Loss: 0.00002852
Iteration 142/1000 | Loss: 0.00002852
Iteration 143/1000 | Loss: 0.00002852
Iteration 144/1000 | Loss: 0.00002852
Iteration 145/1000 | Loss: 0.00002851
Iteration 146/1000 | Loss: 0.00002851
Iteration 147/1000 | Loss: 0.00002851
Iteration 148/1000 | Loss: 0.00002851
Iteration 149/1000 | Loss: 0.00002851
Iteration 150/1000 | Loss: 0.00002851
Iteration 151/1000 | Loss: 0.00002850
Iteration 152/1000 | Loss: 0.00002850
Iteration 153/1000 | Loss: 0.00002850
Iteration 154/1000 | Loss: 0.00002850
Iteration 155/1000 | Loss: 0.00002850
Iteration 156/1000 | Loss: 0.00002850
Iteration 157/1000 | Loss: 0.00002850
Iteration 158/1000 | Loss: 0.00002850
Iteration 159/1000 | Loss: 0.00002849
Iteration 160/1000 | Loss: 0.00002849
Iteration 161/1000 | Loss: 0.00002849
Iteration 162/1000 | Loss: 0.00002849
Iteration 163/1000 | Loss: 0.00002849
Iteration 164/1000 | Loss: 0.00002848
Iteration 165/1000 | Loss: 0.00002848
Iteration 166/1000 | Loss: 0.00002848
Iteration 167/1000 | Loss: 0.00002848
Iteration 168/1000 | Loss: 0.00002848
Iteration 169/1000 | Loss: 0.00002848
Iteration 170/1000 | Loss: 0.00002848
Iteration 171/1000 | Loss: 0.00002847
Iteration 172/1000 | Loss: 0.00002847
Iteration 173/1000 | Loss: 0.00002847
Iteration 174/1000 | Loss: 0.00002847
Iteration 175/1000 | Loss: 0.00002847
Iteration 176/1000 | Loss: 0.00002847
Iteration 177/1000 | Loss: 0.00002847
Iteration 178/1000 | Loss: 0.00002897
Iteration 179/1000 | Loss: 0.00002897
Iteration 180/1000 | Loss: 0.00002860
Iteration 181/1000 | Loss: 0.00002847
Iteration 182/1000 | Loss: 0.00002847
Iteration 183/1000 | Loss: 0.00002847
Iteration 184/1000 | Loss: 0.00002847
Iteration 185/1000 | Loss: 0.00002847
Iteration 186/1000 | Loss: 0.00002847
Iteration 187/1000 | Loss: 0.00002847
Iteration 188/1000 | Loss: 0.00002846
Iteration 189/1000 | Loss: 0.00002846
Iteration 190/1000 | Loss: 0.00002846
Iteration 191/1000 | Loss: 0.00002846
Iteration 192/1000 | Loss: 0.00002846
Iteration 193/1000 | Loss: 0.00002846
Iteration 194/1000 | Loss: 0.00002846
Iteration 195/1000 | Loss: 0.00002846
Iteration 196/1000 | Loss: 0.00002846
Iteration 197/1000 | Loss: 0.00002846
Iteration 198/1000 | Loss: 0.00002846
Iteration 199/1000 | Loss: 0.00002846
Iteration 200/1000 | Loss: 0.00002846
Iteration 201/1000 | Loss: 0.00002846
Iteration 202/1000 | Loss: 0.00002846
Iteration 203/1000 | Loss: 0.00002846
Iteration 204/1000 | Loss: 0.00002845
Iteration 205/1000 | Loss: 0.00002845
Iteration 206/1000 | Loss: 0.00002845
Iteration 207/1000 | Loss: 0.00002845
Iteration 208/1000 | Loss: 0.00002845
Iteration 209/1000 | Loss: 0.00002845
Iteration 210/1000 | Loss: 0.00002845
Iteration 211/1000 | Loss: 0.00002845
Iteration 212/1000 | Loss: 0.00002845
Iteration 213/1000 | Loss: 0.00002845
Iteration 214/1000 | Loss: 0.00002845
Iteration 215/1000 | Loss: 0.00002845
Iteration 216/1000 | Loss: 0.00002845
Iteration 217/1000 | Loss: 0.00002845
Iteration 218/1000 | Loss: 0.00002845
Iteration 219/1000 | Loss: 0.00002845
Iteration 220/1000 | Loss: 0.00002845
Iteration 221/1000 | Loss: 0.00002845
Iteration 222/1000 | Loss: 0.00002845
Iteration 223/1000 | Loss: 0.00002845
Iteration 224/1000 | Loss: 0.00002845
Iteration 225/1000 | Loss: 0.00002845
Iteration 226/1000 | Loss: 0.00002845
Iteration 227/1000 | Loss: 0.00002845
Iteration 228/1000 | Loss: 0.00002845
Iteration 229/1000 | Loss: 0.00002845
Iteration 230/1000 | Loss: 0.00002845
Iteration 231/1000 | Loss: 0.00002845
Iteration 232/1000 | Loss: 0.00002845
Iteration 233/1000 | Loss: 0.00002845
Iteration 234/1000 | Loss: 0.00002845
Iteration 235/1000 | Loss: 0.00002845
Iteration 236/1000 | Loss: 0.00002845
Iteration 237/1000 | Loss: 0.00002845
Iteration 238/1000 | Loss: 0.00002845
Iteration 239/1000 | Loss: 0.00002845
Iteration 240/1000 | Loss: 0.00002845
Iteration 241/1000 | Loss: 0.00002845
Iteration 242/1000 | Loss: 0.00002845
Iteration 243/1000 | Loss: 0.00002845
Iteration 244/1000 | Loss: 0.00002845
Iteration 245/1000 | Loss: 0.00002845
Iteration 246/1000 | Loss: 0.00002845
Iteration 247/1000 | Loss: 0.00002845
Iteration 248/1000 | Loss: 0.00002845
Iteration 249/1000 | Loss: 0.00002845
Iteration 250/1000 | Loss: 0.00002845
Iteration 251/1000 | Loss: 0.00002845
Iteration 252/1000 | Loss: 0.00002845
Iteration 253/1000 | Loss: 0.00002845
Iteration 254/1000 | Loss: 0.00002845
Iteration 255/1000 | Loss: 0.00002845
Iteration 256/1000 | Loss: 0.00002845
Iteration 257/1000 | Loss: 0.00002845
Iteration 258/1000 | Loss: 0.00002845
Iteration 259/1000 | Loss: 0.00002845
Iteration 260/1000 | Loss: 0.00002845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.8448645025491714e-05, 2.8448645025491714e-05, 2.8448645025491714e-05, 2.8448645025491714e-05, 2.8448645025491714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8448645025491714e-05

Optimization complete. Final v2v error: 3.9405250549316406 mm

Highest mean error: 20.691112518310547 mm for frame 19

Lowest mean error: 2.9299960136413574 mm for frame 2

Saving results

Total time: 243.5342767238617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046626
Iteration 2/25 | Loss: 0.00202377
Iteration 3/25 | Loss: 0.00149424
Iteration 4/25 | Loss: 0.00146571
Iteration 5/25 | Loss: 0.00145813
Iteration 6/25 | Loss: 0.00145531
Iteration 7/25 | Loss: 0.00145507
Iteration 8/25 | Loss: 0.00145507
Iteration 9/25 | Loss: 0.00145507
Iteration 10/25 | Loss: 0.00145507
Iteration 11/25 | Loss: 0.00145507
Iteration 12/25 | Loss: 0.00145507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014550708001479506, 0.0014550708001479506, 0.0014550708001479506, 0.0014550708001479506, 0.0014550708001479506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014550708001479506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79988891
Iteration 2/25 | Loss: 0.00144835
Iteration 3/25 | Loss: 0.00144835
Iteration 4/25 | Loss: 0.00144835
Iteration 5/25 | Loss: 0.00144835
Iteration 6/25 | Loss: 0.00144835
Iteration 7/25 | Loss: 0.00144835
Iteration 8/25 | Loss: 0.00144835
Iteration 9/25 | Loss: 0.00144835
Iteration 10/25 | Loss: 0.00144835
Iteration 11/25 | Loss: 0.00144835
Iteration 12/25 | Loss: 0.00144835
Iteration 13/25 | Loss: 0.00144835
Iteration 14/25 | Loss: 0.00144835
Iteration 15/25 | Loss: 0.00144835
Iteration 16/25 | Loss: 0.00144835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001448345254175365, 0.001448345254175365, 0.001448345254175365, 0.001448345254175365, 0.001448345254175365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001448345254175365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144835
Iteration 2/1000 | Loss: 0.00006939
Iteration 3/1000 | Loss: 0.00005277
Iteration 4/1000 | Loss: 0.00004684
Iteration 5/1000 | Loss: 0.00004391
Iteration 6/1000 | Loss: 0.00004215
Iteration 7/1000 | Loss: 0.00004092
Iteration 8/1000 | Loss: 0.00004010
Iteration 9/1000 | Loss: 0.00003946
Iteration 10/1000 | Loss: 0.00003903
Iteration 11/1000 | Loss: 0.00003876
Iteration 12/1000 | Loss: 0.00003853
Iteration 13/1000 | Loss: 0.00003833
Iteration 14/1000 | Loss: 0.00003817
Iteration 15/1000 | Loss: 0.00003799
Iteration 16/1000 | Loss: 0.00003785
Iteration 17/1000 | Loss: 0.00003770
Iteration 18/1000 | Loss: 0.00003770
Iteration 19/1000 | Loss: 0.00003755
Iteration 20/1000 | Loss: 0.00003751
Iteration 21/1000 | Loss: 0.00003738
Iteration 22/1000 | Loss: 0.00003736
Iteration 23/1000 | Loss: 0.00003733
Iteration 24/1000 | Loss: 0.00003726
Iteration 25/1000 | Loss: 0.00003726
Iteration 26/1000 | Loss: 0.00003721
Iteration 27/1000 | Loss: 0.00003720
Iteration 28/1000 | Loss: 0.00003715
Iteration 29/1000 | Loss: 0.00003714
Iteration 30/1000 | Loss: 0.00003713
Iteration 31/1000 | Loss: 0.00003712
Iteration 32/1000 | Loss: 0.00003712
Iteration 33/1000 | Loss: 0.00003712
Iteration 34/1000 | Loss: 0.00003712
Iteration 35/1000 | Loss: 0.00003711
Iteration 36/1000 | Loss: 0.00003711
Iteration 37/1000 | Loss: 0.00003711
Iteration 38/1000 | Loss: 0.00003711
Iteration 39/1000 | Loss: 0.00003711
Iteration 40/1000 | Loss: 0.00003711
Iteration 41/1000 | Loss: 0.00003711
Iteration 42/1000 | Loss: 0.00003711
Iteration 43/1000 | Loss: 0.00003711
Iteration 44/1000 | Loss: 0.00003711
Iteration 45/1000 | Loss: 0.00003711
Iteration 46/1000 | Loss: 0.00003711
Iteration 47/1000 | Loss: 0.00003710
Iteration 48/1000 | Loss: 0.00003710
Iteration 49/1000 | Loss: 0.00003710
Iteration 50/1000 | Loss: 0.00003710
Iteration 51/1000 | Loss: 0.00003709
Iteration 52/1000 | Loss: 0.00003709
Iteration 53/1000 | Loss: 0.00003709
Iteration 54/1000 | Loss: 0.00003709
Iteration 55/1000 | Loss: 0.00003709
Iteration 56/1000 | Loss: 0.00003709
Iteration 57/1000 | Loss: 0.00003708
Iteration 58/1000 | Loss: 0.00003708
Iteration 59/1000 | Loss: 0.00003708
Iteration 60/1000 | Loss: 0.00003708
Iteration 61/1000 | Loss: 0.00003708
Iteration 62/1000 | Loss: 0.00003708
Iteration 63/1000 | Loss: 0.00003708
Iteration 64/1000 | Loss: 0.00003708
Iteration 65/1000 | Loss: 0.00003708
Iteration 66/1000 | Loss: 0.00003708
Iteration 67/1000 | Loss: 0.00003707
Iteration 68/1000 | Loss: 0.00003707
Iteration 69/1000 | Loss: 0.00003707
Iteration 70/1000 | Loss: 0.00003707
Iteration 71/1000 | Loss: 0.00003707
Iteration 72/1000 | Loss: 0.00003706
Iteration 73/1000 | Loss: 0.00003706
Iteration 74/1000 | Loss: 0.00003706
Iteration 75/1000 | Loss: 0.00003706
Iteration 76/1000 | Loss: 0.00003706
Iteration 77/1000 | Loss: 0.00003706
Iteration 78/1000 | Loss: 0.00003706
Iteration 79/1000 | Loss: 0.00003706
Iteration 80/1000 | Loss: 0.00003706
Iteration 81/1000 | Loss: 0.00003706
Iteration 82/1000 | Loss: 0.00003705
Iteration 83/1000 | Loss: 0.00003705
Iteration 84/1000 | Loss: 0.00003705
Iteration 85/1000 | Loss: 0.00003705
Iteration 86/1000 | Loss: 0.00003705
Iteration 87/1000 | Loss: 0.00003705
Iteration 88/1000 | Loss: 0.00003705
Iteration 89/1000 | Loss: 0.00003705
Iteration 90/1000 | Loss: 0.00003705
Iteration 91/1000 | Loss: 0.00003705
Iteration 92/1000 | Loss: 0.00003705
Iteration 93/1000 | Loss: 0.00003705
Iteration 94/1000 | Loss: 0.00003705
Iteration 95/1000 | Loss: 0.00003705
Iteration 96/1000 | Loss: 0.00003704
Iteration 97/1000 | Loss: 0.00003704
Iteration 98/1000 | Loss: 0.00003704
Iteration 99/1000 | Loss: 0.00003704
Iteration 100/1000 | Loss: 0.00003704
Iteration 101/1000 | Loss: 0.00003704
Iteration 102/1000 | Loss: 0.00003704
Iteration 103/1000 | Loss: 0.00003704
Iteration 104/1000 | Loss: 0.00003704
Iteration 105/1000 | Loss: 0.00003704
Iteration 106/1000 | Loss: 0.00003704
Iteration 107/1000 | Loss: 0.00003704
Iteration 108/1000 | Loss: 0.00003704
Iteration 109/1000 | Loss: 0.00003703
Iteration 110/1000 | Loss: 0.00003703
Iteration 111/1000 | Loss: 0.00003703
Iteration 112/1000 | Loss: 0.00003703
Iteration 113/1000 | Loss: 0.00003703
Iteration 114/1000 | Loss: 0.00003703
Iteration 115/1000 | Loss: 0.00003703
Iteration 116/1000 | Loss: 0.00003703
Iteration 117/1000 | Loss: 0.00003703
Iteration 118/1000 | Loss: 0.00003703
Iteration 119/1000 | Loss: 0.00003703
Iteration 120/1000 | Loss: 0.00003703
Iteration 121/1000 | Loss: 0.00003703
Iteration 122/1000 | Loss: 0.00003703
Iteration 123/1000 | Loss: 0.00003703
Iteration 124/1000 | Loss: 0.00003702
Iteration 125/1000 | Loss: 0.00003702
Iteration 126/1000 | Loss: 0.00003702
Iteration 127/1000 | Loss: 0.00003702
Iteration 128/1000 | Loss: 0.00003702
Iteration 129/1000 | Loss: 0.00003702
Iteration 130/1000 | Loss: 0.00003702
Iteration 131/1000 | Loss: 0.00003702
Iteration 132/1000 | Loss: 0.00003702
Iteration 133/1000 | Loss: 0.00003702
Iteration 134/1000 | Loss: 0.00003702
Iteration 135/1000 | Loss: 0.00003702
Iteration 136/1000 | Loss: 0.00003702
Iteration 137/1000 | Loss: 0.00003702
Iteration 138/1000 | Loss: 0.00003702
Iteration 139/1000 | Loss: 0.00003701
Iteration 140/1000 | Loss: 0.00003701
Iteration 141/1000 | Loss: 0.00003701
Iteration 142/1000 | Loss: 0.00003701
Iteration 143/1000 | Loss: 0.00003701
Iteration 144/1000 | Loss: 0.00003701
Iteration 145/1000 | Loss: 0.00003701
Iteration 146/1000 | Loss: 0.00003701
Iteration 147/1000 | Loss: 0.00003701
Iteration 148/1000 | Loss: 0.00003701
Iteration 149/1000 | Loss: 0.00003701
Iteration 150/1000 | Loss: 0.00003701
Iteration 151/1000 | Loss: 0.00003701
Iteration 152/1000 | Loss: 0.00003701
Iteration 153/1000 | Loss: 0.00003701
Iteration 154/1000 | Loss: 0.00003701
Iteration 155/1000 | Loss: 0.00003701
Iteration 156/1000 | Loss: 0.00003701
Iteration 157/1000 | Loss: 0.00003701
Iteration 158/1000 | Loss: 0.00003700
Iteration 159/1000 | Loss: 0.00003700
Iteration 160/1000 | Loss: 0.00003700
Iteration 161/1000 | Loss: 0.00003700
Iteration 162/1000 | Loss: 0.00003700
Iteration 163/1000 | Loss: 0.00003700
Iteration 164/1000 | Loss: 0.00003700
Iteration 165/1000 | Loss: 0.00003700
Iteration 166/1000 | Loss: 0.00003700
Iteration 167/1000 | Loss: 0.00003700
Iteration 168/1000 | Loss: 0.00003700
Iteration 169/1000 | Loss: 0.00003700
Iteration 170/1000 | Loss: 0.00003700
Iteration 171/1000 | Loss: 0.00003700
Iteration 172/1000 | Loss: 0.00003700
Iteration 173/1000 | Loss: 0.00003700
Iteration 174/1000 | Loss: 0.00003700
Iteration 175/1000 | Loss: 0.00003700
Iteration 176/1000 | Loss: 0.00003700
Iteration 177/1000 | Loss: 0.00003700
Iteration 178/1000 | Loss: 0.00003700
Iteration 179/1000 | Loss: 0.00003700
Iteration 180/1000 | Loss: 0.00003700
Iteration 181/1000 | Loss: 0.00003700
Iteration 182/1000 | Loss: 0.00003700
Iteration 183/1000 | Loss: 0.00003700
Iteration 184/1000 | Loss: 0.00003700
Iteration 185/1000 | Loss: 0.00003700
Iteration 186/1000 | Loss: 0.00003700
Iteration 187/1000 | Loss: 0.00003700
Iteration 188/1000 | Loss: 0.00003700
Iteration 189/1000 | Loss: 0.00003700
Iteration 190/1000 | Loss: 0.00003700
Iteration 191/1000 | Loss: 0.00003700
Iteration 192/1000 | Loss: 0.00003700
Iteration 193/1000 | Loss: 0.00003700
Iteration 194/1000 | Loss: 0.00003700
Iteration 195/1000 | Loss: 0.00003700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [3.699754597619176e-05, 3.699754597619176e-05, 3.699754597619176e-05, 3.699754597619176e-05, 3.699754597619176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.699754597619176e-05

Optimization complete. Final v2v error: 4.843618869781494 mm

Highest mean error: 5.926259994506836 mm for frame 66

Lowest mean error: 3.911458969116211 mm for frame 29

Saving results

Total time: 50.1607301235199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741295
Iteration 2/25 | Loss: 0.00151737
Iteration 3/25 | Loss: 0.00143449
Iteration 4/25 | Loss: 0.00142195
Iteration 5/25 | Loss: 0.00141545
Iteration 6/25 | Loss: 0.00141291
Iteration 7/25 | Loss: 0.00141227
Iteration 8/25 | Loss: 0.00141227
Iteration 9/25 | Loss: 0.00141227
Iteration 10/25 | Loss: 0.00141227
Iteration 11/25 | Loss: 0.00141227
Iteration 12/25 | Loss: 0.00141227
Iteration 13/25 | Loss: 0.00141227
Iteration 14/25 | Loss: 0.00141227
Iteration 15/25 | Loss: 0.00141227
Iteration 16/25 | Loss: 0.00141227
Iteration 17/25 | Loss: 0.00141227
Iteration 18/25 | Loss: 0.00141227
Iteration 19/25 | Loss: 0.00141227
Iteration 20/25 | Loss: 0.00141227
Iteration 21/25 | Loss: 0.00141227
Iteration 22/25 | Loss: 0.00141227
Iteration 23/25 | Loss: 0.00141227
Iteration 24/25 | Loss: 0.00141227
Iteration 25/25 | Loss: 0.00141227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014122715219855309, 0.0014122715219855309, 0.0014122715219855309, 0.0014122715219855309, 0.0014122715219855309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014122715219855309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.59419250
Iteration 2/25 | Loss: 0.00224232
Iteration 3/25 | Loss: 0.00224230
Iteration 4/25 | Loss: 0.00224230
Iteration 5/25 | Loss: 0.00224230
Iteration 6/25 | Loss: 0.00224230
Iteration 7/25 | Loss: 0.00224230
Iteration 8/25 | Loss: 0.00224230
Iteration 9/25 | Loss: 0.00224230
Iteration 10/25 | Loss: 0.00224230
Iteration 11/25 | Loss: 0.00224230
Iteration 12/25 | Loss: 0.00224230
Iteration 13/25 | Loss: 0.00224230
Iteration 14/25 | Loss: 0.00224230
Iteration 15/25 | Loss: 0.00224230
Iteration 16/25 | Loss: 0.00224230
Iteration 17/25 | Loss: 0.00224230
Iteration 18/25 | Loss: 0.00224230
Iteration 19/25 | Loss: 0.00224230
Iteration 20/25 | Loss: 0.00224230
Iteration 21/25 | Loss: 0.00224230
Iteration 22/25 | Loss: 0.00224230
Iteration 23/25 | Loss: 0.00224230
Iteration 24/25 | Loss: 0.00224230
Iteration 25/25 | Loss: 0.00224230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224230
Iteration 2/1000 | Loss: 0.00004062
Iteration 3/1000 | Loss: 0.00002810
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002361
Iteration 6/1000 | Loss: 0.00002283
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002100
Iteration 10/1000 | Loss: 0.00002065
Iteration 11/1000 | Loss: 0.00002064
Iteration 12/1000 | Loss: 0.00002041
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00002015
Iteration 16/1000 | Loss: 0.00002015
Iteration 17/1000 | Loss: 0.00002013
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002009
Iteration 21/1000 | Loss: 0.00002008
Iteration 22/1000 | Loss: 0.00002008
Iteration 23/1000 | Loss: 0.00002003
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002001
Iteration 26/1000 | Loss: 0.00002001
Iteration 27/1000 | Loss: 0.00002001
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001999
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001997
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001993
Iteration 39/1000 | Loss: 0.00001993
Iteration 40/1000 | Loss: 0.00001993
Iteration 41/1000 | Loss: 0.00001993
Iteration 42/1000 | Loss: 0.00001993
Iteration 43/1000 | Loss: 0.00001993
Iteration 44/1000 | Loss: 0.00001993
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001992
Iteration 47/1000 | Loss: 0.00001992
Iteration 48/1000 | Loss: 0.00001992
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001991
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001989
Iteration 56/1000 | Loss: 0.00001989
Iteration 57/1000 | Loss: 0.00001989
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001989
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001988
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.9883540517184883e-05, 1.9883540517184883e-05, 1.9883540517184883e-05, 1.9883540517184883e-05, 1.9883540517184883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9883540517184883e-05

Optimization complete. Final v2v error: 3.840974807739258 mm

Highest mean error: 4.246051788330078 mm for frame 106

Lowest mean error: 3.412081718444824 mm for frame 169

Saving results

Total time: 32.71568441390991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977777
Iteration 2/25 | Loss: 0.00152788
Iteration 3/25 | Loss: 0.00143714
Iteration 4/25 | Loss: 0.00141831
Iteration 5/25 | Loss: 0.00141455
Iteration 6/25 | Loss: 0.00141440
Iteration 7/25 | Loss: 0.00141440
Iteration 8/25 | Loss: 0.00141440
Iteration 9/25 | Loss: 0.00141440
Iteration 10/25 | Loss: 0.00141440
Iteration 11/25 | Loss: 0.00141440
Iteration 12/25 | Loss: 0.00141440
Iteration 13/25 | Loss: 0.00141440
Iteration 14/25 | Loss: 0.00141440
Iteration 15/25 | Loss: 0.00141440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014144045999273658, 0.0014144045999273658, 0.0014144045999273658, 0.0014144045999273658, 0.0014144045999273658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014144045999273658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12433517
Iteration 2/25 | Loss: 0.00231222
Iteration 3/25 | Loss: 0.00231221
Iteration 4/25 | Loss: 0.00231221
Iteration 5/25 | Loss: 0.00231221
Iteration 6/25 | Loss: 0.00231221
Iteration 7/25 | Loss: 0.00231221
Iteration 8/25 | Loss: 0.00231221
Iteration 9/25 | Loss: 0.00231221
Iteration 10/25 | Loss: 0.00231221
Iteration 11/25 | Loss: 0.00231221
Iteration 12/25 | Loss: 0.00231221
Iteration 13/25 | Loss: 0.00231221
Iteration 14/25 | Loss: 0.00231221
Iteration 15/25 | Loss: 0.00231221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002312211785465479, 0.002312211785465479, 0.002312211785465479, 0.002312211785465479, 0.002312211785465479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002312211785465479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231221
Iteration 2/1000 | Loss: 0.00004674
Iteration 3/1000 | Loss: 0.00002959
Iteration 4/1000 | Loss: 0.00002505
Iteration 5/1000 | Loss: 0.00002368
Iteration 6/1000 | Loss: 0.00002281
Iteration 7/1000 | Loss: 0.00002204
Iteration 8/1000 | Loss: 0.00002148
Iteration 9/1000 | Loss: 0.00002122
Iteration 10/1000 | Loss: 0.00002098
Iteration 11/1000 | Loss: 0.00002085
Iteration 12/1000 | Loss: 0.00002082
Iteration 13/1000 | Loss: 0.00002081
Iteration 14/1000 | Loss: 0.00002080
Iteration 15/1000 | Loss: 0.00002073
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002061
Iteration 24/1000 | Loss: 0.00002061
Iteration 25/1000 | Loss: 0.00002061
Iteration 26/1000 | Loss: 0.00002060
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002058
Iteration 29/1000 | Loss: 0.00002051
Iteration 30/1000 | Loss: 0.00002051
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00002051
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00002049
Iteration 35/1000 | Loss: 0.00002048
Iteration 36/1000 | Loss: 0.00002048
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002047
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002047
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002046
Iteration 44/1000 | Loss: 0.00002046
Iteration 45/1000 | Loss: 0.00002046
Iteration 46/1000 | Loss: 0.00002045
Iteration 47/1000 | Loss: 0.00002045
Iteration 48/1000 | Loss: 0.00002045
Iteration 49/1000 | Loss: 0.00002045
Iteration 50/1000 | Loss: 0.00002045
Iteration 51/1000 | Loss: 0.00002045
Iteration 52/1000 | Loss: 0.00002044
Iteration 53/1000 | Loss: 0.00002044
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002043
Iteration 56/1000 | Loss: 0.00002042
Iteration 57/1000 | Loss: 0.00002042
Iteration 58/1000 | Loss: 0.00002041
Iteration 59/1000 | Loss: 0.00002041
Iteration 60/1000 | Loss: 0.00002041
Iteration 61/1000 | Loss: 0.00002041
Iteration 62/1000 | Loss: 0.00002041
Iteration 63/1000 | Loss: 0.00002040
Iteration 64/1000 | Loss: 0.00002040
Iteration 65/1000 | Loss: 0.00002040
Iteration 66/1000 | Loss: 0.00002040
Iteration 67/1000 | Loss: 0.00002040
Iteration 68/1000 | Loss: 0.00002039
Iteration 69/1000 | Loss: 0.00002039
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002039
Iteration 72/1000 | Loss: 0.00002039
Iteration 73/1000 | Loss: 0.00002038
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002037
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002034
Iteration 86/1000 | Loss: 0.00002033
Iteration 87/1000 | Loss: 0.00002033
Iteration 88/1000 | Loss: 0.00002033
Iteration 89/1000 | Loss: 0.00002033
Iteration 90/1000 | Loss: 0.00002032
Iteration 91/1000 | Loss: 0.00002032
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002030
Iteration 96/1000 | Loss: 0.00002030
Iteration 97/1000 | Loss: 0.00002030
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002027
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002026
Iteration 111/1000 | Loss: 0.00002026
Iteration 112/1000 | Loss: 0.00002026
Iteration 113/1000 | Loss: 0.00002026
Iteration 114/1000 | Loss: 0.00002026
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002026
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002024
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002023
Iteration 132/1000 | Loss: 0.00002023
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00002022
Iteration 135/1000 | Loss: 0.00002022
Iteration 136/1000 | Loss: 0.00002022
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002022
Iteration 139/1000 | Loss: 0.00002022
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002021
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002021
Iteration 146/1000 | Loss: 0.00002021
Iteration 147/1000 | Loss: 0.00002021
Iteration 148/1000 | Loss: 0.00002021
Iteration 149/1000 | Loss: 0.00002021
Iteration 150/1000 | Loss: 0.00002021
Iteration 151/1000 | Loss: 0.00002021
Iteration 152/1000 | Loss: 0.00002020
Iteration 153/1000 | Loss: 0.00002020
Iteration 154/1000 | Loss: 0.00002020
Iteration 155/1000 | Loss: 0.00002020
Iteration 156/1000 | Loss: 0.00002020
Iteration 157/1000 | Loss: 0.00002020
Iteration 158/1000 | Loss: 0.00002020
Iteration 159/1000 | Loss: 0.00002020
Iteration 160/1000 | Loss: 0.00002020
Iteration 161/1000 | Loss: 0.00002020
Iteration 162/1000 | Loss: 0.00002020
Iteration 163/1000 | Loss: 0.00002019
Iteration 164/1000 | Loss: 0.00002019
Iteration 165/1000 | Loss: 0.00002019
Iteration 166/1000 | Loss: 0.00002019
Iteration 167/1000 | Loss: 0.00002019
Iteration 168/1000 | Loss: 0.00002019
Iteration 169/1000 | Loss: 0.00002019
Iteration 170/1000 | Loss: 0.00002019
Iteration 171/1000 | Loss: 0.00002019
Iteration 172/1000 | Loss: 0.00002019
Iteration 173/1000 | Loss: 0.00002019
Iteration 174/1000 | Loss: 0.00002019
Iteration 175/1000 | Loss: 0.00002019
Iteration 176/1000 | Loss: 0.00002019
Iteration 177/1000 | Loss: 0.00002019
Iteration 178/1000 | Loss: 0.00002019
Iteration 179/1000 | Loss: 0.00002019
Iteration 180/1000 | Loss: 0.00002019
Iteration 181/1000 | Loss: 0.00002019
Iteration 182/1000 | Loss: 0.00002019
Iteration 183/1000 | Loss: 0.00002019
Iteration 184/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.0187997506582178e-05, 2.0187997506582178e-05, 2.0187997506582178e-05, 2.0187997506582178e-05, 2.0187997506582178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0187997506582178e-05

Optimization complete. Final v2v error: 3.851820230484009 mm

Highest mean error: 4.063368797302246 mm for frame 163

Lowest mean error: 3.577376365661621 mm for frame 146

Saving results

Total time: 41.78996014595032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428423
Iteration 2/25 | Loss: 0.00162379
Iteration 3/25 | Loss: 0.00142401
Iteration 4/25 | Loss: 0.00140577
Iteration 5/25 | Loss: 0.00140363
Iteration 6/25 | Loss: 0.00140321
Iteration 7/25 | Loss: 0.00140321
Iteration 8/25 | Loss: 0.00140321
Iteration 9/25 | Loss: 0.00140321
Iteration 10/25 | Loss: 0.00140321
Iteration 11/25 | Loss: 0.00140321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001403210568241775, 0.001403210568241775, 0.001403210568241775, 0.001403210568241775, 0.001403210568241775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001403210568241775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16256917
Iteration 2/25 | Loss: 0.00211491
Iteration 3/25 | Loss: 0.00211491
Iteration 4/25 | Loss: 0.00211491
Iteration 5/25 | Loss: 0.00211491
Iteration 6/25 | Loss: 0.00211491
Iteration 7/25 | Loss: 0.00211491
Iteration 8/25 | Loss: 0.00211491
Iteration 9/25 | Loss: 0.00211491
Iteration 10/25 | Loss: 0.00211491
Iteration 11/25 | Loss: 0.00211491
Iteration 12/25 | Loss: 0.00211491
Iteration 13/25 | Loss: 0.00211491
Iteration 14/25 | Loss: 0.00211491
Iteration 15/25 | Loss: 0.00211491
Iteration 16/25 | Loss: 0.00211491
Iteration 17/25 | Loss: 0.00211491
Iteration 18/25 | Loss: 0.00211491
Iteration 19/25 | Loss: 0.00211491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002114908304065466, 0.002114908304065466, 0.002114908304065466, 0.002114908304065466, 0.002114908304065466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002114908304065466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211491
Iteration 2/1000 | Loss: 0.00003921
Iteration 3/1000 | Loss: 0.00002532
Iteration 4/1000 | Loss: 0.00002208
Iteration 5/1000 | Loss: 0.00002055
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001770
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001755
Iteration 17/1000 | Loss: 0.00001754
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001746
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001739
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001735
Iteration 31/1000 | Loss: 0.00001735
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001728
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001725
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001718
Iteration 64/1000 | Loss: 0.00001718
Iteration 65/1000 | Loss: 0.00001718
Iteration 66/1000 | Loss: 0.00001718
Iteration 67/1000 | Loss: 0.00001718
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001717
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001717
Iteration 73/1000 | Loss: 0.00001717
Iteration 74/1000 | Loss: 0.00001717
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001717
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001716
Iteration 80/1000 | Loss: 0.00001716
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001713
Iteration 88/1000 | Loss: 0.00001713
Iteration 89/1000 | Loss: 0.00001713
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001713
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001712
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001712
Iteration 119/1000 | Loss: 0.00001712
Iteration 120/1000 | Loss: 0.00001712
Iteration 121/1000 | Loss: 0.00001712
Iteration 122/1000 | Loss: 0.00001712
Iteration 123/1000 | Loss: 0.00001712
Iteration 124/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.711502409307286e-05, 1.711502409307286e-05, 1.711502409307286e-05, 1.711502409307286e-05, 1.711502409307286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711502409307286e-05

Optimization complete. Final v2v error: 3.6032044887542725 mm

Highest mean error: 3.8461201190948486 mm for frame 19

Lowest mean error: 3.367661714553833 mm for frame 98

Saving results

Total time: 32.73799514770508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141959
Iteration 2/25 | Loss: 0.00246882
Iteration 3/25 | Loss: 0.00195036
Iteration 4/25 | Loss: 0.00193305
Iteration 5/25 | Loss: 0.00173419
Iteration 6/25 | Loss: 0.00168190
Iteration 7/25 | Loss: 0.00160875
Iteration 8/25 | Loss: 0.00154692
Iteration 9/25 | Loss: 0.00152489
Iteration 10/25 | Loss: 0.00150260
Iteration 11/25 | Loss: 0.00149496
Iteration 12/25 | Loss: 0.00148947
Iteration 13/25 | Loss: 0.00148532
Iteration 14/25 | Loss: 0.00148057
Iteration 15/25 | Loss: 0.00147853
Iteration 16/25 | Loss: 0.00147484
Iteration 17/25 | Loss: 0.00147966
Iteration 18/25 | Loss: 0.00148134
Iteration 19/25 | Loss: 0.00147904
Iteration 20/25 | Loss: 0.00147593
Iteration 21/25 | Loss: 0.00147490
Iteration 22/25 | Loss: 0.00147509
Iteration 23/25 | Loss: 0.00147525
Iteration 24/25 | Loss: 0.00146959
Iteration 25/25 | Loss: 0.00146968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25446725
Iteration 2/25 | Loss: 0.00268014
Iteration 3/25 | Loss: 0.00268013
Iteration 4/25 | Loss: 0.00237558
Iteration 5/25 | Loss: 0.00237558
Iteration 6/25 | Loss: 0.00237558
Iteration 7/25 | Loss: 0.00237558
Iteration 8/25 | Loss: 0.00237558
Iteration 9/25 | Loss: 0.00237558
Iteration 10/25 | Loss: 0.00237558
Iteration 11/25 | Loss: 0.00237558
Iteration 12/25 | Loss: 0.00237558
Iteration 13/25 | Loss: 0.00237558
Iteration 14/25 | Loss: 0.00237558
Iteration 15/25 | Loss: 0.00237558
Iteration 16/25 | Loss: 0.00237558
Iteration 17/25 | Loss: 0.00237558
Iteration 18/25 | Loss: 0.00237558
Iteration 19/25 | Loss: 0.00237558
Iteration 20/25 | Loss: 0.00237558
Iteration 21/25 | Loss: 0.00237558
Iteration 22/25 | Loss: 0.00237558
Iteration 23/25 | Loss: 0.00237558
Iteration 24/25 | Loss: 0.00237558
Iteration 25/25 | Loss: 0.00237558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237558
Iteration 2/1000 | Loss: 0.00125482
Iteration 3/1000 | Loss: 0.00025799
Iteration 4/1000 | Loss: 0.00110393
Iteration 5/1000 | Loss: 0.00017924
Iteration 6/1000 | Loss: 0.00032341
Iteration 7/1000 | Loss: 0.00027771
Iteration 8/1000 | Loss: 0.00064704
Iteration 9/1000 | Loss: 0.00016153
Iteration 10/1000 | Loss: 0.00062014
Iteration 11/1000 | Loss: 0.00016436
Iteration 12/1000 | Loss: 0.00017672
Iteration 13/1000 | Loss: 0.00023857
Iteration 14/1000 | Loss: 0.00019645
Iteration 15/1000 | Loss: 0.00013872
Iteration 16/1000 | Loss: 0.00007815
Iteration 17/1000 | Loss: 0.00015031
Iteration 18/1000 | Loss: 0.00020895
Iteration 19/1000 | Loss: 0.00015789
Iteration 20/1000 | Loss: 0.00052718
Iteration 21/1000 | Loss: 0.00021994
Iteration 22/1000 | Loss: 0.00023763
Iteration 23/1000 | Loss: 0.00043385
Iteration 24/1000 | Loss: 0.00046385
Iteration 25/1000 | Loss: 0.00039203
Iteration 26/1000 | Loss: 0.00087710
Iteration 27/1000 | Loss: 0.00147226
Iteration 28/1000 | Loss: 0.00021696
Iteration 29/1000 | Loss: 0.00008033
Iteration 30/1000 | Loss: 0.00006076
Iteration 31/1000 | Loss: 0.00006605
Iteration 32/1000 | Loss: 0.00015085
Iteration 33/1000 | Loss: 0.00032351
Iteration 34/1000 | Loss: 0.00005927
Iteration 35/1000 | Loss: 0.00006830
Iteration 36/1000 | Loss: 0.00039903
Iteration 37/1000 | Loss: 0.00064395
Iteration 38/1000 | Loss: 0.00017094
Iteration 39/1000 | Loss: 0.00074875
Iteration 40/1000 | Loss: 0.00037525
Iteration 41/1000 | Loss: 0.00056398
Iteration 42/1000 | Loss: 0.00041574
Iteration 43/1000 | Loss: 0.00011489
Iteration 44/1000 | Loss: 0.00027642
Iteration 45/1000 | Loss: 0.00042345
Iteration 46/1000 | Loss: 0.00018682
Iteration 47/1000 | Loss: 0.00028390
Iteration 48/1000 | Loss: 0.00043506
Iteration 49/1000 | Loss: 0.00035486
Iteration 50/1000 | Loss: 0.00006329
Iteration 51/1000 | Loss: 0.00061126
Iteration 52/1000 | Loss: 0.00039871
Iteration 53/1000 | Loss: 0.00076627
Iteration 54/1000 | Loss: 0.00026973
Iteration 55/1000 | Loss: 0.00008763
Iteration 56/1000 | Loss: 0.00013533
Iteration 57/1000 | Loss: 0.00045662
Iteration 58/1000 | Loss: 0.00051928
Iteration 59/1000 | Loss: 0.00048037
Iteration 60/1000 | Loss: 0.00017436
Iteration 61/1000 | Loss: 0.00014457
Iteration 62/1000 | Loss: 0.00009342
Iteration 63/1000 | Loss: 0.00016572
Iteration 64/1000 | Loss: 0.00015908
Iteration 65/1000 | Loss: 0.00021793
Iteration 66/1000 | Loss: 0.00009276
Iteration 67/1000 | Loss: 0.00018001
Iteration 68/1000 | Loss: 0.00017179
Iteration 69/1000 | Loss: 0.00024813
Iteration 70/1000 | Loss: 0.00030943
Iteration 71/1000 | Loss: 0.00008973
Iteration 72/1000 | Loss: 0.00030194
Iteration 73/1000 | Loss: 0.00009643
Iteration 74/1000 | Loss: 0.00005052
Iteration 75/1000 | Loss: 0.00011066
Iteration 76/1000 | Loss: 0.00010447
Iteration 77/1000 | Loss: 0.00004990
Iteration 78/1000 | Loss: 0.00004338
Iteration 79/1000 | Loss: 0.00004755
Iteration 80/1000 | Loss: 0.00049110
Iteration 81/1000 | Loss: 0.00028210
Iteration 82/1000 | Loss: 0.00017371
Iteration 83/1000 | Loss: 0.00009438
Iteration 84/1000 | Loss: 0.00005229
Iteration 85/1000 | Loss: 0.00008569
Iteration 86/1000 | Loss: 0.00005164
Iteration 87/1000 | Loss: 0.00010130
Iteration 88/1000 | Loss: 0.00004277
Iteration 89/1000 | Loss: 0.00004395
Iteration 90/1000 | Loss: 0.00044057
Iteration 91/1000 | Loss: 0.00021490
Iteration 92/1000 | Loss: 0.00027056
Iteration 93/1000 | Loss: 0.00004386
Iteration 94/1000 | Loss: 0.00030702
Iteration 95/1000 | Loss: 0.00037635
Iteration 96/1000 | Loss: 0.00038208
Iteration 97/1000 | Loss: 0.00038981
Iteration 98/1000 | Loss: 0.00026357
Iteration 99/1000 | Loss: 0.00042022
Iteration 100/1000 | Loss: 0.00004898
Iteration 101/1000 | Loss: 0.00034982
Iteration 102/1000 | Loss: 0.00032216
Iteration 103/1000 | Loss: 0.00004754
Iteration 104/1000 | Loss: 0.00003968
Iteration 105/1000 | Loss: 0.00003360
Iteration 106/1000 | Loss: 0.00004142
Iteration 107/1000 | Loss: 0.00004170
Iteration 108/1000 | Loss: 0.00003653
Iteration 109/1000 | Loss: 0.00003934
Iteration 110/1000 | Loss: 0.00003133
Iteration 111/1000 | Loss: 0.00002978
Iteration 112/1000 | Loss: 0.00002892
Iteration 113/1000 | Loss: 0.00003158
Iteration 114/1000 | Loss: 0.00003941
Iteration 115/1000 | Loss: 0.00003413
Iteration 116/1000 | Loss: 0.00051082
Iteration 117/1000 | Loss: 0.00027737
Iteration 118/1000 | Loss: 0.00037260
Iteration 119/1000 | Loss: 0.00004655
Iteration 120/1000 | Loss: 0.00003668
Iteration 121/1000 | Loss: 0.00003812
Iteration 122/1000 | Loss: 0.00003793
Iteration 123/1000 | Loss: 0.00004087
Iteration 124/1000 | Loss: 0.00004368
Iteration 125/1000 | Loss: 0.00003802
Iteration 126/1000 | Loss: 0.00004194
Iteration 127/1000 | Loss: 0.00003728
Iteration 128/1000 | Loss: 0.00003840
Iteration 129/1000 | Loss: 0.00004381
Iteration 130/1000 | Loss: 0.00004579
Iteration 131/1000 | Loss: 0.00003969
Iteration 132/1000 | Loss: 0.00003992
Iteration 133/1000 | Loss: 0.00003956
Iteration 134/1000 | Loss: 0.00003423
Iteration 135/1000 | Loss: 0.00003844
Iteration 136/1000 | Loss: 0.00004168
Iteration 137/1000 | Loss: 0.00004293
Iteration 138/1000 | Loss: 0.00004272
Iteration 139/1000 | Loss: 0.00003454
Iteration 140/1000 | Loss: 0.00004692
Iteration 141/1000 | Loss: 0.00003695
Iteration 142/1000 | Loss: 0.00003596
Iteration 143/1000 | Loss: 0.00002685
Iteration 144/1000 | Loss: 0.00002572
Iteration 145/1000 | Loss: 0.00002762
Iteration 146/1000 | Loss: 0.00002565
Iteration 147/1000 | Loss: 0.00002524
Iteration 148/1000 | Loss: 0.00002501
Iteration 149/1000 | Loss: 0.00002501
Iteration 150/1000 | Loss: 0.00003687
Iteration 151/1000 | Loss: 0.00002849
Iteration 152/1000 | Loss: 0.00002587
Iteration 153/1000 | Loss: 0.00003335
Iteration 154/1000 | Loss: 0.00003747
Iteration 155/1000 | Loss: 0.00002791
Iteration 156/1000 | Loss: 0.00003578
Iteration 157/1000 | Loss: 0.00003028
Iteration 158/1000 | Loss: 0.00002808
Iteration 159/1000 | Loss: 0.00003580
Iteration 160/1000 | Loss: 0.00003475
Iteration 161/1000 | Loss: 0.00003782
Iteration 162/1000 | Loss: 0.00003419
Iteration 163/1000 | Loss: 0.00003282
Iteration 164/1000 | Loss: 0.00003152
Iteration 165/1000 | Loss: 0.00002464
Iteration 166/1000 | Loss: 0.00003131
Iteration 167/1000 | Loss: 0.00003155
Iteration 168/1000 | Loss: 0.00003603
Iteration 169/1000 | Loss: 0.00003119
Iteration 170/1000 | Loss: 0.00003174
Iteration 171/1000 | Loss: 0.00003598
Iteration 172/1000 | Loss: 0.00003085
Iteration 173/1000 | Loss: 0.00003027
Iteration 174/1000 | Loss: 0.00003043
Iteration 175/1000 | Loss: 0.00002960
Iteration 176/1000 | Loss: 0.00003475
Iteration 177/1000 | Loss: 0.00003503
Iteration 178/1000 | Loss: 0.00003668
Iteration 179/1000 | Loss: 0.00004384
Iteration 180/1000 | Loss: 0.00003649
Iteration 181/1000 | Loss: 0.00003411
Iteration 182/1000 | Loss: 0.00003406
Iteration 183/1000 | Loss: 0.00002553
Iteration 184/1000 | Loss: 0.00002475
Iteration 185/1000 | Loss: 0.00002435
Iteration 186/1000 | Loss: 0.00002433
Iteration 187/1000 | Loss: 0.00002433
Iteration 188/1000 | Loss: 0.00002433
Iteration 189/1000 | Loss: 0.00002432
Iteration 190/1000 | Loss: 0.00002432
Iteration 191/1000 | Loss: 0.00002431
Iteration 192/1000 | Loss: 0.00002431
Iteration 193/1000 | Loss: 0.00002431
Iteration 194/1000 | Loss: 0.00002431
Iteration 195/1000 | Loss: 0.00002431
Iteration 196/1000 | Loss: 0.00002431
Iteration 197/1000 | Loss: 0.00002431
Iteration 198/1000 | Loss: 0.00002431
Iteration 199/1000 | Loss: 0.00002431
Iteration 200/1000 | Loss: 0.00002431
Iteration 201/1000 | Loss: 0.00002431
Iteration 202/1000 | Loss: 0.00002430
Iteration 203/1000 | Loss: 0.00002430
Iteration 204/1000 | Loss: 0.00002430
Iteration 205/1000 | Loss: 0.00002430
Iteration 206/1000 | Loss: 0.00002430
Iteration 207/1000 | Loss: 0.00002430
Iteration 208/1000 | Loss: 0.00002430
Iteration 209/1000 | Loss: 0.00002430
Iteration 210/1000 | Loss: 0.00002430
Iteration 211/1000 | Loss: 0.00002430
Iteration 212/1000 | Loss: 0.00002429
Iteration 213/1000 | Loss: 0.00002429
Iteration 214/1000 | Loss: 0.00002429
Iteration 215/1000 | Loss: 0.00002429
Iteration 216/1000 | Loss: 0.00002429
Iteration 217/1000 | Loss: 0.00002429
Iteration 218/1000 | Loss: 0.00002429
Iteration 219/1000 | Loss: 0.00002429
Iteration 220/1000 | Loss: 0.00002429
Iteration 221/1000 | Loss: 0.00002428
Iteration 222/1000 | Loss: 0.00002428
Iteration 223/1000 | Loss: 0.00002428
Iteration 224/1000 | Loss: 0.00002428
Iteration 225/1000 | Loss: 0.00002428
Iteration 226/1000 | Loss: 0.00002428
Iteration 227/1000 | Loss: 0.00002427
Iteration 228/1000 | Loss: 0.00002427
Iteration 229/1000 | Loss: 0.00002426
Iteration 230/1000 | Loss: 0.00002426
Iteration 231/1000 | Loss: 0.00002424
Iteration 232/1000 | Loss: 0.00002424
Iteration 233/1000 | Loss: 0.00002424
Iteration 234/1000 | Loss: 0.00002423
Iteration 235/1000 | Loss: 0.00002423
Iteration 236/1000 | Loss: 0.00002423
Iteration 237/1000 | Loss: 0.00002423
Iteration 238/1000 | Loss: 0.00002422
Iteration 239/1000 | Loss: 0.00002422
Iteration 240/1000 | Loss: 0.00002422
Iteration 241/1000 | Loss: 0.00002422
Iteration 242/1000 | Loss: 0.00002422
Iteration 243/1000 | Loss: 0.00002421
Iteration 244/1000 | Loss: 0.00002421
Iteration 245/1000 | Loss: 0.00002420
Iteration 246/1000 | Loss: 0.00003147
Iteration 247/1000 | Loss: 0.00003117
Iteration 248/1000 | Loss: 0.00003531
Iteration 249/1000 | Loss: 0.00002583
Iteration 250/1000 | Loss: 0.00002452
Iteration 251/1000 | Loss: 0.00002917
Iteration 252/1000 | Loss: 0.00003449
Iteration 253/1000 | Loss: 0.00003458
Iteration 254/1000 | Loss: 0.00002700
Iteration 255/1000 | Loss: 0.00003039
Iteration 256/1000 | Loss: 0.00002429
Iteration 257/1000 | Loss: 0.00002416
Iteration 258/1000 | Loss: 0.00002415
Iteration 259/1000 | Loss: 0.00002414
Iteration 260/1000 | Loss: 0.00002412
Iteration 261/1000 | Loss: 0.00002412
Iteration 262/1000 | Loss: 0.00002411
Iteration 263/1000 | Loss: 0.00002411
Iteration 264/1000 | Loss: 0.00002411
Iteration 265/1000 | Loss: 0.00002411
Iteration 266/1000 | Loss: 0.00002411
Iteration 267/1000 | Loss: 0.00002411
Iteration 268/1000 | Loss: 0.00002410
Iteration 269/1000 | Loss: 0.00002410
Iteration 270/1000 | Loss: 0.00002410
Iteration 271/1000 | Loss: 0.00002410
Iteration 272/1000 | Loss: 0.00002410
Iteration 273/1000 | Loss: 0.00002410
Iteration 274/1000 | Loss: 0.00002410
Iteration 275/1000 | Loss: 0.00002410
Iteration 276/1000 | Loss: 0.00002410
Iteration 277/1000 | Loss: 0.00002410
Iteration 278/1000 | Loss: 0.00002410
Iteration 279/1000 | Loss: 0.00002410
Iteration 280/1000 | Loss: 0.00002409
Iteration 281/1000 | Loss: 0.00002409
Iteration 282/1000 | Loss: 0.00002409
Iteration 283/1000 | Loss: 0.00002409
Iteration 284/1000 | Loss: 0.00002409
Iteration 285/1000 | Loss: 0.00002409
Iteration 286/1000 | Loss: 0.00002409
Iteration 287/1000 | Loss: 0.00002409
Iteration 288/1000 | Loss: 0.00002409
Iteration 289/1000 | Loss: 0.00002409
Iteration 290/1000 | Loss: 0.00002409
Iteration 291/1000 | Loss: 0.00002409
Iteration 292/1000 | Loss: 0.00002409
Iteration 293/1000 | Loss: 0.00002409
Iteration 294/1000 | Loss: 0.00002409
Iteration 295/1000 | Loss: 0.00002409
Iteration 296/1000 | Loss: 0.00002409
Iteration 297/1000 | Loss: 0.00002409
Iteration 298/1000 | Loss: 0.00002409
Iteration 299/1000 | Loss: 0.00002409
Iteration 300/1000 | Loss: 0.00002409
Iteration 301/1000 | Loss: 0.00002409
Iteration 302/1000 | Loss: 0.00002409
Iteration 303/1000 | Loss: 0.00002409
Iteration 304/1000 | Loss: 0.00002409
Iteration 305/1000 | Loss: 0.00002409
Iteration 306/1000 | Loss: 0.00002409
Iteration 307/1000 | Loss: 0.00002409
Iteration 308/1000 | Loss: 0.00002409
Iteration 309/1000 | Loss: 0.00002409
Iteration 310/1000 | Loss: 0.00002409
Iteration 311/1000 | Loss: 0.00002409
Iteration 312/1000 | Loss: 0.00002409
Iteration 313/1000 | Loss: 0.00002409
Iteration 314/1000 | Loss: 0.00002409
Iteration 315/1000 | Loss: 0.00002409
Iteration 316/1000 | Loss: 0.00002409
Iteration 317/1000 | Loss: 0.00002409
Iteration 318/1000 | Loss: 0.00002409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [2.4092230887617916e-05, 2.4092230887617916e-05, 2.4092230887617916e-05, 2.4092230887617916e-05, 2.4092230887617916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4092230887617916e-05

Optimization complete. Final v2v error: 4.255746841430664 mm

Highest mean error: 8.581785202026367 mm for frame 8

Lowest mean error: 3.704514265060425 mm for frame 16

Saving results

Total time: 365.5315008163452
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413689
Iteration 2/25 | Loss: 0.00145288
Iteration 3/25 | Loss: 0.00137645
Iteration 4/25 | Loss: 0.00136900
Iteration 5/25 | Loss: 0.00136669
Iteration 6/25 | Loss: 0.00136647
Iteration 7/25 | Loss: 0.00136647
Iteration 8/25 | Loss: 0.00136647
Iteration 9/25 | Loss: 0.00136647
Iteration 10/25 | Loss: 0.00136647
Iteration 11/25 | Loss: 0.00136647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013664689613506198, 0.0013664689613506198, 0.0013664689613506198, 0.0013664689613506198, 0.0013664689613506198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013664689613506198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10009503
Iteration 2/25 | Loss: 0.00232593
Iteration 3/25 | Loss: 0.00232593
Iteration 4/25 | Loss: 0.00232593
Iteration 5/25 | Loss: 0.00232593
Iteration 6/25 | Loss: 0.00232593
Iteration 7/25 | Loss: 0.00232593
Iteration 8/25 | Loss: 0.00232593
Iteration 9/25 | Loss: 0.00232593
Iteration 10/25 | Loss: 0.00232593
Iteration 11/25 | Loss: 0.00232593
Iteration 12/25 | Loss: 0.00232593
Iteration 13/25 | Loss: 0.00232593
Iteration 14/25 | Loss: 0.00232593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002325930166989565, 0.002325930166989565, 0.002325930166989565, 0.002325930166989565, 0.002325930166989565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002325930166989565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232593
Iteration 2/1000 | Loss: 0.00002950
Iteration 3/1000 | Loss: 0.00002456
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002136
Iteration 6/1000 | Loss: 0.00002058
Iteration 7/1000 | Loss: 0.00002023
Iteration 8/1000 | Loss: 0.00001990
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001972
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001970
Iteration 17/1000 | Loss: 0.00001967
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001965
Iteration 22/1000 | Loss: 0.00001964
Iteration 23/1000 | Loss: 0.00001964
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001953
Iteration 31/1000 | Loss: 0.00001953
Iteration 32/1000 | Loss: 0.00001952
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001945
Iteration 39/1000 | Loss: 0.00001943
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001935
Iteration 54/1000 | Loss: 0.00001935
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001934
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001932
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001931
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001928
Iteration 80/1000 | Loss: 0.00001928
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001926
Iteration 95/1000 | Loss: 0.00001926
Iteration 96/1000 | Loss: 0.00001926
Iteration 97/1000 | Loss: 0.00001925
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001924
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001924
Iteration 114/1000 | Loss: 0.00001924
Iteration 115/1000 | Loss: 0.00001924
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Iteration 137/1000 | Loss: 0.00001922
Iteration 138/1000 | Loss: 0.00001922
Iteration 139/1000 | Loss: 0.00001922
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00001922
Iteration 142/1000 | Loss: 0.00001922
Iteration 143/1000 | Loss: 0.00001922
Iteration 144/1000 | Loss: 0.00001922
Iteration 145/1000 | Loss: 0.00001922
Iteration 146/1000 | Loss: 0.00001922
Iteration 147/1000 | Loss: 0.00001922
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001921
Iteration 150/1000 | Loss: 0.00001921
Iteration 151/1000 | Loss: 0.00001921
Iteration 152/1000 | Loss: 0.00001921
Iteration 153/1000 | Loss: 0.00001921
Iteration 154/1000 | Loss: 0.00001921
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001921
Iteration 159/1000 | Loss: 0.00001921
Iteration 160/1000 | Loss: 0.00001921
Iteration 161/1000 | Loss: 0.00001921
Iteration 162/1000 | Loss: 0.00001921
Iteration 163/1000 | Loss: 0.00001921
Iteration 164/1000 | Loss: 0.00001921
Iteration 165/1000 | Loss: 0.00001921
Iteration 166/1000 | Loss: 0.00001921
Iteration 167/1000 | Loss: 0.00001921
Iteration 168/1000 | Loss: 0.00001921
Iteration 169/1000 | Loss: 0.00001921
Iteration 170/1000 | Loss: 0.00001921
Iteration 171/1000 | Loss: 0.00001921
Iteration 172/1000 | Loss: 0.00001921
Iteration 173/1000 | Loss: 0.00001921
Iteration 174/1000 | Loss: 0.00001921
Iteration 175/1000 | Loss: 0.00001921
Iteration 176/1000 | Loss: 0.00001921
Iteration 177/1000 | Loss: 0.00001921
Iteration 178/1000 | Loss: 0.00001921
Iteration 179/1000 | Loss: 0.00001921
Iteration 180/1000 | Loss: 0.00001921
Iteration 181/1000 | Loss: 0.00001921
Iteration 182/1000 | Loss: 0.00001921
Iteration 183/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.920959766721353e-05, 1.920959766721353e-05, 1.920959766721353e-05, 1.920959766721353e-05, 1.920959766721353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.920959766721353e-05

Optimization complete. Final v2v error: 3.772191047668457 mm

Highest mean error: 4.129276275634766 mm for frame 84

Lowest mean error: 3.4011776447296143 mm for frame 19

Saving results

Total time: 37.331780195236206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111208
Iteration 2/25 | Loss: 0.00248430
Iteration 3/25 | Loss: 0.00219087
Iteration 4/25 | Loss: 0.00212314
Iteration 5/25 | Loss: 0.00207598
Iteration 6/25 | Loss: 0.00206546
Iteration 7/25 | Loss: 0.00206044
Iteration 8/25 | Loss: 0.00205868
Iteration 9/25 | Loss: 0.00205381
Iteration 10/25 | Loss: 0.00204373
Iteration 11/25 | Loss: 0.00203054
Iteration 12/25 | Loss: 0.00200718
Iteration 13/25 | Loss: 0.00199070
Iteration 14/25 | Loss: 0.00204813
Iteration 15/25 | Loss: 0.00192547
Iteration 16/25 | Loss: 0.00179325
Iteration 17/25 | Loss: 0.00173533
Iteration 18/25 | Loss: 0.00171608
Iteration 19/25 | Loss: 0.00169420
Iteration 20/25 | Loss: 0.00169269
Iteration 21/25 | Loss: 0.00168685
Iteration 22/25 | Loss: 0.00167800
Iteration 23/25 | Loss: 0.00166843
Iteration 24/25 | Loss: 0.00165982
Iteration 25/25 | Loss: 0.00165076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16744423
Iteration 2/25 | Loss: 0.00372945
Iteration 3/25 | Loss: 0.00372945
Iteration 4/25 | Loss: 0.00372945
Iteration 5/25 | Loss: 0.00372945
Iteration 6/25 | Loss: 0.00372945
Iteration 7/25 | Loss: 0.00372945
Iteration 8/25 | Loss: 0.00372945
Iteration 9/25 | Loss: 0.00372945
Iteration 10/25 | Loss: 0.00372945
Iteration 11/25 | Loss: 0.00372945
Iteration 12/25 | Loss: 0.00372945
Iteration 13/25 | Loss: 0.00372945
Iteration 14/25 | Loss: 0.00372945
Iteration 15/25 | Loss: 0.00372945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003729448886588216, 0.003729448886588216, 0.003729448886588216, 0.003729448886588216, 0.003729448886588216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003729448886588216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00372945
Iteration 2/1000 | Loss: 0.00068522
Iteration 3/1000 | Loss: 0.00148789
Iteration 4/1000 | Loss: 0.00035204
Iteration 5/1000 | Loss: 0.00034893
Iteration 6/1000 | Loss: 0.00030689
Iteration 7/1000 | Loss: 0.00103132
Iteration 8/1000 | Loss: 0.00167721
Iteration 9/1000 | Loss: 0.00034492
Iteration 10/1000 | Loss: 0.00069486
Iteration 11/1000 | Loss: 0.00019293
Iteration 12/1000 | Loss: 0.00102546
Iteration 13/1000 | Loss: 0.00014902
Iteration 14/1000 | Loss: 0.00031999
Iteration 15/1000 | Loss: 0.00029193
Iteration 16/1000 | Loss: 0.00023020
Iteration 17/1000 | Loss: 0.00009961
Iteration 18/1000 | Loss: 0.00008790
Iteration 19/1000 | Loss: 0.00008093
Iteration 20/1000 | Loss: 0.00007621
Iteration 21/1000 | Loss: 0.00057444
Iteration 22/1000 | Loss: 0.00146534
Iteration 23/1000 | Loss: 0.00026893
Iteration 24/1000 | Loss: 0.00023426
Iteration 25/1000 | Loss: 0.00007009
Iteration 26/1000 | Loss: 0.00006224
Iteration 27/1000 | Loss: 0.00005838
Iteration 28/1000 | Loss: 0.00005570
Iteration 29/1000 | Loss: 0.00050843
Iteration 30/1000 | Loss: 0.00006422
Iteration 31/1000 | Loss: 0.00005511
Iteration 32/1000 | Loss: 0.00005240
Iteration 33/1000 | Loss: 0.00005079
Iteration 34/1000 | Loss: 0.00056532
Iteration 35/1000 | Loss: 0.00033590
Iteration 36/1000 | Loss: 0.00035265
Iteration 37/1000 | Loss: 0.00005186
Iteration 38/1000 | Loss: 0.00004834
Iteration 39/1000 | Loss: 0.00004712
Iteration 40/1000 | Loss: 0.00117098
Iteration 41/1000 | Loss: 0.00061737
Iteration 42/1000 | Loss: 0.00095592
Iteration 43/1000 | Loss: 0.00005758
Iteration 44/1000 | Loss: 0.00004655
Iteration 45/1000 | Loss: 0.00004329
Iteration 46/1000 | Loss: 0.00004121
Iteration 47/1000 | Loss: 0.00003982
Iteration 48/1000 | Loss: 0.00003924
Iteration 49/1000 | Loss: 0.00003881
Iteration 50/1000 | Loss: 0.00003859
Iteration 51/1000 | Loss: 0.00003826
Iteration 52/1000 | Loss: 0.00003812
Iteration 53/1000 | Loss: 0.00003792
Iteration 54/1000 | Loss: 0.00003789
Iteration 55/1000 | Loss: 0.00003788
Iteration 56/1000 | Loss: 0.00049606
Iteration 57/1000 | Loss: 0.00073306
Iteration 58/1000 | Loss: 0.00211230
Iteration 59/1000 | Loss: 0.00136810
Iteration 60/1000 | Loss: 0.00117086
Iteration 61/1000 | Loss: 0.00169588
Iteration 62/1000 | Loss: 0.00037213
Iteration 63/1000 | Loss: 0.00113990
Iteration 64/1000 | Loss: 0.00115663
Iteration 65/1000 | Loss: 0.00064489
Iteration 66/1000 | Loss: 0.00040594
Iteration 67/1000 | Loss: 0.00087872
Iteration 68/1000 | Loss: 0.00137910
Iteration 69/1000 | Loss: 0.00071964
Iteration 70/1000 | Loss: 0.00006115
Iteration 71/1000 | Loss: 0.00004802
Iteration 72/1000 | Loss: 0.00004354
Iteration 73/1000 | Loss: 0.00004017
Iteration 74/1000 | Loss: 0.00003807
Iteration 75/1000 | Loss: 0.00003680
Iteration 76/1000 | Loss: 0.00003608
Iteration 77/1000 | Loss: 0.00003568
Iteration 78/1000 | Loss: 0.00003546
Iteration 79/1000 | Loss: 0.00003526
Iteration 80/1000 | Loss: 0.00003522
Iteration 81/1000 | Loss: 0.00003521
Iteration 82/1000 | Loss: 0.00003520
Iteration 83/1000 | Loss: 0.00003520
Iteration 84/1000 | Loss: 0.00003518
Iteration 85/1000 | Loss: 0.00003518
Iteration 86/1000 | Loss: 0.00003518
Iteration 87/1000 | Loss: 0.00003517
Iteration 88/1000 | Loss: 0.00003516
Iteration 89/1000 | Loss: 0.00003516
Iteration 90/1000 | Loss: 0.00003516
Iteration 91/1000 | Loss: 0.00003515
Iteration 92/1000 | Loss: 0.00003515
Iteration 93/1000 | Loss: 0.00003513
Iteration 94/1000 | Loss: 0.00003512
Iteration 95/1000 | Loss: 0.00003512
Iteration 96/1000 | Loss: 0.00003512
Iteration 97/1000 | Loss: 0.00003512
Iteration 98/1000 | Loss: 0.00003512
Iteration 99/1000 | Loss: 0.00003512
Iteration 100/1000 | Loss: 0.00003512
Iteration 101/1000 | Loss: 0.00003512
Iteration 102/1000 | Loss: 0.00003512
Iteration 103/1000 | Loss: 0.00003512
Iteration 104/1000 | Loss: 0.00003511
Iteration 105/1000 | Loss: 0.00003510
Iteration 106/1000 | Loss: 0.00003509
Iteration 107/1000 | Loss: 0.00003509
Iteration 108/1000 | Loss: 0.00003506
Iteration 109/1000 | Loss: 0.00003506
Iteration 110/1000 | Loss: 0.00003505
Iteration 111/1000 | Loss: 0.00003504
Iteration 112/1000 | Loss: 0.00003503
Iteration 113/1000 | Loss: 0.00003503
Iteration 114/1000 | Loss: 0.00003503
Iteration 115/1000 | Loss: 0.00003503
Iteration 116/1000 | Loss: 0.00003502
Iteration 117/1000 | Loss: 0.00003502
Iteration 118/1000 | Loss: 0.00003502
Iteration 119/1000 | Loss: 0.00003502
Iteration 120/1000 | Loss: 0.00003502
Iteration 121/1000 | Loss: 0.00003502
Iteration 122/1000 | Loss: 0.00003502
Iteration 123/1000 | Loss: 0.00003502
Iteration 124/1000 | Loss: 0.00003502
Iteration 125/1000 | Loss: 0.00003502
Iteration 126/1000 | Loss: 0.00003502
Iteration 127/1000 | Loss: 0.00003501
Iteration 128/1000 | Loss: 0.00003501
Iteration 129/1000 | Loss: 0.00003501
Iteration 130/1000 | Loss: 0.00003501
Iteration 131/1000 | Loss: 0.00003501
Iteration 132/1000 | Loss: 0.00003501
Iteration 133/1000 | Loss: 0.00003500
Iteration 134/1000 | Loss: 0.00003500
Iteration 135/1000 | Loss: 0.00003500
Iteration 136/1000 | Loss: 0.00003500
Iteration 137/1000 | Loss: 0.00003500
Iteration 138/1000 | Loss: 0.00003499
Iteration 139/1000 | Loss: 0.00003499
Iteration 140/1000 | Loss: 0.00003499
Iteration 141/1000 | Loss: 0.00003499
Iteration 142/1000 | Loss: 0.00003498
Iteration 143/1000 | Loss: 0.00003498
Iteration 144/1000 | Loss: 0.00003497
Iteration 145/1000 | Loss: 0.00003497
Iteration 146/1000 | Loss: 0.00003496
Iteration 147/1000 | Loss: 0.00003496
Iteration 148/1000 | Loss: 0.00003496
Iteration 149/1000 | Loss: 0.00087890
Iteration 150/1000 | Loss: 0.00122560
Iteration 151/1000 | Loss: 0.00088053
Iteration 152/1000 | Loss: 0.00079781
Iteration 153/1000 | Loss: 0.00020919
Iteration 154/1000 | Loss: 0.00028499
Iteration 155/1000 | Loss: 0.00013138
Iteration 156/1000 | Loss: 0.00064413
Iteration 157/1000 | Loss: 0.00097294
Iteration 158/1000 | Loss: 0.00051364
Iteration 159/1000 | Loss: 0.00060277
Iteration 160/1000 | Loss: 0.00081762
Iteration 161/1000 | Loss: 0.00076016
Iteration 162/1000 | Loss: 0.00047635
Iteration 163/1000 | Loss: 0.00042863
Iteration 164/1000 | Loss: 0.00024331
Iteration 165/1000 | Loss: 0.00051582
Iteration 166/1000 | Loss: 0.00046690
Iteration 167/1000 | Loss: 0.00037148
Iteration 168/1000 | Loss: 0.00041983
Iteration 169/1000 | Loss: 0.00027561
Iteration 170/1000 | Loss: 0.00020877
Iteration 171/1000 | Loss: 0.00013249
Iteration 172/1000 | Loss: 0.00010160
Iteration 173/1000 | Loss: 0.00033921
Iteration 174/1000 | Loss: 0.00031700
Iteration 175/1000 | Loss: 0.00018429
Iteration 176/1000 | Loss: 0.00024751
Iteration 177/1000 | Loss: 0.00038367
Iteration 178/1000 | Loss: 0.00030107
Iteration 179/1000 | Loss: 0.00031909
Iteration 180/1000 | Loss: 0.00025210
Iteration 181/1000 | Loss: 0.00022759
Iteration 182/1000 | Loss: 0.00017460
Iteration 183/1000 | Loss: 0.00034369
Iteration 184/1000 | Loss: 0.00004733
Iteration 185/1000 | Loss: 0.00003825
Iteration 186/1000 | Loss: 0.00017226
Iteration 187/1000 | Loss: 0.00013763
Iteration 188/1000 | Loss: 0.00007012
Iteration 189/1000 | Loss: 0.00030086
Iteration 190/1000 | Loss: 0.00029389
Iteration 191/1000 | Loss: 0.00027292
Iteration 192/1000 | Loss: 0.00051269
Iteration 193/1000 | Loss: 0.00029295
Iteration 194/1000 | Loss: 0.00027849
Iteration 195/1000 | Loss: 0.00023183
Iteration 196/1000 | Loss: 0.00003712
Iteration 197/1000 | Loss: 0.00003440
Iteration 198/1000 | Loss: 0.00003172
Iteration 199/1000 | Loss: 0.00015075
Iteration 200/1000 | Loss: 0.00003390
Iteration 201/1000 | Loss: 0.00003056
Iteration 202/1000 | Loss: 0.00002913
Iteration 203/1000 | Loss: 0.00002843
Iteration 204/1000 | Loss: 0.00002791
Iteration 205/1000 | Loss: 0.00002767
Iteration 206/1000 | Loss: 0.00002742
Iteration 207/1000 | Loss: 0.00002730
Iteration 208/1000 | Loss: 0.00002719
Iteration 209/1000 | Loss: 0.00002719
Iteration 210/1000 | Loss: 0.00002715
Iteration 211/1000 | Loss: 0.00002714
Iteration 212/1000 | Loss: 0.00002714
Iteration 213/1000 | Loss: 0.00002714
Iteration 214/1000 | Loss: 0.00002713
Iteration 215/1000 | Loss: 0.00002713
Iteration 216/1000 | Loss: 0.00002713
Iteration 217/1000 | Loss: 0.00002713
Iteration 218/1000 | Loss: 0.00002713
Iteration 219/1000 | Loss: 0.00002713
Iteration 220/1000 | Loss: 0.00002713
Iteration 221/1000 | Loss: 0.00002712
Iteration 222/1000 | Loss: 0.00002709
Iteration 223/1000 | Loss: 0.00002709
Iteration 224/1000 | Loss: 0.00002708
Iteration 225/1000 | Loss: 0.00002708
Iteration 226/1000 | Loss: 0.00002708
Iteration 227/1000 | Loss: 0.00002708
Iteration 228/1000 | Loss: 0.00002707
Iteration 229/1000 | Loss: 0.00002707
Iteration 230/1000 | Loss: 0.00002707
Iteration 231/1000 | Loss: 0.00002706
Iteration 232/1000 | Loss: 0.00002706
Iteration 233/1000 | Loss: 0.00002705
Iteration 234/1000 | Loss: 0.00002705
Iteration 235/1000 | Loss: 0.00002704
Iteration 236/1000 | Loss: 0.00002704
Iteration 237/1000 | Loss: 0.00002704
Iteration 238/1000 | Loss: 0.00002703
Iteration 239/1000 | Loss: 0.00002703
Iteration 240/1000 | Loss: 0.00002703
Iteration 241/1000 | Loss: 0.00002702
Iteration 242/1000 | Loss: 0.00002702
Iteration 243/1000 | Loss: 0.00002702
Iteration 244/1000 | Loss: 0.00002702
Iteration 245/1000 | Loss: 0.00002701
Iteration 246/1000 | Loss: 0.00002701
Iteration 247/1000 | Loss: 0.00002701
Iteration 248/1000 | Loss: 0.00002700
Iteration 249/1000 | Loss: 0.00002700
Iteration 250/1000 | Loss: 0.00002700
Iteration 251/1000 | Loss: 0.00002700
Iteration 252/1000 | Loss: 0.00002699
Iteration 253/1000 | Loss: 0.00002699
Iteration 254/1000 | Loss: 0.00002699
Iteration 255/1000 | Loss: 0.00002697
Iteration 256/1000 | Loss: 0.00002697
Iteration 257/1000 | Loss: 0.00002697
Iteration 258/1000 | Loss: 0.00002697
Iteration 259/1000 | Loss: 0.00002697
Iteration 260/1000 | Loss: 0.00002696
Iteration 261/1000 | Loss: 0.00002696
Iteration 262/1000 | Loss: 0.00002696
Iteration 263/1000 | Loss: 0.00002696
Iteration 264/1000 | Loss: 0.00002696
Iteration 265/1000 | Loss: 0.00002696
Iteration 266/1000 | Loss: 0.00002695
Iteration 267/1000 | Loss: 0.00002695
Iteration 268/1000 | Loss: 0.00002695
Iteration 269/1000 | Loss: 0.00002695
Iteration 270/1000 | Loss: 0.00002695
Iteration 271/1000 | Loss: 0.00002695
Iteration 272/1000 | Loss: 0.00048881
Iteration 273/1000 | Loss: 0.00028256
Iteration 274/1000 | Loss: 0.00002722
Iteration 275/1000 | Loss: 0.00002695
Iteration 276/1000 | Loss: 0.00002694
Iteration 277/1000 | Loss: 0.00002694
Iteration 278/1000 | Loss: 0.00002694
Iteration 279/1000 | Loss: 0.00002693
Iteration 280/1000 | Loss: 0.00002693
Iteration 281/1000 | Loss: 0.00002692
Iteration 282/1000 | Loss: 0.00002692
Iteration 283/1000 | Loss: 0.00047730
Iteration 284/1000 | Loss: 0.00044347
Iteration 285/1000 | Loss: 0.00005450
Iteration 286/1000 | Loss: 0.00056961
Iteration 287/1000 | Loss: 0.00053679
Iteration 288/1000 | Loss: 0.00004513
Iteration 289/1000 | Loss: 0.00003474
Iteration 290/1000 | Loss: 0.00015993
Iteration 291/1000 | Loss: 0.00056321
Iteration 292/1000 | Loss: 0.00054137
Iteration 293/1000 | Loss: 0.00004044
Iteration 294/1000 | Loss: 0.00003187
Iteration 295/1000 | Loss: 0.00002748
Iteration 296/1000 | Loss: 0.00002592
Iteration 297/1000 | Loss: 0.00002510
Iteration 298/1000 | Loss: 0.00002472
Iteration 299/1000 | Loss: 0.00002470
Iteration 300/1000 | Loss: 0.00002469
Iteration 301/1000 | Loss: 0.00002456
Iteration 302/1000 | Loss: 0.00002456
Iteration 303/1000 | Loss: 0.00002446
Iteration 304/1000 | Loss: 0.00002444
Iteration 305/1000 | Loss: 0.00002444
Iteration 306/1000 | Loss: 0.00002440
Iteration 307/1000 | Loss: 0.00002437
Iteration 308/1000 | Loss: 0.00002437
Iteration 309/1000 | Loss: 0.00002434
Iteration 310/1000 | Loss: 0.00002434
Iteration 311/1000 | Loss: 0.00002433
Iteration 312/1000 | Loss: 0.00002433
Iteration 313/1000 | Loss: 0.00002432
Iteration 314/1000 | Loss: 0.00002432
Iteration 315/1000 | Loss: 0.00002432
Iteration 316/1000 | Loss: 0.00002432
Iteration 317/1000 | Loss: 0.00002432
Iteration 318/1000 | Loss: 0.00002432
Iteration 319/1000 | Loss: 0.00002432
Iteration 320/1000 | Loss: 0.00002432
Iteration 321/1000 | Loss: 0.00002432
Iteration 322/1000 | Loss: 0.00002432
Iteration 323/1000 | Loss: 0.00002432
Iteration 324/1000 | Loss: 0.00002432
Iteration 325/1000 | Loss: 0.00002431
Iteration 326/1000 | Loss: 0.00002431
Iteration 327/1000 | Loss: 0.00002431
Iteration 328/1000 | Loss: 0.00002428
Iteration 329/1000 | Loss: 0.00002428
Iteration 330/1000 | Loss: 0.00002428
Iteration 331/1000 | Loss: 0.00002428
Iteration 332/1000 | Loss: 0.00002428
Iteration 333/1000 | Loss: 0.00002428
Iteration 334/1000 | Loss: 0.00002427
Iteration 335/1000 | Loss: 0.00002427
Iteration 336/1000 | Loss: 0.00002427
Iteration 337/1000 | Loss: 0.00002427
Iteration 338/1000 | Loss: 0.00002427
Iteration 339/1000 | Loss: 0.00002426
Iteration 340/1000 | Loss: 0.00002426
Iteration 341/1000 | Loss: 0.00002426
Iteration 342/1000 | Loss: 0.00002426
Iteration 343/1000 | Loss: 0.00002426
Iteration 344/1000 | Loss: 0.00002426
Iteration 345/1000 | Loss: 0.00002425
Iteration 346/1000 | Loss: 0.00002425
Iteration 347/1000 | Loss: 0.00002425
Iteration 348/1000 | Loss: 0.00002425
Iteration 349/1000 | Loss: 0.00002425
Iteration 350/1000 | Loss: 0.00002425
Iteration 351/1000 | Loss: 0.00002425
Iteration 352/1000 | Loss: 0.00002425
Iteration 353/1000 | Loss: 0.00002425
Iteration 354/1000 | Loss: 0.00002425
Iteration 355/1000 | Loss: 0.00002425
Iteration 356/1000 | Loss: 0.00002425
Iteration 357/1000 | Loss: 0.00002425
Iteration 358/1000 | Loss: 0.00002424
Iteration 359/1000 | Loss: 0.00002424
Iteration 360/1000 | Loss: 0.00002424
Iteration 361/1000 | Loss: 0.00002424
Iteration 362/1000 | Loss: 0.00002424
Iteration 363/1000 | Loss: 0.00002424
Iteration 364/1000 | Loss: 0.00002423
Iteration 365/1000 | Loss: 0.00002423
Iteration 366/1000 | Loss: 0.00002423
Iteration 367/1000 | Loss: 0.00002423
Iteration 368/1000 | Loss: 0.00002423
Iteration 369/1000 | Loss: 0.00002423
Iteration 370/1000 | Loss: 0.00002423
Iteration 371/1000 | Loss: 0.00002423
Iteration 372/1000 | Loss: 0.00002423
Iteration 373/1000 | Loss: 0.00002423
Iteration 374/1000 | Loss: 0.00002423
Iteration 375/1000 | Loss: 0.00002423
Iteration 376/1000 | Loss: 0.00002423
Iteration 377/1000 | Loss: 0.00002423
Iteration 378/1000 | Loss: 0.00002423
Iteration 379/1000 | Loss: 0.00002422
Iteration 380/1000 | Loss: 0.00002422
Iteration 381/1000 | Loss: 0.00002422
Iteration 382/1000 | Loss: 0.00002422
Iteration 383/1000 | Loss: 0.00002422
Iteration 384/1000 | Loss: 0.00002422
Iteration 385/1000 | Loss: 0.00002422
Iteration 386/1000 | Loss: 0.00002422
Iteration 387/1000 | Loss: 0.00002422
Iteration 388/1000 | Loss: 0.00002422
Iteration 389/1000 | Loss: 0.00002422
Iteration 390/1000 | Loss: 0.00002422
Iteration 391/1000 | Loss: 0.00002421
Iteration 392/1000 | Loss: 0.00002421
Iteration 393/1000 | Loss: 0.00002421
Iteration 394/1000 | Loss: 0.00002421
Iteration 395/1000 | Loss: 0.00002421
Iteration 396/1000 | Loss: 0.00002421
Iteration 397/1000 | Loss: 0.00002421
Iteration 398/1000 | Loss: 0.00002421
Iteration 399/1000 | Loss: 0.00002421
Iteration 400/1000 | Loss: 0.00002421
Iteration 401/1000 | Loss: 0.00002421
Iteration 402/1000 | Loss: 0.00002421
Iteration 403/1000 | Loss: 0.00002421
Iteration 404/1000 | Loss: 0.00002420
Iteration 405/1000 | Loss: 0.00002420
Iteration 406/1000 | Loss: 0.00002420
Iteration 407/1000 | Loss: 0.00002420
Iteration 408/1000 | Loss: 0.00002420
Iteration 409/1000 | Loss: 0.00002420
Iteration 410/1000 | Loss: 0.00002420
Iteration 411/1000 | Loss: 0.00002420
Iteration 412/1000 | Loss: 0.00002420
Iteration 413/1000 | Loss: 0.00002420
Iteration 414/1000 | Loss: 0.00002420
Iteration 415/1000 | Loss: 0.00002420
Iteration 416/1000 | Loss: 0.00002420
Iteration 417/1000 | Loss: 0.00002420
Iteration 418/1000 | Loss: 0.00002420
Iteration 419/1000 | Loss: 0.00002420
Iteration 420/1000 | Loss: 0.00002420
Iteration 421/1000 | Loss: 0.00002420
Iteration 422/1000 | Loss: 0.00002420
Iteration 423/1000 | Loss: 0.00002420
Iteration 424/1000 | Loss: 0.00002419
Iteration 425/1000 | Loss: 0.00002419
Iteration 426/1000 | Loss: 0.00002419
Iteration 427/1000 | Loss: 0.00002419
Iteration 428/1000 | Loss: 0.00002419
Iteration 429/1000 | Loss: 0.00002419
Iteration 430/1000 | Loss: 0.00002419
Iteration 431/1000 | Loss: 0.00002419
Iteration 432/1000 | Loss: 0.00002419
Iteration 433/1000 | Loss: 0.00002419
Iteration 434/1000 | Loss: 0.00002419
Iteration 435/1000 | Loss: 0.00002419
Iteration 436/1000 | Loss: 0.00002419
Iteration 437/1000 | Loss: 0.00002419
Iteration 438/1000 | Loss: 0.00002419
Iteration 439/1000 | Loss: 0.00002419
Iteration 440/1000 | Loss: 0.00002419
Iteration 441/1000 | Loss: 0.00002419
Iteration 442/1000 | Loss: 0.00002419
Iteration 443/1000 | Loss: 0.00002419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 443. Stopping optimization.
Last 5 losses: [2.4190652766264975e-05, 2.4190652766264975e-05, 2.4190652766264975e-05, 2.4190652766264975e-05, 2.4190652766264975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4190652766264975e-05

Optimization complete. Final v2v error: 4.292543888092041 mm

Highest mean error: 4.654430389404297 mm for frame 140

Lowest mean error: 3.7393531799316406 mm for frame 3

Saving results

Total time: 292.46224093437195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885739
Iteration 2/25 | Loss: 0.00153183
Iteration 3/25 | Loss: 0.00138796
Iteration 4/25 | Loss: 0.00137725
Iteration 5/25 | Loss: 0.00137438
Iteration 6/25 | Loss: 0.00137364
Iteration 7/25 | Loss: 0.00137364
Iteration 8/25 | Loss: 0.00137364
Iteration 9/25 | Loss: 0.00137364
Iteration 10/25 | Loss: 0.00137364
Iteration 11/25 | Loss: 0.00137364
Iteration 12/25 | Loss: 0.00137364
Iteration 13/25 | Loss: 0.00137364
Iteration 14/25 | Loss: 0.00137364
Iteration 15/25 | Loss: 0.00137364
Iteration 16/25 | Loss: 0.00137364
Iteration 17/25 | Loss: 0.00137364
Iteration 18/25 | Loss: 0.00137364
Iteration 19/25 | Loss: 0.00137364
Iteration 20/25 | Loss: 0.00137364
Iteration 21/25 | Loss: 0.00137364
Iteration 22/25 | Loss: 0.00137364
Iteration 23/25 | Loss: 0.00137364
Iteration 24/25 | Loss: 0.00137364
Iteration 25/25 | Loss: 0.00137364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.17785931
Iteration 2/25 | Loss: 0.00204409
Iteration 3/25 | Loss: 0.00204404
Iteration 4/25 | Loss: 0.00204404
Iteration 5/25 | Loss: 0.00204404
Iteration 6/25 | Loss: 0.00204404
Iteration 7/25 | Loss: 0.00204404
Iteration 8/25 | Loss: 0.00204404
Iteration 9/25 | Loss: 0.00204404
Iteration 10/25 | Loss: 0.00204404
Iteration 11/25 | Loss: 0.00204404
Iteration 12/25 | Loss: 0.00204404
Iteration 13/25 | Loss: 0.00204404
Iteration 14/25 | Loss: 0.00204404
Iteration 15/25 | Loss: 0.00204404
Iteration 16/25 | Loss: 0.00204404
Iteration 17/25 | Loss: 0.00204404
Iteration 18/25 | Loss: 0.00204404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002044037450104952, 0.002044037450104952, 0.002044037450104952, 0.002044037450104952, 0.002044037450104952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002044037450104952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204404
Iteration 2/1000 | Loss: 0.00004666
Iteration 3/1000 | Loss: 0.00003157
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002524
Iteration 6/1000 | Loss: 0.00002431
Iteration 7/1000 | Loss: 0.00002353
Iteration 8/1000 | Loss: 0.00002308
Iteration 9/1000 | Loss: 0.00002270
Iteration 10/1000 | Loss: 0.00002238
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00002214
Iteration 13/1000 | Loss: 0.00002208
Iteration 14/1000 | Loss: 0.00002200
Iteration 15/1000 | Loss: 0.00002200
Iteration 16/1000 | Loss: 0.00002195
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002194
Iteration 19/1000 | Loss: 0.00002193
Iteration 20/1000 | Loss: 0.00002193
Iteration 21/1000 | Loss: 0.00002193
Iteration 22/1000 | Loss: 0.00002192
Iteration 23/1000 | Loss: 0.00002192
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002188
Iteration 27/1000 | Loss: 0.00002188
Iteration 28/1000 | Loss: 0.00002186
Iteration 29/1000 | Loss: 0.00002186
Iteration 30/1000 | Loss: 0.00002185
Iteration 31/1000 | Loss: 0.00002185
Iteration 32/1000 | Loss: 0.00002184
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002183
Iteration 35/1000 | Loss: 0.00002183
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002181
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002180
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002179
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002178
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002177
Iteration 53/1000 | Loss: 0.00002177
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002176
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002175
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002174
Iteration 63/1000 | Loss: 0.00002174
Iteration 64/1000 | Loss: 0.00002174
Iteration 65/1000 | Loss: 0.00002173
Iteration 66/1000 | Loss: 0.00002173
Iteration 67/1000 | Loss: 0.00002172
Iteration 68/1000 | Loss: 0.00002172
Iteration 69/1000 | Loss: 0.00002172
Iteration 70/1000 | Loss: 0.00002172
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002172
Iteration 73/1000 | Loss: 0.00002172
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002170
Iteration 77/1000 | Loss: 0.00002169
Iteration 78/1000 | Loss: 0.00002169
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00002169
Iteration 81/1000 | Loss: 0.00002168
Iteration 82/1000 | Loss: 0.00002168
Iteration 83/1000 | Loss: 0.00002168
Iteration 84/1000 | Loss: 0.00002168
Iteration 85/1000 | Loss: 0.00002167
Iteration 86/1000 | Loss: 0.00002167
Iteration 87/1000 | Loss: 0.00002167
Iteration 88/1000 | Loss: 0.00002166
Iteration 89/1000 | Loss: 0.00002166
Iteration 90/1000 | Loss: 0.00002166
Iteration 91/1000 | Loss: 0.00002165
Iteration 92/1000 | Loss: 0.00002165
Iteration 93/1000 | Loss: 0.00002165
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002164
Iteration 96/1000 | Loss: 0.00002164
Iteration 97/1000 | Loss: 0.00002164
Iteration 98/1000 | Loss: 0.00002164
Iteration 99/1000 | Loss: 0.00002164
Iteration 100/1000 | Loss: 0.00002163
Iteration 101/1000 | Loss: 0.00002163
Iteration 102/1000 | Loss: 0.00002163
Iteration 103/1000 | Loss: 0.00002162
Iteration 104/1000 | Loss: 0.00002162
Iteration 105/1000 | Loss: 0.00002162
Iteration 106/1000 | Loss: 0.00002162
Iteration 107/1000 | Loss: 0.00002162
Iteration 108/1000 | Loss: 0.00002162
Iteration 109/1000 | Loss: 0.00002161
Iteration 110/1000 | Loss: 0.00002161
Iteration 111/1000 | Loss: 0.00002161
Iteration 112/1000 | Loss: 0.00002161
Iteration 113/1000 | Loss: 0.00002161
Iteration 114/1000 | Loss: 0.00002161
Iteration 115/1000 | Loss: 0.00002161
Iteration 116/1000 | Loss: 0.00002160
Iteration 117/1000 | Loss: 0.00002160
Iteration 118/1000 | Loss: 0.00002160
Iteration 119/1000 | Loss: 0.00002160
Iteration 120/1000 | Loss: 0.00002160
Iteration 121/1000 | Loss: 0.00002160
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002159
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002159
Iteration 126/1000 | Loss: 0.00002159
Iteration 127/1000 | Loss: 0.00002159
Iteration 128/1000 | Loss: 0.00002159
Iteration 129/1000 | Loss: 0.00002159
Iteration 130/1000 | Loss: 0.00002159
Iteration 131/1000 | Loss: 0.00002159
Iteration 132/1000 | Loss: 0.00002159
Iteration 133/1000 | Loss: 0.00002159
Iteration 134/1000 | Loss: 0.00002159
Iteration 135/1000 | Loss: 0.00002159
Iteration 136/1000 | Loss: 0.00002158
Iteration 137/1000 | Loss: 0.00002158
Iteration 138/1000 | Loss: 0.00002158
Iteration 139/1000 | Loss: 0.00002158
Iteration 140/1000 | Loss: 0.00002158
Iteration 141/1000 | Loss: 0.00002158
Iteration 142/1000 | Loss: 0.00002158
Iteration 143/1000 | Loss: 0.00002158
Iteration 144/1000 | Loss: 0.00002158
Iteration 145/1000 | Loss: 0.00002158
Iteration 146/1000 | Loss: 0.00002158
Iteration 147/1000 | Loss: 0.00002158
Iteration 148/1000 | Loss: 0.00002157
Iteration 149/1000 | Loss: 0.00002157
Iteration 150/1000 | Loss: 0.00002157
Iteration 151/1000 | Loss: 0.00002157
Iteration 152/1000 | Loss: 0.00002157
Iteration 153/1000 | Loss: 0.00002157
Iteration 154/1000 | Loss: 0.00002157
Iteration 155/1000 | Loss: 0.00002157
Iteration 156/1000 | Loss: 0.00002157
Iteration 157/1000 | Loss: 0.00002157
Iteration 158/1000 | Loss: 0.00002157
Iteration 159/1000 | Loss: 0.00002157
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002157
Iteration 165/1000 | Loss: 0.00002156
Iteration 166/1000 | Loss: 0.00002156
Iteration 167/1000 | Loss: 0.00002156
Iteration 168/1000 | Loss: 0.00002156
Iteration 169/1000 | Loss: 0.00002156
Iteration 170/1000 | Loss: 0.00002156
Iteration 171/1000 | Loss: 0.00002156
Iteration 172/1000 | Loss: 0.00002156
Iteration 173/1000 | Loss: 0.00002156
Iteration 174/1000 | Loss: 0.00002156
Iteration 175/1000 | Loss: 0.00002156
Iteration 176/1000 | Loss: 0.00002156
Iteration 177/1000 | Loss: 0.00002156
Iteration 178/1000 | Loss: 0.00002156
Iteration 179/1000 | Loss: 0.00002156
Iteration 180/1000 | Loss: 0.00002156
Iteration 181/1000 | Loss: 0.00002156
Iteration 182/1000 | Loss: 0.00002156
Iteration 183/1000 | Loss: 0.00002156
Iteration 184/1000 | Loss: 0.00002156
Iteration 185/1000 | Loss: 0.00002156
Iteration 186/1000 | Loss: 0.00002156
Iteration 187/1000 | Loss: 0.00002156
Iteration 188/1000 | Loss: 0.00002156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.15597956412239e-05, 2.15597956412239e-05, 2.15597956412239e-05, 2.15597956412239e-05, 2.15597956412239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.15597956412239e-05

Optimization complete. Final v2v error: 3.9576351642608643 mm

Highest mean error: 4.903299808502197 mm for frame 29

Lowest mean error: 3.360327959060669 mm for frame 124

Saving results

Total time: 43.861663818359375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927239
Iteration 2/25 | Loss: 0.00145675
Iteration 3/25 | Loss: 0.00137146
Iteration 4/25 | Loss: 0.00136605
Iteration 5/25 | Loss: 0.00136435
Iteration 6/25 | Loss: 0.00136422
Iteration 7/25 | Loss: 0.00136422
Iteration 8/25 | Loss: 0.00136422
Iteration 9/25 | Loss: 0.00136422
Iteration 10/25 | Loss: 0.00136422
Iteration 11/25 | Loss: 0.00136422
Iteration 12/25 | Loss: 0.00136422
Iteration 13/25 | Loss: 0.00136422
Iteration 14/25 | Loss: 0.00136422
Iteration 15/25 | Loss: 0.00136422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013642233097925782, 0.0013642233097925782, 0.0013642233097925782, 0.0013642233097925782, 0.0013642233097925782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013642233097925782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49703264
Iteration 2/25 | Loss: 0.00202859
Iteration 3/25 | Loss: 0.00202859
Iteration 4/25 | Loss: 0.00202859
Iteration 5/25 | Loss: 0.00202859
Iteration 6/25 | Loss: 0.00202859
Iteration 7/25 | Loss: 0.00202859
Iteration 8/25 | Loss: 0.00202859
Iteration 9/25 | Loss: 0.00202859
Iteration 10/25 | Loss: 0.00202859
Iteration 11/25 | Loss: 0.00202859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002028591465204954, 0.002028591465204954, 0.002028591465204954, 0.002028591465204954, 0.002028591465204954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002028591465204954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202859
Iteration 2/1000 | Loss: 0.00002947
Iteration 3/1000 | Loss: 0.00002100
Iteration 4/1000 | Loss: 0.00001845
Iteration 5/1000 | Loss: 0.00001780
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001652
Iteration 12/1000 | Loss: 0.00001652
Iteration 13/1000 | Loss: 0.00001649
Iteration 14/1000 | Loss: 0.00001649
Iteration 15/1000 | Loss: 0.00001647
Iteration 16/1000 | Loss: 0.00001643
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001640
Iteration 19/1000 | Loss: 0.00001639
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001639
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001637
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001637
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001637
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001636
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001635
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001628
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001620
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001617
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001616
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001616
Iteration 72/1000 | Loss: 0.00001616
Iteration 73/1000 | Loss: 0.00001616
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001616
Iteration 90/1000 | Loss: 0.00001616
Iteration 91/1000 | Loss: 0.00001616
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001616
Iteration 96/1000 | Loss: 0.00001616
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001616
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.6156724086613394e-05, 1.6156724086613394e-05, 1.6156724086613394e-05, 1.6156724086613394e-05, 1.6156724086613394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6156724086613394e-05

Optimization complete. Final v2v error: 3.4781250953674316 mm

Highest mean error: 3.770390510559082 mm for frame 47

Lowest mean error: 3.274380922317505 mm for frame 0

Saving results

Total time: 26.359710931777954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583912
Iteration 2/25 | Loss: 0.00151660
Iteration 3/25 | Loss: 0.00136224
Iteration 4/25 | Loss: 0.00135500
Iteration 5/25 | Loss: 0.00135130
Iteration 6/25 | Loss: 0.00135075
Iteration 7/25 | Loss: 0.00135075
Iteration 8/25 | Loss: 0.00135075
Iteration 9/25 | Loss: 0.00135075
Iteration 10/25 | Loss: 0.00135075
Iteration 11/25 | Loss: 0.00135075
Iteration 12/25 | Loss: 0.00135075
Iteration 13/25 | Loss: 0.00135075
Iteration 14/25 | Loss: 0.00135075
Iteration 15/25 | Loss: 0.00135075
Iteration 16/25 | Loss: 0.00135075
Iteration 17/25 | Loss: 0.00135075
Iteration 18/25 | Loss: 0.00135075
Iteration 19/25 | Loss: 0.00135075
Iteration 20/25 | Loss: 0.00135075
Iteration 21/25 | Loss: 0.00135075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013507468393072486, 0.0013507468393072486, 0.0013507468393072486, 0.0013507468393072486, 0.0013507468393072486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013507468393072486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.68275642
Iteration 2/25 | Loss: 0.00129936
Iteration 3/25 | Loss: 0.00129932
Iteration 4/25 | Loss: 0.00129932
Iteration 5/25 | Loss: 0.00129932
Iteration 6/25 | Loss: 0.00129932
Iteration 7/25 | Loss: 0.00129932
Iteration 8/25 | Loss: 0.00129932
Iteration 9/25 | Loss: 0.00129932
Iteration 10/25 | Loss: 0.00129932
Iteration 11/25 | Loss: 0.00129932
Iteration 12/25 | Loss: 0.00129932
Iteration 13/25 | Loss: 0.00129932
Iteration 14/25 | Loss: 0.00129932
Iteration 15/25 | Loss: 0.00129932
Iteration 16/25 | Loss: 0.00129932
Iteration 17/25 | Loss: 0.00129932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012993153650313616, 0.0012993153650313616, 0.0012993153650313616, 0.0012993153650313616, 0.0012993153650313616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012993153650313616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129932
Iteration 2/1000 | Loss: 0.00003474
Iteration 3/1000 | Loss: 0.00002507
Iteration 4/1000 | Loss: 0.00002402
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00002235
Iteration 8/1000 | Loss: 0.00002214
Iteration 9/1000 | Loss: 0.00002191
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002173
Iteration 13/1000 | Loss: 0.00002172
Iteration 14/1000 | Loss: 0.00002171
Iteration 15/1000 | Loss: 0.00002169
Iteration 16/1000 | Loss: 0.00002169
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002169
Iteration 19/1000 | Loss: 0.00002169
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002169
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002166
Iteration 30/1000 | Loss: 0.00002165
Iteration 31/1000 | Loss: 0.00002165
Iteration 32/1000 | Loss: 0.00002165
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002163
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002162
Iteration 41/1000 | Loss: 0.00002162
Iteration 42/1000 | Loss: 0.00002162
Iteration 43/1000 | Loss: 0.00002162
Iteration 44/1000 | Loss: 0.00002162
Iteration 45/1000 | Loss: 0.00002162
Iteration 46/1000 | Loss: 0.00002162
Iteration 47/1000 | Loss: 0.00002162
Iteration 48/1000 | Loss: 0.00002162
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002158
Iteration 51/1000 | Loss: 0.00002158
Iteration 52/1000 | Loss: 0.00002157
Iteration 53/1000 | Loss: 0.00002155
Iteration 54/1000 | Loss: 0.00002152
Iteration 55/1000 | Loss: 0.00002151
Iteration 56/1000 | Loss: 0.00002151
Iteration 57/1000 | Loss: 0.00002150
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002148
Iteration 62/1000 | Loss: 0.00002148
Iteration 63/1000 | Loss: 0.00002148
Iteration 64/1000 | Loss: 0.00002148
Iteration 65/1000 | Loss: 0.00002147
Iteration 66/1000 | Loss: 0.00002147
Iteration 67/1000 | Loss: 0.00002147
Iteration 68/1000 | Loss: 0.00002147
Iteration 69/1000 | Loss: 0.00002146
Iteration 70/1000 | Loss: 0.00002145
Iteration 71/1000 | Loss: 0.00002145
Iteration 72/1000 | Loss: 0.00002144
Iteration 73/1000 | Loss: 0.00002144
Iteration 74/1000 | Loss: 0.00002144
Iteration 75/1000 | Loss: 0.00002143
Iteration 76/1000 | Loss: 0.00002143
Iteration 77/1000 | Loss: 0.00002143
Iteration 78/1000 | Loss: 0.00002142
Iteration 79/1000 | Loss: 0.00002142
Iteration 80/1000 | Loss: 0.00002142
Iteration 81/1000 | Loss: 0.00002142
Iteration 82/1000 | Loss: 0.00002142
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002141
Iteration 85/1000 | Loss: 0.00002140
Iteration 86/1000 | Loss: 0.00002140
Iteration 87/1000 | Loss: 0.00002140
Iteration 88/1000 | Loss: 0.00002140
Iteration 89/1000 | Loss: 0.00002139
Iteration 90/1000 | Loss: 0.00002139
Iteration 91/1000 | Loss: 0.00002139
Iteration 92/1000 | Loss: 0.00002138
Iteration 93/1000 | Loss: 0.00002138
Iteration 94/1000 | Loss: 0.00002138
Iteration 95/1000 | Loss: 0.00002137
Iteration 96/1000 | Loss: 0.00002137
Iteration 97/1000 | Loss: 0.00002137
Iteration 98/1000 | Loss: 0.00002137
Iteration 99/1000 | Loss: 0.00002137
Iteration 100/1000 | Loss: 0.00002136
Iteration 101/1000 | Loss: 0.00002136
Iteration 102/1000 | Loss: 0.00002136
Iteration 103/1000 | Loss: 0.00002136
Iteration 104/1000 | Loss: 0.00002136
Iteration 105/1000 | Loss: 0.00002136
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002135
Iteration 109/1000 | Loss: 0.00002135
Iteration 110/1000 | Loss: 0.00002135
Iteration 111/1000 | Loss: 0.00002135
Iteration 112/1000 | Loss: 0.00002135
Iteration 113/1000 | Loss: 0.00002135
Iteration 114/1000 | Loss: 0.00002135
Iteration 115/1000 | Loss: 0.00002134
Iteration 116/1000 | Loss: 0.00002134
Iteration 117/1000 | Loss: 0.00002134
Iteration 118/1000 | Loss: 0.00002132
Iteration 119/1000 | Loss: 0.00002132
Iteration 120/1000 | Loss: 0.00002132
Iteration 121/1000 | Loss: 0.00002132
Iteration 122/1000 | Loss: 0.00002132
Iteration 123/1000 | Loss: 0.00002132
Iteration 124/1000 | Loss: 0.00002132
Iteration 125/1000 | Loss: 0.00002132
Iteration 126/1000 | Loss: 0.00002132
Iteration 127/1000 | Loss: 0.00002132
Iteration 128/1000 | Loss: 0.00002132
Iteration 129/1000 | Loss: 0.00002131
Iteration 130/1000 | Loss: 0.00002131
Iteration 131/1000 | Loss: 0.00002131
Iteration 132/1000 | Loss: 0.00002131
Iteration 133/1000 | Loss: 0.00002131
Iteration 134/1000 | Loss: 0.00002131
Iteration 135/1000 | Loss: 0.00002131
Iteration 136/1000 | Loss: 0.00002131
Iteration 137/1000 | Loss: 0.00002130
Iteration 138/1000 | Loss: 0.00002130
Iteration 139/1000 | Loss: 0.00002130
Iteration 140/1000 | Loss: 0.00002129
Iteration 141/1000 | Loss: 0.00002129
Iteration 142/1000 | Loss: 0.00002129
Iteration 143/1000 | Loss: 0.00002129
Iteration 144/1000 | Loss: 0.00002129
Iteration 145/1000 | Loss: 0.00002128
Iteration 146/1000 | Loss: 0.00002128
Iteration 147/1000 | Loss: 0.00002128
Iteration 148/1000 | Loss: 0.00002128
Iteration 149/1000 | Loss: 0.00002128
Iteration 150/1000 | Loss: 0.00002127
Iteration 151/1000 | Loss: 0.00002127
Iteration 152/1000 | Loss: 0.00002127
Iteration 153/1000 | Loss: 0.00002126
Iteration 154/1000 | Loss: 0.00002126
Iteration 155/1000 | Loss: 0.00002126
Iteration 156/1000 | Loss: 0.00002125
Iteration 157/1000 | Loss: 0.00002125
Iteration 158/1000 | Loss: 0.00002125
Iteration 159/1000 | Loss: 0.00002124
Iteration 160/1000 | Loss: 0.00002124
Iteration 161/1000 | Loss: 0.00002124
Iteration 162/1000 | Loss: 0.00002124
Iteration 163/1000 | Loss: 0.00002124
Iteration 164/1000 | Loss: 0.00002124
Iteration 165/1000 | Loss: 0.00002124
Iteration 166/1000 | Loss: 0.00002123
Iteration 167/1000 | Loss: 0.00002123
Iteration 168/1000 | Loss: 0.00002123
Iteration 169/1000 | Loss: 0.00002122
Iteration 170/1000 | Loss: 0.00002122
Iteration 171/1000 | Loss: 0.00002122
Iteration 172/1000 | Loss: 0.00002122
Iteration 173/1000 | Loss: 0.00002121
Iteration 174/1000 | Loss: 0.00002121
Iteration 175/1000 | Loss: 0.00002121
Iteration 176/1000 | Loss: 0.00002121
Iteration 177/1000 | Loss: 0.00002120
Iteration 178/1000 | Loss: 0.00002120
Iteration 179/1000 | Loss: 0.00002120
Iteration 180/1000 | Loss: 0.00002120
Iteration 181/1000 | Loss: 0.00002120
Iteration 182/1000 | Loss: 0.00002119
Iteration 183/1000 | Loss: 0.00002119
Iteration 184/1000 | Loss: 0.00002119
Iteration 185/1000 | Loss: 0.00002119
Iteration 186/1000 | Loss: 0.00002119
Iteration 187/1000 | Loss: 0.00002119
Iteration 188/1000 | Loss: 0.00002119
Iteration 189/1000 | Loss: 0.00002118
Iteration 190/1000 | Loss: 0.00002118
Iteration 191/1000 | Loss: 0.00002118
Iteration 192/1000 | Loss: 0.00002118
Iteration 193/1000 | Loss: 0.00002118
Iteration 194/1000 | Loss: 0.00002118
Iteration 195/1000 | Loss: 0.00002118
Iteration 196/1000 | Loss: 0.00002117
Iteration 197/1000 | Loss: 0.00002117
Iteration 198/1000 | Loss: 0.00002117
Iteration 199/1000 | Loss: 0.00002117
Iteration 200/1000 | Loss: 0.00002117
Iteration 201/1000 | Loss: 0.00002117
Iteration 202/1000 | Loss: 0.00002117
Iteration 203/1000 | Loss: 0.00002117
Iteration 204/1000 | Loss: 0.00002117
Iteration 205/1000 | Loss: 0.00002117
Iteration 206/1000 | Loss: 0.00002117
Iteration 207/1000 | Loss: 0.00002117
Iteration 208/1000 | Loss: 0.00002117
Iteration 209/1000 | Loss: 0.00002117
Iteration 210/1000 | Loss: 0.00002117
Iteration 211/1000 | Loss: 0.00002116
Iteration 212/1000 | Loss: 0.00002116
Iteration 213/1000 | Loss: 0.00002116
Iteration 214/1000 | Loss: 0.00002116
Iteration 215/1000 | Loss: 0.00002116
Iteration 216/1000 | Loss: 0.00002116
Iteration 217/1000 | Loss: 0.00002116
Iteration 218/1000 | Loss: 0.00002116
Iteration 219/1000 | Loss: 0.00002115
Iteration 220/1000 | Loss: 0.00002115
Iteration 221/1000 | Loss: 0.00002115
Iteration 222/1000 | Loss: 0.00002115
Iteration 223/1000 | Loss: 0.00002115
Iteration 224/1000 | Loss: 0.00002115
Iteration 225/1000 | Loss: 0.00002115
Iteration 226/1000 | Loss: 0.00002115
Iteration 227/1000 | Loss: 0.00002115
Iteration 228/1000 | Loss: 0.00002115
Iteration 229/1000 | Loss: 0.00002115
Iteration 230/1000 | Loss: 0.00002115
Iteration 231/1000 | Loss: 0.00002115
Iteration 232/1000 | Loss: 0.00002115
Iteration 233/1000 | Loss: 0.00002114
Iteration 234/1000 | Loss: 0.00002114
Iteration 235/1000 | Loss: 0.00002114
Iteration 236/1000 | Loss: 0.00002114
Iteration 237/1000 | Loss: 0.00002114
Iteration 238/1000 | Loss: 0.00002114
Iteration 239/1000 | Loss: 0.00002114
Iteration 240/1000 | Loss: 0.00002114
Iteration 241/1000 | Loss: 0.00002114
Iteration 242/1000 | Loss: 0.00002114
Iteration 243/1000 | Loss: 0.00002114
Iteration 244/1000 | Loss: 0.00002114
Iteration 245/1000 | Loss: 0.00002114
Iteration 246/1000 | Loss: 0.00002114
Iteration 247/1000 | Loss: 0.00002114
Iteration 248/1000 | Loss: 0.00002114
Iteration 249/1000 | Loss: 0.00002114
Iteration 250/1000 | Loss: 0.00002114
Iteration 251/1000 | Loss: 0.00002114
Iteration 252/1000 | Loss: 0.00002114
Iteration 253/1000 | Loss: 0.00002114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.1135974748176523e-05, 2.1135974748176523e-05, 2.1135974748176523e-05, 2.1135974748176523e-05, 2.1135974748176523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1135974748176523e-05

Optimization complete. Final v2v error: 3.9077963829040527 mm

Highest mean error: 4.135760307312012 mm for frame 225

Lowest mean error: 3.723904609680176 mm for frame 152

Saving results

Total time: 48.01006817817688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01176528
Iteration 2/25 | Loss: 0.00199047
Iteration 3/25 | Loss: 0.00152452
Iteration 4/25 | Loss: 0.00149558
Iteration 5/25 | Loss: 0.00149028
Iteration 6/25 | Loss: 0.00148851
Iteration 7/25 | Loss: 0.00148851
Iteration 8/25 | Loss: 0.00148851
Iteration 9/25 | Loss: 0.00148851
Iteration 10/25 | Loss: 0.00148851
Iteration 11/25 | Loss: 0.00148851
Iteration 12/25 | Loss: 0.00148851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014885087730363011, 0.0014885087730363011, 0.0014885087730363011, 0.0014885087730363011, 0.0014885087730363011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014885087730363011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13771176
Iteration 2/25 | Loss: 0.00154917
Iteration 3/25 | Loss: 0.00154917
Iteration 4/25 | Loss: 0.00154916
Iteration 5/25 | Loss: 0.00154916
Iteration 6/25 | Loss: 0.00154916
Iteration 7/25 | Loss: 0.00154916
Iteration 8/25 | Loss: 0.00154916
Iteration 9/25 | Loss: 0.00154916
Iteration 10/25 | Loss: 0.00154916
Iteration 11/25 | Loss: 0.00154916
Iteration 12/25 | Loss: 0.00154916
Iteration 13/25 | Loss: 0.00154916
Iteration 14/25 | Loss: 0.00154916
Iteration 15/25 | Loss: 0.00154916
Iteration 16/25 | Loss: 0.00154916
Iteration 17/25 | Loss: 0.00154916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015491622034460306, 0.0015491622034460306, 0.0015491622034460306, 0.0015491622034460306, 0.0015491622034460306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015491622034460306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154916
Iteration 2/1000 | Loss: 0.00006935
Iteration 3/1000 | Loss: 0.00004483
Iteration 4/1000 | Loss: 0.00003921
Iteration 5/1000 | Loss: 0.00003743
Iteration 6/1000 | Loss: 0.00003593
Iteration 7/1000 | Loss: 0.00003489
Iteration 8/1000 | Loss: 0.00003415
Iteration 9/1000 | Loss: 0.00003364
Iteration 10/1000 | Loss: 0.00003332
Iteration 11/1000 | Loss: 0.00003309
Iteration 12/1000 | Loss: 0.00003292
Iteration 13/1000 | Loss: 0.00003282
Iteration 14/1000 | Loss: 0.00003275
Iteration 15/1000 | Loss: 0.00003262
Iteration 16/1000 | Loss: 0.00003257
Iteration 17/1000 | Loss: 0.00003251
Iteration 18/1000 | Loss: 0.00003242
Iteration 19/1000 | Loss: 0.00003237
Iteration 20/1000 | Loss: 0.00003236
Iteration 21/1000 | Loss: 0.00003233
Iteration 22/1000 | Loss: 0.00003233
Iteration 23/1000 | Loss: 0.00003233
Iteration 24/1000 | Loss: 0.00003229
Iteration 25/1000 | Loss: 0.00003228
Iteration 26/1000 | Loss: 0.00003228
Iteration 27/1000 | Loss: 0.00003226
Iteration 28/1000 | Loss: 0.00003226
Iteration 29/1000 | Loss: 0.00003226
Iteration 30/1000 | Loss: 0.00003226
Iteration 31/1000 | Loss: 0.00003225
Iteration 32/1000 | Loss: 0.00003222
Iteration 33/1000 | Loss: 0.00003222
Iteration 34/1000 | Loss: 0.00003221
Iteration 35/1000 | Loss: 0.00003221
Iteration 36/1000 | Loss: 0.00003218
Iteration 37/1000 | Loss: 0.00003218
Iteration 38/1000 | Loss: 0.00003217
Iteration 39/1000 | Loss: 0.00003216
Iteration 40/1000 | Loss: 0.00003216
Iteration 41/1000 | Loss: 0.00003215
Iteration 42/1000 | Loss: 0.00003215
Iteration 43/1000 | Loss: 0.00003215
Iteration 44/1000 | Loss: 0.00003214
Iteration 45/1000 | Loss: 0.00003214
Iteration 46/1000 | Loss: 0.00003213
Iteration 47/1000 | Loss: 0.00003212
Iteration 48/1000 | Loss: 0.00003212
Iteration 49/1000 | Loss: 0.00003211
Iteration 50/1000 | Loss: 0.00003211
Iteration 51/1000 | Loss: 0.00003210
Iteration 52/1000 | Loss: 0.00003210
Iteration 53/1000 | Loss: 0.00003209
Iteration 54/1000 | Loss: 0.00003209
Iteration 55/1000 | Loss: 0.00003206
Iteration 56/1000 | Loss: 0.00003206
Iteration 57/1000 | Loss: 0.00003204
Iteration 58/1000 | Loss: 0.00003204
Iteration 59/1000 | Loss: 0.00003203
Iteration 60/1000 | Loss: 0.00003203
Iteration 61/1000 | Loss: 0.00003201
Iteration 62/1000 | Loss: 0.00003201
Iteration 63/1000 | Loss: 0.00003201
Iteration 64/1000 | Loss: 0.00003200
Iteration 65/1000 | Loss: 0.00003200
Iteration 66/1000 | Loss: 0.00003200
Iteration 67/1000 | Loss: 0.00003200
Iteration 68/1000 | Loss: 0.00003200
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003200
Iteration 71/1000 | Loss: 0.00003200
Iteration 72/1000 | Loss: 0.00003200
Iteration 73/1000 | Loss: 0.00003200
Iteration 74/1000 | Loss: 0.00003200
Iteration 75/1000 | Loss: 0.00003199
Iteration 76/1000 | Loss: 0.00003199
Iteration 77/1000 | Loss: 0.00003199
Iteration 78/1000 | Loss: 0.00003199
Iteration 79/1000 | Loss: 0.00003199
Iteration 80/1000 | Loss: 0.00003199
Iteration 81/1000 | Loss: 0.00003199
Iteration 82/1000 | Loss: 0.00003199
Iteration 83/1000 | Loss: 0.00003199
Iteration 84/1000 | Loss: 0.00003198
Iteration 85/1000 | Loss: 0.00003198
Iteration 86/1000 | Loss: 0.00003198
Iteration 87/1000 | Loss: 0.00003197
Iteration 88/1000 | Loss: 0.00003197
Iteration 89/1000 | Loss: 0.00003197
Iteration 90/1000 | Loss: 0.00003197
Iteration 91/1000 | Loss: 0.00003196
Iteration 92/1000 | Loss: 0.00003196
Iteration 93/1000 | Loss: 0.00003196
Iteration 94/1000 | Loss: 0.00003196
Iteration 95/1000 | Loss: 0.00003196
Iteration 96/1000 | Loss: 0.00003196
Iteration 97/1000 | Loss: 0.00003196
Iteration 98/1000 | Loss: 0.00003196
Iteration 99/1000 | Loss: 0.00003196
Iteration 100/1000 | Loss: 0.00003195
Iteration 101/1000 | Loss: 0.00003195
Iteration 102/1000 | Loss: 0.00003195
Iteration 103/1000 | Loss: 0.00003195
Iteration 104/1000 | Loss: 0.00003195
Iteration 105/1000 | Loss: 0.00003195
Iteration 106/1000 | Loss: 0.00003195
Iteration 107/1000 | Loss: 0.00003194
Iteration 108/1000 | Loss: 0.00003194
Iteration 109/1000 | Loss: 0.00003194
Iteration 110/1000 | Loss: 0.00003194
Iteration 111/1000 | Loss: 0.00003194
Iteration 112/1000 | Loss: 0.00003194
Iteration 113/1000 | Loss: 0.00003193
Iteration 114/1000 | Loss: 0.00003193
Iteration 115/1000 | Loss: 0.00003193
Iteration 116/1000 | Loss: 0.00003193
Iteration 117/1000 | Loss: 0.00003193
Iteration 118/1000 | Loss: 0.00003193
Iteration 119/1000 | Loss: 0.00003193
Iteration 120/1000 | Loss: 0.00003192
Iteration 121/1000 | Loss: 0.00003192
Iteration 122/1000 | Loss: 0.00003192
Iteration 123/1000 | Loss: 0.00003192
Iteration 124/1000 | Loss: 0.00003192
Iteration 125/1000 | Loss: 0.00003192
Iteration 126/1000 | Loss: 0.00003192
Iteration 127/1000 | Loss: 0.00003192
Iteration 128/1000 | Loss: 0.00003192
Iteration 129/1000 | Loss: 0.00003191
Iteration 130/1000 | Loss: 0.00003191
Iteration 131/1000 | Loss: 0.00003191
Iteration 132/1000 | Loss: 0.00003191
Iteration 133/1000 | Loss: 0.00003191
Iteration 134/1000 | Loss: 0.00003191
Iteration 135/1000 | Loss: 0.00003191
Iteration 136/1000 | Loss: 0.00003191
Iteration 137/1000 | Loss: 0.00003191
Iteration 138/1000 | Loss: 0.00003190
Iteration 139/1000 | Loss: 0.00003190
Iteration 140/1000 | Loss: 0.00003190
Iteration 141/1000 | Loss: 0.00003190
Iteration 142/1000 | Loss: 0.00003190
Iteration 143/1000 | Loss: 0.00003189
Iteration 144/1000 | Loss: 0.00003189
Iteration 145/1000 | Loss: 0.00003189
Iteration 146/1000 | Loss: 0.00003189
Iteration 147/1000 | Loss: 0.00003189
Iteration 148/1000 | Loss: 0.00003189
Iteration 149/1000 | Loss: 0.00003189
Iteration 150/1000 | Loss: 0.00003189
Iteration 151/1000 | Loss: 0.00003189
Iteration 152/1000 | Loss: 0.00003189
Iteration 153/1000 | Loss: 0.00003189
Iteration 154/1000 | Loss: 0.00003189
Iteration 155/1000 | Loss: 0.00003189
Iteration 156/1000 | Loss: 0.00003189
Iteration 157/1000 | Loss: 0.00003189
Iteration 158/1000 | Loss: 0.00003188
Iteration 159/1000 | Loss: 0.00003188
Iteration 160/1000 | Loss: 0.00003188
Iteration 161/1000 | Loss: 0.00003188
Iteration 162/1000 | Loss: 0.00003188
Iteration 163/1000 | Loss: 0.00003188
Iteration 164/1000 | Loss: 0.00003188
Iteration 165/1000 | Loss: 0.00003188
Iteration 166/1000 | Loss: 0.00003188
Iteration 167/1000 | Loss: 0.00003188
Iteration 168/1000 | Loss: 0.00003188
Iteration 169/1000 | Loss: 0.00003188
Iteration 170/1000 | Loss: 0.00003188
Iteration 171/1000 | Loss: 0.00003188
Iteration 172/1000 | Loss: 0.00003187
Iteration 173/1000 | Loss: 0.00003187
Iteration 174/1000 | Loss: 0.00003187
Iteration 175/1000 | Loss: 0.00003187
Iteration 176/1000 | Loss: 0.00003187
Iteration 177/1000 | Loss: 0.00003187
Iteration 178/1000 | Loss: 0.00003187
Iteration 179/1000 | Loss: 0.00003187
Iteration 180/1000 | Loss: 0.00003187
Iteration 181/1000 | Loss: 0.00003187
Iteration 182/1000 | Loss: 0.00003186
Iteration 183/1000 | Loss: 0.00003186
Iteration 184/1000 | Loss: 0.00003186
Iteration 185/1000 | Loss: 0.00003186
Iteration 186/1000 | Loss: 0.00003186
Iteration 187/1000 | Loss: 0.00003186
Iteration 188/1000 | Loss: 0.00003186
Iteration 189/1000 | Loss: 0.00003186
Iteration 190/1000 | Loss: 0.00003186
Iteration 191/1000 | Loss: 0.00003186
Iteration 192/1000 | Loss: 0.00003186
Iteration 193/1000 | Loss: 0.00003185
Iteration 194/1000 | Loss: 0.00003185
Iteration 195/1000 | Loss: 0.00003185
Iteration 196/1000 | Loss: 0.00003185
Iteration 197/1000 | Loss: 0.00003185
Iteration 198/1000 | Loss: 0.00003185
Iteration 199/1000 | Loss: 0.00003185
Iteration 200/1000 | Loss: 0.00003185
Iteration 201/1000 | Loss: 0.00003185
Iteration 202/1000 | Loss: 0.00003185
Iteration 203/1000 | Loss: 0.00003185
Iteration 204/1000 | Loss: 0.00003185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.18519669235684e-05, 3.18519669235684e-05, 3.18519669235684e-05, 3.18519669235684e-05, 3.18519669235684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.18519669235684e-05

Optimization complete. Final v2v error: 4.55129337310791 mm

Highest mean error: 5.941345691680908 mm for frame 79

Lowest mean error: 3.8336219787597656 mm for frame 1

Saving results

Total time: 53.271926403045654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053351
Iteration 2/25 | Loss: 0.00320289
Iteration 3/25 | Loss: 0.00258396
Iteration 4/25 | Loss: 0.00255880
Iteration 5/25 | Loss: 0.00221068
Iteration 6/25 | Loss: 0.00204323
Iteration 7/25 | Loss: 0.00192874
Iteration 8/25 | Loss: 0.00187789
Iteration 9/25 | Loss: 0.00185264
Iteration 10/25 | Loss: 0.00182887
Iteration 11/25 | Loss: 0.00180617
Iteration 12/25 | Loss: 0.00180455
Iteration 13/25 | Loss: 0.00180391
Iteration 14/25 | Loss: 0.00179789
Iteration 15/25 | Loss: 0.00179752
Iteration 16/25 | Loss: 0.00179693
Iteration 17/25 | Loss: 0.00179611
Iteration 18/25 | Loss: 0.00179574
Iteration 19/25 | Loss: 0.00179650
Iteration 20/25 | Loss: 0.00179614
Iteration 21/25 | Loss: 0.00179656
Iteration 22/25 | Loss: 0.00179666
Iteration 23/25 | Loss: 0.00179596
Iteration 24/25 | Loss: 0.00179613
Iteration 25/25 | Loss: 0.00179524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14545953
Iteration 2/25 | Loss: 0.00565456
Iteration 3/25 | Loss: 0.00565456
Iteration 4/25 | Loss: 0.00565456
Iteration 5/25 | Loss: 0.00565456
Iteration 6/25 | Loss: 0.00565456
Iteration 7/25 | Loss: 0.00565456
Iteration 8/25 | Loss: 0.00565456
Iteration 9/25 | Loss: 0.00565456
Iteration 10/25 | Loss: 0.00565456
Iteration 11/25 | Loss: 0.00565456
Iteration 12/25 | Loss: 0.00565456
Iteration 13/25 | Loss: 0.00565456
Iteration 14/25 | Loss: 0.00565456
Iteration 15/25 | Loss: 0.00565456
Iteration 16/25 | Loss: 0.00565456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005654558539390564, 0.005654558539390564, 0.005654558539390564, 0.005654558539390564, 0.005654558539390564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005654558539390564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00565456
Iteration 2/1000 | Loss: 0.00063081
Iteration 3/1000 | Loss: 0.00050441
Iteration 4/1000 | Loss: 0.00043246
Iteration 5/1000 | Loss: 0.00038588
Iteration 6/1000 | Loss: 0.00035820
Iteration 7/1000 | Loss: 0.00033584
Iteration 8/1000 | Loss: 0.00031592
Iteration 9/1000 | Loss: 0.00030127
Iteration 10/1000 | Loss: 0.00028787
Iteration 11/1000 | Loss: 0.00027307
Iteration 12/1000 | Loss: 0.00026358
Iteration 13/1000 | Loss: 0.00025092
Iteration 14/1000 | Loss: 0.00178210
Iteration 15/1000 | Loss: 0.01463533
Iteration 16/1000 | Loss: 0.00116943
Iteration 17/1000 | Loss: 0.00046217
Iteration 18/1000 | Loss: 0.00031674
Iteration 19/1000 | Loss: 0.00022976
Iteration 20/1000 | Loss: 0.00014552
Iteration 21/1000 | Loss: 0.00010284
Iteration 22/1000 | Loss: 0.00007720
Iteration 23/1000 | Loss: 0.00006209
Iteration 24/1000 | Loss: 0.00005271
Iteration 25/1000 | Loss: 0.00004539
Iteration 26/1000 | Loss: 0.00004094
Iteration 27/1000 | Loss: 0.00003716
Iteration 28/1000 | Loss: 0.00003393
Iteration 29/1000 | Loss: 0.00003088
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002777
Iteration 32/1000 | Loss: 0.00002667
Iteration 33/1000 | Loss: 0.00002612
Iteration 34/1000 | Loss: 0.00002581
Iteration 35/1000 | Loss: 0.00002550
Iteration 36/1000 | Loss: 0.00002543
Iteration 37/1000 | Loss: 0.00002537
Iteration 38/1000 | Loss: 0.00002536
Iteration 39/1000 | Loss: 0.00002522
Iteration 40/1000 | Loss: 0.00002521
Iteration 41/1000 | Loss: 0.00002521
Iteration 42/1000 | Loss: 0.00002520
Iteration 43/1000 | Loss: 0.00002520
Iteration 44/1000 | Loss: 0.00002520
Iteration 45/1000 | Loss: 0.00002517
Iteration 46/1000 | Loss: 0.00002517
Iteration 47/1000 | Loss: 0.00002516
Iteration 48/1000 | Loss: 0.00002515
Iteration 49/1000 | Loss: 0.00002515
Iteration 50/1000 | Loss: 0.00002515
Iteration 51/1000 | Loss: 0.00002515
Iteration 52/1000 | Loss: 0.00002515
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002514
Iteration 57/1000 | Loss: 0.00002514
Iteration 58/1000 | Loss: 0.00002514
Iteration 59/1000 | Loss: 0.00002514
Iteration 60/1000 | Loss: 0.00002514
Iteration 61/1000 | Loss: 0.00002514
Iteration 62/1000 | Loss: 0.00002514
Iteration 63/1000 | Loss: 0.00002513
Iteration 64/1000 | Loss: 0.00002513
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002513
Iteration 68/1000 | Loss: 0.00002513
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002512
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002512
Iteration 76/1000 | Loss: 0.00002512
Iteration 77/1000 | Loss: 0.00002511
Iteration 78/1000 | Loss: 0.00002511
Iteration 79/1000 | Loss: 0.00002511
Iteration 80/1000 | Loss: 0.00002511
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002510
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002509
Iteration 85/1000 | Loss: 0.00002509
Iteration 86/1000 | Loss: 0.00002509
Iteration 87/1000 | Loss: 0.00002509
Iteration 88/1000 | Loss: 0.00002509
Iteration 89/1000 | Loss: 0.00002509
Iteration 90/1000 | Loss: 0.00002509
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002509
Iteration 94/1000 | Loss: 0.00002509
Iteration 95/1000 | Loss: 0.00002509
Iteration 96/1000 | Loss: 0.00002509
Iteration 97/1000 | Loss: 0.00002509
Iteration 98/1000 | Loss: 0.00002509
Iteration 99/1000 | Loss: 0.00002509
Iteration 100/1000 | Loss: 0.00002509
Iteration 101/1000 | Loss: 0.00002509
Iteration 102/1000 | Loss: 0.00002509
Iteration 103/1000 | Loss: 0.00002509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.5088851543841884e-05, 2.5088851543841884e-05, 2.5088851543841884e-05, 2.5088851543841884e-05, 2.5088851543841884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5088851543841884e-05

Optimization complete. Final v2v error: 4.178112506866455 mm

Highest mean error: 10.127570152282715 mm for frame 80

Lowest mean error: 3.804541826248169 mm for frame 230

Saving results

Total time: 115.50266861915588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01153051
Iteration 2/25 | Loss: 0.00222449
Iteration 3/25 | Loss: 0.00161558
Iteration 4/25 | Loss: 0.00156904
Iteration 5/25 | Loss: 0.00156679
Iteration 6/25 | Loss: 0.00155521
Iteration 7/25 | Loss: 0.00154577
Iteration 8/25 | Loss: 0.00153758
Iteration 9/25 | Loss: 0.00152952
Iteration 10/25 | Loss: 0.00152310
Iteration 11/25 | Loss: 0.00151678
Iteration 12/25 | Loss: 0.00151607
Iteration 13/25 | Loss: 0.00150940
Iteration 14/25 | Loss: 0.00151189
Iteration 15/25 | Loss: 0.00151116
Iteration 16/25 | Loss: 0.00150949
Iteration 17/25 | Loss: 0.00150882
Iteration 18/25 | Loss: 0.00150864
Iteration 19/25 | Loss: 0.00150862
Iteration 20/25 | Loss: 0.00150862
Iteration 21/25 | Loss: 0.00150861
Iteration 22/25 | Loss: 0.00150861
Iteration 23/25 | Loss: 0.00150861
Iteration 24/25 | Loss: 0.00150861
Iteration 25/25 | Loss: 0.00150861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69435227
Iteration 2/25 | Loss: 0.00155244
Iteration 3/25 | Loss: 0.00155242
Iteration 4/25 | Loss: 0.00155242
Iteration 5/25 | Loss: 0.00155242
Iteration 6/25 | Loss: 0.00155242
Iteration 7/25 | Loss: 0.00155242
Iteration 8/25 | Loss: 0.00155242
Iteration 9/25 | Loss: 0.00155242
Iteration 10/25 | Loss: 0.00155242
Iteration 11/25 | Loss: 0.00155242
Iteration 12/25 | Loss: 0.00155242
Iteration 13/25 | Loss: 0.00155242
Iteration 14/25 | Loss: 0.00155242
Iteration 15/25 | Loss: 0.00155242
Iteration 16/25 | Loss: 0.00155242
Iteration 17/25 | Loss: 0.00155242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001552421716041863, 0.001552421716041863, 0.001552421716041863, 0.001552421716041863, 0.001552421716041863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001552421716041863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155242
Iteration 2/1000 | Loss: 0.00007751
Iteration 3/1000 | Loss: 0.00009749
Iteration 4/1000 | Loss: 0.00004534
Iteration 5/1000 | Loss: 0.00004094
Iteration 6/1000 | Loss: 0.00007638
Iteration 7/1000 | Loss: 0.00003776
Iteration 8/1000 | Loss: 0.00009581
Iteration 9/1000 | Loss: 0.00003687
Iteration 10/1000 | Loss: 0.00003631
Iteration 11/1000 | Loss: 0.00003568
Iteration 12/1000 | Loss: 0.00003512
Iteration 13/1000 | Loss: 0.00003466
Iteration 14/1000 | Loss: 0.00003438
Iteration 15/1000 | Loss: 0.00008055
Iteration 16/1000 | Loss: 0.00003985
Iteration 17/1000 | Loss: 0.00003739
Iteration 18/1000 | Loss: 0.00003396
Iteration 19/1000 | Loss: 0.00018398
Iteration 20/1000 | Loss: 0.00015179
Iteration 21/1000 | Loss: 0.00003820
Iteration 22/1000 | Loss: 0.00015205
Iteration 23/1000 | Loss: 0.00004217
Iteration 24/1000 | Loss: 0.00003729
Iteration 25/1000 | Loss: 0.00003569
Iteration 26/1000 | Loss: 0.00003446
Iteration 27/1000 | Loss: 0.00003356
Iteration 28/1000 | Loss: 0.00003312
Iteration 29/1000 | Loss: 0.00003290
Iteration 30/1000 | Loss: 0.00003273
Iteration 31/1000 | Loss: 0.00003260
Iteration 32/1000 | Loss: 0.00003259
Iteration 33/1000 | Loss: 0.00003259
Iteration 34/1000 | Loss: 0.00003257
Iteration 35/1000 | Loss: 0.00003257
Iteration 36/1000 | Loss: 0.00003257
Iteration 37/1000 | Loss: 0.00003257
Iteration 38/1000 | Loss: 0.00003256
Iteration 39/1000 | Loss: 0.00003255
Iteration 40/1000 | Loss: 0.00003255
Iteration 41/1000 | Loss: 0.00003255
Iteration 42/1000 | Loss: 0.00003254
Iteration 43/1000 | Loss: 0.00003254
Iteration 44/1000 | Loss: 0.00003254
Iteration 45/1000 | Loss: 0.00003253
Iteration 46/1000 | Loss: 0.00003253
Iteration 47/1000 | Loss: 0.00003253
Iteration 48/1000 | Loss: 0.00003253
Iteration 49/1000 | Loss: 0.00003253
Iteration 50/1000 | Loss: 0.00003253
Iteration 51/1000 | Loss: 0.00003252
Iteration 52/1000 | Loss: 0.00003252
Iteration 53/1000 | Loss: 0.00003252
Iteration 54/1000 | Loss: 0.00003252
Iteration 55/1000 | Loss: 0.00003252
Iteration 56/1000 | Loss: 0.00003252
Iteration 57/1000 | Loss: 0.00003252
Iteration 58/1000 | Loss: 0.00003252
Iteration 59/1000 | Loss: 0.00003252
Iteration 60/1000 | Loss: 0.00003252
Iteration 61/1000 | Loss: 0.00003249
Iteration 62/1000 | Loss: 0.00003248
Iteration 63/1000 | Loss: 0.00003247
Iteration 64/1000 | Loss: 0.00003246
Iteration 65/1000 | Loss: 0.00003240
Iteration 66/1000 | Loss: 0.00003239
Iteration 67/1000 | Loss: 0.00003239
Iteration 68/1000 | Loss: 0.00003237
Iteration 69/1000 | Loss: 0.00003234
Iteration 70/1000 | Loss: 0.00003232
Iteration 71/1000 | Loss: 0.00003232
Iteration 72/1000 | Loss: 0.00003231
Iteration 73/1000 | Loss: 0.00003227
Iteration 74/1000 | Loss: 0.00003227
Iteration 75/1000 | Loss: 0.00003225
Iteration 76/1000 | Loss: 0.00003224
Iteration 77/1000 | Loss: 0.00003224
Iteration 78/1000 | Loss: 0.00003224
Iteration 79/1000 | Loss: 0.00003223
Iteration 80/1000 | Loss: 0.00003223
Iteration 81/1000 | Loss: 0.00003223
Iteration 82/1000 | Loss: 0.00003222
Iteration 83/1000 | Loss: 0.00003221
Iteration 84/1000 | Loss: 0.00003221
Iteration 85/1000 | Loss: 0.00003220
Iteration 86/1000 | Loss: 0.00003220
Iteration 87/1000 | Loss: 0.00003220
Iteration 88/1000 | Loss: 0.00003220
Iteration 89/1000 | Loss: 0.00003219
Iteration 90/1000 | Loss: 0.00003219
Iteration 91/1000 | Loss: 0.00003219
Iteration 92/1000 | Loss: 0.00003219
Iteration 93/1000 | Loss: 0.00003219
Iteration 94/1000 | Loss: 0.00003218
Iteration 95/1000 | Loss: 0.00003218
Iteration 96/1000 | Loss: 0.00003218
Iteration 97/1000 | Loss: 0.00003218
Iteration 98/1000 | Loss: 0.00003218
Iteration 99/1000 | Loss: 0.00003218
Iteration 100/1000 | Loss: 0.00003218
Iteration 101/1000 | Loss: 0.00003218
Iteration 102/1000 | Loss: 0.00003218
Iteration 103/1000 | Loss: 0.00003218
Iteration 104/1000 | Loss: 0.00003218
Iteration 105/1000 | Loss: 0.00003218
Iteration 106/1000 | Loss: 0.00003217
Iteration 107/1000 | Loss: 0.00003217
Iteration 108/1000 | Loss: 0.00003217
Iteration 109/1000 | Loss: 0.00003217
Iteration 110/1000 | Loss: 0.00003217
Iteration 111/1000 | Loss: 0.00003217
Iteration 112/1000 | Loss: 0.00003217
Iteration 113/1000 | Loss: 0.00003217
Iteration 114/1000 | Loss: 0.00003217
Iteration 115/1000 | Loss: 0.00003216
Iteration 116/1000 | Loss: 0.00003216
Iteration 117/1000 | Loss: 0.00003216
Iteration 118/1000 | Loss: 0.00003216
Iteration 119/1000 | Loss: 0.00003215
Iteration 120/1000 | Loss: 0.00003215
Iteration 121/1000 | Loss: 0.00003215
Iteration 122/1000 | Loss: 0.00003215
Iteration 123/1000 | Loss: 0.00003214
Iteration 124/1000 | Loss: 0.00003214
Iteration 125/1000 | Loss: 0.00003214
Iteration 126/1000 | Loss: 0.00003214
Iteration 127/1000 | Loss: 0.00003213
Iteration 128/1000 | Loss: 0.00003213
Iteration 129/1000 | Loss: 0.00003213
Iteration 130/1000 | Loss: 0.00003213
Iteration 131/1000 | Loss: 0.00003213
Iteration 132/1000 | Loss: 0.00003212
Iteration 133/1000 | Loss: 0.00003212
Iteration 134/1000 | Loss: 0.00003212
Iteration 135/1000 | Loss: 0.00003212
Iteration 136/1000 | Loss: 0.00003212
Iteration 137/1000 | Loss: 0.00003212
Iteration 138/1000 | Loss: 0.00003211
Iteration 139/1000 | Loss: 0.00003211
Iteration 140/1000 | Loss: 0.00003211
Iteration 141/1000 | Loss: 0.00003211
Iteration 142/1000 | Loss: 0.00003211
Iteration 143/1000 | Loss: 0.00003210
Iteration 144/1000 | Loss: 0.00003210
Iteration 145/1000 | Loss: 0.00003210
Iteration 146/1000 | Loss: 0.00003210
Iteration 147/1000 | Loss: 0.00003210
Iteration 148/1000 | Loss: 0.00003210
Iteration 149/1000 | Loss: 0.00003210
Iteration 150/1000 | Loss: 0.00003210
Iteration 151/1000 | Loss: 0.00003209
Iteration 152/1000 | Loss: 0.00003209
Iteration 153/1000 | Loss: 0.00003209
Iteration 154/1000 | Loss: 0.00003209
Iteration 155/1000 | Loss: 0.00003209
Iteration 156/1000 | Loss: 0.00003209
Iteration 157/1000 | Loss: 0.00003209
Iteration 158/1000 | Loss: 0.00003209
Iteration 159/1000 | Loss: 0.00003208
Iteration 160/1000 | Loss: 0.00003208
Iteration 161/1000 | Loss: 0.00003208
Iteration 162/1000 | Loss: 0.00003208
Iteration 163/1000 | Loss: 0.00003208
Iteration 164/1000 | Loss: 0.00003208
Iteration 165/1000 | Loss: 0.00003208
Iteration 166/1000 | Loss: 0.00003208
Iteration 167/1000 | Loss: 0.00003208
Iteration 168/1000 | Loss: 0.00003208
Iteration 169/1000 | Loss: 0.00003208
Iteration 170/1000 | Loss: 0.00003208
Iteration 171/1000 | Loss: 0.00003208
Iteration 172/1000 | Loss: 0.00003208
Iteration 173/1000 | Loss: 0.00003208
Iteration 174/1000 | Loss: 0.00003208
Iteration 175/1000 | Loss: 0.00003208
Iteration 176/1000 | Loss: 0.00003207
Iteration 177/1000 | Loss: 0.00003207
Iteration 178/1000 | Loss: 0.00003207
Iteration 179/1000 | Loss: 0.00003207
Iteration 180/1000 | Loss: 0.00003207
Iteration 181/1000 | Loss: 0.00003207
Iteration 182/1000 | Loss: 0.00003207
Iteration 183/1000 | Loss: 0.00003207
Iteration 184/1000 | Loss: 0.00003207
Iteration 185/1000 | Loss: 0.00003207
Iteration 186/1000 | Loss: 0.00003207
Iteration 187/1000 | Loss: 0.00003207
Iteration 188/1000 | Loss: 0.00003207
Iteration 189/1000 | Loss: 0.00003207
Iteration 190/1000 | Loss: 0.00003207
Iteration 191/1000 | Loss: 0.00003207
Iteration 192/1000 | Loss: 0.00003207
Iteration 193/1000 | Loss: 0.00003206
Iteration 194/1000 | Loss: 0.00003206
Iteration 195/1000 | Loss: 0.00003206
Iteration 196/1000 | Loss: 0.00003206
Iteration 197/1000 | Loss: 0.00003206
Iteration 198/1000 | Loss: 0.00003206
Iteration 199/1000 | Loss: 0.00003206
Iteration 200/1000 | Loss: 0.00003206
Iteration 201/1000 | Loss: 0.00003206
Iteration 202/1000 | Loss: 0.00003206
Iteration 203/1000 | Loss: 0.00003206
Iteration 204/1000 | Loss: 0.00003206
Iteration 205/1000 | Loss: 0.00003206
Iteration 206/1000 | Loss: 0.00003206
Iteration 207/1000 | Loss: 0.00003206
Iteration 208/1000 | Loss: 0.00003206
Iteration 209/1000 | Loss: 0.00003206
Iteration 210/1000 | Loss: 0.00003206
Iteration 211/1000 | Loss: 0.00003206
Iteration 212/1000 | Loss: 0.00003205
Iteration 213/1000 | Loss: 0.00003205
Iteration 214/1000 | Loss: 0.00003205
Iteration 215/1000 | Loss: 0.00003205
Iteration 216/1000 | Loss: 0.00003205
Iteration 217/1000 | Loss: 0.00003205
Iteration 218/1000 | Loss: 0.00003205
Iteration 219/1000 | Loss: 0.00003205
Iteration 220/1000 | Loss: 0.00003205
Iteration 221/1000 | Loss: 0.00003205
Iteration 222/1000 | Loss: 0.00003205
Iteration 223/1000 | Loss: 0.00003205
Iteration 224/1000 | Loss: 0.00003205
Iteration 225/1000 | Loss: 0.00003205
Iteration 226/1000 | Loss: 0.00003205
Iteration 227/1000 | Loss: 0.00003204
Iteration 228/1000 | Loss: 0.00003204
Iteration 229/1000 | Loss: 0.00003204
Iteration 230/1000 | Loss: 0.00003204
Iteration 231/1000 | Loss: 0.00003204
Iteration 232/1000 | Loss: 0.00003204
Iteration 233/1000 | Loss: 0.00003204
Iteration 234/1000 | Loss: 0.00003204
Iteration 235/1000 | Loss: 0.00003204
Iteration 236/1000 | Loss: 0.00003203
Iteration 237/1000 | Loss: 0.00003203
Iteration 238/1000 | Loss: 0.00003203
Iteration 239/1000 | Loss: 0.00003203
Iteration 240/1000 | Loss: 0.00003203
Iteration 241/1000 | Loss: 0.00003203
Iteration 242/1000 | Loss: 0.00003203
Iteration 243/1000 | Loss: 0.00003203
Iteration 244/1000 | Loss: 0.00003203
Iteration 245/1000 | Loss: 0.00003202
Iteration 246/1000 | Loss: 0.00003202
Iteration 247/1000 | Loss: 0.00003202
Iteration 248/1000 | Loss: 0.00003202
Iteration 249/1000 | Loss: 0.00003202
Iteration 250/1000 | Loss: 0.00003202
Iteration 251/1000 | Loss: 0.00003202
Iteration 252/1000 | Loss: 0.00003202
Iteration 253/1000 | Loss: 0.00003202
Iteration 254/1000 | Loss: 0.00003202
Iteration 255/1000 | Loss: 0.00003202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [3.202247535227798e-05, 3.202247535227798e-05, 3.202247535227798e-05, 3.202247535227798e-05, 3.202247535227798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.202247535227798e-05

Optimization complete. Final v2v error: 4.765323638916016 mm

Highest mean error: 5.996579647064209 mm for frame 172

Lowest mean error: 3.875647783279419 mm for frame 227

Saving results

Total time: 105.25023245811462
