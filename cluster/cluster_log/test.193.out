Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=193, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10808-10863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809861
Iteration 2/25 | Loss: 0.00151994
Iteration 3/25 | Loss: 0.00128926
Iteration 4/25 | Loss: 0.00126989
Iteration 5/25 | Loss: 0.00126628
Iteration 6/25 | Loss: 0.00126597
Iteration 7/25 | Loss: 0.00126597
Iteration 8/25 | Loss: 0.00126597
Iteration 9/25 | Loss: 0.00126597
Iteration 10/25 | Loss: 0.00126597
Iteration 11/25 | Loss: 0.00126597
Iteration 12/25 | Loss: 0.00126597
Iteration 13/25 | Loss: 0.00126597
Iteration 14/25 | Loss: 0.00126597
Iteration 15/25 | Loss: 0.00126597
Iteration 16/25 | Loss: 0.00126597
Iteration 17/25 | Loss: 0.00126597
Iteration 18/25 | Loss: 0.00126597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012659691274166107, 0.0012659691274166107, 0.0012659691274166107, 0.0012659691274166107, 0.0012659691274166107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659691274166107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90555894
Iteration 2/25 | Loss: 0.00066365
Iteration 3/25 | Loss: 0.00066365
Iteration 4/25 | Loss: 0.00066365
Iteration 5/25 | Loss: 0.00066365
Iteration 6/25 | Loss: 0.00066365
Iteration 7/25 | Loss: 0.00066365
Iteration 8/25 | Loss: 0.00066365
Iteration 9/25 | Loss: 0.00066365
Iteration 10/25 | Loss: 0.00066365
Iteration 11/25 | Loss: 0.00066365
Iteration 12/25 | Loss: 0.00066365
Iteration 13/25 | Loss: 0.00066365
Iteration 14/25 | Loss: 0.00066365
Iteration 15/25 | Loss: 0.00066365
Iteration 16/25 | Loss: 0.00066365
Iteration 17/25 | Loss: 0.00066365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006636457401327789, 0.0006636457401327789, 0.0006636457401327789, 0.0006636457401327789, 0.0006636457401327789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006636457401327789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066365
Iteration 2/1000 | Loss: 0.00003354
Iteration 3/1000 | Loss: 0.00002648
Iteration 4/1000 | Loss: 0.00002460
Iteration 5/1000 | Loss: 0.00002362
Iteration 6/1000 | Loss: 0.00002309
Iteration 7/1000 | Loss: 0.00002257
Iteration 8/1000 | Loss: 0.00002229
Iteration 9/1000 | Loss: 0.00002205
Iteration 10/1000 | Loss: 0.00002186
Iteration 11/1000 | Loss: 0.00002173
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002166
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002166
Iteration 16/1000 | Loss: 0.00002166
Iteration 17/1000 | Loss: 0.00002166
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002166
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002166
Iteration 22/1000 | Loss: 0.00002165
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002165
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002152
Iteration 27/1000 | Loss: 0.00002151
Iteration 28/1000 | Loss: 0.00002151
Iteration 29/1000 | Loss: 0.00002149
Iteration 30/1000 | Loss: 0.00002148
Iteration 31/1000 | Loss: 0.00002146
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002138
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002137
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002135
Iteration 49/1000 | Loss: 0.00002135
Iteration 50/1000 | Loss: 0.00002134
Iteration 51/1000 | Loss: 0.00002133
Iteration 52/1000 | Loss: 0.00002133
Iteration 53/1000 | Loss: 0.00002130
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002128
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002126
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002125
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002123
Iteration 71/1000 | Loss: 0.00002123
Iteration 72/1000 | Loss: 0.00002123
Iteration 73/1000 | Loss: 0.00002123
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002122
Iteration 77/1000 | Loss: 0.00002122
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002120
Iteration 83/1000 | Loss: 0.00002120
Iteration 84/1000 | Loss: 0.00002120
Iteration 85/1000 | Loss: 0.00002120
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00002120
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002120
Iteration 90/1000 | Loss: 0.00002120
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002119
Iteration 94/1000 | Loss: 0.00002119
Iteration 95/1000 | Loss: 0.00002119
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002118
Iteration 100/1000 | Loss: 0.00002118
Iteration 101/1000 | Loss: 0.00002118
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002117
Iteration 104/1000 | Loss: 0.00002117
Iteration 105/1000 | Loss: 0.00002117
Iteration 106/1000 | Loss: 0.00002117
Iteration 107/1000 | Loss: 0.00002117
Iteration 108/1000 | Loss: 0.00002116
Iteration 109/1000 | Loss: 0.00002116
Iteration 110/1000 | Loss: 0.00002116
Iteration 111/1000 | Loss: 0.00002116
Iteration 112/1000 | Loss: 0.00002116
Iteration 113/1000 | Loss: 0.00002116
Iteration 114/1000 | Loss: 0.00002115
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002112
Iteration 119/1000 | Loss: 0.00002112
Iteration 120/1000 | Loss: 0.00002111
Iteration 121/1000 | Loss: 0.00002111
Iteration 122/1000 | Loss: 0.00002111
Iteration 123/1000 | Loss: 0.00002111
Iteration 124/1000 | Loss: 0.00002111
Iteration 125/1000 | Loss: 0.00002111
Iteration 126/1000 | Loss: 0.00002111
Iteration 127/1000 | Loss: 0.00002111
Iteration 128/1000 | Loss: 0.00002111
Iteration 129/1000 | Loss: 0.00002111
Iteration 130/1000 | Loss: 0.00002111
Iteration 131/1000 | Loss: 0.00002110
Iteration 132/1000 | Loss: 0.00002110
Iteration 133/1000 | Loss: 0.00002110
Iteration 134/1000 | Loss: 0.00002110
Iteration 135/1000 | Loss: 0.00002110
Iteration 136/1000 | Loss: 0.00002110
Iteration 137/1000 | Loss: 0.00002109
Iteration 138/1000 | Loss: 0.00002109
Iteration 139/1000 | Loss: 0.00002109
Iteration 140/1000 | Loss: 0.00002109
Iteration 141/1000 | Loss: 0.00002108
Iteration 142/1000 | Loss: 0.00002108
Iteration 143/1000 | Loss: 0.00002108
Iteration 144/1000 | Loss: 0.00002108
Iteration 145/1000 | Loss: 0.00002108
Iteration 146/1000 | Loss: 0.00002108
Iteration 147/1000 | Loss: 0.00002108
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002107
Iteration 150/1000 | Loss: 0.00002107
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002107
Iteration 155/1000 | Loss: 0.00002106
Iteration 156/1000 | Loss: 0.00002106
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002106
Iteration 160/1000 | Loss: 0.00002105
Iteration 161/1000 | Loss: 0.00002105
Iteration 162/1000 | Loss: 0.00002105
Iteration 163/1000 | Loss: 0.00002105
Iteration 164/1000 | Loss: 0.00002105
Iteration 165/1000 | Loss: 0.00002105
Iteration 166/1000 | Loss: 0.00002105
Iteration 167/1000 | Loss: 0.00002105
Iteration 168/1000 | Loss: 0.00002105
Iteration 169/1000 | Loss: 0.00002105
Iteration 170/1000 | Loss: 0.00002105
Iteration 171/1000 | Loss: 0.00002105
Iteration 172/1000 | Loss: 0.00002105
Iteration 173/1000 | Loss: 0.00002105
Iteration 174/1000 | Loss: 0.00002105
Iteration 175/1000 | Loss: 0.00002105
Iteration 176/1000 | Loss: 0.00002105
Iteration 177/1000 | Loss: 0.00002105
Iteration 178/1000 | Loss: 0.00002105
Iteration 179/1000 | Loss: 0.00002105
Iteration 180/1000 | Loss: 0.00002105
Iteration 181/1000 | Loss: 0.00002105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.1046116671641357e-05, 2.1046116671641357e-05, 2.1046116671641357e-05, 2.1046116671641357e-05, 2.1046116671641357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1046116671641357e-05

Optimization complete. Final v2v error: 3.822918653488159 mm

Highest mean error: 4.019599914550781 mm for frame 121

Lowest mean error: 3.6975154876708984 mm for frame 136

Saving results

Total time: 39.131218671798706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739542
Iteration 2/25 | Loss: 0.00185970
Iteration 3/25 | Loss: 0.00143416
Iteration 4/25 | Loss: 0.00138750
Iteration 5/25 | Loss: 0.00137498
Iteration 6/25 | Loss: 0.00135986
Iteration 7/25 | Loss: 0.00134789
Iteration 8/25 | Loss: 0.00134043
Iteration 9/25 | Loss: 0.00133676
Iteration 10/25 | Loss: 0.00133471
Iteration 11/25 | Loss: 0.00133432
Iteration 12/25 | Loss: 0.00133417
Iteration 13/25 | Loss: 0.00133417
Iteration 14/25 | Loss: 0.00133417
Iteration 15/25 | Loss: 0.00133417
Iteration 16/25 | Loss: 0.00133417
Iteration 17/25 | Loss: 0.00133417
Iteration 18/25 | Loss: 0.00133416
Iteration 19/25 | Loss: 0.00133416
Iteration 20/25 | Loss: 0.00133416
Iteration 21/25 | Loss: 0.00133416
Iteration 22/25 | Loss: 0.00133416
Iteration 23/25 | Loss: 0.00133416
Iteration 24/25 | Loss: 0.00133416
Iteration 25/25 | Loss: 0.00133416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31728768
Iteration 2/25 | Loss: 0.00089023
Iteration 3/25 | Loss: 0.00089020
Iteration 4/25 | Loss: 0.00089020
Iteration 5/25 | Loss: 0.00089020
Iteration 6/25 | Loss: 0.00089020
Iteration 7/25 | Loss: 0.00089020
Iteration 8/25 | Loss: 0.00089020
Iteration 9/25 | Loss: 0.00089020
Iteration 10/25 | Loss: 0.00089020
Iteration 11/25 | Loss: 0.00089020
Iteration 12/25 | Loss: 0.00089020
Iteration 13/25 | Loss: 0.00089020
Iteration 14/25 | Loss: 0.00089020
Iteration 15/25 | Loss: 0.00089020
Iteration 16/25 | Loss: 0.00089020
Iteration 17/25 | Loss: 0.00089020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008901977562345564, 0.0008901977562345564, 0.0008901977562345564, 0.0008901977562345564, 0.0008901977562345564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008901977562345564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089020
Iteration 2/1000 | Loss: 0.00004757
Iteration 3/1000 | Loss: 0.00003194
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00002647
Iteration 6/1000 | Loss: 0.00002527
Iteration 7/1000 | Loss: 0.00002470
Iteration 8/1000 | Loss: 0.00002414
Iteration 9/1000 | Loss: 0.00002367
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002278
Iteration 13/1000 | Loss: 0.00002262
Iteration 14/1000 | Loss: 0.00002253
Iteration 15/1000 | Loss: 0.00002247
Iteration 16/1000 | Loss: 0.00002244
Iteration 17/1000 | Loss: 0.00002244
Iteration 18/1000 | Loss: 0.00002244
Iteration 19/1000 | Loss: 0.00002244
Iteration 20/1000 | Loss: 0.00002243
Iteration 21/1000 | Loss: 0.00002240
Iteration 22/1000 | Loss: 0.00002239
Iteration 23/1000 | Loss: 0.00002238
Iteration 24/1000 | Loss: 0.00002237
Iteration 25/1000 | Loss: 0.00002237
Iteration 26/1000 | Loss: 0.00002236
Iteration 27/1000 | Loss: 0.00002234
Iteration 28/1000 | Loss: 0.00002233
Iteration 29/1000 | Loss: 0.00002233
Iteration 30/1000 | Loss: 0.00002232
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002231
Iteration 33/1000 | Loss: 0.00002231
Iteration 34/1000 | Loss: 0.00002230
Iteration 35/1000 | Loss: 0.00002230
Iteration 36/1000 | Loss: 0.00002229
Iteration 37/1000 | Loss: 0.00002229
Iteration 38/1000 | Loss: 0.00002228
Iteration 39/1000 | Loss: 0.00002227
Iteration 40/1000 | Loss: 0.00002227
Iteration 41/1000 | Loss: 0.00002227
Iteration 42/1000 | Loss: 0.00002226
Iteration 43/1000 | Loss: 0.00002226
Iteration 44/1000 | Loss: 0.00002225
Iteration 45/1000 | Loss: 0.00002225
Iteration 46/1000 | Loss: 0.00002224
Iteration 47/1000 | Loss: 0.00002224
Iteration 48/1000 | Loss: 0.00002223
Iteration 49/1000 | Loss: 0.00002223
Iteration 50/1000 | Loss: 0.00002223
Iteration 51/1000 | Loss: 0.00002223
Iteration 52/1000 | Loss: 0.00002222
Iteration 53/1000 | Loss: 0.00002222
Iteration 54/1000 | Loss: 0.00002221
Iteration 55/1000 | Loss: 0.00002221
Iteration 56/1000 | Loss: 0.00002221
Iteration 57/1000 | Loss: 0.00002221
Iteration 58/1000 | Loss: 0.00002221
Iteration 59/1000 | Loss: 0.00002220
Iteration 60/1000 | Loss: 0.00002220
Iteration 61/1000 | Loss: 0.00002220
Iteration 62/1000 | Loss: 0.00002220
Iteration 63/1000 | Loss: 0.00002219
Iteration 64/1000 | Loss: 0.00002219
Iteration 65/1000 | Loss: 0.00002218
Iteration 66/1000 | Loss: 0.00002218
Iteration 67/1000 | Loss: 0.00002218
Iteration 68/1000 | Loss: 0.00002218
Iteration 69/1000 | Loss: 0.00002218
Iteration 70/1000 | Loss: 0.00002218
Iteration 71/1000 | Loss: 0.00002218
Iteration 72/1000 | Loss: 0.00002218
Iteration 73/1000 | Loss: 0.00002218
Iteration 74/1000 | Loss: 0.00002217
Iteration 75/1000 | Loss: 0.00002217
Iteration 76/1000 | Loss: 0.00002217
Iteration 77/1000 | Loss: 0.00002216
Iteration 78/1000 | Loss: 0.00002216
Iteration 79/1000 | Loss: 0.00002216
Iteration 80/1000 | Loss: 0.00002216
Iteration 81/1000 | Loss: 0.00002215
Iteration 82/1000 | Loss: 0.00002215
Iteration 83/1000 | Loss: 0.00002215
Iteration 84/1000 | Loss: 0.00002215
Iteration 85/1000 | Loss: 0.00002214
Iteration 86/1000 | Loss: 0.00002214
Iteration 87/1000 | Loss: 0.00002214
Iteration 88/1000 | Loss: 0.00002214
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002214
Iteration 91/1000 | Loss: 0.00002214
Iteration 92/1000 | Loss: 0.00002213
Iteration 93/1000 | Loss: 0.00002213
Iteration 94/1000 | Loss: 0.00002213
Iteration 95/1000 | Loss: 0.00002213
Iteration 96/1000 | Loss: 0.00002213
Iteration 97/1000 | Loss: 0.00002212
Iteration 98/1000 | Loss: 0.00002212
Iteration 99/1000 | Loss: 0.00002211
Iteration 100/1000 | Loss: 0.00002211
Iteration 101/1000 | Loss: 0.00002211
Iteration 102/1000 | Loss: 0.00002211
Iteration 103/1000 | Loss: 0.00002211
Iteration 104/1000 | Loss: 0.00002210
Iteration 105/1000 | Loss: 0.00002210
Iteration 106/1000 | Loss: 0.00002210
Iteration 107/1000 | Loss: 0.00002210
Iteration 108/1000 | Loss: 0.00002210
Iteration 109/1000 | Loss: 0.00002210
Iteration 110/1000 | Loss: 0.00002210
Iteration 111/1000 | Loss: 0.00002210
Iteration 112/1000 | Loss: 0.00002209
Iteration 113/1000 | Loss: 0.00002209
Iteration 114/1000 | Loss: 0.00002209
Iteration 115/1000 | Loss: 0.00002208
Iteration 116/1000 | Loss: 0.00002208
Iteration 117/1000 | Loss: 0.00002208
Iteration 118/1000 | Loss: 0.00002208
Iteration 119/1000 | Loss: 0.00002208
Iteration 120/1000 | Loss: 0.00002208
Iteration 121/1000 | Loss: 0.00002208
Iteration 122/1000 | Loss: 0.00002207
Iteration 123/1000 | Loss: 0.00002207
Iteration 124/1000 | Loss: 0.00002207
Iteration 125/1000 | Loss: 0.00002207
Iteration 126/1000 | Loss: 0.00002207
Iteration 127/1000 | Loss: 0.00002207
Iteration 128/1000 | Loss: 0.00002207
Iteration 129/1000 | Loss: 0.00002207
Iteration 130/1000 | Loss: 0.00002207
Iteration 131/1000 | Loss: 0.00002207
Iteration 132/1000 | Loss: 0.00002207
Iteration 133/1000 | Loss: 0.00002207
Iteration 134/1000 | Loss: 0.00002207
Iteration 135/1000 | Loss: 0.00002206
Iteration 136/1000 | Loss: 0.00002206
Iteration 137/1000 | Loss: 0.00002206
Iteration 138/1000 | Loss: 0.00002206
Iteration 139/1000 | Loss: 0.00002206
Iteration 140/1000 | Loss: 0.00002206
Iteration 141/1000 | Loss: 0.00002206
Iteration 142/1000 | Loss: 0.00002205
Iteration 143/1000 | Loss: 0.00002205
Iteration 144/1000 | Loss: 0.00002205
Iteration 145/1000 | Loss: 0.00002205
Iteration 146/1000 | Loss: 0.00002205
Iteration 147/1000 | Loss: 0.00002205
Iteration 148/1000 | Loss: 0.00002205
Iteration 149/1000 | Loss: 0.00002205
Iteration 150/1000 | Loss: 0.00002205
Iteration 151/1000 | Loss: 0.00002205
Iteration 152/1000 | Loss: 0.00002205
Iteration 153/1000 | Loss: 0.00002205
Iteration 154/1000 | Loss: 0.00002205
Iteration 155/1000 | Loss: 0.00002205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.2052534404792823e-05, 2.2052534404792823e-05, 2.2052534404792823e-05, 2.2052534404792823e-05, 2.2052534404792823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2052534404792823e-05

Optimization complete. Final v2v error: 3.966496706008911 mm

Highest mean error: 4.9809250831604 mm for frame 181

Lowest mean error: 3.5420801639556885 mm for frame 215

Saving results

Total time: 57.67239999771118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929649
Iteration 2/25 | Loss: 0.00181775
Iteration 3/25 | Loss: 0.00147711
Iteration 4/25 | Loss: 0.00140756
Iteration 5/25 | Loss: 0.00144797
Iteration 6/25 | Loss: 0.00143842
Iteration 7/25 | Loss: 0.00140700
Iteration 8/25 | Loss: 0.00134499
Iteration 9/25 | Loss: 0.00133174
Iteration 10/25 | Loss: 0.00131931
Iteration 11/25 | Loss: 0.00131329
Iteration 12/25 | Loss: 0.00130614
Iteration 13/25 | Loss: 0.00130838
Iteration 14/25 | Loss: 0.00130647
Iteration 15/25 | Loss: 0.00130797
Iteration 16/25 | Loss: 0.00130309
Iteration 17/25 | Loss: 0.00129578
Iteration 18/25 | Loss: 0.00129039
Iteration 19/25 | Loss: 0.00128859
Iteration 20/25 | Loss: 0.00128509
Iteration 21/25 | Loss: 0.00128287
Iteration 22/25 | Loss: 0.00128225
Iteration 23/25 | Loss: 0.00128212
Iteration 24/25 | Loss: 0.00128516
Iteration 25/25 | Loss: 0.00128215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42909241
Iteration 2/25 | Loss: 0.00141995
Iteration 3/25 | Loss: 0.00119873
Iteration 4/25 | Loss: 0.00119872
Iteration 5/25 | Loss: 0.00119872
Iteration 6/25 | Loss: 0.00119872
Iteration 7/25 | Loss: 0.00119872
Iteration 8/25 | Loss: 0.00119872
Iteration 9/25 | Loss: 0.00119872
Iteration 10/25 | Loss: 0.00119872
Iteration 11/25 | Loss: 0.00119872
Iteration 12/25 | Loss: 0.00119872
Iteration 13/25 | Loss: 0.00119872
Iteration 14/25 | Loss: 0.00119872
Iteration 15/25 | Loss: 0.00119872
Iteration 16/25 | Loss: 0.00119872
Iteration 17/25 | Loss: 0.00119872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011987218167632818, 0.0011987218167632818, 0.0011987218167632818, 0.0011987218167632818, 0.0011987218167632818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011987218167632818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119872
Iteration 2/1000 | Loss: 0.00009002
Iteration 3/1000 | Loss: 0.00024210
Iteration 4/1000 | Loss: 0.00009390
Iteration 5/1000 | Loss: 0.00008274
Iteration 6/1000 | Loss: 0.00008939
Iteration 7/1000 | Loss: 0.00006689
Iteration 8/1000 | Loss: 0.00006829
Iteration 9/1000 | Loss: 0.00010188
Iteration 10/1000 | Loss: 0.00008295
Iteration 11/1000 | Loss: 0.00026033
Iteration 12/1000 | Loss: 0.00009552
Iteration 13/1000 | Loss: 0.00010769
Iteration 14/1000 | Loss: 0.00010150
Iteration 15/1000 | Loss: 0.00017537
Iteration 16/1000 | Loss: 0.00009902
Iteration 17/1000 | Loss: 0.00010097
Iteration 18/1000 | Loss: 0.00009785
Iteration 19/1000 | Loss: 0.00010077
Iteration 20/1000 | Loss: 0.00010403
Iteration 21/1000 | Loss: 0.00087089
Iteration 22/1000 | Loss: 0.00011480
Iteration 23/1000 | Loss: 0.00015033
Iteration 24/1000 | Loss: 0.00062394
Iteration 25/1000 | Loss: 0.00020295
Iteration 26/1000 | Loss: 0.00017285
Iteration 27/1000 | Loss: 0.00018022
Iteration 28/1000 | Loss: 0.00004556
Iteration 29/1000 | Loss: 0.00004468
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00018433
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00003091
Iteration 35/1000 | Loss: 0.00003605
Iteration 36/1000 | Loss: 0.00002325
Iteration 37/1000 | Loss: 0.00002970
Iteration 38/1000 | Loss: 0.00002638
Iteration 39/1000 | Loss: 0.00002512
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002651
Iteration 42/1000 | Loss: 0.00009998
Iteration 43/1000 | Loss: 0.00003246
Iteration 44/1000 | Loss: 0.00003586
Iteration 45/1000 | Loss: 0.00005965
Iteration 46/1000 | Loss: 0.00003774
Iteration 47/1000 | Loss: 0.00005238
Iteration 48/1000 | Loss: 0.00003948
Iteration 49/1000 | Loss: 0.00003724
Iteration 50/1000 | Loss: 0.00005148
Iteration 51/1000 | Loss: 0.00003961
Iteration 52/1000 | Loss: 0.00005742
Iteration 53/1000 | Loss: 0.00003816
Iteration 54/1000 | Loss: 0.00005821
Iteration 55/1000 | Loss: 0.00003755
Iteration 56/1000 | Loss: 0.00004218
Iteration 57/1000 | Loss: 0.00003022
Iteration 58/1000 | Loss: 0.00003852
Iteration 59/1000 | Loss: 0.00002996
Iteration 60/1000 | Loss: 0.00002996
Iteration 61/1000 | Loss: 0.00010984
Iteration 62/1000 | Loss: 0.00003385
Iteration 63/1000 | Loss: 0.00004788
Iteration 64/1000 | Loss: 0.00002005
Iteration 65/1000 | Loss: 0.00001794
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00006037
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00034623
Iteration 71/1000 | Loss: 0.00014363
Iteration 72/1000 | Loss: 0.00020351
Iteration 73/1000 | Loss: 0.00003574
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00022984
Iteration 76/1000 | Loss: 0.00019680
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00029537
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00004004
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001408
Iteration 87/1000 | Loss: 0.00008925
Iteration 88/1000 | Loss: 0.00007318
Iteration 89/1000 | Loss: 0.00001387
Iteration 90/1000 | Loss: 0.00007029
Iteration 91/1000 | Loss: 0.00006431
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001359
Iteration 97/1000 | Loss: 0.00007927
Iteration 98/1000 | Loss: 0.00009156
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001316
Iteration 124/1000 | Loss: 0.00001316
Iteration 125/1000 | Loss: 0.00001316
Iteration 126/1000 | Loss: 0.00001316
Iteration 127/1000 | Loss: 0.00001316
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00013795
Iteration 130/1000 | Loss: 0.00013795
Iteration 131/1000 | Loss: 0.00009359
Iteration 132/1000 | Loss: 0.00001984
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001308
Iteration 144/1000 | Loss: 0.00001308
Iteration 145/1000 | Loss: 0.00001308
Iteration 146/1000 | Loss: 0.00001308
Iteration 147/1000 | Loss: 0.00001308
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001306
Iteration 152/1000 | Loss: 0.00001306
Iteration 153/1000 | Loss: 0.00001306
Iteration 154/1000 | Loss: 0.00001305
Iteration 155/1000 | Loss: 0.00001301
Iteration 156/1000 | Loss: 0.00001301
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001298
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001297
Iteration 175/1000 | Loss: 0.00001297
Iteration 176/1000 | Loss: 0.00001297
Iteration 177/1000 | Loss: 0.00001297
Iteration 178/1000 | Loss: 0.00001297
Iteration 179/1000 | Loss: 0.00001297
Iteration 180/1000 | Loss: 0.00001297
Iteration 181/1000 | Loss: 0.00001296
Iteration 182/1000 | Loss: 0.00001296
Iteration 183/1000 | Loss: 0.00001296
Iteration 184/1000 | Loss: 0.00001296
Iteration 185/1000 | Loss: 0.00001296
Iteration 186/1000 | Loss: 0.00001296
Iteration 187/1000 | Loss: 0.00001296
Iteration 188/1000 | Loss: 0.00001296
Iteration 189/1000 | Loss: 0.00001296
Iteration 190/1000 | Loss: 0.00001296
Iteration 191/1000 | Loss: 0.00001296
Iteration 192/1000 | Loss: 0.00001296
Iteration 193/1000 | Loss: 0.00001296
Iteration 194/1000 | Loss: 0.00001296
Iteration 195/1000 | Loss: 0.00001296
Iteration 196/1000 | Loss: 0.00001296
Iteration 197/1000 | Loss: 0.00001296
Iteration 198/1000 | Loss: 0.00001296
Iteration 199/1000 | Loss: 0.00001296
Iteration 200/1000 | Loss: 0.00001296
Iteration 201/1000 | Loss: 0.00001296
Iteration 202/1000 | Loss: 0.00001296
Iteration 203/1000 | Loss: 0.00001296
Iteration 204/1000 | Loss: 0.00001296
Iteration 205/1000 | Loss: 0.00001296
Iteration 206/1000 | Loss: 0.00001296
Iteration 207/1000 | Loss: 0.00001296
Iteration 208/1000 | Loss: 0.00001296
Iteration 209/1000 | Loss: 0.00001296
Iteration 210/1000 | Loss: 0.00001296
Iteration 211/1000 | Loss: 0.00001296
Iteration 212/1000 | Loss: 0.00001296
Iteration 213/1000 | Loss: 0.00001296
Iteration 214/1000 | Loss: 0.00001296
Iteration 215/1000 | Loss: 0.00001296
Iteration 216/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.295882339036325e-05, 1.295882339036325e-05, 1.295882339036325e-05, 1.295882339036325e-05, 1.295882339036325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.295882339036325e-05

Optimization complete. Final v2v error: 3.089465379714966 mm

Highest mean error: 3.878601312637329 mm for frame 23

Lowest mean error: 2.7541394233703613 mm for frame 61

Saving results

Total time: 199.51542043685913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792757
Iteration 2/25 | Loss: 0.00134945
Iteration 3/25 | Loss: 0.00125413
Iteration 4/25 | Loss: 0.00124010
Iteration 5/25 | Loss: 0.00123657
Iteration 6/25 | Loss: 0.00123602
Iteration 7/25 | Loss: 0.00123602
Iteration 8/25 | Loss: 0.00123602
Iteration 9/25 | Loss: 0.00123602
Iteration 10/25 | Loss: 0.00123602
Iteration 11/25 | Loss: 0.00123602
Iteration 12/25 | Loss: 0.00123602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012360240798443556, 0.0012360240798443556, 0.0012360240798443556, 0.0012360240798443556, 0.0012360240798443556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012360240798443556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39517498
Iteration 2/25 | Loss: 0.00092584
Iteration 3/25 | Loss: 0.00092584
Iteration 4/25 | Loss: 0.00092584
Iteration 5/25 | Loss: 0.00092584
Iteration 6/25 | Loss: 0.00092584
Iteration 7/25 | Loss: 0.00092584
Iteration 8/25 | Loss: 0.00092584
Iteration 9/25 | Loss: 0.00092584
Iteration 10/25 | Loss: 0.00092584
Iteration 11/25 | Loss: 0.00092584
Iteration 12/25 | Loss: 0.00092584
Iteration 13/25 | Loss: 0.00092584
Iteration 14/25 | Loss: 0.00092584
Iteration 15/25 | Loss: 0.00092584
Iteration 16/25 | Loss: 0.00092584
Iteration 17/25 | Loss: 0.00092584
Iteration 18/25 | Loss: 0.00092584
Iteration 19/25 | Loss: 0.00092584
Iteration 20/25 | Loss: 0.00092584
Iteration 21/25 | Loss: 0.00092584
Iteration 22/25 | Loss: 0.00092584
Iteration 23/25 | Loss: 0.00092584
Iteration 24/25 | Loss: 0.00092584
Iteration 25/25 | Loss: 0.00092584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092584
Iteration 2/1000 | Loss: 0.00003145
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00001861
Iteration 5/1000 | Loss: 0.00001749
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001643
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001566
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001470
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001460
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001451
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001440
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001436
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001423
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001422
Iteration 125/1000 | Loss: 0.00001422
Iteration 126/1000 | Loss: 0.00001422
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001421
Iteration 137/1000 | Loss: 0.00001421
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001421
Iteration 145/1000 | Loss: 0.00001421
Iteration 146/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.4212795576895587e-05, 1.4212795576895587e-05, 1.4212795576895587e-05, 1.4212795576895587e-05, 1.4212795576895587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4212795576895587e-05

Optimization complete. Final v2v error: 3.2313342094421387 mm

Highest mean error: 3.9700615406036377 mm for frame 86

Lowest mean error: 3.0416486263275146 mm for frame 135

Saving results

Total time: 36.898066997528076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769650
Iteration 2/25 | Loss: 0.00148251
Iteration 3/25 | Loss: 0.00127176
Iteration 4/25 | Loss: 0.00124715
Iteration 5/25 | Loss: 0.00124800
Iteration 6/25 | Loss: 0.00125201
Iteration 7/25 | Loss: 0.00125530
Iteration 8/25 | Loss: 0.00124332
Iteration 9/25 | Loss: 0.00123346
Iteration 10/25 | Loss: 0.00122756
Iteration 11/25 | Loss: 0.00122659
Iteration 12/25 | Loss: 0.00122646
Iteration 13/25 | Loss: 0.00122646
Iteration 14/25 | Loss: 0.00122645
Iteration 15/25 | Loss: 0.00122645
Iteration 16/25 | Loss: 0.00122645
Iteration 17/25 | Loss: 0.00122645
Iteration 18/25 | Loss: 0.00122645
Iteration 19/25 | Loss: 0.00122645
Iteration 20/25 | Loss: 0.00122645
Iteration 21/25 | Loss: 0.00122645
Iteration 22/25 | Loss: 0.00122645
Iteration 23/25 | Loss: 0.00122645
Iteration 24/25 | Loss: 0.00122645
Iteration 25/25 | Loss: 0.00122645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34350979
Iteration 2/25 | Loss: 0.00075088
Iteration 3/25 | Loss: 0.00075087
Iteration 4/25 | Loss: 0.00075087
Iteration 5/25 | Loss: 0.00075087
Iteration 6/25 | Loss: 0.00075087
Iteration 7/25 | Loss: 0.00075087
Iteration 8/25 | Loss: 0.00075087
Iteration 9/25 | Loss: 0.00075087
Iteration 10/25 | Loss: 0.00075087
Iteration 11/25 | Loss: 0.00075087
Iteration 12/25 | Loss: 0.00075087
Iteration 13/25 | Loss: 0.00075087
Iteration 14/25 | Loss: 0.00075087
Iteration 15/25 | Loss: 0.00075087
Iteration 16/25 | Loss: 0.00075087
Iteration 17/25 | Loss: 0.00075087
Iteration 18/25 | Loss: 0.00075087
Iteration 19/25 | Loss: 0.00075087
Iteration 20/25 | Loss: 0.00075087
Iteration 21/25 | Loss: 0.00075087
Iteration 22/25 | Loss: 0.00075087
Iteration 23/25 | Loss: 0.00075087
Iteration 24/25 | Loss: 0.00075087
Iteration 25/25 | Loss: 0.00075087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075087
Iteration 2/1000 | Loss: 0.00003494
Iteration 3/1000 | Loss: 0.00002482
Iteration 4/1000 | Loss: 0.00002025
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001690
Iteration 9/1000 | Loss: 0.00001655
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001587
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001563
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001553
Iteration 21/1000 | Loss: 0.00001552
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001544
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001543
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001540
Iteration 46/1000 | Loss: 0.00001540
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001538
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001536
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001534
Iteration 66/1000 | Loss: 0.00001534
Iteration 67/1000 | Loss: 0.00001534
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001533
Iteration 71/1000 | Loss: 0.00001533
Iteration 72/1000 | Loss: 0.00001533
Iteration 73/1000 | Loss: 0.00001532
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001532
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001531
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001530
Iteration 84/1000 | Loss: 0.00001530
Iteration 85/1000 | Loss: 0.00001530
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001529
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001528
Iteration 93/1000 | Loss: 0.00001528
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001526
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001525
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001523
Iteration 117/1000 | Loss: 0.00001523
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001521
Iteration 125/1000 | Loss: 0.00001521
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001520
Iteration 134/1000 | Loss: 0.00001520
Iteration 135/1000 | Loss: 0.00001520
Iteration 136/1000 | Loss: 0.00001520
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001520
Iteration 139/1000 | Loss: 0.00001520
Iteration 140/1000 | Loss: 0.00001520
Iteration 141/1000 | Loss: 0.00001519
Iteration 142/1000 | Loss: 0.00001519
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Iteration 145/1000 | Loss: 0.00001519
Iteration 146/1000 | Loss: 0.00001519
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001518
Iteration 149/1000 | Loss: 0.00001518
Iteration 150/1000 | Loss: 0.00001518
Iteration 151/1000 | Loss: 0.00001518
Iteration 152/1000 | Loss: 0.00001518
Iteration 153/1000 | Loss: 0.00001518
Iteration 154/1000 | Loss: 0.00001518
Iteration 155/1000 | Loss: 0.00001518
Iteration 156/1000 | Loss: 0.00001518
Iteration 157/1000 | Loss: 0.00001518
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001517
Iteration 162/1000 | Loss: 0.00001517
Iteration 163/1000 | Loss: 0.00001517
Iteration 164/1000 | Loss: 0.00001517
Iteration 165/1000 | Loss: 0.00001517
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001517
Iteration 168/1000 | Loss: 0.00001517
Iteration 169/1000 | Loss: 0.00001517
Iteration 170/1000 | Loss: 0.00001517
Iteration 171/1000 | Loss: 0.00001517
Iteration 172/1000 | Loss: 0.00001517
Iteration 173/1000 | Loss: 0.00001517
Iteration 174/1000 | Loss: 0.00001517
Iteration 175/1000 | Loss: 0.00001517
Iteration 176/1000 | Loss: 0.00001517
Iteration 177/1000 | Loss: 0.00001517
Iteration 178/1000 | Loss: 0.00001517
Iteration 179/1000 | Loss: 0.00001517
Iteration 180/1000 | Loss: 0.00001517
Iteration 181/1000 | Loss: 0.00001517
Iteration 182/1000 | Loss: 0.00001517
Iteration 183/1000 | Loss: 0.00001517
Iteration 184/1000 | Loss: 0.00001517
Iteration 185/1000 | Loss: 0.00001517
Iteration 186/1000 | Loss: 0.00001517
Iteration 187/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.516605607321253e-05, 1.516605607321253e-05, 1.516605607321253e-05, 1.516605607321253e-05, 1.516605607321253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.516605607321253e-05

Optimization complete. Final v2v error: 3.2711997032165527 mm

Highest mean error: 3.677563428878784 mm for frame 80

Lowest mean error: 3.038355827331543 mm for frame 104

Saving results

Total time: 50.99406051635742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992242
Iteration 2/25 | Loss: 0.00249745
Iteration 3/25 | Loss: 0.00209330
Iteration 4/25 | Loss: 0.00202651
Iteration 5/25 | Loss: 0.00191459
Iteration 6/25 | Loss: 0.00173958
Iteration 7/25 | Loss: 0.00141659
Iteration 8/25 | Loss: 0.00135544
Iteration 9/25 | Loss: 0.00134737
Iteration 10/25 | Loss: 0.00134583
Iteration 11/25 | Loss: 0.00134552
Iteration 12/25 | Loss: 0.00134552
Iteration 13/25 | Loss: 0.00134552
Iteration 14/25 | Loss: 0.00134552
Iteration 15/25 | Loss: 0.00134552
Iteration 16/25 | Loss: 0.00134552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013455249136313796, 0.0013455249136313796, 0.0013455249136313796, 0.0013455249136313796, 0.0013455249136313796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013455249136313796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33836722
Iteration 2/25 | Loss: 0.00082924
Iteration 3/25 | Loss: 0.00082924
Iteration 4/25 | Loss: 0.00082924
Iteration 5/25 | Loss: 0.00082924
Iteration 6/25 | Loss: 0.00082923
Iteration 7/25 | Loss: 0.00082923
Iteration 8/25 | Loss: 0.00082923
Iteration 9/25 | Loss: 0.00082923
Iteration 10/25 | Loss: 0.00082923
Iteration 11/25 | Loss: 0.00082923
Iteration 12/25 | Loss: 0.00082923
Iteration 13/25 | Loss: 0.00082923
Iteration 14/25 | Loss: 0.00082923
Iteration 15/25 | Loss: 0.00082923
Iteration 16/25 | Loss: 0.00082923
Iteration 17/25 | Loss: 0.00082923
Iteration 18/25 | Loss: 0.00082923
Iteration 19/25 | Loss: 0.00082923
Iteration 20/25 | Loss: 0.00082923
Iteration 21/25 | Loss: 0.00082923
Iteration 22/25 | Loss: 0.00082923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008292333222925663, 0.0008292333222925663, 0.0008292333222925663, 0.0008292333222925663, 0.0008292333222925663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008292333222925663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082923
Iteration 2/1000 | Loss: 0.00004240
Iteration 3/1000 | Loss: 0.00003097
Iteration 4/1000 | Loss: 0.00002930
Iteration 5/1000 | Loss: 0.00002833
Iteration 6/1000 | Loss: 0.00002778
Iteration 7/1000 | Loss: 0.00002721
Iteration 8/1000 | Loss: 0.00002676
Iteration 9/1000 | Loss: 0.00002638
Iteration 10/1000 | Loss: 0.00002613
Iteration 11/1000 | Loss: 0.00002593
Iteration 12/1000 | Loss: 0.00002574
Iteration 13/1000 | Loss: 0.00002560
Iteration 14/1000 | Loss: 0.00002557
Iteration 15/1000 | Loss: 0.00002553
Iteration 16/1000 | Loss: 0.00002552
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002548
Iteration 19/1000 | Loss: 0.00002548
Iteration 20/1000 | Loss: 0.00002547
Iteration 21/1000 | Loss: 0.00002539
Iteration 22/1000 | Loss: 0.00002537
Iteration 23/1000 | Loss: 0.00002537
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002537
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002537
Iteration 28/1000 | Loss: 0.00002537
Iteration 29/1000 | Loss: 0.00002536
Iteration 30/1000 | Loss: 0.00002536
Iteration 31/1000 | Loss: 0.00002536
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002536
Iteration 34/1000 | Loss: 0.00002536
Iteration 35/1000 | Loss: 0.00002536
Iteration 36/1000 | Loss: 0.00002536
Iteration 37/1000 | Loss: 0.00002535
Iteration 38/1000 | Loss: 0.00002535
Iteration 39/1000 | Loss: 0.00002535
Iteration 40/1000 | Loss: 0.00002535
Iteration 41/1000 | Loss: 0.00002535
Iteration 42/1000 | Loss: 0.00002535
Iteration 43/1000 | Loss: 0.00002535
Iteration 44/1000 | Loss: 0.00002535
Iteration 45/1000 | Loss: 0.00002535
Iteration 46/1000 | Loss: 0.00002534
Iteration 47/1000 | Loss: 0.00002534
Iteration 48/1000 | Loss: 0.00002534
Iteration 49/1000 | Loss: 0.00002533
Iteration 50/1000 | Loss: 0.00002532
Iteration 51/1000 | Loss: 0.00002532
Iteration 52/1000 | Loss: 0.00002532
Iteration 53/1000 | Loss: 0.00002531
Iteration 54/1000 | Loss: 0.00002531
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00002531
Iteration 59/1000 | Loss: 0.00002531
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002530
Iteration 62/1000 | Loss: 0.00002530
Iteration 63/1000 | Loss: 0.00002529
Iteration 64/1000 | Loss: 0.00002529
Iteration 65/1000 | Loss: 0.00002528
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002528
Iteration 68/1000 | Loss: 0.00002528
Iteration 69/1000 | Loss: 0.00002528
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002527
Iteration 73/1000 | Loss: 0.00002527
Iteration 74/1000 | Loss: 0.00002527
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002526
Iteration 77/1000 | Loss: 0.00002526
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002525
Iteration 80/1000 | Loss: 0.00002525
Iteration 81/1000 | Loss: 0.00002525
Iteration 82/1000 | Loss: 0.00002525
Iteration 83/1000 | Loss: 0.00002525
Iteration 84/1000 | Loss: 0.00002525
Iteration 85/1000 | Loss: 0.00002525
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.5246494260500185e-05, 2.5246494260500185e-05, 2.5246494260500185e-05, 2.5246494260500185e-05, 2.5246494260500185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5246494260500185e-05

Optimization complete. Final v2v error: 4.337281227111816 mm

Highest mean error: 4.45832633972168 mm for frame 0

Lowest mean error: 4.080888271331787 mm for frame 238

Saving results

Total time: 49.05771517753601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390322
Iteration 2/25 | Loss: 0.00135550
Iteration 3/25 | Loss: 0.00123244
Iteration 4/25 | Loss: 0.00122439
Iteration 5/25 | Loss: 0.00122093
Iteration 6/25 | Loss: 0.00122093
Iteration 7/25 | Loss: 0.00122093
Iteration 8/25 | Loss: 0.00122093
Iteration 9/25 | Loss: 0.00122093
Iteration 10/25 | Loss: 0.00122093
Iteration 11/25 | Loss: 0.00122093
Iteration 12/25 | Loss: 0.00122093
Iteration 13/25 | Loss: 0.00122093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012209308333694935, 0.0012209308333694935, 0.0012209308333694935, 0.0012209308333694935, 0.0012209308333694935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012209308333694935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48133504
Iteration 2/25 | Loss: 0.00090859
Iteration 3/25 | Loss: 0.00090859
Iteration 4/25 | Loss: 0.00090859
Iteration 5/25 | Loss: 0.00090859
Iteration 6/25 | Loss: 0.00090859
Iteration 7/25 | Loss: 0.00090859
Iteration 8/25 | Loss: 0.00090859
Iteration 9/25 | Loss: 0.00090859
Iteration 10/25 | Loss: 0.00090859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009085871279239655, 0.0009085871279239655, 0.0009085871279239655, 0.0009085871279239655, 0.0009085871279239655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009085871279239655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090859
Iteration 2/1000 | Loss: 0.00002683
Iteration 3/1000 | Loss: 0.00001690
Iteration 4/1000 | Loss: 0.00001506
Iteration 5/1000 | Loss: 0.00001417
Iteration 6/1000 | Loss: 0.00001365
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001317
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001248
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001204
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001200
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001198
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001191
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001190
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001188
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.179828541353345e-05, 1.179828541353345e-05, 1.179828541353345e-05, 1.179828541353345e-05, 1.179828541353345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.179828541353345e-05

Optimization complete. Final v2v error: 2.957915782928467 mm

Highest mean error: 3.1611328125 mm for frame 178

Lowest mean error: 2.827875852584839 mm for frame 211

Saving results

Total time: 35.84417510032654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555793
Iteration 2/25 | Loss: 0.00160135
Iteration 3/25 | Loss: 0.00134849
Iteration 4/25 | Loss: 0.00132840
Iteration 5/25 | Loss: 0.00132484
Iteration 6/25 | Loss: 0.00132422
Iteration 7/25 | Loss: 0.00132422
Iteration 8/25 | Loss: 0.00132422
Iteration 9/25 | Loss: 0.00132422
Iteration 10/25 | Loss: 0.00132422
Iteration 11/25 | Loss: 0.00132422
Iteration 12/25 | Loss: 0.00132422
Iteration 13/25 | Loss: 0.00132422
Iteration 14/25 | Loss: 0.00132422
Iteration 15/25 | Loss: 0.00132422
Iteration 16/25 | Loss: 0.00132422
Iteration 17/25 | Loss: 0.00132422
Iteration 18/25 | Loss: 0.00132422
Iteration 19/25 | Loss: 0.00132422
Iteration 20/25 | Loss: 0.00132422
Iteration 21/25 | Loss: 0.00132422
Iteration 22/25 | Loss: 0.00132422
Iteration 23/25 | Loss: 0.00132422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013242245186120272, 0.0013242245186120272, 0.0013242245186120272, 0.0013242245186120272, 0.0013242245186120272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013242245186120272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42891502
Iteration 2/25 | Loss: 0.00090991
Iteration 3/25 | Loss: 0.00090981
Iteration 4/25 | Loss: 0.00090981
Iteration 5/25 | Loss: 0.00090981
Iteration 6/25 | Loss: 0.00090981
Iteration 7/25 | Loss: 0.00090981
Iteration 8/25 | Loss: 0.00090981
Iteration 9/25 | Loss: 0.00090981
Iteration 10/25 | Loss: 0.00090981
Iteration 11/25 | Loss: 0.00090981
Iteration 12/25 | Loss: 0.00090981
Iteration 13/25 | Loss: 0.00090981
Iteration 14/25 | Loss: 0.00090981
Iteration 15/25 | Loss: 0.00090981
Iteration 16/25 | Loss: 0.00090981
Iteration 17/25 | Loss: 0.00090981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009098078007809818, 0.0009098078007809818, 0.0009098078007809818, 0.0009098078007809818, 0.0009098078007809818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009098078007809818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090981
Iteration 2/1000 | Loss: 0.00003955
Iteration 3/1000 | Loss: 0.00002615
Iteration 4/1000 | Loss: 0.00002320
Iteration 5/1000 | Loss: 0.00002212
Iteration 6/1000 | Loss: 0.00002127
Iteration 7/1000 | Loss: 0.00002077
Iteration 8/1000 | Loss: 0.00002042
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001974
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001963
Iteration 15/1000 | Loss: 0.00001959
Iteration 16/1000 | Loss: 0.00001955
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001939
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001936
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001926
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001925
Iteration 38/1000 | Loss: 0.00001925
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001923
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001921
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00001921
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001920
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001916
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001910
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001908
Iteration 107/1000 | Loss: 0.00001908
Iteration 108/1000 | Loss: 0.00001908
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001905
Iteration 129/1000 | Loss: 0.00001905
Iteration 130/1000 | Loss: 0.00001905
Iteration 131/1000 | Loss: 0.00001905
Iteration 132/1000 | Loss: 0.00001905
Iteration 133/1000 | Loss: 0.00001905
Iteration 134/1000 | Loss: 0.00001904
Iteration 135/1000 | Loss: 0.00001904
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001904
Iteration 138/1000 | Loss: 0.00001904
Iteration 139/1000 | Loss: 0.00001904
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001903
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001903
Iteration 153/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.9026312656933442e-05, 1.9026312656933442e-05, 1.9026312656933442e-05, 1.9026312656933442e-05, 1.9026312656933442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9026312656933442e-05

Optimization complete. Final v2v error: 3.6470119953155518 mm

Highest mean error: 5.333737850189209 mm for frame 187

Lowest mean error: 3.175724744796753 mm for frame 147

Saving results

Total time: 42.46758556365967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520824
Iteration 2/25 | Loss: 0.00141904
Iteration 3/25 | Loss: 0.00129376
Iteration 4/25 | Loss: 0.00128200
Iteration 5/25 | Loss: 0.00127899
Iteration 6/25 | Loss: 0.00127809
Iteration 7/25 | Loss: 0.00127781
Iteration 8/25 | Loss: 0.00127770
Iteration 9/25 | Loss: 0.00127770
Iteration 10/25 | Loss: 0.00127770
Iteration 11/25 | Loss: 0.00127770
Iteration 12/25 | Loss: 0.00127770
Iteration 13/25 | Loss: 0.00127770
Iteration 14/25 | Loss: 0.00127770
Iteration 15/25 | Loss: 0.00127770
Iteration 16/25 | Loss: 0.00127770
Iteration 17/25 | Loss: 0.00127770
Iteration 18/25 | Loss: 0.00127770
Iteration 19/25 | Loss: 0.00127770
Iteration 20/25 | Loss: 0.00127770
Iteration 21/25 | Loss: 0.00127770
Iteration 22/25 | Loss: 0.00127770
Iteration 23/25 | Loss: 0.00127770
Iteration 24/25 | Loss: 0.00127770
Iteration 25/25 | Loss: 0.00127770

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.98110533
Iteration 2/25 | Loss: 0.00105607
Iteration 3/25 | Loss: 0.00105605
Iteration 4/25 | Loss: 0.00105605
Iteration 5/25 | Loss: 0.00105605
Iteration 6/25 | Loss: 0.00105605
Iteration 7/25 | Loss: 0.00105604
Iteration 8/25 | Loss: 0.00105604
Iteration 9/25 | Loss: 0.00105604
Iteration 10/25 | Loss: 0.00105604
Iteration 11/25 | Loss: 0.00105604
Iteration 12/25 | Loss: 0.00105604
Iteration 13/25 | Loss: 0.00105604
Iteration 14/25 | Loss: 0.00105604
Iteration 15/25 | Loss: 0.00105604
Iteration 16/25 | Loss: 0.00105604
Iteration 17/25 | Loss: 0.00105604
Iteration 18/25 | Loss: 0.00105604
Iteration 19/25 | Loss: 0.00105604
Iteration 20/25 | Loss: 0.00105604
Iteration 21/25 | Loss: 0.00105604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010560438968241215, 0.0010560438968241215, 0.0010560438968241215, 0.0010560438968241215, 0.0010560438968241215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010560438968241215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105604
Iteration 2/1000 | Loss: 0.00003494
Iteration 3/1000 | Loss: 0.00002540
Iteration 4/1000 | Loss: 0.00002278
Iteration 5/1000 | Loss: 0.00002135
Iteration 6/1000 | Loss: 0.00002043
Iteration 7/1000 | Loss: 0.00001980
Iteration 8/1000 | Loss: 0.00001939
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001862
Iteration 11/1000 | Loss: 0.00001841
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001796
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001795
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001773
Iteration 32/1000 | Loss: 0.00001773
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001772
Iteration 35/1000 | Loss: 0.00001771
Iteration 36/1000 | Loss: 0.00001771
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001770
Iteration 41/1000 | Loss: 0.00001769
Iteration 42/1000 | Loss: 0.00001769
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001768
Iteration 45/1000 | Loss: 0.00001767
Iteration 46/1000 | Loss: 0.00001767
Iteration 47/1000 | Loss: 0.00001767
Iteration 48/1000 | Loss: 0.00001766
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001766
Iteration 51/1000 | Loss: 0.00001765
Iteration 52/1000 | Loss: 0.00001765
Iteration 53/1000 | Loss: 0.00001765
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001764
Iteration 56/1000 | Loss: 0.00001764
Iteration 57/1000 | Loss: 0.00001763
Iteration 58/1000 | Loss: 0.00001763
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001763
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001756
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001755
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001754
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001753
Iteration 96/1000 | Loss: 0.00001753
Iteration 97/1000 | Loss: 0.00001753
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001748
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001746
Iteration 129/1000 | Loss: 0.00001746
Iteration 130/1000 | Loss: 0.00001746
Iteration 131/1000 | Loss: 0.00001746
Iteration 132/1000 | Loss: 0.00001746
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001745
Iteration 137/1000 | Loss: 0.00001745
Iteration 138/1000 | Loss: 0.00001745
Iteration 139/1000 | Loss: 0.00001745
Iteration 140/1000 | Loss: 0.00001745
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001744
Iteration 147/1000 | Loss: 0.00001744
Iteration 148/1000 | Loss: 0.00001744
Iteration 149/1000 | Loss: 0.00001744
Iteration 150/1000 | Loss: 0.00001744
Iteration 151/1000 | Loss: 0.00001743
Iteration 152/1000 | Loss: 0.00001743
Iteration 153/1000 | Loss: 0.00001743
Iteration 154/1000 | Loss: 0.00001743
Iteration 155/1000 | Loss: 0.00001743
Iteration 156/1000 | Loss: 0.00001743
Iteration 157/1000 | Loss: 0.00001742
Iteration 158/1000 | Loss: 0.00001742
Iteration 159/1000 | Loss: 0.00001742
Iteration 160/1000 | Loss: 0.00001742
Iteration 161/1000 | Loss: 0.00001742
Iteration 162/1000 | Loss: 0.00001742
Iteration 163/1000 | Loss: 0.00001742
Iteration 164/1000 | Loss: 0.00001742
Iteration 165/1000 | Loss: 0.00001742
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Iteration 172/1000 | Loss: 0.00001741
Iteration 173/1000 | Loss: 0.00001741
Iteration 174/1000 | Loss: 0.00001741
Iteration 175/1000 | Loss: 0.00001741
Iteration 176/1000 | Loss: 0.00001741
Iteration 177/1000 | Loss: 0.00001741
Iteration 178/1000 | Loss: 0.00001741
Iteration 179/1000 | Loss: 0.00001741
Iteration 180/1000 | Loss: 0.00001741
Iteration 181/1000 | Loss: 0.00001741
Iteration 182/1000 | Loss: 0.00001741
Iteration 183/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.7407914128853008e-05, 1.7407914128853008e-05, 1.7407914128853008e-05, 1.7407914128853008e-05, 1.7407914128853008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7407914128853008e-05

Optimization complete. Final v2v error: 3.4918951988220215 mm

Highest mean error: 4.544955253601074 mm for frame 59

Lowest mean error: 2.767320156097412 mm for frame 74

Saving results

Total time: 52.979066610336304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803013
Iteration 2/25 | Loss: 0.00127373
Iteration 3/25 | Loss: 0.00122309
Iteration 4/25 | Loss: 0.00121636
Iteration 5/25 | Loss: 0.00121433
Iteration 6/25 | Loss: 0.00121418
Iteration 7/25 | Loss: 0.00121418
Iteration 8/25 | Loss: 0.00121418
Iteration 9/25 | Loss: 0.00121418
Iteration 10/25 | Loss: 0.00121418
Iteration 11/25 | Loss: 0.00121418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012141835177317262, 0.0012141835177317262, 0.0012141835177317262, 0.0012141835177317262, 0.0012141835177317262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012141835177317262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.12018919
Iteration 2/25 | Loss: 0.00092952
Iteration 3/25 | Loss: 0.00092950
Iteration 4/25 | Loss: 0.00092950
Iteration 5/25 | Loss: 0.00092950
Iteration 6/25 | Loss: 0.00092950
Iteration 7/25 | Loss: 0.00092950
Iteration 8/25 | Loss: 0.00092950
Iteration 9/25 | Loss: 0.00092950
Iteration 10/25 | Loss: 0.00092950
Iteration 11/25 | Loss: 0.00092950
Iteration 12/25 | Loss: 0.00092950
Iteration 13/25 | Loss: 0.00092950
Iteration 14/25 | Loss: 0.00092950
Iteration 15/25 | Loss: 0.00092950
Iteration 16/25 | Loss: 0.00092950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009295023046433926, 0.0009295023046433926, 0.0009295023046433926, 0.0009295023046433926, 0.0009295023046433926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009295023046433926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092950
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001586
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001378
Iteration 10/1000 | Loss: 0.00001361
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001319
Iteration 16/1000 | Loss: 0.00001319
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001310
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001295
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001290
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001280
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001276
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001271
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001270
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001267
Iteration 95/1000 | Loss: 0.00001267
Iteration 96/1000 | Loss: 0.00001267
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001265
Iteration 111/1000 | Loss: 0.00001265
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001264
Iteration 122/1000 | Loss: 0.00001264
Iteration 123/1000 | Loss: 0.00001264
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001263
Iteration 127/1000 | Loss: 0.00001263
Iteration 128/1000 | Loss: 0.00001263
Iteration 129/1000 | Loss: 0.00001263
Iteration 130/1000 | Loss: 0.00001263
Iteration 131/1000 | Loss: 0.00001263
Iteration 132/1000 | Loss: 0.00001263
Iteration 133/1000 | Loss: 0.00001263
Iteration 134/1000 | Loss: 0.00001263
Iteration 135/1000 | Loss: 0.00001263
Iteration 136/1000 | Loss: 0.00001263
Iteration 137/1000 | Loss: 0.00001263
Iteration 138/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2634015547519084e-05, 1.2634015547519084e-05, 1.2634015547519084e-05, 1.2634015547519084e-05, 1.2634015547519084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2634015547519084e-05

Optimization complete. Final v2v error: 3.0292322635650635 mm

Highest mean error: 3.1717164516448975 mm for frame 67

Lowest mean error: 2.8277812004089355 mm for frame 2

Saving results

Total time: 34.069281816482544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027847
Iteration 2/25 | Loss: 0.00194838
Iteration 3/25 | Loss: 0.00155456
Iteration 4/25 | Loss: 0.00146168
Iteration 5/25 | Loss: 0.00147740
Iteration 6/25 | Loss: 0.00144107
Iteration 7/25 | Loss: 0.00143368
Iteration 8/25 | Loss: 0.00141148
Iteration 9/25 | Loss: 0.00139840
Iteration 10/25 | Loss: 0.00138469
Iteration 11/25 | Loss: 0.00138578
Iteration 12/25 | Loss: 0.00137542
Iteration 13/25 | Loss: 0.00137200
Iteration 14/25 | Loss: 0.00137543
Iteration 15/25 | Loss: 0.00136625
Iteration 16/25 | Loss: 0.00136501
Iteration 17/25 | Loss: 0.00137038
Iteration 18/25 | Loss: 0.00135858
Iteration 19/25 | Loss: 0.00135728
Iteration 20/25 | Loss: 0.00136142
Iteration 21/25 | Loss: 0.00135493
Iteration 22/25 | Loss: 0.00135281
Iteration 23/25 | Loss: 0.00135116
Iteration 24/25 | Loss: 0.00135014
Iteration 25/25 | Loss: 0.00135073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63117039
Iteration 2/25 | Loss: 0.00137861
Iteration 3/25 | Loss: 0.00130143
Iteration 4/25 | Loss: 0.00130143
Iteration 5/25 | Loss: 0.00130143
Iteration 6/25 | Loss: 0.00130143
Iteration 7/25 | Loss: 0.00130143
Iteration 8/25 | Loss: 0.00130143
Iteration 9/25 | Loss: 0.00130143
Iteration 10/25 | Loss: 0.00130143
Iteration 11/25 | Loss: 0.00130143
Iteration 12/25 | Loss: 0.00130142
Iteration 13/25 | Loss: 0.00130142
Iteration 14/25 | Loss: 0.00130142
Iteration 15/25 | Loss: 0.00130142
Iteration 16/25 | Loss: 0.00130142
Iteration 17/25 | Loss: 0.00130142
Iteration 18/25 | Loss: 0.00130142
Iteration 19/25 | Loss: 0.00130142
Iteration 20/25 | Loss: 0.00130142
Iteration 21/25 | Loss: 0.00130142
Iteration 22/25 | Loss: 0.00130142
Iteration 23/25 | Loss: 0.00130142
Iteration 24/25 | Loss: 0.00130142
Iteration 25/25 | Loss: 0.00130142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130142
Iteration 2/1000 | Loss: 0.00027774
Iteration 3/1000 | Loss: 0.00002881
Iteration 4/1000 | Loss: 0.00002530
Iteration 5/1000 | Loss: 0.00002389
Iteration 6/1000 | Loss: 0.00005013
Iteration 7/1000 | Loss: 0.00004119
Iteration 8/1000 | Loss: 0.00005928
Iteration 9/1000 | Loss: 0.00003907
Iteration 10/1000 | Loss: 0.00003870
Iteration 11/1000 | Loss: 0.00003843
Iteration 12/1000 | Loss: 0.00002297
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00007357
Iteration 15/1000 | Loss: 0.00002099
Iteration 16/1000 | Loss: 0.00003209
Iteration 17/1000 | Loss: 0.00002054
Iteration 18/1000 | Loss: 0.00004573
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00002013
Iteration 23/1000 | Loss: 0.00002013
Iteration 24/1000 | Loss: 0.00002013
Iteration 25/1000 | Loss: 0.00002012
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002009
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00002005
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002001
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001993
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001984
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001982
Iteration 63/1000 | Loss: 0.00001982
Iteration 64/1000 | Loss: 0.00001973
Iteration 65/1000 | Loss: 0.00005092
Iteration 66/1000 | Loss: 0.00001979
Iteration 67/1000 | Loss: 0.00001969
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001967
Iteration 70/1000 | Loss: 0.00001967
Iteration 71/1000 | Loss: 0.00001967
Iteration 72/1000 | Loss: 0.00001967
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001965
Iteration 80/1000 | Loss: 0.00001965
Iteration 81/1000 | Loss: 0.00001964
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001961
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001960
Iteration 89/1000 | Loss: 0.00001960
Iteration 90/1000 | Loss: 0.00001959
Iteration 91/1000 | Loss: 0.00001959
Iteration 92/1000 | Loss: 0.00001959
Iteration 93/1000 | Loss: 0.00001959
Iteration 94/1000 | Loss: 0.00004614
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001957
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001956
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001956
Iteration 101/1000 | Loss: 0.00001956
Iteration 102/1000 | Loss: 0.00001956
Iteration 103/1000 | Loss: 0.00001956
Iteration 104/1000 | Loss: 0.00001956
Iteration 105/1000 | Loss: 0.00001956
Iteration 106/1000 | Loss: 0.00001956
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00001956
Iteration 109/1000 | Loss: 0.00001955
Iteration 110/1000 | Loss: 0.00001955
Iteration 111/1000 | Loss: 0.00001955
Iteration 112/1000 | Loss: 0.00001955
Iteration 113/1000 | Loss: 0.00001955
Iteration 114/1000 | Loss: 0.00001955
Iteration 115/1000 | Loss: 0.00001955
Iteration 116/1000 | Loss: 0.00001955
Iteration 117/1000 | Loss: 0.00001955
Iteration 118/1000 | Loss: 0.00001955
Iteration 119/1000 | Loss: 0.00001955
Iteration 120/1000 | Loss: 0.00001955
Iteration 121/1000 | Loss: 0.00001955
Iteration 122/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.9547467672964558e-05, 1.9547467672964558e-05, 1.9547467672964558e-05, 1.9547467672964558e-05, 1.9547467672964558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9547467672964558e-05

Optimization complete. Final v2v error: 3.7928972244262695 mm

Highest mean error: 4.620272159576416 mm for frame 210

Lowest mean error: 3.5120794773101807 mm for frame 63

Saving results

Total time: 103.61675786972046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780989
Iteration 2/25 | Loss: 0.00148645
Iteration 3/25 | Loss: 0.00131199
Iteration 4/25 | Loss: 0.00130191
Iteration 5/25 | Loss: 0.00130126
Iteration 6/25 | Loss: 0.00130126
Iteration 7/25 | Loss: 0.00130126
Iteration 8/25 | Loss: 0.00130126
Iteration 9/25 | Loss: 0.00130126
Iteration 10/25 | Loss: 0.00130126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001301262527704239, 0.001301262527704239, 0.001301262527704239, 0.001301262527704239, 0.001301262527704239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001301262527704239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32265151
Iteration 2/25 | Loss: 0.00074368
Iteration 3/25 | Loss: 0.00074367
Iteration 4/25 | Loss: 0.00074367
Iteration 5/25 | Loss: 0.00074367
Iteration 6/25 | Loss: 0.00074367
Iteration 7/25 | Loss: 0.00074367
Iteration 8/25 | Loss: 0.00074367
Iteration 9/25 | Loss: 0.00074367
Iteration 10/25 | Loss: 0.00074367
Iteration 11/25 | Loss: 0.00074367
Iteration 12/25 | Loss: 0.00074367
Iteration 13/25 | Loss: 0.00074367
Iteration 14/25 | Loss: 0.00074367
Iteration 15/25 | Loss: 0.00074367
Iteration 16/25 | Loss: 0.00074367
Iteration 17/25 | Loss: 0.00074367
Iteration 18/25 | Loss: 0.00074367
Iteration 19/25 | Loss: 0.00074367
Iteration 20/25 | Loss: 0.00074367
Iteration 21/25 | Loss: 0.00074367
Iteration 22/25 | Loss: 0.00074367
Iteration 23/25 | Loss: 0.00074367
Iteration 24/25 | Loss: 0.00074367
Iteration 25/25 | Loss: 0.00074367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074367
Iteration 2/1000 | Loss: 0.00003392
Iteration 3/1000 | Loss: 0.00002549
Iteration 4/1000 | Loss: 0.00002379
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002191
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001996
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001994
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001988
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001987
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00001984
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001984
Iteration 30/1000 | Loss: 0.00001983
Iteration 31/1000 | Loss: 0.00001983
Iteration 32/1000 | Loss: 0.00001983
Iteration 33/1000 | Loss: 0.00001982
Iteration 34/1000 | Loss: 0.00001982
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001980
Iteration 40/1000 | Loss: 0.00001980
Iteration 41/1000 | Loss: 0.00001980
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001979
Iteration 44/1000 | Loss: 0.00001978
Iteration 45/1000 | Loss: 0.00001977
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001976
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.9762957890634425e-05, 1.9762957890634425e-05, 1.9762957890634425e-05, 1.9762957890634425e-05, 1.9762957890634425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9762957890634425e-05

Optimization complete. Final v2v error: 3.692996025085449 mm

Highest mean error: 3.934331178665161 mm for frame 55

Lowest mean error: 3.4640440940856934 mm for frame 119

Saving results

Total time: 28.878725051879883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718979
Iteration 2/25 | Loss: 0.00175773
Iteration 3/25 | Loss: 0.00142467
Iteration 4/25 | Loss: 0.00136637
Iteration 5/25 | Loss: 0.00134216
Iteration 6/25 | Loss: 0.00133784
Iteration 7/25 | Loss: 0.00133495
Iteration 8/25 | Loss: 0.00133451
Iteration 9/25 | Loss: 0.00133482
Iteration 10/25 | Loss: 0.00133577
Iteration 11/25 | Loss: 0.00133789
Iteration 12/25 | Loss: 0.00133603
Iteration 13/25 | Loss: 0.00133729
Iteration 14/25 | Loss: 0.00133618
Iteration 15/25 | Loss: 0.00133772
Iteration 16/25 | Loss: 0.00133249
Iteration 17/25 | Loss: 0.00133000
Iteration 18/25 | Loss: 0.00132960
Iteration 19/25 | Loss: 0.00132952
Iteration 20/25 | Loss: 0.00132946
Iteration 21/25 | Loss: 0.00132946
Iteration 22/25 | Loss: 0.00132946
Iteration 23/25 | Loss: 0.00132946
Iteration 24/25 | Loss: 0.00132945
Iteration 25/25 | Loss: 0.00132945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.04630375
Iteration 2/25 | Loss: 0.00189682
Iteration 3/25 | Loss: 0.00189640
Iteration 4/25 | Loss: 0.00189640
Iteration 5/25 | Loss: 0.00189640
Iteration 6/25 | Loss: 0.00189640
Iteration 7/25 | Loss: 0.00189640
Iteration 8/25 | Loss: 0.00189640
Iteration 9/25 | Loss: 0.00189640
Iteration 10/25 | Loss: 0.00189640
Iteration 11/25 | Loss: 0.00189640
Iteration 12/25 | Loss: 0.00189640
Iteration 13/25 | Loss: 0.00189640
Iteration 14/25 | Loss: 0.00189640
Iteration 15/25 | Loss: 0.00189640
Iteration 16/25 | Loss: 0.00189640
Iteration 17/25 | Loss: 0.00189640
Iteration 18/25 | Loss: 0.00189640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018963990733027458, 0.0018963990733027458, 0.0018963990733027458, 0.0018963990733027458, 0.0018963990733027458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018963990733027458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189640
Iteration 2/1000 | Loss: 0.00018616
Iteration 3/1000 | Loss: 0.00012200
Iteration 4/1000 | Loss: 0.00009569
Iteration 5/1000 | Loss: 0.00008451
Iteration 6/1000 | Loss: 0.00007890
Iteration 7/1000 | Loss: 0.00009853
Iteration 8/1000 | Loss: 0.00008918
Iteration 9/1000 | Loss: 0.00007329
Iteration 10/1000 | Loss: 0.00007151
Iteration 11/1000 | Loss: 0.00007003
Iteration 12/1000 | Loss: 0.00006874
Iteration 13/1000 | Loss: 0.00006775
Iteration 14/1000 | Loss: 0.00006692
Iteration 15/1000 | Loss: 0.00006620
Iteration 16/1000 | Loss: 0.00006559
Iteration 17/1000 | Loss: 0.00006494
Iteration 18/1000 | Loss: 0.00006438
Iteration 19/1000 | Loss: 0.00006385
Iteration 20/1000 | Loss: 0.00006334
Iteration 21/1000 | Loss: 0.00109578
Iteration 22/1000 | Loss: 0.00069587
Iteration 23/1000 | Loss: 0.00035782
Iteration 24/1000 | Loss: 0.00063994
Iteration 25/1000 | Loss: 0.00008061
Iteration 26/1000 | Loss: 0.00007584
Iteration 27/1000 | Loss: 0.00019876
Iteration 28/1000 | Loss: 0.00007191
Iteration 29/1000 | Loss: 0.00006975
Iteration 30/1000 | Loss: 0.00006845
Iteration 31/1000 | Loss: 0.00006735
Iteration 32/1000 | Loss: 0.00006627
Iteration 33/1000 | Loss: 0.00006519
Iteration 34/1000 | Loss: 0.00006421
Iteration 35/1000 | Loss: 0.00006374
Iteration 36/1000 | Loss: 0.00006339
Iteration 37/1000 | Loss: 0.00006306
Iteration 38/1000 | Loss: 0.00006275
Iteration 39/1000 | Loss: 0.00007846
Iteration 40/1000 | Loss: 0.00006239
Iteration 41/1000 | Loss: 0.00006197
Iteration 42/1000 | Loss: 0.00006194
Iteration 43/1000 | Loss: 0.00006171
Iteration 44/1000 | Loss: 0.00042811
Iteration 45/1000 | Loss: 0.00076919
Iteration 46/1000 | Loss: 0.00136065
Iteration 47/1000 | Loss: 0.00848249
Iteration 48/1000 | Loss: 0.00919947
Iteration 49/1000 | Loss: 0.00195927
Iteration 50/1000 | Loss: 0.00103624
Iteration 51/1000 | Loss: 0.00114410
Iteration 52/1000 | Loss: 0.00036039
Iteration 53/1000 | Loss: 0.00013159
Iteration 54/1000 | Loss: 0.00009674
Iteration 55/1000 | Loss: 0.00007801
Iteration 56/1000 | Loss: 0.00006593
Iteration 57/1000 | Loss: 0.00005749
Iteration 58/1000 | Loss: 0.00005174
Iteration 59/1000 | Loss: 0.00004632
Iteration 60/1000 | Loss: 0.00008044
Iteration 61/1000 | Loss: 0.00004142
Iteration 62/1000 | Loss: 0.00003757
Iteration 63/1000 | Loss: 0.00005805
Iteration 64/1000 | Loss: 0.00003683
Iteration 65/1000 | Loss: 0.00003314
Iteration 66/1000 | Loss: 0.00003113
Iteration 67/1000 | Loss: 0.00002997
Iteration 68/1000 | Loss: 0.00002918
Iteration 69/1000 | Loss: 0.00002863
Iteration 70/1000 | Loss: 0.00002817
Iteration 71/1000 | Loss: 0.00002782
Iteration 72/1000 | Loss: 0.00002761
Iteration 73/1000 | Loss: 0.00002747
Iteration 74/1000 | Loss: 0.00002728
Iteration 75/1000 | Loss: 0.00002724
Iteration 76/1000 | Loss: 0.00002706
Iteration 77/1000 | Loss: 0.00002695
Iteration 78/1000 | Loss: 0.00002687
Iteration 79/1000 | Loss: 0.00002682
Iteration 80/1000 | Loss: 0.00002678
Iteration 81/1000 | Loss: 0.00002676
Iteration 82/1000 | Loss: 0.00002675
Iteration 83/1000 | Loss: 0.00002673
Iteration 84/1000 | Loss: 0.00002669
Iteration 85/1000 | Loss: 0.00002665
Iteration 86/1000 | Loss: 0.00002659
Iteration 87/1000 | Loss: 0.00002659
Iteration 88/1000 | Loss: 0.00002659
Iteration 89/1000 | Loss: 0.00002659
Iteration 90/1000 | Loss: 0.00002659
Iteration 91/1000 | Loss: 0.00002659
Iteration 92/1000 | Loss: 0.00002659
Iteration 93/1000 | Loss: 0.00002659
Iteration 94/1000 | Loss: 0.00002659
Iteration 95/1000 | Loss: 0.00002659
Iteration 96/1000 | Loss: 0.00002658
Iteration 97/1000 | Loss: 0.00002658
Iteration 98/1000 | Loss: 0.00002658
Iteration 99/1000 | Loss: 0.00002658
Iteration 100/1000 | Loss: 0.00002658
Iteration 101/1000 | Loss: 0.00002658
Iteration 102/1000 | Loss: 0.00002657
Iteration 103/1000 | Loss: 0.00002656
Iteration 104/1000 | Loss: 0.00002656
Iteration 105/1000 | Loss: 0.00002656
Iteration 106/1000 | Loss: 0.00002656
Iteration 107/1000 | Loss: 0.00002656
Iteration 108/1000 | Loss: 0.00002656
Iteration 109/1000 | Loss: 0.00002656
Iteration 110/1000 | Loss: 0.00002655
Iteration 111/1000 | Loss: 0.00002655
Iteration 112/1000 | Loss: 0.00002655
Iteration 113/1000 | Loss: 0.00002655
Iteration 114/1000 | Loss: 0.00002654
Iteration 115/1000 | Loss: 0.00002654
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002653
Iteration 118/1000 | Loss: 0.00002653
Iteration 119/1000 | Loss: 0.00002653
Iteration 120/1000 | Loss: 0.00002653
Iteration 121/1000 | Loss: 0.00002652
Iteration 122/1000 | Loss: 0.00002652
Iteration 123/1000 | Loss: 0.00002652
Iteration 124/1000 | Loss: 0.00002652
Iteration 125/1000 | Loss: 0.00002652
Iteration 126/1000 | Loss: 0.00002652
Iteration 127/1000 | Loss: 0.00002651
Iteration 128/1000 | Loss: 0.00002651
Iteration 129/1000 | Loss: 0.00002651
Iteration 130/1000 | Loss: 0.00002651
Iteration 131/1000 | Loss: 0.00002650
Iteration 132/1000 | Loss: 0.00002650
Iteration 133/1000 | Loss: 0.00002650
Iteration 134/1000 | Loss: 0.00002650
Iteration 135/1000 | Loss: 0.00002650
Iteration 136/1000 | Loss: 0.00002650
Iteration 137/1000 | Loss: 0.00002650
Iteration 138/1000 | Loss: 0.00002649
Iteration 139/1000 | Loss: 0.00002649
Iteration 140/1000 | Loss: 0.00002649
Iteration 141/1000 | Loss: 0.00002649
Iteration 142/1000 | Loss: 0.00002649
Iteration 143/1000 | Loss: 0.00002649
Iteration 144/1000 | Loss: 0.00002648
Iteration 145/1000 | Loss: 0.00002648
Iteration 146/1000 | Loss: 0.00002648
Iteration 147/1000 | Loss: 0.00002648
Iteration 148/1000 | Loss: 0.00002648
Iteration 149/1000 | Loss: 0.00002648
Iteration 150/1000 | Loss: 0.00002648
Iteration 151/1000 | Loss: 0.00002648
Iteration 152/1000 | Loss: 0.00002647
Iteration 153/1000 | Loss: 0.00002647
Iteration 154/1000 | Loss: 0.00002647
Iteration 155/1000 | Loss: 0.00002647
Iteration 156/1000 | Loss: 0.00002647
Iteration 157/1000 | Loss: 0.00002647
Iteration 158/1000 | Loss: 0.00002647
Iteration 159/1000 | Loss: 0.00002647
Iteration 160/1000 | Loss: 0.00002647
Iteration 161/1000 | Loss: 0.00002647
Iteration 162/1000 | Loss: 0.00002647
Iteration 163/1000 | Loss: 0.00002647
Iteration 164/1000 | Loss: 0.00002647
Iteration 165/1000 | Loss: 0.00002647
Iteration 166/1000 | Loss: 0.00002647
Iteration 167/1000 | Loss: 0.00002647
Iteration 168/1000 | Loss: 0.00002647
Iteration 169/1000 | Loss: 0.00002647
Iteration 170/1000 | Loss: 0.00002647
Iteration 171/1000 | Loss: 0.00002647
Iteration 172/1000 | Loss: 0.00002647
Iteration 173/1000 | Loss: 0.00002647
Iteration 174/1000 | Loss: 0.00002647
Iteration 175/1000 | Loss: 0.00002647
Iteration 176/1000 | Loss: 0.00002647
Iteration 177/1000 | Loss: 0.00002647
Iteration 178/1000 | Loss: 0.00002647
Iteration 179/1000 | Loss: 0.00002647
Iteration 180/1000 | Loss: 0.00002647
Iteration 181/1000 | Loss: 0.00002647
Iteration 182/1000 | Loss: 0.00002647
Iteration 183/1000 | Loss: 0.00002647
Iteration 184/1000 | Loss: 0.00002647
Iteration 185/1000 | Loss: 0.00002647
Iteration 186/1000 | Loss: 0.00002647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.6470677767065354e-05, 2.6470677767065354e-05, 2.6470677767065354e-05, 2.6470677767065354e-05, 2.6470677767065354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6470677767065354e-05

Optimization complete. Final v2v error: 4.134626388549805 mm

Highest mean error: 12.29104995727539 mm for frame 48

Lowest mean error: 3.049478769302368 mm for frame 79

Saving results

Total time: 152.73630261421204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495732
Iteration 2/25 | Loss: 0.00132710
Iteration 3/25 | Loss: 0.00124099
Iteration 4/25 | Loss: 0.00122880
Iteration 5/25 | Loss: 0.00122607
Iteration 6/25 | Loss: 0.00122545
Iteration 7/25 | Loss: 0.00122545
Iteration 8/25 | Loss: 0.00122545
Iteration 9/25 | Loss: 0.00122545
Iteration 10/25 | Loss: 0.00122545
Iteration 11/25 | Loss: 0.00122545
Iteration 12/25 | Loss: 0.00122545
Iteration 13/25 | Loss: 0.00122545
Iteration 14/25 | Loss: 0.00122545
Iteration 15/25 | Loss: 0.00122545
Iteration 16/25 | Loss: 0.00122545
Iteration 17/25 | Loss: 0.00122545
Iteration 18/25 | Loss: 0.00122545
Iteration 19/25 | Loss: 0.00122545
Iteration 20/25 | Loss: 0.00122545
Iteration 21/25 | Loss: 0.00122545
Iteration 22/25 | Loss: 0.00122545
Iteration 23/25 | Loss: 0.00122545
Iteration 24/25 | Loss: 0.00122545
Iteration 25/25 | Loss: 0.00122545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97428274
Iteration 2/25 | Loss: 0.00086106
Iteration 3/25 | Loss: 0.00086106
Iteration 4/25 | Loss: 0.00086106
Iteration 5/25 | Loss: 0.00086106
Iteration 6/25 | Loss: 0.00086105
Iteration 7/25 | Loss: 0.00086105
Iteration 8/25 | Loss: 0.00086105
Iteration 9/25 | Loss: 0.00086105
Iteration 10/25 | Loss: 0.00086105
Iteration 11/25 | Loss: 0.00086105
Iteration 12/25 | Loss: 0.00086105
Iteration 13/25 | Loss: 0.00086105
Iteration 14/25 | Loss: 0.00086105
Iteration 15/25 | Loss: 0.00086105
Iteration 16/25 | Loss: 0.00086105
Iteration 17/25 | Loss: 0.00086105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008610536460764706, 0.0008610536460764706, 0.0008610536460764706, 0.0008610536460764706, 0.0008610536460764706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008610536460764706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086105
Iteration 2/1000 | Loss: 0.00004396
Iteration 3/1000 | Loss: 0.00003010
Iteration 4/1000 | Loss: 0.00002518
Iteration 5/1000 | Loss: 0.00002382
Iteration 6/1000 | Loss: 0.00002281
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002076
Iteration 10/1000 | Loss: 0.00002049
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001947
Iteration 14/1000 | Loss: 0.00001924
Iteration 15/1000 | Loss: 0.00001890
Iteration 16/1000 | Loss: 0.00001878
Iteration 17/1000 | Loss: 0.00001864
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001860
Iteration 20/1000 | Loss: 0.00001860
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00001856
Iteration 23/1000 | Loss: 0.00001853
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001848
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001838
Iteration 33/1000 | Loss: 0.00001837
Iteration 34/1000 | Loss: 0.00001836
Iteration 35/1000 | Loss: 0.00001836
Iteration 36/1000 | Loss: 0.00001836
Iteration 37/1000 | Loss: 0.00001835
Iteration 38/1000 | Loss: 0.00001835
Iteration 39/1000 | Loss: 0.00001835
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001834
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001833
Iteration 50/1000 | Loss: 0.00001832
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001832
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001830
Iteration 62/1000 | Loss: 0.00001830
Iteration 63/1000 | Loss: 0.00001830
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001828
Iteration 92/1000 | Loss: 0.00001828
Iteration 93/1000 | Loss: 0.00001828
Iteration 94/1000 | Loss: 0.00001828
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.828493805078324e-05, 1.828493805078324e-05, 1.828493805078324e-05, 1.828493805078324e-05, 1.828493805078324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.828493805078324e-05

Optimization complete. Final v2v error: 3.611271858215332 mm

Highest mean error: 3.6419265270233154 mm for frame 38

Lowest mean error: 3.5889904499053955 mm for frame 56

Saving results

Total time: 36.39277100563049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531165
Iteration 2/25 | Loss: 0.00136441
Iteration 3/25 | Loss: 0.00127025
Iteration 4/25 | Loss: 0.00125826
Iteration 5/25 | Loss: 0.00125506
Iteration 6/25 | Loss: 0.00125424
Iteration 7/25 | Loss: 0.00125424
Iteration 8/25 | Loss: 0.00125424
Iteration 9/25 | Loss: 0.00125424
Iteration 10/25 | Loss: 0.00125424
Iteration 11/25 | Loss: 0.00125424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012542377226054668, 0.0012542377226054668, 0.0012542377226054668, 0.0012542377226054668, 0.0012542377226054668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012542377226054668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07310438
Iteration 2/25 | Loss: 0.00121392
Iteration 3/25 | Loss: 0.00121392
Iteration 4/25 | Loss: 0.00121392
Iteration 5/25 | Loss: 0.00121392
Iteration 6/25 | Loss: 0.00121392
Iteration 7/25 | Loss: 0.00121392
Iteration 8/25 | Loss: 0.00121392
Iteration 9/25 | Loss: 0.00121392
Iteration 10/25 | Loss: 0.00121392
Iteration 11/25 | Loss: 0.00121392
Iteration 12/25 | Loss: 0.00121392
Iteration 13/25 | Loss: 0.00121392
Iteration 14/25 | Loss: 0.00121392
Iteration 15/25 | Loss: 0.00121392
Iteration 16/25 | Loss: 0.00121392
Iteration 17/25 | Loss: 0.00121392
Iteration 18/25 | Loss: 0.00121392
Iteration 19/25 | Loss: 0.00121392
Iteration 20/25 | Loss: 0.00121392
Iteration 21/25 | Loss: 0.00121392
Iteration 22/25 | Loss: 0.00121392
Iteration 23/25 | Loss: 0.00121392
Iteration 24/25 | Loss: 0.00121392
Iteration 25/25 | Loss: 0.00121392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121392
Iteration 2/1000 | Loss: 0.00003392
Iteration 3/1000 | Loss: 0.00002091
Iteration 4/1000 | Loss: 0.00001631
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001403
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001393
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001342
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001328
Iteration 33/1000 | Loss: 0.00001328
Iteration 34/1000 | Loss: 0.00001327
Iteration 35/1000 | Loss: 0.00001327
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001324
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001315
Iteration 82/1000 | Loss: 0.00001314
Iteration 83/1000 | Loss: 0.00001314
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001306
Iteration 137/1000 | Loss: 0.00001306
Iteration 138/1000 | Loss: 0.00001306
Iteration 139/1000 | Loss: 0.00001305
Iteration 140/1000 | Loss: 0.00001305
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001304
Iteration 145/1000 | Loss: 0.00001304
Iteration 146/1000 | Loss: 0.00001303
Iteration 147/1000 | Loss: 0.00001303
Iteration 148/1000 | Loss: 0.00001303
Iteration 149/1000 | Loss: 0.00001303
Iteration 150/1000 | Loss: 0.00001303
Iteration 151/1000 | Loss: 0.00001303
Iteration 152/1000 | Loss: 0.00001303
Iteration 153/1000 | Loss: 0.00001303
Iteration 154/1000 | Loss: 0.00001303
Iteration 155/1000 | Loss: 0.00001302
Iteration 156/1000 | Loss: 0.00001302
Iteration 157/1000 | Loss: 0.00001301
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001301
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001300
Iteration 164/1000 | Loss: 0.00001300
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001298
Iteration 176/1000 | Loss: 0.00001298
Iteration 177/1000 | Loss: 0.00001298
Iteration 178/1000 | Loss: 0.00001298
Iteration 179/1000 | Loss: 0.00001298
Iteration 180/1000 | Loss: 0.00001298
Iteration 181/1000 | Loss: 0.00001298
Iteration 182/1000 | Loss: 0.00001298
Iteration 183/1000 | Loss: 0.00001298
Iteration 184/1000 | Loss: 0.00001298
Iteration 185/1000 | Loss: 0.00001298
Iteration 186/1000 | Loss: 0.00001298
Iteration 187/1000 | Loss: 0.00001298
Iteration 188/1000 | Loss: 0.00001298
Iteration 189/1000 | Loss: 0.00001298
Iteration 190/1000 | Loss: 0.00001298
Iteration 191/1000 | Loss: 0.00001298
Iteration 192/1000 | Loss: 0.00001298
Iteration 193/1000 | Loss: 0.00001298
Iteration 194/1000 | Loss: 0.00001298
Iteration 195/1000 | Loss: 0.00001298
Iteration 196/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.2982500265934505e-05, 1.2982500265934505e-05, 1.2982500265934505e-05, 1.2982500265934505e-05, 1.2982500265934505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2982500265934505e-05

Optimization complete. Final v2v error: 3.0593020915985107 mm

Highest mean error: 3.6952106952667236 mm for frame 62

Lowest mean error: 2.7742531299591064 mm for frame 1

Saving results

Total time: 39.55207419395447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828611
Iteration 2/25 | Loss: 0.00133900
Iteration 3/25 | Loss: 0.00123756
Iteration 4/25 | Loss: 0.00119943
Iteration 5/25 | Loss: 0.00119523
Iteration 6/25 | Loss: 0.00119281
Iteration 7/25 | Loss: 0.00119236
Iteration 8/25 | Loss: 0.00119232
Iteration 9/25 | Loss: 0.00119232
Iteration 10/25 | Loss: 0.00119232
Iteration 11/25 | Loss: 0.00119232
Iteration 12/25 | Loss: 0.00119232
Iteration 13/25 | Loss: 0.00119232
Iteration 14/25 | Loss: 0.00119232
Iteration 15/25 | Loss: 0.00119232
Iteration 16/25 | Loss: 0.00119232
Iteration 17/25 | Loss: 0.00119232
Iteration 18/25 | Loss: 0.00119232
Iteration 19/25 | Loss: 0.00119232
Iteration 20/25 | Loss: 0.00119232
Iteration 21/25 | Loss: 0.00119232
Iteration 22/25 | Loss: 0.00119232
Iteration 23/25 | Loss: 0.00119232
Iteration 24/25 | Loss: 0.00119232
Iteration 25/25 | Loss: 0.00119232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68958473
Iteration 2/25 | Loss: 0.00100476
Iteration 3/25 | Loss: 0.00100476
Iteration 4/25 | Loss: 0.00100476
Iteration 5/25 | Loss: 0.00100476
Iteration 6/25 | Loss: 0.00100475
Iteration 7/25 | Loss: 0.00100475
Iteration 8/25 | Loss: 0.00100475
Iteration 9/25 | Loss: 0.00100470
Iteration 10/25 | Loss: 0.00100469
Iteration 11/25 | Loss: 0.00100469
Iteration 12/25 | Loss: 0.00100469
Iteration 13/25 | Loss: 0.00100469
Iteration 14/25 | Loss: 0.00100469
Iteration 15/25 | Loss: 0.00100469
Iteration 16/25 | Loss: 0.00100469
Iteration 17/25 | Loss: 0.00100469
Iteration 18/25 | Loss: 0.00100469
Iteration 19/25 | Loss: 0.00100469
Iteration 20/25 | Loss: 0.00100469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010046928655356169, 0.0010046928655356169, 0.0010046928655356169, 0.0010046928655356169, 0.0010046928655356169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010046928655356169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100469
Iteration 2/1000 | Loss: 0.00001950
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001728
Iteration 5/1000 | Loss: 0.00001275
Iteration 6/1000 | Loss: 0.00001304
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001193
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001146
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001101
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001084
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001085
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001064
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001064
Iteration 45/1000 | Loss: 0.00001064
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001063
Iteration 48/1000 | Loss: 0.00001063
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001063
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001063
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001063
Iteration 63/1000 | Loss: 0.00001063
Iteration 64/1000 | Loss: 0.00001063
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.0626836228766479e-05, 1.0626836228766479e-05, 1.0626836228766479e-05, 1.0626836228766479e-05, 1.0626836228766479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0626836228766479e-05

Optimization complete. Final v2v error: 2.8291213512420654 mm

Highest mean error: 3.0764060020446777 mm for frame 161

Lowest mean error: 2.6316444873809814 mm for frame 230

Saving results

Total time: 43.216129541397095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410160
Iteration 2/25 | Loss: 0.00129281
Iteration 3/25 | Loss: 0.00120184
Iteration 4/25 | Loss: 0.00119160
Iteration 5/25 | Loss: 0.00118931
Iteration 6/25 | Loss: 0.00118906
Iteration 7/25 | Loss: 0.00118906
Iteration 8/25 | Loss: 0.00118906
Iteration 9/25 | Loss: 0.00118906
Iteration 10/25 | Loss: 0.00118906
Iteration 11/25 | Loss: 0.00118906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011890630703419447, 0.0011890630703419447, 0.0011890630703419447, 0.0011890630703419447, 0.0011890630703419447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011890630703419447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35908520
Iteration 2/25 | Loss: 0.00109420
Iteration 3/25 | Loss: 0.00109420
Iteration 4/25 | Loss: 0.00109420
Iteration 5/25 | Loss: 0.00109420
Iteration 6/25 | Loss: 0.00109420
Iteration 7/25 | Loss: 0.00109420
Iteration 8/25 | Loss: 0.00109420
Iteration 9/25 | Loss: 0.00109420
Iteration 10/25 | Loss: 0.00109420
Iteration 11/25 | Loss: 0.00109420
Iteration 12/25 | Loss: 0.00109420
Iteration 13/25 | Loss: 0.00109420
Iteration 14/25 | Loss: 0.00109420
Iteration 15/25 | Loss: 0.00109420
Iteration 16/25 | Loss: 0.00109420
Iteration 17/25 | Loss: 0.00109420
Iteration 18/25 | Loss: 0.00109420
Iteration 19/25 | Loss: 0.00109420
Iteration 20/25 | Loss: 0.00109420
Iteration 21/25 | Loss: 0.00109420
Iteration 22/25 | Loss: 0.00109420
Iteration 23/25 | Loss: 0.00109420
Iteration 24/25 | Loss: 0.00109420
Iteration 25/25 | Loss: 0.00109420

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109420
Iteration 2/1000 | Loss: 0.00002441
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001172
Iteration 7/1000 | Loss: 0.00001118
Iteration 8/1000 | Loss: 0.00001089
Iteration 9/1000 | Loss: 0.00001074
Iteration 10/1000 | Loss: 0.00001064
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001039
Iteration 16/1000 | Loss: 0.00001037
Iteration 17/1000 | Loss: 0.00001036
Iteration 18/1000 | Loss: 0.00001035
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001025
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001021
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001015
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001015
Iteration 32/1000 | Loss: 0.00001015
Iteration 33/1000 | Loss: 0.00001015
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001015
Iteration 36/1000 | Loss: 0.00001015
Iteration 37/1000 | Loss: 0.00001015
Iteration 38/1000 | Loss: 0.00001015
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001014
Iteration 41/1000 | Loss: 0.00001014
Iteration 42/1000 | Loss: 0.00001014
Iteration 43/1000 | Loss: 0.00001014
Iteration 44/1000 | Loss: 0.00001014
Iteration 45/1000 | Loss: 0.00001014
Iteration 46/1000 | Loss: 0.00001014
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001014
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001014
Iteration 51/1000 | Loss: 0.00001014
Iteration 52/1000 | Loss: 0.00001014
Iteration 53/1000 | Loss: 0.00001014
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001014
Iteration 60/1000 | Loss: 0.00001014
Iteration 61/1000 | Loss: 0.00001014
Iteration 62/1000 | Loss: 0.00001014
Iteration 63/1000 | Loss: 0.00001012
Iteration 64/1000 | Loss: 0.00001011
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001008
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001007
Iteration 69/1000 | Loss: 0.00001007
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000993
Iteration 85/1000 | Loss: 0.00000993
Iteration 86/1000 | Loss: 0.00000993
Iteration 87/1000 | Loss: 0.00000993
Iteration 88/1000 | Loss: 0.00000993
Iteration 89/1000 | Loss: 0.00000993
Iteration 90/1000 | Loss: 0.00000993
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000992
Iteration 93/1000 | Loss: 0.00000991
Iteration 94/1000 | Loss: 0.00000991
Iteration 95/1000 | Loss: 0.00000991
Iteration 96/1000 | Loss: 0.00000990
Iteration 97/1000 | Loss: 0.00000990
Iteration 98/1000 | Loss: 0.00000990
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000989
Iteration 101/1000 | Loss: 0.00000989
Iteration 102/1000 | Loss: 0.00000989
Iteration 103/1000 | Loss: 0.00000989
Iteration 104/1000 | Loss: 0.00000988
Iteration 105/1000 | Loss: 0.00000988
Iteration 106/1000 | Loss: 0.00000988
Iteration 107/1000 | Loss: 0.00000987
Iteration 108/1000 | Loss: 0.00000987
Iteration 109/1000 | Loss: 0.00000987
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000986
Iteration 112/1000 | Loss: 0.00000986
Iteration 113/1000 | Loss: 0.00000986
Iteration 114/1000 | Loss: 0.00000986
Iteration 115/1000 | Loss: 0.00000986
Iteration 116/1000 | Loss: 0.00000986
Iteration 117/1000 | Loss: 0.00000985
Iteration 118/1000 | Loss: 0.00000985
Iteration 119/1000 | Loss: 0.00000985
Iteration 120/1000 | Loss: 0.00000985
Iteration 121/1000 | Loss: 0.00000985
Iteration 122/1000 | Loss: 0.00000984
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000983
Iteration 127/1000 | Loss: 0.00000982
Iteration 128/1000 | Loss: 0.00000982
Iteration 129/1000 | Loss: 0.00000982
Iteration 130/1000 | Loss: 0.00000981
Iteration 131/1000 | Loss: 0.00000980
Iteration 132/1000 | Loss: 0.00000980
Iteration 133/1000 | Loss: 0.00000980
Iteration 134/1000 | Loss: 0.00000979
Iteration 135/1000 | Loss: 0.00000979
Iteration 136/1000 | Loss: 0.00000979
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000977
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000977
Iteration 148/1000 | Loss: 0.00000977
Iteration 149/1000 | Loss: 0.00000977
Iteration 150/1000 | Loss: 0.00000977
Iteration 151/1000 | Loss: 0.00000976
Iteration 152/1000 | Loss: 0.00000976
Iteration 153/1000 | Loss: 0.00000976
Iteration 154/1000 | Loss: 0.00000976
Iteration 155/1000 | Loss: 0.00000976
Iteration 156/1000 | Loss: 0.00000976
Iteration 157/1000 | Loss: 0.00000976
Iteration 158/1000 | Loss: 0.00000976
Iteration 159/1000 | Loss: 0.00000976
Iteration 160/1000 | Loss: 0.00000976
Iteration 161/1000 | Loss: 0.00000975
Iteration 162/1000 | Loss: 0.00000975
Iteration 163/1000 | Loss: 0.00000975
Iteration 164/1000 | Loss: 0.00000975
Iteration 165/1000 | Loss: 0.00000975
Iteration 166/1000 | Loss: 0.00000975
Iteration 167/1000 | Loss: 0.00000975
Iteration 168/1000 | Loss: 0.00000975
Iteration 169/1000 | Loss: 0.00000975
Iteration 170/1000 | Loss: 0.00000975
Iteration 171/1000 | Loss: 0.00000974
Iteration 172/1000 | Loss: 0.00000974
Iteration 173/1000 | Loss: 0.00000974
Iteration 174/1000 | Loss: 0.00000974
Iteration 175/1000 | Loss: 0.00000973
Iteration 176/1000 | Loss: 0.00000973
Iteration 177/1000 | Loss: 0.00000973
Iteration 178/1000 | Loss: 0.00000973
Iteration 179/1000 | Loss: 0.00000973
Iteration 180/1000 | Loss: 0.00000973
Iteration 181/1000 | Loss: 0.00000973
Iteration 182/1000 | Loss: 0.00000973
Iteration 183/1000 | Loss: 0.00000973
Iteration 184/1000 | Loss: 0.00000973
Iteration 185/1000 | Loss: 0.00000972
Iteration 186/1000 | Loss: 0.00000972
Iteration 187/1000 | Loss: 0.00000972
Iteration 188/1000 | Loss: 0.00000972
Iteration 189/1000 | Loss: 0.00000972
Iteration 190/1000 | Loss: 0.00000972
Iteration 191/1000 | Loss: 0.00000972
Iteration 192/1000 | Loss: 0.00000972
Iteration 193/1000 | Loss: 0.00000972
Iteration 194/1000 | Loss: 0.00000971
Iteration 195/1000 | Loss: 0.00000971
Iteration 196/1000 | Loss: 0.00000971
Iteration 197/1000 | Loss: 0.00000971
Iteration 198/1000 | Loss: 0.00000971
Iteration 199/1000 | Loss: 0.00000971
Iteration 200/1000 | Loss: 0.00000971
Iteration 201/1000 | Loss: 0.00000971
Iteration 202/1000 | Loss: 0.00000971
Iteration 203/1000 | Loss: 0.00000971
Iteration 204/1000 | Loss: 0.00000971
Iteration 205/1000 | Loss: 0.00000970
Iteration 206/1000 | Loss: 0.00000970
Iteration 207/1000 | Loss: 0.00000970
Iteration 208/1000 | Loss: 0.00000970
Iteration 209/1000 | Loss: 0.00000970
Iteration 210/1000 | Loss: 0.00000970
Iteration 211/1000 | Loss: 0.00000970
Iteration 212/1000 | Loss: 0.00000970
Iteration 213/1000 | Loss: 0.00000970
Iteration 214/1000 | Loss: 0.00000970
Iteration 215/1000 | Loss: 0.00000970
Iteration 216/1000 | Loss: 0.00000970
Iteration 217/1000 | Loss: 0.00000970
Iteration 218/1000 | Loss: 0.00000970
Iteration 219/1000 | Loss: 0.00000970
Iteration 220/1000 | Loss: 0.00000970
Iteration 221/1000 | Loss: 0.00000970
Iteration 222/1000 | Loss: 0.00000970
Iteration 223/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [9.702999705041293e-06, 9.702999705041293e-06, 9.702999705041293e-06, 9.702999705041293e-06, 9.702999705041293e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.702999705041293e-06

Optimization complete. Final v2v error: 2.66660737991333 mm

Highest mean error: 3.5554332733154297 mm for frame 54

Lowest mean error: 2.4975874423980713 mm for frame 111

Saving results

Total time: 40.517935037612915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465017
Iteration 2/25 | Loss: 0.00169582
Iteration 3/25 | Loss: 0.00141535
Iteration 4/25 | Loss: 0.00136154
Iteration 5/25 | Loss: 0.00134971
Iteration 6/25 | Loss: 0.00134005
Iteration 7/25 | Loss: 0.00132129
Iteration 8/25 | Loss: 0.00129661
Iteration 9/25 | Loss: 0.00129377
Iteration 10/25 | Loss: 0.00127618
Iteration 11/25 | Loss: 0.00126246
Iteration 12/25 | Loss: 0.00125729
Iteration 13/25 | Loss: 0.00125568
Iteration 14/25 | Loss: 0.00125516
Iteration 15/25 | Loss: 0.00125503
Iteration 16/25 | Loss: 0.00125503
Iteration 17/25 | Loss: 0.00125503
Iteration 18/25 | Loss: 0.00125503
Iteration 19/25 | Loss: 0.00125503
Iteration 20/25 | Loss: 0.00125503
Iteration 21/25 | Loss: 0.00125503
Iteration 22/25 | Loss: 0.00125502
Iteration 23/25 | Loss: 0.00125502
Iteration 24/25 | Loss: 0.00125502
Iteration 25/25 | Loss: 0.00125502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38253820
Iteration 2/25 | Loss: 0.00101473
Iteration 3/25 | Loss: 0.00101472
Iteration 4/25 | Loss: 0.00101472
Iteration 5/25 | Loss: 0.00101472
Iteration 6/25 | Loss: 0.00101472
Iteration 7/25 | Loss: 0.00101472
Iteration 8/25 | Loss: 0.00101472
Iteration 9/25 | Loss: 0.00101472
Iteration 10/25 | Loss: 0.00101472
Iteration 11/25 | Loss: 0.00101472
Iteration 12/25 | Loss: 0.00101472
Iteration 13/25 | Loss: 0.00101472
Iteration 14/25 | Loss: 0.00101472
Iteration 15/25 | Loss: 0.00101472
Iteration 16/25 | Loss: 0.00101472
Iteration 17/25 | Loss: 0.00101472
Iteration 18/25 | Loss: 0.00101472
Iteration 19/25 | Loss: 0.00101472
Iteration 20/25 | Loss: 0.00101472
Iteration 21/25 | Loss: 0.00101472
Iteration 22/25 | Loss: 0.00101472
Iteration 23/25 | Loss: 0.00101472
Iteration 24/25 | Loss: 0.00101472
Iteration 25/25 | Loss: 0.00101472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101472
Iteration 2/1000 | Loss: 0.00003206
Iteration 3/1000 | Loss: 0.00002274
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001702
Iteration 12/1000 | Loss: 0.00016102
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001531
Iteration 18/1000 | Loss: 0.00001520
Iteration 19/1000 | Loss: 0.00001505
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001493
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001490
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001489
Iteration 33/1000 | Loss: 0.00001489
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001487
Iteration 37/1000 | Loss: 0.00001487
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001481
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001481
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001480
Iteration 63/1000 | Loss: 0.00001480
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001478
Iteration 96/1000 | Loss: 0.00001478
Iteration 97/1000 | Loss: 0.00001478
Iteration 98/1000 | Loss: 0.00001478
Iteration 99/1000 | Loss: 0.00001478
Iteration 100/1000 | Loss: 0.00001477
Iteration 101/1000 | Loss: 0.00001477
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001477
Iteration 106/1000 | Loss: 0.00001477
Iteration 107/1000 | Loss: 0.00001476
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001475
Iteration 115/1000 | Loss: 0.00001475
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4732709132658783e-05, 1.4732709132658783e-05, 1.4732709132658783e-05, 1.4732709132658783e-05, 1.4732709132658783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4732709132658783e-05

Optimization complete. Final v2v error: 3.2747251987457275 mm

Highest mean error: 4.006110191345215 mm for frame 98

Lowest mean error: 3.106621265411377 mm for frame 54

Saving results

Total time: 56.52970337867737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845407
Iteration 2/25 | Loss: 0.00131735
Iteration 3/25 | Loss: 0.00122575
Iteration 4/25 | Loss: 0.00121613
Iteration 5/25 | Loss: 0.00121400
Iteration 6/25 | Loss: 0.00121400
Iteration 7/25 | Loss: 0.00121400
Iteration 8/25 | Loss: 0.00121400
Iteration 9/25 | Loss: 0.00121400
Iteration 10/25 | Loss: 0.00121400
Iteration 11/25 | Loss: 0.00121400
Iteration 12/25 | Loss: 0.00121400
Iteration 13/25 | Loss: 0.00121400
Iteration 14/25 | Loss: 0.00121400
Iteration 15/25 | Loss: 0.00121400
Iteration 16/25 | Loss: 0.00121400
Iteration 17/25 | Loss: 0.00121400
Iteration 18/25 | Loss: 0.00121400
Iteration 19/25 | Loss: 0.00121400
Iteration 20/25 | Loss: 0.00121400
Iteration 21/25 | Loss: 0.00121400
Iteration 22/25 | Loss: 0.00121400
Iteration 23/25 | Loss: 0.00121400
Iteration 24/25 | Loss: 0.00121400
Iteration 25/25 | Loss: 0.00121400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.88975239
Iteration 2/25 | Loss: 0.00097480
Iteration 3/25 | Loss: 0.00097480
Iteration 4/25 | Loss: 0.00097480
Iteration 5/25 | Loss: 0.00097480
Iteration 6/25 | Loss: 0.00097480
Iteration 7/25 | Loss: 0.00097480
Iteration 8/25 | Loss: 0.00097480
Iteration 9/25 | Loss: 0.00097480
Iteration 10/25 | Loss: 0.00097480
Iteration 11/25 | Loss: 0.00097480
Iteration 12/25 | Loss: 0.00097480
Iteration 13/25 | Loss: 0.00097480
Iteration 14/25 | Loss: 0.00097480
Iteration 15/25 | Loss: 0.00097480
Iteration 16/25 | Loss: 0.00097480
Iteration 17/25 | Loss: 0.00097480
Iteration 18/25 | Loss: 0.00097480
Iteration 19/25 | Loss: 0.00097480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009747975855134428, 0.0009747975855134428, 0.0009747975855134428, 0.0009747975855134428, 0.0009747975855134428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009747975855134428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097480
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001551
Iteration 4/1000 | Loss: 0.00001411
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001279
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001151
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001128
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001106
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001099
Iteration 38/1000 | Loss: 0.00001099
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001096
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001092
Iteration 58/1000 | Loss: 0.00001091
Iteration 59/1000 | Loss: 0.00001091
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001089
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001086
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001085
Iteration 74/1000 | Loss: 0.00001085
Iteration 75/1000 | Loss: 0.00001085
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001083
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001081
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001079
Iteration 91/1000 | Loss: 0.00001079
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001078
Iteration 94/1000 | Loss: 0.00001078
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001077
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001072
Iteration 126/1000 | Loss: 0.00001072
Iteration 127/1000 | Loss: 0.00001072
Iteration 128/1000 | Loss: 0.00001072
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001072
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001071
Iteration 142/1000 | Loss: 0.00001071
Iteration 143/1000 | Loss: 0.00001071
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001070
Iteration 147/1000 | Loss: 0.00001070
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.069114568963414e-05, 1.069114568963414e-05, 1.069114568963414e-05, 1.069114568963414e-05, 1.069114568963414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.069114568963414e-05

Optimization complete. Final v2v error: 2.820464611053467 mm

Highest mean error: 3.061371326446533 mm for frame 239

Lowest mean error: 2.643486261367798 mm for frame 134

Saving results

Total time: 42.78403687477112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425249
Iteration 2/25 | Loss: 0.00134112
Iteration 3/25 | Loss: 0.00123873
Iteration 4/25 | Loss: 0.00122557
Iteration 5/25 | Loss: 0.00122207
Iteration 6/25 | Loss: 0.00122122
Iteration 7/25 | Loss: 0.00122122
Iteration 8/25 | Loss: 0.00122122
Iteration 9/25 | Loss: 0.00122122
Iteration 10/25 | Loss: 0.00122122
Iteration 11/25 | Loss: 0.00122122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001221219077706337, 0.001221219077706337, 0.001221219077706337, 0.001221219077706337, 0.001221219077706337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001221219077706337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33702505
Iteration 2/25 | Loss: 0.00108813
Iteration 3/25 | Loss: 0.00108813
Iteration 4/25 | Loss: 0.00108813
Iteration 5/25 | Loss: 0.00108813
Iteration 6/25 | Loss: 0.00108813
Iteration 7/25 | Loss: 0.00108813
Iteration 8/25 | Loss: 0.00108813
Iteration 9/25 | Loss: 0.00108813
Iteration 10/25 | Loss: 0.00108813
Iteration 11/25 | Loss: 0.00108813
Iteration 12/25 | Loss: 0.00108813
Iteration 13/25 | Loss: 0.00108813
Iteration 14/25 | Loss: 0.00108813
Iteration 15/25 | Loss: 0.00108813
Iteration 16/25 | Loss: 0.00108813
Iteration 17/25 | Loss: 0.00108813
Iteration 18/25 | Loss: 0.00108813
Iteration 19/25 | Loss: 0.00108813
Iteration 20/25 | Loss: 0.00108813
Iteration 21/25 | Loss: 0.00108813
Iteration 22/25 | Loss: 0.00108813
Iteration 23/25 | Loss: 0.00108813
Iteration 24/25 | Loss: 0.00108813
Iteration 25/25 | Loss: 0.00108813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108813
Iteration 2/1000 | Loss: 0.00003041
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001571
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001192
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001178
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001178
Iteration 107/1000 | Loss: 0.00001178
Iteration 108/1000 | Loss: 0.00001178
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001177
Iteration 113/1000 | Loss: 0.00001177
Iteration 114/1000 | Loss: 0.00001177
Iteration 115/1000 | Loss: 0.00001177
Iteration 116/1000 | Loss: 0.00001176
Iteration 117/1000 | Loss: 0.00001176
Iteration 118/1000 | Loss: 0.00001176
Iteration 119/1000 | Loss: 0.00001176
Iteration 120/1000 | Loss: 0.00001176
Iteration 121/1000 | Loss: 0.00001176
Iteration 122/1000 | Loss: 0.00001176
Iteration 123/1000 | Loss: 0.00001176
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001176
Iteration 126/1000 | Loss: 0.00001175
Iteration 127/1000 | Loss: 0.00001175
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001175
Iteration 130/1000 | Loss: 0.00001175
Iteration 131/1000 | Loss: 0.00001175
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001173
Iteration 135/1000 | Loss: 0.00001173
Iteration 136/1000 | Loss: 0.00001173
Iteration 137/1000 | Loss: 0.00001173
Iteration 138/1000 | Loss: 0.00001173
Iteration 139/1000 | Loss: 0.00001173
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001172
Iteration 147/1000 | Loss: 0.00001172
Iteration 148/1000 | Loss: 0.00001172
Iteration 149/1000 | Loss: 0.00001172
Iteration 150/1000 | Loss: 0.00001172
Iteration 151/1000 | Loss: 0.00001172
Iteration 152/1000 | Loss: 0.00001172
Iteration 153/1000 | Loss: 0.00001172
Iteration 154/1000 | Loss: 0.00001172
Iteration 155/1000 | Loss: 0.00001172
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001170
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001170
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001170
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001169
Iteration 172/1000 | Loss: 0.00001169
Iteration 173/1000 | Loss: 0.00001169
Iteration 174/1000 | Loss: 0.00001169
Iteration 175/1000 | Loss: 0.00001169
Iteration 176/1000 | Loss: 0.00001169
Iteration 177/1000 | Loss: 0.00001169
Iteration 178/1000 | Loss: 0.00001169
Iteration 179/1000 | Loss: 0.00001169
Iteration 180/1000 | Loss: 0.00001169
Iteration 181/1000 | Loss: 0.00001169
Iteration 182/1000 | Loss: 0.00001169
Iteration 183/1000 | Loss: 0.00001169
Iteration 184/1000 | Loss: 0.00001169
Iteration 185/1000 | Loss: 0.00001169
Iteration 186/1000 | Loss: 0.00001169
Iteration 187/1000 | Loss: 0.00001169
Iteration 188/1000 | Loss: 0.00001169
Iteration 189/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.168969083664706e-05, 1.168969083664706e-05, 1.168969083664706e-05, 1.168969083664706e-05, 1.168969083664706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.168969083664706e-05

Optimization complete. Final v2v error: 2.9264137744903564 mm

Highest mean error: 3.9116930961608887 mm for frame 60

Lowest mean error: 2.6522738933563232 mm for frame 180

Saving results

Total time: 45.23183274269104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439695
Iteration 2/25 | Loss: 0.00129821
Iteration 3/25 | Loss: 0.00123472
Iteration 4/25 | Loss: 0.00122137
Iteration 5/25 | Loss: 0.00121775
Iteration 6/25 | Loss: 0.00121701
Iteration 7/25 | Loss: 0.00121701
Iteration 8/25 | Loss: 0.00121701
Iteration 9/25 | Loss: 0.00121701
Iteration 10/25 | Loss: 0.00121701
Iteration 11/25 | Loss: 0.00121701
Iteration 12/25 | Loss: 0.00121701
Iteration 13/25 | Loss: 0.00121701
Iteration 14/25 | Loss: 0.00121701
Iteration 15/25 | Loss: 0.00121701
Iteration 16/25 | Loss: 0.00121701
Iteration 17/25 | Loss: 0.00121701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012170070549473166, 0.0012170070549473166, 0.0012170070549473166, 0.0012170070549473166, 0.0012170070549473166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012170070549473166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43090439
Iteration 2/25 | Loss: 0.00102178
Iteration 3/25 | Loss: 0.00102178
Iteration 4/25 | Loss: 0.00102178
Iteration 5/25 | Loss: 0.00102178
Iteration 6/25 | Loss: 0.00102178
Iteration 7/25 | Loss: 0.00102178
Iteration 8/25 | Loss: 0.00102178
Iteration 9/25 | Loss: 0.00102178
Iteration 10/25 | Loss: 0.00102178
Iteration 11/25 | Loss: 0.00102178
Iteration 12/25 | Loss: 0.00102178
Iteration 13/25 | Loss: 0.00102178
Iteration 14/25 | Loss: 0.00102178
Iteration 15/25 | Loss: 0.00102178
Iteration 16/25 | Loss: 0.00102178
Iteration 17/25 | Loss: 0.00102178
Iteration 18/25 | Loss: 0.00102178
Iteration 19/25 | Loss: 0.00102178
Iteration 20/25 | Loss: 0.00102178
Iteration 21/25 | Loss: 0.00102178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010217794915661216, 0.0010217794915661216, 0.0010217794915661216, 0.0010217794915661216, 0.0010217794915661216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010217794915661216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102178
Iteration 2/1000 | Loss: 0.00002592
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001711
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001590
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001512
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001438
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001430
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001424
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001402
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001398
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001397
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001395
Iteration 45/1000 | Loss: 0.00001394
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001393
Iteration 49/1000 | Loss: 0.00001393
Iteration 50/1000 | Loss: 0.00001392
Iteration 51/1000 | Loss: 0.00001392
Iteration 52/1000 | Loss: 0.00001391
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001391
Iteration 55/1000 | Loss: 0.00001391
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001377
Iteration 77/1000 | Loss: 0.00001377
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001376
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001375
Iteration 82/1000 | Loss: 0.00001375
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001373
Iteration 91/1000 | Loss: 0.00001373
Iteration 92/1000 | Loss: 0.00001373
Iteration 93/1000 | Loss: 0.00001373
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001371
Iteration 98/1000 | Loss: 0.00001371
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001370
Iteration 102/1000 | Loss: 0.00001370
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001367
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001365
Iteration 142/1000 | Loss: 0.00001365
Iteration 143/1000 | Loss: 0.00001365
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001364
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001364
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001364
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001364
Iteration 161/1000 | Loss: 0.00001364
Iteration 162/1000 | Loss: 0.00001364
Iteration 163/1000 | Loss: 0.00001364
Iteration 164/1000 | Loss: 0.00001364
Iteration 165/1000 | Loss: 0.00001364
Iteration 166/1000 | Loss: 0.00001364
Iteration 167/1000 | Loss: 0.00001364
Iteration 168/1000 | Loss: 0.00001364
Iteration 169/1000 | Loss: 0.00001364
Iteration 170/1000 | Loss: 0.00001364
Iteration 171/1000 | Loss: 0.00001364
Iteration 172/1000 | Loss: 0.00001364
Iteration 173/1000 | Loss: 0.00001364
Iteration 174/1000 | Loss: 0.00001364
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001363
Iteration 180/1000 | Loss: 0.00001363
Iteration 181/1000 | Loss: 0.00001363
Iteration 182/1000 | Loss: 0.00001363
Iteration 183/1000 | Loss: 0.00001363
Iteration 184/1000 | Loss: 0.00001363
Iteration 185/1000 | Loss: 0.00001363
Iteration 186/1000 | Loss: 0.00001363
Iteration 187/1000 | Loss: 0.00001363
Iteration 188/1000 | Loss: 0.00001363
Iteration 189/1000 | Loss: 0.00001363
Iteration 190/1000 | Loss: 0.00001363
Iteration 191/1000 | Loss: 0.00001363
Iteration 192/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.3631432921101805e-05, 1.3631432921101805e-05, 1.3631432921101805e-05, 1.3631432921101805e-05, 1.3631432921101805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3631432921101805e-05

Optimization complete. Final v2v error: 3.156554937362671 mm

Highest mean error: 3.6077253818511963 mm for frame 77

Lowest mean error: 3.0259125232696533 mm for frame 24

Saving results

Total time: 40.112629413604736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775048
Iteration 2/25 | Loss: 0.00142443
Iteration 3/25 | Loss: 0.00128508
Iteration 4/25 | Loss: 0.00127532
Iteration 5/25 | Loss: 0.00127336
Iteration 6/25 | Loss: 0.00127336
Iteration 7/25 | Loss: 0.00127336
Iteration 8/25 | Loss: 0.00127336
Iteration 9/25 | Loss: 0.00127336
Iteration 10/25 | Loss: 0.00127336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012733567273244262, 0.0012733567273244262, 0.0012733567273244262, 0.0012733567273244262, 0.0012733567273244262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012733567273244262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29592609
Iteration 2/25 | Loss: 0.00072464
Iteration 3/25 | Loss: 0.00072459
Iteration 4/25 | Loss: 0.00072459
Iteration 5/25 | Loss: 0.00072459
Iteration 6/25 | Loss: 0.00072459
Iteration 7/25 | Loss: 0.00072459
Iteration 8/25 | Loss: 0.00072459
Iteration 9/25 | Loss: 0.00072459
Iteration 10/25 | Loss: 0.00072459
Iteration 11/25 | Loss: 0.00072459
Iteration 12/25 | Loss: 0.00072459
Iteration 13/25 | Loss: 0.00072459
Iteration 14/25 | Loss: 0.00072459
Iteration 15/25 | Loss: 0.00072459
Iteration 16/25 | Loss: 0.00072459
Iteration 17/25 | Loss: 0.00072459
Iteration 18/25 | Loss: 0.00072459
Iteration 19/25 | Loss: 0.00072459
Iteration 20/25 | Loss: 0.00072459
Iteration 21/25 | Loss: 0.00072459
Iteration 22/25 | Loss: 0.00072459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007245898596011102, 0.0007245898596011102, 0.0007245898596011102, 0.0007245898596011102, 0.0007245898596011102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007245898596011102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072459
Iteration 2/1000 | Loss: 0.00002657
Iteration 3/1000 | Loss: 0.00001955
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001685
Iteration 6/1000 | Loss: 0.00001643
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001526
Iteration 11/1000 | Loss: 0.00001523
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001467
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001462
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001460
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001434
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001432
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001421
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001420
Iteration 57/1000 | Loss: 0.00001420
Iteration 58/1000 | Loss: 0.00001420
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001419
Iteration 62/1000 | Loss: 0.00001419
Iteration 63/1000 | Loss: 0.00001419
Iteration 64/1000 | Loss: 0.00001419
Iteration 65/1000 | Loss: 0.00001419
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001410
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001410
Iteration 81/1000 | Loss: 0.00001410
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001410
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001409
Iteration 90/1000 | Loss: 0.00001409
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001408
Iteration 94/1000 | Loss: 0.00001408
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001402
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001401
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001399
Iteration 121/1000 | Loss: 0.00001399
Iteration 122/1000 | Loss: 0.00001399
Iteration 123/1000 | Loss: 0.00001399
Iteration 124/1000 | Loss: 0.00001399
Iteration 125/1000 | Loss: 0.00001399
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001399
Iteration 131/1000 | Loss: 0.00001399
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001398
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001397
Iteration 139/1000 | Loss: 0.00001397
Iteration 140/1000 | Loss: 0.00001397
Iteration 141/1000 | Loss: 0.00001397
Iteration 142/1000 | Loss: 0.00001396
Iteration 143/1000 | Loss: 0.00001396
Iteration 144/1000 | Loss: 0.00001396
Iteration 145/1000 | Loss: 0.00001396
Iteration 146/1000 | Loss: 0.00001396
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001395
Iteration 153/1000 | Loss: 0.00001395
Iteration 154/1000 | Loss: 0.00001395
Iteration 155/1000 | Loss: 0.00001395
Iteration 156/1000 | Loss: 0.00001395
Iteration 157/1000 | Loss: 0.00001395
Iteration 158/1000 | Loss: 0.00001395
Iteration 159/1000 | Loss: 0.00001395
Iteration 160/1000 | Loss: 0.00001395
Iteration 161/1000 | Loss: 0.00001395
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001394
Iteration 166/1000 | Loss: 0.00001394
Iteration 167/1000 | Loss: 0.00001394
Iteration 168/1000 | Loss: 0.00001394
Iteration 169/1000 | Loss: 0.00001394
Iteration 170/1000 | Loss: 0.00001394
Iteration 171/1000 | Loss: 0.00001394
Iteration 172/1000 | Loss: 0.00001394
Iteration 173/1000 | Loss: 0.00001394
Iteration 174/1000 | Loss: 0.00001394
Iteration 175/1000 | Loss: 0.00001394
Iteration 176/1000 | Loss: 0.00001394
Iteration 177/1000 | Loss: 0.00001394
Iteration 178/1000 | Loss: 0.00001394
Iteration 179/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.3938777556177229e-05, 1.3938777556177229e-05, 1.3938777556177229e-05, 1.3938777556177229e-05, 1.3938777556177229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3938777556177229e-05

Optimization complete. Final v2v error: 3.1712331771850586 mm

Highest mean error: 3.3415868282318115 mm for frame 47

Lowest mean error: 3.0899407863616943 mm for frame 115

Saving results

Total time: 41.30271339416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791429
Iteration 2/25 | Loss: 0.00135384
Iteration 3/25 | Loss: 0.00121494
Iteration 4/25 | Loss: 0.00119515
Iteration 5/25 | Loss: 0.00119087
Iteration 6/25 | Loss: 0.00119073
Iteration 7/25 | Loss: 0.00119073
Iteration 8/25 | Loss: 0.00119073
Iteration 9/25 | Loss: 0.00119073
Iteration 10/25 | Loss: 0.00119073
Iteration 11/25 | Loss: 0.00119073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011907272273674607, 0.0011907272273674607, 0.0011907272273674607, 0.0011907272273674607, 0.0011907272273674607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011907272273674607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34142804
Iteration 2/25 | Loss: 0.00105093
Iteration 3/25 | Loss: 0.00105093
Iteration 4/25 | Loss: 0.00105093
Iteration 5/25 | Loss: 0.00105093
Iteration 6/25 | Loss: 0.00105093
Iteration 7/25 | Loss: 0.00105093
Iteration 8/25 | Loss: 0.00105093
Iteration 9/25 | Loss: 0.00105093
Iteration 10/25 | Loss: 0.00105093
Iteration 11/25 | Loss: 0.00105093
Iteration 12/25 | Loss: 0.00105093
Iteration 13/25 | Loss: 0.00105093
Iteration 14/25 | Loss: 0.00105093
Iteration 15/25 | Loss: 0.00105093
Iteration 16/25 | Loss: 0.00105093
Iteration 17/25 | Loss: 0.00105093
Iteration 18/25 | Loss: 0.00105093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010509303538128734, 0.0010509303538128734, 0.0010509303538128734, 0.0010509303538128734, 0.0010509303538128734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010509303538128734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105093
Iteration 2/1000 | Loss: 0.00003330
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001624
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001129
Iteration 11/1000 | Loss: 0.00001096
Iteration 12/1000 | Loss: 0.00001082
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001071
Iteration 15/1000 | Loss: 0.00001070
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001067
Iteration 18/1000 | Loss: 0.00001063
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001055
Iteration 23/1000 | Loss: 0.00001055
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001046
Iteration 37/1000 | Loss: 0.00001046
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001045
Iteration 40/1000 | Loss: 0.00001045
Iteration 41/1000 | Loss: 0.00001044
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001043
Iteration 44/1000 | Loss: 0.00001043
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001040
Iteration 50/1000 | Loss: 0.00001040
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001038
Iteration 55/1000 | Loss: 0.00001038
Iteration 56/1000 | Loss: 0.00001038
Iteration 57/1000 | Loss: 0.00001037
Iteration 58/1000 | Loss: 0.00001037
Iteration 59/1000 | Loss: 0.00001037
Iteration 60/1000 | Loss: 0.00001037
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001036
Iteration 63/1000 | Loss: 0.00001035
Iteration 64/1000 | Loss: 0.00001035
Iteration 65/1000 | Loss: 0.00001035
Iteration 66/1000 | Loss: 0.00001034
Iteration 67/1000 | Loss: 0.00001034
Iteration 68/1000 | Loss: 0.00001034
Iteration 69/1000 | Loss: 0.00001033
Iteration 70/1000 | Loss: 0.00001033
Iteration 71/1000 | Loss: 0.00001033
Iteration 72/1000 | Loss: 0.00001032
Iteration 73/1000 | Loss: 0.00001032
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001029
Iteration 83/1000 | Loss: 0.00001029
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001028
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001028
Iteration 92/1000 | Loss: 0.00001028
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001027
Iteration 96/1000 | Loss: 0.00001027
Iteration 97/1000 | Loss: 0.00001027
Iteration 98/1000 | Loss: 0.00001027
Iteration 99/1000 | Loss: 0.00001027
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001027
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001026
Iteration 110/1000 | Loss: 0.00001026
Iteration 111/1000 | Loss: 0.00001026
Iteration 112/1000 | Loss: 0.00001026
Iteration 113/1000 | Loss: 0.00001026
Iteration 114/1000 | Loss: 0.00001026
Iteration 115/1000 | Loss: 0.00001026
Iteration 116/1000 | Loss: 0.00001026
Iteration 117/1000 | Loss: 0.00001026
Iteration 118/1000 | Loss: 0.00001026
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001025
Iteration 122/1000 | Loss: 0.00001025
Iteration 123/1000 | Loss: 0.00001025
Iteration 124/1000 | Loss: 0.00001025
Iteration 125/1000 | Loss: 0.00001025
Iteration 126/1000 | Loss: 0.00001025
Iteration 127/1000 | Loss: 0.00001025
Iteration 128/1000 | Loss: 0.00001024
Iteration 129/1000 | Loss: 0.00001024
Iteration 130/1000 | Loss: 0.00001024
Iteration 131/1000 | Loss: 0.00001024
Iteration 132/1000 | Loss: 0.00001024
Iteration 133/1000 | Loss: 0.00001024
Iteration 134/1000 | Loss: 0.00001024
Iteration 135/1000 | Loss: 0.00001024
Iteration 136/1000 | Loss: 0.00001024
Iteration 137/1000 | Loss: 0.00001023
Iteration 138/1000 | Loss: 0.00001023
Iteration 139/1000 | Loss: 0.00001023
Iteration 140/1000 | Loss: 0.00001023
Iteration 141/1000 | Loss: 0.00001023
Iteration 142/1000 | Loss: 0.00001023
Iteration 143/1000 | Loss: 0.00001023
Iteration 144/1000 | Loss: 0.00001023
Iteration 145/1000 | Loss: 0.00001023
Iteration 146/1000 | Loss: 0.00001023
Iteration 147/1000 | Loss: 0.00001023
Iteration 148/1000 | Loss: 0.00001023
Iteration 149/1000 | Loss: 0.00001023
Iteration 150/1000 | Loss: 0.00001022
Iteration 151/1000 | Loss: 0.00001022
Iteration 152/1000 | Loss: 0.00001022
Iteration 153/1000 | Loss: 0.00001022
Iteration 154/1000 | Loss: 0.00001022
Iteration 155/1000 | Loss: 0.00001022
Iteration 156/1000 | Loss: 0.00001022
Iteration 157/1000 | Loss: 0.00001022
Iteration 158/1000 | Loss: 0.00001022
Iteration 159/1000 | Loss: 0.00001022
Iteration 160/1000 | Loss: 0.00001022
Iteration 161/1000 | Loss: 0.00001021
Iteration 162/1000 | Loss: 0.00001021
Iteration 163/1000 | Loss: 0.00001021
Iteration 164/1000 | Loss: 0.00001021
Iteration 165/1000 | Loss: 0.00001021
Iteration 166/1000 | Loss: 0.00001021
Iteration 167/1000 | Loss: 0.00001021
Iteration 168/1000 | Loss: 0.00001021
Iteration 169/1000 | Loss: 0.00001021
Iteration 170/1000 | Loss: 0.00001021
Iteration 171/1000 | Loss: 0.00001021
Iteration 172/1000 | Loss: 0.00001021
Iteration 173/1000 | Loss: 0.00001020
Iteration 174/1000 | Loss: 0.00001020
Iteration 175/1000 | Loss: 0.00001020
Iteration 176/1000 | Loss: 0.00001020
Iteration 177/1000 | Loss: 0.00001020
Iteration 178/1000 | Loss: 0.00001020
Iteration 179/1000 | Loss: 0.00001020
Iteration 180/1000 | Loss: 0.00001020
Iteration 181/1000 | Loss: 0.00001020
Iteration 182/1000 | Loss: 0.00001019
Iteration 183/1000 | Loss: 0.00001019
Iteration 184/1000 | Loss: 0.00001019
Iteration 185/1000 | Loss: 0.00001019
Iteration 186/1000 | Loss: 0.00001019
Iteration 187/1000 | Loss: 0.00001019
Iteration 188/1000 | Loss: 0.00001018
Iteration 189/1000 | Loss: 0.00001018
Iteration 190/1000 | Loss: 0.00001018
Iteration 191/1000 | Loss: 0.00001017
Iteration 192/1000 | Loss: 0.00001017
Iteration 193/1000 | Loss: 0.00001017
Iteration 194/1000 | Loss: 0.00001017
Iteration 195/1000 | Loss: 0.00001017
Iteration 196/1000 | Loss: 0.00001017
Iteration 197/1000 | Loss: 0.00001017
Iteration 198/1000 | Loss: 0.00001017
Iteration 199/1000 | Loss: 0.00001017
Iteration 200/1000 | Loss: 0.00001016
Iteration 201/1000 | Loss: 0.00001016
Iteration 202/1000 | Loss: 0.00001016
Iteration 203/1000 | Loss: 0.00001016
Iteration 204/1000 | Loss: 0.00001016
Iteration 205/1000 | Loss: 0.00001016
Iteration 206/1000 | Loss: 0.00001016
Iteration 207/1000 | Loss: 0.00001016
Iteration 208/1000 | Loss: 0.00001016
Iteration 209/1000 | Loss: 0.00001016
Iteration 210/1000 | Loss: 0.00001016
Iteration 211/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.0157058568438515e-05, 1.0157058568438515e-05, 1.0157058568438515e-05, 1.0157058568438515e-05, 1.0157058568438515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0157058568438515e-05

Optimization complete. Final v2v error: 2.751159906387329 mm

Highest mean error: 2.9282994270324707 mm for frame 109

Lowest mean error: 2.62359619140625 mm for frame 129

Saving results

Total time: 46.6306688785553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913827
Iteration 2/25 | Loss: 0.00146833
Iteration 3/25 | Loss: 0.00136536
Iteration 4/25 | Loss: 0.00135552
Iteration 5/25 | Loss: 0.00135305
Iteration 6/25 | Loss: 0.00135305
Iteration 7/25 | Loss: 0.00135305
Iteration 8/25 | Loss: 0.00135305
Iteration 9/25 | Loss: 0.00135305
Iteration 10/25 | Loss: 0.00135305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013530516298487782, 0.0013530516298487782, 0.0013530516298487782, 0.0013530516298487782, 0.0013530516298487782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013530516298487782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98072529
Iteration 2/25 | Loss: 0.00114932
Iteration 3/25 | Loss: 0.00114932
Iteration 4/25 | Loss: 0.00114931
Iteration 5/25 | Loss: 0.00114931
Iteration 6/25 | Loss: 0.00114931
Iteration 7/25 | Loss: 0.00114931
Iteration 8/25 | Loss: 0.00114931
Iteration 9/25 | Loss: 0.00114931
Iteration 10/25 | Loss: 0.00114931
Iteration 11/25 | Loss: 0.00114931
Iteration 12/25 | Loss: 0.00114931
Iteration 13/25 | Loss: 0.00114931
Iteration 14/25 | Loss: 0.00114931
Iteration 15/25 | Loss: 0.00114931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011493132915347815, 0.0011493132915347815, 0.0011493132915347815, 0.0011493132915347815, 0.0011493132915347815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011493132915347815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114931
Iteration 2/1000 | Loss: 0.00004763
Iteration 3/1000 | Loss: 0.00003124
Iteration 4/1000 | Loss: 0.00002741
Iteration 5/1000 | Loss: 0.00002598
Iteration 6/1000 | Loss: 0.00002499
Iteration 7/1000 | Loss: 0.00002450
Iteration 8/1000 | Loss: 0.00002400
Iteration 9/1000 | Loss: 0.00002367
Iteration 10/1000 | Loss: 0.00002339
Iteration 11/1000 | Loss: 0.00002306
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002279
Iteration 14/1000 | Loss: 0.00002275
Iteration 15/1000 | Loss: 0.00002260
Iteration 16/1000 | Loss: 0.00002258
Iteration 17/1000 | Loss: 0.00002253
Iteration 18/1000 | Loss: 0.00002249
Iteration 19/1000 | Loss: 0.00002249
Iteration 20/1000 | Loss: 0.00002244
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002243
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002241
Iteration 26/1000 | Loss: 0.00002241
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002238
Iteration 29/1000 | Loss: 0.00002238
Iteration 30/1000 | Loss: 0.00002238
Iteration 31/1000 | Loss: 0.00002236
Iteration 32/1000 | Loss: 0.00002235
Iteration 33/1000 | Loss: 0.00002231
Iteration 34/1000 | Loss: 0.00002231
Iteration 35/1000 | Loss: 0.00002229
Iteration 36/1000 | Loss: 0.00002229
Iteration 37/1000 | Loss: 0.00002228
Iteration 38/1000 | Loss: 0.00002228
Iteration 39/1000 | Loss: 0.00002228
Iteration 40/1000 | Loss: 0.00002228
Iteration 41/1000 | Loss: 0.00002228
Iteration 42/1000 | Loss: 0.00002228
Iteration 43/1000 | Loss: 0.00002228
Iteration 44/1000 | Loss: 0.00002228
Iteration 45/1000 | Loss: 0.00002227
Iteration 46/1000 | Loss: 0.00002227
Iteration 47/1000 | Loss: 0.00002227
Iteration 48/1000 | Loss: 0.00002227
Iteration 49/1000 | Loss: 0.00002227
Iteration 50/1000 | Loss: 0.00002227
Iteration 51/1000 | Loss: 0.00002227
Iteration 52/1000 | Loss: 0.00002227
Iteration 53/1000 | Loss: 0.00002227
Iteration 54/1000 | Loss: 0.00002227
Iteration 55/1000 | Loss: 0.00002227
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002226
Iteration 58/1000 | Loss: 0.00002226
Iteration 59/1000 | Loss: 0.00002225
Iteration 60/1000 | Loss: 0.00002225
Iteration 61/1000 | Loss: 0.00002224
Iteration 62/1000 | Loss: 0.00002224
Iteration 63/1000 | Loss: 0.00002222
Iteration 64/1000 | Loss: 0.00002222
Iteration 65/1000 | Loss: 0.00002222
Iteration 66/1000 | Loss: 0.00002222
Iteration 67/1000 | Loss: 0.00002222
Iteration 68/1000 | Loss: 0.00002222
Iteration 69/1000 | Loss: 0.00002221
Iteration 70/1000 | Loss: 0.00002221
Iteration 71/1000 | Loss: 0.00002219
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002216
Iteration 74/1000 | Loss: 0.00002216
Iteration 75/1000 | Loss: 0.00002215
Iteration 76/1000 | Loss: 0.00002215
Iteration 77/1000 | Loss: 0.00002214
Iteration 78/1000 | Loss: 0.00002214
Iteration 79/1000 | Loss: 0.00002214
Iteration 80/1000 | Loss: 0.00002213
Iteration 81/1000 | Loss: 0.00002213
Iteration 82/1000 | Loss: 0.00002213
Iteration 83/1000 | Loss: 0.00002213
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002212
Iteration 86/1000 | Loss: 0.00002212
Iteration 87/1000 | Loss: 0.00002212
Iteration 88/1000 | Loss: 0.00002212
Iteration 89/1000 | Loss: 0.00002211
Iteration 90/1000 | Loss: 0.00002211
Iteration 91/1000 | Loss: 0.00002211
Iteration 92/1000 | Loss: 0.00002211
Iteration 93/1000 | Loss: 0.00002211
Iteration 94/1000 | Loss: 0.00002210
Iteration 95/1000 | Loss: 0.00002210
Iteration 96/1000 | Loss: 0.00002210
Iteration 97/1000 | Loss: 0.00002210
Iteration 98/1000 | Loss: 0.00002210
Iteration 99/1000 | Loss: 0.00002209
Iteration 100/1000 | Loss: 0.00002209
Iteration 101/1000 | Loss: 0.00002209
Iteration 102/1000 | Loss: 0.00002209
Iteration 103/1000 | Loss: 0.00002209
Iteration 104/1000 | Loss: 0.00002209
Iteration 105/1000 | Loss: 0.00002209
Iteration 106/1000 | Loss: 0.00002209
Iteration 107/1000 | Loss: 0.00002208
Iteration 108/1000 | Loss: 0.00002208
Iteration 109/1000 | Loss: 0.00002208
Iteration 110/1000 | Loss: 0.00002208
Iteration 111/1000 | Loss: 0.00002208
Iteration 112/1000 | Loss: 0.00002208
Iteration 113/1000 | Loss: 0.00002208
Iteration 114/1000 | Loss: 0.00002207
Iteration 115/1000 | Loss: 0.00002207
Iteration 116/1000 | Loss: 0.00002207
Iteration 117/1000 | Loss: 0.00002207
Iteration 118/1000 | Loss: 0.00002207
Iteration 119/1000 | Loss: 0.00002206
Iteration 120/1000 | Loss: 0.00002206
Iteration 121/1000 | Loss: 0.00002206
Iteration 122/1000 | Loss: 0.00002206
Iteration 123/1000 | Loss: 0.00002206
Iteration 124/1000 | Loss: 0.00002206
Iteration 125/1000 | Loss: 0.00002206
Iteration 126/1000 | Loss: 0.00002206
Iteration 127/1000 | Loss: 0.00002206
Iteration 128/1000 | Loss: 0.00002206
Iteration 129/1000 | Loss: 0.00002206
Iteration 130/1000 | Loss: 0.00002206
Iteration 131/1000 | Loss: 0.00002205
Iteration 132/1000 | Loss: 0.00002205
Iteration 133/1000 | Loss: 0.00002205
Iteration 134/1000 | Loss: 0.00002205
Iteration 135/1000 | Loss: 0.00002205
Iteration 136/1000 | Loss: 0.00002205
Iteration 137/1000 | Loss: 0.00002205
Iteration 138/1000 | Loss: 0.00002205
Iteration 139/1000 | Loss: 0.00002205
Iteration 140/1000 | Loss: 0.00002205
Iteration 141/1000 | Loss: 0.00002205
Iteration 142/1000 | Loss: 0.00002205
Iteration 143/1000 | Loss: 0.00002205
Iteration 144/1000 | Loss: 0.00002205
Iteration 145/1000 | Loss: 0.00002204
Iteration 146/1000 | Loss: 0.00002204
Iteration 147/1000 | Loss: 0.00002204
Iteration 148/1000 | Loss: 0.00002204
Iteration 149/1000 | Loss: 0.00002204
Iteration 150/1000 | Loss: 0.00002204
Iteration 151/1000 | Loss: 0.00002203
Iteration 152/1000 | Loss: 0.00002203
Iteration 153/1000 | Loss: 0.00002203
Iteration 154/1000 | Loss: 0.00002203
Iteration 155/1000 | Loss: 0.00002203
Iteration 156/1000 | Loss: 0.00002203
Iteration 157/1000 | Loss: 0.00002203
Iteration 158/1000 | Loss: 0.00002203
Iteration 159/1000 | Loss: 0.00002203
Iteration 160/1000 | Loss: 0.00002202
Iteration 161/1000 | Loss: 0.00002202
Iteration 162/1000 | Loss: 0.00002202
Iteration 163/1000 | Loss: 0.00002202
Iteration 164/1000 | Loss: 0.00002202
Iteration 165/1000 | Loss: 0.00002202
Iteration 166/1000 | Loss: 0.00002202
Iteration 167/1000 | Loss: 0.00002201
Iteration 168/1000 | Loss: 0.00002201
Iteration 169/1000 | Loss: 0.00002201
Iteration 170/1000 | Loss: 0.00002201
Iteration 171/1000 | Loss: 0.00002201
Iteration 172/1000 | Loss: 0.00002201
Iteration 173/1000 | Loss: 0.00002201
Iteration 174/1000 | Loss: 0.00002201
Iteration 175/1000 | Loss: 0.00002201
Iteration 176/1000 | Loss: 0.00002201
Iteration 177/1000 | Loss: 0.00002201
Iteration 178/1000 | Loss: 0.00002201
Iteration 179/1000 | Loss: 0.00002201
Iteration 180/1000 | Loss: 0.00002201
Iteration 181/1000 | Loss: 0.00002201
Iteration 182/1000 | Loss: 0.00002201
Iteration 183/1000 | Loss: 0.00002201
Iteration 184/1000 | Loss: 0.00002201
Iteration 185/1000 | Loss: 0.00002201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.2013015041011386e-05, 2.2013015041011386e-05, 2.2013015041011386e-05, 2.2013015041011386e-05, 2.2013015041011386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2013015041011386e-05

Optimization complete. Final v2v error: 3.8525097370147705 mm

Highest mean error: 4.556949138641357 mm for frame 81

Lowest mean error: 3.362384557723999 mm for frame 0

Saving results

Total time: 41.461036682128906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807544
Iteration 2/25 | Loss: 0.00130656
Iteration 3/25 | Loss: 0.00121959
Iteration 4/25 | Loss: 0.00121200
Iteration 5/25 | Loss: 0.00121001
Iteration 6/25 | Loss: 0.00121001
Iteration 7/25 | Loss: 0.00121001
Iteration 8/25 | Loss: 0.00121001
Iteration 9/25 | Loss: 0.00121001
Iteration 10/25 | Loss: 0.00121001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012100113090127707, 0.0012100113090127707, 0.0012100113090127707, 0.0012100113090127707, 0.0012100113090127707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012100113090127707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34892786
Iteration 2/25 | Loss: 0.00109156
Iteration 3/25 | Loss: 0.00109155
Iteration 4/25 | Loss: 0.00109155
Iteration 5/25 | Loss: 0.00109155
Iteration 6/25 | Loss: 0.00109155
Iteration 7/25 | Loss: 0.00109155
Iteration 8/25 | Loss: 0.00109155
Iteration 9/25 | Loss: 0.00109155
Iteration 10/25 | Loss: 0.00109155
Iteration 11/25 | Loss: 0.00109155
Iteration 12/25 | Loss: 0.00109155
Iteration 13/25 | Loss: 0.00109155
Iteration 14/25 | Loss: 0.00109155
Iteration 15/25 | Loss: 0.00109155
Iteration 16/25 | Loss: 0.00109155
Iteration 17/25 | Loss: 0.00109155
Iteration 18/25 | Loss: 0.00109155
Iteration 19/25 | Loss: 0.00109155
Iteration 20/25 | Loss: 0.00109155
Iteration 21/25 | Loss: 0.00109155
Iteration 22/25 | Loss: 0.00109155
Iteration 23/25 | Loss: 0.00109155
Iteration 24/25 | Loss: 0.00109155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010915502207353711, 0.0010915502207353711, 0.0010915502207353711, 0.0010915502207353711, 0.0010915502207353711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010915502207353711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109155
Iteration 2/1000 | Loss: 0.00003122
Iteration 3/1000 | Loss: 0.00002046
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001294
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001168
Iteration 13/1000 | Loss: 0.00001158
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001130
Iteration 22/1000 | Loss: 0.00001130
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001130
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001129
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001124
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001124
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001123
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001117
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001109
Iteration 69/1000 | Loss: 0.00001109
Iteration 70/1000 | Loss: 0.00001109
Iteration 71/1000 | Loss: 0.00001109
Iteration 72/1000 | Loss: 0.00001108
Iteration 73/1000 | Loss: 0.00001108
Iteration 74/1000 | Loss: 0.00001108
Iteration 75/1000 | Loss: 0.00001108
Iteration 76/1000 | Loss: 0.00001107
Iteration 77/1000 | Loss: 0.00001107
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001101
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001100
Iteration 99/1000 | Loss: 0.00001099
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001099
Iteration 102/1000 | Loss: 0.00001099
Iteration 103/1000 | Loss: 0.00001099
Iteration 104/1000 | Loss: 0.00001099
Iteration 105/1000 | Loss: 0.00001099
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001097
Iteration 112/1000 | Loss: 0.00001097
Iteration 113/1000 | Loss: 0.00001097
Iteration 114/1000 | Loss: 0.00001097
Iteration 115/1000 | Loss: 0.00001096
Iteration 116/1000 | Loss: 0.00001096
Iteration 117/1000 | Loss: 0.00001096
Iteration 118/1000 | Loss: 0.00001095
Iteration 119/1000 | Loss: 0.00001095
Iteration 120/1000 | Loss: 0.00001095
Iteration 121/1000 | Loss: 0.00001094
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001091
Iteration 131/1000 | Loss: 0.00001091
Iteration 132/1000 | Loss: 0.00001091
Iteration 133/1000 | Loss: 0.00001091
Iteration 134/1000 | Loss: 0.00001091
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001090
Iteration 138/1000 | Loss: 0.00001090
Iteration 139/1000 | Loss: 0.00001089
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001089
Iteration 144/1000 | Loss: 0.00001089
Iteration 145/1000 | Loss: 0.00001089
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001089
Iteration 150/1000 | Loss: 0.00001089
Iteration 151/1000 | Loss: 0.00001089
Iteration 152/1000 | Loss: 0.00001088
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001088
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001088
Iteration 159/1000 | Loss: 0.00001088
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001087
Iteration 163/1000 | Loss: 0.00001087
Iteration 164/1000 | Loss: 0.00001087
Iteration 165/1000 | Loss: 0.00001087
Iteration 166/1000 | Loss: 0.00001087
Iteration 167/1000 | Loss: 0.00001087
Iteration 168/1000 | Loss: 0.00001087
Iteration 169/1000 | Loss: 0.00001087
Iteration 170/1000 | Loss: 0.00001087
Iteration 171/1000 | Loss: 0.00001086
Iteration 172/1000 | Loss: 0.00001086
Iteration 173/1000 | Loss: 0.00001086
Iteration 174/1000 | Loss: 0.00001086
Iteration 175/1000 | Loss: 0.00001086
Iteration 176/1000 | Loss: 0.00001086
Iteration 177/1000 | Loss: 0.00001086
Iteration 178/1000 | Loss: 0.00001085
Iteration 179/1000 | Loss: 0.00001085
Iteration 180/1000 | Loss: 0.00001085
Iteration 181/1000 | Loss: 0.00001085
Iteration 182/1000 | Loss: 0.00001085
Iteration 183/1000 | Loss: 0.00001085
Iteration 184/1000 | Loss: 0.00001084
Iteration 185/1000 | Loss: 0.00001084
Iteration 186/1000 | Loss: 0.00001084
Iteration 187/1000 | Loss: 0.00001084
Iteration 188/1000 | Loss: 0.00001084
Iteration 189/1000 | Loss: 0.00001084
Iteration 190/1000 | Loss: 0.00001084
Iteration 191/1000 | Loss: 0.00001084
Iteration 192/1000 | Loss: 0.00001084
Iteration 193/1000 | Loss: 0.00001084
Iteration 194/1000 | Loss: 0.00001084
Iteration 195/1000 | Loss: 0.00001083
Iteration 196/1000 | Loss: 0.00001083
Iteration 197/1000 | Loss: 0.00001083
Iteration 198/1000 | Loss: 0.00001083
Iteration 199/1000 | Loss: 0.00001083
Iteration 200/1000 | Loss: 0.00001083
Iteration 201/1000 | Loss: 0.00001083
Iteration 202/1000 | Loss: 0.00001083
Iteration 203/1000 | Loss: 0.00001083
Iteration 204/1000 | Loss: 0.00001083
Iteration 205/1000 | Loss: 0.00001083
Iteration 206/1000 | Loss: 0.00001083
Iteration 207/1000 | Loss: 0.00001083
Iteration 208/1000 | Loss: 0.00001083
Iteration 209/1000 | Loss: 0.00001083
Iteration 210/1000 | Loss: 0.00001083
Iteration 211/1000 | Loss: 0.00001082
Iteration 212/1000 | Loss: 0.00001082
Iteration 213/1000 | Loss: 0.00001082
Iteration 214/1000 | Loss: 0.00001082
Iteration 215/1000 | Loss: 0.00001082
Iteration 216/1000 | Loss: 0.00001082
Iteration 217/1000 | Loss: 0.00001082
Iteration 218/1000 | Loss: 0.00001082
Iteration 219/1000 | Loss: 0.00001082
Iteration 220/1000 | Loss: 0.00001082
Iteration 221/1000 | Loss: 0.00001082
Iteration 222/1000 | Loss: 0.00001082
Iteration 223/1000 | Loss: 0.00001082
Iteration 224/1000 | Loss: 0.00001082
Iteration 225/1000 | Loss: 0.00001082
Iteration 226/1000 | Loss: 0.00001082
Iteration 227/1000 | Loss: 0.00001082
Iteration 228/1000 | Loss: 0.00001082
Iteration 229/1000 | Loss: 0.00001081
Iteration 230/1000 | Loss: 0.00001081
Iteration 231/1000 | Loss: 0.00001081
Iteration 232/1000 | Loss: 0.00001081
Iteration 233/1000 | Loss: 0.00001081
Iteration 234/1000 | Loss: 0.00001081
Iteration 235/1000 | Loss: 0.00001081
Iteration 236/1000 | Loss: 0.00001081
Iteration 237/1000 | Loss: 0.00001081
Iteration 238/1000 | Loss: 0.00001081
Iteration 239/1000 | Loss: 0.00001081
Iteration 240/1000 | Loss: 0.00001081
Iteration 241/1000 | Loss: 0.00001081
Iteration 242/1000 | Loss: 0.00001081
Iteration 243/1000 | Loss: 0.00001081
Iteration 244/1000 | Loss: 0.00001081
Iteration 245/1000 | Loss: 0.00001081
Iteration 246/1000 | Loss: 0.00001081
Iteration 247/1000 | Loss: 0.00001081
Iteration 248/1000 | Loss: 0.00001081
Iteration 249/1000 | Loss: 0.00001081
Iteration 250/1000 | Loss: 0.00001081
Iteration 251/1000 | Loss: 0.00001080
Iteration 252/1000 | Loss: 0.00001080
Iteration 253/1000 | Loss: 0.00001080
Iteration 254/1000 | Loss: 0.00001080
Iteration 255/1000 | Loss: 0.00001080
Iteration 256/1000 | Loss: 0.00001080
Iteration 257/1000 | Loss: 0.00001080
Iteration 258/1000 | Loss: 0.00001080
Iteration 259/1000 | Loss: 0.00001080
Iteration 260/1000 | Loss: 0.00001080
Iteration 261/1000 | Loss: 0.00001080
Iteration 262/1000 | Loss: 0.00001080
Iteration 263/1000 | Loss: 0.00001080
Iteration 264/1000 | Loss: 0.00001080
Iteration 265/1000 | Loss: 0.00001080
Iteration 266/1000 | Loss: 0.00001080
Iteration 267/1000 | Loss: 0.00001080
Iteration 268/1000 | Loss: 0.00001080
Iteration 269/1000 | Loss: 0.00001080
Iteration 270/1000 | Loss: 0.00001080
Iteration 271/1000 | Loss: 0.00001080
Iteration 272/1000 | Loss: 0.00001080
Iteration 273/1000 | Loss: 0.00001079
Iteration 274/1000 | Loss: 0.00001079
Iteration 275/1000 | Loss: 0.00001079
Iteration 276/1000 | Loss: 0.00001079
Iteration 277/1000 | Loss: 0.00001079
Iteration 278/1000 | Loss: 0.00001079
Iteration 279/1000 | Loss: 0.00001079
Iteration 280/1000 | Loss: 0.00001079
Iteration 281/1000 | Loss: 0.00001079
Iteration 282/1000 | Loss: 0.00001079
Iteration 283/1000 | Loss: 0.00001079
Iteration 284/1000 | Loss: 0.00001079
Iteration 285/1000 | Loss: 0.00001079
Iteration 286/1000 | Loss: 0.00001079
Iteration 287/1000 | Loss: 0.00001079
Iteration 288/1000 | Loss: 0.00001079
Iteration 289/1000 | Loss: 0.00001078
Iteration 290/1000 | Loss: 0.00001078
Iteration 291/1000 | Loss: 0.00001078
Iteration 292/1000 | Loss: 0.00001078
Iteration 293/1000 | Loss: 0.00001078
Iteration 294/1000 | Loss: 0.00001078
Iteration 295/1000 | Loss: 0.00001078
Iteration 296/1000 | Loss: 0.00001078
Iteration 297/1000 | Loss: 0.00001078
Iteration 298/1000 | Loss: 0.00001078
Iteration 299/1000 | Loss: 0.00001078
Iteration 300/1000 | Loss: 0.00001078
Iteration 301/1000 | Loss: 0.00001078
Iteration 302/1000 | Loss: 0.00001078
Iteration 303/1000 | Loss: 0.00001078
Iteration 304/1000 | Loss: 0.00001078
Iteration 305/1000 | Loss: 0.00001078
Iteration 306/1000 | Loss: 0.00001078
Iteration 307/1000 | Loss: 0.00001078
Iteration 308/1000 | Loss: 0.00001078
Iteration 309/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.0779861440823879e-05, 1.0779861440823879e-05, 1.0779861440823879e-05, 1.0779861440823879e-05, 1.0779861440823879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0779861440823879e-05

Optimization complete. Final v2v error: 2.769914150238037 mm

Highest mean error: 3.788919448852539 mm for frame 65

Lowest mean error: 2.459686040878296 mm for frame 153

Saving results

Total time: 45.89278316497803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838119
Iteration 2/25 | Loss: 0.00144038
Iteration 3/25 | Loss: 0.00127954
Iteration 4/25 | Loss: 0.00126163
Iteration 5/25 | Loss: 0.00125607
Iteration 6/25 | Loss: 0.00125559
Iteration 7/25 | Loss: 0.00125559
Iteration 8/25 | Loss: 0.00125559
Iteration 9/25 | Loss: 0.00125559
Iteration 10/25 | Loss: 0.00125559
Iteration 11/25 | Loss: 0.00125559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012555895373225212, 0.0012555895373225212, 0.0012555895373225212, 0.0012555895373225212, 0.0012555895373225212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012555895373225212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89941895
Iteration 2/25 | Loss: 0.00062601
Iteration 3/25 | Loss: 0.00062601
Iteration 4/25 | Loss: 0.00062601
Iteration 5/25 | Loss: 0.00062601
Iteration 6/25 | Loss: 0.00062601
Iteration 7/25 | Loss: 0.00062601
Iteration 8/25 | Loss: 0.00062601
Iteration 9/25 | Loss: 0.00062601
Iteration 10/25 | Loss: 0.00062601
Iteration 11/25 | Loss: 0.00062601
Iteration 12/25 | Loss: 0.00062601
Iteration 13/25 | Loss: 0.00062601
Iteration 14/25 | Loss: 0.00062601
Iteration 15/25 | Loss: 0.00062601
Iteration 16/25 | Loss: 0.00062601
Iteration 17/25 | Loss: 0.00062601
Iteration 18/25 | Loss: 0.00062601
Iteration 19/25 | Loss: 0.00062601
Iteration 20/25 | Loss: 0.00062601
Iteration 21/25 | Loss: 0.00062601
Iteration 22/25 | Loss: 0.00062601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006260086083784699, 0.0006260086083784699, 0.0006260086083784699, 0.0006260086083784699, 0.0006260086083784699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006260086083784699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062601
Iteration 2/1000 | Loss: 0.00003986
Iteration 3/1000 | Loss: 0.00003247
Iteration 4/1000 | Loss: 0.00003009
Iteration 5/1000 | Loss: 0.00002873
Iteration 6/1000 | Loss: 0.00002802
Iteration 7/1000 | Loss: 0.00002752
Iteration 8/1000 | Loss: 0.00002718
Iteration 9/1000 | Loss: 0.00002691
Iteration 10/1000 | Loss: 0.00002681
Iteration 11/1000 | Loss: 0.00002681
Iteration 12/1000 | Loss: 0.00002681
Iteration 13/1000 | Loss: 0.00002681
Iteration 14/1000 | Loss: 0.00002681
Iteration 15/1000 | Loss: 0.00002665
Iteration 16/1000 | Loss: 0.00002652
Iteration 17/1000 | Loss: 0.00002648
Iteration 18/1000 | Loss: 0.00002648
Iteration 19/1000 | Loss: 0.00002648
Iteration 20/1000 | Loss: 0.00002647
Iteration 21/1000 | Loss: 0.00002647
Iteration 22/1000 | Loss: 0.00002646
Iteration 23/1000 | Loss: 0.00002645
Iteration 24/1000 | Loss: 0.00002643
Iteration 25/1000 | Loss: 0.00002643
Iteration 26/1000 | Loss: 0.00002643
Iteration 27/1000 | Loss: 0.00002643
Iteration 28/1000 | Loss: 0.00002642
Iteration 29/1000 | Loss: 0.00002642
Iteration 30/1000 | Loss: 0.00002642
Iteration 31/1000 | Loss: 0.00002642
Iteration 32/1000 | Loss: 0.00002642
Iteration 33/1000 | Loss: 0.00002642
Iteration 34/1000 | Loss: 0.00002642
Iteration 35/1000 | Loss: 0.00002641
Iteration 36/1000 | Loss: 0.00002641
Iteration 37/1000 | Loss: 0.00002641
Iteration 38/1000 | Loss: 0.00002641
Iteration 39/1000 | Loss: 0.00002640
Iteration 40/1000 | Loss: 0.00002640
Iteration 41/1000 | Loss: 0.00002640
Iteration 42/1000 | Loss: 0.00002640
Iteration 43/1000 | Loss: 0.00002640
Iteration 44/1000 | Loss: 0.00002639
Iteration 45/1000 | Loss: 0.00002638
Iteration 46/1000 | Loss: 0.00002638
Iteration 47/1000 | Loss: 0.00002637
Iteration 48/1000 | Loss: 0.00002636
Iteration 49/1000 | Loss: 0.00002636
Iteration 50/1000 | Loss: 0.00002636
Iteration 51/1000 | Loss: 0.00002636
Iteration 52/1000 | Loss: 0.00002636
Iteration 53/1000 | Loss: 0.00002636
Iteration 54/1000 | Loss: 0.00002636
Iteration 55/1000 | Loss: 0.00002636
Iteration 56/1000 | Loss: 0.00002636
Iteration 57/1000 | Loss: 0.00002636
Iteration 58/1000 | Loss: 0.00002636
Iteration 59/1000 | Loss: 0.00002636
Iteration 60/1000 | Loss: 0.00002636
Iteration 61/1000 | Loss: 0.00002636
Iteration 62/1000 | Loss: 0.00002636
Iteration 63/1000 | Loss: 0.00002636
Iteration 64/1000 | Loss: 0.00002636
Iteration 65/1000 | Loss: 0.00002636
Iteration 66/1000 | Loss: 0.00002636
Iteration 67/1000 | Loss: 0.00002636
Iteration 68/1000 | Loss: 0.00002636
Iteration 69/1000 | Loss: 0.00002636
Iteration 70/1000 | Loss: 0.00002636
Iteration 71/1000 | Loss: 0.00002636
Iteration 72/1000 | Loss: 0.00002636
Iteration 73/1000 | Loss: 0.00002636
Iteration 74/1000 | Loss: 0.00002636
Iteration 75/1000 | Loss: 0.00002636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.6358904506196268e-05, 2.6358904506196268e-05, 2.6358904506196268e-05, 2.6358904506196268e-05, 2.6358904506196268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6358904506196268e-05

Optimization complete. Final v2v error: 4.331559658050537 mm

Highest mean error: 4.402939796447754 mm for frame 2

Lowest mean error: 4.240893363952637 mm for frame 111

Saving results

Total time: 28.36202359199524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872448
Iteration 2/25 | Loss: 0.00130056
Iteration 3/25 | Loss: 0.00121845
Iteration 4/25 | Loss: 0.00120876
Iteration 5/25 | Loss: 0.00120594
Iteration 6/25 | Loss: 0.00120594
Iteration 7/25 | Loss: 0.00120594
Iteration 8/25 | Loss: 0.00120594
Iteration 9/25 | Loss: 0.00120594
Iteration 10/25 | Loss: 0.00120594
Iteration 11/25 | Loss: 0.00120594
Iteration 12/25 | Loss: 0.00120594
Iteration 13/25 | Loss: 0.00120594
Iteration 14/25 | Loss: 0.00120594
Iteration 15/25 | Loss: 0.00120594
Iteration 16/25 | Loss: 0.00120594
Iteration 17/25 | Loss: 0.00120594
Iteration 18/25 | Loss: 0.00120594
Iteration 19/25 | Loss: 0.00120594
Iteration 20/25 | Loss: 0.00120594
Iteration 21/25 | Loss: 0.00120594
Iteration 22/25 | Loss: 0.00120594
Iteration 23/25 | Loss: 0.00120594
Iteration 24/25 | Loss: 0.00120594
Iteration 25/25 | Loss: 0.00120594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77263975
Iteration 2/25 | Loss: 0.00106223
Iteration 3/25 | Loss: 0.00106223
Iteration 4/25 | Loss: 0.00106223
Iteration 5/25 | Loss: 0.00106223
Iteration 6/25 | Loss: 0.00106223
Iteration 7/25 | Loss: 0.00106223
Iteration 8/25 | Loss: 0.00106222
Iteration 9/25 | Loss: 0.00106222
Iteration 10/25 | Loss: 0.00106222
Iteration 11/25 | Loss: 0.00106222
Iteration 12/25 | Loss: 0.00106222
Iteration 13/25 | Loss: 0.00106222
Iteration 14/25 | Loss: 0.00106222
Iteration 15/25 | Loss: 0.00106222
Iteration 16/25 | Loss: 0.00106222
Iteration 17/25 | Loss: 0.00106222
Iteration 18/25 | Loss: 0.00106222
Iteration 19/25 | Loss: 0.00106222
Iteration 20/25 | Loss: 0.00106222
Iteration 21/25 | Loss: 0.00106222
Iteration 22/25 | Loss: 0.00106222
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010622239205986261, 0.0010622239205986261, 0.0010622239205986261, 0.0010622239205986261, 0.0010622239205986261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010622239205986261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106222
Iteration 2/1000 | Loss: 0.00002635
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001378
Iteration 5/1000 | Loss: 0.00001281
Iteration 6/1000 | Loss: 0.00001215
Iteration 7/1000 | Loss: 0.00001161
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001146
Iteration 11/1000 | Loss: 0.00001118
Iteration 12/1000 | Loss: 0.00001095
Iteration 13/1000 | Loss: 0.00001088
Iteration 14/1000 | Loss: 0.00001083
Iteration 15/1000 | Loss: 0.00001082
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001080
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001074
Iteration 21/1000 | Loss: 0.00001073
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001063
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001058
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001054
Iteration 33/1000 | Loss: 0.00001054
Iteration 34/1000 | Loss: 0.00001053
Iteration 35/1000 | Loss: 0.00001052
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001049
Iteration 38/1000 | Loss: 0.00001047
Iteration 39/1000 | Loss: 0.00001045
Iteration 40/1000 | Loss: 0.00001045
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001038
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001030
Iteration 47/1000 | Loss: 0.00001026
Iteration 48/1000 | Loss: 0.00001026
Iteration 49/1000 | Loss: 0.00001023
Iteration 50/1000 | Loss: 0.00001023
Iteration 51/1000 | Loss: 0.00001023
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001017
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001016
Iteration 63/1000 | Loss: 0.00001016
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001015
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001013
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001009
Iteration 82/1000 | Loss: 0.00001009
Iteration 83/1000 | Loss: 0.00001009
Iteration 84/1000 | Loss: 0.00001008
Iteration 85/1000 | Loss: 0.00001008
Iteration 86/1000 | Loss: 0.00001008
Iteration 87/1000 | Loss: 0.00001007
Iteration 88/1000 | Loss: 0.00001007
Iteration 89/1000 | Loss: 0.00001007
Iteration 90/1000 | Loss: 0.00001006
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001006
Iteration 93/1000 | Loss: 0.00001006
Iteration 94/1000 | Loss: 0.00001006
Iteration 95/1000 | Loss: 0.00001005
Iteration 96/1000 | Loss: 0.00001005
Iteration 97/1000 | Loss: 0.00001005
Iteration 98/1000 | Loss: 0.00001005
Iteration 99/1000 | Loss: 0.00001005
Iteration 100/1000 | Loss: 0.00001004
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001004
Iteration 103/1000 | Loss: 0.00001004
Iteration 104/1000 | Loss: 0.00001004
Iteration 105/1000 | Loss: 0.00001004
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00000999
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000998
Iteration 117/1000 | Loss: 0.00000998
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000997
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000995
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000994
Iteration 143/1000 | Loss: 0.00000994
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000993
Iteration 147/1000 | Loss: 0.00000993
Iteration 148/1000 | Loss: 0.00000993
Iteration 149/1000 | Loss: 0.00000993
Iteration 150/1000 | Loss: 0.00000993
Iteration 151/1000 | Loss: 0.00000993
Iteration 152/1000 | Loss: 0.00000993
Iteration 153/1000 | Loss: 0.00000993
Iteration 154/1000 | Loss: 0.00000993
Iteration 155/1000 | Loss: 0.00000993
Iteration 156/1000 | Loss: 0.00000993
Iteration 157/1000 | Loss: 0.00000992
Iteration 158/1000 | Loss: 0.00000992
Iteration 159/1000 | Loss: 0.00000992
Iteration 160/1000 | Loss: 0.00000992
Iteration 161/1000 | Loss: 0.00000992
Iteration 162/1000 | Loss: 0.00000992
Iteration 163/1000 | Loss: 0.00000992
Iteration 164/1000 | Loss: 0.00000992
Iteration 165/1000 | Loss: 0.00000992
Iteration 166/1000 | Loss: 0.00000992
Iteration 167/1000 | Loss: 0.00000992
Iteration 168/1000 | Loss: 0.00000992
Iteration 169/1000 | Loss: 0.00000992
Iteration 170/1000 | Loss: 0.00000992
Iteration 171/1000 | Loss: 0.00000992
Iteration 172/1000 | Loss: 0.00000992
Iteration 173/1000 | Loss: 0.00000991
Iteration 174/1000 | Loss: 0.00000991
Iteration 175/1000 | Loss: 0.00000991
Iteration 176/1000 | Loss: 0.00000991
Iteration 177/1000 | Loss: 0.00000991
Iteration 178/1000 | Loss: 0.00000991
Iteration 179/1000 | Loss: 0.00000991
Iteration 180/1000 | Loss: 0.00000991
Iteration 181/1000 | Loss: 0.00000991
Iteration 182/1000 | Loss: 0.00000991
Iteration 183/1000 | Loss: 0.00000991
Iteration 184/1000 | Loss: 0.00000990
Iteration 185/1000 | Loss: 0.00000990
Iteration 186/1000 | Loss: 0.00000990
Iteration 187/1000 | Loss: 0.00000990
Iteration 188/1000 | Loss: 0.00000990
Iteration 189/1000 | Loss: 0.00000990
Iteration 190/1000 | Loss: 0.00000990
Iteration 191/1000 | Loss: 0.00000990
Iteration 192/1000 | Loss: 0.00000990
Iteration 193/1000 | Loss: 0.00000990
Iteration 194/1000 | Loss: 0.00000990
Iteration 195/1000 | Loss: 0.00000990
Iteration 196/1000 | Loss: 0.00000990
Iteration 197/1000 | Loss: 0.00000990
Iteration 198/1000 | Loss: 0.00000990
Iteration 199/1000 | Loss: 0.00000990
Iteration 200/1000 | Loss: 0.00000990
Iteration 201/1000 | Loss: 0.00000989
Iteration 202/1000 | Loss: 0.00000989
Iteration 203/1000 | Loss: 0.00000989
Iteration 204/1000 | Loss: 0.00000989
Iteration 205/1000 | Loss: 0.00000989
Iteration 206/1000 | Loss: 0.00000989
Iteration 207/1000 | Loss: 0.00000989
Iteration 208/1000 | Loss: 0.00000988
Iteration 209/1000 | Loss: 0.00000988
Iteration 210/1000 | Loss: 0.00000988
Iteration 211/1000 | Loss: 0.00000988
Iteration 212/1000 | Loss: 0.00000988
Iteration 213/1000 | Loss: 0.00000988
Iteration 214/1000 | Loss: 0.00000988
Iteration 215/1000 | Loss: 0.00000988
Iteration 216/1000 | Loss: 0.00000988
Iteration 217/1000 | Loss: 0.00000988
Iteration 218/1000 | Loss: 0.00000988
Iteration 219/1000 | Loss: 0.00000988
Iteration 220/1000 | Loss: 0.00000988
Iteration 221/1000 | Loss: 0.00000988
Iteration 222/1000 | Loss: 0.00000988
Iteration 223/1000 | Loss: 0.00000988
Iteration 224/1000 | Loss: 0.00000988
Iteration 225/1000 | Loss: 0.00000988
Iteration 226/1000 | Loss: 0.00000988
Iteration 227/1000 | Loss: 0.00000988
Iteration 228/1000 | Loss: 0.00000988
Iteration 229/1000 | Loss: 0.00000988
Iteration 230/1000 | Loss: 0.00000988
Iteration 231/1000 | Loss: 0.00000988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [9.879378922050819e-06, 9.879378922050819e-06, 9.879378922050819e-06, 9.879378922050819e-06, 9.879378922050819e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.879378922050819e-06

Optimization complete. Final v2v error: 2.68508243560791 mm

Highest mean error: 3.003657102584839 mm for frame 228

Lowest mean error: 2.4855520725250244 mm for frame 76

Saving results

Total time: 48.936370849609375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742870
Iteration 2/25 | Loss: 0.00142310
Iteration 3/25 | Loss: 0.00132743
Iteration 4/25 | Loss: 0.00131942
Iteration 5/25 | Loss: 0.00131691
Iteration 6/25 | Loss: 0.00131678
Iteration 7/25 | Loss: 0.00131678
Iteration 8/25 | Loss: 0.00131678
Iteration 9/25 | Loss: 0.00131678
Iteration 10/25 | Loss: 0.00131678
Iteration 11/25 | Loss: 0.00131678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013167819706723094, 0.0013167819706723094, 0.0013167819706723094, 0.0013167819706723094, 0.0013167819706723094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013167819706723094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41243267
Iteration 2/25 | Loss: 0.00103540
Iteration 3/25 | Loss: 0.00103539
Iteration 4/25 | Loss: 0.00103539
Iteration 5/25 | Loss: 0.00103539
Iteration 6/25 | Loss: 0.00103539
Iteration 7/25 | Loss: 0.00103539
Iteration 8/25 | Loss: 0.00103539
Iteration 9/25 | Loss: 0.00103539
Iteration 10/25 | Loss: 0.00103539
Iteration 11/25 | Loss: 0.00103539
Iteration 12/25 | Loss: 0.00103539
Iteration 13/25 | Loss: 0.00103539
Iteration 14/25 | Loss: 0.00103539
Iteration 15/25 | Loss: 0.00103539
Iteration 16/25 | Loss: 0.00103539
Iteration 17/25 | Loss: 0.00103539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001035388675518334, 0.001035388675518334, 0.001035388675518334, 0.001035388675518334, 0.001035388675518334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001035388675518334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103539
Iteration 2/1000 | Loss: 0.00003662
Iteration 3/1000 | Loss: 0.00002455
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002068
Iteration 6/1000 | Loss: 0.00001979
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001876
Iteration 10/1000 | Loss: 0.00001876
Iteration 11/1000 | Loss: 0.00001863
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001811
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001806
Iteration 24/1000 | Loss: 0.00001806
Iteration 25/1000 | Loss: 0.00001806
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001805
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001800
Iteration 33/1000 | Loss: 0.00001800
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001797
Iteration 36/1000 | Loss: 0.00001797
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001796
Iteration 39/1000 | Loss: 0.00001792
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001791
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001790
Iteration 45/1000 | Loss: 0.00001790
Iteration 46/1000 | Loss: 0.00001790
Iteration 47/1000 | Loss: 0.00001789
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001785
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001783
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001772
Iteration 92/1000 | Loss: 0.00001772
Iteration 93/1000 | Loss: 0.00001772
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001771
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001769
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001768
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.7678996300674044e-05, 1.7678996300674044e-05, 1.7678996300674044e-05, 1.7678996300674044e-05, 1.7678996300674044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7678996300674044e-05

Optimization complete. Final v2v error: 3.5125954151153564 mm

Highest mean error: 3.7111408710479736 mm for frame 137

Lowest mean error: 3.2376277446746826 mm for frame 160

Saving results

Total time: 35.661110401153564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962685
Iteration 2/25 | Loss: 0.00962684
Iteration 3/25 | Loss: 0.00259235
Iteration 4/25 | Loss: 0.00207566
Iteration 5/25 | Loss: 0.00205293
Iteration 6/25 | Loss: 0.00203903
Iteration 7/25 | Loss: 0.00179543
Iteration 8/25 | Loss: 0.00163675
Iteration 9/25 | Loss: 0.00154735
Iteration 10/25 | Loss: 0.00150573
Iteration 11/25 | Loss: 0.00148705
Iteration 12/25 | Loss: 0.00145527
Iteration 13/25 | Loss: 0.00142153
Iteration 14/25 | Loss: 0.00141507
Iteration 15/25 | Loss: 0.00140692
Iteration 16/25 | Loss: 0.00138929
Iteration 17/25 | Loss: 0.00138829
Iteration 18/25 | Loss: 0.00137583
Iteration 19/25 | Loss: 0.00137060
Iteration 20/25 | Loss: 0.00136927
Iteration 21/25 | Loss: 0.00136884
Iteration 22/25 | Loss: 0.00136856
Iteration 23/25 | Loss: 0.00136830
Iteration 24/25 | Loss: 0.00137090
Iteration 25/25 | Loss: 0.00137998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32727683
Iteration 2/25 | Loss: 0.00226637
Iteration 3/25 | Loss: 0.00206492
Iteration 4/25 | Loss: 0.00206487
Iteration 5/25 | Loss: 0.00206487
Iteration 6/25 | Loss: 0.00206487
Iteration 7/25 | Loss: 0.00206487
Iteration 8/25 | Loss: 0.00206487
Iteration 9/25 | Loss: 0.00206487
Iteration 10/25 | Loss: 0.00206487
Iteration 11/25 | Loss: 0.00206487
Iteration 12/25 | Loss: 0.00206487
Iteration 13/25 | Loss: 0.00206487
Iteration 14/25 | Loss: 0.00206487
Iteration 15/25 | Loss: 0.00206487
Iteration 16/25 | Loss: 0.00206487
Iteration 17/25 | Loss: 0.00206487
Iteration 18/25 | Loss: 0.00206487
Iteration 19/25 | Loss: 0.00206487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020648667123168707, 0.0020648667123168707, 0.0020648667123168707, 0.0020648667123168707, 0.0020648667123168707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020648667123168707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206487
Iteration 2/1000 | Loss: 0.00134555
Iteration 3/1000 | Loss: 0.00015881
Iteration 4/1000 | Loss: 0.00010009
Iteration 5/1000 | Loss: 0.00023372
Iteration 6/1000 | Loss: 0.00012706
Iteration 7/1000 | Loss: 0.00015943
Iteration 8/1000 | Loss: 0.00005066
Iteration 9/1000 | Loss: 0.00004099
Iteration 10/1000 | Loss: 0.00012632
Iteration 11/1000 | Loss: 0.00029085
Iteration 12/1000 | Loss: 0.00020182
Iteration 13/1000 | Loss: 0.00005616
Iteration 14/1000 | Loss: 0.00004246
Iteration 15/1000 | Loss: 0.00003628
Iteration 16/1000 | Loss: 0.00003259
Iteration 17/1000 | Loss: 0.00002856
Iteration 18/1000 | Loss: 0.00002662
Iteration 19/1000 | Loss: 0.00002496
Iteration 20/1000 | Loss: 0.00002359
Iteration 21/1000 | Loss: 0.00002249
Iteration 22/1000 | Loss: 0.00002146
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00002430
Iteration 26/1000 | Loss: 0.00002636
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00002062
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00001818
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001683
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001670
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001661
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001660
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001658
Iteration 65/1000 | Loss: 0.00001658
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001658
Iteration 68/1000 | Loss: 0.00001658
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001657
Iteration 78/1000 | Loss: 0.00001657
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001657
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001656
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001655
Iteration 102/1000 | Loss: 0.00001655
Iteration 103/1000 | Loss: 0.00001655
Iteration 104/1000 | Loss: 0.00001655
Iteration 105/1000 | Loss: 0.00001655
Iteration 106/1000 | Loss: 0.00001655
Iteration 107/1000 | Loss: 0.00001654
Iteration 108/1000 | Loss: 0.00001654
Iteration 109/1000 | Loss: 0.00001654
Iteration 110/1000 | Loss: 0.00001654
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001654
Iteration 114/1000 | Loss: 0.00001654
Iteration 115/1000 | Loss: 0.00001654
Iteration 116/1000 | Loss: 0.00001654
Iteration 117/1000 | Loss: 0.00001654
Iteration 118/1000 | Loss: 0.00001654
Iteration 119/1000 | Loss: 0.00001654
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00001653
Iteration 123/1000 | Loss: 0.00001653
Iteration 124/1000 | Loss: 0.00001653
Iteration 125/1000 | Loss: 0.00001653
Iteration 126/1000 | Loss: 0.00001653
Iteration 127/1000 | Loss: 0.00001653
Iteration 128/1000 | Loss: 0.00001653
Iteration 129/1000 | Loss: 0.00001653
Iteration 130/1000 | Loss: 0.00001653
Iteration 131/1000 | Loss: 0.00001653
Iteration 132/1000 | Loss: 0.00001653
Iteration 133/1000 | Loss: 0.00001653
Iteration 134/1000 | Loss: 0.00001653
Iteration 135/1000 | Loss: 0.00001653
Iteration 136/1000 | Loss: 0.00001653
Iteration 137/1000 | Loss: 0.00001653
Iteration 138/1000 | Loss: 0.00001653
Iteration 139/1000 | Loss: 0.00001653
Iteration 140/1000 | Loss: 0.00001652
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001652
Iteration 143/1000 | Loss: 0.00001652
Iteration 144/1000 | Loss: 0.00001652
Iteration 145/1000 | Loss: 0.00001652
Iteration 146/1000 | Loss: 0.00001652
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001652
Iteration 150/1000 | Loss: 0.00001652
Iteration 151/1000 | Loss: 0.00001652
Iteration 152/1000 | Loss: 0.00001652
Iteration 153/1000 | Loss: 0.00001652
Iteration 154/1000 | Loss: 0.00001652
Iteration 155/1000 | Loss: 0.00001652
Iteration 156/1000 | Loss: 0.00001651
Iteration 157/1000 | Loss: 0.00001651
Iteration 158/1000 | Loss: 0.00001651
Iteration 159/1000 | Loss: 0.00001651
Iteration 160/1000 | Loss: 0.00001651
Iteration 161/1000 | Loss: 0.00001651
Iteration 162/1000 | Loss: 0.00001651
Iteration 163/1000 | Loss: 0.00001651
Iteration 164/1000 | Loss: 0.00001651
Iteration 165/1000 | Loss: 0.00001651
Iteration 166/1000 | Loss: 0.00001651
Iteration 167/1000 | Loss: 0.00001651
Iteration 168/1000 | Loss: 0.00001651
Iteration 169/1000 | Loss: 0.00001651
Iteration 170/1000 | Loss: 0.00001651
Iteration 171/1000 | Loss: 0.00001650
Iteration 172/1000 | Loss: 0.00001650
Iteration 173/1000 | Loss: 0.00001650
Iteration 174/1000 | Loss: 0.00001650
Iteration 175/1000 | Loss: 0.00001650
Iteration 176/1000 | Loss: 0.00001650
Iteration 177/1000 | Loss: 0.00001650
Iteration 178/1000 | Loss: 0.00001650
Iteration 179/1000 | Loss: 0.00001650
Iteration 180/1000 | Loss: 0.00001649
Iteration 181/1000 | Loss: 0.00001649
Iteration 182/1000 | Loss: 0.00001649
Iteration 183/1000 | Loss: 0.00001649
Iteration 184/1000 | Loss: 0.00001649
Iteration 185/1000 | Loss: 0.00001649
Iteration 186/1000 | Loss: 0.00001649
Iteration 187/1000 | Loss: 0.00001649
Iteration 188/1000 | Loss: 0.00001649
Iteration 189/1000 | Loss: 0.00001649
Iteration 190/1000 | Loss: 0.00001649
Iteration 191/1000 | Loss: 0.00001649
Iteration 192/1000 | Loss: 0.00001649
Iteration 193/1000 | Loss: 0.00001648
Iteration 194/1000 | Loss: 0.00001648
Iteration 195/1000 | Loss: 0.00001648
Iteration 196/1000 | Loss: 0.00001648
Iteration 197/1000 | Loss: 0.00001648
Iteration 198/1000 | Loss: 0.00001648
Iteration 199/1000 | Loss: 0.00001648
Iteration 200/1000 | Loss: 0.00001648
Iteration 201/1000 | Loss: 0.00001647
Iteration 202/1000 | Loss: 0.00001647
Iteration 203/1000 | Loss: 0.00001647
Iteration 204/1000 | Loss: 0.00001647
Iteration 205/1000 | Loss: 0.00001647
Iteration 206/1000 | Loss: 0.00001647
Iteration 207/1000 | Loss: 0.00001647
Iteration 208/1000 | Loss: 0.00001647
Iteration 209/1000 | Loss: 0.00001647
Iteration 210/1000 | Loss: 0.00001647
Iteration 211/1000 | Loss: 0.00001647
Iteration 212/1000 | Loss: 0.00001646
Iteration 213/1000 | Loss: 0.00001646
Iteration 214/1000 | Loss: 0.00001646
Iteration 215/1000 | Loss: 0.00001646
Iteration 216/1000 | Loss: 0.00001646
Iteration 217/1000 | Loss: 0.00001646
Iteration 218/1000 | Loss: 0.00001646
Iteration 219/1000 | Loss: 0.00001646
Iteration 220/1000 | Loss: 0.00001646
Iteration 221/1000 | Loss: 0.00001646
Iteration 222/1000 | Loss: 0.00001646
Iteration 223/1000 | Loss: 0.00001646
Iteration 224/1000 | Loss: 0.00001646
Iteration 225/1000 | Loss: 0.00001645
Iteration 226/1000 | Loss: 0.00001645
Iteration 227/1000 | Loss: 0.00001644
Iteration 228/1000 | Loss: 0.00001644
Iteration 229/1000 | Loss: 0.00001644
Iteration 230/1000 | Loss: 0.00001644
Iteration 231/1000 | Loss: 0.00001644
Iteration 232/1000 | Loss: 0.00001644
Iteration 233/1000 | Loss: 0.00001644
Iteration 234/1000 | Loss: 0.00001644
Iteration 235/1000 | Loss: 0.00001644
Iteration 236/1000 | Loss: 0.00001644
Iteration 237/1000 | Loss: 0.00001644
Iteration 238/1000 | Loss: 0.00001644
Iteration 239/1000 | Loss: 0.00001644
Iteration 240/1000 | Loss: 0.00001644
Iteration 241/1000 | Loss: 0.00001644
Iteration 242/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.643910763959866e-05, 1.643910763959866e-05, 1.643910763959866e-05, 1.643910763959866e-05, 1.643910763959866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.643910763959866e-05

Optimization complete. Final v2v error: 3.422490119934082 mm

Highest mean error: 4.59785795211792 mm for frame 16

Lowest mean error: 2.947298288345337 mm for frame 214

Saving results

Total time: 118.52010726928711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780870
Iteration 2/25 | Loss: 0.00189369
Iteration 3/25 | Loss: 0.00136644
Iteration 4/25 | Loss: 0.00131624
Iteration 5/25 | Loss: 0.00131047
Iteration 6/25 | Loss: 0.00130921
Iteration 7/25 | Loss: 0.00130921
Iteration 8/25 | Loss: 0.00130921
Iteration 9/25 | Loss: 0.00130921
Iteration 10/25 | Loss: 0.00130921
Iteration 11/25 | Loss: 0.00130921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013092050794512033, 0.0013092050794512033, 0.0013092050794512033, 0.0013092050794512033, 0.0013092050794512033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013092050794512033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35661006
Iteration 2/25 | Loss: 0.00097376
Iteration 3/25 | Loss: 0.00097374
Iteration 4/25 | Loss: 0.00097374
Iteration 5/25 | Loss: 0.00097374
Iteration 6/25 | Loss: 0.00097374
Iteration 7/25 | Loss: 0.00097374
Iteration 8/25 | Loss: 0.00097374
Iteration 9/25 | Loss: 0.00097374
Iteration 10/25 | Loss: 0.00097374
Iteration 11/25 | Loss: 0.00097374
Iteration 12/25 | Loss: 0.00097374
Iteration 13/25 | Loss: 0.00097374
Iteration 14/25 | Loss: 0.00097374
Iteration 15/25 | Loss: 0.00097374
Iteration 16/25 | Loss: 0.00097374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009737414657138288, 0.0009737414657138288, 0.0009737414657138288, 0.0009737414657138288, 0.0009737414657138288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009737414657138288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097374
Iteration 2/1000 | Loss: 0.00004125
Iteration 3/1000 | Loss: 0.00002973
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002188
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002128
Iteration 9/1000 | Loss: 0.00002102
Iteration 10/1000 | Loss: 0.00002081
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002059
Iteration 13/1000 | Loss: 0.00002057
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002048
Iteration 20/1000 | Loss: 0.00002046
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00002043
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002038
Iteration 32/1000 | Loss: 0.00002037
Iteration 33/1000 | Loss: 0.00002037
Iteration 34/1000 | Loss: 0.00002037
Iteration 35/1000 | Loss: 0.00002037
Iteration 36/1000 | Loss: 0.00002037
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002037
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002036
Iteration 45/1000 | Loss: 0.00002036
Iteration 46/1000 | Loss: 0.00002036
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00002036
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002033
Iteration 54/1000 | Loss: 0.00002033
Iteration 55/1000 | Loss: 0.00002033
Iteration 56/1000 | Loss: 0.00002032
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002031
Iteration 59/1000 | Loss: 0.00002031
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002031
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002029
Iteration 67/1000 | Loss: 0.00002029
Iteration 68/1000 | Loss: 0.00002029
Iteration 69/1000 | Loss: 0.00002029
Iteration 70/1000 | Loss: 0.00002029
Iteration 71/1000 | Loss: 0.00002028
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002028
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002026
Iteration 79/1000 | Loss: 0.00002026
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002024
Iteration 92/1000 | Loss: 0.00002024
Iteration 93/1000 | Loss: 0.00002024
Iteration 94/1000 | Loss: 0.00002024
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002024
Iteration 100/1000 | Loss: 0.00002024
Iteration 101/1000 | Loss: 0.00002024
Iteration 102/1000 | Loss: 0.00002024
Iteration 103/1000 | Loss: 0.00002024
Iteration 104/1000 | Loss: 0.00002024
Iteration 105/1000 | Loss: 0.00002024
Iteration 106/1000 | Loss: 0.00002024
Iteration 107/1000 | Loss: 0.00002024
Iteration 108/1000 | Loss: 0.00002024
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002024
Iteration 111/1000 | Loss: 0.00002024
Iteration 112/1000 | Loss: 0.00002024
Iteration 113/1000 | Loss: 0.00002024
Iteration 114/1000 | Loss: 0.00002024
Iteration 115/1000 | Loss: 0.00002024
Iteration 116/1000 | Loss: 0.00002024
Iteration 117/1000 | Loss: 0.00002024
Iteration 118/1000 | Loss: 0.00002024
Iteration 119/1000 | Loss: 0.00002024
Iteration 120/1000 | Loss: 0.00002024
Iteration 121/1000 | Loss: 0.00002024
Iteration 122/1000 | Loss: 0.00002024
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002024
Iteration 125/1000 | Loss: 0.00002024
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002024
Iteration 130/1000 | Loss: 0.00002024
Iteration 131/1000 | Loss: 0.00002024
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002024
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00002024
Iteration 136/1000 | Loss: 0.00002024
Iteration 137/1000 | Loss: 0.00002024
Iteration 138/1000 | Loss: 0.00002024
Iteration 139/1000 | Loss: 0.00002024
Iteration 140/1000 | Loss: 0.00002024
Iteration 141/1000 | Loss: 0.00002024
Iteration 142/1000 | Loss: 0.00002024
Iteration 143/1000 | Loss: 0.00002024
Iteration 144/1000 | Loss: 0.00002024
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.024095920205582e-05, 2.024095920205582e-05, 2.024095920205582e-05, 2.024095920205582e-05, 2.024095920205582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.024095920205582e-05

Optimization complete. Final v2v error: 3.795198917388916 mm

Highest mean error: 4.332597732543945 mm for frame 135

Lowest mean error: 3.5337882041931152 mm for frame 109

Saving results

Total time: 33.758376598358154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793146
Iteration 2/25 | Loss: 0.00200758
Iteration 3/25 | Loss: 0.00137769
Iteration 4/25 | Loss: 0.00132038
Iteration 5/25 | Loss: 0.00131330
Iteration 6/25 | Loss: 0.00131243
Iteration 7/25 | Loss: 0.00131243
Iteration 8/25 | Loss: 0.00131243
Iteration 9/25 | Loss: 0.00131243
Iteration 10/25 | Loss: 0.00131243
Iteration 11/25 | Loss: 0.00131243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001312430715188384, 0.001312430715188384, 0.001312430715188384, 0.001312430715188384, 0.001312430715188384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001312430715188384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28076553
Iteration 2/25 | Loss: 0.00089789
Iteration 3/25 | Loss: 0.00089789
Iteration 4/25 | Loss: 0.00089789
Iteration 5/25 | Loss: 0.00089789
Iteration 6/25 | Loss: 0.00089789
Iteration 7/25 | Loss: 0.00089789
Iteration 8/25 | Loss: 0.00089789
Iteration 9/25 | Loss: 0.00089789
Iteration 10/25 | Loss: 0.00089789
Iteration 11/25 | Loss: 0.00089789
Iteration 12/25 | Loss: 0.00089789
Iteration 13/25 | Loss: 0.00089789
Iteration 14/25 | Loss: 0.00089789
Iteration 15/25 | Loss: 0.00089789
Iteration 16/25 | Loss: 0.00089789
Iteration 17/25 | Loss: 0.00089789
Iteration 18/25 | Loss: 0.00089789
Iteration 19/25 | Loss: 0.00089789
Iteration 20/25 | Loss: 0.00089789
Iteration 21/25 | Loss: 0.00089789
Iteration 22/25 | Loss: 0.00089789
Iteration 23/25 | Loss: 0.00089789
Iteration 24/25 | Loss: 0.00089789
Iteration 25/25 | Loss: 0.00089789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000897885300219059, 0.000897885300219059, 0.000897885300219059, 0.000897885300219059, 0.000897885300219059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000897885300219059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089789
Iteration 2/1000 | Loss: 0.00004488
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002614
Iteration 5/1000 | Loss: 0.00002456
Iteration 6/1000 | Loss: 0.00002357
Iteration 7/1000 | Loss: 0.00002297
Iteration 8/1000 | Loss: 0.00002254
Iteration 9/1000 | Loss: 0.00002226
Iteration 10/1000 | Loss: 0.00002193
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002132
Iteration 14/1000 | Loss: 0.00002128
Iteration 15/1000 | Loss: 0.00002125
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002103
Iteration 19/1000 | Loss: 0.00002102
Iteration 20/1000 | Loss: 0.00002099
Iteration 21/1000 | Loss: 0.00002098
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002097
Iteration 24/1000 | Loss: 0.00002093
Iteration 25/1000 | Loss: 0.00002091
Iteration 26/1000 | Loss: 0.00002090
Iteration 27/1000 | Loss: 0.00002089
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00002089
Iteration 30/1000 | Loss: 0.00002088
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002088
Iteration 33/1000 | Loss: 0.00002088
Iteration 34/1000 | Loss: 0.00002088
Iteration 35/1000 | Loss: 0.00002087
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002084
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002083
Iteration 40/1000 | Loss: 0.00002083
Iteration 41/1000 | Loss: 0.00002083
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002082
Iteration 49/1000 | Loss: 0.00002082
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002080
Iteration 52/1000 | Loss: 0.00002080
Iteration 53/1000 | Loss: 0.00002080
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00002079
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002079
Iteration 60/1000 | Loss: 0.00002079
Iteration 61/1000 | Loss: 0.00002076
Iteration 62/1000 | Loss: 0.00002075
Iteration 63/1000 | Loss: 0.00002075
Iteration 64/1000 | Loss: 0.00002075
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002072
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002069
Iteration 83/1000 | Loss: 0.00002069
Iteration 84/1000 | Loss: 0.00002069
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002067
Iteration 94/1000 | Loss: 0.00002067
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00002067
Iteration 97/1000 | Loss: 0.00002067
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002066
Iteration 109/1000 | Loss: 0.00002066
Iteration 110/1000 | Loss: 0.00002066
Iteration 111/1000 | Loss: 0.00002065
Iteration 112/1000 | Loss: 0.00002065
Iteration 113/1000 | Loss: 0.00002065
Iteration 114/1000 | Loss: 0.00002065
Iteration 115/1000 | Loss: 0.00002065
Iteration 116/1000 | Loss: 0.00002065
Iteration 117/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.065439912257716e-05, 2.065439912257716e-05, 2.065439912257716e-05, 2.065439912257716e-05, 2.065439912257716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.065439912257716e-05

Optimization complete. Final v2v error: 3.763139486312866 mm

Highest mean error: 4.657407283782959 mm for frame 111

Lowest mean error: 3.3282346725463867 mm for frame 98

Saving results

Total time: 43.04819846153259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432828
Iteration 2/25 | Loss: 0.00135587
Iteration 3/25 | Loss: 0.00124674
Iteration 4/25 | Loss: 0.00123194
Iteration 5/25 | Loss: 0.00122826
Iteration 6/25 | Loss: 0.00122721
Iteration 7/25 | Loss: 0.00122687
Iteration 8/25 | Loss: 0.00122687
Iteration 9/25 | Loss: 0.00122687
Iteration 10/25 | Loss: 0.00122687
Iteration 11/25 | Loss: 0.00122687
Iteration 12/25 | Loss: 0.00122687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012268689461052418, 0.0012268689461052418, 0.0012268689461052418, 0.0012268689461052418, 0.0012268689461052418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012268689461052418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45698047
Iteration 2/25 | Loss: 0.00104970
Iteration 3/25 | Loss: 0.00104970
Iteration 4/25 | Loss: 0.00104970
Iteration 5/25 | Loss: 0.00104970
Iteration 6/25 | Loss: 0.00104970
Iteration 7/25 | Loss: 0.00104970
Iteration 8/25 | Loss: 0.00104970
Iteration 9/25 | Loss: 0.00104970
Iteration 10/25 | Loss: 0.00104970
Iteration 11/25 | Loss: 0.00104970
Iteration 12/25 | Loss: 0.00104970
Iteration 13/25 | Loss: 0.00104970
Iteration 14/25 | Loss: 0.00104970
Iteration 15/25 | Loss: 0.00104970
Iteration 16/25 | Loss: 0.00104970
Iteration 17/25 | Loss: 0.00104970
Iteration 18/25 | Loss: 0.00104970
Iteration 19/25 | Loss: 0.00104970
Iteration 20/25 | Loss: 0.00104970
Iteration 21/25 | Loss: 0.00104970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010496953036636114, 0.0010496953036636114, 0.0010496953036636114, 0.0010496953036636114, 0.0010496953036636114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010496953036636114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104970
Iteration 2/1000 | Loss: 0.00003913
Iteration 3/1000 | Loss: 0.00002445
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001752
Iteration 7/1000 | Loss: 0.00001689
Iteration 8/1000 | Loss: 0.00001651
Iteration 9/1000 | Loss: 0.00001608
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001535
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001488
Iteration 18/1000 | Loss: 0.00001480
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001468
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001459
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001441
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001439
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001433
Iteration 107/1000 | Loss: 0.00001433
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001430
Iteration 129/1000 | Loss: 0.00001430
Iteration 130/1000 | Loss: 0.00001430
Iteration 131/1000 | Loss: 0.00001430
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001428
Iteration 151/1000 | Loss: 0.00001428
Iteration 152/1000 | Loss: 0.00001428
Iteration 153/1000 | Loss: 0.00001428
Iteration 154/1000 | Loss: 0.00001428
Iteration 155/1000 | Loss: 0.00001428
Iteration 156/1000 | Loss: 0.00001428
Iteration 157/1000 | Loss: 0.00001428
Iteration 158/1000 | Loss: 0.00001428
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001427
Iteration 173/1000 | Loss: 0.00001427
Iteration 174/1000 | Loss: 0.00001427
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001427
Iteration 177/1000 | Loss: 0.00001427
Iteration 178/1000 | Loss: 0.00001427
Iteration 179/1000 | Loss: 0.00001427
Iteration 180/1000 | Loss: 0.00001427
Iteration 181/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.4265678146330174e-05, 1.4265678146330174e-05, 1.4265678146330174e-05, 1.4265678146330174e-05, 1.4265678146330174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4265678146330174e-05

Optimization complete. Final v2v error: 3.193891763687134 mm

Highest mean error: 4.536161422729492 mm for frame 46

Lowest mean error: 2.84271502494812 mm for frame 86

Saving results

Total time: 44.869314193725586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466449
Iteration 2/25 | Loss: 0.00138799
Iteration 3/25 | Loss: 0.00127021
Iteration 4/25 | Loss: 0.00125865
Iteration 5/25 | Loss: 0.00125516
Iteration 6/25 | Loss: 0.00125516
Iteration 7/25 | Loss: 0.00125516
Iteration 8/25 | Loss: 0.00125516
Iteration 9/25 | Loss: 0.00125516
Iteration 10/25 | Loss: 0.00125516
Iteration 11/25 | Loss: 0.00125516
Iteration 12/25 | Loss: 0.00125516
Iteration 13/25 | Loss: 0.00125516
Iteration 14/25 | Loss: 0.00125516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012551621766760945, 0.0012551621766760945, 0.0012551621766760945, 0.0012551621766760945, 0.0012551621766760945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012551621766760945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36789620
Iteration 2/25 | Loss: 0.00096177
Iteration 3/25 | Loss: 0.00096177
Iteration 4/25 | Loss: 0.00096177
Iteration 5/25 | Loss: 0.00096177
Iteration 6/25 | Loss: 0.00096177
Iteration 7/25 | Loss: 0.00096177
Iteration 8/25 | Loss: 0.00096176
Iteration 9/25 | Loss: 0.00096176
Iteration 10/25 | Loss: 0.00096176
Iteration 11/25 | Loss: 0.00096176
Iteration 12/25 | Loss: 0.00096176
Iteration 13/25 | Loss: 0.00096176
Iteration 14/25 | Loss: 0.00096176
Iteration 15/25 | Loss: 0.00096176
Iteration 16/25 | Loss: 0.00096176
Iteration 17/25 | Loss: 0.00096176
Iteration 18/25 | Loss: 0.00096176
Iteration 19/25 | Loss: 0.00096176
Iteration 20/25 | Loss: 0.00096176
Iteration 21/25 | Loss: 0.00096176
Iteration 22/25 | Loss: 0.00096176
Iteration 23/25 | Loss: 0.00096176
Iteration 24/25 | Loss: 0.00096176
Iteration 25/25 | Loss: 0.00096176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096176
Iteration 2/1000 | Loss: 0.00003325
Iteration 3/1000 | Loss: 0.00002451
Iteration 4/1000 | Loss: 0.00002214
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001896
Iteration 9/1000 | Loss: 0.00001858
Iteration 10/1000 | Loss: 0.00001823
Iteration 11/1000 | Loss: 0.00001810
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001765
Iteration 18/1000 | Loss: 0.00001764
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001753
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001744
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001716
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001714
Iteration 99/1000 | Loss: 0.00001714
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001714
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001713
Iteration 113/1000 | Loss: 0.00001713
Iteration 114/1000 | Loss: 0.00001713
Iteration 115/1000 | Loss: 0.00001713
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001713
Iteration 118/1000 | Loss: 0.00001713
Iteration 119/1000 | Loss: 0.00001713
Iteration 120/1000 | Loss: 0.00001713
Iteration 121/1000 | Loss: 0.00001713
Iteration 122/1000 | Loss: 0.00001713
Iteration 123/1000 | Loss: 0.00001713
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.7125512385973707e-05, 1.7125512385973707e-05, 1.7125512385973707e-05, 1.7125512385973707e-05, 1.7125512385973707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7125512385973707e-05

Optimization complete. Final v2v error: 3.49239182472229 mm

Highest mean error: 3.845921754837036 mm for frame 15

Lowest mean error: 3.0447428226470947 mm for frame 189

Saving results

Total time: 43.999762773513794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453270
Iteration 2/25 | Loss: 0.00140160
Iteration 3/25 | Loss: 0.00126683
Iteration 4/25 | Loss: 0.00125269
Iteration 5/25 | Loss: 0.00124732
Iteration 6/25 | Loss: 0.00124682
Iteration 7/25 | Loss: 0.00124671
Iteration 8/25 | Loss: 0.00124671
Iteration 9/25 | Loss: 0.00124671
Iteration 10/25 | Loss: 0.00124671
Iteration 11/25 | Loss: 0.00124671
Iteration 12/25 | Loss: 0.00124671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012467103078961372, 0.0012467103078961372, 0.0012467103078961372, 0.0012467103078961372, 0.0012467103078961372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012467103078961372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35648930
Iteration 2/25 | Loss: 0.00121758
Iteration 3/25 | Loss: 0.00121758
Iteration 4/25 | Loss: 0.00121758
Iteration 5/25 | Loss: 0.00121757
Iteration 6/25 | Loss: 0.00121757
Iteration 7/25 | Loss: 0.00121757
Iteration 8/25 | Loss: 0.00121757
Iteration 9/25 | Loss: 0.00121757
Iteration 10/25 | Loss: 0.00121757
Iteration 11/25 | Loss: 0.00121757
Iteration 12/25 | Loss: 0.00121757
Iteration 13/25 | Loss: 0.00121757
Iteration 14/25 | Loss: 0.00121757
Iteration 15/25 | Loss: 0.00121757
Iteration 16/25 | Loss: 0.00121757
Iteration 17/25 | Loss: 0.00121757
Iteration 18/25 | Loss: 0.00121757
Iteration 19/25 | Loss: 0.00121757
Iteration 20/25 | Loss: 0.00121757
Iteration 21/25 | Loss: 0.00121757
Iteration 22/25 | Loss: 0.00121757
Iteration 23/25 | Loss: 0.00121757
Iteration 24/25 | Loss: 0.00121757
Iteration 25/25 | Loss: 0.00121757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121757
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001457
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001361
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001332
Iteration 64/1000 | Loss: 0.00001332
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001329
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001320
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001318
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001313
Iteration 108/1000 | Loss: 0.00001313
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001313
Iteration 112/1000 | Loss: 0.00001313
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001313
Iteration 115/1000 | Loss: 0.00001313
Iteration 116/1000 | Loss: 0.00001313
Iteration 117/1000 | Loss: 0.00001313
Iteration 118/1000 | Loss: 0.00001313
Iteration 119/1000 | Loss: 0.00001313
Iteration 120/1000 | Loss: 0.00001313
Iteration 121/1000 | Loss: 0.00001313
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001313
Iteration 125/1000 | Loss: 0.00001313
Iteration 126/1000 | Loss: 0.00001313
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.3126108569849748e-05, 1.3126108569849748e-05, 1.3126108569849748e-05, 1.3126108569849748e-05, 1.3126108569849748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3126108569849748e-05

Optimization complete. Final v2v error: 3.0405659675598145 mm

Highest mean error: 3.502081871032715 mm for frame 77

Lowest mean error: 2.7605764865875244 mm for frame 209

Saving results

Total time: 38.67638611793518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399572
Iteration 2/25 | Loss: 0.00131293
Iteration 3/25 | Loss: 0.00122573
Iteration 4/25 | Loss: 0.00121897
Iteration 5/25 | Loss: 0.00121805
Iteration 6/25 | Loss: 0.00121805
Iteration 7/25 | Loss: 0.00121805
Iteration 8/25 | Loss: 0.00121805
Iteration 9/25 | Loss: 0.00121805
Iteration 10/25 | Loss: 0.00121805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012180485064163804, 0.0012180485064163804, 0.0012180485064163804, 0.0012180485064163804, 0.0012180485064163804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012180485064163804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53526068
Iteration 2/25 | Loss: 0.00089586
Iteration 3/25 | Loss: 0.00089586
Iteration 4/25 | Loss: 0.00089586
Iteration 5/25 | Loss: 0.00089586
Iteration 6/25 | Loss: 0.00089586
Iteration 7/25 | Loss: 0.00089586
Iteration 8/25 | Loss: 0.00089586
Iteration 9/25 | Loss: 0.00089586
Iteration 10/25 | Loss: 0.00089586
Iteration 11/25 | Loss: 0.00089586
Iteration 12/25 | Loss: 0.00089586
Iteration 13/25 | Loss: 0.00089586
Iteration 14/25 | Loss: 0.00089586
Iteration 15/25 | Loss: 0.00089586
Iteration 16/25 | Loss: 0.00089586
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008958589169196784, 0.0008958589169196784, 0.0008958589169196784, 0.0008958589169196784, 0.0008958589169196784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008958589169196784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089586
Iteration 2/1000 | Loss: 0.00002236
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001452
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001204
Iteration 9/1000 | Loss: 0.00001165
Iteration 10/1000 | Loss: 0.00001137
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001117
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001115
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001103
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001102
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001102
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001101
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001093
Iteration 45/1000 | Loss: 0.00001092
Iteration 46/1000 | Loss: 0.00001092
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001089
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001087
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001086
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001082
Iteration 66/1000 | Loss: 0.00001082
Iteration 67/1000 | Loss: 0.00001082
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001067
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001066
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001062
Iteration 87/1000 | Loss: 0.00001062
Iteration 88/1000 | Loss: 0.00001062
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001060
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001058
Iteration 105/1000 | Loss: 0.00001058
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001055
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001055
Iteration 120/1000 | Loss: 0.00001055
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001053
Iteration 126/1000 | Loss: 0.00001053
Iteration 127/1000 | Loss: 0.00001053
Iteration 128/1000 | Loss: 0.00001053
Iteration 129/1000 | Loss: 0.00001052
Iteration 130/1000 | Loss: 0.00001052
Iteration 131/1000 | Loss: 0.00001052
Iteration 132/1000 | Loss: 0.00001052
Iteration 133/1000 | Loss: 0.00001051
Iteration 134/1000 | Loss: 0.00001051
Iteration 135/1000 | Loss: 0.00001051
Iteration 136/1000 | Loss: 0.00001050
Iteration 137/1000 | Loss: 0.00001049
Iteration 138/1000 | Loss: 0.00001049
Iteration 139/1000 | Loss: 0.00001049
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001048
Iteration 142/1000 | Loss: 0.00001048
Iteration 143/1000 | Loss: 0.00001048
Iteration 144/1000 | Loss: 0.00001048
Iteration 145/1000 | Loss: 0.00001048
Iteration 146/1000 | Loss: 0.00001048
Iteration 147/1000 | Loss: 0.00001047
Iteration 148/1000 | Loss: 0.00001047
Iteration 149/1000 | Loss: 0.00001047
Iteration 150/1000 | Loss: 0.00001047
Iteration 151/1000 | Loss: 0.00001047
Iteration 152/1000 | Loss: 0.00001047
Iteration 153/1000 | Loss: 0.00001047
Iteration 154/1000 | Loss: 0.00001047
Iteration 155/1000 | Loss: 0.00001047
Iteration 156/1000 | Loss: 0.00001047
Iteration 157/1000 | Loss: 0.00001047
Iteration 158/1000 | Loss: 0.00001046
Iteration 159/1000 | Loss: 0.00001046
Iteration 160/1000 | Loss: 0.00001046
Iteration 161/1000 | Loss: 0.00001046
Iteration 162/1000 | Loss: 0.00001046
Iteration 163/1000 | Loss: 0.00001046
Iteration 164/1000 | Loss: 0.00001046
Iteration 165/1000 | Loss: 0.00001046
Iteration 166/1000 | Loss: 0.00001046
Iteration 167/1000 | Loss: 0.00001046
Iteration 168/1000 | Loss: 0.00001046
Iteration 169/1000 | Loss: 0.00001046
Iteration 170/1000 | Loss: 0.00001046
Iteration 171/1000 | Loss: 0.00001045
Iteration 172/1000 | Loss: 0.00001045
Iteration 173/1000 | Loss: 0.00001045
Iteration 174/1000 | Loss: 0.00001045
Iteration 175/1000 | Loss: 0.00001045
Iteration 176/1000 | Loss: 0.00001044
Iteration 177/1000 | Loss: 0.00001044
Iteration 178/1000 | Loss: 0.00001044
Iteration 179/1000 | Loss: 0.00001044
Iteration 180/1000 | Loss: 0.00001044
Iteration 181/1000 | Loss: 0.00001044
Iteration 182/1000 | Loss: 0.00001044
Iteration 183/1000 | Loss: 0.00001044
Iteration 184/1000 | Loss: 0.00001044
Iteration 185/1000 | Loss: 0.00001044
Iteration 186/1000 | Loss: 0.00001044
Iteration 187/1000 | Loss: 0.00001044
Iteration 188/1000 | Loss: 0.00001043
Iteration 189/1000 | Loss: 0.00001043
Iteration 190/1000 | Loss: 0.00001043
Iteration 191/1000 | Loss: 0.00001043
Iteration 192/1000 | Loss: 0.00001043
Iteration 193/1000 | Loss: 0.00001043
Iteration 194/1000 | Loss: 0.00001043
Iteration 195/1000 | Loss: 0.00001043
Iteration 196/1000 | Loss: 0.00001043
Iteration 197/1000 | Loss: 0.00001043
Iteration 198/1000 | Loss: 0.00001043
Iteration 199/1000 | Loss: 0.00001042
Iteration 200/1000 | Loss: 0.00001042
Iteration 201/1000 | Loss: 0.00001042
Iteration 202/1000 | Loss: 0.00001042
Iteration 203/1000 | Loss: 0.00001042
Iteration 204/1000 | Loss: 0.00001042
Iteration 205/1000 | Loss: 0.00001042
Iteration 206/1000 | Loss: 0.00001042
Iteration 207/1000 | Loss: 0.00001042
Iteration 208/1000 | Loss: 0.00001042
Iteration 209/1000 | Loss: 0.00001042
Iteration 210/1000 | Loss: 0.00001042
Iteration 211/1000 | Loss: 0.00001042
Iteration 212/1000 | Loss: 0.00001042
Iteration 213/1000 | Loss: 0.00001042
Iteration 214/1000 | Loss: 0.00001042
Iteration 215/1000 | Loss: 0.00001042
Iteration 216/1000 | Loss: 0.00001042
Iteration 217/1000 | Loss: 0.00001042
Iteration 218/1000 | Loss: 0.00001042
Iteration 219/1000 | Loss: 0.00001042
Iteration 220/1000 | Loss: 0.00001042
Iteration 221/1000 | Loss: 0.00001042
Iteration 222/1000 | Loss: 0.00001042
Iteration 223/1000 | Loss: 0.00001042
Iteration 224/1000 | Loss: 0.00001042
Iteration 225/1000 | Loss: 0.00001042
Iteration 226/1000 | Loss: 0.00001042
Iteration 227/1000 | Loss: 0.00001042
Iteration 228/1000 | Loss: 0.00001042
Iteration 229/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.0422895684314426e-05, 1.0422895684314426e-05, 1.0422895684314426e-05, 1.0422895684314426e-05, 1.0422895684314426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0422895684314426e-05

Optimization complete. Final v2v error: 2.7574260234832764 mm

Highest mean error: 2.8991010189056396 mm for frame 93

Lowest mean error: 2.631927490234375 mm for frame 1

Saving results

Total time: 47.30819797515869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472087
Iteration 2/25 | Loss: 0.00130136
Iteration 3/25 | Loss: 0.00123988
Iteration 4/25 | Loss: 0.00122957
Iteration 5/25 | Loss: 0.00122579
Iteration 6/25 | Loss: 0.00122515
Iteration 7/25 | Loss: 0.00122515
Iteration 8/25 | Loss: 0.00122515
Iteration 9/25 | Loss: 0.00122515
Iteration 10/25 | Loss: 0.00122515
Iteration 11/25 | Loss: 0.00122515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012251504231244326, 0.0012251504231244326, 0.0012251504231244326, 0.0012251504231244326, 0.0012251504231244326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012251504231244326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49932289
Iteration 2/25 | Loss: 0.00104261
Iteration 3/25 | Loss: 0.00104261
Iteration 4/25 | Loss: 0.00104261
Iteration 5/25 | Loss: 0.00104261
Iteration 6/25 | Loss: 0.00104261
Iteration 7/25 | Loss: 0.00104261
Iteration 8/25 | Loss: 0.00104261
Iteration 9/25 | Loss: 0.00104261
Iteration 10/25 | Loss: 0.00104261
Iteration 11/25 | Loss: 0.00104261
Iteration 12/25 | Loss: 0.00104261
Iteration 13/25 | Loss: 0.00104261
Iteration 14/25 | Loss: 0.00104261
Iteration 15/25 | Loss: 0.00104261
Iteration 16/25 | Loss: 0.00104261
Iteration 17/25 | Loss: 0.00104261
Iteration 18/25 | Loss: 0.00104261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010426058433949947, 0.0010426058433949947, 0.0010426058433949947, 0.0010426058433949947, 0.0010426058433949947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010426058433949947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104261
Iteration 2/1000 | Loss: 0.00003190
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001494
Iteration 6/1000 | Loss: 0.00001416
Iteration 7/1000 | Loss: 0.00001383
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001322
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001314
Iteration 13/1000 | Loss: 0.00001313
Iteration 14/1000 | Loss: 0.00001301
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001290
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001277
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001273
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001266
Iteration 29/1000 | Loss: 0.00001265
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001259
Iteration 39/1000 | Loss: 0.00001258
Iteration 40/1000 | Loss: 0.00001258
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001254
Iteration 43/1000 | Loss: 0.00001254
Iteration 44/1000 | Loss: 0.00001253
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001242
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Iteration 118/1000 | Loss: 0.00001233
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001228
Iteration 138/1000 | Loss: 0.00001228
Iteration 139/1000 | Loss: 0.00001228
Iteration 140/1000 | Loss: 0.00001228
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001226
Iteration 150/1000 | Loss: 0.00001226
Iteration 151/1000 | Loss: 0.00001226
Iteration 152/1000 | Loss: 0.00001226
Iteration 153/1000 | Loss: 0.00001226
Iteration 154/1000 | Loss: 0.00001226
Iteration 155/1000 | Loss: 0.00001226
Iteration 156/1000 | Loss: 0.00001226
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001226
Iteration 166/1000 | Loss: 0.00001226
Iteration 167/1000 | Loss: 0.00001226
Iteration 168/1000 | Loss: 0.00001226
Iteration 169/1000 | Loss: 0.00001226
Iteration 170/1000 | Loss: 0.00001226
Iteration 171/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2257647540536709e-05, 1.2257647540536709e-05, 1.2257647540536709e-05, 1.2257647540536709e-05, 1.2257647540536709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2257647540536709e-05

Optimization complete. Final v2v error: 2.9415283203125 mm

Highest mean error: 3.511401653289795 mm for frame 39

Lowest mean error: 2.6973540782928467 mm for frame 97

Saving results

Total time: 37.94309377670288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044015
Iteration 2/25 | Loss: 0.00203944
Iteration 3/25 | Loss: 0.00152462
Iteration 4/25 | Loss: 0.00147117
Iteration 5/25 | Loss: 0.00146127
Iteration 6/25 | Loss: 0.00143256
Iteration 7/25 | Loss: 0.00139722
Iteration 8/25 | Loss: 0.00138364
Iteration 9/25 | Loss: 0.00138004
Iteration 10/25 | Loss: 0.00138654
Iteration 11/25 | Loss: 0.00137686
Iteration 12/25 | Loss: 0.00137098
Iteration 13/25 | Loss: 0.00136882
Iteration 14/25 | Loss: 0.00136758
Iteration 15/25 | Loss: 0.00136615
Iteration 16/25 | Loss: 0.00136466
Iteration 17/25 | Loss: 0.00136414
Iteration 18/25 | Loss: 0.00136150
Iteration 19/25 | Loss: 0.00136454
Iteration 20/25 | Loss: 0.00136343
Iteration 21/25 | Loss: 0.00136221
Iteration 22/25 | Loss: 0.00136149
Iteration 23/25 | Loss: 0.00136266
Iteration 24/25 | Loss: 0.00135851
Iteration 25/25 | Loss: 0.00135936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14485788
Iteration 2/25 | Loss: 0.00196658
Iteration 3/25 | Loss: 0.00142216
Iteration 4/25 | Loss: 0.00142216
Iteration 5/25 | Loss: 0.00142216
Iteration 6/25 | Loss: 0.00142216
Iteration 7/25 | Loss: 0.00142216
Iteration 8/25 | Loss: 0.00142216
Iteration 9/25 | Loss: 0.00142216
Iteration 10/25 | Loss: 0.00142216
Iteration 11/25 | Loss: 0.00142216
Iteration 12/25 | Loss: 0.00142216
Iteration 13/25 | Loss: 0.00142216
Iteration 14/25 | Loss: 0.00142216
Iteration 15/25 | Loss: 0.00142216
Iteration 16/25 | Loss: 0.00142216
Iteration 17/25 | Loss: 0.00142216
Iteration 18/25 | Loss: 0.00142216
Iteration 19/25 | Loss: 0.00142216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001422161585651338, 0.001422161585651338, 0.001422161585651338, 0.001422161585651338, 0.001422161585651338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001422161585651338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142216
Iteration 2/1000 | Loss: 0.00035697
Iteration 3/1000 | Loss: 0.00036343
Iteration 4/1000 | Loss: 0.00028167
Iteration 5/1000 | Loss: 0.00014325
Iteration 6/1000 | Loss: 0.00010538
Iteration 7/1000 | Loss: 0.00006425
Iteration 8/1000 | Loss: 0.00004040
Iteration 9/1000 | Loss: 0.00003373
Iteration 10/1000 | Loss: 0.00006847
Iteration 11/1000 | Loss: 0.00004495
Iteration 12/1000 | Loss: 0.00007663
Iteration 13/1000 | Loss: 0.00015517
Iteration 14/1000 | Loss: 0.00025019
Iteration 15/1000 | Loss: 0.00019904
Iteration 16/1000 | Loss: 0.00027230
Iteration 17/1000 | Loss: 0.00018671
Iteration 18/1000 | Loss: 0.00003332
Iteration 19/1000 | Loss: 0.00003032
Iteration 20/1000 | Loss: 0.00020892
Iteration 21/1000 | Loss: 0.00014691
Iteration 22/1000 | Loss: 0.00020714
Iteration 23/1000 | Loss: 0.00005243
Iteration 24/1000 | Loss: 0.00004075
Iteration 25/1000 | Loss: 0.00002979
Iteration 26/1000 | Loss: 0.00002795
Iteration 27/1000 | Loss: 0.00014875
Iteration 28/1000 | Loss: 0.00014049
Iteration 29/1000 | Loss: 0.00015210
Iteration 30/1000 | Loss: 0.00013458
Iteration 31/1000 | Loss: 0.00012155
Iteration 32/1000 | Loss: 0.00007780
Iteration 33/1000 | Loss: 0.00011740
Iteration 34/1000 | Loss: 0.00011397
Iteration 35/1000 | Loss: 0.00015488
Iteration 36/1000 | Loss: 0.00002892
Iteration 37/1000 | Loss: 0.00005219
Iteration 38/1000 | Loss: 0.00004211
Iteration 39/1000 | Loss: 0.00002523
Iteration 40/1000 | Loss: 0.00004309
Iteration 41/1000 | Loss: 0.00002476
Iteration 42/1000 | Loss: 0.00015418
Iteration 43/1000 | Loss: 0.00016181
Iteration 44/1000 | Loss: 0.00014372
Iteration 45/1000 | Loss: 0.00016047
Iteration 46/1000 | Loss: 0.00003513
Iteration 47/1000 | Loss: 0.00015923
Iteration 48/1000 | Loss: 0.00015892
Iteration 49/1000 | Loss: 0.00044794
Iteration 50/1000 | Loss: 0.00026047
Iteration 51/1000 | Loss: 0.00020711
Iteration 52/1000 | Loss: 0.00003713
Iteration 53/1000 | Loss: 0.00017012
Iteration 54/1000 | Loss: 0.00017091
Iteration 55/1000 | Loss: 0.00016746
Iteration 56/1000 | Loss: 0.00009675
Iteration 57/1000 | Loss: 0.00013692
Iteration 58/1000 | Loss: 0.00002730
Iteration 59/1000 | Loss: 0.00003070
Iteration 60/1000 | Loss: 0.00002380
Iteration 61/1000 | Loss: 0.00002305
Iteration 62/1000 | Loss: 0.00002267
Iteration 63/1000 | Loss: 0.00002262
Iteration 64/1000 | Loss: 0.00002606
Iteration 65/1000 | Loss: 0.00002251
Iteration 66/1000 | Loss: 0.00002476
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00015708
Iteration 69/1000 | Loss: 0.00016275
Iteration 70/1000 | Loss: 0.00011275
Iteration 71/1000 | Loss: 0.00006059
Iteration 72/1000 | Loss: 0.00002510
Iteration 73/1000 | Loss: 0.00002436
Iteration 74/1000 | Loss: 0.00002394
Iteration 75/1000 | Loss: 0.00002369
Iteration 76/1000 | Loss: 0.00002369
Iteration 77/1000 | Loss: 0.00002745
Iteration 78/1000 | Loss: 0.00016218
Iteration 79/1000 | Loss: 0.00011773
Iteration 80/1000 | Loss: 0.00017169
Iteration 81/1000 | Loss: 0.00002779
Iteration 82/1000 | Loss: 0.00002485
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002228
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002223
Iteration 89/1000 | Loss: 0.00002223
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002223
Iteration 92/1000 | Loss: 0.00002222
Iteration 93/1000 | Loss: 0.00002222
Iteration 94/1000 | Loss: 0.00002222
Iteration 95/1000 | Loss: 0.00002222
Iteration 96/1000 | Loss: 0.00002221
Iteration 97/1000 | Loss: 0.00002221
Iteration 98/1000 | Loss: 0.00002220
Iteration 99/1000 | Loss: 0.00002220
Iteration 100/1000 | Loss: 0.00002220
Iteration 101/1000 | Loss: 0.00002219
Iteration 102/1000 | Loss: 0.00002219
Iteration 103/1000 | Loss: 0.00002218
Iteration 104/1000 | Loss: 0.00002218
Iteration 105/1000 | Loss: 0.00002217
Iteration 106/1000 | Loss: 0.00002217
Iteration 107/1000 | Loss: 0.00002217
Iteration 108/1000 | Loss: 0.00002216
Iteration 109/1000 | Loss: 0.00002216
Iteration 110/1000 | Loss: 0.00002215
Iteration 111/1000 | Loss: 0.00002214
Iteration 112/1000 | Loss: 0.00002627
Iteration 113/1000 | Loss: 0.00002212
Iteration 114/1000 | Loss: 0.00002212
Iteration 115/1000 | Loss: 0.00002210
Iteration 116/1000 | Loss: 0.00002210
Iteration 117/1000 | Loss: 0.00002210
Iteration 118/1000 | Loss: 0.00002210
Iteration 119/1000 | Loss: 0.00002210
Iteration 120/1000 | Loss: 0.00002209
Iteration 121/1000 | Loss: 0.00002209
Iteration 122/1000 | Loss: 0.00002209
Iteration 123/1000 | Loss: 0.00002209
Iteration 124/1000 | Loss: 0.00002209
Iteration 125/1000 | Loss: 0.00002209
Iteration 126/1000 | Loss: 0.00002209
Iteration 127/1000 | Loss: 0.00002209
Iteration 128/1000 | Loss: 0.00002209
Iteration 129/1000 | Loss: 0.00002209
Iteration 130/1000 | Loss: 0.00002209
Iteration 131/1000 | Loss: 0.00002209
Iteration 132/1000 | Loss: 0.00002209
Iteration 133/1000 | Loss: 0.00002208
Iteration 134/1000 | Loss: 0.00002208
Iteration 135/1000 | Loss: 0.00002208
Iteration 136/1000 | Loss: 0.00002208
Iteration 137/1000 | Loss: 0.00002208
Iteration 138/1000 | Loss: 0.00002208
Iteration 139/1000 | Loss: 0.00002207
Iteration 140/1000 | Loss: 0.00002207
Iteration 141/1000 | Loss: 0.00002207
Iteration 142/1000 | Loss: 0.00002206
Iteration 143/1000 | Loss: 0.00002206
Iteration 144/1000 | Loss: 0.00002206
Iteration 145/1000 | Loss: 0.00002205
Iteration 146/1000 | Loss: 0.00002205
Iteration 147/1000 | Loss: 0.00002205
Iteration 148/1000 | Loss: 0.00002205
Iteration 149/1000 | Loss: 0.00002205
Iteration 150/1000 | Loss: 0.00002205
Iteration 151/1000 | Loss: 0.00002204
Iteration 152/1000 | Loss: 0.00002204
Iteration 153/1000 | Loss: 0.00002204
Iteration 154/1000 | Loss: 0.00002204
Iteration 155/1000 | Loss: 0.00002204
Iteration 156/1000 | Loss: 0.00002204
Iteration 157/1000 | Loss: 0.00002204
Iteration 158/1000 | Loss: 0.00002203
Iteration 159/1000 | Loss: 0.00002203
Iteration 160/1000 | Loss: 0.00002203
Iteration 161/1000 | Loss: 0.00002203
Iteration 162/1000 | Loss: 0.00002202
Iteration 163/1000 | Loss: 0.00002202
Iteration 164/1000 | Loss: 0.00002202
Iteration 165/1000 | Loss: 0.00002202
Iteration 166/1000 | Loss: 0.00002201
Iteration 167/1000 | Loss: 0.00002200
Iteration 168/1000 | Loss: 0.00002200
Iteration 169/1000 | Loss: 0.00002199
Iteration 170/1000 | Loss: 0.00002199
Iteration 171/1000 | Loss: 0.00002198
Iteration 172/1000 | Loss: 0.00002197
Iteration 173/1000 | Loss: 0.00002195
Iteration 174/1000 | Loss: 0.00002191
Iteration 175/1000 | Loss: 0.00002191
Iteration 176/1000 | Loss: 0.00002190
Iteration 177/1000 | Loss: 0.00002189
Iteration 178/1000 | Loss: 0.00002189
Iteration 179/1000 | Loss: 0.00002189
Iteration 180/1000 | Loss: 0.00002189
Iteration 181/1000 | Loss: 0.00002189
Iteration 182/1000 | Loss: 0.00002188
Iteration 183/1000 | Loss: 0.00002188
Iteration 184/1000 | Loss: 0.00002188
Iteration 185/1000 | Loss: 0.00002188
Iteration 186/1000 | Loss: 0.00002188
Iteration 187/1000 | Loss: 0.00002187
Iteration 188/1000 | Loss: 0.00002187
Iteration 189/1000 | Loss: 0.00002187
Iteration 190/1000 | Loss: 0.00002187
Iteration 191/1000 | Loss: 0.00002186
Iteration 192/1000 | Loss: 0.00015030
Iteration 193/1000 | Loss: 0.00014270
Iteration 194/1000 | Loss: 0.00006326
Iteration 195/1000 | Loss: 0.00002460
Iteration 196/1000 | Loss: 0.00015183
Iteration 197/1000 | Loss: 0.00011086
Iteration 198/1000 | Loss: 0.00043308
Iteration 199/1000 | Loss: 0.00029759
Iteration 200/1000 | Loss: 0.00017583
Iteration 201/1000 | Loss: 0.00011876
Iteration 202/1000 | Loss: 0.00013297
Iteration 203/1000 | Loss: 0.00011619
Iteration 204/1000 | Loss: 0.00003335
Iteration 205/1000 | Loss: 0.00011142
Iteration 206/1000 | Loss: 0.00002887
Iteration 207/1000 | Loss: 0.00002696
Iteration 208/1000 | Loss: 0.00002551
Iteration 209/1000 | Loss: 0.00002465
Iteration 210/1000 | Loss: 0.00003060
Iteration 211/1000 | Loss: 0.00002353
Iteration 212/1000 | Loss: 0.00017408
Iteration 213/1000 | Loss: 0.00012745
Iteration 214/1000 | Loss: 0.00009875
Iteration 215/1000 | Loss: 0.00009903
Iteration 216/1000 | Loss: 0.00004196
Iteration 217/1000 | Loss: 0.00002710
Iteration 218/1000 | Loss: 0.00002632
Iteration 219/1000 | Loss: 0.00002518
Iteration 220/1000 | Loss: 0.00002445
Iteration 221/1000 | Loss: 0.00002344
Iteration 222/1000 | Loss: 0.00002251
Iteration 223/1000 | Loss: 0.00002189
Iteration 224/1000 | Loss: 0.00002151
Iteration 225/1000 | Loss: 0.00002134
Iteration 226/1000 | Loss: 0.00002131
Iteration 227/1000 | Loss: 0.00002122
Iteration 228/1000 | Loss: 0.00002122
Iteration 229/1000 | Loss: 0.00002120
Iteration 230/1000 | Loss: 0.00002120
Iteration 231/1000 | Loss: 0.00002119
Iteration 232/1000 | Loss: 0.00002119
Iteration 233/1000 | Loss: 0.00002119
Iteration 234/1000 | Loss: 0.00002119
Iteration 235/1000 | Loss: 0.00002119
Iteration 236/1000 | Loss: 0.00002119
Iteration 237/1000 | Loss: 0.00002118
Iteration 238/1000 | Loss: 0.00002118
Iteration 239/1000 | Loss: 0.00002118
Iteration 240/1000 | Loss: 0.00002118
Iteration 241/1000 | Loss: 0.00002117
Iteration 242/1000 | Loss: 0.00002117
Iteration 243/1000 | Loss: 0.00002117
Iteration 244/1000 | Loss: 0.00002117
Iteration 245/1000 | Loss: 0.00002116
Iteration 246/1000 | Loss: 0.00002116
Iteration 247/1000 | Loss: 0.00002116
Iteration 248/1000 | Loss: 0.00002116
Iteration 249/1000 | Loss: 0.00002116
Iteration 250/1000 | Loss: 0.00002115
Iteration 251/1000 | Loss: 0.00002115
Iteration 252/1000 | Loss: 0.00002115
Iteration 253/1000 | Loss: 0.00002114
Iteration 254/1000 | Loss: 0.00002114
Iteration 255/1000 | Loss: 0.00002113
Iteration 256/1000 | Loss: 0.00002113
Iteration 257/1000 | Loss: 0.00002112
Iteration 258/1000 | Loss: 0.00002112
Iteration 259/1000 | Loss: 0.00002112
Iteration 260/1000 | Loss: 0.00002111
Iteration 261/1000 | Loss: 0.00002111
Iteration 262/1000 | Loss: 0.00002111
Iteration 263/1000 | Loss: 0.00002111
Iteration 264/1000 | Loss: 0.00002111
Iteration 265/1000 | Loss: 0.00002111
Iteration 266/1000 | Loss: 0.00002111
Iteration 267/1000 | Loss: 0.00002111
Iteration 268/1000 | Loss: 0.00002111
Iteration 269/1000 | Loss: 0.00002111
Iteration 270/1000 | Loss: 0.00002111
Iteration 271/1000 | Loss: 0.00002110
Iteration 272/1000 | Loss: 0.00002110
Iteration 273/1000 | Loss: 0.00002110
Iteration 274/1000 | Loss: 0.00002110
Iteration 275/1000 | Loss: 0.00002110
Iteration 276/1000 | Loss: 0.00002110
Iteration 277/1000 | Loss: 0.00002110
Iteration 278/1000 | Loss: 0.00002110
Iteration 279/1000 | Loss: 0.00002110
Iteration 280/1000 | Loss: 0.00002110
Iteration 281/1000 | Loss: 0.00002110
Iteration 282/1000 | Loss: 0.00002110
Iteration 283/1000 | Loss: 0.00002110
Iteration 284/1000 | Loss: 0.00002110
Iteration 285/1000 | Loss: 0.00002110
Iteration 286/1000 | Loss: 0.00002110
Iteration 287/1000 | Loss: 0.00002110
Iteration 288/1000 | Loss: 0.00002110
Iteration 289/1000 | Loss: 0.00002110
Iteration 290/1000 | Loss: 0.00002110
Iteration 291/1000 | Loss: 0.00002110
Iteration 292/1000 | Loss: 0.00002110
Iteration 293/1000 | Loss: 0.00002110
Iteration 294/1000 | Loss: 0.00002110
Iteration 295/1000 | Loss: 0.00002110
Iteration 296/1000 | Loss: 0.00002110
Iteration 297/1000 | Loss: 0.00002110
Iteration 298/1000 | Loss: 0.00002110
Iteration 299/1000 | Loss: 0.00002110
Iteration 300/1000 | Loss: 0.00002110
Iteration 301/1000 | Loss: 0.00002110
Iteration 302/1000 | Loss: 0.00002110
Iteration 303/1000 | Loss: 0.00002110
Iteration 304/1000 | Loss: 0.00002110
Iteration 305/1000 | Loss: 0.00002110
Iteration 306/1000 | Loss: 0.00002110
Iteration 307/1000 | Loss: 0.00002110
Iteration 308/1000 | Loss: 0.00002110
Iteration 309/1000 | Loss: 0.00002110
Iteration 310/1000 | Loss: 0.00002110
Iteration 311/1000 | Loss: 0.00002110
Iteration 312/1000 | Loss: 0.00002110
Iteration 313/1000 | Loss: 0.00002110
Iteration 314/1000 | Loss: 0.00002110
Iteration 315/1000 | Loss: 0.00002110
Iteration 316/1000 | Loss: 0.00002110
Iteration 317/1000 | Loss: 0.00002110
Iteration 318/1000 | Loss: 0.00002110
Iteration 319/1000 | Loss: 0.00002110
Iteration 320/1000 | Loss: 0.00002110
Iteration 321/1000 | Loss: 0.00002110
Iteration 322/1000 | Loss: 0.00002110
Iteration 323/1000 | Loss: 0.00002110
Iteration 324/1000 | Loss: 0.00002110
Iteration 325/1000 | Loss: 0.00002110
Iteration 326/1000 | Loss: 0.00002110
Iteration 327/1000 | Loss: 0.00002110
Iteration 328/1000 | Loss: 0.00002110
Iteration 329/1000 | Loss: 0.00002110
Iteration 330/1000 | Loss: 0.00002110
Iteration 331/1000 | Loss: 0.00002110
Iteration 332/1000 | Loss: 0.00002110
Iteration 333/1000 | Loss: 0.00002110
Iteration 334/1000 | Loss: 0.00002110
Iteration 335/1000 | Loss: 0.00002110
Iteration 336/1000 | Loss: 0.00002110
Iteration 337/1000 | Loss: 0.00002110
Iteration 338/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.1103924154886045e-05, 2.1103924154886045e-05, 2.1103924154886045e-05, 2.1103924154886045e-05, 2.1103924154886045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1103924154886045e-05

Optimization complete. Final v2v error: 3.810025215148926 mm

Highest mean error: 5.229284286499023 mm for frame 184

Lowest mean error: 3.4486842155456543 mm for frame 115

Saving results

Total time: 256.1944947242737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429389
Iteration 2/25 | Loss: 0.00133763
Iteration 3/25 | Loss: 0.00123412
Iteration 4/25 | Loss: 0.00122218
Iteration 5/25 | Loss: 0.00121910
Iteration 6/25 | Loss: 0.00121836
Iteration 7/25 | Loss: 0.00121836
Iteration 8/25 | Loss: 0.00121836
Iteration 9/25 | Loss: 0.00121836
Iteration 10/25 | Loss: 0.00121836
Iteration 11/25 | Loss: 0.00121836
Iteration 12/25 | Loss: 0.00121836
Iteration 13/25 | Loss: 0.00121836
Iteration 14/25 | Loss: 0.00121836
Iteration 15/25 | Loss: 0.00121836
Iteration 16/25 | Loss: 0.00121836
Iteration 17/25 | Loss: 0.00121836
Iteration 18/25 | Loss: 0.00121836
Iteration 19/25 | Loss: 0.00121836
Iteration 20/25 | Loss: 0.00121836
Iteration 21/25 | Loss: 0.00121836
Iteration 22/25 | Loss: 0.00121836
Iteration 23/25 | Loss: 0.00121836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012183582875877619, 0.0012183582875877619, 0.0012183582875877619, 0.0012183582875877619, 0.0012183582875877619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012183582875877619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.58221722
Iteration 2/25 | Loss: 0.00097045
Iteration 3/25 | Loss: 0.00097044
Iteration 4/25 | Loss: 0.00097044
Iteration 5/25 | Loss: 0.00097044
Iteration 6/25 | Loss: 0.00097044
Iteration 7/25 | Loss: 0.00097044
Iteration 8/25 | Loss: 0.00097044
Iteration 9/25 | Loss: 0.00097044
Iteration 10/25 | Loss: 0.00097044
Iteration 11/25 | Loss: 0.00097044
Iteration 12/25 | Loss: 0.00097044
Iteration 13/25 | Loss: 0.00097044
Iteration 14/25 | Loss: 0.00097044
Iteration 15/25 | Loss: 0.00097044
Iteration 16/25 | Loss: 0.00097044
Iteration 17/25 | Loss: 0.00097044
Iteration 18/25 | Loss: 0.00097044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009704382391646504, 0.0009704382391646504, 0.0009704382391646504, 0.0009704382391646504, 0.0009704382391646504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009704382391646504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097044
Iteration 2/1000 | Loss: 0.00002880
Iteration 3/1000 | Loss: 0.00002051
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001472
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001339
Iteration 13/1000 | Loss: 0.00001333
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001319
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001310
Iteration 20/1000 | Loss: 0.00001308
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001284
Iteration 53/1000 | Loss: 0.00001284
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001283
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001283
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001282
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001275
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001274
Iteration 81/1000 | Loss: 0.00001274
Iteration 82/1000 | Loss: 0.00001274
Iteration 83/1000 | Loss: 0.00001274
Iteration 84/1000 | Loss: 0.00001274
Iteration 85/1000 | Loss: 0.00001273
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001272
Iteration 89/1000 | Loss: 0.00001272
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001270
Iteration 94/1000 | Loss: 0.00001270
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001267
Iteration 109/1000 | Loss: 0.00001267
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001265
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001264
Iteration 126/1000 | Loss: 0.00001264
Iteration 127/1000 | Loss: 0.00001264
Iteration 128/1000 | Loss: 0.00001264
Iteration 129/1000 | Loss: 0.00001264
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001264
Iteration 139/1000 | Loss: 0.00001264
Iteration 140/1000 | Loss: 0.00001264
Iteration 141/1000 | Loss: 0.00001264
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001263
Iteration 150/1000 | Loss: 0.00001263
Iteration 151/1000 | Loss: 0.00001263
Iteration 152/1000 | Loss: 0.00001263
Iteration 153/1000 | Loss: 0.00001263
Iteration 154/1000 | Loss: 0.00001263
Iteration 155/1000 | Loss: 0.00001263
Iteration 156/1000 | Loss: 0.00001262
Iteration 157/1000 | Loss: 0.00001262
Iteration 158/1000 | Loss: 0.00001262
Iteration 159/1000 | Loss: 0.00001262
Iteration 160/1000 | Loss: 0.00001262
Iteration 161/1000 | Loss: 0.00001262
Iteration 162/1000 | Loss: 0.00001262
Iteration 163/1000 | Loss: 0.00001262
Iteration 164/1000 | Loss: 0.00001262
Iteration 165/1000 | Loss: 0.00001262
Iteration 166/1000 | Loss: 0.00001262
Iteration 167/1000 | Loss: 0.00001262
Iteration 168/1000 | Loss: 0.00001262
Iteration 169/1000 | Loss: 0.00001262
Iteration 170/1000 | Loss: 0.00001262
Iteration 171/1000 | Loss: 0.00001262
Iteration 172/1000 | Loss: 0.00001262
Iteration 173/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.2621205314644612e-05, 1.2621205314644612e-05, 1.2621205314644612e-05, 1.2621205314644612e-05, 1.2621205314644612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2621205314644612e-05

Optimization complete. Final v2v error: 3.0350289344787598 mm

Highest mean error: 3.603273868560791 mm for frame 82

Lowest mean error: 2.6993837356567383 mm for frame 33

Saving results

Total time: 40.00794053077698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813586
Iteration 2/25 | Loss: 0.00138292
Iteration 3/25 | Loss: 0.00126036
Iteration 4/25 | Loss: 0.00123764
Iteration 5/25 | Loss: 0.00123349
Iteration 6/25 | Loss: 0.00123041
Iteration 7/25 | Loss: 0.00122959
Iteration 8/25 | Loss: 0.00122927
Iteration 9/25 | Loss: 0.00122912
Iteration 10/25 | Loss: 0.00122979
Iteration 11/25 | Loss: 0.00122870
Iteration 12/25 | Loss: 0.00122817
Iteration 13/25 | Loss: 0.00122797
Iteration 14/25 | Loss: 0.00122794
Iteration 15/25 | Loss: 0.00122794
Iteration 16/25 | Loss: 0.00122794
Iteration 17/25 | Loss: 0.00122794
Iteration 18/25 | Loss: 0.00122793
Iteration 19/25 | Loss: 0.00122793
Iteration 20/25 | Loss: 0.00122793
Iteration 21/25 | Loss: 0.00122793
Iteration 22/25 | Loss: 0.00122793
Iteration 23/25 | Loss: 0.00122793
Iteration 24/25 | Loss: 0.00122793
Iteration 25/25 | Loss: 0.00122793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61961961
Iteration 2/25 | Loss: 0.00090165
Iteration 3/25 | Loss: 0.00090164
Iteration 4/25 | Loss: 0.00090164
Iteration 5/25 | Loss: 0.00090163
Iteration 6/25 | Loss: 0.00090163
Iteration 7/25 | Loss: 0.00090163
Iteration 8/25 | Loss: 0.00090163
Iteration 9/25 | Loss: 0.00090163
Iteration 10/25 | Loss: 0.00090163
Iteration 11/25 | Loss: 0.00090163
Iteration 12/25 | Loss: 0.00090163
Iteration 13/25 | Loss: 0.00090163
Iteration 14/25 | Loss: 0.00090163
Iteration 15/25 | Loss: 0.00090163
Iteration 16/25 | Loss: 0.00090163
Iteration 17/25 | Loss: 0.00090163
Iteration 18/25 | Loss: 0.00090163
Iteration 19/25 | Loss: 0.00090163
Iteration 20/25 | Loss: 0.00090163
Iteration 21/25 | Loss: 0.00090163
Iteration 22/25 | Loss: 0.00090163
Iteration 23/25 | Loss: 0.00090163
Iteration 24/25 | Loss: 0.00090163
Iteration 25/25 | Loss: 0.00090163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090163
Iteration 2/1000 | Loss: 0.00002540
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001827
Iteration 5/1000 | Loss: 0.00001711
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001617
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001541
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001539
Iteration 14/1000 | Loss: 0.00001538
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00001516
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001510
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001510
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001503
Iteration 30/1000 | Loss: 0.00001503
Iteration 31/1000 | Loss: 0.00001502
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001496
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001492
Iteration 39/1000 | Loss: 0.00001492
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001482
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001481
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001481
Iteration 91/1000 | Loss: 0.00001481
Iteration 92/1000 | Loss: 0.00001481
Iteration 93/1000 | Loss: 0.00001481
Iteration 94/1000 | Loss: 0.00001481
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001480
Iteration 97/1000 | Loss: 0.00001480
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001479
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001479
Iteration 117/1000 | Loss: 0.00001479
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001479
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001478
Iteration 127/1000 | Loss: 0.00001478
Iteration 128/1000 | Loss: 0.00001478
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001478
Iteration 131/1000 | Loss: 0.00001477
Iteration 132/1000 | Loss: 0.00001477
Iteration 133/1000 | Loss: 0.00001477
Iteration 134/1000 | Loss: 0.00001477
Iteration 135/1000 | Loss: 0.00001477
Iteration 136/1000 | Loss: 0.00001477
Iteration 137/1000 | Loss: 0.00001477
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001477
Iteration 141/1000 | Loss: 0.00001477
Iteration 142/1000 | Loss: 0.00001476
Iteration 143/1000 | Loss: 0.00001476
Iteration 144/1000 | Loss: 0.00001476
Iteration 145/1000 | Loss: 0.00001476
Iteration 146/1000 | Loss: 0.00001476
Iteration 147/1000 | Loss: 0.00001475
Iteration 148/1000 | Loss: 0.00001475
Iteration 149/1000 | Loss: 0.00001475
Iteration 150/1000 | Loss: 0.00001474
Iteration 151/1000 | Loss: 0.00001474
Iteration 152/1000 | Loss: 0.00001474
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Iteration 156/1000 | Loss: 0.00001474
Iteration 157/1000 | Loss: 0.00001474
Iteration 158/1000 | Loss: 0.00001474
Iteration 159/1000 | Loss: 0.00001474
Iteration 160/1000 | Loss: 0.00001474
Iteration 161/1000 | Loss: 0.00001474
Iteration 162/1000 | Loss: 0.00001474
Iteration 163/1000 | Loss: 0.00001474
Iteration 164/1000 | Loss: 0.00001474
Iteration 165/1000 | Loss: 0.00001474
Iteration 166/1000 | Loss: 0.00001474
Iteration 167/1000 | Loss: 0.00001474
Iteration 168/1000 | Loss: 0.00001474
Iteration 169/1000 | Loss: 0.00001474
Iteration 170/1000 | Loss: 0.00001474
Iteration 171/1000 | Loss: 0.00001474
Iteration 172/1000 | Loss: 0.00001474
Iteration 173/1000 | Loss: 0.00001474
Iteration 174/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.4735266631760169e-05, 1.4735266631760169e-05, 1.4735266631760169e-05, 1.4735266631760169e-05, 1.4735266631760169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4735266631760169e-05

Optimization complete. Final v2v error: 3.254459857940674 mm

Highest mean error: 3.659674882888794 mm for frame 205

Lowest mean error: 2.982041597366333 mm for frame 107

Saving results

Total time: 58.85426449775696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803057
Iteration 2/25 | Loss: 0.00142592
Iteration 3/25 | Loss: 0.00124072
Iteration 4/25 | Loss: 0.00122287
Iteration 5/25 | Loss: 0.00122072
Iteration 6/25 | Loss: 0.00122007
Iteration 7/25 | Loss: 0.00122006
Iteration 8/25 | Loss: 0.00122006
Iteration 9/25 | Loss: 0.00122006
Iteration 10/25 | Loss: 0.00122006
Iteration 11/25 | Loss: 0.00122006
Iteration 12/25 | Loss: 0.00122006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012200584169477224, 0.0012200584169477224, 0.0012200584169477224, 0.0012200584169477224, 0.0012200584169477224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012200584169477224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35336816
Iteration 2/25 | Loss: 0.00101235
Iteration 3/25 | Loss: 0.00101234
Iteration 4/25 | Loss: 0.00101234
Iteration 5/25 | Loss: 0.00101234
Iteration 6/25 | Loss: 0.00101234
Iteration 7/25 | Loss: 0.00101234
Iteration 8/25 | Loss: 0.00101234
Iteration 9/25 | Loss: 0.00101234
Iteration 10/25 | Loss: 0.00101234
Iteration 11/25 | Loss: 0.00101234
Iteration 12/25 | Loss: 0.00101234
Iteration 13/25 | Loss: 0.00101234
Iteration 14/25 | Loss: 0.00101234
Iteration 15/25 | Loss: 0.00101234
Iteration 16/25 | Loss: 0.00101234
Iteration 17/25 | Loss: 0.00101234
Iteration 18/25 | Loss: 0.00101234
Iteration 19/25 | Loss: 0.00101234
Iteration 20/25 | Loss: 0.00101234
Iteration 21/25 | Loss: 0.00101234
Iteration 22/25 | Loss: 0.00101234
Iteration 23/25 | Loss: 0.00101234
Iteration 24/25 | Loss: 0.00101234
Iteration 25/25 | Loss: 0.00101234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101234
Iteration 2/1000 | Loss: 0.00002298
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001419
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001276
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001193
Iteration 9/1000 | Loss: 0.00001168
Iteration 10/1000 | Loss: 0.00001144
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001132
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001124
Iteration 16/1000 | Loss: 0.00001121
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001118
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001108
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001107
Iteration 37/1000 | Loss: 0.00001107
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001104
Iteration 43/1000 | Loss: 0.00001104
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001103
Iteration 46/1000 | Loss: 0.00001103
Iteration 47/1000 | Loss: 0.00001103
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001097
Iteration 63/1000 | Loss: 0.00001097
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001095
Iteration 79/1000 | Loss: 0.00001095
Iteration 80/1000 | Loss: 0.00001095
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001094
Iteration 83/1000 | Loss: 0.00001094
Iteration 84/1000 | Loss: 0.00001094
Iteration 85/1000 | Loss: 0.00001094
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001093
Iteration 91/1000 | Loss: 0.00001093
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001091
Iteration 97/1000 | Loss: 0.00001091
Iteration 98/1000 | Loss: 0.00001091
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001086
Iteration 125/1000 | Loss: 0.00001086
Iteration 126/1000 | Loss: 0.00001086
Iteration 127/1000 | Loss: 0.00001086
Iteration 128/1000 | Loss: 0.00001086
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001085
Iteration 131/1000 | Loss: 0.00001085
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001084
Iteration 134/1000 | Loss: 0.00001084
Iteration 135/1000 | Loss: 0.00001084
Iteration 136/1000 | Loss: 0.00001084
Iteration 137/1000 | Loss: 0.00001084
Iteration 138/1000 | Loss: 0.00001084
Iteration 139/1000 | Loss: 0.00001084
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001083
Iteration 142/1000 | Loss: 0.00001083
Iteration 143/1000 | Loss: 0.00001083
Iteration 144/1000 | Loss: 0.00001083
Iteration 145/1000 | Loss: 0.00001083
Iteration 146/1000 | Loss: 0.00001083
Iteration 147/1000 | Loss: 0.00001082
Iteration 148/1000 | Loss: 0.00001082
Iteration 149/1000 | Loss: 0.00001082
Iteration 150/1000 | Loss: 0.00001082
Iteration 151/1000 | Loss: 0.00001081
Iteration 152/1000 | Loss: 0.00001081
Iteration 153/1000 | Loss: 0.00001081
Iteration 154/1000 | Loss: 0.00001081
Iteration 155/1000 | Loss: 0.00001081
Iteration 156/1000 | Loss: 0.00001081
Iteration 157/1000 | Loss: 0.00001080
Iteration 158/1000 | Loss: 0.00001080
Iteration 159/1000 | Loss: 0.00001080
Iteration 160/1000 | Loss: 0.00001080
Iteration 161/1000 | Loss: 0.00001080
Iteration 162/1000 | Loss: 0.00001080
Iteration 163/1000 | Loss: 0.00001079
Iteration 164/1000 | Loss: 0.00001079
Iteration 165/1000 | Loss: 0.00001079
Iteration 166/1000 | Loss: 0.00001079
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001078
Iteration 169/1000 | Loss: 0.00001077
Iteration 170/1000 | Loss: 0.00001077
Iteration 171/1000 | Loss: 0.00001077
Iteration 172/1000 | Loss: 0.00001077
Iteration 173/1000 | Loss: 0.00001076
Iteration 174/1000 | Loss: 0.00001076
Iteration 175/1000 | Loss: 0.00001076
Iteration 176/1000 | Loss: 0.00001076
Iteration 177/1000 | Loss: 0.00001076
Iteration 178/1000 | Loss: 0.00001076
Iteration 179/1000 | Loss: 0.00001076
Iteration 180/1000 | Loss: 0.00001076
Iteration 181/1000 | Loss: 0.00001076
Iteration 182/1000 | Loss: 0.00001076
Iteration 183/1000 | Loss: 0.00001076
Iteration 184/1000 | Loss: 0.00001075
Iteration 185/1000 | Loss: 0.00001075
Iteration 186/1000 | Loss: 0.00001075
Iteration 187/1000 | Loss: 0.00001075
Iteration 188/1000 | Loss: 0.00001075
Iteration 189/1000 | Loss: 0.00001075
Iteration 190/1000 | Loss: 0.00001075
Iteration 191/1000 | Loss: 0.00001075
Iteration 192/1000 | Loss: 0.00001074
Iteration 193/1000 | Loss: 0.00001074
Iteration 194/1000 | Loss: 0.00001074
Iteration 195/1000 | Loss: 0.00001074
Iteration 196/1000 | Loss: 0.00001074
Iteration 197/1000 | Loss: 0.00001074
Iteration 198/1000 | Loss: 0.00001074
Iteration 199/1000 | Loss: 0.00001074
Iteration 200/1000 | Loss: 0.00001074
Iteration 201/1000 | Loss: 0.00001074
Iteration 202/1000 | Loss: 0.00001074
Iteration 203/1000 | Loss: 0.00001074
Iteration 204/1000 | Loss: 0.00001074
Iteration 205/1000 | Loss: 0.00001074
Iteration 206/1000 | Loss: 0.00001073
Iteration 207/1000 | Loss: 0.00001073
Iteration 208/1000 | Loss: 0.00001073
Iteration 209/1000 | Loss: 0.00001073
Iteration 210/1000 | Loss: 0.00001073
Iteration 211/1000 | Loss: 0.00001073
Iteration 212/1000 | Loss: 0.00001073
Iteration 213/1000 | Loss: 0.00001073
Iteration 214/1000 | Loss: 0.00001073
Iteration 215/1000 | Loss: 0.00001073
Iteration 216/1000 | Loss: 0.00001072
Iteration 217/1000 | Loss: 0.00001072
Iteration 218/1000 | Loss: 0.00001072
Iteration 219/1000 | Loss: 0.00001072
Iteration 220/1000 | Loss: 0.00001072
Iteration 221/1000 | Loss: 0.00001072
Iteration 222/1000 | Loss: 0.00001072
Iteration 223/1000 | Loss: 0.00001072
Iteration 224/1000 | Loss: 0.00001072
Iteration 225/1000 | Loss: 0.00001072
Iteration 226/1000 | Loss: 0.00001072
Iteration 227/1000 | Loss: 0.00001072
Iteration 228/1000 | Loss: 0.00001071
Iteration 229/1000 | Loss: 0.00001071
Iteration 230/1000 | Loss: 0.00001071
Iteration 231/1000 | Loss: 0.00001071
Iteration 232/1000 | Loss: 0.00001071
Iteration 233/1000 | Loss: 0.00001071
Iteration 234/1000 | Loss: 0.00001071
Iteration 235/1000 | Loss: 0.00001071
Iteration 236/1000 | Loss: 0.00001071
Iteration 237/1000 | Loss: 0.00001071
Iteration 238/1000 | Loss: 0.00001071
Iteration 239/1000 | Loss: 0.00001071
Iteration 240/1000 | Loss: 0.00001071
Iteration 241/1000 | Loss: 0.00001071
Iteration 242/1000 | Loss: 0.00001071
Iteration 243/1000 | Loss: 0.00001071
Iteration 244/1000 | Loss: 0.00001071
Iteration 245/1000 | Loss: 0.00001071
Iteration 246/1000 | Loss: 0.00001071
Iteration 247/1000 | Loss: 0.00001071
Iteration 248/1000 | Loss: 0.00001071
Iteration 249/1000 | Loss: 0.00001071
Iteration 250/1000 | Loss: 0.00001071
Iteration 251/1000 | Loss: 0.00001071
Iteration 252/1000 | Loss: 0.00001071
Iteration 253/1000 | Loss: 0.00001071
Iteration 254/1000 | Loss: 0.00001071
Iteration 255/1000 | Loss: 0.00001071
Iteration 256/1000 | Loss: 0.00001071
Iteration 257/1000 | Loss: 0.00001071
Iteration 258/1000 | Loss: 0.00001071
Iteration 259/1000 | Loss: 0.00001071
Iteration 260/1000 | Loss: 0.00001071
Iteration 261/1000 | Loss: 0.00001071
Iteration 262/1000 | Loss: 0.00001071
Iteration 263/1000 | Loss: 0.00001071
Iteration 264/1000 | Loss: 0.00001071
Iteration 265/1000 | Loss: 0.00001071
Iteration 266/1000 | Loss: 0.00001071
Iteration 267/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.0707422006817069e-05, 1.0707422006817069e-05, 1.0707422006817069e-05, 1.0707422006817069e-05, 1.0707422006817069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0707422006817069e-05

Optimization complete. Final v2v error: 2.8154754638671875 mm

Highest mean error: 3.106618642807007 mm for frame 87

Lowest mean error: 2.696453809738159 mm for frame 8

Saving results

Total time: 42.554898738861084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475076
Iteration 2/25 | Loss: 0.00130476
Iteration 3/25 | Loss: 0.00123349
Iteration 4/25 | Loss: 0.00122002
Iteration 5/25 | Loss: 0.00121501
Iteration 6/25 | Loss: 0.00121384
Iteration 7/25 | Loss: 0.00121383
Iteration 8/25 | Loss: 0.00121383
Iteration 9/25 | Loss: 0.00121383
Iteration 10/25 | Loss: 0.00121383
Iteration 11/25 | Loss: 0.00121383
Iteration 12/25 | Loss: 0.00121383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012138279853388667, 0.0012138279853388667, 0.0012138279853388667, 0.0012138279853388667, 0.0012138279853388667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012138279853388667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.83305073
Iteration 2/25 | Loss: 0.00097438
Iteration 3/25 | Loss: 0.00097437
Iteration 4/25 | Loss: 0.00097437
Iteration 5/25 | Loss: 0.00097437
Iteration 6/25 | Loss: 0.00097437
Iteration 7/25 | Loss: 0.00097437
Iteration 8/25 | Loss: 0.00097437
Iteration 9/25 | Loss: 0.00097437
Iteration 10/25 | Loss: 0.00097437
Iteration 11/25 | Loss: 0.00097437
Iteration 12/25 | Loss: 0.00097437
Iteration 13/25 | Loss: 0.00097437
Iteration 14/25 | Loss: 0.00097437
Iteration 15/25 | Loss: 0.00097437
Iteration 16/25 | Loss: 0.00097437
Iteration 17/25 | Loss: 0.00097437
Iteration 18/25 | Loss: 0.00097437
Iteration 19/25 | Loss: 0.00097437
Iteration 20/25 | Loss: 0.00097437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009743690025061369, 0.0009743690025061369, 0.0009743690025061369, 0.0009743690025061369, 0.0009743690025061369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009743690025061369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097437
Iteration 2/1000 | Loss: 0.00002766
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001350
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001296
Iteration 21/1000 | Loss: 0.00001296
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001288
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001287
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001281
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001280
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001276
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001270
Iteration 65/1000 | Loss: 0.00001270
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001266
Iteration 75/1000 | Loss: 0.00001266
Iteration 76/1000 | Loss: 0.00001266
Iteration 77/1000 | Loss: 0.00001265
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001263
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001260
Iteration 92/1000 | Loss: 0.00001260
Iteration 93/1000 | Loss: 0.00001260
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001259
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001256
Iteration 130/1000 | Loss: 0.00001256
Iteration 131/1000 | Loss: 0.00001256
Iteration 132/1000 | Loss: 0.00001256
Iteration 133/1000 | Loss: 0.00001256
Iteration 134/1000 | Loss: 0.00001256
Iteration 135/1000 | Loss: 0.00001256
Iteration 136/1000 | Loss: 0.00001256
Iteration 137/1000 | Loss: 0.00001256
Iteration 138/1000 | Loss: 0.00001256
Iteration 139/1000 | Loss: 0.00001256
Iteration 140/1000 | Loss: 0.00001256
Iteration 141/1000 | Loss: 0.00001256
Iteration 142/1000 | Loss: 0.00001256
Iteration 143/1000 | Loss: 0.00001256
Iteration 144/1000 | Loss: 0.00001256
Iteration 145/1000 | Loss: 0.00001256
Iteration 146/1000 | Loss: 0.00001255
Iteration 147/1000 | Loss: 0.00001255
Iteration 148/1000 | Loss: 0.00001255
Iteration 149/1000 | Loss: 0.00001255
Iteration 150/1000 | Loss: 0.00001255
Iteration 151/1000 | Loss: 0.00001255
Iteration 152/1000 | Loss: 0.00001255
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2553817214211449e-05, 1.2553817214211449e-05, 1.2553817214211449e-05, 1.2553817214211449e-05, 1.2553817214211449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2553817214211449e-05

Optimization complete. Final v2v error: 3.056666135787964 mm

Highest mean error: 3.4170210361480713 mm for frame 105

Lowest mean error: 2.887667179107666 mm for frame 25

Saving results

Total time: 37.789896726608276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575546
Iteration 2/25 | Loss: 0.00147351
Iteration 3/25 | Loss: 0.00133640
Iteration 4/25 | Loss: 0.00130698
Iteration 5/25 | Loss: 0.00129670
Iteration 6/25 | Loss: 0.00129350
Iteration 7/25 | Loss: 0.00129422
Iteration 8/25 | Loss: 0.00129212
Iteration 9/25 | Loss: 0.00129058
Iteration 10/25 | Loss: 0.00129543
Iteration 11/25 | Loss: 0.00128893
Iteration 12/25 | Loss: 0.00128809
Iteration 13/25 | Loss: 0.00128721
Iteration 14/25 | Loss: 0.00128673
Iteration 15/25 | Loss: 0.00128654
Iteration 16/25 | Loss: 0.00128641
Iteration 17/25 | Loss: 0.00129101
Iteration 18/25 | Loss: 0.00128753
Iteration 19/25 | Loss: 0.00128617
Iteration 20/25 | Loss: 0.00128590
Iteration 21/25 | Loss: 0.00128563
Iteration 22/25 | Loss: 0.00128545
Iteration 23/25 | Loss: 0.00129008
Iteration 24/25 | Loss: 0.00128713
Iteration 25/25 | Loss: 0.00128357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16381764
Iteration 2/25 | Loss: 0.00097779
Iteration 3/25 | Loss: 0.00097772
Iteration 4/25 | Loss: 0.00097772
Iteration 5/25 | Loss: 0.00097772
Iteration 6/25 | Loss: 0.00097772
Iteration 7/25 | Loss: 0.00097772
Iteration 8/25 | Loss: 0.00097772
Iteration 9/25 | Loss: 0.00097772
Iteration 10/25 | Loss: 0.00097772
Iteration 11/25 | Loss: 0.00097772
Iteration 12/25 | Loss: 0.00097772
Iteration 13/25 | Loss: 0.00097772
Iteration 14/25 | Loss: 0.00097772
Iteration 15/25 | Loss: 0.00097772
Iteration 16/25 | Loss: 0.00097772
Iteration 17/25 | Loss: 0.00097772
Iteration 18/25 | Loss: 0.00097772
Iteration 19/25 | Loss: 0.00097772
Iteration 20/25 | Loss: 0.00097772
Iteration 21/25 | Loss: 0.00097772
Iteration 22/25 | Loss: 0.00097772
Iteration 23/25 | Loss: 0.00097772
Iteration 24/25 | Loss: 0.00097772
Iteration 25/25 | Loss: 0.00097772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097772
Iteration 2/1000 | Loss: 0.00004214
Iteration 3/1000 | Loss: 0.00003068
Iteration 4/1000 | Loss: 0.00002792
Iteration 5/1000 | Loss: 0.00002691
Iteration 6/1000 | Loss: 0.00002603
Iteration 7/1000 | Loss: 0.00002552
Iteration 8/1000 | Loss: 0.00002492
Iteration 9/1000 | Loss: 0.00002455
Iteration 10/1000 | Loss: 0.00002430
Iteration 11/1000 | Loss: 0.00002400
Iteration 12/1000 | Loss: 0.00002379
Iteration 13/1000 | Loss: 0.00002362
Iteration 14/1000 | Loss: 0.00002362
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002335
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002329
Iteration 19/1000 | Loss: 0.00002321
Iteration 20/1000 | Loss: 0.00002316
Iteration 21/1000 | Loss: 0.00002315
Iteration 22/1000 | Loss: 0.00002313
Iteration 23/1000 | Loss: 0.00002313
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002312
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002311
Iteration 29/1000 | Loss: 0.00002311
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00002310
Iteration 32/1000 | Loss: 0.00002309
Iteration 33/1000 | Loss: 0.00002307
Iteration 34/1000 | Loss: 0.00002307
Iteration 35/1000 | Loss: 0.00002306
Iteration 36/1000 | Loss: 0.00002305
Iteration 37/1000 | Loss: 0.00002305
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002304
Iteration 40/1000 | Loss: 0.00002304
Iteration 41/1000 | Loss: 0.00002304
Iteration 42/1000 | Loss: 0.00002304
Iteration 43/1000 | Loss: 0.00002304
Iteration 44/1000 | Loss: 0.00002304
Iteration 45/1000 | Loss: 0.00002304
Iteration 46/1000 | Loss: 0.00002304
Iteration 47/1000 | Loss: 0.00002303
Iteration 48/1000 | Loss: 0.00002303
Iteration 49/1000 | Loss: 0.00002303
Iteration 50/1000 | Loss: 0.00002302
Iteration 51/1000 | Loss: 0.00002301
Iteration 52/1000 | Loss: 0.00002301
Iteration 53/1000 | Loss: 0.00002301
Iteration 54/1000 | Loss: 0.00002300
Iteration 55/1000 | Loss: 0.00002299
Iteration 56/1000 | Loss: 0.00002298
Iteration 57/1000 | Loss: 0.00002298
Iteration 58/1000 | Loss: 0.00002298
Iteration 59/1000 | Loss: 0.00002297
Iteration 60/1000 | Loss: 0.00002297
Iteration 61/1000 | Loss: 0.00002296
Iteration 62/1000 | Loss: 0.00002296
Iteration 63/1000 | Loss: 0.00002295
Iteration 64/1000 | Loss: 0.00002295
Iteration 65/1000 | Loss: 0.00002294
Iteration 66/1000 | Loss: 0.00002294
Iteration 67/1000 | Loss: 0.00002293
Iteration 68/1000 | Loss: 0.00002293
Iteration 69/1000 | Loss: 0.00002293
Iteration 70/1000 | Loss: 0.00002293
Iteration 71/1000 | Loss: 0.00002293
Iteration 72/1000 | Loss: 0.00002293
Iteration 73/1000 | Loss: 0.00002293
Iteration 74/1000 | Loss: 0.00002293
Iteration 75/1000 | Loss: 0.00002292
Iteration 76/1000 | Loss: 0.00002292
Iteration 77/1000 | Loss: 0.00002292
Iteration 78/1000 | Loss: 0.00002292
Iteration 79/1000 | Loss: 0.00002292
Iteration 80/1000 | Loss: 0.00002292
Iteration 81/1000 | Loss: 0.00002291
Iteration 82/1000 | Loss: 0.00002290
Iteration 83/1000 | Loss: 0.00002290
Iteration 84/1000 | Loss: 0.00002290
Iteration 85/1000 | Loss: 0.00002290
Iteration 86/1000 | Loss: 0.00002289
Iteration 87/1000 | Loss: 0.00002289
Iteration 88/1000 | Loss: 0.00002289
Iteration 89/1000 | Loss: 0.00002289
Iteration 90/1000 | Loss: 0.00002289
Iteration 91/1000 | Loss: 0.00002289
Iteration 92/1000 | Loss: 0.00002289
Iteration 93/1000 | Loss: 0.00002288
Iteration 94/1000 | Loss: 0.00002288
Iteration 95/1000 | Loss: 0.00002288
Iteration 96/1000 | Loss: 0.00002288
Iteration 97/1000 | Loss: 0.00002288
Iteration 98/1000 | Loss: 0.00002288
Iteration 99/1000 | Loss: 0.00002288
Iteration 100/1000 | Loss: 0.00002287
Iteration 101/1000 | Loss: 0.00002287
Iteration 102/1000 | Loss: 0.00002287
Iteration 103/1000 | Loss: 0.00002287
Iteration 104/1000 | Loss: 0.00002287
Iteration 105/1000 | Loss: 0.00002287
Iteration 106/1000 | Loss: 0.00002287
Iteration 107/1000 | Loss: 0.00002287
Iteration 108/1000 | Loss: 0.00002287
Iteration 109/1000 | Loss: 0.00002287
Iteration 110/1000 | Loss: 0.00002287
Iteration 111/1000 | Loss: 0.00002286
Iteration 112/1000 | Loss: 0.00002286
Iteration 113/1000 | Loss: 0.00002286
Iteration 114/1000 | Loss: 0.00002286
Iteration 115/1000 | Loss: 0.00002286
Iteration 116/1000 | Loss: 0.00002286
Iteration 117/1000 | Loss: 0.00002286
Iteration 118/1000 | Loss: 0.00002286
Iteration 119/1000 | Loss: 0.00002286
Iteration 120/1000 | Loss: 0.00002286
Iteration 121/1000 | Loss: 0.00002286
Iteration 122/1000 | Loss: 0.00002285
Iteration 123/1000 | Loss: 0.00002285
Iteration 124/1000 | Loss: 0.00002285
Iteration 125/1000 | Loss: 0.00002285
Iteration 126/1000 | Loss: 0.00002285
Iteration 127/1000 | Loss: 0.00002285
Iteration 128/1000 | Loss: 0.00002285
Iteration 129/1000 | Loss: 0.00002285
Iteration 130/1000 | Loss: 0.00002285
Iteration 131/1000 | Loss: 0.00002285
Iteration 132/1000 | Loss: 0.00002285
Iteration 133/1000 | Loss: 0.00002285
Iteration 134/1000 | Loss: 0.00002285
Iteration 135/1000 | Loss: 0.00002285
Iteration 136/1000 | Loss: 0.00002285
Iteration 137/1000 | Loss: 0.00002285
Iteration 138/1000 | Loss: 0.00002284
Iteration 139/1000 | Loss: 0.00002284
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002284
Iteration 142/1000 | Loss: 0.00002284
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002284
Iteration 145/1000 | Loss: 0.00002284
Iteration 146/1000 | Loss: 0.00002284
Iteration 147/1000 | Loss: 0.00002284
Iteration 148/1000 | Loss: 0.00002284
Iteration 149/1000 | Loss: 0.00002284
Iteration 150/1000 | Loss: 0.00002284
Iteration 151/1000 | Loss: 0.00002284
Iteration 152/1000 | Loss: 0.00002283
Iteration 153/1000 | Loss: 0.00002283
Iteration 154/1000 | Loss: 0.00002283
Iteration 155/1000 | Loss: 0.00002283
Iteration 156/1000 | Loss: 0.00002283
Iteration 157/1000 | Loss: 0.00002283
Iteration 158/1000 | Loss: 0.00002283
Iteration 159/1000 | Loss: 0.00002283
Iteration 160/1000 | Loss: 0.00002283
Iteration 161/1000 | Loss: 0.00002283
Iteration 162/1000 | Loss: 0.00002282
Iteration 163/1000 | Loss: 0.00002282
Iteration 164/1000 | Loss: 0.00002282
Iteration 165/1000 | Loss: 0.00002282
Iteration 166/1000 | Loss: 0.00002282
Iteration 167/1000 | Loss: 0.00002282
Iteration 168/1000 | Loss: 0.00002282
Iteration 169/1000 | Loss: 0.00002282
Iteration 170/1000 | Loss: 0.00002282
Iteration 171/1000 | Loss: 0.00002282
Iteration 172/1000 | Loss: 0.00002282
Iteration 173/1000 | Loss: 0.00002281
Iteration 174/1000 | Loss: 0.00002281
Iteration 175/1000 | Loss: 0.00002281
Iteration 176/1000 | Loss: 0.00002281
Iteration 177/1000 | Loss: 0.00002281
Iteration 178/1000 | Loss: 0.00002281
Iteration 179/1000 | Loss: 0.00002281
Iteration 180/1000 | Loss: 0.00002281
Iteration 181/1000 | Loss: 0.00002281
Iteration 182/1000 | Loss: 0.00002281
Iteration 183/1000 | Loss: 0.00002281
Iteration 184/1000 | Loss: 0.00002281
Iteration 185/1000 | Loss: 0.00002281
Iteration 186/1000 | Loss: 0.00002281
Iteration 187/1000 | Loss: 0.00002280
Iteration 188/1000 | Loss: 0.00002280
Iteration 189/1000 | Loss: 0.00002280
Iteration 190/1000 | Loss: 0.00002280
Iteration 191/1000 | Loss: 0.00002280
Iteration 192/1000 | Loss: 0.00002280
Iteration 193/1000 | Loss: 0.00002280
Iteration 194/1000 | Loss: 0.00002280
Iteration 195/1000 | Loss: 0.00002280
Iteration 196/1000 | Loss: 0.00002279
Iteration 197/1000 | Loss: 0.00002279
Iteration 198/1000 | Loss: 0.00002279
Iteration 199/1000 | Loss: 0.00002279
Iteration 200/1000 | Loss: 0.00002279
Iteration 201/1000 | Loss: 0.00002279
Iteration 202/1000 | Loss: 0.00002279
Iteration 203/1000 | Loss: 0.00002279
Iteration 204/1000 | Loss: 0.00002279
Iteration 205/1000 | Loss: 0.00002279
Iteration 206/1000 | Loss: 0.00002279
Iteration 207/1000 | Loss: 0.00002279
Iteration 208/1000 | Loss: 0.00002279
Iteration 209/1000 | Loss: 0.00002279
Iteration 210/1000 | Loss: 0.00002279
Iteration 211/1000 | Loss: 0.00002279
Iteration 212/1000 | Loss: 0.00002279
Iteration 213/1000 | Loss: 0.00002279
Iteration 214/1000 | Loss: 0.00002279
Iteration 215/1000 | Loss: 0.00002278
Iteration 216/1000 | Loss: 0.00002278
Iteration 217/1000 | Loss: 0.00002278
Iteration 218/1000 | Loss: 0.00002278
Iteration 219/1000 | Loss: 0.00002278
Iteration 220/1000 | Loss: 0.00002278
Iteration 221/1000 | Loss: 0.00002278
Iteration 222/1000 | Loss: 0.00002278
Iteration 223/1000 | Loss: 0.00002278
Iteration 224/1000 | Loss: 0.00002278
Iteration 225/1000 | Loss: 0.00002278
Iteration 226/1000 | Loss: 0.00002278
Iteration 227/1000 | Loss: 0.00002278
Iteration 228/1000 | Loss: 0.00002278
Iteration 229/1000 | Loss: 0.00002278
Iteration 230/1000 | Loss: 0.00002278
Iteration 231/1000 | Loss: 0.00002278
Iteration 232/1000 | Loss: 0.00002278
Iteration 233/1000 | Loss: 0.00002278
Iteration 234/1000 | Loss: 0.00002278
Iteration 235/1000 | Loss: 0.00002278
Iteration 236/1000 | Loss: 0.00002278
Iteration 237/1000 | Loss: 0.00002278
Iteration 238/1000 | Loss: 0.00002278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.2780364815844223e-05, 2.2780364815844223e-05, 2.2780364815844223e-05, 2.2780364815844223e-05, 2.2780364815844223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2780364815844223e-05

Optimization complete. Final v2v error: 3.886894941329956 mm

Highest mean error: 5.960751533508301 mm for frame 114

Lowest mean error: 3.224154472351074 mm for frame 0

Saving results

Total time: 82.26042151451111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466133
Iteration 2/25 | Loss: 0.00111470
Iteration 3/25 | Loss: 0.00098343
Iteration 4/25 | Loss: 0.00097283
Iteration 5/25 | Loss: 0.00096918
Iteration 6/25 | Loss: 0.00096830
Iteration 7/25 | Loss: 0.00096830
Iteration 8/25 | Loss: 0.00096830
Iteration 9/25 | Loss: 0.00096830
Iteration 10/25 | Loss: 0.00096830
Iteration 11/25 | Loss: 0.00096830
Iteration 12/25 | Loss: 0.00096830
Iteration 13/25 | Loss: 0.00096830
Iteration 14/25 | Loss: 0.00096830
Iteration 15/25 | Loss: 0.00096830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009682984091341496, 0.0009682984091341496, 0.0009682984091341496, 0.0009682984091341496, 0.0009682984091341496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009682984091341496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53855646
Iteration 2/25 | Loss: 0.00044456
Iteration 3/25 | Loss: 0.00044454
Iteration 4/25 | Loss: 0.00044454
Iteration 5/25 | Loss: 0.00044454
Iteration 6/25 | Loss: 0.00044454
Iteration 7/25 | Loss: 0.00044454
Iteration 8/25 | Loss: 0.00044454
Iteration 9/25 | Loss: 0.00044454
Iteration 10/25 | Loss: 0.00044454
Iteration 11/25 | Loss: 0.00044454
Iteration 12/25 | Loss: 0.00044454
Iteration 13/25 | Loss: 0.00044454
Iteration 14/25 | Loss: 0.00044454
Iteration 15/25 | Loss: 0.00044454
Iteration 16/25 | Loss: 0.00044454
Iteration 17/25 | Loss: 0.00044454
Iteration 18/25 | Loss: 0.00044454
Iteration 19/25 | Loss: 0.00044454
Iteration 20/25 | Loss: 0.00044454
Iteration 21/25 | Loss: 0.00044454
Iteration 22/25 | Loss: 0.00044454
Iteration 23/25 | Loss: 0.00044454
Iteration 24/25 | Loss: 0.00044454
Iteration 25/25 | Loss: 0.00044454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044454
Iteration 2/1000 | Loss: 0.00003095
Iteration 3/1000 | Loss: 0.00002059
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001543
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001474
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001473
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001470
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001456
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001453
Iteration 136/1000 | Loss: 0.00001453
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001452
Iteration 141/1000 | Loss: 0.00001452
Iteration 142/1000 | Loss: 0.00001452
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001452
Iteration 145/1000 | Loss: 0.00001452
Iteration 146/1000 | Loss: 0.00001452
Iteration 147/1000 | Loss: 0.00001452
Iteration 148/1000 | Loss: 0.00001452
Iteration 149/1000 | Loss: 0.00001451
Iteration 150/1000 | Loss: 0.00001451
Iteration 151/1000 | Loss: 0.00001451
Iteration 152/1000 | Loss: 0.00001451
Iteration 153/1000 | Loss: 0.00001451
Iteration 154/1000 | Loss: 0.00001451
Iteration 155/1000 | Loss: 0.00001451
Iteration 156/1000 | Loss: 0.00001451
Iteration 157/1000 | Loss: 0.00001451
Iteration 158/1000 | Loss: 0.00001451
Iteration 159/1000 | Loss: 0.00001451
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001450
Iteration 163/1000 | Loss: 0.00001450
Iteration 164/1000 | Loss: 0.00001450
Iteration 165/1000 | Loss: 0.00001450
Iteration 166/1000 | Loss: 0.00001450
Iteration 167/1000 | Loss: 0.00001450
Iteration 168/1000 | Loss: 0.00001450
Iteration 169/1000 | Loss: 0.00001450
Iteration 170/1000 | Loss: 0.00001450
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001449
Iteration 173/1000 | Loss: 0.00001449
Iteration 174/1000 | Loss: 0.00001449
Iteration 175/1000 | Loss: 0.00001449
Iteration 176/1000 | Loss: 0.00001449
Iteration 177/1000 | Loss: 0.00001449
Iteration 178/1000 | Loss: 0.00001449
Iteration 179/1000 | Loss: 0.00001449
Iteration 180/1000 | Loss: 0.00001449
Iteration 181/1000 | Loss: 0.00001448
Iteration 182/1000 | Loss: 0.00001448
Iteration 183/1000 | Loss: 0.00001448
Iteration 184/1000 | Loss: 0.00001448
Iteration 185/1000 | Loss: 0.00001448
Iteration 186/1000 | Loss: 0.00001448
Iteration 187/1000 | Loss: 0.00001448
Iteration 188/1000 | Loss: 0.00001447
Iteration 189/1000 | Loss: 0.00001447
Iteration 190/1000 | Loss: 0.00001447
Iteration 191/1000 | Loss: 0.00001447
Iteration 192/1000 | Loss: 0.00001447
Iteration 193/1000 | Loss: 0.00001447
Iteration 194/1000 | Loss: 0.00001447
Iteration 195/1000 | Loss: 0.00001447
Iteration 196/1000 | Loss: 0.00001447
Iteration 197/1000 | Loss: 0.00001446
Iteration 198/1000 | Loss: 0.00001446
Iteration 199/1000 | Loss: 0.00001446
Iteration 200/1000 | Loss: 0.00001446
Iteration 201/1000 | Loss: 0.00001446
Iteration 202/1000 | Loss: 0.00001446
Iteration 203/1000 | Loss: 0.00001446
Iteration 204/1000 | Loss: 0.00001446
Iteration 205/1000 | Loss: 0.00001446
Iteration 206/1000 | Loss: 0.00001446
Iteration 207/1000 | Loss: 0.00001446
Iteration 208/1000 | Loss: 0.00001445
Iteration 209/1000 | Loss: 0.00001445
Iteration 210/1000 | Loss: 0.00001445
Iteration 211/1000 | Loss: 0.00001445
Iteration 212/1000 | Loss: 0.00001445
Iteration 213/1000 | Loss: 0.00001445
Iteration 214/1000 | Loss: 0.00001445
Iteration 215/1000 | Loss: 0.00001445
Iteration 216/1000 | Loss: 0.00001444
Iteration 217/1000 | Loss: 0.00001444
Iteration 218/1000 | Loss: 0.00001444
Iteration 219/1000 | Loss: 0.00001444
Iteration 220/1000 | Loss: 0.00001444
Iteration 221/1000 | Loss: 0.00001444
Iteration 222/1000 | Loss: 0.00001444
Iteration 223/1000 | Loss: 0.00001444
Iteration 224/1000 | Loss: 0.00001444
Iteration 225/1000 | Loss: 0.00001444
Iteration 226/1000 | Loss: 0.00001444
Iteration 227/1000 | Loss: 0.00001444
Iteration 228/1000 | Loss: 0.00001444
Iteration 229/1000 | Loss: 0.00001443
Iteration 230/1000 | Loss: 0.00001443
Iteration 231/1000 | Loss: 0.00001443
Iteration 232/1000 | Loss: 0.00001443
Iteration 233/1000 | Loss: 0.00001443
Iteration 234/1000 | Loss: 0.00001443
Iteration 235/1000 | Loss: 0.00001443
Iteration 236/1000 | Loss: 0.00001443
Iteration 237/1000 | Loss: 0.00001443
Iteration 238/1000 | Loss: 0.00001443
Iteration 239/1000 | Loss: 0.00001443
Iteration 240/1000 | Loss: 0.00001443
Iteration 241/1000 | Loss: 0.00001443
Iteration 242/1000 | Loss: 0.00001443
Iteration 243/1000 | Loss: 0.00001443
Iteration 244/1000 | Loss: 0.00001443
Iteration 245/1000 | Loss: 0.00001443
Iteration 246/1000 | Loss: 0.00001443
Iteration 247/1000 | Loss: 0.00001443
Iteration 248/1000 | Loss: 0.00001443
Iteration 249/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.443312612536829e-05, 1.443312612536829e-05, 1.443312612536829e-05, 1.443312612536829e-05, 1.443312612536829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.443312612536829e-05

Optimization complete. Final v2v error: 3.10644793510437 mm

Highest mean error: 3.628120183944702 mm for frame 19

Lowest mean error: 2.618624448776245 mm for frame 2

Saving results

Total time: 37.650073766708374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086371
Iteration 2/25 | Loss: 0.00245507
Iteration 3/25 | Loss: 0.00178761
Iteration 4/25 | Loss: 0.00145176
Iteration 5/25 | Loss: 0.00143844
Iteration 6/25 | Loss: 0.00138831
Iteration 7/25 | Loss: 0.00125923
Iteration 8/25 | Loss: 0.00119545
Iteration 9/25 | Loss: 0.00115380
Iteration 10/25 | Loss: 0.00106020
Iteration 11/25 | Loss: 0.00102691
Iteration 12/25 | Loss: 0.00102499
Iteration 13/25 | Loss: 0.00100247
Iteration 14/25 | Loss: 0.00099462
Iteration 15/25 | Loss: 0.00099276
Iteration 16/25 | Loss: 0.00099840
Iteration 17/25 | Loss: 0.00099787
Iteration 18/25 | Loss: 0.00099572
Iteration 19/25 | Loss: 0.00099099
Iteration 20/25 | Loss: 0.00099028
Iteration 21/25 | Loss: 0.00098795
Iteration 22/25 | Loss: 0.00098805
Iteration 23/25 | Loss: 0.00098612
Iteration 24/25 | Loss: 0.00098710
Iteration 25/25 | Loss: 0.00098638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51839185
Iteration 2/25 | Loss: 0.00096156
Iteration 3/25 | Loss: 0.00084918
Iteration 4/25 | Loss: 0.00084917
Iteration 5/25 | Loss: 0.00084917
Iteration 6/25 | Loss: 0.00084917
Iteration 7/25 | Loss: 0.00084917
Iteration 8/25 | Loss: 0.00084917
Iteration 9/25 | Loss: 0.00084917
Iteration 10/25 | Loss: 0.00084917
Iteration 11/25 | Loss: 0.00084917
Iteration 12/25 | Loss: 0.00084917
Iteration 13/25 | Loss: 0.00084917
Iteration 14/25 | Loss: 0.00084917
Iteration 15/25 | Loss: 0.00084917
Iteration 16/25 | Loss: 0.00084917
Iteration 17/25 | Loss: 0.00084917
Iteration 18/25 | Loss: 0.00084917
Iteration 19/25 | Loss: 0.00084917
Iteration 20/25 | Loss: 0.00084917
Iteration 21/25 | Loss: 0.00084917
Iteration 22/25 | Loss: 0.00084917
Iteration 23/25 | Loss: 0.00084917
Iteration 24/25 | Loss: 0.00084917
Iteration 25/25 | Loss: 0.00084917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084917
Iteration 2/1000 | Loss: 0.00032164
Iteration 3/1000 | Loss: 0.00047443
Iteration 4/1000 | Loss: 0.00007954
Iteration 5/1000 | Loss: 0.00007021
Iteration 6/1000 | Loss: 0.00014563
Iteration 7/1000 | Loss: 0.00025859
Iteration 8/1000 | Loss: 0.00250480
Iteration 9/1000 | Loss: 0.00029648
Iteration 10/1000 | Loss: 0.00010401
Iteration 11/1000 | Loss: 0.00006327
Iteration 12/1000 | Loss: 0.00005757
Iteration 13/1000 | Loss: 0.00007089
Iteration 14/1000 | Loss: 0.00071913
Iteration 15/1000 | Loss: 0.00005889
Iteration 16/1000 | Loss: 0.00005057
Iteration 17/1000 | Loss: 0.00004564
Iteration 18/1000 | Loss: 0.00099186
Iteration 19/1000 | Loss: 0.00348928
Iteration 20/1000 | Loss: 0.00065133
Iteration 21/1000 | Loss: 0.00040566
Iteration 22/1000 | Loss: 0.00008825
Iteration 23/1000 | Loss: 0.00005752
Iteration 24/1000 | Loss: 0.00004382
Iteration 25/1000 | Loss: 0.00003387
Iteration 26/1000 | Loss: 0.00002832
Iteration 27/1000 | Loss: 0.00002471
Iteration 28/1000 | Loss: 0.00002301
Iteration 29/1000 | Loss: 0.00002197
Iteration 30/1000 | Loss: 0.00093969
Iteration 31/1000 | Loss: 0.00042736
Iteration 32/1000 | Loss: 0.00002335
Iteration 33/1000 | Loss: 0.00002097
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00087383
Iteration 36/1000 | Loss: 0.00051867
Iteration 37/1000 | Loss: 0.00031639
Iteration 38/1000 | Loss: 0.00004286
Iteration 39/1000 | Loss: 0.00002584
Iteration 40/1000 | Loss: 0.00002168
Iteration 41/1000 | Loss: 0.00028710
Iteration 42/1000 | Loss: 0.00018918
Iteration 43/1000 | Loss: 0.00008379
Iteration 44/1000 | Loss: 0.00006713
Iteration 45/1000 | Loss: 0.00014516
Iteration 46/1000 | Loss: 0.00005151
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00016952
Iteration 49/1000 | Loss: 0.00008639
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00014746
Iteration 53/1000 | Loss: 0.00021817
Iteration 54/1000 | Loss: 0.00010938
Iteration 55/1000 | Loss: 0.00004636
Iteration 56/1000 | Loss: 0.00019934
Iteration 57/1000 | Loss: 0.00010384
Iteration 58/1000 | Loss: 0.00010157
Iteration 59/1000 | Loss: 0.00012216
Iteration 60/1000 | Loss: 0.00013962
Iteration 61/1000 | Loss: 0.00015456
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00027012
Iteration 64/1000 | Loss: 0.00003187
Iteration 65/1000 | Loss: 0.00002291
Iteration 66/1000 | Loss: 0.00001969
Iteration 67/1000 | Loss: 0.00001730
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001496
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001486
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001483
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001481
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001479
Iteration 101/1000 | Loss: 0.00001479
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001477
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001478
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001469
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001468
Iteration 125/1000 | Loss: 0.00001479
Iteration 126/1000 | Loss: 0.00001479
Iteration 127/1000 | Loss: 0.00001479
Iteration 128/1000 | Loss: 0.00001479
Iteration 129/1000 | Loss: 0.00001479
Iteration 130/1000 | Loss: 0.00001479
Iteration 131/1000 | Loss: 0.00001479
Iteration 132/1000 | Loss: 0.00001473
Iteration 133/1000 | Loss: 0.00001473
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001470
Iteration 136/1000 | Loss: 0.00001470
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001466
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001461
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001461
Iteration 157/1000 | Loss: 0.00001461
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001460
Iteration 177/1000 | Loss: 0.00001460
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.45974418046535e-05, 1.45974418046535e-05, 1.45974418046535e-05, 1.45974418046535e-05, 1.45974418046535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.45974418046535e-05

Optimization complete. Final v2v error: 3.098153591156006 mm

Highest mean error: 8.549822807312012 mm for frame 47

Lowest mean error: 2.4197282791137695 mm for frame 14

Saving results

Total time: 158.9532175064087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085297
Iteration 2/25 | Loss: 0.00247039
Iteration 3/25 | Loss: 0.00163702
Iteration 4/25 | Loss: 0.00143753
Iteration 5/25 | Loss: 0.00132723
Iteration 6/25 | Loss: 0.00119562
Iteration 7/25 | Loss: 0.00119538
Iteration 8/25 | Loss: 0.00120728
Iteration 9/25 | Loss: 0.00117520
Iteration 10/25 | Loss: 0.00115011
Iteration 11/25 | Loss: 0.00113911
Iteration 12/25 | Loss: 0.00114136
Iteration 13/25 | Loss: 0.00114804
Iteration 14/25 | Loss: 0.00114064
Iteration 15/25 | Loss: 0.00112593
Iteration 16/25 | Loss: 0.00112490
Iteration 17/25 | Loss: 0.00112434
Iteration 18/25 | Loss: 0.00112407
Iteration 19/25 | Loss: 0.00112384
Iteration 20/25 | Loss: 0.00112367
Iteration 21/25 | Loss: 0.00112353
Iteration 22/25 | Loss: 0.00112348
Iteration 23/25 | Loss: 0.00112348
Iteration 24/25 | Loss: 0.00112348
Iteration 25/25 | Loss: 0.00112348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46571577
Iteration 2/25 | Loss: 0.00165068
Iteration 3/25 | Loss: 0.00151532
Iteration 4/25 | Loss: 0.00151532
Iteration 5/25 | Loss: 0.00151531
Iteration 6/25 | Loss: 0.00151531
Iteration 7/25 | Loss: 0.00151531
Iteration 8/25 | Loss: 0.00151531
Iteration 9/25 | Loss: 0.00151531
Iteration 10/25 | Loss: 0.00151531
Iteration 11/25 | Loss: 0.00151531
Iteration 12/25 | Loss: 0.00151531
Iteration 13/25 | Loss: 0.00151531
Iteration 14/25 | Loss: 0.00151531
Iteration 15/25 | Loss: 0.00151531
Iteration 16/25 | Loss: 0.00151531
Iteration 17/25 | Loss: 0.00151531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015153115382418036, 0.0015153115382418036, 0.0015153115382418036, 0.0015153115382418036, 0.0015153115382418036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015153115382418036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151531
Iteration 2/1000 | Loss: 0.00034623
Iteration 3/1000 | Loss: 0.00029987
Iteration 4/1000 | Loss: 0.00013110
Iteration 5/1000 | Loss: 0.00048514
Iteration 6/1000 | Loss: 0.00013947
Iteration 7/1000 | Loss: 0.00063064
Iteration 8/1000 | Loss: 0.00008271
Iteration 9/1000 | Loss: 0.00012067
Iteration 10/1000 | Loss: 0.00048674
Iteration 11/1000 | Loss: 0.00066596
Iteration 12/1000 | Loss: 0.00040740
Iteration 13/1000 | Loss: 0.00041522
Iteration 14/1000 | Loss: 0.00052369
Iteration 15/1000 | Loss: 0.00011126
Iteration 16/1000 | Loss: 0.00008946
Iteration 17/1000 | Loss: 0.00016939
Iteration 18/1000 | Loss: 0.00007889
Iteration 19/1000 | Loss: 0.00013845
Iteration 20/1000 | Loss: 0.00006631
Iteration 21/1000 | Loss: 0.00030334
Iteration 22/1000 | Loss: 0.00006310
Iteration 23/1000 | Loss: 0.00010311
Iteration 24/1000 | Loss: 0.00067304
Iteration 25/1000 | Loss: 0.00046493
Iteration 26/1000 | Loss: 0.00011791
Iteration 27/1000 | Loss: 0.00007513
Iteration 28/1000 | Loss: 0.00007381
Iteration 29/1000 | Loss: 0.00010185
Iteration 30/1000 | Loss: 0.00006078
Iteration 31/1000 | Loss: 0.00010026
Iteration 32/1000 | Loss: 0.00005366
Iteration 33/1000 | Loss: 0.00012567
Iteration 34/1000 | Loss: 0.00434594
Iteration 35/1000 | Loss: 0.00031538
Iteration 36/1000 | Loss: 0.00013911
Iteration 37/1000 | Loss: 0.00012107
Iteration 38/1000 | Loss: 0.00022789
Iteration 39/1000 | Loss: 0.00010604
Iteration 40/1000 | Loss: 0.00006506
Iteration 41/1000 | Loss: 0.00006893
Iteration 42/1000 | Loss: 0.00003800
Iteration 43/1000 | Loss: 0.00005726
Iteration 44/1000 | Loss: 0.00002763
Iteration 45/1000 | Loss: 0.00008150
Iteration 46/1000 | Loss: 0.00003405
Iteration 47/1000 | Loss: 0.00002452
Iteration 48/1000 | Loss: 0.00012739
Iteration 49/1000 | Loss: 0.00003280
Iteration 50/1000 | Loss: 0.00002185
Iteration 51/1000 | Loss: 0.00007261
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00004303
Iteration 54/1000 | Loss: 0.00002660
Iteration 55/1000 | Loss: 0.00002370
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001882
Iteration 59/1000 | Loss: 0.00033119
Iteration 60/1000 | Loss: 0.00010976
Iteration 61/1000 | Loss: 0.00007102
Iteration 62/1000 | Loss: 0.00003641
Iteration 63/1000 | Loss: 0.00007779
Iteration 64/1000 | Loss: 0.00004070
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00005501
Iteration 67/1000 | Loss: 0.00011091
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00010048
Iteration 70/1000 | Loss: 0.00002292
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00005661
Iteration 73/1000 | Loss: 0.00002557
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00005482
Iteration 76/1000 | Loss: 0.00002945
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00004525
Iteration 82/1000 | Loss: 0.00001495
Iteration 83/1000 | Loss: 0.00002905
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001486
Iteration 96/1000 | Loss: 0.00001486
Iteration 97/1000 | Loss: 0.00003785
Iteration 98/1000 | Loss: 0.00012092
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00002745
Iteration 102/1000 | Loss: 0.00002622
Iteration 103/1000 | Loss: 0.00005295
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001886
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00002491
Iteration 108/1000 | Loss: 0.00002819
Iteration 109/1000 | Loss: 0.00003106
Iteration 110/1000 | Loss: 0.00003250
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001935
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002797
Iteration 115/1000 | Loss: 0.00002871
Iteration 116/1000 | Loss: 0.00003480
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00002100
Iteration 119/1000 | Loss: 0.00001492
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00002662
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001491
Iteration 132/1000 | Loss: 0.00001491
Iteration 133/1000 | Loss: 0.00001491
Iteration 134/1000 | Loss: 0.00001491
Iteration 135/1000 | Loss: 0.00001491
Iteration 136/1000 | Loss: 0.00001491
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001491
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001491
Iteration 142/1000 | Loss: 0.00001491
Iteration 143/1000 | Loss: 0.00001491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4911897778802086e-05, 1.4911897778802086e-05, 1.4911897778802086e-05, 1.4911897778802086e-05, 1.4911897778802086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4911897778802086e-05

Optimization complete. Final v2v error: 3.2588119506835938 mm

Highest mean error: 3.8504247665405273 mm for frame 85

Lowest mean error: 2.6908624172210693 mm for frame 7

Saving results

Total time: 178.77971506118774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805598
Iteration 2/25 | Loss: 0.00168895
Iteration 3/25 | Loss: 0.00132146
Iteration 4/25 | Loss: 0.00123771
Iteration 5/25 | Loss: 0.00121517
Iteration 6/25 | Loss: 0.00121014
Iteration 7/25 | Loss: 0.00120855
Iteration 8/25 | Loss: 0.00120849
Iteration 9/25 | Loss: 0.00120849
Iteration 10/25 | Loss: 0.00120849
Iteration 11/25 | Loss: 0.00120849
Iteration 12/25 | Loss: 0.00120849
Iteration 13/25 | Loss: 0.00120849
Iteration 14/25 | Loss: 0.00120849
Iteration 15/25 | Loss: 0.00120849
Iteration 16/25 | Loss: 0.00120849
Iteration 17/25 | Loss: 0.00120849
Iteration 18/25 | Loss: 0.00120849
Iteration 19/25 | Loss: 0.00120849
Iteration 20/25 | Loss: 0.00120849
Iteration 21/25 | Loss: 0.00120849
Iteration 22/25 | Loss: 0.00120849
Iteration 23/25 | Loss: 0.00120849
Iteration 24/25 | Loss: 0.00120849
Iteration 25/25 | Loss: 0.00120849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25749111
Iteration 2/25 | Loss: 0.00085028
Iteration 3/25 | Loss: 0.00085025
Iteration 4/25 | Loss: 0.00085025
Iteration 5/25 | Loss: 0.00085025
Iteration 6/25 | Loss: 0.00085025
Iteration 7/25 | Loss: 0.00085025
Iteration 8/25 | Loss: 0.00085025
Iteration 9/25 | Loss: 0.00085025
Iteration 10/25 | Loss: 0.00085025
Iteration 11/25 | Loss: 0.00085025
Iteration 12/25 | Loss: 0.00085025
Iteration 13/25 | Loss: 0.00085025
Iteration 14/25 | Loss: 0.00085025
Iteration 15/25 | Loss: 0.00085025
Iteration 16/25 | Loss: 0.00085025
Iteration 17/25 | Loss: 0.00085025
Iteration 18/25 | Loss: 0.00085025
Iteration 19/25 | Loss: 0.00085025
Iteration 20/25 | Loss: 0.00085025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008502464625053108, 0.0008502464625053108, 0.0008502464625053108, 0.0008502464625053108, 0.0008502464625053108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008502464625053108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085025
Iteration 2/1000 | Loss: 0.00010281
Iteration 3/1000 | Loss: 0.00005868
Iteration 4/1000 | Loss: 0.00004636
Iteration 5/1000 | Loss: 0.00004264
Iteration 6/1000 | Loss: 0.00004069
Iteration 7/1000 | Loss: 0.00003960
Iteration 8/1000 | Loss: 0.00003872
Iteration 9/1000 | Loss: 0.00003783
Iteration 10/1000 | Loss: 0.00003722
Iteration 11/1000 | Loss: 0.00003676
Iteration 12/1000 | Loss: 0.00003647
Iteration 13/1000 | Loss: 0.00003615
Iteration 14/1000 | Loss: 0.00003591
Iteration 15/1000 | Loss: 0.00003571
Iteration 16/1000 | Loss: 0.00003555
Iteration 17/1000 | Loss: 0.00003545
Iteration 18/1000 | Loss: 0.00003537
Iteration 19/1000 | Loss: 0.00003537
Iteration 20/1000 | Loss: 0.00003526
Iteration 21/1000 | Loss: 0.00003519
Iteration 22/1000 | Loss: 0.00003513
Iteration 23/1000 | Loss: 0.00003509
Iteration 24/1000 | Loss: 0.00003508
Iteration 25/1000 | Loss: 0.00003506
Iteration 26/1000 | Loss: 0.00003506
Iteration 27/1000 | Loss: 0.00003506
Iteration 28/1000 | Loss: 0.00003506
Iteration 29/1000 | Loss: 0.00003506
Iteration 30/1000 | Loss: 0.00003506
Iteration 31/1000 | Loss: 0.00003506
Iteration 32/1000 | Loss: 0.00003506
Iteration 33/1000 | Loss: 0.00003505
Iteration 34/1000 | Loss: 0.00003505
Iteration 35/1000 | Loss: 0.00003505
Iteration 36/1000 | Loss: 0.00003505
Iteration 37/1000 | Loss: 0.00003505
Iteration 38/1000 | Loss: 0.00003505
Iteration 39/1000 | Loss: 0.00003505
Iteration 40/1000 | Loss: 0.00003505
Iteration 41/1000 | Loss: 0.00003505
Iteration 42/1000 | Loss: 0.00003505
Iteration 43/1000 | Loss: 0.00003504
Iteration 44/1000 | Loss: 0.00003504
Iteration 45/1000 | Loss: 0.00003504
Iteration 46/1000 | Loss: 0.00003504
Iteration 47/1000 | Loss: 0.00003503
Iteration 48/1000 | Loss: 0.00003503
Iteration 49/1000 | Loss: 0.00003503
Iteration 50/1000 | Loss: 0.00003503
Iteration 51/1000 | Loss: 0.00003502
Iteration 52/1000 | Loss: 0.00003502
Iteration 53/1000 | Loss: 0.00003502
Iteration 54/1000 | Loss: 0.00003502
Iteration 55/1000 | Loss: 0.00003502
Iteration 56/1000 | Loss: 0.00003501
Iteration 57/1000 | Loss: 0.00003501
Iteration 58/1000 | Loss: 0.00003501
Iteration 59/1000 | Loss: 0.00003501
Iteration 60/1000 | Loss: 0.00003501
Iteration 61/1000 | Loss: 0.00003501
Iteration 62/1000 | Loss: 0.00003500
Iteration 63/1000 | Loss: 0.00003500
Iteration 64/1000 | Loss: 0.00003500
Iteration 65/1000 | Loss: 0.00003500
Iteration 66/1000 | Loss: 0.00003500
Iteration 67/1000 | Loss: 0.00003500
Iteration 68/1000 | Loss: 0.00003499
Iteration 69/1000 | Loss: 0.00003499
Iteration 70/1000 | Loss: 0.00003499
Iteration 71/1000 | Loss: 0.00003499
Iteration 72/1000 | Loss: 0.00003499
Iteration 73/1000 | Loss: 0.00003499
Iteration 74/1000 | Loss: 0.00003499
Iteration 75/1000 | Loss: 0.00003499
Iteration 76/1000 | Loss: 0.00003499
Iteration 77/1000 | Loss: 0.00003499
Iteration 78/1000 | Loss: 0.00003499
Iteration 79/1000 | Loss: 0.00003499
Iteration 80/1000 | Loss: 0.00003499
Iteration 81/1000 | Loss: 0.00003499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [3.499068407109007e-05, 3.499068407109007e-05, 3.499068407109007e-05, 3.499068407109007e-05, 3.499068407109007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.499068407109007e-05

Optimization complete. Final v2v error: 4.812847137451172 mm

Highest mean error: 6.497402191162109 mm for frame 54

Lowest mean error: 3.2649118900299072 mm for frame 222

Saving results

Total time: 50.70379662513733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400570
Iteration 2/25 | Loss: 0.00119688
Iteration 3/25 | Loss: 0.00101155
Iteration 4/25 | Loss: 0.00099445
Iteration 5/25 | Loss: 0.00099011
Iteration 6/25 | Loss: 0.00098958
Iteration 7/25 | Loss: 0.00098958
Iteration 8/25 | Loss: 0.00098958
Iteration 9/25 | Loss: 0.00098958
Iteration 10/25 | Loss: 0.00098958
Iteration 11/25 | Loss: 0.00098958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009895757539197803, 0.0009895757539197803, 0.0009895757539197803, 0.0009895757539197803, 0.0009895757539197803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009895757539197803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70370483
Iteration 2/25 | Loss: 0.00041109
Iteration 3/25 | Loss: 0.00041109
Iteration 4/25 | Loss: 0.00041109
Iteration 5/25 | Loss: 0.00041109
Iteration 6/25 | Loss: 0.00041109
Iteration 7/25 | Loss: 0.00041109
Iteration 8/25 | Loss: 0.00041109
Iteration 9/25 | Loss: 0.00041109
Iteration 10/25 | Loss: 0.00041109
Iteration 11/25 | Loss: 0.00041109
Iteration 12/25 | Loss: 0.00041109
Iteration 13/25 | Loss: 0.00041109
Iteration 14/25 | Loss: 0.00041109
Iteration 15/25 | Loss: 0.00041109
Iteration 16/25 | Loss: 0.00041109
Iteration 17/25 | Loss: 0.00041109
Iteration 18/25 | Loss: 0.00041109
Iteration 19/25 | Loss: 0.00041109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004110859299544245, 0.0004110859299544245, 0.0004110859299544245, 0.0004110859299544245, 0.0004110859299544245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004110859299544245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041109
Iteration 2/1000 | Loss: 0.00002871
Iteration 3/1000 | Loss: 0.00001543
Iteration 4/1000 | Loss: 0.00001307
Iteration 5/1000 | Loss: 0.00001178
Iteration 6/1000 | Loss: 0.00001115
Iteration 7/1000 | Loss: 0.00001075
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001055
Iteration 10/1000 | Loss: 0.00001034
Iteration 11/1000 | Loss: 0.00001030
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00001003
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000999
Iteration 19/1000 | Loss: 0.00000998
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000998
Iteration 22/1000 | Loss: 0.00000997
Iteration 23/1000 | Loss: 0.00000997
Iteration 24/1000 | Loss: 0.00000996
Iteration 25/1000 | Loss: 0.00000994
Iteration 26/1000 | Loss: 0.00000994
Iteration 27/1000 | Loss: 0.00000993
Iteration 28/1000 | Loss: 0.00000993
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000992
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000991
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000989
Iteration 38/1000 | Loss: 0.00000989
Iteration 39/1000 | Loss: 0.00000989
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000988
Iteration 44/1000 | Loss: 0.00000988
Iteration 45/1000 | Loss: 0.00000988
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000988
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000985
Iteration 62/1000 | Loss: 0.00000985
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Iteration 65/1000 | Loss: 0.00000984
Iteration 66/1000 | Loss: 0.00000984
Iteration 67/1000 | Loss: 0.00000984
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000984
Iteration 71/1000 | Loss: 0.00000984
Iteration 72/1000 | Loss: 0.00000984
Iteration 73/1000 | Loss: 0.00000984
Iteration 74/1000 | Loss: 0.00000984
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000984
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000984
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [9.842727195064072e-06, 9.842727195064072e-06, 9.842727195064072e-06, 9.842727195064072e-06, 9.842727195064072e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.842727195064072e-06

Optimization complete. Final v2v error: 2.7327218055725098 mm

Highest mean error: 3.0502755641937256 mm for frame 52

Lowest mean error: 2.469533920288086 mm for frame 202

Saving results

Total time: 32.499552726745605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121837
Iteration 2/25 | Loss: 0.01121837
Iteration 3/25 | Loss: 0.01121836
Iteration 4/25 | Loss: 0.01121836
Iteration 5/25 | Loss: 0.01121836
Iteration 6/25 | Loss: 0.01121836
Iteration 7/25 | Loss: 0.01121836
Iteration 8/25 | Loss: 0.01121836
Iteration 9/25 | Loss: 0.01121836
Iteration 10/25 | Loss: 0.01121836
Iteration 11/25 | Loss: 0.01121836
Iteration 12/25 | Loss: 0.01121835
Iteration 13/25 | Loss: 0.01121835
Iteration 14/25 | Loss: 0.01121835
Iteration 15/25 | Loss: 0.01121835
Iteration 16/25 | Loss: 0.01121835
Iteration 17/25 | Loss: 0.01121835
Iteration 18/25 | Loss: 0.01121835
Iteration 19/25 | Loss: 0.01121834
Iteration 20/25 | Loss: 0.01121834
Iteration 21/25 | Loss: 0.01121834
Iteration 22/25 | Loss: 0.01121834
Iteration 23/25 | Loss: 0.01121834
Iteration 24/25 | Loss: 0.01121834
Iteration 25/25 | Loss: 0.01121834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54416561
Iteration 2/25 | Loss: 0.11329421
Iteration 3/25 | Loss: 0.11290754
Iteration 4/25 | Loss: 0.11290752
Iteration 5/25 | Loss: 0.11290752
Iteration 6/25 | Loss: 0.11290751
Iteration 7/25 | Loss: 0.11290751
Iteration 8/25 | Loss: 0.11290751
Iteration 9/25 | Loss: 0.11290751
Iteration 10/25 | Loss: 0.11290750
Iteration 11/25 | Loss: 0.11290750
Iteration 12/25 | Loss: 0.11290750
Iteration 13/25 | Loss: 0.11290750
Iteration 14/25 | Loss: 0.11290750
Iteration 15/25 | Loss: 0.11290750
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.11290749907493591, 0.11290749907493591, 0.11290749907493591, 0.11290749907493591, 0.11290749907493591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11290749907493591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11290750
Iteration 2/1000 | Loss: 0.00145876
Iteration 3/1000 | Loss: 0.00027627
Iteration 4/1000 | Loss: 0.00021215
Iteration 5/1000 | Loss: 0.00074393
Iteration 6/1000 | Loss: 0.00017215
Iteration 7/1000 | Loss: 0.00012447
Iteration 8/1000 | Loss: 0.00003042
Iteration 9/1000 | Loss: 0.00015980
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002259
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00002502
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001054
Iteration 19/1000 | Loss: 0.00001013
Iteration 20/1000 | Loss: 0.00000999
Iteration 21/1000 | Loss: 0.00000978
Iteration 22/1000 | Loss: 0.00000977
Iteration 23/1000 | Loss: 0.00000963
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000950
Iteration 26/1000 | Loss: 0.00000946
Iteration 27/1000 | Loss: 0.00000945
Iteration 28/1000 | Loss: 0.00000944
Iteration 29/1000 | Loss: 0.00000940
Iteration 30/1000 | Loss: 0.00000937
Iteration 31/1000 | Loss: 0.00000936
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000935
Iteration 34/1000 | Loss: 0.00000933
Iteration 35/1000 | Loss: 0.00000932
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000931
Iteration 38/1000 | Loss: 0.00000931
Iteration 39/1000 | Loss: 0.00000931
Iteration 40/1000 | Loss: 0.00000930
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000928
Iteration 43/1000 | Loss: 0.00000927
Iteration 44/1000 | Loss: 0.00000926
Iteration 45/1000 | Loss: 0.00000926
Iteration 46/1000 | Loss: 0.00000925
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000924
Iteration 51/1000 | Loss: 0.00000924
Iteration 52/1000 | Loss: 0.00000923
Iteration 53/1000 | Loss: 0.00000923
Iteration 54/1000 | Loss: 0.00000922
Iteration 55/1000 | Loss: 0.00000922
Iteration 56/1000 | Loss: 0.00000921
Iteration 57/1000 | Loss: 0.00000920
Iteration 58/1000 | Loss: 0.00000920
Iteration 59/1000 | Loss: 0.00000920
Iteration 60/1000 | Loss: 0.00000920
Iteration 61/1000 | Loss: 0.00000920
Iteration 62/1000 | Loss: 0.00000920
Iteration 63/1000 | Loss: 0.00000919
Iteration 64/1000 | Loss: 0.00000919
Iteration 65/1000 | Loss: 0.00000918
Iteration 66/1000 | Loss: 0.00000918
Iteration 67/1000 | Loss: 0.00000917
Iteration 68/1000 | Loss: 0.00000917
Iteration 69/1000 | Loss: 0.00000917
Iteration 70/1000 | Loss: 0.00000917
Iteration 71/1000 | Loss: 0.00000917
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000916
Iteration 74/1000 | Loss: 0.00000916
Iteration 75/1000 | Loss: 0.00000916
Iteration 76/1000 | Loss: 0.00000916
Iteration 77/1000 | Loss: 0.00000915
Iteration 78/1000 | Loss: 0.00000915
Iteration 79/1000 | Loss: 0.00000915
Iteration 80/1000 | Loss: 0.00000915
Iteration 81/1000 | Loss: 0.00000915
Iteration 82/1000 | Loss: 0.00000915
Iteration 83/1000 | Loss: 0.00000914
Iteration 84/1000 | Loss: 0.00000914
Iteration 85/1000 | Loss: 0.00000914
Iteration 86/1000 | Loss: 0.00000914
Iteration 87/1000 | Loss: 0.00000914
Iteration 88/1000 | Loss: 0.00000914
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000913
Iteration 91/1000 | Loss: 0.00000913
Iteration 92/1000 | Loss: 0.00000913
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000913
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000913
Iteration 97/1000 | Loss: 0.00000912
Iteration 98/1000 | Loss: 0.00000912
Iteration 99/1000 | Loss: 0.00000911
Iteration 100/1000 | Loss: 0.00000911
Iteration 101/1000 | Loss: 0.00000911
Iteration 102/1000 | Loss: 0.00000911
Iteration 103/1000 | Loss: 0.00000911
Iteration 104/1000 | Loss: 0.00000911
Iteration 105/1000 | Loss: 0.00000911
Iteration 106/1000 | Loss: 0.00000910
Iteration 107/1000 | Loss: 0.00000910
Iteration 108/1000 | Loss: 0.00000910
Iteration 109/1000 | Loss: 0.00000910
Iteration 110/1000 | Loss: 0.00000909
Iteration 111/1000 | Loss: 0.00000909
Iteration 112/1000 | Loss: 0.00000909
Iteration 113/1000 | Loss: 0.00000909
Iteration 114/1000 | Loss: 0.00000909
Iteration 115/1000 | Loss: 0.00000908
Iteration 116/1000 | Loss: 0.00000908
Iteration 117/1000 | Loss: 0.00000908
Iteration 118/1000 | Loss: 0.00000908
Iteration 119/1000 | Loss: 0.00000908
Iteration 120/1000 | Loss: 0.00000908
Iteration 121/1000 | Loss: 0.00000908
Iteration 122/1000 | Loss: 0.00000908
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000907
Iteration 125/1000 | Loss: 0.00000907
Iteration 126/1000 | Loss: 0.00000907
Iteration 127/1000 | Loss: 0.00000907
Iteration 128/1000 | Loss: 0.00000907
Iteration 129/1000 | Loss: 0.00000907
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000907
Iteration 134/1000 | Loss: 0.00000907
Iteration 135/1000 | Loss: 0.00000907
Iteration 136/1000 | Loss: 0.00000907
Iteration 137/1000 | Loss: 0.00000907
Iteration 138/1000 | Loss: 0.00000907
Iteration 139/1000 | Loss: 0.00000906
Iteration 140/1000 | Loss: 0.00000906
Iteration 141/1000 | Loss: 0.00000906
Iteration 142/1000 | Loss: 0.00000906
Iteration 143/1000 | Loss: 0.00000905
Iteration 144/1000 | Loss: 0.00000905
Iteration 145/1000 | Loss: 0.00000905
Iteration 146/1000 | Loss: 0.00000905
Iteration 147/1000 | Loss: 0.00000905
Iteration 148/1000 | Loss: 0.00000905
Iteration 149/1000 | Loss: 0.00000905
Iteration 150/1000 | Loss: 0.00000905
Iteration 151/1000 | Loss: 0.00000904
Iteration 152/1000 | Loss: 0.00000904
Iteration 153/1000 | Loss: 0.00000904
Iteration 154/1000 | Loss: 0.00000904
Iteration 155/1000 | Loss: 0.00000904
Iteration 156/1000 | Loss: 0.00000904
Iteration 157/1000 | Loss: 0.00000904
Iteration 158/1000 | Loss: 0.00000904
Iteration 159/1000 | Loss: 0.00000904
Iteration 160/1000 | Loss: 0.00000904
Iteration 161/1000 | Loss: 0.00000904
Iteration 162/1000 | Loss: 0.00000904
Iteration 163/1000 | Loss: 0.00000904
Iteration 164/1000 | Loss: 0.00000904
Iteration 165/1000 | Loss: 0.00000904
Iteration 166/1000 | Loss: 0.00000904
Iteration 167/1000 | Loss: 0.00000904
Iteration 168/1000 | Loss: 0.00000903
Iteration 169/1000 | Loss: 0.00000903
Iteration 170/1000 | Loss: 0.00000903
Iteration 171/1000 | Loss: 0.00000903
Iteration 172/1000 | Loss: 0.00000903
Iteration 173/1000 | Loss: 0.00000903
Iteration 174/1000 | Loss: 0.00000903
Iteration 175/1000 | Loss: 0.00000903
Iteration 176/1000 | Loss: 0.00000903
Iteration 177/1000 | Loss: 0.00000903
Iteration 178/1000 | Loss: 0.00000903
Iteration 179/1000 | Loss: 0.00000903
Iteration 180/1000 | Loss: 0.00000902
Iteration 181/1000 | Loss: 0.00000902
Iteration 182/1000 | Loss: 0.00000902
Iteration 183/1000 | Loss: 0.00000902
Iteration 184/1000 | Loss: 0.00000902
Iteration 185/1000 | Loss: 0.00000902
Iteration 186/1000 | Loss: 0.00000902
Iteration 187/1000 | Loss: 0.00000902
Iteration 188/1000 | Loss: 0.00000902
Iteration 189/1000 | Loss: 0.00000902
Iteration 190/1000 | Loss: 0.00000902
Iteration 191/1000 | Loss: 0.00000902
Iteration 192/1000 | Loss: 0.00000902
Iteration 193/1000 | Loss: 0.00000902
Iteration 194/1000 | Loss: 0.00000902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [9.018393939186353e-06, 9.018393939186353e-06, 9.018393939186353e-06, 9.018393939186353e-06, 9.018393939186353e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.018393939186353e-06

Optimization complete. Final v2v error: 2.6220216751098633 mm

Highest mean error: 2.911580801010132 mm for frame 97

Lowest mean error: 2.483968496322632 mm for frame 179

Saving results

Total time: 57.85609984397888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125150
Iteration 2/25 | Loss: 0.00170042
Iteration 3/25 | Loss: 0.00126949
Iteration 4/25 | Loss: 0.00123312
Iteration 5/25 | Loss: 0.00122311
Iteration 6/25 | Loss: 0.00122080
Iteration 7/25 | Loss: 0.00122080
Iteration 8/25 | Loss: 0.00122080
Iteration 9/25 | Loss: 0.00122080
Iteration 10/25 | Loss: 0.00122080
Iteration 11/25 | Loss: 0.00122080
Iteration 12/25 | Loss: 0.00122080
Iteration 13/25 | Loss: 0.00122080
Iteration 14/25 | Loss: 0.00122080
Iteration 15/25 | Loss: 0.00122080
Iteration 16/25 | Loss: 0.00122080
Iteration 17/25 | Loss: 0.00122080
Iteration 18/25 | Loss: 0.00122080
Iteration 19/25 | Loss: 0.00122080
Iteration 20/25 | Loss: 0.00122080
Iteration 21/25 | Loss: 0.00122080
Iteration 22/25 | Loss: 0.00122080
Iteration 23/25 | Loss: 0.00122080
Iteration 24/25 | Loss: 0.00122080
Iteration 25/25 | Loss: 0.00122080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43981099
Iteration 2/25 | Loss: 0.00068374
Iteration 3/25 | Loss: 0.00068374
Iteration 4/25 | Loss: 0.00068374
Iteration 5/25 | Loss: 0.00068374
Iteration 6/25 | Loss: 0.00068374
Iteration 7/25 | Loss: 0.00068374
Iteration 8/25 | Loss: 0.00068373
Iteration 9/25 | Loss: 0.00068373
Iteration 10/25 | Loss: 0.00068373
Iteration 11/25 | Loss: 0.00068373
Iteration 12/25 | Loss: 0.00068373
Iteration 13/25 | Loss: 0.00068373
Iteration 14/25 | Loss: 0.00068373
Iteration 15/25 | Loss: 0.00068373
Iteration 16/25 | Loss: 0.00068373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006837346008978784, 0.0006837346008978784, 0.0006837346008978784, 0.0006837346008978784, 0.0006837346008978784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006837346008978784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068373
Iteration 2/1000 | Loss: 0.00006570
Iteration 3/1000 | Loss: 0.00004340
Iteration 4/1000 | Loss: 0.00003975
Iteration 5/1000 | Loss: 0.00003766
Iteration 6/1000 | Loss: 0.00003669
Iteration 7/1000 | Loss: 0.00003594
Iteration 8/1000 | Loss: 0.00003551
Iteration 9/1000 | Loss: 0.00003515
Iteration 10/1000 | Loss: 0.00003489
Iteration 11/1000 | Loss: 0.00003471
Iteration 12/1000 | Loss: 0.00003459
Iteration 13/1000 | Loss: 0.00003454
Iteration 14/1000 | Loss: 0.00003452
Iteration 15/1000 | Loss: 0.00003446
Iteration 16/1000 | Loss: 0.00003446
Iteration 17/1000 | Loss: 0.00003445
Iteration 18/1000 | Loss: 0.00003444
Iteration 19/1000 | Loss: 0.00003442
Iteration 20/1000 | Loss: 0.00003442
Iteration 21/1000 | Loss: 0.00003442
Iteration 22/1000 | Loss: 0.00003441
Iteration 23/1000 | Loss: 0.00003440
Iteration 24/1000 | Loss: 0.00003439
Iteration 25/1000 | Loss: 0.00003439
Iteration 26/1000 | Loss: 0.00003438
Iteration 27/1000 | Loss: 0.00003438
Iteration 28/1000 | Loss: 0.00003438
Iteration 29/1000 | Loss: 0.00003437
Iteration 30/1000 | Loss: 0.00003436
Iteration 31/1000 | Loss: 0.00003435
Iteration 32/1000 | Loss: 0.00003432
Iteration 33/1000 | Loss: 0.00003432
Iteration 34/1000 | Loss: 0.00003430
Iteration 35/1000 | Loss: 0.00003430
Iteration 36/1000 | Loss: 0.00003429
Iteration 37/1000 | Loss: 0.00003429
Iteration 38/1000 | Loss: 0.00003428
Iteration 39/1000 | Loss: 0.00003427
Iteration 40/1000 | Loss: 0.00003427
Iteration 41/1000 | Loss: 0.00003427
Iteration 42/1000 | Loss: 0.00003427
Iteration 43/1000 | Loss: 0.00003426
Iteration 44/1000 | Loss: 0.00003426
Iteration 45/1000 | Loss: 0.00003426
Iteration 46/1000 | Loss: 0.00003424
Iteration 47/1000 | Loss: 0.00003423
Iteration 48/1000 | Loss: 0.00003423
Iteration 49/1000 | Loss: 0.00003422
Iteration 50/1000 | Loss: 0.00003422
Iteration 51/1000 | Loss: 0.00003422
Iteration 52/1000 | Loss: 0.00003422
Iteration 53/1000 | Loss: 0.00003422
Iteration 54/1000 | Loss: 0.00003422
Iteration 55/1000 | Loss: 0.00003422
Iteration 56/1000 | Loss: 0.00003422
Iteration 57/1000 | Loss: 0.00003422
Iteration 58/1000 | Loss: 0.00003422
Iteration 59/1000 | Loss: 0.00003422
Iteration 60/1000 | Loss: 0.00003421
Iteration 61/1000 | Loss: 0.00003421
Iteration 62/1000 | Loss: 0.00003421
Iteration 63/1000 | Loss: 0.00003421
Iteration 64/1000 | Loss: 0.00003420
Iteration 65/1000 | Loss: 0.00003420
Iteration 66/1000 | Loss: 0.00003420
Iteration 67/1000 | Loss: 0.00003420
Iteration 68/1000 | Loss: 0.00003420
Iteration 69/1000 | Loss: 0.00003420
Iteration 70/1000 | Loss: 0.00003419
Iteration 71/1000 | Loss: 0.00003419
Iteration 72/1000 | Loss: 0.00003419
Iteration 73/1000 | Loss: 0.00003419
Iteration 74/1000 | Loss: 0.00003419
Iteration 75/1000 | Loss: 0.00003419
Iteration 76/1000 | Loss: 0.00003419
Iteration 77/1000 | Loss: 0.00003419
Iteration 78/1000 | Loss: 0.00003419
Iteration 79/1000 | Loss: 0.00003419
Iteration 80/1000 | Loss: 0.00003419
Iteration 81/1000 | Loss: 0.00003419
Iteration 82/1000 | Loss: 0.00003419
Iteration 83/1000 | Loss: 0.00003418
Iteration 84/1000 | Loss: 0.00003418
Iteration 85/1000 | Loss: 0.00003418
Iteration 86/1000 | Loss: 0.00003418
Iteration 87/1000 | Loss: 0.00003418
Iteration 88/1000 | Loss: 0.00003417
Iteration 89/1000 | Loss: 0.00003417
Iteration 90/1000 | Loss: 0.00003417
Iteration 91/1000 | Loss: 0.00003417
Iteration 92/1000 | Loss: 0.00003416
Iteration 93/1000 | Loss: 0.00003416
Iteration 94/1000 | Loss: 0.00003416
Iteration 95/1000 | Loss: 0.00003416
Iteration 96/1000 | Loss: 0.00003416
Iteration 97/1000 | Loss: 0.00003416
Iteration 98/1000 | Loss: 0.00003416
Iteration 99/1000 | Loss: 0.00003416
Iteration 100/1000 | Loss: 0.00003416
Iteration 101/1000 | Loss: 0.00003416
Iteration 102/1000 | Loss: 0.00003416
Iteration 103/1000 | Loss: 0.00003416
Iteration 104/1000 | Loss: 0.00003415
Iteration 105/1000 | Loss: 0.00003415
Iteration 106/1000 | Loss: 0.00003415
Iteration 107/1000 | Loss: 0.00003415
Iteration 108/1000 | Loss: 0.00003415
Iteration 109/1000 | Loss: 0.00003414
Iteration 110/1000 | Loss: 0.00003414
Iteration 111/1000 | Loss: 0.00003414
Iteration 112/1000 | Loss: 0.00003414
Iteration 113/1000 | Loss: 0.00003414
Iteration 114/1000 | Loss: 0.00003414
Iteration 115/1000 | Loss: 0.00003414
Iteration 116/1000 | Loss: 0.00003414
Iteration 117/1000 | Loss: 0.00003414
Iteration 118/1000 | Loss: 0.00003414
Iteration 119/1000 | Loss: 0.00003414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [3.4137305192416534e-05, 3.4137305192416534e-05, 3.4137305192416534e-05, 3.4137305192416534e-05, 3.4137305192416534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4137305192416534e-05

Optimization complete. Final v2v error: 4.707335472106934 mm

Highest mean error: 5.194443225860596 mm for frame 46

Lowest mean error: 3.8866288661956787 mm for frame 0

Saving results

Total time: 40.2783362865448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119508
Iteration 2/25 | Loss: 0.01119508
Iteration 3/25 | Loss: 0.01119508
Iteration 4/25 | Loss: 0.01119508
Iteration 5/25 | Loss: 0.01119508
Iteration 6/25 | Loss: 0.00203581
Iteration 7/25 | Loss: 0.00127803
Iteration 8/25 | Loss: 0.00117804
Iteration 9/25 | Loss: 0.00107339
Iteration 10/25 | Loss: 0.00106108
Iteration 11/25 | Loss: 0.00102088
Iteration 12/25 | Loss: 0.00100666
Iteration 13/25 | Loss: 0.00098055
Iteration 14/25 | Loss: 0.00096759
Iteration 15/25 | Loss: 0.00096261
Iteration 16/25 | Loss: 0.00095918
Iteration 17/25 | Loss: 0.00095464
Iteration 18/25 | Loss: 0.00096096
Iteration 19/25 | Loss: 0.00095823
Iteration 20/25 | Loss: 0.00095931
Iteration 21/25 | Loss: 0.00095050
Iteration 22/25 | Loss: 0.00094541
Iteration 23/25 | Loss: 0.00094488
Iteration 24/25 | Loss: 0.00093990
Iteration 25/25 | Loss: 0.00094083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57860994
Iteration 2/25 | Loss: 0.00069247
Iteration 3/25 | Loss: 0.00067372
Iteration 4/25 | Loss: 0.00067372
Iteration 5/25 | Loss: 0.00067372
Iteration 6/25 | Loss: 0.00067371
Iteration 7/25 | Loss: 0.00067371
Iteration 8/25 | Loss: 0.00067371
Iteration 9/25 | Loss: 0.00067371
Iteration 10/25 | Loss: 0.00067371
Iteration 11/25 | Loss: 0.00067371
Iteration 12/25 | Loss: 0.00067371
Iteration 13/25 | Loss: 0.00067371
Iteration 14/25 | Loss: 0.00067371
Iteration 15/25 | Loss: 0.00067371
Iteration 16/25 | Loss: 0.00067371
Iteration 17/25 | Loss: 0.00067371
Iteration 18/25 | Loss: 0.00067371
Iteration 19/25 | Loss: 0.00067371
Iteration 20/25 | Loss: 0.00067371
Iteration 21/25 | Loss: 0.00067371
Iteration 22/25 | Loss: 0.00067371
Iteration 23/25 | Loss: 0.00067371
Iteration 24/25 | Loss: 0.00067371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006737125222571194, 0.0006737125222571194, 0.0006737125222571194, 0.0006737125222571194, 0.0006737125222571194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006737125222571194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067371
Iteration 2/1000 | Loss: 0.00022948
Iteration 3/1000 | Loss: 0.00023248
Iteration 4/1000 | Loss: 0.00016221
Iteration 5/1000 | Loss: 0.00002807
Iteration 6/1000 | Loss: 0.00007331
Iteration 7/1000 | Loss: 0.00005504
Iteration 8/1000 | Loss: 0.00014148
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00004974
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00003889
Iteration 14/1000 | Loss: 0.00001648
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001556
Iteration 18/1000 | Loss: 0.00001555
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001963
Iteration 22/1000 | Loss: 0.00005932
Iteration 23/1000 | Loss: 0.00002312
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001653
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001468
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001388
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001358
Iteration 40/1000 | Loss: 0.00001357
Iteration 41/1000 | Loss: 0.00001356
Iteration 42/1000 | Loss: 0.00001355
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.3234277503215708e-05, 1.3234277503215708e-05, 1.3234277503215708e-05, 1.3234277503215708e-05, 1.3234277503215708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3234277503215708e-05

Optimization complete. Final v2v error: 2.881638526916504 mm

Highest mean error: 11.359318733215332 mm for frame 203

Lowest mean error: 2.392728567123413 mm for frame 6

Saving results

Total time: 106.23527002334595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835086
Iteration 2/25 | Loss: 0.00108628
Iteration 3/25 | Loss: 0.00095682
Iteration 4/25 | Loss: 0.00094801
Iteration 5/25 | Loss: 0.00094478
Iteration 6/25 | Loss: 0.00094409
Iteration 7/25 | Loss: 0.00094409
Iteration 8/25 | Loss: 0.00094409
Iteration 9/25 | Loss: 0.00094409
Iteration 10/25 | Loss: 0.00094409
Iteration 11/25 | Loss: 0.00094409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009440908906981349, 0.0009440908906981349, 0.0009440908906981349, 0.0009440908906981349, 0.0009440908906981349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009440908906981349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43155062
Iteration 2/25 | Loss: 0.00048028
Iteration 3/25 | Loss: 0.00048027
Iteration 4/25 | Loss: 0.00048027
Iteration 5/25 | Loss: 0.00048027
Iteration 6/25 | Loss: 0.00048027
Iteration 7/25 | Loss: 0.00048027
Iteration 8/25 | Loss: 0.00048027
Iteration 9/25 | Loss: 0.00048027
Iteration 10/25 | Loss: 0.00048027
Iteration 11/25 | Loss: 0.00048027
Iteration 12/25 | Loss: 0.00048027
Iteration 13/25 | Loss: 0.00048027
Iteration 14/25 | Loss: 0.00048027
Iteration 15/25 | Loss: 0.00048027
Iteration 16/25 | Loss: 0.00048027
Iteration 17/25 | Loss: 0.00048027
Iteration 18/25 | Loss: 0.00048027
Iteration 19/25 | Loss: 0.00048027
Iteration 20/25 | Loss: 0.00048027
Iteration 21/25 | Loss: 0.00048027
Iteration 22/25 | Loss: 0.00048027
Iteration 23/25 | Loss: 0.00048027
Iteration 24/25 | Loss: 0.00048027
Iteration 25/25 | Loss: 0.00048027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048027
Iteration 2/1000 | Loss: 0.00002940
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001254
Iteration 5/1000 | Loss: 0.00001159
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001034
Iteration 8/1000 | Loss: 0.00001010
Iteration 9/1000 | Loss: 0.00000998
Iteration 10/1000 | Loss: 0.00000996
Iteration 11/1000 | Loss: 0.00000993
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000968
Iteration 14/1000 | Loss: 0.00000959
Iteration 15/1000 | Loss: 0.00000958
Iteration 16/1000 | Loss: 0.00000958
Iteration 17/1000 | Loss: 0.00000957
Iteration 18/1000 | Loss: 0.00000956
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000955
Iteration 21/1000 | Loss: 0.00000954
Iteration 22/1000 | Loss: 0.00000954
Iteration 23/1000 | Loss: 0.00000953
Iteration 24/1000 | Loss: 0.00000952
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000951
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000950
Iteration 29/1000 | Loss: 0.00000949
Iteration 30/1000 | Loss: 0.00000949
Iteration 31/1000 | Loss: 0.00000948
Iteration 32/1000 | Loss: 0.00000948
Iteration 33/1000 | Loss: 0.00000948
Iteration 34/1000 | Loss: 0.00000948
Iteration 35/1000 | Loss: 0.00000947
Iteration 36/1000 | Loss: 0.00000947
Iteration 37/1000 | Loss: 0.00000946
Iteration 38/1000 | Loss: 0.00000945
Iteration 39/1000 | Loss: 0.00000945
Iteration 40/1000 | Loss: 0.00000945
Iteration 41/1000 | Loss: 0.00000943
Iteration 42/1000 | Loss: 0.00000942
Iteration 43/1000 | Loss: 0.00000942
Iteration 44/1000 | Loss: 0.00000941
Iteration 45/1000 | Loss: 0.00000941
Iteration 46/1000 | Loss: 0.00000939
Iteration 47/1000 | Loss: 0.00000938
Iteration 48/1000 | Loss: 0.00000938
Iteration 49/1000 | Loss: 0.00000938
Iteration 50/1000 | Loss: 0.00000937
Iteration 51/1000 | Loss: 0.00000937
Iteration 52/1000 | Loss: 0.00000937
Iteration 53/1000 | Loss: 0.00000935
Iteration 54/1000 | Loss: 0.00000935
Iteration 55/1000 | Loss: 0.00000935
Iteration 56/1000 | Loss: 0.00000934
Iteration 57/1000 | Loss: 0.00000934
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000933
Iteration 60/1000 | Loss: 0.00000933
Iteration 61/1000 | Loss: 0.00000932
Iteration 62/1000 | Loss: 0.00000932
Iteration 63/1000 | Loss: 0.00000932
Iteration 64/1000 | Loss: 0.00000931
Iteration 65/1000 | Loss: 0.00000931
Iteration 66/1000 | Loss: 0.00000931
Iteration 67/1000 | Loss: 0.00000931
Iteration 68/1000 | Loss: 0.00000931
Iteration 69/1000 | Loss: 0.00000931
Iteration 70/1000 | Loss: 0.00000930
Iteration 71/1000 | Loss: 0.00000930
Iteration 72/1000 | Loss: 0.00000929
Iteration 73/1000 | Loss: 0.00000929
Iteration 74/1000 | Loss: 0.00000929
Iteration 75/1000 | Loss: 0.00000929
Iteration 76/1000 | Loss: 0.00000929
Iteration 77/1000 | Loss: 0.00000928
Iteration 78/1000 | Loss: 0.00000928
Iteration 79/1000 | Loss: 0.00000928
Iteration 80/1000 | Loss: 0.00000928
Iteration 81/1000 | Loss: 0.00000928
Iteration 82/1000 | Loss: 0.00000928
Iteration 83/1000 | Loss: 0.00000928
Iteration 84/1000 | Loss: 0.00000928
Iteration 85/1000 | Loss: 0.00000927
Iteration 86/1000 | Loss: 0.00000927
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000926
Iteration 91/1000 | Loss: 0.00000925
Iteration 92/1000 | Loss: 0.00000925
Iteration 93/1000 | Loss: 0.00000925
Iteration 94/1000 | Loss: 0.00000925
Iteration 95/1000 | Loss: 0.00000925
Iteration 96/1000 | Loss: 0.00000924
Iteration 97/1000 | Loss: 0.00000924
Iteration 98/1000 | Loss: 0.00000924
Iteration 99/1000 | Loss: 0.00000923
Iteration 100/1000 | Loss: 0.00000922
Iteration 101/1000 | Loss: 0.00000922
Iteration 102/1000 | Loss: 0.00000922
Iteration 103/1000 | Loss: 0.00000922
Iteration 104/1000 | Loss: 0.00000922
Iteration 105/1000 | Loss: 0.00000921
Iteration 106/1000 | Loss: 0.00000921
Iteration 107/1000 | Loss: 0.00000921
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000921
Iteration 111/1000 | Loss: 0.00000921
Iteration 112/1000 | Loss: 0.00000920
Iteration 113/1000 | Loss: 0.00000920
Iteration 114/1000 | Loss: 0.00000920
Iteration 115/1000 | Loss: 0.00000920
Iteration 116/1000 | Loss: 0.00000920
Iteration 117/1000 | Loss: 0.00000920
Iteration 118/1000 | Loss: 0.00000920
Iteration 119/1000 | Loss: 0.00000920
Iteration 120/1000 | Loss: 0.00000919
Iteration 121/1000 | Loss: 0.00000919
Iteration 122/1000 | Loss: 0.00000919
Iteration 123/1000 | Loss: 0.00000919
Iteration 124/1000 | Loss: 0.00000919
Iteration 125/1000 | Loss: 0.00000919
Iteration 126/1000 | Loss: 0.00000919
Iteration 127/1000 | Loss: 0.00000918
Iteration 128/1000 | Loss: 0.00000918
Iteration 129/1000 | Loss: 0.00000918
Iteration 130/1000 | Loss: 0.00000918
Iteration 131/1000 | Loss: 0.00000918
Iteration 132/1000 | Loss: 0.00000918
Iteration 133/1000 | Loss: 0.00000918
Iteration 134/1000 | Loss: 0.00000918
Iteration 135/1000 | Loss: 0.00000918
Iteration 136/1000 | Loss: 0.00000917
Iteration 137/1000 | Loss: 0.00000917
Iteration 138/1000 | Loss: 0.00000917
Iteration 139/1000 | Loss: 0.00000917
Iteration 140/1000 | Loss: 0.00000917
Iteration 141/1000 | Loss: 0.00000917
Iteration 142/1000 | Loss: 0.00000917
Iteration 143/1000 | Loss: 0.00000917
Iteration 144/1000 | Loss: 0.00000917
Iteration 145/1000 | Loss: 0.00000917
Iteration 146/1000 | Loss: 0.00000917
Iteration 147/1000 | Loss: 0.00000917
Iteration 148/1000 | Loss: 0.00000917
Iteration 149/1000 | Loss: 0.00000917
Iteration 150/1000 | Loss: 0.00000916
Iteration 151/1000 | Loss: 0.00000916
Iteration 152/1000 | Loss: 0.00000916
Iteration 153/1000 | Loss: 0.00000916
Iteration 154/1000 | Loss: 0.00000916
Iteration 155/1000 | Loss: 0.00000916
Iteration 156/1000 | Loss: 0.00000916
Iteration 157/1000 | Loss: 0.00000916
Iteration 158/1000 | Loss: 0.00000916
Iteration 159/1000 | Loss: 0.00000916
Iteration 160/1000 | Loss: 0.00000916
Iteration 161/1000 | Loss: 0.00000916
Iteration 162/1000 | Loss: 0.00000916
Iteration 163/1000 | Loss: 0.00000915
Iteration 164/1000 | Loss: 0.00000915
Iteration 165/1000 | Loss: 0.00000915
Iteration 166/1000 | Loss: 0.00000915
Iteration 167/1000 | Loss: 0.00000915
Iteration 168/1000 | Loss: 0.00000915
Iteration 169/1000 | Loss: 0.00000915
Iteration 170/1000 | Loss: 0.00000915
Iteration 171/1000 | Loss: 0.00000915
Iteration 172/1000 | Loss: 0.00000915
Iteration 173/1000 | Loss: 0.00000915
Iteration 174/1000 | Loss: 0.00000915
Iteration 175/1000 | Loss: 0.00000915
Iteration 176/1000 | Loss: 0.00000915
Iteration 177/1000 | Loss: 0.00000915
Iteration 178/1000 | Loss: 0.00000915
Iteration 179/1000 | Loss: 0.00000915
Iteration 180/1000 | Loss: 0.00000914
Iteration 181/1000 | Loss: 0.00000914
Iteration 182/1000 | Loss: 0.00000914
Iteration 183/1000 | Loss: 0.00000914
Iteration 184/1000 | Loss: 0.00000914
Iteration 185/1000 | Loss: 0.00000914
Iteration 186/1000 | Loss: 0.00000914
Iteration 187/1000 | Loss: 0.00000914
Iteration 188/1000 | Loss: 0.00000914
Iteration 189/1000 | Loss: 0.00000914
Iteration 190/1000 | Loss: 0.00000914
Iteration 191/1000 | Loss: 0.00000914
Iteration 192/1000 | Loss: 0.00000914
Iteration 193/1000 | Loss: 0.00000914
Iteration 194/1000 | Loss: 0.00000914
Iteration 195/1000 | Loss: 0.00000914
Iteration 196/1000 | Loss: 0.00000914
Iteration 197/1000 | Loss: 0.00000914
Iteration 198/1000 | Loss: 0.00000914
Iteration 199/1000 | Loss: 0.00000914
Iteration 200/1000 | Loss: 0.00000914
Iteration 201/1000 | Loss: 0.00000914
Iteration 202/1000 | Loss: 0.00000914
Iteration 203/1000 | Loss: 0.00000913
Iteration 204/1000 | Loss: 0.00000913
Iteration 205/1000 | Loss: 0.00000913
Iteration 206/1000 | Loss: 0.00000913
Iteration 207/1000 | Loss: 0.00000913
Iteration 208/1000 | Loss: 0.00000913
Iteration 209/1000 | Loss: 0.00000913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.134872925642412e-06, 9.134872925642412e-06, 9.134872925642412e-06, 9.134872925642412e-06, 9.134872925642412e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.134872925642412e-06

Optimization complete. Final v2v error: 2.589564800262451 mm

Highest mean error: 2.7470414638519287 mm for frame 167

Lowest mean error: 2.4435994625091553 mm for frame 119

Saving results

Total time: 39.154744148254395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898010
Iteration 2/25 | Loss: 0.00117921
Iteration 3/25 | Loss: 0.00105356
Iteration 4/25 | Loss: 0.00101919
Iteration 5/25 | Loss: 0.00101122
Iteration 6/25 | Loss: 0.00100967
Iteration 7/25 | Loss: 0.00100967
Iteration 8/25 | Loss: 0.00100967
Iteration 9/25 | Loss: 0.00100967
Iteration 10/25 | Loss: 0.00100967
Iteration 11/25 | Loss: 0.00100967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010096679907292128, 0.0010096679907292128, 0.0010096679907292128, 0.0010096679907292128, 0.0010096679907292128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010096679907292128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37102926
Iteration 2/25 | Loss: 0.00043237
Iteration 3/25 | Loss: 0.00043236
Iteration 4/25 | Loss: 0.00043236
Iteration 5/25 | Loss: 0.00043236
Iteration 6/25 | Loss: 0.00043236
Iteration 7/25 | Loss: 0.00043236
Iteration 8/25 | Loss: 0.00043236
Iteration 9/25 | Loss: 0.00043236
Iteration 10/25 | Loss: 0.00043236
Iteration 11/25 | Loss: 0.00043236
Iteration 12/25 | Loss: 0.00043236
Iteration 13/25 | Loss: 0.00043236
Iteration 14/25 | Loss: 0.00043236
Iteration 15/25 | Loss: 0.00043236
Iteration 16/25 | Loss: 0.00043236
Iteration 17/25 | Loss: 0.00043236
Iteration 18/25 | Loss: 0.00043236
Iteration 19/25 | Loss: 0.00043236
Iteration 20/25 | Loss: 0.00043236
Iteration 21/25 | Loss: 0.00043236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004323578905314207, 0.0004323578905314207, 0.0004323578905314207, 0.0004323578905314207, 0.0004323578905314207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004323578905314207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043236
Iteration 2/1000 | Loss: 0.00004466
Iteration 3/1000 | Loss: 0.00002573
Iteration 4/1000 | Loss: 0.00002244
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001940
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001830
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001777
Iteration 12/1000 | Loss: 0.00001773
Iteration 13/1000 | Loss: 0.00001770
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001744
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001742
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001739
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001738
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001737
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001732
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001731
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001727
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001725
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001721
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001721
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001721
Iteration 100/1000 | Loss: 0.00001721
Iteration 101/1000 | Loss: 0.00001721
Iteration 102/1000 | Loss: 0.00001721
Iteration 103/1000 | Loss: 0.00001721
Iteration 104/1000 | Loss: 0.00001721
Iteration 105/1000 | Loss: 0.00001721
Iteration 106/1000 | Loss: 0.00001721
Iteration 107/1000 | Loss: 0.00001721
Iteration 108/1000 | Loss: 0.00001721
Iteration 109/1000 | Loss: 0.00001721
Iteration 110/1000 | Loss: 0.00001721
Iteration 111/1000 | Loss: 0.00001721
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001721
Iteration 114/1000 | Loss: 0.00001721
Iteration 115/1000 | Loss: 0.00001721
Iteration 116/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.720757245493587e-05, 1.720757245493587e-05, 1.720757245493587e-05, 1.720757245493587e-05, 1.720757245493587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720757245493587e-05

Optimization complete. Final v2v error: 3.5238454341888428 mm

Highest mean error: 4.161705493927002 mm for frame 177

Lowest mean error: 2.937260389328003 mm for frame 17

Saving results

Total time: 38.72242617607117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881443
Iteration 2/25 | Loss: 0.00137060
Iteration 3/25 | Loss: 0.00108831
Iteration 4/25 | Loss: 0.00105014
Iteration 5/25 | Loss: 0.00104332
Iteration 6/25 | Loss: 0.00104204
Iteration 7/25 | Loss: 0.00104192
Iteration 8/25 | Loss: 0.00104192
Iteration 9/25 | Loss: 0.00104192
Iteration 10/25 | Loss: 0.00104192
Iteration 11/25 | Loss: 0.00104192
Iteration 12/25 | Loss: 0.00104192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010419206228107214, 0.0010419206228107214, 0.0010419206228107214, 0.0010419206228107214, 0.0010419206228107214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010419206228107214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00484037
Iteration 2/25 | Loss: 0.00028201
Iteration 3/25 | Loss: 0.00028200
Iteration 4/25 | Loss: 0.00028199
Iteration 5/25 | Loss: 0.00028199
Iteration 6/25 | Loss: 0.00028199
Iteration 7/25 | Loss: 0.00028199
Iteration 8/25 | Loss: 0.00028199
Iteration 9/25 | Loss: 0.00028199
Iteration 10/25 | Loss: 0.00028199
Iteration 11/25 | Loss: 0.00028199
Iteration 12/25 | Loss: 0.00028199
Iteration 13/25 | Loss: 0.00028199
Iteration 14/25 | Loss: 0.00028199
Iteration 15/25 | Loss: 0.00028199
Iteration 16/25 | Loss: 0.00028199
Iteration 17/25 | Loss: 0.00028199
Iteration 18/25 | Loss: 0.00028199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00028199292137287557, 0.00028199292137287557, 0.00028199292137287557, 0.00028199292137287557, 0.00028199292137287557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028199292137287557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028199
Iteration 2/1000 | Loss: 0.00006372
Iteration 3/1000 | Loss: 0.00003737
Iteration 4/1000 | Loss: 0.00003106
Iteration 5/1000 | Loss: 0.00002818
Iteration 6/1000 | Loss: 0.00002563
Iteration 7/1000 | Loss: 0.00002454
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002334
Iteration 10/1000 | Loss: 0.00002306
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002247
Iteration 14/1000 | Loss: 0.00002246
Iteration 15/1000 | Loss: 0.00002242
Iteration 16/1000 | Loss: 0.00002242
Iteration 17/1000 | Loss: 0.00002241
Iteration 18/1000 | Loss: 0.00002241
Iteration 19/1000 | Loss: 0.00002240
Iteration 20/1000 | Loss: 0.00002240
Iteration 21/1000 | Loss: 0.00002240
Iteration 22/1000 | Loss: 0.00002240
Iteration 23/1000 | Loss: 0.00002239
Iteration 24/1000 | Loss: 0.00002234
Iteration 25/1000 | Loss: 0.00002234
Iteration 26/1000 | Loss: 0.00002234
Iteration 27/1000 | Loss: 0.00002233
Iteration 28/1000 | Loss: 0.00002233
Iteration 29/1000 | Loss: 0.00002233
Iteration 30/1000 | Loss: 0.00002233
Iteration 31/1000 | Loss: 0.00002233
Iteration 32/1000 | Loss: 0.00002233
Iteration 33/1000 | Loss: 0.00002233
Iteration 34/1000 | Loss: 0.00002232
Iteration 35/1000 | Loss: 0.00002232
Iteration 36/1000 | Loss: 0.00002232
Iteration 37/1000 | Loss: 0.00002232
Iteration 38/1000 | Loss: 0.00002232
Iteration 39/1000 | Loss: 0.00002232
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00002231
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002231
Iteration 44/1000 | Loss: 0.00002230
Iteration 45/1000 | Loss: 0.00002230
Iteration 46/1000 | Loss: 0.00002230
Iteration 47/1000 | Loss: 0.00002230
Iteration 48/1000 | Loss: 0.00002230
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002229
Iteration 51/1000 | Loss: 0.00002229
Iteration 52/1000 | Loss: 0.00002229
Iteration 53/1000 | Loss: 0.00002229
Iteration 54/1000 | Loss: 0.00002229
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002229
Iteration 61/1000 | Loss: 0.00002229
Iteration 62/1000 | Loss: 0.00002229
Iteration 63/1000 | Loss: 0.00002229
Iteration 64/1000 | Loss: 0.00002229
Iteration 65/1000 | Loss: 0.00002229
Iteration 66/1000 | Loss: 0.00002229
Iteration 67/1000 | Loss: 0.00002229
Iteration 68/1000 | Loss: 0.00002229
Iteration 69/1000 | Loss: 0.00002229
Iteration 70/1000 | Loss: 0.00002229
Iteration 71/1000 | Loss: 0.00002229
Iteration 72/1000 | Loss: 0.00002229
Iteration 73/1000 | Loss: 0.00002229
Iteration 74/1000 | Loss: 0.00002229
Iteration 75/1000 | Loss: 0.00002229
Iteration 76/1000 | Loss: 0.00002229
Iteration 77/1000 | Loss: 0.00002229
Iteration 78/1000 | Loss: 0.00002229
Iteration 79/1000 | Loss: 0.00002229
Iteration 80/1000 | Loss: 0.00002229
Iteration 81/1000 | Loss: 0.00002229
Iteration 82/1000 | Loss: 0.00002229
Iteration 83/1000 | Loss: 0.00002229
Iteration 84/1000 | Loss: 0.00002229
Iteration 85/1000 | Loss: 0.00002229
Iteration 86/1000 | Loss: 0.00002229
Iteration 87/1000 | Loss: 0.00002229
Iteration 88/1000 | Loss: 0.00002229
Iteration 89/1000 | Loss: 0.00002229
Iteration 90/1000 | Loss: 0.00002229
Iteration 91/1000 | Loss: 0.00002229
Iteration 92/1000 | Loss: 0.00002229
Iteration 93/1000 | Loss: 0.00002229
Iteration 94/1000 | Loss: 0.00002229
Iteration 95/1000 | Loss: 0.00002229
Iteration 96/1000 | Loss: 0.00002229
Iteration 97/1000 | Loss: 0.00002229
Iteration 98/1000 | Loss: 0.00002229
Iteration 99/1000 | Loss: 0.00002229
Iteration 100/1000 | Loss: 0.00002229
Iteration 101/1000 | Loss: 0.00002229
Iteration 102/1000 | Loss: 0.00002229
Iteration 103/1000 | Loss: 0.00002229
Iteration 104/1000 | Loss: 0.00002229
Iteration 105/1000 | Loss: 0.00002229
Iteration 106/1000 | Loss: 0.00002229
Iteration 107/1000 | Loss: 0.00002229
Iteration 108/1000 | Loss: 0.00002229
Iteration 109/1000 | Loss: 0.00002229
Iteration 110/1000 | Loss: 0.00002229
Iteration 111/1000 | Loss: 0.00002229
Iteration 112/1000 | Loss: 0.00002229
Iteration 113/1000 | Loss: 0.00002229
Iteration 114/1000 | Loss: 0.00002229
Iteration 115/1000 | Loss: 0.00002229
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002229
Iteration 119/1000 | Loss: 0.00002229
Iteration 120/1000 | Loss: 0.00002229
Iteration 121/1000 | Loss: 0.00002229
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002229
Iteration 130/1000 | Loss: 0.00002229
Iteration 131/1000 | Loss: 0.00002229
Iteration 132/1000 | Loss: 0.00002229
Iteration 133/1000 | Loss: 0.00002229
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.228660559921991e-05, 2.228660559921991e-05, 2.228660559921991e-05, 2.228660559921991e-05, 2.228660559921991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.228660559921991e-05

Optimization complete. Final v2v error: 4.0153303146362305 mm

Highest mean error: 4.368387699127197 mm for frame 126

Lowest mean error: 3.8467605113983154 mm for frame 22

Saving results

Total time: 32.82279944419861
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01138884
Iteration 2/25 | Loss: 0.00315172
Iteration 3/25 | Loss: 0.00250774
Iteration 4/25 | Loss: 0.00184427
Iteration 5/25 | Loss: 0.00168760
Iteration 6/25 | Loss: 0.00148211
Iteration 7/25 | Loss: 0.00132888
Iteration 8/25 | Loss: 0.00122697
Iteration 9/25 | Loss: 0.00118873
Iteration 10/25 | Loss: 0.00116886
Iteration 11/25 | Loss: 0.00114670
Iteration 12/25 | Loss: 0.00111200
Iteration 13/25 | Loss: 0.00109869
Iteration 14/25 | Loss: 0.00109443
Iteration 15/25 | Loss: 0.00109406
Iteration 16/25 | Loss: 0.00109367
Iteration 17/25 | Loss: 0.00109392
Iteration 18/25 | Loss: 0.00109369
Iteration 19/25 | Loss: 0.00109336
Iteration 20/25 | Loss: 0.00109371
Iteration 21/25 | Loss: 0.00109627
Iteration 22/25 | Loss: 0.00108978
Iteration 23/25 | Loss: 0.00108831
Iteration 24/25 | Loss: 0.00108866
Iteration 25/25 | Loss: 0.00108831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38263738
Iteration 2/25 | Loss: 0.00080366
Iteration 3/25 | Loss: 0.00080366
Iteration 4/25 | Loss: 0.00080365
Iteration 5/25 | Loss: 0.00080365
Iteration 6/25 | Loss: 0.00080365
Iteration 7/25 | Loss: 0.00080365
Iteration 8/25 | Loss: 0.00080365
Iteration 9/25 | Loss: 0.00080365
Iteration 10/25 | Loss: 0.00080365
Iteration 11/25 | Loss: 0.00080365
Iteration 12/25 | Loss: 0.00080365
Iteration 13/25 | Loss: 0.00080365
Iteration 14/25 | Loss: 0.00080365
Iteration 15/25 | Loss: 0.00080365
Iteration 16/25 | Loss: 0.00080365
Iteration 17/25 | Loss: 0.00080365
Iteration 18/25 | Loss: 0.00080365
Iteration 19/25 | Loss: 0.00080365
Iteration 20/25 | Loss: 0.00080365
Iteration 21/25 | Loss: 0.00080365
Iteration 22/25 | Loss: 0.00080365
Iteration 23/25 | Loss: 0.00080365
Iteration 24/25 | Loss: 0.00080365
Iteration 25/25 | Loss: 0.00080365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080365
Iteration 2/1000 | Loss: 0.00032864
Iteration 3/1000 | Loss: 0.00080825
Iteration 4/1000 | Loss: 0.00014511
Iteration 5/1000 | Loss: 0.00008045
Iteration 6/1000 | Loss: 0.00007199
Iteration 7/1000 | Loss: 0.00006138
Iteration 8/1000 | Loss: 0.00006526
Iteration 9/1000 | Loss: 0.00032132
Iteration 10/1000 | Loss: 0.00006200
Iteration 11/1000 | Loss: 0.00007570
Iteration 12/1000 | Loss: 0.00005883
Iteration 13/1000 | Loss: 0.00006587
Iteration 14/1000 | Loss: 0.00034007
Iteration 15/1000 | Loss: 0.00146162
Iteration 16/1000 | Loss: 0.00128722
Iteration 17/1000 | Loss: 0.00021995
Iteration 18/1000 | Loss: 0.00007432
Iteration 19/1000 | Loss: 0.00006924
Iteration 20/1000 | Loss: 0.00007152
Iteration 21/1000 | Loss: 0.00005783
Iteration 22/1000 | Loss: 0.00414391
Iteration 23/1000 | Loss: 0.00422687
Iteration 24/1000 | Loss: 0.00007725
Iteration 25/1000 | Loss: 0.00004698
Iteration 26/1000 | Loss: 0.00003616
Iteration 27/1000 | Loss: 0.00088337
Iteration 28/1000 | Loss: 0.00002890
Iteration 29/1000 | Loss: 0.00002505
Iteration 30/1000 | Loss: 0.00002275
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00001936
Iteration 34/1000 | Loss: 0.00001843
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001667
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001637
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001623
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001620
Iteration 50/1000 | Loss: 0.00001620
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001609
Iteration 96/1000 | Loss: 0.00001609
Iteration 97/1000 | Loss: 0.00001609
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001607
Iteration 105/1000 | Loss: 0.00001607
Iteration 106/1000 | Loss: 0.00001607
Iteration 107/1000 | Loss: 0.00001607
Iteration 108/1000 | Loss: 0.00001607
Iteration 109/1000 | Loss: 0.00001607
Iteration 110/1000 | Loss: 0.00001607
Iteration 111/1000 | Loss: 0.00001607
Iteration 112/1000 | Loss: 0.00001607
Iteration 113/1000 | Loss: 0.00001607
Iteration 114/1000 | Loss: 0.00001607
Iteration 115/1000 | Loss: 0.00001607
Iteration 116/1000 | Loss: 0.00001607
Iteration 117/1000 | Loss: 0.00001607
Iteration 118/1000 | Loss: 0.00001607
Iteration 119/1000 | Loss: 0.00001607
Iteration 120/1000 | Loss: 0.00001607
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6070402125478722e-05, 1.6070402125478722e-05, 1.6070402125478722e-05, 1.6070402125478722e-05, 1.6070402125478722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6070402125478722e-05

Optimization complete. Final v2v error: 3.420206308364868 mm

Highest mean error: 4.041094779968262 mm for frame 79

Lowest mean error: 3.139650821685791 mm for frame 60

Saving results

Total time: 108.06960535049438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866166
Iteration 2/25 | Loss: 0.00139928
Iteration 3/25 | Loss: 0.00110285
Iteration 4/25 | Loss: 0.00106788
Iteration 5/25 | Loss: 0.00106325
Iteration 6/25 | Loss: 0.00106252
Iteration 7/25 | Loss: 0.00106252
Iteration 8/25 | Loss: 0.00106252
Iteration 9/25 | Loss: 0.00106252
Iteration 10/25 | Loss: 0.00106252
Iteration 11/25 | Loss: 0.00106252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010625157738104463, 0.0010625157738104463, 0.0010625157738104463, 0.0010625157738104463, 0.0010625157738104463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010625157738104463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35062420
Iteration 2/25 | Loss: 0.00045703
Iteration 3/25 | Loss: 0.00045701
Iteration 4/25 | Loss: 0.00045701
Iteration 5/25 | Loss: 0.00045701
Iteration 6/25 | Loss: 0.00045701
Iteration 7/25 | Loss: 0.00045701
Iteration 8/25 | Loss: 0.00045701
Iteration 9/25 | Loss: 0.00045701
Iteration 10/25 | Loss: 0.00045701
Iteration 11/25 | Loss: 0.00045701
Iteration 12/25 | Loss: 0.00045701
Iteration 13/25 | Loss: 0.00045701
Iteration 14/25 | Loss: 0.00045701
Iteration 15/25 | Loss: 0.00045701
Iteration 16/25 | Loss: 0.00045701
Iteration 17/25 | Loss: 0.00045701
Iteration 18/25 | Loss: 0.00045701
Iteration 19/25 | Loss: 0.00045701
Iteration 20/25 | Loss: 0.00045701
Iteration 21/25 | Loss: 0.00045701
Iteration 22/25 | Loss: 0.00045701
Iteration 23/25 | Loss: 0.00045701
Iteration 24/25 | Loss: 0.00045701
Iteration 25/25 | Loss: 0.00045701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045701
Iteration 2/1000 | Loss: 0.00005213
Iteration 3/1000 | Loss: 0.00002492
Iteration 4/1000 | Loss: 0.00001948
Iteration 5/1000 | Loss: 0.00001782
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001597
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001513
Iteration 13/1000 | Loss: 0.00001508
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001496
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001490
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001487
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001486
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001481
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001478
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001465
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001464
Iteration 92/1000 | Loss: 0.00001464
Iteration 93/1000 | Loss: 0.00001464
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001464
Iteration 98/1000 | Loss: 0.00001464
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001463
Iteration 104/1000 | Loss: 0.00001463
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001463
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001462
Iteration 114/1000 | Loss: 0.00001462
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001462
Iteration 117/1000 | Loss: 0.00001461
Iteration 118/1000 | Loss: 0.00001461
Iteration 119/1000 | Loss: 0.00001461
Iteration 120/1000 | Loss: 0.00001461
Iteration 121/1000 | Loss: 0.00001460
Iteration 122/1000 | Loss: 0.00001460
Iteration 123/1000 | Loss: 0.00001460
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001459
Iteration 126/1000 | Loss: 0.00001459
Iteration 127/1000 | Loss: 0.00001459
Iteration 128/1000 | Loss: 0.00001459
Iteration 129/1000 | Loss: 0.00001459
Iteration 130/1000 | Loss: 0.00001459
Iteration 131/1000 | Loss: 0.00001459
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001458
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001456
Iteration 154/1000 | Loss: 0.00001456
Iteration 155/1000 | Loss: 0.00001456
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001456
Iteration 160/1000 | Loss: 0.00001456
Iteration 161/1000 | Loss: 0.00001456
Iteration 162/1000 | Loss: 0.00001456
Iteration 163/1000 | Loss: 0.00001456
Iteration 164/1000 | Loss: 0.00001456
Iteration 165/1000 | Loss: 0.00001455
Iteration 166/1000 | Loss: 0.00001455
Iteration 167/1000 | Loss: 0.00001455
Iteration 168/1000 | Loss: 0.00001455
Iteration 169/1000 | Loss: 0.00001455
Iteration 170/1000 | Loss: 0.00001455
Iteration 171/1000 | Loss: 0.00001455
Iteration 172/1000 | Loss: 0.00001455
Iteration 173/1000 | Loss: 0.00001455
Iteration 174/1000 | Loss: 0.00001455
Iteration 175/1000 | Loss: 0.00001454
Iteration 176/1000 | Loss: 0.00001454
Iteration 177/1000 | Loss: 0.00001454
Iteration 178/1000 | Loss: 0.00001454
Iteration 179/1000 | Loss: 0.00001454
Iteration 180/1000 | Loss: 0.00001454
Iteration 181/1000 | Loss: 0.00001454
Iteration 182/1000 | Loss: 0.00001454
Iteration 183/1000 | Loss: 0.00001454
Iteration 184/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4541447853844147e-05, 1.4541447853844147e-05, 1.4541447853844147e-05, 1.4541447853844147e-05, 1.4541447853844147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4541447853844147e-05

Optimization complete. Final v2v error: 3.316601276397705 mm

Highest mean error: 4.137026309967041 mm for frame 89

Lowest mean error: 2.849672794342041 mm for frame 156

Saving results

Total time: 44.52353549003601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5553/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5553/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439668
Iteration 2/25 | Loss: 0.00131725
Iteration 3/25 | Loss: 0.00109177
Iteration 4/25 | Loss: 0.00106497
Iteration 5/25 | Loss: 0.00105907
Iteration 6/25 | Loss: 0.00105733
Iteration 7/25 | Loss: 0.00105682
Iteration 8/25 | Loss: 0.00105682
Iteration 9/25 | Loss: 0.00105682
Iteration 10/25 | Loss: 0.00105682
Iteration 11/25 | Loss: 0.00105682
Iteration 12/25 | Loss: 0.00105682
Iteration 13/25 | Loss: 0.00105682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010568165453150868, 0.0010568165453150868, 0.0010568165453150868, 0.0010568165453150868, 0.0010568165453150868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010568165453150868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.00698423
Iteration 2/25 | Loss: 0.00045424
Iteration 3/25 | Loss: 0.00045420
Iteration 4/25 | Loss: 0.00045420
Iteration 5/25 | Loss: 0.00045420
Iteration 6/25 | Loss: 0.00045420
Iteration 7/25 | Loss: 0.00045420
Iteration 8/25 | Loss: 0.00045420
Iteration 9/25 | Loss: 0.00045420
Iteration 10/25 | Loss: 0.00045420
Iteration 11/25 | Loss: 0.00045420
Iteration 12/25 | Loss: 0.00045420
Iteration 13/25 | Loss: 0.00045420
Iteration 14/25 | Loss: 0.00045420
Iteration 15/25 | Loss: 0.00045420
Iteration 16/25 | Loss: 0.00045420
Iteration 17/25 | Loss: 0.00045420
Iteration 18/25 | Loss: 0.00045420
Iteration 19/25 | Loss: 0.00045420
Iteration 20/25 | Loss: 0.00045420
Iteration 21/25 | Loss: 0.00045420
Iteration 22/25 | Loss: 0.00045420
Iteration 23/25 | Loss: 0.00045420
Iteration 24/25 | Loss: 0.00045420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004541951057035476, 0.0004541951057035476, 0.0004541951057035476, 0.0004541951057035476, 0.0004541951057035476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004541951057035476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045420
Iteration 2/1000 | Loss: 0.00005041
Iteration 3/1000 | Loss: 0.00002807
Iteration 4/1000 | Loss: 0.00002497
Iteration 5/1000 | Loss: 0.00002329
Iteration 6/1000 | Loss: 0.00002224
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002016
Iteration 10/1000 | Loss: 0.00001987
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001947
Iteration 13/1000 | Loss: 0.00001947
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001941
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001937
Iteration 20/1000 | Loss: 0.00001937
Iteration 21/1000 | Loss: 0.00001936
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001933
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001930
Iteration 29/1000 | Loss: 0.00001930
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001929
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001927
Iteration 35/1000 | Loss: 0.00001927
Iteration 36/1000 | Loss: 0.00001927
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001926
Iteration 40/1000 | Loss: 0.00001926
Iteration 41/1000 | Loss: 0.00001926
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001926
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001926
Iteration 51/1000 | Loss: 0.00001926
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001924
Iteration 63/1000 | Loss: 0.00001924
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001920
Iteration 80/1000 | Loss: 0.00001920
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001919
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001918
Iteration 85/1000 | Loss: 0.00001918
Iteration 86/1000 | Loss: 0.00001918
Iteration 87/1000 | Loss: 0.00001918
Iteration 88/1000 | Loss: 0.00001918
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001917
Iteration 94/1000 | Loss: 0.00001917
Iteration 95/1000 | Loss: 0.00001917
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001915
Iteration 102/1000 | Loss: 0.00001915
Iteration 103/1000 | Loss: 0.00001915
Iteration 104/1000 | Loss: 0.00001915
Iteration 105/1000 | Loss: 0.00001915
Iteration 106/1000 | Loss: 0.00001915
Iteration 107/1000 | Loss: 0.00001915
Iteration 108/1000 | Loss: 0.00001914
Iteration 109/1000 | Loss: 0.00001914
Iteration 110/1000 | Loss: 0.00001914
Iteration 111/1000 | Loss: 0.00001914
Iteration 112/1000 | Loss: 0.00001914
Iteration 113/1000 | Loss: 0.00001913
Iteration 114/1000 | Loss: 0.00001913
Iteration 115/1000 | Loss: 0.00001913
Iteration 116/1000 | Loss: 0.00001913
Iteration 117/1000 | Loss: 0.00001913
Iteration 118/1000 | Loss: 0.00001913
Iteration 119/1000 | Loss: 0.00001913
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001912
Iteration 122/1000 | Loss: 0.00001912
Iteration 123/1000 | Loss: 0.00001912
Iteration 124/1000 | Loss: 0.00001911
Iteration 125/1000 | Loss: 0.00001911
Iteration 126/1000 | Loss: 0.00001911
Iteration 127/1000 | Loss: 0.00001911
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001910
Iteration 133/1000 | Loss: 0.00001910
Iteration 134/1000 | Loss: 0.00001910
Iteration 135/1000 | Loss: 0.00001910
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001909
Iteration 140/1000 | Loss: 0.00001909
Iteration 141/1000 | Loss: 0.00001909
Iteration 142/1000 | Loss: 0.00001909
Iteration 143/1000 | Loss: 0.00001909
Iteration 144/1000 | Loss: 0.00001909
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001909
Iteration 149/1000 | Loss: 0.00001908
Iteration 150/1000 | Loss: 0.00001908
Iteration 151/1000 | Loss: 0.00001908
Iteration 152/1000 | Loss: 0.00001908
Iteration 153/1000 | Loss: 0.00001908
Iteration 154/1000 | Loss: 0.00001908
Iteration 155/1000 | Loss: 0.00001908
Iteration 156/1000 | Loss: 0.00001907
Iteration 157/1000 | Loss: 0.00001907
Iteration 158/1000 | Loss: 0.00001907
Iteration 159/1000 | Loss: 0.00001907
Iteration 160/1000 | Loss: 0.00001907
Iteration 161/1000 | Loss: 0.00001907
Iteration 162/1000 | Loss: 0.00001907
Iteration 163/1000 | Loss: 0.00001907
Iteration 164/1000 | Loss: 0.00001906
Iteration 165/1000 | Loss: 0.00001906
Iteration 166/1000 | Loss: 0.00001906
Iteration 167/1000 | Loss: 0.00001906
Iteration 168/1000 | Loss: 0.00001906
Iteration 169/1000 | Loss: 0.00001906
Iteration 170/1000 | Loss: 0.00001905
Iteration 171/1000 | Loss: 0.00001905
Iteration 172/1000 | Loss: 0.00001905
Iteration 173/1000 | Loss: 0.00001905
Iteration 174/1000 | Loss: 0.00001905
Iteration 175/1000 | Loss: 0.00001905
Iteration 176/1000 | Loss: 0.00001905
Iteration 177/1000 | Loss: 0.00001905
Iteration 178/1000 | Loss: 0.00001905
Iteration 179/1000 | Loss: 0.00001905
Iteration 180/1000 | Loss: 0.00001905
Iteration 181/1000 | Loss: 0.00001905
Iteration 182/1000 | Loss: 0.00001904
Iteration 183/1000 | Loss: 0.00001904
Iteration 184/1000 | Loss: 0.00001904
Iteration 185/1000 | Loss: 0.00001904
Iteration 186/1000 | Loss: 0.00001904
Iteration 187/1000 | Loss: 0.00001904
Iteration 188/1000 | Loss: 0.00001903
Iteration 189/1000 | Loss: 0.00001903
Iteration 190/1000 | Loss: 0.00001903
Iteration 191/1000 | Loss: 0.00001903
Iteration 192/1000 | Loss: 0.00001903
Iteration 193/1000 | Loss: 0.00001903
Iteration 194/1000 | Loss: 0.00001903
Iteration 195/1000 | Loss: 0.00001903
Iteration 196/1000 | Loss: 0.00001903
Iteration 197/1000 | Loss: 0.00001903
Iteration 198/1000 | Loss: 0.00001903
Iteration 199/1000 | Loss: 0.00001903
Iteration 200/1000 | Loss: 0.00001903
Iteration 201/1000 | Loss: 0.00001903
Iteration 202/1000 | Loss: 0.00001903
Iteration 203/1000 | Loss: 0.00001903
Iteration 204/1000 | Loss: 0.00001903
Iteration 205/1000 | Loss: 0.00001903
Iteration 206/1000 | Loss: 0.00001903
Iteration 207/1000 | Loss: 0.00001903
Iteration 208/1000 | Loss: 0.00001903
Iteration 209/1000 | Loss: 0.00001903
Iteration 210/1000 | Loss: 0.00001903
Iteration 211/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.902528492792044e-05, 1.902528492792044e-05, 1.902528492792044e-05, 1.902528492792044e-05, 1.902528492792044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.902528492792044e-05

Optimization complete. Final v2v error: 3.7900359630584717 mm

Highest mean error: 4.402287483215332 mm for frame 32

Lowest mean error: 3.2921621799468994 mm for frame 135

Saving results

Total time: 39.95454502105713
