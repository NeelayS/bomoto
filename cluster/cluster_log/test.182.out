Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=182, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10192-10247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910866
Iteration 2/25 | Loss: 0.00126478
Iteration 3/25 | Loss: 0.00115705
Iteration 4/25 | Loss: 0.00113624
Iteration 5/25 | Loss: 0.00112903
Iteration 6/25 | Loss: 0.00112740
Iteration 7/25 | Loss: 0.00112740
Iteration 8/25 | Loss: 0.00112740
Iteration 9/25 | Loss: 0.00112740
Iteration 10/25 | Loss: 0.00112740
Iteration 11/25 | Loss: 0.00112740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001127399504184723, 0.001127399504184723, 0.001127399504184723, 0.001127399504184723, 0.001127399504184723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127399504184723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35351861
Iteration 2/25 | Loss: 0.00084071
Iteration 3/25 | Loss: 0.00084069
Iteration 4/25 | Loss: 0.00084069
Iteration 5/25 | Loss: 0.00084069
Iteration 6/25 | Loss: 0.00084069
Iteration 7/25 | Loss: 0.00084069
Iteration 8/25 | Loss: 0.00084069
Iteration 9/25 | Loss: 0.00084069
Iteration 10/25 | Loss: 0.00084069
Iteration 11/25 | Loss: 0.00084069
Iteration 12/25 | Loss: 0.00084069
Iteration 13/25 | Loss: 0.00084069
Iteration 14/25 | Loss: 0.00084069
Iteration 15/25 | Loss: 0.00084069
Iteration 16/25 | Loss: 0.00084069
Iteration 17/25 | Loss: 0.00084069
Iteration 18/25 | Loss: 0.00084069
Iteration 19/25 | Loss: 0.00084069
Iteration 20/25 | Loss: 0.00084069
Iteration 21/25 | Loss: 0.00084069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008406902197748423, 0.0008406902197748423, 0.0008406902197748423, 0.0008406902197748423, 0.0008406902197748423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008406902197748423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084069
Iteration 2/1000 | Loss: 0.00004596
Iteration 3/1000 | Loss: 0.00003049
Iteration 4/1000 | Loss: 0.00002404
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00002154
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002028
Iteration 9/1000 | Loss: 0.00001980
Iteration 10/1000 | Loss: 0.00001956
Iteration 11/1000 | Loss: 0.00001947
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001921
Iteration 14/1000 | Loss: 0.00001910
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001876
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001875
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00001874
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001874
Iteration 43/1000 | Loss: 0.00001874
Iteration 44/1000 | Loss: 0.00001874
Iteration 45/1000 | Loss: 0.00001874
Iteration 46/1000 | Loss: 0.00001873
Iteration 47/1000 | Loss: 0.00001873
Iteration 48/1000 | Loss: 0.00001873
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001871
Iteration 51/1000 | Loss: 0.00001871
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001870
Iteration 54/1000 | Loss: 0.00001870
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001870
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001865
Iteration 72/1000 | Loss: 0.00001865
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001864
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001859
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001856
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001850
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001848
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001847
Iteration 175/1000 | Loss: 0.00001847
Iteration 176/1000 | Loss: 0.00001847
Iteration 177/1000 | Loss: 0.00001847
Iteration 178/1000 | Loss: 0.00001846
Iteration 179/1000 | Loss: 0.00001846
Iteration 180/1000 | Loss: 0.00001846
Iteration 181/1000 | Loss: 0.00001846
Iteration 182/1000 | Loss: 0.00001846
Iteration 183/1000 | Loss: 0.00001846
Iteration 184/1000 | Loss: 0.00001846
Iteration 185/1000 | Loss: 0.00001846
Iteration 186/1000 | Loss: 0.00001845
Iteration 187/1000 | Loss: 0.00001845
Iteration 188/1000 | Loss: 0.00001845
Iteration 189/1000 | Loss: 0.00001845
Iteration 190/1000 | Loss: 0.00001845
Iteration 191/1000 | Loss: 0.00001845
Iteration 192/1000 | Loss: 0.00001844
Iteration 193/1000 | Loss: 0.00001844
Iteration 194/1000 | Loss: 0.00001844
Iteration 195/1000 | Loss: 0.00001844
Iteration 196/1000 | Loss: 0.00001844
Iteration 197/1000 | Loss: 0.00001844
Iteration 198/1000 | Loss: 0.00001844
Iteration 199/1000 | Loss: 0.00001844
Iteration 200/1000 | Loss: 0.00001844
Iteration 201/1000 | Loss: 0.00001844
Iteration 202/1000 | Loss: 0.00001844
Iteration 203/1000 | Loss: 0.00001844
Iteration 204/1000 | Loss: 0.00001844
Iteration 205/1000 | Loss: 0.00001844
Iteration 206/1000 | Loss: 0.00001844
Iteration 207/1000 | Loss: 0.00001844
Iteration 208/1000 | Loss: 0.00001844
Iteration 209/1000 | Loss: 0.00001844
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Iteration 213/1000 | Loss: 0.00001843
Iteration 214/1000 | Loss: 0.00001843
Iteration 215/1000 | Loss: 0.00001843
Iteration 216/1000 | Loss: 0.00001843
Iteration 217/1000 | Loss: 0.00001843
Iteration 218/1000 | Loss: 0.00001843
Iteration 219/1000 | Loss: 0.00001843
Iteration 220/1000 | Loss: 0.00001843
Iteration 221/1000 | Loss: 0.00001843
Iteration 222/1000 | Loss: 0.00001843
Iteration 223/1000 | Loss: 0.00001842
Iteration 224/1000 | Loss: 0.00001842
Iteration 225/1000 | Loss: 0.00001842
Iteration 226/1000 | Loss: 0.00001842
Iteration 227/1000 | Loss: 0.00001842
Iteration 228/1000 | Loss: 0.00001842
Iteration 229/1000 | Loss: 0.00001842
Iteration 230/1000 | Loss: 0.00001842
Iteration 231/1000 | Loss: 0.00001842
Iteration 232/1000 | Loss: 0.00001842
Iteration 233/1000 | Loss: 0.00001842
Iteration 234/1000 | Loss: 0.00001842
Iteration 235/1000 | Loss: 0.00001842
Iteration 236/1000 | Loss: 0.00001842
Iteration 237/1000 | Loss: 0.00001842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.8420001651975326e-05, 1.8420001651975326e-05, 1.8420001651975326e-05, 1.8420001651975326e-05, 1.8420001651975326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8420001651975326e-05

Optimization complete. Final v2v error: 3.5576157569885254 mm

Highest mean error: 5.598461627960205 mm for frame 70

Lowest mean error: 3.0723469257354736 mm for frame 1

Saving results

Total time: 45.18785119056702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460963
Iteration 2/25 | Loss: 0.00128506
Iteration 3/25 | Loss: 0.00112547
Iteration 4/25 | Loss: 0.00111549
Iteration 5/25 | Loss: 0.00111331
Iteration 6/25 | Loss: 0.00111295
Iteration 7/25 | Loss: 0.00111295
Iteration 8/25 | Loss: 0.00111295
Iteration 9/25 | Loss: 0.00111295
Iteration 10/25 | Loss: 0.00111295
Iteration 11/25 | Loss: 0.00111295
Iteration 12/25 | Loss: 0.00111295
Iteration 13/25 | Loss: 0.00111295
Iteration 14/25 | Loss: 0.00111295
Iteration 15/25 | Loss: 0.00111295
Iteration 16/25 | Loss: 0.00111295
Iteration 17/25 | Loss: 0.00111295
Iteration 18/25 | Loss: 0.00111295
Iteration 19/25 | Loss: 0.00111295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011129464255645871, 0.0011129464255645871, 0.0011129464255645871, 0.0011129464255645871, 0.0011129464255645871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011129464255645871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45450711
Iteration 2/25 | Loss: 0.00096161
Iteration 3/25 | Loss: 0.00096161
Iteration 4/25 | Loss: 0.00096161
Iteration 5/25 | Loss: 0.00096161
Iteration 6/25 | Loss: 0.00096161
Iteration 7/25 | Loss: 0.00096161
Iteration 8/25 | Loss: 0.00096161
Iteration 9/25 | Loss: 0.00096161
Iteration 10/25 | Loss: 0.00096161
Iteration 11/25 | Loss: 0.00096161
Iteration 12/25 | Loss: 0.00096161
Iteration 13/25 | Loss: 0.00096161
Iteration 14/25 | Loss: 0.00096161
Iteration 15/25 | Loss: 0.00096161
Iteration 16/25 | Loss: 0.00096161
Iteration 17/25 | Loss: 0.00096161
Iteration 18/25 | Loss: 0.00096161
Iteration 19/25 | Loss: 0.00096161
Iteration 20/25 | Loss: 0.00096161
Iteration 21/25 | Loss: 0.00096161
Iteration 22/25 | Loss: 0.00096161
Iteration 23/25 | Loss: 0.00096161
Iteration 24/25 | Loss: 0.00096161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009616071474738419, 0.0009616071474738419, 0.0009616071474738419, 0.0009616071474738419, 0.0009616071474738419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009616071474738419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096161
Iteration 2/1000 | Loss: 0.00003182
Iteration 3/1000 | Loss: 0.00002124
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001442
Iteration 9/1000 | Loss: 0.00001417
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001354
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001335
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001333
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001332
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001322
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001319
Iteration 35/1000 | Loss: 0.00001319
Iteration 36/1000 | Loss: 0.00001319
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001316
Iteration 48/1000 | Loss: 0.00001316
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001315
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001311
Iteration 62/1000 | Loss: 0.00001310
Iteration 63/1000 | Loss: 0.00001304
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001299
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001294
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001293
Iteration 91/1000 | Loss: 0.00001293
Iteration 92/1000 | Loss: 0.00001293
Iteration 93/1000 | Loss: 0.00001293
Iteration 94/1000 | Loss: 0.00001293
Iteration 95/1000 | Loss: 0.00001293
Iteration 96/1000 | Loss: 0.00001293
Iteration 97/1000 | Loss: 0.00001293
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001292
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001291
Iteration 103/1000 | Loss: 0.00001291
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001290
Iteration 108/1000 | Loss: 0.00001290
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001290
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001287
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001287
Iteration 126/1000 | Loss: 0.00001287
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001287
Iteration 129/1000 | Loss: 0.00001286
Iteration 130/1000 | Loss: 0.00001286
Iteration 131/1000 | Loss: 0.00001286
Iteration 132/1000 | Loss: 0.00001286
Iteration 133/1000 | Loss: 0.00001286
Iteration 134/1000 | Loss: 0.00001286
Iteration 135/1000 | Loss: 0.00001286
Iteration 136/1000 | Loss: 0.00001286
Iteration 137/1000 | Loss: 0.00001286
Iteration 138/1000 | Loss: 0.00001286
Iteration 139/1000 | Loss: 0.00001286
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001286
Iteration 142/1000 | Loss: 0.00001285
Iteration 143/1000 | Loss: 0.00001285
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001285
Iteration 147/1000 | Loss: 0.00001284
Iteration 148/1000 | Loss: 0.00001284
Iteration 149/1000 | Loss: 0.00001284
Iteration 150/1000 | Loss: 0.00001284
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001284
Iteration 154/1000 | Loss: 0.00001284
Iteration 155/1000 | Loss: 0.00001284
Iteration 156/1000 | Loss: 0.00001284
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001283
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001283
Iteration 162/1000 | Loss: 0.00001283
Iteration 163/1000 | Loss: 0.00001283
Iteration 164/1000 | Loss: 0.00001283
Iteration 165/1000 | Loss: 0.00001283
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001283
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001282
Iteration 174/1000 | Loss: 0.00001282
Iteration 175/1000 | Loss: 0.00001282
Iteration 176/1000 | Loss: 0.00001282
Iteration 177/1000 | Loss: 0.00001282
Iteration 178/1000 | Loss: 0.00001282
Iteration 179/1000 | Loss: 0.00001282
Iteration 180/1000 | Loss: 0.00001282
Iteration 181/1000 | Loss: 0.00001282
Iteration 182/1000 | Loss: 0.00001282
Iteration 183/1000 | Loss: 0.00001281
Iteration 184/1000 | Loss: 0.00001281
Iteration 185/1000 | Loss: 0.00001281
Iteration 186/1000 | Loss: 0.00001281
Iteration 187/1000 | Loss: 0.00001281
Iteration 188/1000 | Loss: 0.00001281
Iteration 189/1000 | Loss: 0.00001281
Iteration 190/1000 | Loss: 0.00001281
Iteration 191/1000 | Loss: 0.00001281
Iteration 192/1000 | Loss: 0.00001281
Iteration 193/1000 | Loss: 0.00001281
Iteration 194/1000 | Loss: 0.00001281
Iteration 195/1000 | Loss: 0.00001281
Iteration 196/1000 | Loss: 0.00001281
Iteration 197/1000 | Loss: 0.00001280
Iteration 198/1000 | Loss: 0.00001280
Iteration 199/1000 | Loss: 0.00001280
Iteration 200/1000 | Loss: 0.00001280
Iteration 201/1000 | Loss: 0.00001280
Iteration 202/1000 | Loss: 0.00001280
Iteration 203/1000 | Loss: 0.00001279
Iteration 204/1000 | Loss: 0.00001279
Iteration 205/1000 | Loss: 0.00001279
Iteration 206/1000 | Loss: 0.00001279
Iteration 207/1000 | Loss: 0.00001279
Iteration 208/1000 | Loss: 0.00001279
Iteration 209/1000 | Loss: 0.00001279
Iteration 210/1000 | Loss: 0.00001279
Iteration 211/1000 | Loss: 0.00001279
Iteration 212/1000 | Loss: 0.00001279
Iteration 213/1000 | Loss: 0.00001279
Iteration 214/1000 | Loss: 0.00001278
Iteration 215/1000 | Loss: 0.00001278
Iteration 216/1000 | Loss: 0.00001278
Iteration 217/1000 | Loss: 0.00001278
Iteration 218/1000 | Loss: 0.00001278
Iteration 219/1000 | Loss: 0.00001278
Iteration 220/1000 | Loss: 0.00001278
Iteration 221/1000 | Loss: 0.00001278
Iteration 222/1000 | Loss: 0.00001278
Iteration 223/1000 | Loss: 0.00001278
Iteration 224/1000 | Loss: 0.00001278
Iteration 225/1000 | Loss: 0.00001278
Iteration 226/1000 | Loss: 0.00001278
Iteration 227/1000 | Loss: 0.00001278
Iteration 228/1000 | Loss: 0.00001278
Iteration 229/1000 | Loss: 0.00001278
Iteration 230/1000 | Loss: 0.00001278
Iteration 231/1000 | Loss: 0.00001278
Iteration 232/1000 | Loss: 0.00001278
Iteration 233/1000 | Loss: 0.00001278
Iteration 234/1000 | Loss: 0.00001278
Iteration 235/1000 | Loss: 0.00001278
Iteration 236/1000 | Loss: 0.00001278
Iteration 237/1000 | Loss: 0.00001278
Iteration 238/1000 | Loss: 0.00001278
Iteration 239/1000 | Loss: 0.00001278
Iteration 240/1000 | Loss: 0.00001278
Iteration 241/1000 | Loss: 0.00001278
Iteration 242/1000 | Loss: 0.00001278
Iteration 243/1000 | Loss: 0.00001278
Iteration 244/1000 | Loss: 0.00001278
Iteration 245/1000 | Loss: 0.00001278
Iteration 246/1000 | Loss: 0.00001278
Iteration 247/1000 | Loss: 0.00001278
Iteration 248/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.2777520169038326e-05, 1.2777520169038326e-05, 1.2777520169038326e-05, 1.2777520169038326e-05, 1.2777520169038326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2777520169038326e-05

Optimization complete. Final v2v error: 2.92911434173584 mm

Highest mean error: 3.4857003688812256 mm for frame 15

Lowest mean error: 2.308016300201416 mm for frame 108

Saving results

Total time: 45.7443425655365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865251
Iteration 2/25 | Loss: 0.00206707
Iteration 3/25 | Loss: 0.00189189
Iteration 4/25 | Loss: 0.00144942
Iteration 5/25 | Loss: 0.00146810
Iteration 6/25 | Loss: 0.00141342
Iteration 7/25 | Loss: 0.00138844
Iteration 8/25 | Loss: 0.00138043
Iteration 9/25 | Loss: 0.00136738
Iteration 10/25 | Loss: 0.00136569
Iteration 11/25 | Loss: 0.00136359
Iteration 12/25 | Loss: 0.00136523
Iteration 13/25 | Loss: 0.00137661
Iteration 14/25 | Loss: 0.00137030
Iteration 15/25 | Loss: 0.00136273
Iteration 16/25 | Loss: 0.00135096
Iteration 17/25 | Loss: 0.00134475
Iteration 18/25 | Loss: 0.00134453
Iteration 19/25 | Loss: 0.00134358
Iteration 20/25 | Loss: 0.00134738
Iteration 21/25 | Loss: 0.00134483
Iteration 22/25 | Loss: 0.00134194
Iteration 23/25 | Loss: 0.00134367
Iteration 24/25 | Loss: 0.00134327
Iteration 25/25 | Loss: 0.00134108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77627897
Iteration 2/25 | Loss: 0.00352936
Iteration 3/25 | Loss: 0.00318131
Iteration 4/25 | Loss: 0.00318131
Iteration 5/25 | Loss: 0.00318131
Iteration 6/25 | Loss: 0.00318131
Iteration 7/25 | Loss: 0.00318131
Iteration 8/25 | Loss: 0.00318131
Iteration 9/25 | Loss: 0.00318131
Iteration 10/25 | Loss: 0.00318131
Iteration 11/25 | Loss: 0.00318131
Iteration 12/25 | Loss: 0.00318131
Iteration 13/25 | Loss: 0.00318131
Iteration 14/25 | Loss: 0.00318131
Iteration 15/25 | Loss: 0.00318131
Iteration 16/25 | Loss: 0.00318131
Iteration 17/25 | Loss: 0.00318131
Iteration 18/25 | Loss: 0.00318131
Iteration 19/25 | Loss: 0.00318131
Iteration 20/25 | Loss: 0.00318131
Iteration 21/25 | Loss: 0.00318131
Iteration 22/25 | Loss: 0.00318131
Iteration 23/25 | Loss: 0.00318131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0031813073437660933, 0.0031813073437660933, 0.0031813073437660933, 0.0031813073437660933, 0.0031813073437660933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031813073437660933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00318131
Iteration 2/1000 | Loss: 0.00081163
Iteration 3/1000 | Loss: 0.00101057
Iteration 4/1000 | Loss: 0.00229605
Iteration 5/1000 | Loss: 0.00271158
Iteration 6/1000 | Loss: 0.00119588
Iteration 7/1000 | Loss: 0.00016819
Iteration 8/1000 | Loss: 0.00093388
Iteration 9/1000 | Loss: 0.00113977
Iteration 10/1000 | Loss: 0.00079280
Iteration 11/1000 | Loss: 0.00062243
Iteration 12/1000 | Loss: 0.00012939
Iteration 13/1000 | Loss: 0.00005147
Iteration 14/1000 | Loss: 0.00004088
Iteration 15/1000 | Loss: 0.00003375
Iteration 16/1000 | Loss: 0.00070347
Iteration 17/1000 | Loss: 0.00020367
Iteration 18/1000 | Loss: 0.00064514
Iteration 19/1000 | Loss: 0.00018032
Iteration 20/1000 | Loss: 0.00031070
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002091
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001489
Iteration 34/1000 | Loss: 0.00001480
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001393
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001384
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00001381
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001354
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001354
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001351
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001351
Iteration 111/1000 | Loss: 0.00001351
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001349
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001347
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001344
Iteration 137/1000 | Loss: 0.00001344
Iteration 138/1000 | Loss: 0.00001344
Iteration 139/1000 | Loss: 0.00001344
Iteration 140/1000 | Loss: 0.00001344
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001341
Iteration 156/1000 | Loss: 0.00001341
Iteration 157/1000 | Loss: 0.00001341
Iteration 158/1000 | Loss: 0.00001341
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001340
Iteration 164/1000 | Loss: 0.00001340
Iteration 165/1000 | Loss: 0.00001340
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001340
Iteration 175/1000 | Loss: 0.00001340
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001340
Iteration 178/1000 | Loss: 0.00001340
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001339
Iteration 188/1000 | Loss: 0.00001339
Iteration 189/1000 | Loss: 0.00001339
Iteration 190/1000 | Loss: 0.00001339
Iteration 191/1000 | Loss: 0.00001339
Iteration 192/1000 | Loss: 0.00001339
Iteration 193/1000 | Loss: 0.00001339
Iteration 194/1000 | Loss: 0.00001339
Iteration 195/1000 | Loss: 0.00001339
Iteration 196/1000 | Loss: 0.00001339
Iteration 197/1000 | Loss: 0.00001339
Iteration 198/1000 | Loss: 0.00001339
Iteration 199/1000 | Loss: 0.00001339
Iteration 200/1000 | Loss: 0.00001339
Iteration 201/1000 | Loss: 0.00001339
Iteration 202/1000 | Loss: 0.00001339
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001338
Iteration 205/1000 | Loss: 0.00001338
Iteration 206/1000 | Loss: 0.00001338
Iteration 207/1000 | Loss: 0.00001338
Iteration 208/1000 | Loss: 0.00001338
Iteration 209/1000 | Loss: 0.00001338
Iteration 210/1000 | Loss: 0.00001338
Iteration 211/1000 | Loss: 0.00001338
Iteration 212/1000 | Loss: 0.00001338
Iteration 213/1000 | Loss: 0.00001338
Iteration 214/1000 | Loss: 0.00001337
Iteration 215/1000 | Loss: 0.00001337
Iteration 216/1000 | Loss: 0.00001337
Iteration 217/1000 | Loss: 0.00001337
Iteration 218/1000 | Loss: 0.00001337
Iteration 219/1000 | Loss: 0.00001337
Iteration 220/1000 | Loss: 0.00001337
Iteration 221/1000 | Loss: 0.00001337
Iteration 222/1000 | Loss: 0.00001337
Iteration 223/1000 | Loss: 0.00001337
Iteration 224/1000 | Loss: 0.00001337
Iteration 225/1000 | Loss: 0.00001337
Iteration 226/1000 | Loss: 0.00001337
Iteration 227/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.3370789019973017e-05, 1.3370789019973017e-05, 1.3370789019973017e-05, 1.3370789019973017e-05, 1.3370789019973017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3370789019973017e-05

Optimization complete. Final v2v error: 3.0232183933258057 mm

Highest mean error: 4.904669761657715 mm for frame 62

Lowest mean error: 2.3697917461395264 mm for frame 114

Saving results

Total time: 120.02663230895996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522903
Iteration 2/25 | Loss: 0.00134612
Iteration 3/25 | Loss: 0.00113869
Iteration 4/25 | Loss: 0.00111753
Iteration 5/25 | Loss: 0.00111397
Iteration 6/25 | Loss: 0.00111339
Iteration 7/25 | Loss: 0.00111339
Iteration 8/25 | Loss: 0.00111339
Iteration 9/25 | Loss: 0.00111339
Iteration 10/25 | Loss: 0.00111339
Iteration 11/25 | Loss: 0.00111339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011133886873722076, 0.0011133886873722076, 0.0011133886873722076, 0.0011133886873722076, 0.0011133886873722076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011133886873722076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55829012
Iteration 2/25 | Loss: 0.00068498
Iteration 3/25 | Loss: 0.00068496
Iteration 4/25 | Loss: 0.00068496
Iteration 5/25 | Loss: 0.00068496
Iteration 6/25 | Loss: 0.00068496
Iteration 7/25 | Loss: 0.00068496
Iteration 8/25 | Loss: 0.00068496
Iteration 9/25 | Loss: 0.00068496
Iteration 10/25 | Loss: 0.00068496
Iteration 11/25 | Loss: 0.00068496
Iteration 12/25 | Loss: 0.00068496
Iteration 13/25 | Loss: 0.00068496
Iteration 14/25 | Loss: 0.00068496
Iteration 15/25 | Loss: 0.00068496
Iteration 16/25 | Loss: 0.00068496
Iteration 17/25 | Loss: 0.00068496
Iteration 18/25 | Loss: 0.00068496
Iteration 19/25 | Loss: 0.00068496
Iteration 20/25 | Loss: 0.00068496
Iteration 21/25 | Loss: 0.00068496
Iteration 22/25 | Loss: 0.00068496
Iteration 23/25 | Loss: 0.00068496
Iteration 24/25 | Loss: 0.00068496
Iteration 25/25 | Loss: 0.00068496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068496
Iteration 2/1000 | Loss: 0.00002235
Iteration 3/1000 | Loss: 0.00001547
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001325
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001235
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001143
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001115
Iteration 20/1000 | Loss: 0.00001114
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001103
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001098
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001089
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001088
Iteration 49/1000 | Loss: 0.00001088
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001086
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001083
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001079
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001078
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001074
Iteration 88/1000 | Loss: 0.00001074
Iteration 89/1000 | Loss: 0.00001074
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001073
Iteration 94/1000 | Loss: 0.00001072
Iteration 95/1000 | Loss: 0.00001072
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001068
Iteration 111/1000 | Loss: 0.00001068
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001065
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001064
Iteration 129/1000 | Loss: 0.00001064
Iteration 130/1000 | Loss: 0.00001064
Iteration 131/1000 | Loss: 0.00001064
Iteration 132/1000 | Loss: 0.00001064
Iteration 133/1000 | Loss: 0.00001064
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001063
Iteration 137/1000 | Loss: 0.00001063
Iteration 138/1000 | Loss: 0.00001063
Iteration 139/1000 | Loss: 0.00001063
Iteration 140/1000 | Loss: 0.00001063
Iteration 141/1000 | Loss: 0.00001063
Iteration 142/1000 | Loss: 0.00001063
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001062
Iteration 155/1000 | Loss: 0.00001062
Iteration 156/1000 | Loss: 0.00001062
Iteration 157/1000 | Loss: 0.00001062
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001062
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.061343755281996e-05, 1.061343755281996e-05, 1.061343755281996e-05, 1.061343755281996e-05, 1.061343755281996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.061343755281996e-05

Optimization complete. Final v2v error: 2.7921109199523926 mm

Highest mean error: 3.0918328762054443 mm for frame 12

Lowest mean error: 2.450683116912842 mm for frame 104

Saving results

Total time: 46.92918157577515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594819
Iteration 2/25 | Loss: 0.00145897
Iteration 3/25 | Loss: 0.00119458
Iteration 4/25 | Loss: 0.00116413
Iteration 5/25 | Loss: 0.00115965
Iteration 6/25 | Loss: 0.00115809
Iteration 7/25 | Loss: 0.00115798
Iteration 8/25 | Loss: 0.00115798
Iteration 9/25 | Loss: 0.00115798
Iteration 10/25 | Loss: 0.00115798
Iteration 11/25 | Loss: 0.00115798
Iteration 12/25 | Loss: 0.00115798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011579780839383602, 0.0011579780839383602, 0.0011579780839383602, 0.0011579780839383602, 0.0011579780839383602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011579780839383602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15788531
Iteration 2/25 | Loss: 0.00068617
Iteration 3/25 | Loss: 0.00068617
Iteration 4/25 | Loss: 0.00068617
Iteration 5/25 | Loss: 0.00068617
Iteration 6/25 | Loss: 0.00068617
Iteration 7/25 | Loss: 0.00068617
Iteration 8/25 | Loss: 0.00068617
Iteration 9/25 | Loss: 0.00068617
Iteration 10/25 | Loss: 0.00068617
Iteration 11/25 | Loss: 0.00068616
Iteration 12/25 | Loss: 0.00068617
Iteration 13/25 | Loss: 0.00068616
Iteration 14/25 | Loss: 0.00068616
Iteration 15/25 | Loss: 0.00068616
Iteration 16/25 | Loss: 0.00068616
Iteration 17/25 | Loss: 0.00068616
Iteration 18/25 | Loss: 0.00068616
Iteration 19/25 | Loss: 0.00068616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006861648871563375, 0.0006861648871563375, 0.0006861648871563375, 0.0006861648871563375, 0.0006861648871563375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006861648871563375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068616
Iteration 2/1000 | Loss: 0.00004227
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00001849
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001632
Iteration 7/1000 | Loss: 0.00001591
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001535
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001490
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001489
Iteration 18/1000 | Loss: 0.00001489
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001486
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001482
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001480
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001477
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001472
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001470
Iteration 63/1000 | Loss: 0.00001470
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001468
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001467
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001466
Iteration 73/1000 | Loss: 0.00001466
Iteration 74/1000 | Loss: 0.00001466
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001465
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001464
Iteration 83/1000 | Loss: 0.00001464
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001464
Iteration 86/1000 | Loss: 0.00001464
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001463
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001460
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001459
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001458
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001457
Iteration 118/1000 | Loss: 0.00001457
Iteration 119/1000 | Loss: 0.00001457
Iteration 120/1000 | Loss: 0.00001457
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001457
Iteration 123/1000 | Loss: 0.00001457
Iteration 124/1000 | Loss: 0.00001457
Iteration 125/1000 | Loss: 0.00001457
Iteration 126/1000 | Loss: 0.00001457
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001456
Iteration 130/1000 | Loss: 0.00001456
Iteration 131/1000 | Loss: 0.00001456
Iteration 132/1000 | Loss: 0.00001456
Iteration 133/1000 | Loss: 0.00001456
Iteration 134/1000 | Loss: 0.00001456
Iteration 135/1000 | Loss: 0.00001456
Iteration 136/1000 | Loss: 0.00001456
Iteration 137/1000 | Loss: 0.00001456
Iteration 138/1000 | Loss: 0.00001456
Iteration 139/1000 | Loss: 0.00001456
Iteration 140/1000 | Loss: 0.00001456
Iteration 141/1000 | Loss: 0.00001456
Iteration 142/1000 | Loss: 0.00001456
Iteration 143/1000 | Loss: 0.00001456
Iteration 144/1000 | Loss: 0.00001456
Iteration 145/1000 | Loss: 0.00001456
Iteration 146/1000 | Loss: 0.00001456
Iteration 147/1000 | Loss: 0.00001456
Iteration 148/1000 | Loss: 0.00001456
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001456
Iteration 151/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.4564161574526224e-05, 1.4564161574526224e-05, 1.4564161574526224e-05, 1.4564161574526224e-05, 1.4564161574526224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4564161574526224e-05

Optimization complete. Final v2v error: 3.061248779296875 mm

Highest mean error: 4.743281364440918 mm for frame 55

Lowest mean error: 2.6804826259613037 mm for frame 133

Saving results

Total time: 34.80602741241455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491775
Iteration 2/25 | Loss: 0.00121497
Iteration 3/25 | Loss: 0.00112401
Iteration 4/25 | Loss: 0.00110753
Iteration 5/25 | Loss: 0.00110194
Iteration 6/25 | Loss: 0.00110048
Iteration 7/25 | Loss: 0.00110046
Iteration 8/25 | Loss: 0.00110046
Iteration 9/25 | Loss: 0.00110046
Iteration 10/25 | Loss: 0.00110046
Iteration 11/25 | Loss: 0.00110046
Iteration 12/25 | Loss: 0.00110046
Iteration 13/25 | Loss: 0.00110046
Iteration 14/25 | Loss: 0.00110046
Iteration 15/25 | Loss: 0.00110046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011004633270204067, 0.0011004633270204067, 0.0011004633270204067, 0.0011004633270204067, 0.0011004633270204067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011004633270204067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.39987087
Iteration 2/25 | Loss: 0.00089090
Iteration 3/25 | Loss: 0.00089088
Iteration 4/25 | Loss: 0.00089088
Iteration 5/25 | Loss: 0.00089088
Iteration 6/25 | Loss: 0.00089088
Iteration 7/25 | Loss: 0.00089088
Iteration 8/25 | Loss: 0.00089088
Iteration 9/25 | Loss: 0.00089088
Iteration 10/25 | Loss: 0.00089088
Iteration 11/25 | Loss: 0.00089088
Iteration 12/25 | Loss: 0.00089088
Iteration 13/25 | Loss: 0.00089088
Iteration 14/25 | Loss: 0.00089088
Iteration 15/25 | Loss: 0.00089088
Iteration 16/25 | Loss: 0.00089088
Iteration 17/25 | Loss: 0.00089088
Iteration 18/25 | Loss: 0.00089088
Iteration 19/25 | Loss: 0.00089088
Iteration 20/25 | Loss: 0.00089088
Iteration 21/25 | Loss: 0.00089088
Iteration 22/25 | Loss: 0.00089088
Iteration 23/25 | Loss: 0.00089088
Iteration 24/25 | Loss: 0.00089088
Iteration 25/25 | Loss: 0.00089088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089088
Iteration 2/1000 | Loss: 0.00002832
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001866
Iteration 5/1000 | Loss: 0.00001801
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001696
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001566
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001537
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001523
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001522
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001519
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001518
Iteration 53/1000 | Loss: 0.00001518
Iteration 54/1000 | Loss: 0.00001518
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001517
Iteration 58/1000 | Loss: 0.00001517
Iteration 59/1000 | Loss: 0.00001517
Iteration 60/1000 | Loss: 0.00001517
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001516
Iteration 63/1000 | Loss: 0.00001516
Iteration 64/1000 | Loss: 0.00001516
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001515
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001509
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001508
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001507
Iteration 96/1000 | Loss: 0.00001507
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001501
Iteration 129/1000 | Loss: 0.00001501
Iteration 130/1000 | Loss: 0.00001501
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001500
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001499
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001497
Iteration 149/1000 | Loss: 0.00001497
Iteration 150/1000 | Loss: 0.00001497
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001497
Iteration 153/1000 | Loss: 0.00001497
Iteration 154/1000 | Loss: 0.00001497
Iteration 155/1000 | Loss: 0.00001497
Iteration 156/1000 | Loss: 0.00001497
Iteration 157/1000 | Loss: 0.00001497
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001496
Iteration 161/1000 | Loss: 0.00001496
Iteration 162/1000 | Loss: 0.00001496
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001495
Iteration 169/1000 | Loss: 0.00001495
Iteration 170/1000 | Loss: 0.00001495
Iteration 171/1000 | Loss: 0.00001495
Iteration 172/1000 | Loss: 0.00001495
Iteration 173/1000 | Loss: 0.00001495
Iteration 174/1000 | Loss: 0.00001495
Iteration 175/1000 | Loss: 0.00001495
Iteration 176/1000 | Loss: 0.00001495
Iteration 177/1000 | Loss: 0.00001495
Iteration 178/1000 | Loss: 0.00001495
Iteration 179/1000 | Loss: 0.00001495
Iteration 180/1000 | Loss: 0.00001495
Iteration 181/1000 | Loss: 0.00001495
Iteration 182/1000 | Loss: 0.00001495
Iteration 183/1000 | Loss: 0.00001495
Iteration 184/1000 | Loss: 0.00001495
Iteration 185/1000 | Loss: 0.00001494
Iteration 186/1000 | Loss: 0.00001494
Iteration 187/1000 | Loss: 0.00001494
Iteration 188/1000 | Loss: 0.00001494
Iteration 189/1000 | Loss: 0.00001494
Iteration 190/1000 | Loss: 0.00001494
Iteration 191/1000 | Loss: 0.00001494
Iteration 192/1000 | Loss: 0.00001494
Iteration 193/1000 | Loss: 0.00001494
Iteration 194/1000 | Loss: 0.00001494
Iteration 195/1000 | Loss: 0.00001494
Iteration 196/1000 | Loss: 0.00001494
Iteration 197/1000 | Loss: 0.00001494
Iteration 198/1000 | Loss: 0.00001494
Iteration 199/1000 | Loss: 0.00001494
Iteration 200/1000 | Loss: 0.00001494
Iteration 201/1000 | Loss: 0.00001494
Iteration 202/1000 | Loss: 0.00001494
Iteration 203/1000 | Loss: 0.00001494
Iteration 204/1000 | Loss: 0.00001494
Iteration 205/1000 | Loss: 0.00001494
Iteration 206/1000 | Loss: 0.00001494
Iteration 207/1000 | Loss: 0.00001494
Iteration 208/1000 | Loss: 0.00001494
Iteration 209/1000 | Loss: 0.00001494
Iteration 210/1000 | Loss: 0.00001494
Iteration 211/1000 | Loss: 0.00001494
Iteration 212/1000 | Loss: 0.00001494
Iteration 213/1000 | Loss: 0.00001494
Iteration 214/1000 | Loss: 0.00001494
Iteration 215/1000 | Loss: 0.00001494
Iteration 216/1000 | Loss: 0.00001494
Iteration 217/1000 | Loss: 0.00001494
Iteration 218/1000 | Loss: 0.00001494
Iteration 219/1000 | Loss: 0.00001494
Iteration 220/1000 | Loss: 0.00001494
Iteration 221/1000 | Loss: 0.00001494
Iteration 222/1000 | Loss: 0.00001494
Iteration 223/1000 | Loss: 0.00001494
Iteration 224/1000 | Loss: 0.00001494
Iteration 225/1000 | Loss: 0.00001494
Iteration 226/1000 | Loss: 0.00001494
Iteration 227/1000 | Loss: 0.00001494
Iteration 228/1000 | Loss: 0.00001494
Iteration 229/1000 | Loss: 0.00001494
Iteration 230/1000 | Loss: 0.00001494
Iteration 231/1000 | Loss: 0.00001494
Iteration 232/1000 | Loss: 0.00001494
Iteration 233/1000 | Loss: 0.00001494
Iteration 234/1000 | Loss: 0.00001494
Iteration 235/1000 | Loss: 0.00001494
Iteration 236/1000 | Loss: 0.00001494
Iteration 237/1000 | Loss: 0.00001494
Iteration 238/1000 | Loss: 0.00001494
Iteration 239/1000 | Loss: 0.00001494
Iteration 240/1000 | Loss: 0.00001494
Iteration 241/1000 | Loss: 0.00001494
Iteration 242/1000 | Loss: 0.00001494
Iteration 243/1000 | Loss: 0.00001494
Iteration 244/1000 | Loss: 0.00001494
Iteration 245/1000 | Loss: 0.00001494
Iteration 246/1000 | Loss: 0.00001494
Iteration 247/1000 | Loss: 0.00001494
Iteration 248/1000 | Loss: 0.00001494
Iteration 249/1000 | Loss: 0.00001494
Iteration 250/1000 | Loss: 0.00001494
Iteration 251/1000 | Loss: 0.00001494
Iteration 252/1000 | Loss: 0.00001494
Iteration 253/1000 | Loss: 0.00001494
Iteration 254/1000 | Loss: 0.00001494
Iteration 255/1000 | Loss: 0.00001494
Iteration 256/1000 | Loss: 0.00001494
Iteration 257/1000 | Loss: 0.00001494
Iteration 258/1000 | Loss: 0.00001494
Iteration 259/1000 | Loss: 0.00001494
Iteration 260/1000 | Loss: 0.00001494
Iteration 261/1000 | Loss: 0.00001494
Iteration 262/1000 | Loss: 0.00001494
Iteration 263/1000 | Loss: 0.00001494
Iteration 264/1000 | Loss: 0.00001494
Iteration 265/1000 | Loss: 0.00001494
Iteration 266/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.4942635061743204e-05, 1.4942635061743204e-05, 1.4942635061743204e-05, 1.4942635061743204e-05, 1.4942635061743204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4942635061743204e-05

Optimization complete. Final v2v error: 3.2675960063934326 mm

Highest mean error: 3.9236252307891846 mm for frame 36

Lowest mean error: 2.8161702156066895 mm for frame 132

Saving results

Total time: 43.945176124572754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663315
Iteration 2/25 | Loss: 0.00133005
Iteration 3/25 | Loss: 0.00122541
Iteration 4/25 | Loss: 0.00121623
Iteration 5/25 | Loss: 0.00121371
Iteration 6/25 | Loss: 0.00121358
Iteration 7/25 | Loss: 0.00121358
Iteration 8/25 | Loss: 0.00121358
Iteration 9/25 | Loss: 0.00121358
Iteration 10/25 | Loss: 0.00121358
Iteration 11/25 | Loss: 0.00121358
Iteration 12/25 | Loss: 0.00121358
Iteration 13/25 | Loss: 0.00121358
Iteration 14/25 | Loss: 0.00121358
Iteration 15/25 | Loss: 0.00121358
Iteration 16/25 | Loss: 0.00121358
Iteration 17/25 | Loss: 0.00121358
Iteration 18/25 | Loss: 0.00121358
Iteration 19/25 | Loss: 0.00121358
Iteration 20/25 | Loss: 0.00121358
Iteration 21/25 | Loss: 0.00121358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012135831639170647, 0.0012135831639170647, 0.0012135831639170647, 0.0012135831639170647, 0.0012135831639170647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012135831639170647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.05534744
Iteration 2/25 | Loss: 0.00074997
Iteration 3/25 | Loss: 0.00074994
Iteration 4/25 | Loss: 0.00074994
Iteration 5/25 | Loss: 0.00074994
Iteration 6/25 | Loss: 0.00074994
Iteration 7/25 | Loss: 0.00074994
Iteration 8/25 | Loss: 0.00074994
Iteration 9/25 | Loss: 0.00074994
Iteration 10/25 | Loss: 0.00074994
Iteration 11/25 | Loss: 0.00074994
Iteration 12/25 | Loss: 0.00074994
Iteration 13/25 | Loss: 0.00074994
Iteration 14/25 | Loss: 0.00074994
Iteration 15/25 | Loss: 0.00074994
Iteration 16/25 | Loss: 0.00074994
Iteration 17/25 | Loss: 0.00074994
Iteration 18/25 | Loss: 0.00074994
Iteration 19/25 | Loss: 0.00074994
Iteration 20/25 | Loss: 0.00074994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007499384810216725, 0.0007499384810216725, 0.0007499384810216725, 0.0007499384810216725, 0.0007499384810216725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007499384810216725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074994
Iteration 2/1000 | Loss: 0.00003422
Iteration 3/1000 | Loss: 0.00002156
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001781
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001667
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001620
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001613
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001612
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001593
Iteration 45/1000 | Loss: 0.00001593
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00001592
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001589
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001585
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001584
Iteration 85/1000 | Loss: 0.00001584
Iteration 86/1000 | Loss: 0.00001584
Iteration 87/1000 | Loss: 0.00001584
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001583
Iteration 90/1000 | Loss: 0.00001583
Iteration 91/1000 | Loss: 0.00001583
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001582
Iteration 94/1000 | Loss: 0.00001582
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001582
Iteration 97/1000 | Loss: 0.00001582
Iteration 98/1000 | Loss: 0.00001582
Iteration 99/1000 | Loss: 0.00001582
Iteration 100/1000 | Loss: 0.00001582
Iteration 101/1000 | Loss: 0.00001582
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001582
Iteration 104/1000 | Loss: 0.00001582
Iteration 105/1000 | Loss: 0.00001582
Iteration 106/1000 | Loss: 0.00001582
Iteration 107/1000 | Loss: 0.00001581
Iteration 108/1000 | Loss: 0.00001581
Iteration 109/1000 | Loss: 0.00001581
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001580
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001577
Iteration 125/1000 | Loss: 0.00001577
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001577
Iteration 129/1000 | Loss: 0.00001577
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001576
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001573
Iteration 150/1000 | Loss: 0.00001573
Iteration 151/1000 | Loss: 0.00001573
Iteration 152/1000 | Loss: 0.00001573
Iteration 153/1000 | Loss: 0.00001573
Iteration 154/1000 | Loss: 0.00001572
Iteration 155/1000 | Loss: 0.00001572
Iteration 156/1000 | Loss: 0.00001572
Iteration 157/1000 | Loss: 0.00001572
Iteration 158/1000 | Loss: 0.00001572
Iteration 159/1000 | Loss: 0.00001572
Iteration 160/1000 | Loss: 0.00001572
Iteration 161/1000 | Loss: 0.00001572
Iteration 162/1000 | Loss: 0.00001572
Iteration 163/1000 | Loss: 0.00001572
Iteration 164/1000 | Loss: 0.00001571
Iteration 165/1000 | Loss: 0.00001571
Iteration 166/1000 | Loss: 0.00001571
Iteration 167/1000 | Loss: 0.00001571
Iteration 168/1000 | Loss: 0.00001571
Iteration 169/1000 | Loss: 0.00001571
Iteration 170/1000 | Loss: 0.00001571
Iteration 171/1000 | Loss: 0.00001570
Iteration 172/1000 | Loss: 0.00001570
Iteration 173/1000 | Loss: 0.00001570
Iteration 174/1000 | Loss: 0.00001570
Iteration 175/1000 | Loss: 0.00001570
Iteration 176/1000 | Loss: 0.00001570
Iteration 177/1000 | Loss: 0.00001570
Iteration 178/1000 | Loss: 0.00001569
Iteration 179/1000 | Loss: 0.00001569
Iteration 180/1000 | Loss: 0.00001569
Iteration 181/1000 | Loss: 0.00001569
Iteration 182/1000 | Loss: 0.00001569
Iteration 183/1000 | Loss: 0.00001568
Iteration 184/1000 | Loss: 0.00001568
Iteration 185/1000 | Loss: 0.00001568
Iteration 186/1000 | Loss: 0.00001568
Iteration 187/1000 | Loss: 0.00001568
Iteration 188/1000 | Loss: 0.00001568
Iteration 189/1000 | Loss: 0.00001568
Iteration 190/1000 | Loss: 0.00001567
Iteration 191/1000 | Loss: 0.00001567
Iteration 192/1000 | Loss: 0.00001567
Iteration 193/1000 | Loss: 0.00001567
Iteration 194/1000 | Loss: 0.00001567
Iteration 195/1000 | Loss: 0.00001567
Iteration 196/1000 | Loss: 0.00001567
Iteration 197/1000 | Loss: 0.00001567
Iteration 198/1000 | Loss: 0.00001567
Iteration 199/1000 | Loss: 0.00001567
Iteration 200/1000 | Loss: 0.00001567
Iteration 201/1000 | Loss: 0.00001567
Iteration 202/1000 | Loss: 0.00001567
Iteration 203/1000 | Loss: 0.00001567
Iteration 204/1000 | Loss: 0.00001567
Iteration 205/1000 | Loss: 0.00001567
Iteration 206/1000 | Loss: 0.00001567
Iteration 207/1000 | Loss: 0.00001567
Iteration 208/1000 | Loss: 0.00001567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.5672934750909917e-05, 1.5672934750909917e-05, 1.5672934750909917e-05, 1.5672934750909917e-05, 1.5672934750909917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5672934750909917e-05

Optimization complete. Final v2v error: 3.3038840293884277 mm

Highest mean error: 3.811307430267334 mm for frame 61

Lowest mean error: 3.03253436088562 mm for frame 193

Saving results

Total time: 40.88796901702881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770748
Iteration 2/25 | Loss: 0.00131257
Iteration 3/25 | Loss: 0.00118595
Iteration 4/25 | Loss: 0.00116866
Iteration 5/25 | Loss: 0.00116455
Iteration 6/25 | Loss: 0.00116397
Iteration 7/25 | Loss: 0.00116397
Iteration 8/25 | Loss: 0.00116397
Iteration 9/25 | Loss: 0.00116397
Iteration 10/25 | Loss: 0.00116397
Iteration 11/25 | Loss: 0.00116397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001163969049230218, 0.001163969049230218, 0.001163969049230218, 0.001163969049230218, 0.001163969049230218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163969049230218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15519428
Iteration 2/25 | Loss: 0.00077554
Iteration 3/25 | Loss: 0.00077554
Iteration 4/25 | Loss: 0.00077554
Iteration 5/25 | Loss: 0.00077554
Iteration 6/25 | Loss: 0.00077554
Iteration 7/25 | Loss: 0.00077554
Iteration 8/25 | Loss: 0.00077554
Iteration 9/25 | Loss: 0.00077554
Iteration 10/25 | Loss: 0.00077554
Iteration 11/25 | Loss: 0.00077554
Iteration 12/25 | Loss: 0.00077554
Iteration 13/25 | Loss: 0.00077554
Iteration 14/25 | Loss: 0.00077554
Iteration 15/25 | Loss: 0.00077554
Iteration 16/25 | Loss: 0.00077554
Iteration 17/25 | Loss: 0.00077554
Iteration 18/25 | Loss: 0.00077554
Iteration 19/25 | Loss: 0.00077554
Iteration 20/25 | Loss: 0.00077554
Iteration 21/25 | Loss: 0.00077554
Iteration 22/25 | Loss: 0.00077554
Iteration 23/25 | Loss: 0.00077554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007755352999083698, 0.0007755352999083698, 0.0007755352999083698, 0.0007755352999083698, 0.0007755352999083698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007755352999083698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077554
Iteration 2/1000 | Loss: 0.00004283
Iteration 3/1000 | Loss: 0.00002460
Iteration 4/1000 | Loss: 0.00001925
Iteration 5/1000 | Loss: 0.00001810
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001624
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001593
Iteration 25/1000 | Loss: 0.00001593
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001579
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001573
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001573
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001571
Iteration 68/1000 | Loss: 0.00001571
Iteration 69/1000 | Loss: 0.00001571
Iteration 70/1000 | Loss: 0.00001571
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001570
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001569
Iteration 78/1000 | Loss: 0.00001569
Iteration 79/1000 | Loss: 0.00001569
Iteration 80/1000 | Loss: 0.00001568
Iteration 81/1000 | Loss: 0.00001568
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001563
Iteration 102/1000 | Loss: 0.00001563
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001562
Iteration 109/1000 | Loss: 0.00001562
Iteration 110/1000 | Loss: 0.00001562
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001561
Iteration 113/1000 | Loss: 0.00001561
Iteration 114/1000 | Loss: 0.00001561
Iteration 115/1000 | Loss: 0.00001561
Iteration 116/1000 | Loss: 0.00001561
Iteration 117/1000 | Loss: 0.00001561
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001560
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001560
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001560
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001558
Iteration 146/1000 | Loss: 0.00001558
Iteration 147/1000 | Loss: 0.00001558
Iteration 148/1000 | Loss: 0.00001558
Iteration 149/1000 | Loss: 0.00001558
Iteration 150/1000 | Loss: 0.00001558
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001557
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001557
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001555
Iteration 161/1000 | Loss: 0.00001555
Iteration 162/1000 | Loss: 0.00001555
Iteration 163/1000 | Loss: 0.00001555
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001554
Iteration 166/1000 | Loss: 0.00001554
Iteration 167/1000 | Loss: 0.00001554
Iteration 168/1000 | Loss: 0.00001554
Iteration 169/1000 | Loss: 0.00001554
Iteration 170/1000 | Loss: 0.00001554
Iteration 171/1000 | Loss: 0.00001554
Iteration 172/1000 | Loss: 0.00001554
Iteration 173/1000 | Loss: 0.00001554
Iteration 174/1000 | Loss: 0.00001554
Iteration 175/1000 | Loss: 0.00001554
Iteration 176/1000 | Loss: 0.00001554
Iteration 177/1000 | Loss: 0.00001553
Iteration 178/1000 | Loss: 0.00001553
Iteration 179/1000 | Loss: 0.00001553
Iteration 180/1000 | Loss: 0.00001553
Iteration 181/1000 | Loss: 0.00001553
Iteration 182/1000 | Loss: 0.00001553
Iteration 183/1000 | Loss: 0.00001553
Iteration 184/1000 | Loss: 0.00001553
Iteration 185/1000 | Loss: 0.00001553
Iteration 186/1000 | Loss: 0.00001553
Iteration 187/1000 | Loss: 0.00001553
Iteration 188/1000 | Loss: 0.00001553
Iteration 189/1000 | Loss: 0.00001553
Iteration 190/1000 | Loss: 0.00001552
Iteration 191/1000 | Loss: 0.00001552
Iteration 192/1000 | Loss: 0.00001552
Iteration 193/1000 | Loss: 0.00001552
Iteration 194/1000 | Loss: 0.00001552
Iteration 195/1000 | Loss: 0.00001552
Iteration 196/1000 | Loss: 0.00001552
Iteration 197/1000 | Loss: 0.00001552
Iteration 198/1000 | Loss: 0.00001552
Iteration 199/1000 | Loss: 0.00001552
Iteration 200/1000 | Loss: 0.00001552
Iteration 201/1000 | Loss: 0.00001552
Iteration 202/1000 | Loss: 0.00001552
Iteration 203/1000 | Loss: 0.00001552
Iteration 204/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.551673085486982e-05, 1.551673085486982e-05, 1.551673085486982e-05, 1.551673085486982e-05, 1.551673085486982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.551673085486982e-05

Optimization complete. Final v2v error: 3.277493715286255 mm

Highest mean error: 4.344616413116455 mm for frame 211

Lowest mean error: 2.699383497238159 mm for frame 144

Saving results

Total time: 44.19467544555664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794069
Iteration 2/25 | Loss: 0.00135980
Iteration 3/25 | Loss: 0.00111001
Iteration 4/25 | Loss: 0.00109504
Iteration 5/25 | Loss: 0.00108983
Iteration 6/25 | Loss: 0.00108831
Iteration 7/25 | Loss: 0.00108831
Iteration 8/25 | Loss: 0.00108831
Iteration 9/25 | Loss: 0.00108831
Iteration 10/25 | Loss: 0.00108831
Iteration 11/25 | Loss: 0.00108831
Iteration 12/25 | Loss: 0.00108831
Iteration 13/25 | Loss: 0.00108831
Iteration 14/25 | Loss: 0.00108831
Iteration 15/25 | Loss: 0.00108831
Iteration 16/25 | Loss: 0.00108831
Iteration 17/25 | Loss: 0.00108831
Iteration 18/25 | Loss: 0.00108831
Iteration 19/25 | Loss: 0.00108831
Iteration 20/25 | Loss: 0.00108831
Iteration 21/25 | Loss: 0.00108831
Iteration 22/25 | Loss: 0.00108831
Iteration 23/25 | Loss: 0.00108831
Iteration 24/25 | Loss: 0.00108831
Iteration 25/25 | Loss: 0.00108831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11183584
Iteration 2/25 | Loss: 0.00085467
Iteration 3/25 | Loss: 0.00085467
Iteration 4/25 | Loss: 0.00085467
Iteration 5/25 | Loss: 0.00085466
Iteration 6/25 | Loss: 0.00085466
Iteration 7/25 | Loss: 0.00085466
Iteration 8/25 | Loss: 0.00085466
Iteration 9/25 | Loss: 0.00085466
Iteration 10/25 | Loss: 0.00085466
Iteration 11/25 | Loss: 0.00085466
Iteration 12/25 | Loss: 0.00085466
Iteration 13/25 | Loss: 0.00085466
Iteration 14/25 | Loss: 0.00085466
Iteration 15/25 | Loss: 0.00085466
Iteration 16/25 | Loss: 0.00085466
Iteration 17/25 | Loss: 0.00085466
Iteration 18/25 | Loss: 0.00085466
Iteration 19/25 | Loss: 0.00085466
Iteration 20/25 | Loss: 0.00085466
Iteration 21/25 | Loss: 0.00085466
Iteration 22/25 | Loss: 0.00085466
Iteration 23/25 | Loss: 0.00085466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008546635508537292, 0.0008546635508537292, 0.0008546635508537292, 0.0008546635508537292, 0.0008546635508537292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008546635508537292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085466
Iteration 2/1000 | Loss: 0.00004008
Iteration 3/1000 | Loss: 0.00002735
Iteration 4/1000 | Loss: 0.00002144
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001667
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001390
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001377
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001373
Iteration 32/1000 | Loss: 0.00001373
Iteration 33/1000 | Loss: 0.00001373
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001369
Iteration 38/1000 | Loss: 0.00001368
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001366
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001352
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001351
Iteration 78/1000 | Loss: 0.00001351
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001350
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001348
Iteration 108/1000 | Loss: 0.00001348
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.3484430382959545e-05, 1.3484430382959545e-05, 1.3484430382959545e-05, 1.3484430382959545e-05, 1.3484430382959545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3484430382959545e-05

Optimization complete. Final v2v error: 2.99454665184021 mm

Highest mean error: 4.146059989929199 mm for frame 81

Lowest mean error: 2.283726215362549 mm for frame 197

Saving results

Total time: 44.7927360534668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418445
Iteration 2/25 | Loss: 0.00144453
Iteration 3/25 | Loss: 0.00120860
Iteration 4/25 | Loss: 0.00117315
Iteration 5/25 | Loss: 0.00116609
Iteration 6/25 | Loss: 0.00116434
Iteration 7/25 | Loss: 0.00116434
Iteration 8/25 | Loss: 0.00116434
Iteration 9/25 | Loss: 0.00116434
Iteration 10/25 | Loss: 0.00116434
Iteration 11/25 | Loss: 0.00116434
Iteration 12/25 | Loss: 0.00116434
Iteration 13/25 | Loss: 0.00116434
Iteration 14/25 | Loss: 0.00116434
Iteration 15/25 | Loss: 0.00116434
Iteration 16/25 | Loss: 0.00116434
Iteration 17/25 | Loss: 0.00116434
Iteration 18/25 | Loss: 0.00116434
Iteration 19/25 | Loss: 0.00116434
Iteration 20/25 | Loss: 0.00116434
Iteration 21/25 | Loss: 0.00116434
Iteration 22/25 | Loss: 0.00116434
Iteration 23/25 | Loss: 0.00116434
Iteration 24/25 | Loss: 0.00116434
Iteration 25/25 | Loss: 0.00116434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35843670
Iteration 2/25 | Loss: 0.00056640
Iteration 3/25 | Loss: 0.00056640
Iteration 4/25 | Loss: 0.00056640
Iteration 5/25 | Loss: 0.00056640
Iteration 6/25 | Loss: 0.00056640
Iteration 7/25 | Loss: 0.00056640
Iteration 8/25 | Loss: 0.00056640
Iteration 9/25 | Loss: 0.00056640
Iteration 10/25 | Loss: 0.00056640
Iteration 11/25 | Loss: 0.00056640
Iteration 12/25 | Loss: 0.00056640
Iteration 13/25 | Loss: 0.00056640
Iteration 14/25 | Loss: 0.00056640
Iteration 15/25 | Loss: 0.00056640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005663976771757007, 0.0005663976771757007, 0.0005663976771757007, 0.0005663976771757007, 0.0005663976771757007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005663976771757007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056640
Iteration 2/1000 | Loss: 0.00004494
Iteration 3/1000 | Loss: 0.00002642
Iteration 4/1000 | Loss: 0.00002319
Iteration 5/1000 | Loss: 0.00002229
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002082
Iteration 8/1000 | Loss: 0.00002036
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001960
Iteration 12/1000 | Loss: 0.00001931
Iteration 13/1000 | Loss: 0.00001917
Iteration 14/1000 | Loss: 0.00001914
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001904
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001895
Iteration 21/1000 | Loss: 0.00001894
Iteration 22/1000 | Loss: 0.00001892
Iteration 23/1000 | Loss: 0.00001891
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001890
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001888
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001884
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001881
Iteration 38/1000 | Loss: 0.00001881
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001881
Iteration 43/1000 | Loss: 0.00001880
Iteration 44/1000 | Loss: 0.00001880
Iteration 45/1000 | Loss: 0.00001880
Iteration 46/1000 | Loss: 0.00001879
Iteration 47/1000 | Loss: 0.00001879
Iteration 48/1000 | Loss: 0.00001879
Iteration 49/1000 | Loss: 0.00001878
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001877
Iteration 53/1000 | Loss: 0.00001877
Iteration 54/1000 | Loss: 0.00001877
Iteration 55/1000 | Loss: 0.00001877
Iteration 56/1000 | Loss: 0.00001877
Iteration 57/1000 | Loss: 0.00001877
Iteration 58/1000 | Loss: 0.00001877
Iteration 59/1000 | Loss: 0.00001876
Iteration 60/1000 | Loss: 0.00001876
Iteration 61/1000 | Loss: 0.00001876
Iteration 62/1000 | Loss: 0.00001875
Iteration 63/1000 | Loss: 0.00001875
Iteration 64/1000 | Loss: 0.00001875
Iteration 65/1000 | Loss: 0.00001874
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001874
Iteration 68/1000 | Loss: 0.00001874
Iteration 69/1000 | Loss: 0.00001873
Iteration 70/1000 | Loss: 0.00001873
Iteration 71/1000 | Loss: 0.00001873
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001871
Iteration 76/1000 | Loss: 0.00001871
Iteration 77/1000 | Loss: 0.00001871
Iteration 78/1000 | Loss: 0.00001871
Iteration 79/1000 | Loss: 0.00001871
Iteration 80/1000 | Loss: 0.00001871
Iteration 81/1000 | Loss: 0.00001871
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001870
Iteration 84/1000 | Loss: 0.00001870
Iteration 85/1000 | Loss: 0.00001870
Iteration 86/1000 | Loss: 0.00001870
Iteration 87/1000 | Loss: 0.00001870
Iteration 88/1000 | Loss: 0.00001870
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001869
Iteration 94/1000 | Loss: 0.00001869
Iteration 95/1000 | Loss: 0.00001869
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001869
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001869
Iteration 104/1000 | Loss: 0.00001869
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001868
Iteration 107/1000 | Loss: 0.00001868
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Iteration 123/1000 | Loss: 0.00001868
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8678538253880106e-05, 1.8678538253880106e-05, 1.8678538253880106e-05, 1.8678538253880106e-05, 1.8678538253880106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8678538253880106e-05

Optimization complete. Final v2v error: 3.609710693359375 mm

Highest mean error: 3.9672768115997314 mm for frame 62

Lowest mean error: 3.160609006881714 mm for frame 0

Saving results

Total time: 38.9624125957489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475831
Iteration 2/25 | Loss: 0.00117746
Iteration 3/25 | Loss: 0.00109646
Iteration 4/25 | Loss: 0.00108361
Iteration 5/25 | Loss: 0.00107998
Iteration 6/25 | Loss: 0.00107937
Iteration 7/25 | Loss: 0.00107937
Iteration 8/25 | Loss: 0.00107937
Iteration 9/25 | Loss: 0.00107937
Iteration 10/25 | Loss: 0.00107937
Iteration 11/25 | Loss: 0.00107937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001079366309568286, 0.001079366309568286, 0.001079366309568286, 0.001079366309568286, 0.001079366309568286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001079366309568286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71397495
Iteration 2/25 | Loss: 0.00080505
Iteration 3/25 | Loss: 0.00080504
Iteration 4/25 | Loss: 0.00080504
Iteration 5/25 | Loss: 0.00080504
Iteration 6/25 | Loss: 0.00080504
Iteration 7/25 | Loss: 0.00080504
Iteration 8/25 | Loss: 0.00080504
Iteration 9/25 | Loss: 0.00080504
Iteration 10/25 | Loss: 0.00080504
Iteration 11/25 | Loss: 0.00080504
Iteration 12/25 | Loss: 0.00080504
Iteration 13/25 | Loss: 0.00080504
Iteration 14/25 | Loss: 0.00080504
Iteration 15/25 | Loss: 0.00080504
Iteration 16/25 | Loss: 0.00080504
Iteration 17/25 | Loss: 0.00080504
Iteration 18/25 | Loss: 0.00080504
Iteration 19/25 | Loss: 0.00080504
Iteration 20/25 | Loss: 0.00080504
Iteration 21/25 | Loss: 0.00080504
Iteration 22/25 | Loss: 0.00080504
Iteration 23/25 | Loss: 0.00080504
Iteration 24/25 | Loss: 0.00080504
Iteration 25/25 | Loss: 0.00080504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080504
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00001799
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001384
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001241
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001168
Iteration 12/1000 | Loss: 0.00001163
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001149
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001141
Iteration 21/1000 | Loss: 0.00001140
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001124
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001109
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001103
Iteration 48/1000 | Loss: 0.00001103
Iteration 49/1000 | Loss: 0.00001102
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001094
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001093
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001086
Iteration 113/1000 | Loss: 0.00001086
Iteration 114/1000 | Loss: 0.00001086
Iteration 115/1000 | Loss: 0.00001086
Iteration 116/1000 | Loss: 0.00001086
Iteration 117/1000 | Loss: 0.00001086
Iteration 118/1000 | Loss: 0.00001086
Iteration 119/1000 | Loss: 0.00001086
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001085
Iteration 124/1000 | Loss: 0.00001085
Iteration 125/1000 | Loss: 0.00001085
Iteration 126/1000 | Loss: 0.00001085
Iteration 127/1000 | Loss: 0.00001085
Iteration 128/1000 | Loss: 0.00001085
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001085
Iteration 131/1000 | Loss: 0.00001085
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001085
Iteration 140/1000 | Loss: 0.00001085
Iteration 141/1000 | Loss: 0.00001085
Iteration 142/1000 | Loss: 0.00001085
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001084
Iteration 148/1000 | Loss: 0.00001084
Iteration 149/1000 | Loss: 0.00001084
Iteration 150/1000 | Loss: 0.00001084
Iteration 151/1000 | Loss: 0.00001084
Iteration 152/1000 | Loss: 0.00001084
Iteration 153/1000 | Loss: 0.00001084
Iteration 154/1000 | Loss: 0.00001084
Iteration 155/1000 | Loss: 0.00001083
Iteration 156/1000 | Loss: 0.00001083
Iteration 157/1000 | Loss: 0.00001083
Iteration 158/1000 | Loss: 0.00001083
Iteration 159/1000 | Loss: 0.00001083
Iteration 160/1000 | Loss: 0.00001083
Iteration 161/1000 | Loss: 0.00001083
Iteration 162/1000 | Loss: 0.00001083
Iteration 163/1000 | Loss: 0.00001083
Iteration 164/1000 | Loss: 0.00001083
Iteration 165/1000 | Loss: 0.00001083
Iteration 166/1000 | Loss: 0.00001083
Iteration 167/1000 | Loss: 0.00001083
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001083
Iteration 170/1000 | Loss: 0.00001083
Iteration 171/1000 | Loss: 0.00001083
Iteration 172/1000 | Loss: 0.00001083
Iteration 173/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.0830174687725957e-05, 1.0830174687725957e-05, 1.0830174687725957e-05, 1.0830174687725957e-05, 1.0830174687725957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0830174687725957e-05

Optimization complete. Final v2v error: 2.853395938873291 mm

Highest mean error: 3.3146867752075195 mm for frame 126

Lowest mean error: 2.6690733432769775 mm for frame 15

Saving results

Total time: 39.70313334465027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474493
Iteration 2/25 | Loss: 0.00118858
Iteration 3/25 | Loss: 0.00111974
Iteration 4/25 | Loss: 0.00110889
Iteration 5/25 | Loss: 0.00110570
Iteration 6/25 | Loss: 0.00110570
Iteration 7/25 | Loss: 0.00110570
Iteration 8/25 | Loss: 0.00110570
Iteration 9/25 | Loss: 0.00110570
Iteration 10/25 | Loss: 0.00110570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011056950315833092, 0.0011056950315833092, 0.0011056950315833092, 0.0011056950315833092, 0.0011056950315833092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011056950315833092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37985873
Iteration 2/25 | Loss: 0.00085251
Iteration 3/25 | Loss: 0.00085251
Iteration 4/25 | Loss: 0.00085251
Iteration 5/25 | Loss: 0.00085251
Iteration 6/25 | Loss: 0.00085251
Iteration 7/25 | Loss: 0.00085251
Iteration 8/25 | Loss: 0.00085251
Iteration 9/25 | Loss: 0.00085251
Iteration 10/25 | Loss: 0.00085251
Iteration 11/25 | Loss: 0.00085251
Iteration 12/25 | Loss: 0.00085251
Iteration 13/25 | Loss: 0.00085251
Iteration 14/25 | Loss: 0.00085251
Iteration 15/25 | Loss: 0.00085251
Iteration 16/25 | Loss: 0.00085251
Iteration 17/25 | Loss: 0.00085251
Iteration 18/25 | Loss: 0.00085251
Iteration 19/25 | Loss: 0.00085251
Iteration 20/25 | Loss: 0.00085251
Iteration 21/25 | Loss: 0.00085251
Iteration 22/25 | Loss: 0.00085251
Iteration 23/25 | Loss: 0.00085251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008525062585249543, 0.0008525062585249543, 0.0008525062585249543, 0.0008525062585249543, 0.0008525062585249543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008525062585249543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085251
Iteration 2/1000 | Loss: 0.00002179
Iteration 3/1000 | Loss: 0.00001704
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001408
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001365
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001361
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001349
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001347
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001344
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001328
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001321
Iteration 103/1000 | Loss: 0.00001321
Iteration 104/1000 | Loss: 0.00001321
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001320
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001315
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001314
Iteration 140/1000 | Loss: 0.00001314
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001313
Iteration 144/1000 | Loss: 0.00001313
Iteration 145/1000 | Loss: 0.00001313
Iteration 146/1000 | Loss: 0.00001313
Iteration 147/1000 | Loss: 0.00001313
Iteration 148/1000 | Loss: 0.00001312
Iteration 149/1000 | Loss: 0.00001312
Iteration 150/1000 | Loss: 0.00001312
Iteration 151/1000 | Loss: 0.00001312
Iteration 152/1000 | Loss: 0.00001311
Iteration 153/1000 | Loss: 0.00001311
Iteration 154/1000 | Loss: 0.00001311
Iteration 155/1000 | Loss: 0.00001311
Iteration 156/1000 | Loss: 0.00001311
Iteration 157/1000 | Loss: 0.00001311
Iteration 158/1000 | Loss: 0.00001311
Iteration 159/1000 | Loss: 0.00001311
Iteration 160/1000 | Loss: 0.00001311
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001310
Iteration 164/1000 | Loss: 0.00001310
Iteration 165/1000 | Loss: 0.00001310
Iteration 166/1000 | Loss: 0.00001310
Iteration 167/1000 | Loss: 0.00001310
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001308
Iteration 191/1000 | Loss: 0.00001307
Iteration 192/1000 | Loss: 0.00001307
Iteration 193/1000 | Loss: 0.00001307
Iteration 194/1000 | Loss: 0.00001307
Iteration 195/1000 | Loss: 0.00001307
Iteration 196/1000 | Loss: 0.00001307
Iteration 197/1000 | Loss: 0.00001307
Iteration 198/1000 | Loss: 0.00001307
Iteration 199/1000 | Loss: 0.00001307
Iteration 200/1000 | Loss: 0.00001307
Iteration 201/1000 | Loss: 0.00001307
Iteration 202/1000 | Loss: 0.00001307
Iteration 203/1000 | Loss: 0.00001307
Iteration 204/1000 | Loss: 0.00001307
Iteration 205/1000 | Loss: 0.00001307
Iteration 206/1000 | Loss: 0.00001307
Iteration 207/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.307134243688779e-05, 1.307134243688779e-05, 1.307134243688779e-05, 1.307134243688779e-05, 1.307134243688779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.307134243688779e-05

Optimization complete. Final v2v error: 2.9916322231292725 mm

Highest mean error: 3.183601140975952 mm for frame 178

Lowest mean error: 2.755089521408081 mm for frame 17

Saving results

Total time: 45.23134779930115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468035
Iteration 2/25 | Loss: 0.00119108
Iteration 3/25 | Loss: 0.00111296
Iteration 4/25 | Loss: 0.00110056
Iteration 5/25 | Loss: 0.00109615
Iteration 6/25 | Loss: 0.00109529
Iteration 7/25 | Loss: 0.00109529
Iteration 8/25 | Loss: 0.00109529
Iteration 9/25 | Loss: 0.00109529
Iteration 10/25 | Loss: 0.00109529
Iteration 11/25 | Loss: 0.00109529
Iteration 12/25 | Loss: 0.00109529
Iteration 13/25 | Loss: 0.00109529
Iteration 14/25 | Loss: 0.00109529
Iteration 15/25 | Loss: 0.00109529
Iteration 16/25 | Loss: 0.00109529
Iteration 17/25 | Loss: 0.00109529
Iteration 18/25 | Loss: 0.00109529
Iteration 19/25 | Loss: 0.00109529
Iteration 20/25 | Loss: 0.00109529
Iteration 21/25 | Loss: 0.00109529
Iteration 22/25 | Loss: 0.00109529
Iteration 23/25 | Loss: 0.00109529
Iteration 24/25 | Loss: 0.00109529
Iteration 25/25 | Loss: 0.00109529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50574255
Iteration 2/25 | Loss: 0.00085686
Iteration 3/25 | Loss: 0.00085685
Iteration 4/25 | Loss: 0.00085685
Iteration 5/25 | Loss: 0.00085685
Iteration 6/25 | Loss: 0.00085685
Iteration 7/25 | Loss: 0.00085685
Iteration 8/25 | Loss: 0.00085685
Iteration 9/25 | Loss: 0.00085685
Iteration 10/25 | Loss: 0.00085685
Iteration 11/25 | Loss: 0.00085685
Iteration 12/25 | Loss: 0.00085685
Iteration 13/25 | Loss: 0.00085685
Iteration 14/25 | Loss: 0.00085685
Iteration 15/25 | Loss: 0.00085685
Iteration 16/25 | Loss: 0.00085685
Iteration 17/25 | Loss: 0.00085685
Iteration 18/25 | Loss: 0.00085685
Iteration 19/25 | Loss: 0.00085685
Iteration 20/25 | Loss: 0.00085685
Iteration 21/25 | Loss: 0.00085685
Iteration 22/25 | Loss: 0.00085685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008568514022044837, 0.0008568514022044837, 0.0008568514022044837, 0.0008568514022044837, 0.0008568514022044837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008568514022044837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085685
Iteration 2/1000 | Loss: 0.00003153
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001355
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001188
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001143
Iteration 13/1000 | Loss: 0.00001143
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001110
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001091
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001084
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001083
Iteration 58/1000 | Loss: 0.00001083
Iteration 59/1000 | Loss: 0.00001083
Iteration 60/1000 | Loss: 0.00001083
Iteration 61/1000 | Loss: 0.00001083
Iteration 62/1000 | Loss: 0.00001083
Iteration 63/1000 | Loss: 0.00001083
Iteration 64/1000 | Loss: 0.00001083
Iteration 65/1000 | Loss: 0.00001083
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001082
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001080
Iteration 82/1000 | Loss: 0.00001080
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001079
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001078
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001073
Iteration 112/1000 | Loss: 0.00001073
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001069
Iteration 132/1000 | Loss: 0.00001069
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001068
Iteration 137/1000 | Loss: 0.00001068
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001067
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001067
Iteration 145/1000 | Loss: 0.00001066
Iteration 146/1000 | Loss: 0.00001066
Iteration 147/1000 | Loss: 0.00001066
Iteration 148/1000 | Loss: 0.00001066
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001066
Iteration 153/1000 | Loss: 0.00001066
Iteration 154/1000 | Loss: 0.00001066
Iteration 155/1000 | Loss: 0.00001066
Iteration 156/1000 | Loss: 0.00001066
Iteration 157/1000 | Loss: 0.00001066
Iteration 158/1000 | Loss: 0.00001066
Iteration 159/1000 | Loss: 0.00001066
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001066
Iteration 162/1000 | Loss: 0.00001066
Iteration 163/1000 | Loss: 0.00001066
Iteration 164/1000 | Loss: 0.00001066
Iteration 165/1000 | Loss: 0.00001065
Iteration 166/1000 | Loss: 0.00001065
Iteration 167/1000 | Loss: 0.00001065
Iteration 168/1000 | Loss: 0.00001065
Iteration 169/1000 | Loss: 0.00001065
Iteration 170/1000 | Loss: 0.00001065
Iteration 171/1000 | Loss: 0.00001064
Iteration 172/1000 | Loss: 0.00001064
Iteration 173/1000 | Loss: 0.00001064
Iteration 174/1000 | Loss: 0.00001064
Iteration 175/1000 | Loss: 0.00001064
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001064
Iteration 182/1000 | Loss: 0.00001064
Iteration 183/1000 | Loss: 0.00001064
Iteration 184/1000 | Loss: 0.00001064
Iteration 185/1000 | Loss: 0.00001064
Iteration 186/1000 | Loss: 0.00001064
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.0639227184583433e-05, 1.0639227184583433e-05, 1.0639227184583433e-05, 1.0639227184583433e-05, 1.0639227184583433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0639227184583433e-05

Optimization complete. Final v2v error: 2.733114719390869 mm

Highest mean error: 3.31829571723938 mm for frame 40

Lowest mean error: 2.4681594371795654 mm for frame 101

Saving results

Total time: 37.970964193344116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014480
Iteration 2/25 | Loss: 0.01014480
Iteration 3/25 | Loss: 0.01014480
Iteration 4/25 | Loss: 0.01014479
Iteration 5/25 | Loss: 0.01014479
Iteration 6/25 | Loss: 0.01014479
Iteration 7/25 | Loss: 0.01014479
Iteration 8/25 | Loss: 0.01014479
Iteration 9/25 | Loss: 0.01014479
Iteration 10/25 | Loss: 0.01014479
Iteration 11/25 | Loss: 0.01014478
Iteration 12/25 | Loss: 0.01014478
Iteration 13/25 | Loss: 0.01014478
Iteration 14/25 | Loss: 0.01014478
Iteration 15/25 | Loss: 0.01014478
Iteration 16/25 | Loss: 0.01014478
Iteration 17/25 | Loss: 0.01014478
Iteration 18/25 | Loss: 0.01014477
Iteration 19/25 | Loss: 0.01014477
Iteration 20/25 | Loss: 0.01014477
Iteration 21/25 | Loss: 0.01014477
Iteration 22/25 | Loss: 0.01014477
Iteration 23/25 | Loss: 0.01014476
Iteration 24/25 | Loss: 0.01014476
Iteration 25/25 | Loss: 0.01014476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52074790
Iteration 2/25 | Loss: 0.13636087
Iteration 3/25 | Loss: 0.13522978
Iteration 4/25 | Loss: 0.13262285
Iteration 5/25 | Loss: 0.13245311
Iteration 6/25 | Loss: 0.13245311
Iteration 7/25 | Loss: 0.13245310
Iteration 8/25 | Loss: 0.13245310
Iteration 9/25 | Loss: 0.13245310
Iteration 10/25 | Loss: 0.13245308
Iteration 11/25 | Loss: 0.13245307
Iteration 12/25 | Loss: 0.13245307
Iteration 13/25 | Loss: 0.13245307
Iteration 14/25 | Loss: 0.13245307
Iteration 15/25 | Loss: 0.13245307
Iteration 16/25 | Loss: 0.13245307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.1324530690908432, 0.1324530690908432, 0.1324530690908432, 0.1324530690908432, 0.1324530690908432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1324530690908432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13245307
Iteration 2/1000 | Loss: 0.00224940
Iteration 3/1000 | Loss: 0.00135552
Iteration 4/1000 | Loss: 0.00078012
Iteration 5/1000 | Loss: 0.00025677
Iteration 6/1000 | Loss: 0.00088572
Iteration 7/1000 | Loss: 0.00105920
Iteration 8/1000 | Loss: 0.00006823
Iteration 9/1000 | Loss: 0.00004371
Iteration 10/1000 | Loss: 0.00021450
Iteration 11/1000 | Loss: 0.00031673
Iteration 12/1000 | Loss: 0.00217360
Iteration 13/1000 | Loss: 0.00056963
Iteration 14/1000 | Loss: 0.00071032
Iteration 15/1000 | Loss: 0.00164811
Iteration 16/1000 | Loss: 0.00218175
Iteration 17/1000 | Loss: 0.00189645
Iteration 18/1000 | Loss: 0.00222227
Iteration 19/1000 | Loss: 0.00203175
Iteration 20/1000 | Loss: 0.00007499
Iteration 21/1000 | Loss: 0.00003782
Iteration 22/1000 | Loss: 0.00002690
Iteration 23/1000 | Loss: 0.00002334
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00010808
Iteration 26/1000 | Loss: 0.00012240
Iteration 27/1000 | Loss: 0.00063365
Iteration 28/1000 | Loss: 0.00140591
Iteration 29/1000 | Loss: 0.00050198
Iteration 30/1000 | Loss: 0.00047563
Iteration 31/1000 | Loss: 0.00002730
Iteration 32/1000 | Loss: 0.00004677
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00006270
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001773
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001660
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00018142
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001407
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00009507
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001362
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001361
Iteration 57/1000 | Loss: 0.00001360
Iteration 58/1000 | Loss: 0.00001360
Iteration 59/1000 | Loss: 0.00001360
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001354
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001353
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001348
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.3464743460644968e-05, 1.3464743460644968e-05, 1.3464743460644968e-05, 1.3464743460644968e-05, 1.3464743460644968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3464743460644968e-05

Optimization complete. Final v2v error: 3.1403753757476807 mm

Highest mean error: 3.9007909297943115 mm for frame 165

Lowest mean error: 2.735745668411255 mm for frame 57

Saving results

Total time: 92.93181777000427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391622
Iteration 2/25 | Loss: 0.00119319
Iteration 3/25 | Loss: 0.00109753
Iteration 4/25 | Loss: 0.00108789
Iteration 5/25 | Loss: 0.00108363
Iteration 6/25 | Loss: 0.00108196
Iteration 7/25 | Loss: 0.00108173
Iteration 8/25 | Loss: 0.00108173
Iteration 9/25 | Loss: 0.00108173
Iteration 10/25 | Loss: 0.00108173
Iteration 11/25 | Loss: 0.00108173
Iteration 12/25 | Loss: 0.00108173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010817329166457057, 0.0010817329166457057, 0.0010817329166457057, 0.0010817329166457057, 0.0010817329166457057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010817329166457057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59451926
Iteration 2/25 | Loss: 0.00072937
Iteration 3/25 | Loss: 0.00072937
Iteration 4/25 | Loss: 0.00072937
Iteration 5/25 | Loss: 0.00072936
Iteration 6/25 | Loss: 0.00072936
Iteration 7/25 | Loss: 0.00072936
Iteration 8/25 | Loss: 0.00072936
Iteration 9/25 | Loss: 0.00072936
Iteration 10/25 | Loss: 0.00072936
Iteration 11/25 | Loss: 0.00072936
Iteration 12/25 | Loss: 0.00072936
Iteration 13/25 | Loss: 0.00072936
Iteration 14/25 | Loss: 0.00072936
Iteration 15/25 | Loss: 0.00072936
Iteration 16/25 | Loss: 0.00072936
Iteration 17/25 | Loss: 0.00072936
Iteration 18/25 | Loss: 0.00072936
Iteration 19/25 | Loss: 0.00072936
Iteration 20/25 | Loss: 0.00072936
Iteration 21/25 | Loss: 0.00072936
Iteration 22/25 | Loss: 0.00072936
Iteration 23/25 | Loss: 0.00072936
Iteration 24/25 | Loss: 0.00072936
Iteration 25/25 | Loss: 0.00072936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072936
Iteration 2/1000 | Loss: 0.00002756
Iteration 3/1000 | Loss: 0.00001800
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001107
Iteration 6/1000 | Loss: 0.00001050
Iteration 7/1000 | Loss: 0.00001007
Iteration 8/1000 | Loss: 0.00000978
Iteration 9/1000 | Loss: 0.00000971
Iteration 10/1000 | Loss: 0.00000958
Iteration 11/1000 | Loss: 0.00000936
Iteration 12/1000 | Loss: 0.00000930
Iteration 13/1000 | Loss: 0.00000922
Iteration 14/1000 | Loss: 0.00000916
Iteration 15/1000 | Loss: 0.00000912
Iteration 16/1000 | Loss: 0.00000912
Iteration 17/1000 | Loss: 0.00000910
Iteration 18/1000 | Loss: 0.00000904
Iteration 19/1000 | Loss: 0.00000904
Iteration 20/1000 | Loss: 0.00000904
Iteration 21/1000 | Loss: 0.00000903
Iteration 22/1000 | Loss: 0.00000902
Iteration 23/1000 | Loss: 0.00000902
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000901
Iteration 26/1000 | Loss: 0.00000901
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000900
Iteration 29/1000 | Loss: 0.00000900
Iteration 30/1000 | Loss: 0.00000900
Iteration 31/1000 | Loss: 0.00000899
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000898
Iteration 34/1000 | Loss: 0.00000898
Iteration 35/1000 | Loss: 0.00000898
Iteration 36/1000 | Loss: 0.00000897
Iteration 37/1000 | Loss: 0.00000897
Iteration 38/1000 | Loss: 0.00000897
Iteration 39/1000 | Loss: 0.00000897
Iteration 40/1000 | Loss: 0.00000897
Iteration 41/1000 | Loss: 0.00000897
Iteration 42/1000 | Loss: 0.00000897
Iteration 43/1000 | Loss: 0.00000897
Iteration 44/1000 | Loss: 0.00000896
Iteration 45/1000 | Loss: 0.00000896
Iteration 46/1000 | Loss: 0.00000895
Iteration 47/1000 | Loss: 0.00000894
Iteration 48/1000 | Loss: 0.00000894
Iteration 49/1000 | Loss: 0.00000894
Iteration 50/1000 | Loss: 0.00000894
Iteration 51/1000 | Loss: 0.00000893
Iteration 52/1000 | Loss: 0.00000893
Iteration 53/1000 | Loss: 0.00000892
Iteration 54/1000 | Loss: 0.00000892
Iteration 55/1000 | Loss: 0.00000891
Iteration 56/1000 | Loss: 0.00000890
Iteration 57/1000 | Loss: 0.00000890
Iteration 58/1000 | Loss: 0.00000889
Iteration 59/1000 | Loss: 0.00000889
Iteration 60/1000 | Loss: 0.00000888
Iteration 61/1000 | Loss: 0.00000888
Iteration 62/1000 | Loss: 0.00000887
Iteration 63/1000 | Loss: 0.00000887
Iteration 64/1000 | Loss: 0.00000886
Iteration 65/1000 | Loss: 0.00000886
Iteration 66/1000 | Loss: 0.00000886
Iteration 67/1000 | Loss: 0.00000886
Iteration 68/1000 | Loss: 0.00000886
Iteration 69/1000 | Loss: 0.00000886
Iteration 70/1000 | Loss: 0.00000886
Iteration 71/1000 | Loss: 0.00000886
Iteration 72/1000 | Loss: 0.00000886
Iteration 73/1000 | Loss: 0.00000886
Iteration 74/1000 | Loss: 0.00000886
Iteration 75/1000 | Loss: 0.00000886
Iteration 76/1000 | Loss: 0.00000886
Iteration 77/1000 | Loss: 0.00000886
Iteration 78/1000 | Loss: 0.00000886
Iteration 79/1000 | Loss: 0.00000886
Iteration 80/1000 | Loss: 0.00000886
Iteration 81/1000 | Loss: 0.00000886
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000886
Iteration 88/1000 | Loss: 0.00000886
Iteration 89/1000 | Loss: 0.00000886
Iteration 90/1000 | Loss: 0.00000886
Iteration 91/1000 | Loss: 0.00000886
Iteration 92/1000 | Loss: 0.00000886
Iteration 93/1000 | Loss: 0.00000886
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00000886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [8.856075510266237e-06, 8.856075510266237e-06, 8.856075510266237e-06, 8.856075510266237e-06, 8.856075510266237e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.856075510266237e-06

Optimization complete. Final v2v error: 2.5526363849639893 mm

Highest mean error: 2.9025559425354004 mm for frame 110

Lowest mean error: 2.307126760482788 mm for frame 21

Saving results

Total time: 31.738111972808838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450618
Iteration 2/25 | Loss: 0.00139405
Iteration 3/25 | Loss: 0.00117029
Iteration 4/25 | Loss: 0.00115301
Iteration 5/25 | Loss: 0.00115012
Iteration 6/25 | Loss: 0.00114911
Iteration 7/25 | Loss: 0.00114885
Iteration 8/25 | Loss: 0.00114885
Iteration 9/25 | Loss: 0.00114885
Iteration 10/25 | Loss: 0.00114885
Iteration 11/25 | Loss: 0.00114885
Iteration 12/25 | Loss: 0.00114885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011488518211990595, 0.0011488518211990595, 0.0011488518211990595, 0.0011488518211990595, 0.0011488518211990595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011488518211990595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41888189
Iteration 2/25 | Loss: 0.00081795
Iteration 3/25 | Loss: 0.00081794
Iteration 4/25 | Loss: 0.00081794
Iteration 5/25 | Loss: 0.00081794
Iteration 6/25 | Loss: 0.00081794
Iteration 7/25 | Loss: 0.00081794
Iteration 8/25 | Loss: 0.00081794
Iteration 9/25 | Loss: 0.00081794
Iteration 10/25 | Loss: 0.00081794
Iteration 11/25 | Loss: 0.00081794
Iteration 12/25 | Loss: 0.00081794
Iteration 13/25 | Loss: 0.00081794
Iteration 14/25 | Loss: 0.00081794
Iteration 15/25 | Loss: 0.00081794
Iteration 16/25 | Loss: 0.00081794
Iteration 17/25 | Loss: 0.00081794
Iteration 18/25 | Loss: 0.00081794
Iteration 19/25 | Loss: 0.00081794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000817935389932245, 0.000817935389932245, 0.000817935389932245, 0.000817935389932245, 0.000817935389932245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000817935389932245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081794
Iteration 2/1000 | Loss: 0.00003898
Iteration 3/1000 | Loss: 0.00002232
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00001716
Iteration 6/1000 | Loss: 0.00001641
Iteration 7/1000 | Loss: 0.00001602
Iteration 8/1000 | Loss: 0.00001564
Iteration 9/1000 | Loss: 0.00001535
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001497
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001480
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001459
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001457
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001443
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001442
Iteration 46/1000 | Loss: 0.00001441
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001440
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001440
Iteration 55/1000 | Loss: 0.00001440
Iteration 56/1000 | Loss: 0.00001440
Iteration 57/1000 | Loss: 0.00001440
Iteration 58/1000 | Loss: 0.00001440
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001439
Iteration 61/1000 | Loss: 0.00001439
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001438
Iteration 64/1000 | Loss: 0.00001438
Iteration 65/1000 | Loss: 0.00001438
Iteration 66/1000 | Loss: 0.00001438
Iteration 67/1000 | Loss: 0.00001438
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001438
Iteration 95/1000 | Loss: 0.00001438
Iteration 96/1000 | Loss: 0.00001438
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001438
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00001438
Iteration 103/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.437835453543812e-05, 1.437835453543812e-05, 1.437835453543812e-05, 1.437835453543812e-05, 1.437835453543812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.437835453543812e-05

Optimization complete. Final v2v error: 3.1037824153900146 mm

Highest mean error: 3.6528828144073486 mm for frame 54

Lowest mean error: 2.3512721061706543 mm for frame 0

Saving results

Total time: 34.738571643829346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857719
Iteration 2/25 | Loss: 0.00288842
Iteration 3/25 | Loss: 0.00178323
Iteration 4/25 | Loss: 0.00146441
Iteration 5/25 | Loss: 0.00143411
Iteration 6/25 | Loss: 0.00143356
Iteration 7/25 | Loss: 0.00140529
Iteration 8/25 | Loss: 0.00138111
Iteration 9/25 | Loss: 0.00136877
Iteration 10/25 | Loss: 0.00136651
Iteration 11/25 | Loss: 0.00136821
Iteration 12/25 | Loss: 0.00136303
Iteration 13/25 | Loss: 0.00135907
Iteration 14/25 | Loss: 0.00136106
Iteration 15/25 | Loss: 0.00136046
Iteration 16/25 | Loss: 0.00135751
Iteration 17/25 | Loss: 0.00135722
Iteration 18/25 | Loss: 0.00135699
Iteration 19/25 | Loss: 0.00135677
Iteration 20/25 | Loss: 0.00135975
Iteration 21/25 | Loss: 0.00135781
Iteration 22/25 | Loss: 0.00135628
Iteration 23/25 | Loss: 0.00135779
Iteration 24/25 | Loss: 0.00135612
Iteration 25/25 | Loss: 0.00135610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.03300142
Iteration 2/25 | Loss: 0.00494399
Iteration 3/25 | Loss: 0.00266321
Iteration 4/25 | Loss: 0.00265798
Iteration 5/25 | Loss: 0.00265798
Iteration 6/25 | Loss: 0.00265797
Iteration 7/25 | Loss: 0.00265797
Iteration 8/25 | Loss: 0.00265797
Iteration 9/25 | Loss: 0.00265797
Iteration 10/25 | Loss: 0.00265797
Iteration 11/25 | Loss: 0.00265797
Iteration 12/25 | Loss: 0.00265797
Iteration 13/25 | Loss: 0.00265797
Iteration 14/25 | Loss: 0.00265797
Iteration 15/25 | Loss: 0.00265797
Iteration 16/25 | Loss: 0.00265797
Iteration 17/25 | Loss: 0.00265797
Iteration 18/25 | Loss: 0.00265797
Iteration 19/25 | Loss: 0.00265797
Iteration 20/25 | Loss: 0.00265797
Iteration 21/25 | Loss: 0.00265797
Iteration 22/25 | Loss: 0.00265797
Iteration 23/25 | Loss: 0.00265797
Iteration 24/25 | Loss: 0.00265797
Iteration 25/25 | Loss: 0.00265797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265797
Iteration 2/1000 | Loss: 0.00422179
Iteration 3/1000 | Loss: 0.00450354
Iteration 4/1000 | Loss: 0.00553932
Iteration 5/1000 | Loss: 0.00049120
Iteration 6/1000 | Loss: 0.00039741
Iteration 7/1000 | Loss: 0.00101775
Iteration 8/1000 | Loss: 0.00208881
Iteration 9/1000 | Loss: 0.00395889
Iteration 10/1000 | Loss: 0.00147449
Iteration 11/1000 | Loss: 0.00497441
Iteration 12/1000 | Loss: 0.00091048
Iteration 13/1000 | Loss: 0.00213031
Iteration 14/1000 | Loss: 0.00087185
Iteration 15/1000 | Loss: 0.00157767
Iteration 16/1000 | Loss: 0.00034022
Iteration 17/1000 | Loss: 0.00154726
Iteration 18/1000 | Loss: 0.00050235
Iteration 19/1000 | Loss: 0.00102834
Iteration 20/1000 | Loss: 0.00475408
Iteration 21/1000 | Loss: 0.00155173
Iteration 22/1000 | Loss: 0.00071715
Iteration 23/1000 | Loss: 0.00143324
Iteration 24/1000 | Loss: 0.00063060
Iteration 25/1000 | Loss: 0.00187731
Iteration 26/1000 | Loss: 0.00112910
Iteration 27/1000 | Loss: 0.00042238
Iteration 28/1000 | Loss: 0.00039884
Iteration 29/1000 | Loss: 0.00288205
Iteration 30/1000 | Loss: 0.00058232
Iteration 31/1000 | Loss: 0.00042196
Iteration 32/1000 | Loss: 0.00045057
Iteration 33/1000 | Loss: 0.00096217
Iteration 34/1000 | Loss: 0.00411275
Iteration 35/1000 | Loss: 0.00060914
Iteration 36/1000 | Loss: 0.00138592
Iteration 37/1000 | Loss: 0.00046877
Iteration 38/1000 | Loss: 0.00010354
Iteration 39/1000 | Loss: 0.00009650
Iteration 40/1000 | Loss: 0.00082788
Iteration 41/1000 | Loss: 0.00046309
Iteration 42/1000 | Loss: 0.00010451
Iteration 43/1000 | Loss: 0.00069732
Iteration 44/1000 | Loss: 0.00088589
Iteration 45/1000 | Loss: 0.00108779
Iteration 46/1000 | Loss: 0.00065604
Iteration 47/1000 | Loss: 0.00064817
Iteration 48/1000 | Loss: 0.00180232
Iteration 49/1000 | Loss: 0.00219905
Iteration 50/1000 | Loss: 0.00619321
Iteration 51/1000 | Loss: 0.00292112
Iteration 52/1000 | Loss: 0.00630763
Iteration 53/1000 | Loss: 0.00266379
Iteration 54/1000 | Loss: 0.00440806
Iteration 55/1000 | Loss: 0.00322040
Iteration 56/1000 | Loss: 0.00273252
Iteration 57/1000 | Loss: 0.00126074
Iteration 58/1000 | Loss: 0.00383821
Iteration 59/1000 | Loss: 0.00174662
Iteration 60/1000 | Loss: 0.00202630
Iteration 61/1000 | Loss: 0.00159827
Iteration 62/1000 | Loss: 0.00154092
Iteration 63/1000 | Loss: 0.00094182
Iteration 64/1000 | Loss: 0.00141850
Iteration 65/1000 | Loss: 0.00086735
Iteration 66/1000 | Loss: 0.00171566
Iteration 67/1000 | Loss: 0.00120757
Iteration 68/1000 | Loss: 0.00111553
Iteration 69/1000 | Loss: 0.00151374
Iteration 70/1000 | Loss: 0.00179606
Iteration 71/1000 | Loss: 0.00252316
Iteration 72/1000 | Loss: 0.00181714
Iteration 73/1000 | Loss: 0.00169266
Iteration 74/1000 | Loss: 0.00322184
Iteration 75/1000 | Loss: 0.00109772
Iteration 76/1000 | Loss: 0.00164467
Iteration 77/1000 | Loss: 0.00076399
Iteration 78/1000 | Loss: 0.00182816
Iteration 79/1000 | Loss: 0.00148336
Iteration 80/1000 | Loss: 0.00034507
Iteration 81/1000 | Loss: 0.00060936
Iteration 82/1000 | Loss: 0.00019290
Iteration 83/1000 | Loss: 0.00033294
Iteration 84/1000 | Loss: 0.00096806
Iteration 85/1000 | Loss: 0.00075409
Iteration 86/1000 | Loss: 0.00080750
Iteration 87/1000 | Loss: 0.00078305
Iteration 88/1000 | Loss: 0.00052317
Iteration 89/1000 | Loss: 0.00070345
Iteration 90/1000 | Loss: 0.00027403
Iteration 91/1000 | Loss: 0.00008193
Iteration 92/1000 | Loss: 0.00160437
Iteration 93/1000 | Loss: 0.00050112
Iteration 94/1000 | Loss: 0.00086343
Iteration 95/1000 | Loss: 0.00060103
Iteration 96/1000 | Loss: 0.00084186
Iteration 97/1000 | Loss: 0.00065900
Iteration 98/1000 | Loss: 0.00074499
Iteration 99/1000 | Loss: 0.00079124
Iteration 100/1000 | Loss: 0.00136500
Iteration 101/1000 | Loss: 0.00028069
Iteration 102/1000 | Loss: 0.00062345
Iteration 103/1000 | Loss: 0.00079688
Iteration 104/1000 | Loss: 0.00069614
Iteration 105/1000 | Loss: 0.00073026
Iteration 106/1000 | Loss: 0.00038969
Iteration 107/1000 | Loss: 0.00053147
Iteration 108/1000 | Loss: 0.00011744
Iteration 109/1000 | Loss: 0.00012604
Iteration 110/1000 | Loss: 0.00046719
Iteration 111/1000 | Loss: 0.00076631
Iteration 112/1000 | Loss: 0.00036838
Iteration 113/1000 | Loss: 0.00061910
Iteration 114/1000 | Loss: 0.00035887
Iteration 115/1000 | Loss: 0.00005989
Iteration 116/1000 | Loss: 0.00013178
Iteration 117/1000 | Loss: 0.00007279
Iteration 118/1000 | Loss: 0.00054974
Iteration 119/1000 | Loss: 0.00037216
Iteration 120/1000 | Loss: 0.00015040
Iteration 121/1000 | Loss: 0.00007090
Iteration 122/1000 | Loss: 0.00029230
Iteration 123/1000 | Loss: 0.00029353
Iteration 124/1000 | Loss: 0.00006540
Iteration 125/1000 | Loss: 0.00005107
Iteration 126/1000 | Loss: 0.00007832
Iteration 127/1000 | Loss: 0.00072091
Iteration 128/1000 | Loss: 0.00314751
Iteration 129/1000 | Loss: 0.00140571
Iteration 130/1000 | Loss: 0.00010699
Iteration 131/1000 | Loss: 0.00040741
Iteration 132/1000 | Loss: 0.00058295
Iteration 133/1000 | Loss: 0.00041341
Iteration 134/1000 | Loss: 0.00063067
Iteration 135/1000 | Loss: 0.00039039
Iteration 136/1000 | Loss: 0.00065018
Iteration 137/1000 | Loss: 0.00267172
Iteration 138/1000 | Loss: 0.00054889
Iteration 139/1000 | Loss: 0.00064456
Iteration 140/1000 | Loss: 0.00005639
Iteration 141/1000 | Loss: 0.00107713
Iteration 142/1000 | Loss: 0.00088991
Iteration 143/1000 | Loss: 0.00039954
Iteration 144/1000 | Loss: 0.00036913
Iteration 145/1000 | Loss: 0.00004493
Iteration 146/1000 | Loss: 0.00055470
Iteration 147/1000 | Loss: 0.00044248
Iteration 148/1000 | Loss: 0.00031454
Iteration 149/1000 | Loss: 0.00061395
Iteration 150/1000 | Loss: 0.00036355
Iteration 151/1000 | Loss: 0.00066266
Iteration 152/1000 | Loss: 0.00014482
Iteration 153/1000 | Loss: 0.00016816
Iteration 154/1000 | Loss: 0.00013227
Iteration 155/1000 | Loss: 0.00010071
Iteration 156/1000 | Loss: 0.00003552
Iteration 157/1000 | Loss: 0.00003349
Iteration 158/1000 | Loss: 0.00003267
Iteration 159/1000 | Loss: 0.00007970
Iteration 160/1000 | Loss: 0.00053355
Iteration 161/1000 | Loss: 0.00045221
Iteration 162/1000 | Loss: 0.00007336
Iteration 163/1000 | Loss: 0.00060649
Iteration 164/1000 | Loss: 0.00057384
Iteration 165/1000 | Loss: 0.00004762
Iteration 166/1000 | Loss: 0.00062781
Iteration 167/1000 | Loss: 0.00145393
Iteration 168/1000 | Loss: 0.00045976
Iteration 169/1000 | Loss: 0.00005230
Iteration 170/1000 | Loss: 0.00004325
Iteration 171/1000 | Loss: 0.00003756
Iteration 172/1000 | Loss: 0.00028608
Iteration 173/1000 | Loss: 0.00023963
Iteration 174/1000 | Loss: 0.00003914
Iteration 175/1000 | Loss: 0.00067577
Iteration 176/1000 | Loss: 0.00132424
Iteration 177/1000 | Loss: 0.00093088
Iteration 178/1000 | Loss: 0.00033136
Iteration 179/1000 | Loss: 0.00080451
Iteration 180/1000 | Loss: 0.00073560
Iteration 181/1000 | Loss: 0.00056267
Iteration 182/1000 | Loss: 0.00057483
Iteration 183/1000 | Loss: 0.00084801
Iteration 184/1000 | Loss: 0.00009580
Iteration 185/1000 | Loss: 0.00062360
Iteration 186/1000 | Loss: 0.00030562
Iteration 187/1000 | Loss: 0.00002872
Iteration 188/1000 | Loss: 0.00009401
Iteration 189/1000 | Loss: 0.00065628
Iteration 190/1000 | Loss: 0.00017317
Iteration 191/1000 | Loss: 0.00046308
Iteration 192/1000 | Loss: 0.00046610
Iteration 193/1000 | Loss: 0.00077221
Iteration 194/1000 | Loss: 0.00044125
Iteration 195/1000 | Loss: 0.00022890
Iteration 196/1000 | Loss: 0.00005786
Iteration 197/1000 | Loss: 0.00015407
Iteration 198/1000 | Loss: 0.00047400
Iteration 199/1000 | Loss: 0.00009214
Iteration 200/1000 | Loss: 0.00002372
Iteration 201/1000 | Loss: 0.00002108
Iteration 202/1000 | Loss: 0.00004148
Iteration 203/1000 | Loss: 0.00002272
Iteration 204/1000 | Loss: 0.00002004
Iteration 205/1000 | Loss: 0.00001852
Iteration 206/1000 | Loss: 0.00047153
Iteration 207/1000 | Loss: 0.00025401
Iteration 208/1000 | Loss: 0.00048550
Iteration 209/1000 | Loss: 0.00012165
Iteration 210/1000 | Loss: 0.00043519
Iteration 211/1000 | Loss: 0.00007740
Iteration 212/1000 | Loss: 0.00005014
Iteration 213/1000 | Loss: 0.00036691
Iteration 214/1000 | Loss: 0.00011964
Iteration 215/1000 | Loss: 0.00009716
Iteration 216/1000 | Loss: 0.00002623
Iteration 217/1000 | Loss: 0.00002396
Iteration 218/1000 | Loss: 0.00001742
Iteration 219/1000 | Loss: 0.00001716
Iteration 220/1000 | Loss: 0.00006101
Iteration 221/1000 | Loss: 0.00002422
Iteration 222/1000 | Loss: 0.00004644
Iteration 223/1000 | Loss: 0.00038613
Iteration 224/1000 | Loss: 0.00008276
Iteration 225/1000 | Loss: 0.00002096
Iteration 226/1000 | Loss: 0.00001708
Iteration 227/1000 | Loss: 0.00001672
Iteration 228/1000 | Loss: 0.00001662
Iteration 229/1000 | Loss: 0.00004897
Iteration 230/1000 | Loss: 0.00003483
Iteration 231/1000 | Loss: 0.00002670
Iteration 232/1000 | Loss: 0.00001650
Iteration 233/1000 | Loss: 0.00001633
Iteration 234/1000 | Loss: 0.00001632
Iteration 235/1000 | Loss: 0.00001629
Iteration 236/1000 | Loss: 0.00001629
Iteration 237/1000 | Loss: 0.00001628
Iteration 238/1000 | Loss: 0.00001628
Iteration 239/1000 | Loss: 0.00001628
Iteration 240/1000 | Loss: 0.00001628
Iteration 241/1000 | Loss: 0.00001628
Iteration 242/1000 | Loss: 0.00001628
Iteration 243/1000 | Loss: 0.00001627
Iteration 244/1000 | Loss: 0.00001627
Iteration 245/1000 | Loss: 0.00001627
Iteration 246/1000 | Loss: 0.00001627
Iteration 247/1000 | Loss: 0.00001626
Iteration 248/1000 | Loss: 0.00001626
Iteration 249/1000 | Loss: 0.00001626
Iteration 250/1000 | Loss: 0.00001626
Iteration 251/1000 | Loss: 0.00001626
Iteration 252/1000 | Loss: 0.00001625
Iteration 253/1000 | Loss: 0.00004077
Iteration 254/1000 | Loss: 0.00001751
Iteration 255/1000 | Loss: 0.00034525
Iteration 256/1000 | Loss: 0.00004867
Iteration 257/1000 | Loss: 0.00002970
Iteration 258/1000 | Loss: 0.00020788
Iteration 259/1000 | Loss: 0.00001742
Iteration 260/1000 | Loss: 0.00001649
Iteration 261/1000 | Loss: 0.00003072
Iteration 262/1000 | Loss: 0.00001633
Iteration 263/1000 | Loss: 0.00006219
Iteration 264/1000 | Loss: 0.00001630
Iteration 265/1000 | Loss: 0.00004056
Iteration 266/1000 | Loss: 0.00002773
Iteration 267/1000 | Loss: 0.00001619
Iteration 268/1000 | Loss: 0.00001619
Iteration 269/1000 | Loss: 0.00001619
Iteration 270/1000 | Loss: 0.00001619
Iteration 271/1000 | Loss: 0.00001619
Iteration 272/1000 | Loss: 0.00001618
Iteration 273/1000 | Loss: 0.00001618
Iteration 274/1000 | Loss: 0.00001618
Iteration 275/1000 | Loss: 0.00001618
Iteration 276/1000 | Loss: 0.00001618
Iteration 277/1000 | Loss: 0.00001618
Iteration 278/1000 | Loss: 0.00001618
Iteration 279/1000 | Loss: 0.00001618
Iteration 280/1000 | Loss: 0.00001618
Iteration 281/1000 | Loss: 0.00001618
Iteration 282/1000 | Loss: 0.00001618
Iteration 283/1000 | Loss: 0.00001618
Iteration 284/1000 | Loss: 0.00001618
Iteration 285/1000 | Loss: 0.00001618
Iteration 286/1000 | Loss: 0.00001618
Iteration 287/1000 | Loss: 0.00001618
Iteration 288/1000 | Loss: 0.00001618
Iteration 289/1000 | Loss: 0.00001617
Iteration 290/1000 | Loss: 0.00001617
Iteration 291/1000 | Loss: 0.00001617
Iteration 292/1000 | Loss: 0.00001617
Iteration 293/1000 | Loss: 0.00001617
Iteration 294/1000 | Loss: 0.00001617
Iteration 295/1000 | Loss: 0.00001617
Iteration 296/1000 | Loss: 0.00001617
Iteration 297/1000 | Loss: 0.00001617
Iteration 298/1000 | Loss: 0.00001616
Iteration 299/1000 | Loss: 0.00001616
Iteration 300/1000 | Loss: 0.00001616
Iteration 301/1000 | Loss: 0.00001616
Iteration 302/1000 | Loss: 0.00003198
Iteration 303/1000 | Loss: 0.00025946
Iteration 304/1000 | Loss: 0.00006166
Iteration 305/1000 | Loss: 0.00002965
Iteration 306/1000 | Loss: 0.00001624
Iteration 307/1000 | Loss: 0.00003265
Iteration 308/1000 | Loss: 0.00001615
Iteration 309/1000 | Loss: 0.00001615
Iteration 310/1000 | Loss: 0.00001615
Iteration 311/1000 | Loss: 0.00001615
Iteration 312/1000 | Loss: 0.00001615
Iteration 313/1000 | Loss: 0.00001615
Iteration 314/1000 | Loss: 0.00001615
Iteration 315/1000 | Loss: 0.00001615
Iteration 316/1000 | Loss: 0.00001615
Iteration 317/1000 | Loss: 0.00001614
Iteration 318/1000 | Loss: 0.00001614
Iteration 319/1000 | Loss: 0.00001614
Iteration 320/1000 | Loss: 0.00001614
Iteration 321/1000 | Loss: 0.00001614
Iteration 322/1000 | Loss: 0.00001614
Iteration 323/1000 | Loss: 0.00001613
Iteration 324/1000 | Loss: 0.00001613
Iteration 325/1000 | Loss: 0.00001613
Iteration 326/1000 | Loss: 0.00001613
Iteration 327/1000 | Loss: 0.00001612
Iteration 328/1000 | Loss: 0.00001612
Iteration 329/1000 | Loss: 0.00001612
Iteration 330/1000 | Loss: 0.00001612
Iteration 331/1000 | Loss: 0.00001612
Iteration 332/1000 | Loss: 0.00001612
Iteration 333/1000 | Loss: 0.00001612
Iteration 334/1000 | Loss: 0.00001612
Iteration 335/1000 | Loss: 0.00001612
Iteration 336/1000 | Loss: 0.00001611
Iteration 337/1000 | Loss: 0.00001611
Iteration 338/1000 | Loss: 0.00001611
Iteration 339/1000 | Loss: 0.00001611
Iteration 340/1000 | Loss: 0.00001611
Iteration 341/1000 | Loss: 0.00001611
Iteration 342/1000 | Loss: 0.00001611
Iteration 343/1000 | Loss: 0.00001611
Iteration 344/1000 | Loss: 0.00001611
Iteration 345/1000 | Loss: 0.00001610
Iteration 346/1000 | Loss: 0.00028198
Iteration 347/1000 | Loss: 0.00007242
Iteration 348/1000 | Loss: 0.00002211
Iteration 349/1000 | Loss: 0.00003037
Iteration 350/1000 | Loss: 0.00001641
Iteration 351/1000 | Loss: 0.00001626
Iteration 352/1000 | Loss: 0.00001617
Iteration 353/1000 | Loss: 0.00001617
Iteration 354/1000 | Loss: 0.00001616
Iteration 355/1000 | Loss: 0.00001616
Iteration 356/1000 | Loss: 0.00001616
Iteration 357/1000 | Loss: 0.00001616
Iteration 358/1000 | Loss: 0.00001616
Iteration 359/1000 | Loss: 0.00001615
Iteration 360/1000 | Loss: 0.00001615
Iteration 361/1000 | Loss: 0.00001615
Iteration 362/1000 | Loss: 0.00001615
Iteration 363/1000 | Loss: 0.00001615
Iteration 364/1000 | Loss: 0.00001615
Iteration 365/1000 | Loss: 0.00001615
Iteration 366/1000 | Loss: 0.00001615
Iteration 367/1000 | Loss: 0.00001615
Iteration 368/1000 | Loss: 0.00001614
Iteration 369/1000 | Loss: 0.00001614
Iteration 370/1000 | Loss: 0.00001614
Iteration 371/1000 | Loss: 0.00001614
Iteration 372/1000 | Loss: 0.00001614
Iteration 373/1000 | Loss: 0.00001614
Iteration 374/1000 | Loss: 0.00001614
Iteration 375/1000 | Loss: 0.00001614
Iteration 376/1000 | Loss: 0.00001614
Iteration 377/1000 | Loss: 0.00001614
Iteration 378/1000 | Loss: 0.00001613
Iteration 379/1000 | Loss: 0.00001613
Iteration 380/1000 | Loss: 0.00001613
Iteration 381/1000 | Loss: 0.00001613
Iteration 382/1000 | Loss: 0.00005221
Iteration 383/1000 | Loss: 0.00058733
Iteration 384/1000 | Loss: 0.00015268
Iteration 385/1000 | Loss: 0.00088502
Iteration 386/1000 | Loss: 0.00025805
Iteration 387/1000 | Loss: 0.00014073
Iteration 388/1000 | Loss: 0.00001906
Iteration 389/1000 | Loss: 0.00001705
Iteration 390/1000 | Loss: 0.00005167
Iteration 391/1000 | Loss: 0.00013003
Iteration 392/1000 | Loss: 0.00001561
Iteration 393/1000 | Loss: 0.00006995
Iteration 394/1000 | Loss: 0.00002269
Iteration 395/1000 | Loss: 0.00014942
Iteration 396/1000 | Loss: 0.00002026
Iteration 397/1000 | Loss: 0.00001445
Iteration 398/1000 | Loss: 0.00003740
Iteration 399/1000 | Loss: 0.00001417
Iteration 400/1000 | Loss: 0.00001947
Iteration 401/1000 | Loss: 0.00001405
Iteration 402/1000 | Loss: 0.00004755
Iteration 403/1000 | Loss: 0.00003755
Iteration 404/1000 | Loss: 0.00001554
Iteration 405/1000 | Loss: 0.00001382
Iteration 406/1000 | Loss: 0.00001381
Iteration 407/1000 | Loss: 0.00001380
Iteration 408/1000 | Loss: 0.00001380
Iteration 409/1000 | Loss: 0.00001380
Iteration 410/1000 | Loss: 0.00004279
Iteration 411/1000 | Loss: 0.00002081
Iteration 412/1000 | Loss: 0.00003020
Iteration 413/1000 | Loss: 0.00001396
Iteration 414/1000 | Loss: 0.00001372
Iteration 415/1000 | Loss: 0.00001372
Iteration 416/1000 | Loss: 0.00001371
Iteration 417/1000 | Loss: 0.00001371
Iteration 418/1000 | Loss: 0.00001370
Iteration 419/1000 | Loss: 0.00001370
Iteration 420/1000 | Loss: 0.00001370
Iteration 421/1000 | Loss: 0.00001369
Iteration 422/1000 | Loss: 0.00001369
Iteration 423/1000 | Loss: 0.00001369
Iteration 424/1000 | Loss: 0.00001369
Iteration 425/1000 | Loss: 0.00001369
Iteration 426/1000 | Loss: 0.00001368
Iteration 427/1000 | Loss: 0.00001368
Iteration 428/1000 | Loss: 0.00001368
Iteration 429/1000 | Loss: 0.00001367
Iteration 430/1000 | Loss: 0.00001367
Iteration 431/1000 | Loss: 0.00001367
Iteration 432/1000 | Loss: 0.00001367
Iteration 433/1000 | Loss: 0.00001366
Iteration 434/1000 | Loss: 0.00001366
Iteration 435/1000 | Loss: 0.00001366
Iteration 436/1000 | Loss: 0.00001366
Iteration 437/1000 | Loss: 0.00001366
Iteration 438/1000 | Loss: 0.00001366
Iteration 439/1000 | Loss: 0.00001366
Iteration 440/1000 | Loss: 0.00001366
Iteration 441/1000 | Loss: 0.00001366
Iteration 442/1000 | Loss: 0.00001366
Iteration 443/1000 | Loss: 0.00001366
Iteration 444/1000 | Loss: 0.00001366
Iteration 445/1000 | Loss: 0.00001366
Iteration 446/1000 | Loss: 0.00001366
Iteration 447/1000 | Loss: 0.00001366
Iteration 448/1000 | Loss: 0.00001366
Iteration 449/1000 | Loss: 0.00001366
Iteration 450/1000 | Loss: 0.00001366
Iteration 451/1000 | Loss: 0.00001366
Iteration 452/1000 | Loss: 0.00001365
Iteration 453/1000 | Loss: 0.00001365
Iteration 454/1000 | Loss: 0.00001365
Iteration 455/1000 | Loss: 0.00001365
Iteration 456/1000 | Loss: 0.00001365
Iteration 457/1000 | Loss: 0.00001365
Iteration 458/1000 | Loss: 0.00001365
Iteration 459/1000 | Loss: 0.00001365
Iteration 460/1000 | Loss: 0.00001365
Iteration 461/1000 | Loss: 0.00001365
Iteration 462/1000 | Loss: 0.00001365
Iteration 463/1000 | Loss: 0.00001365
Iteration 464/1000 | Loss: 0.00001365
Iteration 465/1000 | Loss: 0.00001365
Iteration 466/1000 | Loss: 0.00001365
Iteration 467/1000 | Loss: 0.00001365
Iteration 468/1000 | Loss: 0.00001365
Iteration 469/1000 | Loss: 0.00001365
Iteration 470/1000 | Loss: 0.00001365
Iteration 471/1000 | Loss: 0.00001365
Iteration 472/1000 | Loss: 0.00001365
Iteration 473/1000 | Loss: 0.00001365
Iteration 474/1000 | Loss: 0.00001365
Iteration 475/1000 | Loss: 0.00001365
Iteration 476/1000 | Loss: 0.00001365
Iteration 477/1000 | Loss: 0.00001365
Iteration 478/1000 | Loss: 0.00001365
Iteration 479/1000 | Loss: 0.00001365
Iteration 480/1000 | Loss: 0.00001365
Iteration 481/1000 | Loss: 0.00001365
Iteration 482/1000 | Loss: 0.00001365
Iteration 483/1000 | Loss: 0.00001365
Iteration 484/1000 | Loss: 0.00001365
Iteration 485/1000 | Loss: 0.00001365
Iteration 486/1000 | Loss: 0.00001365
Iteration 487/1000 | Loss: 0.00001365
Iteration 488/1000 | Loss: 0.00001365
Iteration 489/1000 | Loss: 0.00001365
Iteration 490/1000 | Loss: 0.00001365
Iteration 491/1000 | Loss: 0.00001365
Iteration 492/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 492. Stopping optimization.
Last 5 losses: [1.3648952517542057e-05, 1.3648952517542057e-05, 1.3648952517542057e-05, 1.3648952517542057e-05, 1.3648952517542057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3648952517542057e-05

Optimization complete. Final v2v error: 3.027559518814087 mm

Highest mean error: 6.636748790740967 mm for frame 71

Lowest mean error: 2.4285941123962402 mm for frame 0

Saving results

Total time: 501.1976161003113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819500
Iteration 2/25 | Loss: 0.00124866
Iteration 3/25 | Loss: 0.00112173
Iteration 4/25 | Loss: 0.00110988
Iteration 5/25 | Loss: 0.00110333
Iteration 6/25 | Loss: 0.00110181
Iteration 7/25 | Loss: 0.00110138
Iteration 8/25 | Loss: 0.00110093
Iteration 9/25 | Loss: 0.00110079
Iteration 10/25 | Loss: 0.00110147
Iteration 11/25 | Loss: 0.00110074
Iteration 12/25 | Loss: 0.00109997
Iteration 13/25 | Loss: 0.00109975
Iteration 14/25 | Loss: 0.00109969
Iteration 15/25 | Loss: 0.00109969
Iteration 16/25 | Loss: 0.00109969
Iteration 17/25 | Loss: 0.00109969
Iteration 18/25 | Loss: 0.00109969
Iteration 19/25 | Loss: 0.00109968
Iteration 20/25 | Loss: 0.00109968
Iteration 21/25 | Loss: 0.00109968
Iteration 22/25 | Loss: 0.00109968
Iteration 23/25 | Loss: 0.00109968
Iteration 24/25 | Loss: 0.00109968
Iteration 25/25 | Loss: 0.00109968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61940742
Iteration 2/25 | Loss: 0.00072125
Iteration 3/25 | Loss: 0.00072124
Iteration 4/25 | Loss: 0.00072123
Iteration 5/25 | Loss: 0.00072123
Iteration 6/25 | Loss: 0.00072123
Iteration 7/25 | Loss: 0.00072123
Iteration 8/25 | Loss: 0.00072123
Iteration 9/25 | Loss: 0.00072123
Iteration 10/25 | Loss: 0.00072123
Iteration 11/25 | Loss: 0.00072123
Iteration 12/25 | Loss: 0.00072123
Iteration 13/25 | Loss: 0.00072123
Iteration 14/25 | Loss: 0.00072123
Iteration 15/25 | Loss: 0.00072123
Iteration 16/25 | Loss: 0.00072123
Iteration 17/25 | Loss: 0.00072123
Iteration 18/25 | Loss: 0.00072123
Iteration 19/25 | Loss: 0.00072123
Iteration 20/25 | Loss: 0.00072123
Iteration 21/25 | Loss: 0.00072123
Iteration 22/25 | Loss: 0.00072123
Iteration 23/25 | Loss: 0.00072123
Iteration 24/25 | Loss: 0.00072123
Iteration 25/25 | Loss: 0.00072123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072123
Iteration 2/1000 | Loss: 0.00002251
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001363
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001354
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001350
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001347
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001333
Iteration 59/1000 | Loss: 0.00001333
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001332
Iteration 63/1000 | Loss: 0.00001332
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.3188824595999904e-05, 1.3188824595999904e-05, 1.3188824595999904e-05, 1.3188824595999904e-05, 1.3188824595999904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3188824595999904e-05

Optimization complete. Final v2v error: 3.076312303543091 mm

Highest mean error: 3.484684467315674 mm for frame 204

Lowest mean error: 2.799138069152832 mm for frame 107

Saving results

Total time: 55.28524351119995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01158643
Iteration 2/25 | Loss: 0.01158643
Iteration 3/25 | Loss: 0.00438198
Iteration 4/25 | Loss: 0.00288883
Iteration 5/25 | Loss: 0.00284483
Iteration 6/25 | Loss: 0.00265920
Iteration 7/25 | Loss: 0.00242527
Iteration 8/25 | Loss: 0.00222961
Iteration 9/25 | Loss: 0.00212306
Iteration 10/25 | Loss: 0.00210577
Iteration 11/25 | Loss: 0.00202200
Iteration 12/25 | Loss: 0.00200999
Iteration 13/25 | Loss: 0.00200932
Iteration 14/25 | Loss: 0.00202540
Iteration 15/25 | Loss: 0.00198859
Iteration 16/25 | Loss: 0.00198918
Iteration 17/25 | Loss: 0.00196286
Iteration 18/25 | Loss: 0.00195832
Iteration 19/25 | Loss: 0.00195750
Iteration 20/25 | Loss: 0.00195692
Iteration 21/25 | Loss: 0.00195662
Iteration 22/25 | Loss: 0.00195635
Iteration 23/25 | Loss: 0.00195629
Iteration 24/25 | Loss: 0.00195629
Iteration 25/25 | Loss: 0.00195629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50442326
Iteration 2/25 | Loss: 0.04206454
Iteration 3/25 | Loss: 0.00621608
Iteration 4/25 | Loss: 0.00621607
Iteration 5/25 | Loss: 0.00621607
Iteration 6/25 | Loss: 0.00621607
Iteration 7/25 | Loss: 0.00621607
Iteration 8/25 | Loss: 0.00621607
Iteration 9/25 | Loss: 0.00621607
Iteration 10/25 | Loss: 0.00621607
Iteration 11/25 | Loss: 0.00621607
Iteration 12/25 | Loss: 0.00621607
Iteration 13/25 | Loss: 0.00621607
Iteration 14/25 | Loss: 0.00621607
Iteration 15/25 | Loss: 0.00621607
Iteration 16/25 | Loss: 0.00621607
Iteration 17/25 | Loss: 0.00621607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.006216069217771292, 0.006216069217771292, 0.006216069217771292, 0.006216069217771292, 0.006216069217771292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006216069217771292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00621607
Iteration 2/1000 | Loss: 0.00091689
Iteration 3/1000 | Loss: 0.00047979
Iteration 4/1000 | Loss: 0.00042591
Iteration 5/1000 | Loss: 0.00038020
Iteration 6/1000 | Loss: 0.00034488
Iteration 7/1000 | Loss: 0.00031158
Iteration 8/1000 | Loss: 0.00278796
Iteration 9/1000 | Loss: 0.01848560
Iteration 10/1000 | Loss: 0.00828137
Iteration 11/1000 | Loss: 0.00089273
Iteration 12/1000 | Loss: 0.00047356
Iteration 13/1000 | Loss: 0.00031906
Iteration 14/1000 | Loss: 0.00024294
Iteration 15/1000 | Loss: 0.00051192
Iteration 16/1000 | Loss: 0.00014498
Iteration 17/1000 | Loss: 0.00010813
Iteration 18/1000 | Loss: 0.00009122
Iteration 19/1000 | Loss: 0.00008199
Iteration 20/1000 | Loss: 0.00007475
Iteration 21/1000 | Loss: 0.00006856
Iteration 22/1000 | Loss: 0.00006365
Iteration 23/1000 | Loss: 0.00005996
Iteration 24/1000 | Loss: 0.00005703
Iteration 25/1000 | Loss: 0.00005469
Iteration 26/1000 | Loss: 0.00005304
Iteration 27/1000 | Loss: 0.00014901
Iteration 28/1000 | Loss: 0.00019026
Iteration 29/1000 | Loss: 0.00018784
Iteration 30/1000 | Loss: 0.00012249
Iteration 31/1000 | Loss: 0.00005407
Iteration 32/1000 | Loss: 0.00005105
Iteration 33/1000 | Loss: 0.00004954
Iteration 34/1000 | Loss: 0.00004845
Iteration 35/1000 | Loss: 0.00004765
Iteration 36/1000 | Loss: 0.00004718
Iteration 37/1000 | Loss: 0.00004691
Iteration 38/1000 | Loss: 0.00004671
Iteration 39/1000 | Loss: 0.00004656
Iteration 40/1000 | Loss: 0.00004641
Iteration 41/1000 | Loss: 0.00004629
Iteration 42/1000 | Loss: 0.00004625
Iteration 43/1000 | Loss: 0.00004624
Iteration 44/1000 | Loss: 0.00004620
Iteration 45/1000 | Loss: 0.00004619
Iteration 46/1000 | Loss: 0.00004608
Iteration 47/1000 | Loss: 0.00004608
Iteration 48/1000 | Loss: 0.00004605
Iteration 49/1000 | Loss: 0.00004605
Iteration 50/1000 | Loss: 0.00004604
Iteration 51/1000 | Loss: 0.00004604
Iteration 52/1000 | Loss: 0.00004602
Iteration 53/1000 | Loss: 0.00004601
Iteration 54/1000 | Loss: 0.00004601
Iteration 55/1000 | Loss: 0.00004601
Iteration 56/1000 | Loss: 0.00004601
Iteration 57/1000 | Loss: 0.00004601
Iteration 58/1000 | Loss: 0.00004601
Iteration 59/1000 | Loss: 0.00004601
Iteration 60/1000 | Loss: 0.00004600
Iteration 61/1000 | Loss: 0.00004600
Iteration 62/1000 | Loss: 0.00004600
Iteration 63/1000 | Loss: 0.00004600
Iteration 64/1000 | Loss: 0.00004600
Iteration 65/1000 | Loss: 0.00004600
Iteration 66/1000 | Loss: 0.00004600
Iteration 67/1000 | Loss: 0.00004600
Iteration 68/1000 | Loss: 0.00004600
Iteration 69/1000 | Loss: 0.00004600
Iteration 70/1000 | Loss: 0.00004599
Iteration 71/1000 | Loss: 0.00004599
Iteration 72/1000 | Loss: 0.00004599
Iteration 73/1000 | Loss: 0.00004599
Iteration 74/1000 | Loss: 0.00004599
Iteration 75/1000 | Loss: 0.00004599
Iteration 76/1000 | Loss: 0.00004599
Iteration 77/1000 | Loss: 0.00004599
Iteration 78/1000 | Loss: 0.00004599
Iteration 79/1000 | Loss: 0.00004599
Iteration 80/1000 | Loss: 0.00004599
Iteration 81/1000 | Loss: 0.00004599
Iteration 82/1000 | Loss: 0.00004599
Iteration 83/1000 | Loss: 0.00004599
Iteration 84/1000 | Loss: 0.00004598
Iteration 85/1000 | Loss: 0.00004598
Iteration 86/1000 | Loss: 0.00004598
Iteration 87/1000 | Loss: 0.00004598
Iteration 88/1000 | Loss: 0.00004598
Iteration 89/1000 | Loss: 0.00004598
Iteration 90/1000 | Loss: 0.00004598
Iteration 91/1000 | Loss: 0.00004598
Iteration 92/1000 | Loss: 0.00004598
Iteration 93/1000 | Loss: 0.00004598
Iteration 94/1000 | Loss: 0.00004598
Iteration 95/1000 | Loss: 0.00004597
Iteration 96/1000 | Loss: 0.00004597
Iteration 97/1000 | Loss: 0.00004597
Iteration 98/1000 | Loss: 0.00004597
Iteration 99/1000 | Loss: 0.00004597
Iteration 100/1000 | Loss: 0.00004597
Iteration 101/1000 | Loss: 0.00004597
Iteration 102/1000 | Loss: 0.00004597
Iteration 103/1000 | Loss: 0.00004597
Iteration 104/1000 | Loss: 0.00004597
Iteration 105/1000 | Loss: 0.00004597
Iteration 106/1000 | Loss: 0.00004597
Iteration 107/1000 | Loss: 0.00004597
Iteration 108/1000 | Loss: 0.00004597
Iteration 109/1000 | Loss: 0.00004596
Iteration 110/1000 | Loss: 0.00004596
Iteration 111/1000 | Loss: 0.00004596
Iteration 112/1000 | Loss: 0.00004596
Iteration 113/1000 | Loss: 0.00004596
Iteration 114/1000 | Loss: 0.00004596
Iteration 115/1000 | Loss: 0.00004596
Iteration 116/1000 | Loss: 0.00004596
Iteration 117/1000 | Loss: 0.00004595
Iteration 118/1000 | Loss: 0.00004595
Iteration 119/1000 | Loss: 0.00004595
Iteration 120/1000 | Loss: 0.00004595
Iteration 121/1000 | Loss: 0.00004595
Iteration 122/1000 | Loss: 0.00004595
Iteration 123/1000 | Loss: 0.00004595
Iteration 124/1000 | Loss: 0.00004595
Iteration 125/1000 | Loss: 0.00004595
Iteration 126/1000 | Loss: 0.00004595
Iteration 127/1000 | Loss: 0.00004595
Iteration 128/1000 | Loss: 0.00004595
Iteration 129/1000 | Loss: 0.00004595
Iteration 130/1000 | Loss: 0.00004595
Iteration 131/1000 | Loss: 0.00004595
Iteration 132/1000 | Loss: 0.00004595
Iteration 133/1000 | Loss: 0.00004595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [4.594805432134308e-05, 4.594805432134308e-05, 4.594805432134308e-05, 4.594805432134308e-05, 4.594805432134308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.594805432134308e-05

Optimization complete. Final v2v error: 5.301507949829102 mm

Highest mean error: 6.0312957763671875 mm for frame 132

Lowest mean error: 3.6842687129974365 mm for frame 4

Saving results

Total time: 119.70831966400146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819753
Iteration 2/25 | Loss: 0.00145705
Iteration 3/25 | Loss: 0.00113517
Iteration 4/25 | Loss: 0.00112484
Iteration 5/25 | Loss: 0.00112322
Iteration 6/25 | Loss: 0.00112314
Iteration 7/25 | Loss: 0.00112314
Iteration 8/25 | Loss: 0.00112314
Iteration 9/25 | Loss: 0.00112314
Iteration 10/25 | Loss: 0.00112314
Iteration 11/25 | Loss: 0.00112314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011231395183131099, 0.0011231395183131099, 0.0011231395183131099, 0.0011231395183131099, 0.0011231395183131099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011231395183131099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31502080
Iteration 2/25 | Loss: 0.00056593
Iteration 3/25 | Loss: 0.00056592
Iteration 4/25 | Loss: 0.00056592
Iteration 5/25 | Loss: 0.00056592
Iteration 6/25 | Loss: 0.00056592
Iteration 7/25 | Loss: 0.00056592
Iteration 8/25 | Loss: 0.00056592
Iteration 9/25 | Loss: 0.00056592
Iteration 10/25 | Loss: 0.00056592
Iteration 11/25 | Loss: 0.00056592
Iteration 12/25 | Loss: 0.00056592
Iteration 13/25 | Loss: 0.00056592
Iteration 14/25 | Loss: 0.00056592
Iteration 15/25 | Loss: 0.00056592
Iteration 16/25 | Loss: 0.00056592
Iteration 17/25 | Loss: 0.00056592
Iteration 18/25 | Loss: 0.00056592
Iteration 19/25 | Loss: 0.00056592
Iteration 20/25 | Loss: 0.00056592
Iteration 21/25 | Loss: 0.00056592
Iteration 22/25 | Loss: 0.00056592
Iteration 23/25 | Loss: 0.00056592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005659167654812336, 0.0005659167654812336, 0.0005659167654812336, 0.0005659167654812336, 0.0005659167654812336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005659167654812336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056592
Iteration 2/1000 | Loss: 0.00002522
Iteration 3/1000 | Loss: 0.00001817
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001465
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001264
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001249
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001238
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001232
Iteration 49/1000 | Loss: 0.00001232
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001225
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001224
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001223
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001223
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.2224736565258354e-05, 1.2224736565258354e-05, 1.2224736565258354e-05, 1.2224736565258354e-05, 1.2224736565258354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2224736565258354e-05

Optimization complete. Final v2v error: 2.955919027328491 mm

Highest mean error: 3.011871814727783 mm for frame 99

Lowest mean error: 2.9067349433898926 mm for frame 141

Saving results

Total time: 32.79938554763794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898693
Iteration 2/25 | Loss: 0.00164292
Iteration 3/25 | Loss: 0.00134922
Iteration 4/25 | Loss: 0.00132913
Iteration 5/25 | Loss: 0.00132601
Iteration 6/25 | Loss: 0.00132589
Iteration 7/25 | Loss: 0.00132589
Iteration 8/25 | Loss: 0.00132589
Iteration 9/25 | Loss: 0.00132589
Iteration 10/25 | Loss: 0.00132589
Iteration 11/25 | Loss: 0.00132589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013258912367746234, 0.0013258912367746234, 0.0013258912367746234, 0.0013258912367746234, 0.0013258912367746234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013258912367746234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55126679
Iteration 2/25 | Loss: 0.00075705
Iteration 3/25 | Loss: 0.00075705
Iteration 4/25 | Loss: 0.00075705
Iteration 5/25 | Loss: 0.00075705
Iteration 6/25 | Loss: 0.00075705
Iteration 7/25 | Loss: 0.00075704
Iteration 8/25 | Loss: 0.00075704
Iteration 9/25 | Loss: 0.00075704
Iteration 10/25 | Loss: 0.00075704
Iteration 11/25 | Loss: 0.00075704
Iteration 12/25 | Loss: 0.00075704
Iteration 13/25 | Loss: 0.00075704
Iteration 14/25 | Loss: 0.00075704
Iteration 15/25 | Loss: 0.00075704
Iteration 16/25 | Loss: 0.00075704
Iteration 17/25 | Loss: 0.00075704
Iteration 18/25 | Loss: 0.00075704
Iteration 19/25 | Loss: 0.00075704
Iteration 20/25 | Loss: 0.00075704
Iteration 21/25 | Loss: 0.00075704
Iteration 22/25 | Loss: 0.00075704
Iteration 23/25 | Loss: 0.00075704
Iteration 24/25 | Loss: 0.00075704
Iteration 25/25 | Loss: 0.00075704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075704
Iteration 2/1000 | Loss: 0.00005244
Iteration 3/1000 | Loss: 0.00003581
Iteration 4/1000 | Loss: 0.00003092
Iteration 5/1000 | Loss: 0.00002934
Iteration 6/1000 | Loss: 0.00002844
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002748
Iteration 9/1000 | Loss: 0.00002696
Iteration 10/1000 | Loss: 0.00002667
Iteration 11/1000 | Loss: 0.00002643
Iteration 12/1000 | Loss: 0.00002640
Iteration 13/1000 | Loss: 0.00002619
Iteration 14/1000 | Loss: 0.00002615
Iteration 15/1000 | Loss: 0.00002604
Iteration 16/1000 | Loss: 0.00002593
Iteration 17/1000 | Loss: 0.00002587
Iteration 18/1000 | Loss: 0.00002586
Iteration 19/1000 | Loss: 0.00002577
Iteration 20/1000 | Loss: 0.00002577
Iteration 21/1000 | Loss: 0.00002577
Iteration 22/1000 | Loss: 0.00002577
Iteration 23/1000 | Loss: 0.00002576
Iteration 24/1000 | Loss: 0.00002576
Iteration 25/1000 | Loss: 0.00002576
Iteration 26/1000 | Loss: 0.00002576
Iteration 27/1000 | Loss: 0.00002576
Iteration 28/1000 | Loss: 0.00002576
Iteration 29/1000 | Loss: 0.00002576
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002575
Iteration 32/1000 | Loss: 0.00002575
Iteration 33/1000 | Loss: 0.00002575
Iteration 34/1000 | Loss: 0.00002575
Iteration 35/1000 | Loss: 0.00002575
Iteration 36/1000 | Loss: 0.00002575
Iteration 37/1000 | Loss: 0.00002575
Iteration 38/1000 | Loss: 0.00002575
Iteration 39/1000 | Loss: 0.00002574
Iteration 40/1000 | Loss: 0.00002574
Iteration 41/1000 | Loss: 0.00002574
Iteration 42/1000 | Loss: 0.00002574
Iteration 43/1000 | Loss: 0.00002574
Iteration 44/1000 | Loss: 0.00002574
Iteration 45/1000 | Loss: 0.00002574
Iteration 46/1000 | Loss: 0.00002574
Iteration 47/1000 | Loss: 0.00002574
Iteration 48/1000 | Loss: 0.00002573
Iteration 49/1000 | Loss: 0.00002573
Iteration 50/1000 | Loss: 0.00002573
Iteration 51/1000 | Loss: 0.00002573
Iteration 52/1000 | Loss: 0.00002573
Iteration 53/1000 | Loss: 0.00002573
Iteration 54/1000 | Loss: 0.00002573
Iteration 55/1000 | Loss: 0.00002573
Iteration 56/1000 | Loss: 0.00002573
Iteration 57/1000 | Loss: 0.00002573
Iteration 58/1000 | Loss: 0.00002573
Iteration 59/1000 | Loss: 0.00002573
Iteration 60/1000 | Loss: 0.00002573
Iteration 61/1000 | Loss: 0.00002573
Iteration 62/1000 | Loss: 0.00002572
Iteration 63/1000 | Loss: 0.00002572
Iteration 64/1000 | Loss: 0.00002572
Iteration 65/1000 | Loss: 0.00002572
Iteration 66/1000 | Loss: 0.00002571
Iteration 67/1000 | Loss: 0.00002571
Iteration 68/1000 | Loss: 0.00002571
Iteration 69/1000 | Loss: 0.00002570
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002570
Iteration 72/1000 | Loss: 0.00002570
Iteration 73/1000 | Loss: 0.00002570
Iteration 74/1000 | Loss: 0.00002570
Iteration 75/1000 | Loss: 0.00002570
Iteration 76/1000 | Loss: 0.00002570
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002569
Iteration 81/1000 | Loss: 0.00002569
Iteration 82/1000 | Loss: 0.00002568
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002568
Iteration 85/1000 | Loss: 0.00002568
Iteration 86/1000 | Loss: 0.00002568
Iteration 87/1000 | Loss: 0.00002568
Iteration 88/1000 | Loss: 0.00002568
Iteration 89/1000 | Loss: 0.00002568
Iteration 90/1000 | Loss: 0.00002567
Iteration 91/1000 | Loss: 0.00002567
Iteration 92/1000 | Loss: 0.00002567
Iteration 93/1000 | Loss: 0.00002567
Iteration 94/1000 | Loss: 0.00002567
Iteration 95/1000 | Loss: 0.00002567
Iteration 96/1000 | Loss: 0.00002566
Iteration 97/1000 | Loss: 0.00002566
Iteration 98/1000 | Loss: 0.00002566
Iteration 99/1000 | Loss: 0.00002566
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002566
Iteration 102/1000 | Loss: 0.00002566
Iteration 103/1000 | Loss: 0.00002565
Iteration 104/1000 | Loss: 0.00002565
Iteration 105/1000 | Loss: 0.00002565
Iteration 106/1000 | Loss: 0.00002565
Iteration 107/1000 | Loss: 0.00002565
Iteration 108/1000 | Loss: 0.00002565
Iteration 109/1000 | Loss: 0.00002565
Iteration 110/1000 | Loss: 0.00002565
Iteration 111/1000 | Loss: 0.00002564
Iteration 112/1000 | Loss: 0.00002564
Iteration 113/1000 | Loss: 0.00002564
Iteration 114/1000 | Loss: 0.00002564
Iteration 115/1000 | Loss: 0.00002564
Iteration 116/1000 | Loss: 0.00002564
Iteration 117/1000 | Loss: 0.00002564
Iteration 118/1000 | Loss: 0.00002564
Iteration 119/1000 | Loss: 0.00002564
Iteration 120/1000 | Loss: 0.00002563
Iteration 121/1000 | Loss: 0.00002563
Iteration 122/1000 | Loss: 0.00002563
Iteration 123/1000 | Loss: 0.00002563
Iteration 124/1000 | Loss: 0.00002563
Iteration 125/1000 | Loss: 0.00002563
Iteration 126/1000 | Loss: 0.00002563
Iteration 127/1000 | Loss: 0.00002563
Iteration 128/1000 | Loss: 0.00002563
Iteration 129/1000 | Loss: 0.00002563
Iteration 130/1000 | Loss: 0.00002562
Iteration 131/1000 | Loss: 0.00002562
Iteration 132/1000 | Loss: 0.00002562
Iteration 133/1000 | Loss: 0.00002562
Iteration 134/1000 | Loss: 0.00002562
Iteration 135/1000 | Loss: 0.00002562
Iteration 136/1000 | Loss: 0.00002562
Iteration 137/1000 | Loss: 0.00002562
Iteration 138/1000 | Loss: 0.00002562
Iteration 139/1000 | Loss: 0.00002562
Iteration 140/1000 | Loss: 0.00002562
Iteration 141/1000 | Loss: 0.00002562
Iteration 142/1000 | Loss: 0.00002561
Iteration 143/1000 | Loss: 0.00002561
Iteration 144/1000 | Loss: 0.00002561
Iteration 145/1000 | Loss: 0.00002561
Iteration 146/1000 | Loss: 0.00002561
Iteration 147/1000 | Loss: 0.00002561
Iteration 148/1000 | Loss: 0.00002561
Iteration 149/1000 | Loss: 0.00002561
Iteration 150/1000 | Loss: 0.00002561
Iteration 151/1000 | Loss: 0.00002561
Iteration 152/1000 | Loss: 0.00002561
Iteration 153/1000 | Loss: 0.00002561
Iteration 154/1000 | Loss: 0.00002561
Iteration 155/1000 | Loss: 0.00002561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.5611774617573246e-05, 2.5611774617573246e-05, 2.5611774617573246e-05, 2.5611774617573246e-05, 2.5611774617573246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5611774617573246e-05

Optimization complete. Final v2v error: 4.204833984375 mm

Highest mean error: 4.8175225257873535 mm for frame 17

Lowest mean error: 3.9620795249938965 mm for frame 86

Saving results

Total time: 38.89363241195679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416312
Iteration 2/25 | Loss: 0.00117258
Iteration 3/25 | Loss: 0.00109809
Iteration 4/25 | Loss: 0.00108340
Iteration 5/25 | Loss: 0.00107915
Iteration 6/25 | Loss: 0.00107854
Iteration 7/25 | Loss: 0.00107854
Iteration 8/25 | Loss: 0.00107854
Iteration 9/25 | Loss: 0.00107854
Iteration 10/25 | Loss: 0.00107854
Iteration 11/25 | Loss: 0.00107854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010785391787067056, 0.0010785391787067056, 0.0010785391787067056, 0.0010785391787067056, 0.0010785391787067056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010785391787067056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37271070
Iteration 2/25 | Loss: 0.00078419
Iteration 3/25 | Loss: 0.00078418
Iteration 4/25 | Loss: 0.00078418
Iteration 5/25 | Loss: 0.00078418
Iteration 6/25 | Loss: 0.00078418
Iteration 7/25 | Loss: 0.00078418
Iteration 8/25 | Loss: 0.00078418
Iteration 9/25 | Loss: 0.00078418
Iteration 10/25 | Loss: 0.00078418
Iteration 11/25 | Loss: 0.00078418
Iteration 12/25 | Loss: 0.00078418
Iteration 13/25 | Loss: 0.00078418
Iteration 14/25 | Loss: 0.00078418
Iteration 15/25 | Loss: 0.00078418
Iteration 16/25 | Loss: 0.00078418
Iteration 17/25 | Loss: 0.00078418
Iteration 18/25 | Loss: 0.00078418
Iteration 19/25 | Loss: 0.00078418
Iteration 20/25 | Loss: 0.00078418
Iteration 21/25 | Loss: 0.00078418
Iteration 22/25 | Loss: 0.00078418
Iteration 23/25 | Loss: 0.00078418
Iteration 24/25 | Loss: 0.00078418
Iteration 25/25 | Loss: 0.00078418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078418
Iteration 2/1000 | Loss: 0.00002426
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001369
Iteration 5/1000 | Loss: 0.00001315
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001215
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001167
Iteration 15/1000 | Loss: 0.00001159
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001117
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001108
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001107
Iteration 96/1000 | Loss: 0.00001107
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001106
Iteration 101/1000 | Loss: 0.00001106
Iteration 102/1000 | Loss: 0.00001106
Iteration 103/1000 | Loss: 0.00001105
Iteration 104/1000 | Loss: 0.00001105
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001105
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001104
Iteration 118/1000 | Loss: 0.00001103
Iteration 119/1000 | Loss: 0.00001103
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001101
Iteration 132/1000 | Loss: 0.00001101
Iteration 133/1000 | Loss: 0.00001101
Iteration 134/1000 | Loss: 0.00001101
Iteration 135/1000 | Loss: 0.00001101
Iteration 136/1000 | Loss: 0.00001101
Iteration 137/1000 | Loss: 0.00001101
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001100
Iteration 144/1000 | Loss: 0.00001100
Iteration 145/1000 | Loss: 0.00001100
Iteration 146/1000 | Loss: 0.00001100
Iteration 147/1000 | Loss: 0.00001100
Iteration 148/1000 | Loss: 0.00001100
Iteration 149/1000 | Loss: 0.00001099
Iteration 150/1000 | Loss: 0.00001099
Iteration 151/1000 | Loss: 0.00001099
Iteration 152/1000 | Loss: 0.00001099
Iteration 153/1000 | Loss: 0.00001099
Iteration 154/1000 | Loss: 0.00001099
Iteration 155/1000 | Loss: 0.00001099
Iteration 156/1000 | Loss: 0.00001099
Iteration 157/1000 | Loss: 0.00001099
Iteration 158/1000 | Loss: 0.00001099
Iteration 159/1000 | Loss: 0.00001099
Iteration 160/1000 | Loss: 0.00001098
Iteration 161/1000 | Loss: 0.00001098
Iteration 162/1000 | Loss: 0.00001098
Iteration 163/1000 | Loss: 0.00001098
Iteration 164/1000 | Loss: 0.00001098
Iteration 165/1000 | Loss: 0.00001098
Iteration 166/1000 | Loss: 0.00001098
Iteration 167/1000 | Loss: 0.00001098
Iteration 168/1000 | Loss: 0.00001098
Iteration 169/1000 | Loss: 0.00001098
Iteration 170/1000 | Loss: 0.00001098
Iteration 171/1000 | Loss: 0.00001098
Iteration 172/1000 | Loss: 0.00001098
Iteration 173/1000 | Loss: 0.00001098
Iteration 174/1000 | Loss: 0.00001098
Iteration 175/1000 | Loss: 0.00001098
Iteration 176/1000 | Loss: 0.00001098
Iteration 177/1000 | Loss: 0.00001098
Iteration 178/1000 | Loss: 0.00001098
Iteration 179/1000 | Loss: 0.00001098
Iteration 180/1000 | Loss: 0.00001098
Iteration 181/1000 | Loss: 0.00001098
Iteration 182/1000 | Loss: 0.00001098
Iteration 183/1000 | Loss: 0.00001098
Iteration 184/1000 | Loss: 0.00001098
Iteration 185/1000 | Loss: 0.00001098
Iteration 186/1000 | Loss: 0.00001098
Iteration 187/1000 | Loss: 0.00001098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0976750672853086e-05, 1.0976750672853086e-05, 1.0976750672853086e-05, 1.0976750672853086e-05, 1.0976750672853086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0976750672853086e-05

Optimization complete. Final v2v error: 2.8645517826080322 mm

Highest mean error: 3.0953176021575928 mm for frame 89

Lowest mean error: 2.6616275310516357 mm for frame 116

Saving results

Total time: 38.22673511505127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030741
Iteration 2/25 | Loss: 0.00205240
Iteration 3/25 | Loss: 0.00171984
Iteration 4/25 | Loss: 0.00154107
Iteration 5/25 | Loss: 0.00141003
Iteration 6/25 | Loss: 0.00137542
Iteration 7/25 | Loss: 0.00128263
Iteration 8/25 | Loss: 0.00127161
Iteration 9/25 | Loss: 0.00121485
Iteration 10/25 | Loss: 0.00119338
Iteration 11/25 | Loss: 0.00118052
Iteration 12/25 | Loss: 0.00116695
Iteration 13/25 | Loss: 0.00116181
Iteration 14/25 | Loss: 0.00116054
Iteration 15/25 | Loss: 0.00115922
Iteration 16/25 | Loss: 0.00116452
Iteration 17/25 | Loss: 0.00116323
Iteration 18/25 | Loss: 0.00115995
Iteration 19/25 | Loss: 0.00115572
Iteration 20/25 | Loss: 0.00115369
Iteration 21/25 | Loss: 0.00114860
Iteration 22/25 | Loss: 0.00115811
Iteration 23/25 | Loss: 0.00114919
Iteration 24/25 | Loss: 0.00114517
Iteration 25/25 | Loss: 0.00114406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56462502
Iteration 2/25 | Loss: 0.00098221
Iteration 3/25 | Loss: 0.00098221
Iteration 4/25 | Loss: 0.00098221
Iteration 5/25 | Loss: 0.00098221
Iteration 6/25 | Loss: 0.00098221
Iteration 7/25 | Loss: 0.00098221
Iteration 8/25 | Loss: 0.00098221
Iteration 9/25 | Loss: 0.00098221
Iteration 10/25 | Loss: 0.00098221
Iteration 11/25 | Loss: 0.00098221
Iteration 12/25 | Loss: 0.00098221
Iteration 13/25 | Loss: 0.00098221
Iteration 14/25 | Loss: 0.00098221
Iteration 15/25 | Loss: 0.00098221
Iteration 16/25 | Loss: 0.00098221
Iteration 17/25 | Loss: 0.00098221
Iteration 18/25 | Loss: 0.00098221
Iteration 19/25 | Loss: 0.00098221
Iteration 20/25 | Loss: 0.00098221
Iteration 21/25 | Loss: 0.00098221
Iteration 22/25 | Loss: 0.00098221
Iteration 23/25 | Loss: 0.00098221
Iteration 24/25 | Loss: 0.00098221
Iteration 25/25 | Loss: 0.00098221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098221
Iteration 2/1000 | Loss: 0.00018417
Iteration 3/1000 | Loss: 0.00052823
Iteration 4/1000 | Loss: 0.00006762
Iteration 5/1000 | Loss: 0.00012498
Iteration 6/1000 | Loss: 0.00024891
Iteration 7/1000 | Loss: 0.00057200
Iteration 8/1000 | Loss: 0.00054765
Iteration 9/1000 | Loss: 0.00036630
Iteration 10/1000 | Loss: 0.00041332
Iteration 11/1000 | Loss: 0.00009750
Iteration 12/1000 | Loss: 0.00014265
Iteration 13/1000 | Loss: 0.00023975
Iteration 14/1000 | Loss: 0.00020474
Iteration 15/1000 | Loss: 0.00020225
Iteration 16/1000 | Loss: 0.00018740
Iteration 17/1000 | Loss: 0.00017643
Iteration 18/1000 | Loss: 0.00014977
Iteration 19/1000 | Loss: 0.00018071
Iteration 20/1000 | Loss: 0.00010693
Iteration 21/1000 | Loss: 0.00022897
Iteration 22/1000 | Loss: 0.00024961
Iteration 23/1000 | Loss: 0.00020632
Iteration 24/1000 | Loss: 0.00033079
Iteration 25/1000 | Loss: 0.00016342
Iteration 26/1000 | Loss: 0.00028023
Iteration 27/1000 | Loss: 0.00005140
Iteration 28/1000 | Loss: 0.00005589
Iteration 29/1000 | Loss: 0.00031910
Iteration 30/1000 | Loss: 0.00082541
Iteration 31/1000 | Loss: 0.00030233
Iteration 32/1000 | Loss: 0.00010031
Iteration 33/1000 | Loss: 0.00066770
Iteration 34/1000 | Loss: 0.00003541
Iteration 35/1000 | Loss: 0.00004937
Iteration 36/1000 | Loss: 0.00004520
Iteration 37/1000 | Loss: 0.00005072
Iteration 38/1000 | Loss: 0.00005137
Iteration 39/1000 | Loss: 0.00005643
Iteration 40/1000 | Loss: 0.00004910
Iteration 41/1000 | Loss: 0.00004730
Iteration 42/1000 | Loss: 0.00004518
Iteration 43/1000 | Loss: 0.00002929
Iteration 44/1000 | Loss: 0.00003830
Iteration 45/1000 | Loss: 0.00003818
Iteration 46/1000 | Loss: 0.00003791
Iteration 47/1000 | Loss: 0.00003919
Iteration 48/1000 | Loss: 0.00004335
Iteration 49/1000 | Loss: 0.00003865
Iteration 50/1000 | Loss: 0.00003941
Iteration 51/1000 | Loss: 0.00094767
Iteration 52/1000 | Loss: 0.00005309
Iteration 53/1000 | Loss: 0.00007082
Iteration 54/1000 | Loss: 0.00004762
Iteration 55/1000 | Loss: 0.00003359
Iteration 56/1000 | Loss: 0.00004387
Iteration 57/1000 | Loss: 0.00003636
Iteration 58/1000 | Loss: 0.00004388
Iteration 59/1000 | Loss: 0.00003619
Iteration 60/1000 | Loss: 0.00004166
Iteration 61/1000 | Loss: 0.00006377
Iteration 62/1000 | Loss: 0.00003837
Iteration 63/1000 | Loss: 0.00005078
Iteration 64/1000 | Loss: 0.00002929
Iteration 65/1000 | Loss: 0.00003348
Iteration 66/1000 | Loss: 0.00003281
Iteration 67/1000 | Loss: 0.00003413
Iteration 68/1000 | Loss: 0.00004292
Iteration 69/1000 | Loss: 0.00003916
Iteration 70/1000 | Loss: 0.00003632
Iteration 71/1000 | Loss: 0.00003757
Iteration 72/1000 | Loss: 0.00003776
Iteration 73/1000 | Loss: 0.00005422
Iteration 74/1000 | Loss: 0.00002717
Iteration 75/1000 | Loss: 0.00003727
Iteration 76/1000 | Loss: 0.00003034
Iteration 77/1000 | Loss: 0.00003653
Iteration 78/1000 | Loss: 0.00003776
Iteration 79/1000 | Loss: 0.00003502
Iteration 80/1000 | Loss: 0.00004268
Iteration 81/1000 | Loss: 0.00002799
Iteration 82/1000 | Loss: 0.00004015
Iteration 83/1000 | Loss: 0.00003239
Iteration 84/1000 | Loss: 0.00003318
Iteration 85/1000 | Loss: 0.00003125
Iteration 86/1000 | Loss: 0.00002896
Iteration 87/1000 | Loss: 0.00002867
Iteration 88/1000 | Loss: 0.00003634
Iteration 89/1000 | Loss: 0.00002838
Iteration 90/1000 | Loss: 0.00003413
Iteration 91/1000 | Loss: 0.00002892
Iteration 92/1000 | Loss: 0.00003394
Iteration 93/1000 | Loss: 0.00003625
Iteration 94/1000 | Loss: 0.00003332
Iteration 95/1000 | Loss: 0.00002960
Iteration 96/1000 | Loss: 0.00002891
Iteration 97/1000 | Loss: 0.00003566
Iteration 98/1000 | Loss: 0.00002880
Iteration 99/1000 | Loss: 0.00003489
Iteration 100/1000 | Loss: 0.00002812
Iteration 101/1000 | Loss: 0.00002972
Iteration 102/1000 | Loss: 0.00002774
Iteration 103/1000 | Loss: 0.00003286
Iteration 104/1000 | Loss: 0.00002758
Iteration 105/1000 | Loss: 0.00003267
Iteration 106/1000 | Loss: 0.00002733
Iteration 107/1000 | Loss: 0.00002890
Iteration 108/1000 | Loss: 0.00002474
Iteration 109/1000 | Loss: 0.00002883
Iteration 110/1000 | Loss: 0.00002710
Iteration 111/1000 | Loss: 0.00003117
Iteration 112/1000 | Loss: 0.00003600
Iteration 113/1000 | Loss: 0.00003062
Iteration 114/1000 | Loss: 0.00002883
Iteration 115/1000 | Loss: 0.00002591
Iteration 116/1000 | Loss: 0.00002916
Iteration 117/1000 | Loss: 0.00003142
Iteration 118/1000 | Loss: 0.00003382
Iteration 119/1000 | Loss: 0.00003014
Iteration 120/1000 | Loss: 0.00003001
Iteration 121/1000 | Loss: 0.00002991
Iteration 122/1000 | Loss: 0.00002554
Iteration 123/1000 | Loss: 0.00002709
Iteration 124/1000 | Loss: 0.00003155
Iteration 125/1000 | Loss: 0.00003648
Iteration 126/1000 | Loss: 0.00004257
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001415
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001234
Iteration 134/1000 | Loss: 0.00001233
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001197
Iteration 139/1000 | Loss: 0.00001185
Iteration 140/1000 | Loss: 0.00001184
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001182
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001177
Iteration 151/1000 | Loss: 0.00001177
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001174
Iteration 157/1000 | Loss: 0.00001174
Iteration 158/1000 | Loss: 0.00001174
Iteration 159/1000 | Loss: 0.00001174
Iteration 160/1000 | Loss: 0.00001174
Iteration 161/1000 | Loss: 0.00001173
Iteration 162/1000 | Loss: 0.00001173
Iteration 163/1000 | Loss: 0.00001173
Iteration 164/1000 | Loss: 0.00001173
Iteration 165/1000 | Loss: 0.00001173
Iteration 166/1000 | Loss: 0.00001173
Iteration 167/1000 | Loss: 0.00001173
Iteration 168/1000 | Loss: 0.00001173
Iteration 169/1000 | Loss: 0.00001173
Iteration 170/1000 | Loss: 0.00001173
Iteration 171/1000 | Loss: 0.00001172
Iteration 172/1000 | Loss: 0.00001172
Iteration 173/1000 | Loss: 0.00001171
Iteration 174/1000 | Loss: 0.00001171
Iteration 175/1000 | Loss: 0.00001171
Iteration 176/1000 | Loss: 0.00001171
Iteration 177/1000 | Loss: 0.00001171
Iteration 178/1000 | Loss: 0.00001171
Iteration 179/1000 | Loss: 0.00001171
Iteration 180/1000 | Loss: 0.00001171
Iteration 181/1000 | Loss: 0.00001171
Iteration 182/1000 | Loss: 0.00001171
Iteration 183/1000 | Loss: 0.00001171
Iteration 184/1000 | Loss: 0.00001171
Iteration 185/1000 | Loss: 0.00001170
Iteration 186/1000 | Loss: 0.00001170
Iteration 187/1000 | Loss: 0.00001170
Iteration 188/1000 | Loss: 0.00001170
Iteration 189/1000 | Loss: 0.00001170
Iteration 190/1000 | Loss: 0.00001170
Iteration 191/1000 | Loss: 0.00001170
Iteration 192/1000 | Loss: 0.00001170
Iteration 193/1000 | Loss: 0.00001170
Iteration 194/1000 | Loss: 0.00001170
Iteration 195/1000 | Loss: 0.00001170
Iteration 196/1000 | Loss: 0.00001170
Iteration 197/1000 | Loss: 0.00001170
Iteration 198/1000 | Loss: 0.00001170
Iteration 199/1000 | Loss: 0.00001170
Iteration 200/1000 | Loss: 0.00001170
Iteration 201/1000 | Loss: 0.00001170
Iteration 202/1000 | Loss: 0.00001169
Iteration 203/1000 | Loss: 0.00001169
Iteration 204/1000 | Loss: 0.00001169
Iteration 205/1000 | Loss: 0.00001169
Iteration 206/1000 | Loss: 0.00001169
Iteration 207/1000 | Loss: 0.00001169
Iteration 208/1000 | Loss: 0.00001169
Iteration 209/1000 | Loss: 0.00001169
Iteration 210/1000 | Loss: 0.00001169
Iteration 211/1000 | Loss: 0.00001169
Iteration 212/1000 | Loss: 0.00001169
Iteration 213/1000 | Loss: 0.00001169
Iteration 214/1000 | Loss: 0.00001169
Iteration 215/1000 | Loss: 0.00001169
Iteration 216/1000 | Loss: 0.00001169
Iteration 217/1000 | Loss: 0.00001168
Iteration 218/1000 | Loss: 0.00001168
Iteration 219/1000 | Loss: 0.00001168
Iteration 220/1000 | Loss: 0.00001168
Iteration 221/1000 | Loss: 0.00001168
Iteration 222/1000 | Loss: 0.00001168
Iteration 223/1000 | Loss: 0.00001168
Iteration 224/1000 | Loss: 0.00001168
Iteration 225/1000 | Loss: 0.00001168
Iteration 226/1000 | Loss: 0.00001168
Iteration 227/1000 | Loss: 0.00001168
Iteration 228/1000 | Loss: 0.00001168
Iteration 229/1000 | Loss: 0.00001168
Iteration 230/1000 | Loss: 0.00001168
Iteration 231/1000 | Loss: 0.00001168
Iteration 232/1000 | Loss: 0.00001167
Iteration 233/1000 | Loss: 0.00001167
Iteration 234/1000 | Loss: 0.00001167
Iteration 235/1000 | Loss: 0.00001167
Iteration 236/1000 | Loss: 0.00001167
Iteration 237/1000 | Loss: 0.00001167
Iteration 238/1000 | Loss: 0.00001167
Iteration 239/1000 | Loss: 0.00001167
Iteration 240/1000 | Loss: 0.00001167
Iteration 241/1000 | Loss: 0.00001167
Iteration 242/1000 | Loss: 0.00001167
Iteration 243/1000 | Loss: 0.00001166
Iteration 244/1000 | Loss: 0.00001166
Iteration 245/1000 | Loss: 0.00001166
Iteration 246/1000 | Loss: 0.00001166
Iteration 247/1000 | Loss: 0.00001166
Iteration 248/1000 | Loss: 0.00001166
Iteration 249/1000 | Loss: 0.00001166
Iteration 250/1000 | Loss: 0.00001166
Iteration 251/1000 | Loss: 0.00001166
Iteration 252/1000 | Loss: 0.00001166
Iteration 253/1000 | Loss: 0.00001165
Iteration 254/1000 | Loss: 0.00001165
Iteration 255/1000 | Loss: 0.00001165
Iteration 256/1000 | Loss: 0.00001165
Iteration 257/1000 | Loss: 0.00001165
Iteration 258/1000 | Loss: 0.00001165
Iteration 259/1000 | Loss: 0.00001165
Iteration 260/1000 | Loss: 0.00001165
Iteration 261/1000 | Loss: 0.00001165
Iteration 262/1000 | Loss: 0.00001165
Iteration 263/1000 | Loss: 0.00001165
Iteration 264/1000 | Loss: 0.00001165
Iteration 265/1000 | Loss: 0.00001165
Iteration 266/1000 | Loss: 0.00001165
Iteration 267/1000 | Loss: 0.00001165
Iteration 268/1000 | Loss: 0.00001165
Iteration 269/1000 | Loss: 0.00001165
Iteration 270/1000 | Loss: 0.00001165
Iteration 271/1000 | Loss: 0.00001165
Iteration 272/1000 | Loss: 0.00001165
Iteration 273/1000 | Loss: 0.00001165
Iteration 274/1000 | Loss: 0.00001165
Iteration 275/1000 | Loss: 0.00001165
Iteration 276/1000 | Loss: 0.00001165
Iteration 277/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.1648131476249546e-05, 1.1648131476249546e-05, 1.1648131476249546e-05, 1.1648131476249546e-05, 1.1648131476249546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1648131476249546e-05

Optimization complete. Final v2v error: 2.916740894317627 mm

Highest mean error: 3.6950995922088623 mm for frame 107

Lowest mean error: 2.645686149597168 mm for frame 86

Saving results

Total time: 235.85378694534302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770208
Iteration 2/25 | Loss: 0.00156678
Iteration 3/25 | Loss: 0.00125305
Iteration 4/25 | Loss: 0.00123417
Iteration 5/25 | Loss: 0.00123272
Iteration 6/25 | Loss: 0.00123272
Iteration 7/25 | Loss: 0.00123272
Iteration 8/25 | Loss: 0.00123272
Iteration 9/25 | Loss: 0.00123272
Iteration 10/25 | Loss: 0.00123272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012327182339504361, 0.0012327182339504361, 0.0012327182339504361, 0.0012327182339504361, 0.0012327182339504361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012327182339504361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42577052
Iteration 2/25 | Loss: 0.00079997
Iteration 3/25 | Loss: 0.00079994
Iteration 4/25 | Loss: 0.00079994
Iteration 5/25 | Loss: 0.00079994
Iteration 6/25 | Loss: 0.00079994
Iteration 7/25 | Loss: 0.00079994
Iteration 8/25 | Loss: 0.00079994
Iteration 9/25 | Loss: 0.00079994
Iteration 10/25 | Loss: 0.00079994
Iteration 11/25 | Loss: 0.00079994
Iteration 12/25 | Loss: 0.00079994
Iteration 13/25 | Loss: 0.00079994
Iteration 14/25 | Loss: 0.00079994
Iteration 15/25 | Loss: 0.00079994
Iteration 16/25 | Loss: 0.00079994
Iteration 17/25 | Loss: 0.00079994
Iteration 18/25 | Loss: 0.00079994
Iteration 19/25 | Loss: 0.00079994
Iteration 20/25 | Loss: 0.00079994
Iteration 21/25 | Loss: 0.00079994
Iteration 22/25 | Loss: 0.00079994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007999374647624791, 0.0007999374647624791, 0.0007999374647624791, 0.0007999374647624791, 0.0007999374647624791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007999374647624791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079994
Iteration 2/1000 | Loss: 0.00003267
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002117
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00001964
Iteration 10/1000 | Loss: 0.00001943
Iteration 11/1000 | Loss: 0.00001922
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001896
Iteration 17/1000 | Loss: 0.00001895
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001895
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001888
Iteration 23/1000 | Loss: 0.00001888
Iteration 24/1000 | Loss: 0.00001888
Iteration 25/1000 | Loss: 0.00001887
Iteration 26/1000 | Loss: 0.00001887
Iteration 27/1000 | Loss: 0.00001887
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001886
Iteration 30/1000 | Loss: 0.00001886
Iteration 31/1000 | Loss: 0.00001886
Iteration 32/1000 | Loss: 0.00001886
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001885
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00001884
Iteration 38/1000 | Loss: 0.00001883
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001880
Iteration 50/1000 | Loss: 0.00001873
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001864
Iteration 89/1000 | Loss: 0.00001864
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001860
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001859
Iteration 112/1000 | Loss: 0.00001859
Iteration 113/1000 | Loss: 0.00001859
Iteration 114/1000 | Loss: 0.00001859
Iteration 115/1000 | Loss: 0.00001859
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001858
Iteration 129/1000 | Loss: 0.00001858
Iteration 130/1000 | Loss: 0.00001858
Iteration 131/1000 | Loss: 0.00001858
Iteration 132/1000 | Loss: 0.00001858
Iteration 133/1000 | Loss: 0.00001858
Iteration 134/1000 | Loss: 0.00001858
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001858
Iteration 137/1000 | Loss: 0.00001858
Iteration 138/1000 | Loss: 0.00001857
Iteration 139/1000 | Loss: 0.00001857
Iteration 140/1000 | Loss: 0.00001857
Iteration 141/1000 | Loss: 0.00001857
Iteration 142/1000 | Loss: 0.00001857
Iteration 143/1000 | Loss: 0.00001857
Iteration 144/1000 | Loss: 0.00001857
Iteration 145/1000 | Loss: 0.00001857
Iteration 146/1000 | Loss: 0.00001857
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.8570764950709417e-05, 1.8570764950709417e-05, 1.8570764950709417e-05, 1.8570764950709417e-05, 1.8570764950709417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8570764950709417e-05

Optimization complete. Final v2v error: 3.6124396324157715 mm

Highest mean error: 3.748154878616333 mm for frame 2

Lowest mean error: 3.434746026992798 mm for frame 212

Saving results

Total time: 41.069546937942505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005533
Iteration 2/25 | Loss: 0.00183339
Iteration 3/25 | Loss: 0.00126827
Iteration 4/25 | Loss: 0.00124318
Iteration 5/25 | Loss: 0.00123919
Iteration 6/25 | Loss: 0.00123801
Iteration 7/25 | Loss: 0.00123801
Iteration 8/25 | Loss: 0.00123801
Iteration 9/25 | Loss: 0.00123801
Iteration 10/25 | Loss: 0.00123801
Iteration 11/25 | Loss: 0.00123801
Iteration 12/25 | Loss: 0.00123801
Iteration 13/25 | Loss: 0.00123801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012380146654322743, 0.0012380146654322743, 0.0012380146654322743, 0.0012380146654322743, 0.0012380146654322743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012380146654322743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.46974877
Iteration 2/25 | Loss: 0.00081037
Iteration 3/25 | Loss: 0.00081037
Iteration 4/25 | Loss: 0.00081037
Iteration 5/25 | Loss: 0.00081037
Iteration 6/25 | Loss: 0.00081037
Iteration 7/25 | Loss: 0.00081037
Iteration 8/25 | Loss: 0.00081037
Iteration 9/25 | Loss: 0.00081037
Iteration 10/25 | Loss: 0.00081037
Iteration 11/25 | Loss: 0.00081037
Iteration 12/25 | Loss: 0.00081037
Iteration 13/25 | Loss: 0.00081037
Iteration 14/25 | Loss: 0.00081037
Iteration 15/25 | Loss: 0.00081037
Iteration 16/25 | Loss: 0.00081037
Iteration 17/25 | Loss: 0.00081037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008103694999590516, 0.0008103694999590516, 0.0008103694999590516, 0.0008103694999590516, 0.0008103694999590516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008103694999590516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081037
Iteration 2/1000 | Loss: 0.00004111
Iteration 3/1000 | Loss: 0.00002638
Iteration 4/1000 | Loss: 0.00002392
Iteration 5/1000 | Loss: 0.00002230
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00002046
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001977
Iteration 12/1000 | Loss: 0.00001964
Iteration 13/1000 | Loss: 0.00001961
Iteration 14/1000 | Loss: 0.00001960
Iteration 15/1000 | Loss: 0.00001956
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001938
Iteration 25/1000 | Loss: 0.00001937
Iteration 26/1000 | Loss: 0.00001937
Iteration 27/1000 | Loss: 0.00001937
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00001937
Iteration 31/1000 | Loss: 0.00001937
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001935
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001934
Iteration 42/1000 | Loss: 0.00001933
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001933
Iteration 45/1000 | Loss: 0.00001933
Iteration 46/1000 | Loss: 0.00001932
Iteration 47/1000 | Loss: 0.00001932
Iteration 48/1000 | Loss: 0.00001932
Iteration 49/1000 | Loss: 0.00001932
Iteration 50/1000 | Loss: 0.00001932
Iteration 51/1000 | Loss: 0.00001932
Iteration 52/1000 | Loss: 0.00001932
Iteration 53/1000 | Loss: 0.00001932
Iteration 54/1000 | Loss: 0.00001931
Iteration 55/1000 | Loss: 0.00001931
Iteration 56/1000 | Loss: 0.00001931
Iteration 57/1000 | Loss: 0.00001931
Iteration 58/1000 | Loss: 0.00001931
Iteration 59/1000 | Loss: 0.00001931
Iteration 60/1000 | Loss: 0.00001931
Iteration 61/1000 | Loss: 0.00001931
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001929
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001929
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001928
Iteration 76/1000 | Loss: 0.00001928
Iteration 77/1000 | Loss: 0.00001928
Iteration 78/1000 | Loss: 0.00001928
Iteration 79/1000 | Loss: 0.00001928
Iteration 80/1000 | Loss: 0.00001927
Iteration 81/1000 | Loss: 0.00001927
Iteration 82/1000 | Loss: 0.00001927
Iteration 83/1000 | Loss: 0.00001926
Iteration 84/1000 | Loss: 0.00001926
Iteration 85/1000 | Loss: 0.00001926
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001924
Iteration 91/1000 | Loss: 0.00001924
Iteration 92/1000 | Loss: 0.00001924
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001923
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001922
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001921
Iteration 105/1000 | Loss: 0.00001921
Iteration 106/1000 | Loss: 0.00001921
Iteration 107/1000 | Loss: 0.00001920
Iteration 108/1000 | Loss: 0.00001920
Iteration 109/1000 | Loss: 0.00001920
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001919
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001917
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001916
Iteration 124/1000 | Loss: 0.00001916
Iteration 125/1000 | Loss: 0.00001916
Iteration 126/1000 | Loss: 0.00001916
Iteration 127/1000 | Loss: 0.00001915
Iteration 128/1000 | Loss: 0.00001915
Iteration 129/1000 | Loss: 0.00001915
Iteration 130/1000 | Loss: 0.00001915
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001915
Iteration 134/1000 | Loss: 0.00001915
Iteration 135/1000 | Loss: 0.00001915
Iteration 136/1000 | Loss: 0.00001915
Iteration 137/1000 | Loss: 0.00001915
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.91534618352307e-05, 1.91534618352307e-05, 1.91534618352307e-05, 1.91534618352307e-05, 1.91534618352307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.91534618352307e-05

Optimization complete. Final v2v error: 3.5765669345855713 mm

Highest mean error: 4.444275856018066 mm for frame 30

Lowest mean error: 2.9991495609283447 mm for frame 9

Saving results

Total time: 40.43974852561951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403754
Iteration 2/25 | Loss: 0.00119642
Iteration 3/25 | Loss: 0.00109998
Iteration 4/25 | Loss: 0.00108943
Iteration 5/25 | Loss: 0.00108657
Iteration 6/25 | Loss: 0.00108576
Iteration 7/25 | Loss: 0.00108576
Iteration 8/25 | Loss: 0.00108576
Iteration 9/25 | Loss: 0.00108576
Iteration 10/25 | Loss: 0.00108576
Iteration 11/25 | Loss: 0.00108576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010857589077204466, 0.0010857589077204466, 0.0010857589077204466, 0.0010857589077204466, 0.0010857589077204466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010857589077204466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34673083
Iteration 2/25 | Loss: 0.00093555
Iteration 3/25 | Loss: 0.00093553
Iteration 4/25 | Loss: 0.00093553
Iteration 5/25 | Loss: 0.00093553
Iteration 6/25 | Loss: 0.00093553
Iteration 7/25 | Loss: 0.00093553
Iteration 8/25 | Loss: 0.00093553
Iteration 9/25 | Loss: 0.00093553
Iteration 10/25 | Loss: 0.00093553
Iteration 11/25 | Loss: 0.00093553
Iteration 12/25 | Loss: 0.00093553
Iteration 13/25 | Loss: 0.00093553
Iteration 14/25 | Loss: 0.00093553
Iteration 15/25 | Loss: 0.00093553
Iteration 16/25 | Loss: 0.00093553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009355272632092237, 0.0009355272632092237, 0.0009355272632092237, 0.0009355272632092237, 0.0009355272632092237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009355272632092237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093553
Iteration 2/1000 | Loss: 0.00004458
Iteration 3/1000 | Loss: 0.00002756
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001633
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001348
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001313
Iteration 18/1000 | Loss: 0.00001308
Iteration 19/1000 | Loss: 0.00001308
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001281
Iteration 71/1000 | Loss: 0.00001281
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001280
Iteration 74/1000 | Loss: 0.00001280
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001279
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001275
Iteration 91/1000 | Loss: 0.00001275
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001272
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001270
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001268
Iteration 127/1000 | Loss: 0.00001268
Iteration 128/1000 | Loss: 0.00001268
Iteration 129/1000 | Loss: 0.00001268
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Iteration 139/1000 | Loss: 0.00001267
Iteration 140/1000 | Loss: 0.00001267
Iteration 141/1000 | Loss: 0.00001267
Iteration 142/1000 | Loss: 0.00001267
Iteration 143/1000 | Loss: 0.00001267
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001266
Iteration 148/1000 | Loss: 0.00001266
Iteration 149/1000 | Loss: 0.00001266
Iteration 150/1000 | Loss: 0.00001266
Iteration 151/1000 | Loss: 0.00001266
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001266
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001265
Iteration 164/1000 | Loss: 0.00001265
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001265
Iteration 171/1000 | Loss: 0.00001265
Iteration 172/1000 | Loss: 0.00001265
Iteration 173/1000 | Loss: 0.00001265
Iteration 174/1000 | Loss: 0.00001265
Iteration 175/1000 | Loss: 0.00001265
Iteration 176/1000 | Loss: 0.00001265
Iteration 177/1000 | Loss: 0.00001265
Iteration 178/1000 | Loss: 0.00001265
Iteration 179/1000 | Loss: 0.00001264
Iteration 180/1000 | Loss: 0.00001264
Iteration 181/1000 | Loss: 0.00001264
Iteration 182/1000 | Loss: 0.00001264
Iteration 183/1000 | Loss: 0.00001264
Iteration 184/1000 | Loss: 0.00001264
Iteration 185/1000 | Loss: 0.00001264
Iteration 186/1000 | Loss: 0.00001264
Iteration 187/1000 | Loss: 0.00001264
Iteration 188/1000 | Loss: 0.00001264
Iteration 189/1000 | Loss: 0.00001264
Iteration 190/1000 | Loss: 0.00001264
Iteration 191/1000 | Loss: 0.00001263
Iteration 192/1000 | Loss: 0.00001263
Iteration 193/1000 | Loss: 0.00001263
Iteration 194/1000 | Loss: 0.00001263
Iteration 195/1000 | Loss: 0.00001263
Iteration 196/1000 | Loss: 0.00001263
Iteration 197/1000 | Loss: 0.00001263
Iteration 198/1000 | Loss: 0.00001263
Iteration 199/1000 | Loss: 0.00001263
Iteration 200/1000 | Loss: 0.00001263
Iteration 201/1000 | Loss: 0.00001263
Iteration 202/1000 | Loss: 0.00001263
Iteration 203/1000 | Loss: 0.00001263
Iteration 204/1000 | Loss: 0.00001263
Iteration 205/1000 | Loss: 0.00001263
Iteration 206/1000 | Loss: 0.00001263
Iteration 207/1000 | Loss: 0.00001263
Iteration 208/1000 | Loss: 0.00001262
Iteration 209/1000 | Loss: 0.00001262
Iteration 210/1000 | Loss: 0.00001262
Iteration 211/1000 | Loss: 0.00001262
Iteration 212/1000 | Loss: 0.00001262
Iteration 213/1000 | Loss: 0.00001262
Iteration 214/1000 | Loss: 0.00001262
Iteration 215/1000 | Loss: 0.00001261
Iteration 216/1000 | Loss: 0.00001261
Iteration 217/1000 | Loss: 0.00001261
Iteration 218/1000 | Loss: 0.00001261
Iteration 219/1000 | Loss: 0.00001261
Iteration 220/1000 | Loss: 0.00001261
Iteration 221/1000 | Loss: 0.00001260
Iteration 222/1000 | Loss: 0.00001260
Iteration 223/1000 | Loss: 0.00001260
Iteration 224/1000 | Loss: 0.00001260
Iteration 225/1000 | Loss: 0.00001260
Iteration 226/1000 | Loss: 0.00001260
Iteration 227/1000 | Loss: 0.00001260
Iteration 228/1000 | Loss: 0.00001260
Iteration 229/1000 | Loss: 0.00001260
Iteration 230/1000 | Loss: 0.00001260
Iteration 231/1000 | Loss: 0.00001260
Iteration 232/1000 | Loss: 0.00001260
Iteration 233/1000 | Loss: 0.00001260
Iteration 234/1000 | Loss: 0.00001260
Iteration 235/1000 | Loss: 0.00001259
Iteration 236/1000 | Loss: 0.00001259
Iteration 237/1000 | Loss: 0.00001259
Iteration 238/1000 | Loss: 0.00001259
Iteration 239/1000 | Loss: 0.00001259
Iteration 240/1000 | Loss: 0.00001259
Iteration 241/1000 | Loss: 0.00001259
Iteration 242/1000 | Loss: 0.00001259
Iteration 243/1000 | Loss: 0.00001259
Iteration 244/1000 | Loss: 0.00001259
Iteration 245/1000 | Loss: 0.00001259
Iteration 246/1000 | Loss: 0.00001259
Iteration 247/1000 | Loss: 0.00001259
Iteration 248/1000 | Loss: 0.00001259
Iteration 249/1000 | Loss: 0.00001259
Iteration 250/1000 | Loss: 0.00001259
Iteration 251/1000 | Loss: 0.00001259
Iteration 252/1000 | Loss: 0.00001259
Iteration 253/1000 | Loss: 0.00001259
Iteration 254/1000 | Loss: 0.00001259
Iteration 255/1000 | Loss: 0.00001259
Iteration 256/1000 | Loss: 0.00001259
Iteration 257/1000 | Loss: 0.00001259
Iteration 258/1000 | Loss: 0.00001259
Iteration 259/1000 | Loss: 0.00001259
Iteration 260/1000 | Loss: 0.00001259
Iteration 261/1000 | Loss: 0.00001259
Iteration 262/1000 | Loss: 0.00001259
Iteration 263/1000 | Loss: 0.00001259
Iteration 264/1000 | Loss: 0.00001259
Iteration 265/1000 | Loss: 0.00001259
Iteration 266/1000 | Loss: 0.00001259
Iteration 267/1000 | Loss: 0.00001259
Iteration 268/1000 | Loss: 0.00001259
Iteration 269/1000 | Loss: 0.00001259
Iteration 270/1000 | Loss: 0.00001259
Iteration 271/1000 | Loss: 0.00001259
Iteration 272/1000 | Loss: 0.00001259
Iteration 273/1000 | Loss: 0.00001259
Iteration 274/1000 | Loss: 0.00001259
Iteration 275/1000 | Loss: 0.00001259
Iteration 276/1000 | Loss: 0.00001259
Iteration 277/1000 | Loss: 0.00001259
Iteration 278/1000 | Loss: 0.00001259
Iteration 279/1000 | Loss: 0.00001259
Iteration 280/1000 | Loss: 0.00001259
Iteration 281/1000 | Loss: 0.00001259
Iteration 282/1000 | Loss: 0.00001259
Iteration 283/1000 | Loss: 0.00001259
Iteration 284/1000 | Loss: 0.00001259
Iteration 285/1000 | Loss: 0.00001259
Iteration 286/1000 | Loss: 0.00001259
Iteration 287/1000 | Loss: 0.00001259
Iteration 288/1000 | Loss: 0.00001259
Iteration 289/1000 | Loss: 0.00001259
Iteration 290/1000 | Loss: 0.00001259
Iteration 291/1000 | Loss: 0.00001259
Iteration 292/1000 | Loss: 0.00001259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.25931383081479e-05, 1.25931383081479e-05, 1.25931383081479e-05, 1.25931383081479e-05, 1.25931383081479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.25931383081479e-05

Optimization complete. Final v2v error: 2.8839573860168457 mm

Highest mean error: 4.849878787994385 mm for frame 83

Lowest mean error: 2.385497570037842 mm for frame 167

Saving results

Total time: 46.77431035041809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399191
Iteration 2/25 | Loss: 0.00131269
Iteration 3/25 | Loss: 0.00110133
Iteration 4/25 | Loss: 0.00108467
Iteration 5/25 | Loss: 0.00108227
Iteration 6/25 | Loss: 0.00108164
Iteration 7/25 | Loss: 0.00108164
Iteration 8/25 | Loss: 0.00108164
Iteration 9/25 | Loss: 0.00108164
Iteration 10/25 | Loss: 0.00108164
Iteration 11/25 | Loss: 0.00108164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010816371068358421, 0.0010816371068358421, 0.0010816371068358421, 0.0010816371068358421, 0.0010816371068358421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010816371068358421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34224832
Iteration 2/25 | Loss: 0.00062921
Iteration 3/25 | Loss: 0.00062920
Iteration 4/25 | Loss: 0.00062920
Iteration 5/25 | Loss: 0.00062920
Iteration 6/25 | Loss: 0.00062920
Iteration 7/25 | Loss: 0.00062920
Iteration 8/25 | Loss: 0.00062920
Iteration 9/25 | Loss: 0.00062920
Iteration 10/25 | Loss: 0.00062920
Iteration 11/25 | Loss: 0.00062920
Iteration 12/25 | Loss: 0.00062920
Iteration 13/25 | Loss: 0.00062920
Iteration 14/25 | Loss: 0.00062920
Iteration 15/25 | Loss: 0.00062920
Iteration 16/25 | Loss: 0.00062920
Iteration 17/25 | Loss: 0.00062920
Iteration 18/25 | Loss: 0.00062920
Iteration 19/25 | Loss: 0.00062920
Iteration 20/25 | Loss: 0.00062920
Iteration 21/25 | Loss: 0.00062920
Iteration 22/25 | Loss: 0.00062920
Iteration 23/25 | Loss: 0.00062920
Iteration 24/25 | Loss: 0.00062920
Iteration 25/25 | Loss: 0.00062920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062920
Iteration 2/1000 | Loss: 0.00002357
Iteration 3/1000 | Loss: 0.00001613
Iteration 4/1000 | Loss: 0.00001458
Iteration 5/1000 | Loss: 0.00001351
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001155
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001132
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001129
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001128
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001125
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001118
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001094
Iteration 87/1000 | Loss: 0.00001094
Iteration 88/1000 | Loss: 0.00001094
Iteration 89/1000 | Loss: 0.00001093
Iteration 90/1000 | Loss: 0.00001093
Iteration 91/1000 | Loss: 0.00001093
Iteration 92/1000 | Loss: 0.00001092
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001091
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001087
Iteration 108/1000 | Loss: 0.00001087
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001086
Iteration 112/1000 | Loss: 0.00001086
Iteration 113/1000 | Loss: 0.00001086
Iteration 114/1000 | Loss: 0.00001086
Iteration 115/1000 | Loss: 0.00001086
Iteration 116/1000 | Loss: 0.00001085
Iteration 117/1000 | Loss: 0.00001085
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001083
Iteration 120/1000 | Loss: 0.00001083
Iteration 121/1000 | Loss: 0.00001083
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001083
Iteration 124/1000 | Loss: 0.00001082
Iteration 125/1000 | Loss: 0.00001082
Iteration 126/1000 | Loss: 0.00001082
Iteration 127/1000 | Loss: 0.00001082
Iteration 128/1000 | Loss: 0.00001082
Iteration 129/1000 | Loss: 0.00001082
Iteration 130/1000 | Loss: 0.00001082
Iteration 131/1000 | Loss: 0.00001082
Iteration 132/1000 | Loss: 0.00001082
Iteration 133/1000 | Loss: 0.00001082
Iteration 134/1000 | Loss: 0.00001082
Iteration 135/1000 | Loss: 0.00001082
Iteration 136/1000 | Loss: 0.00001082
Iteration 137/1000 | Loss: 0.00001081
Iteration 138/1000 | Loss: 0.00001081
Iteration 139/1000 | Loss: 0.00001081
Iteration 140/1000 | Loss: 0.00001081
Iteration 141/1000 | Loss: 0.00001081
Iteration 142/1000 | Loss: 0.00001081
Iteration 143/1000 | Loss: 0.00001081
Iteration 144/1000 | Loss: 0.00001080
Iteration 145/1000 | Loss: 0.00001080
Iteration 146/1000 | Loss: 0.00001080
Iteration 147/1000 | Loss: 0.00001080
Iteration 148/1000 | Loss: 0.00001080
Iteration 149/1000 | Loss: 0.00001080
Iteration 150/1000 | Loss: 0.00001080
Iteration 151/1000 | Loss: 0.00001080
Iteration 152/1000 | Loss: 0.00001080
Iteration 153/1000 | Loss: 0.00001080
Iteration 154/1000 | Loss: 0.00001079
Iteration 155/1000 | Loss: 0.00001079
Iteration 156/1000 | Loss: 0.00001079
Iteration 157/1000 | Loss: 0.00001079
Iteration 158/1000 | Loss: 0.00001079
Iteration 159/1000 | Loss: 0.00001079
Iteration 160/1000 | Loss: 0.00001079
Iteration 161/1000 | Loss: 0.00001078
Iteration 162/1000 | Loss: 0.00001078
Iteration 163/1000 | Loss: 0.00001078
Iteration 164/1000 | Loss: 0.00001078
Iteration 165/1000 | Loss: 0.00001078
Iteration 166/1000 | Loss: 0.00001078
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001078
Iteration 169/1000 | Loss: 0.00001078
Iteration 170/1000 | Loss: 0.00001078
Iteration 171/1000 | Loss: 0.00001078
Iteration 172/1000 | Loss: 0.00001078
Iteration 173/1000 | Loss: 0.00001078
Iteration 174/1000 | Loss: 0.00001078
Iteration 175/1000 | Loss: 0.00001078
Iteration 176/1000 | Loss: 0.00001078
Iteration 177/1000 | Loss: 0.00001077
Iteration 178/1000 | Loss: 0.00001077
Iteration 179/1000 | Loss: 0.00001077
Iteration 180/1000 | Loss: 0.00001077
Iteration 181/1000 | Loss: 0.00001077
Iteration 182/1000 | Loss: 0.00001077
Iteration 183/1000 | Loss: 0.00001077
Iteration 184/1000 | Loss: 0.00001077
Iteration 185/1000 | Loss: 0.00001077
Iteration 186/1000 | Loss: 0.00001077
Iteration 187/1000 | Loss: 0.00001077
Iteration 188/1000 | Loss: 0.00001077
Iteration 189/1000 | Loss: 0.00001077
Iteration 190/1000 | Loss: 0.00001077
Iteration 191/1000 | Loss: 0.00001077
Iteration 192/1000 | Loss: 0.00001076
Iteration 193/1000 | Loss: 0.00001076
Iteration 194/1000 | Loss: 0.00001076
Iteration 195/1000 | Loss: 0.00001076
Iteration 196/1000 | Loss: 0.00001076
Iteration 197/1000 | Loss: 0.00001076
Iteration 198/1000 | Loss: 0.00001076
Iteration 199/1000 | Loss: 0.00001076
Iteration 200/1000 | Loss: 0.00001076
Iteration 201/1000 | Loss: 0.00001076
Iteration 202/1000 | Loss: 0.00001076
Iteration 203/1000 | Loss: 0.00001076
Iteration 204/1000 | Loss: 0.00001076
Iteration 205/1000 | Loss: 0.00001076
Iteration 206/1000 | Loss: 0.00001076
Iteration 207/1000 | Loss: 0.00001076
Iteration 208/1000 | Loss: 0.00001076
Iteration 209/1000 | Loss: 0.00001076
Iteration 210/1000 | Loss: 0.00001076
Iteration 211/1000 | Loss: 0.00001076
Iteration 212/1000 | Loss: 0.00001076
Iteration 213/1000 | Loss: 0.00001076
Iteration 214/1000 | Loss: 0.00001076
Iteration 215/1000 | Loss: 0.00001076
Iteration 216/1000 | Loss: 0.00001076
Iteration 217/1000 | Loss: 0.00001076
Iteration 218/1000 | Loss: 0.00001076
Iteration 219/1000 | Loss: 0.00001076
Iteration 220/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.0758269127109088e-05, 1.0758269127109088e-05, 1.0758269127109088e-05, 1.0758269127109088e-05, 1.0758269127109088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0758269127109088e-05

Optimization complete. Final v2v error: 2.823361873626709 mm

Highest mean error: 2.9182848930358887 mm for frame 24

Lowest mean error: 2.7281785011291504 mm for frame 130

Saving results

Total time: 40.363288164138794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436569
Iteration 2/25 | Loss: 0.00125143
Iteration 3/25 | Loss: 0.00114391
Iteration 4/25 | Loss: 0.00112986
Iteration 5/25 | Loss: 0.00112640
Iteration 6/25 | Loss: 0.00112640
Iteration 7/25 | Loss: 0.00112640
Iteration 8/25 | Loss: 0.00112640
Iteration 9/25 | Loss: 0.00112640
Iteration 10/25 | Loss: 0.00112640
Iteration 11/25 | Loss: 0.00112640
Iteration 12/25 | Loss: 0.00112640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001126402523368597, 0.001126402523368597, 0.001126402523368597, 0.001126402523368597, 0.001126402523368597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001126402523368597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35261786
Iteration 2/25 | Loss: 0.00066796
Iteration 3/25 | Loss: 0.00066796
Iteration 4/25 | Loss: 0.00066796
Iteration 5/25 | Loss: 0.00066796
Iteration 6/25 | Loss: 0.00066796
Iteration 7/25 | Loss: 0.00066796
Iteration 8/25 | Loss: 0.00066796
Iteration 9/25 | Loss: 0.00066796
Iteration 10/25 | Loss: 0.00066796
Iteration 11/25 | Loss: 0.00066796
Iteration 12/25 | Loss: 0.00066796
Iteration 13/25 | Loss: 0.00066796
Iteration 14/25 | Loss: 0.00066796
Iteration 15/25 | Loss: 0.00066796
Iteration 16/25 | Loss: 0.00066796
Iteration 17/25 | Loss: 0.00066796
Iteration 18/25 | Loss: 0.00066796
Iteration 19/25 | Loss: 0.00066796
Iteration 20/25 | Loss: 0.00066796
Iteration 21/25 | Loss: 0.00066796
Iteration 22/25 | Loss: 0.00066796
Iteration 23/25 | Loss: 0.00066796
Iteration 24/25 | Loss: 0.00066796
Iteration 25/25 | Loss: 0.00066796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066796
Iteration 2/1000 | Loss: 0.00003671
Iteration 3/1000 | Loss: 0.00002052
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001800
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001650
Iteration 9/1000 | Loss: 0.00001621
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001595
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001588
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001556
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001542
Iteration 26/1000 | Loss: 0.00001542
Iteration 27/1000 | Loss: 0.00001542
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001539
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001539
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [1.5389023246825673e-05, 1.5389023246825673e-05, 1.5389023246825673e-05, 1.5389023246825673e-05, 1.5389023246825673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5389023246825673e-05

Optimization complete. Final v2v error: 3.311718463897705 mm

Highest mean error: 4.373740196228027 mm for frame 230

Lowest mean error: 2.781815528869629 mm for frame 89

Saving results

Total time: 33.16995644569397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413319
Iteration 2/25 | Loss: 0.00123922
Iteration 3/25 | Loss: 0.00114149
Iteration 4/25 | Loss: 0.00112484
Iteration 5/25 | Loss: 0.00111976
Iteration 6/25 | Loss: 0.00111901
Iteration 7/25 | Loss: 0.00111901
Iteration 8/25 | Loss: 0.00111901
Iteration 9/25 | Loss: 0.00111901
Iteration 10/25 | Loss: 0.00111901
Iteration 11/25 | Loss: 0.00111901
Iteration 12/25 | Loss: 0.00111901
Iteration 13/25 | Loss: 0.00111901
Iteration 14/25 | Loss: 0.00111901
Iteration 15/25 | Loss: 0.00111901
Iteration 16/25 | Loss: 0.00111901
Iteration 17/25 | Loss: 0.00111901
Iteration 18/25 | Loss: 0.00111901
Iteration 19/25 | Loss: 0.00111901
Iteration 20/25 | Loss: 0.00111901
Iteration 21/25 | Loss: 0.00111901
Iteration 22/25 | Loss: 0.00111901
Iteration 23/25 | Loss: 0.00111901
Iteration 24/25 | Loss: 0.00111901
Iteration 25/25 | Loss: 0.00111901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38181937
Iteration 2/25 | Loss: 0.00083102
Iteration 3/25 | Loss: 0.00083102
Iteration 4/25 | Loss: 0.00083102
Iteration 5/25 | Loss: 0.00083102
Iteration 6/25 | Loss: 0.00083102
Iteration 7/25 | Loss: 0.00083102
Iteration 8/25 | Loss: 0.00083102
Iteration 9/25 | Loss: 0.00083102
Iteration 10/25 | Loss: 0.00083102
Iteration 11/25 | Loss: 0.00083102
Iteration 12/25 | Loss: 0.00083102
Iteration 13/25 | Loss: 0.00083102
Iteration 14/25 | Loss: 0.00083102
Iteration 15/25 | Loss: 0.00083102
Iteration 16/25 | Loss: 0.00083102
Iteration 17/25 | Loss: 0.00083102
Iteration 18/25 | Loss: 0.00083102
Iteration 19/25 | Loss: 0.00083102
Iteration 20/25 | Loss: 0.00083102
Iteration 21/25 | Loss: 0.00083102
Iteration 22/25 | Loss: 0.00083102
Iteration 23/25 | Loss: 0.00083102
Iteration 24/25 | Loss: 0.00083102
Iteration 25/25 | Loss: 0.00083102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083102
Iteration 2/1000 | Loss: 0.00003045
Iteration 3/1000 | Loss: 0.00002276
Iteration 4/1000 | Loss: 0.00001954
Iteration 5/1000 | Loss: 0.00001862
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001691
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001611
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001580
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001577
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001570
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001562
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001554
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001553
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001552
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001543
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001541
Iteration 83/1000 | Loss: 0.00001541
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001541
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001541
Iteration 91/1000 | Loss: 0.00001541
Iteration 92/1000 | Loss: 0.00001541
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001539
Iteration 96/1000 | Loss: 0.00001539
Iteration 97/1000 | Loss: 0.00001539
Iteration 98/1000 | Loss: 0.00001539
Iteration 99/1000 | Loss: 0.00001539
Iteration 100/1000 | Loss: 0.00001539
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001538
Iteration 103/1000 | Loss: 0.00001538
Iteration 104/1000 | Loss: 0.00001538
Iteration 105/1000 | Loss: 0.00001538
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001538
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001537
Iteration 111/1000 | Loss: 0.00001537
Iteration 112/1000 | Loss: 0.00001537
Iteration 113/1000 | Loss: 0.00001537
Iteration 114/1000 | Loss: 0.00001537
Iteration 115/1000 | Loss: 0.00001537
Iteration 116/1000 | Loss: 0.00001537
Iteration 117/1000 | Loss: 0.00001536
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001536
Iteration 121/1000 | Loss: 0.00001536
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001536
Iteration 129/1000 | Loss: 0.00001536
Iteration 130/1000 | Loss: 0.00001536
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001536
Iteration 133/1000 | Loss: 0.00001535
Iteration 134/1000 | Loss: 0.00001535
Iteration 135/1000 | Loss: 0.00001535
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001535
Iteration 140/1000 | Loss: 0.00001535
Iteration 141/1000 | Loss: 0.00001535
Iteration 142/1000 | Loss: 0.00001535
Iteration 143/1000 | Loss: 0.00001535
Iteration 144/1000 | Loss: 0.00001535
Iteration 145/1000 | Loss: 0.00001535
Iteration 146/1000 | Loss: 0.00001535
Iteration 147/1000 | Loss: 0.00001535
Iteration 148/1000 | Loss: 0.00001535
Iteration 149/1000 | Loss: 0.00001535
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001534
Iteration 153/1000 | Loss: 0.00001534
Iteration 154/1000 | Loss: 0.00001534
Iteration 155/1000 | Loss: 0.00001534
Iteration 156/1000 | Loss: 0.00001534
Iteration 157/1000 | Loss: 0.00001534
Iteration 158/1000 | Loss: 0.00001534
Iteration 159/1000 | Loss: 0.00001534
Iteration 160/1000 | Loss: 0.00001533
Iteration 161/1000 | Loss: 0.00001533
Iteration 162/1000 | Loss: 0.00001533
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001533
Iteration 165/1000 | Loss: 0.00001533
Iteration 166/1000 | Loss: 0.00001533
Iteration 167/1000 | Loss: 0.00001533
Iteration 168/1000 | Loss: 0.00001533
Iteration 169/1000 | Loss: 0.00001533
Iteration 170/1000 | Loss: 0.00001533
Iteration 171/1000 | Loss: 0.00001533
Iteration 172/1000 | Loss: 0.00001533
Iteration 173/1000 | Loss: 0.00001533
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001533
Iteration 178/1000 | Loss: 0.00001533
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.5330531823565252e-05, 1.5330531823565252e-05, 1.5330531823565252e-05, 1.5330531823565252e-05, 1.5330531823565252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5330531823565252e-05

Optimization complete. Final v2v error: 3.3229899406433105 mm

Highest mean error: 3.79317307472229 mm for frame 14

Lowest mean error: 3.088834524154663 mm for frame 33

Saving results

Total time: 39.513097047805786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977378
Iteration 2/25 | Loss: 0.00205927
Iteration 3/25 | Loss: 0.00152736
Iteration 4/25 | Loss: 0.00155291
Iteration 5/25 | Loss: 0.00132802
Iteration 6/25 | Loss: 0.00127146
Iteration 7/25 | Loss: 0.00124657
Iteration 8/25 | Loss: 0.00119851
Iteration 9/25 | Loss: 0.00118567
Iteration 10/25 | Loss: 0.00117790
Iteration 11/25 | Loss: 0.00117663
Iteration 12/25 | Loss: 0.00117510
Iteration 13/25 | Loss: 0.00117422
Iteration 14/25 | Loss: 0.00117253
Iteration 15/25 | Loss: 0.00117103
Iteration 16/25 | Loss: 0.00117242
Iteration 17/25 | Loss: 0.00117002
Iteration 18/25 | Loss: 0.00116930
Iteration 19/25 | Loss: 0.00116884
Iteration 20/25 | Loss: 0.00116834
Iteration 21/25 | Loss: 0.00116841
Iteration 22/25 | Loss: 0.00116822
Iteration 23/25 | Loss: 0.00116841
Iteration 24/25 | Loss: 0.00116815
Iteration 25/25 | Loss: 0.00116787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32655609
Iteration 2/25 | Loss: 0.00068023
Iteration 3/25 | Loss: 0.00068023
Iteration 4/25 | Loss: 0.00068023
Iteration 5/25 | Loss: 0.00068022
Iteration 6/25 | Loss: 0.00068022
Iteration 7/25 | Loss: 0.00068022
Iteration 8/25 | Loss: 0.00068022
Iteration 9/25 | Loss: 0.00068022
Iteration 10/25 | Loss: 0.00068022
Iteration 11/25 | Loss: 0.00068022
Iteration 12/25 | Loss: 0.00068022
Iteration 13/25 | Loss: 0.00068022
Iteration 14/25 | Loss: 0.00068022
Iteration 15/25 | Loss: 0.00068022
Iteration 16/25 | Loss: 0.00068022
Iteration 17/25 | Loss: 0.00068022
Iteration 18/25 | Loss: 0.00068022
Iteration 19/25 | Loss: 0.00068022
Iteration 20/25 | Loss: 0.00068022
Iteration 21/25 | Loss: 0.00068022
Iteration 22/25 | Loss: 0.00068022
Iteration 23/25 | Loss: 0.00068022
Iteration 24/25 | Loss: 0.00068022
Iteration 25/25 | Loss: 0.00068022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068022
Iteration 2/1000 | Loss: 0.00006446
Iteration 3/1000 | Loss: 0.00003655
Iteration 4/1000 | Loss: 0.00008022
Iteration 5/1000 | Loss: 0.00005337
Iteration 6/1000 | Loss: 0.00005419
Iteration 7/1000 | Loss: 0.00005266
Iteration 8/1000 | Loss: 0.00006694
Iteration 9/1000 | Loss: 0.00006724
Iteration 10/1000 | Loss: 0.00005388
Iteration 11/1000 | Loss: 0.00006495
Iteration 12/1000 | Loss: 0.00007401
Iteration 13/1000 | Loss: 0.00005501
Iteration 14/1000 | Loss: 0.00005177
Iteration 15/1000 | Loss: 0.00005509
Iteration 16/1000 | Loss: 0.00005668
Iteration 17/1000 | Loss: 0.00006180
Iteration 18/1000 | Loss: 0.00007412
Iteration 19/1000 | Loss: 0.00006166
Iteration 20/1000 | Loss: 0.00004831
Iteration 21/1000 | Loss: 0.00004728
Iteration 22/1000 | Loss: 0.00006842
Iteration 23/1000 | Loss: 0.00005858
Iteration 24/1000 | Loss: 0.00006489
Iteration 25/1000 | Loss: 0.00005964
Iteration 26/1000 | Loss: 0.00005048
Iteration 27/1000 | Loss: 0.00005381
Iteration 28/1000 | Loss: 0.00005989
Iteration 29/1000 | Loss: 0.00008050
Iteration 30/1000 | Loss: 0.00009213
Iteration 31/1000 | Loss: 0.00006605
Iteration 32/1000 | Loss: 0.00003463
Iteration 33/1000 | Loss: 0.00003843
Iteration 34/1000 | Loss: 0.00003161
Iteration 35/1000 | Loss: 0.00003101
Iteration 36/1000 | Loss: 0.00003582
Iteration 37/1000 | Loss: 0.00003825
Iteration 38/1000 | Loss: 0.00004531
Iteration 39/1000 | Loss: 0.00004414
Iteration 40/1000 | Loss: 0.00005803
Iteration 41/1000 | Loss: 0.00004973
Iteration 42/1000 | Loss: 0.00005617
Iteration 43/1000 | Loss: 0.00004903
Iteration 44/1000 | Loss: 0.00005760
Iteration 45/1000 | Loss: 0.00007274
Iteration 46/1000 | Loss: 0.00005400
Iteration 47/1000 | Loss: 0.00006018
Iteration 48/1000 | Loss: 0.00004979
Iteration 49/1000 | Loss: 0.00005625
Iteration 50/1000 | Loss: 0.00004977
Iteration 51/1000 | Loss: 0.00004518
Iteration 52/1000 | Loss: 0.00004382
Iteration 53/1000 | Loss: 0.00003833
Iteration 54/1000 | Loss: 0.00005265
Iteration 55/1000 | Loss: 0.00004232
Iteration 56/1000 | Loss: 0.00004457
Iteration 57/1000 | Loss: 0.00004516
Iteration 58/1000 | Loss: 0.00004662
Iteration 59/1000 | Loss: 0.00005821
Iteration 60/1000 | Loss: 0.00003505
Iteration 61/1000 | Loss: 0.00003541
Iteration 62/1000 | Loss: 0.00003118
Iteration 63/1000 | Loss: 0.00003333
Iteration 64/1000 | Loss: 0.00004971
Iteration 65/1000 | Loss: 0.00004520
Iteration 66/1000 | Loss: 0.00004746
Iteration 67/1000 | Loss: 0.00003885
Iteration 68/1000 | Loss: 0.00004542
Iteration 69/1000 | Loss: 0.00007497
Iteration 70/1000 | Loss: 0.00002774
Iteration 71/1000 | Loss: 0.00002373
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002081
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00001972
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001911
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001898
Iteration 82/1000 | Loss: 0.00001898
Iteration 83/1000 | Loss: 0.00001898
Iteration 84/1000 | Loss: 0.00001898
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001897
Iteration 93/1000 | Loss: 0.00001897
Iteration 94/1000 | Loss: 0.00001897
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001897
Iteration 98/1000 | Loss: 0.00001897
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001897
Iteration 104/1000 | Loss: 0.00001897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.897301262943074e-05, 1.897301262943074e-05, 1.897301262943074e-05, 1.897301262943074e-05, 1.897301262943074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.897301262943074e-05

Optimization complete. Final v2v error: 3.666470527648926 mm

Highest mean error: 5.3189568519592285 mm for frame 228

Lowest mean error: 3.4937384128570557 mm for frame 223

Saving results

Total time: 177.8611695766449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380018
Iteration 2/25 | Loss: 0.00114765
Iteration 3/25 | Loss: 0.00106422
Iteration 4/25 | Loss: 0.00105080
Iteration 5/25 | Loss: 0.00104600
Iteration 6/25 | Loss: 0.00104529
Iteration 7/25 | Loss: 0.00104529
Iteration 8/25 | Loss: 0.00104529
Iteration 9/25 | Loss: 0.00104529
Iteration 10/25 | Loss: 0.00104529
Iteration 11/25 | Loss: 0.00104529
Iteration 12/25 | Loss: 0.00104529
Iteration 13/25 | Loss: 0.00104526
Iteration 14/25 | Loss: 0.00104526
Iteration 15/25 | Loss: 0.00104526
Iteration 16/25 | Loss: 0.00104526
Iteration 17/25 | Loss: 0.00104526
Iteration 18/25 | Loss: 0.00104526
Iteration 19/25 | Loss: 0.00104526
Iteration 20/25 | Loss: 0.00104526
Iteration 21/25 | Loss: 0.00104526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001045259996317327, 0.001045259996317327, 0.001045259996317327, 0.001045259996317327, 0.001045259996317327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001045259996317327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43861759
Iteration 2/25 | Loss: 0.00089388
Iteration 3/25 | Loss: 0.00089388
Iteration 4/25 | Loss: 0.00089388
Iteration 5/25 | Loss: 0.00089388
Iteration 6/25 | Loss: 0.00089388
Iteration 7/25 | Loss: 0.00089388
Iteration 8/25 | Loss: 0.00089388
Iteration 9/25 | Loss: 0.00089388
Iteration 10/25 | Loss: 0.00089388
Iteration 11/25 | Loss: 0.00089388
Iteration 12/25 | Loss: 0.00089388
Iteration 13/25 | Loss: 0.00089388
Iteration 14/25 | Loss: 0.00089388
Iteration 15/25 | Loss: 0.00089388
Iteration 16/25 | Loss: 0.00089388
Iteration 17/25 | Loss: 0.00089388
Iteration 18/25 | Loss: 0.00089388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008938808459788561, 0.0008938808459788561, 0.0008938808459788561, 0.0008938808459788561, 0.0008938808459788561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008938808459788561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089388
Iteration 2/1000 | Loss: 0.00001544
Iteration 3/1000 | Loss: 0.00001026
Iteration 4/1000 | Loss: 0.00000915
Iteration 5/1000 | Loss: 0.00000872
Iteration 6/1000 | Loss: 0.00000834
Iteration 7/1000 | Loss: 0.00000818
Iteration 8/1000 | Loss: 0.00000798
Iteration 9/1000 | Loss: 0.00000775
Iteration 10/1000 | Loss: 0.00000773
Iteration 11/1000 | Loss: 0.00000770
Iteration 12/1000 | Loss: 0.00000770
Iteration 13/1000 | Loss: 0.00000760
Iteration 14/1000 | Loss: 0.00000755
Iteration 15/1000 | Loss: 0.00000755
Iteration 16/1000 | Loss: 0.00000755
Iteration 17/1000 | Loss: 0.00000755
Iteration 18/1000 | Loss: 0.00000755
Iteration 19/1000 | Loss: 0.00000755
Iteration 20/1000 | Loss: 0.00000755
Iteration 21/1000 | Loss: 0.00000755
Iteration 22/1000 | Loss: 0.00000754
Iteration 23/1000 | Loss: 0.00000754
Iteration 24/1000 | Loss: 0.00000754
Iteration 25/1000 | Loss: 0.00000754
Iteration 26/1000 | Loss: 0.00000751
Iteration 27/1000 | Loss: 0.00000751
Iteration 28/1000 | Loss: 0.00000751
Iteration 29/1000 | Loss: 0.00000749
Iteration 30/1000 | Loss: 0.00000748
Iteration 31/1000 | Loss: 0.00000746
Iteration 32/1000 | Loss: 0.00000744
Iteration 33/1000 | Loss: 0.00000743
Iteration 34/1000 | Loss: 0.00000743
Iteration 35/1000 | Loss: 0.00000742
Iteration 36/1000 | Loss: 0.00000742
Iteration 37/1000 | Loss: 0.00000741
Iteration 38/1000 | Loss: 0.00000740
Iteration 39/1000 | Loss: 0.00000740
Iteration 40/1000 | Loss: 0.00000737
Iteration 41/1000 | Loss: 0.00000737
Iteration 42/1000 | Loss: 0.00000736
Iteration 43/1000 | Loss: 0.00000736
Iteration 44/1000 | Loss: 0.00000736
Iteration 45/1000 | Loss: 0.00000735
Iteration 46/1000 | Loss: 0.00000735
Iteration 47/1000 | Loss: 0.00000734
Iteration 48/1000 | Loss: 0.00000733
Iteration 49/1000 | Loss: 0.00000733
Iteration 50/1000 | Loss: 0.00000732
Iteration 51/1000 | Loss: 0.00000732
Iteration 52/1000 | Loss: 0.00000732
Iteration 53/1000 | Loss: 0.00000731
Iteration 54/1000 | Loss: 0.00000730
Iteration 55/1000 | Loss: 0.00000729
Iteration 56/1000 | Loss: 0.00000728
Iteration 57/1000 | Loss: 0.00000727
Iteration 58/1000 | Loss: 0.00000727
Iteration 59/1000 | Loss: 0.00000727
Iteration 60/1000 | Loss: 0.00000726
Iteration 61/1000 | Loss: 0.00000725
Iteration 62/1000 | Loss: 0.00000725
Iteration 63/1000 | Loss: 0.00000725
Iteration 64/1000 | Loss: 0.00000725
Iteration 65/1000 | Loss: 0.00000725
Iteration 66/1000 | Loss: 0.00000724
Iteration 67/1000 | Loss: 0.00000724
Iteration 68/1000 | Loss: 0.00000724
Iteration 69/1000 | Loss: 0.00000724
Iteration 70/1000 | Loss: 0.00000724
Iteration 71/1000 | Loss: 0.00000724
Iteration 72/1000 | Loss: 0.00000724
Iteration 73/1000 | Loss: 0.00000723
Iteration 74/1000 | Loss: 0.00000723
Iteration 75/1000 | Loss: 0.00000722
Iteration 76/1000 | Loss: 0.00000722
Iteration 77/1000 | Loss: 0.00000722
Iteration 78/1000 | Loss: 0.00000721
Iteration 79/1000 | Loss: 0.00000721
Iteration 80/1000 | Loss: 0.00000721
Iteration 81/1000 | Loss: 0.00000721
Iteration 82/1000 | Loss: 0.00000721
Iteration 83/1000 | Loss: 0.00000721
Iteration 84/1000 | Loss: 0.00000721
Iteration 85/1000 | Loss: 0.00000720
Iteration 86/1000 | Loss: 0.00000720
Iteration 87/1000 | Loss: 0.00000720
Iteration 88/1000 | Loss: 0.00000720
Iteration 89/1000 | Loss: 0.00000719
Iteration 90/1000 | Loss: 0.00000719
Iteration 91/1000 | Loss: 0.00000719
Iteration 92/1000 | Loss: 0.00000719
Iteration 93/1000 | Loss: 0.00000718
Iteration 94/1000 | Loss: 0.00000718
Iteration 95/1000 | Loss: 0.00000717
Iteration 96/1000 | Loss: 0.00000717
Iteration 97/1000 | Loss: 0.00000717
Iteration 98/1000 | Loss: 0.00000716
Iteration 99/1000 | Loss: 0.00000716
Iteration 100/1000 | Loss: 0.00000716
Iteration 101/1000 | Loss: 0.00000715
Iteration 102/1000 | Loss: 0.00000715
Iteration 103/1000 | Loss: 0.00000714
Iteration 104/1000 | Loss: 0.00000713
Iteration 105/1000 | Loss: 0.00000713
Iteration 106/1000 | Loss: 0.00000713
Iteration 107/1000 | Loss: 0.00000712
Iteration 108/1000 | Loss: 0.00000712
Iteration 109/1000 | Loss: 0.00000711
Iteration 110/1000 | Loss: 0.00000711
Iteration 111/1000 | Loss: 0.00000711
Iteration 112/1000 | Loss: 0.00000711
Iteration 113/1000 | Loss: 0.00000710
Iteration 114/1000 | Loss: 0.00000710
Iteration 115/1000 | Loss: 0.00000710
Iteration 116/1000 | Loss: 0.00000710
Iteration 117/1000 | Loss: 0.00000710
Iteration 118/1000 | Loss: 0.00000710
Iteration 119/1000 | Loss: 0.00000710
Iteration 120/1000 | Loss: 0.00000710
Iteration 121/1000 | Loss: 0.00000710
Iteration 122/1000 | Loss: 0.00000709
Iteration 123/1000 | Loss: 0.00000709
Iteration 124/1000 | Loss: 0.00000709
Iteration 125/1000 | Loss: 0.00000709
Iteration 126/1000 | Loss: 0.00000709
Iteration 127/1000 | Loss: 0.00000709
Iteration 128/1000 | Loss: 0.00000709
Iteration 129/1000 | Loss: 0.00000709
Iteration 130/1000 | Loss: 0.00000709
Iteration 131/1000 | Loss: 0.00000709
Iteration 132/1000 | Loss: 0.00000709
Iteration 133/1000 | Loss: 0.00000709
Iteration 134/1000 | Loss: 0.00000709
Iteration 135/1000 | Loss: 0.00000708
Iteration 136/1000 | Loss: 0.00000708
Iteration 137/1000 | Loss: 0.00000708
Iteration 138/1000 | Loss: 0.00000708
Iteration 139/1000 | Loss: 0.00000708
Iteration 140/1000 | Loss: 0.00000708
Iteration 141/1000 | Loss: 0.00000707
Iteration 142/1000 | Loss: 0.00000707
Iteration 143/1000 | Loss: 0.00000707
Iteration 144/1000 | Loss: 0.00000707
Iteration 145/1000 | Loss: 0.00000707
Iteration 146/1000 | Loss: 0.00000707
Iteration 147/1000 | Loss: 0.00000707
Iteration 148/1000 | Loss: 0.00000707
Iteration 149/1000 | Loss: 0.00000707
Iteration 150/1000 | Loss: 0.00000707
Iteration 151/1000 | Loss: 0.00000706
Iteration 152/1000 | Loss: 0.00000706
Iteration 153/1000 | Loss: 0.00000705
Iteration 154/1000 | Loss: 0.00000705
Iteration 155/1000 | Loss: 0.00000705
Iteration 156/1000 | Loss: 0.00000705
Iteration 157/1000 | Loss: 0.00000705
Iteration 158/1000 | Loss: 0.00000705
Iteration 159/1000 | Loss: 0.00000705
Iteration 160/1000 | Loss: 0.00000705
Iteration 161/1000 | Loss: 0.00000705
Iteration 162/1000 | Loss: 0.00000704
Iteration 163/1000 | Loss: 0.00000704
Iteration 164/1000 | Loss: 0.00000704
Iteration 165/1000 | Loss: 0.00000704
Iteration 166/1000 | Loss: 0.00000704
Iteration 167/1000 | Loss: 0.00000704
Iteration 168/1000 | Loss: 0.00000704
Iteration 169/1000 | Loss: 0.00000704
Iteration 170/1000 | Loss: 0.00000703
Iteration 171/1000 | Loss: 0.00000703
Iteration 172/1000 | Loss: 0.00000703
Iteration 173/1000 | Loss: 0.00000703
Iteration 174/1000 | Loss: 0.00000703
Iteration 175/1000 | Loss: 0.00000703
Iteration 176/1000 | Loss: 0.00000702
Iteration 177/1000 | Loss: 0.00000702
Iteration 178/1000 | Loss: 0.00000702
Iteration 179/1000 | Loss: 0.00000702
Iteration 180/1000 | Loss: 0.00000702
Iteration 181/1000 | Loss: 0.00000701
Iteration 182/1000 | Loss: 0.00000701
Iteration 183/1000 | Loss: 0.00000701
Iteration 184/1000 | Loss: 0.00000701
Iteration 185/1000 | Loss: 0.00000701
Iteration 186/1000 | Loss: 0.00000701
Iteration 187/1000 | Loss: 0.00000701
Iteration 188/1000 | Loss: 0.00000701
Iteration 189/1000 | Loss: 0.00000701
Iteration 190/1000 | Loss: 0.00000701
Iteration 191/1000 | Loss: 0.00000701
Iteration 192/1000 | Loss: 0.00000701
Iteration 193/1000 | Loss: 0.00000701
Iteration 194/1000 | Loss: 0.00000701
Iteration 195/1000 | Loss: 0.00000701
Iteration 196/1000 | Loss: 0.00000701
Iteration 197/1000 | Loss: 0.00000701
Iteration 198/1000 | Loss: 0.00000701
Iteration 199/1000 | Loss: 0.00000701
Iteration 200/1000 | Loss: 0.00000701
Iteration 201/1000 | Loss: 0.00000701
Iteration 202/1000 | Loss: 0.00000700
Iteration 203/1000 | Loss: 0.00000700
Iteration 204/1000 | Loss: 0.00000700
Iteration 205/1000 | Loss: 0.00000700
Iteration 206/1000 | Loss: 0.00000700
Iteration 207/1000 | Loss: 0.00000700
Iteration 208/1000 | Loss: 0.00000700
Iteration 209/1000 | Loss: 0.00000700
Iteration 210/1000 | Loss: 0.00000700
Iteration 211/1000 | Loss: 0.00000700
Iteration 212/1000 | Loss: 0.00000699
Iteration 213/1000 | Loss: 0.00000699
Iteration 214/1000 | Loss: 0.00000699
Iteration 215/1000 | Loss: 0.00000699
Iteration 216/1000 | Loss: 0.00000698
Iteration 217/1000 | Loss: 0.00000698
Iteration 218/1000 | Loss: 0.00000698
Iteration 219/1000 | Loss: 0.00000698
Iteration 220/1000 | Loss: 0.00000698
Iteration 221/1000 | Loss: 0.00000698
Iteration 222/1000 | Loss: 0.00000698
Iteration 223/1000 | Loss: 0.00000698
Iteration 224/1000 | Loss: 0.00000698
Iteration 225/1000 | Loss: 0.00000697
Iteration 226/1000 | Loss: 0.00000697
Iteration 227/1000 | Loss: 0.00000697
Iteration 228/1000 | Loss: 0.00000697
Iteration 229/1000 | Loss: 0.00000697
Iteration 230/1000 | Loss: 0.00000697
Iteration 231/1000 | Loss: 0.00000697
Iteration 232/1000 | Loss: 0.00000697
Iteration 233/1000 | Loss: 0.00000697
Iteration 234/1000 | Loss: 0.00000697
Iteration 235/1000 | Loss: 0.00000697
Iteration 236/1000 | Loss: 0.00000697
Iteration 237/1000 | Loss: 0.00000697
Iteration 238/1000 | Loss: 0.00000697
Iteration 239/1000 | Loss: 0.00000697
Iteration 240/1000 | Loss: 0.00000697
Iteration 241/1000 | Loss: 0.00000697
Iteration 242/1000 | Loss: 0.00000697
Iteration 243/1000 | Loss: 0.00000697
Iteration 244/1000 | Loss: 0.00000697
Iteration 245/1000 | Loss: 0.00000696
Iteration 246/1000 | Loss: 0.00000696
Iteration 247/1000 | Loss: 0.00000696
Iteration 248/1000 | Loss: 0.00000696
Iteration 249/1000 | Loss: 0.00000696
Iteration 250/1000 | Loss: 0.00000696
Iteration 251/1000 | Loss: 0.00000696
Iteration 252/1000 | Loss: 0.00000696
Iteration 253/1000 | Loss: 0.00000696
Iteration 254/1000 | Loss: 0.00000696
Iteration 255/1000 | Loss: 0.00000696
Iteration 256/1000 | Loss: 0.00000696
Iteration 257/1000 | Loss: 0.00000696
Iteration 258/1000 | Loss: 0.00000696
Iteration 259/1000 | Loss: 0.00000696
Iteration 260/1000 | Loss: 0.00000696
Iteration 261/1000 | Loss: 0.00000696
Iteration 262/1000 | Loss: 0.00000696
Iteration 263/1000 | Loss: 0.00000696
Iteration 264/1000 | Loss: 0.00000696
Iteration 265/1000 | Loss: 0.00000696
Iteration 266/1000 | Loss: 0.00000696
Iteration 267/1000 | Loss: 0.00000696
Iteration 268/1000 | Loss: 0.00000696
Iteration 269/1000 | Loss: 0.00000696
Iteration 270/1000 | Loss: 0.00000696
Iteration 271/1000 | Loss: 0.00000696
Iteration 272/1000 | Loss: 0.00000696
Iteration 273/1000 | Loss: 0.00000696
Iteration 274/1000 | Loss: 0.00000696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [6.964339718251722e-06, 6.964339718251722e-06, 6.964339718251722e-06, 6.964339718251722e-06, 6.964339718251722e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.964339718251722e-06

Optimization complete. Final v2v error: 2.3007266521453857 mm

Highest mean error: 2.370680332183838 mm for frame 135

Lowest mean error: 2.2590315341949463 mm for frame 52

Saving results

Total time: 42.845980644226074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035584
Iteration 2/25 | Loss: 0.00331362
Iteration 3/25 | Loss: 0.00282392
Iteration 4/25 | Loss: 0.00270692
Iteration 5/25 | Loss: 0.00263355
Iteration 6/25 | Loss: 0.00255277
Iteration 7/25 | Loss: 0.00247386
Iteration 8/25 | Loss: 0.00241237
Iteration 9/25 | Loss: 0.00236104
Iteration 10/25 | Loss: 0.00234317
Iteration 11/25 | Loss: 0.00234244
Iteration 12/25 | Loss: 0.00232056
Iteration 13/25 | Loss: 0.00230563
Iteration 14/25 | Loss: 0.00228205
Iteration 15/25 | Loss: 0.00227310
Iteration 16/25 | Loss: 0.00226273
Iteration 17/25 | Loss: 0.00224916
Iteration 18/25 | Loss: 0.00222756
Iteration 19/25 | Loss: 0.00220566
Iteration 20/25 | Loss: 0.00220289
Iteration 21/25 | Loss: 0.00219368
Iteration 22/25 | Loss: 0.00219595
Iteration 23/25 | Loss: 0.00219133
Iteration 24/25 | Loss: 0.00218967
Iteration 25/25 | Loss: 0.00218555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11276352
Iteration 2/25 | Loss: 0.00451768
Iteration 3/25 | Loss: 0.00451759
Iteration 4/25 | Loss: 0.00451759
Iteration 5/25 | Loss: 0.00451759
Iteration 6/25 | Loss: 0.00451759
Iteration 7/25 | Loss: 0.00451759
Iteration 8/25 | Loss: 0.00451759
Iteration 9/25 | Loss: 0.00451759
Iteration 10/25 | Loss: 0.00451759
Iteration 11/25 | Loss: 0.00451758
Iteration 12/25 | Loss: 0.00451758
Iteration 13/25 | Loss: 0.00451758
Iteration 14/25 | Loss: 0.00451758
Iteration 15/25 | Loss: 0.00451758
Iteration 16/25 | Loss: 0.00451758
Iteration 17/25 | Loss: 0.00451758
Iteration 18/25 | Loss: 0.00451758
Iteration 19/25 | Loss: 0.00451758
Iteration 20/25 | Loss: 0.00451758
Iteration 21/25 | Loss: 0.00451758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004517584107816219, 0.004517584107816219, 0.004517584107816219, 0.004517584107816219, 0.004517584107816219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004517584107816219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00451758
Iteration 2/1000 | Loss: 0.00396699
Iteration 3/1000 | Loss: 0.00269219
Iteration 4/1000 | Loss: 0.00520684
Iteration 5/1000 | Loss: 0.00782967
Iteration 6/1000 | Loss: 0.00354073
Iteration 7/1000 | Loss: 0.00447620
Iteration 8/1000 | Loss: 0.00251232
Iteration 9/1000 | Loss: 0.00220190
Iteration 10/1000 | Loss: 0.00253863
Iteration 11/1000 | Loss: 0.00222633
Iteration 12/1000 | Loss: 0.00089957
Iteration 13/1000 | Loss: 0.00290528
Iteration 14/1000 | Loss: 0.00456664
Iteration 15/1000 | Loss: 0.00439280
Iteration 16/1000 | Loss: 0.00343513
Iteration 17/1000 | Loss: 0.00199841
Iteration 18/1000 | Loss: 0.00077596
Iteration 19/1000 | Loss: 0.00101234
Iteration 20/1000 | Loss: 0.00090989
Iteration 21/1000 | Loss: 0.00087713
Iteration 22/1000 | Loss: 0.00097129
Iteration 23/1000 | Loss: 0.00128279
Iteration 24/1000 | Loss: 0.00116391
Iteration 25/1000 | Loss: 0.00071364
Iteration 26/1000 | Loss: 0.00050930
Iteration 27/1000 | Loss: 0.00080699
Iteration 28/1000 | Loss: 0.00125490
Iteration 29/1000 | Loss: 0.00222191
Iteration 30/1000 | Loss: 0.01001094
Iteration 31/1000 | Loss: 0.02498395
Iteration 32/1000 | Loss: 0.01823581
Iteration 33/1000 | Loss: 0.01433142
Iteration 34/1000 | Loss: 0.01108882
Iteration 35/1000 | Loss: 0.00482029
Iteration 36/1000 | Loss: 0.00576314
Iteration 37/1000 | Loss: 0.00760814
Iteration 38/1000 | Loss: 0.00621424
Iteration 39/1000 | Loss: 0.00398013
Iteration 40/1000 | Loss: 0.00418373
Iteration 41/1000 | Loss: 0.00481208
Iteration 42/1000 | Loss: 0.00366012
Iteration 43/1000 | Loss: 0.00403573
Iteration 44/1000 | Loss: 0.00311204
Iteration 45/1000 | Loss: 0.00290420
Iteration 46/1000 | Loss: 0.00282118
Iteration 47/1000 | Loss: 0.00292703
Iteration 48/1000 | Loss: 0.00301796
Iteration 49/1000 | Loss: 0.00089399
Iteration 50/1000 | Loss: 0.00141201
Iteration 51/1000 | Loss: 0.00159414
Iteration 52/1000 | Loss: 0.00097042
Iteration 53/1000 | Loss: 0.00113458
Iteration 54/1000 | Loss: 0.00057808
Iteration 55/1000 | Loss: 0.00077334
Iteration 56/1000 | Loss: 0.00051147
Iteration 57/1000 | Loss: 0.00041730
Iteration 58/1000 | Loss: 0.00091615
Iteration 59/1000 | Loss: 0.00082727
Iteration 60/1000 | Loss: 0.00071226
Iteration 61/1000 | Loss: 0.00077759
Iteration 62/1000 | Loss: 0.00060741
Iteration 63/1000 | Loss: 0.00101167
Iteration 64/1000 | Loss: 0.00081260
Iteration 65/1000 | Loss: 0.00058745
Iteration 66/1000 | Loss: 0.00040981
Iteration 67/1000 | Loss: 0.00032562
Iteration 68/1000 | Loss: 0.00042220
Iteration 69/1000 | Loss: 0.00039676
Iteration 70/1000 | Loss: 0.00114154
Iteration 71/1000 | Loss: 0.00046013
Iteration 72/1000 | Loss: 0.00049197
Iteration 73/1000 | Loss: 0.00095633
Iteration 74/1000 | Loss: 0.00047810
Iteration 75/1000 | Loss: 0.00046192
Iteration 76/1000 | Loss: 0.00039976
Iteration 77/1000 | Loss: 0.00027855
Iteration 78/1000 | Loss: 0.00046756
Iteration 79/1000 | Loss: 0.00068649
Iteration 80/1000 | Loss: 0.00053869
Iteration 81/1000 | Loss: 0.00134425
Iteration 82/1000 | Loss: 0.00064339
Iteration 83/1000 | Loss: 0.00058722
Iteration 84/1000 | Loss: 0.00035771
Iteration 85/1000 | Loss: 0.00041692
Iteration 86/1000 | Loss: 0.00069065
Iteration 87/1000 | Loss: 0.00166500
Iteration 88/1000 | Loss: 0.00125091
Iteration 89/1000 | Loss: 0.00150324
Iteration 90/1000 | Loss: 0.00083354
Iteration 91/1000 | Loss: 0.00053260
Iteration 92/1000 | Loss: 0.00046063
Iteration 93/1000 | Loss: 0.00083015
Iteration 94/1000 | Loss: 0.00084641
Iteration 95/1000 | Loss: 0.00053017
Iteration 96/1000 | Loss: 0.00044498
Iteration 97/1000 | Loss: 0.00030703
Iteration 98/1000 | Loss: 0.00045544
Iteration 99/1000 | Loss: 0.00049849
Iteration 100/1000 | Loss: 0.00044966
Iteration 101/1000 | Loss: 0.00040962
Iteration 102/1000 | Loss: 0.00035788
Iteration 103/1000 | Loss: 0.00034022
Iteration 104/1000 | Loss: 0.00039844
Iteration 105/1000 | Loss: 0.00047874
Iteration 106/1000 | Loss: 0.00029543
Iteration 107/1000 | Loss: 0.00031581
Iteration 108/1000 | Loss: 0.00061601
Iteration 109/1000 | Loss: 0.00036549
Iteration 110/1000 | Loss: 0.00034222
Iteration 111/1000 | Loss: 0.00034552
Iteration 112/1000 | Loss: 0.00033368
Iteration 113/1000 | Loss: 0.00059819
Iteration 114/1000 | Loss: 0.00043281
Iteration 115/1000 | Loss: 0.00044415
Iteration 116/1000 | Loss: 0.00054658
Iteration 117/1000 | Loss: 0.00053438
Iteration 118/1000 | Loss: 0.00036208
Iteration 119/1000 | Loss: 0.00034705
Iteration 120/1000 | Loss: 0.00036315
Iteration 121/1000 | Loss: 0.00044353
Iteration 122/1000 | Loss: 0.00054732
Iteration 123/1000 | Loss: 0.00054408
Iteration 124/1000 | Loss: 0.00029497
Iteration 125/1000 | Loss: 0.00027442
Iteration 126/1000 | Loss: 0.00030015
Iteration 127/1000 | Loss: 0.00050920
Iteration 128/1000 | Loss: 0.00076984
Iteration 129/1000 | Loss: 0.00053110
Iteration 130/1000 | Loss: 0.00073860
Iteration 131/1000 | Loss: 0.00142657
Iteration 132/1000 | Loss: 0.00085317
Iteration 133/1000 | Loss: 0.00047245
Iteration 134/1000 | Loss: 0.00049942
Iteration 135/1000 | Loss: 0.00032027
Iteration 136/1000 | Loss: 0.00032041
Iteration 137/1000 | Loss: 0.00051526
Iteration 138/1000 | Loss: 0.00042154
Iteration 139/1000 | Loss: 0.00043120
Iteration 140/1000 | Loss: 0.00035204
Iteration 141/1000 | Loss: 0.00028491
Iteration 142/1000 | Loss: 0.00036404
Iteration 143/1000 | Loss: 0.00040428
Iteration 144/1000 | Loss: 0.00039668
Iteration 145/1000 | Loss: 0.00039459
Iteration 146/1000 | Loss: 0.00040591
Iteration 147/1000 | Loss: 0.00039835
Iteration 148/1000 | Loss: 0.00099068
Iteration 149/1000 | Loss: 0.00213148
Iteration 150/1000 | Loss: 0.00163277
Iteration 151/1000 | Loss: 0.00072972
Iteration 152/1000 | Loss: 0.00055898
Iteration 153/1000 | Loss: 0.00044601
Iteration 154/1000 | Loss: 0.00037972
Iteration 155/1000 | Loss: 0.00081777
Iteration 156/1000 | Loss: 0.00043571
Iteration 157/1000 | Loss: 0.00084172
Iteration 158/1000 | Loss: 0.00030415
Iteration 159/1000 | Loss: 0.00026547
Iteration 160/1000 | Loss: 0.00023247
Iteration 161/1000 | Loss: 0.00088391
Iteration 162/1000 | Loss: 0.00067354
Iteration 163/1000 | Loss: 0.00065285
Iteration 164/1000 | Loss: 0.00072047
Iteration 165/1000 | Loss: 0.00032464
Iteration 166/1000 | Loss: 0.00042058
Iteration 167/1000 | Loss: 0.00063981
Iteration 168/1000 | Loss: 0.00076400
Iteration 169/1000 | Loss: 0.00023752
Iteration 170/1000 | Loss: 0.00022122
Iteration 171/1000 | Loss: 0.00020977
Iteration 172/1000 | Loss: 0.00084803
Iteration 173/1000 | Loss: 0.00090757
Iteration 174/1000 | Loss: 0.00036447
Iteration 175/1000 | Loss: 0.00036861
Iteration 176/1000 | Loss: 0.00033423
Iteration 177/1000 | Loss: 0.00036883
Iteration 178/1000 | Loss: 0.00021240
Iteration 179/1000 | Loss: 0.00033276
Iteration 180/1000 | Loss: 0.00084677
Iteration 181/1000 | Loss: 0.00048626
Iteration 182/1000 | Loss: 0.00056683
Iteration 183/1000 | Loss: 0.00085298
Iteration 184/1000 | Loss: 0.00031120
Iteration 185/1000 | Loss: 0.00046212
Iteration 186/1000 | Loss: 0.00030035
Iteration 187/1000 | Loss: 0.00033924
Iteration 188/1000 | Loss: 0.00046910
Iteration 189/1000 | Loss: 0.00036822
Iteration 190/1000 | Loss: 0.00072891
Iteration 191/1000 | Loss: 0.00031303
Iteration 192/1000 | Loss: 0.00044956
Iteration 193/1000 | Loss: 0.00017486
Iteration 194/1000 | Loss: 0.00063450
Iteration 195/1000 | Loss: 0.00017229
Iteration 196/1000 | Loss: 0.00016598
Iteration 197/1000 | Loss: 0.00041137
Iteration 198/1000 | Loss: 0.00034383
Iteration 199/1000 | Loss: 0.00031841
Iteration 200/1000 | Loss: 0.00064378
Iteration 201/1000 | Loss: 0.00030531
Iteration 202/1000 | Loss: 0.00018563
Iteration 203/1000 | Loss: 0.00020781
Iteration 204/1000 | Loss: 0.00019580
Iteration 205/1000 | Loss: 0.00076021
Iteration 206/1000 | Loss: 0.00039429
Iteration 207/1000 | Loss: 0.00041264
Iteration 208/1000 | Loss: 0.00014156
Iteration 209/1000 | Loss: 0.00013589
Iteration 210/1000 | Loss: 0.00022135
Iteration 211/1000 | Loss: 0.00048716
Iteration 212/1000 | Loss: 0.00014561
Iteration 213/1000 | Loss: 0.00026058
Iteration 214/1000 | Loss: 0.00023038
Iteration 215/1000 | Loss: 0.00024526
Iteration 216/1000 | Loss: 0.00033687
Iteration 217/1000 | Loss: 0.00044639
Iteration 218/1000 | Loss: 0.00016684
Iteration 219/1000 | Loss: 0.00011676
Iteration 220/1000 | Loss: 0.00011255
Iteration 221/1000 | Loss: 0.00019080
Iteration 222/1000 | Loss: 0.00032434
Iteration 223/1000 | Loss: 0.00022014
Iteration 224/1000 | Loss: 0.00023509
Iteration 225/1000 | Loss: 0.00019082
Iteration 226/1000 | Loss: 0.00022605
Iteration 227/1000 | Loss: 0.00039652
Iteration 228/1000 | Loss: 0.00010379
Iteration 229/1000 | Loss: 0.00037026
Iteration 230/1000 | Loss: 0.00009993
Iteration 231/1000 | Loss: 0.00053431
Iteration 232/1000 | Loss: 0.00029549
Iteration 233/1000 | Loss: 0.00009081
Iteration 234/1000 | Loss: 0.00019156
Iteration 235/1000 | Loss: 0.00043800
Iteration 236/1000 | Loss: 0.00008636
Iteration 237/1000 | Loss: 0.00007271
Iteration 238/1000 | Loss: 0.00006832
Iteration 239/1000 | Loss: 0.00016518
Iteration 240/1000 | Loss: 0.00010342
Iteration 241/1000 | Loss: 0.00006216
Iteration 242/1000 | Loss: 0.00006960
Iteration 243/1000 | Loss: 0.00006082
Iteration 244/1000 | Loss: 0.00020618
Iteration 245/1000 | Loss: 0.00006386
Iteration 246/1000 | Loss: 0.00022091
Iteration 247/1000 | Loss: 0.00030214
Iteration 248/1000 | Loss: 0.00009200
Iteration 249/1000 | Loss: 0.00006932
Iteration 250/1000 | Loss: 0.00005821
Iteration 251/1000 | Loss: 0.00005527
Iteration 252/1000 | Loss: 0.00005329
Iteration 253/1000 | Loss: 0.00005206
Iteration 254/1000 | Loss: 0.00005094
Iteration 255/1000 | Loss: 0.00005010
Iteration 256/1000 | Loss: 0.00006716
Iteration 257/1000 | Loss: 0.00006281
Iteration 258/1000 | Loss: 0.00005038
Iteration 259/1000 | Loss: 0.00006069
Iteration 260/1000 | Loss: 0.00005801
Iteration 261/1000 | Loss: 0.00004925
Iteration 262/1000 | Loss: 0.00006512
Iteration 263/1000 | Loss: 0.00005861
Iteration 264/1000 | Loss: 0.00006173
Iteration 265/1000 | Loss: 0.00005148
Iteration 266/1000 | Loss: 0.00004963
Iteration 267/1000 | Loss: 0.00004889
Iteration 268/1000 | Loss: 0.00006675
Iteration 269/1000 | Loss: 0.00007005
Iteration 270/1000 | Loss: 0.00006603
Iteration 271/1000 | Loss: 0.00006996
Iteration 272/1000 | Loss: 0.00005474
Iteration 273/1000 | Loss: 0.00006196
Iteration 274/1000 | Loss: 0.00020448
Iteration 275/1000 | Loss: 0.00013764
Iteration 276/1000 | Loss: 0.00017606
Iteration 277/1000 | Loss: 0.00014405
Iteration 278/1000 | Loss: 0.00005668
Iteration 279/1000 | Loss: 0.00005494
Iteration 280/1000 | Loss: 0.00006095
Iteration 281/1000 | Loss: 0.00007312
Iteration 282/1000 | Loss: 0.00006660
Iteration 283/1000 | Loss: 0.00005476
Iteration 284/1000 | Loss: 0.00005918
Iteration 285/1000 | Loss: 0.00005493
Iteration 286/1000 | Loss: 0.00005911
Iteration 287/1000 | Loss: 0.00005475
Iteration 288/1000 | Loss: 0.00004777
Iteration 289/1000 | Loss: 0.00004739
Iteration 290/1000 | Loss: 0.00004714
Iteration 291/1000 | Loss: 0.00007241
Iteration 292/1000 | Loss: 0.00006451
Iteration 293/1000 | Loss: 0.00005302
Iteration 294/1000 | Loss: 0.00004904
Iteration 295/1000 | Loss: 0.00005212
Iteration 296/1000 | Loss: 0.00004823
Iteration 297/1000 | Loss: 0.00004754
Iteration 298/1000 | Loss: 0.00004699
Iteration 299/1000 | Loss: 0.00004676
Iteration 300/1000 | Loss: 0.00004656
Iteration 301/1000 | Loss: 0.00004641
Iteration 302/1000 | Loss: 0.00004630
Iteration 303/1000 | Loss: 0.00004611
Iteration 304/1000 | Loss: 0.00004598
Iteration 305/1000 | Loss: 0.00004591
Iteration 306/1000 | Loss: 0.00004590
Iteration 307/1000 | Loss: 0.00004589
Iteration 308/1000 | Loss: 0.00004589
Iteration 309/1000 | Loss: 0.00004588
Iteration 310/1000 | Loss: 0.00004587
Iteration 311/1000 | Loss: 0.00004587
Iteration 312/1000 | Loss: 0.00004586
Iteration 313/1000 | Loss: 0.00004586
Iteration 314/1000 | Loss: 0.00004586
Iteration 315/1000 | Loss: 0.00004585
Iteration 316/1000 | Loss: 0.00004584
Iteration 317/1000 | Loss: 0.00004582
Iteration 318/1000 | Loss: 0.00004582
Iteration 319/1000 | Loss: 0.00004580
Iteration 320/1000 | Loss: 0.00004580
Iteration 321/1000 | Loss: 0.00004580
Iteration 322/1000 | Loss: 0.00004580
Iteration 323/1000 | Loss: 0.00004579
Iteration 324/1000 | Loss: 0.00004579
Iteration 325/1000 | Loss: 0.00004579
Iteration 326/1000 | Loss: 0.00004579
Iteration 327/1000 | Loss: 0.00004579
Iteration 328/1000 | Loss: 0.00004578
Iteration 329/1000 | Loss: 0.00004578
Iteration 330/1000 | Loss: 0.00004578
Iteration 331/1000 | Loss: 0.00004578
Iteration 332/1000 | Loss: 0.00004578
Iteration 333/1000 | Loss: 0.00004577
Iteration 334/1000 | Loss: 0.00004577
Iteration 335/1000 | Loss: 0.00004577
Iteration 336/1000 | Loss: 0.00004577
Iteration 337/1000 | Loss: 0.00004577
Iteration 338/1000 | Loss: 0.00004577
Iteration 339/1000 | Loss: 0.00004577
Iteration 340/1000 | Loss: 0.00004577
Iteration 341/1000 | Loss: 0.00004577
Iteration 342/1000 | Loss: 0.00004577
Iteration 343/1000 | Loss: 0.00004577
Iteration 344/1000 | Loss: 0.00004577
Iteration 345/1000 | Loss: 0.00004576
Iteration 346/1000 | Loss: 0.00004576
Iteration 347/1000 | Loss: 0.00004576
Iteration 348/1000 | Loss: 0.00004576
Iteration 349/1000 | Loss: 0.00004576
Iteration 350/1000 | Loss: 0.00004576
Iteration 351/1000 | Loss: 0.00004576
Iteration 352/1000 | Loss: 0.00004575
Iteration 353/1000 | Loss: 0.00004575
Iteration 354/1000 | Loss: 0.00004575
Iteration 355/1000 | Loss: 0.00004575
Iteration 356/1000 | Loss: 0.00004575
Iteration 357/1000 | Loss: 0.00004575
Iteration 358/1000 | Loss: 0.00004575
Iteration 359/1000 | Loss: 0.00004574
Iteration 360/1000 | Loss: 0.00004574
Iteration 361/1000 | Loss: 0.00004574
Iteration 362/1000 | Loss: 0.00004574
Iteration 363/1000 | Loss: 0.00004574
Iteration 364/1000 | Loss: 0.00004574
Iteration 365/1000 | Loss: 0.00004574
Iteration 366/1000 | Loss: 0.00004574
Iteration 367/1000 | Loss: 0.00004574
Iteration 368/1000 | Loss: 0.00004574
Iteration 369/1000 | Loss: 0.00004574
Iteration 370/1000 | Loss: 0.00004573
Iteration 371/1000 | Loss: 0.00004573
Iteration 372/1000 | Loss: 0.00004573
Iteration 373/1000 | Loss: 0.00004573
Iteration 374/1000 | Loss: 0.00004573
Iteration 375/1000 | Loss: 0.00004573
Iteration 376/1000 | Loss: 0.00004573
Iteration 377/1000 | Loss: 0.00004573
Iteration 378/1000 | Loss: 0.00004573
Iteration 379/1000 | Loss: 0.00004573
Iteration 380/1000 | Loss: 0.00004573
Iteration 381/1000 | Loss: 0.00004573
Iteration 382/1000 | Loss: 0.00004572
Iteration 383/1000 | Loss: 0.00004572
Iteration 384/1000 | Loss: 0.00004572
Iteration 385/1000 | Loss: 0.00004572
Iteration 386/1000 | Loss: 0.00004572
Iteration 387/1000 | Loss: 0.00004572
Iteration 388/1000 | Loss: 0.00004572
Iteration 389/1000 | Loss: 0.00004572
Iteration 390/1000 | Loss: 0.00004572
Iteration 391/1000 | Loss: 0.00004572
Iteration 392/1000 | Loss: 0.00004572
Iteration 393/1000 | Loss: 0.00004572
Iteration 394/1000 | Loss: 0.00004572
Iteration 395/1000 | Loss: 0.00004571
Iteration 396/1000 | Loss: 0.00004571
Iteration 397/1000 | Loss: 0.00004571
Iteration 398/1000 | Loss: 0.00004571
Iteration 399/1000 | Loss: 0.00004571
Iteration 400/1000 | Loss: 0.00004571
Iteration 401/1000 | Loss: 0.00004571
Iteration 402/1000 | Loss: 0.00004571
Iteration 403/1000 | Loss: 0.00004571
Iteration 404/1000 | Loss: 0.00004571
Iteration 405/1000 | Loss: 0.00004571
Iteration 406/1000 | Loss: 0.00004571
Iteration 407/1000 | Loss: 0.00004571
Iteration 408/1000 | Loss: 0.00004571
Iteration 409/1000 | Loss: 0.00004571
Iteration 410/1000 | Loss: 0.00004571
Iteration 411/1000 | Loss: 0.00004571
Iteration 412/1000 | Loss: 0.00004571
Iteration 413/1000 | Loss: 0.00004571
Iteration 414/1000 | Loss: 0.00004571
Iteration 415/1000 | Loss: 0.00004571
Iteration 416/1000 | Loss: 0.00004571
Iteration 417/1000 | Loss: 0.00004571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 417. Stopping optimization.
Last 5 losses: [4.570699456962757e-05, 4.570699456962757e-05, 4.570699456962757e-05, 4.570699456962757e-05, 4.570699456962757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.570699456962757e-05

Optimization complete. Final v2v error: 5.5755720138549805 mm

Highest mean error: 7.67185115814209 mm for frame 214

Lowest mean error: 4.208807468414307 mm for frame 1

Saving results

Total time: 544.4494969844818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005868
Iteration 2/25 | Loss: 0.00274547
Iteration 3/25 | Loss: 0.00206190
Iteration 4/25 | Loss: 0.00193048
Iteration 5/25 | Loss: 0.00185973
Iteration 6/25 | Loss: 0.00183740
Iteration 7/25 | Loss: 0.00181252
Iteration 8/25 | Loss: 0.00179340
Iteration 9/25 | Loss: 0.00178310
Iteration 10/25 | Loss: 0.00177888
Iteration 11/25 | Loss: 0.00177770
Iteration 12/25 | Loss: 0.00177723
Iteration 13/25 | Loss: 0.00177710
Iteration 14/25 | Loss: 0.00177708
Iteration 15/25 | Loss: 0.00177707
Iteration 16/25 | Loss: 0.00177706
Iteration 17/25 | Loss: 0.00177706
Iteration 18/25 | Loss: 0.00177706
Iteration 19/25 | Loss: 0.00177706
Iteration 20/25 | Loss: 0.00177706
Iteration 21/25 | Loss: 0.00177706
Iteration 22/25 | Loss: 0.00177706
Iteration 23/25 | Loss: 0.00177706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017770648701116443, 0.0017770648701116443, 0.0017770648701116443, 0.0017770648701116443, 0.0017770648701116443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017770648701116443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33624160
Iteration 2/25 | Loss: 0.00354171
Iteration 3/25 | Loss: 0.00353807
Iteration 4/25 | Loss: 0.00353807
Iteration 5/25 | Loss: 0.00353807
Iteration 6/25 | Loss: 0.00353807
Iteration 7/25 | Loss: 0.00353807
Iteration 8/25 | Loss: 0.00353807
Iteration 9/25 | Loss: 0.00353807
Iteration 10/25 | Loss: 0.00353807
Iteration 11/25 | Loss: 0.00353807
Iteration 12/25 | Loss: 0.00353807
Iteration 13/25 | Loss: 0.00353807
Iteration 14/25 | Loss: 0.00353807
Iteration 15/25 | Loss: 0.00353807
Iteration 16/25 | Loss: 0.00353807
Iteration 17/25 | Loss: 0.00353807
Iteration 18/25 | Loss: 0.00353807
Iteration 19/25 | Loss: 0.00353807
Iteration 20/25 | Loss: 0.00353807
Iteration 21/25 | Loss: 0.00353807
Iteration 22/25 | Loss: 0.00353807
Iteration 23/25 | Loss: 0.00353807
Iteration 24/25 | Loss: 0.00353807
Iteration 25/25 | Loss: 0.00353807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00353807
Iteration 2/1000 | Loss: 0.00056603
Iteration 3/1000 | Loss: 0.00045432
Iteration 4/1000 | Loss: 0.00042972
Iteration 5/1000 | Loss: 0.00036567
Iteration 6/1000 | Loss: 0.00033593
Iteration 7/1000 | Loss: 0.00032014
Iteration 8/1000 | Loss: 0.00032399
Iteration 9/1000 | Loss: 0.00037648
Iteration 10/1000 | Loss: 0.00028636
Iteration 11/1000 | Loss: 0.00995667
Iteration 12/1000 | Loss: 0.00708339
Iteration 13/1000 | Loss: 0.00139635
Iteration 14/1000 | Loss: 0.00051016
Iteration 15/1000 | Loss: 0.00035201
Iteration 16/1000 | Loss: 0.00017421
Iteration 17/1000 | Loss: 0.00040918
Iteration 18/1000 | Loss: 0.00017682
Iteration 19/1000 | Loss: 0.00070848
Iteration 20/1000 | Loss: 0.00008925
Iteration 21/1000 | Loss: 0.00034998
Iteration 22/1000 | Loss: 0.00005970
Iteration 23/1000 | Loss: 0.00004591
Iteration 24/1000 | Loss: 0.00003889
Iteration 25/1000 | Loss: 0.00003405
Iteration 26/1000 | Loss: 0.00003015
Iteration 27/1000 | Loss: 0.00002621
Iteration 28/1000 | Loss: 0.00002404
Iteration 29/1000 | Loss: 0.00033603
Iteration 30/1000 | Loss: 0.00021836
Iteration 31/1000 | Loss: 0.00032296
Iteration 32/1000 | Loss: 0.00009241
Iteration 33/1000 | Loss: 0.00002943
Iteration 34/1000 | Loss: 0.00002466
Iteration 35/1000 | Loss: 0.00002254
Iteration 36/1000 | Loss: 0.00002147
Iteration 37/1000 | Loss: 0.00035233
Iteration 38/1000 | Loss: 0.00016195
Iteration 39/1000 | Loss: 0.00018197
Iteration 40/1000 | Loss: 0.00002335
Iteration 41/1000 | Loss: 0.00036601
Iteration 42/1000 | Loss: 0.00004026
Iteration 43/1000 | Loss: 0.00002524
Iteration 44/1000 | Loss: 0.00001925
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001598
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001551
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001542
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001541
Iteration 125/1000 | Loss: 0.00001541
Iteration 126/1000 | Loss: 0.00001541
Iteration 127/1000 | Loss: 0.00001541
Iteration 128/1000 | Loss: 0.00001540
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001540
Iteration 131/1000 | Loss: 0.00001540
Iteration 132/1000 | Loss: 0.00001540
Iteration 133/1000 | Loss: 0.00001540
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001539
Iteration 138/1000 | Loss: 0.00001539
Iteration 139/1000 | Loss: 0.00001539
Iteration 140/1000 | Loss: 0.00001539
Iteration 141/1000 | Loss: 0.00001539
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001539
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001538
Iteration 146/1000 | Loss: 0.00001538
Iteration 147/1000 | Loss: 0.00001538
Iteration 148/1000 | Loss: 0.00001538
Iteration 149/1000 | Loss: 0.00001538
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.5369896573247388e-05, 1.5369896573247388e-05, 1.5369896573247388e-05, 1.5369896573247388e-05, 1.5369896573247388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5369896573247388e-05

Optimization complete. Final v2v error: 3.3386924266815186 mm

Highest mean error: 4.003652095794678 mm for frame 72

Lowest mean error: 3.2831859588623047 mm for frame 131

Saving results

Total time: 99.50725674629211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423691
Iteration 2/25 | Loss: 0.00094187
Iteration 3/25 | Loss: 0.00081028
Iteration 4/25 | Loss: 0.00078125
Iteration 5/25 | Loss: 0.00077623
Iteration 6/25 | Loss: 0.00077493
Iteration 7/25 | Loss: 0.00077458
Iteration 8/25 | Loss: 0.00077458
Iteration 9/25 | Loss: 0.00077458
Iteration 10/25 | Loss: 0.00077458
Iteration 11/25 | Loss: 0.00077458
Iteration 12/25 | Loss: 0.00077458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000774580636061728, 0.000774580636061728, 0.000774580636061728, 0.000774580636061728, 0.000774580636061728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000774580636061728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53701448
Iteration 2/25 | Loss: 0.00080679
Iteration 3/25 | Loss: 0.00080679
Iteration 4/25 | Loss: 0.00080679
Iteration 5/25 | Loss: 0.00080679
Iteration 6/25 | Loss: 0.00080678
Iteration 7/25 | Loss: 0.00080678
Iteration 8/25 | Loss: 0.00080678
Iteration 9/25 | Loss: 0.00080678
Iteration 10/25 | Loss: 0.00080678
Iteration 11/25 | Loss: 0.00080678
Iteration 12/25 | Loss: 0.00080678
Iteration 13/25 | Loss: 0.00080678
Iteration 14/25 | Loss: 0.00080678
Iteration 15/25 | Loss: 0.00080678
Iteration 16/25 | Loss: 0.00080678
Iteration 17/25 | Loss: 0.00080678
Iteration 18/25 | Loss: 0.00080678
Iteration 19/25 | Loss: 0.00080678
Iteration 20/25 | Loss: 0.00080678
Iteration 21/25 | Loss: 0.00080678
Iteration 22/25 | Loss: 0.00080678
Iteration 23/25 | Loss: 0.00080678
Iteration 24/25 | Loss: 0.00080678
Iteration 25/25 | Loss: 0.00080678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080678
Iteration 2/1000 | Loss: 0.00004497
Iteration 3/1000 | Loss: 0.00002839
Iteration 4/1000 | Loss: 0.00002502
Iteration 5/1000 | Loss: 0.00002420
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002265
Iteration 8/1000 | Loss: 0.00002222
Iteration 9/1000 | Loss: 0.00002196
Iteration 10/1000 | Loss: 0.00002195
Iteration 11/1000 | Loss: 0.00002177
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002157
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002152
Iteration 18/1000 | Loss: 0.00002147
Iteration 19/1000 | Loss: 0.00002146
Iteration 20/1000 | Loss: 0.00002146
Iteration 21/1000 | Loss: 0.00002145
Iteration 22/1000 | Loss: 0.00002145
Iteration 23/1000 | Loss: 0.00002145
Iteration 24/1000 | Loss: 0.00002145
Iteration 25/1000 | Loss: 0.00002144
Iteration 26/1000 | Loss: 0.00002144
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002142
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002142
Iteration 33/1000 | Loss: 0.00002141
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002140
Iteration 39/1000 | Loss: 0.00002140
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002136
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002135
Iteration 49/1000 | Loss: 0.00002135
Iteration 50/1000 | Loss: 0.00002134
Iteration 51/1000 | Loss: 0.00002134
Iteration 52/1000 | Loss: 0.00002134
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002133
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002132
Iteration 60/1000 | Loss: 0.00002131
Iteration 61/1000 | Loss: 0.00002131
Iteration 62/1000 | Loss: 0.00002131
Iteration 63/1000 | Loss: 0.00002130
Iteration 64/1000 | Loss: 0.00002130
Iteration 65/1000 | Loss: 0.00002129
Iteration 66/1000 | Loss: 0.00002129
Iteration 67/1000 | Loss: 0.00002129
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002128
Iteration 70/1000 | Loss: 0.00002128
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00002125
Iteration 76/1000 | Loss: 0.00002125
Iteration 77/1000 | Loss: 0.00002125
Iteration 78/1000 | Loss: 0.00002125
Iteration 79/1000 | Loss: 0.00002124
Iteration 80/1000 | Loss: 0.00002124
Iteration 81/1000 | Loss: 0.00002124
Iteration 82/1000 | Loss: 0.00002124
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002123
Iteration 85/1000 | Loss: 0.00002123
Iteration 86/1000 | Loss: 0.00002122
Iteration 87/1000 | Loss: 0.00002122
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002119
Iteration 95/1000 | Loss: 0.00002119
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002119
Iteration 102/1000 | Loss: 0.00002119
Iteration 103/1000 | Loss: 0.00002118
Iteration 104/1000 | Loss: 0.00002118
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00002118
Iteration 107/1000 | Loss: 0.00002118
Iteration 108/1000 | Loss: 0.00002118
Iteration 109/1000 | Loss: 0.00002118
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002117
Iteration 113/1000 | Loss: 0.00002117
Iteration 114/1000 | Loss: 0.00002117
Iteration 115/1000 | Loss: 0.00002117
Iteration 116/1000 | Loss: 0.00002117
Iteration 117/1000 | Loss: 0.00002117
Iteration 118/1000 | Loss: 0.00002117
Iteration 119/1000 | Loss: 0.00002117
Iteration 120/1000 | Loss: 0.00002117
Iteration 121/1000 | Loss: 0.00002117
Iteration 122/1000 | Loss: 0.00002117
Iteration 123/1000 | Loss: 0.00002116
Iteration 124/1000 | Loss: 0.00002116
Iteration 125/1000 | Loss: 0.00002116
Iteration 126/1000 | Loss: 0.00002116
Iteration 127/1000 | Loss: 0.00002116
Iteration 128/1000 | Loss: 0.00002116
Iteration 129/1000 | Loss: 0.00002116
Iteration 130/1000 | Loss: 0.00002116
Iteration 131/1000 | Loss: 0.00002116
Iteration 132/1000 | Loss: 0.00002116
Iteration 133/1000 | Loss: 0.00002116
Iteration 134/1000 | Loss: 0.00002115
Iteration 135/1000 | Loss: 0.00002115
Iteration 136/1000 | Loss: 0.00002115
Iteration 137/1000 | Loss: 0.00002115
Iteration 138/1000 | Loss: 0.00002115
Iteration 139/1000 | Loss: 0.00002115
Iteration 140/1000 | Loss: 0.00002115
Iteration 141/1000 | Loss: 0.00002115
Iteration 142/1000 | Loss: 0.00002115
Iteration 143/1000 | Loss: 0.00002115
Iteration 144/1000 | Loss: 0.00002115
Iteration 145/1000 | Loss: 0.00002115
Iteration 146/1000 | Loss: 0.00002115
Iteration 147/1000 | Loss: 0.00002115
Iteration 148/1000 | Loss: 0.00002114
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00002114
Iteration 151/1000 | Loss: 0.00002114
Iteration 152/1000 | Loss: 0.00002114
Iteration 153/1000 | Loss: 0.00002114
Iteration 154/1000 | Loss: 0.00002114
Iteration 155/1000 | Loss: 0.00002114
Iteration 156/1000 | Loss: 0.00002113
Iteration 157/1000 | Loss: 0.00002113
Iteration 158/1000 | Loss: 0.00002113
Iteration 159/1000 | Loss: 0.00002113
Iteration 160/1000 | Loss: 0.00002113
Iteration 161/1000 | Loss: 0.00002113
Iteration 162/1000 | Loss: 0.00002113
Iteration 163/1000 | Loss: 0.00002113
Iteration 164/1000 | Loss: 0.00002112
Iteration 165/1000 | Loss: 0.00002112
Iteration 166/1000 | Loss: 0.00002112
Iteration 167/1000 | Loss: 0.00002112
Iteration 168/1000 | Loss: 0.00002112
Iteration 169/1000 | Loss: 0.00002112
Iteration 170/1000 | Loss: 0.00002112
Iteration 171/1000 | Loss: 0.00002112
Iteration 172/1000 | Loss: 0.00002112
Iteration 173/1000 | Loss: 0.00002112
Iteration 174/1000 | Loss: 0.00002112
Iteration 175/1000 | Loss: 0.00002112
Iteration 176/1000 | Loss: 0.00002112
Iteration 177/1000 | Loss: 0.00002112
Iteration 178/1000 | Loss: 0.00002112
Iteration 179/1000 | Loss: 0.00002112
Iteration 180/1000 | Loss: 0.00002112
Iteration 181/1000 | Loss: 0.00002111
Iteration 182/1000 | Loss: 0.00002111
Iteration 183/1000 | Loss: 0.00002111
Iteration 184/1000 | Loss: 0.00002111
Iteration 185/1000 | Loss: 0.00002111
Iteration 186/1000 | Loss: 0.00002111
Iteration 187/1000 | Loss: 0.00002111
Iteration 188/1000 | Loss: 0.00002111
Iteration 189/1000 | Loss: 0.00002111
Iteration 190/1000 | Loss: 0.00002111
Iteration 191/1000 | Loss: 0.00002111
Iteration 192/1000 | Loss: 0.00002111
Iteration 193/1000 | Loss: 0.00002110
Iteration 194/1000 | Loss: 0.00002110
Iteration 195/1000 | Loss: 0.00002110
Iteration 196/1000 | Loss: 0.00002110
Iteration 197/1000 | Loss: 0.00002110
Iteration 198/1000 | Loss: 0.00002110
Iteration 199/1000 | Loss: 0.00002110
Iteration 200/1000 | Loss: 0.00002110
Iteration 201/1000 | Loss: 0.00002110
Iteration 202/1000 | Loss: 0.00002110
Iteration 203/1000 | Loss: 0.00002110
Iteration 204/1000 | Loss: 0.00002110
Iteration 205/1000 | Loss: 0.00002110
Iteration 206/1000 | Loss: 0.00002110
Iteration 207/1000 | Loss: 0.00002110
Iteration 208/1000 | Loss: 0.00002110
Iteration 209/1000 | Loss: 0.00002110
Iteration 210/1000 | Loss: 0.00002110
Iteration 211/1000 | Loss: 0.00002110
Iteration 212/1000 | Loss: 0.00002110
Iteration 213/1000 | Loss: 0.00002110
Iteration 214/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.1095433112350293e-05, 2.1095433112350293e-05, 2.1095433112350293e-05, 2.1095433112350293e-05, 2.1095433112350293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1095433112350293e-05

Optimization complete. Final v2v error: 3.855499267578125 mm

Highest mean error: 4.724339008331299 mm for frame 83

Lowest mean error: 3.478602886199951 mm for frame 46

Saving results

Total time: 40.51378679275513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856371
Iteration 2/25 | Loss: 0.00127748
Iteration 3/25 | Loss: 0.00095572
Iteration 4/25 | Loss: 0.00087022
Iteration 5/25 | Loss: 0.00088760
Iteration 6/25 | Loss: 0.00085578
Iteration 7/25 | Loss: 0.00082032
Iteration 8/25 | Loss: 0.00080138
Iteration 9/25 | Loss: 0.00079498
Iteration 10/25 | Loss: 0.00079239
Iteration 11/25 | Loss: 0.00079187
Iteration 12/25 | Loss: 0.00079180
Iteration 13/25 | Loss: 0.00079180
Iteration 14/25 | Loss: 0.00079180
Iteration 15/25 | Loss: 0.00079179
Iteration 16/25 | Loss: 0.00079179
Iteration 17/25 | Loss: 0.00079179
Iteration 18/25 | Loss: 0.00079179
Iteration 19/25 | Loss: 0.00079179
Iteration 20/25 | Loss: 0.00079179
Iteration 21/25 | Loss: 0.00079179
Iteration 22/25 | Loss: 0.00079179
Iteration 23/25 | Loss: 0.00079178
Iteration 24/25 | Loss: 0.00079178
Iteration 25/25 | Loss: 0.00079178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43864608
Iteration 2/25 | Loss: 0.00085061
Iteration 3/25 | Loss: 0.00085057
Iteration 4/25 | Loss: 0.00085057
Iteration 5/25 | Loss: 0.00085057
Iteration 6/25 | Loss: 0.00085057
Iteration 7/25 | Loss: 0.00085057
Iteration 8/25 | Loss: 0.00085057
Iteration 9/25 | Loss: 0.00085057
Iteration 10/25 | Loss: 0.00085057
Iteration 11/25 | Loss: 0.00085057
Iteration 12/25 | Loss: 0.00085057
Iteration 13/25 | Loss: 0.00085057
Iteration 14/25 | Loss: 0.00085057
Iteration 15/25 | Loss: 0.00085057
Iteration 16/25 | Loss: 0.00085057
Iteration 17/25 | Loss: 0.00085057
Iteration 18/25 | Loss: 0.00085057
Iteration 19/25 | Loss: 0.00085057
Iteration 20/25 | Loss: 0.00085057
Iteration 21/25 | Loss: 0.00085057
Iteration 22/25 | Loss: 0.00085057
Iteration 23/25 | Loss: 0.00085057
Iteration 24/25 | Loss: 0.00085057
Iteration 25/25 | Loss: 0.00085057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085057
Iteration 2/1000 | Loss: 0.00003583
Iteration 3/1000 | Loss: 0.00002894
Iteration 4/1000 | Loss: 0.00002443
Iteration 5/1000 | Loss: 0.00002296
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002082
Iteration 9/1000 | Loss: 0.00002038
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001941
Iteration 18/1000 | Loss: 0.00001940
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001939
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001938
Iteration 25/1000 | Loss: 0.00001937
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001929
Iteration 33/1000 | Loss: 0.00001929
Iteration 34/1000 | Loss: 0.00001929
Iteration 35/1000 | Loss: 0.00001929
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001928
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001926
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001925
Iteration 54/1000 | Loss: 0.00001925
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001922
Iteration 80/1000 | Loss: 0.00001922
Iteration 81/1000 | Loss: 0.00001922
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001922
Iteration 88/1000 | Loss: 0.00001922
Iteration 89/1000 | Loss: 0.00001922
Iteration 90/1000 | Loss: 0.00001922
Iteration 91/1000 | Loss: 0.00001922
Iteration 92/1000 | Loss: 0.00001922
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001922
Iteration 95/1000 | Loss: 0.00001922
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001922
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.9215634893043898e-05, 1.9215634893043898e-05, 1.9215634893043898e-05, 1.9215634893043898e-05, 1.9215634893043898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9215634893043898e-05

Optimization complete. Final v2v error: 3.6621665954589844 mm

Highest mean error: 4.323368072509766 mm for frame 1

Lowest mean error: 3.089916944503784 mm for frame 21

Saving results

Total time: 45.918034076690674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082273
Iteration 2/25 | Loss: 0.01082273
Iteration 3/25 | Loss: 0.01082273
Iteration 4/25 | Loss: 0.01082273
Iteration 5/25 | Loss: 0.01082273
Iteration 6/25 | Loss: 0.01082272
Iteration 7/25 | Loss: 0.01082272
Iteration 8/25 | Loss: 0.01082272
Iteration 9/25 | Loss: 0.01082272
Iteration 10/25 | Loss: 0.01082271
Iteration 11/25 | Loss: 0.01082271
Iteration 12/25 | Loss: 0.01082271
Iteration 13/25 | Loss: 0.01082271
Iteration 14/25 | Loss: 0.01082271
Iteration 15/25 | Loss: 0.01082271
Iteration 16/25 | Loss: 0.01082271
Iteration 17/25 | Loss: 0.01082271
Iteration 18/25 | Loss: 0.01082270
Iteration 19/25 | Loss: 0.01082270
Iteration 20/25 | Loss: 0.01082270
Iteration 21/25 | Loss: 0.01082270
Iteration 22/25 | Loss: 0.01082270
Iteration 23/25 | Loss: 0.01082269
Iteration 24/25 | Loss: 0.01082269
Iteration 25/25 | Loss: 0.01082269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27154231
Iteration 2/25 | Loss: 0.11438206
Iteration 3/25 | Loss: 0.11177593
Iteration 4/25 | Loss: 0.11082290
Iteration 5/25 | Loss: 0.11034108
Iteration 6/25 | Loss: 0.11034103
Iteration 7/25 | Loss: 0.11030992
Iteration 8/25 | Loss: 0.11030991
Iteration 9/25 | Loss: 0.11030991
Iteration 10/25 | Loss: 0.11030991
Iteration 11/25 | Loss: 0.11030991
Iteration 12/25 | Loss: 0.11030991
Iteration 13/25 | Loss: 0.11030991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.1103099137544632, 0.1103099137544632, 0.1103099137544632, 0.1103099137544632, 0.1103099137544632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1103099137544632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11030991
Iteration 2/1000 | Loss: 0.00196295
Iteration 3/1000 | Loss: 0.00208671
Iteration 4/1000 | Loss: 0.00061545
Iteration 5/1000 | Loss: 0.00039260
Iteration 6/1000 | Loss: 0.00018151
Iteration 7/1000 | Loss: 0.00017349
Iteration 8/1000 | Loss: 0.00017764
Iteration 9/1000 | Loss: 0.00011000
Iteration 10/1000 | Loss: 0.00016368
Iteration 11/1000 | Loss: 0.00009360
Iteration 12/1000 | Loss: 0.00021060
Iteration 13/1000 | Loss: 0.00005492
Iteration 14/1000 | Loss: 0.00015359
Iteration 15/1000 | Loss: 0.00005302
Iteration 16/1000 | Loss: 0.00014661
Iteration 17/1000 | Loss: 0.00004456
Iteration 18/1000 | Loss: 0.00007595
Iteration 19/1000 | Loss: 0.00025922
Iteration 20/1000 | Loss: 0.00003973
Iteration 21/1000 | Loss: 0.00009089
Iteration 22/1000 | Loss: 0.00009162
Iteration 23/1000 | Loss: 0.00003623
Iteration 24/1000 | Loss: 0.00008975
Iteration 25/1000 | Loss: 0.00004481
Iteration 26/1000 | Loss: 0.00006461
Iteration 27/1000 | Loss: 0.00004203
Iteration 28/1000 | Loss: 0.00003220
Iteration 29/1000 | Loss: 0.00007959
Iteration 30/1000 | Loss: 0.00012010
Iteration 31/1000 | Loss: 0.00003402
Iteration 32/1000 | Loss: 0.00006368
Iteration 33/1000 | Loss: 0.00008894
Iteration 34/1000 | Loss: 0.00003013
Iteration 35/1000 | Loss: 0.00003131
Iteration 36/1000 | Loss: 0.00007951
Iteration 37/1000 | Loss: 0.00002852
Iteration 38/1000 | Loss: 0.00004695
Iteration 39/1000 | Loss: 0.00013499
Iteration 40/1000 | Loss: 0.00002853
Iteration 41/1000 | Loss: 0.00002815
Iteration 42/1000 | Loss: 0.00004328
Iteration 43/1000 | Loss: 0.00002701
Iteration 44/1000 | Loss: 0.00008538
Iteration 45/1000 | Loss: 0.00002656
Iteration 46/1000 | Loss: 0.00002627
Iteration 47/1000 | Loss: 0.00002625
Iteration 48/1000 | Loss: 0.00006042
Iteration 49/1000 | Loss: 0.00002906
Iteration 50/1000 | Loss: 0.00004287
Iteration 51/1000 | Loss: 0.00008008
Iteration 52/1000 | Loss: 0.00004919
Iteration 53/1000 | Loss: 0.00007223
Iteration 54/1000 | Loss: 0.00003574
Iteration 55/1000 | Loss: 0.00002913
Iteration 56/1000 | Loss: 0.00002585
Iteration 57/1000 | Loss: 0.00002585
Iteration 58/1000 | Loss: 0.00002585
Iteration 59/1000 | Loss: 0.00002585
Iteration 60/1000 | Loss: 0.00002585
Iteration 61/1000 | Loss: 0.00002585
Iteration 62/1000 | Loss: 0.00002585
Iteration 63/1000 | Loss: 0.00002584
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00002583
Iteration 66/1000 | Loss: 0.00002582
Iteration 67/1000 | Loss: 0.00002581
Iteration 68/1000 | Loss: 0.00002581
Iteration 69/1000 | Loss: 0.00002581
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002579
Iteration 73/1000 | Loss: 0.00002579
Iteration 74/1000 | Loss: 0.00002578
Iteration 75/1000 | Loss: 0.00002577
Iteration 76/1000 | Loss: 0.00002577
Iteration 77/1000 | Loss: 0.00002577
Iteration 78/1000 | Loss: 0.00002577
Iteration 79/1000 | Loss: 0.00002577
Iteration 80/1000 | Loss: 0.00002576
Iteration 81/1000 | Loss: 0.00002576
Iteration 82/1000 | Loss: 0.00002576
Iteration 83/1000 | Loss: 0.00002576
Iteration 84/1000 | Loss: 0.00002576
Iteration 85/1000 | Loss: 0.00002575
Iteration 86/1000 | Loss: 0.00002574
Iteration 87/1000 | Loss: 0.00015323
Iteration 88/1000 | Loss: 0.00002631
Iteration 89/1000 | Loss: 0.00008882
Iteration 90/1000 | Loss: 0.00003468
Iteration 91/1000 | Loss: 0.00002958
Iteration 92/1000 | Loss: 0.00003050
Iteration 93/1000 | Loss: 0.00002557
Iteration 94/1000 | Loss: 0.00002557
Iteration 95/1000 | Loss: 0.00002622
Iteration 96/1000 | Loss: 0.00002558
Iteration 97/1000 | Loss: 0.00002554
Iteration 98/1000 | Loss: 0.00002554
Iteration 99/1000 | Loss: 0.00002554
Iteration 100/1000 | Loss: 0.00002554
Iteration 101/1000 | Loss: 0.00002554
Iteration 102/1000 | Loss: 0.00002554
Iteration 103/1000 | Loss: 0.00002554
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00002554
Iteration 106/1000 | Loss: 0.00002554
Iteration 107/1000 | Loss: 0.00002554
Iteration 108/1000 | Loss: 0.00002553
Iteration 109/1000 | Loss: 0.00002553
Iteration 110/1000 | Loss: 0.00002553
Iteration 111/1000 | Loss: 0.00002553
Iteration 112/1000 | Loss: 0.00002553
Iteration 113/1000 | Loss: 0.00002553
Iteration 114/1000 | Loss: 0.00002553
Iteration 115/1000 | Loss: 0.00002553
Iteration 116/1000 | Loss: 0.00002553
Iteration 117/1000 | Loss: 0.00002553
Iteration 118/1000 | Loss: 0.00002553
Iteration 119/1000 | Loss: 0.00002553
Iteration 120/1000 | Loss: 0.00002553
Iteration 121/1000 | Loss: 0.00002552
Iteration 122/1000 | Loss: 0.00002552
Iteration 123/1000 | Loss: 0.00002551
Iteration 124/1000 | Loss: 0.00002551
Iteration 125/1000 | Loss: 0.00002551
Iteration 126/1000 | Loss: 0.00002551
Iteration 127/1000 | Loss: 0.00002551
Iteration 128/1000 | Loss: 0.00002551
Iteration 129/1000 | Loss: 0.00002551
Iteration 130/1000 | Loss: 0.00002551
Iteration 131/1000 | Loss: 0.00002551
Iteration 132/1000 | Loss: 0.00002551
Iteration 133/1000 | Loss: 0.00002551
Iteration 134/1000 | Loss: 0.00002551
Iteration 135/1000 | Loss: 0.00002551
Iteration 136/1000 | Loss: 0.00002551
Iteration 137/1000 | Loss: 0.00002550
Iteration 138/1000 | Loss: 0.00002550
Iteration 139/1000 | Loss: 0.00002550
Iteration 140/1000 | Loss: 0.00002550
Iteration 141/1000 | Loss: 0.00002550
Iteration 142/1000 | Loss: 0.00002550
Iteration 143/1000 | Loss: 0.00002550
Iteration 144/1000 | Loss: 0.00002630
Iteration 145/1000 | Loss: 0.00003358
Iteration 146/1000 | Loss: 0.00004949
Iteration 147/1000 | Loss: 0.00002571
Iteration 148/1000 | Loss: 0.00003100
Iteration 149/1000 | Loss: 0.00002547
Iteration 150/1000 | Loss: 0.00002545
Iteration 151/1000 | Loss: 0.00002545
Iteration 152/1000 | Loss: 0.00002545
Iteration 153/1000 | Loss: 0.00002545
Iteration 154/1000 | Loss: 0.00002567
Iteration 155/1000 | Loss: 0.00002580
Iteration 156/1000 | Loss: 0.00004522
Iteration 157/1000 | Loss: 0.00010914
Iteration 158/1000 | Loss: 0.00002560
Iteration 159/1000 | Loss: 0.00002763
Iteration 160/1000 | Loss: 0.00002674
Iteration 161/1000 | Loss: 0.00002563
Iteration 162/1000 | Loss: 0.00002542
Iteration 163/1000 | Loss: 0.00002542
Iteration 164/1000 | Loss: 0.00002542
Iteration 165/1000 | Loss: 0.00002540
Iteration 166/1000 | Loss: 0.00002538
Iteration 167/1000 | Loss: 0.00002538
Iteration 168/1000 | Loss: 0.00002538
Iteration 169/1000 | Loss: 0.00002538
Iteration 170/1000 | Loss: 0.00002538
Iteration 171/1000 | Loss: 0.00002538
Iteration 172/1000 | Loss: 0.00002546
Iteration 173/1000 | Loss: 0.00002538
Iteration 174/1000 | Loss: 0.00002538
Iteration 175/1000 | Loss: 0.00002538
Iteration 176/1000 | Loss: 0.00002538
Iteration 177/1000 | Loss: 0.00002538
Iteration 178/1000 | Loss: 0.00002538
Iteration 179/1000 | Loss: 0.00002538
Iteration 180/1000 | Loss: 0.00002538
Iteration 181/1000 | Loss: 0.00002538
Iteration 182/1000 | Loss: 0.00002538
Iteration 183/1000 | Loss: 0.00002538
Iteration 184/1000 | Loss: 0.00002538
Iteration 185/1000 | Loss: 0.00002538
Iteration 186/1000 | Loss: 0.00002538
Iteration 187/1000 | Loss: 0.00002538
Iteration 188/1000 | Loss: 0.00002538
Iteration 189/1000 | Loss: 0.00002538
Iteration 190/1000 | Loss: 0.00002538
Iteration 191/1000 | Loss: 0.00002538
Iteration 192/1000 | Loss: 0.00002538
Iteration 193/1000 | Loss: 0.00002538
Iteration 194/1000 | Loss: 0.00002538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.537786349421367e-05, 2.537786349421367e-05, 2.537786349421367e-05, 2.537786349421367e-05, 2.537786349421367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.537786349421367e-05

Optimization complete. Final v2v error: 4.1874542236328125 mm

Highest mean error: 5.784470558166504 mm for frame 192

Lowest mean error: 3.4046967029571533 mm for frame 180

Saving results

Total time: 137.31906366348267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514851
Iteration 2/25 | Loss: 0.00124033
Iteration 3/25 | Loss: 0.00095947
Iteration 4/25 | Loss: 0.00091324
Iteration 5/25 | Loss: 0.00090399
Iteration 6/25 | Loss: 0.00090260
Iteration 7/25 | Loss: 0.00090258
Iteration 8/25 | Loss: 0.00090258
Iteration 9/25 | Loss: 0.00090258
Iteration 10/25 | Loss: 0.00090258
Iteration 11/25 | Loss: 0.00090258
Iteration 12/25 | Loss: 0.00090258
Iteration 13/25 | Loss: 0.00090258
Iteration 14/25 | Loss: 0.00090258
Iteration 15/25 | Loss: 0.00090258
Iteration 16/25 | Loss: 0.00090258
Iteration 17/25 | Loss: 0.00090258
Iteration 18/25 | Loss: 0.00090258
Iteration 19/25 | Loss: 0.00090258
Iteration 20/25 | Loss: 0.00090258
Iteration 21/25 | Loss: 0.00090258
Iteration 22/25 | Loss: 0.00090258
Iteration 23/25 | Loss: 0.00090258
Iteration 24/25 | Loss: 0.00090258
Iteration 25/25 | Loss: 0.00090258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55725586
Iteration 2/25 | Loss: 0.00086080
Iteration 3/25 | Loss: 0.00086080
Iteration 4/25 | Loss: 0.00086080
Iteration 5/25 | Loss: 0.00086080
Iteration 6/25 | Loss: 0.00086080
Iteration 7/25 | Loss: 0.00086080
Iteration 8/25 | Loss: 0.00086080
Iteration 9/25 | Loss: 0.00086080
Iteration 10/25 | Loss: 0.00086079
Iteration 11/25 | Loss: 0.00086079
Iteration 12/25 | Loss: 0.00086079
Iteration 13/25 | Loss: 0.00086079
Iteration 14/25 | Loss: 0.00086079
Iteration 15/25 | Loss: 0.00086079
Iteration 16/25 | Loss: 0.00086079
Iteration 17/25 | Loss: 0.00086079
Iteration 18/25 | Loss: 0.00086079
Iteration 19/25 | Loss: 0.00086079
Iteration 20/25 | Loss: 0.00086079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008607947384007275, 0.0008607947384007275, 0.0008607947384007275, 0.0008607947384007275, 0.0008607947384007275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008607947384007275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086079
Iteration 2/1000 | Loss: 0.00004763
Iteration 3/1000 | Loss: 0.00003396
Iteration 4/1000 | Loss: 0.00003131
Iteration 5/1000 | Loss: 0.00003013
Iteration 6/1000 | Loss: 0.00002930
Iteration 7/1000 | Loss: 0.00002865
Iteration 8/1000 | Loss: 0.00002825
Iteration 9/1000 | Loss: 0.00002793
Iteration 10/1000 | Loss: 0.00002774
Iteration 11/1000 | Loss: 0.00002769
Iteration 12/1000 | Loss: 0.00002761
Iteration 13/1000 | Loss: 0.00002755
Iteration 14/1000 | Loss: 0.00002753
Iteration 15/1000 | Loss: 0.00002752
Iteration 16/1000 | Loss: 0.00002752
Iteration 17/1000 | Loss: 0.00002752
Iteration 18/1000 | Loss: 0.00002751
Iteration 19/1000 | Loss: 0.00002751
Iteration 20/1000 | Loss: 0.00002750
Iteration 21/1000 | Loss: 0.00002750
Iteration 22/1000 | Loss: 0.00002747
Iteration 23/1000 | Loss: 0.00002747
Iteration 24/1000 | Loss: 0.00002746
Iteration 25/1000 | Loss: 0.00002746
Iteration 26/1000 | Loss: 0.00002745
Iteration 27/1000 | Loss: 0.00002745
Iteration 28/1000 | Loss: 0.00002745
Iteration 29/1000 | Loss: 0.00002745
Iteration 30/1000 | Loss: 0.00002744
Iteration 31/1000 | Loss: 0.00002744
Iteration 32/1000 | Loss: 0.00002744
Iteration 33/1000 | Loss: 0.00002744
Iteration 34/1000 | Loss: 0.00002744
Iteration 35/1000 | Loss: 0.00002743
Iteration 36/1000 | Loss: 0.00002743
Iteration 37/1000 | Loss: 0.00002743
Iteration 38/1000 | Loss: 0.00002743
Iteration 39/1000 | Loss: 0.00002743
Iteration 40/1000 | Loss: 0.00002743
Iteration 41/1000 | Loss: 0.00002743
Iteration 42/1000 | Loss: 0.00002743
Iteration 43/1000 | Loss: 0.00002743
Iteration 44/1000 | Loss: 0.00002743
Iteration 45/1000 | Loss: 0.00002743
Iteration 46/1000 | Loss: 0.00002743
Iteration 47/1000 | Loss: 0.00002743
Iteration 48/1000 | Loss: 0.00002742
Iteration 49/1000 | Loss: 0.00002742
Iteration 50/1000 | Loss: 0.00002742
Iteration 51/1000 | Loss: 0.00002742
Iteration 52/1000 | Loss: 0.00002742
Iteration 53/1000 | Loss: 0.00002742
Iteration 54/1000 | Loss: 0.00002742
Iteration 55/1000 | Loss: 0.00002741
Iteration 56/1000 | Loss: 0.00002741
Iteration 57/1000 | Loss: 0.00002741
Iteration 58/1000 | Loss: 0.00002741
Iteration 59/1000 | Loss: 0.00002741
Iteration 60/1000 | Loss: 0.00002741
Iteration 61/1000 | Loss: 0.00002741
Iteration 62/1000 | Loss: 0.00002741
Iteration 63/1000 | Loss: 0.00002741
Iteration 64/1000 | Loss: 0.00002741
Iteration 65/1000 | Loss: 0.00002741
Iteration 66/1000 | Loss: 0.00002741
Iteration 67/1000 | Loss: 0.00002741
Iteration 68/1000 | Loss: 0.00002741
Iteration 69/1000 | Loss: 0.00002740
Iteration 70/1000 | Loss: 0.00002740
Iteration 71/1000 | Loss: 0.00002740
Iteration 72/1000 | Loss: 0.00002740
Iteration 73/1000 | Loss: 0.00002740
Iteration 74/1000 | Loss: 0.00002740
Iteration 75/1000 | Loss: 0.00002740
Iteration 76/1000 | Loss: 0.00002740
Iteration 77/1000 | Loss: 0.00002740
Iteration 78/1000 | Loss: 0.00002740
Iteration 79/1000 | Loss: 0.00002740
Iteration 80/1000 | Loss: 0.00002739
Iteration 81/1000 | Loss: 0.00002739
Iteration 82/1000 | Loss: 0.00002739
Iteration 83/1000 | Loss: 0.00002739
Iteration 84/1000 | Loss: 0.00002739
Iteration 85/1000 | Loss: 0.00002739
Iteration 86/1000 | Loss: 0.00002739
Iteration 87/1000 | Loss: 0.00002739
Iteration 88/1000 | Loss: 0.00002739
Iteration 89/1000 | Loss: 0.00002739
Iteration 90/1000 | Loss: 0.00002739
Iteration 91/1000 | Loss: 0.00002739
Iteration 92/1000 | Loss: 0.00002739
Iteration 93/1000 | Loss: 0.00002739
Iteration 94/1000 | Loss: 0.00002738
Iteration 95/1000 | Loss: 0.00002738
Iteration 96/1000 | Loss: 0.00002738
Iteration 97/1000 | Loss: 0.00002738
Iteration 98/1000 | Loss: 0.00002738
Iteration 99/1000 | Loss: 0.00002738
Iteration 100/1000 | Loss: 0.00002738
Iteration 101/1000 | Loss: 0.00002738
Iteration 102/1000 | Loss: 0.00002737
Iteration 103/1000 | Loss: 0.00002737
Iteration 104/1000 | Loss: 0.00002737
Iteration 105/1000 | Loss: 0.00002737
Iteration 106/1000 | Loss: 0.00002737
Iteration 107/1000 | Loss: 0.00002737
Iteration 108/1000 | Loss: 0.00002737
Iteration 109/1000 | Loss: 0.00002737
Iteration 110/1000 | Loss: 0.00002737
Iteration 111/1000 | Loss: 0.00002737
Iteration 112/1000 | Loss: 0.00002737
Iteration 113/1000 | Loss: 0.00002737
Iteration 114/1000 | Loss: 0.00002737
Iteration 115/1000 | Loss: 0.00002737
Iteration 116/1000 | Loss: 0.00002737
Iteration 117/1000 | Loss: 0.00002737
Iteration 118/1000 | Loss: 0.00002737
Iteration 119/1000 | Loss: 0.00002737
Iteration 120/1000 | Loss: 0.00002737
Iteration 121/1000 | Loss: 0.00002737
Iteration 122/1000 | Loss: 0.00002737
Iteration 123/1000 | Loss: 0.00002737
Iteration 124/1000 | Loss: 0.00002737
Iteration 125/1000 | Loss: 0.00002737
Iteration 126/1000 | Loss: 0.00002737
Iteration 127/1000 | Loss: 0.00002737
Iteration 128/1000 | Loss: 0.00002737
Iteration 129/1000 | Loss: 0.00002737
Iteration 130/1000 | Loss: 0.00002737
Iteration 131/1000 | Loss: 0.00002737
Iteration 132/1000 | Loss: 0.00002737
Iteration 133/1000 | Loss: 0.00002737
Iteration 134/1000 | Loss: 0.00002737
Iteration 135/1000 | Loss: 0.00002737
Iteration 136/1000 | Loss: 0.00002737
Iteration 137/1000 | Loss: 0.00002737
Iteration 138/1000 | Loss: 0.00002737
Iteration 139/1000 | Loss: 0.00002737
Iteration 140/1000 | Loss: 0.00002737
Iteration 141/1000 | Loss: 0.00002737
Iteration 142/1000 | Loss: 0.00002737
Iteration 143/1000 | Loss: 0.00002737
Iteration 144/1000 | Loss: 0.00002737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.736596616159659e-05, 2.736596616159659e-05, 2.736596616159659e-05, 2.736596616159659e-05, 2.736596616159659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.736596616159659e-05

Optimization complete. Final v2v error: 4.351198673248291 mm

Highest mean error: 5.087283611297607 mm for frame 196

Lowest mean error: 3.8074302673339844 mm for frame 0

Saving results

Total time: 37.81279897689819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781705
Iteration 2/25 | Loss: 0.00125167
Iteration 3/25 | Loss: 0.00085653
Iteration 4/25 | Loss: 0.00077822
Iteration 5/25 | Loss: 0.00076274
Iteration 6/25 | Loss: 0.00075001
Iteration 7/25 | Loss: 0.00075061
Iteration 8/25 | Loss: 0.00074863
Iteration 9/25 | Loss: 0.00074511
Iteration 10/25 | Loss: 0.00074326
Iteration 11/25 | Loss: 0.00074146
Iteration 12/25 | Loss: 0.00073732
Iteration 13/25 | Loss: 0.00073426
Iteration 14/25 | Loss: 0.00073356
Iteration 15/25 | Loss: 0.00073338
Iteration 16/25 | Loss: 0.00073336
Iteration 17/25 | Loss: 0.00073335
Iteration 18/25 | Loss: 0.00073335
Iteration 19/25 | Loss: 0.00073335
Iteration 20/25 | Loss: 0.00073335
Iteration 21/25 | Loss: 0.00073335
Iteration 22/25 | Loss: 0.00073335
Iteration 23/25 | Loss: 0.00073335
Iteration 24/25 | Loss: 0.00073335
Iteration 25/25 | Loss: 0.00073335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16576052
Iteration 2/25 | Loss: 0.00080320
Iteration 3/25 | Loss: 0.00080320
Iteration 4/25 | Loss: 0.00080320
Iteration 5/25 | Loss: 0.00080320
Iteration 6/25 | Loss: 0.00080320
Iteration 7/25 | Loss: 0.00080320
Iteration 8/25 | Loss: 0.00080320
Iteration 9/25 | Loss: 0.00080320
Iteration 10/25 | Loss: 0.00080320
Iteration 11/25 | Loss: 0.00080320
Iteration 12/25 | Loss: 0.00080320
Iteration 13/25 | Loss: 0.00080320
Iteration 14/25 | Loss: 0.00080320
Iteration 15/25 | Loss: 0.00080320
Iteration 16/25 | Loss: 0.00080320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008032005280256271, 0.0008032005280256271, 0.0008032005280256271, 0.0008032005280256271, 0.0008032005280256271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008032005280256271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080320
Iteration 2/1000 | Loss: 0.00002396
Iteration 3/1000 | Loss: 0.00001695
Iteration 4/1000 | Loss: 0.00001571
Iteration 5/1000 | Loss: 0.00001506
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001415
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001397
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001389
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001387
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001371
Iteration 70/1000 | Loss: 0.00001371
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001368
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.365415118925739e-05, 1.365415118925739e-05, 1.365415118925739e-05, 1.365415118925739e-05, 1.365415118925739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.365415118925739e-05

Optimization complete. Final v2v error: 3.1474781036376953 mm

Highest mean error: 3.5733563899993896 mm for frame 103

Lowest mean error: 2.8568925857543945 mm for frame 198

Saving results

Total time: 56.01651382446289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857963
Iteration 2/25 | Loss: 0.00159493
Iteration 3/25 | Loss: 0.00103641
Iteration 4/25 | Loss: 0.00092230
Iteration 5/25 | Loss: 0.00084538
Iteration 6/25 | Loss: 0.00084950
Iteration 7/25 | Loss: 0.00084935
Iteration 8/25 | Loss: 0.00086191
Iteration 9/25 | Loss: 0.00084143
Iteration 10/25 | Loss: 0.00080849
Iteration 11/25 | Loss: 0.00078780
Iteration 12/25 | Loss: 0.00077279
Iteration 13/25 | Loss: 0.00076731
Iteration 14/25 | Loss: 0.00076745
Iteration 15/25 | Loss: 0.00076094
Iteration 16/25 | Loss: 0.00075778
Iteration 17/25 | Loss: 0.00075982
Iteration 18/25 | Loss: 0.00076617
Iteration 19/25 | Loss: 0.00075889
Iteration 20/25 | Loss: 0.00075463
Iteration 21/25 | Loss: 0.00075352
Iteration 22/25 | Loss: 0.00075281
Iteration 23/25 | Loss: 0.00075203
Iteration 24/25 | Loss: 0.00075178
Iteration 25/25 | Loss: 0.00075329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45602560
Iteration 2/25 | Loss: 0.00089005
Iteration 3/25 | Loss: 0.00089001
Iteration 4/25 | Loss: 0.00089001
Iteration 5/25 | Loss: 0.00089001
Iteration 6/25 | Loss: 0.00089001
Iteration 7/25 | Loss: 0.00089001
Iteration 8/25 | Loss: 0.00089001
Iteration 9/25 | Loss: 0.00089001
Iteration 10/25 | Loss: 0.00089001
Iteration 11/25 | Loss: 0.00089001
Iteration 12/25 | Loss: 0.00089001
Iteration 13/25 | Loss: 0.00089001
Iteration 14/25 | Loss: 0.00089001
Iteration 15/25 | Loss: 0.00089001
Iteration 16/25 | Loss: 0.00089001
Iteration 17/25 | Loss: 0.00089001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008900104439817369, 0.0008900104439817369, 0.0008900104439817369, 0.0008900104439817369, 0.0008900104439817369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008900104439817369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089001
Iteration 2/1000 | Loss: 0.00002828
Iteration 3/1000 | Loss: 0.00002171
Iteration 4/1000 | Loss: 0.00002678
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00003264
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00003504
Iteration 9/1000 | Loss: 0.00004334
Iteration 10/1000 | Loss: 0.00003240
Iteration 11/1000 | Loss: 0.00004019
Iteration 12/1000 | Loss: 0.00003581
Iteration 13/1000 | Loss: 0.00003282
Iteration 14/1000 | Loss: 0.00003341
Iteration 15/1000 | Loss: 0.00003976
Iteration 16/1000 | Loss: 0.00002645
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00002359
Iteration 19/1000 | Loss: 0.00004001
Iteration 20/1000 | Loss: 0.00003837
Iteration 21/1000 | Loss: 0.00003195
Iteration 22/1000 | Loss: 0.00003825
Iteration 23/1000 | Loss: 0.00003201
Iteration 24/1000 | Loss: 0.00003839
Iteration 25/1000 | Loss: 0.00003217
Iteration 26/1000 | Loss: 0.00004279
Iteration 27/1000 | Loss: 0.00003060
Iteration 28/1000 | Loss: 0.00003967
Iteration 29/1000 | Loss: 0.00002542
Iteration 30/1000 | Loss: 0.00005161
Iteration 31/1000 | Loss: 0.00002993
Iteration 32/1000 | Loss: 0.00003104
Iteration 33/1000 | Loss: 0.00003907
Iteration 34/1000 | Loss: 0.00003332
Iteration 35/1000 | Loss: 0.00003825
Iteration 36/1000 | Loss: 0.00002393
Iteration 37/1000 | Loss: 0.00003431
Iteration 38/1000 | Loss: 0.00002946
Iteration 39/1000 | Loss: 0.00003080
Iteration 40/1000 | Loss: 0.00003076
Iteration 41/1000 | Loss: 0.00003329
Iteration 42/1000 | Loss: 0.00003246
Iteration 43/1000 | Loss: 0.00002153
Iteration 44/1000 | Loss: 0.00004452
Iteration 45/1000 | Loss: 0.00003218
Iteration 46/1000 | Loss: 0.00002888
Iteration 47/1000 | Loss: 0.00003060
Iteration 48/1000 | Loss: 0.00004401
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001487
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001447
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001442
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001434
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00001426
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001426
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001424
Iteration 119/1000 | Loss: 0.00001424
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001423
Iteration 125/1000 | Loss: 0.00001422
Iteration 126/1000 | Loss: 0.00001422
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001419
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001419
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4189942703524139e-05, 1.4189942703524139e-05, 1.4189942703524139e-05, 1.4189942703524139e-05, 1.4189942703524139e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4189942703524139e-05

Optimization complete. Final v2v error: 3.2175347805023193 mm

Highest mean error: 4.265983581542969 mm for frame 90

Lowest mean error: 2.714383602142334 mm for frame 226

Saving results

Total time: 148.1363570690155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420969
Iteration 2/25 | Loss: 0.00097780
Iteration 3/25 | Loss: 0.00075646
Iteration 4/25 | Loss: 0.00071469
Iteration 5/25 | Loss: 0.00070595
Iteration 6/25 | Loss: 0.00070389
Iteration 7/25 | Loss: 0.00070327
Iteration 8/25 | Loss: 0.00070320
Iteration 9/25 | Loss: 0.00070320
Iteration 10/25 | Loss: 0.00070320
Iteration 11/25 | Loss: 0.00070320
Iteration 12/25 | Loss: 0.00070320
Iteration 13/25 | Loss: 0.00070320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007031969144009054, 0.0007031969144009054, 0.0007031969144009054, 0.0007031969144009054, 0.0007031969144009054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007031969144009054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55248475
Iteration 2/25 | Loss: 0.00083094
Iteration 3/25 | Loss: 0.00083093
Iteration 4/25 | Loss: 0.00083093
Iteration 5/25 | Loss: 0.00083093
Iteration 6/25 | Loss: 0.00083093
Iteration 7/25 | Loss: 0.00083093
Iteration 8/25 | Loss: 0.00083093
Iteration 9/25 | Loss: 0.00083093
Iteration 10/25 | Loss: 0.00083093
Iteration 11/25 | Loss: 0.00083093
Iteration 12/25 | Loss: 0.00083093
Iteration 13/25 | Loss: 0.00083093
Iteration 14/25 | Loss: 0.00083093
Iteration 15/25 | Loss: 0.00083093
Iteration 16/25 | Loss: 0.00083093
Iteration 17/25 | Loss: 0.00083093
Iteration 18/25 | Loss: 0.00083093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008309287950396538, 0.0008309287950396538, 0.0008309287950396538, 0.0008309287950396538, 0.0008309287950396538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008309287950396538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083093
Iteration 2/1000 | Loss: 0.00003550
Iteration 3/1000 | Loss: 0.00001899
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001202
Iteration 10/1000 | Loss: 0.00001202
Iteration 11/1000 | Loss: 0.00001194
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001161
Iteration 20/1000 | Loss: 0.00001160
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001157
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001157
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001156
Iteration 31/1000 | Loss: 0.00001156
Iteration 32/1000 | Loss: 0.00001156
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001153
Iteration 38/1000 | Loss: 0.00001151
Iteration 39/1000 | Loss: 0.00001151
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001151
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001150
Iteration 49/1000 | Loss: 0.00001150
Iteration 50/1000 | Loss: 0.00001150
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001149
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001147
Iteration 58/1000 | Loss: 0.00001147
Iteration 59/1000 | Loss: 0.00001147
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001147
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001146
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001141
Iteration 107/1000 | Loss: 0.00001141
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001140
Iteration 117/1000 | Loss: 0.00001140
Iteration 118/1000 | Loss: 0.00001140
Iteration 119/1000 | Loss: 0.00001140
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001138
Iteration 125/1000 | Loss: 0.00001138
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001138
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001136
Iteration 135/1000 | Loss: 0.00001136
Iteration 136/1000 | Loss: 0.00001136
Iteration 137/1000 | Loss: 0.00001136
Iteration 138/1000 | Loss: 0.00001136
Iteration 139/1000 | Loss: 0.00001135
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001134
Iteration 146/1000 | Loss: 0.00001134
Iteration 147/1000 | Loss: 0.00001134
Iteration 148/1000 | Loss: 0.00001134
Iteration 149/1000 | Loss: 0.00001134
Iteration 150/1000 | Loss: 0.00001133
Iteration 151/1000 | Loss: 0.00001133
Iteration 152/1000 | Loss: 0.00001133
Iteration 153/1000 | Loss: 0.00001133
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001133
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001133
Iteration 158/1000 | Loss: 0.00001133
Iteration 159/1000 | Loss: 0.00001132
Iteration 160/1000 | Loss: 0.00001132
Iteration 161/1000 | Loss: 0.00001132
Iteration 162/1000 | Loss: 0.00001132
Iteration 163/1000 | Loss: 0.00001132
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00001132
Iteration 166/1000 | Loss: 0.00001132
Iteration 167/1000 | Loss: 0.00001132
Iteration 168/1000 | Loss: 0.00001132
Iteration 169/1000 | Loss: 0.00001132
Iteration 170/1000 | Loss: 0.00001132
Iteration 171/1000 | Loss: 0.00001132
Iteration 172/1000 | Loss: 0.00001132
Iteration 173/1000 | Loss: 0.00001132
Iteration 174/1000 | Loss: 0.00001132
Iteration 175/1000 | Loss: 0.00001132
Iteration 176/1000 | Loss: 0.00001132
Iteration 177/1000 | Loss: 0.00001132
Iteration 178/1000 | Loss: 0.00001132
Iteration 179/1000 | Loss: 0.00001132
Iteration 180/1000 | Loss: 0.00001132
Iteration 181/1000 | Loss: 0.00001132
Iteration 182/1000 | Loss: 0.00001132
Iteration 183/1000 | Loss: 0.00001132
Iteration 184/1000 | Loss: 0.00001132
Iteration 185/1000 | Loss: 0.00001132
Iteration 186/1000 | Loss: 0.00001132
Iteration 187/1000 | Loss: 0.00001132
Iteration 188/1000 | Loss: 0.00001132
Iteration 189/1000 | Loss: 0.00001132
Iteration 190/1000 | Loss: 0.00001132
Iteration 191/1000 | Loss: 0.00001132
Iteration 192/1000 | Loss: 0.00001132
Iteration 193/1000 | Loss: 0.00001132
Iteration 194/1000 | Loss: 0.00001132
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.1321822057652753e-05, 1.1321822057652753e-05, 1.1321822057652753e-05, 1.1321822057652753e-05, 1.1321822057652753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1321822057652753e-05

Optimization complete. Final v2v error: 2.8280060291290283 mm

Highest mean error: 4.0916218757629395 mm for frame 55

Lowest mean error: 2.4928781986236572 mm for frame 36

Saving results

Total time: 39.74094581604004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859664
Iteration 2/25 | Loss: 0.00170240
Iteration 3/25 | Loss: 0.00110297
Iteration 4/25 | Loss: 0.00107588
Iteration 5/25 | Loss: 0.00106796
Iteration 6/25 | Loss: 0.00106701
Iteration 7/25 | Loss: 0.00106701
Iteration 8/25 | Loss: 0.00106701
Iteration 9/25 | Loss: 0.00106701
Iteration 10/25 | Loss: 0.00106701
Iteration 11/25 | Loss: 0.00106701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010670135961845517, 0.0010670135961845517, 0.0010670135961845517, 0.0010670135961845517, 0.0010670135961845517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670135961845517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61651134
Iteration 2/25 | Loss: 0.00044158
Iteration 3/25 | Loss: 0.00044158
Iteration 4/25 | Loss: 0.00044158
Iteration 5/25 | Loss: 0.00044158
Iteration 6/25 | Loss: 0.00044158
Iteration 7/25 | Loss: 0.00044158
Iteration 8/25 | Loss: 0.00044158
Iteration 9/25 | Loss: 0.00044158
Iteration 10/25 | Loss: 0.00044158
Iteration 11/25 | Loss: 0.00044158
Iteration 12/25 | Loss: 0.00044158
Iteration 13/25 | Loss: 0.00044158
Iteration 14/25 | Loss: 0.00044158
Iteration 15/25 | Loss: 0.00044158
Iteration 16/25 | Loss: 0.00044158
Iteration 17/25 | Loss: 0.00044158
Iteration 18/25 | Loss: 0.00044158
Iteration 19/25 | Loss: 0.00044158
Iteration 20/25 | Loss: 0.00044158
Iteration 21/25 | Loss: 0.00044158
Iteration 22/25 | Loss: 0.00044158
Iteration 23/25 | Loss: 0.00044158
Iteration 24/25 | Loss: 0.00044158
Iteration 25/25 | Loss: 0.00044158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044158
Iteration 2/1000 | Loss: 0.00006709
Iteration 3/1000 | Loss: 0.00004799
Iteration 4/1000 | Loss: 0.00004204
Iteration 5/1000 | Loss: 0.00003959
Iteration 6/1000 | Loss: 0.00003869
Iteration 7/1000 | Loss: 0.00003799
Iteration 8/1000 | Loss: 0.00003731
Iteration 9/1000 | Loss: 0.00003672
Iteration 10/1000 | Loss: 0.00003635
Iteration 11/1000 | Loss: 0.00003604
Iteration 12/1000 | Loss: 0.00003573
Iteration 13/1000 | Loss: 0.00003543
Iteration 14/1000 | Loss: 0.00003514
Iteration 15/1000 | Loss: 0.00003491
Iteration 16/1000 | Loss: 0.00003480
Iteration 17/1000 | Loss: 0.00003463
Iteration 18/1000 | Loss: 0.00003461
Iteration 19/1000 | Loss: 0.00003446
Iteration 20/1000 | Loss: 0.00003444
Iteration 21/1000 | Loss: 0.00003433
Iteration 22/1000 | Loss: 0.00003433
Iteration 23/1000 | Loss: 0.00003432
Iteration 24/1000 | Loss: 0.00003431
Iteration 25/1000 | Loss: 0.00003431
Iteration 26/1000 | Loss: 0.00003430
Iteration 27/1000 | Loss: 0.00003429
Iteration 28/1000 | Loss: 0.00003429
Iteration 29/1000 | Loss: 0.00003429
Iteration 30/1000 | Loss: 0.00003428
Iteration 31/1000 | Loss: 0.00003428
Iteration 32/1000 | Loss: 0.00003428
Iteration 33/1000 | Loss: 0.00003428
Iteration 34/1000 | Loss: 0.00003426
Iteration 35/1000 | Loss: 0.00003426
Iteration 36/1000 | Loss: 0.00003426
Iteration 37/1000 | Loss: 0.00003426
Iteration 38/1000 | Loss: 0.00003426
Iteration 39/1000 | Loss: 0.00003426
Iteration 40/1000 | Loss: 0.00003426
Iteration 41/1000 | Loss: 0.00003425
Iteration 42/1000 | Loss: 0.00003425
Iteration 43/1000 | Loss: 0.00003425
Iteration 44/1000 | Loss: 0.00003425
Iteration 45/1000 | Loss: 0.00003425
Iteration 46/1000 | Loss: 0.00003425
Iteration 47/1000 | Loss: 0.00003425
Iteration 48/1000 | Loss: 0.00003425
Iteration 49/1000 | Loss: 0.00003424
Iteration 50/1000 | Loss: 0.00003424
Iteration 51/1000 | Loss: 0.00003422
Iteration 52/1000 | Loss: 0.00003422
Iteration 53/1000 | Loss: 0.00003421
Iteration 54/1000 | Loss: 0.00003421
Iteration 55/1000 | Loss: 0.00003421
Iteration 56/1000 | Loss: 0.00003421
Iteration 57/1000 | Loss: 0.00003421
Iteration 58/1000 | Loss: 0.00003420
Iteration 59/1000 | Loss: 0.00003420
Iteration 60/1000 | Loss: 0.00003420
Iteration 61/1000 | Loss: 0.00003420
Iteration 62/1000 | Loss: 0.00003420
Iteration 63/1000 | Loss: 0.00003420
Iteration 64/1000 | Loss: 0.00003420
Iteration 65/1000 | Loss: 0.00003420
Iteration 66/1000 | Loss: 0.00003420
Iteration 67/1000 | Loss: 0.00003420
Iteration 68/1000 | Loss: 0.00003419
Iteration 69/1000 | Loss: 0.00003419
Iteration 70/1000 | Loss: 0.00003418
Iteration 71/1000 | Loss: 0.00003418
Iteration 72/1000 | Loss: 0.00003418
Iteration 73/1000 | Loss: 0.00003418
Iteration 74/1000 | Loss: 0.00003418
Iteration 75/1000 | Loss: 0.00003417
Iteration 76/1000 | Loss: 0.00003417
Iteration 77/1000 | Loss: 0.00003417
Iteration 78/1000 | Loss: 0.00003417
Iteration 79/1000 | Loss: 0.00003417
Iteration 80/1000 | Loss: 0.00003416
Iteration 81/1000 | Loss: 0.00003416
Iteration 82/1000 | Loss: 0.00003416
Iteration 83/1000 | Loss: 0.00003416
Iteration 84/1000 | Loss: 0.00003416
Iteration 85/1000 | Loss: 0.00003416
Iteration 86/1000 | Loss: 0.00003415
Iteration 87/1000 | Loss: 0.00003415
Iteration 88/1000 | Loss: 0.00003415
Iteration 89/1000 | Loss: 0.00003415
Iteration 90/1000 | Loss: 0.00003414
Iteration 91/1000 | Loss: 0.00003414
Iteration 92/1000 | Loss: 0.00003414
Iteration 93/1000 | Loss: 0.00003414
Iteration 94/1000 | Loss: 0.00003413
Iteration 95/1000 | Loss: 0.00003413
Iteration 96/1000 | Loss: 0.00003413
Iteration 97/1000 | Loss: 0.00003413
Iteration 98/1000 | Loss: 0.00003413
Iteration 99/1000 | Loss: 0.00003413
Iteration 100/1000 | Loss: 0.00003413
Iteration 101/1000 | Loss: 0.00003413
Iteration 102/1000 | Loss: 0.00003412
Iteration 103/1000 | Loss: 0.00003412
Iteration 104/1000 | Loss: 0.00003412
Iteration 105/1000 | Loss: 0.00003412
Iteration 106/1000 | Loss: 0.00003412
Iteration 107/1000 | Loss: 0.00003412
Iteration 108/1000 | Loss: 0.00003412
Iteration 109/1000 | Loss: 0.00003412
Iteration 110/1000 | Loss: 0.00003412
Iteration 111/1000 | Loss: 0.00003411
Iteration 112/1000 | Loss: 0.00003411
Iteration 113/1000 | Loss: 0.00003411
Iteration 114/1000 | Loss: 0.00003411
Iteration 115/1000 | Loss: 0.00003411
Iteration 116/1000 | Loss: 0.00003411
Iteration 117/1000 | Loss: 0.00003411
Iteration 118/1000 | Loss: 0.00003411
Iteration 119/1000 | Loss: 0.00003411
Iteration 120/1000 | Loss: 0.00003411
Iteration 121/1000 | Loss: 0.00003411
Iteration 122/1000 | Loss: 0.00003411
Iteration 123/1000 | Loss: 0.00003410
Iteration 124/1000 | Loss: 0.00003410
Iteration 125/1000 | Loss: 0.00003410
Iteration 126/1000 | Loss: 0.00003410
Iteration 127/1000 | Loss: 0.00003409
Iteration 128/1000 | Loss: 0.00003409
Iteration 129/1000 | Loss: 0.00003409
Iteration 130/1000 | Loss: 0.00003409
Iteration 131/1000 | Loss: 0.00003409
Iteration 132/1000 | Loss: 0.00003409
Iteration 133/1000 | Loss: 0.00003409
Iteration 134/1000 | Loss: 0.00003409
Iteration 135/1000 | Loss: 0.00003409
Iteration 136/1000 | Loss: 0.00003409
Iteration 137/1000 | Loss: 0.00003409
Iteration 138/1000 | Loss: 0.00003409
Iteration 139/1000 | Loss: 0.00003409
Iteration 140/1000 | Loss: 0.00003409
Iteration 141/1000 | Loss: 0.00003408
Iteration 142/1000 | Loss: 0.00003408
Iteration 143/1000 | Loss: 0.00003408
Iteration 144/1000 | Loss: 0.00003408
Iteration 145/1000 | Loss: 0.00003408
Iteration 146/1000 | Loss: 0.00003408
Iteration 147/1000 | Loss: 0.00003408
Iteration 148/1000 | Loss: 0.00003408
Iteration 149/1000 | Loss: 0.00003408
Iteration 150/1000 | Loss: 0.00003407
Iteration 151/1000 | Loss: 0.00003407
Iteration 152/1000 | Loss: 0.00003407
Iteration 153/1000 | Loss: 0.00003407
Iteration 154/1000 | Loss: 0.00003407
Iteration 155/1000 | Loss: 0.00003407
Iteration 156/1000 | Loss: 0.00003407
Iteration 157/1000 | Loss: 0.00003407
Iteration 158/1000 | Loss: 0.00003407
Iteration 159/1000 | Loss: 0.00003407
Iteration 160/1000 | Loss: 0.00003406
Iteration 161/1000 | Loss: 0.00003406
Iteration 162/1000 | Loss: 0.00003406
Iteration 163/1000 | Loss: 0.00003406
Iteration 164/1000 | Loss: 0.00003406
Iteration 165/1000 | Loss: 0.00003406
Iteration 166/1000 | Loss: 0.00003406
Iteration 167/1000 | Loss: 0.00003406
Iteration 168/1000 | Loss: 0.00003405
Iteration 169/1000 | Loss: 0.00003405
Iteration 170/1000 | Loss: 0.00003405
Iteration 171/1000 | Loss: 0.00003405
Iteration 172/1000 | Loss: 0.00003405
Iteration 173/1000 | Loss: 0.00003405
Iteration 174/1000 | Loss: 0.00003405
Iteration 175/1000 | Loss: 0.00003405
Iteration 176/1000 | Loss: 0.00003405
Iteration 177/1000 | Loss: 0.00003405
Iteration 178/1000 | Loss: 0.00003405
Iteration 179/1000 | Loss: 0.00003404
Iteration 180/1000 | Loss: 0.00003404
Iteration 181/1000 | Loss: 0.00003404
Iteration 182/1000 | Loss: 0.00003404
Iteration 183/1000 | Loss: 0.00003404
Iteration 184/1000 | Loss: 0.00003404
Iteration 185/1000 | Loss: 0.00003404
Iteration 186/1000 | Loss: 0.00003404
Iteration 187/1000 | Loss: 0.00003404
Iteration 188/1000 | Loss: 0.00003404
Iteration 189/1000 | Loss: 0.00003404
Iteration 190/1000 | Loss: 0.00003404
Iteration 191/1000 | Loss: 0.00003404
Iteration 192/1000 | Loss: 0.00003404
Iteration 193/1000 | Loss: 0.00003404
Iteration 194/1000 | Loss: 0.00003403
Iteration 195/1000 | Loss: 0.00003403
Iteration 196/1000 | Loss: 0.00003403
Iteration 197/1000 | Loss: 0.00003403
Iteration 198/1000 | Loss: 0.00003403
Iteration 199/1000 | Loss: 0.00003403
Iteration 200/1000 | Loss: 0.00003403
Iteration 201/1000 | Loss: 0.00003403
Iteration 202/1000 | Loss: 0.00003403
Iteration 203/1000 | Loss: 0.00003403
Iteration 204/1000 | Loss: 0.00003402
Iteration 205/1000 | Loss: 0.00003402
Iteration 206/1000 | Loss: 0.00003402
Iteration 207/1000 | Loss: 0.00003402
Iteration 208/1000 | Loss: 0.00003402
Iteration 209/1000 | Loss: 0.00003402
Iteration 210/1000 | Loss: 0.00003402
Iteration 211/1000 | Loss: 0.00003401
Iteration 212/1000 | Loss: 0.00003401
Iteration 213/1000 | Loss: 0.00003401
Iteration 214/1000 | Loss: 0.00003401
Iteration 215/1000 | Loss: 0.00003401
Iteration 216/1000 | Loss: 0.00003401
Iteration 217/1000 | Loss: 0.00003401
Iteration 218/1000 | Loss: 0.00003400
Iteration 219/1000 | Loss: 0.00003400
Iteration 220/1000 | Loss: 0.00003400
Iteration 221/1000 | Loss: 0.00003400
Iteration 222/1000 | Loss: 0.00003400
Iteration 223/1000 | Loss: 0.00003400
Iteration 224/1000 | Loss: 0.00003400
Iteration 225/1000 | Loss: 0.00003400
Iteration 226/1000 | Loss: 0.00003400
Iteration 227/1000 | Loss: 0.00003399
Iteration 228/1000 | Loss: 0.00003399
Iteration 229/1000 | Loss: 0.00003399
Iteration 230/1000 | Loss: 0.00003399
Iteration 231/1000 | Loss: 0.00003399
Iteration 232/1000 | Loss: 0.00003399
Iteration 233/1000 | Loss: 0.00003399
Iteration 234/1000 | Loss: 0.00003399
Iteration 235/1000 | Loss: 0.00003399
Iteration 236/1000 | Loss: 0.00003398
Iteration 237/1000 | Loss: 0.00003398
Iteration 238/1000 | Loss: 0.00003398
Iteration 239/1000 | Loss: 0.00003398
Iteration 240/1000 | Loss: 0.00003398
Iteration 241/1000 | Loss: 0.00003398
Iteration 242/1000 | Loss: 0.00003398
Iteration 243/1000 | Loss: 0.00003398
Iteration 244/1000 | Loss: 0.00003398
Iteration 245/1000 | Loss: 0.00003398
Iteration 246/1000 | Loss: 0.00003398
Iteration 247/1000 | Loss: 0.00003397
Iteration 248/1000 | Loss: 0.00003397
Iteration 249/1000 | Loss: 0.00003397
Iteration 250/1000 | Loss: 0.00003397
Iteration 251/1000 | Loss: 0.00003397
Iteration 252/1000 | Loss: 0.00003397
Iteration 253/1000 | Loss: 0.00003397
Iteration 254/1000 | Loss: 0.00003397
Iteration 255/1000 | Loss: 0.00003397
Iteration 256/1000 | Loss: 0.00003397
Iteration 257/1000 | Loss: 0.00003397
Iteration 258/1000 | Loss: 0.00003397
Iteration 259/1000 | Loss: 0.00003397
Iteration 260/1000 | Loss: 0.00003397
Iteration 261/1000 | Loss: 0.00003397
Iteration 262/1000 | Loss: 0.00003397
Iteration 263/1000 | Loss: 0.00003397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [3.3969809010159224e-05, 3.3969809010159224e-05, 3.3969809010159224e-05, 3.3969809010159224e-05, 3.3969809010159224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3969809010159224e-05

Optimization complete. Final v2v error: 4.684192180633545 mm

Highest mean error: 5.763934135437012 mm for frame 164

Lowest mean error: 3.71531343460083 mm for frame 10

Saving results

Total time: 51.559505224227905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066723
Iteration 2/25 | Loss: 0.01066723
Iteration 3/25 | Loss: 0.01066723
Iteration 4/25 | Loss: 0.01066723
Iteration 5/25 | Loss: 0.01066722
Iteration 6/25 | Loss: 0.01066722
Iteration 7/25 | Loss: 0.01066722
Iteration 8/25 | Loss: 0.01066722
Iteration 9/25 | Loss: 0.01066722
Iteration 10/25 | Loss: 0.01066722
Iteration 11/25 | Loss: 0.01066721
Iteration 12/25 | Loss: 0.01066721
Iteration 13/25 | Loss: 0.01066721
Iteration 14/25 | Loss: 0.01066720
Iteration 15/25 | Loss: 0.01066720
Iteration 16/25 | Loss: 0.01066720
Iteration 17/25 | Loss: 0.01066720
Iteration 18/25 | Loss: 0.01066720
Iteration 19/25 | Loss: 0.01066719
Iteration 20/25 | Loss: 0.01066719
Iteration 21/25 | Loss: 0.01066719
Iteration 22/25 | Loss: 0.01066719
Iteration 23/25 | Loss: 0.01066719
Iteration 24/25 | Loss: 0.01066719
Iteration 25/25 | Loss: 0.01066718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.86236334
Iteration 2/25 | Loss: 0.10208385
Iteration 3/25 | Loss: 0.10143698
Iteration 4/25 | Loss: 0.10143672
Iteration 5/25 | Loss: 0.10143672
Iteration 6/25 | Loss: 0.10143672
Iteration 7/25 | Loss: 0.10143672
Iteration 8/25 | Loss: 0.10143672
Iteration 9/25 | Loss: 0.10143672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.10143671929836273, 0.10143671929836273, 0.10143671929836273, 0.10143671929836273, 0.10143671929836273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10143671929836273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10143672
Iteration 2/1000 | Loss: 0.00907163
Iteration 3/1000 | Loss: 0.00285652
Iteration 4/1000 | Loss: 0.00206997
Iteration 5/1000 | Loss: 0.00133566
Iteration 6/1000 | Loss: 0.00062005
Iteration 7/1000 | Loss: 0.00039818
Iteration 8/1000 | Loss: 0.00072350
Iteration 9/1000 | Loss: 0.00033675
Iteration 10/1000 | Loss: 0.00063346
Iteration 11/1000 | Loss: 0.00137905
Iteration 12/1000 | Loss: 0.00026404
Iteration 13/1000 | Loss: 0.00012833
Iteration 14/1000 | Loss: 0.00011778
Iteration 15/1000 | Loss: 0.00093447
Iteration 16/1000 | Loss: 0.00008545
Iteration 17/1000 | Loss: 0.00009298
Iteration 18/1000 | Loss: 0.00015426
Iteration 19/1000 | Loss: 0.00009268
Iteration 20/1000 | Loss: 0.00005722
Iteration 21/1000 | Loss: 0.00047624
Iteration 22/1000 | Loss: 0.00102919
Iteration 23/1000 | Loss: 0.00008102
Iteration 24/1000 | Loss: 0.00043711
Iteration 25/1000 | Loss: 0.00023840
Iteration 26/1000 | Loss: 0.00013721
Iteration 27/1000 | Loss: 0.00064595
Iteration 28/1000 | Loss: 0.00026649
Iteration 29/1000 | Loss: 0.00004785
Iteration 30/1000 | Loss: 0.00009261
Iteration 31/1000 | Loss: 0.00003659
Iteration 32/1000 | Loss: 0.00015064
Iteration 33/1000 | Loss: 0.00003447
Iteration 34/1000 | Loss: 0.00003591
Iteration 35/1000 | Loss: 0.00042404
Iteration 36/1000 | Loss: 0.00010465
Iteration 37/1000 | Loss: 0.00005058
Iteration 38/1000 | Loss: 0.00003083
Iteration 39/1000 | Loss: 0.00022924
Iteration 40/1000 | Loss: 0.00002999
Iteration 41/1000 | Loss: 0.00003047
Iteration 42/1000 | Loss: 0.00013854
Iteration 43/1000 | Loss: 0.00002914
Iteration 44/1000 | Loss: 0.00007308
Iteration 45/1000 | Loss: 0.00002873
Iteration 46/1000 | Loss: 0.00002844
Iteration 47/1000 | Loss: 0.00002814
Iteration 48/1000 | Loss: 0.00002791
Iteration 49/1000 | Loss: 0.00015477
Iteration 50/1000 | Loss: 0.00002799
Iteration 51/1000 | Loss: 0.00002771
Iteration 52/1000 | Loss: 0.00003847
Iteration 53/1000 | Loss: 0.00002765
Iteration 54/1000 | Loss: 0.00002762
Iteration 55/1000 | Loss: 0.00002762
Iteration 56/1000 | Loss: 0.00002756
Iteration 57/1000 | Loss: 0.00004540
Iteration 58/1000 | Loss: 0.00002755
Iteration 59/1000 | Loss: 0.00002752
Iteration 60/1000 | Loss: 0.00002752
Iteration 61/1000 | Loss: 0.00002746
Iteration 62/1000 | Loss: 0.00003806
Iteration 63/1000 | Loss: 0.00002830
Iteration 64/1000 | Loss: 0.00002736
Iteration 65/1000 | Loss: 0.00002736
Iteration 66/1000 | Loss: 0.00002735
Iteration 67/1000 | Loss: 0.00002735
Iteration 68/1000 | Loss: 0.00002735
Iteration 69/1000 | Loss: 0.00002735
Iteration 70/1000 | Loss: 0.00002735
Iteration 71/1000 | Loss: 0.00002735
Iteration 72/1000 | Loss: 0.00002735
Iteration 73/1000 | Loss: 0.00002735
Iteration 74/1000 | Loss: 0.00002735
Iteration 75/1000 | Loss: 0.00002735
Iteration 76/1000 | Loss: 0.00002735
Iteration 77/1000 | Loss: 0.00002734
Iteration 78/1000 | Loss: 0.00002734
Iteration 79/1000 | Loss: 0.00002734
Iteration 80/1000 | Loss: 0.00002734
Iteration 81/1000 | Loss: 0.00002734
Iteration 82/1000 | Loss: 0.00002734
Iteration 83/1000 | Loss: 0.00002734
Iteration 84/1000 | Loss: 0.00002734
Iteration 85/1000 | Loss: 0.00002734
Iteration 86/1000 | Loss: 0.00002734
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002731
Iteration 89/1000 | Loss: 0.00002731
Iteration 90/1000 | Loss: 0.00002731
Iteration 91/1000 | Loss: 0.00002731
Iteration 92/1000 | Loss: 0.00002731
Iteration 93/1000 | Loss: 0.00002731
Iteration 94/1000 | Loss: 0.00002731
Iteration 95/1000 | Loss: 0.00002731
Iteration 96/1000 | Loss: 0.00002731
Iteration 97/1000 | Loss: 0.00002731
Iteration 98/1000 | Loss: 0.00002731
Iteration 99/1000 | Loss: 0.00002731
Iteration 100/1000 | Loss: 0.00002731
Iteration 101/1000 | Loss: 0.00002731
Iteration 102/1000 | Loss: 0.00002730
Iteration 103/1000 | Loss: 0.00002730
Iteration 104/1000 | Loss: 0.00002730
Iteration 105/1000 | Loss: 0.00002730
Iteration 106/1000 | Loss: 0.00002730
Iteration 107/1000 | Loss: 0.00002730
Iteration 108/1000 | Loss: 0.00002730
Iteration 109/1000 | Loss: 0.00002730
Iteration 110/1000 | Loss: 0.00002729
Iteration 111/1000 | Loss: 0.00002729
Iteration 112/1000 | Loss: 0.00002729
Iteration 113/1000 | Loss: 0.00002729
Iteration 114/1000 | Loss: 0.00002729
Iteration 115/1000 | Loss: 0.00002729
Iteration 116/1000 | Loss: 0.00002728
Iteration 117/1000 | Loss: 0.00002728
Iteration 118/1000 | Loss: 0.00002728
Iteration 119/1000 | Loss: 0.00002728
Iteration 120/1000 | Loss: 0.00002728
Iteration 121/1000 | Loss: 0.00002728
Iteration 122/1000 | Loss: 0.00002728
Iteration 123/1000 | Loss: 0.00002728
Iteration 124/1000 | Loss: 0.00002728
Iteration 125/1000 | Loss: 0.00002728
Iteration 126/1000 | Loss: 0.00002728
Iteration 127/1000 | Loss: 0.00002728
Iteration 128/1000 | Loss: 0.00002728
Iteration 129/1000 | Loss: 0.00002728
Iteration 130/1000 | Loss: 0.00002728
Iteration 131/1000 | Loss: 0.00002728
Iteration 132/1000 | Loss: 0.00002728
Iteration 133/1000 | Loss: 0.00002728
Iteration 134/1000 | Loss: 0.00002728
Iteration 135/1000 | Loss: 0.00002728
Iteration 136/1000 | Loss: 0.00002728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.7279293135507032e-05, 2.7279293135507032e-05, 2.7279293135507032e-05, 2.7279293135507032e-05, 2.7279293135507032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7279293135507032e-05

Optimization complete. Final v2v error: 4.22755765914917 mm

Highest mean error: 17.696613311767578 mm for frame 194

Lowest mean error: 3.7296738624572754 mm for frame 85

Saving results

Total time: 102.73554372787476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035134
Iteration 2/25 | Loss: 0.01035134
Iteration 3/25 | Loss: 0.01035134
Iteration 4/25 | Loss: 0.01035134
Iteration 5/25 | Loss: 0.01035133
Iteration 6/25 | Loss: 0.01035133
Iteration 7/25 | Loss: 0.01035133
Iteration 8/25 | Loss: 0.01035133
Iteration 9/25 | Loss: 0.01035133
Iteration 10/25 | Loss: 0.01035133
Iteration 11/25 | Loss: 0.01035132
Iteration 12/25 | Loss: 0.01035132
Iteration 13/25 | Loss: 0.01035132
Iteration 14/25 | Loss: 0.01035132
Iteration 15/25 | Loss: 0.01035132
Iteration 16/25 | Loss: 0.01035131
Iteration 17/25 | Loss: 0.01035131
Iteration 18/25 | Loss: 0.01035131
Iteration 19/25 | Loss: 0.01035131
Iteration 20/25 | Loss: 0.01035131
Iteration 21/25 | Loss: 0.01035130
Iteration 22/25 | Loss: 0.01035130
Iteration 23/25 | Loss: 0.01035130
Iteration 24/25 | Loss: 0.01035130
Iteration 25/25 | Loss: 0.01035130

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67587686
Iteration 2/25 | Loss: 0.17945534
Iteration 3/25 | Loss: 0.17897707
Iteration 4/25 | Loss: 0.17897205
Iteration 5/25 | Loss: 0.17563227
Iteration 6/25 | Loss: 0.17577316
Iteration 7/25 | Loss: 0.17531857
Iteration 8/25 | Loss: 0.17490782
Iteration 9/25 | Loss: 0.17489995
Iteration 10/25 | Loss: 0.17483117
Iteration 11/25 | Loss: 0.17483117
Iteration 12/25 | Loss: 0.17483117
Iteration 13/25 | Loss: 0.17483117
Iteration 14/25 | Loss: 0.17483112
Iteration 15/25 | Loss: 0.17483112
Iteration 16/25 | Loss: 0.17483112
Iteration 17/25 | Loss: 0.17483112
Iteration 18/25 | Loss: 0.17483112
Iteration 19/25 | Loss: 0.17483112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.17483112215995789, 0.17483112215995789, 0.17483112215995789, 0.17483112215995789, 0.17483112215995789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17483112215995789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17483112
Iteration 2/1000 | Loss: 0.00682226
Iteration 3/1000 | Loss: 0.00241967
Iteration 4/1000 | Loss: 0.00134769
Iteration 5/1000 | Loss: 0.00079804
Iteration 6/1000 | Loss: 0.00149312
Iteration 7/1000 | Loss: 0.00121599
Iteration 8/1000 | Loss: 0.00022649
Iteration 9/1000 | Loss: 0.00064771
Iteration 10/1000 | Loss: 0.00273715
Iteration 11/1000 | Loss: 0.00012494
Iteration 12/1000 | Loss: 0.00018115
Iteration 13/1000 | Loss: 0.00031090
Iteration 14/1000 | Loss: 0.00036207
Iteration 15/1000 | Loss: 0.00118414
Iteration 16/1000 | Loss: 0.00017171
Iteration 17/1000 | Loss: 0.00030465
Iteration 18/1000 | Loss: 0.00004749
Iteration 19/1000 | Loss: 0.00015091
Iteration 20/1000 | Loss: 0.00040673
Iteration 21/1000 | Loss: 0.00003579
Iteration 22/1000 | Loss: 0.00010164
Iteration 23/1000 | Loss: 0.00002942
Iteration 24/1000 | Loss: 0.00023672
Iteration 25/1000 | Loss: 0.00002751
Iteration 26/1000 | Loss: 0.00006841
Iteration 27/1000 | Loss: 0.00002386
Iteration 28/1000 | Loss: 0.00013599
Iteration 29/1000 | Loss: 0.00002177
Iteration 30/1000 | Loss: 0.00004999
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00004671
Iteration 33/1000 | Loss: 0.00009000
Iteration 34/1000 | Loss: 0.00013896
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001867
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00010060
Iteration 46/1000 | Loss: 0.00006568
Iteration 47/1000 | Loss: 0.00022281
Iteration 48/1000 | Loss: 0.00006815
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00004896
Iteration 51/1000 | Loss: 0.00003315
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001849
Iteration 69/1000 | Loss: 0.00001849
Iteration 70/1000 | Loss: 0.00001849
Iteration 71/1000 | Loss: 0.00001848
Iteration 72/1000 | Loss: 0.00001848
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001847
Iteration 77/1000 | Loss: 0.00001847
Iteration 78/1000 | Loss: 0.00001846
Iteration 79/1000 | Loss: 0.00001846
Iteration 80/1000 | Loss: 0.00001845
Iteration 81/1000 | Loss: 0.00001845
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001845
Iteration 86/1000 | Loss: 0.00001845
Iteration 87/1000 | Loss: 0.00001844
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00001842
Iteration 91/1000 | Loss: 0.00001842
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00008119
Iteration 110/1000 | Loss: 0.00002287
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001835
Iteration 116/1000 | Loss: 0.00001835
Iteration 117/1000 | Loss: 0.00001835
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001834
Iteration 122/1000 | Loss: 0.00001834
Iteration 123/1000 | Loss: 0.00005154
Iteration 124/1000 | Loss: 0.00003765
Iteration 125/1000 | Loss: 0.00003630
Iteration 126/1000 | Loss: 0.00002664
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001832
Iteration 135/1000 | Loss: 0.00003243
Iteration 136/1000 | Loss: 0.00001839
Iteration 137/1000 | Loss: 0.00001832
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001829
Iteration 140/1000 | Loss: 0.00001829
Iteration 141/1000 | Loss: 0.00001829
Iteration 142/1000 | Loss: 0.00001829
Iteration 143/1000 | Loss: 0.00001829
Iteration 144/1000 | Loss: 0.00001828
Iteration 145/1000 | Loss: 0.00001828
Iteration 146/1000 | Loss: 0.00001828
Iteration 147/1000 | Loss: 0.00001828
Iteration 148/1000 | Loss: 0.00001828
Iteration 149/1000 | Loss: 0.00001828
Iteration 150/1000 | Loss: 0.00001828
Iteration 151/1000 | Loss: 0.00001827
Iteration 152/1000 | Loss: 0.00001827
Iteration 153/1000 | Loss: 0.00001827
Iteration 154/1000 | Loss: 0.00001827
Iteration 155/1000 | Loss: 0.00001827
Iteration 156/1000 | Loss: 0.00001826
Iteration 157/1000 | Loss: 0.00001826
Iteration 158/1000 | Loss: 0.00001826
Iteration 159/1000 | Loss: 0.00001826
Iteration 160/1000 | Loss: 0.00001826
Iteration 161/1000 | Loss: 0.00001826
Iteration 162/1000 | Loss: 0.00001826
Iteration 163/1000 | Loss: 0.00001826
Iteration 164/1000 | Loss: 0.00001826
Iteration 165/1000 | Loss: 0.00001826
Iteration 166/1000 | Loss: 0.00001826
Iteration 167/1000 | Loss: 0.00001826
Iteration 168/1000 | Loss: 0.00001826
Iteration 169/1000 | Loss: 0.00001826
Iteration 170/1000 | Loss: 0.00001826
Iteration 171/1000 | Loss: 0.00001826
Iteration 172/1000 | Loss: 0.00001826
Iteration 173/1000 | Loss: 0.00001826
Iteration 174/1000 | Loss: 0.00001826
Iteration 175/1000 | Loss: 0.00001826
Iteration 176/1000 | Loss: 0.00001826
Iteration 177/1000 | Loss: 0.00001826
Iteration 178/1000 | Loss: 0.00001826
Iteration 179/1000 | Loss: 0.00001826
Iteration 180/1000 | Loss: 0.00001826
Iteration 181/1000 | Loss: 0.00001826
Iteration 182/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.8255228496855125e-05, 1.8255228496855125e-05, 1.8255228496855125e-05, 1.8255228496855125e-05, 1.8255228496855125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8255228496855125e-05

Optimization complete. Final v2v error: 3.6318323612213135 mm

Highest mean error: 4.313471794128418 mm for frame 18

Lowest mean error: 3.3279643058776855 mm for frame 198

Saving results

Total time: 115.64801239967346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042194
Iteration 2/25 | Loss: 0.00143465
Iteration 3/25 | Loss: 0.00099880
Iteration 4/25 | Loss: 0.00090759
Iteration 5/25 | Loss: 0.00087503
Iteration 6/25 | Loss: 0.00086688
Iteration 7/25 | Loss: 0.00084851
Iteration 8/25 | Loss: 0.00082260
Iteration 9/25 | Loss: 0.00079220
Iteration 10/25 | Loss: 0.00077511
Iteration 11/25 | Loss: 0.00075514
Iteration 12/25 | Loss: 0.00074526
Iteration 13/25 | Loss: 0.00074243
Iteration 14/25 | Loss: 0.00074137
Iteration 15/25 | Loss: 0.00074623
Iteration 16/25 | Loss: 0.00073850
Iteration 17/25 | Loss: 0.00073586
Iteration 18/25 | Loss: 0.00073506
Iteration 19/25 | Loss: 0.00073492
Iteration 20/25 | Loss: 0.00073485
Iteration 21/25 | Loss: 0.00073485
Iteration 22/25 | Loss: 0.00073484
Iteration 23/25 | Loss: 0.00073484
Iteration 24/25 | Loss: 0.00073481
Iteration 25/25 | Loss: 0.00073481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61823428
Iteration 2/25 | Loss: 0.00100589
Iteration 3/25 | Loss: 0.00100589
Iteration 4/25 | Loss: 0.00092383
Iteration 5/25 | Loss: 0.00092382
Iteration 6/25 | Loss: 0.00092382
Iteration 7/25 | Loss: 0.00092382
Iteration 8/25 | Loss: 0.00092382
Iteration 9/25 | Loss: 0.00092382
Iteration 10/25 | Loss: 0.00092382
Iteration 11/25 | Loss: 0.00092382
Iteration 12/25 | Loss: 0.00092382
Iteration 13/25 | Loss: 0.00092382
Iteration 14/25 | Loss: 0.00092382
Iteration 15/25 | Loss: 0.00092382
Iteration 16/25 | Loss: 0.00092382
Iteration 17/25 | Loss: 0.00092382
Iteration 18/25 | Loss: 0.00092382
Iteration 19/25 | Loss: 0.00092382
Iteration 20/25 | Loss: 0.00092382
Iteration 21/25 | Loss: 0.00092382
Iteration 22/25 | Loss: 0.00092382
Iteration 23/25 | Loss: 0.00092382
Iteration 24/25 | Loss: 0.00092382
Iteration 25/25 | Loss: 0.00092382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092382
Iteration 2/1000 | Loss: 0.00011424
Iteration 3/1000 | Loss: 0.00002547
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00001956
Iteration 6/1000 | Loss: 0.00001885
Iteration 7/1000 | Loss: 0.00001826
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00048175
Iteration 11/1000 | Loss: 0.00023859
Iteration 12/1000 | Loss: 0.00156543
Iteration 13/1000 | Loss: 0.00055386
Iteration 14/1000 | Loss: 0.00164758
Iteration 15/1000 | Loss: 0.00123533
Iteration 16/1000 | Loss: 0.00011876
Iteration 17/1000 | Loss: 0.00003703
Iteration 18/1000 | Loss: 0.00004117
Iteration 19/1000 | Loss: 0.00002496
Iteration 20/1000 | Loss: 0.00001839
Iteration 21/1000 | Loss: 0.00024998
Iteration 22/1000 | Loss: 0.00002463
Iteration 23/1000 | Loss: 0.00017027
Iteration 24/1000 | Loss: 0.00001902
Iteration 25/1000 | Loss: 0.00012994
Iteration 26/1000 | Loss: 0.00011285
Iteration 27/1000 | Loss: 0.00009619
Iteration 28/1000 | Loss: 0.00003821
Iteration 29/1000 | Loss: 0.00009158
Iteration 30/1000 | Loss: 0.00003145
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00019701
Iteration 34/1000 | Loss: 0.00002950
Iteration 35/1000 | Loss: 0.00006021
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001176
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001171
Iteration 63/1000 | Loss: 0.00001171
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001163
Iteration 102/1000 | Loss: 0.00001163
Iteration 103/1000 | Loss: 0.00001163
Iteration 104/1000 | Loss: 0.00001163
Iteration 105/1000 | Loss: 0.00001162
Iteration 106/1000 | Loss: 0.00001162
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001162
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Iteration 152/1000 | Loss: 0.00001158
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001155
Iteration 168/1000 | Loss: 0.00001155
Iteration 169/1000 | Loss: 0.00001155
Iteration 170/1000 | Loss: 0.00001155
Iteration 171/1000 | Loss: 0.00001154
Iteration 172/1000 | Loss: 0.00001154
Iteration 173/1000 | Loss: 0.00001154
Iteration 174/1000 | Loss: 0.00001154
Iteration 175/1000 | Loss: 0.00001154
Iteration 176/1000 | Loss: 0.00001154
Iteration 177/1000 | Loss: 0.00001154
Iteration 178/1000 | Loss: 0.00001153
Iteration 179/1000 | Loss: 0.00001153
Iteration 180/1000 | Loss: 0.00001153
Iteration 181/1000 | Loss: 0.00001153
Iteration 182/1000 | Loss: 0.00001153
Iteration 183/1000 | Loss: 0.00001153
Iteration 184/1000 | Loss: 0.00001153
Iteration 185/1000 | Loss: 0.00001153
Iteration 186/1000 | Loss: 0.00001153
Iteration 187/1000 | Loss: 0.00001153
Iteration 188/1000 | Loss: 0.00001153
Iteration 189/1000 | Loss: 0.00001153
Iteration 190/1000 | Loss: 0.00001153
Iteration 191/1000 | Loss: 0.00001153
Iteration 192/1000 | Loss: 0.00001153
Iteration 193/1000 | Loss: 0.00001153
Iteration 194/1000 | Loss: 0.00001153
Iteration 195/1000 | Loss: 0.00001153
Iteration 196/1000 | Loss: 0.00001153
Iteration 197/1000 | Loss: 0.00001153
Iteration 198/1000 | Loss: 0.00001153
Iteration 199/1000 | Loss: 0.00001152
Iteration 200/1000 | Loss: 0.00001152
Iteration 201/1000 | Loss: 0.00001152
Iteration 202/1000 | Loss: 0.00001152
Iteration 203/1000 | Loss: 0.00001152
Iteration 204/1000 | Loss: 0.00001152
Iteration 205/1000 | Loss: 0.00001152
Iteration 206/1000 | Loss: 0.00001152
Iteration 207/1000 | Loss: 0.00001152
Iteration 208/1000 | Loss: 0.00001152
Iteration 209/1000 | Loss: 0.00001152
Iteration 210/1000 | Loss: 0.00001152
Iteration 211/1000 | Loss: 0.00001152
Iteration 212/1000 | Loss: 0.00001152
Iteration 213/1000 | Loss: 0.00001152
Iteration 214/1000 | Loss: 0.00001152
Iteration 215/1000 | Loss: 0.00001152
Iteration 216/1000 | Loss: 0.00001152
Iteration 217/1000 | Loss: 0.00001152
Iteration 218/1000 | Loss: 0.00001152
Iteration 219/1000 | Loss: 0.00001152
Iteration 220/1000 | Loss: 0.00001152
Iteration 221/1000 | Loss: 0.00001152
Iteration 222/1000 | Loss: 0.00001152
Iteration 223/1000 | Loss: 0.00001152
Iteration 224/1000 | Loss: 0.00001152
Iteration 225/1000 | Loss: 0.00001152
Iteration 226/1000 | Loss: 0.00001152
Iteration 227/1000 | Loss: 0.00001152
Iteration 228/1000 | Loss: 0.00001152
Iteration 229/1000 | Loss: 0.00001152
Iteration 230/1000 | Loss: 0.00001152
Iteration 231/1000 | Loss: 0.00001152
Iteration 232/1000 | Loss: 0.00001152
Iteration 233/1000 | Loss: 0.00001152
Iteration 234/1000 | Loss: 0.00001152
Iteration 235/1000 | Loss: 0.00001152
Iteration 236/1000 | Loss: 0.00001152
Iteration 237/1000 | Loss: 0.00001152
Iteration 238/1000 | Loss: 0.00001152
Iteration 239/1000 | Loss: 0.00001152
Iteration 240/1000 | Loss: 0.00001152
Iteration 241/1000 | Loss: 0.00001152
Iteration 242/1000 | Loss: 0.00001152
Iteration 243/1000 | Loss: 0.00001152
Iteration 244/1000 | Loss: 0.00001152
Iteration 245/1000 | Loss: 0.00001152
Iteration 246/1000 | Loss: 0.00001152
Iteration 247/1000 | Loss: 0.00001152
Iteration 248/1000 | Loss: 0.00001152
Iteration 249/1000 | Loss: 0.00001152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.1515504411363509e-05, 1.1515504411363509e-05, 1.1515504411363509e-05, 1.1515504411363509e-05, 1.1515504411363509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1515504411363509e-05

Optimization complete. Final v2v error: 2.8833563327789307 mm

Highest mean error: 3.4669578075408936 mm for frame 66

Lowest mean error: 2.6777701377868652 mm for frame 20

Saving results

Total time: 107.12283992767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026976
Iteration 2/25 | Loss: 0.01026975
Iteration 3/25 | Loss: 0.00293952
Iteration 4/25 | Loss: 0.00207590
Iteration 5/25 | Loss: 0.00212959
Iteration 6/25 | Loss: 0.00184178
Iteration 7/25 | Loss: 0.00169370
Iteration 8/25 | Loss: 0.00167188
Iteration 9/25 | Loss: 0.00166209
Iteration 10/25 | Loss: 0.00165813
Iteration 11/25 | Loss: 0.00165637
Iteration 12/25 | Loss: 0.00165624
Iteration 13/25 | Loss: 0.00165624
Iteration 14/25 | Loss: 0.00165624
Iteration 15/25 | Loss: 0.00165624
Iteration 16/25 | Loss: 0.00165624
Iteration 17/25 | Loss: 0.00165624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016562355449423194, 0.0016562355449423194, 0.0016562355449423194, 0.0016562355449423194, 0.0016562355449423194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016562355449423194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56369603
Iteration 2/25 | Loss: 0.00837120
Iteration 3/25 | Loss: 0.00689696
Iteration 4/25 | Loss: 0.00689696
Iteration 5/25 | Loss: 0.00689696
Iteration 6/25 | Loss: 0.00689695
Iteration 7/25 | Loss: 0.00689695
Iteration 8/25 | Loss: 0.00689695
Iteration 9/25 | Loss: 0.00689695
Iteration 10/25 | Loss: 0.00689695
Iteration 11/25 | Loss: 0.00689695
Iteration 12/25 | Loss: 0.00689695
Iteration 13/25 | Loss: 0.00689695
Iteration 14/25 | Loss: 0.00689695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00689695356413722, 0.00689695356413722, 0.00689695356413722, 0.00689695356413722, 0.00689695356413722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00689695356413722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00689695
Iteration 2/1000 | Loss: 0.00382932
Iteration 3/1000 | Loss: 0.00084254
Iteration 4/1000 | Loss: 0.00067688
Iteration 5/1000 | Loss: 0.00100670
Iteration 6/1000 | Loss: 0.00054930
Iteration 7/1000 | Loss: 0.03151492
Iteration 8/1000 | Loss: 0.00984046
Iteration 9/1000 | Loss: 0.00123235
Iteration 10/1000 | Loss: 0.00080749
Iteration 11/1000 | Loss: 0.00043709
Iteration 12/1000 | Loss: 0.00013854
Iteration 13/1000 | Loss: 0.00007567
Iteration 14/1000 | Loss: 0.00005609
Iteration 15/1000 | Loss: 0.00007163
Iteration 16/1000 | Loss: 0.00003965
Iteration 17/1000 | Loss: 0.00003326
Iteration 18/1000 | Loss: 0.00002911
Iteration 19/1000 | Loss: 0.00002643
Iteration 20/1000 | Loss: 0.00002468
Iteration 21/1000 | Loss: 0.00002275
Iteration 22/1000 | Loss: 0.00002143
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001918
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001909
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001904
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001900
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001899
Iteration 47/1000 | Loss: 0.00001899
Iteration 48/1000 | Loss: 0.00001899
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001898
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001896
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001894
Iteration 83/1000 | Loss: 0.00001894
Iteration 84/1000 | Loss: 0.00001894
Iteration 85/1000 | Loss: 0.00001894
Iteration 86/1000 | Loss: 0.00001894
Iteration 87/1000 | Loss: 0.00001894
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001893
Iteration 93/1000 | Loss: 0.00001893
Iteration 94/1000 | Loss: 0.00001893
Iteration 95/1000 | Loss: 0.00001893
Iteration 96/1000 | Loss: 0.00001893
Iteration 97/1000 | Loss: 0.00001893
Iteration 98/1000 | Loss: 0.00001893
Iteration 99/1000 | Loss: 0.00001893
Iteration 100/1000 | Loss: 0.00001893
Iteration 101/1000 | Loss: 0.00001893
Iteration 102/1000 | Loss: 0.00001893
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001893
Iteration 106/1000 | Loss: 0.00001892
Iteration 107/1000 | Loss: 0.00001892
Iteration 108/1000 | Loss: 0.00001892
Iteration 109/1000 | Loss: 0.00001892
Iteration 110/1000 | Loss: 0.00001892
Iteration 111/1000 | Loss: 0.00001892
Iteration 112/1000 | Loss: 0.00001892
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001891
Iteration 118/1000 | Loss: 0.00001891
Iteration 119/1000 | Loss: 0.00001891
Iteration 120/1000 | Loss: 0.00001891
Iteration 121/1000 | Loss: 0.00001891
Iteration 122/1000 | Loss: 0.00001891
Iteration 123/1000 | Loss: 0.00001891
Iteration 124/1000 | Loss: 0.00001891
Iteration 125/1000 | Loss: 0.00001891
Iteration 126/1000 | Loss: 0.00001891
Iteration 127/1000 | Loss: 0.00001891
Iteration 128/1000 | Loss: 0.00001891
Iteration 129/1000 | Loss: 0.00001891
Iteration 130/1000 | Loss: 0.00001891
Iteration 131/1000 | Loss: 0.00001891
Iteration 132/1000 | Loss: 0.00001891
Iteration 133/1000 | Loss: 0.00001891
Iteration 134/1000 | Loss: 0.00001891
Iteration 135/1000 | Loss: 0.00001891
Iteration 136/1000 | Loss: 0.00001891
Iteration 137/1000 | Loss: 0.00001891
Iteration 138/1000 | Loss: 0.00001891
Iteration 139/1000 | Loss: 0.00001891
Iteration 140/1000 | Loss: 0.00001891
Iteration 141/1000 | Loss: 0.00001891
Iteration 142/1000 | Loss: 0.00001891
Iteration 143/1000 | Loss: 0.00001891
Iteration 144/1000 | Loss: 0.00001891
Iteration 145/1000 | Loss: 0.00001891
Iteration 146/1000 | Loss: 0.00001891
Iteration 147/1000 | Loss: 0.00001891
Iteration 148/1000 | Loss: 0.00001891
Iteration 149/1000 | Loss: 0.00001891
Iteration 150/1000 | Loss: 0.00001891
Iteration 151/1000 | Loss: 0.00001891
Iteration 152/1000 | Loss: 0.00001891
Iteration 153/1000 | Loss: 0.00001891
Iteration 154/1000 | Loss: 0.00001891
Iteration 155/1000 | Loss: 0.00001891
Iteration 156/1000 | Loss: 0.00001891
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001891
Iteration 160/1000 | Loss: 0.00001891
Iteration 161/1000 | Loss: 0.00001891
Iteration 162/1000 | Loss: 0.00001891
Iteration 163/1000 | Loss: 0.00001891
Iteration 164/1000 | Loss: 0.00001891
Iteration 165/1000 | Loss: 0.00001891
Iteration 166/1000 | Loss: 0.00001891
Iteration 167/1000 | Loss: 0.00001891
Iteration 168/1000 | Loss: 0.00001891
Iteration 169/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.8908338461187668e-05, 1.8908338461187668e-05, 1.8908338461187668e-05, 1.8908338461187668e-05, 1.8908338461187668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8908338461187668e-05

Optimization complete. Final v2v error: 3.7036349773406982 mm

Highest mean error: 3.7719457149505615 mm for frame 215

Lowest mean error: 3.6606202125549316 mm for frame 29

Saving results

Total time: 72.43354225158691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058647
Iteration 2/25 | Loss: 0.00160951
Iteration 3/25 | Loss: 0.00104322
Iteration 4/25 | Loss: 0.00095450
Iteration 5/25 | Loss: 0.00087111
Iteration 6/25 | Loss: 0.00084541
Iteration 7/25 | Loss: 0.00086091
Iteration 8/25 | Loss: 0.00082204
Iteration 9/25 | Loss: 0.00080316
Iteration 10/25 | Loss: 0.00079791
Iteration 11/25 | Loss: 0.00078844
Iteration 12/25 | Loss: 0.00078553
Iteration 13/25 | Loss: 0.00078363
Iteration 14/25 | Loss: 0.00077472
Iteration 15/25 | Loss: 0.00077247
Iteration 16/25 | Loss: 0.00076653
Iteration 17/25 | Loss: 0.00076339
Iteration 18/25 | Loss: 0.00077202
Iteration 19/25 | Loss: 0.00077239
Iteration 20/25 | Loss: 0.00077248
Iteration 21/25 | Loss: 0.00076686
Iteration 22/25 | Loss: 0.00076235
Iteration 23/25 | Loss: 0.00076139
Iteration 24/25 | Loss: 0.00075974
Iteration 25/25 | Loss: 0.00076166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54875839
Iteration 2/25 | Loss: 0.00128829
Iteration 3/25 | Loss: 0.00128829
Iteration 4/25 | Loss: 0.00128829
Iteration 5/25 | Loss: 0.00128829
Iteration 6/25 | Loss: 0.00128829
Iteration 7/25 | Loss: 0.00128829
Iteration 8/25 | Loss: 0.00128829
Iteration 9/25 | Loss: 0.00128829
Iteration 10/25 | Loss: 0.00128829
Iteration 11/25 | Loss: 0.00128829
Iteration 12/25 | Loss: 0.00128829
Iteration 13/25 | Loss: 0.00128829
Iteration 14/25 | Loss: 0.00128829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012882889714092016, 0.0012882889714092016, 0.0012882889714092016, 0.0012882889714092016, 0.0012882889714092016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012882889714092016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128829
Iteration 2/1000 | Loss: 0.00016490
Iteration 3/1000 | Loss: 0.00245342
Iteration 4/1000 | Loss: 0.00055498
Iteration 5/1000 | Loss: 0.00027474
Iteration 6/1000 | Loss: 0.00039797
Iteration 7/1000 | Loss: 0.00008691
Iteration 8/1000 | Loss: 0.00020494
Iteration 9/1000 | Loss: 0.00009730
Iteration 10/1000 | Loss: 0.00012716
Iteration 11/1000 | Loss: 0.00020756
Iteration 12/1000 | Loss: 0.00028107
Iteration 13/1000 | Loss: 0.00022219
Iteration 14/1000 | Loss: 0.00017309
Iteration 15/1000 | Loss: 0.00035021
Iteration 16/1000 | Loss: 0.00017267
Iteration 17/1000 | Loss: 0.00012080
Iteration 18/1000 | Loss: 0.00027297
Iteration 19/1000 | Loss: 0.00005752
Iteration 20/1000 | Loss: 0.00006515
Iteration 21/1000 | Loss: 0.00002946
Iteration 22/1000 | Loss: 0.00026331
Iteration 23/1000 | Loss: 0.00021797
Iteration 24/1000 | Loss: 0.00015709
Iteration 25/1000 | Loss: 0.00017346
Iteration 26/1000 | Loss: 0.00020418
Iteration 27/1000 | Loss: 0.00021887
Iteration 28/1000 | Loss: 0.00014935
Iteration 29/1000 | Loss: 0.00013232
Iteration 30/1000 | Loss: 0.00011370
Iteration 31/1000 | Loss: 0.00019298
Iteration 32/1000 | Loss: 0.00036077
Iteration 33/1000 | Loss: 0.00004303
Iteration 34/1000 | Loss: 0.00004826
Iteration 35/1000 | Loss: 0.00005081
Iteration 36/1000 | Loss: 0.00003715
Iteration 37/1000 | Loss: 0.00004091
Iteration 38/1000 | Loss: 0.00004126
Iteration 39/1000 | Loss: 0.00002805
Iteration 40/1000 | Loss: 0.00003946
Iteration 41/1000 | Loss: 0.00005386
Iteration 42/1000 | Loss: 0.00004375
Iteration 43/1000 | Loss: 0.00002260
Iteration 44/1000 | Loss: 0.00004178
Iteration 45/1000 | Loss: 0.00004400
Iteration 46/1000 | Loss: 0.00004090
Iteration 47/1000 | Loss: 0.00004389
Iteration 48/1000 | Loss: 0.00003663
Iteration 49/1000 | Loss: 0.00003674
Iteration 50/1000 | Loss: 0.00003977
Iteration 51/1000 | Loss: 0.00004080
Iteration 52/1000 | Loss: 0.00003465
Iteration 53/1000 | Loss: 0.00004061
Iteration 54/1000 | Loss: 0.00004125
Iteration 55/1000 | Loss: 0.00003981
Iteration 56/1000 | Loss: 0.00003464
Iteration 57/1000 | Loss: 0.00003981
Iteration 58/1000 | Loss: 0.00003471
Iteration 59/1000 | Loss: 0.00003849
Iteration 60/1000 | Loss: 0.00002209
Iteration 61/1000 | Loss: 0.00003922
Iteration 62/1000 | Loss: 0.00003424
Iteration 63/1000 | Loss: 0.00004015
Iteration 64/1000 | Loss: 0.00006419
Iteration 65/1000 | Loss: 0.00004575
Iteration 66/1000 | Loss: 0.00004048
Iteration 67/1000 | Loss: 0.00003867
Iteration 68/1000 | Loss: 0.00004472
Iteration 69/1000 | Loss: 0.00003784
Iteration 70/1000 | Loss: 0.00031518
Iteration 71/1000 | Loss: 0.00004374
Iteration 72/1000 | Loss: 0.00002976
Iteration 73/1000 | Loss: 0.00002358
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002083
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001734
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001537
Iteration 81/1000 | Loss: 0.00001493
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001441
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001424
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001419
Iteration 121/1000 | Loss: 0.00001419
Iteration 122/1000 | Loss: 0.00001419
Iteration 123/1000 | Loss: 0.00001418
Iteration 124/1000 | Loss: 0.00001418
Iteration 125/1000 | Loss: 0.00001418
Iteration 126/1000 | Loss: 0.00001417
Iteration 127/1000 | Loss: 0.00001417
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001417
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001415
Iteration 134/1000 | Loss: 0.00001415
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001413
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001412
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Iteration 144/1000 | Loss: 0.00001411
Iteration 145/1000 | Loss: 0.00001411
Iteration 146/1000 | Loss: 0.00001410
Iteration 147/1000 | Loss: 0.00001410
Iteration 148/1000 | Loss: 0.00001410
Iteration 149/1000 | Loss: 0.00001410
Iteration 150/1000 | Loss: 0.00001410
Iteration 151/1000 | Loss: 0.00001410
Iteration 152/1000 | Loss: 0.00001410
Iteration 153/1000 | Loss: 0.00001409
Iteration 154/1000 | Loss: 0.00001409
Iteration 155/1000 | Loss: 0.00001409
Iteration 156/1000 | Loss: 0.00001409
Iteration 157/1000 | Loss: 0.00001409
Iteration 158/1000 | Loss: 0.00001409
Iteration 159/1000 | Loss: 0.00001409
Iteration 160/1000 | Loss: 0.00001409
Iteration 161/1000 | Loss: 0.00001409
Iteration 162/1000 | Loss: 0.00001409
Iteration 163/1000 | Loss: 0.00001409
Iteration 164/1000 | Loss: 0.00001409
Iteration 165/1000 | Loss: 0.00001409
Iteration 166/1000 | Loss: 0.00001409
Iteration 167/1000 | Loss: 0.00001409
Iteration 168/1000 | Loss: 0.00001408
Iteration 169/1000 | Loss: 0.00001408
Iteration 170/1000 | Loss: 0.00001408
Iteration 171/1000 | Loss: 0.00001408
Iteration 172/1000 | Loss: 0.00001408
Iteration 173/1000 | Loss: 0.00001408
Iteration 174/1000 | Loss: 0.00001408
Iteration 175/1000 | Loss: 0.00001408
Iteration 176/1000 | Loss: 0.00001408
Iteration 177/1000 | Loss: 0.00001408
Iteration 178/1000 | Loss: 0.00001408
Iteration 179/1000 | Loss: 0.00001408
Iteration 180/1000 | Loss: 0.00001408
Iteration 181/1000 | Loss: 0.00001408
Iteration 182/1000 | Loss: 0.00001408
Iteration 183/1000 | Loss: 0.00001408
Iteration 184/1000 | Loss: 0.00001408
Iteration 185/1000 | Loss: 0.00001407
Iteration 186/1000 | Loss: 0.00001407
Iteration 187/1000 | Loss: 0.00001407
Iteration 188/1000 | Loss: 0.00001407
Iteration 189/1000 | Loss: 0.00001407
Iteration 190/1000 | Loss: 0.00001407
Iteration 191/1000 | Loss: 0.00001407
Iteration 192/1000 | Loss: 0.00001407
Iteration 193/1000 | Loss: 0.00001407
Iteration 194/1000 | Loss: 0.00001407
Iteration 195/1000 | Loss: 0.00001407
Iteration 196/1000 | Loss: 0.00001407
Iteration 197/1000 | Loss: 0.00001407
Iteration 198/1000 | Loss: 0.00001407
Iteration 199/1000 | Loss: 0.00001407
Iteration 200/1000 | Loss: 0.00001407
Iteration 201/1000 | Loss: 0.00001407
Iteration 202/1000 | Loss: 0.00001407
Iteration 203/1000 | Loss: 0.00001407
Iteration 204/1000 | Loss: 0.00001406
Iteration 205/1000 | Loss: 0.00001406
Iteration 206/1000 | Loss: 0.00001406
Iteration 207/1000 | Loss: 0.00001406
Iteration 208/1000 | Loss: 0.00001406
Iteration 209/1000 | Loss: 0.00001406
Iteration 210/1000 | Loss: 0.00001406
Iteration 211/1000 | Loss: 0.00001406
Iteration 212/1000 | Loss: 0.00001406
Iteration 213/1000 | Loss: 0.00001406
Iteration 214/1000 | Loss: 0.00001406
Iteration 215/1000 | Loss: 0.00001406
Iteration 216/1000 | Loss: 0.00001406
Iteration 217/1000 | Loss: 0.00001406
Iteration 218/1000 | Loss: 0.00001406
Iteration 219/1000 | Loss: 0.00001406
Iteration 220/1000 | Loss: 0.00001406
Iteration 221/1000 | Loss: 0.00001406
Iteration 222/1000 | Loss: 0.00001406
Iteration 223/1000 | Loss: 0.00001406
Iteration 224/1000 | Loss: 0.00001406
Iteration 225/1000 | Loss: 0.00001406
Iteration 226/1000 | Loss: 0.00001406
Iteration 227/1000 | Loss: 0.00001406
Iteration 228/1000 | Loss: 0.00001406
Iteration 229/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.4057763110031374e-05, 1.4057763110031374e-05, 1.4057763110031374e-05, 1.4057763110031374e-05, 1.4057763110031374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4057763110031374e-05

Optimization complete. Final v2v error: 3.082555055618286 mm

Highest mean error: 4.6056389808654785 mm for frame 67

Lowest mean error: 2.662578821182251 mm for frame 10

Saving results

Total time: 172.345778465271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798656
Iteration 2/25 | Loss: 0.00150235
Iteration 3/25 | Loss: 0.00098850
Iteration 4/25 | Loss: 0.00093467
Iteration 5/25 | Loss: 0.00088100
Iteration 6/25 | Loss: 0.00097800
Iteration 7/25 | Loss: 0.00087769
Iteration 8/25 | Loss: 0.00081329
Iteration 9/25 | Loss: 0.00079646
Iteration 10/25 | Loss: 0.00079749
Iteration 11/25 | Loss: 0.00077313
Iteration 12/25 | Loss: 0.00077098
Iteration 13/25 | Loss: 0.00077143
Iteration 14/25 | Loss: 0.00076842
Iteration 15/25 | Loss: 0.00076669
Iteration 16/25 | Loss: 0.00076730
Iteration 17/25 | Loss: 0.00076603
Iteration 18/25 | Loss: 0.00076253
Iteration 19/25 | Loss: 0.00076139
Iteration 20/25 | Loss: 0.00076061
Iteration 21/25 | Loss: 0.00076272
Iteration 22/25 | Loss: 0.00076424
Iteration 23/25 | Loss: 0.00076059
Iteration 24/25 | Loss: 0.00075727
Iteration 25/25 | Loss: 0.00075836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78340721
Iteration 2/25 | Loss: 0.00092818
Iteration 3/25 | Loss: 0.00092818
Iteration 4/25 | Loss: 0.00092818
Iteration 5/25 | Loss: 0.00092818
Iteration 6/25 | Loss: 0.00092818
Iteration 7/25 | Loss: 0.00092818
Iteration 8/25 | Loss: 0.00092818
Iteration 9/25 | Loss: 0.00092818
Iteration 10/25 | Loss: 0.00092818
Iteration 11/25 | Loss: 0.00092818
Iteration 12/25 | Loss: 0.00092818
Iteration 13/25 | Loss: 0.00092818
Iteration 14/25 | Loss: 0.00092818
Iteration 15/25 | Loss: 0.00092818
Iteration 16/25 | Loss: 0.00092818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000928181572817266, 0.000928181572817266, 0.000928181572817266, 0.000928181572817266, 0.000928181572817266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000928181572817266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092818
Iteration 2/1000 | Loss: 0.00004076
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00002260
Iteration 5/1000 | Loss: 0.00015630
Iteration 6/1000 | Loss: 0.00009334
Iteration 7/1000 | Loss: 0.00013003
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001683
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001555
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001533
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001529
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001527
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001522
Iteration 28/1000 | Loss: 0.00001521
Iteration 29/1000 | Loss: 0.00001518
Iteration 30/1000 | Loss: 0.00001512
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001505
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001500
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001498
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001495
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001495
Iteration 65/1000 | Loss: 0.00001494
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001494
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001493
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001492
Iteration 75/1000 | Loss: 0.00001492
Iteration 76/1000 | Loss: 0.00001492
Iteration 77/1000 | Loss: 0.00001492
Iteration 78/1000 | Loss: 0.00001492
Iteration 79/1000 | Loss: 0.00001492
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001491
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001486
Iteration 114/1000 | Loss: 0.00001486
Iteration 115/1000 | Loss: 0.00001486
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001485
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001485
Iteration 127/1000 | Loss: 0.00001485
Iteration 128/1000 | Loss: 0.00001485
Iteration 129/1000 | Loss: 0.00001485
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001484
Iteration 132/1000 | Loss: 0.00001484
Iteration 133/1000 | Loss: 0.00001484
Iteration 134/1000 | Loss: 0.00001484
Iteration 135/1000 | Loss: 0.00001484
Iteration 136/1000 | Loss: 0.00001484
Iteration 137/1000 | Loss: 0.00001484
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001483
Iteration 140/1000 | Loss: 0.00001483
Iteration 141/1000 | Loss: 0.00001483
Iteration 142/1000 | Loss: 0.00001483
Iteration 143/1000 | Loss: 0.00001483
Iteration 144/1000 | Loss: 0.00001483
Iteration 145/1000 | Loss: 0.00001483
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001483
Iteration 148/1000 | Loss: 0.00001482
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001481
Iteration 154/1000 | Loss: 0.00001481
Iteration 155/1000 | Loss: 0.00001481
Iteration 156/1000 | Loss: 0.00001481
Iteration 157/1000 | Loss: 0.00001481
Iteration 158/1000 | Loss: 0.00001481
Iteration 159/1000 | Loss: 0.00001481
Iteration 160/1000 | Loss: 0.00001481
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001480
Iteration 164/1000 | Loss: 0.00001480
Iteration 165/1000 | Loss: 0.00001480
Iteration 166/1000 | Loss: 0.00001480
Iteration 167/1000 | Loss: 0.00001480
Iteration 168/1000 | Loss: 0.00001480
Iteration 169/1000 | Loss: 0.00001480
Iteration 170/1000 | Loss: 0.00001480
Iteration 171/1000 | Loss: 0.00001480
Iteration 172/1000 | Loss: 0.00001480
Iteration 173/1000 | Loss: 0.00001480
Iteration 174/1000 | Loss: 0.00001480
Iteration 175/1000 | Loss: 0.00001480
Iteration 176/1000 | Loss: 0.00001480
Iteration 177/1000 | Loss: 0.00001480
Iteration 178/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.480207902204711e-05, 1.480207902204711e-05, 1.480207902204711e-05, 1.480207902204711e-05, 1.480207902204711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.480207902204711e-05

Optimization complete. Final v2v error: 3.189004898071289 mm

Highest mean error: 4.600318431854248 mm for frame 72

Lowest mean error: 2.5683889389038086 mm for frame 235

Saving results

Total time: 95.29330468177795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398523
Iteration 2/25 | Loss: 0.00090433
Iteration 3/25 | Loss: 0.00073859
Iteration 4/25 | Loss: 0.00071775
Iteration 5/25 | Loss: 0.00071213
Iteration 6/25 | Loss: 0.00071037
Iteration 7/25 | Loss: 0.00070998
Iteration 8/25 | Loss: 0.00070998
Iteration 9/25 | Loss: 0.00070998
Iteration 10/25 | Loss: 0.00070998
Iteration 11/25 | Loss: 0.00070998
Iteration 12/25 | Loss: 0.00070998
Iteration 13/25 | Loss: 0.00070998
Iteration 14/25 | Loss: 0.00070998
Iteration 15/25 | Loss: 0.00070998
Iteration 16/25 | Loss: 0.00070998
Iteration 17/25 | Loss: 0.00070998
Iteration 18/25 | Loss: 0.00070998
Iteration 19/25 | Loss: 0.00070998
Iteration 20/25 | Loss: 0.00070998
Iteration 21/25 | Loss: 0.00070998
Iteration 22/25 | Loss: 0.00070998
Iteration 23/25 | Loss: 0.00070998
Iteration 24/25 | Loss: 0.00070998
Iteration 25/25 | Loss: 0.00070998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52321172
Iteration 2/25 | Loss: 0.00077845
Iteration 3/25 | Loss: 0.00077845
Iteration 4/25 | Loss: 0.00077845
Iteration 5/25 | Loss: 0.00077845
Iteration 6/25 | Loss: 0.00077845
Iteration 7/25 | Loss: 0.00077844
Iteration 8/25 | Loss: 0.00077844
Iteration 9/25 | Loss: 0.00077844
Iteration 10/25 | Loss: 0.00077844
Iteration 11/25 | Loss: 0.00077844
Iteration 12/25 | Loss: 0.00077844
Iteration 13/25 | Loss: 0.00077844
Iteration 14/25 | Loss: 0.00077844
Iteration 15/25 | Loss: 0.00077844
Iteration 16/25 | Loss: 0.00077844
Iteration 17/25 | Loss: 0.00077844
Iteration 18/25 | Loss: 0.00077844
Iteration 19/25 | Loss: 0.00077844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000778443762101233, 0.000778443762101233, 0.000778443762101233, 0.000778443762101233, 0.000778443762101233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000778443762101233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077844
Iteration 2/1000 | Loss: 0.00002650
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001621
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001585
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001560
Iteration 15/1000 | Loss: 0.00001560
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001549
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001546
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001546
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001543
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001543
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001540
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001540
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001540
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001539
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001536
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001534
Iteration 68/1000 | Loss: 0.00001534
Iteration 69/1000 | Loss: 0.00001534
Iteration 70/1000 | Loss: 0.00001534
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001533
Iteration 73/1000 | Loss: 0.00001533
Iteration 74/1000 | Loss: 0.00001533
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001532
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001532
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001531
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001531
Iteration 84/1000 | Loss: 0.00001531
Iteration 85/1000 | Loss: 0.00001530
Iteration 86/1000 | Loss: 0.00001530
Iteration 87/1000 | Loss: 0.00001530
Iteration 88/1000 | Loss: 0.00001530
Iteration 89/1000 | Loss: 0.00001530
Iteration 90/1000 | Loss: 0.00001529
Iteration 91/1000 | Loss: 0.00001529
Iteration 92/1000 | Loss: 0.00001529
Iteration 93/1000 | Loss: 0.00001529
Iteration 94/1000 | Loss: 0.00001529
Iteration 95/1000 | Loss: 0.00001529
Iteration 96/1000 | Loss: 0.00001529
Iteration 97/1000 | Loss: 0.00001528
Iteration 98/1000 | Loss: 0.00001528
Iteration 99/1000 | Loss: 0.00001528
Iteration 100/1000 | Loss: 0.00001528
Iteration 101/1000 | Loss: 0.00001528
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001527
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001525
Iteration 125/1000 | Loss: 0.00001524
Iteration 126/1000 | Loss: 0.00001524
Iteration 127/1000 | Loss: 0.00001524
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Iteration 141/1000 | Loss: 0.00001523
Iteration 142/1000 | Loss: 0.00001523
Iteration 143/1000 | Loss: 0.00001523
Iteration 144/1000 | Loss: 0.00001523
Iteration 145/1000 | Loss: 0.00001523
Iteration 146/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.5227879885060247e-05, 1.5227879885060247e-05, 1.5227879885060247e-05, 1.5227879885060247e-05, 1.5227879885060247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5227879885060247e-05

Optimization complete. Final v2v error: 3.278992176055908 mm

Highest mean error: 3.577535629272461 mm for frame 114

Lowest mean error: 2.9964334964752197 mm for frame 5

Saving results

Total time: 37.03455400466919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611673
Iteration 2/25 | Loss: 0.00099077
Iteration 3/25 | Loss: 0.00086254
Iteration 4/25 | Loss: 0.00084100
Iteration 5/25 | Loss: 0.00083374
Iteration 6/25 | Loss: 0.00083218
Iteration 7/25 | Loss: 0.00083217
Iteration 8/25 | Loss: 0.00083217
Iteration 9/25 | Loss: 0.00083217
Iteration 10/25 | Loss: 0.00083217
Iteration 11/25 | Loss: 0.00083217
Iteration 12/25 | Loss: 0.00083217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008321661734953523, 0.0008321661734953523, 0.0008321661734953523, 0.0008321661734953523, 0.0008321661734953523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008321661734953523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53030717
Iteration 2/25 | Loss: 0.00089501
Iteration 3/25 | Loss: 0.00089497
Iteration 4/25 | Loss: 0.00089497
Iteration 5/25 | Loss: 0.00089497
Iteration 6/25 | Loss: 0.00089497
Iteration 7/25 | Loss: 0.00089497
Iteration 8/25 | Loss: 0.00089497
Iteration 9/25 | Loss: 0.00089497
Iteration 10/25 | Loss: 0.00089497
Iteration 11/25 | Loss: 0.00089497
Iteration 12/25 | Loss: 0.00089497
Iteration 13/25 | Loss: 0.00089497
Iteration 14/25 | Loss: 0.00089497
Iteration 15/25 | Loss: 0.00089497
Iteration 16/25 | Loss: 0.00089497
Iteration 17/25 | Loss: 0.00089497
Iteration 18/25 | Loss: 0.00089497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008949690964072943, 0.0008949690964072943, 0.0008949690964072943, 0.0008949690964072943, 0.0008949690964072943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008949690964072943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089497
Iteration 2/1000 | Loss: 0.00004322
Iteration 3/1000 | Loss: 0.00002691
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002116
Iteration 7/1000 | Loss: 0.00002063
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001983
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001962
Iteration 15/1000 | Loss: 0.00001957
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00001950
Iteration 20/1000 | Loss: 0.00001950
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00001943
Iteration 35/1000 | Loss: 0.00001943
Iteration 36/1000 | Loss: 0.00001943
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001940
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001938
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001937
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001937
Iteration 63/1000 | Loss: 0.00001937
Iteration 64/1000 | Loss: 0.00001937
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001937
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001936
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001935
Iteration 76/1000 | Loss: 0.00001935
Iteration 77/1000 | Loss: 0.00001935
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001934
Iteration 83/1000 | Loss: 0.00001934
Iteration 84/1000 | Loss: 0.00001934
Iteration 85/1000 | Loss: 0.00001934
Iteration 86/1000 | Loss: 0.00001934
Iteration 87/1000 | Loss: 0.00001934
Iteration 88/1000 | Loss: 0.00001933
Iteration 89/1000 | Loss: 0.00001933
Iteration 90/1000 | Loss: 0.00001933
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001932
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001930
Iteration 99/1000 | Loss: 0.00001930
Iteration 100/1000 | Loss: 0.00001930
Iteration 101/1000 | Loss: 0.00001930
Iteration 102/1000 | Loss: 0.00001929
Iteration 103/1000 | Loss: 0.00001929
Iteration 104/1000 | Loss: 0.00001929
Iteration 105/1000 | Loss: 0.00001929
Iteration 106/1000 | Loss: 0.00001929
Iteration 107/1000 | Loss: 0.00001928
Iteration 108/1000 | Loss: 0.00001928
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001928
Iteration 111/1000 | Loss: 0.00001928
Iteration 112/1000 | Loss: 0.00001928
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001928
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001927
Iteration 118/1000 | Loss: 0.00001927
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001927
Iteration 127/1000 | Loss: 0.00001927
Iteration 128/1000 | Loss: 0.00001927
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001927
Iteration 132/1000 | Loss: 0.00001926
Iteration 133/1000 | Loss: 0.00001926
Iteration 134/1000 | Loss: 0.00001926
Iteration 135/1000 | Loss: 0.00001926
Iteration 136/1000 | Loss: 0.00001926
Iteration 137/1000 | Loss: 0.00001926
Iteration 138/1000 | Loss: 0.00001926
Iteration 139/1000 | Loss: 0.00001926
Iteration 140/1000 | Loss: 0.00001926
Iteration 141/1000 | Loss: 0.00001926
Iteration 142/1000 | Loss: 0.00001926
Iteration 143/1000 | Loss: 0.00001925
Iteration 144/1000 | Loss: 0.00001925
Iteration 145/1000 | Loss: 0.00001925
Iteration 146/1000 | Loss: 0.00001925
Iteration 147/1000 | Loss: 0.00001925
Iteration 148/1000 | Loss: 0.00001925
Iteration 149/1000 | Loss: 0.00001925
Iteration 150/1000 | Loss: 0.00001925
Iteration 151/1000 | Loss: 0.00001925
Iteration 152/1000 | Loss: 0.00001924
Iteration 153/1000 | Loss: 0.00001924
Iteration 154/1000 | Loss: 0.00001924
Iteration 155/1000 | Loss: 0.00001924
Iteration 156/1000 | Loss: 0.00001924
Iteration 157/1000 | Loss: 0.00001924
Iteration 158/1000 | Loss: 0.00001924
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001923
Iteration 161/1000 | Loss: 0.00001923
Iteration 162/1000 | Loss: 0.00001923
Iteration 163/1000 | Loss: 0.00001923
Iteration 164/1000 | Loss: 0.00001923
Iteration 165/1000 | Loss: 0.00001923
Iteration 166/1000 | Loss: 0.00001922
Iteration 167/1000 | Loss: 0.00001922
Iteration 168/1000 | Loss: 0.00001922
Iteration 169/1000 | Loss: 0.00001922
Iteration 170/1000 | Loss: 0.00001922
Iteration 171/1000 | Loss: 0.00001922
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001922
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001922
Iteration 178/1000 | Loss: 0.00001922
Iteration 179/1000 | Loss: 0.00001922
Iteration 180/1000 | Loss: 0.00001922
Iteration 181/1000 | Loss: 0.00001922
Iteration 182/1000 | Loss: 0.00001922
Iteration 183/1000 | Loss: 0.00001922
Iteration 184/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.9219720343244262e-05, 1.9219720343244262e-05, 1.9219720343244262e-05, 1.9219720343244262e-05, 1.9219720343244262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9219720343244262e-05

Optimization complete. Final v2v error: 3.6870830059051514 mm

Highest mean error: 4.128546714782715 mm for frame 215

Lowest mean error: 3.1578006744384766 mm for frame 239

Saving results

Total time: 43.006771087646484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413362
Iteration 2/25 | Loss: 0.00083539
Iteration 3/25 | Loss: 0.00075803
Iteration 4/25 | Loss: 0.00073959
Iteration 5/25 | Loss: 0.00073478
Iteration 6/25 | Loss: 0.00073337
Iteration 7/25 | Loss: 0.00073320
Iteration 8/25 | Loss: 0.00073320
Iteration 9/25 | Loss: 0.00073320
Iteration 10/25 | Loss: 0.00073320
Iteration 11/25 | Loss: 0.00073320
Iteration 12/25 | Loss: 0.00073320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007332042441703379, 0.0007332042441703379, 0.0007332042441703379, 0.0007332042441703379, 0.0007332042441703379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007332042441703379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.32240582
Iteration 2/25 | Loss: 0.00085805
Iteration 3/25 | Loss: 0.00085805
Iteration 4/25 | Loss: 0.00085805
Iteration 5/25 | Loss: 0.00085805
Iteration 6/25 | Loss: 0.00085805
Iteration 7/25 | Loss: 0.00085805
Iteration 8/25 | Loss: 0.00085805
Iteration 9/25 | Loss: 0.00085805
Iteration 10/25 | Loss: 0.00085805
Iteration 11/25 | Loss: 0.00085805
Iteration 12/25 | Loss: 0.00085805
Iteration 13/25 | Loss: 0.00085805
Iteration 14/25 | Loss: 0.00085805
Iteration 15/25 | Loss: 0.00085805
Iteration 16/25 | Loss: 0.00085805
Iteration 17/25 | Loss: 0.00085805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008580460562370718, 0.0008580460562370718, 0.0008580460562370718, 0.0008580460562370718, 0.0008580460562370718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008580460562370718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085805
Iteration 2/1000 | Loss: 0.00003152
Iteration 3/1000 | Loss: 0.00002219
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001839
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001839
Iteration 18/1000 | Loss: 0.00001838
Iteration 19/1000 | Loss: 0.00001837
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001835
Iteration 22/1000 | Loss: 0.00001835
Iteration 23/1000 | Loss: 0.00001835
Iteration 24/1000 | Loss: 0.00001835
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001831
Iteration 30/1000 | Loss: 0.00001830
Iteration 31/1000 | Loss: 0.00001829
Iteration 32/1000 | Loss: 0.00001825
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001824
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001817
Iteration 37/1000 | Loss: 0.00001816
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001810
Iteration 48/1000 | Loss: 0.00001810
Iteration 49/1000 | Loss: 0.00001810
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001808
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.8080734662362374e-05, 1.8080734662362374e-05, 1.8080734662362374e-05, 1.8080734662362374e-05, 1.8080734662362374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8080734662362374e-05

Optimization complete. Final v2v error: 3.5994632244110107 mm

Highest mean error: 3.865617513656616 mm for frame 169

Lowest mean error: 3.396388053894043 mm for frame 11

Saving results

Total time: 35.62523293495178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481879
Iteration 2/25 | Loss: 0.00089381
Iteration 3/25 | Loss: 0.00076220
Iteration 4/25 | Loss: 0.00073568
Iteration 5/25 | Loss: 0.00073072
Iteration 6/25 | Loss: 0.00072999
Iteration 7/25 | Loss: 0.00072999
Iteration 8/25 | Loss: 0.00072999
Iteration 9/25 | Loss: 0.00072999
Iteration 10/25 | Loss: 0.00072999
Iteration 11/25 | Loss: 0.00072999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007299924618564546, 0.0007299924618564546, 0.0007299924618564546, 0.0007299924618564546, 0.0007299924618564546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007299924618564546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.08196449
Iteration 2/25 | Loss: 0.00078556
Iteration 3/25 | Loss: 0.00078556
Iteration 4/25 | Loss: 0.00078556
Iteration 5/25 | Loss: 0.00078556
Iteration 6/25 | Loss: 0.00078556
Iteration 7/25 | Loss: 0.00078556
Iteration 8/25 | Loss: 0.00078556
Iteration 9/25 | Loss: 0.00078556
Iteration 10/25 | Loss: 0.00078556
Iteration 11/25 | Loss: 0.00078556
Iteration 12/25 | Loss: 0.00078556
Iteration 13/25 | Loss: 0.00078556
Iteration 14/25 | Loss: 0.00078556
Iteration 15/25 | Loss: 0.00078556
Iteration 16/25 | Loss: 0.00078556
Iteration 17/25 | Loss: 0.00078556
Iteration 18/25 | Loss: 0.00078556
Iteration 19/25 | Loss: 0.00078556
Iteration 20/25 | Loss: 0.00078556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007855577277950943, 0.0007855577277950943, 0.0007855577277950943, 0.0007855577277950943, 0.0007855577277950943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007855577277950943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078556
Iteration 2/1000 | Loss: 0.00002428
Iteration 3/1000 | Loss: 0.00001780
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001550
Iteration 7/1000 | Loss: 0.00001523
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001478
Iteration 11/1000 | Loss: 0.00001469
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001453
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001450
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001442
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001437
Iteration 44/1000 | Loss: 0.00001437
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001437
Iteration 48/1000 | Loss: 0.00001437
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001433
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001419
Iteration 76/1000 | Loss: 0.00001419
Iteration 77/1000 | Loss: 0.00001418
Iteration 78/1000 | Loss: 0.00001418
Iteration 79/1000 | Loss: 0.00001418
Iteration 80/1000 | Loss: 0.00001417
Iteration 81/1000 | Loss: 0.00001417
Iteration 82/1000 | Loss: 0.00001417
Iteration 83/1000 | Loss: 0.00001417
Iteration 84/1000 | Loss: 0.00001417
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001416
Iteration 88/1000 | Loss: 0.00001416
Iteration 89/1000 | Loss: 0.00001416
Iteration 90/1000 | Loss: 0.00001416
Iteration 91/1000 | Loss: 0.00001416
Iteration 92/1000 | Loss: 0.00001416
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001414
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001414
Iteration 110/1000 | Loss: 0.00001414
Iteration 111/1000 | Loss: 0.00001414
Iteration 112/1000 | Loss: 0.00001414
Iteration 113/1000 | Loss: 0.00001414
Iteration 114/1000 | Loss: 0.00001414
Iteration 115/1000 | Loss: 0.00001414
Iteration 116/1000 | Loss: 0.00001414
Iteration 117/1000 | Loss: 0.00001414
Iteration 118/1000 | Loss: 0.00001414
Iteration 119/1000 | Loss: 0.00001414
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4142539839667734e-05, 1.4142539839667734e-05, 1.4142539839667734e-05, 1.4142539839667734e-05, 1.4142539839667734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4142539839667734e-05

Optimization complete. Final v2v error: 3.1996328830718994 mm

Highest mean error: 3.4181389808654785 mm for frame 80

Lowest mean error: 3.059701919555664 mm for frame 253

Saving results

Total time: 38.386253356933594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881830
Iteration 2/25 | Loss: 0.00081514
Iteration 3/25 | Loss: 0.00070776
Iteration 4/25 | Loss: 0.00068570
Iteration 5/25 | Loss: 0.00067911
Iteration 6/25 | Loss: 0.00067763
Iteration 7/25 | Loss: 0.00067722
Iteration 8/25 | Loss: 0.00067722
Iteration 9/25 | Loss: 0.00067722
Iteration 10/25 | Loss: 0.00067722
Iteration 11/25 | Loss: 0.00067722
Iteration 12/25 | Loss: 0.00067722
Iteration 13/25 | Loss: 0.00067722
Iteration 14/25 | Loss: 0.00067722
Iteration 15/25 | Loss: 0.00067722
Iteration 16/25 | Loss: 0.00067722
Iteration 17/25 | Loss: 0.00067722
Iteration 18/25 | Loss: 0.00067722
Iteration 19/25 | Loss: 0.00067722
Iteration 20/25 | Loss: 0.00067722
Iteration 21/25 | Loss: 0.00067722
Iteration 22/25 | Loss: 0.00067722
Iteration 23/25 | Loss: 0.00067722
Iteration 24/25 | Loss: 0.00067722
Iteration 25/25 | Loss: 0.00067722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78905272
Iteration 2/25 | Loss: 0.00074128
Iteration 3/25 | Loss: 0.00074128
Iteration 4/25 | Loss: 0.00074128
Iteration 5/25 | Loss: 0.00074128
Iteration 6/25 | Loss: 0.00074127
Iteration 7/25 | Loss: 0.00074127
Iteration 8/25 | Loss: 0.00074127
Iteration 9/25 | Loss: 0.00074127
Iteration 10/25 | Loss: 0.00074127
Iteration 11/25 | Loss: 0.00074127
Iteration 12/25 | Loss: 0.00074127
Iteration 13/25 | Loss: 0.00074127
Iteration 14/25 | Loss: 0.00074127
Iteration 15/25 | Loss: 0.00074127
Iteration 16/25 | Loss: 0.00074127
Iteration 17/25 | Loss: 0.00074127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007412734557874501, 0.0007412734557874501, 0.0007412734557874501, 0.0007412734557874501, 0.0007412734557874501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007412734557874501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074127
Iteration 2/1000 | Loss: 0.00002258
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001322
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001236
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001190
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001177
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001177
Iteration 110/1000 | Loss: 0.00001176
Iteration 111/1000 | Loss: 0.00001176
Iteration 112/1000 | Loss: 0.00001176
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001174
Iteration 124/1000 | Loss: 0.00001174
Iteration 125/1000 | Loss: 0.00001174
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001173
Iteration 130/1000 | Loss: 0.00001173
Iteration 131/1000 | Loss: 0.00001173
Iteration 132/1000 | Loss: 0.00001173
Iteration 133/1000 | Loss: 0.00001173
Iteration 134/1000 | Loss: 0.00001173
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001170
Iteration 161/1000 | Loss: 0.00001170
Iteration 162/1000 | Loss: 0.00001170
Iteration 163/1000 | Loss: 0.00001170
Iteration 164/1000 | Loss: 0.00001170
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001170
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001169
Iteration 170/1000 | Loss: 0.00001169
Iteration 171/1000 | Loss: 0.00001169
Iteration 172/1000 | Loss: 0.00001169
Iteration 173/1000 | Loss: 0.00001169
Iteration 174/1000 | Loss: 0.00001169
Iteration 175/1000 | Loss: 0.00001169
Iteration 176/1000 | Loss: 0.00001169
Iteration 177/1000 | Loss: 0.00001169
Iteration 178/1000 | Loss: 0.00001169
Iteration 179/1000 | Loss: 0.00001169
Iteration 180/1000 | Loss: 0.00001169
Iteration 181/1000 | Loss: 0.00001169
Iteration 182/1000 | Loss: 0.00001169
Iteration 183/1000 | Loss: 0.00001169
Iteration 184/1000 | Loss: 0.00001168
Iteration 185/1000 | Loss: 0.00001168
Iteration 186/1000 | Loss: 0.00001168
Iteration 187/1000 | Loss: 0.00001168
Iteration 188/1000 | Loss: 0.00001168
Iteration 189/1000 | Loss: 0.00001168
Iteration 190/1000 | Loss: 0.00001168
Iteration 191/1000 | Loss: 0.00001168
Iteration 192/1000 | Loss: 0.00001168
Iteration 193/1000 | Loss: 0.00001168
Iteration 194/1000 | Loss: 0.00001168
Iteration 195/1000 | Loss: 0.00001168
Iteration 196/1000 | Loss: 0.00001168
Iteration 197/1000 | Loss: 0.00001168
Iteration 198/1000 | Loss: 0.00001168
Iteration 199/1000 | Loss: 0.00001168
Iteration 200/1000 | Loss: 0.00001168
Iteration 201/1000 | Loss: 0.00001168
Iteration 202/1000 | Loss: 0.00001168
Iteration 203/1000 | Loss: 0.00001168
Iteration 204/1000 | Loss: 0.00001168
Iteration 205/1000 | Loss: 0.00001168
Iteration 206/1000 | Loss: 0.00001168
Iteration 207/1000 | Loss: 0.00001168
Iteration 208/1000 | Loss: 0.00001168
Iteration 209/1000 | Loss: 0.00001168
Iteration 210/1000 | Loss: 0.00001168
Iteration 211/1000 | Loss: 0.00001168
Iteration 212/1000 | Loss: 0.00001168
Iteration 213/1000 | Loss: 0.00001168
Iteration 214/1000 | Loss: 0.00001168
Iteration 215/1000 | Loss: 0.00001168
Iteration 216/1000 | Loss: 0.00001168
Iteration 217/1000 | Loss: 0.00001168
Iteration 218/1000 | Loss: 0.00001168
Iteration 219/1000 | Loss: 0.00001168
Iteration 220/1000 | Loss: 0.00001168
Iteration 221/1000 | Loss: 0.00001168
Iteration 222/1000 | Loss: 0.00001168
Iteration 223/1000 | Loss: 0.00001168
Iteration 224/1000 | Loss: 0.00001168
Iteration 225/1000 | Loss: 0.00001168
Iteration 226/1000 | Loss: 0.00001168
Iteration 227/1000 | Loss: 0.00001168
Iteration 228/1000 | Loss: 0.00001168
Iteration 229/1000 | Loss: 0.00001168
Iteration 230/1000 | Loss: 0.00001168
Iteration 231/1000 | Loss: 0.00001168
Iteration 232/1000 | Loss: 0.00001168
Iteration 233/1000 | Loss: 0.00001168
Iteration 234/1000 | Loss: 0.00001168
Iteration 235/1000 | Loss: 0.00001168
Iteration 236/1000 | Loss: 0.00001168
Iteration 237/1000 | Loss: 0.00001168
Iteration 238/1000 | Loss: 0.00001168
Iteration 239/1000 | Loss: 0.00001168
Iteration 240/1000 | Loss: 0.00001168
Iteration 241/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.1681142495945096e-05, 1.1681142495945096e-05, 1.1681142495945096e-05, 1.1681142495945096e-05, 1.1681142495945096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1681142495945096e-05

Optimization complete. Final v2v error: 2.930119037628174 mm

Highest mean error: 3.537527084350586 mm for frame 44

Lowest mean error: 2.811314821243286 mm for frame 98

Saving results

Total time: 38.28925633430481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842235
Iteration 2/25 | Loss: 0.00111721
Iteration 3/25 | Loss: 0.00082021
Iteration 4/25 | Loss: 0.00078043
Iteration 5/25 | Loss: 0.00077479
Iteration 6/25 | Loss: 0.00077395
Iteration 7/25 | Loss: 0.00077395
Iteration 8/25 | Loss: 0.00077395
Iteration 9/25 | Loss: 0.00077395
Iteration 10/25 | Loss: 0.00077395
Iteration 11/25 | Loss: 0.00077395
Iteration 12/25 | Loss: 0.00077395
Iteration 13/25 | Loss: 0.00077395
Iteration 14/25 | Loss: 0.00077395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007739468128420413, 0.0007739468128420413, 0.0007739468128420413, 0.0007739468128420413, 0.0007739468128420413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007739468128420413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41859674
Iteration 2/25 | Loss: 0.00082478
Iteration 3/25 | Loss: 0.00082477
Iteration 4/25 | Loss: 0.00082477
Iteration 5/25 | Loss: 0.00082477
Iteration 6/25 | Loss: 0.00082477
Iteration 7/25 | Loss: 0.00082477
Iteration 8/25 | Loss: 0.00082477
Iteration 9/25 | Loss: 0.00082477
Iteration 10/25 | Loss: 0.00082477
Iteration 11/25 | Loss: 0.00082477
Iteration 12/25 | Loss: 0.00082477
Iteration 13/25 | Loss: 0.00082477
Iteration 14/25 | Loss: 0.00082477
Iteration 15/25 | Loss: 0.00082477
Iteration 16/25 | Loss: 0.00082477
Iteration 17/25 | Loss: 0.00082477
Iteration 18/25 | Loss: 0.00082477
Iteration 19/25 | Loss: 0.00082477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008247673395089805, 0.0008247673395089805, 0.0008247673395089805, 0.0008247673395089805, 0.0008247673395089805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008247673395089805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082477
Iteration 2/1000 | Loss: 0.00002634
Iteration 3/1000 | Loss: 0.00002105
Iteration 4/1000 | Loss: 0.00001992
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001810
Iteration 7/1000 | Loss: 0.00001773
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001750
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00001741
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001727
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001706
Iteration 91/1000 | Loss: 0.00001706
Iteration 92/1000 | Loss: 0.00001706
Iteration 93/1000 | Loss: 0.00001706
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00001703
Iteration 115/1000 | Loss: 0.00001703
Iteration 116/1000 | Loss: 0.00001703
Iteration 117/1000 | Loss: 0.00001703
Iteration 118/1000 | Loss: 0.00001703
Iteration 119/1000 | Loss: 0.00001702
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001702
Iteration 123/1000 | Loss: 0.00001701
Iteration 124/1000 | Loss: 0.00001701
Iteration 125/1000 | Loss: 0.00001701
Iteration 126/1000 | Loss: 0.00001701
Iteration 127/1000 | Loss: 0.00001701
Iteration 128/1000 | Loss: 0.00001701
Iteration 129/1000 | Loss: 0.00001700
Iteration 130/1000 | Loss: 0.00001700
Iteration 131/1000 | Loss: 0.00001700
Iteration 132/1000 | Loss: 0.00001700
Iteration 133/1000 | Loss: 0.00001700
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001699
Iteration 142/1000 | Loss: 0.00001699
Iteration 143/1000 | Loss: 0.00001699
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001699
Iteration 147/1000 | Loss: 0.00001699
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001698
Iteration 156/1000 | Loss: 0.00001698
Iteration 157/1000 | Loss: 0.00001698
Iteration 158/1000 | Loss: 0.00001697
Iteration 159/1000 | Loss: 0.00001697
Iteration 160/1000 | Loss: 0.00001697
Iteration 161/1000 | Loss: 0.00001696
Iteration 162/1000 | Loss: 0.00001696
Iteration 163/1000 | Loss: 0.00001696
Iteration 164/1000 | Loss: 0.00001696
Iteration 165/1000 | Loss: 0.00001696
Iteration 166/1000 | Loss: 0.00001696
Iteration 167/1000 | Loss: 0.00001696
Iteration 168/1000 | Loss: 0.00001696
Iteration 169/1000 | Loss: 0.00001695
Iteration 170/1000 | Loss: 0.00001695
Iteration 171/1000 | Loss: 0.00001695
Iteration 172/1000 | Loss: 0.00001695
Iteration 173/1000 | Loss: 0.00001695
Iteration 174/1000 | Loss: 0.00001695
Iteration 175/1000 | Loss: 0.00001695
Iteration 176/1000 | Loss: 0.00001695
Iteration 177/1000 | Loss: 0.00001695
Iteration 178/1000 | Loss: 0.00001695
Iteration 179/1000 | Loss: 0.00001695
Iteration 180/1000 | Loss: 0.00001695
Iteration 181/1000 | Loss: 0.00001695
Iteration 182/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.69538107002154e-05, 1.69538107002154e-05, 1.69538107002154e-05, 1.69538107002154e-05, 1.69538107002154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.69538107002154e-05

Optimization complete. Final v2v error: 3.5040013790130615 mm

Highest mean error: 3.742302179336548 mm for frame 179

Lowest mean error: 3.3301610946655273 mm for frame 111

Saving results

Total time: 38.81477761268616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898533
Iteration 2/25 | Loss: 0.00163154
Iteration 3/25 | Loss: 0.00108925
Iteration 4/25 | Loss: 0.00099414
Iteration 5/25 | Loss: 0.00094700
Iteration 6/25 | Loss: 0.00101593
Iteration 7/25 | Loss: 0.00092948
Iteration 8/25 | Loss: 0.00088440
Iteration 9/25 | Loss: 0.00085543
Iteration 10/25 | Loss: 0.00086083
Iteration 11/25 | Loss: 0.00085661
Iteration 12/25 | Loss: 0.00083449
Iteration 13/25 | Loss: 0.00080516
Iteration 14/25 | Loss: 0.00080291
Iteration 15/25 | Loss: 0.00079486
Iteration 16/25 | Loss: 0.00079929
Iteration 17/25 | Loss: 0.00078740
Iteration 18/25 | Loss: 0.00078242
Iteration 19/25 | Loss: 0.00078208
Iteration 20/25 | Loss: 0.00078112
Iteration 21/25 | Loss: 0.00077989
Iteration 22/25 | Loss: 0.00077902
Iteration 23/25 | Loss: 0.00077868
Iteration 24/25 | Loss: 0.00077854
Iteration 25/25 | Loss: 0.00077853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.68517923
Iteration 2/25 | Loss: 0.00092814
Iteration 3/25 | Loss: 0.00092807
Iteration 4/25 | Loss: 0.00092807
Iteration 5/25 | Loss: 0.00092807
Iteration 6/25 | Loss: 0.00092807
Iteration 7/25 | Loss: 0.00092807
Iteration 8/25 | Loss: 0.00092807
Iteration 9/25 | Loss: 0.00092807
Iteration 10/25 | Loss: 0.00092807
Iteration 11/25 | Loss: 0.00092807
Iteration 12/25 | Loss: 0.00092807
Iteration 13/25 | Loss: 0.00092807
Iteration 14/25 | Loss: 0.00092807
Iteration 15/25 | Loss: 0.00092807
Iteration 16/25 | Loss: 0.00092807
Iteration 17/25 | Loss: 0.00092807
Iteration 18/25 | Loss: 0.00092807
Iteration 19/25 | Loss: 0.00092807
Iteration 20/25 | Loss: 0.00092807
Iteration 21/25 | Loss: 0.00092807
Iteration 22/25 | Loss: 0.00092807
Iteration 23/25 | Loss: 0.00092807
Iteration 24/25 | Loss: 0.00092807
Iteration 25/25 | Loss: 0.00092807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092807
Iteration 2/1000 | Loss: 0.00004013
Iteration 3/1000 | Loss: 0.00007945
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00002002
Iteration 6/1000 | Loss: 0.00013531
Iteration 7/1000 | Loss: 0.00011532
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002681
Iteration 10/1000 | Loss: 0.00001827
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001791
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001754
Iteration 26/1000 | Loss: 0.00001753
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001741
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001737
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001736
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001731
Iteration 48/1000 | Loss: 0.00001731
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001727
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001723
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001721
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001721
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001718
Iteration 122/1000 | Loss: 0.00001718
Iteration 123/1000 | Loss: 0.00001717
Iteration 124/1000 | Loss: 0.00001717
Iteration 125/1000 | Loss: 0.00001717
Iteration 126/1000 | Loss: 0.00001717
Iteration 127/1000 | Loss: 0.00001717
Iteration 128/1000 | Loss: 0.00001717
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00001717
Iteration 131/1000 | Loss: 0.00001717
Iteration 132/1000 | Loss: 0.00001716
Iteration 133/1000 | Loss: 0.00001716
Iteration 134/1000 | Loss: 0.00001716
Iteration 135/1000 | Loss: 0.00001716
Iteration 136/1000 | Loss: 0.00001716
Iteration 137/1000 | Loss: 0.00001716
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001715
Iteration 143/1000 | Loss: 0.00001715
Iteration 144/1000 | Loss: 0.00001715
Iteration 145/1000 | Loss: 0.00001715
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001714
Iteration 157/1000 | Loss: 0.00001714
Iteration 158/1000 | Loss: 0.00001714
Iteration 159/1000 | Loss: 0.00001714
Iteration 160/1000 | Loss: 0.00001714
Iteration 161/1000 | Loss: 0.00001714
Iteration 162/1000 | Loss: 0.00001714
Iteration 163/1000 | Loss: 0.00001714
Iteration 164/1000 | Loss: 0.00001714
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00001714
Iteration 167/1000 | Loss: 0.00001713
Iteration 168/1000 | Loss: 0.00001713
Iteration 169/1000 | Loss: 0.00001713
Iteration 170/1000 | Loss: 0.00001713
Iteration 171/1000 | Loss: 0.00001713
Iteration 172/1000 | Loss: 0.00001713
Iteration 173/1000 | Loss: 0.00001713
Iteration 174/1000 | Loss: 0.00001713
Iteration 175/1000 | Loss: 0.00001713
Iteration 176/1000 | Loss: 0.00001713
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001713
Iteration 181/1000 | Loss: 0.00001713
Iteration 182/1000 | Loss: 0.00001712
Iteration 183/1000 | Loss: 0.00001712
Iteration 184/1000 | Loss: 0.00001712
Iteration 185/1000 | Loss: 0.00001712
Iteration 186/1000 | Loss: 0.00001712
Iteration 187/1000 | Loss: 0.00001712
Iteration 188/1000 | Loss: 0.00001712
Iteration 189/1000 | Loss: 0.00001712
Iteration 190/1000 | Loss: 0.00001712
Iteration 191/1000 | Loss: 0.00001712
Iteration 192/1000 | Loss: 0.00001712
Iteration 193/1000 | Loss: 0.00001712
Iteration 194/1000 | Loss: 0.00001712
Iteration 195/1000 | Loss: 0.00001712
Iteration 196/1000 | Loss: 0.00001711
Iteration 197/1000 | Loss: 0.00001711
Iteration 198/1000 | Loss: 0.00001711
Iteration 199/1000 | Loss: 0.00001711
Iteration 200/1000 | Loss: 0.00001711
Iteration 201/1000 | Loss: 0.00001711
Iteration 202/1000 | Loss: 0.00001711
Iteration 203/1000 | Loss: 0.00001711
Iteration 204/1000 | Loss: 0.00001711
Iteration 205/1000 | Loss: 0.00001710
Iteration 206/1000 | Loss: 0.00001710
Iteration 207/1000 | Loss: 0.00001710
Iteration 208/1000 | Loss: 0.00001710
Iteration 209/1000 | Loss: 0.00001710
Iteration 210/1000 | Loss: 0.00001710
Iteration 211/1000 | Loss: 0.00001710
Iteration 212/1000 | Loss: 0.00001710
Iteration 213/1000 | Loss: 0.00001710
Iteration 214/1000 | Loss: 0.00001710
Iteration 215/1000 | Loss: 0.00001710
Iteration 216/1000 | Loss: 0.00001710
Iteration 217/1000 | Loss: 0.00001710
Iteration 218/1000 | Loss: 0.00001710
Iteration 219/1000 | Loss: 0.00001710
Iteration 220/1000 | Loss: 0.00001710
Iteration 221/1000 | Loss: 0.00001710
Iteration 222/1000 | Loss: 0.00001710
Iteration 223/1000 | Loss: 0.00001710
Iteration 224/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.7095841030823067e-05, 1.7095841030823067e-05, 1.7095841030823067e-05, 1.7095841030823067e-05, 1.7095841030823067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7095841030823067e-05

Optimization complete. Final v2v error: 3.4709291458129883 mm

Highest mean error: 4.099212646484375 mm for frame 167

Lowest mean error: 2.861867666244507 mm for frame 51

Saving results

Total time: 80.87739586830139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742001
Iteration 2/25 | Loss: 0.00165426
Iteration 3/25 | Loss: 0.00104388
Iteration 4/25 | Loss: 0.00088553
Iteration 5/25 | Loss: 0.00086932
Iteration 6/25 | Loss: 0.00085420
Iteration 7/25 | Loss: 0.00084094
Iteration 8/25 | Loss: 0.00083828
Iteration 9/25 | Loss: 0.00083783
Iteration 10/25 | Loss: 0.00083764
Iteration 11/25 | Loss: 0.00083749
Iteration 12/25 | Loss: 0.00083745
Iteration 13/25 | Loss: 0.00083745
Iteration 14/25 | Loss: 0.00083745
Iteration 15/25 | Loss: 0.00083745
Iteration 16/25 | Loss: 0.00083745
Iteration 17/25 | Loss: 0.00083745
Iteration 18/25 | Loss: 0.00083745
Iteration 19/25 | Loss: 0.00083745
Iteration 20/25 | Loss: 0.00083745
Iteration 21/25 | Loss: 0.00083745
Iteration 22/25 | Loss: 0.00083745
Iteration 23/25 | Loss: 0.00083745
Iteration 24/25 | Loss: 0.00083745
Iteration 25/25 | Loss: 0.00083745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.66412020
Iteration 2/25 | Loss: 0.00113485
Iteration 3/25 | Loss: 0.00113477
Iteration 4/25 | Loss: 0.00113477
Iteration 5/25 | Loss: 0.00113477
Iteration 6/25 | Loss: 0.00113477
Iteration 7/25 | Loss: 0.00113477
Iteration 8/25 | Loss: 0.00113477
Iteration 9/25 | Loss: 0.00113477
Iteration 10/25 | Loss: 0.00113477
Iteration 11/25 | Loss: 0.00113477
Iteration 12/25 | Loss: 0.00113477
Iteration 13/25 | Loss: 0.00113477
Iteration 14/25 | Loss: 0.00113477
Iteration 15/25 | Loss: 0.00113477
Iteration 16/25 | Loss: 0.00113477
Iteration 17/25 | Loss: 0.00113477
Iteration 18/25 | Loss: 0.00113477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011347660329192877, 0.0011347660329192877, 0.0011347660329192877, 0.0011347660329192877, 0.0011347660329192877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011347660329192877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113477
Iteration 2/1000 | Loss: 0.00004980
Iteration 3/1000 | Loss: 0.00003192
Iteration 4/1000 | Loss: 0.00002658
Iteration 5/1000 | Loss: 0.00002446
Iteration 6/1000 | Loss: 0.00030730
Iteration 7/1000 | Loss: 0.00002497
Iteration 8/1000 | Loss: 0.00002266
Iteration 9/1000 | Loss: 0.00002173
Iteration 10/1000 | Loss: 0.00002097
Iteration 11/1000 | Loss: 0.00002057
Iteration 12/1000 | Loss: 0.00002031
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00001998
Iteration 15/1000 | Loss: 0.00001993
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00001985
Iteration 20/1000 | Loss: 0.00001984
Iteration 21/1000 | Loss: 0.00001982
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001972
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001964
Iteration 28/1000 | Loss: 0.00001964
Iteration 29/1000 | Loss: 0.00001964
Iteration 30/1000 | Loss: 0.00001963
Iteration 31/1000 | Loss: 0.00001963
Iteration 32/1000 | Loss: 0.00001962
Iteration 33/1000 | Loss: 0.00001962
Iteration 34/1000 | Loss: 0.00001962
Iteration 35/1000 | Loss: 0.00001962
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001961
Iteration 38/1000 | Loss: 0.00001961
Iteration 39/1000 | Loss: 0.00001960
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001960
Iteration 43/1000 | Loss: 0.00001959
Iteration 44/1000 | Loss: 0.00001959
Iteration 45/1000 | Loss: 0.00001959
Iteration 46/1000 | Loss: 0.00001959
Iteration 47/1000 | Loss: 0.00001958
Iteration 48/1000 | Loss: 0.00001958
Iteration 49/1000 | Loss: 0.00001957
Iteration 50/1000 | Loss: 0.00001957
Iteration 51/1000 | Loss: 0.00001957
Iteration 52/1000 | Loss: 0.00001957
Iteration 53/1000 | Loss: 0.00001957
Iteration 54/1000 | Loss: 0.00001957
Iteration 55/1000 | Loss: 0.00001957
Iteration 56/1000 | Loss: 0.00001957
Iteration 57/1000 | Loss: 0.00001957
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001955
Iteration 68/1000 | Loss: 0.00001955
Iteration 69/1000 | Loss: 0.00001955
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001955
Iteration 77/1000 | Loss: 0.00001955
Iteration 78/1000 | Loss: 0.00001954
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001954
Iteration 82/1000 | Loss: 0.00001954
Iteration 83/1000 | Loss: 0.00001954
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001954
Iteration 87/1000 | Loss: 0.00001954
Iteration 88/1000 | Loss: 0.00001954
Iteration 89/1000 | Loss: 0.00001954
Iteration 90/1000 | Loss: 0.00001954
Iteration 91/1000 | Loss: 0.00001954
Iteration 92/1000 | Loss: 0.00001954
Iteration 93/1000 | Loss: 0.00001954
Iteration 94/1000 | Loss: 0.00001953
Iteration 95/1000 | Loss: 0.00001953
Iteration 96/1000 | Loss: 0.00001953
Iteration 97/1000 | Loss: 0.00001953
Iteration 98/1000 | Loss: 0.00001953
Iteration 99/1000 | Loss: 0.00001953
Iteration 100/1000 | Loss: 0.00001953
Iteration 101/1000 | Loss: 0.00001953
Iteration 102/1000 | Loss: 0.00001953
Iteration 103/1000 | Loss: 0.00001953
Iteration 104/1000 | Loss: 0.00001953
Iteration 105/1000 | Loss: 0.00001953
Iteration 106/1000 | Loss: 0.00001953
Iteration 107/1000 | Loss: 0.00001953
Iteration 108/1000 | Loss: 0.00001953
Iteration 109/1000 | Loss: 0.00001953
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.9531600628397427e-05, 1.9531600628397427e-05, 1.9531600628397427e-05, 1.9531600628397427e-05, 1.9531600628397427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9531600628397427e-05

Optimization complete. Final v2v error: 3.6674551963806152 mm

Highest mean error: 4.6463212966918945 mm for frame 115

Lowest mean error: 2.958839178085327 mm for frame 235

Saving results

Total time: 54.939157009124756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858579
Iteration 2/25 | Loss: 0.00124647
Iteration 3/25 | Loss: 0.00084983
Iteration 4/25 | Loss: 0.00080266
Iteration 5/25 | Loss: 0.00078774
Iteration 6/25 | Loss: 0.00078593
Iteration 7/25 | Loss: 0.00078593
Iteration 8/25 | Loss: 0.00078593
Iteration 9/25 | Loss: 0.00078593
Iteration 10/25 | Loss: 0.00078593
Iteration 11/25 | Loss: 0.00078593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007859297329559922, 0.0007859297329559922, 0.0007859297329559922, 0.0007859297329559922, 0.0007859297329559922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007859297329559922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55647290
Iteration 2/25 | Loss: 0.00106675
Iteration 3/25 | Loss: 0.00106675
Iteration 4/25 | Loss: 0.00106675
Iteration 5/25 | Loss: 0.00106675
Iteration 6/25 | Loss: 0.00106675
Iteration 7/25 | Loss: 0.00106675
Iteration 8/25 | Loss: 0.00106675
Iteration 9/25 | Loss: 0.00106675
Iteration 10/25 | Loss: 0.00106675
Iteration 11/25 | Loss: 0.00106675
Iteration 12/25 | Loss: 0.00106675
Iteration 13/25 | Loss: 0.00106675
Iteration 14/25 | Loss: 0.00106675
Iteration 15/25 | Loss: 0.00106675
Iteration 16/25 | Loss: 0.00106675
Iteration 17/25 | Loss: 0.00106675
Iteration 18/25 | Loss: 0.00106675
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010667454916983843, 0.0010667454916983843, 0.0010667454916983843, 0.0010667454916983843, 0.0010667454916983843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010667454916983843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106675
Iteration 2/1000 | Loss: 0.00003681
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001871
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001728
Iteration 11/1000 | Loss: 0.00001728
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001723
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001687
Iteration 21/1000 | Loss: 0.00001686
Iteration 22/1000 | Loss: 0.00001684
Iteration 23/1000 | Loss: 0.00001683
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00001681
Iteration 27/1000 | Loss: 0.00001681
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001680
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001680
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001680
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001680
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001680
Iteration 64/1000 | Loss: 0.00001680
Iteration 65/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.6802980098873377e-05, 1.6802980098873377e-05, 1.6802980098873377e-05, 1.6802980098873377e-05, 1.6802980098873377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6802980098873377e-05

Optimization complete. Final v2v error: 3.4663236141204834 mm

Highest mean error: 3.728116035461426 mm for frame 51

Lowest mean error: 3.2130868434906006 mm for frame 95

Saving results

Total time: 30.971120357513428
