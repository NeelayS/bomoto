Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=142, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7952-8007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079572
Iteration 2/25 | Loss: 0.00187442
Iteration 3/25 | Loss: 0.00141960
Iteration 4/25 | Loss: 0.00140057
Iteration 5/25 | Loss: 0.00139663
Iteration 6/25 | Loss: 0.00139549
Iteration 7/25 | Loss: 0.00139534
Iteration 8/25 | Loss: 0.00139534
Iteration 9/25 | Loss: 0.00139534
Iteration 10/25 | Loss: 0.00139534
Iteration 11/25 | Loss: 0.00139534
Iteration 12/25 | Loss: 0.00139534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013953351881355047, 0.0013953351881355047, 0.0013953351881355047, 0.0013953351881355047, 0.0013953351881355047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013953351881355047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67024809
Iteration 2/25 | Loss: 0.00110737
Iteration 3/25 | Loss: 0.00110737
Iteration 4/25 | Loss: 0.00110737
Iteration 5/25 | Loss: 0.00110737
Iteration 6/25 | Loss: 0.00110737
Iteration 7/25 | Loss: 0.00110737
Iteration 8/25 | Loss: 0.00110737
Iteration 9/25 | Loss: 0.00110737
Iteration 10/25 | Loss: 0.00110737
Iteration 11/25 | Loss: 0.00110737
Iteration 12/25 | Loss: 0.00110737
Iteration 13/25 | Loss: 0.00110737
Iteration 14/25 | Loss: 0.00110737
Iteration 15/25 | Loss: 0.00110737
Iteration 16/25 | Loss: 0.00110737
Iteration 17/25 | Loss: 0.00110737
Iteration 18/25 | Loss: 0.00110737
Iteration 19/25 | Loss: 0.00110737
Iteration 20/25 | Loss: 0.00110737
Iteration 21/25 | Loss: 0.00110737
Iteration 22/25 | Loss: 0.00110737
Iteration 23/25 | Loss: 0.00110737
Iteration 24/25 | Loss: 0.00110737
Iteration 25/25 | Loss: 0.00110737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110737
Iteration 2/1000 | Loss: 0.00015469
Iteration 3/1000 | Loss: 0.00010672
Iteration 4/1000 | Loss: 0.00008356
Iteration 5/1000 | Loss: 0.00007494
Iteration 6/1000 | Loss: 0.00007172
Iteration 7/1000 | Loss: 0.00007012
Iteration 8/1000 | Loss: 0.00006878
Iteration 9/1000 | Loss: 0.00006739
Iteration 10/1000 | Loss: 0.00006617
Iteration 11/1000 | Loss: 0.00006514
Iteration 12/1000 | Loss: 0.00006374
Iteration 13/1000 | Loss: 0.00006279
Iteration 14/1000 | Loss: 0.00006213
Iteration 15/1000 | Loss: 0.00006168
Iteration 16/1000 | Loss: 0.00006137
Iteration 17/1000 | Loss: 0.00006107
Iteration 18/1000 | Loss: 0.00006079
Iteration 19/1000 | Loss: 0.00006056
Iteration 20/1000 | Loss: 0.00006050
Iteration 21/1000 | Loss: 0.00006030
Iteration 22/1000 | Loss: 0.00006016
Iteration 23/1000 | Loss: 0.00006003
Iteration 24/1000 | Loss: 0.00005993
Iteration 25/1000 | Loss: 0.00005982
Iteration 26/1000 | Loss: 0.00005982
Iteration 27/1000 | Loss: 0.00005980
Iteration 28/1000 | Loss: 0.00005978
Iteration 29/1000 | Loss: 0.00005974
Iteration 30/1000 | Loss: 0.00005969
Iteration 31/1000 | Loss: 0.00005969
Iteration 32/1000 | Loss: 0.00005968
Iteration 33/1000 | Loss: 0.00005967
Iteration 34/1000 | Loss: 0.00005967
Iteration 35/1000 | Loss: 0.00005966
Iteration 36/1000 | Loss: 0.00005963
Iteration 37/1000 | Loss: 0.00005962
Iteration 38/1000 | Loss: 0.00005961
Iteration 39/1000 | Loss: 0.00005961
Iteration 40/1000 | Loss: 0.00005961
Iteration 41/1000 | Loss: 0.00005960
Iteration 42/1000 | Loss: 0.00005958
Iteration 43/1000 | Loss: 0.00005957
Iteration 44/1000 | Loss: 0.00005957
Iteration 45/1000 | Loss: 0.00005957
Iteration 46/1000 | Loss: 0.00005956
Iteration 47/1000 | Loss: 0.00005956
Iteration 48/1000 | Loss: 0.00005955
Iteration 49/1000 | Loss: 0.00005955
Iteration 50/1000 | Loss: 0.00005955
Iteration 51/1000 | Loss: 0.00005955
Iteration 52/1000 | Loss: 0.00005954
Iteration 53/1000 | Loss: 0.00005953
Iteration 54/1000 | Loss: 0.00005953
Iteration 55/1000 | Loss: 0.00005953
Iteration 56/1000 | Loss: 0.00005953
Iteration 57/1000 | Loss: 0.00005952
Iteration 58/1000 | Loss: 0.00005952
Iteration 59/1000 | Loss: 0.00005952
Iteration 60/1000 | Loss: 0.00005952
Iteration 61/1000 | Loss: 0.00005952
Iteration 62/1000 | Loss: 0.00005952
Iteration 63/1000 | Loss: 0.00005952
Iteration 64/1000 | Loss: 0.00005951
Iteration 65/1000 | Loss: 0.00005951
Iteration 66/1000 | Loss: 0.00005951
Iteration 67/1000 | Loss: 0.00005950
Iteration 68/1000 | Loss: 0.00005950
Iteration 69/1000 | Loss: 0.00005950
Iteration 70/1000 | Loss: 0.00005950
Iteration 71/1000 | Loss: 0.00005949
Iteration 72/1000 | Loss: 0.00005949
Iteration 73/1000 | Loss: 0.00005949
Iteration 74/1000 | Loss: 0.00005949
Iteration 75/1000 | Loss: 0.00005948
Iteration 76/1000 | Loss: 0.00005948
Iteration 77/1000 | Loss: 0.00005948
Iteration 78/1000 | Loss: 0.00005947
Iteration 79/1000 | Loss: 0.00005947
Iteration 80/1000 | Loss: 0.00005947
Iteration 81/1000 | Loss: 0.00005947
Iteration 82/1000 | Loss: 0.00005947
Iteration 83/1000 | Loss: 0.00005946
Iteration 84/1000 | Loss: 0.00005946
Iteration 85/1000 | Loss: 0.00005946
Iteration 86/1000 | Loss: 0.00005946
Iteration 87/1000 | Loss: 0.00005945
Iteration 88/1000 | Loss: 0.00005945
Iteration 89/1000 | Loss: 0.00005945
Iteration 90/1000 | Loss: 0.00005945
Iteration 91/1000 | Loss: 0.00005945
Iteration 92/1000 | Loss: 0.00005944
Iteration 93/1000 | Loss: 0.00005944
Iteration 94/1000 | Loss: 0.00005944
Iteration 95/1000 | Loss: 0.00005943
Iteration 96/1000 | Loss: 0.00005943
Iteration 97/1000 | Loss: 0.00005943
Iteration 98/1000 | Loss: 0.00005943
Iteration 99/1000 | Loss: 0.00005942
Iteration 100/1000 | Loss: 0.00005942
Iteration 101/1000 | Loss: 0.00005942
Iteration 102/1000 | Loss: 0.00005942
Iteration 103/1000 | Loss: 0.00005942
Iteration 104/1000 | Loss: 0.00005941
Iteration 105/1000 | Loss: 0.00005941
Iteration 106/1000 | Loss: 0.00005941
Iteration 107/1000 | Loss: 0.00005941
Iteration 108/1000 | Loss: 0.00005941
Iteration 109/1000 | Loss: 0.00005941
Iteration 110/1000 | Loss: 0.00005941
Iteration 111/1000 | Loss: 0.00005941
Iteration 112/1000 | Loss: 0.00005940
Iteration 113/1000 | Loss: 0.00005940
Iteration 114/1000 | Loss: 0.00005940
Iteration 115/1000 | Loss: 0.00005940
Iteration 116/1000 | Loss: 0.00005940
Iteration 117/1000 | Loss: 0.00005940
Iteration 118/1000 | Loss: 0.00005940
Iteration 119/1000 | Loss: 0.00005940
Iteration 120/1000 | Loss: 0.00005939
Iteration 121/1000 | Loss: 0.00005939
Iteration 122/1000 | Loss: 0.00005939
Iteration 123/1000 | Loss: 0.00005939
Iteration 124/1000 | Loss: 0.00005938
Iteration 125/1000 | Loss: 0.00005938
Iteration 126/1000 | Loss: 0.00005938
Iteration 127/1000 | Loss: 0.00005938
Iteration 128/1000 | Loss: 0.00005938
Iteration 129/1000 | Loss: 0.00005938
Iteration 130/1000 | Loss: 0.00005938
Iteration 131/1000 | Loss: 0.00005938
Iteration 132/1000 | Loss: 0.00005938
Iteration 133/1000 | Loss: 0.00005937
Iteration 134/1000 | Loss: 0.00005937
Iteration 135/1000 | Loss: 0.00005937
Iteration 136/1000 | Loss: 0.00005937
Iteration 137/1000 | Loss: 0.00005937
Iteration 138/1000 | Loss: 0.00005937
Iteration 139/1000 | Loss: 0.00005937
Iteration 140/1000 | Loss: 0.00005936
Iteration 141/1000 | Loss: 0.00005936
Iteration 142/1000 | Loss: 0.00005936
Iteration 143/1000 | Loss: 0.00005935
Iteration 144/1000 | Loss: 0.00005935
Iteration 145/1000 | Loss: 0.00005935
Iteration 146/1000 | Loss: 0.00005935
Iteration 147/1000 | Loss: 0.00005935
Iteration 148/1000 | Loss: 0.00005935
Iteration 149/1000 | Loss: 0.00005935
Iteration 150/1000 | Loss: 0.00005935
Iteration 151/1000 | Loss: 0.00005935
Iteration 152/1000 | Loss: 0.00005935
Iteration 153/1000 | Loss: 0.00005935
Iteration 154/1000 | Loss: 0.00005935
Iteration 155/1000 | Loss: 0.00005935
Iteration 156/1000 | Loss: 0.00005935
Iteration 157/1000 | Loss: 0.00005935
Iteration 158/1000 | Loss: 0.00005935
Iteration 159/1000 | Loss: 0.00005935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [5.9353413234930485e-05, 5.9353413234930485e-05, 5.9353413234930485e-05, 5.9353413234930485e-05, 5.9353413234930485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.9353413234930485e-05

Optimization complete. Final v2v error: 5.908357620239258 mm

Highest mean error: 6.495296955108643 mm for frame 79

Lowest mean error: 4.3505778312683105 mm for frame 20

Saving results

Total time: 56.9226930141449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536533
Iteration 2/25 | Loss: 0.00149081
Iteration 3/25 | Loss: 0.00125610
Iteration 4/25 | Loss: 0.00122603
Iteration 5/25 | Loss: 0.00122327
Iteration 6/25 | Loss: 0.00122312
Iteration 7/25 | Loss: 0.00122312
Iteration 8/25 | Loss: 0.00122312
Iteration 9/25 | Loss: 0.00122312
Iteration 10/25 | Loss: 0.00122312
Iteration 11/25 | Loss: 0.00122312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001223117345944047, 0.001223117345944047, 0.001223117345944047, 0.001223117345944047, 0.001223117345944047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001223117345944047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.96143150
Iteration 2/25 | Loss: 0.00120573
Iteration 3/25 | Loss: 0.00120573
Iteration 4/25 | Loss: 0.00120573
Iteration 5/25 | Loss: 0.00120573
Iteration 6/25 | Loss: 0.00120573
Iteration 7/25 | Loss: 0.00120573
Iteration 8/25 | Loss: 0.00120573
Iteration 9/25 | Loss: 0.00120573
Iteration 10/25 | Loss: 0.00120573
Iteration 11/25 | Loss: 0.00120573
Iteration 12/25 | Loss: 0.00120572
Iteration 13/25 | Loss: 0.00120572
Iteration 14/25 | Loss: 0.00120572
Iteration 15/25 | Loss: 0.00120573
Iteration 16/25 | Loss: 0.00120572
Iteration 17/25 | Loss: 0.00120572
Iteration 18/25 | Loss: 0.00120572
Iteration 19/25 | Loss: 0.00120572
Iteration 20/25 | Loss: 0.00120573
Iteration 21/25 | Loss: 0.00120572
Iteration 22/25 | Loss: 0.00120573
Iteration 23/25 | Loss: 0.00120572
Iteration 24/25 | Loss: 0.00120572
Iteration 25/25 | Loss: 0.00120573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120572
Iteration 2/1000 | Loss: 0.00008021
Iteration 3/1000 | Loss: 0.00003975
Iteration 4/1000 | Loss: 0.00003134
Iteration 5/1000 | Loss: 0.00002903
Iteration 6/1000 | Loss: 0.00002757
Iteration 7/1000 | Loss: 0.00002693
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002591
Iteration 10/1000 | Loss: 0.00002547
Iteration 11/1000 | Loss: 0.00002515
Iteration 12/1000 | Loss: 0.00002493
Iteration 13/1000 | Loss: 0.00002491
Iteration 14/1000 | Loss: 0.00002477
Iteration 15/1000 | Loss: 0.00002476
Iteration 16/1000 | Loss: 0.00002476
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002476
Iteration 19/1000 | Loss: 0.00002476
Iteration 20/1000 | Loss: 0.00002474
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002473
Iteration 23/1000 | Loss: 0.00002473
Iteration 24/1000 | Loss: 0.00002472
Iteration 25/1000 | Loss: 0.00002472
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002471
Iteration 28/1000 | Loss: 0.00002470
Iteration 29/1000 | Loss: 0.00002470
Iteration 30/1000 | Loss: 0.00002470
Iteration 31/1000 | Loss: 0.00002467
Iteration 32/1000 | Loss: 0.00002467
Iteration 33/1000 | Loss: 0.00002467
Iteration 34/1000 | Loss: 0.00002466
Iteration 35/1000 | Loss: 0.00002464
Iteration 36/1000 | Loss: 0.00002464
Iteration 37/1000 | Loss: 0.00002464
Iteration 38/1000 | Loss: 0.00002464
Iteration 39/1000 | Loss: 0.00002464
Iteration 40/1000 | Loss: 0.00002464
Iteration 41/1000 | Loss: 0.00002463
Iteration 42/1000 | Loss: 0.00002463
Iteration 43/1000 | Loss: 0.00002463
Iteration 44/1000 | Loss: 0.00002463
Iteration 45/1000 | Loss: 0.00002463
Iteration 46/1000 | Loss: 0.00002462
Iteration 47/1000 | Loss: 0.00002462
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002460
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002459
Iteration 57/1000 | Loss: 0.00002458
Iteration 58/1000 | Loss: 0.00002456
Iteration 59/1000 | Loss: 0.00002456
Iteration 60/1000 | Loss: 0.00002456
Iteration 61/1000 | Loss: 0.00002456
Iteration 62/1000 | Loss: 0.00002456
Iteration 63/1000 | Loss: 0.00002455
Iteration 64/1000 | Loss: 0.00002455
Iteration 65/1000 | Loss: 0.00002455
Iteration 66/1000 | Loss: 0.00002455
Iteration 67/1000 | Loss: 0.00002455
Iteration 68/1000 | Loss: 0.00002455
Iteration 69/1000 | Loss: 0.00002455
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002454
Iteration 75/1000 | Loss: 0.00002454
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002452
Iteration 78/1000 | Loss: 0.00002452
Iteration 79/1000 | Loss: 0.00002452
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002451
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002451
Iteration 85/1000 | Loss: 0.00002451
Iteration 86/1000 | Loss: 0.00002450
Iteration 87/1000 | Loss: 0.00002450
Iteration 88/1000 | Loss: 0.00002450
Iteration 89/1000 | Loss: 0.00002450
Iteration 90/1000 | Loss: 0.00002449
Iteration 91/1000 | Loss: 0.00002449
Iteration 92/1000 | Loss: 0.00002449
Iteration 93/1000 | Loss: 0.00002449
Iteration 94/1000 | Loss: 0.00002449
Iteration 95/1000 | Loss: 0.00002448
Iteration 96/1000 | Loss: 0.00002448
Iteration 97/1000 | Loss: 0.00002448
Iteration 98/1000 | Loss: 0.00002448
Iteration 99/1000 | Loss: 0.00002447
Iteration 100/1000 | Loss: 0.00002447
Iteration 101/1000 | Loss: 0.00002447
Iteration 102/1000 | Loss: 0.00002447
Iteration 103/1000 | Loss: 0.00002447
Iteration 104/1000 | Loss: 0.00002447
Iteration 105/1000 | Loss: 0.00002446
Iteration 106/1000 | Loss: 0.00002446
Iteration 107/1000 | Loss: 0.00002446
Iteration 108/1000 | Loss: 0.00002446
Iteration 109/1000 | Loss: 0.00002446
Iteration 110/1000 | Loss: 0.00002446
Iteration 111/1000 | Loss: 0.00002446
Iteration 112/1000 | Loss: 0.00002446
Iteration 113/1000 | Loss: 0.00002445
Iteration 114/1000 | Loss: 0.00002445
Iteration 115/1000 | Loss: 0.00002445
Iteration 116/1000 | Loss: 0.00002445
Iteration 117/1000 | Loss: 0.00002445
Iteration 118/1000 | Loss: 0.00002445
Iteration 119/1000 | Loss: 0.00002445
Iteration 120/1000 | Loss: 0.00002445
Iteration 121/1000 | Loss: 0.00002444
Iteration 122/1000 | Loss: 0.00002444
Iteration 123/1000 | Loss: 0.00002444
Iteration 124/1000 | Loss: 0.00002444
Iteration 125/1000 | Loss: 0.00002444
Iteration 126/1000 | Loss: 0.00002444
Iteration 127/1000 | Loss: 0.00002444
Iteration 128/1000 | Loss: 0.00002444
Iteration 129/1000 | Loss: 0.00002444
Iteration 130/1000 | Loss: 0.00002444
Iteration 131/1000 | Loss: 0.00002444
Iteration 132/1000 | Loss: 0.00002444
Iteration 133/1000 | Loss: 0.00002444
Iteration 134/1000 | Loss: 0.00002444
Iteration 135/1000 | Loss: 0.00002444
Iteration 136/1000 | Loss: 0.00002444
Iteration 137/1000 | Loss: 0.00002444
Iteration 138/1000 | Loss: 0.00002444
Iteration 139/1000 | Loss: 0.00002444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.4440529159619473e-05, 2.4440529159619473e-05, 2.4440529159619473e-05, 2.4440529159619473e-05, 2.4440529159619473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4440529159619473e-05

Optimization complete. Final v2v error: 4.228352069854736 mm

Highest mean error: 4.863073348999023 mm for frame 59

Lowest mean error: 3.772233247756958 mm for frame 29

Saving results

Total time: 35.49221682548523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084621
Iteration 2/25 | Loss: 0.01084621
Iteration 3/25 | Loss: 0.01084620
Iteration 4/25 | Loss: 0.01084620
Iteration 5/25 | Loss: 0.01084620
Iteration 6/25 | Loss: 0.01084620
Iteration 7/25 | Loss: 0.01084620
Iteration 8/25 | Loss: 0.01084620
Iteration 9/25 | Loss: 0.01084620
Iteration 10/25 | Loss: 0.01084620
Iteration 11/25 | Loss: 0.01084620
Iteration 12/25 | Loss: 0.01084620
Iteration 13/25 | Loss: 0.01084620
Iteration 14/25 | Loss: 0.01084620
Iteration 15/25 | Loss: 0.01084619
Iteration 16/25 | Loss: 0.01084619
Iteration 17/25 | Loss: 0.01084619
Iteration 18/25 | Loss: 0.01084619
Iteration 19/25 | Loss: 0.01084619
Iteration 20/25 | Loss: 0.01084619
Iteration 21/25 | Loss: 0.01084619
Iteration 22/25 | Loss: 0.01084619
Iteration 23/25 | Loss: 0.01084619
Iteration 24/25 | Loss: 0.01084619
Iteration 25/25 | Loss: 0.01084619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72870255
Iteration 2/25 | Loss: 0.08641815
Iteration 3/25 | Loss: 0.08579811
Iteration 4/25 | Loss: 0.08571331
Iteration 5/25 | Loss: 0.08571330
Iteration 6/25 | Loss: 0.08571330
Iteration 7/25 | Loss: 0.08571328
Iteration 8/25 | Loss: 0.08571328
Iteration 9/25 | Loss: 0.08571328
Iteration 10/25 | Loss: 0.08571328
Iteration 11/25 | Loss: 0.08571328
Iteration 12/25 | Loss: 0.08571328
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08571328222751617, 0.08571328222751617, 0.08571328222751617, 0.08571328222751617, 0.08571328222751617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08571328222751617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08571328
Iteration 2/1000 | Loss: 0.00506544
Iteration 3/1000 | Loss: 0.00399049
Iteration 4/1000 | Loss: 0.00342301
Iteration 5/1000 | Loss: 0.00030667
Iteration 6/1000 | Loss: 0.00035771
Iteration 7/1000 | Loss: 0.00045681
Iteration 8/1000 | Loss: 0.00053012
Iteration 9/1000 | Loss: 0.00017552
Iteration 10/1000 | Loss: 0.00035115
Iteration 11/1000 | Loss: 0.00007229
Iteration 12/1000 | Loss: 0.00012137
Iteration 13/1000 | Loss: 0.00010754
Iteration 14/1000 | Loss: 0.00037744
Iteration 15/1000 | Loss: 0.00071454
Iteration 16/1000 | Loss: 0.00016824
Iteration 17/1000 | Loss: 0.00004558
Iteration 18/1000 | Loss: 0.00014860
Iteration 19/1000 | Loss: 0.00014120
Iteration 20/1000 | Loss: 0.00037434
Iteration 21/1000 | Loss: 0.00034812
Iteration 22/1000 | Loss: 0.00278832
Iteration 23/1000 | Loss: 0.00016302
Iteration 24/1000 | Loss: 0.00009548
Iteration 25/1000 | Loss: 0.00003749
Iteration 26/1000 | Loss: 0.00003227
Iteration 27/1000 | Loss: 0.00018237
Iteration 28/1000 | Loss: 0.00004373
Iteration 29/1000 | Loss: 0.00004471
Iteration 30/1000 | Loss: 0.00005111
Iteration 31/1000 | Loss: 0.00002782
Iteration 32/1000 | Loss: 0.00002655
Iteration 33/1000 | Loss: 0.00004023
Iteration 34/1000 | Loss: 0.00002544
Iteration 35/1000 | Loss: 0.00012241
Iteration 36/1000 | Loss: 0.00003208
Iteration 37/1000 | Loss: 0.00002643
Iteration 38/1000 | Loss: 0.00003596
Iteration 39/1000 | Loss: 0.00002401
Iteration 40/1000 | Loss: 0.00004522
Iteration 41/1000 | Loss: 0.00002861
Iteration 42/1000 | Loss: 0.00002327
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00019744
Iteration 45/1000 | Loss: 0.00003269
Iteration 46/1000 | Loss: 0.00003899
Iteration 47/1000 | Loss: 0.00012848
Iteration 48/1000 | Loss: 0.00003317
Iteration 49/1000 | Loss: 0.00002474
Iteration 50/1000 | Loss: 0.00002184
Iteration 51/1000 | Loss: 0.00002879
Iteration 52/1000 | Loss: 0.00002323
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002254
Iteration 55/1000 | Loss: 0.00002285
Iteration 56/1000 | Loss: 0.00002250
Iteration 57/1000 | Loss: 0.00002443
Iteration 58/1000 | Loss: 0.00002122
Iteration 59/1000 | Loss: 0.00002111
Iteration 60/1000 | Loss: 0.00002220
Iteration 61/1000 | Loss: 0.00008984
Iteration 62/1000 | Loss: 0.00002723
Iteration 63/1000 | Loss: 0.00004168
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002198
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002101
Iteration 68/1000 | Loss: 0.00002097
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00002096
Iteration 71/1000 | Loss: 0.00002091
Iteration 72/1000 | Loss: 0.00002091
Iteration 73/1000 | Loss: 0.00002091
Iteration 74/1000 | Loss: 0.00002091
Iteration 75/1000 | Loss: 0.00002091
Iteration 76/1000 | Loss: 0.00002091
Iteration 77/1000 | Loss: 0.00002091
Iteration 78/1000 | Loss: 0.00002090
Iteration 79/1000 | Loss: 0.00002090
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002211
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002370
Iteration 85/1000 | Loss: 0.00002256
Iteration 86/1000 | Loss: 0.00002256
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002193
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002076
Iteration 91/1000 | Loss: 0.00002074
Iteration 92/1000 | Loss: 0.00002074
Iteration 93/1000 | Loss: 0.00002074
Iteration 94/1000 | Loss: 0.00002181
Iteration 95/1000 | Loss: 0.00002180
Iteration 96/1000 | Loss: 0.00002143
Iteration 97/1000 | Loss: 0.00002073
Iteration 98/1000 | Loss: 0.00002071
Iteration 99/1000 | Loss: 0.00002071
Iteration 100/1000 | Loss: 0.00002071
Iteration 101/1000 | Loss: 0.00002070
Iteration 102/1000 | Loss: 0.00002070
Iteration 103/1000 | Loss: 0.00002070
Iteration 104/1000 | Loss: 0.00002070
Iteration 105/1000 | Loss: 0.00002070
Iteration 106/1000 | Loss: 0.00002070
Iteration 107/1000 | Loss: 0.00002070
Iteration 108/1000 | Loss: 0.00002070
Iteration 109/1000 | Loss: 0.00002070
Iteration 110/1000 | Loss: 0.00002069
Iteration 111/1000 | Loss: 0.00002069
Iteration 112/1000 | Loss: 0.00002069
Iteration 113/1000 | Loss: 0.00002069
Iteration 114/1000 | Loss: 0.00002069
Iteration 115/1000 | Loss: 0.00002068
Iteration 116/1000 | Loss: 0.00002068
Iteration 117/1000 | Loss: 0.00002068
Iteration 118/1000 | Loss: 0.00002498
Iteration 119/1000 | Loss: 0.00002068
Iteration 120/1000 | Loss: 0.00002067
Iteration 121/1000 | Loss: 0.00002067
Iteration 122/1000 | Loss: 0.00002067
Iteration 123/1000 | Loss: 0.00002066
Iteration 124/1000 | Loss: 0.00002186
Iteration 125/1000 | Loss: 0.00002833
Iteration 126/1000 | Loss: 0.00002067
Iteration 127/1000 | Loss: 0.00002066
Iteration 128/1000 | Loss: 0.00002066
Iteration 129/1000 | Loss: 0.00002065
Iteration 130/1000 | Loss: 0.00002065
Iteration 131/1000 | Loss: 0.00002065
Iteration 132/1000 | Loss: 0.00002065
Iteration 133/1000 | Loss: 0.00002065
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00002065
Iteration 136/1000 | Loss: 0.00002065
Iteration 137/1000 | Loss: 0.00002065
Iteration 138/1000 | Loss: 0.00002064
Iteration 139/1000 | Loss: 0.00002064
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00002063
Iteration 142/1000 | Loss: 0.00002063
Iteration 143/1000 | Loss: 0.00002062
Iteration 144/1000 | Loss: 0.00002062
Iteration 145/1000 | Loss: 0.00002062
Iteration 146/1000 | Loss: 0.00002062
Iteration 147/1000 | Loss: 0.00002061
Iteration 148/1000 | Loss: 0.00002061
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002059
Iteration 152/1000 | Loss: 0.00002059
Iteration 153/1000 | Loss: 0.00002058
Iteration 154/1000 | Loss: 0.00002058
Iteration 155/1000 | Loss: 0.00002058
Iteration 156/1000 | Loss: 0.00002058
Iteration 157/1000 | Loss: 0.00002057
Iteration 158/1000 | Loss: 0.00002057
Iteration 159/1000 | Loss: 0.00002057
Iteration 160/1000 | Loss: 0.00002056
Iteration 161/1000 | Loss: 0.00002056
Iteration 162/1000 | Loss: 0.00002056
Iteration 163/1000 | Loss: 0.00002056
Iteration 164/1000 | Loss: 0.00002056
Iteration 165/1000 | Loss: 0.00002056
Iteration 166/1000 | Loss: 0.00002056
Iteration 167/1000 | Loss: 0.00002056
Iteration 168/1000 | Loss: 0.00002056
Iteration 169/1000 | Loss: 0.00002056
Iteration 170/1000 | Loss: 0.00002056
Iteration 171/1000 | Loss: 0.00002056
Iteration 172/1000 | Loss: 0.00002056
Iteration 173/1000 | Loss: 0.00002056
Iteration 174/1000 | Loss: 0.00002055
Iteration 175/1000 | Loss: 0.00002055
Iteration 176/1000 | Loss: 0.00002055
Iteration 177/1000 | Loss: 0.00002055
Iteration 178/1000 | Loss: 0.00003775
Iteration 179/1000 | Loss: 0.00003775
Iteration 180/1000 | Loss: 0.00002270
Iteration 181/1000 | Loss: 0.00002054
Iteration 182/1000 | Loss: 0.00002054
Iteration 183/1000 | Loss: 0.00002054
Iteration 184/1000 | Loss: 0.00002054
Iteration 185/1000 | Loss: 0.00002054
Iteration 186/1000 | Loss: 0.00002053
Iteration 187/1000 | Loss: 0.00002053
Iteration 188/1000 | Loss: 0.00002053
Iteration 189/1000 | Loss: 0.00002053
Iteration 190/1000 | Loss: 0.00002053
Iteration 191/1000 | Loss: 0.00002053
Iteration 192/1000 | Loss: 0.00002053
Iteration 193/1000 | Loss: 0.00002052
Iteration 194/1000 | Loss: 0.00002052
Iteration 195/1000 | Loss: 0.00002052
Iteration 196/1000 | Loss: 0.00002052
Iteration 197/1000 | Loss: 0.00002052
Iteration 198/1000 | Loss: 0.00002052
Iteration 199/1000 | Loss: 0.00002052
Iteration 200/1000 | Loss: 0.00002052
Iteration 201/1000 | Loss: 0.00002052
Iteration 202/1000 | Loss: 0.00002052
Iteration 203/1000 | Loss: 0.00002052
Iteration 204/1000 | Loss: 0.00002052
Iteration 205/1000 | Loss: 0.00002052
Iteration 206/1000 | Loss: 0.00002052
Iteration 207/1000 | Loss: 0.00002052
Iteration 208/1000 | Loss: 0.00002052
Iteration 209/1000 | Loss: 0.00002052
Iteration 210/1000 | Loss: 0.00002052
Iteration 211/1000 | Loss: 0.00002052
Iteration 212/1000 | Loss: 0.00002052
Iteration 213/1000 | Loss: 0.00002052
Iteration 214/1000 | Loss: 0.00002052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.0517047232715413e-05, 2.0517047232715413e-05, 2.0517047232715413e-05, 2.0517047232715413e-05, 2.0517047232715413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0517047232715413e-05

Optimization complete. Final v2v error: 3.365980386734009 mm

Highest mean error: 20.91020393371582 mm for frame 217

Lowest mean error: 2.7966151237487793 mm for frame 120

Saving results

Total time: 144.76726174354553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387494
Iteration 2/25 | Loss: 0.00124188
Iteration 3/25 | Loss: 0.00116998
Iteration 4/25 | Loss: 0.00116239
Iteration 5/25 | Loss: 0.00116042
Iteration 6/25 | Loss: 0.00115977
Iteration 7/25 | Loss: 0.00115977
Iteration 8/25 | Loss: 0.00115977
Iteration 9/25 | Loss: 0.00115977
Iteration 10/25 | Loss: 0.00115977
Iteration 11/25 | Loss: 0.00115977
Iteration 12/25 | Loss: 0.00115977
Iteration 13/25 | Loss: 0.00115977
Iteration 14/25 | Loss: 0.00115977
Iteration 15/25 | Loss: 0.00115977
Iteration 16/25 | Loss: 0.00115977
Iteration 17/25 | Loss: 0.00115977
Iteration 18/25 | Loss: 0.00115977
Iteration 19/25 | Loss: 0.00115977
Iteration 20/25 | Loss: 0.00115977
Iteration 21/25 | Loss: 0.00115977
Iteration 22/25 | Loss: 0.00115977
Iteration 23/25 | Loss: 0.00115977
Iteration 24/25 | Loss: 0.00115977
Iteration 25/25 | Loss: 0.00115977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60538781
Iteration 2/25 | Loss: 0.00133746
Iteration 3/25 | Loss: 0.00133746
Iteration 4/25 | Loss: 0.00133745
Iteration 5/25 | Loss: 0.00133745
Iteration 6/25 | Loss: 0.00133745
Iteration 7/25 | Loss: 0.00133745
Iteration 8/25 | Loss: 0.00133745
Iteration 9/25 | Loss: 0.00133745
Iteration 10/25 | Loss: 0.00133745
Iteration 11/25 | Loss: 0.00133745
Iteration 12/25 | Loss: 0.00133745
Iteration 13/25 | Loss: 0.00133745
Iteration 14/25 | Loss: 0.00133745
Iteration 15/25 | Loss: 0.00133745
Iteration 16/25 | Loss: 0.00133745
Iteration 17/25 | Loss: 0.00133745
Iteration 18/25 | Loss: 0.00133745
Iteration 19/25 | Loss: 0.00133745
Iteration 20/25 | Loss: 0.00133745
Iteration 21/25 | Loss: 0.00133745
Iteration 22/25 | Loss: 0.00133745
Iteration 23/25 | Loss: 0.00133745
Iteration 24/25 | Loss: 0.00133745
Iteration 25/25 | Loss: 0.00133745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133745
Iteration 2/1000 | Loss: 0.00004661
Iteration 3/1000 | Loss: 0.00001958
Iteration 4/1000 | Loss: 0.00001642
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001427
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001335
Iteration 9/1000 | Loss: 0.00001305
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001234
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001228
Iteration 36/1000 | Loss: 0.00001228
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001227
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001226
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001224
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001222
Iteration 76/1000 | Loss: 0.00001222
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001221
Iteration 87/1000 | Loss: 0.00001221
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001221
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001220
Iteration 94/1000 | Loss: 0.00001220
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001219
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001219
Iteration 107/1000 | Loss: 0.00001219
Iteration 108/1000 | Loss: 0.00001219
Iteration 109/1000 | Loss: 0.00001219
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001219
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001217
Iteration 120/1000 | Loss: 0.00001217
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001217
Iteration 127/1000 | Loss: 0.00001217
Iteration 128/1000 | Loss: 0.00001217
Iteration 129/1000 | Loss: 0.00001217
Iteration 130/1000 | Loss: 0.00001217
Iteration 131/1000 | Loss: 0.00001217
Iteration 132/1000 | Loss: 0.00001217
Iteration 133/1000 | Loss: 0.00001217
Iteration 134/1000 | Loss: 0.00001217
Iteration 135/1000 | Loss: 0.00001216
Iteration 136/1000 | Loss: 0.00001216
Iteration 137/1000 | Loss: 0.00001216
Iteration 138/1000 | Loss: 0.00001216
Iteration 139/1000 | Loss: 0.00001216
Iteration 140/1000 | Loss: 0.00001216
Iteration 141/1000 | Loss: 0.00001216
Iteration 142/1000 | Loss: 0.00001216
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001215
Iteration 145/1000 | Loss: 0.00001215
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001215
Iteration 148/1000 | Loss: 0.00001215
Iteration 149/1000 | Loss: 0.00001215
Iteration 150/1000 | Loss: 0.00001215
Iteration 151/1000 | Loss: 0.00001215
Iteration 152/1000 | Loss: 0.00001215
Iteration 153/1000 | Loss: 0.00001215
Iteration 154/1000 | Loss: 0.00001214
Iteration 155/1000 | Loss: 0.00001214
Iteration 156/1000 | Loss: 0.00001214
Iteration 157/1000 | Loss: 0.00001214
Iteration 158/1000 | Loss: 0.00001214
Iteration 159/1000 | Loss: 0.00001214
Iteration 160/1000 | Loss: 0.00001214
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001213
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001213
Iteration 166/1000 | Loss: 0.00001213
Iteration 167/1000 | Loss: 0.00001213
Iteration 168/1000 | Loss: 0.00001213
Iteration 169/1000 | Loss: 0.00001213
Iteration 170/1000 | Loss: 0.00001213
Iteration 171/1000 | Loss: 0.00001213
Iteration 172/1000 | Loss: 0.00001213
Iteration 173/1000 | Loss: 0.00001213
Iteration 174/1000 | Loss: 0.00001213
Iteration 175/1000 | Loss: 0.00001213
Iteration 176/1000 | Loss: 0.00001213
Iteration 177/1000 | Loss: 0.00001213
Iteration 178/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.2129166861996055e-05, 1.2129166861996055e-05, 1.2129166861996055e-05, 1.2129166861996055e-05, 1.2129166861996055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2129166861996055e-05

Optimization complete. Final v2v error: 3.0079433917999268 mm

Highest mean error: 3.3074231147766113 mm for frame 83

Lowest mean error: 2.8440909385681152 mm for frame 15

Saving results

Total time: 37.13813400268555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520422
Iteration 2/25 | Loss: 0.00133344
Iteration 3/25 | Loss: 0.00124032
Iteration 4/25 | Loss: 0.00116307
Iteration 5/25 | Loss: 0.00115587
Iteration 6/25 | Loss: 0.00115419
Iteration 7/25 | Loss: 0.00115402
Iteration 8/25 | Loss: 0.00115402
Iteration 9/25 | Loss: 0.00115402
Iteration 10/25 | Loss: 0.00115402
Iteration 11/25 | Loss: 0.00115402
Iteration 12/25 | Loss: 0.00115402
Iteration 13/25 | Loss: 0.00115402
Iteration 14/25 | Loss: 0.00115402
Iteration 15/25 | Loss: 0.00115402
Iteration 16/25 | Loss: 0.00115402
Iteration 17/25 | Loss: 0.00115402
Iteration 18/25 | Loss: 0.00115402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011540179839357734, 0.0011540179839357734, 0.0011540179839357734, 0.0011540179839357734, 0.0011540179839357734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011540179839357734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.83362913
Iteration 2/25 | Loss: 0.00142091
Iteration 3/25 | Loss: 0.00142088
Iteration 4/25 | Loss: 0.00142088
Iteration 5/25 | Loss: 0.00142088
Iteration 6/25 | Loss: 0.00142088
Iteration 7/25 | Loss: 0.00142088
Iteration 8/25 | Loss: 0.00142088
Iteration 9/25 | Loss: 0.00142088
Iteration 10/25 | Loss: 0.00142088
Iteration 11/25 | Loss: 0.00142088
Iteration 12/25 | Loss: 0.00142088
Iteration 13/25 | Loss: 0.00142088
Iteration 14/25 | Loss: 0.00142088
Iteration 15/25 | Loss: 0.00142088
Iteration 16/25 | Loss: 0.00142088
Iteration 17/25 | Loss: 0.00142088
Iteration 18/25 | Loss: 0.00142088
Iteration 19/25 | Loss: 0.00142088
Iteration 20/25 | Loss: 0.00142088
Iteration 21/25 | Loss: 0.00142088
Iteration 22/25 | Loss: 0.00142088
Iteration 23/25 | Loss: 0.00142088
Iteration 24/25 | Loss: 0.00142088
Iteration 25/25 | Loss: 0.00142088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142088
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00002223
Iteration 4/1000 | Loss: 0.00001829
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001534
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001359
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00001307
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001292
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001288
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001288
Iteration 78/1000 | Loss: 0.00001288
Iteration 79/1000 | Loss: 0.00001288
Iteration 80/1000 | Loss: 0.00001288
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.287774011871079e-05, 1.287774011871079e-05, 1.287774011871079e-05, 1.287774011871079e-05, 1.287774011871079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.287774011871079e-05

Optimization complete. Final v2v error: 3.1117255687713623 mm

Highest mean error: 3.4706544876098633 mm for frame 198

Lowest mean error: 2.762674331665039 mm for frame 0

Saving results

Total time: 37.11005735397339
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502078
Iteration 2/25 | Loss: 0.00142363
Iteration 3/25 | Loss: 0.00131642
Iteration 4/25 | Loss: 0.00129490
Iteration 5/25 | Loss: 0.00128780
Iteration 6/25 | Loss: 0.00128673
Iteration 7/25 | Loss: 0.00128673
Iteration 8/25 | Loss: 0.00128673
Iteration 9/25 | Loss: 0.00128673
Iteration 10/25 | Loss: 0.00128673
Iteration 11/25 | Loss: 0.00128673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012867300538346171, 0.0012867300538346171, 0.0012867300538346171, 0.0012867300538346171, 0.0012867300538346171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012867300538346171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20456111
Iteration 2/25 | Loss: 0.00230334
Iteration 3/25 | Loss: 0.00230334
Iteration 4/25 | Loss: 0.00230334
Iteration 5/25 | Loss: 0.00230334
Iteration 6/25 | Loss: 0.00230334
Iteration 7/25 | Loss: 0.00230334
Iteration 8/25 | Loss: 0.00230334
Iteration 9/25 | Loss: 0.00230334
Iteration 10/25 | Loss: 0.00230334
Iteration 11/25 | Loss: 0.00230334
Iteration 12/25 | Loss: 0.00230334
Iteration 13/25 | Loss: 0.00230334
Iteration 14/25 | Loss: 0.00230334
Iteration 15/25 | Loss: 0.00230334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023033374454826117, 0.0023033374454826117, 0.0023033374454826117, 0.0023033374454826117, 0.0023033374454826117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023033374454826117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230334
Iteration 2/1000 | Loss: 0.00009294
Iteration 3/1000 | Loss: 0.00005124
Iteration 4/1000 | Loss: 0.00004029
Iteration 5/1000 | Loss: 0.00003597
Iteration 6/1000 | Loss: 0.00003251
Iteration 7/1000 | Loss: 0.00003030
Iteration 8/1000 | Loss: 0.00002853
Iteration 9/1000 | Loss: 0.00002752
Iteration 10/1000 | Loss: 0.00002621
Iteration 11/1000 | Loss: 0.00002505
Iteration 12/1000 | Loss: 0.00002421
Iteration 13/1000 | Loss: 0.00002373
Iteration 14/1000 | Loss: 0.00002340
Iteration 15/1000 | Loss: 0.00002318
Iteration 16/1000 | Loss: 0.00002295
Iteration 17/1000 | Loss: 0.00002292
Iteration 18/1000 | Loss: 0.00002283
Iteration 19/1000 | Loss: 0.00002268
Iteration 20/1000 | Loss: 0.00002264
Iteration 21/1000 | Loss: 0.00002264
Iteration 22/1000 | Loss: 0.00002263
Iteration 23/1000 | Loss: 0.00002261
Iteration 24/1000 | Loss: 0.00002260
Iteration 25/1000 | Loss: 0.00002258
Iteration 26/1000 | Loss: 0.00002257
Iteration 27/1000 | Loss: 0.00002254
Iteration 28/1000 | Loss: 0.00002254
Iteration 29/1000 | Loss: 0.00002253
Iteration 30/1000 | Loss: 0.00002253
Iteration 31/1000 | Loss: 0.00002253
Iteration 32/1000 | Loss: 0.00002252
Iteration 33/1000 | Loss: 0.00002252
Iteration 34/1000 | Loss: 0.00002250
Iteration 35/1000 | Loss: 0.00002248
Iteration 36/1000 | Loss: 0.00002248
Iteration 37/1000 | Loss: 0.00002248
Iteration 38/1000 | Loss: 0.00002248
Iteration 39/1000 | Loss: 0.00002247
Iteration 40/1000 | Loss: 0.00002247
Iteration 41/1000 | Loss: 0.00002247
Iteration 42/1000 | Loss: 0.00002247
Iteration 43/1000 | Loss: 0.00002246
Iteration 44/1000 | Loss: 0.00002245
Iteration 45/1000 | Loss: 0.00002245
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002243
Iteration 50/1000 | Loss: 0.00002243
Iteration 51/1000 | Loss: 0.00002242
Iteration 52/1000 | Loss: 0.00002242
Iteration 53/1000 | Loss: 0.00002242
Iteration 54/1000 | Loss: 0.00002242
Iteration 55/1000 | Loss: 0.00002242
Iteration 56/1000 | Loss: 0.00002241
Iteration 57/1000 | Loss: 0.00002241
Iteration 58/1000 | Loss: 0.00002241
Iteration 59/1000 | Loss: 0.00002240
Iteration 60/1000 | Loss: 0.00002240
Iteration 61/1000 | Loss: 0.00002240
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002239
Iteration 66/1000 | Loss: 0.00002239
Iteration 67/1000 | Loss: 0.00002239
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002238
Iteration 70/1000 | Loss: 0.00002237
Iteration 71/1000 | Loss: 0.00002237
Iteration 72/1000 | Loss: 0.00002237
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002235
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002232
Iteration 82/1000 | Loss: 0.00002232
Iteration 83/1000 | Loss: 0.00002232
Iteration 84/1000 | Loss: 0.00002232
Iteration 85/1000 | Loss: 0.00002231
Iteration 86/1000 | Loss: 0.00002231
Iteration 87/1000 | Loss: 0.00002231
Iteration 88/1000 | Loss: 0.00002231
Iteration 89/1000 | Loss: 0.00002230
Iteration 90/1000 | Loss: 0.00002230
Iteration 91/1000 | Loss: 0.00002230
Iteration 92/1000 | Loss: 0.00002229
Iteration 93/1000 | Loss: 0.00002229
Iteration 94/1000 | Loss: 0.00002228
Iteration 95/1000 | Loss: 0.00002228
Iteration 96/1000 | Loss: 0.00002228
Iteration 97/1000 | Loss: 0.00002227
Iteration 98/1000 | Loss: 0.00002227
Iteration 99/1000 | Loss: 0.00002227
Iteration 100/1000 | Loss: 0.00002226
Iteration 101/1000 | Loss: 0.00002226
Iteration 102/1000 | Loss: 0.00002226
Iteration 103/1000 | Loss: 0.00002226
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002226
Iteration 106/1000 | Loss: 0.00002226
Iteration 107/1000 | Loss: 0.00002225
Iteration 108/1000 | Loss: 0.00002225
Iteration 109/1000 | Loss: 0.00002225
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002224
Iteration 112/1000 | Loss: 0.00002224
Iteration 113/1000 | Loss: 0.00002223
Iteration 114/1000 | Loss: 0.00002223
Iteration 115/1000 | Loss: 0.00002223
Iteration 116/1000 | Loss: 0.00002222
Iteration 117/1000 | Loss: 0.00002222
Iteration 118/1000 | Loss: 0.00002222
Iteration 119/1000 | Loss: 0.00002222
Iteration 120/1000 | Loss: 0.00002222
Iteration 121/1000 | Loss: 0.00002222
Iteration 122/1000 | Loss: 0.00002222
Iteration 123/1000 | Loss: 0.00002222
Iteration 124/1000 | Loss: 0.00002222
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Iteration 127/1000 | Loss: 0.00002221
Iteration 128/1000 | Loss: 0.00002221
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002220
Iteration 131/1000 | Loss: 0.00002220
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002219
Iteration 135/1000 | Loss: 0.00002219
Iteration 136/1000 | Loss: 0.00002219
Iteration 137/1000 | Loss: 0.00002219
Iteration 138/1000 | Loss: 0.00002219
Iteration 139/1000 | Loss: 0.00002219
Iteration 140/1000 | Loss: 0.00002219
Iteration 141/1000 | Loss: 0.00002219
Iteration 142/1000 | Loss: 0.00002219
Iteration 143/1000 | Loss: 0.00002219
Iteration 144/1000 | Loss: 0.00002219
Iteration 145/1000 | Loss: 0.00002218
Iteration 146/1000 | Loss: 0.00002218
Iteration 147/1000 | Loss: 0.00002218
Iteration 148/1000 | Loss: 0.00002218
Iteration 149/1000 | Loss: 0.00002218
Iteration 150/1000 | Loss: 0.00002218
Iteration 151/1000 | Loss: 0.00002218
Iteration 152/1000 | Loss: 0.00002218
Iteration 153/1000 | Loss: 0.00002218
Iteration 154/1000 | Loss: 0.00002218
Iteration 155/1000 | Loss: 0.00002218
Iteration 156/1000 | Loss: 0.00002218
Iteration 157/1000 | Loss: 0.00002218
Iteration 158/1000 | Loss: 0.00002218
Iteration 159/1000 | Loss: 0.00002218
Iteration 160/1000 | Loss: 0.00002218
Iteration 161/1000 | Loss: 0.00002218
Iteration 162/1000 | Loss: 0.00002217
Iteration 163/1000 | Loss: 0.00002217
Iteration 164/1000 | Loss: 0.00002217
Iteration 165/1000 | Loss: 0.00002217
Iteration 166/1000 | Loss: 0.00002217
Iteration 167/1000 | Loss: 0.00002217
Iteration 168/1000 | Loss: 0.00002216
Iteration 169/1000 | Loss: 0.00002216
Iteration 170/1000 | Loss: 0.00002216
Iteration 171/1000 | Loss: 0.00002216
Iteration 172/1000 | Loss: 0.00002216
Iteration 173/1000 | Loss: 0.00002216
Iteration 174/1000 | Loss: 0.00002216
Iteration 175/1000 | Loss: 0.00002216
Iteration 176/1000 | Loss: 0.00002216
Iteration 177/1000 | Loss: 0.00002216
Iteration 178/1000 | Loss: 0.00002216
Iteration 179/1000 | Loss: 0.00002216
Iteration 180/1000 | Loss: 0.00002215
Iteration 181/1000 | Loss: 0.00002215
Iteration 182/1000 | Loss: 0.00002215
Iteration 183/1000 | Loss: 0.00002215
Iteration 184/1000 | Loss: 0.00002215
Iteration 185/1000 | Loss: 0.00002215
Iteration 186/1000 | Loss: 0.00002215
Iteration 187/1000 | Loss: 0.00002215
Iteration 188/1000 | Loss: 0.00002215
Iteration 189/1000 | Loss: 0.00002215
Iteration 190/1000 | Loss: 0.00002215
Iteration 191/1000 | Loss: 0.00002215
Iteration 192/1000 | Loss: 0.00002214
Iteration 193/1000 | Loss: 0.00002214
Iteration 194/1000 | Loss: 0.00002214
Iteration 195/1000 | Loss: 0.00002214
Iteration 196/1000 | Loss: 0.00002214
Iteration 197/1000 | Loss: 0.00002214
Iteration 198/1000 | Loss: 0.00002214
Iteration 199/1000 | Loss: 0.00002214
Iteration 200/1000 | Loss: 0.00002214
Iteration 201/1000 | Loss: 0.00002214
Iteration 202/1000 | Loss: 0.00002213
Iteration 203/1000 | Loss: 0.00002213
Iteration 204/1000 | Loss: 0.00002213
Iteration 205/1000 | Loss: 0.00002213
Iteration 206/1000 | Loss: 0.00002213
Iteration 207/1000 | Loss: 0.00002213
Iteration 208/1000 | Loss: 0.00002213
Iteration 209/1000 | Loss: 0.00002213
Iteration 210/1000 | Loss: 0.00002213
Iteration 211/1000 | Loss: 0.00002213
Iteration 212/1000 | Loss: 0.00002213
Iteration 213/1000 | Loss: 0.00002213
Iteration 214/1000 | Loss: 0.00002213
Iteration 215/1000 | Loss: 0.00002213
Iteration 216/1000 | Loss: 0.00002213
Iteration 217/1000 | Loss: 0.00002213
Iteration 218/1000 | Loss: 0.00002213
Iteration 219/1000 | Loss: 0.00002213
Iteration 220/1000 | Loss: 0.00002213
Iteration 221/1000 | Loss: 0.00002213
Iteration 222/1000 | Loss: 0.00002213
Iteration 223/1000 | Loss: 0.00002213
Iteration 224/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.2133204765850678e-05, 2.2133204765850678e-05, 2.2133204765850678e-05, 2.2133204765850678e-05, 2.2133204765850678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2133204765850678e-05

Optimization complete. Final v2v error: 3.931217908859253 mm

Highest mean error: 4.292010307312012 mm for frame 20

Lowest mean error: 3.6192209720611572 mm for frame 173

Saving results

Total time: 51.18675780296326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522024
Iteration 2/25 | Loss: 0.00129651
Iteration 3/25 | Loss: 0.00117776
Iteration 4/25 | Loss: 0.00117046
Iteration 5/25 | Loss: 0.00116888
Iteration 6/25 | Loss: 0.00116888
Iteration 7/25 | Loss: 0.00116888
Iteration 8/25 | Loss: 0.00116888
Iteration 9/25 | Loss: 0.00116888
Iteration 10/25 | Loss: 0.00116888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011688773520290852, 0.0011688773520290852, 0.0011688773520290852, 0.0011688773520290852, 0.0011688773520290852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011688773520290852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76613563
Iteration 2/25 | Loss: 0.00087402
Iteration 3/25 | Loss: 0.00087401
Iteration 4/25 | Loss: 0.00087401
Iteration 5/25 | Loss: 0.00087401
Iteration 6/25 | Loss: 0.00087401
Iteration 7/25 | Loss: 0.00087401
Iteration 8/25 | Loss: 0.00087401
Iteration 9/25 | Loss: 0.00087401
Iteration 10/25 | Loss: 0.00087401
Iteration 11/25 | Loss: 0.00087401
Iteration 12/25 | Loss: 0.00087401
Iteration 13/25 | Loss: 0.00087401
Iteration 14/25 | Loss: 0.00087401
Iteration 15/25 | Loss: 0.00087401
Iteration 16/25 | Loss: 0.00087401
Iteration 17/25 | Loss: 0.00087401
Iteration 18/25 | Loss: 0.00087401
Iteration 19/25 | Loss: 0.00087401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00087400694610551, 0.00087400694610551, 0.00087400694610551, 0.00087400694610551, 0.00087400694610551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00087400694610551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087401
Iteration 2/1000 | Loss: 0.00003562
Iteration 3/1000 | Loss: 0.00002064
Iteration 4/1000 | Loss: 0.00001781
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00001636
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001581
Iteration 11/1000 | Loss: 0.00001580
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001577
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001569
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001563
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001563
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.5629355402779765e-05, 1.5629355402779765e-05, 1.5629355402779765e-05, 1.5629355402779765e-05, 1.5629355402779765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5629355402779765e-05

Optimization complete. Final v2v error: 3.3035714626312256 mm

Highest mean error: 3.4506032466888428 mm for frame 238

Lowest mean error: 3.1877481937408447 mm for frame 265

Saving results

Total time: 29.245140314102173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866009
Iteration 2/25 | Loss: 0.00122537
Iteration 3/25 | Loss: 0.00115101
Iteration 4/25 | Loss: 0.00113962
Iteration 5/25 | Loss: 0.00113679
Iteration 6/25 | Loss: 0.00113608
Iteration 7/25 | Loss: 0.00113608
Iteration 8/25 | Loss: 0.00113608
Iteration 9/25 | Loss: 0.00113608
Iteration 10/25 | Loss: 0.00113608
Iteration 11/25 | Loss: 0.00113608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011360762873664498, 0.0011360762873664498, 0.0011360762873664498, 0.0011360762873664498, 0.0011360762873664498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011360762873664498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.00255823
Iteration 2/25 | Loss: 0.00128970
Iteration 3/25 | Loss: 0.00128970
Iteration 4/25 | Loss: 0.00128970
Iteration 5/25 | Loss: 0.00128969
Iteration 6/25 | Loss: 0.00128969
Iteration 7/25 | Loss: 0.00128969
Iteration 8/25 | Loss: 0.00128969
Iteration 9/25 | Loss: 0.00128969
Iteration 10/25 | Loss: 0.00128969
Iteration 11/25 | Loss: 0.00128969
Iteration 12/25 | Loss: 0.00128969
Iteration 13/25 | Loss: 0.00128969
Iteration 14/25 | Loss: 0.00128969
Iteration 15/25 | Loss: 0.00128969
Iteration 16/25 | Loss: 0.00128969
Iteration 17/25 | Loss: 0.00128969
Iteration 18/25 | Loss: 0.00128969
Iteration 19/25 | Loss: 0.00128969
Iteration 20/25 | Loss: 0.00128969
Iteration 21/25 | Loss: 0.00128969
Iteration 22/25 | Loss: 0.00128969
Iteration 23/25 | Loss: 0.00128969
Iteration 24/25 | Loss: 0.00128969
Iteration 25/25 | Loss: 0.00128969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128969
Iteration 2/1000 | Loss: 0.00005384
Iteration 3/1000 | Loss: 0.00002165
Iteration 4/1000 | Loss: 0.00001858
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001439
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001411
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001407
Iteration 26/1000 | Loss: 0.00001407
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001406
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001400
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001393
Iteration 88/1000 | Loss: 0.00001393
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001392
Iteration 95/1000 | Loss: 0.00001392
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001389
Iteration 121/1000 | Loss: 0.00001389
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001388
Iteration 127/1000 | Loss: 0.00001388
Iteration 128/1000 | Loss: 0.00001388
Iteration 129/1000 | Loss: 0.00001388
Iteration 130/1000 | Loss: 0.00001388
Iteration 131/1000 | Loss: 0.00001388
Iteration 132/1000 | Loss: 0.00001388
Iteration 133/1000 | Loss: 0.00001388
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00001388
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001388
Iteration 146/1000 | Loss: 0.00001388
Iteration 147/1000 | Loss: 0.00001388
Iteration 148/1000 | Loss: 0.00001388
Iteration 149/1000 | Loss: 0.00001388
Iteration 150/1000 | Loss: 0.00001388
Iteration 151/1000 | Loss: 0.00001388
Iteration 152/1000 | Loss: 0.00001388
Iteration 153/1000 | Loss: 0.00001388
Iteration 154/1000 | Loss: 0.00001388
Iteration 155/1000 | Loss: 0.00001388
Iteration 156/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.3875092918169685e-05, 1.3875092918169685e-05, 1.3875092918169685e-05, 1.3875092918169685e-05, 1.3875092918169685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3875092918169685e-05

Optimization complete. Final v2v error: 3.216693162918091 mm

Highest mean error: 3.589749574661255 mm for frame 80

Lowest mean error: 2.8854198455810547 mm for frame 88

Saving results

Total time: 34.79410171508789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398209
Iteration 2/25 | Loss: 0.00133461
Iteration 3/25 | Loss: 0.00123014
Iteration 4/25 | Loss: 0.00120822
Iteration 5/25 | Loss: 0.00120080
Iteration 6/25 | Loss: 0.00119862
Iteration 7/25 | Loss: 0.00119815
Iteration 8/25 | Loss: 0.00119815
Iteration 9/25 | Loss: 0.00119815
Iteration 10/25 | Loss: 0.00119815
Iteration 11/25 | Loss: 0.00119815
Iteration 12/25 | Loss: 0.00119815
Iteration 13/25 | Loss: 0.00119815
Iteration 14/25 | Loss: 0.00119815
Iteration 15/25 | Loss: 0.00119815
Iteration 16/25 | Loss: 0.00119815
Iteration 17/25 | Loss: 0.00119815
Iteration 18/25 | Loss: 0.00119815
Iteration 19/25 | Loss: 0.00119815
Iteration 20/25 | Loss: 0.00119815
Iteration 21/25 | Loss: 0.00119815
Iteration 22/25 | Loss: 0.00119815
Iteration 23/25 | Loss: 0.00119815
Iteration 24/25 | Loss: 0.00119815
Iteration 25/25 | Loss: 0.00119815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33339572
Iteration 2/25 | Loss: 0.00200231
Iteration 3/25 | Loss: 0.00200231
Iteration 4/25 | Loss: 0.00200230
Iteration 5/25 | Loss: 0.00200230
Iteration 6/25 | Loss: 0.00200230
Iteration 7/25 | Loss: 0.00200230
Iteration 8/25 | Loss: 0.00200230
Iteration 9/25 | Loss: 0.00200230
Iteration 10/25 | Loss: 0.00200230
Iteration 11/25 | Loss: 0.00200230
Iteration 12/25 | Loss: 0.00200230
Iteration 13/25 | Loss: 0.00200230
Iteration 14/25 | Loss: 0.00200230
Iteration 15/25 | Loss: 0.00200230
Iteration 16/25 | Loss: 0.00200230
Iteration 17/25 | Loss: 0.00200230
Iteration 18/25 | Loss: 0.00200230
Iteration 19/25 | Loss: 0.00200230
Iteration 20/25 | Loss: 0.00200230
Iteration 21/25 | Loss: 0.00200230
Iteration 22/25 | Loss: 0.00200230
Iteration 23/25 | Loss: 0.00200230
Iteration 24/25 | Loss: 0.00200230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020023018587380648, 0.0020023018587380648, 0.0020023018587380648, 0.0020023018587380648, 0.0020023018587380648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020023018587380648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200230
Iteration 2/1000 | Loss: 0.00010388
Iteration 3/1000 | Loss: 0.00004841
Iteration 4/1000 | Loss: 0.00003258
Iteration 5/1000 | Loss: 0.00002670
Iteration 6/1000 | Loss: 0.00002424
Iteration 7/1000 | Loss: 0.00002260
Iteration 8/1000 | Loss: 0.00002177
Iteration 9/1000 | Loss: 0.00002109
Iteration 10/1000 | Loss: 0.00002060
Iteration 11/1000 | Loss: 0.00002020
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001973
Iteration 14/1000 | Loss: 0.00001969
Iteration 15/1000 | Loss: 0.00001957
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001946
Iteration 19/1000 | Loss: 0.00001942
Iteration 20/1000 | Loss: 0.00001936
Iteration 21/1000 | Loss: 0.00001934
Iteration 22/1000 | Loss: 0.00001933
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001930
Iteration 32/1000 | Loss: 0.00001929
Iteration 33/1000 | Loss: 0.00001929
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001928
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001926
Iteration 40/1000 | Loss: 0.00001926
Iteration 41/1000 | Loss: 0.00001926
Iteration 42/1000 | Loss: 0.00001925
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001925
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001923
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001922
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001921
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001918
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001918
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001916
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001916
Iteration 90/1000 | Loss: 0.00001916
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001916
Iteration 99/1000 | Loss: 0.00001916
Iteration 100/1000 | Loss: 0.00001916
Iteration 101/1000 | Loss: 0.00001916
Iteration 102/1000 | Loss: 0.00001916
Iteration 103/1000 | Loss: 0.00001916
Iteration 104/1000 | Loss: 0.00001916
Iteration 105/1000 | Loss: 0.00001916
Iteration 106/1000 | Loss: 0.00001916
Iteration 107/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.9157005226588808e-05, 1.9157005226588808e-05, 1.9157005226588808e-05, 1.9157005226588808e-05, 1.9157005226588808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9157005226588808e-05

Optimization complete. Final v2v error: 3.6376359462738037 mm

Highest mean error: 4.569359302520752 mm for frame 75

Lowest mean error: 2.8717844486236572 mm for frame 86

Saving results

Total time: 38.73642039299011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976637
Iteration 2/25 | Loss: 0.00267057
Iteration 3/25 | Loss: 0.00187779
Iteration 4/25 | Loss: 0.00180001
Iteration 5/25 | Loss: 0.00175583
Iteration 6/25 | Loss: 0.00170626
Iteration 7/25 | Loss: 0.00170496
Iteration 8/25 | Loss: 0.00168561
Iteration 9/25 | Loss: 0.00168996
Iteration 10/25 | Loss: 0.00168377
Iteration 11/25 | Loss: 0.00167747
Iteration 12/25 | Loss: 0.00167456
Iteration 13/25 | Loss: 0.00167308
Iteration 14/25 | Loss: 0.00167221
Iteration 15/25 | Loss: 0.00167139
Iteration 16/25 | Loss: 0.00167112
Iteration 17/25 | Loss: 0.00167099
Iteration 18/25 | Loss: 0.00167091
Iteration 19/25 | Loss: 0.00167090
Iteration 20/25 | Loss: 0.00167089
Iteration 21/25 | Loss: 0.00167089
Iteration 22/25 | Loss: 0.00167089
Iteration 23/25 | Loss: 0.00167089
Iteration 24/25 | Loss: 0.00167089
Iteration 25/25 | Loss: 0.00167088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27326465
Iteration 2/25 | Loss: 0.00552583
Iteration 3/25 | Loss: 0.00552581
Iteration 4/25 | Loss: 0.00552581
Iteration 5/25 | Loss: 0.00552580
Iteration 6/25 | Loss: 0.00552580
Iteration 7/25 | Loss: 0.00552580
Iteration 8/25 | Loss: 0.00552580
Iteration 9/25 | Loss: 0.00552580
Iteration 10/25 | Loss: 0.00552580
Iteration 11/25 | Loss: 0.00552580
Iteration 12/25 | Loss: 0.00552580
Iteration 13/25 | Loss: 0.00552580
Iteration 14/25 | Loss: 0.00552580
Iteration 15/25 | Loss: 0.00552580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005525802727788687, 0.005525802727788687, 0.005525802727788687, 0.005525802727788687, 0.005525802727788687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005525802727788687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00552580
Iteration 2/1000 | Loss: 0.00117450
Iteration 3/1000 | Loss: 0.00056598
Iteration 4/1000 | Loss: 0.00177493
Iteration 5/1000 | Loss: 0.00506288
Iteration 6/1000 | Loss: 0.01006686
Iteration 7/1000 | Loss: 0.00177838
Iteration 8/1000 | Loss: 0.00510778
Iteration 9/1000 | Loss: 0.00631652
Iteration 10/1000 | Loss: 0.00530946
Iteration 11/1000 | Loss: 0.00255242
Iteration 12/1000 | Loss: 0.00092562
Iteration 13/1000 | Loss: 0.00319942
Iteration 14/1000 | Loss: 0.00206531
Iteration 15/1000 | Loss: 0.00150041
Iteration 16/1000 | Loss: 0.00036923
Iteration 17/1000 | Loss: 0.00126001
Iteration 18/1000 | Loss: 0.00102354
Iteration 19/1000 | Loss: 0.00076271
Iteration 20/1000 | Loss: 0.00039468
Iteration 21/1000 | Loss: 0.00048907
Iteration 22/1000 | Loss: 0.00048157
Iteration 23/1000 | Loss: 0.00029881
Iteration 24/1000 | Loss: 0.00015904
Iteration 25/1000 | Loss: 0.00332540
Iteration 26/1000 | Loss: 0.00736914
Iteration 27/1000 | Loss: 0.00100470
Iteration 28/1000 | Loss: 0.00058175
Iteration 29/1000 | Loss: 0.00071810
Iteration 30/1000 | Loss: 0.00074113
Iteration 31/1000 | Loss: 0.00026362
Iteration 32/1000 | Loss: 0.00038797
Iteration 33/1000 | Loss: 0.00036865
Iteration 34/1000 | Loss: 0.00041761
Iteration 35/1000 | Loss: 0.00036799
Iteration 36/1000 | Loss: 0.00034049
Iteration 37/1000 | Loss: 0.00161360
Iteration 38/1000 | Loss: 0.00018293
Iteration 39/1000 | Loss: 0.00216125
Iteration 40/1000 | Loss: 0.00155542
Iteration 41/1000 | Loss: 0.00211520
Iteration 42/1000 | Loss: 0.00206721
Iteration 43/1000 | Loss: 0.00124072
Iteration 44/1000 | Loss: 0.00021246
Iteration 45/1000 | Loss: 0.00080786
Iteration 46/1000 | Loss: 0.00024896
Iteration 47/1000 | Loss: 0.00021139
Iteration 48/1000 | Loss: 0.00017424
Iteration 49/1000 | Loss: 0.00059559
Iteration 50/1000 | Loss: 0.00095675
Iteration 51/1000 | Loss: 0.00050325
Iteration 52/1000 | Loss: 0.00051238
Iteration 53/1000 | Loss: 0.00094631
Iteration 54/1000 | Loss: 0.00072010
Iteration 55/1000 | Loss: 0.00085802
Iteration 56/1000 | Loss: 0.00098203
Iteration 57/1000 | Loss: 0.00075621
Iteration 58/1000 | Loss: 0.00084062
Iteration 59/1000 | Loss: 0.00053788
Iteration 60/1000 | Loss: 0.00081931
Iteration 61/1000 | Loss: 0.00102658
Iteration 62/1000 | Loss: 0.00061175
Iteration 63/1000 | Loss: 0.00049926
Iteration 64/1000 | Loss: 0.00052162
Iteration 65/1000 | Loss: 0.00108840
Iteration 66/1000 | Loss: 0.00057884
Iteration 67/1000 | Loss: 0.00073487
Iteration 68/1000 | Loss: 0.00089134
Iteration 69/1000 | Loss: 0.00072892
Iteration 70/1000 | Loss: 0.00041599
Iteration 71/1000 | Loss: 0.00012849
Iteration 72/1000 | Loss: 0.00011811
Iteration 73/1000 | Loss: 0.00018455
Iteration 74/1000 | Loss: 0.00025197
Iteration 75/1000 | Loss: 0.00045133
Iteration 76/1000 | Loss: 0.00205671
Iteration 77/1000 | Loss: 0.00098632
Iteration 78/1000 | Loss: 0.00290651
Iteration 79/1000 | Loss: 0.00121968
Iteration 80/1000 | Loss: 0.00133700
Iteration 81/1000 | Loss: 0.00097874
Iteration 82/1000 | Loss: 0.00075690
Iteration 83/1000 | Loss: 0.00077211
Iteration 84/1000 | Loss: 0.00048525
Iteration 85/1000 | Loss: 0.00122548
Iteration 86/1000 | Loss: 0.00063202
Iteration 87/1000 | Loss: 0.00067399
Iteration 88/1000 | Loss: 0.00033009
Iteration 89/1000 | Loss: 0.00029629
Iteration 90/1000 | Loss: 0.00089747
Iteration 91/1000 | Loss: 0.00027709
Iteration 92/1000 | Loss: 0.00061840
Iteration 93/1000 | Loss: 0.00059261
Iteration 94/1000 | Loss: 0.00098255
Iteration 95/1000 | Loss: 0.00071329
Iteration 96/1000 | Loss: 0.00117498
Iteration 97/1000 | Loss: 0.00099579
Iteration 98/1000 | Loss: 0.00087127
Iteration 99/1000 | Loss: 0.00122834
Iteration 100/1000 | Loss: 0.00231276
Iteration 101/1000 | Loss: 0.00151659
Iteration 102/1000 | Loss: 0.00121413
Iteration 103/1000 | Loss: 0.00179103
Iteration 104/1000 | Loss: 0.00125070
Iteration 105/1000 | Loss: 0.00089897
Iteration 106/1000 | Loss: 0.00182916
Iteration 107/1000 | Loss: 0.00142724
Iteration 108/1000 | Loss: 0.00235267
Iteration 109/1000 | Loss: 0.00232059
Iteration 110/1000 | Loss: 0.00191036
Iteration 111/1000 | Loss: 0.00190313
Iteration 112/1000 | Loss: 0.00152778
Iteration 113/1000 | Loss: 0.00095514
Iteration 114/1000 | Loss: 0.00178851
Iteration 115/1000 | Loss: 0.00160327
Iteration 116/1000 | Loss: 0.00138401
Iteration 117/1000 | Loss: 0.00081504
Iteration 118/1000 | Loss: 0.00115911
Iteration 119/1000 | Loss: 0.00153670
Iteration 120/1000 | Loss: 0.00180693
Iteration 121/1000 | Loss: 0.00098745
Iteration 122/1000 | Loss: 0.00097581
Iteration 123/1000 | Loss: 0.00090521
Iteration 124/1000 | Loss: 0.00108144
Iteration 125/1000 | Loss: 0.00103364
Iteration 126/1000 | Loss: 0.00134778
Iteration 127/1000 | Loss: 0.00121853
Iteration 128/1000 | Loss: 0.00037681
Iteration 129/1000 | Loss: 0.00064698
Iteration 130/1000 | Loss: 0.00046899
Iteration 131/1000 | Loss: 0.00084488
Iteration 132/1000 | Loss: 0.00085992
Iteration 133/1000 | Loss: 0.00096162
Iteration 134/1000 | Loss: 0.00071744
Iteration 135/1000 | Loss: 0.00051067
Iteration 136/1000 | Loss: 0.00023958
Iteration 137/1000 | Loss: 0.00022168
Iteration 138/1000 | Loss: 0.00113707
Iteration 139/1000 | Loss: 0.00076468
Iteration 140/1000 | Loss: 0.00062046
Iteration 141/1000 | Loss: 0.00078711
Iteration 142/1000 | Loss: 0.00030637
Iteration 143/1000 | Loss: 0.00049505
Iteration 144/1000 | Loss: 0.00029917
Iteration 145/1000 | Loss: 0.00048171
Iteration 146/1000 | Loss: 0.00023836
Iteration 147/1000 | Loss: 0.00050872
Iteration 148/1000 | Loss: 0.00114949
Iteration 149/1000 | Loss: 0.00028968
Iteration 150/1000 | Loss: 0.00036316
Iteration 151/1000 | Loss: 0.00046030
Iteration 152/1000 | Loss: 0.00041911
Iteration 153/1000 | Loss: 0.00088016
Iteration 154/1000 | Loss: 0.00067934
Iteration 155/1000 | Loss: 0.00146025
Iteration 156/1000 | Loss: 0.00164479
Iteration 157/1000 | Loss: 0.00255956
Iteration 158/1000 | Loss: 0.00074646
Iteration 159/1000 | Loss: 0.00113349
Iteration 160/1000 | Loss: 0.00134615
Iteration 161/1000 | Loss: 0.00079047
Iteration 162/1000 | Loss: 0.00122648
Iteration 163/1000 | Loss: 0.00119406
Iteration 164/1000 | Loss: 0.00117332
Iteration 165/1000 | Loss: 0.00118280
Iteration 166/1000 | Loss: 0.00084582
Iteration 167/1000 | Loss: 0.00055596
Iteration 168/1000 | Loss: 0.00017067
Iteration 169/1000 | Loss: 0.00149443
Iteration 170/1000 | Loss: 0.00146864
Iteration 171/1000 | Loss: 0.00045237
Iteration 172/1000 | Loss: 0.00025160
Iteration 173/1000 | Loss: 0.00031479
Iteration 174/1000 | Loss: 0.00016928
Iteration 175/1000 | Loss: 0.00009953
Iteration 176/1000 | Loss: 0.00010216
Iteration 177/1000 | Loss: 0.00005249
Iteration 178/1000 | Loss: 0.00062944
Iteration 179/1000 | Loss: 0.00060743
Iteration 180/1000 | Loss: 0.00025801
Iteration 181/1000 | Loss: 0.00006934
Iteration 182/1000 | Loss: 0.00016681
Iteration 183/1000 | Loss: 0.00006947
Iteration 184/1000 | Loss: 0.00003952
Iteration 185/1000 | Loss: 0.00003594
Iteration 186/1000 | Loss: 0.00026328
Iteration 187/1000 | Loss: 0.00032692
Iteration 188/1000 | Loss: 0.00015693
Iteration 189/1000 | Loss: 0.00004511
Iteration 190/1000 | Loss: 0.00007231
Iteration 191/1000 | Loss: 0.00021637
Iteration 192/1000 | Loss: 0.00012928
Iteration 193/1000 | Loss: 0.00020684
Iteration 194/1000 | Loss: 0.00014444
Iteration 195/1000 | Loss: 0.00015536
Iteration 196/1000 | Loss: 0.00026085
Iteration 197/1000 | Loss: 0.00014038
Iteration 198/1000 | Loss: 0.00010622
Iteration 199/1000 | Loss: 0.00039171
Iteration 200/1000 | Loss: 0.00022415
Iteration 201/1000 | Loss: 0.00029309
Iteration 202/1000 | Loss: 0.00045012
Iteration 203/1000 | Loss: 0.00014127
Iteration 204/1000 | Loss: 0.00003572
Iteration 205/1000 | Loss: 0.00003188
Iteration 206/1000 | Loss: 0.00003103
Iteration 207/1000 | Loss: 0.00015227
Iteration 208/1000 | Loss: 0.00027162
Iteration 209/1000 | Loss: 0.00037332
Iteration 210/1000 | Loss: 0.00029768
Iteration 211/1000 | Loss: 0.00016892
Iteration 212/1000 | Loss: 0.00026103
Iteration 213/1000 | Loss: 0.00025656
Iteration 214/1000 | Loss: 0.00005616
Iteration 215/1000 | Loss: 0.00011656
Iteration 216/1000 | Loss: 0.00020442
Iteration 217/1000 | Loss: 0.00028511
Iteration 218/1000 | Loss: 0.00026309
Iteration 219/1000 | Loss: 0.00048121
Iteration 220/1000 | Loss: 0.00042055
Iteration 221/1000 | Loss: 0.00088831
Iteration 222/1000 | Loss: 0.00054943
Iteration 223/1000 | Loss: 0.00021592
Iteration 224/1000 | Loss: 0.00088440
Iteration 225/1000 | Loss: 0.00102231
Iteration 226/1000 | Loss: 0.00010676
Iteration 227/1000 | Loss: 0.00016597
Iteration 228/1000 | Loss: 0.00042272
Iteration 229/1000 | Loss: 0.00003954
Iteration 230/1000 | Loss: 0.00020563
Iteration 231/1000 | Loss: 0.00013714
Iteration 232/1000 | Loss: 0.00003641
Iteration 233/1000 | Loss: 0.00009639
Iteration 234/1000 | Loss: 0.00019292
Iteration 235/1000 | Loss: 0.00015802
Iteration 236/1000 | Loss: 0.00031170
Iteration 237/1000 | Loss: 0.00028868
Iteration 238/1000 | Loss: 0.00030202
Iteration 239/1000 | Loss: 0.00028615
Iteration 240/1000 | Loss: 0.00067221
Iteration 241/1000 | Loss: 0.00029291
Iteration 242/1000 | Loss: 0.00059891
Iteration 243/1000 | Loss: 0.00064530
Iteration 244/1000 | Loss: 0.00006072
Iteration 245/1000 | Loss: 0.00004556
Iteration 246/1000 | Loss: 0.00017068
Iteration 247/1000 | Loss: 0.00004135
Iteration 248/1000 | Loss: 0.00003626
Iteration 249/1000 | Loss: 0.00003318
Iteration 250/1000 | Loss: 0.00003084
Iteration 251/1000 | Loss: 0.00002911
Iteration 252/1000 | Loss: 0.00002700
Iteration 253/1000 | Loss: 0.00002599
Iteration 254/1000 | Loss: 0.00002525
Iteration 255/1000 | Loss: 0.00065048
Iteration 256/1000 | Loss: 0.00059210
Iteration 257/1000 | Loss: 0.00006216
Iteration 258/1000 | Loss: 0.00002656
Iteration 259/1000 | Loss: 0.00002488
Iteration 260/1000 | Loss: 0.00002454
Iteration 261/1000 | Loss: 0.00002428
Iteration 262/1000 | Loss: 0.00064385
Iteration 263/1000 | Loss: 0.00015255
Iteration 264/1000 | Loss: 0.00012054
Iteration 265/1000 | Loss: 0.00003177
Iteration 266/1000 | Loss: 0.00002746
Iteration 267/1000 | Loss: 0.00002479
Iteration 268/1000 | Loss: 0.00056885
Iteration 269/1000 | Loss: 0.00023572
Iteration 270/1000 | Loss: 0.00030323
Iteration 271/1000 | Loss: 0.00003780
Iteration 272/1000 | Loss: 0.00003117
Iteration 273/1000 | Loss: 0.00002772
Iteration 274/1000 | Loss: 0.00002454
Iteration 275/1000 | Loss: 0.00002204
Iteration 276/1000 | Loss: 0.00002073
Iteration 277/1000 | Loss: 0.00002006
Iteration 278/1000 | Loss: 0.00001980
Iteration 279/1000 | Loss: 0.00001966
Iteration 280/1000 | Loss: 0.00001954
Iteration 281/1000 | Loss: 0.00001950
Iteration 282/1000 | Loss: 0.00001944
Iteration 283/1000 | Loss: 0.00001942
Iteration 284/1000 | Loss: 0.00001941
Iteration 285/1000 | Loss: 0.00001940
Iteration 286/1000 | Loss: 0.00001940
Iteration 287/1000 | Loss: 0.00001940
Iteration 288/1000 | Loss: 0.00001939
Iteration 289/1000 | Loss: 0.00001938
Iteration 290/1000 | Loss: 0.00001938
Iteration 291/1000 | Loss: 0.00001938
Iteration 292/1000 | Loss: 0.00001938
Iteration 293/1000 | Loss: 0.00001938
Iteration 294/1000 | Loss: 0.00001937
Iteration 295/1000 | Loss: 0.00001937
Iteration 296/1000 | Loss: 0.00001937
Iteration 297/1000 | Loss: 0.00001937
Iteration 298/1000 | Loss: 0.00001937
Iteration 299/1000 | Loss: 0.00001937
Iteration 300/1000 | Loss: 0.00001937
Iteration 301/1000 | Loss: 0.00001937
Iteration 302/1000 | Loss: 0.00001937
Iteration 303/1000 | Loss: 0.00001937
Iteration 304/1000 | Loss: 0.00001937
Iteration 305/1000 | Loss: 0.00001936
Iteration 306/1000 | Loss: 0.00001936
Iteration 307/1000 | Loss: 0.00001936
Iteration 308/1000 | Loss: 0.00001936
Iteration 309/1000 | Loss: 0.00001936
Iteration 310/1000 | Loss: 0.00001936
Iteration 311/1000 | Loss: 0.00001936
Iteration 312/1000 | Loss: 0.00001936
Iteration 313/1000 | Loss: 0.00001935
Iteration 314/1000 | Loss: 0.00001935
Iteration 315/1000 | Loss: 0.00001935
Iteration 316/1000 | Loss: 0.00001935
Iteration 317/1000 | Loss: 0.00001934
Iteration 318/1000 | Loss: 0.00001934
Iteration 319/1000 | Loss: 0.00001934
Iteration 320/1000 | Loss: 0.00001934
Iteration 321/1000 | Loss: 0.00001934
Iteration 322/1000 | Loss: 0.00001933
Iteration 323/1000 | Loss: 0.00001933
Iteration 324/1000 | Loss: 0.00001933
Iteration 325/1000 | Loss: 0.00001932
Iteration 326/1000 | Loss: 0.00001932
Iteration 327/1000 | Loss: 0.00001932
Iteration 328/1000 | Loss: 0.00001932
Iteration 329/1000 | Loss: 0.00001932
Iteration 330/1000 | Loss: 0.00001932
Iteration 331/1000 | Loss: 0.00001931
Iteration 332/1000 | Loss: 0.00001931
Iteration 333/1000 | Loss: 0.00001931
Iteration 334/1000 | Loss: 0.00001931
Iteration 335/1000 | Loss: 0.00001931
Iteration 336/1000 | Loss: 0.00001931
Iteration 337/1000 | Loss: 0.00001931
Iteration 338/1000 | Loss: 0.00001930
Iteration 339/1000 | Loss: 0.00001930
Iteration 340/1000 | Loss: 0.00001930
Iteration 341/1000 | Loss: 0.00001929
Iteration 342/1000 | Loss: 0.00001929
Iteration 343/1000 | Loss: 0.00001929
Iteration 344/1000 | Loss: 0.00001929
Iteration 345/1000 | Loss: 0.00001929
Iteration 346/1000 | Loss: 0.00001929
Iteration 347/1000 | Loss: 0.00001929
Iteration 348/1000 | Loss: 0.00001929
Iteration 349/1000 | Loss: 0.00001928
Iteration 350/1000 | Loss: 0.00001928
Iteration 351/1000 | Loss: 0.00001928
Iteration 352/1000 | Loss: 0.00001928
Iteration 353/1000 | Loss: 0.00001928
Iteration 354/1000 | Loss: 0.00001928
Iteration 355/1000 | Loss: 0.00001928
Iteration 356/1000 | Loss: 0.00001928
Iteration 357/1000 | Loss: 0.00001928
Iteration 358/1000 | Loss: 0.00001928
Iteration 359/1000 | Loss: 0.00001927
Iteration 360/1000 | Loss: 0.00001927
Iteration 361/1000 | Loss: 0.00001927
Iteration 362/1000 | Loss: 0.00001927
Iteration 363/1000 | Loss: 0.00001927
Iteration 364/1000 | Loss: 0.00001926
Iteration 365/1000 | Loss: 0.00001926
Iteration 366/1000 | Loss: 0.00001926
Iteration 367/1000 | Loss: 0.00001926
Iteration 368/1000 | Loss: 0.00001926
Iteration 369/1000 | Loss: 0.00001926
Iteration 370/1000 | Loss: 0.00001926
Iteration 371/1000 | Loss: 0.00001926
Iteration 372/1000 | Loss: 0.00001926
Iteration 373/1000 | Loss: 0.00001926
Iteration 374/1000 | Loss: 0.00001926
Iteration 375/1000 | Loss: 0.00001926
Iteration 376/1000 | Loss: 0.00001925
Iteration 377/1000 | Loss: 0.00001925
Iteration 378/1000 | Loss: 0.00001925
Iteration 379/1000 | Loss: 0.00001925
Iteration 380/1000 | Loss: 0.00001925
Iteration 381/1000 | Loss: 0.00001924
Iteration 382/1000 | Loss: 0.00001924
Iteration 383/1000 | Loss: 0.00001924
Iteration 384/1000 | Loss: 0.00001924
Iteration 385/1000 | Loss: 0.00001924
Iteration 386/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 386. Stopping optimization.
Last 5 losses: [1.9242417693021707e-05, 1.9242417693021707e-05, 1.9242417693021707e-05, 1.9242417693021707e-05, 1.9242417693021707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9242417693021707e-05

Optimization complete. Final v2v error: 3.28303861618042 mm

Highest mean error: 12.732640266418457 mm for frame 21

Lowest mean error: 2.9663584232330322 mm for frame 106

Saving results

Total time: 491.0254158973694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783820
Iteration 2/25 | Loss: 0.00175637
Iteration 3/25 | Loss: 0.00134199
Iteration 4/25 | Loss: 0.00130484
Iteration 5/25 | Loss: 0.00127324
Iteration 6/25 | Loss: 0.00126243
Iteration 7/25 | Loss: 0.00125979
Iteration 8/25 | Loss: 0.00125620
Iteration 9/25 | Loss: 0.00125516
Iteration 10/25 | Loss: 0.00125637
Iteration 11/25 | Loss: 0.00125271
Iteration 12/25 | Loss: 0.00125461
Iteration 13/25 | Loss: 0.00125354
Iteration 14/25 | Loss: 0.00125388
Iteration 15/25 | Loss: 0.00125304
Iteration 16/25 | Loss: 0.00125266
Iteration 17/25 | Loss: 0.00125343
Iteration 18/25 | Loss: 0.00125216
Iteration 19/25 | Loss: 0.00125321
Iteration 20/25 | Loss: 0.00125263
Iteration 21/25 | Loss: 0.00125073
Iteration 22/25 | Loss: 0.00125053
Iteration 23/25 | Loss: 0.00125042
Iteration 24/25 | Loss: 0.00125374
Iteration 25/25 | Loss: 0.00125387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63148713
Iteration 2/25 | Loss: 0.00141488
Iteration 3/25 | Loss: 0.00141488
Iteration 4/25 | Loss: 0.00141488
Iteration 5/25 | Loss: 0.00141488
Iteration 6/25 | Loss: 0.00141488
Iteration 7/25 | Loss: 0.00141488
Iteration 8/25 | Loss: 0.00141488
Iteration 9/25 | Loss: 0.00141488
Iteration 10/25 | Loss: 0.00141488
Iteration 11/25 | Loss: 0.00141488
Iteration 12/25 | Loss: 0.00141488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014148775953799486, 0.0014148775953799486, 0.0014148775953799486, 0.0014148775953799486, 0.0014148775953799486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014148775953799486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141488
Iteration 2/1000 | Loss: 0.00008462
Iteration 3/1000 | Loss: 0.00004149
Iteration 4/1000 | Loss: 0.00003430
Iteration 5/1000 | Loss: 0.00003069
Iteration 6/1000 | Loss: 0.00002897
Iteration 7/1000 | Loss: 0.00002791
Iteration 8/1000 | Loss: 0.00014287
Iteration 9/1000 | Loss: 0.00003299
Iteration 10/1000 | Loss: 0.00003035
Iteration 11/1000 | Loss: 0.00002920
Iteration 12/1000 | Loss: 0.00002839
Iteration 13/1000 | Loss: 0.00002795
Iteration 14/1000 | Loss: 0.00017981
Iteration 15/1000 | Loss: 0.00011957
Iteration 16/1000 | Loss: 0.00016412
Iteration 17/1000 | Loss: 0.00013309
Iteration 18/1000 | Loss: 0.00012409
Iteration 19/1000 | Loss: 0.00004521
Iteration 20/1000 | Loss: 0.00004737
Iteration 21/1000 | Loss: 0.00002887
Iteration 22/1000 | Loss: 0.00002713
Iteration 23/1000 | Loss: 0.00002642
Iteration 24/1000 | Loss: 0.00002592
Iteration 25/1000 | Loss: 0.00002553
Iteration 26/1000 | Loss: 0.00002526
Iteration 27/1000 | Loss: 0.00002507
Iteration 28/1000 | Loss: 0.00002493
Iteration 29/1000 | Loss: 0.00002489
Iteration 30/1000 | Loss: 0.00002485
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002475
Iteration 34/1000 | Loss: 0.00002471
Iteration 35/1000 | Loss: 0.00002469
Iteration 36/1000 | Loss: 0.00002469
Iteration 37/1000 | Loss: 0.00002469
Iteration 38/1000 | Loss: 0.00002469
Iteration 39/1000 | Loss: 0.00002469
Iteration 40/1000 | Loss: 0.00002469
Iteration 41/1000 | Loss: 0.00002469
Iteration 42/1000 | Loss: 0.00002468
Iteration 43/1000 | Loss: 0.00002468
Iteration 44/1000 | Loss: 0.00002467
Iteration 45/1000 | Loss: 0.00002466
Iteration 46/1000 | Loss: 0.00002465
Iteration 47/1000 | Loss: 0.00002465
Iteration 48/1000 | Loss: 0.00002465
Iteration 49/1000 | Loss: 0.00002464
Iteration 50/1000 | Loss: 0.00002464
Iteration 51/1000 | Loss: 0.00002464
Iteration 52/1000 | Loss: 0.00002464
Iteration 53/1000 | Loss: 0.00002464
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002463
Iteration 56/1000 | Loss: 0.00002463
Iteration 57/1000 | Loss: 0.00002463
Iteration 58/1000 | Loss: 0.00002462
Iteration 59/1000 | Loss: 0.00002462
Iteration 60/1000 | Loss: 0.00002462
Iteration 61/1000 | Loss: 0.00002462
Iteration 62/1000 | Loss: 0.00002462
Iteration 63/1000 | Loss: 0.00002461
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002461
Iteration 66/1000 | Loss: 0.00002461
Iteration 67/1000 | Loss: 0.00002460
Iteration 68/1000 | Loss: 0.00002460
Iteration 69/1000 | Loss: 0.00002460
Iteration 70/1000 | Loss: 0.00002460
Iteration 71/1000 | Loss: 0.00002459
Iteration 72/1000 | Loss: 0.00002459
Iteration 73/1000 | Loss: 0.00002459
Iteration 74/1000 | Loss: 0.00002459
Iteration 75/1000 | Loss: 0.00002459
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002457
Iteration 80/1000 | Loss: 0.00002457
Iteration 81/1000 | Loss: 0.00002457
Iteration 82/1000 | Loss: 0.00002457
Iteration 83/1000 | Loss: 0.00002457
Iteration 84/1000 | Loss: 0.00002456
Iteration 85/1000 | Loss: 0.00002456
Iteration 86/1000 | Loss: 0.00002456
Iteration 87/1000 | Loss: 0.00002456
Iteration 88/1000 | Loss: 0.00002456
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002455
Iteration 92/1000 | Loss: 0.00002455
Iteration 93/1000 | Loss: 0.00002455
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002455
Iteration 98/1000 | Loss: 0.00002455
Iteration 99/1000 | Loss: 0.00002455
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002454
Iteration 105/1000 | Loss: 0.00002454
Iteration 106/1000 | Loss: 0.00002454
Iteration 107/1000 | Loss: 0.00002454
Iteration 108/1000 | Loss: 0.00002454
Iteration 109/1000 | Loss: 0.00002454
Iteration 110/1000 | Loss: 0.00002454
Iteration 111/1000 | Loss: 0.00002454
Iteration 112/1000 | Loss: 0.00002453
Iteration 113/1000 | Loss: 0.00002453
Iteration 114/1000 | Loss: 0.00002453
Iteration 115/1000 | Loss: 0.00002453
Iteration 116/1000 | Loss: 0.00002453
Iteration 117/1000 | Loss: 0.00002453
Iteration 118/1000 | Loss: 0.00002453
Iteration 119/1000 | Loss: 0.00002453
Iteration 120/1000 | Loss: 0.00002453
Iteration 121/1000 | Loss: 0.00002453
Iteration 122/1000 | Loss: 0.00002453
Iteration 123/1000 | Loss: 0.00002453
Iteration 124/1000 | Loss: 0.00002453
Iteration 125/1000 | Loss: 0.00002453
Iteration 126/1000 | Loss: 0.00002453
Iteration 127/1000 | Loss: 0.00002452
Iteration 128/1000 | Loss: 0.00002452
Iteration 129/1000 | Loss: 0.00002452
Iteration 130/1000 | Loss: 0.00002452
Iteration 131/1000 | Loss: 0.00002452
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002452
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002451
Iteration 140/1000 | Loss: 0.00002451
Iteration 141/1000 | Loss: 0.00002451
Iteration 142/1000 | Loss: 0.00002451
Iteration 143/1000 | Loss: 0.00002451
Iteration 144/1000 | Loss: 0.00002451
Iteration 145/1000 | Loss: 0.00002451
Iteration 146/1000 | Loss: 0.00002450
Iteration 147/1000 | Loss: 0.00002450
Iteration 148/1000 | Loss: 0.00002450
Iteration 149/1000 | Loss: 0.00002450
Iteration 150/1000 | Loss: 0.00002450
Iteration 151/1000 | Loss: 0.00002449
Iteration 152/1000 | Loss: 0.00002449
Iteration 153/1000 | Loss: 0.00002449
Iteration 154/1000 | Loss: 0.00002449
Iteration 155/1000 | Loss: 0.00002449
Iteration 156/1000 | Loss: 0.00002449
Iteration 157/1000 | Loss: 0.00002449
Iteration 158/1000 | Loss: 0.00002449
Iteration 159/1000 | Loss: 0.00002449
Iteration 160/1000 | Loss: 0.00002449
Iteration 161/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.448658960929606e-05, 2.448658960929606e-05, 2.448658960929606e-05, 2.448658960929606e-05, 2.448658960929606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.448658960929606e-05

Optimization complete. Final v2v error: 4.088366508483887 mm

Highest mean error: 13.26157283782959 mm for frame 211

Lowest mean error: 3.2629623413085938 mm for frame 229

Saving results

Total time: 108.56084704399109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359266
Iteration 2/25 | Loss: 0.00119015
Iteration 3/25 | Loss: 0.00112473
Iteration 4/25 | Loss: 0.00111608
Iteration 5/25 | Loss: 0.00111308
Iteration 6/25 | Loss: 0.00111207
Iteration 7/25 | Loss: 0.00111207
Iteration 8/25 | Loss: 0.00111207
Iteration 9/25 | Loss: 0.00111207
Iteration 10/25 | Loss: 0.00111207
Iteration 11/25 | Loss: 0.00111207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011120729614049196, 0.0011120729614049196, 0.0011120729614049196, 0.0011120729614049196, 0.0011120729614049196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011120729614049196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30282736
Iteration 2/25 | Loss: 0.00139261
Iteration 3/25 | Loss: 0.00139261
Iteration 4/25 | Loss: 0.00139261
Iteration 5/25 | Loss: 0.00139261
Iteration 6/25 | Loss: 0.00139261
Iteration 7/25 | Loss: 0.00139261
Iteration 8/25 | Loss: 0.00139261
Iteration 9/25 | Loss: 0.00139261
Iteration 10/25 | Loss: 0.00139261
Iteration 11/25 | Loss: 0.00139261
Iteration 12/25 | Loss: 0.00139261
Iteration 13/25 | Loss: 0.00139261
Iteration 14/25 | Loss: 0.00139261
Iteration 15/25 | Loss: 0.00139261
Iteration 16/25 | Loss: 0.00139261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013926104875281453, 0.0013926104875281453, 0.0013926104875281453, 0.0013926104875281453, 0.0013926104875281453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013926104875281453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139261
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00001544
Iteration 4/1000 | Loss: 0.00001342
Iteration 5/1000 | Loss: 0.00001230
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001133
Iteration 9/1000 | Loss: 0.00001104
Iteration 10/1000 | Loss: 0.00001097
Iteration 11/1000 | Loss: 0.00001094
Iteration 12/1000 | Loss: 0.00001077
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001063
Iteration 15/1000 | Loss: 0.00001063
Iteration 16/1000 | Loss: 0.00001062
Iteration 17/1000 | Loss: 0.00001062
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001061
Iteration 20/1000 | Loss: 0.00001061
Iteration 21/1000 | Loss: 0.00001061
Iteration 22/1000 | Loss: 0.00001060
Iteration 23/1000 | Loss: 0.00001059
Iteration 24/1000 | Loss: 0.00001059
Iteration 25/1000 | Loss: 0.00001059
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001059
Iteration 29/1000 | Loss: 0.00001059
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001059
Iteration 33/1000 | Loss: 0.00001059
Iteration 34/1000 | Loss: 0.00001059
Iteration 35/1000 | Loss: 0.00001058
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001058
Iteration 48/1000 | Loss: 0.00001058
Iteration 49/1000 | Loss: 0.00001058
Iteration 50/1000 | Loss: 0.00001058
Iteration 51/1000 | Loss: 0.00001058
Iteration 52/1000 | Loss: 0.00001058
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001058
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001058
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.058259658748284e-05, 1.058259658748284e-05, 1.058259658748284e-05, 1.058259658748284e-05, 1.058259658748284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.058259658748284e-05

Optimization complete. Final v2v error: 2.825244188308716 mm

Highest mean error: 2.994025468826294 mm for frame 5

Lowest mean error: 2.692065954208374 mm for frame 133

Saving results

Total time: 26.309162378311157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850623
Iteration 2/25 | Loss: 0.00133974
Iteration 3/25 | Loss: 0.00118177
Iteration 4/25 | Loss: 0.00117815
Iteration 5/25 | Loss: 0.00115038
Iteration 6/25 | Loss: 0.00114688
Iteration 7/25 | Loss: 0.00114341
Iteration 8/25 | Loss: 0.00114159
Iteration 9/25 | Loss: 0.00113997
Iteration 10/25 | Loss: 0.00113928
Iteration 11/25 | Loss: 0.00113905
Iteration 12/25 | Loss: 0.00113888
Iteration 13/25 | Loss: 0.00113879
Iteration 14/25 | Loss: 0.00114157
Iteration 15/25 | Loss: 0.00113685
Iteration 16/25 | Loss: 0.00113590
Iteration 17/25 | Loss: 0.00113561
Iteration 18/25 | Loss: 0.00113557
Iteration 19/25 | Loss: 0.00113557
Iteration 20/25 | Loss: 0.00113557
Iteration 21/25 | Loss: 0.00113556
Iteration 22/25 | Loss: 0.00113556
Iteration 23/25 | Loss: 0.00113556
Iteration 24/25 | Loss: 0.00113556
Iteration 25/25 | Loss: 0.00113556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35803914
Iteration 2/25 | Loss: 0.00148683
Iteration 3/25 | Loss: 0.00148675
Iteration 4/25 | Loss: 0.00148675
Iteration 5/25 | Loss: 0.00148675
Iteration 6/25 | Loss: 0.00148675
Iteration 7/25 | Loss: 0.00148675
Iteration 8/25 | Loss: 0.00148675
Iteration 9/25 | Loss: 0.00148674
Iteration 10/25 | Loss: 0.00148674
Iteration 11/25 | Loss: 0.00148674
Iteration 12/25 | Loss: 0.00148674
Iteration 13/25 | Loss: 0.00148674
Iteration 14/25 | Loss: 0.00148674
Iteration 15/25 | Loss: 0.00148674
Iteration 16/25 | Loss: 0.00148674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014867446152493358, 0.0014867446152493358, 0.0014867446152493358, 0.0014867446152493358, 0.0014867446152493358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014867446152493358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148674
Iteration 2/1000 | Loss: 0.00005926
Iteration 3/1000 | Loss: 0.00002490
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001842
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001580
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001443
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001438
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001432
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001428
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001421
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001420
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001414
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001412
Iteration 41/1000 | Loss: 0.00001412
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001410
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001404
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001403
Iteration 73/1000 | Loss: 0.00001403
Iteration 74/1000 | Loss: 0.00001403
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001403
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001402
Iteration 83/1000 | Loss: 0.00001402
Iteration 84/1000 | Loss: 0.00001402
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001401
Iteration 88/1000 | Loss: 0.00001401
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001401
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001399
Iteration 96/1000 | Loss: 0.00001399
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001399
Iteration 105/1000 | Loss: 0.00001399
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001398
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001398
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001395
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001394
Iteration 166/1000 | Loss: 0.00001394
Iteration 167/1000 | Loss: 0.00001394
Iteration 168/1000 | Loss: 0.00001394
Iteration 169/1000 | Loss: 0.00001394
Iteration 170/1000 | Loss: 0.00001393
Iteration 171/1000 | Loss: 0.00001393
Iteration 172/1000 | Loss: 0.00001393
Iteration 173/1000 | Loss: 0.00001393
Iteration 174/1000 | Loss: 0.00001393
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001393
Iteration 180/1000 | Loss: 0.00001393
Iteration 181/1000 | Loss: 0.00001393
Iteration 182/1000 | Loss: 0.00001393
Iteration 183/1000 | Loss: 0.00001393
Iteration 184/1000 | Loss: 0.00001393
Iteration 185/1000 | Loss: 0.00001393
Iteration 186/1000 | Loss: 0.00001393
Iteration 187/1000 | Loss: 0.00001393
Iteration 188/1000 | Loss: 0.00001393
Iteration 189/1000 | Loss: 0.00001392
Iteration 190/1000 | Loss: 0.00001392
Iteration 191/1000 | Loss: 0.00001392
Iteration 192/1000 | Loss: 0.00001392
Iteration 193/1000 | Loss: 0.00001392
Iteration 194/1000 | Loss: 0.00001392
Iteration 195/1000 | Loss: 0.00001392
Iteration 196/1000 | Loss: 0.00001392
Iteration 197/1000 | Loss: 0.00001392
Iteration 198/1000 | Loss: 0.00001392
Iteration 199/1000 | Loss: 0.00001391
Iteration 200/1000 | Loss: 0.00001391
Iteration 201/1000 | Loss: 0.00001391
Iteration 202/1000 | Loss: 0.00001391
Iteration 203/1000 | Loss: 0.00001391
Iteration 204/1000 | Loss: 0.00001391
Iteration 205/1000 | Loss: 0.00001391
Iteration 206/1000 | Loss: 0.00001391
Iteration 207/1000 | Loss: 0.00001391
Iteration 208/1000 | Loss: 0.00001391
Iteration 209/1000 | Loss: 0.00001391
Iteration 210/1000 | Loss: 0.00001391
Iteration 211/1000 | Loss: 0.00001391
Iteration 212/1000 | Loss: 0.00001391
Iteration 213/1000 | Loss: 0.00001391
Iteration 214/1000 | Loss: 0.00001390
Iteration 215/1000 | Loss: 0.00001390
Iteration 216/1000 | Loss: 0.00001390
Iteration 217/1000 | Loss: 0.00001390
Iteration 218/1000 | Loss: 0.00001390
Iteration 219/1000 | Loss: 0.00001390
Iteration 220/1000 | Loss: 0.00001390
Iteration 221/1000 | Loss: 0.00001390
Iteration 222/1000 | Loss: 0.00001390
Iteration 223/1000 | Loss: 0.00001390
Iteration 224/1000 | Loss: 0.00001390
Iteration 225/1000 | Loss: 0.00001390
Iteration 226/1000 | Loss: 0.00001390
Iteration 227/1000 | Loss: 0.00001390
Iteration 228/1000 | Loss: 0.00001390
Iteration 229/1000 | Loss: 0.00001390
Iteration 230/1000 | Loss: 0.00001390
Iteration 231/1000 | Loss: 0.00001390
Iteration 232/1000 | Loss: 0.00001390
Iteration 233/1000 | Loss: 0.00001390
Iteration 234/1000 | Loss: 0.00001390
Iteration 235/1000 | Loss: 0.00001389
Iteration 236/1000 | Loss: 0.00001389
Iteration 237/1000 | Loss: 0.00001389
Iteration 238/1000 | Loss: 0.00001389
Iteration 239/1000 | Loss: 0.00001389
Iteration 240/1000 | Loss: 0.00001389
Iteration 241/1000 | Loss: 0.00001389
Iteration 242/1000 | Loss: 0.00001389
Iteration 243/1000 | Loss: 0.00001389
Iteration 244/1000 | Loss: 0.00001389
Iteration 245/1000 | Loss: 0.00001389
Iteration 246/1000 | Loss: 0.00001389
Iteration 247/1000 | Loss: 0.00001389
Iteration 248/1000 | Loss: 0.00001389
Iteration 249/1000 | Loss: 0.00001389
Iteration 250/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.3891632079321425e-05, 1.3891632079321425e-05, 1.3891632079321425e-05, 1.3891632079321425e-05, 1.3891632079321425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3891632079321425e-05

Optimization complete. Final v2v error: 3.0592691898345947 mm

Highest mean error: 4.445188999176025 mm for frame 223

Lowest mean error: 2.646627187728882 mm for frame 154

Saving results

Total time: 73.72967290878296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826695
Iteration 2/25 | Loss: 0.00135949
Iteration 3/25 | Loss: 0.00118339
Iteration 4/25 | Loss: 0.00116780
Iteration 5/25 | Loss: 0.00116415
Iteration 6/25 | Loss: 0.00116386
Iteration 7/25 | Loss: 0.00116386
Iteration 8/25 | Loss: 0.00116386
Iteration 9/25 | Loss: 0.00116386
Iteration 10/25 | Loss: 0.00116386
Iteration 11/25 | Loss: 0.00116386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001163858687505126, 0.001163858687505126, 0.001163858687505126, 0.001163858687505126, 0.001163858687505126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163858687505126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23379993
Iteration 2/25 | Loss: 0.00100879
Iteration 3/25 | Loss: 0.00100874
Iteration 4/25 | Loss: 0.00100874
Iteration 5/25 | Loss: 0.00100874
Iteration 6/25 | Loss: 0.00100874
Iteration 7/25 | Loss: 0.00100874
Iteration 8/25 | Loss: 0.00100874
Iteration 9/25 | Loss: 0.00100874
Iteration 10/25 | Loss: 0.00100874
Iteration 11/25 | Loss: 0.00100874
Iteration 12/25 | Loss: 0.00100874
Iteration 13/25 | Loss: 0.00100874
Iteration 14/25 | Loss: 0.00100874
Iteration 15/25 | Loss: 0.00100874
Iteration 16/25 | Loss: 0.00100874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010087356204167008, 0.0010087356204167008, 0.0010087356204167008, 0.0010087356204167008, 0.0010087356204167008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010087356204167008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100874
Iteration 2/1000 | Loss: 0.00004245
Iteration 3/1000 | Loss: 0.00002151
Iteration 4/1000 | Loss: 0.00001818
Iteration 5/1000 | Loss: 0.00001692
Iteration 6/1000 | Loss: 0.00001606
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001450
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001417
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001411
Iteration 16/1000 | Loss: 0.00001406
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001403
Iteration 19/1000 | Loss: 0.00001403
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001400
Iteration 30/1000 | Loss: 0.00001400
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001398
Iteration 39/1000 | Loss: 0.00001398
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.3980769836052787e-05, 1.3980769836052787e-05, 1.3980769836052787e-05, 1.3980769836052787e-05, 1.3980769836052787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3980769836052787e-05

Optimization complete. Final v2v error: 3.264443874359131 mm

Highest mean error: 3.563394546508789 mm for frame 30

Lowest mean error: 3.094507932662964 mm for frame 162

Saving results

Total time: 27.684250593185425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736308
Iteration 2/25 | Loss: 0.00152376
Iteration 3/25 | Loss: 0.00128224
Iteration 4/25 | Loss: 0.00125423
Iteration 5/25 | Loss: 0.00122294
Iteration 6/25 | Loss: 0.00121832
Iteration 7/25 | Loss: 0.00121712
Iteration 8/25 | Loss: 0.00121672
Iteration 9/25 | Loss: 0.00121656
Iteration 10/25 | Loss: 0.00121655
Iteration 11/25 | Loss: 0.00121655
Iteration 12/25 | Loss: 0.00121655
Iteration 13/25 | Loss: 0.00121655
Iteration 14/25 | Loss: 0.00121655
Iteration 15/25 | Loss: 0.00121655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012165469815954566, 0.0012165469815954566, 0.0012165469815954566, 0.0012165469815954566, 0.0012165469815954566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012165469815954566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39240909
Iteration 2/25 | Loss: 0.00176888
Iteration 3/25 | Loss: 0.00176887
Iteration 4/25 | Loss: 0.00176887
Iteration 5/25 | Loss: 0.00176887
Iteration 6/25 | Loss: 0.00176887
Iteration 7/25 | Loss: 0.00176887
Iteration 8/25 | Loss: 0.00176887
Iteration 9/25 | Loss: 0.00176887
Iteration 10/25 | Loss: 0.00176887
Iteration 11/25 | Loss: 0.00176887
Iteration 12/25 | Loss: 0.00176887
Iteration 13/25 | Loss: 0.00176887
Iteration 14/25 | Loss: 0.00176887
Iteration 15/25 | Loss: 0.00176887
Iteration 16/25 | Loss: 0.00176887
Iteration 17/25 | Loss: 0.00176887
Iteration 18/25 | Loss: 0.00176887
Iteration 19/25 | Loss: 0.00176887
Iteration 20/25 | Loss: 0.00176887
Iteration 21/25 | Loss: 0.00176887
Iteration 22/25 | Loss: 0.00176887
Iteration 23/25 | Loss: 0.00176887
Iteration 24/25 | Loss: 0.00176887
Iteration 25/25 | Loss: 0.00176887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176887
Iteration 2/1000 | Loss: 0.00009496
Iteration 3/1000 | Loss: 0.00004223
Iteration 4/1000 | Loss: 0.00002742
Iteration 5/1000 | Loss: 0.00002401
Iteration 6/1000 | Loss: 0.00002204
Iteration 7/1000 | Loss: 0.00002118
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002014
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001934
Iteration 12/1000 | Loss: 0.00001910
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001877
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001861
Iteration 29/1000 | Loss: 0.00001860
Iteration 30/1000 | Loss: 0.00001860
Iteration 31/1000 | Loss: 0.00001859
Iteration 32/1000 | Loss: 0.00001859
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00001857
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001856
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001854
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00001854
Iteration 46/1000 | Loss: 0.00001854
Iteration 47/1000 | Loss: 0.00001854
Iteration 48/1000 | Loss: 0.00001853
Iteration 49/1000 | Loss: 0.00001853
Iteration 50/1000 | Loss: 0.00001853
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001853
Iteration 57/1000 | Loss: 0.00001853
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001852
Iteration 63/1000 | Loss: 0.00001852
Iteration 64/1000 | Loss: 0.00001852
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001851
Iteration 69/1000 | Loss: 0.00001851
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001848
Iteration 81/1000 | Loss: 0.00001848
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001846
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001844
Iteration 92/1000 | Loss: 0.00001844
Iteration 93/1000 | Loss: 0.00001844
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001843
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Iteration 109/1000 | Loss: 0.00001842
Iteration 110/1000 | Loss: 0.00001842
Iteration 111/1000 | Loss: 0.00001842
Iteration 112/1000 | Loss: 0.00001842
Iteration 113/1000 | Loss: 0.00001842
Iteration 114/1000 | Loss: 0.00001842
Iteration 115/1000 | Loss: 0.00001842
Iteration 116/1000 | Loss: 0.00001842
Iteration 117/1000 | Loss: 0.00001842
Iteration 118/1000 | Loss: 0.00001842
Iteration 119/1000 | Loss: 0.00001842
Iteration 120/1000 | Loss: 0.00001842
Iteration 121/1000 | Loss: 0.00001842
Iteration 122/1000 | Loss: 0.00001841
Iteration 123/1000 | Loss: 0.00001841
Iteration 124/1000 | Loss: 0.00001841
Iteration 125/1000 | Loss: 0.00001841
Iteration 126/1000 | Loss: 0.00001841
Iteration 127/1000 | Loss: 0.00001841
Iteration 128/1000 | Loss: 0.00001841
Iteration 129/1000 | Loss: 0.00001841
Iteration 130/1000 | Loss: 0.00001841
Iteration 131/1000 | Loss: 0.00001841
Iteration 132/1000 | Loss: 0.00001841
Iteration 133/1000 | Loss: 0.00001841
Iteration 134/1000 | Loss: 0.00001841
Iteration 135/1000 | Loss: 0.00001841
Iteration 136/1000 | Loss: 0.00001841
Iteration 137/1000 | Loss: 0.00001840
Iteration 138/1000 | Loss: 0.00001840
Iteration 139/1000 | Loss: 0.00001840
Iteration 140/1000 | Loss: 0.00001840
Iteration 141/1000 | Loss: 0.00001840
Iteration 142/1000 | Loss: 0.00001840
Iteration 143/1000 | Loss: 0.00001840
Iteration 144/1000 | Loss: 0.00001840
Iteration 145/1000 | Loss: 0.00001840
Iteration 146/1000 | Loss: 0.00001840
Iteration 147/1000 | Loss: 0.00001840
Iteration 148/1000 | Loss: 0.00001840
Iteration 149/1000 | Loss: 0.00001840
Iteration 150/1000 | Loss: 0.00001840
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001840
Iteration 156/1000 | Loss: 0.00001840
Iteration 157/1000 | Loss: 0.00001840
Iteration 158/1000 | Loss: 0.00001839
Iteration 159/1000 | Loss: 0.00001839
Iteration 160/1000 | Loss: 0.00001839
Iteration 161/1000 | Loss: 0.00001839
Iteration 162/1000 | Loss: 0.00001839
Iteration 163/1000 | Loss: 0.00001839
Iteration 164/1000 | Loss: 0.00001839
Iteration 165/1000 | Loss: 0.00001839
Iteration 166/1000 | Loss: 0.00001839
Iteration 167/1000 | Loss: 0.00001839
Iteration 168/1000 | Loss: 0.00001839
Iteration 169/1000 | Loss: 0.00001839
Iteration 170/1000 | Loss: 0.00001839
Iteration 171/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.8392613128526136e-05, 1.8392613128526136e-05, 1.8392613128526136e-05, 1.8392613128526136e-05, 1.8392613128526136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8392613128526136e-05

Optimization complete. Final v2v error: 3.7246878147125244 mm

Highest mean error: 4.301129341125488 mm for frame 83

Lowest mean error: 3.2299013137817383 mm for frame 4

Saving results

Total time: 51.47891283035278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465057
Iteration 2/25 | Loss: 0.00185015
Iteration 3/25 | Loss: 0.00154713
Iteration 4/25 | Loss: 0.00163570
Iteration 5/25 | Loss: 0.00154174
Iteration 6/25 | Loss: 0.00142416
Iteration 7/25 | Loss: 0.00139934
Iteration 8/25 | Loss: 0.00139443
Iteration 9/25 | Loss: 0.00138950
Iteration 10/25 | Loss: 0.00138645
Iteration 11/25 | Loss: 0.00139127
Iteration 12/25 | Loss: 0.00139121
Iteration 13/25 | Loss: 0.00138575
Iteration 14/25 | Loss: 0.00138306
Iteration 15/25 | Loss: 0.00138246
Iteration 16/25 | Loss: 0.00138233
Iteration 17/25 | Loss: 0.00138230
Iteration 18/25 | Loss: 0.00138230
Iteration 19/25 | Loss: 0.00138230
Iteration 20/25 | Loss: 0.00138230
Iteration 21/25 | Loss: 0.00138230
Iteration 22/25 | Loss: 0.00138230
Iteration 23/25 | Loss: 0.00138230
Iteration 24/25 | Loss: 0.00138230
Iteration 25/25 | Loss: 0.00138230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25830638
Iteration 2/25 | Loss: 0.00185637
Iteration 3/25 | Loss: 0.00185637
Iteration 4/25 | Loss: 0.00185637
Iteration 5/25 | Loss: 0.00185637
Iteration 6/25 | Loss: 0.00185637
Iteration 7/25 | Loss: 0.00185637
Iteration 8/25 | Loss: 0.00185637
Iteration 9/25 | Loss: 0.00185637
Iteration 10/25 | Loss: 0.00185637
Iteration 11/25 | Loss: 0.00185637
Iteration 12/25 | Loss: 0.00185637
Iteration 13/25 | Loss: 0.00185637
Iteration 14/25 | Loss: 0.00185637
Iteration 15/25 | Loss: 0.00185637
Iteration 16/25 | Loss: 0.00185637
Iteration 17/25 | Loss: 0.00185637
Iteration 18/25 | Loss: 0.00185637
Iteration 19/25 | Loss: 0.00185637
Iteration 20/25 | Loss: 0.00185637
Iteration 21/25 | Loss: 0.00185637
Iteration 22/25 | Loss: 0.00185637
Iteration 23/25 | Loss: 0.00185637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018563664052635431, 0.0018563664052635431, 0.0018563664052635431, 0.0018563664052635431, 0.0018563664052635431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018563664052635431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185637
Iteration 2/1000 | Loss: 0.00057272
Iteration 3/1000 | Loss: 0.00053390
Iteration 4/1000 | Loss: 0.00040588
Iteration 5/1000 | Loss: 0.00015085
Iteration 6/1000 | Loss: 0.00006408
Iteration 7/1000 | Loss: 0.00012238
Iteration 8/1000 | Loss: 0.00032332
Iteration 9/1000 | Loss: 0.00022846
Iteration 10/1000 | Loss: 0.00030732
Iteration 11/1000 | Loss: 0.00013221
Iteration 12/1000 | Loss: 0.00006193
Iteration 13/1000 | Loss: 0.00005934
Iteration 14/1000 | Loss: 0.00005747
Iteration 15/1000 | Loss: 0.00005618
Iteration 16/1000 | Loss: 0.00022928
Iteration 17/1000 | Loss: 0.00033296
Iteration 18/1000 | Loss: 0.00018035
Iteration 19/1000 | Loss: 0.00034376
Iteration 20/1000 | Loss: 0.00019178
Iteration 21/1000 | Loss: 0.00040631
Iteration 22/1000 | Loss: 0.00020818
Iteration 23/1000 | Loss: 0.00041216
Iteration 24/1000 | Loss: 0.00025305
Iteration 25/1000 | Loss: 0.00052014
Iteration 26/1000 | Loss: 0.00029572
Iteration 27/1000 | Loss: 0.00058204
Iteration 28/1000 | Loss: 0.00041970
Iteration 29/1000 | Loss: 0.00054485
Iteration 30/1000 | Loss: 0.00029681
Iteration 31/1000 | Loss: 0.00006256
Iteration 32/1000 | Loss: 0.00011844
Iteration 33/1000 | Loss: 0.00032254
Iteration 34/1000 | Loss: 0.00022802
Iteration 35/1000 | Loss: 0.00036489
Iteration 36/1000 | Loss: 0.00023283
Iteration 37/1000 | Loss: 0.00039022
Iteration 38/1000 | Loss: 0.00023319
Iteration 39/1000 | Loss: 0.00034047
Iteration 40/1000 | Loss: 0.00025291
Iteration 41/1000 | Loss: 0.00007692
Iteration 42/1000 | Loss: 0.00006348
Iteration 43/1000 | Loss: 0.00005927
Iteration 44/1000 | Loss: 0.00005702
Iteration 45/1000 | Loss: 0.00018065
Iteration 46/1000 | Loss: 0.00012710
Iteration 47/1000 | Loss: 0.00006715
Iteration 48/1000 | Loss: 0.00005563
Iteration 49/1000 | Loss: 0.00005316
Iteration 50/1000 | Loss: 0.00005191
Iteration 51/1000 | Loss: 0.00005100
Iteration 52/1000 | Loss: 0.00005037
Iteration 53/1000 | Loss: 0.00004997
Iteration 54/1000 | Loss: 0.00006269
Iteration 55/1000 | Loss: 0.00006278
Iteration 56/1000 | Loss: 0.00005620
Iteration 57/1000 | Loss: 0.00005005
Iteration 58/1000 | Loss: 0.00005788
Iteration 59/1000 | Loss: 0.00005457
Iteration 60/1000 | Loss: 0.00005714
Iteration 61/1000 | Loss: 0.00005476
Iteration 62/1000 | Loss: 0.00005650
Iteration 63/1000 | Loss: 0.00005190
Iteration 64/1000 | Loss: 0.00005044
Iteration 65/1000 | Loss: 0.00004978
Iteration 66/1000 | Loss: 0.00004945
Iteration 67/1000 | Loss: 0.00004917
Iteration 68/1000 | Loss: 0.00004913
Iteration 69/1000 | Loss: 0.00004896
Iteration 70/1000 | Loss: 0.00004875
Iteration 71/1000 | Loss: 0.00017283
Iteration 72/1000 | Loss: 0.00005801
Iteration 73/1000 | Loss: 0.00005554
Iteration 74/1000 | Loss: 0.00005415
Iteration 75/1000 | Loss: 0.00005345
Iteration 76/1000 | Loss: 0.00005287
Iteration 77/1000 | Loss: 0.00011230
Iteration 78/1000 | Loss: 0.00014476
Iteration 79/1000 | Loss: 0.00005658
Iteration 80/1000 | Loss: 0.00005497
Iteration 81/1000 | Loss: 0.00006554
Iteration 82/1000 | Loss: 0.00006328
Iteration 83/1000 | Loss: 0.00005389
Iteration 84/1000 | Loss: 0.00005275
Iteration 85/1000 | Loss: 0.00005200
Iteration 86/1000 | Loss: 0.00005114
Iteration 87/1000 | Loss: 0.00009446
Iteration 88/1000 | Loss: 0.00005182
Iteration 89/1000 | Loss: 0.00005080
Iteration 90/1000 | Loss: 0.00005008
Iteration 91/1000 | Loss: 0.00004954
Iteration 92/1000 | Loss: 0.00004925
Iteration 93/1000 | Loss: 0.00004903
Iteration 94/1000 | Loss: 0.00004903
Iteration 95/1000 | Loss: 0.00004901
Iteration 96/1000 | Loss: 0.00004901
Iteration 97/1000 | Loss: 0.00004886
Iteration 98/1000 | Loss: 0.00004883
Iteration 99/1000 | Loss: 0.00004880
Iteration 100/1000 | Loss: 0.00004878
Iteration 101/1000 | Loss: 0.00004877
Iteration 102/1000 | Loss: 0.00004875
Iteration 103/1000 | Loss: 0.00004875
Iteration 104/1000 | Loss: 0.00004875
Iteration 105/1000 | Loss: 0.00004872
Iteration 106/1000 | Loss: 0.00004872
Iteration 107/1000 | Loss: 0.00004872
Iteration 108/1000 | Loss: 0.00004871
Iteration 109/1000 | Loss: 0.00004871
Iteration 110/1000 | Loss: 0.00004870
Iteration 111/1000 | Loss: 0.00004870
Iteration 112/1000 | Loss: 0.00004870
Iteration 113/1000 | Loss: 0.00004870
Iteration 114/1000 | Loss: 0.00004869
Iteration 115/1000 | Loss: 0.00004869
Iteration 116/1000 | Loss: 0.00004869
Iteration 117/1000 | Loss: 0.00004869
Iteration 118/1000 | Loss: 0.00004868
Iteration 119/1000 | Loss: 0.00004868
Iteration 120/1000 | Loss: 0.00004868
Iteration 121/1000 | Loss: 0.00004868
Iteration 122/1000 | Loss: 0.00004868
Iteration 123/1000 | Loss: 0.00004868
Iteration 124/1000 | Loss: 0.00004868
Iteration 125/1000 | Loss: 0.00004867
Iteration 126/1000 | Loss: 0.00004867
Iteration 127/1000 | Loss: 0.00004867
Iteration 128/1000 | Loss: 0.00004867
Iteration 129/1000 | Loss: 0.00004867
Iteration 130/1000 | Loss: 0.00004867
Iteration 131/1000 | Loss: 0.00004867
Iteration 132/1000 | Loss: 0.00004867
Iteration 133/1000 | Loss: 0.00004867
Iteration 134/1000 | Loss: 0.00004866
Iteration 135/1000 | Loss: 0.00004866
Iteration 136/1000 | Loss: 0.00004866
Iteration 137/1000 | Loss: 0.00004866
Iteration 138/1000 | Loss: 0.00004865
Iteration 139/1000 | Loss: 0.00004865
Iteration 140/1000 | Loss: 0.00004865
Iteration 141/1000 | Loss: 0.00004865
Iteration 142/1000 | Loss: 0.00004865
Iteration 143/1000 | Loss: 0.00004865
Iteration 144/1000 | Loss: 0.00004865
Iteration 145/1000 | Loss: 0.00004865
Iteration 146/1000 | Loss: 0.00004864
Iteration 147/1000 | Loss: 0.00004864
Iteration 148/1000 | Loss: 0.00004864
Iteration 149/1000 | Loss: 0.00004864
Iteration 150/1000 | Loss: 0.00004864
Iteration 151/1000 | Loss: 0.00004864
Iteration 152/1000 | Loss: 0.00004864
Iteration 153/1000 | Loss: 0.00004864
Iteration 154/1000 | Loss: 0.00004864
Iteration 155/1000 | Loss: 0.00004864
Iteration 156/1000 | Loss: 0.00004864
Iteration 157/1000 | Loss: 0.00004863
Iteration 158/1000 | Loss: 0.00004863
Iteration 159/1000 | Loss: 0.00004863
Iteration 160/1000 | Loss: 0.00004863
Iteration 161/1000 | Loss: 0.00004863
Iteration 162/1000 | Loss: 0.00004863
Iteration 163/1000 | Loss: 0.00004863
Iteration 164/1000 | Loss: 0.00004863
Iteration 165/1000 | Loss: 0.00004863
Iteration 166/1000 | Loss: 0.00004863
Iteration 167/1000 | Loss: 0.00004863
Iteration 168/1000 | Loss: 0.00004863
Iteration 169/1000 | Loss: 0.00004863
Iteration 170/1000 | Loss: 0.00004863
Iteration 171/1000 | Loss: 0.00004863
Iteration 172/1000 | Loss: 0.00004863
Iteration 173/1000 | Loss: 0.00004863
Iteration 174/1000 | Loss: 0.00004863
Iteration 175/1000 | Loss: 0.00004863
Iteration 176/1000 | Loss: 0.00004863
Iteration 177/1000 | Loss: 0.00004863
Iteration 178/1000 | Loss: 0.00004863
Iteration 179/1000 | Loss: 0.00004863
Iteration 180/1000 | Loss: 0.00004863
Iteration 181/1000 | Loss: 0.00004863
Iteration 182/1000 | Loss: 0.00004863
Iteration 183/1000 | Loss: 0.00004863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [4.863102367380634e-05, 4.863102367380634e-05, 4.863102367380634e-05, 4.863102367380634e-05, 4.863102367380634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.863102367380634e-05

Optimization complete. Final v2v error: 4.967892646789551 mm

Highest mean error: 7.77882194519043 mm for frame 83

Lowest mean error: 4.14080286026001 mm for frame 6

Saving results

Total time: 189.75427556037903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00670998
Iteration 2/25 | Loss: 0.00146979
Iteration 3/25 | Loss: 0.00130747
Iteration 4/25 | Loss: 0.00129138
Iteration 5/25 | Loss: 0.00127277
Iteration 6/25 | Loss: 0.00125711
Iteration 7/25 | Loss: 0.00127096
Iteration 8/25 | Loss: 0.00125110
Iteration 9/25 | Loss: 0.00125965
Iteration 10/25 | Loss: 0.00125045
Iteration 11/25 | Loss: 0.00125038
Iteration 12/25 | Loss: 0.00125037
Iteration 13/25 | Loss: 0.00125037
Iteration 14/25 | Loss: 0.00125037
Iteration 15/25 | Loss: 0.00125037
Iteration 16/25 | Loss: 0.00125037
Iteration 17/25 | Loss: 0.00125037
Iteration 18/25 | Loss: 0.00125037
Iteration 19/25 | Loss: 0.00125037
Iteration 20/25 | Loss: 0.00125037
Iteration 21/25 | Loss: 0.00125037
Iteration 22/25 | Loss: 0.00125037
Iteration 23/25 | Loss: 0.00125037
Iteration 24/25 | Loss: 0.00125037
Iteration 25/25 | Loss: 0.00125037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61250055
Iteration 2/25 | Loss: 0.00209713
Iteration 3/25 | Loss: 0.00209713
Iteration 4/25 | Loss: 0.00209713
Iteration 5/25 | Loss: 0.00209713
Iteration 6/25 | Loss: 0.00209713
Iteration 7/25 | Loss: 0.00209713
Iteration 8/25 | Loss: 0.00209713
Iteration 9/25 | Loss: 0.00209713
Iteration 10/25 | Loss: 0.00209713
Iteration 11/25 | Loss: 0.00209713
Iteration 12/25 | Loss: 0.00209712
Iteration 13/25 | Loss: 0.00209713
Iteration 14/25 | Loss: 0.00209713
Iteration 15/25 | Loss: 0.00209712
Iteration 16/25 | Loss: 0.00209713
Iteration 17/25 | Loss: 0.00209712
Iteration 18/25 | Loss: 0.00209713
Iteration 19/25 | Loss: 0.00209712
Iteration 20/25 | Loss: 0.00209713
Iteration 21/25 | Loss: 0.00209713
Iteration 22/25 | Loss: 0.00209713
Iteration 23/25 | Loss: 0.00209712
Iteration 24/25 | Loss: 0.00209713
Iteration 25/25 | Loss: 0.00209713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209713
Iteration 2/1000 | Loss: 0.00010687
Iteration 3/1000 | Loss: 0.00006244
Iteration 4/1000 | Loss: 0.00004201
Iteration 5/1000 | Loss: 0.00003394
Iteration 6/1000 | Loss: 0.00003052
Iteration 7/1000 | Loss: 0.00002772
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00023398
Iteration 11/1000 | Loss: 0.00002468
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00002389
Iteration 14/1000 | Loss: 0.00002356
Iteration 15/1000 | Loss: 0.00002328
Iteration 16/1000 | Loss: 0.00002308
Iteration 17/1000 | Loss: 0.00002296
Iteration 18/1000 | Loss: 0.00002288
Iteration 19/1000 | Loss: 0.00002287
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00002286
Iteration 23/1000 | Loss: 0.00002286
Iteration 24/1000 | Loss: 0.00002282
Iteration 25/1000 | Loss: 0.00002279
Iteration 26/1000 | Loss: 0.00002276
Iteration 27/1000 | Loss: 0.00002276
Iteration 28/1000 | Loss: 0.00002275
Iteration 29/1000 | Loss: 0.00002273
Iteration 30/1000 | Loss: 0.00002273
Iteration 31/1000 | Loss: 0.00002272
Iteration 32/1000 | Loss: 0.00002270
Iteration 33/1000 | Loss: 0.00002269
Iteration 34/1000 | Loss: 0.00002268
Iteration 35/1000 | Loss: 0.00002267
Iteration 36/1000 | Loss: 0.00002266
Iteration 37/1000 | Loss: 0.00002266
Iteration 38/1000 | Loss: 0.00002266
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002263
Iteration 45/1000 | Loss: 0.00002263
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002262
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002259
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002258
Iteration 58/1000 | Loss: 0.00002258
Iteration 59/1000 | Loss: 0.00002258
Iteration 60/1000 | Loss: 0.00002257
Iteration 61/1000 | Loss: 0.00002257
Iteration 62/1000 | Loss: 0.00002257
Iteration 63/1000 | Loss: 0.00002256
Iteration 64/1000 | Loss: 0.00002256
Iteration 65/1000 | Loss: 0.00002256
Iteration 66/1000 | Loss: 0.00002255
Iteration 67/1000 | Loss: 0.00002255
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00002254
Iteration 70/1000 | Loss: 0.00002254
Iteration 71/1000 | Loss: 0.00002254
Iteration 72/1000 | Loss: 0.00002254
Iteration 73/1000 | Loss: 0.00002253
Iteration 74/1000 | Loss: 0.00002253
Iteration 75/1000 | Loss: 0.00002253
Iteration 76/1000 | Loss: 0.00002253
Iteration 77/1000 | Loss: 0.00002252
Iteration 78/1000 | Loss: 0.00002252
Iteration 79/1000 | Loss: 0.00002252
Iteration 80/1000 | Loss: 0.00002252
Iteration 81/1000 | Loss: 0.00002252
Iteration 82/1000 | Loss: 0.00002252
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002251
Iteration 85/1000 | Loss: 0.00002251
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00002251
Iteration 88/1000 | Loss: 0.00002251
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002250
Iteration 91/1000 | Loss: 0.00002250
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002248
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002248
Iteration 100/1000 | Loss: 0.00002248
Iteration 101/1000 | Loss: 0.00002248
Iteration 102/1000 | Loss: 0.00002248
Iteration 103/1000 | Loss: 0.00002248
Iteration 104/1000 | Loss: 0.00002247
Iteration 105/1000 | Loss: 0.00002247
Iteration 106/1000 | Loss: 0.00002247
Iteration 107/1000 | Loss: 0.00002247
Iteration 108/1000 | Loss: 0.00002247
Iteration 109/1000 | Loss: 0.00002247
Iteration 110/1000 | Loss: 0.00002247
Iteration 111/1000 | Loss: 0.00002247
Iteration 112/1000 | Loss: 0.00002246
Iteration 113/1000 | Loss: 0.00002246
Iteration 114/1000 | Loss: 0.00002246
Iteration 115/1000 | Loss: 0.00002246
Iteration 116/1000 | Loss: 0.00002246
Iteration 117/1000 | Loss: 0.00002246
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002245
Iteration 120/1000 | Loss: 0.00002245
Iteration 121/1000 | Loss: 0.00002245
Iteration 122/1000 | Loss: 0.00002245
Iteration 123/1000 | Loss: 0.00002245
Iteration 124/1000 | Loss: 0.00002245
Iteration 125/1000 | Loss: 0.00002245
Iteration 126/1000 | Loss: 0.00002245
Iteration 127/1000 | Loss: 0.00002245
Iteration 128/1000 | Loss: 0.00002245
Iteration 129/1000 | Loss: 0.00002245
Iteration 130/1000 | Loss: 0.00002245
Iteration 131/1000 | Loss: 0.00002244
Iteration 132/1000 | Loss: 0.00002244
Iteration 133/1000 | Loss: 0.00002244
Iteration 134/1000 | Loss: 0.00002244
Iteration 135/1000 | Loss: 0.00002244
Iteration 136/1000 | Loss: 0.00002244
Iteration 137/1000 | Loss: 0.00002244
Iteration 138/1000 | Loss: 0.00002244
Iteration 139/1000 | Loss: 0.00002244
Iteration 140/1000 | Loss: 0.00002244
Iteration 141/1000 | Loss: 0.00002244
Iteration 142/1000 | Loss: 0.00002244
Iteration 143/1000 | Loss: 0.00002244
Iteration 144/1000 | Loss: 0.00002244
Iteration 145/1000 | Loss: 0.00002243
Iteration 146/1000 | Loss: 0.00002243
Iteration 147/1000 | Loss: 0.00002243
Iteration 148/1000 | Loss: 0.00002243
Iteration 149/1000 | Loss: 0.00002243
Iteration 150/1000 | Loss: 0.00002243
Iteration 151/1000 | Loss: 0.00002243
Iteration 152/1000 | Loss: 0.00002243
Iteration 153/1000 | Loss: 0.00002243
Iteration 154/1000 | Loss: 0.00002243
Iteration 155/1000 | Loss: 0.00002243
Iteration 156/1000 | Loss: 0.00002243
Iteration 157/1000 | Loss: 0.00002243
Iteration 158/1000 | Loss: 0.00002243
Iteration 159/1000 | Loss: 0.00002243
Iteration 160/1000 | Loss: 0.00002243
Iteration 161/1000 | Loss: 0.00002243
Iteration 162/1000 | Loss: 0.00002243
Iteration 163/1000 | Loss: 0.00002243
Iteration 164/1000 | Loss: 0.00002243
Iteration 165/1000 | Loss: 0.00002243
Iteration 166/1000 | Loss: 0.00002243
Iteration 167/1000 | Loss: 0.00002243
Iteration 168/1000 | Loss: 0.00002243
Iteration 169/1000 | Loss: 0.00002243
Iteration 170/1000 | Loss: 0.00002243
Iteration 171/1000 | Loss: 0.00002243
Iteration 172/1000 | Loss: 0.00002243
Iteration 173/1000 | Loss: 0.00002243
Iteration 174/1000 | Loss: 0.00002243
Iteration 175/1000 | Loss: 0.00002243
Iteration 176/1000 | Loss: 0.00002243
Iteration 177/1000 | Loss: 0.00002243
Iteration 178/1000 | Loss: 0.00002243
Iteration 179/1000 | Loss: 0.00002243
Iteration 180/1000 | Loss: 0.00002243
Iteration 181/1000 | Loss: 0.00002243
Iteration 182/1000 | Loss: 0.00002243
Iteration 183/1000 | Loss: 0.00002243
Iteration 184/1000 | Loss: 0.00002243
Iteration 185/1000 | Loss: 0.00002243
Iteration 186/1000 | Loss: 0.00002243
Iteration 187/1000 | Loss: 0.00002243
Iteration 188/1000 | Loss: 0.00002243
Iteration 189/1000 | Loss: 0.00002243
Iteration 190/1000 | Loss: 0.00002243
Iteration 191/1000 | Loss: 0.00002243
Iteration 192/1000 | Loss: 0.00002243
Iteration 193/1000 | Loss: 0.00002243
Iteration 194/1000 | Loss: 0.00002243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.242745176772587e-05, 2.242745176772587e-05, 2.242745176772587e-05, 2.242745176772587e-05, 2.242745176772587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.242745176772587e-05

Optimization complete. Final v2v error: 3.9212634563446045 mm

Highest mean error: 5.271386623382568 mm for frame 51

Lowest mean error: 3.099766492843628 mm for frame 80

Saving results

Total time: 56.86247682571411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521453
Iteration 2/25 | Loss: 0.00150584
Iteration 3/25 | Loss: 0.00122838
Iteration 4/25 | Loss: 0.00119864
Iteration 5/25 | Loss: 0.00119076
Iteration 6/25 | Loss: 0.00118967
Iteration 7/25 | Loss: 0.00118967
Iteration 8/25 | Loss: 0.00118967
Iteration 9/25 | Loss: 0.00118967
Iteration 10/25 | Loss: 0.00118967
Iteration 11/25 | Loss: 0.00118967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011896658688783646, 0.0011896658688783646, 0.0011896658688783646, 0.0011896658688783646, 0.0011896658688783646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011896658688783646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69715959
Iteration 2/25 | Loss: 0.00135251
Iteration 3/25 | Loss: 0.00135251
Iteration 4/25 | Loss: 0.00135251
Iteration 5/25 | Loss: 0.00135251
Iteration 6/25 | Loss: 0.00135251
Iteration 7/25 | Loss: 0.00135251
Iteration 8/25 | Loss: 0.00135251
Iteration 9/25 | Loss: 0.00135251
Iteration 10/25 | Loss: 0.00135251
Iteration 11/25 | Loss: 0.00135251
Iteration 12/25 | Loss: 0.00135251
Iteration 13/25 | Loss: 0.00135251
Iteration 14/25 | Loss: 0.00135251
Iteration 15/25 | Loss: 0.00135251
Iteration 16/25 | Loss: 0.00135251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013525082031264901, 0.0013525082031264901, 0.0013525082031264901, 0.0013525082031264901, 0.0013525082031264901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013525082031264901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135251
Iteration 2/1000 | Loss: 0.00007332
Iteration 3/1000 | Loss: 0.00003891
Iteration 4/1000 | Loss: 0.00003185
Iteration 5/1000 | Loss: 0.00002938
Iteration 6/1000 | Loss: 0.00002794
Iteration 7/1000 | Loss: 0.00002728
Iteration 8/1000 | Loss: 0.00002668
Iteration 9/1000 | Loss: 0.00002635
Iteration 10/1000 | Loss: 0.00002597
Iteration 11/1000 | Loss: 0.00002572
Iteration 12/1000 | Loss: 0.00002552
Iteration 13/1000 | Loss: 0.00002539
Iteration 14/1000 | Loss: 0.00002539
Iteration 15/1000 | Loss: 0.00002537
Iteration 16/1000 | Loss: 0.00002537
Iteration 17/1000 | Loss: 0.00002536
Iteration 18/1000 | Loss: 0.00002531
Iteration 19/1000 | Loss: 0.00002530
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00002502
Iteration 22/1000 | Loss: 0.00002488
Iteration 23/1000 | Loss: 0.00002475
Iteration 24/1000 | Loss: 0.00002467
Iteration 25/1000 | Loss: 0.00002456
Iteration 26/1000 | Loss: 0.00002456
Iteration 27/1000 | Loss: 0.00002442
Iteration 28/1000 | Loss: 0.00002426
Iteration 29/1000 | Loss: 0.00002420
Iteration 30/1000 | Loss: 0.00002419
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00002409
Iteration 33/1000 | Loss: 0.00002408
Iteration 34/1000 | Loss: 0.00002402
Iteration 35/1000 | Loss: 0.00002402
Iteration 36/1000 | Loss: 0.00002402
Iteration 37/1000 | Loss: 0.00002402
Iteration 38/1000 | Loss: 0.00002402
Iteration 39/1000 | Loss: 0.00002402
Iteration 40/1000 | Loss: 0.00002402
Iteration 41/1000 | Loss: 0.00002401
Iteration 42/1000 | Loss: 0.00002400
Iteration 43/1000 | Loss: 0.00002400
Iteration 44/1000 | Loss: 0.00002399
Iteration 45/1000 | Loss: 0.00002399
Iteration 46/1000 | Loss: 0.00002399
Iteration 47/1000 | Loss: 0.00002399
Iteration 48/1000 | Loss: 0.00002398
Iteration 49/1000 | Loss: 0.00002398
Iteration 50/1000 | Loss: 0.00002398
Iteration 51/1000 | Loss: 0.00002398
Iteration 52/1000 | Loss: 0.00002398
Iteration 53/1000 | Loss: 0.00002398
Iteration 54/1000 | Loss: 0.00002398
Iteration 55/1000 | Loss: 0.00002398
Iteration 56/1000 | Loss: 0.00002398
Iteration 57/1000 | Loss: 0.00002398
Iteration 58/1000 | Loss: 0.00002397
Iteration 59/1000 | Loss: 0.00002397
Iteration 60/1000 | Loss: 0.00002397
Iteration 61/1000 | Loss: 0.00002397
Iteration 62/1000 | Loss: 0.00002397
Iteration 63/1000 | Loss: 0.00002397
Iteration 64/1000 | Loss: 0.00002397
Iteration 65/1000 | Loss: 0.00002397
Iteration 66/1000 | Loss: 0.00002396
Iteration 67/1000 | Loss: 0.00002396
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002396
Iteration 72/1000 | Loss: 0.00002395
Iteration 73/1000 | Loss: 0.00002395
Iteration 74/1000 | Loss: 0.00002395
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002395
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002394
Iteration 81/1000 | Loss: 0.00002394
Iteration 82/1000 | Loss: 0.00002393
Iteration 83/1000 | Loss: 0.00002393
Iteration 84/1000 | Loss: 0.00002393
Iteration 85/1000 | Loss: 0.00002393
Iteration 86/1000 | Loss: 0.00002392
Iteration 87/1000 | Loss: 0.00002392
Iteration 88/1000 | Loss: 0.00002392
Iteration 89/1000 | Loss: 0.00002392
Iteration 90/1000 | Loss: 0.00002391
Iteration 91/1000 | Loss: 0.00002391
Iteration 92/1000 | Loss: 0.00002391
Iteration 93/1000 | Loss: 0.00002390
Iteration 94/1000 | Loss: 0.00002390
Iteration 95/1000 | Loss: 0.00002390
Iteration 96/1000 | Loss: 0.00002390
Iteration 97/1000 | Loss: 0.00002390
Iteration 98/1000 | Loss: 0.00002389
Iteration 99/1000 | Loss: 0.00002389
Iteration 100/1000 | Loss: 0.00002389
Iteration 101/1000 | Loss: 0.00002389
Iteration 102/1000 | Loss: 0.00002388
Iteration 103/1000 | Loss: 0.00002388
Iteration 104/1000 | Loss: 0.00002387
Iteration 105/1000 | Loss: 0.00002387
Iteration 106/1000 | Loss: 0.00002387
Iteration 107/1000 | Loss: 0.00002386
Iteration 108/1000 | Loss: 0.00002386
Iteration 109/1000 | Loss: 0.00002386
Iteration 110/1000 | Loss: 0.00002386
Iteration 111/1000 | Loss: 0.00002386
Iteration 112/1000 | Loss: 0.00002385
Iteration 113/1000 | Loss: 0.00002385
Iteration 114/1000 | Loss: 0.00002385
Iteration 115/1000 | Loss: 0.00002385
Iteration 116/1000 | Loss: 0.00002385
Iteration 117/1000 | Loss: 0.00002385
Iteration 118/1000 | Loss: 0.00002385
Iteration 119/1000 | Loss: 0.00002385
Iteration 120/1000 | Loss: 0.00002385
Iteration 121/1000 | Loss: 0.00002385
Iteration 122/1000 | Loss: 0.00002385
Iteration 123/1000 | Loss: 0.00002384
Iteration 124/1000 | Loss: 0.00002384
Iteration 125/1000 | Loss: 0.00002384
Iteration 126/1000 | Loss: 0.00002384
Iteration 127/1000 | Loss: 0.00002384
Iteration 128/1000 | Loss: 0.00002384
Iteration 129/1000 | Loss: 0.00002384
Iteration 130/1000 | Loss: 0.00002384
Iteration 131/1000 | Loss: 0.00002384
Iteration 132/1000 | Loss: 0.00002384
Iteration 133/1000 | Loss: 0.00002384
Iteration 134/1000 | Loss: 0.00002384
Iteration 135/1000 | Loss: 0.00002384
Iteration 136/1000 | Loss: 0.00002384
Iteration 137/1000 | Loss: 0.00002384
Iteration 138/1000 | Loss: 0.00002384
Iteration 139/1000 | Loss: 0.00002384
Iteration 140/1000 | Loss: 0.00002384
Iteration 141/1000 | Loss: 0.00002383
Iteration 142/1000 | Loss: 0.00002383
Iteration 143/1000 | Loss: 0.00002383
Iteration 144/1000 | Loss: 0.00002383
Iteration 145/1000 | Loss: 0.00002383
Iteration 146/1000 | Loss: 0.00002383
Iteration 147/1000 | Loss: 0.00002383
Iteration 148/1000 | Loss: 0.00002383
Iteration 149/1000 | Loss: 0.00002383
Iteration 150/1000 | Loss: 0.00002383
Iteration 151/1000 | Loss: 0.00002383
Iteration 152/1000 | Loss: 0.00002383
Iteration 153/1000 | Loss: 0.00002383
Iteration 154/1000 | Loss: 0.00002383
Iteration 155/1000 | Loss: 0.00002382
Iteration 156/1000 | Loss: 0.00002382
Iteration 157/1000 | Loss: 0.00002382
Iteration 158/1000 | Loss: 0.00002382
Iteration 159/1000 | Loss: 0.00002382
Iteration 160/1000 | Loss: 0.00002382
Iteration 161/1000 | Loss: 0.00002382
Iteration 162/1000 | Loss: 0.00002382
Iteration 163/1000 | Loss: 0.00002382
Iteration 164/1000 | Loss: 0.00002382
Iteration 165/1000 | Loss: 0.00002382
Iteration 166/1000 | Loss: 0.00002381
Iteration 167/1000 | Loss: 0.00002381
Iteration 168/1000 | Loss: 0.00002381
Iteration 169/1000 | Loss: 0.00002381
Iteration 170/1000 | Loss: 0.00002381
Iteration 171/1000 | Loss: 0.00002381
Iteration 172/1000 | Loss: 0.00002381
Iteration 173/1000 | Loss: 0.00002381
Iteration 174/1000 | Loss: 0.00002381
Iteration 175/1000 | Loss: 0.00002381
Iteration 176/1000 | Loss: 0.00002381
Iteration 177/1000 | Loss: 0.00002381
Iteration 178/1000 | Loss: 0.00002381
Iteration 179/1000 | Loss: 0.00002380
Iteration 180/1000 | Loss: 0.00002380
Iteration 181/1000 | Loss: 0.00002380
Iteration 182/1000 | Loss: 0.00002380
Iteration 183/1000 | Loss: 0.00002380
Iteration 184/1000 | Loss: 0.00002380
Iteration 185/1000 | Loss: 0.00002380
Iteration 186/1000 | Loss: 0.00002380
Iteration 187/1000 | Loss: 0.00002380
Iteration 188/1000 | Loss: 0.00002380
Iteration 189/1000 | Loss: 0.00002380
Iteration 190/1000 | Loss: 0.00002380
Iteration 191/1000 | Loss: 0.00002380
Iteration 192/1000 | Loss: 0.00002380
Iteration 193/1000 | Loss: 0.00002380
Iteration 194/1000 | Loss: 0.00002380
Iteration 195/1000 | Loss: 0.00002380
Iteration 196/1000 | Loss: 0.00002380
Iteration 197/1000 | Loss: 0.00002380
Iteration 198/1000 | Loss: 0.00002380
Iteration 199/1000 | Loss: 0.00002380
Iteration 200/1000 | Loss: 0.00002380
Iteration 201/1000 | Loss: 0.00002380
Iteration 202/1000 | Loss: 0.00002380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.379549368924927e-05, 2.379549368924927e-05, 2.379549368924927e-05, 2.379549368924927e-05, 2.379549368924927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.379549368924927e-05

Optimization complete. Final v2v error: 4.057047367095947 mm

Highest mean error: 4.248969078063965 mm for frame 193

Lowest mean error: 3.813220739364624 mm for frame 177

Saving results

Total time: 54.754658460617065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915929
Iteration 2/25 | Loss: 0.00272503
Iteration 3/25 | Loss: 0.00214907
Iteration 4/25 | Loss: 0.00197840
Iteration 5/25 | Loss: 0.00143176
Iteration 6/25 | Loss: 0.00135639
Iteration 7/25 | Loss: 0.00133921
Iteration 8/25 | Loss: 0.00133515
Iteration 9/25 | Loss: 0.00133367
Iteration 10/25 | Loss: 0.00133260
Iteration 11/25 | Loss: 0.00133120
Iteration 12/25 | Loss: 0.00133094
Iteration 13/25 | Loss: 0.00133081
Iteration 14/25 | Loss: 0.00133070
Iteration 15/25 | Loss: 0.00133061
Iteration 16/25 | Loss: 0.00133050
Iteration 17/25 | Loss: 0.00132989
Iteration 18/25 | Loss: 0.00132924
Iteration 19/25 | Loss: 0.00132910
Iteration 20/25 | Loss: 0.00132908
Iteration 21/25 | Loss: 0.00132908
Iteration 22/25 | Loss: 0.00132907
Iteration 23/25 | Loss: 0.00132906
Iteration 24/25 | Loss: 0.00132906
Iteration 25/25 | Loss: 0.00132904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23189867
Iteration 2/25 | Loss: 0.00081840
Iteration 3/25 | Loss: 0.00081840
Iteration 4/25 | Loss: 0.00081840
Iteration 5/25 | Loss: 0.00081840
Iteration 6/25 | Loss: 0.00081840
Iteration 7/25 | Loss: 0.00081840
Iteration 8/25 | Loss: 0.00081840
Iteration 9/25 | Loss: 0.00081840
Iteration 10/25 | Loss: 0.00081840
Iteration 11/25 | Loss: 0.00081840
Iteration 12/25 | Loss: 0.00081840
Iteration 13/25 | Loss: 0.00081840
Iteration 14/25 | Loss: 0.00081840
Iteration 15/25 | Loss: 0.00081840
Iteration 16/25 | Loss: 0.00081840
Iteration 17/25 | Loss: 0.00081840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008183950558304787, 0.0008183950558304787, 0.0008183950558304787, 0.0008183950558304787, 0.0008183950558304787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008183950558304787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081840
Iteration 2/1000 | Loss: 0.00006332
Iteration 3/1000 | Loss: 0.00003307
Iteration 4/1000 | Loss: 0.00002747
Iteration 5/1000 | Loss: 0.00002620
Iteration 6/1000 | Loss: 0.00002496
Iteration 7/1000 | Loss: 0.00002426
Iteration 8/1000 | Loss: 0.00002351
Iteration 9/1000 | Loss: 0.00002303
Iteration 10/1000 | Loss: 0.00002253
Iteration 11/1000 | Loss: 0.00002220
Iteration 12/1000 | Loss: 0.00002199
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002189
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002186
Iteration 17/1000 | Loss: 0.00002184
Iteration 18/1000 | Loss: 0.00002182
Iteration 19/1000 | Loss: 0.00002178
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002174
Iteration 22/1000 | Loss: 0.00002174
Iteration 23/1000 | Loss: 0.00002174
Iteration 24/1000 | Loss: 0.00002174
Iteration 25/1000 | Loss: 0.00002174
Iteration 26/1000 | Loss: 0.00002174
Iteration 27/1000 | Loss: 0.00002174
Iteration 28/1000 | Loss: 0.00002170
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002170
Iteration 34/1000 | Loss: 0.00002169
Iteration 35/1000 | Loss: 0.00002169
Iteration 36/1000 | Loss: 0.00002169
Iteration 37/1000 | Loss: 0.00002168
Iteration 38/1000 | Loss: 0.00002168
Iteration 39/1000 | Loss: 0.00002168
Iteration 40/1000 | Loss: 0.00002168
Iteration 41/1000 | Loss: 0.00002168
Iteration 42/1000 | Loss: 0.00002167
Iteration 43/1000 | Loss: 0.00002167
Iteration 44/1000 | Loss: 0.00002167
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002167
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002166
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002166
Iteration 52/1000 | Loss: 0.00002166
Iteration 53/1000 | Loss: 0.00002166
Iteration 54/1000 | Loss: 0.00002166
Iteration 55/1000 | Loss: 0.00002166
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002165
Iteration 58/1000 | Loss: 0.00002165
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002165
Iteration 61/1000 | Loss: 0.00002165
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002165
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002164
Iteration 68/1000 | Loss: 0.00002164
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002162
Iteration 80/1000 | Loss: 0.00002162
Iteration 81/1000 | Loss: 0.00002162
Iteration 82/1000 | Loss: 0.00002162
Iteration 83/1000 | Loss: 0.00002162
Iteration 84/1000 | Loss: 0.00002162
Iteration 85/1000 | Loss: 0.00002161
Iteration 86/1000 | Loss: 0.00002161
Iteration 87/1000 | Loss: 0.00002161
Iteration 88/1000 | Loss: 0.00002161
Iteration 89/1000 | Loss: 0.00002161
Iteration 90/1000 | Loss: 0.00002160
Iteration 91/1000 | Loss: 0.00002160
Iteration 92/1000 | Loss: 0.00002160
Iteration 93/1000 | Loss: 0.00002160
Iteration 94/1000 | Loss: 0.00002160
Iteration 95/1000 | Loss: 0.00002160
Iteration 96/1000 | Loss: 0.00002160
Iteration 97/1000 | Loss: 0.00002160
Iteration 98/1000 | Loss: 0.00002160
Iteration 99/1000 | Loss: 0.00002160
Iteration 100/1000 | Loss: 0.00002160
Iteration 101/1000 | Loss: 0.00002160
Iteration 102/1000 | Loss: 0.00002160
Iteration 103/1000 | Loss: 0.00002160
Iteration 104/1000 | Loss: 0.00002160
Iteration 105/1000 | Loss: 0.00002160
Iteration 106/1000 | Loss: 0.00002160
Iteration 107/1000 | Loss: 0.00002160
Iteration 108/1000 | Loss: 0.00002160
Iteration 109/1000 | Loss: 0.00002160
Iteration 110/1000 | Loss: 0.00002160
Iteration 111/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.1600058971671388e-05, 2.1600058971671388e-05, 2.1600058971671388e-05, 2.1600058971671388e-05, 2.1600058971671388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1600058971671388e-05

Optimization complete. Final v2v error: 3.9829611778259277 mm

Highest mean error: 4.370337009429932 mm for frame 198

Lowest mean error: 3.6682302951812744 mm for frame 7

Saving results

Total time: 66.74008846282959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831843
Iteration 2/25 | Loss: 0.00196017
Iteration 3/25 | Loss: 0.00150987
Iteration 4/25 | Loss: 0.00139102
Iteration 5/25 | Loss: 0.00130662
Iteration 6/25 | Loss: 0.00136248
Iteration 7/25 | Loss: 0.00140217
Iteration 8/25 | Loss: 0.00126091
Iteration 9/25 | Loss: 0.00130699
Iteration 10/25 | Loss: 0.00123067
Iteration 11/25 | Loss: 0.00122243
Iteration 12/25 | Loss: 0.00121064
Iteration 13/25 | Loss: 0.00120841
Iteration 14/25 | Loss: 0.00120829
Iteration 15/25 | Loss: 0.00120697
Iteration 16/25 | Loss: 0.00120695
Iteration 17/25 | Loss: 0.00120526
Iteration 18/25 | Loss: 0.00120312
Iteration 19/25 | Loss: 0.00120247
Iteration 20/25 | Loss: 0.00120201
Iteration 21/25 | Loss: 0.00120141
Iteration 22/25 | Loss: 0.00120146
Iteration 23/25 | Loss: 0.00120124
Iteration 24/25 | Loss: 0.00120105
Iteration 25/25 | Loss: 0.00120104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40959406
Iteration 2/25 | Loss: 0.00149729
Iteration 3/25 | Loss: 0.00149729
Iteration 4/25 | Loss: 0.00149728
Iteration 5/25 | Loss: 0.00149728
Iteration 6/25 | Loss: 0.00149728
Iteration 7/25 | Loss: 0.00149728
Iteration 8/25 | Loss: 0.00149728
Iteration 9/25 | Loss: 0.00149728
Iteration 10/25 | Loss: 0.00149728
Iteration 11/25 | Loss: 0.00149728
Iteration 12/25 | Loss: 0.00149728
Iteration 13/25 | Loss: 0.00149728
Iteration 14/25 | Loss: 0.00149728
Iteration 15/25 | Loss: 0.00149728
Iteration 16/25 | Loss: 0.00149728
Iteration 17/25 | Loss: 0.00149728
Iteration 18/25 | Loss: 0.00149728
Iteration 19/25 | Loss: 0.00149728
Iteration 20/25 | Loss: 0.00149728
Iteration 21/25 | Loss: 0.00149728
Iteration 22/25 | Loss: 0.00149728
Iteration 23/25 | Loss: 0.00149728
Iteration 24/25 | Loss: 0.00149728
Iteration 25/25 | Loss: 0.00149728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149728
Iteration 2/1000 | Loss: 0.00015137
Iteration 3/1000 | Loss: 0.00007729
Iteration 4/1000 | Loss: 0.00005069
Iteration 5/1000 | Loss: 0.00004474
Iteration 6/1000 | Loss: 0.00004113
Iteration 7/1000 | Loss: 0.00003934
Iteration 8/1000 | Loss: 0.00003822
Iteration 9/1000 | Loss: 0.00003761
Iteration 10/1000 | Loss: 0.00003691
Iteration 11/1000 | Loss: 0.00003644
Iteration 12/1000 | Loss: 0.00003617
Iteration 13/1000 | Loss: 0.00003580
Iteration 14/1000 | Loss: 0.00003550
Iteration 15/1000 | Loss: 0.00003525
Iteration 16/1000 | Loss: 0.00003511
Iteration 17/1000 | Loss: 0.00003507
Iteration 18/1000 | Loss: 0.00003507
Iteration 19/1000 | Loss: 0.00003507
Iteration 20/1000 | Loss: 0.00003507
Iteration 21/1000 | Loss: 0.00003506
Iteration 22/1000 | Loss: 0.00003506
Iteration 23/1000 | Loss: 0.00003505
Iteration 24/1000 | Loss: 0.00003504
Iteration 25/1000 | Loss: 0.00003504
Iteration 26/1000 | Loss: 0.00003502
Iteration 27/1000 | Loss: 0.00003502
Iteration 28/1000 | Loss: 0.00003502
Iteration 29/1000 | Loss: 0.00003501
Iteration 30/1000 | Loss: 0.00003501
Iteration 31/1000 | Loss: 0.00003500
Iteration 32/1000 | Loss: 0.00003500
Iteration 33/1000 | Loss: 0.00003500
Iteration 34/1000 | Loss: 0.00003499
Iteration 35/1000 | Loss: 0.00003499
Iteration 36/1000 | Loss: 0.00003499
Iteration 37/1000 | Loss: 0.00003499
Iteration 38/1000 | Loss: 0.00003498
Iteration 39/1000 | Loss: 0.00003498
Iteration 40/1000 | Loss: 0.00003497
Iteration 41/1000 | Loss: 0.00003491
Iteration 42/1000 | Loss: 0.00003491
Iteration 43/1000 | Loss: 0.00003490
Iteration 44/1000 | Loss: 0.00003490
Iteration 45/1000 | Loss: 0.00003490
Iteration 46/1000 | Loss: 0.00003489
Iteration 47/1000 | Loss: 0.00003491
Iteration 48/1000 | Loss: 0.00003490
Iteration 49/1000 | Loss: 0.00003490
Iteration 50/1000 | Loss: 0.00003490
Iteration 51/1000 | Loss: 0.00003490
Iteration 52/1000 | Loss: 0.00003489
Iteration 53/1000 | Loss: 0.00003489
Iteration 54/1000 | Loss: 0.00003489
Iteration 55/1000 | Loss: 0.00003488
Iteration 56/1000 | Loss: 0.00003488
Iteration 57/1000 | Loss: 0.00003488
Iteration 58/1000 | Loss: 0.00003488
Iteration 59/1000 | Loss: 0.00003488
Iteration 60/1000 | Loss: 0.00003488
Iteration 61/1000 | Loss: 0.00003488
Iteration 62/1000 | Loss: 0.00003487
Iteration 63/1000 | Loss: 0.00003487
Iteration 64/1000 | Loss: 0.00003487
Iteration 65/1000 | Loss: 0.00003487
Iteration 66/1000 | Loss: 0.00003487
Iteration 67/1000 | Loss: 0.00003487
Iteration 68/1000 | Loss: 0.00003487
Iteration 69/1000 | Loss: 0.00003486
Iteration 70/1000 | Loss: 0.00003486
Iteration 71/1000 | Loss: 0.00003486
Iteration 72/1000 | Loss: 0.00003486
Iteration 73/1000 | Loss: 0.00003485
Iteration 74/1000 | Loss: 0.00003485
Iteration 75/1000 | Loss: 0.00003485
Iteration 76/1000 | Loss: 0.00003485
Iteration 77/1000 | Loss: 0.00003484
Iteration 78/1000 | Loss: 0.00003484
Iteration 79/1000 | Loss: 0.00003484
Iteration 80/1000 | Loss: 0.00003483
Iteration 81/1000 | Loss: 0.00003483
Iteration 82/1000 | Loss: 0.00003483
Iteration 83/1000 | Loss: 0.00003483
Iteration 84/1000 | Loss: 0.00003483
Iteration 85/1000 | Loss: 0.00003483
Iteration 86/1000 | Loss: 0.00003483
Iteration 87/1000 | Loss: 0.00003483
Iteration 88/1000 | Loss: 0.00003482
Iteration 89/1000 | Loss: 0.00003482
Iteration 90/1000 | Loss: 0.00003482
Iteration 91/1000 | Loss: 0.00003482
Iteration 92/1000 | Loss: 0.00003482
Iteration 93/1000 | Loss: 0.00003482
Iteration 94/1000 | Loss: 0.00003481
Iteration 95/1000 | Loss: 0.00003481
Iteration 96/1000 | Loss: 0.00003481
Iteration 97/1000 | Loss: 0.00003481
Iteration 98/1000 | Loss: 0.00003480
Iteration 99/1000 | Loss: 0.00003480
Iteration 100/1000 | Loss: 0.00003480
Iteration 101/1000 | Loss: 0.00003480
Iteration 102/1000 | Loss: 0.00003480
Iteration 103/1000 | Loss: 0.00003480
Iteration 104/1000 | Loss: 0.00003480
Iteration 105/1000 | Loss: 0.00003480
Iteration 106/1000 | Loss: 0.00003480
Iteration 107/1000 | Loss: 0.00003479
Iteration 108/1000 | Loss: 0.00003479
Iteration 109/1000 | Loss: 0.00003479
Iteration 110/1000 | Loss: 0.00003479
Iteration 111/1000 | Loss: 0.00003479
Iteration 112/1000 | Loss: 0.00003479
Iteration 113/1000 | Loss: 0.00003479
Iteration 114/1000 | Loss: 0.00003479
Iteration 115/1000 | Loss: 0.00003479
Iteration 116/1000 | Loss: 0.00003478
Iteration 117/1000 | Loss: 0.00003478
Iteration 118/1000 | Loss: 0.00003478
Iteration 119/1000 | Loss: 0.00003478
Iteration 120/1000 | Loss: 0.00003475
Iteration 121/1000 | Loss: 0.00003475
Iteration 122/1000 | Loss: 0.00003475
Iteration 123/1000 | Loss: 0.00003475
Iteration 124/1000 | Loss: 0.00003475
Iteration 125/1000 | Loss: 0.00003475
Iteration 126/1000 | Loss: 0.00003475
Iteration 127/1000 | Loss: 0.00003475
Iteration 128/1000 | Loss: 0.00003475
Iteration 129/1000 | Loss: 0.00003475
Iteration 130/1000 | Loss: 0.00003475
Iteration 131/1000 | Loss: 0.00003474
Iteration 132/1000 | Loss: 0.00003474
Iteration 133/1000 | Loss: 0.00003474
Iteration 134/1000 | Loss: 0.00003474
Iteration 135/1000 | Loss: 0.00003474
Iteration 136/1000 | Loss: 0.00003474
Iteration 137/1000 | Loss: 0.00003474
Iteration 138/1000 | Loss: 0.00003474
Iteration 139/1000 | Loss: 0.00003474
Iteration 140/1000 | Loss: 0.00003474
Iteration 141/1000 | Loss: 0.00003474
Iteration 142/1000 | Loss: 0.00003474
Iteration 143/1000 | Loss: 0.00003474
Iteration 144/1000 | Loss: 0.00003473
Iteration 145/1000 | Loss: 0.00003473
Iteration 146/1000 | Loss: 0.00003473
Iteration 147/1000 | Loss: 0.00003473
Iteration 148/1000 | Loss: 0.00003473
Iteration 149/1000 | Loss: 0.00003472
Iteration 150/1000 | Loss: 0.00003472
Iteration 151/1000 | Loss: 0.00003472
Iteration 152/1000 | Loss: 0.00003472
Iteration 153/1000 | Loss: 0.00003472
Iteration 154/1000 | Loss: 0.00003472
Iteration 155/1000 | Loss: 0.00003472
Iteration 156/1000 | Loss: 0.00003472
Iteration 157/1000 | Loss: 0.00003472
Iteration 158/1000 | Loss: 0.00003472
Iteration 159/1000 | Loss: 0.00003472
Iteration 160/1000 | Loss: 0.00003471
Iteration 161/1000 | Loss: 0.00003471
Iteration 162/1000 | Loss: 0.00003471
Iteration 163/1000 | Loss: 0.00003471
Iteration 164/1000 | Loss: 0.00003471
Iteration 165/1000 | Loss: 0.00003471
Iteration 166/1000 | Loss: 0.00003471
Iteration 167/1000 | Loss: 0.00003471
Iteration 168/1000 | Loss: 0.00003471
Iteration 169/1000 | Loss: 0.00003471
Iteration 170/1000 | Loss: 0.00003471
Iteration 171/1000 | Loss: 0.00003471
Iteration 172/1000 | Loss: 0.00003471
Iteration 173/1000 | Loss: 0.00003471
Iteration 174/1000 | Loss: 0.00003470
Iteration 175/1000 | Loss: 0.00003470
Iteration 176/1000 | Loss: 0.00003470
Iteration 177/1000 | Loss: 0.00003470
Iteration 178/1000 | Loss: 0.00003470
Iteration 179/1000 | Loss: 0.00003470
Iteration 180/1000 | Loss: 0.00003470
Iteration 181/1000 | Loss: 0.00003470
Iteration 182/1000 | Loss: 0.00003470
Iteration 183/1000 | Loss: 0.00003470
Iteration 184/1000 | Loss: 0.00003469
Iteration 185/1000 | Loss: 0.00003469
Iteration 186/1000 | Loss: 0.00003469
Iteration 187/1000 | Loss: 0.00003469
Iteration 188/1000 | Loss: 0.00003469
Iteration 189/1000 | Loss: 0.00003469
Iteration 190/1000 | Loss: 0.00003469
Iteration 191/1000 | Loss: 0.00003469
Iteration 192/1000 | Loss: 0.00003469
Iteration 193/1000 | Loss: 0.00003469
Iteration 194/1000 | Loss: 0.00003468
Iteration 195/1000 | Loss: 0.00003468
Iteration 196/1000 | Loss: 0.00003468
Iteration 197/1000 | Loss: 0.00003468
Iteration 198/1000 | Loss: 0.00003467
Iteration 199/1000 | Loss: 0.00003467
Iteration 200/1000 | Loss: 0.00003467
Iteration 201/1000 | Loss: 0.00003467
Iteration 202/1000 | Loss: 0.00003467
Iteration 203/1000 | Loss: 0.00003467
Iteration 204/1000 | Loss: 0.00003467
Iteration 205/1000 | Loss: 0.00003467
Iteration 206/1000 | Loss: 0.00003467
Iteration 207/1000 | Loss: 0.00003467
Iteration 208/1000 | Loss: 0.00003467
Iteration 209/1000 | Loss: 0.00003466
Iteration 210/1000 | Loss: 0.00003466
Iteration 211/1000 | Loss: 0.00003466
Iteration 212/1000 | Loss: 0.00003466
Iteration 213/1000 | Loss: 0.00003466
Iteration 214/1000 | Loss: 0.00003466
Iteration 215/1000 | Loss: 0.00003465
Iteration 216/1000 | Loss: 0.00003465
Iteration 217/1000 | Loss: 0.00003465
Iteration 218/1000 | Loss: 0.00003465
Iteration 219/1000 | Loss: 0.00003465
Iteration 220/1000 | Loss: 0.00003465
Iteration 221/1000 | Loss: 0.00003464
Iteration 222/1000 | Loss: 0.00003464
Iteration 223/1000 | Loss: 0.00003464
Iteration 224/1000 | Loss: 0.00003464
Iteration 225/1000 | Loss: 0.00003464
Iteration 226/1000 | Loss: 0.00003464
Iteration 227/1000 | Loss: 0.00003464
Iteration 228/1000 | Loss: 0.00003464
Iteration 229/1000 | Loss: 0.00003464
Iteration 230/1000 | Loss: 0.00003464
Iteration 231/1000 | Loss: 0.00003464
Iteration 232/1000 | Loss: 0.00003464
Iteration 233/1000 | Loss: 0.00003463
Iteration 234/1000 | Loss: 0.00003463
Iteration 235/1000 | Loss: 0.00003463
Iteration 236/1000 | Loss: 0.00003463
Iteration 237/1000 | Loss: 0.00003463
Iteration 238/1000 | Loss: 0.00003463
Iteration 239/1000 | Loss: 0.00003462
Iteration 240/1000 | Loss: 0.00003462
Iteration 241/1000 | Loss: 0.00003473
Iteration 242/1000 | Loss: 0.00003472
Iteration 243/1000 | Loss: 0.00003471
Iteration 244/1000 | Loss: 0.00003470
Iteration 245/1000 | Loss: 0.00003468
Iteration 246/1000 | Loss: 0.00003467
Iteration 247/1000 | Loss: 0.00003467
Iteration 248/1000 | Loss: 0.00003467
Iteration 249/1000 | Loss: 0.00003467
Iteration 250/1000 | Loss: 0.00003467
Iteration 251/1000 | Loss: 0.00003467
Iteration 252/1000 | Loss: 0.00003467
Iteration 253/1000 | Loss: 0.00003467
Iteration 254/1000 | Loss: 0.00003467
Iteration 255/1000 | Loss: 0.00003467
Iteration 256/1000 | Loss: 0.00003467
Iteration 257/1000 | Loss: 0.00003467
Iteration 258/1000 | Loss: 0.00003467
Iteration 259/1000 | Loss: 0.00003467
Iteration 260/1000 | Loss: 0.00003467
Iteration 261/1000 | Loss: 0.00003467
Iteration 262/1000 | Loss: 0.00003467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [3.466696944087744e-05, 3.466696944087744e-05, 3.466696944087744e-05, 3.466696944087744e-05, 3.466696944087744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.466696944087744e-05

Optimization complete. Final v2v error: 4.093750953674316 mm

Highest mean error: 20.378616333007812 mm for frame 52

Lowest mean error: 3.118197441101074 mm for frame 107

Saving results

Total time: 81.40484571456909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863446
Iteration 2/25 | Loss: 0.00146105
Iteration 3/25 | Loss: 0.00130736
Iteration 4/25 | Loss: 0.00128941
Iteration 5/25 | Loss: 0.00128520
Iteration 6/25 | Loss: 0.00128399
Iteration 7/25 | Loss: 0.00128369
Iteration 8/25 | Loss: 0.00128369
Iteration 9/25 | Loss: 0.00128369
Iteration 10/25 | Loss: 0.00128369
Iteration 11/25 | Loss: 0.00128369
Iteration 12/25 | Loss: 0.00128369
Iteration 13/25 | Loss: 0.00128369
Iteration 14/25 | Loss: 0.00128369
Iteration 15/25 | Loss: 0.00128369
Iteration 16/25 | Loss: 0.00128369
Iteration 17/25 | Loss: 0.00128369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012836860259994864, 0.0012836860259994864, 0.0012836860259994864, 0.0012836860259994864, 0.0012836860259994864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012836860259994864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29098463
Iteration 2/25 | Loss: 0.00282543
Iteration 3/25 | Loss: 0.00282543
Iteration 4/25 | Loss: 0.00282542
Iteration 5/25 | Loss: 0.00282542
Iteration 6/25 | Loss: 0.00282542
Iteration 7/25 | Loss: 0.00282542
Iteration 8/25 | Loss: 0.00282542
Iteration 9/25 | Loss: 0.00282542
Iteration 10/25 | Loss: 0.00282542
Iteration 11/25 | Loss: 0.00282542
Iteration 12/25 | Loss: 0.00282542
Iteration 13/25 | Loss: 0.00282542
Iteration 14/25 | Loss: 0.00282542
Iteration 15/25 | Loss: 0.00282542
Iteration 16/25 | Loss: 0.00282542
Iteration 17/25 | Loss: 0.00282542
Iteration 18/25 | Loss: 0.00282542
Iteration 19/25 | Loss: 0.00282542
Iteration 20/25 | Loss: 0.00282542
Iteration 21/25 | Loss: 0.00282542
Iteration 22/25 | Loss: 0.00282542
Iteration 23/25 | Loss: 0.00282542
Iteration 24/25 | Loss: 0.00282542
Iteration 25/25 | Loss: 0.00282542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282542
Iteration 2/1000 | Loss: 0.00019933
Iteration 3/1000 | Loss: 0.00013415
Iteration 4/1000 | Loss: 0.00010940
Iteration 5/1000 | Loss: 0.00009901
Iteration 6/1000 | Loss: 0.00009304
Iteration 7/1000 | Loss: 0.00008927
Iteration 8/1000 | Loss: 0.00008600
Iteration 9/1000 | Loss: 0.00008302
Iteration 10/1000 | Loss: 0.00007959
Iteration 11/1000 | Loss: 0.00007814
Iteration 12/1000 | Loss: 0.00007685
Iteration 13/1000 | Loss: 0.00007529
Iteration 14/1000 | Loss: 0.00007410
Iteration 15/1000 | Loss: 0.00007327
Iteration 16/1000 | Loss: 0.00007250
Iteration 17/1000 | Loss: 0.00007188
Iteration 18/1000 | Loss: 0.00007133
Iteration 19/1000 | Loss: 0.00007085
Iteration 20/1000 | Loss: 0.00007043
Iteration 21/1000 | Loss: 0.00007003
Iteration 22/1000 | Loss: 0.00006978
Iteration 23/1000 | Loss: 0.00006954
Iteration 24/1000 | Loss: 0.00006934
Iteration 25/1000 | Loss: 0.00006913
Iteration 26/1000 | Loss: 0.00006903
Iteration 27/1000 | Loss: 0.00006902
Iteration 28/1000 | Loss: 0.00006900
Iteration 29/1000 | Loss: 0.00006896
Iteration 30/1000 | Loss: 0.00006895
Iteration 31/1000 | Loss: 0.00006894
Iteration 32/1000 | Loss: 0.00006892
Iteration 33/1000 | Loss: 0.00006888
Iteration 34/1000 | Loss: 0.00006877
Iteration 35/1000 | Loss: 0.00006873
Iteration 36/1000 | Loss: 0.00006873
Iteration 37/1000 | Loss: 0.00006872
Iteration 38/1000 | Loss: 0.00006872
Iteration 39/1000 | Loss: 0.00006864
Iteration 40/1000 | Loss: 0.00006864
Iteration 41/1000 | Loss: 0.00006863
Iteration 42/1000 | Loss: 0.00006862
Iteration 43/1000 | Loss: 0.00006862
Iteration 44/1000 | Loss: 0.00006862
Iteration 45/1000 | Loss: 0.00006861
Iteration 46/1000 | Loss: 0.00006861
Iteration 47/1000 | Loss: 0.00006861
Iteration 48/1000 | Loss: 0.00006861
Iteration 49/1000 | Loss: 0.00006861
Iteration 50/1000 | Loss: 0.00006861
Iteration 51/1000 | Loss: 0.00006861
Iteration 52/1000 | Loss: 0.00006861
Iteration 53/1000 | Loss: 0.00006860
Iteration 54/1000 | Loss: 0.00006860
Iteration 55/1000 | Loss: 0.00006860
Iteration 56/1000 | Loss: 0.00006860
Iteration 57/1000 | Loss: 0.00006860
Iteration 58/1000 | Loss: 0.00006860
Iteration 59/1000 | Loss: 0.00006859
Iteration 60/1000 | Loss: 0.00006859
Iteration 61/1000 | Loss: 0.00006859
Iteration 62/1000 | Loss: 0.00006858
Iteration 63/1000 | Loss: 0.00006858
Iteration 64/1000 | Loss: 0.00006858
Iteration 65/1000 | Loss: 0.00006857
Iteration 66/1000 | Loss: 0.00006857
Iteration 67/1000 | Loss: 0.00006857
Iteration 68/1000 | Loss: 0.00006857
Iteration 69/1000 | Loss: 0.00006857
Iteration 70/1000 | Loss: 0.00006857
Iteration 71/1000 | Loss: 0.00006857
Iteration 72/1000 | Loss: 0.00006857
Iteration 73/1000 | Loss: 0.00006857
Iteration 74/1000 | Loss: 0.00006857
Iteration 75/1000 | Loss: 0.00006856
Iteration 76/1000 | Loss: 0.00006856
Iteration 77/1000 | Loss: 0.00006856
Iteration 78/1000 | Loss: 0.00006856
Iteration 79/1000 | Loss: 0.00006856
Iteration 80/1000 | Loss: 0.00006856
Iteration 81/1000 | Loss: 0.00006856
Iteration 82/1000 | Loss: 0.00006856
Iteration 83/1000 | Loss: 0.00006855
Iteration 84/1000 | Loss: 0.00006854
Iteration 85/1000 | Loss: 0.00006853
Iteration 86/1000 | Loss: 0.00006853
Iteration 87/1000 | Loss: 0.00006852
Iteration 88/1000 | Loss: 0.00006852
Iteration 89/1000 | Loss: 0.00006852
Iteration 90/1000 | Loss: 0.00006852
Iteration 91/1000 | Loss: 0.00006852
Iteration 92/1000 | Loss: 0.00006851
Iteration 93/1000 | Loss: 0.00006851
Iteration 94/1000 | Loss: 0.00006851
Iteration 95/1000 | Loss: 0.00006851
Iteration 96/1000 | Loss: 0.00006851
Iteration 97/1000 | Loss: 0.00006851
Iteration 98/1000 | Loss: 0.00006850
Iteration 99/1000 | Loss: 0.00006850
Iteration 100/1000 | Loss: 0.00006849
Iteration 101/1000 | Loss: 0.00006849
Iteration 102/1000 | Loss: 0.00006849
Iteration 103/1000 | Loss: 0.00006849
Iteration 104/1000 | Loss: 0.00006848
Iteration 105/1000 | Loss: 0.00006848
Iteration 106/1000 | Loss: 0.00006848
Iteration 107/1000 | Loss: 0.00006848
Iteration 108/1000 | Loss: 0.00006848
Iteration 109/1000 | Loss: 0.00006847
Iteration 110/1000 | Loss: 0.00006847
Iteration 111/1000 | Loss: 0.00006847
Iteration 112/1000 | Loss: 0.00006847
Iteration 113/1000 | Loss: 0.00006847
Iteration 114/1000 | Loss: 0.00006847
Iteration 115/1000 | Loss: 0.00006847
Iteration 116/1000 | Loss: 0.00006847
Iteration 117/1000 | Loss: 0.00006847
Iteration 118/1000 | Loss: 0.00006846
Iteration 119/1000 | Loss: 0.00006846
Iteration 120/1000 | Loss: 0.00006846
Iteration 121/1000 | Loss: 0.00006846
Iteration 122/1000 | Loss: 0.00006846
Iteration 123/1000 | Loss: 0.00006845
Iteration 124/1000 | Loss: 0.00006845
Iteration 125/1000 | Loss: 0.00006845
Iteration 126/1000 | Loss: 0.00006845
Iteration 127/1000 | Loss: 0.00006844
Iteration 128/1000 | Loss: 0.00006844
Iteration 129/1000 | Loss: 0.00006844
Iteration 130/1000 | Loss: 0.00006843
Iteration 131/1000 | Loss: 0.00006843
Iteration 132/1000 | Loss: 0.00006843
Iteration 133/1000 | Loss: 0.00006843
Iteration 134/1000 | Loss: 0.00006842
Iteration 135/1000 | Loss: 0.00006842
Iteration 136/1000 | Loss: 0.00006842
Iteration 137/1000 | Loss: 0.00006842
Iteration 138/1000 | Loss: 0.00006842
Iteration 139/1000 | Loss: 0.00006842
Iteration 140/1000 | Loss: 0.00006842
Iteration 141/1000 | Loss: 0.00006842
Iteration 142/1000 | Loss: 0.00006842
Iteration 143/1000 | Loss: 0.00006841
Iteration 144/1000 | Loss: 0.00006841
Iteration 145/1000 | Loss: 0.00006841
Iteration 146/1000 | Loss: 0.00006841
Iteration 147/1000 | Loss: 0.00006841
Iteration 148/1000 | Loss: 0.00006841
Iteration 149/1000 | Loss: 0.00006841
Iteration 150/1000 | Loss: 0.00006841
Iteration 151/1000 | Loss: 0.00006841
Iteration 152/1000 | Loss: 0.00006840
Iteration 153/1000 | Loss: 0.00006840
Iteration 154/1000 | Loss: 0.00006840
Iteration 155/1000 | Loss: 0.00006840
Iteration 156/1000 | Loss: 0.00006840
Iteration 157/1000 | Loss: 0.00006840
Iteration 158/1000 | Loss: 0.00006840
Iteration 159/1000 | Loss: 0.00006840
Iteration 160/1000 | Loss: 0.00006840
Iteration 161/1000 | Loss: 0.00006839
Iteration 162/1000 | Loss: 0.00006839
Iteration 163/1000 | Loss: 0.00006839
Iteration 164/1000 | Loss: 0.00006839
Iteration 165/1000 | Loss: 0.00006839
Iteration 166/1000 | Loss: 0.00006839
Iteration 167/1000 | Loss: 0.00006839
Iteration 168/1000 | Loss: 0.00006839
Iteration 169/1000 | Loss: 0.00006839
Iteration 170/1000 | Loss: 0.00006839
Iteration 171/1000 | Loss: 0.00006838
Iteration 172/1000 | Loss: 0.00006838
Iteration 173/1000 | Loss: 0.00006838
Iteration 174/1000 | Loss: 0.00006838
Iteration 175/1000 | Loss: 0.00006838
Iteration 176/1000 | Loss: 0.00006838
Iteration 177/1000 | Loss: 0.00006838
Iteration 178/1000 | Loss: 0.00006838
Iteration 179/1000 | Loss: 0.00006838
Iteration 180/1000 | Loss: 0.00006838
Iteration 181/1000 | Loss: 0.00006838
Iteration 182/1000 | Loss: 0.00006837
Iteration 183/1000 | Loss: 0.00006837
Iteration 184/1000 | Loss: 0.00006837
Iteration 185/1000 | Loss: 0.00006837
Iteration 186/1000 | Loss: 0.00006837
Iteration 187/1000 | Loss: 0.00006837
Iteration 188/1000 | Loss: 0.00006837
Iteration 189/1000 | Loss: 0.00006837
Iteration 190/1000 | Loss: 0.00006837
Iteration 191/1000 | Loss: 0.00006837
Iteration 192/1000 | Loss: 0.00006837
Iteration 193/1000 | Loss: 0.00006837
Iteration 194/1000 | Loss: 0.00006837
Iteration 195/1000 | Loss: 0.00006837
Iteration 196/1000 | Loss: 0.00006837
Iteration 197/1000 | Loss: 0.00006837
Iteration 198/1000 | Loss: 0.00006837
Iteration 199/1000 | Loss: 0.00006837
Iteration 200/1000 | Loss: 0.00006837
Iteration 201/1000 | Loss: 0.00006837
Iteration 202/1000 | Loss: 0.00006837
Iteration 203/1000 | Loss: 0.00006837
Iteration 204/1000 | Loss: 0.00006837
Iteration 205/1000 | Loss: 0.00006837
Iteration 206/1000 | Loss: 0.00006837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [6.837078399257734e-05, 6.837078399257734e-05, 6.837078399257734e-05, 6.837078399257734e-05, 6.837078399257734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.837078399257734e-05

Optimization complete. Final v2v error: 4.234126091003418 mm

Highest mean error: 13.262654304504395 mm for frame 86

Lowest mean error: 3.1747398376464844 mm for frame 136

Saving results

Total time: 64.23003816604614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381395
Iteration 2/25 | Loss: 0.00118967
Iteration 3/25 | Loss: 0.00112393
Iteration 4/25 | Loss: 0.00111161
Iteration 5/25 | Loss: 0.00110748
Iteration 6/25 | Loss: 0.00110647
Iteration 7/25 | Loss: 0.00110647
Iteration 8/25 | Loss: 0.00110647
Iteration 9/25 | Loss: 0.00110647
Iteration 10/25 | Loss: 0.00110647
Iteration 11/25 | Loss: 0.00110647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011064676800742745, 0.0011064676800742745, 0.0011064676800742745, 0.0011064676800742745, 0.0011064676800742745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011064676800742745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38698888
Iteration 2/25 | Loss: 0.00133969
Iteration 3/25 | Loss: 0.00133969
Iteration 4/25 | Loss: 0.00133969
Iteration 5/25 | Loss: 0.00133969
Iteration 6/25 | Loss: 0.00133969
Iteration 7/25 | Loss: 0.00133969
Iteration 8/25 | Loss: 0.00133969
Iteration 9/25 | Loss: 0.00133968
Iteration 10/25 | Loss: 0.00133968
Iteration 11/25 | Loss: 0.00133968
Iteration 12/25 | Loss: 0.00133968
Iteration 13/25 | Loss: 0.00133968
Iteration 14/25 | Loss: 0.00133968
Iteration 15/25 | Loss: 0.00133968
Iteration 16/25 | Loss: 0.00133968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013396843569353223, 0.0013396843569353223, 0.0013396843569353223, 0.0013396843569353223, 0.0013396843569353223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013396843569353223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133968
Iteration 2/1000 | Loss: 0.00003710
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00001446
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001263
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001205
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001140
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001131
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001130
Iteration 52/1000 | Loss: 0.00001130
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001130
Iteration 55/1000 | Loss: 0.00001130
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001130
Iteration 58/1000 | Loss: 0.00001130
Iteration 59/1000 | Loss: 0.00001130
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.129670454247389e-05, 1.129670454247389e-05, 1.129670454247389e-05, 1.129670454247389e-05, 1.129670454247389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.129670454247389e-05

Optimization complete. Final v2v error: 2.9113290309906006 mm

Highest mean error: 3.188765525817871 mm for frame 114

Lowest mean error: 2.7312190532684326 mm for frame 23

Saving results

Total time: 27.455838918685913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_1312/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_1312/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886127
Iteration 2/25 | Loss: 0.00131097
Iteration 3/25 | Loss: 0.00119423
Iteration 4/25 | Loss: 0.00117975
Iteration 5/25 | Loss: 0.00117585
Iteration 6/25 | Loss: 0.00117525
Iteration 7/25 | Loss: 0.00117525
Iteration 8/25 | Loss: 0.00117525
Iteration 9/25 | Loss: 0.00117525
Iteration 10/25 | Loss: 0.00117525
Iteration 11/25 | Loss: 0.00117525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011752540012821555, 0.0011752540012821555, 0.0011752540012821555, 0.0011752540012821555, 0.0011752540012821555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011752540012821555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29786408
Iteration 2/25 | Loss: 0.00134923
Iteration 3/25 | Loss: 0.00134921
Iteration 4/25 | Loss: 0.00134921
Iteration 5/25 | Loss: 0.00134921
Iteration 6/25 | Loss: 0.00134921
Iteration 7/25 | Loss: 0.00134921
Iteration 8/25 | Loss: 0.00134921
Iteration 9/25 | Loss: 0.00134921
Iteration 10/25 | Loss: 0.00134921
Iteration 11/25 | Loss: 0.00134921
Iteration 12/25 | Loss: 0.00134921
Iteration 13/25 | Loss: 0.00134921
Iteration 14/25 | Loss: 0.00134921
Iteration 15/25 | Loss: 0.00134921
Iteration 16/25 | Loss: 0.00134921
Iteration 17/25 | Loss: 0.00134921
Iteration 18/25 | Loss: 0.00134921
Iteration 19/25 | Loss: 0.00134921
Iteration 20/25 | Loss: 0.00134921
Iteration 21/25 | Loss: 0.00134921
Iteration 22/25 | Loss: 0.00134921
Iteration 23/25 | Loss: 0.00134921
Iteration 24/25 | Loss: 0.00134921
Iteration 25/25 | Loss: 0.00134921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134921
Iteration 2/1000 | Loss: 0.00004493
Iteration 3/1000 | Loss: 0.00002334
Iteration 4/1000 | Loss: 0.00001946
Iteration 5/1000 | Loss: 0.00001733
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001357
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001322
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001296
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001291
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001290
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001289
Iteration 30/1000 | Loss: 0.00001289
Iteration 31/1000 | Loss: 0.00001289
Iteration 32/1000 | Loss: 0.00001288
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001287
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001286
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001285
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001284
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001284
Iteration 53/1000 | Loss: 0.00001284
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001283
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001281
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001280
Iteration 90/1000 | Loss: 0.00001280
Iteration 91/1000 | Loss: 0.00001280
Iteration 92/1000 | Loss: 0.00001280
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001279
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001279
Iteration 135/1000 | Loss: 0.00001279
Iteration 136/1000 | Loss: 0.00001279
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001279
Iteration 140/1000 | Loss: 0.00001279
Iteration 141/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.2792611414624844e-05, 1.2792611414624844e-05, 1.2792611414624844e-05, 1.2792611414624844e-05, 1.2792611414624844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2792611414624844e-05

Optimization complete. Final v2v error: 3.190443277359009 mm

Highest mean error: 3.3530619144439697 mm for frame 133

Lowest mean error: 2.7859973907470703 mm for frame 0

Saving results

Total time: 32.99977731704712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081776
Iteration 2/25 | Loss: 0.00223318
Iteration 3/25 | Loss: 0.00181939
Iteration 4/25 | Loss: 0.00168728
Iteration 5/25 | Loss: 0.00154584
Iteration 6/25 | Loss: 0.00132024
Iteration 7/25 | Loss: 0.00121827
Iteration 8/25 | Loss: 0.00117423
Iteration 9/25 | Loss: 0.00116369
Iteration 10/25 | Loss: 0.00116194
Iteration 11/25 | Loss: 0.00116778
Iteration 12/25 | Loss: 0.00126033
Iteration 13/25 | Loss: 0.00118381
Iteration 14/25 | Loss: 0.00113864
Iteration 15/25 | Loss: 0.00112356
Iteration 16/25 | Loss: 0.00112077
Iteration 17/25 | Loss: 0.00112000
Iteration 18/25 | Loss: 0.00111983
Iteration 19/25 | Loss: 0.00111980
Iteration 20/25 | Loss: 0.00111979
Iteration 21/25 | Loss: 0.00111979
Iteration 22/25 | Loss: 0.00111979
Iteration 23/25 | Loss: 0.00111978
Iteration 24/25 | Loss: 0.00111978
Iteration 25/25 | Loss: 0.00111978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28403568
Iteration 2/25 | Loss: 0.00094129
Iteration 3/25 | Loss: 0.00094128
Iteration 4/25 | Loss: 0.00094128
Iteration 5/25 | Loss: 0.00094128
Iteration 6/25 | Loss: 0.00094128
Iteration 7/25 | Loss: 0.00094128
Iteration 8/25 | Loss: 0.00094128
Iteration 9/25 | Loss: 0.00094128
Iteration 10/25 | Loss: 0.00094128
Iteration 11/25 | Loss: 0.00094128
Iteration 12/25 | Loss: 0.00094128
Iteration 13/25 | Loss: 0.00094128
Iteration 14/25 | Loss: 0.00094128
Iteration 15/25 | Loss: 0.00094128
Iteration 16/25 | Loss: 0.00094128
Iteration 17/25 | Loss: 0.00094128
Iteration 18/25 | Loss: 0.00094128
Iteration 19/25 | Loss: 0.00094128
Iteration 20/25 | Loss: 0.00094128
Iteration 21/25 | Loss: 0.00094128
Iteration 22/25 | Loss: 0.00094128
Iteration 23/25 | Loss: 0.00094128
Iteration 24/25 | Loss: 0.00094128
Iteration 25/25 | Loss: 0.00094128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094128
Iteration 2/1000 | Loss: 0.00010191
Iteration 3/1000 | Loss: 0.00007310
Iteration 4/1000 | Loss: 0.00003888
Iteration 5/1000 | Loss: 0.00003203
Iteration 6/1000 | Loss: 0.00002832
Iteration 7/1000 | Loss: 0.00002690
Iteration 8/1000 | Loss: 0.00002532
Iteration 9/1000 | Loss: 0.00002434
Iteration 10/1000 | Loss: 0.00002374
Iteration 11/1000 | Loss: 0.00002333
Iteration 12/1000 | Loss: 0.00002310
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002278
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002272
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002269
Iteration 19/1000 | Loss: 0.00002268
Iteration 20/1000 | Loss: 0.00002268
Iteration 21/1000 | Loss: 0.00002266
Iteration 22/1000 | Loss: 0.00002266
Iteration 23/1000 | Loss: 0.00002265
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002265
Iteration 26/1000 | Loss: 0.00002265
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00002265
Iteration 30/1000 | Loss: 0.00002265
Iteration 31/1000 | Loss: 0.00002265
Iteration 32/1000 | Loss: 0.00002264
Iteration 33/1000 | Loss: 0.00002264
Iteration 34/1000 | Loss: 0.00002263
Iteration 35/1000 | Loss: 0.00002262
Iteration 36/1000 | Loss: 0.00002262
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002261
Iteration 39/1000 | Loss: 0.00002260
Iteration 40/1000 | Loss: 0.00002259
Iteration 41/1000 | Loss: 0.00002259
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00002259
Iteration 44/1000 | Loss: 0.00002258
Iteration 45/1000 | Loss: 0.00002257
Iteration 46/1000 | Loss: 0.00002257
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002253
Iteration 51/1000 | Loss: 0.00002252
Iteration 52/1000 | Loss: 0.00011208
Iteration 53/1000 | Loss: 0.00002589
Iteration 54/1000 | Loss: 0.00002366
Iteration 55/1000 | Loss: 0.00002256
Iteration 56/1000 | Loss: 0.00002228
Iteration 57/1000 | Loss: 0.00002216
Iteration 58/1000 | Loss: 0.00002214
Iteration 59/1000 | Loss: 0.00002214
Iteration 60/1000 | Loss: 0.00002214
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002213
Iteration 64/1000 | Loss: 0.00002213
Iteration 65/1000 | Loss: 0.00002213
Iteration 66/1000 | Loss: 0.00002213
Iteration 67/1000 | Loss: 0.00002213
Iteration 68/1000 | Loss: 0.00002212
Iteration 69/1000 | Loss: 0.00002212
Iteration 70/1000 | Loss: 0.00002208
Iteration 71/1000 | Loss: 0.00002205
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002200
Iteration 74/1000 | Loss: 0.00002199
Iteration 75/1000 | Loss: 0.00002198
Iteration 76/1000 | Loss: 0.00002192
Iteration 77/1000 | Loss: 0.00002182
Iteration 78/1000 | Loss: 0.00002181
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002177
Iteration 81/1000 | Loss: 0.00002177
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002177
Iteration 84/1000 | Loss: 0.00002177
Iteration 85/1000 | Loss: 0.00002177
Iteration 86/1000 | Loss: 0.00002177
Iteration 87/1000 | Loss: 0.00002177
Iteration 88/1000 | Loss: 0.00002177
Iteration 89/1000 | Loss: 0.00002177
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002176
Iteration 92/1000 | Loss: 0.00002176
Iteration 93/1000 | Loss: 0.00002176
Iteration 94/1000 | Loss: 0.00002176
Iteration 95/1000 | Loss: 0.00002175
Iteration 96/1000 | Loss: 0.00002175
Iteration 97/1000 | Loss: 0.00002174
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002173
Iteration 100/1000 | Loss: 0.00002173
Iteration 101/1000 | Loss: 0.00002173
Iteration 102/1000 | Loss: 0.00002173
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002171
Iteration 107/1000 | Loss: 0.00002171
Iteration 108/1000 | Loss: 0.00002171
Iteration 109/1000 | Loss: 0.00002171
Iteration 110/1000 | Loss: 0.00002171
Iteration 111/1000 | Loss: 0.00002171
Iteration 112/1000 | Loss: 0.00002170
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002170
Iteration 116/1000 | Loss: 0.00002170
Iteration 117/1000 | Loss: 0.00002170
Iteration 118/1000 | Loss: 0.00002170
Iteration 119/1000 | Loss: 0.00002169
Iteration 120/1000 | Loss: 0.00002169
Iteration 121/1000 | Loss: 0.00002169
Iteration 122/1000 | Loss: 0.00002169
Iteration 123/1000 | Loss: 0.00002169
Iteration 124/1000 | Loss: 0.00002169
Iteration 125/1000 | Loss: 0.00002168
Iteration 126/1000 | Loss: 0.00002168
Iteration 127/1000 | Loss: 0.00002168
Iteration 128/1000 | Loss: 0.00002168
Iteration 129/1000 | Loss: 0.00002168
Iteration 130/1000 | Loss: 0.00002168
Iteration 131/1000 | Loss: 0.00002168
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002167
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00002167
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002167
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002166
Iteration 144/1000 | Loss: 0.00002166
Iteration 145/1000 | Loss: 0.00002166
Iteration 146/1000 | Loss: 0.00002166
Iteration 147/1000 | Loss: 0.00002166
Iteration 148/1000 | Loss: 0.00002165
Iteration 149/1000 | Loss: 0.00002165
Iteration 150/1000 | Loss: 0.00002165
Iteration 151/1000 | Loss: 0.00002165
Iteration 152/1000 | Loss: 0.00002165
Iteration 153/1000 | Loss: 0.00002165
Iteration 154/1000 | Loss: 0.00002165
Iteration 155/1000 | Loss: 0.00002165
Iteration 156/1000 | Loss: 0.00002165
Iteration 157/1000 | Loss: 0.00002165
Iteration 158/1000 | Loss: 0.00002165
Iteration 159/1000 | Loss: 0.00002165
Iteration 160/1000 | Loss: 0.00002165
Iteration 161/1000 | Loss: 0.00002165
Iteration 162/1000 | Loss: 0.00002165
Iteration 163/1000 | Loss: 0.00002165
Iteration 164/1000 | Loss: 0.00002165
Iteration 165/1000 | Loss: 0.00002165
Iteration 166/1000 | Loss: 0.00002165
Iteration 167/1000 | Loss: 0.00002165
Iteration 168/1000 | Loss: 0.00002165
Iteration 169/1000 | Loss: 0.00002165
Iteration 170/1000 | Loss: 0.00002165
Iteration 171/1000 | Loss: 0.00002165
Iteration 172/1000 | Loss: 0.00002165
Iteration 173/1000 | Loss: 0.00002165
Iteration 174/1000 | Loss: 0.00002165
Iteration 175/1000 | Loss: 0.00002165
Iteration 176/1000 | Loss: 0.00002165
Iteration 177/1000 | Loss: 0.00002165
Iteration 178/1000 | Loss: 0.00002165
Iteration 179/1000 | Loss: 0.00002165
Iteration 180/1000 | Loss: 0.00002165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.1652844225172885e-05, 2.1652844225172885e-05, 2.1652844225172885e-05, 2.1652844225172885e-05, 2.1652844225172885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1652844225172885e-05

Optimization complete. Final v2v error: 3.766716241836548 mm

Highest mean error: 4.6856560707092285 mm for frame 115

Lowest mean error: 3.110814094543457 mm for frame 212

Saving results

Total time: 86.58908557891846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923047
Iteration 2/25 | Loss: 0.00114295
Iteration 3/25 | Loss: 0.00101072
Iteration 4/25 | Loss: 0.00099989
Iteration 5/25 | Loss: 0.00099679
Iteration 6/25 | Loss: 0.00099648
Iteration 7/25 | Loss: 0.00099648
Iteration 8/25 | Loss: 0.00099648
Iteration 9/25 | Loss: 0.00099648
Iteration 10/25 | Loss: 0.00099648
Iteration 11/25 | Loss: 0.00099648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009964794153347611, 0.0009964794153347611, 0.0009964794153347611, 0.0009964794153347611, 0.0009964794153347611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009964794153347611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69301319
Iteration 2/25 | Loss: 0.00119150
Iteration 3/25 | Loss: 0.00119150
Iteration 4/25 | Loss: 0.00119150
Iteration 5/25 | Loss: 0.00119150
Iteration 6/25 | Loss: 0.00119150
Iteration 7/25 | Loss: 0.00119150
Iteration 8/25 | Loss: 0.00119150
Iteration 9/25 | Loss: 0.00119150
Iteration 10/25 | Loss: 0.00119150
Iteration 11/25 | Loss: 0.00119150
Iteration 12/25 | Loss: 0.00119150
Iteration 13/25 | Loss: 0.00119150
Iteration 14/25 | Loss: 0.00119150
Iteration 15/25 | Loss: 0.00119150
Iteration 16/25 | Loss: 0.00119150
Iteration 17/25 | Loss: 0.00119150
Iteration 18/25 | Loss: 0.00119150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011914997594431043, 0.0011914997594431043, 0.0011914997594431043, 0.0011914997594431043, 0.0011914997594431043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011914997594431043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119150
Iteration 2/1000 | Loss: 0.00002361
Iteration 3/1000 | Loss: 0.00001579
Iteration 4/1000 | Loss: 0.00001376
Iteration 5/1000 | Loss: 0.00001280
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001196
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001186
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001182
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001176
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001175
Iteration 18/1000 | Loss: 0.00001175
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001172
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001168
Iteration 32/1000 | Loss: 0.00001165
Iteration 33/1000 | Loss: 0.00001165
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001164
Iteration 40/1000 | Loss: 0.00001164
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001164
Iteration 43/1000 | Loss: 0.00001164
Iteration 44/1000 | Loss: 0.00001164
Iteration 45/1000 | Loss: 0.00001163
Iteration 46/1000 | Loss: 0.00001163
Iteration 47/1000 | Loss: 0.00001163
Iteration 48/1000 | Loss: 0.00001163
Iteration 49/1000 | Loss: 0.00001163
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001163
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001152
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001150
Iteration 112/1000 | Loss: 0.00001150
Iteration 113/1000 | Loss: 0.00001150
Iteration 114/1000 | Loss: 0.00001150
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001149
Iteration 117/1000 | Loss: 0.00001149
Iteration 118/1000 | Loss: 0.00001149
Iteration 119/1000 | Loss: 0.00001149
Iteration 120/1000 | Loss: 0.00001149
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001144
Iteration 136/1000 | Loss: 0.00001144
Iteration 137/1000 | Loss: 0.00001144
Iteration 138/1000 | Loss: 0.00001144
Iteration 139/1000 | Loss: 0.00001144
Iteration 140/1000 | Loss: 0.00001144
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001143
Iteration 147/1000 | Loss: 0.00001143
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001143
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1433288818807341e-05, 1.1433288818807341e-05, 1.1433288818807341e-05, 1.1433288818807341e-05, 1.1433288818807341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1433288818807341e-05

Optimization complete. Final v2v error: 2.849374532699585 mm

Highest mean error: 3.1633129119873047 mm for frame 167

Lowest mean error: 2.6154837608337402 mm for frame 27

Saving results

Total time: 35.878795862197876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906064
Iteration 2/25 | Loss: 0.00150144
Iteration 3/25 | Loss: 0.00118236
Iteration 4/25 | Loss: 0.00113240
Iteration 5/25 | Loss: 0.00112888
Iteration 6/25 | Loss: 0.00111662
Iteration 7/25 | Loss: 0.00110987
Iteration 8/25 | Loss: 0.00110561
Iteration 9/25 | Loss: 0.00110256
Iteration 10/25 | Loss: 0.00109699
Iteration 11/25 | Loss: 0.00109351
Iteration 12/25 | Loss: 0.00109229
Iteration 13/25 | Loss: 0.00109201
Iteration 14/25 | Loss: 0.00109194
Iteration 15/25 | Loss: 0.00109194
Iteration 16/25 | Loss: 0.00109194
Iteration 17/25 | Loss: 0.00109194
Iteration 18/25 | Loss: 0.00109194
Iteration 19/25 | Loss: 0.00109194
Iteration 20/25 | Loss: 0.00109194
Iteration 21/25 | Loss: 0.00109194
Iteration 22/25 | Loss: 0.00109193
Iteration 23/25 | Loss: 0.00109193
Iteration 24/25 | Loss: 0.00109193
Iteration 25/25 | Loss: 0.00109193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83277941
Iteration 2/25 | Loss: 0.00063465
Iteration 3/25 | Loss: 0.00063465
Iteration 4/25 | Loss: 0.00063465
Iteration 5/25 | Loss: 0.00063465
Iteration 6/25 | Loss: 0.00063465
Iteration 7/25 | Loss: 0.00063465
Iteration 8/25 | Loss: 0.00063465
Iteration 9/25 | Loss: 0.00063465
Iteration 10/25 | Loss: 0.00063465
Iteration 11/25 | Loss: 0.00063465
Iteration 12/25 | Loss: 0.00063465
Iteration 13/25 | Loss: 0.00063465
Iteration 14/25 | Loss: 0.00063465
Iteration 15/25 | Loss: 0.00063465
Iteration 16/25 | Loss: 0.00063465
Iteration 17/25 | Loss: 0.00063465
Iteration 18/25 | Loss: 0.00063465
Iteration 19/25 | Loss: 0.00063465
Iteration 20/25 | Loss: 0.00063465
Iteration 21/25 | Loss: 0.00063465
Iteration 22/25 | Loss: 0.00063465
Iteration 23/25 | Loss: 0.00063465
Iteration 24/25 | Loss: 0.00063465
Iteration 25/25 | Loss: 0.00063465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063465
Iteration 2/1000 | Loss: 0.00005328
Iteration 3/1000 | Loss: 0.00004218
Iteration 4/1000 | Loss: 0.00003892
Iteration 5/1000 | Loss: 0.00003681
Iteration 6/1000 | Loss: 0.00003546
Iteration 7/1000 | Loss: 0.00003391
Iteration 8/1000 | Loss: 0.00003335
Iteration 9/1000 | Loss: 0.00003296
Iteration 10/1000 | Loss: 0.00003264
Iteration 11/1000 | Loss: 0.00003237
Iteration 12/1000 | Loss: 0.00003235
Iteration 13/1000 | Loss: 0.00003212
Iteration 14/1000 | Loss: 0.00003208
Iteration 15/1000 | Loss: 0.00003205
Iteration 16/1000 | Loss: 0.00003204
Iteration 17/1000 | Loss: 0.00003204
Iteration 18/1000 | Loss: 0.00003203
Iteration 19/1000 | Loss: 0.00003202
Iteration 20/1000 | Loss: 0.00003193
Iteration 21/1000 | Loss: 0.00003193
Iteration 22/1000 | Loss: 0.00003193
Iteration 23/1000 | Loss: 0.00003193
Iteration 24/1000 | Loss: 0.00003193
Iteration 25/1000 | Loss: 0.00003193
Iteration 26/1000 | Loss: 0.00003193
Iteration 27/1000 | Loss: 0.00003192
Iteration 28/1000 | Loss: 0.00003192
Iteration 29/1000 | Loss: 0.00003192
Iteration 30/1000 | Loss: 0.00003192
Iteration 31/1000 | Loss: 0.00003192
Iteration 32/1000 | Loss: 0.00003192
Iteration 33/1000 | Loss: 0.00003191
Iteration 34/1000 | Loss: 0.00003191
Iteration 35/1000 | Loss: 0.00003191
Iteration 36/1000 | Loss: 0.00003191
Iteration 37/1000 | Loss: 0.00003191
Iteration 38/1000 | Loss: 0.00003191
Iteration 39/1000 | Loss: 0.00003190
Iteration 40/1000 | Loss: 0.00003190
Iteration 41/1000 | Loss: 0.00003190
Iteration 42/1000 | Loss: 0.00003190
Iteration 43/1000 | Loss: 0.00003189
Iteration 44/1000 | Loss: 0.00003189
Iteration 45/1000 | Loss: 0.00003189
Iteration 46/1000 | Loss: 0.00003189
Iteration 47/1000 | Loss: 0.00003189
Iteration 48/1000 | Loss: 0.00003189
Iteration 49/1000 | Loss: 0.00003189
Iteration 50/1000 | Loss: 0.00003188
Iteration 51/1000 | Loss: 0.00003188
Iteration 52/1000 | Loss: 0.00003188
Iteration 53/1000 | Loss: 0.00003187
Iteration 54/1000 | Loss: 0.00003187
Iteration 55/1000 | Loss: 0.00003187
Iteration 56/1000 | Loss: 0.00003187
Iteration 57/1000 | Loss: 0.00003187
Iteration 58/1000 | Loss: 0.00003187
Iteration 59/1000 | Loss: 0.00003187
Iteration 60/1000 | Loss: 0.00003186
Iteration 61/1000 | Loss: 0.00003186
Iteration 62/1000 | Loss: 0.00003186
Iteration 63/1000 | Loss: 0.00003186
Iteration 64/1000 | Loss: 0.00003186
Iteration 65/1000 | Loss: 0.00003186
Iteration 66/1000 | Loss: 0.00003186
Iteration 67/1000 | Loss: 0.00003186
Iteration 68/1000 | Loss: 0.00003186
Iteration 69/1000 | Loss: 0.00003185
Iteration 70/1000 | Loss: 0.00003185
Iteration 71/1000 | Loss: 0.00003185
Iteration 72/1000 | Loss: 0.00003185
Iteration 73/1000 | Loss: 0.00003185
Iteration 74/1000 | Loss: 0.00003185
Iteration 75/1000 | Loss: 0.00003185
Iteration 76/1000 | Loss: 0.00003184
Iteration 77/1000 | Loss: 0.00003184
Iteration 78/1000 | Loss: 0.00003184
Iteration 79/1000 | Loss: 0.00003183
Iteration 80/1000 | Loss: 0.00003183
Iteration 81/1000 | Loss: 0.00003183
Iteration 82/1000 | Loss: 0.00003183
Iteration 83/1000 | Loss: 0.00003183
Iteration 84/1000 | Loss: 0.00003183
Iteration 85/1000 | Loss: 0.00003183
Iteration 86/1000 | Loss: 0.00003183
Iteration 87/1000 | Loss: 0.00003183
Iteration 88/1000 | Loss: 0.00003183
Iteration 89/1000 | Loss: 0.00003183
Iteration 90/1000 | Loss: 0.00003183
Iteration 91/1000 | Loss: 0.00003182
Iteration 92/1000 | Loss: 0.00003182
Iteration 93/1000 | Loss: 0.00003182
Iteration 94/1000 | Loss: 0.00003182
Iteration 95/1000 | Loss: 0.00003182
Iteration 96/1000 | Loss: 0.00003182
Iteration 97/1000 | Loss: 0.00003182
Iteration 98/1000 | Loss: 0.00003182
Iteration 99/1000 | Loss: 0.00003182
Iteration 100/1000 | Loss: 0.00003182
Iteration 101/1000 | Loss: 0.00003182
Iteration 102/1000 | Loss: 0.00003182
Iteration 103/1000 | Loss: 0.00003182
Iteration 104/1000 | Loss: 0.00003182
Iteration 105/1000 | Loss: 0.00003182
Iteration 106/1000 | Loss: 0.00003182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.181850115652196e-05, 3.181850115652196e-05, 3.181850115652196e-05, 3.181850115652196e-05, 3.181850115652196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.181850115652196e-05

Optimization complete. Final v2v error: 4.6517205238342285 mm

Highest mean error: 4.742090225219727 mm for frame 147

Lowest mean error: 4.4887776374816895 mm for frame 125

Saving results

Total time: 47.730283975601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401171
Iteration 2/25 | Loss: 0.00110778
Iteration 3/25 | Loss: 0.00100285
Iteration 4/25 | Loss: 0.00099188
Iteration 5/25 | Loss: 0.00098887
Iteration 6/25 | Loss: 0.00098758
Iteration 7/25 | Loss: 0.00098736
Iteration 8/25 | Loss: 0.00098736
Iteration 9/25 | Loss: 0.00098736
Iteration 10/25 | Loss: 0.00098736
Iteration 11/25 | Loss: 0.00098736
Iteration 12/25 | Loss: 0.00098736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009873552480712533, 0.0009873552480712533, 0.0009873552480712533, 0.0009873552480712533, 0.0009873552480712533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009873552480712533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35440481
Iteration 2/25 | Loss: 0.00110960
Iteration 3/25 | Loss: 0.00110960
Iteration 4/25 | Loss: 0.00110960
Iteration 5/25 | Loss: 0.00110960
Iteration 6/25 | Loss: 0.00110960
Iteration 7/25 | Loss: 0.00110960
Iteration 8/25 | Loss: 0.00110960
Iteration 9/25 | Loss: 0.00110960
Iteration 10/25 | Loss: 0.00110960
Iteration 11/25 | Loss: 0.00110960
Iteration 12/25 | Loss: 0.00110960
Iteration 13/25 | Loss: 0.00110960
Iteration 14/25 | Loss: 0.00110960
Iteration 15/25 | Loss: 0.00110960
Iteration 16/25 | Loss: 0.00110960
Iteration 17/25 | Loss: 0.00110960
Iteration 18/25 | Loss: 0.00110960
Iteration 19/25 | Loss: 0.00110960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011096012312918901, 0.0011096012312918901, 0.0011096012312918901, 0.0011096012312918901, 0.0011096012312918901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011096012312918901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110960
Iteration 2/1000 | Loss: 0.00003304
Iteration 3/1000 | Loss: 0.00001541
Iteration 4/1000 | Loss: 0.00001278
Iteration 5/1000 | Loss: 0.00001171
Iteration 6/1000 | Loss: 0.00001119
Iteration 7/1000 | Loss: 0.00001082
Iteration 8/1000 | Loss: 0.00001072
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001042
Iteration 11/1000 | Loss: 0.00001040
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001028
Iteration 14/1000 | Loss: 0.00001024
Iteration 15/1000 | Loss: 0.00001024
Iteration 16/1000 | Loss: 0.00001024
Iteration 17/1000 | Loss: 0.00001022
Iteration 18/1000 | Loss: 0.00001022
Iteration 19/1000 | Loss: 0.00001022
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001012
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001011
Iteration 30/1000 | Loss: 0.00001010
Iteration 31/1000 | Loss: 0.00001010
Iteration 32/1000 | Loss: 0.00001009
Iteration 33/1000 | Loss: 0.00001009
Iteration 34/1000 | Loss: 0.00001008
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001007
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001006
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001005
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001003
Iteration 49/1000 | Loss: 0.00001003
Iteration 50/1000 | Loss: 0.00001003
Iteration 51/1000 | Loss: 0.00001002
Iteration 52/1000 | Loss: 0.00001002
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001001
Iteration 55/1000 | Loss: 0.00001001
Iteration 56/1000 | Loss: 0.00001001
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00001000
Iteration 59/1000 | Loss: 0.00001000
Iteration 60/1000 | Loss: 0.00001000
Iteration 61/1000 | Loss: 0.00000998
Iteration 62/1000 | Loss: 0.00000998
Iteration 63/1000 | Loss: 0.00000998
Iteration 64/1000 | Loss: 0.00000998
Iteration 65/1000 | Loss: 0.00000998
Iteration 66/1000 | Loss: 0.00000998
Iteration 67/1000 | Loss: 0.00000998
Iteration 68/1000 | Loss: 0.00000998
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000997
Iteration 71/1000 | Loss: 0.00000997
Iteration 72/1000 | Loss: 0.00000997
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000996
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000995
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000994
Iteration 85/1000 | Loss: 0.00000994
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000993
Iteration 90/1000 | Loss: 0.00000993
Iteration 91/1000 | Loss: 0.00000993
Iteration 92/1000 | Loss: 0.00000993
Iteration 93/1000 | Loss: 0.00000993
Iteration 94/1000 | Loss: 0.00000993
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000991
Iteration 99/1000 | Loss: 0.00000991
Iteration 100/1000 | Loss: 0.00000991
Iteration 101/1000 | Loss: 0.00000991
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Iteration 104/1000 | Loss: 0.00000991
Iteration 105/1000 | Loss: 0.00000991
Iteration 106/1000 | Loss: 0.00000991
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000991
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000990
Iteration 116/1000 | Loss: 0.00000990
Iteration 117/1000 | Loss: 0.00000990
Iteration 118/1000 | Loss: 0.00000990
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000990
Iteration 123/1000 | Loss: 0.00000990
Iteration 124/1000 | Loss: 0.00000990
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00000990
Iteration 129/1000 | Loss: 0.00000990
Iteration 130/1000 | Loss: 0.00000990
Iteration 131/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [9.898803909891285e-06, 9.898803909891285e-06, 9.898803909891285e-06, 9.898803909891285e-06, 9.898803909891285e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.898803909891285e-06

Optimization complete. Final v2v error: 2.657543897628784 mm

Highest mean error: 3.6535496711730957 mm for frame 45

Lowest mean error: 2.245049238204956 mm for frame 22

Saving results

Total time: 32.38117432594299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790384
Iteration 2/25 | Loss: 0.00134119
Iteration 3/25 | Loss: 0.00111401
Iteration 4/25 | Loss: 0.00106487
Iteration 5/25 | Loss: 0.00105553
Iteration 6/25 | Loss: 0.00105876
Iteration 7/25 | Loss: 0.00105064
Iteration 8/25 | Loss: 0.00104814
Iteration 9/25 | Loss: 0.00104789
Iteration 10/25 | Loss: 0.00104748
Iteration 11/25 | Loss: 0.00104705
Iteration 12/25 | Loss: 0.00104695
Iteration 13/25 | Loss: 0.00104687
Iteration 14/25 | Loss: 0.00104687
Iteration 15/25 | Loss: 0.00104687
Iteration 16/25 | Loss: 0.00104687
Iteration 17/25 | Loss: 0.00104686
Iteration 18/25 | Loss: 0.00104686
Iteration 19/25 | Loss: 0.00104686
Iteration 20/25 | Loss: 0.00104686
Iteration 21/25 | Loss: 0.00104686
Iteration 22/25 | Loss: 0.00104686
Iteration 23/25 | Loss: 0.00104686
Iteration 24/25 | Loss: 0.00104686
Iteration 25/25 | Loss: 0.00104686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02786160
Iteration 2/25 | Loss: 0.00128231
Iteration 3/25 | Loss: 0.00128231
Iteration 4/25 | Loss: 0.00128231
Iteration 5/25 | Loss: 0.00128231
Iteration 6/25 | Loss: 0.00128231
Iteration 7/25 | Loss: 0.00128231
Iteration 8/25 | Loss: 0.00128231
Iteration 9/25 | Loss: 0.00128231
Iteration 10/25 | Loss: 0.00128231
Iteration 11/25 | Loss: 0.00128231
Iteration 12/25 | Loss: 0.00128231
Iteration 13/25 | Loss: 0.00128231
Iteration 14/25 | Loss: 0.00128231
Iteration 15/25 | Loss: 0.00128231
Iteration 16/25 | Loss: 0.00128231
Iteration 17/25 | Loss: 0.00128231
Iteration 18/25 | Loss: 0.00128231
Iteration 19/25 | Loss: 0.00128231
Iteration 20/25 | Loss: 0.00128231
Iteration 21/25 | Loss: 0.00128231
Iteration 22/25 | Loss: 0.00128231
Iteration 23/25 | Loss: 0.00128231
Iteration 24/25 | Loss: 0.00128231
Iteration 25/25 | Loss: 0.00128231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128231
Iteration 2/1000 | Loss: 0.00003102
Iteration 3/1000 | Loss: 0.00001961
Iteration 4/1000 | Loss: 0.00001777
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001575
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001569
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001564
Iteration 29/1000 | Loss: 0.00001564
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001562
Iteration 42/1000 | Loss: 0.00001562
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001556
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001554
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001551
Iteration 64/1000 | Loss: 0.00001551
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001551
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001550
Iteration 73/1000 | Loss: 0.00001550
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.5489935321966186e-05, 1.5489935321966186e-05, 1.5489935321966186e-05, 1.5489935321966186e-05, 1.5489935321966186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5489935321966186e-05

Optimization complete. Final v2v error: 3.360056161880493 mm

Highest mean error: 8.708316802978516 mm for frame 18

Lowest mean error: 3.165506601333618 mm for frame 60

Saving results

Total time: 44.37992882728577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889393
Iteration 2/25 | Loss: 0.00101965
Iteration 3/25 | Loss: 0.00093362
Iteration 4/25 | Loss: 0.00092484
Iteration 5/25 | Loss: 0.00092201
Iteration 6/25 | Loss: 0.00092125
Iteration 7/25 | Loss: 0.00092125
Iteration 8/25 | Loss: 0.00092125
Iteration 9/25 | Loss: 0.00092125
Iteration 10/25 | Loss: 0.00092125
Iteration 11/25 | Loss: 0.00092125
Iteration 12/25 | Loss: 0.00092125
Iteration 13/25 | Loss: 0.00092125
Iteration 14/25 | Loss: 0.00092125
Iteration 15/25 | Loss: 0.00092125
Iteration 16/25 | Loss: 0.00092125
Iteration 17/25 | Loss: 0.00092125
Iteration 18/25 | Loss: 0.00092125
Iteration 19/25 | Loss: 0.00092125
Iteration 20/25 | Loss: 0.00092125
Iteration 21/25 | Loss: 0.00092125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009212520672008395, 0.0009212520672008395, 0.0009212520672008395, 0.0009212520672008395, 0.0009212520672008395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009212520672008395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58006036
Iteration 2/25 | Loss: 0.00105892
Iteration 3/25 | Loss: 0.00105892
Iteration 4/25 | Loss: 0.00105892
Iteration 5/25 | Loss: 0.00105892
Iteration 6/25 | Loss: 0.00105892
Iteration 7/25 | Loss: 0.00105891
Iteration 8/25 | Loss: 0.00105891
Iteration 9/25 | Loss: 0.00105891
Iteration 10/25 | Loss: 0.00105891
Iteration 11/25 | Loss: 0.00105891
Iteration 12/25 | Loss: 0.00105891
Iteration 13/25 | Loss: 0.00105891
Iteration 14/25 | Loss: 0.00105891
Iteration 15/25 | Loss: 0.00105891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010589141165837646, 0.0010589141165837646, 0.0010589141165837646, 0.0010589141165837646, 0.0010589141165837646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010589141165837646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105891
Iteration 2/1000 | Loss: 0.00002299
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001061
Iteration 6/1000 | Loss: 0.00000971
Iteration 7/1000 | Loss: 0.00000936
Iteration 8/1000 | Loss: 0.00000923
Iteration 9/1000 | Loss: 0.00000922
Iteration 10/1000 | Loss: 0.00000919
Iteration 11/1000 | Loss: 0.00000914
Iteration 12/1000 | Loss: 0.00000913
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000910
Iteration 15/1000 | Loss: 0.00000908
Iteration 16/1000 | Loss: 0.00000907
Iteration 17/1000 | Loss: 0.00000907
Iteration 18/1000 | Loss: 0.00000906
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000905
Iteration 21/1000 | Loss: 0.00000905
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000904
Iteration 24/1000 | Loss: 0.00000904
Iteration 25/1000 | Loss: 0.00000904
Iteration 26/1000 | Loss: 0.00000904
Iteration 27/1000 | Loss: 0.00000904
Iteration 28/1000 | Loss: 0.00000904
Iteration 29/1000 | Loss: 0.00000904
Iteration 30/1000 | Loss: 0.00000904
Iteration 31/1000 | Loss: 0.00000904
Iteration 32/1000 | Loss: 0.00000903
Iteration 33/1000 | Loss: 0.00000902
Iteration 34/1000 | Loss: 0.00000901
Iteration 35/1000 | Loss: 0.00000901
Iteration 36/1000 | Loss: 0.00000901
Iteration 37/1000 | Loss: 0.00000901
Iteration 38/1000 | Loss: 0.00000900
Iteration 39/1000 | Loss: 0.00000900
Iteration 40/1000 | Loss: 0.00000900
Iteration 41/1000 | Loss: 0.00000900
Iteration 42/1000 | Loss: 0.00000900
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000900
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000899
Iteration 47/1000 | Loss: 0.00000899
Iteration 48/1000 | Loss: 0.00000899
Iteration 49/1000 | Loss: 0.00000899
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000898
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000896
Iteration 56/1000 | Loss: 0.00000896
Iteration 57/1000 | Loss: 0.00000896
Iteration 58/1000 | Loss: 0.00000896
Iteration 59/1000 | Loss: 0.00000896
Iteration 60/1000 | Loss: 0.00000896
Iteration 61/1000 | Loss: 0.00000896
Iteration 62/1000 | Loss: 0.00000896
Iteration 63/1000 | Loss: 0.00000896
Iteration 64/1000 | Loss: 0.00000895
Iteration 65/1000 | Loss: 0.00000895
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000895
Iteration 70/1000 | Loss: 0.00000894
Iteration 71/1000 | Loss: 0.00000893
Iteration 72/1000 | Loss: 0.00000892
Iteration 73/1000 | Loss: 0.00000892
Iteration 74/1000 | Loss: 0.00000891
Iteration 75/1000 | Loss: 0.00000891
Iteration 76/1000 | Loss: 0.00000891
Iteration 77/1000 | Loss: 0.00000891
Iteration 78/1000 | Loss: 0.00000891
Iteration 79/1000 | Loss: 0.00000891
Iteration 80/1000 | Loss: 0.00000891
Iteration 81/1000 | Loss: 0.00000891
Iteration 82/1000 | Loss: 0.00000891
Iteration 83/1000 | Loss: 0.00000891
Iteration 84/1000 | Loss: 0.00000891
Iteration 85/1000 | Loss: 0.00000891
Iteration 86/1000 | Loss: 0.00000891
Iteration 87/1000 | Loss: 0.00000891
Iteration 88/1000 | Loss: 0.00000891
Iteration 89/1000 | Loss: 0.00000891
Iteration 90/1000 | Loss: 0.00000891
Iteration 91/1000 | Loss: 0.00000891
Iteration 92/1000 | Loss: 0.00000891
Iteration 93/1000 | Loss: 0.00000891
Iteration 94/1000 | Loss: 0.00000891
Iteration 95/1000 | Loss: 0.00000891
Iteration 96/1000 | Loss: 0.00000891
Iteration 97/1000 | Loss: 0.00000891
Iteration 98/1000 | Loss: 0.00000891
Iteration 99/1000 | Loss: 0.00000891
Iteration 100/1000 | Loss: 0.00000891
Iteration 101/1000 | Loss: 0.00000891
Iteration 102/1000 | Loss: 0.00000891
Iteration 103/1000 | Loss: 0.00000891
Iteration 104/1000 | Loss: 0.00000891
Iteration 105/1000 | Loss: 0.00000891
Iteration 106/1000 | Loss: 0.00000891
Iteration 107/1000 | Loss: 0.00000891
Iteration 108/1000 | Loss: 0.00000891
Iteration 109/1000 | Loss: 0.00000891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [8.90531373443082e-06, 8.90531373443082e-06, 8.90531373443082e-06, 8.90531373443082e-06, 8.90531373443082e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.90531373443082e-06

Optimization complete. Final v2v error: 2.4968128204345703 mm

Highest mean error: 2.881760835647583 mm for frame 46

Lowest mean error: 2.177912473678589 mm for frame 134

Saving results

Total time: 26.170896530151367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547516
Iteration 2/25 | Loss: 0.00114365
Iteration 3/25 | Loss: 0.00103239
Iteration 4/25 | Loss: 0.00102156
Iteration 5/25 | Loss: 0.00101892
Iteration 6/25 | Loss: 0.00101793
Iteration 7/25 | Loss: 0.00101793
Iteration 8/25 | Loss: 0.00101793
Iteration 9/25 | Loss: 0.00101793
Iteration 10/25 | Loss: 0.00101793
Iteration 11/25 | Loss: 0.00101793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010179320815950632, 0.0010179320815950632, 0.0010179320815950632, 0.0010179320815950632, 0.0010179320815950632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010179320815950632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96710467
Iteration 2/25 | Loss: 0.00120261
Iteration 3/25 | Loss: 0.00120261
Iteration 4/25 | Loss: 0.00120261
Iteration 5/25 | Loss: 0.00120261
Iteration 6/25 | Loss: 0.00120261
Iteration 7/25 | Loss: 0.00120261
Iteration 8/25 | Loss: 0.00120261
Iteration 9/25 | Loss: 0.00120261
Iteration 10/25 | Loss: 0.00120261
Iteration 11/25 | Loss: 0.00120261
Iteration 12/25 | Loss: 0.00120261
Iteration 13/25 | Loss: 0.00120261
Iteration 14/25 | Loss: 0.00120261
Iteration 15/25 | Loss: 0.00120261
Iteration 16/25 | Loss: 0.00120261
Iteration 17/25 | Loss: 0.00120261
Iteration 18/25 | Loss: 0.00120261
Iteration 19/25 | Loss: 0.00120261
Iteration 20/25 | Loss: 0.00120261
Iteration 21/25 | Loss: 0.00120261
Iteration 22/25 | Loss: 0.00120261
Iteration 23/25 | Loss: 0.00120261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012026064796373248, 0.0012026064796373248, 0.0012026064796373248, 0.0012026064796373248, 0.0012026064796373248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012026064796373248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120261
Iteration 2/1000 | Loss: 0.00002593
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001333
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001256
Iteration 46/1000 | Loss: 0.00001256
Iteration 47/1000 | Loss: 0.00001256
Iteration 48/1000 | Loss: 0.00001256
Iteration 49/1000 | Loss: 0.00001256
Iteration 50/1000 | Loss: 0.00001256
Iteration 51/1000 | Loss: 0.00001256
Iteration 52/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.2559095921460539e-05, 1.2559095921460539e-05, 1.2559095921460539e-05, 1.2559095921460539e-05, 1.2559095921460539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2559095921460539e-05

Optimization complete. Final v2v error: 2.9052090644836426 mm

Highest mean error: 3.413827419281006 mm for frame 63

Lowest mean error: 2.5303122997283936 mm for frame 11

Saving results

Total time: 23.209285020828247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890701
Iteration 2/25 | Loss: 0.00120554
Iteration 3/25 | Loss: 0.00098176
Iteration 4/25 | Loss: 0.00096690
Iteration 5/25 | Loss: 0.00096407
Iteration 6/25 | Loss: 0.00096380
Iteration 7/25 | Loss: 0.00096380
Iteration 8/25 | Loss: 0.00096380
Iteration 9/25 | Loss: 0.00096380
Iteration 10/25 | Loss: 0.00096380
Iteration 11/25 | Loss: 0.00096380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009637998882681131, 0.0009637998882681131, 0.0009637998882681131, 0.0009637998882681131, 0.0009637998882681131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009637998882681131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30779600
Iteration 2/25 | Loss: 0.00095696
Iteration 3/25 | Loss: 0.00095695
Iteration 4/25 | Loss: 0.00095695
Iteration 5/25 | Loss: 0.00095695
Iteration 6/25 | Loss: 0.00095695
Iteration 7/25 | Loss: 0.00095695
Iteration 8/25 | Loss: 0.00095695
Iteration 9/25 | Loss: 0.00095695
Iteration 10/25 | Loss: 0.00095695
Iteration 11/25 | Loss: 0.00095695
Iteration 12/25 | Loss: 0.00095695
Iteration 13/25 | Loss: 0.00095695
Iteration 14/25 | Loss: 0.00095695
Iteration 15/25 | Loss: 0.00095695
Iteration 16/25 | Loss: 0.00095695
Iteration 17/25 | Loss: 0.00095695
Iteration 18/25 | Loss: 0.00095695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009569507674314082, 0.0009569507674314082, 0.0009569507674314082, 0.0009569507674314082, 0.0009569507674314082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009569507674314082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095695
Iteration 2/1000 | Loss: 0.00002765
Iteration 3/1000 | Loss: 0.00002197
Iteration 4/1000 | Loss: 0.00002005
Iteration 5/1000 | Loss: 0.00001881
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001689
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001668
Iteration 23/1000 | Loss: 0.00001663
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001659
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001657
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001654
Iteration 39/1000 | Loss: 0.00001653
Iteration 40/1000 | Loss: 0.00001653
Iteration 41/1000 | Loss: 0.00001652
Iteration 42/1000 | Loss: 0.00001652
Iteration 43/1000 | Loss: 0.00001652
Iteration 44/1000 | Loss: 0.00001652
Iteration 45/1000 | Loss: 0.00001652
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001651
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001650
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001645
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001644
Iteration 73/1000 | Loss: 0.00001644
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001643
Iteration 76/1000 | Loss: 0.00001643
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001642
Iteration 80/1000 | Loss: 0.00001642
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001641
Iteration 84/1000 | Loss: 0.00001641
Iteration 85/1000 | Loss: 0.00001641
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001641
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001635
Iteration 136/1000 | Loss: 0.00001635
Iteration 137/1000 | Loss: 0.00001634
Iteration 138/1000 | Loss: 0.00001634
Iteration 139/1000 | Loss: 0.00001634
Iteration 140/1000 | Loss: 0.00001634
Iteration 141/1000 | Loss: 0.00001634
Iteration 142/1000 | Loss: 0.00001634
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001633
Iteration 145/1000 | Loss: 0.00001633
Iteration 146/1000 | Loss: 0.00001632
Iteration 147/1000 | Loss: 0.00001632
Iteration 148/1000 | Loss: 0.00001632
Iteration 149/1000 | Loss: 0.00001632
Iteration 150/1000 | Loss: 0.00001631
Iteration 151/1000 | Loss: 0.00001631
Iteration 152/1000 | Loss: 0.00001630
Iteration 153/1000 | Loss: 0.00001630
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001627
Iteration 163/1000 | Loss: 0.00001627
Iteration 164/1000 | Loss: 0.00001627
Iteration 165/1000 | Loss: 0.00001626
Iteration 166/1000 | Loss: 0.00001626
Iteration 167/1000 | Loss: 0.00001626
Iteration 168/1000 | Loss: 0.00001626
Iteration 169/1000 | Loss: 0.00001626
Iteration 170/1000 | Loss: 0.00001626
Iteration 171/1000 | Loss: 0.00001626
Iteration 172/1000 | Loss: 0.00001626
Iteration 173/1000 | Loss: 0.00001626
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001626
Iteration 176/1000 | Loss: 0.00001626
Iteration 177/1000 | Loss: 0.00001626
Iteration 178/1000 | Loss: 0.00001626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.626458833925426e-05, 1.626458833925426e-05, 1.626458833925426e-05, 1.626458833925426e-05, 1.626458833925426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.626458833925426e-05

Optimization complete. Final v2v error: 3.23425030708313 mm

Highest mean error: 4.159013271331787 mm for frame 29

Lowest mean error: 2.6778182983398438 mm for frame 117

Saving results

Total time: 41.826425313949585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021352
Iteration 2/25 | Loss: 0.00239345
Iteration 3/25 | Loss: 0.00138715
Iteration 4/25 | Loss: 0.00123737
Iteration 5/25 | Loss: 0.00127542
Iteration 6/25 | Loss: 0.00116396
Iteration 7/25 | Loss: 0.00107730
Iteration 8/25 | Loss: 0.00104278
Iteration 9/25 | Loss: 0.00103482
Iteration 10/25 | Loss: 0.00103453
Iteration 11/25 | Loss: 0.00102370
Iteration 12/25 | Loss: 0.00101933
Iteration 13/25 | Loss: 0.00100993
Iteration 14/25 | Loss: 0.00100713
Iteration 15/25 | Loss: 0.00100641
Iteration 16/25 | Loss: 0.00100619
Iteration 17/25 | Loss: 0.00100607
Iteration 18/25 | Loss: 0.00100598
Iteration 19/25 | Loss: 0.00100957
Iteration 20/25 | Loss: 0.00100580
Iteration 21/25 | Loss: 0.00100506
Iteration 22/25 | Loss: 0.00100487
Iteration 23/25 | Loss: 0.00100484
Iteration 24/25 | Loss: 0.00100484
Iteration 25/25 | Loss: 0.00100483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26408970
Iteration 2/25 | Loss: 0.00133022
Iteration 3/25 | Loss: 0.00110243
Iteration 4/25 | Loss: 0.00110243
Iteration 5/25 | Loss: 0.00110243
Iteration 6/25 | Loss: 0.00110243
Iteration 7/25 | Loss: 0.00110243
Iteration 8/25 | Loss: 0.00110243
Iteration 9/25 | Loss: 0.00110243
Iteration 10/25 | Loss: 0.00110243
Iteration 11/25 | Loss: 0.00110243
Iteration 12/25 | Loss: 0.00110243
Iteration 13/25 | Loss: 0.00110243
Iteration 14/25 | Loss: 0.00110243
Iteration 15/25 | Loss: 0.00110243
Iteration 16/25 | Loss: 0.00110243
Iteration 17/25 | Loss: 0.00110243
Iteration 18/25 | Loss: 0.00110243
Iteration 19/25 | Loss: 0.00110243
Iteration 20/25 | Loss: 0.00110243
Iteration 21/25 | Loss: 0.00110243
Iteration 22/25 | Loss: 0.00110243
Iteration 23/25 | Loss: 0.00110243
Iteration 24/25 | Loss: 0.00110243
Iteration 25/25 | Loss: 0.00110243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110243
Iteration 2/1000 | Loss: 0.00027263
Iteration 3/1000 | Loss: 0.00004685
Iteration 4/1000 | Loss: 0.00003832
Iteration 5/1000 | Loss: 0.00003487
Iteration 6/1000 | Loss: 0.00003250
Iteration 7/1000 | Loss: 0.00009030
Iteration 8/1000 | Loss: 0.00003117
Iteration 9/1000 | Loss: 0.00002965
Iteration 10/1000 | Loss: 0.00002834
Iteration 11/1000 | Loss: 0.00002743
Iteration 12/1000 | Loss: 0.00002674
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002582
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00198917
Iteration 17/1000 | Loss: 0.00022390
Iteration 18/1000 | Loss: 0.00002735
Iteration 19/1000 | Loss: 0.00002587
Iteration 20/1000 | Loss: 0.00157912
Iteration 21/1000 | Loss: 0.00022373
Iteration 22/1000 | Loss: 0.00082638
Iteration 23/1000 | Loss: 0.00003346
Iteration 24/1000 | Loss: 0.00002804
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002306
Iteration 27/1000 | Loss: 0.00002183
Iteration 28/1000 | Loss: 0.00048566
Iteration 29/1000 | Loss: 0.00012634
Iteration 30/1000 | Loss: 0.00028721
Iteration 31/1000 | Loss: 0.00009804
Iteration 32/1000 | Loss: 0.00005261
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002059
Iteration 35/1000 | Loss: 0.00036918
Iteration 36/1000 | Loss: 0.00023110
Iteration 37/1000 | Loss: 0.00001987
Iteration 38/1000 | Loss: 0.00031997
Iteration 39/1000 | Loss: 0.00029725
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00075604
Iteration 42/1000 | Loss: 0.00032766
Iteration 43/1000 | Loss: 0.00001931
Iteration 44/1000 | Loss: 0.00071698
Iteration 45/1000 | Loss: 0.00010205
Iteration 46/1000 | Loss: 0.00002906
Iteration 47/1000 | Loss: 0.00002415
Iteration 48/1000 | Loss: 0.00035070
Iteration 49/1000 | Loss: 0.00009981
Iteration 50/1000 | Loss: 0.00028800
Iteration 51/1000 | Loss: 0.00021266
Iteration 52/1000 | Loss: 0.00010828
Iteration 53/1000 | Loss: 0.00012991
Iteration 54/1000 | Loss: 0.00002324
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00026206
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001509
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00041860
Iteration 66/1000 | Loss: 0.00008570
Iteration 67/1000 | Loss: 0.00002154
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00021517
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001446
Iteration 73/1000 | Loss: 0.00001322
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001208
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001142
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001141
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001140
Iteration 129/1000 | Loss: 0.00001140
Iteration 130/1000 | Loss: 0.00001140
Iteration 131/1000 | Loss: 0.00001140
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001139
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Iteration 164/1000 | Loss: 0.00001137
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001136
Iteration 170/1000 | Loss: 0.00001136
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001136
Iteration 174/1000 | Loss: 0.00001136
Iteration 175/1000 | Loss: 0.00001136
Iteration 176/1000 | Loss: 0.00001136
Iteration 177/1000 | Loss: 0.00001136
Iteration 178/1000 | Loss: 0.00001136
Iteration 179/1000 | Loss: 0.00001136
Iteration 180/1000 | Loss: 0.00001136
Iteration 181/1000 | Loss: 0.00001136
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001135
Iteration 188/1000 | Loss: 0.00001135
Iteration 189/1000 | Loss: 0.00001135
Iteration 190/1000 | Loss: 0.00001135
Iteration 191/1000 | Loss: 0.00001135
Iteration 192/1000 | Loss: 0.00001135
Iteration 193/1000 | Loss: 0.00001135
Iteration 194/1000 | Loss: 0.00001134
Iteration 195/1000 | Loss: 0.00001134
Iteration 196/1000 | Loss: 0.00001134
Iteration 197/1000 | Loss: 0.00001134
Iteration 198/1000 | Loss: 0.00001134
Iteration 199/1000 | Loss: 0.00001134
Iteration 200/1000 | Loss: 0.00001134
Iteration 201/1000 | Loss: 0.00001134
Iteration 202/1000 | Loss: 0.00001134
Iteration 203/1000 | Loss: 0.00001134
Iteration 204/1000 | Loss: 0.00001134
Iteration 205/1000 | Loss: 0.00001134
Iteration 206/1000 | Loss: 0.00001134
Iteration 207/1000 | Loss: 0.00001134
Iteration 208/1000 | Loss: 0.00001134
Iteration 209/1000 | Loss: 0.00001134
Iteration 210/1000 | Loss: 0.00001134
Iteration 211/1000 | Loss: 0.00001133
Iteration 212/1000 | Loss: 0.00001133
Iteration 213/1000 | Loss: 0.00001133
Iteration 214/1000 | Loss: 0.00001133
Iteration 215/1000 | Loss: 0.00001133
Iteration 216/1000 | Loss: 0.00001133
Iteration 217/1000 | Loss: 0.00001133
Iteration 218/1000 | Loss: 0.00001133
Iteration 219/1000 | Loss: 0.00001133
Iteration 220/1000 | Loss: 0.00001133
Iteration 221/1000 | Loss: 0.00001133
Iteration 222/1000 | Loss: 0.00001133
Iteration 223/1000 | Loss: 0.00001133
Iteration 224/1000 | Loss: 0.00001133
Iteration 225/1000 | Loss: 0.00001133
Iteration 226/1000 | Loss: 0.00001133
Iteration 227/1000 | Loss: 0.00001133
Iteration 228/1000 | Loss: 0.00001133
Iteration 229/1000 | Loss: 0.00001133
Iteration 230/1000 | Loss: 0.00001133
Iteration 231/1000 | Loss: 0.00001133
Iteration 232/1000 | Loss: 0.00001133
Iteration 233/1000 | Loss: 0.00001133
Iteration 234/1000 | Loss: 0.00001133
Iteration 235/1000 | Loss: 0.00001133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.1327515494485851e-05, 1.1327515494485851e-05, 1.1327515494485851e-05, 1.1327515494485851e-05, 1.1327515494485851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1327515494485851e-05

Optimization complete. Final v2v error: 2.8178365230560303 mm

Highest mean error: 4.411578178405762 mm for frame 27

Lowest mean error: 2.3336312770843506 mm for frame 102

Saving results

Total time: 186.39912867546082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00734467
Iteration 2/25 | Loss: 0.00111043
Iteration 3/25 | Loss: 0.00098434
Iteration 4/25 | Loss: 0.00095777
Iteration 5/25 | Loss: 0.00094732
Iteration 6/25 | Loss: 0.00094491
Iteration 7/25 | Loss: 0.00094331
Iteration 8/25 | Loss: 0.00094216
Iteration 9/25 | Loss: 0.00094143
Iteration 10/25 | Loss: 0.00094100
Iteration 11/25 | Loss: 0.00094061
Iteration 12/25 | Loss: 0.00094049
Iteration 13/25 | Loss: 0.00094048
Iteration 14/25 | Loss: 0.00094048
Iteration 15/25 | Loss: 0.00094047
Iteration 16/25 | Loss: 0.00094047
Iteration 17/25 | Loss: 0.00094047
Iteration 18/25 | Loss: 0.00094047
Iteration 19/25 | Loss: 0.00094047
Iteration 20/25 | Loss: 0.00094047
Iteration 21/25 | Loss: 0.00094047
Iteration 22/25 | Loss: 0.00094047
Iteration 23/25 | Loss: 0.00094047
Iteration 24/25 | Loss: 0.00094046
Iteration 25/25 | Loss: 0.00094046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90044808
Iteration 2/25 | Loss: 0.00121899
Iteration 3/25 | Loss: 0.00121899
Iteration 4/25 | Loss: 0.00121899
Iteration 5/25 | Loss: 0.00121899
Iteration 6/25 | Loss: 0.00121899
Iteration 7/25 | Loss: 0.00121899
Iteration 8/25 | Loss: 0.00121899
Iteration 9/25 | Loss: 0.00121899
Iteration 10/25 | Loss: 0.00121899
Iteration 11/25 | Loss: 0.00121899
Iteration 12/25 | Loss: 0.00121899
Iteration 13/25 | Loss: 0.00121899
Iteration 14/25 | Loss: 0.00121899
Iteration 15/25 | Loss: 0.00121899
Iteration 16/25 | Loss: 0.00121899
Iteration 17/25 | Loss: 0.00121899
Iteration 18/25 | Loss: 0.00121899
Iteration 19/25 | Loss: 0.00121899
Iteration 20/25 | Loss: 0.00121899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001218985766172409, 0.001218985766172409, 0.001218985766172409, 0.001218985766172409, 0.001218985766172409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001218985766172409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121899
Iteration 2/1000 | Loss: 0.00001905
Iteration 3/1000 | Loss: 0.00001318
Iteration 4/1000 | Loss: 0.00001148
Iteration 5/1000 | Loss: 0.00001063
Iteration 6/1000 | Loss: 0.00001023
Iteration 7/1000 | Loss: 0.00000991
Iteration 8/1000 | Loss: 0.00000983
Iteration 9/1000 | Loss: 0.00000972
Iteration 10/1000 | Loss: 0.00000970
Iteration 11/1000 | Loss: 0.00000969
Iteration 12/1000 | Loss: 0.00000968
Iteration 13/1000 | Loss: 0.00000968
Iteration 14/1000 | Loss: 0.00000967
Iteration 15/1000 | Loss: 0.00000967
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000963
Iteration 18/1000 | Loss: 0.00000963
Iteration 19/1000 | Loss: 0.00000962
Iteration 20/1000 | Loss: 0.00000962
Iteration 21/1000 | Loss: 0.00000962
Iteration 22/1000 | Loss: 0.00000959
Iteration 23/1000 | Loss: 0.00000959
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000958
Iteration 26/1000 | Loss: 0.00000958
Iteration 27/1000 | Loss: 0.00000958
Iteration 28/1000 | Loss: 0.00000957
Iteration 29/1000 | Loss: 0.00000957
Iteration 30/1000 | Loss: 0.00000957
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000956
Iteration 33/1000 | Loss: 0.00000956
Iteration 34/1000 | Loss: 0.00000955
Iteration 35/1000 | Loss: 0.00000955
Iteration 36/1000 | Loss: 0.00000954
Iteration 37/1000 | Loss: 0.00000954
Iteration 38/1000 | Loss: 0.00000954
Iteration 39/1000 | Loss: 0.00000954
Iteration 40/1000 | Loss: 0.00000953
Iteration 41/1000 | Loss: 0.00000953
Iteration 42/1000 | Loss: 0.00000953
Iteration 43/1000 | Loss: 0.00000952
Iteration 44/1000 | Loss: 0.00000952
Iteration 45/1000 | Loss: 0.00000951
Iteration 46/1000 | Loss: 0.00000951
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000949
Iteration 49/1000 | Loss: 0.00000949
Iteration 50/1000 | Loss: 0.00000949
Iteration 51/1000 | Loss: 0.00000949
Iteration 52/1000 | Loss: 0.00000949
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000949
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [9.48804517975077e-06, 9.48804517975077e-06, 9.48804517975077e-06, 9.48804517975077e-06, 9.48804517975077e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.48804517975077e-06

Optimization complete. Final v2v error: 2.6173858642578125 mm

Highest mean error: 2.878336191177368 mm for frame 142

Lowest mean error: 2.333604335784912 mm for frame 24

Saving results

Total time: 42.7231388092041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017550
Iteration 2/25 | Loss: 0.01017550
Iteration 3/25 | Loss: 0.01017549
Iteration 4/25 | Loss: 0.01017549
Iteration 5/25 | Loss: 0.01017549
Iteration 6/25 | Loss: 0.01017549
Iteration 7/25 | Loss: 0.01017549
Iteration 8/25 | Loss: 0.01017548
Iteration 9/25 | Loss: 0.01017548
Iteration 10/25 | Loss: 0.01017548
Iteration 11/25 | Loss: 0.01017548
Iteration 12/25 | Loss: 0.01017547
Iteration 13/25 | Loss: 0.01017547
Iteration 14/25 | Loss: 0.01017547
Iteration 15/25 | Loss: 0.01017547
Iteration 16/25 | Loss: 0.01017547
Iteration 17/25 | Loss: 0.01017547
Iteration 18/25 | Loss: 0.01017546
Iteration 19/25 | Loss: 0.01017546
Iteration 20/25 | Loss: 0.01017546
Iteration 21/25 | Loss: 0.01017546
Iteration 22/25 | Loss: 0.01017545
Iteration 23/25 | Loss: 0.01017545
Iteration 24/25 | Loss: 0.01017545
Iteration 25/25 | Loss: 0.01017545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44056177
Iteration 2/25 | Loss: 0.16611893
Iteration 3/25 | Loss: 0.16515167
Iteration 4/25 | Loss: 0.16233701
Iteration 5/25 | Loss: 0.16206065
Iteration 6/25 | Loss: 0.16224764
Iteration 7/25 | Loss: 0.16198801
Iteration 8/25 | Loss: 0.16198801
Iteration 9/25 | Loss: 0.16198799
Iteration 10/25 | Loss: 0.16198799
Iteration 11/25 | Loss: 0.16198798
Iteration 12/25 | Loss: 0.16198798
Iteration 13/25 | Loss: 0.16198798
Iteration 14/25 | Loss: 0.16198798
Iteration 15/25 | Loss: 0.16198798
Iteration 16/25 | Loss: 0.16198798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.16198797523975372, 0.16198797523975372, 0.16198797523975372, 0.16198797523975372, 0.16198797523975372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16198797523975372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16198798
Iteration 2/1000 | Loss: 0.00614730
Iteration 3/1000 | Loss: 0.00273079
Iteration 4/1000 | Loss: 0.00087173
Iteration 5/1000 | Loss: 0.00032796
Iteration 6/1000 | Loss: 0.00017303
Iteration 7/1000 | Loss: 0.00012157
Iteration 8/1000 | Loss: 0.00009353
Iteration 9/1000 | Loss: 0.00007590
Iteration 10/1000 | Loss: 0.00005905
Iteration 11/1000 | Loss: 0.00017423
Iteration 12/1000 | Loss: 0.00004867
Iteration 13/1000 | Loss: 0.00004016
Iteration 14/1000 | Loss: 0.00003476
Iteration 15/1000 | Loss: 0.00003124
Iteration 16/1000 | Loss: 0.00024239
Iteration 17/1000 | Loss: 0.00003405
Iteration 18/1000 | Loss: 0.00002987
Iteration 19/1000 | Loss: 0.00010710
Iteration 20/1000 | Loss: 0.00002869
Iteration 21/1000 | Loss: 0.00002578
Iteration 22/1000 | Loss: 0.00003249
Iteration 23/1000 | Loss: 0.00002379
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002084
Iteration 26/1000 | Loss: 0.00002717
Iteration 27/1000 | Loss: 0.00002630
Iteration 28/1000 | Loss: 0.00002543
Iteration 29/1000 | Loss: 0.00002311
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001817
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001742
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00001698
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001603
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001602
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001599
Iteration 50/1000 | Loss: 0.00001598
Iteration 51/1000 | Loss: 0.00001597
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001594
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001593
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001593
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00001592
Iteration 66/1000 | Loss: 0.00001592
Iteration 67/1000 | Loss: 0.00001592
Iteration 68/1000 | Loss: 0.00001591
Iteration 69/1000 | Loss: 0.00001591
Iteration 70/1000 | Loss: 0.00001591
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001587
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001582
Iteration 116/1000 | Loss: 0.00001582
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001578
Iteration 146/1000 | Loss: 0.00001578
Iteration 147/1000 | Loss: 0.00001578
Iteration 148/1000 | Loss: 0.00001578
Iteration 149/1000 | Loss: 0.00001578
Iteration 150/1000 | Loss: 0.00001578
Iteration 151/1000 | Loss: 0.00001577
Iteration 152/1000 | Loss: 0.00001577
Iteration 153/1000 | Loss: 0.00001577
Iteration 154/1000 | Loss: 0.00001577
Iteration 155/1000 | Loss: 0.00001577
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001577
Iteration 158/1000 | Loss: 0.00001576
Iteration 159/1000 | Loss: 0.00001576
Iteration 160/1000 | Loss: 0.00001576
Iteration 161/1000 | Loss: 0.00001575
Iteration 162/1000 | Loss: 0.00001575
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001575
Iteration 165/1000 | Loss: 0.00001575
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001575
Iteration 168/1000 | Loss: 0.00001574
Iteration 169/1000 | Loss: 0.00001574
Iteration 170/1000 | Loss: 0.00001574
Iteration 171/1000 | Loss: 0.00001574
Iteration 172/1000 | Loss: 0.00001574
Iteration 173/1000 | Loss: 0.00001574
Iteration 174/1000 | Loss: 0.00001574
Iteration 175/1000 | Loss: 0.00001574
Iteration 176/1000 | Loss: 0.00001574
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001573
Iteration 182/1000 | Loss: 0.00001572
Iteration 183/1000 | Loss: 0.00001572
Iteration 184/1000 | Loss: 0.00001572
Iteration 185/1000 | Loss: 0.00001572
Iteration 186/1000 | Loss: 0.00001572
Iteration 187/1000 | Loss: 0.00001572
Iteration 188/1000 | Loss: 0.00001572
Iteration 189/1000 | Loss: 0.00001571
Iteration 190/1000 | Loss: 0.00001571
Iteration 191/1000 | Loss: 0.00001571
Iteration 192/1000 | Loss: 0.00001571
Iteration 193/1000 | Loss: 0.00001571
Iteration 194/1000 | Loss: 0.00001571
Iteration 195/1000 | Loss: 0.00001571
Iteration 196/1000 | Loss: 0.00001571
Iteration 197/1000 | Loss: 0.00001570
Iteration 198/1000 | Loss: 0.00001570
Iteration 199/1000 | Loss: 0.00001570
Iteration 200/1000 | Loss: 0.00001570
Iteration 201/1000 | Loss: 0.00001570
Iteration 202/1000 | Loss: 0.00001570
Iteration 203/1000 | Loss: 0.00001570
Iteration 204/1000 | Loss: 0.00001570
Iteration 205/1000 | Loss: 0.00001570
Iteration 206/1000 | Loss: 0.00001570
Iteration 207/1000 | Loss: 0.00001570
Iteration 208/1000 | Loss: 0.00001570
Iteration 209/1000 | Loss: 0.00001570
Iteration 210/1000 | Loss: 0.00001570
Iteration 211/1000 | Loss: 0.00001570
Iteration 212/1000 | Loss: 0.00001570
Iteration 213/1000 | Loss: 0.00001570
Iteration 214/1000 | Loss: 0.00001570
Iteration 215/1000 | Loss: 0.00001570
Iteration 216/1000 | Loss: 0.00001570
Iteration 217/1000 | Loss: 0.00001570
Iteration 218/1000 | Loss: 0.00001570
Iteration 219/1000 | Loss: 0.00001570
Iteration 220/1000 | Loss: 0.00001570
Iteration 221/1000 | Loss: 0.00001570
Iteration 222/1000 | Loss: 0.00001570
Iteration 223/1000 | Loss: 0.00001570
Iteration 224/1000 | Loss: 0.00001570
Iteration 225/1000 | Loss: 0.00001570
Iteration 226/1000 | Loss: 0.00001570
Iteration 227/1000 | Loss: 0.00001570
Iteration 228/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.569747837493196e-05, 1.569747837493196e-05, 1.569747837493196e-05, 1.569747837493196e-05, 1.569747837493196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.569747837493196e-05

Optimization complete. Final v2v error: 3.2582411766052246 mm

Highest mean error: 11.199782371520996 mm for frame 102

Lowest mean error: 2.974151611328125 mm for frame 210

Saving results

Total time: 95.11240339279175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398370
Iteration 2/25 | Loss: 0.00108634
Iteration 3/25 | Loss: 0.00099532
Iteration 4/25 | Loss: 0.00097269
Iteration 5/25 | Loss: 0.00096673
Iteration 6/25 | Loss: 0.00096586
Iteration 7/25 | Loss: 0.00096586
Iteration 8/25 | Loss: 0.00096586
Iteration 9/25 | Loss: 0.00096586
Iteration 10/25 | Loss: 0.00096586
Iteration 11/25 | Loss: 0.00096586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009658561903052032, 0.0009658561903052032, 0.0009658561903052032, 0.0009658561903052032, 0.0009658561903052032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009658561903052032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42222154
Iteration 2/25 | Loss: 0.00107868
Iteration 3/25 | Loss: 0.00107868
Iteration 4/25 | Loss: 0.00107867
Iteration 5/25 | Loss: 0.00107867
Iteration 6/25 | Loss: 0.00107867
Iteration 7/25 | Loss: 0.00107867
Iteration 8/25 | Loss: 0.00107867
Iteration 9/25 | Loss: 0.00107867
Iteration 10/25 | Loss: 0.00107867
Iteration 11/25 | Loss: 0.00107867
Iteration 12/25 | Loss: 0.00107867
Iteration 13/25 | Loss: 0.00107867
Iteration 14/25 | Loss: 0.00107867
Iteration 15/25 | Loss: 0.00107867
Iteration 16/25 | Loss: 0.00107867
Iteration 17/25 | Loss: 0.00107867
Iteration 18/25 | Loss: 0.00107867
Iteration 19/25 | Loss: 0.00107867
Iteration 20/25 | Loss: 0.00107867
Iteration 21/25 | Loss: 0.00107867
Iteration 22/25 | Loss: 0.00107867
Iteration 23/25 | Loss: 0.00107867
Iteration 24/25 | Loss: 0.00107867
Iteration 25/25 | Loss: 0.00107867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107867
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001667
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001377
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001249
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001216
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001203
Iteration 45/1000 | Loss: 0.00001203
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001189
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001189
Iteration 123/1000 | Loss: 0.00001189
Iteration 124/1000 | Loss: 0.00001189
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001189
Iteration 129/1000 | Loss: 0.00001189
Iteration 130/1000 | Loss: 0.00001189
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.1894898307218682e-05, 1.1894898307218682e-05, 1.1894898307218682e-05, 1.1894898307218682e-05, 1.1894898307218682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1894898307218682e-05

Optimization complete. Final v2v error: 2.9884440898895264 mm

Highest mean error: 3.37245512008667 mm for frame 131

Lowest mean error: 2.6539230346679688 mm for frame 0

Saving results

Total time: 34.700222969055176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814804
Iteration 2/25 | Loss: 0.00118456
Iteration 3/25 | Loss: 0.00105295
Iteration 4/25 | Loss: 0.00103235
Iteration 5/25 | Loss: 0.00102630
Iteration 6/25 | Loss: 0.00102473
Iteration 7/25 | Loss: 0.00102468
Iteration 8/25 | Loss: 0.00102468
Iteration 9/25 | Loss: 0.00102468
Iteration 10/25 | Loss: 0.00102468
Iteration 11/25 | Loss: 0.00102468
Iteration 12/25 | Loss: 0.00102468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010246819583699107, 0.0010246819583699107, 0.0010246819583699107, 0.0010246819583699107, 0.0010246819583699107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010246819583699107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20514238
Iteration 2/25 | Loss: 0.00144052
Iteration 3/25 | Loss: 0.00144052
Iteration 4/25 | Loss: 0.00144052
Iteration 5/25 | Loss: 0.00144052
Iteration 6/25 | Loss: 0.00144052
Iteration 7/25 | Loss: 0.00144052
Iteration 8/25 | Loss: 0.00144052
Iteration 9/25 | Loss: 0.00144052
Iteration 10/25 | Loss: 0.00144052
Iteration 11/25 | Loss: 0.00144052
Iteration 12/25 | Loss: 0.00144052
Iteration 13/25 | Loss: 0.00144052
Iteration 14/25 | Loss: 0.00144052
Iteration 15/25 | Loss: 0.00144052
Iteration 16/25 | Loss: 0.00144052
Iteration 17/25 | Loss: 0.00144052
Iteration 18/25 | Loss: 0.00144052
Iteration 19/25 | Loss: 0.00144052
Iteration 20/25 | Loss: 0.00144052
Iteration 21/25 | Loss: 0.00144052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014405156252905726, 0.0014405156252905726, 0.0014405156252905726, 0.0014405156252905726, 0.0014405156252905726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014405156252905726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144052
Iteration 2/1000 | Loss: 0.00005625
Iteration 3/1000 | Loss: 0.00002909
Iteration 4/1000 | Loss: 0.00002123
Iteration 5/1000 | Loss: 0.00001793
Iteration 6/1000 | Loss: 0.00001693
Iteration 7/1000 | Loss: 0.00001610
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001444
Iteration 14/1000 | Loss: 0.00001440
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001439
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001426
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001414
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001412
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001411
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001408
Iteration 48/1000 | Loss: 0.00001408
Iteration 49/1000 | Loss: 0.00001408
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001405
Iteration 55/1000 | Loss: 0.00001405
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001404
Iteration 58/1000 | Loss: 0.00001404
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001399
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001398
Iteration 89/1000 | Loss: 0.00001398
Iteration 90/1000 | Loss: 0.00001398
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001395
Iteration 107/1000 | Loss: 0.00001395
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001395
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001394
Iteration 117/1000 | Loss: 0.00001394
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001392
Iteration 122/1000 | Loss: 0.00001392
Iteration 123/1000 | Loss: 0.00001392
Iteration 124/1000 | Loss: 0.00001392
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001391
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001390
Iteration 135/1000 | Loss: 0.00001390
Iteration 136/1000 | Loss: 0.00001390
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001389
Iteration 144/1000 | Loss: 0.00001389
Iteration 145/1000 | Loss: 0.00001389
Iteration 146/1000 | Loss: 0.00001389
Iteration 147/1000 | Loss: 0.00001389
Iteration 148/1000 | Loss: 0.00001389
Iteration 149/1000 | Loss: 0.00001389
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001389
Iteration 158/1000 | Loss: 0.00001389
Iteration 159/1000 | Loss: 0.00001389
Iteration 160/1000 | Loss: 0.00001389
Iteration 161/1000 | Loss: 0.00001388
Iteration 162/1000 | Loss: 0.00001388
Iteration 163/1000 | Loss: 0.00001388
Iteration 164/1000 | Loss: 0.00001388
Iteration 165/1000 | Loss: 0.00001388
Iteration 166/1000 | Loss: 0.00001388
Iteration 167/1000 | Loss: 0.00001388
Iteration 168/1000 | Loss: 0.00001388
Iteration 169/1000 | Loss: 0.00001388
Iteration 170/1000 | Loss: 0.00001388
Iteration 171/1000 | Loss: 0.00001388
Iteration 172/1000 | Loss: 0.00001388
Iteration 173/1000 | Loss: 0.00001388
Iteration 174/1000 | Loss: 0.00001388
Iteration 175/1000 | Loss: 0.00001388
Iteration 176/1000 | Loss: 0.00001388
Iteration 177/1000 | Loss: 0.00001388
Iteration 178/1000 | Loss: 0.00001388
Iteration 179/1000 | Loss: 0.00001388
Iteration 180/1000 | Loss: 0.00001388
Iteration 181/1000 | Loss: 0.00001388
Iteration 182/1000 | Loss: 0.00001388
Iteration 183/1000 | Loss: 0.00001388
Iteration 184/1000 | Loss: 0.00001388
Iteration 185/1000 | Loss: 0.00001388
Iteration 186/1000 | Loss: 0.00001388
Iteration 187/1000 | Loss: 0.00001388
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001388
Iteration 193/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.3875356671633199e-05, 1.3875356671633199e-05, 1.3875356671633199e-05, 1.3875356671633199e-05, 1.3875356671633199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3875356671633199e-05

Optimization complete. Final v2v error: 3.1670186519622803 mm

Highest mean error: 3.8106791973114014 mm for frame 94

Lowest mean error: 2.79544734954834 mm for frame 35

Saving results

Total time: 38.88863801956177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506287
Iteration 2/25 | Loss: 0.00126290
Iteration 3/25 | Loss: 0.00105675
Iteration 4/25 | Loss: 0.00104109
Iteration 5/25 | Loss: 0.00103519
Iteration 6/25 | Loss: 0.00103440
Iteration 7/25 | Loss: 0.00103440
Iteration 8/25 | Loss: 0.00103440
Iteration 9/25 | Loss: 0.00103440
Iteration 10/25 | Loss: 0.00103440
Iteration 11/25 | Loss: 0.00103440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010343982139602304, 0.0010343982139602304, 0.0010343982139602304, 0.0010343982139602304, 0.0010343982139602304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010343982139602304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57793039
Iteration 2/25 | Loss: 0.00086572
Iteration 3/25 | Loss: 0.00086572
Iteration 4/25 | Loss: 0.00086571
Iteration 5/25 | Loss: 0.00086571
Iteration 6/25 | Loss: 0.00086571
Iteration 7/25 | Loss: 0.00086571
Iteration 8/25 | Loss: 0.00086571
Iteration 9/25 | Loss: 0.00086571
Iteration 10/25 | Loss: 0.00086571
Iteration 11/25 | Loss: 0.00086571
Iteration 12/25 | Loss: 0.00086571
Iteration 13/25 | Loss: 0.00086571
Iteration 14/25 | Loss: 0.00086571
Iteration 15/25 | Loss: 0.00086571
Iteration 16/25 | Loss: 0.00086571
Iteration 17/25 | Loss: 0.00086571
Iteration 18/25 | Loss: 0.00086571
Iteration 19/25 | Loss: 0.00086571
Iteration 20/25 | Loss: 0.00086571
Iteration 21/25 | Loss: 0.00086571
Iteration 22/25 | Loss: 0.00086571
Iteration 23/25 | Loss: 0.00086571
Iteration 24/25 | Loss: 0.00086571
Iteration 25/25 | Loss: 0.00086571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086571
Iteration 2/1000 | Loss: 0.00004276
Iteration 3/1000 | Loss: 0.00003313
Iteration 4/1000 | Loss: 0.00002978
Iteration 5/1000 | Loss: 0.00002862
Iteration 6/1000 | Loss: 0.00002780
Iteration 7/1000 | Loss: 0.00002707
Iteration 8/1000 | Loss: 0.00002656
Iteration 9/1000 | Loss: 0.00002615
Iteration 10/1000 | Loss: 0.00002588
Iteration 11/1000 | Loss: 0.00002567
Iteration 12/1000 | Loss: 0.00002539
Iteration 13/1000 | Loss: 0.00002525
Iteration 14/1000 | Loss: 0.00002493
Iteration 15/1000 | Loss: 0.00002468
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002432
Iteration 18/1000 | Loss: 0.00002416
Iteration 19/1000 | Loss: 0.00002400
Iteration 20/1000 | Loss: 0.00002381
Iteration 21/1000 | Loss: 0.00002375
Iteration 22/1000 | Loss: 0.00002359
Iteration 23/1000 | Loss: 0.00002343
Iteration 24/1000 | Loss: 0.00002339
Iteration 25/1000 | Loss: 0.00002332
Iteration 26/1000 | Loss: 0.00002328
Iteration 27/1000 | Loss: 0.00002325
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002320
Iteration 30/1000 | Loss: 0.00002320
Iteration 31/1000 | Loss: 0.00002317
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00002310
Iteration 34/1000 | Loss: 0.00002308
Iteration 35/1000 | Loss: 0.00002304
Iteration 36/1000 | Loss: 0.00002304
Iteration 37/1000 | Loss: 0.00002302
Iteration 38/1000 | Loss: 0.00002301
Iteration 39/1000 | Loss: 0.00002301
Iteration 40/1000 | Loss: 0.00002301
Iteration 41/1000 | Loss: 0.00002300
Iteration 42/1000 | Loss: 0.00002300
Iteration 43/1000 | Loss: 0.00002299
Iteration 44/1000 | Loss: 0.00002299
Iteration 45/1000 | Loss: 0.00002299
Iteration 46/1000 | Loss: 0.00002299
Iteration 47/1000 | Loss: 0.00002298
Iteration 48/1000 | Loss: 0.00002297
Iteration 49/1000 | Loss: 0.00002297
Iteration 50/1000 | Loss: 0.00002296
Iteration 51/1000 | Loss: 0.00002296
Iteration 52/1000 | Loss: 0.00002296
Iteration 53/1000 | Loss: 0.00002295
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002295
Iteration 57/1000 | Loss: 0.00002295
Iteration 58/1000 | Loss: 0.00002295
Iteration 59/1000 | Loss: 0.00002295
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002295
Iteration 62/1000 | Loss: 0.00002295
Iteration 63/1000 | Loss: 0.00002293
Iteration 64/1000 | Loss: 0.00002293
Iteration 65/1000 | Loss: 0.00002293
Iteration 66/1000 | Loss: 0.00002293
Iteration 67/1000 | Loss: 0.00002292
Iteration 68/1000 | Loss: 0.00002292
Iteration 69/1000 | Loss: 0.00002292
Iteration 70/1000 | Loss: 0.00002292
Iteration 71/1000 | Loss: 0.00002292
Iteration 72/1000 | Loss: 0.00002291
Iteration 73/1000 | Loss: 0.00002291
Iteration 74/1000 | Loss: 0.00002291
Iteration 75/1000 | Loss: 0.00002291
Iteration 76/1000 | Loss: 0.00002291
Iteration 77/1000 | Loss: 0.00002291
Iteration 78/1000 | Loss: 0.00002291
Iteration 79/1000 | Loss: 0.00002291
Iteration 80/1000 | Loss: 0.00002291
Iteration 81/1000 | Loss: 0.00002291
Iteration 82/1000 | Loss: 0.00002291
Iteration 83/1000 | Loss: 0.00002291
Iteration 84/1000 | Loss: 0.00002290
Iteration 85/1000 | Loss: 0.00002290
Iteration 86/1000 | Loss: 0.00002290
Iteration 87/1000 | Loss: 0.00002290
Iteration 88/1000 | Loss: 0.00002290
Iteration 89/1000 | Loss: 0.00002290
Iteration 90/1000 | Loss: 0.00002289
Iteration 91/1000 | Loss: 0.00002289
Iteration 92/1000 | Loss: 0.00002289
Iteration 93/1000 | Loss: 0.00002289
Iteration 94/1000 | Loss: 0.00002289
Iteration 95/1000 | Loss: 0.00002289
Iteration 96/1000 | Loss: 0.00002289
Iteration 97/1000 | Loss: 0.00002289
Iteration 98/1000 | Loss: 0.00002289
Iteration 99/1000 | Loss: 0.00002289
Iteration 100/1000 | Loss: 0.00002289
Iteration 101/1000 | Loss: 0.00002289
Iteration 102/1000 | Loss: 0.00002288
Iteration 103/1000 | Loss: 0.00002288
Iteration 104/1000 | Loss: 0.00002288
Iteration 105/1000 | Loss: 0.00002288
Iteration 106/1000 | Loss: 0.00002288
Iteration 107/1000 | Loss: 0.00002288
Iteration 108/1000 | Loss: 0.00002288
Iteration 109/1000 | Loss: 0.00002288
Iteration 110/1000 | Loss: 0.00002288
Iteration 111/1000 | Loss: 0.00002288
Iteration 112/1000 | Loss: 0.00002288
Iteration 113/1000 | Loss: 0.00002288
Iteration 114/1000 | Loss: 0.00002287
Iteration 115/1000 | Loss: 0.00002287
Iteration 116/1000 | Loss: 0.00002287
Iteration 117/1000 | Loss: 0.00002287
Iteration 118/1000 | Loss: 0.00002287
Iteration 119/1000 | Loss: 0.00002287
Iteration 120/1000 | Loss: 0.00002286
Iteration 121/1000 | Loss: 0.00002286
Iteration 122/1000 | Loss: 0.00002286
Iteration 123/1000 | Loss: 0.00002286
Iteration 124/1000 | Loss: 0.00002286
Iteration 125/1000 | Loss: 0.00002285
Iteration 126/1000 | Loss: 0.00002285
Iteration 127/1000 | Loss: 0.00002285
Iteration 128/1000 | Loss: 0.00002285
Iteration 129/1000 | Loss: 0.00002285
Iteration 130/1000 | Loss: 0.00002285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.285439222760033e-05, 2.285439222760033e-05, 2.285439222760033e-05, 2.285439222760033e-05, 2.285439222760033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.285439222760033e-05

Optimization complete. Final v2v error: 3.8650262355804443 mm

Highest mean error: 4.11049747467041 mm for frame 57

Lowest mean error: 3.5099780559539795 mm for frame 0

Saving results

Total time: 60.507344007492065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115764
Iteration 2/25 | Loss: 0.00146381
Iteration 3/25 | Loss: 0.00108088
Iteration 4/25 | Loss: 0.00101289
Iteration 5/25 | Loss: 0.00100341
Iteration 6/25 | Loss: 0.00099913
Iteration 7/25 | Loss: 0.00099781
Iteration 8/25 | Loss: 0.00099745
Iteration 9/25 | Loss: 0.00099734
Iteration 10/25 | Loss: 0.00099734
Iteration 11/25 | Loss: 0.00099733
Iteration 12/25 | Loss: 0.00099733
Iteration 13/25 | Loss: 0.00099733
Iteration 14/25 | Loss: 0.00099733
Iteration 15/25 | Loss: 0.00099733
Iteration 16/25 | Loss: 0.00099733
Iteration 17/25 | Loss: 0.00099733
Iteration 18/25 | Loss: 0.00099733
Iteration 19/25 | Loss: 0.00099733
Iteration 20/25 | Loss: 0.00099733
Iteration 21/25 | Loss: 0.00099733
Iteration 22/25 | Loss: 0.00099733
Iteration 23/25 | Loss: 0.00099733
Iteration 24/25 | Loss: 0.00099733
Iteration 25/25 | Loss: 0.00099733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26588297
Iteration 2/25 | Loss: 0.00121580
Iteration 3/25 | Loss: 0.00121580
Iteration 4/25 | Loss: 0.00121580
Iteration 5/25 | Loss: 0.00121580
Iteration 6/25 | Loss: 0.00121580
Iteration 7/25 | Loss: 0.00121580
Iteration 8/25 | Loss: 0.00121580
Iteration 9/25 | Loss: 0.00121580
Iteration 10/25 | Loss: 0.00121580
Iteration 11/25 | Loss: 0.00121579
Iteration 12/25 | Loss: 0.00121579
Iteration 13/25 | Loss: 0.00121579
Iteration 14/25 | Loss: 0.00121579
Iteration 15/25 | Loss: 0.00121579
Iteration 16/25 | Loss: 0.00121579
Iteration 17/25 | Loss: 0.00121579
Iteration 18/25 | Loss: 0.00121579
Iteration 19/25 | Loss: 0.00121579
Iteration 20/25 | Loss: 0.00121579
Iteration 21/25 | Loss: 0.00121579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012157945893704891, 0.0012157945893704891, 0.0012157945893704891, 0.0012157945893704891, 0.0012157945893704891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012157945893704891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121579
Iteration 2/1000 | Loss: 0.00002591
Iteration 3/1000 | Loss: 0.00007640
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001280
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001240
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001213
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001205
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001204
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001204
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Iteration 151/1000 | Loss: 0.00001203
Iteration 152/1000 | Loss: 0.00001203
Iteration 153/1000 | Loss: 0.00001203
Iteration 154/1000 | Loss: 0.00001203
Iteration 155/1000 | Loss: 0.00001202
Iteration 156/1000 | Loss: 0.00001202
Iteration 157/1000 | Loss: 0.00001202
Iteration 158/1000 | Loss: 0.00001202
Iteration 159/1000 | Loss: 0.00001202
Iteration 160/1000 | Loss: 0.00001202
Iteration 161/1000 | Loss: 0.00001201
Iteration 162/1000 | Loss: 0.00001201
Iteration 163/1000 | Loss: 0.00001201
Iteration 164/1000 | Loss: 0.00001201
Iteration 165/1000 | Loss: 0.00001201
Iteration 166/1000 | Loss: 0.00001201
Iteration 167/1000 | Loss: 0.00001201
Iteration 168/1000 | Loss: 0.00001201
Iteration 169/1000 | Loss: 0.00001201
Iteration 170/1000 | Loss: 0.00001201
Iteration 171/1000 | Loss: 0.00001201
Iteration 172/1000 | Loss: 0.00001201
Iteration 173/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.2012545084871817e-05, 1.2012545084871817e-05, 1.2012545084871817e-05, 1.2012545084871817e-05, 1.2012545084871817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2012545084871817e-05

Optimization complete. Final v2v error: 2.9470653533935547 mm

Highest mean error: 3.8434767723083496 mm for frame 20

Lowest mean error: 2.6097583770751953 mm for frame 7

Saving results

Total time: 40.5659544467926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01151211
Iteration 2/25 | Loss: 0.00138807
Iteration 3/25 | Loss: 0.00108786
Iteration 4/25 | Loss: 0.00106138
Iteration 5/25 | Loss: 0.00105694
Iteration 6/25 | Loss: 0.00105617
Iteration 7/25 | Loss: 0.00105614
Iteration 8/25 | Loss: 0.00105614
Iteration 9/25 | Loss: 0.00105614
Iteration 10/25 | Loss: 0.00105614
Iteration 11/25 | Loss: 0.00105614
Iteration 12/25 | Loss: 0.00105614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010561374947428703, 0.0010561374947428703, 0.0010561374947428703, 0.0010561374947428703, 0.0010561374947428703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010561374947428703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79687536
Iteration 2/25 | Loss: 0.00070465
Iteration 3/25 | Loss: 0.00070465
Iteration 4/25 | Loss: 0.00070464
Iteration 5/25 | Loss: 0.00070464
Iteration 6/25 | Loss: 0.00070464
Iteration 7/25 | Loss: 0.00070464
Iteration 8/25 | Loss: 0.00070464
Iteration 9/25 | Loss: 0.00070464
Iteration 10/25 | Loss: 0.00070464
Iteration 11/25 | Loss: 0.00070464
Iteration 12/25 | Loss: 0.00070464
Iteration 13/25 | Loss: 0.00070464
Iteration 14/25 | Loss: 0.00070464
Iteration 15/25 | Loss: 0.00070464
Iteration 16/25 | Loss: 0.00070464
Iteration 17/25 | Loss: 0.00070464
Iteration 18/25 | Loss: 0.00070464
Iteration 19/25 | Loss: 0.00070464
Iteration 20/25 | Loss: 0.00070464
Iteration 21/25 | Loss: 0.00070464
Iteration 22/25 | Loss: 0.00070464
Iteration 23/25 | Loss: 0.00070464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000704642035998404, 0.000704642035998404, 0.000704642035998404, 0.000704642035998404, 0.000704642035998404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000704642035998404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070464
Iteration 2/1000 | Loss: 0.00004661
Iteration 3/1000 | Loss: 0.00003337
Iteration 4/1000 | Loss: 0.00002949
Iteration 5/1000 | Loss: 0.00002818
Iteration 6/1000 | Loss: 0.00002753
Iteration 7/1000 | Loss: 0.00002699
Iteration 8/1000 | Loss: 0.00002664
Iteration 9/1000 | Loss: 0.00002640
Iteration 10/1000 | Loss: 0.00002625
Iteration 11/1000 | Loss: 0.00002618
Iteration 12/1000 | Loss: 0.00002617
Iteration 13/1000 | Loss: 0.00002598
Iteration 14/1000 | Loss: 0.00002576
Iteration 15/1000 | Loss: 0.00002562
Iteration 16/1000 | Loss: 0.00002560
Iteration 17/1000 | Loss: 0.00002560
Iteration 18/1000 | Loss: 0.00002555
Iteration 19/1000 | Loss: 0.00002554
Iteration 20/1000 | Loss: 0.00002553
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002545
Iteration 23/1000 | Loss: 0.00002541
Iteration 24/1000 | Loss: 0.00002541
Iteration 25/1000 | Loss: 0.00002541
Iteration 26/1000 | Loss: 0.00002540
Iteration 27/1000 | Loss: 0.00002535
Iteration 28/1000 | Loss: 0.00002535
Iteration 29/1000 | Loss: 0.00002532
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002531
Iteration 32/1000 | Loss: 0.00002531
Iteration 33/1000 | Loss: 0.00002530
Iteration 34/1000 | Loss: 0.00002530
Iteration 35/1000 | Loss: 0.00002529
Iteration 36/1000 | Loss: 0.00002527
Iteration 37/1000 | Loss: 0.00002527
Iteration 38/1000 | Loss: 0.00002527
Iteration 39/1000 | Loss: 0.00002526
Iteration 40/1000 | Loss: 0.00002526
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002526
Iteration 43/1000 | Loss: 0.00002525
Iteration 44/1000 | Loss: 0.00002523
Iteration 45/1000 | Loss: 0.00002522
Iteration 46/1000 | Loss: 0.00002522
Iteration 47/1000 | Loss: 0.00002522
Iteration 48/1000 | Loss: 0.00002521
Iteration 49/1000 | Loss: 0.00002519
Iteration 50/1000 | Loss: 0.00002518
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00002513
Iteration 56/1000 | Loss: 0.00002513
Iteration 57/1000 | Loss: 0.00002513
Iteration 58/1000 | Loss: 0.00002513
Iteration 59/1000 | Loss: 0.00002513
Iteration 60/1000 | Loss: 0.00002513
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002510
Iteration 63/1000 | Loss: 0.00002510
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002510
Iteration 67/1000 | Loss: 0.00002509
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002508
Iteration 70/1000 | Loss: 0.00002508
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002507
Iteration 73/1000 | Loss: 0.00002507
Iteration 74/1000 | Loss: 0.00002507
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002507
Iteration 78/1000 | Loss: 0.00002506
Iteration 79/1000 | Loss: 0.00002506
Iteration 80/1000 | Loss: 0.00002506
Iteration 81/1000 | Loss: 0.00002506
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002505
Iteration 84/1000 | Loss: 0.00002505
Iteration 85/1000 | Loss: 0.00002505
Iteration 86/1000 | Loss: 0.00002505
Iteration 87/1000 | Loss: 0.00002505
Iteration 88/1000 | Loss: 0.00002504
Iteration 89/1000 | Loss: 0.00002504
Iteration 90/1000 | Loss: 0.00002504
Iteration 91/1000 | Loss: 0.00002504
Iteration 92/1000 | Loss: 0.00002503
Iteration 93/1000 | Loss: 0.00002503
Iteration 94/1000 | Loss: 0.00002503
Iteration 95/1000 | Loss: 0.00002503
Iteration 96/1000 | Loss: 0.00002503
Iteration 97/1000 | Loss: 0.00002503
Iteration 98/1000 | Loss: 0.00002503
Iteration 99/1000 | Loss: 0.00002503
Iteration 100/1000 | Loss: 0.00002503
Iteration 101/1000 | Loss: 0.00002503
Iteration 102/1000 | Loss: 0.00002502
Iteration 103/1000 | Loss: 0.00002502
Iteration 104/1000 | Loss: 0.00002502
Iteration 105/1000 | Loss: 0.00002502
Iteration 106/1000 | Loss: 0.00002502
Iteration 107/1000 | Loss: 0.00002502
Iteration 108/1000 | Loss: 0.00002502
Iteration 109/1000 | Loss: 0.00002502
Iteration 110/1000 | Loss: 0.00002502
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002502
Iteration 113/1000 | Loss: 0.00002502
Iteration 114/1000 | Loss: 0.00002502
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002500
Iteration 117/1000 | Loss: 0.00002500
Iteration 118/1000 | Loss: 0.00002500
Iteration 119/1000 | Loss: 0.00002500
Iteration 120/1000 | Loss: 0.00002500
Iteration 121/1000 | Loss: 0.00002500
Iteration 122/1000 | Loss: 0.00002500
Iteration 123/1000 | Loss: 0.00002500
Iteration 124/1000 | Loss: 0.00002499
Iteration 125/1000 | Loss: 0.00002499
Iteration 126/1000 | Loss: 0.00002499
Iteration 127/1000 | Loss: 0.00002499
Iteration 128/1000 | Loss: 0.00002499
Iteration 129/1000 | Loss: 0.00002499
Iteration 130/1000 | Loss: 0.00002499
Iteration 131/1000 | Loss: 0.00002499
Iteration 132/1000 | Loss: 0.00002499
Iteration 133/1000 | Loss: 0.00002498
Iteration 134/1000 | Loss: 0.00002498
Iteration 135/1000 | Loss: 0.00002498
Iteration 136/1000 | Loss: 0.00002498
Iteration 137/1000 | Loss: 0.00002498
Iteration 138/1000 | Loss: 0.00002498
Iteration 139/1000 | Loss: 0.00002498
Iteration 140/1000 | Loss: 0.00002498
Iteration 141/1000 | Loss: 0.00002498
Iteration 142/1000 | Loss: 0.00002498
Iteration 143/1000 | Loss: 0.00002498
Iteration 144/1000 | Loss: 0.00002498
Iteration 145/1000 | Loss: 0.00002498
Iteration 146/1000 | Loss: 0.00002498
Iteration 147/1000 | Loss: 0.00002498
Iteration 148/1000 | Loss: 0.00002498
Iteration 149/1000 | Loss: 0.00002498
Iteration 150/1000 | Loss: 0.00002498
Iteration 151/1000 | Loss: 0.00002498
Iteration 152/1000 | Loss: 0.00002498
Iteration 153/1000 | Loss: 0.00002498
Iteration 154/1000 | Loss: 0.00002498
Iteration 155/1000 | Loss: 0.00002498
Iteration 156/1000 | Loss: 0.00002498
Iteration 157/1000 | Loss: 0.00002498
Iteration 158/1000 | Loss: 0.00002498
Iteration 159/1000 | Loss: 0.00002498
Iteration 160/1000 | Loss: 0.00002498
Iteration 161/1000 | Loss: 0.00002498
Iteration 162/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.4982236936921254e-05, 2.4982236936921254e-05, 2.4982236936921254e-05, 2.4982236936921254e-05, 2.4982236936921254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4982236936921254e-05

Optimization complete. Final v2v error: 4.094011306762695 mm

Highest mean error: 4.740331172943115 mm for frame 102

Lowest mean error: 3.464930534362793 mm for frame 0

Saving results

Total time: 42.88968873023987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404701
Iteration 2/25 | Loss: 0.00104549
Iteration 3/25 | Loss: 0.00095254
Iteration 4/25 | Loss: 0.00094363
Iteration 5/25 | Loss: 0.00094017
Iteration 6/25 | Loss: 0.00093898
Iteration 7/25 | Loss: 0.00093893
Iteration 8/25 | Loss: 0.00093893
Iteration 9/25 | Loss: 0.00093893
Iteration 10/25 | Loss: 0.00093893
Iteration 11/25 | Loss: 0.00093893
Iteration 12/25 | Loss: 0.00093893
Iteration 13/25 | Loss: 0.00093893
Iteration 14/25 | Loss: 0.00093893
Iteration 15/25 | Loss: 0.00093893
Iteration 16/25 | Loss: 0.00093893
Iteration 17/25 | Loss: 0.00093893
Iteration 18/25 | Loss: 0.00093893
Iteration 19/25 | Loss: 0.00093893
Iteration 20/25 | Loss: 0.00093893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009389254846610129, 0.0009389254846610129, 0.0009389254846610129, 0.0009389254846610129, 0.0009389254846610129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009389254846610129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27293873
Iteration 2/25 | Loss: 0.00114605
Iteration 3/25 | Loss: 0.00114605
Iteration 4/25 | Loss: 0.00114605
Iteration 5/25 | Loss: 0.00114605
Iteration 6/25 | Loss: 0.00114605
Iteration 7/25 | Loss: 0.00114605
Iteration 8/25 | Loss: 0.00114605
Iteration 9/25 | Loss: 0.00114605
Iteration 10/25 | Loss: 0.00114605
Iteration 11/25 | Loss: 0.00114605
Iteration 12/25 | Loss: 0.00114605
Iteration 13/25 | Loss: 0.00114605
Iteration 14/25 | Loss: 0.00114605
Iteration 15/25 | Loss: 0.00114605
Iteration 16/25 | Loss: 0.00114605
Iteration 17/25 | Loss: 0.00114605
Iteration 18/25 | Loss: 0.00114605
Iteration 19/25 | Loss: 0.00114605
Iteration 20/25 | Loss: 0.00114605
Iteration 21/25 | Loss: 0.00114605
Iteration 22/25 | Loss: 0.00114605
Iteration 23/25 | Loss: 0.00114605
Iteration 24/25 | Loss: 0.00114605
Iteration 25/25 | Loss: 0.00114605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114605
Iteration 2/1000 | Loss: 0.00001588
Iteration 3/1000 | Loss: 0.00000982
Iteration 4/1000 | Loss: 0.00000871
Iteration 5/1000 | Loss: 0.00000828
Iteration 6/1000 | Loss: 0.00000806
Iteration 7/1000 | Loss: 0.00000783
Iteration 8/1000 | Loss: 0.00000779
Iteration 9/1000 | Loss: 0.00000776
Iteration 10/1000 | Loss: 0.00000775
Iteration 11/1000 | Loss: 0.00000774
Iteration 12/1000 | Loss: 0.00000771
Iteration 13/1000 | Loss: 0.00000769
Iteration 14/1000 | Loss: 0.00000768
Iteration 15/1000 | Loss: 0.00000768
Iteration 16/1000 | Loss: 0.00000766
Iteration 17/1000 | Loss: 0.00000762
Iteration 18/1000 | Loss: 0.00000761
Iteration 19/1000 | Loss: 0.00000760
Iteration 20/1000 | Loss: 0.00000760
Iteration 21/1000 | Loss: 0.00000759
Iteration 22/1000 | Loss: 0.00000758
Iteration 23/1000 | Loss: 0.00000757
Iteration 24/1000 | Loss: 0.00000756
Iteration 25/1000 | Loss: 0.00000755
Iteration 26/1000 | Loss: 0.00000755
Iteration 27/1000 | Loss: 0.00000755
Iteration 28/1000 | Loss: 0.00000755
Iteration 29/1000 | Loss: 0.00000755
Iteration 30/1000 | Loss: 0.00000755
Iteration 31/1000 | Loss: 0.00000755
Iteration 32/1000 | Loss: 0.00000755
Iteration 33/1000 | Loss: 0.00000755
Iteration 34/1000 | Loss: 0.00000755
Iteration 35/1000 | Loss: 0.00000754
Iteration 36/1000 | Loss: 0.00000754
Iteration 37/1000 | Loss: 0.00000754
Iteration 38/1000 | Loss: 0.00000752
Iteration 39/1000 | Loss: 0.00000749
Iteration 40/1000 | Loss: 0.00000748
Iteration 41/1000 | Loss: 0.00000748
Iteration 42/1000 | Loss: 0.00000748
Iteration 43/1000 | Loss: 0.00000748
Iteration 44/1000 | Loss: 0.00000748
Iteration 45/1000 | Loss: 0.00000746
Iteration 46/1000 | Loss: 0.00000743
Iteration 47/1000 | Loss: 0.00000742
Iteration 48/1000 | Loss: 0.00000741
Iteration 49/1000 | Loss: 0.00000740
Iteration 50/1000 | Loss: 0.00000739
Iteration 51/1000 | Loss: 0.00000739
Iteration 52/1000 | Loss: 0.00000739
Iteration 53/1000 | Loss: 0.00000738
Iteration 54/1000 | Loss: 0.00000738
Iteration 55/1000 | Loss: 0.00000738
Iteration 56/1000 | Loss: 0.00000738
Iteration 57/1000 | Loss: 0.00000738
Iteration 58/1000 | Loss: 0.00000737
Iteration 59/1000 | Loss: 0.00000737
Iteration 60/1000 | Loss: 0.00000737
Iteration 61/1000 | Loss: 0.00000737
Iteration 62/1000 | Loss: 0.00000737
Iteration 63/1000 | Loss: 0.00000737
Iteration 64/1000 | Loss: 0.00000737
Iteration 65/1000 | Loss: 0.00000737
Iteration 66/1000 | Loss: 0.00000736
Iteration 67/1000 | Loss: 0.00000736
Iteration 68/1000 | Loss: 0.00000736
Iteration 69/1000 | Loss: 0.00000736
Iteration 70/1000 | Loss: 0.00000736
Iteration 71/1000 | Loss: 0.00000736
Iteration 72/1000 | Loss: 0.00000736
Iteration 73/1000 | Loss: 0.00000736
Iteration 74/1000 | Loss: 0.00000736
Iteration 75/1000 | Loss: 0.00000736
Iteration 76/1000 | Loss: 0.00000736
Iteration 77/1000 | Loss: 0.00000736
Iteration 78/1000 | Loss: 0.00000735
Iteration 79/1000 | Loss: 0.00000735
Iteration 80/1000 | Loss: 0.00000735
Iteration 81/1000 | Loss: 0.00000734
Iteration 82/1000 | Loss: 0.00000734
Iteration 83/1000 | Loss: 0.00000734
Iteration 84/1000 | Loss: 0.00000734
Iteration 85/1000 | Loss: 0.00000733
Iteration 86/1000 | Loss: 0.00000733
Iteration 87/1000 | Loss: 0.00000733
Iteration 88/1000 | Loss: 0.00000733
Iteration 89/1000 | Loss: 0.00000733
Iteration 90/1000 | Loss: 0.00000732
Iteration 91/1000 | Loss: 0.00000732
Iteration 92/1000 | Loss: 0.00000732
Iteration 93/1000 | Loss: 0.00000731
Iteration 94/1000 | Loss: 0.00000731
Iteration 95/1000 | Loss: 0.00000731
Iteration 96/1000 | Loss: 0.00000731
Iteration 97/1000 | Loss: 0.00000731
Iteration 98/1000 | Loss: 0.00000731
Iteration 99/1000 | Loss: 0.00000731
Iteration 100/1000 | Loss: 0.00000731
Iteration 101/1000 | Loss: 0.00000731
Iteration 102/1000 | Loss: 0.00000730
Iteration 103/1000 | Loss: 0.00000730
Iteration 104/1000 | Loss: 0.00000730
Iteration 105/1000 | Loss: 0.00000730
Iteration 106/1000 | Loss: 0.00000729
Iteration 107/1000 | Loss: 0.00000729
Iteration 108/1000 | Loss: 0.00000728
Iteration 109/1000 | Loss: 0.00000728
Iteration 110/1000 | Loss: 0.00000728
Iteration 111/1000 | Loss: 0.00000728
Iteration 112/1000 | Loss: 0.00000728
Iteration 113/1000 | Loss: 0.00000727
Iteration 114/1000 | Loss: 0.00000727
Iteration 115/1000 | Loss: 0.00000727
Iteration 116/1000 | Loss: 0.00000727
Iteration 117/1000 | Loss: 0.00000727
Iteration 118/1000 | Loss: 0.00000726
Iteration 119/1000 | Loss: 0.00000726
Iteration 120/1000 | Loss: 0.00000725
Iteration 121/1000 | Loss: 0.00000725
Iteration 122/1000 | Loss: 0.00000725
Iteration 123/1000 | Loss: 0.00000724
Iteration 124/1000 | Loss: 0.00000724
Iteration 125/1000 | Loss: 0.00000724
Iteration 126/1000 | Loss: 0.00000724
Iteration 127/1000 | Loss: 0.00000724
Iteration 128/1000 | Loss: 0.00000724
Iteration 129/1000 | Loss: 0.00000724
Iteration 130/1000 | Loss: 0.00000724
Iteration 131/1000 | Loss: 0.00000723
Iteration 132/1000 | Loss: 0.00000723
Iteration 133/1000 | Loss: 0.00000723
Iteration 134/1000 | Loss: 0.00000723
Iteration 135/1000 | Loss: 0.00000723
Iteration 136/1000 | Loss: 0.00000723
Iteration 137/1000 | Loss: 0.00000723
Iteration 138/1000 | Loss: 0.00000723
Iteration 139/1000 | Loss: 0.00000723
Iteration 140/1000 | Loss: 0.00000722
Iteration 141/1000 | Loss: 0.00000722
Iteration 142/1000 | Loss: 0.00000722
Iteration 143/1000 | Loss: 0.00000722
Iteration 144/1000 | Loss: 0.00000722
Iteration 145/1000 | Loss: 0.00000722
Iteration 146/1000 | Loss: 0.00000722
Iteration 147/1000 | Loss: 0.00000722
Iteration 148/1000 | Loss: 0.00000722
Iteration 149/1000 | Loss: 0.00000722
Iteration 150/1000 | Loss: 0.00000722
Iteration 151/1000 | Loss: 0.00000722
Iteration 152/1000 | Loss: 0.00000722
Iteration 153/1000 | Loss: 0.00000722
Iteration 154/1000 | Loss: 0.00000721
Iteration 155/1000 | Loss: 0.00000721
Iteration 156/1000 | Loss: 0.00000721
Iteration 157/1000 | Loss: 0.00000721
Iteration 158/1000 | Loss: 0.00000721
Iteration 159/1000 | Loss: 0.00000721
Iteration 160/1000 | Loss: 0.00000721
Iteration 161/1000 | Loss: 0.00000721
Iteration 162/1000 | Loss: 0.00000721
Iteration 163/1000 | Loss: 0.00000721
Iteration 164/1000 | Loss: 0.00000721
Iteration 165/1000 | Loss: 0.00000721
Iteration 166/1000 | Loss: 0.00000721
Iteration 167/1000 | Loss: 0.00000721
Iteration 168/1000 | Loss: 0.00000721
Iteration 169/1000 | Loss: 0.00000720
Iteration 170/1000 | Loss: 0.00000720
Iteration 171/1000 | Loss: 0.00000720
Iteration 172/1000 | Loss: 0.00000720
Iteration 173/1000 | Loss: 0.00000720
Iteration 174/1000 | Loss: 0.00000720
Iteration 175/1000 | Loss: 0.00000719
Iteration 176/1000 | Loss: 0.00000719
Iteration 177/1000 | Loss: 0.00000719
Iteration 178/1000 | Loss: 0.00000718
Iteration 179/1000 | Loss: 0.00000718
Iteration 180/1000 | Loss: 0.00000718
Iteration 181/1000 | Loss: 0.00000718
Iteration 182/1000 | Loss: 0.00000718
Iteration 183/1000 | Loss: 0.00000718
Iteration 184/1000 | Loss: 0.00000718
Iteration 185/1000 | Loss: 0.00000717
Iteration 186/1000 | Loss: 0.00000717
Iteration 187/1000 | Loss: 0.00000717
Iteration 188/1000 | Loss: 0.00000717
Iteration 189/1000 | Loss: 0.00000717
Iteration 190/1000 | Loss: 0.00000717
Iteration 191/1000 | Loss: 0.00000717
Iteration 192/1000 | Loss: 0.00000716
Iteration 193/1000 | Loss: 0.00000716
Iteration 194/1000 | Loss: 0.00000716
Iteration 195/1000 | Loss: 0.00000716
Iteration 196/1000 | Loss: 0.00000716
Iteration 197/1000 | Loss: 0.00000716
Iteration 198/1000 | Loss: 0.00000716
Iteration 199/1000 | Loss: 0.00000716
Iteration 200/1000 | Loss: 0.00000716
Iteration 201/1000 | Loss: 0.00000716
Iteration 202/1000 | Loss: 0.00000716
Iteration 203/1000 | Loss: 0.00000715
Iteration 204/1000 | Loss: 0.00000715
Iteration 205/1000 | Loss: 0.00000715
Iteration 206/1000 | Loss: 0.00000715
Iteration 207/1000 | Loss: 0.00000715
Iteration 208/1000 | Loss: 0.00000715
Iteration 209/1000 | Loss: 0.00000715
Iteration 210/1000 | Loss: 0.00000715
Iteration 211/1000 | Loss: 0.00000715
Iteration 212/1000 | Loss: 0.00000715
Iteration 213/1000 | Loss: 0.00000715
Iteration 214/1000 | Loss: 0.00000715
Iteration 215/1000 | Loss: 0.00000715
Iteration 216/1000 | Loss: 0.00000715
Iteration 217/1000 | Loss: 0.00000715
Iteration 218/1000 | Loss: 0.00000715
Iteration 219/1000 | Loss: 0.00000715
Iteration 220/1000 | Loss: 0.00000715
Iteration 221/1000 | Loss: 0.00000715
Iteration 222/1000 | Loss: 0.00000715
Iteration 223/1000 | Loss: 0.00000715
Iteration 224/1000 | Loss: 0.00000715
Iteration 225/1000 | Loss: 0.00000715
Iteration 226/1000 | Loss: 0.00000715
Iteration 227/1000 | Loss: 0.00000715
Iteration 228/1000 | Loss: 0.00000715
Iteration 229/1000 | Loss: 0.00000715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [7.150807505240664e-06, 7.150807505240664e-06, 7.150807505240664e-06, 7.150807505240664e-06, 7.150807505240664e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.150807505240664e-06

Optimization complete. Final v2v error: 2.244479179382324 mm

Highest mean error: 2.6718358993530273 mm for frame 29

Lowest mean error: 1.9886823892593384 mm for frame 42

Saving results

Total time: 36.22744870185852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475343
Iteration 2/25 | Loss: 0.00118259
Iteration 3/25 | Loss: 0.00105650
Iteration 4/25 | Loss: 0.00104174
Iteration 5/25 | Loss: 0.00103641
Iteration 6/25 | Loss: 0.00103549
Iteration 7/25 | Loss: 0.00103549
Iteration 8/25 | Loss: 0.00103549
Iteration 9/25 | Loss: 0.00103549
Iteration 10/25 | Loss: 0.00103549
Iteration 11/25 | Loss: 0.00103549
Iteration 12/25 | Loss: 0.00103549
Iteration 13/25 | Loss: 0.00103549
Iteration 14/25 | Loss: 0.00103549
Iteration 15/25 | Loss: 0.00103549
Iteration 16/25 | Loss: 0.00103549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010354880942031741, 0.0010354880942031741, 0.0010354880942031741, 0.0010354880942031741, 0.0010354880942031741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010354880942031741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13119125
Iteration 2/25 | Loss: 0.00123088
Iteration 3/25 | Loss: 0.00123087
Iteration 4/25 | Loss: 0.00123087
Iteration 5/25 | Loss: 0.00123087
Iteration 6/25 | Loss: 0.00123087
Iteration 7/25 | Loss: 0.00123087
Iteration 8/25 | Loss: 0.00123087
Iteration 9/25 | Loss: 0.00123087
Iteration 10/25 | Loss: 0.00123087
Iteration 11/25 | Loss: 0.00123087
Iteration 12/25 | Loss: 0.00123087
Iteration 13/25 | Loss: 0.00123087
Iteration 14/25 | Loss: 0.00123087
Iteration 15/25 | Loss: 0.00123087
Iteration 16/25 | Loss: 0.00123087
Iteration 17/25 | Loss: 0.00123087
Iteration 18/25 | Loss: 0.00123087
Iteration 19/25 | Loss: 0.00123087
Iteration 20/25 | Loss: 0.00123087
Iteration 21/25 | Loss: 0.00123087
Iteration 22/25 | Loss: 0.00123087
Iteration 23/25 | Loss: 0.00123087
Iteration 24/25 | Loss: 0.00123087
Iteration 25/25 | Loss: 0.00123087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123087
Iteration 2/1000 | Loss: 0.00004250
Iteration 3/1000 | Loss: 0.00002775
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001782
Iteration 7/1000 | Loss: 0.00001712
Iteration 8/1000 | Loss: 0.00001677
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001641
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001598
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001589
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001584
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001582
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001581
Iteration 38/1000 | Loss: 0.00001581
Iteration 39/1000 | Loss: 0.00001580
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001570
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001569
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001568
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001556
Iteration 122/1000 | Loss: 0.00001556
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001556
Iteration 125/1000 | Loss: 0.00001556
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001556
Iteration 130/1000 | Loss: 0.00001556
Iteration 131/1000 | Loss: 0.00001556
Iteration 132/1000 | Loss: 0.00001556
Iteration 133/1000 | Loss: 0.00001556
Iteration 134/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.555697417643387e-05, 1.555697417643387e-05, 1.555697417643387e-05, 1.555697417643387e-05, 1.555697417643387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555697417643387e-05

Optimization complete. Final v2v error: 3.337998390197754 mm

Highest mean error: 4.334003448486328 mm for frame 66

Lowest mean error: 2.729513168334961 mm for frame 114

Saving results

Total time: 41.51706051826477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993908
Iteration 2/25 | Loss: 0.00280600
Iteration 3/25 | Loss: 0.00182735
Iteration 4/25 | Loss: 0.00167794
Iteration 5/25 | Loss: 0.00161059
Iteration 6/25 | Loss: 0.00154544
Iteration 7/25 | Loss: 0.00149142
Iteration 8/25 | Loss: 0.00143675
Iteration 9/25 | Loss: 0.00140570
Iteration 10/25 | Loss: 0.00139403
Iteration 11/25 | Loss: 0.00137930
Iteration 12/25 | Loss: 0.00139170
Iteration 13/25 | Loss: 0.00138039
Iteration 14/25 | Loss: 0.00137726
Iteration 15/25 | Loss: 0.00136294
Iteration 16/25 | Loss: 0.00137003
Iteration 17/25 | Loss: 0.00136090
Iteration 18/25 | Loss: 0.00134862
Iteration 19/25 | Loss: 0.00134975
Iteration 20/25 | Loss: 0.00134876
Iteration 21/25 | Loss: 0.00134417
Iteration 22/25 | Loss: 0.00134915
Iteration 23/25 | Loss: 0.00134559
Iteration 24/25 | Loss: 0.00133810
Iteration 25/25 | Loss: 0.00133711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74957728
Iteration 2/25 | Loss: 0.00169026
Iteration 3/25 | Loss: 0.00169026
Iteration 4/25 | Loss: 0.00169026
Iteration 5/25 | Loss: 0.00169026
Iteration 6/25 | Loss: 0.00169026
Iteration 7/25 | Loss: 0.00169026
Iteration 8/25 | Loss: 0.00169026
Iteration 9/25 | Loss: 0.00169026
Iteration 10/25 | Loss: 0.00169026
Iteration 11/25 | Loss: 0.00169026
Iteration 12/25 | Loss: 0.00169026
Iteration 13/25 | Loss: 0.00169026
Iteration 14/25 | Loss: 0.00169026
Iteration 15/25 | Loss: 0.00169026
Iteration 16/25 | Loss: 0.00169026
Iteration 17/25 | Loss: 0.00169026
Iteration 18/25 | Loss: 0.00169026
Iteration 19/25 | Loss: 0.00169026
Iteration 20/25 | Loss: 0.00169026
Iteration 21/25 | Loss: 0.00169026
Iteration 22/25 | Loss: 0.00169026
Iteration 23/25 | Loss: 0.00169026
Iteration 24/25 | Loss: 0.00169026
Iteration 25/25 | Loss: 0.00169026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169026
Iteration 2/1000 | Loss: 0.00025767
Iteration 3/1000 | Loss: 0.00019127
Iteration 4/1000 | Loss: 0.00017202
Iteration 5/1000 | Loss: 0.00015510
Iteration 6/1000 | Loss: 0.00014330
Iteration 7/1000 | Loss: 0.00013732
Iteration 8/1000 | Loss: 0.00013256
Iteration 9/1000 | Loss: 0.00012854
Iteration 10/1000 | Loss: 0.00012682
Iteration 11/1000 | Loss: 0.00012544
Iteration 12/1000 | Loss: 0.00144659
Iteration 13/1000 | Loss: 0.00444491
Iteration 14/1000 | Loss: 0.00059599
Iteration 15/1000 | Loss: 0.00024652
Iteration 16/1000 | Loss: 0.00017912
Iteration 17/1000 | Loss: 0.00014489
Iteration 18/1000 | Loss: 0.00012104
Iteration 19/1000 | Loss: 0.00010222
Iteration 20/1000 | Loss: 0.00008551
Iteration 21/1000 | Loss: 0.00007953
Iteration 22/1000 | Loss: 0.00007543
Iteration 23/1000 | Loss: 0.00007200
Iteration 24/1000 | Loss: 0.00006921
Iteration 25/1000 | Loss: 0.00006748
Iteration 26/1000 | Loss: 0.00006617
Iteration 27/1000 | Loss: 0.00006514
Iteration 28/1000 | Loss: 0.00006409
Iteration 29/1000 | Loss: 0.00006335
Iteration 30/1000 | Loss: 0.00006284
Iteration 31/1000 | Loss: 0.00006255
Iteration 32/1000 | Loss: 0.00006231
Iteration 33/1000 | Loss: 0.00006217
Iteration 34/1000 | Loss: 0.00006215
Iteration 35/1000 | Loss: 0.00006200
Iteration 36/1000 | Loss: 0.00006186
Iteration 37/1000 | Loss: 0.00006173
Iteration 38/1000 | Loss: 0.00006160
Iteration 39/1000 | Loss: 0.00006154
Iteration 40/1000 | Loss: 0.00006144
Iteration 41/1000 | Loss: 0.00006143
Iteration 42/1000 | Loss: 0.00006143
Iteration 43/1000 | Loss: 0.00006143
Iteration 44/1000 | Loss: 0.00006143
Iteration 45/1000 | Loss: 0.00006141
Iteration 46/1000 | Loss: 0.00006140
Iteration 47/1000 | Loss: 0.00006138
Iteration 48/1000 | Loss: 0.00006138
Iteration 49/1000 | Loss: 0.00006137
Iteration 50/1000 | Loss: 0.00006136
Iteration 51/1000 | Loss: 0.00006136
Iteration 52/1000 | Loss: 0.00006135
Iteration 53/1000 | Loss: 0.00006135
Iteration 54/1000 | Loss: 0.00006135
Iteration 55/1000 | Loss: 0.00006134
Iteration 56/1000 | Loss: 0.00006133
Iteration 57/1000 | Loss: 0.00006132
Iteration 58/1000 | Loss: 0.00006131
Iteration 59/1000 | Loss: 0.00006131
Iteration 60/1000 | Loss: 0.00006131
Iteration 61/1000 | Loss: 0.00006130
Iteration 62/1000 | Loss: 0.00006130
Iteration 63/1000 | Loss: 0.00006130
Iteration 64/1000 | Loss: 0.00006130
Iteration 65/1000 | Loss: 0.00006130
Iteration 66/1000 | Loss: 0.00006130
Iteration 67/1000 | Loss: 0.00006130
Iteration 68/1000 | Loss: 0.00006129
Iteration 69/1000 | Loss: 0.00006129
Iteration 70/1000 | Loss: 0.00006128
Iteration 71/1000 | Loss: 0.00006128
Iteration 72/1000 | Loss: 0.00006128
Iteration 73/1000 | Loss: 0.00006128
Iteration 74/1000 | Loss: 0.00006128
Iteration 75/1000 | Loss: 0.00006128
Iteration 76/1000 | Loss: 0.00006127
Iteration 77/1000 | Loss: 0.00006127
Iteration 78/1000 | Loss: 0.00006127
Iteration 79/1000 | Loss: 0.00006127
Iteration 80/1000 | Loss: 0.00006127
Iteration 81/1000 | Loss: 0.00006126
Iteration 82/1000 | Loss: 0.00006126
Iteration 83/1000 | Loss: 0.00006126
Iteration 84/1000 | Loss: 0.00006125
Iteration 85/1000 | Loss: 0.00006125
Iteration 86/1000 | Loss: 0.00006125
Iteration 87/1000 | Loss: 0.00006124
Iteration 88/1000 | Loss: 0.00006124
Iteration 89/1000 | Loss: 0.00006124
Iteration 90/1000 | Loss: 0.00006124
Iteration 91/1000 | Loss: 0.00006124
Iteration 92/1000 | Loss: 0.00006124
Iteration 93/1000 | Loss: 0.00006123
Iteration 94/1000 | Loss: 0.00006123
Iteration 95/1000 | Loss: 0.00006123
Iteration 96/1000 | Loss: 0.00006123
Iteration 97/1000 | Loss: 0.00006122
Iteration 98/1000 | Loss: 0.00006122
Iteration 99/1000 | Loss: 0.00006122
Iteration 100/1000 | Loss: 0.00006122
Iteration 101/1000 | Loss: 0.00006122
Iteration 102/1000 | Loss: 0.00006122
Iteration 103/1000 | Loss: 0.00006122
Iteration 104/1000 | Loss: 0.00006122
Iteration 105/1000 | Loss: 0.00006121
Iteration 106/1000 | Loss: 0.00006121
Iteration 107/1000 | Loss: 0.00006121
Iteration 108/1000 | Loss: 0.00006121
Iteration 109/1000 | Loss: 0.00006121
Iteration 110/1000 | Loss: 0.00006121
Iteration 111/1000 | Loss: 0.00006121
Iteration 112/1000 | Loss: 0.00006121
Iteration 113/1000 | Loss: 0.00006121
Iteration 114/1000 | Loss: 0.00006121
Iteration 115/1000 | Loss: 0.00006121
Iteration 116/1000 | Loss: 0.00006121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [6.120815669419244e-05, 6.120815669419244e-05, 6.120815669419244e-05, 6.120815669419244e-05, 6.120815669419244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.120815669419244e-05

Optimization complete. Final v2v error: 5.656946182250977 mm

Highest mean error: 7.035836219787598 mm for frame 131

Lowest mean error: 3.8575775623321533 mm for frame 62

Saving results

Total time: 121.15307331085205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092079
Iteration 2/25 | Loss: 0.00362463
Iteration 3/25 | Loss: 0.00286739
Iteration 4/25 | Loss: 0.00289160
Iteration 5/25 | Loss: 0.00294863
Iteration 6/25 | Loss: 0.00240167
Iteration 7/25 | Loss: 0.00221786
Iteration 8/25 | Loss: 0.00201735
Iteration 9/25 | Loss: 0.00190618
Iteration 10/25 | Loss: 0.00180949
Iteration 11/25 | Loss: 0.00180508
Iteration 12/25 | Loss: 0.00177206
Iteration 13/25 | Loss: 0.00169247
Iteration 14/25 | Loss: 0.00165779
Iteration 15/25 | Loss: 0.00163752
Iteration 16/25 | Loss: 0.00162728
Iteration 17/25 | Loss: 0.00163271
Iteration 18/25 | Loss: 0.00162646
Iteration 19/25 | Loss: 0.00161120
Iteration 20/25 | Loss: 0.00161382
Iteration 21/25 | Loss: 0.00161779
Iteration 22/25 | Loss: 0.00164863
Iteration 23/25 | Loss: 0.00158674
Iteration 24/25 | Loss: 0.00157725
Iteration 25/25 | Loss: 0.00157530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06693506
Iteration 2/25 | Loss: 0.00481229
Iteration 3/25 | Loss: 0.00481229
Iteration 4/25 | Loss: 0.00481229
Iteration 5/25 | Loss: 0.00481229
Iteration 6/25 | Loss: 0.00481229
Iteration 7/25 | Loss: 0.00481229
Iteration 8/25 | Loss: 0.00481229
Iteration 9/25 | Loss: 0.00481228
Iteration 10/25 | Loss: 0.00481228
Iteration 11/25 | Loss: 0.00481228
Iteration 12/25 | Loss: 0.00481228
Iteration 13/25 | Loss: 0.00481228
Iteration 14/25 | Loss: 0.00481228
Iteration 15/25 | Loss: 0.00481228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004812284372746944, 0.004812284372746944, 0.004812284372746944, 0.004812284372746944, 0.004812284372746944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004812284372746944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00481228
Iteration 2/1000 | Loss: 0.00161960
Iteration 3/1000 | Loss: 0.00246637
Iteration 4/1000 | Loss: 0.00224780
Iteration 5/1000 | Loss: 0.00061078
Iteration 6/1000 | Loss: 0.00055504
Iteration 7/1000 | Loss: 0.00138337
Iteration 8/1000 | Loss: 0.00086688
Iteration 9/1000 | Loss: 0.00034424
Iteration 10/1000 | Loss: 0.00024515
Iteration 11/1000 | Loss: 0.00047106
Iteration 12/1000 | Loss: 0.00023230
Iteration 13/1000 | Loss: 0.00021693
Iteration 14/1000 | Loss: 0.00021369
Iteration 15/1000 | Loss: 0.00020163
Iteration 16/1000 | Loss: 0.00396353
Iteration 17/1000 | Loss: 0.01048126
Iteration 18/1000 | Loss: 0.00487583
Iteration 19/1000 | Loss: 0.00189833
Iteration 20/1000 | Loss: 0.00121858
Iteration 21/1000 | Loss: 0.00203337
Iteration 22/1000 | Loss: 0.00057655
Iteration 23/1000 | Loss: 0.00132703
Iteration 24/1000 | Loss: 0.00024842
Iteration 25/1000 | Loss: 0.00049215
Iteration 26/1000 | Loss: 0.00112351
Iteration 27/1000 | Loss: 0.00030308
Iteration 28/1000 | Loss: 0.00026555
Iteration 29/1000 | Loss: 0.00134988
Iteration 30/1000 | Loss: 0.00089107
Iteration 31/1000 | Loss: 0.00191289
Iteration 32/1000 | Loss: 0.00140626
Iteration 33/1000 | Loss: 0.00187992
Iteration 34/1000 | Loss: 0.00067807
Iteration 35/1000 | Loss: 0.00036747
Iteration 36/1000 | Loss: 0.00021528
Iteration 37/1000 | Loss: 0.00016043
Iteration 38/1000 | Loss: 0.00014114
Iteration 39/1000 | Loss: 0.00008096
Iteration 40/1000 | Loss: 0.00006788
Iteration 41/1000 | Loss: 0.00006223
Iteration 42/1000 | Loss: 0.00018127
Iteration 43/1000 | Loss: 0.00006460
Iteration 44/1000 | Loss: 0.00005715
Iteration 45/1000 | Loss: 0.00034867
Iteration 46/1000 | Loss: 0.00017163
Iteration 47/1000 | Loss: 0.00035047
Iteration 48/1000 | Loss: 0.00083041
Iteration 49/1000 | Loss: 0.00081967
Iteration 50/1000 | Loss: 0.00017931
Iteration 51/1000 | Loss: 0.00015332
Iteration 52/1000 | Loss: 0.00005852
Iteration 53/1000 | Loss: 0.00007409
Iteration 54/1000 | Loss: 0.00005098
Iteration 55/1000 | Loss: 0.00004838
Iteration 56/1000 | Loss: 0.00004673
Iteration 57/1000 | Loss: 0.00004541
Iteration 58/1000 | Loss: 0.00004410
Iteration 59/1000 | Loss: 0.00004320
Iteration 60/1000 | Loss: 0.00004243
Iteration 61/1000 | Loss: 0.00027740
Iteration 62/1000 | Loss: 0.00011625
Iteration 63/1000 | Loss: 0.00025821
Iteration 64/1000 | Loss: 0.00008385
Iteration 65/1000 | Loss: 0.00013389
Iteration 66/1000 | Loss: 0.00024490
Iteration 67/1000 | Loss: 0.00006303
Iteration 68/1000 | Loss: 0.00006119
Iteration 69/1000 | Loss: 0.00006424
Iteration 70/1000 | Loss: 0.00006709
Iteration 71/1000 | Loss: 0.00004545
Iteration 72/1000 | Loss: 0.00004344
Iteration 73/1000 | Loss: 0.00005077
Iteration 74/1000 | Loss: 0.00004068
Iteration 75/1000 | Loss: 0.00007148
Iteration 76/1000 | Loss: 0.00006085
Iteration 77/1000 | Loss: 0.00004658
Iteration 78/1000 | Loss: 0.00004412
Iteration 79/1000 | Loss: 0.00006593
Iteration 80/1000 | Loss: 0.00007020
Iteration 81/1000 | Loss: 0.00008024
Iteration 82/1000 | Loss: 0.00006967
Iteration 83/1000 | Loss: 0.00007608
Iteration 84/1000 | Loss: 0.00007007
Iteration 85/1000 | Loss: 0.00007116
Iteration 86/1000 | Loss: 0.00005714
Iteration 87/1000 | Loss: 0.00005357
Iteration 88/1000 | Loss: 0.00004498
Iteration 89/1000 | Loss: 0.00004342
Iteration 90/1000 | Loss: 0.00004219
Iteration 91/1000 | Loss: 0.00004122
Iteration 92/1000 | Loss: 0.00005637
Iteration 93/1000 | Loss: 0.00005803
Iteration 94/1000 | Loss: 0.00006437
Iteration 95/1000 | Loss: 0.00005852
Iteration 96/1000 | Loss: 0.00004697
Iteration 97/1000 | Loss: 0.00004931
Iteration 98/1000 | Loss: 0.00004410
Iteration 99/1000 | Loss: 0.00006868
Iteration 100/1000 | Loss: 0.00005672
Iteration 101/1000 | Loss: 0.00006787
Iteration 102/1000 | Loss: 0.00025590
Iteration 103/1000 | Loss: 0.00006216
Iteration 104/1000 | Loss: 0.00005289
Iteration 105/1000 | Loss: 0.00004696
Iteration 106/1000 | Loss: 0.00005214
Iteration 107/1000 | Loss: 0.00004204
Iteration 108/1000 | Loss: 0.00004045
Iteration 109/1000 | Loss: 0.00005644
Iteration 110/1000 | Loss: 0.00004960
Iteration 111/1000 | Loss: 0.00003985
Iteration 112/1000 | Loss: 0.00003830
Iteration 113/1000 | Loss: 0.00003788
Iteration 114/1000 | Loss: 0.00005128
Iteration 115/1000 | Loss: 0.00004482
Iteration 116/1000 | Loss: 0.00004747
Iteration 117/1000 | Loss: 0.00004432
Iteration 118/1000 | Loss: 0.00005147
Iteration 119/1000 | Loss: 0.00005015
Iteration 120/1000 | Loss: 0.00004411
Iteration 121/1000 | Loss: 0.00004441
Iteration 122/1000 | Loss: 0.00004686
Iteration 123/1000 | Loss: 0.00004933
Iteration 124/1000 | Loss: 0.00004455
Iteration 125/1000 | Loss: 0.00004666
Iteration 126/1000 | Loss: 0.00004499
Iteration 127/1000 | Loss: 0.00005052
Iteration 128/1000 | Loss: 0.00004622
Iteration 129/1000 | Loss: 0.00004909
Iteration 130/1000 | Loss: 0.00004604
Iteration 131/1000 | Loss: 0.00004880
Iteration 132/1000 | Loss: 0.00004804
Iteration 133/1000 | Loss: 0.00004150
Iteration 134/1000 | Loss: 0.00003812
Iteration 135/1000 | Loss: 0.00004029
Iteration 136/1000 | Loss: 0.00004611
Iteration 137/1000 | Loss: 0.00004203
Iteration 138/1000 | Loss: 0.00005446
Iteration 139/1000 | Loss: 0.00005741
Iteration 140/1000 | Loss: 0.00005085
Iteration 141/1000 | Loss: 0.00005315
Iteration 142/1000 | Loss: 0.00004843
Iteration 143/1000 | Loss: 0.00004713
Iteration 144/1000 | Loss: 0.00004942
Iteration 145/1000 | Loss: 0.00004433
Iteration 146/1000 | Loss: 0.00003871
Iteration 147/1000 | Loss: 0.00004728
Iteration 148/1000 | Loss: 0.00004534
Iteration 149/1000 | Loss: 0.00004547
Iteration 150/1000 | Loss: 0.00004476
Iteration 151/1000 | Loss: 0.00004127
Iteration 152/1000 | Loss: 0.00004775
Iteration 153/1000 | Loss: 0.00005371
Iteration 154/1000 | Loss: 0.00004834
Iteration 155/1000 | Loss: 0.00004249
Iteration 156/1000 | Loss: 0.00003808
Iteration 157/1000 | Loss: 0.00004430
Iteration 158/1000 | Loss: 0.00004272
Iteration 159/1000 | Loss: 0.00005305
Iteration 160/1000 | Loss: 0.00004696
Iteration 161/1000 | Loss: 0.00005022
Iteration 162/1000 | Loss: 0.00004449
Iteration 163/1000 | Loss: 0.00005275
Iteration 164/1000 | Loss: 0.00004494
Iteration 165/1000 | Loss: 0.00005333
Iteration 166/1000 | Loss: 0.00004779
Iteration 167/1000 | Loss: 0.00003751
Iteration 168/1000 | Loss: 0.00004587
Iteration 169/1000 | Loss: 0.00005571
Iteration 170/1000 | Loss: 0.00004413
Iteration 171/1000 | Loss: 0.00004665
Iteration 172/1000 | Loss: 0.00004550
Iteration 173/1000 | Loss: 0.00004633
Iteration 174/1000 | Loss: 0.00004535
Iteration 175/1000 | Loss: 0.00004468
Iteration 176/1000 | Loss: 0.00004530
Iteration 177/1000 | Loss: 0.00005012
Iteration 178/1000 | Loss: 0.00004902
Iteration 179/1000 | Loss: 0.00005100
Iteration 180/1000 | Loss: 0.00005184
Iteration 181/1000 | Loss: 0.00004169
Iteration 182/1000 | Loss: 0.00004299
Iteration 183/1000 | Loss: 0.00004595
Iteration 184/1000 | Loss: 0.00004931
Iteration 185/1000 | Loss: 0.00004578
Iteration 186/1000 | Loss: 0.00004902
Iteration 187/1000 | Loss: 0.00004896
Iteration 188/1000 | Loss: 0.00004908
Iteration 189/1000 | Loss: 0.00004512
Iteration 190/1000 | Loss: 0.00004861
Iteration 191/1000 | Loss: 0.00004513
Iteration 192/1000 | Loss: 0.00004729
Iteration 193/1000 | Loss: 0.00005182
Iteration 194/1000 | Loss: 0.00005019
Iteration 195/1000 | Loss: 0.00005030
Iteration 196/1000 | Loss: 0.00005279
Iteration 197/1000 | Loss: 0.00005001
Iteration 198/1000 | Loss: 0.00005256
Iteration 199/1000 | Loss: 0.00004810
Iteration 200/1000 | Loss: 0.00005311
Iteration 201/1000 | Loss: 0.00004819
Iteration 202/1000 | Loss: 0.00005137
Iteration 203/1000 | Loss: 0.00004427
Iteration 204/1000 | Loss: 0.00004878
Iteration 205/1000 | Loss: 0.00004886
Iteration 206/1000 | Loss: 0.00004008
Iteration 207/1000 | Loss: 0.00005164
Iteration 208/1000 | Loss: 0.00004847
Iteration 209/1000 | Loss: 0.00004419
Iteration 210/1000 | Loss: 0.00004817
Iteration 211/1000 | Loss: 0.00004752
Iteration 212/1000 | Loss: 0.00004672
Iteration 213/1000 | Loss: 0.00004880
Iteration 214/1000 | Loss: 0.00004581
Iteration 215/1000 | Loss: 0.00005540
Iteration 216/1000 | Loss: 0.00005397
Iteration 217/1000 | Loss: 0.00004166
Iteration 218/1000 | Loss: 0.00003971
Iteration 219/1000 | Loss: 0.00003881
Iteration 220/1000 | Loss: 0.00003834
Iteration 221/1000 | Loss: 0.00003790
Iteration 222/1000 | Loss: 0.00003750
Iteration 223/1000 | Loss: 0.00003735
Iteration 224/1000 | Loss: 0.00003734
Iteration 225/1000 | Loss: 0.00003733
Iteration 226/1000 | Loss: 0.00003732
Iteration 227/1000 | Loss: 0.00003731
Iteration 228/1000 | Loss: 0.00003731
Iteration 229/1000 | Loss: 0.00003731
Iteration 230/1000 | Loss: 0.00003730
Iteration 231/1000 | Loss: 0.00003730
Iteration 232/1000 | Loss: 0.00003727
Iteration 233/1000 | Loss: 0.00003724
Iteration 234/1000 | Loss: 0.00003722
Iteration 235/1000 | Loss: 0.00003722
Iteration 236/1000 | Loss: 0.00003721
Iteration 237/1000 | Loss: 0.00003721
Iteration 238/1000 | Loss: 0.00003721
Iteration 239/1000 | Loss: 0.00003721
Iteration 240/1000 | Loss: 0.00003721
Iteration 241/1000 | Loss: 0.00003720
Iteration 242/1000 | Loss: 0.00003720
Iteration 243/1000 | Loss: 0.00003718
Iteration 244/1000 | Loss: 0.00003715
Iteration 245/1000 | Loss: 0.00003708
Iteration 246/1000 | Loss: 0.00003708
Iteration 247/1000 | Loss: 0.00003708
Iteration 248/1000 | Loss: 0.00003708
Iteration 249/1000 | Loss: 0.00003708
Iteration 250/1000 | Loss: 0.00003708
Iteration 251/1000 | Loss: 0.00003708
Iteration 252/1000 | Loss: 0.00003708
Iteration 253/1000 | Loss: 0.00003708
Iteration 254/1000 | Loss: 0.00003708
Iteration 255/1000 | Loss: 0.00003707
Iteration 256/1000 | Loss: 0.00003707
Iteration 257/1000 | Loss: 0.00003707
Iteration 258/1000 | Loss: 0.00003707
Iteration 259/1000 | Loss: 0.00003707
Iteration 260/1000 | Loss: 0.00003707
Iteration 261/1000 | Loss: 0.00003706
Iteration 262/1000 | Loss: 0.00003706
Iteration 263/1000 | Loss: 0.00003706
Iteration 264/1000 | Loss: 0.00003706
Iteration 265/1000 | Loss: 0.00003705
Iteration 266/1000 | Loss: 0.00003705
Iteration 267/1000 | Loss: 0.00003705
Iteration 268/1000 | Loss: 0.00003705
Iteration 269/1000 | Loss: 0.00003705
Iteration 270/1000 | Loss: 0.00003705
Iteration 271/1000 | Loss: 0.00003705
Iteration 272/1000 | Loss: 0.00003705
Iteration 273/1000 | Loss: 0.00003704
Iteration 274/1000 | Loss: 0.00003704
Iteration 275/1000 | Loss: 0.00003704
Iteration 276/1000 | Loss: 0.00003704
Iteration 277/1000 | Loss: 0.00003703
Iteration 278/1000 | Loss: 0.00003703
Iteration 279/1000 | Loss: 0.00003703
Iteration 280/1000 | Loss: 0.00003703
Iteration 281/1000 | Loss: 0.00003703
Iteration 282/1000 | Loss: 0.00003702
Iteration 283/1000 | Loss: 0.00003702
Iteration 284/1000 | Loss: 0.00003702
Iteration 285/1000 | Loss: 0.00003702
Iteration 286/1000 | Loss: 0.00003702
Iteration 287/1000 | Loss: 0.00003702
Iteration 288/1000 | Loss: 0.00003702
Iteration 289/1000 | Loss: 0.00003702
Iteration 290/1000 | Loss: 0.00003702
Iteration 291/1000 | Loss: 0.00003702
Iteration 292/1000 | Loss: 0.00003702
Iteration 293/1000 | Loss: 0.00003702
Iteration 294/1000 | Loss: 0.00003702
Iteration 295/1000 | Loss: 0.00003702
Iteration 296/1000 | Loss: 0.00003701
Iteration 297/1000 | Loss: 0.00003701
Iteration 298/1000 | Loss: 0.00003701
Iteration 299/1000 | Loss: 0.00003701
Iteration 300/1000 | Loss: 0.00003701
Iteration 301/1000 | Loss: 0.00003701
Iteration 302/1000 | Loss: 0.00003701
Iteration 303/1000 | Loss: 0.00003701
Iteration 304/1000 | Loss: 0.00003701
Iteration 305/1000 | Loss: 0.00003700
Iteration 306/1000 | Loss: 0.00003700
Iteration 307/1000 | Loss: 0.00003700
Iteration 308/1000 | Loss: 0.00003700
Iteration 309/1000 | Loss: 0.00003700
Iteration 310/1000 | Loss: 0.00003700
Iteration 311/1000 | Loss: 0.00003700
Iteration 312/1000 | Loss: 0.00003700
Iteration 313/1000 | Loss: 0.00003700
Iteration 314/1000 | Loss: 0.00003700
Iteration 315/1000 | Loss: 0.00003700
Iteration 316/1000 | Loss: 0.00003700
Iteration 317/1000 | Loss: 0.00003700
Iteration 318/1000 | Loss: 0.00003700
Iteration 319/1000 | Loss: 0.00003700
Iteration 320/1000 | Loss: 0.00003700
Iteration 321/1000 | Loss: 0.00003699
Iteration 322/1000 | Loss: 0.00003699
Iteration 323/1000 | Loss: 0.00003699
Iteration 324/1000 | Loss: 0.00003699
Iteration 325/1000 | Loss: 0.00003699
Iteration 326/1000 | Loss: 0.00003699
Iteration 327/1000 | Loss: 0.00003699
Iteration 328/1000 | Loss: 0.00003699
Iteration 329/1000 | Loss: 0.00003699
Iteration 330/1000 | Loss: 0.00003699
Iteration 331/1000 | Loss: 0.00003699
Iteration 332/1000 | Loss: 0.00003699
Iteration 333/1000 | Loss: 0.00003699
Iteration 334/1000 | Loss: 0.00003699
Iteration 335/1000 | Loss: 0.00003699
Iteration 336/1000 | Loss: 0.00003699
Iteration 337/1000 | Loss: 0.00003699
Iteration 338/1000 | Loss: 0.00003699
Iteration 339/1000 | Loss: 0.00003699
Iteration 340/1000 | Loss: 0.00003699
Iteration 341/1000 | Loss: 0.00003699
Iteration 342/1000 | Loss: 0.00003699
Iteration 343/1000 | Loss: 0.00003699
Iteration 344/1000 | Loss: 0.00003699
Iteration 345/1000 | Loss: 0.00003699
Iteration 346/1000 | Loss: 0.00003699
Iteration 347/1000 | Loss: 0.00003699
Iteration 348/1000 | Loss: 0.00003699
Iteration 349/1000 | Loss: 0.00003699
Iteration 350/1000 | Loss: 0.00003699
Iteration 351/1000 | Loss: 0.00003699
Iteration 352/1000 | Loss: 0.00003699
Iteration 353/1000 | Loss: 0.00003699
Iteration 354/1000 | Loss: 0.00003699
Iteration 355/1000 | Loss: 0.00003699
Iteration 356/1000 | Loss: 0.00003699
Iteration 357/1000 | Loss: 0.00003699
Iteration 358/1000 | Loss: 0.00003699
Iteration 359/1000 | Loss: 0.00003699
Iteration 360/1000 | Loss: 0.00003699
Iteration 361/1000 | Loss: 0.00003699
Iteration 362/1000 | Loss: 0.00003699
Iteration 363/1000 | Loss: 0.00003699
Iteration 364/1000 | Loss: 0.00003699
Iteration 365/1000 | Loss: 0.00003699
Iteration 366/1000 | Loss: 0.00003699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 366. Stopping optimization.
Last 5 losses: [3.6988196370657533e-05, 3.6988196370657533e-05, 3.6988196370657533e-05, 3.6988196370657533e-05, 3.6988196370657533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6988196370657533e-05

Optimization complete. Final v2v error: 4.639678955078125 mm

Highest mean error: 9.987993240356445 mm for frame 60

Lowest mean error: 4.214052200317383 mm for frame 45

Saving results

Total time: 370.1540369987488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056253
Iteration 2/25 | Loss: 0.00229409
Iteration 3/25 | Loss: 0.00171063
Iteration 4/25 | Loss: 0.00151674
Iteration 5/25 | Loss: 0.00142309
Iteration 6/25 | Loss: 0.00128433
Iteration 7/25 | Loss: 0.00116459
Iteration 8/25 | Loss: 0.00113456
Iteration 9/25 | Loss: 0.00110747
Iteration 10/25 | Loss: 0.00107621
Iteration 11/25 | Loss: 0.00107380
Iteration 12/25 | Loss: 0.00106126
Iteration 13/25 | Loss: 0.00105503
Iteration 14/25 | Loss: 0.00105665
Iteration 15/25 | Loss: 0.00105443
Iteration 16/25 | Loss: 0.00105566
Iteration 17/25 | Loss: 0.00105127
Iteration 18/25 | Loss: 0.00104274
Iteration 19/25 | Loss: 0.00103995
Iteration 20/25 | Loss: 0.00103904
Iteration 21/25 | Loss: 0.00104563
Iteration 22/25 | Loss: 0.00104261
Iteration 23/25 | Loss: 0.00103849
Iteration 24/25 | Loss: 0.00104232
Iteration 25/25 | Loss: 0.00104171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41143382
Iteration 2/25 | Loss: 0.00130982
Iteration 3/25 | Loss: 0.00130981
Iteration 4/25 | Loss: 0.00125483
Iteration 5/25 | Loss: 0.00125483
Iteration 6/25 | Loss: 0.00125483
Iteration 7/25 | Loss: 0.00125483
Iteration 8/25 | Loss: 0.00125483
Iteration 9/25 | Loss: 0.00125483
Iteration 10/25 | Loss: 0.00125483
Iteration 11/25 | Loss: 0.00125483
Iteration 12/25 | Loss: 0.00125483
Iteration 13/25 | Loss: 0.00125483
Iteration 14/25 | Loss: 0.00125483
Iteration 15/25 | Loss: 0.00125483
Iteration 16/25 | Loss: 0.00125483
Iteration 17/25 | Loss: 0.00125483
Iteration 18/25 | Loss: 0.00125483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012548312079161406, 0.0012548312079161406, 0.0012548312079161406, 0.0012548312079161406, 0.0012548312079161406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012548312079161406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125483
Iteration 2/1000 | Loss: 0.00019194
Iteration 3/1000 | Loss: 0.00012855
Iteration 4/1000 | Loss: 0.00013044
Iteration 5/1000 | Loss: 0.00014197
Iteration 6/1000 | Loss: 0.00011401
Iteration 7/1000 | Loss: 0.00041810
Iteration 8/1000 | Loss: 0.00028210
Iteration 9/1000 | Loss: 0.00008649
Iteration 10/1000 | Loss: 0.00015170
Iteration 11/1000 | Loss: 0.00010489
Iteration 12/1000 | Loss: 0.00012090
Iteration 13/1000 | Loss: 0.00026234
Iteration 14/1000 | Loss: 0.00028074
Iteration 15/1000 | Loss: 0.00016833
Iteration 16/1000 | Loss: 0.00039263
Iteration 17/1000 | Loss: 0.00010922
Iteration 18/1000 | Loss: 0.00012425
Iteration 19/1000 | Loss: 0.00010158
Iteration 20/1000 | Loss: 0.00017250
Iteration 21/1000 | Loss: 0.00005702
Iteration 22/1000 | Loss: 0.00012020
Iteration 23/1000 | Loss: 0.00010685
Iteration 24/1000 | Loss: 0.00026545
Iteration 25/1000 | Loss: 0.00006573
Iteration 26/1000 | Loss: 0.00008295
Iteration 27/1000 | Loss: 0.00011101
Iteration 28/1000 | Loss: 0.00009857
Iteration 29/1000 | Loss: 0.00006699
Iteration 30/1000 | Loss: 0.00005332
Iteration 31/1000 | Loss: 0.00004045
Iteration 32/1000 | Loss: 0.00003476
Iteration 33/1000 | Loss: 0.00002300
Iteration 34/1000 | Loss: 0.00038071
Iteration 35/1000 | Loss: 0.00025258
Iteration 36/1000 | Loss: 0.00010874
Iteration 37/1000 | Loss: 0.00018860
Iteration 38/1000 | Loss: 0.00004640
Iteration 39/1000 | Loss: 0.00004104
Iteration 40/1000 | Loss: 0.00003194
Iteration 41/1000 | Loss: 0.00002633
Iteration 42/1000 | Loss: 0.00003333
Iteration 43/1000 | Loss: 0.00049735
Iteration 44/1000 | Loss: 0.00002672
Iteration 45/1000 | Loss: 0.00003137
Iteration 46/1000 | Loss: 0.00043030
Iteration 47/1000 | Loss: 0.00016741
Iteration 48/1000 | Loss: 0.00005646
Iteration 49/1000 | Loss: 0.00029691
Iteration 50/1000 | Loss: 0.00004378
Iteration 51/1000 | Loss: 0.00011095
Iteration 52/1000 | Loss: 0.00004380
Iteration 53/1000 | Loss: 0.00004281
Iteration 54/1000 | Loss: 0.00029079
Iteration 55/1000 | Loss: 0.00028339
Iteration 56/1000 | Loss: 0.00010819
Iteration 57/1000 | Loss: 0.00026253
Iteration 58/1000 | Loss: 0.00040950
Iteration 59/1000 | Loss: 0.00024402
Iteration 60/1000 | Loss: 0.00045322
Iteration 61/1000 | Loss: 0.00016633
Iteration 62/1000 | Loss: 0.00003471
Iteration 63/1000 | Loss: 0.00015162
Iteration 64/1000 | Loss: 0.00008542
Iteration 65/1000 | Loss: 0.00014270
Iteration 66/1000 | Loss: 0.00004860
Iteration 67/1000 | Loss: 0.00005248
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001942
Iteration 70/1000 | Loss: 0.00002149
Iteration 71/1000 | Loss: 0.00003218
Iteration 72/1000 | Loss: 0.00002879
Iteration 73/1000 | Loss: 0.00002010
Iteration 74/1000 | Loss: 0.00009002
Iteration 75/1000 | Loss: 0.00026395
Iteration 76/1000 | Loss: 0.00017417
Iteration 77/1000 | Loss: 0.00042689
Iteration 78/1000 | Loss: 0.00008256
Iteration 79/1000 | Loss: 0.00026833
Iteration 80/1000 | Loss: 0.00004753
Iteration 81/1000 | Loss: 0.00041805
Iteration 82/1000 | Loss: 0.00041061
Iteration 83/1000 | Loss: 0.00007934
Iteration 84/1000 | Loss: 0.00003982
Iteration 85/1000 | Loss: 0.00008801
Iteration 86/1000 | Loss: 0.00002299
Iteration 87/1000 | Loss: 0.00003126
Iteration 88/1000 | Loss: 0.00034291
Iteration 89/1000 | Loss: 0.00003523
Iteration 90/1000 | Loss: 0.00002671
Iteration 91/1000 | Loss: 0.00008359
Iteration 92/1000 | Loss: 0.00078770
Iteration 93/1000 | Loss: 0.00003952
Iteration 94/1000 | Loss: 0.00003345
Iteration 95/1000 | Loss: 0.00002411
Iteration 96/1000 | Loss: 0.00004878
Iteration 97/1000 | Loss: 0.00002021
Iteration 98/1000 | Loss: 0.00002077
Iteration 99/1000 | Loss: 0.00002269
Iteration 100/1000 | Loss: 0.00002806
Iteration 101/1000 | Loss: 0.00007796
Iteration 102/1000 | Loss: 0.00003213
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001816
Iteration 105/1000 | Loss: 0.00020746
Iteration 106/1000 | Loss: 0.00024304
Iteration 107/1000 | Loss: 0.00024746
Iteration 108/1000 | Loss: 0.00012500
Iteration 109/1000 | Loss: 0.00007957
Iteration 110/1000 | Loss: 0.00002206
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001713
Iteration 114/1000 | Loss: 0.00004189
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00004343
Iteration 117/1000 | Loss: 0.00003083
Iteration 118/1000 | Loss: 0.00003714
Iteration 119/1000 | Loss: 0.00003050
Iteration 120/1000 | Loss: 0.00003559
Iteration 121/1000 | Loss: 0.00003152
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00002705
Iteration 124/1000 | Loss: 0.00014935
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001286
Iteration 133/1000 | Loss: 0.00001185
Iteration 134/1000 | Loss: 0.00001334
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001150
Iteration 145/1000 | Loss: 0.00001150
Iteration 146/1000 | Loss: 0.00001150
Iteration 147/1000 | Loss: 0.00001150
Iteration 148/1000 | Loss: 0.00001150
Iteration 149/1000 | Loss: 0.00001149
Iteration 150/1000 | Loss: 0.00001149
Iteration 151/1000 | Loss: 0.00001149
Iteration 152/1000 | Loss: 0.00001148
Iteration 153/1000 | Loss: 0.00001148
Iteration 154/1000 | Loss: 0.00001147
Iteration 155/1000 | Loss: 0.00001147
Iteration 156/1000 | Loss: 0.00001147
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Iteration 159/1000 | Loss: 0.00001146
Iteration 160/1000 | Loss: 0.00001146
Iteration 161/1000 | Loss: 0.00001146
Iteration 162/1000 | Loss: 0.00001145
Iteration 163/1000 | Loss: 0.00001144
Iteration 164/1000 | Loss: 0.00001144
Iteration 165/1000 | Loss: 0.00001144
Iteration 166/1000 | Loss: 0.00001144
Iteration 167/1000 | Loss: 0.00001144
Iteration 168/1000 | Loss: 0.00001144
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001205
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001141
Iteration 173/1000 | Loss: 0.00001141
Iteration 174/1000 | Loss: 0.00001141
Iteration 175/1000 | Loss: 0.00001141
Iteration 176/1000 | Loss: 0.00001141
Iteration 177/1000 | Loss: 0.00001141
Iteration 178/1000 | Loss: 0.00001141
Iteration 179/1000 | Loss: 0.00001141
Iteration 180/1000 | Loss: 0.00001141
Iteration 181/1000 | Loss: 0.00001141
Iteration 182/1000 | Loss: 0.00001141
Iteration 183/1000 | Loss: 0.00001141
Iteration 184/1000 | Loss: 0.00001141
Iteration 185/1000 | Loss: 0.00001141
Iteration 186/1000 | Loss: 0.00001141
Iteration 187/1000 | Loss: 0.00001141
Iteration 188/1000 | Loss: 0.00001141
Iteration 189/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.140911626862362e-05, 1.140911626862362e-05, 1.140911626862362e-05, 1.140911626862362e-05, 1.140911626862362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.140911626862362e-05

Optimization complete. Final v2v error: 2.877405881881714 mm

Highest mean error: 4.284373760223389 mm for frame 167

Lowest mean error: 2.6011345386505127 mm for frame 15

Saving results

Total time: 271.31075859069824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060290
Iteration 2/25 | Loss: 0.01060289
Iteration 3/25 | Loss: 0.00178596
Iteration 4/25 | Loss: 0.00144682
Iteration 5/25 | Loss: 0.00119710
Iteration 6/25 | Loss: 0.00111607
Iteration 7/25 | Loss: 0.00107816
Iteration 8/25 | Loss: 0.00105287
Iteration 9/25 | Loss: 0.00100217
Iteration 10/25 | Loss: 0.00098222
Iteration 11/25 | Loss: 0.00097059
Iteration 12/25 | Loss: 0.00096822
Iteration 13/25 | Loss: 0.00096985
Iteration 14/25 | Loss: 0.00096971
Iteration 15/25 | Loss: 0.00095905
Iteration 16/25 | Loss: 0.00095625
Iteration 17/25 | Loss: 0.00095544
Iteration 18/25 | Loss: 0.00095535
Iteration 19/25 | Loss: 0.00095535
Iteration 20/25 | Loss: 0.00095534
Iteration 21/25 | Loss: 0.00095534
Iteration 22/25 | Loss: 0.00095534
Iteration 23/25 | Loss: 0.00095534
Iteration 24/25 | Loss: 0.00095533
Iteration 25/25 | Loss: 0.00095533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38255191
Iteration 2/25 | Loss: 0.00117624
Iteration 3/25 | Loss: 0.00117623
Iteration 4/25 | Loss: 0.00117623
Iteration 5/25 | Loss: 0.00117623
Iteration 6/25 | Loss: 0.00117623
Iteration 7/25 | Loss: 0.00117623
Iteration 8/25 | Loss: 0.00117623
Iteration 9/25 | Loss: 0.00117623
Iteration 10/25 | Loss: 0.00117623
Iteration 11/25 | Loss: 0.00117623
Iteration 12/25 | Loss: 0.00117623
Iteration 13/25 | Loss: 0.00117623
Iteration 14/25 | Loss: 0.00117623
Iteration 15/25 | Loss: 0.00117623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011762315407395363, 0.0011762315407395363, 0.0011762315407395363, 0.0011762315407395363, 0.0011762315407395363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011762315407395363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117623
Iteration 2/1000 | Loss: 0.00003333
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001546
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001271
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001247
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001242
Iteration 55/1000 | Loss: 0.00001242
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001238
Iteration 80/1000 | Loss: 0.00001238
Iteration 81/1000 | Loss: 0.00001238
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001237
Iteration 84/1000 | Loss: 0.00001237
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001234
Iteration 116/1000 | Loss: 0.00001234
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001233
Iteration 121/1000 | Loss: 0.00001233
Iteration 122/1000 | Loss: 0.00001233
Iteration 123/1000 | Loss: 0.00001233
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001229
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001229
Iteration 147/1000 | Loss: 0.00001229
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001229
Iteration 151/1000 | Loss: 0.00001229
Iteration 152/1000 | Loss: 0.00001229
Iteration 153/1000 | Loss: 0.00001229
Iteration 154/1000 | Loss: 0.00001228
Iteration 155/1000 | Loss: 0.00001228
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001228
Iteration 163/1000 | Loss: 0.00001228
Iteration 164/1000 | Loss: 0.00001228
Iteration 165/1000 | Loss: 0.00001228
Iteration 166/1000 | Loss: 0.00001228
Iteration 167/1000 | Loss: 0.00001228
Iteration 168/1000 | Loss: 0.00001228
Iteration 169/1000 | Loss: 0.00001228
Iteration 170/1000 | Loss: 0.00001228
Iteration 171/1000 | Loss: 0.00001227
Iteration 172/1000 | Loss: 0.00001227
Iteration 173/1000 | Loss: 0.00001227
Iteration 174/1000 | Loss: 0.00001227
Iteration 175/1000 | Loss: 0.00001227
Iteration 176/1000 | Loss: 0.00001227
Iteration 177/1000 | Loss: 0.00001227
Iteration 178/1000 | Loss: 0.00001227
Iteration 179/1000 | Loss: 0.00001227
Iteration 180/1000 | Loss: 0.00001227
Iteration 181/1000 | Loss: 0.00001227
Iteration 182/1000 | Loss: 0.00001227
Iteration 183/1000 | Loss: 0.00001227
Iteration 184/1000 | Loss: 0.00001227
Iteration 185/1000 | Loss: 0.00001227
Iteration 186/1000 | Loss: 0.00001227
Iteration 187/1000 | Loss: 0.00001226
Iteration 188/1000 | Loss: 0.00001226
Iteration 189/1000 | Loss: 0.00001226
Iteration 190/1000 | Loss: 0.00001226
Iteration 191/1000 | Loss: 0.00001226
Iteration 192/1000 | Loss: 0.00001226
Iteration 193/1000 | Loss: 0.00001226
Iteration 194/1000 | Loss: 0.00001226
Iteration 195/1000 | Loss: 0.00001226
Iteration 196/1000 | Loss: 0.00001226
Iteration 197/1000 | Loss: 0.00001226
Iteration 198/1000 | Loss: 0.00001226
Iteration 199/1000 | Loss: 0.00001226
Iteration 200/1000 | Loss: 0.00001226
Iteration 201/1000 | Loss: 0.00001226
Iteration 202/1000 | Loss: 0.00001226
Iteration 203/1000 | Loss: 0.00001226
Iteration 204/1000 | Loss: 0.00001226
Iteration 205/1000 | Loss: 0.00001226
Iteration 206/1000 | Loss: 0.00001226
Iteration 207/1000 | Loss: 0.00001226
Iteration 208/1000 | Loss: 0.00001226
Iteration 209/1000 | Loss: 0.00001226
Iteration 210/1000 | Loss: 0.00001226
Iteration 211/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.2257429261808284e-05, 1.2257429261808284e-05, 1.2257429261808284e-05, 1.2257429261808284e-05, 1.2257429261808284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2257429261808284e-05

Optimization complete. Final v2v error: 2.6916110515594482 mm

Highest mean error: 11.218544960021973 mm for frame 174

Lowest mean error: 2.28182053565979 mm for frame 29

Saving results

Total time: 61.45564150810242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949881
Iteration 2/25 | Loss: 0.00123067
Iteration 3/25 | Loss: 0.00105899
Iteration 4/25 | Loss: 0.00102549
Iteration 5/25 | Loss: 0.00101388
Iteration 6/25 | Loss: 0.00101097
Iteration 7/25 | Loss: 0.00101058
Iteration 8/25 | Loss: 0.00101058
Iteration 9/25 | Loss: 0.00101058
Iteration 10/25 | Loss: 0.00101058
Iteration 11/25 | Loss: 0.00101058
Iteration 12/25 | Loss: 0.00101058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010105802211910486, 0.0010105802211910486, 0.0010105802211910486, 0.0010105802211910486, 0.0010105802211910486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010105802211910486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30937266
Iteration 2/25 | Loss: 0.00100958
Iteration 3/25 | Loss: 0.00100955
Iteration 4/25 | Loss: 0.00100955
Iteration 5/25 | Loss: 0.00100955
Iteration 6/25 | Loss: 0.00100955
Iteration 7/25 | Loss: 0.00100955
Iteration 8/25 | Loss: 0.00100955
Iteration 9/25 | Loss: 0.00100955
Iteration 10/25 | Loss: 0.00100955
Iteration 11/25 | Loss: 0.00100955
Iteration 12/25 | Loss: 0.00100955
Iteration 13/25 | Loss: 0.00100955
Iteration 14/25 | Loss: 0.00100955
Iteration 15/25 | Loss: 0.00100955
Iteration 16/25 | Loss: 0.00100955
Iteration 17/25 | Loss: 0.00100955
Iteration 18/25 | Loss: 0.00100955
Iteration 19/25 | Loss: 0.00100955
Iteration 20/25 | Loss: 0.00100955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001009550178423524, 0.001009550178423524, 0.001009550178423524, 0.001009550178423524, 0.001009550178423524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001009550178423524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100955
Iteration 2/1000 | Loss: 0.00005962
Iteration 3/1000 | Loss: 0.00003740
Iteration 4/1000 | Loss: 0.00003039
Iteration 5/1000 | Loss: 0.00002675
Iteration 6/1000 | Loss: 0.00002469
Iteration 7/1000 | Loss: 0.00002362
Iteration 8/1000 | Loss: 0.00002270
Iteration 9/1000 | Loss: 0.00002200
Iteration 10/1000 | Loss: 0.00002161
Iteration 11/1000 | Loss: 0.00002139
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002127
Iteration 15/1000 | Loss: 0.00002126
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002086
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002083
Iteration 22/1000 | Loss: 0.00002083
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002083
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00002080
Iteration 29/1000 | Loss: 0.00002079
Iteration 30/1000 | Loss: 0.00002079
Iteration 31/1000 | Loss: 0.00002079
Iteration 32/1000 | Loss: 0.00002079
Iteration 33/1000 | Loss: 0.00002079
Iteration 34/1000 | Loss: 0.00002079
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002079
Iteration 37/1000 | Loss: 0.00002079
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002078
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002076
Iteration 48/1000 | Loss: 0.00002075
Iteration 49/1000 | Loss: 0.00002075
Iteration 50/1000 | Loss: 0.00002075
Iteration 51/1000 | Loss: 0.00002075
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002074
Iteration 56/1000 | Loss: 0.00002074
Iteration 57/1000 | Loss: 0.00002073
Iteration 58/1000 | Loss: 0.00002073
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002072
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002071
Iteration 68/1000 | Loss: 0.00002071
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002071
Iteration 85/1000 | Loss: 0.00002071
Iteration 86/1000 | Loss: 0.00002071
Iteration 87/1000 | Loss: 0.00002071
Iteration 88/1000 | Loss: 0.00002071
Iteration 89/1000 | Loss: 0.00002071
Iteration 90/1000 | Loss: 0.00002071
Iteration 91/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.0707711883005686e-05, 2.0707711883005686e-05, 2.0707711883005686e-05, 2.0707711883005686e-05, 2.0707711883005686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0707711883005686e-05

Optimization complete. Final v2v error: 3.7247872352600098 mm

Highest mean error: 5.571201801300049 mm for frame 67

Lowest mean error: 2.8301334381103516 mm for frame 3

Saving results

Total time: 34.01982927322388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460702
Iteration 2/25 | Loss: 0.00121323
Iteration 3/25 | Loss: 0.00104815
Iteration 4/25 | Loss: 0.00102913
Iteration 5/25 | Loss: 0.00102424
Iteration 6/25 | Loss: 0.00102316
Iteration 7/25 | Loss: 0.00102316
Iteration 8/25 | Loss: 0.00102316
Iteration 9/25 | Loss: 0.00102316
Iteration 10/25 | Loss: 0.00102316
Iteration 11/25 | Loss: 0.00102316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001023158896714449, 0.001023158896714449, 0.001023158896714449, 0.001023158896714449, 0.001023158896714449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001023158896714449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26483703
Iteration 2/25 | Loss: 0.00149462
Iteration 3/25 | Loss: 0.00149461
Iteration 4/25 | Loss: 0.00149461
Iteration 5/25 | Loss: 0.00149461
Iteration 6/25 | Loss: 0.00149461
Iteration 7/25 | Loss: 0.00149461
Iteration 8/25 | Loss: 0.00149460
Iteration 9/25 | Loss: 0.00149460
Iteration 10/25 | Loss: 0.00149460
Iteration 11/25 | Loss: 0.00149460
Iteration 12/25 | Loss: 0.00149460
Iteration 13/25 | Loss: 0.00149460
Iteration 14/25 | Loss: 0.00149460
Iteration 15/25 | Loss: 0.00149460
Iteration 16/25 | Loss: 0.00149460
Iteration 17/25 | Loss: 0.00149460
Iteration 18/25 | Loss: 0.00149460
Iteration 19/25 | Loss: 0.00149460
Iteration 20/25 | Loss: 0.00149460
Iteration 21/25 | Loss: 0.00149460
Iteration 22/25 | Loss: 0.00149460
Iteration 23/25 | Loss: 0.00149460
Iteration 24/25 | Loss: 0.00149460
Iteration 25/25 | Loss: 0.00149460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001494603930041194, 0.001494603930041194, 0.001494603930041194, 0.001494603930041194, 0.001494603930041194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001494603930041194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149460
Iteration 2/1000 | Loss: 0.00004534
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00001938
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001438
Iteration 14/1000 | Loss: 0.00001438
Iteration 15/1000 | Loss: 0.00001437
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001420
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001408
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001407
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001402
Iteration 47/1000 | Loss: 0.00001402
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001401
Iteration 53/1000 | Loss: 0.00001400
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001397
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001396
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001396
Iteration 76/1000 | Loss: 0.00001396
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001395
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001391
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.3906759704696015e-05, 1.3906759704696015e-05, 1.3906759704696015e-05, 1.3906759704696015e-05, 1.3906759704696015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3906759704696015e-05

Optimization complete. Final v2v error: 3.134563446044922 mm

Highest mean error: 3.7925214767456055 mm for frame 110

Lowest mean error: 2.7688305377960205 mm for frame 41

Saving results

Total time: 34.835039377212524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_us_0125/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_us_0125/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081454
Iteration 2/25 | Loss: 0.00216633
Iteration 3/25 | Loss: 0.00199410
Iteration 4/25 | Loss: 0.00132886
Iteration 5/25 | Loss: 0.00129459
Iteration 6/25 | Loss: 0.00128849
Iteration 7/25 | Loss: 0.00120294
Iteration 8/25 | Loss: 0.00112310
Iteration 9/25 | Loss: 0.00110310
Iteration 10/25 | Loss: 0.00107395
Iteration 11/25 | Loss: 0.00105286
Iteration 12/25 | Loss: 0.00105109
Iteration 13/25 | Loss: 0.00104047
Iteration 14/25 | Loss: 0.00104108
Iteration 15/25 | Loss: 0.00103762
Iteration 16/25 | Loss: 0.00103047
Iteration 17/25 | Loss: 0.00102641
Iteration 18/25 | Loss: 0.00102043
Iteration 19/25 | Loss: 0.00101711
Iteration 20/25 | Loss: 0.00101614
Iteration 21/25 | Loss: 0.00101596
Iteration 22/25 | Loss: 0.00101593
Iteration 23/25 | Loss: 0.00101593
Iteration 24/25 | Loss: 0.00101593
Iteration 25/25 | Loss: 0.00101593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32643604
Iteration 2/25 | Loss: 0.00118657
Iteration 3/25 | Loss: 0.00118657
Iteration 4/25 | Loss: 0.00118657
Iteration 5/25 | Loss: 0.00118657
Iteration 6/25 | Loss: 0.00118657
Iteration 7/25 | Loss: 0.00118657
Iteration 8/25 | Loss: 0.00118657
Iteration 9/25 | Loss: 0.00118657
Iteration 10/25 | Loss: 0.00118657
Iteration 11/25 | Loss: 0.00118657
Iteration 12/25 | Loss: 0.00118657
Iteration 13/25 | Loss: 0.00118657
Iteration 14/25 | Loss: 0.00118657
Iteration 15/25 | Loss: 0.00118657
Iteration 16/25 | Loss: 0.00118657
Iteration 17/25 | Loss: 0.00118657
Iteration 18/25 | Loss: 0.00118657
Iteration 19/25 | Loss: 0.00118657
Iteration 20/25 | Loss: 0.00118657
Iteration 21/25 | Loss: 0.00118657
Iteration 22/25 | Loss: 0.00118657
Iteration 23/25 | Loss: 0.00118657
Iteration 24/25 | Loss: 0.00118657
Iteration 25/25 | Loss: 0.00118657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118657
Iteration 2/1000 | Loss: 0.00046799
Iteration 3/1000 | Loss: 0.00003094
Iteration 4/1000 | Loss: 0.00002560
Iteration 5/1000 | Loss: 0.00002335
Iteration 6/1000 | Loss: 0.00004675
Iteration 7/1000 | Loss: 0.00058909
Iteration 8/1000 | Loss: 0.00064740
Iteration 9/1000 | Loss: 0.00092619
Iteration 10/1000 | Loss: 0.00068829
Iteration 11/1000 | Loss: 0.00081681
Iteration 12/1000 | Loss: 0.00059426
Iteration 13/1000 | Loss: 0.00010815
Iteration 14/1000 | Loss: 0.00021731
Iteration 15/1000 | Loss: 0.00002645
Iteration 16/1000 | Loss: 0.00002139
Iteration 17/1000 | Loss: 0.00001842
Iteration 18/1000 | Loss: 0.00001633
Iteration 19/1000 | Loss: 0.00001527
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00004182
Iteration 22/1000 | Loss: 0.00011505
Iteration 23/1000 | Loss: 0.00001398
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001327
Iteration 26/1000 | Loss: 0.00004236
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00003016
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001281
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00004442
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001254
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001253
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001252
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001249
Iteration 73/1000 | Loss: 0.00001249
Iteration 74/1000 | Loss: 0.00001249
Iteration 75/1000 | Loss: 0.00001249
Iteration 76/1000 | Loss: 0.00001249
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001248
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001248
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001247
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001247
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001246
Iteration 130/1000 | Loss: 0.00001246
Iteration 131/1000 | Loss: 0.00001246
Iteration 132/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2464802239264827e-05, 1.2464802239264827e-05, 1.2464802239264827e-05, 1.2464802239264827e-05, 1.2464802239264827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2464802239264827e-05

Optimization complete. Final v2v error: 2.973284959793091 mm

Highest mean error: 8.435675621032715 mm for frame 18

Lowest mean error: 2.462332010269165 mm for frame 100

Saving results

Total time: 90.10492753982544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860934
Iteration 2/25 | Loss: 0.00097917
Iteration 3/25 | Loss: 0.00076031
Iteration 4/25 | Loss: 0.00073378
Iteration 5/25 | Loss: 0.00072617
Iteration 6/25 | Loss: 0.00072441
Iteration 7/25 | Loss: 0.00072420
Iteration 8/25 | Loss: 0.00072420
Iteration 9/25 | Loss: 0.00072420
Iteration 10/25 | Loss: 0.00072420
Iteration 11/25 | Loss: 0.00072420
Iteration 12/25 | Loss: 0.00072420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007242029532790184, 0.0007242029532790184, 0.0007242029532790184, 0.0007242029532790184, 0.0007242029532790184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007242029532790184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83346510
Iteration 2/25 | Loss: 0.00041555
Iteration 3/25 | Loss: 0.00041554
Iteration 4/25 | Loss: 0.00041554
Iteration 5/25 | Loss: 0.00041554
Iteration 6/25 | Loss: 0.00041554
Iteration 7/25 | Loss: 0.00041554
Iteration 8/25 | Loss: 0.00041554
Iteration 9/25 | Loss: 0.00041554
Iteration 10/25 | Loss: 0.00041554
Iteration 11/25 | Loss: 0.00041554
Iteration 12/25 | Loss: 0.00041554
Iteration 13/25 | Loss: 0.00041554
Iteration 14/25 | Loss: 0.00041554
Iteration 15/25 | Loss: 0.00041554
Iteration 16/25 | Loss: 0.00041554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004155421338509768, 0.0004155421338509768, 0.0004155421338509768, 0.0004155421338509768, 0.0004155421338509768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004155421338509768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041554
Iteration 2/1000 | Loss: 0.00004945
Iteration 3/1000 | Loss: 0.00002938
Iteration 4/1000 | Loss: 0.00002643
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002302
Iteration 7/1000 | Loss: 0.00002227
Iteration 8/1000 | Loss: 0.00002172
Iteration 9/1000 | Loss: 0.00002133
Iteration 10/1000 | Loss: 0.00002090
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00002042
Iteration 14/1000 | Loss: 0.00002041
Iteration 15/1000 | Loss: 0.00002036
Iteration 16/1000 | Loss: 0.00002031
Iteration 17/1000 | Loss: 0.00002027
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002025
Iteration 21/1000 | Loss: 0.00002023
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002022
Iteration 25/1000 | Loss: 0.00002022
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002015
Iteration 32/1000 | Loss: 0.00002015
Iteration 33/1000 | Loss: 0.00002014
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002011
Iteration 37/1000 | Loss: 0.00002010
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002006
Iteration 44/1000 | Loss: 0.00002005
Iteration 45/1000 | Loss: 0.00002005
Iteration 46/1000 | Loss: 0.00002005
Iteration 47/1000 | Loss: 0.00002004
Iteration 48/1000 | Loss: 0.00002004
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00002004
Iteration 51/1000 | Loss: 0.00002003
Iteration 52/1000 | Loss: 0.00002003
Iteration 53/1000 | Loss: 0.00002003
Iteration 54/1000 | Loss: 0.00002002
Iteration 55/1000 | Loss: 0.00002002
Iteration 56/1000 | Loss: 0.00002002
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002000
Iteration 64/1000 | Loss: 0.00002000
Iteration 65/1000 | Loss: 0.00002000
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00002000
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00002000
Iteration 72/1000 | Loss: 0.00002000
Iteration 73/1000 | Loss: 0.00001999
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001999
Iteration 86/1000 | Loss: 0.00001998
Iteration 87/1000 | Loss: 0.00001998
Iteration 88/1000 | Loss: 0.00001998
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Iteration 91/1000 | Loss: 0.00001998
Iteration 92/1000 | Loss: 0.00001998
Iteration 93/1000 | Loss: 0.00001998
Iteration 94/1000 | Loss: 0.00001998
Iteration 95/1000 | Loss: 0.00001998
Iteration 96/1000 | Loss: 0.00001998
Iteration 97/1000 | Loss: 0.00001998
Iteration 98/1000 | Loss: 0.00001998
Iteration 99/1000 | Loss: 0.00001997
Iteration 100/1000 | Loss: 0.00001997
Iteration 101/1000 | Loss: 0.00001997
Iteration 102/1000 | Loss: 0.00001997
Iteration 103/1000 | Loss: 0.00001997
Iteration 104/1000 | Loss: 0.00001997
Iteration 105/1000 | Loss: 0.00001997
Iteration 106/1000 | Loss: 0.00001997
Iteration 107/1000 | Loss: 0.00001997
Iteration 108/1000 | Loss: 0.00001997
Iteration 109/1000 | Loss: 0.00001997
Iteration 110/1000 | Loss: 0.00001997
Iteration 111/1000 | Loss: 0.00001997
Iteration 112/1000 | Loss: 0.00001996
Iteration 113/1000 | Loss: 0.00001996
Iteration 114/1000 | Loss: 0.00001996
Iteration 115/1000 | Loss: 0.00001996
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001996
Iteration 118/1000 | Loss: 0.00001996
Iteration 119/1000 | Loss: 0.00001996
Iteration 120/1000 | Loss: 0.00001996
Iteration 121/1000 | Loss: 0.00001996
Iteration 122/1000 | Loss: 0.00001996
Iteration 123/1000 | Loss: 0.00001996
Iteration 124/1000 | Loss: 0.00001996
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001996
Iteration 127/1000 | Loss: 0.00001996
Iteration 128/1000 | Loss: 0.00001996
Iteration 129/1000 | Loss: 0.00001996
Iteration 130/1000 | Loss: 0.00001995
Iteration 131/1000 | Loss: 0.00001995
Iteration 132/1000 | Loss: 0.00001995
Iteration 133/1000 | Loss: 0.00001995
Iteration 134/1000 | Loss: 0.00001995
Iteration 135/1000 | Loss: 0.00001995
Iteration 136/1000 | Loss: 0.00001995
Iteration 137/1000 | Loss: 0.00001995
Iteration 138/1000 | Loss: 0.00001995
Iteration 139/1000 | Loss: 0.00001995
Iteration 140/1000 | Loss: 0.00001995
Iteration 141/1000 | Loss: 0.00001995
Iteration 142/1000 | Loss: 0.00001995
Iteration 143/1000 | Loss: 0.00001995
Iteration 144/1000 | Loss: 0.00001995
Iteration 145/1000 | Loss: 0.00001995
Iteration 146/1000 | Loss: 0.00001995
Iteration 147/1000 | Loss: 0.00001995
Iteration 148/1000 | Loss: 0.00001995
Iteration 149/1000 | Loss: 0.00001995
Iteration 150/1000 | Loss: 0.00001995
Iteration 151/1000 | Loss: 0.00001995
Iteration 152/1000 | Loss: 0.00001995
Iteration 153/1000 | Loss: 0.00001995
Iteration 154/1000 | Loss: 0.00001995
Iteration 155/1000 | Loss: 0.00001995
Iteration 156/1000 | Loss: 0.00001995
Iteration 157/1000 | Loss: 0.00001995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.9953344235545956e-05, 1.9953344235545956e-05, 1.9953344235545956e-05, 1.9953344235545956e-05, 1.9953344235545956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9953344235545956e-05

Optimization complete. Final v2v error: 3.6226894855499268 mm

Highest mean error: 5.572144508361816 mm for frame 155

Lowest mean error: 2.853158473968506 mm for frame 0

Saving results

Total time: 42.682573556900024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055478
Iteration 2/25 | Loss: 0.01055478
Iteration 3/25 | Loss: 0.00198917
Iteration 4/25 | Loss: 0.00123675
Iteration 5/25 | Loss: 0.00119762
Iteration 6/25 | Loss: 0.00083504
Iteration 7/25 | Loss: 0.00081213
Iteration 8/25 | Loss: 0.00080319
Iteration 9/25 | Loss: 0.00080409
Iteration 10/25 | Loss: 0.00079502
Iteration 11/25 | Loss: 0.00080427
Iteration 12/25 | Loss: 0.00080614
Iteration 13/25 | Loss: 0.00078674
Iteration 14/25 | Loss: 0.00078554
Iteration 15/25 | Loss: 0.00078142
Iteration 16/25 | Loss: 0.00077787
Iteration 17/25 | Loss: 0.00077469
Iteration 18/25 | Loss: 0.00076508
Iteration 19/25 | Loss: 0.00076959
Iteration 20/25 | Loss: 0.00076746
Iteration 21/25 | Loss: 0.00075977
Iteration 22/25 | Loss: 0.00074926
Iteration 23/25 | Loss: 0.00074825
Iteration 24/25 | Loss: 0.00074400
Iteration 25/25 | Loss: 0.00074118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51047397
Iteration 2/25 | Loss: 0.00068570
Iteration 3/25 | Loss: 0.00068570
Iteration 4/25 | Loss: 0.00068570
Iteration 5/25 | Loss: 0.00068570
Iteration 6/25 | Loss: 0.00068570
Iteration 7/25 | Loss: 0.00068570
Iteration 8/25 | Loss: 0.00068570
Iteration 9/25 | Loss: 0.00068570
Iteration 10/25 | Loss: 0.00068570
Iteration 11/25 | Loss: 0.00068570
Iteration 12/25 | Loss: 0.00068570
Iteration 13/25 | Loss: 0.00068570
Iteration 14/25 | Loss: 0.00068570
Iteration 15/25 | Loss: 0.00068570
Iteration 16/25 | Loss: 0.00068570
Iteration 17/25 | Loss: 0.00068570
Iteration 18/25 | Loss: 0.00068570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006857002153992653, 0.0006857002153992653, 0.0006857002153992653, 0.0006857002153992653, 0.0006857002153992653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006857002153992653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068570
Iteration 2/1000 | Loss: 0.00006143
Iteration 3/1000 | Loss: 0.00005639
Iteration 4/1000 | Loss: 0.00003259
Iteration 5/1000 | Loss: 0.00004037
Iteration 6/1000 | Loss: 0.00003259
Iteration 7/1000 | Loss: 0.00003101
Iteration 8/1000 | Loss: 0.00003486
Iteration 9/1000 | Loss: 0.00002479
Iteration 10/1000 | Loss: 0.00002413
Iteration 11/1000 | Loss: 0.00002365
Iteration 12/1000 | Loss: 0.00002325
Iteration 13/1000 | Loss: 0.00002288
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002257
Iteration 16/1000 | Loss: 0.00002246
Iteration 17/1000 | Loss: 0.00002245
Iteration 18/1000 | Loss: 0.00002245
Iteration 19/1000 | Loss: 0.00002244
Iteration 20/1000 | Loss: 0.00002244
Iteration 21/1000 | Loss: 0.00002242
Iteration 22/1000 | Loss: 0.00002238
Iteration 23/1000 | Loss: 0.00002237
Iteration 24/1000 | Loss: 0.00002236
Iteration 25/1000 | Loss: 0.00002234
Iteration 26/1000 | Loss: 0.00002225
Iteration 27/1000 | Loss: 0.00002223
Iteration 28/1000 | Loss: 0.00002222
Iteration 29/1000 | Loss: 0.00002222
Iteration 30/1000 | Loss: 0.00002221
Iteration 31/1000 | Loss: 0.00002221
Iteration 32/1000 | Loss: 0.00002220
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002219
Iteration 35/1000 | Loss: 0.00002219
Iteration 36/1000 | Loss: 0.00002218
Iteration 37/1000 | Loss: 0.00002218
Iteration 38/1000 | Loss: 0.00002217
Iteration 39/1000 | Loss: 0.00002217
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002209
Iteration 42/1000 | Loss: 0.00002208
Iteration 43/1000 | Loss: 0.00002207
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002207
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002206
Iteration 49/1000 | Loss: 0.00002205
Iteration 50/1000 | Loss: 0.00002205
Iteration 51/1000 | Loss: 0.00002205
Iteration 52/1000 | Loss: 0.00002204
Iteration 53/1000 | Loss: 0.00002204
Iteration 54/1000 | Loss: 0.00002203
Iteration 55/1000 | Loss: 0.00002203
Iteration 56/1000 | Loss: 0.00002203
Iteration 57/1000 | Loss: 0.00002202
Iteration 58/1000 | Loss: 0.00002201
Iteration 59/1000 | Loss: 0.00002201
Iteration 60/1000 | Loss: 0.00002201
Iteration 61/1000 | Loss: 0.00002201
Iteration 62/1000 | Loss: 0.00002201
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002200
Iteration 65/1000 | Loss: 0.00002200
Iteration 66/1000 | Loss: 0.00002200
Iteration 67/1000 | Loss: 0.00002200
Iteration 68/1000 | Loss: 0.00002200
Iteration 69/1000 | Loss: 0.00002200
Iteration 70/1000 | Loss: 0.00002200
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002199
Iteration 73/1000 | Loss: 0.00002199
Iteration 74/1000 | Loss: 0.00002198
Iteration 75/1000 | Loss: 0.00002198
Iteration 76/1000 | Loss: 0.00002198
Iteration 77/1000 | Loss: 0.00002198
Iteration 78/1000 | Loss: 0.00002198
Iteration 79/1000 | Loss: 0.00002198
Iteration 80/1000 | Loss: 0.00002198
Iteration 81/1000 | Loss: 0.00002198
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002197
Iteration 84/1000 | Loss: 0.00002197
Iteration 85/1000 | Loss: 0.00002197
Iteration 86/1000 | Loss: 0.00002197
Iteration 87/1000 | Loss: 0.00002197
Iteration 88/1000 | Loss: 0.00002197
Iteration 89/1000 | Loss: 0.00002196
Iteration 90/1000 | Loss: 0.00002196
Iteration 91/1000 | Loss: 0.00002196
Iteration 92/1000 | Loss: 0.00002196
Iteration 93/1000 | Loss: 0.00002196
Iteration 94/1000 | Loss: 0.00002196
Iteration 95/1000 | Loss: 0.00002196
Iteration 96/1000 | Loss: 0.00002196
Iteration 97/1000 | Loss: 0.00002196
Iteration 98/1000 | Loss: 0.00002195
Iteration 99/1000 | Loss: 0.00002195
Iteration 100/1000 | Loss: 0.00002195
Iteration 101/1000 | Loss: 0.00002195
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002194
Iteration 104/1000 | Loss: 0.00002194
Iteration 105/1000 | Loss: 0.00002194
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002193
Iteration 110/1000 | Loss: 0.00002193
Iteration 111/1000 | Loss: 0.00002193
Iteration 112/1000 | Loss: 0.00002193
Iteration 113/1000 | Loss: 0.00002193
Iteration 114/1000 | Loss: 0.00002193
Iteration 115/1000 | Loss: 0.00002193
Iteration 116/1000 | Loss: 0.00002193
Iteration 117/1000 | Loss: 0.00002193
Iteration 118/1000 | Loss: 0.00002193
Iteration 119/1000 | Loss: 0.00002193
Iteration 120/1000 | Loss: 0.00002193
Iteration 121/1000 | Loss: 0.00002192
Iteration 122/1000 | Loss: 0.00002192
Iteration 123/1000 | Loss: 0.00002192
Iteration 124/1000 | Loss: 0.00002192
Iteration 125/1000 | Loss: 0.00002192
Iteration 126/1000 | Loss: 0.00002192
Iteration 127/1000 | Loss: 0.00002192
Iteration 128/1000 | Loss: 0.00002192
Iteration 129/1000 | Loss: 0.00002192
Iteration 130/1000 | Loss: 0.00002192
Iteration 131/1000 | Loss: 0.00002192
Iteration 132/1000 | Loss: 0.00002192
Iteration 133/1000 | Loss: 0.00002192
Iteration 134/1000 | Loss: 0.00002192
Iteration 135/1000 | Loss: 0.00002192
Iteration 136/1000 | Loss: 0.00002192
Iteration 137/1000 | Loss: 0.00002192
Iteration 138/1000 | Loss: 0.00002192
Iteration 139/1000 | Loss: 0.00002192
Iteration 140/1000 | Loss: 0.00002192
Iteration 141/1000 | Loss: 0.00002192
Iteration 142/1000 | Loss: 0.00002192
Iteration 143/1000 | Loss: 0.00002192
Iteration 144/1000 | Loss: 0.00002191
Iteration 145/1000 | Loss: 0.00002191
Iteration 146/1000 | Loss: 0.00002191
Iteration 147/1000 | Loss: 0.00002191
Iteration 148/1000 | Loss: 0.00002191
Iteration 149/1000 | Loss: 0.00002191
Iteration 150/1000 | Loss: 0.00002191
Iteration 151/1000 | Loss: 0.00002191
Iteration 152/1000 | Loss: 0.00002191
Iteration 153/1000 | Loss: 0.00002191
Iteration 154/1000 | Loss: 0.00002190
Iteration 155/1000 | Loss: 0.00002190
Iteration 156/1000 | Loss: 0.00002190
Iteration 157/1000 | Loss: 0.00002190
Iteration 158/1000 | Loss: 0.00002190
Iteration 159/1000 | Loss: 0.00002190
Iteration 160/1000 | Loss: 0.00002190
Iteration 161/1000 | Loss: 0.00002189
Iteration 162/1000 | Loss: 0.00002189
Iteration 163/1000 | Loss: 0.00002189
Iteration 164/1000 | Loss: 0.00002189
Iteration 165/1000 | Loss: 0.00002189
Iteration 166/1000 | Loss: 0.00002189
Iteration 167/1000 | Loss: 0.00002189
Iteration 168/1000 | Loss: 0.00002189
Iteration 169/1000 | Loss: 0.00002189
Iteration 170/1000 | Loss: 0.00002189
Iteration 171/1000 | Loss: 0.00002189
Iteration 172/1000 | Loss: 0.00002188
Iteration 173/1000 | Loss: 0.00002188
Iteration 174/1000 | Loss: 0.00002188
Iteration 175/1000 | Loss: 0.00002188
Iteration 176/1000 | Loss: 0.00002188
Iteration 177/1000 | Loss: 0.00002188
Iteration 178/1000 | Loss: 0.00002188
Iteration 179/1000 | Loss: 0.00002188
Iteration 180/1000 | Loss: 0.00002188
Iteration 181/1000 | Loss: 0.00002187
Iteration 182/1000 | Loss: 0.00002187
Iteration 183/1000 | Loss: 0.00002187
Iteration 184/1000 | Loss: 0.00002187
Iteration 185/1000 | Loss: 0.00002187
Iteration 186/1000 | Loss: 0.00002187
Iteration 187/1000 | Loss: 0.00002187
Iteration 188/1000 | Loss: 0.00002187
Iteration 189/1000 | Loss: 0.00002187
Iteration 190/1000 | Loss: 0.00002187
Iteration 191/1000 | Loss: 0.00002187
Iteration 192/1000 | Loss: 0.00002187
Iteration 193/1000 | Loss: 0.00002187
Iteration 194/1000 | Loss: 0.00002187
Iteration 195/1000 | Loss: 0.00002187
Iteration 196/1000 | Loss: 0.00002187
Iteration 197/1000 | Loss: 0.00002187
Iteration 198/1000 | Loss: 0.00002187
Iteration 199/1000 | Loss: 0.00002187
Iteration 200/1000 | Loss: 0.00002187
Iteration 201/1000 | Loss: 0.00002187
Iteration 202/1000 | Loss: 0.00002187
Iteration 203/1000 | Loss: 0.00002187
Iteration 204/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.18682052945951e-05, 2.18682052945951e-05, 2.18682052945951e-05, 2.18682052945951e-05, 2.18682052945951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.18682052945951e-05

Optimization complete. Final v2v error: 3.9918627738952637 mm

Highest mean error: 4.6248297691345215 mm for frame 61

Lowest mean error: 3.53328800201416 mm for frame 79

Saving results

Total time: 89.01545763015747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387399
Iteration 2/25 | Loss: 0.00090192
Iteration 3/25 | Loss: 0.00076568
Iteration 4/25 | Loss: 0.00073679
Iteration 5/25 | Loss: 0.00072749
Iteration 6/25 | Loss: 0.00072567
Iteration 7/25 | Loss: 0.00072494
Iteration 8/25 | Loss: 0.00072470
Iteration 9/25 | Loss: 0.00072470
Iteration 10/25 | Loss: 0.00072470
Iteration 11/25 | Loss: 0.00072470
Iteration 12/25 | Loss: 0.00072470
Iteration 13/25 | Loss: 0.00072470
Iteration 14/25 | Loss: 0.00072470
Iteration 15/25 | Loss: 0.00072470
Iteration 16/25 | Loss: 0.00072470
Iteration 17/25 | Loss: 0.00072470
Iteration 18/25 | Loss: 0.00072470
Iteration 19/25 | Loss: 0.00072470
Iteration 20/25 | Loss: 0.00072470
Iteration 21/25 | Loss: 0.00072470
Iteration 22/25 | Loss: 0.00072470
Iteration 23/25 | Loss: 0.00072470
Iteration 24/25 | Loss: 0.00072470
Iteration 25/25 | Loss: 0.00072470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51986027
Iteration 2/25 | Loss: 0.00065962
Iteration 3/25 | Loss: 0.00065961
Iteration 4/25 | Loss: 0.00065961
Iteration 5/25 | Loss: 0.00065961
Iteration 6/25 | Loss: 0.00065961
Iteration 7/25 | Loss: 0.00065961
Iteration 8/25 | Loss: 0.00065961
Iteration 9/25 | Loss: 0.00065961
Iteration 10/25 | Loss: 0.00065961
Iteration 11/25 | Loss: 0.00065961
Iteration 12/25 | Loss: 0.00065961
Iteration 13/25 | Loss: 0.00065961
Iteration 14/25 | Loss: 0.00065961
Iteration 15/25 | Loss: 0.00065961
Iteration 16/25 | Loss: 0.00065961
Iteration 17/25 | Loss: 0.00065961
Iteration 18/25 | Loss: 0.00065961
Iteration 19/25 | Loss: 0.00065961
Iteration 20/25 | Loss: 0.00065961
Iteration 21/25 | Loss: 0.00065961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006596095627173781, 0.0006596095627173781, 0.0006596095627173781, 0.0006596095627173781, 0.0006596095627173781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006596095627173781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065961
Iteration 2/1000 | Loss: 0.00006040
Iteration 3/1000 | Loss: 0.00004075
Iteration 4/1000 | Loss: 0.00003455
Iteration 5/1000 | Loss: 0.00003155
Iteration 6/1000 | Loss: 0.00002920
Iteration 7/1000 | Loss: 0.00002773
Iteration 8/1000 | Loss: 0.00002651
Iteration 9/1000 | Loss: 0.00002561
Iteration 10/1000 | Loss: 0.00002518
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002458
Iteration 13/1000 | Loss: 0.00002429
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002392
Iteration 16/1000 | Loss: 0.00002386
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002374
Iteration 19/1000 | Loss: 0.00002373
Iteration 20/1000 | Loss: 0.00002370
Iteration 21/1000 | Loss: 0.00002364
Iteration 22/1000 | Loss: 0.00002362
Iteration 23/1000 | Loss: 0.00002361
Iteration 24/1000 | Loss: 0.00002361
Iteration 25/1000 | Loss: 0.00002360
Iteration 26/1000 | Loss: 0.00002360
Iteration 27/1000 | Loss: 0.00002359
Iteration 28/1000 | Loss: 0.00002359
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002358
Iteration 31/1000 | Loss: 0.00002358
Iteration 32/1000 | Loss: 0.00002357
Iteration 33/1000 | Loss: 0.00002357
Iteration 34/1000 | Loss: 0.00002356
Iteration 35/1000 | Loss: 0.00002355
Iteration 36/1000 | Loss: 0.00002355
Iteration 37/1000 | Loss: 0.00002355
Iteration 38/1000 | Loss: 0.00002355
Iteration 39/1000 | Loss: 0.00002355
Iteration 40/1000 | Loss: 0.00002355
Iteration 41/1000 | Loss: 0.00002354
Iteration 42/1000 | Loss: 0.00002354
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002353
Iteration 45/1000 | Loss: 0.00002352
Iteration 46/1000 | Loss: 0.00002352
Iteration 47/1000 | Loss: 0.00002352
Iteration 48/1000 | Loss: 0.00002351
Iteration 49/1000 | Loss: 0.00002351
Iteration 50/1000 | Loss: 0.00002351
Iteration 51/1000 | Loss: 0.00002351
Iteration 52/1000 | Loss: 0.00002351
Iteration 53/1000 | Loss: 0.00002351
Iteration 54/1000 | Loss: 0.00002350
Iteration 55/1000 | Loss: 0.00002350
Iteration 56/1000 | Loss: 0.00002350
Iteration 57/1000 | Loss: 0.00002350
Iteration 58/1000 | Loss: 0.00002350
Iteration 59/1000 | Loss: 0.00002349
Iteration 60/1000 | Loss: 0.00002349
Iteration 61/1000 | Loss: 0.00002349
Iteration 62/1000 | Loss: 0.00002349
Iteration 63/1000 | Loss: 0.00002349
Iteration 64/1000 | Loss: 0.00002349
Iteration 65/1000 | Loss: 0.00002349
Iteration 66/1000 | Loss: 0.00002348
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002348
Iteration 69/1000 | Loss: 0.00002348
Iteration 70/1000 | Loss: 0.00002348
Iteration 71/1000 | Loss: 0.00002348
Iteration 72/1000 | Loss: 0.00002348
Iteration 73/1000 | Loss: 0.00002347
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002347
Iteration 77/1000 | Loss: 0.00002347
Iteration 78/1000 | Loss: 0.00002347
Iteration 79/1000 | Loss: 0.00002346
Iteration 80/1000 | Loss: 0.00002346
Iteration 81/1000 | Loss: 0.00002346
Iteration 82/1000 | Loss: 0.00002346
Iteration 83/1000 | Loss: 0.00002346
Iteration 84/1000 | Loss: 0.00002346
Iteration 85/1000 | Loss: 0.00002346
Iteration 86/1000 | Loss: 0.00002346
Iteration 87/1000 | Loss: 0.00002346
Iteration 88/1000 | Loss: 0.00002346
Iteration 89/1000 | Loss: 0.00002345
Iteration 90/1000 | Loss: 0.00002345
Iteration 91/1000 | Loss: 0.00002345
Iteration 92/1000 | Loss: 0.00002345
Iteration 93/1000 | Loss: 0.00002345
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002344
Iteration 96/1000 | Loss: 0.00002344
Iteration 97/1000 | Loss: 0.00002344
Iteration 98/1000 | Loss: 0.00002344
Iteration 99/1000 | Loss: 0.00002344
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002343
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002341
Iteration 111/1000 | Loss: 0.00002341
Iteration 112/1000 | Loss: 0.00002341
Iteration 113/1000 | Loss: 0.00002341
Iteration 114/1000 | Loss: 0.00002341
Iteration 115/1000 | Loss: 0.00002341
Iteration 116/1000 | Loss: 0.00002341
Iteration 117/1000 | Loss: 0.00002340
Iteration 118/1000 | Loss: 0.00002340
Iteration 119/1000 | Loss: 0.00002340
Iteration 120/1000 | Loss: 0.00002340
Iteration 121/1000 | Loss: 0.00002340
Iteration 122/1000 | Loss: 0.00002340
Iteration 123/1000 | Loss: 0.00002340
Iteration 124/1000 | Loss: 0.00002340
Iteration 125/1000 | Loss: 0.00002340
Iteration 126/1000 | Loss: 0.00002340
Iteration 127/1000 | Loss: 0.00002340
Iteration 128/1000 | Loss: 0.00002340
Iteration 129/1000 | Loss: 0.00002340
Iteration 130/1000 | Loss: 0.00002340
Iteration 131/1000 | Loss: 0.00002340
Iteration 132/1000 | Loss: 0.00002340
Iteration 133/1000 | Loss: 0.00002339
Iteration 134/1000 | Loss: 0.00002339
Iteration 135/1000 | Loss: 0.00002339
Iteration 136/1000 | Loss: 0.00002339
Iteration 137/1000 | Loss: 0.00002339
Iteration 138/1000 | Loss: 0.00002339
Iteration 139/1000 | Loss: 0.00002339
Iteration 140/1000 | Loss: 0.00002339
Iteration 141/1000 | Loss: 0.00002339
Iteration 142/1000 | Loss: 0.00002339
Iteration 143/1000 | Loss: 0.00002339
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002339
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002339
Iteration 152/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.3388583940686658e-05, 2.3388583940686658e-05, 2.3388583940686658e-05, 2.3388583940686658e-05, 2.3388583940686658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3388583940686658e-05

Optimization complete. Final v2v error: 4.055079936981201 mm

Highest mean error: 5.022614479064941 mm for frame 23

Lowest mean error: 3.302729368209839 mm for frame 77

Saving results

Total time: 43.15486288070679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387524
Iteration 2/25 | Loss: 0.00088175
Iteration 3/25 | Loss: 0.00072698
Iteration 4/25 | Loss: 0.00070415
Iteration 5/25 | Loss: 0.00069876
Iteration 6/25 | Loss: 0.00069716
Iteration 7/25 | Loss: 0.00069671
Iteration 8/25 | Loss: 0.00069667
Iteration 9/25 | Loss: 0.00069667
Iteration 10/25 | Loss: 0.00069667
Iteration 11/25 | Loss: 0.00069667
Iteration 12/25 | Loss: 0.00069667
Iteration 13/25 | Loss: 0.00069667
Iteration 14/25 | Loss: 0.00069667
Iteration 15/25 | Loss: 0.00069667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006966710789129138, 0.0006966710789129138, 0.0006966710789129138, 0.0006966710789129138, 0.0006966710789129138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006966710789129138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47256601
Iteration 2/25 | Loss: 0.00059043
Iteration 3/25 | Loss: 0.00059041
Iteration 4/25 | Loss: 0.00059041
Iteration 5/25 | Loss: 0.00059041
Iteration 6/25 | Loss: 0.00059041
Iteration 7/25 | Loss: 0.00059041
Iteration 8/25 | Loss: 0.00059041
Iteration 9/25 | Loss: 0.00059041
Iteration 10/25 | Loss: 0.00059041
Iteration 11/25 | Loss: 0.00059041
Iteration 12/25 | Loss: 0.00059041
Iteration 13/25 | Loss: 0.00059041
Iteration 14/25 | Loss: 0.00059041
Iteration 15/25 | Loss: 0.00059041
Iteration 16/25 | Loss: 0.00059041
Iteration 17/25 | Loss: 0.00059041
Iteration 18/25 | Loss: 0.00059041
Iteration 19/25 | Loss: 0.00059041
Iteration 20/25 | Loss: 0.00059041
Iteration 21/25 | Loss: 0.00059041
Iteration 22/25 | Loss: 0.00059041
Iteration 23/25 | Loss: 0.00059041
Iteration 24/25 | Loss: 0.00059041
Iteration 25/25 | Loss: 0.00059041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059041
Iteration 2/1000 | Loss: 0.00006573
Iteration 3/1000 | Loss: 0.00004412
Iteration 4/1000 | Loss: 0.00002827
Iteration 5/1000 | Loss: 0.00002497
Iteration 6/1000 | Loss: 0.00002219
Iteration 7/1000 | Loss: 0.00002026
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001808
Iteration 10/1000 | Loss: 0.00001778
Iteration 11/1000 | Loss: 0.00001746
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001689
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001637
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001635
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001633
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001629
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001628
Iteration 45/1000 | Loss: 0.00001628
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001627
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001627
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001627
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001626
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001625
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001624
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001624
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001620
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001617
Iteration 133/1000 | Loss: 0.00001617
Iteration 134/1000 | Loss: 0.00001617
Iteration 135/1000 | Loss: 0.00001617
Iteration 136/1000 | Loss: 0.00001617
Iteration 137/1000 | Loss: 0.00001617
Iteration 138/1000 | Loss: 0.00001617
Iteration 139/1000 | Loss: 0.00001617
Iteration 140/1000 | Loss: 0.00001616
Iteration 141/1000 | Loss: 0.00001616
Iteration 142/1000 | Loss: 0.00001616
Iteration 143/1000 | Loss: 0.00001616
Iteration 144/1000 | Loss: 0.00001616
Iteration 145/1000 | Loss: 0.00001616
Iteration 146/1000 | Loss: 0.00001616
Iteration 147/1000 | Loss: 0.00001616
Iteration 148/1000 | Loss: 0.00001616
Iteration 149/1000 | Loss: 0.00001616
Iteration 150/1000 | Loss: 0.00001616
Iteration 151/1000 | Loss: 0.00001616
Iteration 152/1000 | Loss: 0.00001616
Iteration 153/1000 | Loss: 0.00001616
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001616
Iteration 159/1000 | Loss: 0.00001615
Iteration 160/1000 | Loss: 0.00001615
Iteration 161/1000 | Loss: 0.00001615
Iteration 162/1000 | Loss: 0.00001615
Iteration 163/1000 | Loss: 0.00001615
Iteration 164/1000 | Loss: 0.00001615
Iteration 165/1000 | Loss: 0.00001615
Iteration 166/1000 | Loss: 0.00001615
Iteration 167/1000 | Loss: 0.00001615
Iteration 168/1000 | Loss: 0.00001615
Iteration 169/1000 | Loss: 0.00001615
Iteration 170/1000 | Loss: 0.00001615
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.6154168406501412e-05, 1.6154168406501412e-05, 1.6154168406501412e-05, 1.6154168406501412e-05, 1.6154168406501412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6154168406501412e-05

Optimization complete. Final v2v error: 3.429832696914673 mm

Highest mean error: 4.1294660568237305 mm for frame 104

Lowest mean error: 3.0431387424468994 mm for frame 7

Saving results

Total time: 41.39989256858826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963471
Iteration 2/25 | Loss: 0.00133910
Iteration 3/25 | Loss: 0.00094237
Iteration 4/25 | Loss: 0.00083155
Iteration 5/25 | Loss: 0.00079861
Iteration 6/25 | Loss: 0.00078012
Iteration 7/25 | Loss: 0.00077288
Iteration 8/25 | Loss: 0.00077756
Iteration 9/25 | Loss: 0.00077090
Iteration 10/25 | Loss: 0.00077088
Iteration 11/25 | Loss: 0.00077087
Iteration 12/25 | Loss: 0.00077087
Iteration 13/25 | Loss: 0.00077087
Iteration 14/25 | Loss: 0.00077087
Iteration 15/25 | Loss: 0.00077087
Iteration 16/25 | Loss: 0.00077087
Iteration 17/25 | Loss: 0.00077087
Iteration 18/25 | Loss: 0.00077087
Iteration 19/25 | Loss: 0.00077087
Iteration 20/25 | Loss: 0.00077087
Iteration 21/25 | Loss: 0.00077087
Iteration 22/25 | Loss: 0.00077086
Iteration 23/25 | Loss: 0.00077086
Iteration 24/25 | Loss: 0.00077086
Iteration 25/25 | Loss: 0.00077086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60236382
Iteration 2/25 | Loss: 0.00055797
Iteration 3/25 | Loss: 0.00055794
Iteration 4/25 | Loss: 0.00055794
Iteration 5/25 | Loss: 0.00055794
Iteration 6/25 | Loss: 0.00055794
Iteration 7/25 | Loss: 0.00055794
Iteration 8/25 | Loss: 0.00055794
Iteration 9/25 | Loss: 0.00055794
Iteration 10/25 | Loss: 0.00055794
Iteration 11/25 | Loss: 0.00055794
Iteration 12/25 | Loss: 0.00055794
Iteration 13/25 | Loss: 0.00055794
Iteration 14/25 | Loss: 0.00055794
Iteration 15/25 | Loss: 0.00055794
Iteration 16/25 | Loss: 0.00055794
Iteration 17/25 | Loss: 0.00055794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005579378921538591, 0.0005579378921538591, 0.0005579378921538591, 0.0005579378921538591, 0.0005579378921538591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005579378921538591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055794
Iteration 2/1000 | Loss: 0.00005097
Iteration 3/1000 | Loss: 0.00017405
Iteration 4/1000 | Loss: 0.00003339
Iteration 5/1000 | Loss: 0.00009108
Iteration 6/1000 | Loss: 0.00002985
Iteration 7/1000 | Loss: 0.00010306
Iteration 8/1000 | Loss: 0.00002785
Iteration 9/1000 | Loss: 0.00002705
Iteration 10/1000 | Loss: 0.00002644
Iteration 11/1000 | Loss: 0.00011191
Iteration 12/1000 | Loss: 0.00003086
Iteration 13/1000 | Loss: 0.00002818
Iteration 14/1000 | Loss: 0.00002605
Iteration 15/1000 | Loss: 0.00007022
Iteration 16/1000 | Loss: 0.00002591
Iteration 17/1000 | Loss: 0.00004243
Iteration 18/1000 | Loss: 0.00002736
Iteration 19/1000 | Loss: 0.00003717
Iteration 20/1000 | Loss: 0.00002542
Iteration 21/1000 | Loss: 0.00002528
Iteration 22/1000 | Loss: 0.00002526
Iteration 23/1000 | Loss: 0.00002525
Iteration 24/1000 | Loss: 0.00002524
Iteration 25/1000 | Loss: 0.00002523
Iteration 26/1000 | Loss: 0.00002522
Iteration 27/1000 | Loss: 0.00002522
Iteration 28/1000 | Loss: 0.00002521
Iteration 29/1000 | Loss: 0.00002521
Iteration 30/1000 | Loss: 0.00002521
Iteration 31/1000 | Loss: 0.00002521
Iteration 32/1000 | Loss: 0.00002520
Iteration 33/1000 | Loss: 0.00002520
Iteration 34/1000 | Loss: 0.00002520
Iteration 35/1000 | Loss: 0.00002519
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002518
Iteration 38/1000 | Loss: 0.00002518
Iteration 39/1000 | Loss: 0.00002518
Iteration 40/1000 | Loss: 0.00002518
Iteration 41/1000 | Loss: 0.00002518
Iteration 42/1000 | Loss: 0.00002518
Iteration 43/1000 | Loss: 0.00002518
Iteration 44/1000 | Loss: 0.00002518
Iteration 45/1000 | Loss: 0.00002518
Iteration 46/1000 | Loss: 0.00002517
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00002517
Iteration 49/1000 | Loss: 0.00002517
Iteration 50/1000 | Loss: 0.00002516
Iteration 51/1000 | Loss: 0.00002516
Iteration 52/1000 | Loss: 0.00002516
Iteration 53/1000 | Loss: 0.00002516
Iteration 54/1000 | Loss: 0.00002516
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00002515
Iteration 57/1000 | Loss: 0.00002515
Iteration 58/1000 | Loss: 0.00002515
Iteration 59/1000 | Loss: 0.00002515
Iteration 60/1000 | Loss: 0.00002515
Iteration 61/1000 | Loss: 0.00002515
Iteration 62/1000 | Loss: 0.00002515
Iteration 63/1000 | Loss: 0.00002515
Iteration 64/1000 | Loss: 0.00002515
Iteration 65/1000 | Loss: 0.00002515
Iteration 66/1000 | Loss: 0.00002515
Iteration 67/1000 | Loss: 0.00002515
Iteration 68/1000 | Loss: 0.00002515
Iteration 69/1000 | Loss: 0.00002515
Iteration 70/1000 | Loss: 0.00002515
Iteration 71/1000 | Loss: 0.00002515
Iteration 72/1000 | Loss: 0.00002515
Iteration 73/1000 | Loss: 0.00002515
Iteration 74/1000 | Loss: 0.00002515
Iteration 75/1000 | Loss: 0.00002515
Iteration 76/1000 | Loss: 0.00002514
Iteration 77/1000 | Loss: 0.00002514
Iteration 78/1000 | Loss: 0.00002514
Iteration 79/1000 | Loss: 0.00002514
Iteration 80/1000 | Loss: 0.00002514
Iteration 81/1000 | Loss: 0.00002514
Iteration 82/1000 | Loss: 0.00002514
Iteration 83/1000 | Loss: 0.00002514
Iteration 84/1000 | Loss: 0.00002514
Iteration 85/1000 | Loss: 0.00002514
Iteration 86/1000 | Loss: 0.00002514
Iteration 87/1000 | Loss: 0.00002514
Iteration 88/1000 | Loss: 0.00002514
Iteration 89/1000 | Loss: 0.00002514
Iteration 90/1000 | Loss: 0.00002513
Iteration 91/1000 | Loss: 0.00002513
Iteration 92/1000 | Loss: 0.00002513
Iteration 93/1000 | Loss: 0.00002513
Iteration 94/1000 | Loss: 0.00002513
Iteration 95/1000 | Loss: 0.00002513
Iteration 96/1000 | Loss: 0.00002512
Iteration 97/1000 | Loss: 0.00002512
Iteration 98/1000 | Loss: 0.00002512
Iteration 99/1000 | Loss: 0.00002512
Iteration 100/1000 | Loss: 0.00002512
Iteration 101/1000 | Loss: 0.00002512
Iteration 102/1000 | Loss: 0.00002512
Iteration 103/1000 | Loss: 0.00002512
Iteration 104/1000 | Loss: 0.00002512
Iteration 105/1000 | Loss: 0.00002512
Iteration 106/1000 | Loss: 0.00002512
Iteration 107/1000 | Loss: 0.00002512
Iteration 108/1000 | Loss: 0.00002511
Iteration 109/1000 | Loss: 0.00002511
Iteration 110/1000 | Loss: 0.00002511
Iteration 111/1000 | Loss: 0.00002511
Iteration 112/1000 | Loss: 0.00002511
Iteration 113/1000 | Loss: 0.00002511
Iteration 114/1000 | Loss: 0.00002511
Iteration 115/1000 | Loss: 0.00002511
Iteration 116/1000 | Loss: 0.00002511
Iteration 117/1000 | Loss: 0.00002510
Iteration 118/1000 | Loss: 0.00002510
Iteration 119/1000 | Loss: 0.00002510
Iteration 120/1000 | Loss: 0.00002510
Iteration 121/1000 | Loss: 0.00002510
Iteration 122/1000 | Loss: 0.00002510
Iteration 123/1000 | Loss: 0.00002510
Iteration 124/1000 | Loss: 0.00002510
Iteration 125/1000 | Loss: 0.00002510
Iteration 126/1000 | Loss: 0.00002510
Iteration 127/1000 | Loss: 0.00002510
Iteration 128/1000 | Loss: 0.00002509
Iteration 129/1000 | Loss: 0.00002509
Iteration 130/1000 | Loss: 0.00002509
Iteration 131/1000 | Loss: 0.00002509
Iteration 132/1000 | Loss: 0.00002509
Iteration 133/1000 | Loss: 0.00002509
Iteration 134/1000 | Loss: 0.00002508
Iteration 135/1000 | Loss: 0.00010931
Iteration 136/1000 | Loss: 0.00010467
Iteration 137/1000 | Loss: 0.00002508
Iteration 138/1000 | Loss: 0.00009489
Iteration 139/1000 | Loss: 0.00013619
Iteration 140/1000 | Loss: 0.00006674
Iteration 141/1000 | Loss: 0.00008299
Iteration 142/1000 | Loss: 0.00002515
Iteration 143/1000 | Loss: 0.00002511
Iteration 144/1000 | Loss: 0.00002508
Iteration 145/1000 | Loss: 0.00002508
Iteration 146/1000 | Loss: 0.00002508
Iteration 147/1000 | Loss: 0.00002508
Iteration 148/1000 | Loss: 0.00002508
Iteration 149/1000 | Loss: 0.00002508
Iteration 150/1000 | Loss: 0.00002508
Iteration 151/1000 | Loss: 0.00002507
Iteration 152/1000 | Loss: 0.00006471
Iteration 153/1000 | Loss: 0.00002528
Iteration 154/1000 | Loss: 0.00002509
Iteration 155/1000 | Loss: 0.00002508
Iteration 156/1000 | Loss: 0.00002508
Iteration 157/1000 | Loss: 0.00002508
Iteration 158/1000 | Loss: 0.00002507
Iteration 159/1000 | Loss: 0.00002507
Iteration 160/1000 | Loss: 0.00002506
Iteration 161/1000 | Loss: 0.00002506
Iteration 162/1000 | Loss: 0.00002505
Iteration 163/1000 | Loss: 0.00002505
Iteration 164/1000 | Loss: 0.00002505
Iteration 165/1000 | Loss: 0.00002504
Iteration 166/1000 | Loss: 0.00002504
Iteration 167/1000 | Loss: 0.00002504
Iteration 168/1000 | Loss: 0.00002504
Iteration 169/1000 | Loss: 0.00002503
Iteration 170/1000 | Loss: 0.00002503
Iteration 171/1000 | Loss: 0.00002503
Iteration 172/1000 | Loss: 0.00002503
Iteration 173/1000 | Loss: 0.00002503
Iteration 174/1000 | Loss: 0.00002503
Iteration 175/1000 | Loss: 0.00002503
Iteration 176/1000 | Loss: 0.00002503
Iteration 177/1000 | Loss: 0.00002503
Iteration 178/1000 | Loss: 0.00002503
Iteration 179/1000 | Loss: 0.00002503
Iteration 180/1000 | Loss: 0.00002503
Iteration 181/1000 | Loss: 0.00002503
Iteration 182/1000 | Loss: 0.00002503
Iteration 183/1000 | Loss: 0.00002502
Iteration 184/1000 | Loss: 0.00002502
Iteration 185/1000 | Loss: 0.00002502
Iteration 186/1000 | Loss: 0.00002502
Iteration 187/1000 | Loss: 0.00002502
Iteration 188/1000 | Loss: 0.00002502
Iteration 189/1000 | Loss: 0.00002502
Iteration 190/1000 | Loss: 0.00002502
Iteration 191/1000 | Loss: 0.00002502
Iteration 192/1000 | Loss: 0.00002502
Iteration 193/1000 | Loss: 0.00002502
Iteration 194/1000 | Loss: 0.00002502
Iteration 195/1000 | Loss: 0.00002502
Iteration 196/1000 | Loss: 0.00002502
Iteration 197/1000 | Loss: 0.00002502
Iteration 198/1000 | Loss: 0.00002502
Iteration 199/1000 | Loss: 0.00002502
Iteration 200/1000 | Loss: 0.00002502
Iteration 201/1000 | Loss: 0.00002502
Iteration 202/1000 | Loss: 0.00002502
Iteration 203/1000 | Loss: 0.00002502
Iteration 204/1000 | Loss: 0.00002502
Iteration 205/1000 | Loss: 0.00002502
Iteration 206/1000 | Loss: 0.00002502
Iteration 207/1000 | Loss: 0.00002502
Iteration 208/1000 | Loss: 0.00002502
Iteration 209/1000 | Loss: 0.00002502
Iteration 210/1000 | Loss: 0.00002502
Iteration 211/1000 | Loss: 0.00002502
Iteration 212/1000 | Loss: 0.00002502
Iteration 213/1000 | Loss: 0.00002502
Iteration 214/1000 | Loss: 0.00002502
Iteration 215/1000 | Loss: 0.00002502
Iteration 216/1000 | Loss: 0.00002502
Iteration 217/1000 | Loss: 0.00002502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.5024219212355092e-05, 2.5024219212355092e-05, 2.5024219212355092e-05, 2.5024219212355092e-05, 2.5024219212355092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5024219212355092e-05

Optimization complete. Final v2v error: 4.248478889465332 mm

Highest mean error: 5.453344345092773 mm for frame 38

Lowest mean error: 3.833319902420044 mm for frame 151

Saving results

Total time: 68.72825241088867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107306
Iteration 2/25 | Loss: 0.00165986
Iteration 3/25 | Loss: 0.00095894
Iteration 4/25 | Loss: 0.00085146
Iteration 5/25 | Loss: 0.00078834
Iteration 6/25 | Loss: 0.00076966
Iteration 7/25 | Loss: 0.00076622
Iteration 8/25 | Loss: 0.00076132
Iteration 9/25 | Loss: 0.00076099
Iteration 10/25 | Loss: 0.00075861
Iteration 11/25 | Loss: 0.00075808
Iteration 12/25 | Loss: 0.00075772
Iteration 13/25 | Loss: 0.00075527
Iteration 14/25 | Loss: 0.00075357
Iteration 15/25 | Loss: 0.00075286
Iteration 16/25 | Loss: 0.00075211
Iteration 17/25 | Loss: 0.00075168
Iteration 18/25 | Loss: 0.00075152
Iteration 19/25 | Loss: 0.00075146
Iteration 20/25 | Loss: 0.00075145
Iteration 21/25 | Loss: 0.00075145
Iteration 22/25 | Loss: 0.00075144
Iteration 23/25 | Loss: 0.00075143
Iteration 24/25 | Loss: 0.00075143
Iteration 25/25 | Loss: 0.00075143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.54408073
Iteration 2/25 | Loss: 0.00052046
Iteration 3/25 | Loss: 0.00044695
Iteration 4/25 | Loss: 0.00044695
Iteration 5/25 | Loss: 0.00044695
Iteration 6/25 | Loss: 0.00044695
Iteration 7/25 | Loss: 0.00044695
Iteration 8/25 | Loss: 0.00044694
Iteration 9/25 | Loss: 0.00044694
Iteration 10/25 | Loss: 0.00044694
Iteration 11/25 | Loss: 0.00044694
Iteration 12/25 | Loss: 0.00044694
Iteration 13/25 | Loss: 0.00044694
Iteration 14/25 | Loss: 0.00044694
Iteration 15/25 | Loss: 0.00044694
Iteration 16/25 | Loss: 0.00044694
Iteration 17/25 | Loss: 0.00044694
Iteration 18/25 | Loss: 0.00044694
Iteration 19/25 | Loss: 0.00044694
Iteration 20/25 | Loss: 0.00044694
Iteration 21/25 | Loss: 0.00044694
Iteration 22/25 | Loss: 0.00044694
Iteration 23/25 | Loss: 0.00044694
Iteration 24/25 | Loss: 0.00044694
Iteration 25/25 | Loss: 0.00044694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044694
Iteration 2/1000 | Loss: 0.00010333
Iteration 3/1000 | Loss: 0.00009192
Iteration 4/1000 | Loss: 0.00026000
Iteration 5/1000 | Loss: 0.00057294
Iteration 6/1000 | Loss: 0.00107113
Iteration 7/1000 | Loss: 0.00146566
Iteration 8/1000 | Loss: 0.00004700
Iteration 9/1000 | Loss: 0.00004486
Iteration 10/1000 | Loss: 0.00002571
Iteration 11/1000 | Loss: 0.00002153
Iteration 12/1000 | Loss: 0.00002022
Iteration 13/1000 | Loss: 0.00005233
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001941
Iteration 19/1000 | Loss: 0.00006423
Iteration 20/1000 | Loss: 0.00009068
Iteration 21/1000 | Loss: 0.00003324
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00005081
Iteration 24/1000 | Loss: 0.00001941
Iteration 25/1000 | Loss: 0.00004110
Iteration 26/1000 | Loss: 0.00003315
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00003603
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00001858
Iteration 31/1000 | Loss: 0.00001853
Iteration 32/1000 | Loss: 0.00006914
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00003684
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001829
Iteration 37/1000 | Loss: 0.00001828
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001827
Iteration 40/1000 | Loss: 0.00001827
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001826
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001825
Iteration 48/1000 | Loss: 0.00001824
Iteration 49/1000 | Loss: 0.00001824
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001823
Iteration 53/1000 | Loss: 0.00001823
Iteration 54/1000 | Loss: 0.00001823
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001821
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001818
Iteration 81/1000 | Loss: 0.00001818
Iteration 82/1000 | Loss: 0.00001818
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001816
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001816
Iteration 96/1000 | Loss: 0.00001816
Iteration 97/1000 | Loss: 0.00001816
Iteration 98/1000 | Loss: 0.00001816
Iteration 99/1000 | Loss: 0.00001816
Iteration 100/1000 | Loss: 0.00001816
Iteration 101/1000 | Loss: 0.00001816
Iteration 102/1000 | Loss: 0.00001815
Iteration 103/1000 | Loss: 0.00001815
Iteration 104/1000 | Loss: 0.00001815
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001815
Iteration 108/1000 | Loss: 0.00001815
Iteration 109/1000 | Loss: 0.00001815
Iteration 110/1000 | Loss: 0.00001823
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001823
Iteration 113/1000 | Loss: 0.00001823
Iteration 114/1000 | Loss: 0.00001823
Iteration 115/1000 | Loss: 0.00001823
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001821
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00001814
Iteration 123/1000 | Loss: 0.00001814
Iteration 124/1000 | Loss: 0.00001814
Iteration 125/1000 | Loss: 0.00001814
Iteration 126/1000 | Loss: 0.00001814
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001820
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00001820
Iteration 146/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.820397301344201e-05, 1.820397301344201e-05, 1.820397301344201e-05, 1.820397301344201e-05, 1.820397301344201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.820397301344201e-05

Optimization complete. Final v2v error: 3.5887610912323 mm

Highest mean error: 9.255376815795898 mm for frame 81

Lowest mean error: 3.1449432373046875 mm for frame 208

Saving results

Total time: 93.9121036529541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043439
Iteration 2/25 | Loss: 0.01043438
Iteration 3/25 | Loss: 0.01043438
Iteration 4/25 | Loss: 0.01043438
Iteration 5/25 | Loss: 0.01043438
Iteration 6/25 | Loss: 0.01043438
Iteration 7/25 | Loss: 0.01043438
Iteration 8/25 | Loss: 0.01043437
Iteration 9/25 | Loss: 0.01043437
Iteration 10/25 | Loss: 0.01043437
Iteration 11/25 | Loss: 0.01043437
Iteration 12/25 | Loss: 0.01043436
Iteration 13/25 | Loss: 0.01043436
Iteration 14/25 | Loss: 0.01043436
Iteration 15/25 | Loss: 0.01043435
Iteration 16/25 | Loss: 0.01043435
Iteration 17/25 | Loss: 0.01043435
Iteration 18/25 | Loss: 0.01043434
Iteration 19/25 | Loss: 0.01043434
Iteration 20/25 | Loss: 0.01043434
Iteration 21/25 | Loss: 0.01043434
Iteration 22/25 | Loss: 0.01043434
Iteration 23/25 | Loss: 0.01043433
Iteration 24/25 | Loss: 0.01043433
Iteration 25/25 | Loss: 0.01043432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65652180
Iteration 2/25 | Loss: 0.17898864
Iteration 3/25 | Loss: 0.17761984
Iteration 4/25 | Loss: 0.17333522
Iteration 5/25 | Loss: 0.17315024
Iteration 6/25 | Loss: 0.17315024
Iteration 7/25 | Loss: 0.17315021
Iteration 8/25 | Loss: 0.17315021
Iteration 9/25 | Loss: 0.17315021
Iteration 10/25 | Loss: 0.17315021
Iteration 11/25 | Loss: 0.17315021
Iteration 12/25 | Loss: 0.17315021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.1731502115726471, 0.1731502115726471, 0.1731502115726471, 0.1731502115726471, 0.1731502115726471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1731502115726471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17315021
Iteration 2/1000 | Loss: 0.00668128
Iteration 3/1000 | Loss: 0.00260663
Iteration 4/1000 | Loss: 0.00273281
Iteration 5/1000 | Loss: 0.00099197
Iteration 6/1000 | Loss: 0.00095174
Iteration 7/1000 | Loss: 0.00075918
Iteration 8/1000 | Loss: 0.00033753
Iteration 9/1000 | Loss: 0.00034933
Iteration 10/1000 | Loss: 0.00043695
Iteration 11/1000 | Loss: 0.00033909
Iteration 12/1000 | Loss: 0.00033291
Iteration 13/1000 | Loss: 0.00027233
Iteration 14/1000 | Loss: 0.00045541
Iteration 15/1000 | Loss: 0.00041861
Iteration 16/1000 | Loss: 0.00015742
Iteration 17/1000 | Loss: 0.00005918
Iteration 18/1000 | Loss: 0.00009517
Iteration 19/1000 | Loss: 0.00003974
Iteration 20/1000 | Loss: 0.00022313
Iteration 21/1000 | Loss: 0.00004676
Iteration 22/1000 | Loss: 0.00003365
Iteration 23/1000 | Loss: 0.00003189
Iteration 24/1000 | Loss: 0.00004998
Iteration 25/1000 | Loss: 0.00014410
Iteration 26/1000 | Loss: 0.00002851
Iteration 27/1000 | Loss: 0.00002778
Iteration 28/1000 | Loss: 0.00006760
Iteration 29/1000 | Loss: 0.00002710
Iteration 30/1000 | Loss: 0.00002683
Iteration 31/1000 | Loss: 0.00008292
Iteration 32/1000 | Loss: 0.00002656
Iteration 33/1000 | Loss: 0.00002642
Iteration 34/1000 | Loss: 0.00002642
Iteration 35/1000 | Loss: 0.00002642
Iteration 36/1000 | Loss: 0.00002641
Iteration 37/1000 | Loss: 0.00002641
Iteration 38/1000 | Loss: 0.00002641
Iteration 39/1000 | Loss: 0.00002640
Iteration 40/1000 | Loss: 0.00002634
Iteration 41/1000 | Loss: 0.00002633
Iteration 42/1000 | Loss: 0.00002632
Iteration 43/1000 | Loss: 0.00002631
Iteration 44/1000 | Loss: 0.00002631
Iteration 45/1000 | Loss: 0.00002630
Iteration 46/1000 | Loss: 0.00002630
Iteration 47/1000 | Loss: 0.00002629
Iteration 48/1000 | Loss: 0.00002628
Iteration 49/1000 | Loss: 0.00002623
Iteration 50/1000 | Loss: 0.00002623
Iteration 51/1000 | Loss: 0.00002623
Iteration 52/1000 | Loss: 0.00005247
Iteration 53/1000 | Loss: 0.00002670
Iteration 54/1000 | Loss: 0.00002616
Iteration 55/1000 | Loss: 0.00002606
Iteration 56/1000 | Loss: 0.00002606
Iteration 57/1000 | Loss: 0.00002606
Iteration 58/1000 | Loss: 0.00002606
Iteration 59/1000 | Loss: 0.00002606
Iteration 60/1000 | Loss: 0.00002606
Iteration 61/1000 | Loss: 0.00002606
Iteration 62/1000 | Loss: 0.00002605
Iteration 63/1000 | Loss: 0.00002605
Iteration 64/1000 | Loss: 0.00002605
Iteration 65/1000 | Loss: 0.00002605
Iteration 66/1000 | Loss: 0.00002604
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002604
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002604
Iteration 71/1000 | Loss: 0.00002604
Iteration 72/1000 | Loss: 0.00002603
Iteration 73/1000 | Loss: 0.00002603
Iteration 74/1000 | Loss: 0.00002603
Iteration 75/1000 | Loss: 0.00002603
Iteration 76/1000 | Loss: 0.00002603
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002603
Iteration 79/1000 | Loss: 0.00002603
Iteration 80/1000 | Loss: 0.00002603
Iteration 81/1000 | Loss: 0.00002603
Iteration 82/1000 | Loss: 0.00002603
Iteration 83/1000 | Loss: 0.00002603
Iteration 84/1000 | Loss: 0.00002603
Iteration 85/1000 | Loss: 0.00002602
Iteration 86/1000 | Loss: 0.00002602
Iteration 87/1000 | Loss: 0.00002601
Iteration 88/1000 | Loss: 0.00002601
Iteration 89/1000 | Loss: 0.00002601
Iteration 90/1000 | Loss: 0.00002601
Iteration 91/1000 | Loss: 0.00002600
Iteration 92/1000 | Loss: 0.00002600
Iteration 93/1000 | Loss: 0.00002600
Iteration 94/1000 | Loss: 0.00002600
Iteration 95/1000 | Loss: 0.00002600
Iteration 96/1000 | Loss: 0.00002600
Iteration 97/1000 | Loss: 0.00002600
Iteration 98/1000 | Loss: 0.00002600
Iteration 99/1000 | Loss: 0.00002600
Iteration 100/1000 | Loss: 0.00002600
Iteration 101/1000 | Loss: 0.00002600
Iteration 102/1000 | Loss: 0.00002600
Iteration 103/1000 | Loss: 0.00002599
Iteration 104/1000 | Loss: 0.00002599
Iteration 105/1000 | Loss: 0.00002599
Iteration 106/1000 | Loss: 0.00002599
Iteration 107/1000 | Loss: 0.00002599
Iteration 108/1000 | Loss: 0.00002599
Iteration 109/1000 | Loss: 0.00002599
Iteration 110/1000 | Loss: 0.00002599
Iteration 111/1000 | Loss: 0.00002599
Iteration 112/1000 | Loss: 0.00002599
Iteration 113/1000 | Loss: 0.00002599
Iteration 114/1000 | Loss: 0.00002599
Iteration 115/1000 | Loss: 0.00002599
Iteration 116/1000 | Loss: 0.00002599
Iteration 117/1000 | Loss: 0.00002598
Iteration 118/1000 | Loss: 0.00002598
Iteration 119/1000 | Loss: 0.00002598
Iteration 120/1000 | Loss: 0.00002598
Iteration 121/1000 | Loss: 0.00002598
Iteration 122/1000 | Loss: 0.00002598
Iteration 123/1000 | Loss: 0.00002598
Iteration 124/1000 | Loss: 0.00002598
Iteration 125/1000 | Loss: 0.00002598
Iteration 126/1000 | Loss: 0.00002598
Iteration 127/1000 | Loss: 0.00002598
Iteration 128/1000 | Loss: 0.00002597
Iteration 129/1000 | Loss: 0.00002597
Iteration 130/1000 | Loss: 0.00002597
Iteration 131/1000 | Loss: 0.00002597
Iteration 132/1000 | Loss: 0.00002597
Iteration 133/1000 | Loss: 0.00002597
Iteration 134/1000 | Loss: 0.00002597
Iteration 135/1000 | Loss: 0.00002597
Iteration 136/1000 | Loss: 0.00002597
Iteration 137/1000 | Loss: 0.00002597
Iteration 138/1000 | Loss: 0.00002597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.5972694857046008e-05, 2.5972694857046008e-05, 2.5972694857046008e-05, 2.5972694857046008e-05, 2.5972694857046008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5972694857046008e-05

Optimization complete. Final v2v error: 4.391701698303223 mm

Highest mean error: 4.829512119293213 mm for frame 14

Lowest mean error: 3.7470908164978027 mm for frame 1

Saving results

Total time: 79.02874112129211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0513/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0513/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541719
Iteration 2/25 | Loss: 0.00087104
Iteration 3/25 | Loss: 0.00074971
Iteration 4/25 | Loss: 0.00072727
Iteration 5/25 | Loss: 0.00072092
Iteration 6/25 | Loss: 0.00071961
Iteration 7/25 | Loss: 0.00071918
Iteration 8/25 | Loss: 0.00071918
Iteration 9/25 | Loss: 0.00071918
Iteration 10/25 | Loss: 0.00071918
Iteration 11/25 | Loss: 0.00071918
Iteration 12/25 | Loss: 0.00071918
Iteration 13/25 | Loss: 0.00071918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007191812619566917, 0.0007191812619566917, 0.0007191812619566917, 0.0007191812619566917, 0.0007191812619566917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007191812619566917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.83045387
Iteration 2/25 | Loss: 0.00042468
Iteration 3/25 | Loss: 0.00042466
Iteration 4/25 | Loss: 0.00042466
Iteration 5/25 | Loss: 0.00042466
Iteration 6/25 | Loss: 0.00042466
Iteration 7/25 | Loss: 0.00042466
Iteration 8/25 | Loss: 0.00042466
Iteration 9/25 | Loss: 0.00042466
Iteration 10/25 | Loss: 0.00042466
Iteration 11/25 | Loss: 0.00042466
Iteration 12/25 | Loss: 0.00042466
Iteration 13/25 | Loss: 0.00042466
Iteration 14/25 | Loss: 0.00042466
Iteration 15/25 | Loss: 0.00042466
Iteration 16/25 | Loss: 0.00042466
Iteration 17/25 | Loss: 0.00042466
Iteration 18/25 | Loss: 0.00042466
Iteration 19/25 | Loss: 0.00042466
Iteration 20/25 | Loss: 0.00042466
Iteration 21/25 | Loss: 0.00042466
Iteration 22/25 | Loss: 0.00042466
Iteration 23/25 | Loss: 0.00042466
Iteration 24/25 | Loss: 0.00042466
Iteration 25/25 | Loss: 0.00042466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042466
Iteration 2/1000 | Loss: 0.00004552
Iteration 3/1000 | Loss: 0.00003379
Iteration 4/1000 | Loss: 0.00002789
Iteration 5/1000 | Loss: 0.00002549
Iteration 6/1000 | Loss: 0.00002377
Iteration 7/1000 | Loss: 0.00002306
Iteration 8/1000 | Loss: 0.00002260
Iteration 9/1000 | Loss: 0.00002234
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002165
Iteration 13/1000 | Loss: 0.00002158
Iteration 14/1000 | Loss: 0.00002154
Iteration 15/1000 | Loss: 0.00002154
Iteration 16/1000 | Loss: 0.00002153
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002149
Iteration 20/1000 | Loss: 0.00002148
Iteration 21/1000 | Loss: 0.00002143
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002136
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002129
Iteration 26/1000 | Loss: 0.00002127
Iteration 27/1000 | Loss: 0.00002127
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002126
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002124
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00002122
Iteration 34/1000 | Loss: 0.00002122
Iteration 35/1000 | Loss: 0.00002119
Iteration 36/1000 | Loss: 0.00002118
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002117
Iteration 40/1000 | Loss: 0.00002117
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002116
Iteration 43/1000 | Loss: 0.00002116
Iteration 44/1000 | Loss: 0.00002116
Iteration 45/1000 | Loss: 0.00002116
Iteration 46/1000 | Loss: 0.00002116
Iteration 47/1000 | Loss: 0.00002115
Iteration 48/1000 | Loss: 0.00002115
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002114
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00002114
Iteration 56/1000 | Loss: 0.00002113
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00002113
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002112
Iteration 62/1000 | Loss: 0.00002112
Iteration 63/1000 | Loss: 0.00002112
Iteration 64/1000 | Loss: 0.00002112
Iteration 65/1000 | Loss: 0.00002112
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002111
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002110
Iteration 70/1000 | Loss: 0.00002110
Iteration 71/1000 | Loss: 0.00002110
Iteration 72/1000 | Loss: 0.00002110
Iteration 73/1000 | Loss: 0.00002110
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002109
Iteration 76/1000 | Loss: 0.00002109
Iteration 77/1000 | Loss: 0.00002108
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00002108
Iteration 80/1000 | Loss: 0.00002108
Iteration 81/1000 | Loss: 0.00002108
Iteration 82/1000 | Loss: 0.00002107
Iteration 83/1000 | Loss: 0.00002107
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002106
Iteration 86/1000 | Loss: 0.00002106
Iteration 87/1000 | Loss: 0.00002106
Iteration 88/1000 | Loss: 0.00002105
Iteration 89/1000 | Loss: 0.00002105
Iteration 90/1000 | Loss: 0.00002105
Iteration 91/1000 | Loss: 0.00002105
Iteration 92/1000 | Loss: 0.00002104
Iteration 93/1000 | Loss: 0.00002104
Iteration 94/1000 | Loss: 0.00002104
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002104
Iteration 97/1000 | Loss: 0.00002103
Iteration 98/1000 | Loss: 0.00002103
Iteration 99/1000 | Loss: 0.00002103
Iteration 100/1000 | Loss: 0.00002103
Iteration 101/1000 | Loss: 0.00002103
Iteration 102/1000 | Loss: 0.00002103
Iteration 103/1000 | Loss: 0.00002103
Iteration 104/1000 | Loss: 0.00002102
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002102
Iteration 107/1000 | Loss: 0.00002102
Iteration 108/1000 | Loss: 0.00002102
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002102
Iteration 112/1000 | Loss: 0.00002101
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002100
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002100
Iteration 128/1000 | Loss: 0.00002100
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002100
Iteration 131/1000 | Loss: 0.00002100
Iteration 132/1000 | Loss: 0.00002100
Iteration 133/1000 | Loss: 0.00002100
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00002100
Iteration 136/1000 | Loss: 0.00002100
Iteration 137/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.1000301785534248e-05, 2.1000301785534248e-05, 2.1000301785534248e-05, 2.1000301785534248e-05, 2.1000301785534248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1000301785534248e-05

Optimization complete. Final v2v error: 3.9210803508758545 mm

Highest mean error: 4.499048709869385 mm for frame 106

Lowest mean error: 3.392608404159546 mm for frame 0

Saving results

Total time: 39.497217655181885
