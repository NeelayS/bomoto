Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=163, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9128-9183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079270
Iteration 2/25 | Loss: 0.00131166
Iteration 3/25 | Loss: 0.00112148
Iteration 4/25 | Loss: 0.00110156
Iteration 5/25 | Loss: 0.00109444
Iteration 6/25 | Loss: 0.00109317
Iteration 7/25 | Loss: 0.00109276
Iteration 8/25 | Loss: 0.00109276
Iteration 9/25 | Loss: 0.00109276
Iteration 10/25 | Loss: 0.00109276
Iteration 11/25 | Loss: 0.00109276
Iteration 12/25 | Loss: 0.00109276
Iteration 13/25 | Loss: 0.00109276
Iteration 14/25 | Loss: 0.00109276
Iteration 15/25 | Loss: 0.00109276
Iteration 16/25 | Loss: 0.00109276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010927572147920728, 0.0010927572147920728, 0.0010927572147920728, 0.0010927572147920728, 0.0010927572147920728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010927572147920728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71752644
Iteration 2/25 | Loss: 0.00096036
Iteration 3/25 | Loss: 0.00096036
Iteration 4/25 | Loss: 0.00096036
Iteration 5/25 | Loss: 0.00096036
Iteration 6/25 | Loss: 0.00096036
Iteration 7/25 | Loss: 0.00096036
Iteration 8/25 | Loss: 0.00096036
Iteration 9/25 | Loss: 0.00096036
Iteration 10/25 | Loss: 0.00096036
Iteration 11/25 | Loss: 0.00096036
Iteration 12/25 | Loss: 0.00096036
Iteration 13/25 | Loss: 0.00096036
Iteration 14/25 | Loss: 0.00096036
Iteration 15/25 | Loss: 0.00096036
Iteration 16/25 | Loss: 0.00096036
Iteration 17/25 | Loss: 0.00096036
Iteration 18/25 | Loss: 0.00096036
Iteration 19/25 | Loss: 0.00096036
Iteration 20/25 | Loss: 0.00096036
Iteration 21/25 | Loss: 0.00096036
Iteration 22/25 | Loss: 0.00096036
Iteration 23/25 | Loss: 0.00096036
Iteration 24/25 | Loss: 0.00096036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009603617363609374, 0.0009603617363609374, 0.0009603617363609374, 0.0009603617363609374, 0.0009603617363609374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009603617363609374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096036
Iteration 2/1000 | Loss: 0.00004845
Iteration 3/1000 | Loss: 0.00003347
Iteration 4/1000 | Loss: 0.00002875
Iteration 5/1000 | Loss: 0.00002706
Iteration 6/1000 | Loss: 0.00002598
Iteration 7/1000 | Loss: 0.00002540
Iteration 8/1000 | Loss: 0.00002510
Iteration 9/1000 | Loss: 0.00002493
Iteration 10/1000 | Loss: 0.00002489
Iteration 11/1000 | Loss: 0.00002484
Iteration 12/1000 | Loss: 0.00002478
Iteration 13/1000 | Loss: 0.00002478
Iteration 14/1000 | Loss: 0.00002476
Iteration 15/1000 | Loss: 0.00002476
Iteration 16/1000 | Loss: 0.00002475
Iteration 17/1000 | Loss: 0.00002475
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002475
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002473
Iteration 23/1000 | Loss: 0.00002473
Iteration 24/1000 | Loss: 0.00002472
Iteration 25/1000 | Loss: 0.00002471
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002471
Iteration 28/1000 | Loss: 0.00002471
Iteration 29/1000 | Loss: 0.00002471
Iteration 30/1000 | Loss: 0.00002470
Iteration 31/1000 | Loss: 0.00002470
Iteration 32/1000 | Loss: 0.00002470
Iteration 33/1000 | Loss: 0.00002470
Iteration 34/1000 | Loss: 0.00002470
Iteration 35/1000 | Loss: 0.00002470
Iteration 36/1000 | Loss: 0.00002469
Iteration 37/1000 | Loss: 0.00002469
Iteration 38/1000 | Loss: 0.00002469
Iteration 39/1000 | Loss: 0.00002469
Iteration 40/1000 | Loss: 0.00002469
Iteration 41/1000 | Loss: 0.00002469
Iteration 42/1000 | Loss: 0.00002468
Iteration 43/1000 | Loss: 0.00002468
Iteration 44/1000 | Loss: 0.00002468
Iteration 45/1000 | Loss: 0.00002468
Iteration 46/1000 | Loss: 0.00002468
Iteration 47/1000 | Loss: 0.00002468
Iteration 48/1000 | Loss: 0.00002468
Iteration 49/1000 | Loss: 0.00002468
Iteration 50/1000 | Loss: 0.00002468
Iteration 51/1000 | Loss: 0.00002468
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002468
Iteration 54/1000 | Loss: 0.00002468
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002467
Iteration 57/1000 | Loss: 0.00002467
Iteration 58/1000 | Loss: 0.00002467
Iteration 59/1000 | Loss: 0.00002467
Iteration 60/1000 | Loss: 0.00002467
Iteration 61/1000 | Loss: 0.00002467
Iteration 62/1000 | Loss: 0.00002467
Iteration 63/1000 | Loss: 0.00002467
Iteration 64/1000 | Loss: 0.00002466
Iteration 65/1000 | Loss: 0.00002466
Iteration 66/1000 | Loss: 0.00002466
Iteration 67/1000 | Loss: 0.00002466
Iteration 68/1000 | Loss: 0.00002466
Iteration 69/1000 | Loss: 0.00002466
Iteration 70/1000 | Loss: 0.00002466
Iteration 71/1000 | Loss: 0.00002466
Iteration 72/1000 | Loss: 0.00002466
Iteration 73/1000 | Loss: 0.00002466
Iteration 74/1000 | Loss: 0.00002466
Iteration 75/1000 | Loss: 0.00002466
Iteration 76/1000 | Loss: 0.00002466
Iteration 77/1000 | Loss: 0.00002465
Iteration 78/1000 | Loss: 0.00002465
Iteration 79/1000 | Loss: 0.00002465
Iteration 80/1000 | Loss: 0.00002465
Iteration 81/1000 | Loss: 0.00002465
Iteration 82/1000 | Loss: 0.00002465
Iteration 83/1000 | Loss: 0.00002465
Iteration 84/1000 | Loss: 0.00002465
Iteration 85/1000 | Loss: 0.00002465
Iteration 86/1000 | Loss: 0.00002465
Iteration 87/1000 | Loss: 0.00002465
Iteration 88/1000 | Loss: 0.00002465
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002465
Iteration 92/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.4650924387970008e-05, 2.4650924387970008e-05, 2.4650924387970008e-05, 2.4650924387970008e-05, 2.4650924387970008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4650924387970008e-05

Optimization complete. Final v2v error: 4.284680366516113 mm

Highest mean error: 4.663679122924805 mm for frame 53

Lowest mean error: 3.761779546737671 mm for frame 8

Saving results

Total time: 30.112284660339355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435806
Iteration 2/25 | Loss: 0.00133926
Iteration 3/25 | Loss: 0.00111449
Iteration 4/25 | Loss: 0.00108425
Iteration 5/25 | Loss: 0.00107093
Iteration 6/25 | Loss: 0.00106769
Iteration 7/25 | Loss: 0.00106687
Iteration 8/25 | Loss: 0.00106687
Iteration 9/25 | Loss: 0.00106687
Iteration 10/25 | Loss: 0.00106687
Iteration 11/25 | Loss: 0.00106687
Iteration 12/25 | Loss: 0.00106687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010668650502339005, 0.0010668650502339005, 0.0010668650502339005, 0.0010668650502339005, 0.0010668650502339005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010668650502339005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49386418
Iteration 2/25 | Loss: 0.00101610
Iteration 3/25 | Loss: 0.00101610
Iteration 4/25 | Loss: 0.00101609
Iteration 5/25 | Loss: 0.00101609
Iteration 6/25 | Loss: 0.00101609
Iteration 7/25 | Loss: 0.00101609
Iteration 8/25 | Loss: 0.00101609
Iteration 9/25 | Loss: 0.00101609
Iteration 10/25 | Loss: 0.00101609
Iteration 11/25 | Loss: 0.00101609
Iteration 12/25 | Loss: 0.00101609
Iteration 13/25 | Loss: 0.00101609
Iteration 14/25 | Loss: 0.00101609
Iteration 15/25 | Loss: 0.00101609
Iteration 16/25 | Loss: 0.00101609
Iteration 17/25 | Loss: 0.00101609
Iteration 18/25 | Loss: 0.00101609
Iteration 19/25 | Loss: 0.00101609
Iteration 20/25 | Loss: 0.00101609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010160927195101976, 0.0010160927195101976, 0.0010160927195101976, 0.0010160927195101976, 0.0010160927195101976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010160927195101976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101609
Iteration 2/1000 | Loss: 0.00006003
Iteration 3/1000 | Loss: 0.00003815
Iteration 4/1000 | Loss: 0.00003051
Iteration 5/1000 | Loss: 0.00002774
Iteration 6/1000 | Loss: 0.00002657
Iteration 7/1000 | Loss: 0.00002558
Iteration 8/1000 | Loss: 0.00002490
Iteration 9/1000 | Loss: 0.00002432
Iteration 10/1000 | Loss: 0.00002387
Iteration 11/1000 | Loss: 0.00002362
Iteration 12/1000 | Loss: 0.00002346
Iteration 13/1000 | Loss: 0.00002341
Iteration 14/1000 | Loss: 0.00002334
Iteration 15/1000 | Loss: 0.00002327
Iteration 16/1000 | Loss: 0.00002324
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002322
Iteration 19/1000 | Loss: 0.00002321
Iteration 20/1000 | Loss: 0.00002321
Iteration 21/1000 | Loss: 0.00002320
Iteration 22/1000 | Loss: 0.00002319
Iteration 23/1000 | Loss: 0.00002317
Iteration 24/1000 | Loss: 0.00002317
Iteration 25/1000 | Loss: 0.00002317
Iteration 26/1000 | Loss: 0.00002316
Iteration 27/1000 | Loss: 0.00002316
Iteration 28/1000 | Loss: 0.00002315
Iteration 29/1000 | Loss: 0.00002313
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00002313
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00002312
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002310
Iteration 37/1000 | Loss: 0.00002310
Iteration 38/1000 | Loss: 0.00002310
Iteration 39/1000 | Loss: 0.00002310
Iteration 40/1000 | Loss: 0.00002310
Iteration 41/1000 | Loss: 0.00002310
Iteration 42/1000 | Loss: 0.00002309
Iteration 43/1000 | Loss: 0.00002309
Iteration 44/1000 | Loss: 0.00002309
Iteration 45/1000 | Loss: 0.00002309
Iteration 46/1000 | Loss: 0.00002309
Iteration 47/1000 | Loss: 0.00002309
Iteration 48/1000 | Loss: 0.00002309
Iteration 49/1000 | Loss: 0.00002309
Iteration 50/1000 | Loss: 0.00002309
Iteration 51/1000 | Loss: 0.00002309
Iteration 52/1000 | Loss: 0.00002308
Iteration 53/1000 | Loss: 0.00002308
Iteration 54/1000 | Loss: 0.00002308
Iteration 55/1000 | Loss: 0.00002307
Iteration 56/1000 | Loss: 0.00002307
Iteration 57/1000 | Loss: 0.00002307
Iteration 58/1000 | Loss: 0.00002307
Iteration 59/1000 | Loss: 0.00002306
Iteration 60/1000 | Loss: 0.00002306
Iteration 61/1000 | Loss: 0.00002306
Iteration 62/1000 | Loss: 0.00002306
Iteration 63/1000 | Loss: 0.00002305
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002305
Iteration 70/1000 | Loss: 0.00002305
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00002305
Iteration 73/1000 | Loss: 0.00002304
Iteration 74/1000 | Loss: 0.00002304
Iteration 75/1000 | Loss: 0.00002304
Iteration 76/1000 | Loss: 0.00002304
Iteration 77/1000 | Loss: 0.00002304
Iteration 78/1000 | Loss: 0.00002304
Iteration 79/1000 | Loss: 0.00002304
Iteration 80/1000 | Loss: 0.00002304
Iteration 81/1000 | Loss: 0.00002303
Iteration 82/1000 | Loss: 0.00002303
Iteration 83/1000 | Loss: 0.00002303
Iteration 84/1000 | Loss: 0.00002303
Iteration 85/1000 | Loss: 0.00002303
Iteration 86/1000 | Loss: 0.00002303
Iteration 87/1000 | Loss: 0.00002303
Iteration 88/1000 | Loss: 0.00002303
Iteration 89/1000 | Loss: 0.00002303
Iteration 90/1000 | Loss: 0.00002303
Iteration 91/1000 | Loss: 0.00002303
Iteration 92/1000 | Loss: 0.00002303
Iteration 93/1000 | Loss: 0.00002302
Iteration 94/1000 | Loss: 0.00002302
Iteration 95/1000 | Loss: 0.00002302
Iteration 96/1000 | Loss: 0.00002302
Iteration 97/1000 | Loss: 0.00002302
Iteration 98/1000 | Loss: 0.00002302
Iteration 99/1000 | Loss: 0.00002302
Iteration 100/1000 | Loss: 0.00002302
Iteration 101/1000 | Loss: 0.00002302
Iteration 102/1000 | Loss: 0.00002302
Iteration 103/1000 | Loss: 0.00002301
Iteration 104/1000 | Loss: 0.00002301
Iteration 105/1000 | Loss: 0.00002301
Iteration 106/1000 | Loss: 0.00002301
Iteration 107/1000 | Loss: 0.00002301
Iteration 108/1000 | Loss: 0.00002301
Iteration 109/1000 | Loss: 0.00002301
Iteration 110/1000 | Loss: 0.00002301
Iteration 111/1000 | Loss: 0.00002301
Iteration 112/1000 | Loss: 0.00002301
Iteration 113/1000 | Loss: 0.00002301
Iteration 114/1000 | Loss: 0.00002301
Iteration 115/1000 | Loss: 0.00002301
Iteration 116/1000 | Loss: 0.00002301
Iteration 117/1000 | Loss: 0.00002300
Iteration 118/1000 | Loss: 0.00002300
Iteration 119/1000 | Loss: 0.00002300
Iteration 120/1000 | Loss: 0.00002300
Iteration 121/1000 | Loss: 0.00002300
Iteration 122/1000 | Loss: 0.00002300
Iteration 123/1000 | Loss: 0.00002300
Iteration 124/1000 | Loss: 0.00002300
Iteration 125/1000 | Loss: 0.00002300
Iteration 126/1000 | Loss: 0.00002300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.3001834051683545e-05, 2.3001834051683545e-05, 2.3001834051683545e-05, 2.3001834051683545e-05, 2.3001834051683545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3001834051683545e-05

Optimization complete. Final v2v error: 4.086442470550537 mm

Highest mean error: 4.4071550369262695 mm for frame 161

Lowest mean error: 3.8098971843719482 mm for frame 90

Saving results

Total time: 38.265705585479736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122480
Iteration 2/25 | Loss: 0.00247598
Iteration 3/25 | Loss: 0.00204508
Iteration 4/25 | Loss: 0.00196626
Iteration 5/25 | Loss: 0.00192093
Iteration 6/25 | Loss: 0.00177512
Iteration 7/25 | Loss: 0.00149834
Iteration 8/25 | Loss: 0.00125262
Iteration 9/25 | Loss: 0.00116589
Iteration 10/25 | Loss: 0.00115397
Iteration 11/25 | Loss: 0.00115513
Iteration 12/25 | Loss: 0.00114679
Iteration 13/25 | Loss: 0.00113950
Iteration 14/25 | Loss: 0.00113255
Iteration 15/25 | Loss: 0.00112900
Iteration 16/25 | Loss: 0.00112728
Iteration 17/25 | Loss: 0.00112686
Iteration 18/25 | Loss: 0.00112670
Iteration 19/25 | Loss: 0.00112667
Iteration 20/25 | Loss: 0.00112666
Iteration 21/25 | Loss: 0.00112666
Iteration 22/25 | Loss: 0.00112666
Iteration 23/25 | Loss: 0.00112665
Iteration 24/25 | Loss: 0.00112665
Iteration 25/25 | Loss: 0.00112665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42754340
Iteration 2/25 | Loss: 0.00093120
Iteration 3/25 | Loss: 0.00093118
Iteration 4/25 | Loss: 0.00093118
Iteration 5/25 | Loss: 0.00093118
Iteration 6/25 | Loss: 0.00093118
Iteration 7/25 | Loss: 0.00093118
Iteration 8/25 | Loss: 0.00093118
Iteration 9/25 | Loss: 0.00093118
Iteration 10/25 | Loss: 0.00093118
Iteration 11/25 | Loss: 0.00093118
Iteration 12/25 | Loss: 0.00093118
Iteration 13/25 | Loss: 0.00093118
Iteration 14/25 | Loss: 0.00093118
Iteration 15/25 | Loss: 0.00093118
Iteration 16/25 | Loss: 0.00093118
Iteration 17/25 | Loss: 0.00093118
Iteration 18/25 | Loss: 0.00093118
Iteration 19/25 | Loss: 0.00093118
Iteration 20/25 | Loss: 0.00093118
Iteration 21/25 | Loss: 0.00093118
Iteration 22/25 | Loss: 0.00093118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009311811882071197, 0.0009311811882071197, 0.0009311811882071197, 0.0009311811882071197, 0.0009311811882071197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009311811882071197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093118
Iteration 2/1000 | Loss: 0.00004516
Iteration 3/1000 | Loss: 0.00003302
Iteration 4/1000 | Loss: 0.00003102
Iteration 5/1000 | Loss: 0.00002930
Iteration 6/1000 | Loss: 0.00002838
Iteration 7/1000 | Loss: 0.00002767
Iteration 8/1000 | Loss: 0.00002724
Iteration 9/1000 | Loss: 0.00002703
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002679
Iteration 12/1000 | Loss: 0.00002679
Iteration 13/1000 | Loss: 0.00002678
Iteration 14/1000 | Loss: 0.00002678
Iteration 15/1000 | Loss: 0.00002678
Iteration 16/1000 | Loss: 0.00002676
Iteration 17/1000 | Loss: 0.00002673
Iteration 18/1000 | Loss: 0.00002673
Iteration 19/1000 | Loss: 0.00002672
Iteration 20/1000 | Loss: 0.00002672
Iteration 21/1000 | Loss: 0.00002672
Iteration 22/1000 | Loss: 0.00002671
Iteration 23/1000 | Loss: 0.00002671
Iteration 24/1000 | Loss: 0.00002671
Iteration 25/1000 | Loss: 0.00002670
Iteration 26/1000 | Loss: 0.00002670
Iteration 27/1000 | Loss: 0.00002670
Iteration 28/1000 | Loss: 0.00002669
Iteration 29/1000 | Loss: 0.00002669
Iteration 30/1000 | Loss: 0.00002669
Iteration 31/1000 | Loss: 0.00002669
Iteration 32/1000 | Loss: 0.00002669
Iteration 33/1000 | Loss: 0.00002669
Iteration 34/1000 | Loss: 0.00002669
Iteration 35/1000 | Loss: 0.00002669
Iteration 36/1000 | Loss: 0.00002669
Iteration 37/1000 | Loss: 0.00002669
Iteration 38/1000 | Loss: 0.00002669
Iteration 39/1000 | Loss: 0.00002669
Iteration 40/1000 | Loss: 0.00002669
Iteration 41/1000 | Loss: 0.00002669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [2.668947672646027e-05, 2.668947672646027e-05, 2.668947672646027e-05, 2.668947672646027e-05, 2.668947672646027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.668947672646027e-05

Optimization complete. Final v2v error: 4.458025932312012 mm

Highest mean error: 4.734550476074219 mm for frame 31

Lowest mean error: 4.2677507400512695 mm for frame 69

Saving results

Total time: 56.00364446640015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414528
Iteration 2/25 | Loss: 0.00111936
Iteration 3/25 | Loss: 0.00104265
Iteration 4/25 | Loss: 0.00103048
Iteration 5/25 | Loss: 0.00102633
Iteration 6/25 | Loss: 0.00102551
Iteration 7/25 | Loss: 0.00102551
Iteration 8/25 | Loss: 0.00102551
Iteration 9/25 | Loss: 0.00102551
Iteration 10/25 | Loss: 0.00102551
Iteration 11/25 | Loss: 0.00102551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010255068773403764, 0.0010255068773403764, 0.0010255068773403764, 0.0010255068773403764, 0.0010255068773403764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010255068773403764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45760512
Iteration 2/25 | Loss: 0.00087742
Iteration 3/25 | Loss: 0.00087742
Iteration 4/25 | Loss: 0.00087741
Iteration 5/25 | Loss: 0.00087741
Iteration 6/25 | Loss: 0.00087741
Iteration 7/25 | Loss: 0.00087741
Iteration 8/25 | Loss: 0.00087741
Iteration 9/25 | Loss: 0.00087741
Iteration 10/25 | Loss: 0.00087741
Iteration 11/25 | Loss: 0.00087741
Iteration 12/25 | Loss: 0.00087741
Iteration 13/25 | Loss: 0.00087741
Iteration 14/25 | Loss: 0.00087741
Iteration 15/25 | Loss: 0.00087741
Iteration 16/25 | Loss: 0.00087741
Iteration 17/25 | Loss: 0.00087741
Iteration 18/25 | Loss: 0.00087741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008774132584221661, 0.0008774132584221661, 0.0008774132584221661, 0.0008774132584221661, 0.0008774132584221661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008774132584221661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087741
Iteration 2/1000 | Loss: 0.00003270
Iteration 3/1000 | Loss: 0.00002835
Iteration 4/1000 | Loss: 0.00002650
Iteration 5/1000 | Loss: 0.00002545
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002457
Iteration 8/1000 | Loss: 0.00002444
Iteration 9/1000 | Loss: 0.00002443
Iteration 10/1000 | Loss: 0.00002437
Iteration 11/1000 | Loss: 0.00002437
Iteration 12/1000 | Loss: 0.00002436
Iteration 13/1000 | Loss: 0.00002432
Iteration 14/1000 | Loss: 0.00002428
Iteration 15/1000 | Loss: 0.00002427
Iteration 16/1000 | Loss: 0.00002427
Iteration 17/1000 | Loss: 0.00002427
Iteration 18/1000 | Loss: 0.00002427
Iteration 19/1000 | Loss: 0.00002426
Iteration 20/1000 | Loss: 0.00002426
Iteration 21/1000 | Loss: 0.00002426
Iteration 22/1000 | Loss: 0.00002425
Iteration 23/1000 | Loss: 0.00002425
Iteration 24/1000 | Loss: 0.00002425
Iteration 25/1000 | Loss: 0.00002425
Iteration 26/1000 | Loss: 0.00002424
Iteration 27/1000 | Loss: 0.00002422
Iteration 28/1000 | Loss: 0.00002422
Iteration 29/1000 | Loss: 0.00002421
Iteration 30/1000 | Loss: 0.00002421
Iteration 31/1000 | Loss: 0.00002421
Iteration 32/1000 | Loss: 0.00002420
Iteration 33/1000 | Loss: 0.00002420
Iteration 34/1000 | Loss: 0.00002420
Iteration 35/1000 | Loss: 0.00002419
Iteration 36/1000 | Loss: 0.00002419
Iteration 37/1000 | Loss: 0.00002419
Iteration 38/1000 | Loss: 0.00002419
Iteration 39/1000 | Loss: 0.00002418
Iteration 40/1000 | Loss: 0.00002418
Iteration 41/1000 | Loss: 0.00002418
Iteration 42/1000 | Loss: 0.00002417
Iteration 43/1000 | Loss: 0.00002416
Iteration 44/1000 | Loss: 0.00002415
Iteration 45/1000 | Loss: 0.00002415
Iteration 46/1000 | Loss: 0.00002415
Iteration 47/1000 | Loss: 0.00002415
Iteration 48/1000 | Loss: 0.00002414
Iteration 49/1000 | Loss: 0.00002414
Iteration 50/1000 | Loss: 0.00002414
Iteration 51/1000 | Loss: 0.00002414
Iteration 52/1000 | Loss: 0.00002413
Iteration 53/1000 | Loss: 0.00002413
Iteration 54/1000 | Loss: 0.00002412
Iteration 55/1000 | Loss: 0.00002412
Iteration 56/1000 | Loss: 0.00002412
Iteration 57/1000 | Loss: 0.00002412
Iteration 58/1000 | Loss: 0.00002412
Iteration 59/1000 | Loss: 0.00002412
Iteration 60/1000 | Loss: 0.00002412
Iteration 61/1000 | Loss: 0.00002412
Iteration 62/1000 | Loss: 0.00002412
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002411
Iteration 66/1000 | Loss: 0.00002411
Iteration 67/1000 | Loss: 0.00002411
Iteration 68/1000 | Loss: 0.00002411
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002410
Iteration 77/1000 | Loss: 0.00002410
Iteration 78/1000 | Loss: 0.00002410
Iteration 79/1000 | Loss: 0.00002410
Iteration 80/1000 | Loss: 0.00002410
Iteration 81/1000 | Loss: 0.00002410
Iteration 82/1000 | Loss: 0.00002410
Iteration 83/1000 | Loss: 0.00002410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.4098211724776775e-05, 2.4098211724776775e-05, 2.4098211724776775e-05, 2.4098211724776775e-05, 2.4098211724776775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4098211724776775e-05

Optimization complete. Final v2v error: 4.262221336364746 mm

Highest mean error: 4.41917610168457 mm for frame 156

Lowest mean error: 4.017553806304932 mm for frame 0

Saving results

Total time: 29.59632158279419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915739
Iteration 2/25 | Loss: 0.00121084
Iteration 3/25 | Loss: 0.00107275
Iteration 4/25 | Loss: 0.00105376
Iteration 5/25 | Loss: 0.00104757
Iteration 6/25 | Loss: 0.00104597
Iteration 7/25 | Loss: 0.00104583
Iteration 8/25 | Loss: 0.00104583
Iteration 9/25 | Loss: 0.00104583
Iteration 10/25 | Loss: 0.00104583
Iteration 11/25 | Loss: 0.00104583
Iteration 12/25 | Loss: 0.00104583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010458303149789572, 0.0010458303149789572, 0.0010458303149789572, 0.0010458303149789572, 0.0010458303149789572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010458303149789572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47335351
Iteration 2/25 | Loss: 0.00091361
Iteration 3/25 | Loss: 0.00091359
Iteration 4/25 | Loss: 0.00091359
Iteration 5/25 | Loss: 0.00091359
Iteration 6/25 | Loss: 0.00091359
Iteration 7/25 | Loss: 0.00091359
Iteration 8/25 | Loss: 0.00091359
Iteration 9/25 | Loss: 0.00091359
Iteration 10/25 | Loss: 0.00091359
Iteration 11/25 | Loss: 0.00091359
Iteration 12/25 | Loss: 0.00091359
Iteration 13/25 | Loss: 0.00091359
Iteration 14/25 | Loss: 0.00091359
Iteration 15/25 | Loss: 0.00091359
Iteration 16/25 | Loss: 0.00091359
Iteration 17/25 | Loss: 0.00091359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009135865839198232, 0.0009135865839198232, 0.0009135865839198232, 0.0009135865839198232, 0.0009135865839198232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009135865839198232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091359
Iteration 2/1000 | Loss: 0.00003357
Iteration 3/1000 | Loss: 0.00002682
Iteration 4/1000 | Loss: 0.00002411
Iteration 5/1000 | Loss: 0.00002289
Iteration 6/1000 | Loss: 0.00002226
Iteration 7/1000 | Loss: 0.00002165
Iteration 8/1000 | Loss: 0.00002141
Iteration 9/1000 | Loss: 0.00002126
Iteration 10/1000 | Loss: 0.00002118
Iteration 11/1000 | Loss: 0.00002118
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002118
Iteration 14/1000 | Loss: 0.00002118
Iteration 15/1000 | Loss: 0.00002118
Iteration 16/1000 | Loss: 0.00002118
Iteration 17/1000 | Loss: 0.00002117
Iteration 18/1000 | Loss: 0.00002115
Iteration 19/1000 | Loss: 0.00002114
Iteration 20/1000 | Loss: 0.00002114
Iteration 21/1000 | Loss: 0.00002113
Iteration 22/1000 | Loss: 0.00002113
Iteration 23/1000 | Loss: 0.00002112
Iteration 24/1000 | Loss: 0.00002112
Iteration 25/1000 | Loss: 0.00002110
Iteration 26/1000 | Loss: 0.00002110
Iteration 27/1000 | Loss: 0.00002109
Iteration 28/1000 | Loss: 0.00002109
Iteration 29/1000 | Loss: 0.00002109
Iteration 30/1000 | Loss: 0.00002108
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002108
Iteration 33/1000 | Loss: 0.00002108
Iteration 34/1000 | Loss: 0.00002107
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002107
Iteration 37/1000 | Loss: 0.00002107
Iteration 38/1000 | Loss: 0.00002107
Iteration 39/1000 | Loss: 0.00002107
Iteration 40/1000 | Loss: 0.00002107
Iteration 41/1000 | Loss: 0.00002106
Iteration 42/1000 | Loss: 0.00002106
Iteration 43/1000 | Loss: 0.00002106
Iteration 44/1000 | Loss: 0.00002106
Iteration 45/1000 | Loss: 0.00002106
Iteration 46/1000 | Loss: 0.00002105
Iteration 47/1000 | Loss: 0.00002105
Iteration 48/1000 | Loss: 0.00002105
Iteration 49/1000 | Loss: 0.00002105
Iteration 50/1000 | Loss: 0.00002104
Iteration 51/1000 | Loss: 0.00002104
Iteration 52/1000 | Loss: 0.00002104
Iteration 53/1000 | Loss: 0.00002103
Iteration 54/1000 | Loss: 0.00002103
Iteration 55/1000 | Loss: 0.00002103
Iteration 56/1000 | Loss: 0.00002103
Iteration 57/1000 | Loss: 0.00002103
Iteration 58/1000 | Loss: 0.00002103
Iteration 59/1000 | Loss: 0.00002103
Iteration 60/1000 | Loss: 0.00002103
Iteration 61/1000 | Loss: 0.00002102
Iteration 62/1000 | Loss: 0.00002102
Iteration 63/1000 | Loss: 0.00002102
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002101
Iteration 66/1000 | Loss: 0.00002101
Iteration 67/1000 | Loss: 0.00002101
Iteration 68/1000 | Loss: 0.00002101
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002100
Iteration 71/1000 | Loss: 0.00002100
Iteration 72/1000 | Loss: 0.00002100
Iteration 73/1000 | Loss: 0.00002100
Iteration 74/1000 | Loss: 0.00002099
Iteration 75/1000 | Loss: 0.00002099
Iteration 76/1000 | Loss: 0.00002099
Iteration 77/1000 | Loss: 0.00002099
Iteration 78/1000 | Loss: 0.00002099
Iteration 79/1000 | Loss: 0.00002099
Iteration 80/1000 | Loss: 0.00002098
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002098
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00002098
Iteration 87/1000 | Loss: 0.00002098
Iteration 88/1000 | Loss: 0.00002097
Iteration 89/1000 | Loss: 0.00002097
Iteration 90/1000 | Loss: 0.00002097
Iteration 91/1000 | Loss: 0.00002097
Iteration 92/1000 | Loss: 0.00002097
Iteration 93/1000 | Loss: 0.00002097
Iteration 94/1000 | Loss: 0.00002097
Iteration 95/1000 | Loss: 0.00002097
Iteration 96/1000 | Loss: 0.00002097
Iteration 97/1000 | Loss: 0.00002097
Iteration 98/1000 | Loss: 0.00002097
Iteration 99/1000 | Loss: 0.00002097
Iteration 100/1000 | Loss: 0.00002097
Iteration 101/1000 | Loss: 0.00002096
Iteration 102/1000 | Loss: 0.00002096
Iteration 103/1000 | Loss: 0.00002096
Iteration 104/1000 | Loss: 0.00002096
Iteration 105/1000 | Loss: 0.00002096
Iteration 106/1000 | Loss: 0.00002096
Iteration 107/1000 | Loss: 0.00002096
Iteration 108/1000 | Loss: 0.00002096
Iteration 109/1000 | Loss: 0.00002096
Iteration 110/1000 | Loss: 0.00002096
Iteration 111/1000 | Loss: 0.00002096
Iteration 112/1000 | Loss: 0.00002096
Iteration 113/1000 | Loss: 0.00002096
Iteration 114/1000 | Loss: 0.00002096
Iteration 115/1000 | Loss: 0.00002096
Iteration 116/1000 | Loss: 0.00002096
Iteration 117/1000 | Loss: 0.00002096
Iteration 118/1000 | Loss: 0.00002096
Iteration 119/1000 | Loss: 0.00002096
Iteration 120/1000 | Loss: 0.00002096
Iteration 121/1000 | Loss: 0.00002096
Iteration 122/1000 | Loss: 0.00002096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.0958264940418303e-05, 2.0958264940418303e-05, 2.0958264940418303e-05, 2.0958264940418303e-05, 2.0958264940418303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0958264940418303e-05

Optimization complete. Final v2v error: 4.041961669921875 mm

Highest mean error: 4.315516471862793 mm for frame 58

Lowest mean error: 3.772092819213867 mm for frame 128

Saving results

Total time: 30.960163831710815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439889
Iteration 2/25 | Loss: 0.00122592
Iteration 3/25 | Loss: 0.00103083
Iteration 4/25 | Loss: 0.00100215
Iteration 5/25 | Loss: 0.00099825
Iteration 6/25 | Loss: 0.00099789
Iteration 7/25 | Loss: 0.00099789
Iteration 8/25 | Loss: 0.00099789
Iteration 9/25 | Loss: 0.00099789
Iteration 10/25 | Loss: 0.00099789
Iteration 11/25 | Loss: 0.00099789
Iteration 12/25 | Loss: 0.00099789
Iteration 13/25 | Loss: 0.00099789
Iteration 14/25 | Loss: 0.00099789
Iteration 15/25 | Loss: 0.00099789
Iteration 16/25 | Loss: 0.00099789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009978924645110965, 0.0009978924645110965, 0.0009978924645110965, 0.0009978924645110965, 0.0009978924645110965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009978924645110965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43977141
Iteration 2/25 | Loss: 0.00095989
Iteration 3/25 | Loss: 0.00095989
Iteration 4/25 | Loss: 0.00095989
Iteration 5/25 | Loss: 0.00095988
Iteration 6/25 | Loss: 0.00095988
Iteration 7/25 | Loss: 0.00095988
Iteration 8/25 | Loss: 0.00095988
Iteration 9/25 | Loss: 0.00095988
Iteration 10/25 | Loss: 0.00095988
Iteration 11/25 | Loss: 0.00095988
Iteration 12/25 | Loss: 0.00095988
Iteration 13/25 | Loss: 0.00095988
Iteration 14/25 | Loss: 0.00095988
Iteration 15/25 | Loss: 0.00095988
Iteration 16/25 | Loss: 0.00095988
Iteration 17/25 | Loss: 0.00095988
Iteration 18/25 | Loss: 0.00095988
Iteration 19/25 | Loss: 0.00095988
Iteration 20/25 | Loss: 0.00095988
Iteration 21/25 | Loss: 0.00095988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009598833275958896, 0.0009598833275958896, 0.0009598833275958896, 0.0009598833275958896, 0.0009598833275958896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009598833275958896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095988
Iteration 2/1000 | Loss: 0.00002526
Iteration 3/1000 | Loss: 0.00002150
Iteration 4/1000 | Loss: 0.00002016
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001853
Iteration 7/1000 | Loss: 0.00001821
Iteration 8/1000 | Loss: 0.00001817
Iteration 9/1000 | Loss: 0.00001812
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001781
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001774
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00001764
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001759
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001757
Iteration 30/1000 | Loss: 0.00001757
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00001756
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001755
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001754
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001753
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001752
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001749
Iteration 62/1000 | Loss: 0.00001749
Iteration 63/1000 | Loss: 0.00001749
Iteration 64/1000 | Loss: 0.00001749
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001747
Iteration 69/1000 | Loss: 0.00001747
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001747
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001747
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001746
Iteration 84/1000 | Loss: 0.00001746
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001746
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001746
Iteration 99/1000 | Loss: 0.00001746
Iteration 100/1000 | Loss: 0.00001746
Iteration 101/1000 | Loss: 0.00001746
Iteration 102/1000 | Loss: 0.00001746
Iteration 103/1000 | Loss: 0.00001746
Iteration 104/1000 | Loss: 0.00001746
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.7455906345276162e-05, 1.7455906345276162e-05, 1.7455906345276162e-05, 1.7455906345276162e-05, 1.7455906345276162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7455906345276162e-05

Optimization complete. Final v2v error: 3.6594815254211426 mm

Highest mean error: 4.070061206817627 mm for frame 221

Lowest mean error: 3.311396360397339 mm for frame 236

Saving results

Total time: 33.72847318649292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01176535
Iteration 2/25 | Loss: 0.00232874
Iteration 3/25 | Loss: 0.00146094
Iteration 4/25 | Loss: 0.00133578
Iteration 5/25 | Loss: 0.00141175
Iteration 6/25 | Loss: 0.00124578
Iteration 7/25 | Loss: 0.00113779
Iteration 8/25 | Loss: 0.00110406
Iteration 9/25 | Loss: 0.00105453
Iteration 10/25 | Loss: 0.00103148
Iteration 11/25 | Loss: 0.00102086
Iteration 12/25 | Loss: 0.00102084
Iteration 13/25 | Loss: 0.00101927
Iteration 14/25 | Loss: 0.00101996
Iteration 15/25 | Loss: 0.00101924
Iteration 16/25 | Loss: 0.00101968
Iteration 17/25 | Loss: 0.00101927
Iteration 18/25 | Loss: 0.00102008
Iteration 19/25 | Loss: 0.00101929
Iteration 20/25 | Loss: 0.00102000
Iteration 21/25 | Loss: 0.00101945
Iteration 22/25 | Loss: 0.00102000
Iteration 23/25 | Loss: 0.00101953
Iteration 24/25 | Loss: 0.00101997
Iteration 25/25 | Loss: 0.00101885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58407712
Iteration 2/25 | Loss: 0.00085025
Iteration 3/25 | Loss: 0.00080172
Iteration 4/25 | Loss: 0.00080172
Iteration 5/25 | Loss: 0.00080172
Iteration 6/25 | Loss: 0.00080172
Iteration 7/25 | Loss: 0.00080172
Iteration 8/25 | Loss: 0.00080172
Iteration 9/25 | Loss: 0.00080172
Iteration 10/25 | Loss: 0.00080172
Iteration 11/25 | Loss: 0.00080172
Iteration 12/25 | Loss: 0.00080172
Iteration 13/25 | Loss: 0.00080172
Iteration 14/25 | Loss: 0.00080172
Iteration 15/25 | Loss: 0.00080172
Iteration 16/25 | Loss: 0.00080172
Iteration 17/25 | Loss: 0.00080172
Iteration 18/25 | Loss: 0.00080172
Iteration 19/25 | Loss: 0.00080172
Iteration 20/25 | Loss: 0.00080172
Iteration 21/25 | Loss: 0.00080172
Iteration 22/25 | Loss: 0.00080172
Iteration 23/25 | Loss: 0.00080172
Iteration 24/25 | Loss: 0.00080172
Iteration 25/25 | Loss: 0.00080172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080172
Iteration 2/1000 | Loss: 0.00009771
Iteration 3/1000 | Loss: 0.00006731
Iteration 4/1000 | Loss: 0.00004176
Iteration 5/1000 | Loss: 0.00003937
Iteration 6/1000 | Loss: 0.00003792
Iteration 7/1000 | Loss: 0.00005993
Iteration 8/1000 | Loss: 0.00003648
Iteration 9/1000 | Loss: 0.00007692
Iteration 10/1000 | Loss: 0.00003558
Iteration 11/1000 | Loss: 0.00080074
Iteration 12/1000 | Loss: 0.00038608
Iteration 13/1000 | Loss: 0.00003651
Iteration 14/1000 | Loss: 0.00006607
Iteration 15/1000 | Loss: 0.00003526
Iteration 16/1000 | Loss: 0.00078840
Iteration 17/1000 | Loss: 0.00043331
Iteration 18/1000 | Loss: 0.00003540
Iteration 19/1000 | Loss: 0.00003501
Iteration 20/1000 | Loss: 0.00003559
Iteration 21/1000 | Loss: 0.00003559
Iteration 22/1000 | Loss: 0.00003509
Iteration 23/1000 | Loss: 0.00003494
Iteration 24/1000 | Loss: 0.00003494
Iteration 25/1000 | Loss: 0.00003494
Iteration 26/1000 | Loss: 0.00003494
Iteration 27/1000 | Loss: 0.00003494
Iteration 28/1000 | Loss: 0.00003494
Iteration 29/1000 | Loss: 0.00003493
Iteration 30/1000 | Loss: 0.00003493
Iteration 31/1000 | Loss: 0.00003493
Iteration 32/1000 | Loss: 0.00003493
Iteration 33/1000 | Loss: 0.00003493
Iteration 34/1000 | Loss: 0.00003493
Iteration 35/1000 | Loss: 0.00003492
Iteration 36/1000 | Loss: 0.00003492
Iteration 37/1000 | Loss: 0.00003489
Iteration 38/1000 | Loss: 0.00003489
Iteration 39/1000 | Loss: 0.00003486
Iteration 40/1000 | Loss: 0.00003485
Iteration 41/1000 | Loss: 0.00003484
Iteration 42/1000 | Loss: 0.00003483
Iteration 43/1000 | Loss: 0.00003479
Iteration 44/1000 | Loss: 0.00003476
Iteration 45/1000 | Loss: 0.00003476
Iteration 46/1000 | Loss: 0.00003475
Iteration 47/1000 | Loss: 0.00003475
Iteration 48/1000 | Loss: 0.00003475
Iteration 49/1000 | Loss: 0.00003475
Iteration 50/1000 | Loss: 0.00003475
Iteration 51/1000 | Loss: 0.00003475
Iteration 52/1000 | Loss: 0.00003475
Iteration 53/1000 | Loss: 0.00003475
Iteration 54/1000 | Loss: 0.00003475
Iteration 55/1000 | Loss: 0.00003474
Iteration 56/1000 | Loss: 0.00003474
Iteration 57/1000 | Loss: 0.00003474
Iteration 58/1000 | Loss: 0.00003474
Iteration 59/1000 | Loss: 0.00003473
Iteration 60/1000 | Loss: 0.00003473
Iteration 61/1000 | Loss: 0.00003473
Iteration 62/1000 | Loss: 0.00003472
Iteration 63/1000 | Loss: 0.00003472
Iteration 64/1000 | Loss: 0.00003472
Iteration 65/1000 | Loss: 0.00003471
Iteration 66/1000 | Loss: 0.00003471
Iteration 67/1000 | Loss: 0.00003471
Iteration 68/1000 | Loss: 0.00003470
Iteration 69/1000 | Loss: 0.00003470
Iteration 70/1000 | Loss: 0.00003470
Iteration 71/1000 | Loss: 0.00003470
Iteration 72/1000 | Loss: 0.00003470
Iteration 73/1000 | Loss: 0.00003524
Iteration 74/1000 | Loss: 0.00003480
Iteration 75/1000 | Loss: 0.00003469
Iteration 76/1000 | Loss: 0.00003469
Iteration 77/1000 | Loss: 0.00003469
Iteration 78/1000 | Loss: 0.00003469
Iteration 79/1000 | Loss: 0.00003469
Iteration 80/1000 | Loss: 0.00003469
Iteration 81/1000 | Loss: 0.00003468
Iteration 82/1000 | Loss: 0.00003468
Iteration 83/1000 | Loss: 0.00003468
Iteration 84/1000 | Loss: 0.00003468
Iteration 85/1000 | Loss: 0.00003468
Iteration 86/1000 | Loss: 0.00003468
Iteration 87/1000 | Loss: 0.00003468
Iteration 88/1000 | Loss: 0.00003468
Iteration 89/1000 | Loss: 0.00003468
Iteration 90/1000 | Loss: 0.00003468
Iteration 91/1000 | Loss: 0.00003468
Iteration 92/1000 | Loss: 0.00003468
Iteration 93/1000 | Loss: 0.00003467
Iteration 94/1000 | Loss: 0.00003467
Iteration 95/1000 | Loss: 0.00003467
Iteration 96/1000 | Loss: 0.00003467
Iteration 97/1000 | Loss: 0.00003466
Iteration 98/1000 | Loss: 0.00003466
Iteration 99/1000 | Loss: 0.00003465
Iteration 100/1000 | Loss: 0.00003519
Iteration 101/1000 | Loss: 0.00003480
Iteration 102/1000 | Loss: 0.00003461
Iteration 103/1000 | Loss: 0.00003461
Iteration 104/1000 | Loss: 0.00003460
Iteration 105/1000 | Loss: 0.00003460
Iteration 106/1000 | Loss: 0.00003460
Iteration 107/1000 | Loss: 0.00003460
Iteration 108/1000 | Loss: 0.00003460
Iteration 109/1000 | Loss: 0.00003460
Iteration 110/1000 | Loss: 0.00003460
Iteration 111/1000 | Loss: 0.00003460
Iteration 112/1000 | Loss: 0.00003459
Iteration 113/1000 | Loss: 0.00003516
Iteration 114/1000 | Loss: 0.00003487
Iteration 115/1000 | Loss: 0.00003487
Iteration 116/1000 | Loss: 0.00003460
Iteration 117/1000 | Loss: 0.00003460
Iteration 118/1000 | Loss: 0.00003458
Iteration 119/1000 | Loss: 0.00003458
Iteration 120/1000 | Loss: 0.00003458
Iteration 121/1000 | Loss: 0.00003458
Iteration 122/1000 | Loss: 0.00003458
Iteration 123/1000 | Loss: 0.00003458
Iteration 124/1000 | Loss: 0.00003458
Iteration 125/1000 | Loss: 0.00003458
Iteration 126/1000 | Loss: 0.00003514
Iteration 127/1000 | Loss: 0.00003492
Iteration 128/1000 | Loss: 0.00003491
Iteration 129/1000 | Loss: 0.00003491
Iteration 130/1000 | Loss: 0.00003491
Iteration 131/1000 | Loss: 0.00003461
Iteration 132/1000 | Loss: 0.00003459
Iteration 133/1000 | Loss: 0.00003459
Iteration 134/1000 | Loss: 0.00003459
Iteration 135/1000 | Loss: 0.00003458
Iteration 136/1000 | Loss: 0.00003458
Iteration 137/1000 | Loss: 0.00003458
Iteration 138/1000 | Loss: 0.00003458
Iteration 139/1000 | Loss: 0.00003458
Iteration 140/1000 | Loss: 0.00003458
Iteration 141/1000 | Loss: 0.00003458
Iteration 142/1000 | Loss: 0.00003458
Iteration 143/1000 | Loss: 0.00003458
Iteration 144/1000 | Loss: 0.00003458
Iteration 145/1000 | Loss: 0.00003458
Iteration 146/1000 | Loss: 0.00003458
Iteration 147/1000 | Loss: 0.00003509
Iteration 148/1000 | Loss: 0.00003509
Iteration 149/1000 | Loss: 0.00003490
Iteration 150/1000 | Loss: 0.00003459
Iteration 151/1000 | Loss: 0.00003458
Iteration 152/1000 | Loss: 0.00003458
Iteration 153/1000 | Loss: 0.00003457
Iteration 154/1000 | Loss: 0.00003457
Iteration 155/1000 | Loss: 0.00003457
Iteration 156/1000 | Loss: 0.00003506
Iteration 157/1000 | Loss: 0.00003506
Iteration 158/1000 | Loss: 0.00003496
Iteration 159/1000 | Loss: 0.00003457
Iteration 160/1000 | Loss: 0.00003457
Iteration 161/1000 | Loss: 0.00003457
Iteration 162/1000 | Loss: 0.00003457
Iteration 163/1000 | Loss: 0.00003457
Iteration 164/1000 | Loss: 0.00003457
Iteration 165/1000 | Loss: 0.00003457
Iteration 166/1000 | Loss: 0.00003457
Iteration 167/1000 | Loss: 0.00003503
Iteration 168/1000 | Loss: 0.00003493
Iteration 169/1000 | Loss: 0.00003457
Iteration 170/1000 | Loss: 0.00003457
Iteration 171/1000 | Loss: 0.00003457
Iteration 172/1000 | Loss: 0.00003457
Iteration 173/1000 | Loss: 0.00003457
Iteration 174/1000 | Loss: 0.00003502
Iteration 175/1000 | Loss: 0.00003488
Iteration 176/1000 | Loss: 0.00003488
Iteration 177/1000 | Loss: 0.00003459
Iteration 178/1000 | Loss: 0.00003458
Iteration 179/1000 | Loss: 0.00003458
Iteration 180/1000 | Loss: 0.00003458
Iteration 181/1000 | Loss: 0.00003458
Iteration 182/1000 | Loss: 0.00003457
Iteration 183/1000 | Loss: 0.00003457
Iteration 184/1000 | Loss: 0.00003457
Iteration 185/1000 | Loss: 0.00003502
Iteration 186/1000 | Loss: 0.00003490
Iteration 187/1000 | Loss: 0.00003457
Iteration 188/1000 | Loss: 0.00003457
Iteration 189/1000 | Loss: 0.00003457
Iteration 190/1000 | Loss: 0.00003457
Iteration 191/1000 | Loss: 0.00003457
Iteration 192/1000 | Loss: 0.00003501
Iteration 193/1000 | Loss: 0.00003501
Iteration 194/1000 | Loss: 0.00003492
Iteration 195/1000 | Loss: 0.00003458
Iteration 196/1000 | Loss: 0.00003457
Iteration 197/1000 | Loss: 0.00003457
Iteration 198/1000 | Loss: 0.00003457
Iteration 199/1000 | Loss: 0.00003457
Iteration 200/1000 | Loss: 0.00003457
Iteration 201/1000 | Loss: 0.00003457
Iteration 202/1000 | Loss: 0.00003457
Iteration 203/1000 | Loss: 0.00003457
Iteration 204/1000 | Loss: 0.00003457
Iteration 205/1000 | Loss: 0.00003457
Iteration 206/1000 | Loss: 0.00003457
Iteration 207/1000 | Loss: 0.00003499
Iteration 208/1000 | Loss: 0.00003489
Iteration 209/1000 | Loss: 0.00003457
Iteration 210/1000 | Loss: 0.00003457
Iteration 211/1000 | Loss: 0.00003457
Iteration 212/1000 | Loss: 0.00003457
Iteration 213/1000 | Loss: 0.00003498
Iteration 214/1000 | Loss: 0.00003486
Iteration 215/1000 | Loss: 0.00003483
Iteration 216/1000 | Loss: 0.00003483
Iteration 217/1000 | Loss: 0.00003483
Iteration 218/1000 | Loss: 0.00003482
Iteration 219/1000 | Loss: 0.00003466
Iteration 220/1000 | Loss: 0.00003465
Iteration 221/1000 | Loss: 0.00003463
Iteration 222/1000 | Loss: 0.00003463
Iteration 223/1000 | Loss: 0.00003462
Iteration 224/1000 | Loss: 0.00003462
Iteration 225/1000 | Loss: 0.00003461
Iteration 226/1000 | Loss: 0.00003461
Iteration 227/1000 | Loss: 0.00003461
Iteration 228/1000 | Loss: 0.00003461
Iteration 229/1000 | Loss: 0.00003461
Iteration 230/1000 | Loss: 0.00003460
Iteration 231/1000 | Loss: 0.00003460
Iteration 232/1000 | Loss: 0.00003460
Iteration 233/1000 | Loss: 0.00003460
Iteration 234/1000 | Loss: 0.00003460
Iteration 235/1000 | Loss: 0.00003459
Iteration 236/1000 | Loss: 0.00003459
Iteration 237/1000 | Loss: 0.00003459
Iteration 238/1000 | Loss: 0.00003459
Iteration 239/1000 | Loss: 0.00003459
Iteration 240/1000 | Loss: 0.00003459
Iteration 241/1000 | Loss: 0.00003459
Iteration 242/1000 | Loss: 0.00003458
Iteration 243/1000 | Loss: 0.00003458
Iteration 244/1000 | Loss: 0.00003458
Iteration 245/1000 | Loss: 0.00003458
Iteration 246/1000 | Loss: 0.00003458
Iteration 247/1000 | Loss: 0.00003458
Iteration 248/1000 | Loss: 0.00003458
Iteration 249/1000 | Loss: 0.00003457
Iteration 250/1000 | Loss: 0.00003457
Iteration 251/1000 | Loss: 0.00003457
Iteration 252/1000 | Loss: 0.00003457
Iteration 253/1000 | Loss: 0.00003457
Iteration 254/1000 | Loss: 0.00003457
Iteration 255/1000 | Loss: 0.00003457
Iteration 256/1000 | Loss: 0.00003457
Iteration 257/1000 | Loss: 0.00003457
Iteration 258/1000 | Loss: 0.00003457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [3.457418642938137e-05, 3.457418642938137e-05, 3.457418642938137e-05, 3.457418642938137e-05, 3.457418642938137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.457418642938137e-05

Optimization complete. Final v2v error: 4.511205673217773 mm

Highest mean error: 23.00783920288086 mm for frame 38

Lowest mean error: 3.8201630115509033 mm for frame 39

Saving results

Total time: 118.70640349388123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068993
Iteration 2/25 | Loss: 0.00270463
Iteration 3/25 | Loss: 0.00163217
Iteration 4/25 | Loss: 0.00142675
Iteration 5/25 | Loss: 0.00146266
Iteration 6/25 | Loss: 0.00125858
Iteration 7/25 | Loss: 0.00114728
Iteration 8/25 | Loss: 0.00111533
Iteration 9/25 | Loss: 0.00110440
Iteration 10/25 | Loss: 0.00109160
Iteration 11/25 | Loss: 0.00107857
Iteration 12/25 | Loss: 0.00107250
Iteration 13/25 | Loss: 0.00107588
Iteration 14/25 | Loss: 0.00107094
Iteration 15/25 | Loss: 0.00106427
Iteration 16/25 | Loss: 0.00106230
Iteration 17/25 | Loss: 0.00106502
Iteration 18/25 | Loss: 0.00106360
Iteration 19/25 | Loss: 0.00106061
Iteration 20/25 | Loss: 0.00106020
Iteration 21/25 | Loss: 0.00106063
Iteration 22/25 | Loss: 0.00106045
Iteration 23/25 | Loss: 0.00106035
Iteration 24/25 | Loss: 0.00106028
Iteration 25/25 | Loss: 0.00106064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43513560
Iteration 2/25 | Loss: 0.00131332
Iteration 3/25 | Loss: 0.00131332
Iteration 4/25 | Loss: 0.00131332
Iteration 5/25 | Loss: 0.00131332
Iteration 6/25 | Loss: 0.00131332
Iteration 7/25 | Loss: 0.00131332
Iteration 8/25 | Loss: 0.00131332
Iteration 9/25 | Loss: 0.00131332
Iteration 10/25 | Loss: 0.00131332
Iteration 11/25 | Loss: 0.00131332
Iteration 12/25 | Loss: 0.00131332
Iteration 13/25 | Loss: 0.00131332
Iteration 14/25 | Loss: 0.00131332
Iteration 15/25 | Loss: 0.00131332
Iteration 16/25 | Loss: 0.00131332
Iteration 17/25 | Loss: 0.00131332
Iteration 18/25 | Loss: 0.00131332
Iteration 19/25 | Loss: 0.00131332
Iteration 20/25 | Loss: 0.00131332
Iteration 21/25 | Loss: 0.00131332
Iteration 22/25 | Loss: 0.00131332
Iteration 23/25 | Loss: 0.00131332
Iteration 24/25 | Loss: 0.00131332
Iteration 25/25 | Loss: 0.00131332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131332
Iteration 2/1000 | Loss: 0.00010563
Iteration 3/1000 | Loss: 0.00008042
Iteration 4/1000 | Loss: 0.00006084
Iteration 5/1000 | Loss: 0.00007283
Iteration 6/1000 | Loss: 0.00005136
Iteration 7/1000 | Loss: 0.00005738
Iteration 8/1000 | Loss: 0.00005667
Iteration 9/1000 | Loss: 0.00004848
Iteration 10/1000 | Loss: 0.00006828
Iteration 11/1000 | Loss: 0.00005020
Iteration 12/1000 | Loss: 0.00006293
Iteration 13/1000 | Loss: 0.00006675
Iteration 14/1000 | Loss: 0.00006150
Iteration 15/1000 | Loss: 0.00005843
Iteration 16/1000 | Loss: 0.00005593
Iteration 17/1000 | Loss: 0.00005995
Iteration 18/1000 | Loss: 0.00006707
Iteration 19/1000 | Loss: 0.00005737
Iteration 20/1000 | Loss: 0.00005817
Iteration 21/1000 | Loss: 0.00006767
Iteration 22/1000 | Loss: 0.00005935
Iteration 23/1000 | Loss: 0.00067603
Iteration 24/1000 | Loss: 0.00007542
Iteration 25/1000 | Loss: 0.00006172
Iteration 26/1000 | Loss: 0.00005144
Iteration 27/1000 | Loss: 0.00058153
Iteration 28/1000 | Loss: 0.00007799
Iteration 29/1000 | Loss: 0.00061286
Iteration 30/1000 | Loss: 0.00006526
Iteration 31/1000 | Loss: 0.00004682
Iteration 32/1000 | Loss: 0.00005911
Iteration 33/1000 | Loss: 0.00005869
Iteration 34/1000 | Loss: 0.00005182
Iteration 35/1000 | Loss: 0.00065881
Iteration 36/1000 | Loss: 0.00005675
Iteration 37/1000 | Loss: 0.00004551
Iteration 38/1000 | Loss: 0.00061354
Iteration 39/1000 | Loss: 0.00005824
Iteration 40/1000 | Loss: 0.00005772
Iteration 41/1000 | Loss: 0.00004614
Iteration 42/1000 | Loss: 0.00004506
Iteration 43/1000 | Loss: 0.00003875
Iteration 44/1000 | Loss: 0.00003974
Iteration 45/1000 | Loss: 0.00004771
Iteration 46/1000 | Loss: 0.00004119
Iteration 47/1000 | Loss: 0.00005003
Iteration 48/1000 | Loss: 0.00059091
Iteration 49/1000 | Loss: 0.00126099
Iteration 50/1000 | Loss: 0.00007410
Iteration 51/1000 | Loss: 0.00004876
Iteration 52/1000 | Loss: 0.00003601
Iteration 53/1000 | Loss: 0.00003551
Iteration 54/1000 | Loss: 0.00002974
Iteration 55/1000 | Loss: 0.00002782
Iteration 56/1000 | Loss: 0.00002691
Iteration 57/1000 | Loss: 0.00002642
Iteration 58/1000 | Loss: 0.00002593
Iteration 59/1000 | Loss: 0.00002558
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002507
Iteration 62/1000 | Loss: 0.00002490
Iteration 63/1000 | Loss: 0.00002482
Iteration 64/1000 | Loss: 0.00002476
Iteration 65/1000 | Loss: 0.00002466
Iteration 66/1000 | Loss: 0.00002460
Iteration 67/1000 | Loss: 0.00002459
Iteration 68/1000 | Loss: 0.00002459
Iteration 69/1000 | Loss: 0.00002459
Iteration 70/1000 | Loss: 0.00002459
Iteration 71/1000 | Loss: 0.00002456
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002455
Iteration 75/1000 | Loss: 0.00002455
Iteration 76/1000 | Loss: 0.00002455
Iteration 77/1000 | Loss: 0.00002455
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002454
Iteration 81/1000 | Loss: 0.00002452
Iteration 82/1000 | Loss: 0.00002452
Iteration 83/1000 | Loss: 0.00002452
Iteration 84/1000 | Loss: 0.00002451
Iteration 85/1000 | Loss: 0.00002449
Iteration 86/1000 | Loss: 0.00002448
Iteration 87/1000 | Loss: 0.00002448
Iteration 88/1000 | Loss: 0.00002448
Iteration 89/1000 | Loss: 0.00002448
Iteration 90/1000 | Loss: 0.00002447
Iteration 91/1000 | Loss: 0.00002445
Iteration 92/1000 | Loss: 0.00002445
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002445
Iteration 98/1000 | Loss: 0.00002444
Iteration 99/1000 | Loss: 0.00002444
Iteration 100/1000 | Loss: 0.00002444
Iteration 101/1000 | Loss: 0.00002444
Iteration 102/1000 | Loss: 0.00002444
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002444
Iteration 106/1000 | Loss: 0.00002444
Iteration 107/1000 | Loss: 0.00002444
Iteration 108/1000 | Loss: 0.00002443
Iteration 109/1000 | Loss: 0.00002443
Iteration 110/1000 | Loss: 0.00002442
Iteration 111/1000 | Loss: 0.00002442
Iteration 112/1000 | Loss: 0.00002442
Iteration 113/1000 | Loss: 0.00002441
Iteration 114/1000 | Loss: 0.00002441
Iteration 115/1000 | Loss: 0.00002441
Iteration 116/1000 | Loss: 0.00002441
Iteration 117/1000 | Loss: 0.00002441
Iteration 118/1000 | Loss: 0.00002441
Iteration 119/1000 | Loss: 0.00002441
Iteration 120/1000 | Loss: 0.00002441
Iteration 121/1000 | Loss: 0.00002440
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00002440
Iteration 124/1000 | Loss: 0.00002440
Iteration 125/1000 | Loss: 0.00002438
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002437
Iteration 128/1000 | Loss: 0.00002437
Iteration 129/1000 | Loss: 0.00002436
Iteration 130/1000 | Loss: 0.00002434
Iteration 131/1000 | Loss: 0.00002423
Iteration 132/1000 | Loss: 0.00002420
Iteration 133/1000 | Loss: 0.00002417
Iteration 134/1000 | Loss: 0.00002416
Iteration 135/1000 | Loss: 0.00002412
Iteration 136/1000 | Loss: 0.00002410
Iteration 137/1000 | Loss: 0.00002409
Iteration 138/1000 | Loss: 0.00002409
Iteration 139/1000 | Loss: 0.00002408
Iteration 140/1000 | Loss: 0.00002408
Iteration 141/1000 | Loss: 0.00002407
Iteration 142/1000 | Loss: 0.00002406
Iteration 143/1000 | Loss: 0.00002406
Iteration 144/1000 | Loss: 0.00002405
Iteration 145/1000 | Loss: 0.00002404
Iteration 146/1000 | Loss: 0.00002404
Iteration 147/1000 | Loss: 0.00002403
Iteration 148/1000 | Loss: 0.00002403
Iteration 149/1000 | Loss: 0.00002403
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002403
Iteration 152/1000 | Loss: 0.00002403
Iteration 153/1000 | Loss: 0.00002403
Iteration 154/1000 | Loss: 0.00002402
Iteration 155/1000 | Loss: 0.00002402
Iteration 156/1000 | Loss: 0.00002402
Iteration 157/1000 | Loss: 0.00002402
Iteration 158/1000 | Loss: 0.00063639
Iteration 159/1000 | Loss: 0.00018352
Iteration 160/1000 | Loss: 0.00003099
Iteration 161/1000 | Loss: 0.00002803
Iteration 162/1000 | Loss: 0.00002556
Iteration 163/1000 | Loss: 0.00002475
Iteration 164/1000 | Loss: 0.00002445
Iteration 165/1000 | Loss: 0.00002429
Iteration 166/1000 | Loss: 0.00002425
Iteration 167/1000 | Loss: 0.00002421
Iteration 168/1000 | Loss: 0.00002420
Iteration 169/1000 | Loss: 0.00002419
Iteration 170/1000 | Loss: 0.00002419
Iteration 171/1000 | Loss: 0.00002418
Iteration 172/1000 | Loss: 0.00002418
Iteration 173/1000 | Loss: 0.00002418
Iteration 174/1000 | Loss: 0.00002417
Iteration 175/1000 | Loss: 0.00002417
Iteration 176/1000 | Loss: 0.00002415
Iteration 177/1000 | Loss: 0.00002410
Iteration 178/1000 | Loss: 0.00002410
Iteration 179/1000 | Loss: 0.00002403
Iteration 180/1000 | Loss: 0.00002400
Iteration 181/1000 | Loss: 0.00002398
Iteration 182/1000 | Loss: 0.00002397
Iteration 183/1000 | Loss: 0.00002397
Iteration 184/1000 | Loss: 0.00002396
Iteration 185/1000 | Loss: 0.00002396
Iteration 186/1000 | Loss: 0.00002396
Iteration 187/1000 | Loss: 0.00002396
Iteration 188/1000 | Loss: 0.00002396
Iteration 189/1000 | Loss: 0.00002396
Iteration 190/1000 | Loss: 0.00002396
Iteration 191/1000 | Loss: 0.00002396
Iteration 192/1000 | Loss: 0.00002395
Iteration 193/1000 | Loss: 0.00002395
Iteration 194/1000 | Loss: 0.00002395
Iteration 195/1000 | Loss: 0.00002394
Iteration 196/1000 | Loss: 0.00002393
Iteration 197/1000 | Loss: 0.00064529
Iteration 198/1000 | Loss: 0.00039850
Iteration 199/1000 | Loss: 0.00004551
Iteration 200/1000 | Loss: 0.00038586
Iteration 201/1000 | Loss: 0.00004540
Iteration 202/1000 | Loss: 0.00003394
Iteration 203/1000 | Loss: 0.00002669
Iteration 204/1000 | Loss: 0.00002427
Iteration 205/1000 | Loss: 0.00002319
Iteration 206/1000 | Loss: 0.00002268
Iteration 207/1000 | Loss: 0.00002236
Iteration 208/1000 | Loss: 0.00002228
Iteration 209/1000 | Loss: 0.00002227
Iteration 210/1000 | Loss: 0.00002227
Iteration 211/1000 | Loss: 0.00002226
Iteration 212/1000 | Loss: 0.00002226
Iteration 213/1000 | Loss: 0.00002226
Iteration 214/1000 | Loss: 0.00002226
Iteration 215/1000 | Loss: 0.00002225
Iteration 216/1000 | Loss: 0.00002225
Iteration 217/1000 | Loss: 0.00002225
Iteration 218/1000 | Loss: 0.00002225
Iteration 219/1000 | Loss: 0.00002225
Iteration 220/1000 | Loss: 0.00002225
Iteration 221/1000 | Loss: 0.00002225
Iteration 222/1000 | Loss: 0.00002225
Iteration 223/1000 | Loss: 0.00002224
Iteration 224/1000 | Loss: 0.00002224
Iteration 225/1000 | Loss: 0.00002224
Iteration 226/1000 | Loss: 0.00002223
Iteration 227/1000 | Loss: 0.00002222
Iteration 228/1000 | Loss: 0.00002221
Iteration 229/1000 | Loss: 0.00002221
Iteration 230/1000 | Loss: 0.00002221
Iteration 231/1000 | Loss: 0.00002221
Iteration 232/1000 | Loss: 0.00002221
Iteration 233/1000 | Loss: 0.00002220
Iteration 234/1000 | Loss: 0.00002220
Iteration 235/1000 | Loss: 0.00002220
Iteration 236/1000 | Loss: 0.00002220
Iteration 237/1000 | Loss: 0.00002220
Iteration 238/1000 | Loss: 0.00002220
Iteration 239/1000 | Loss: 0.00002220
Iteration 240/1000 | Loss: 0.00002220
Iteration 241/1000 | Loss: 0.00002220
Iteration 242/1000 | Loss: 0.00002220
Iteration 243/1000 | Loss: 0.00002219
Iteration 244/1000 | Loss: 0.00002218
Iteration 245/1000 | Loss: 0.00002218
Iteration 246/1000 | Loss: 0.00002214
Iteration 247/1000 | Loss: 0.00002214
Iteration 248/1000 | Loss: 0.00002212
Iteration 249/1000 | Loss: 0.00002210
Iteration 250/1000 | Loss: 0.00002209
Iteration 251/1000 | Loss: 0.00002208
Iteration 252/1000 | Loss: 0.00002208
Iteration 253/1000 | Loss: 0.00002205
Iteration 254/1000 | Loss: 0.00002203
Iteration 255/1000 | Loss: 0.00002202
Iteration 256/1000 | Loss: 0.00002202
Iteration 257/1000 | Loss: 0.00002202
Iteration 258/1000 | Loss: 0.00002201
Iteration 259/1000 | Loss: 0.00002201
Iteration 260/1000 | Loss: 0.00002201
Iteration 261/1000 | Loss: 0.00002201
Iteration 262/1000 | Loss: 0.00002201
Iteration 263/1000 | Loss: 0.00002201
Iteration 264/1000 | Loss: 0.00002200
Iteration 265/1000 | Loss: 0.00002200
Iteration 266/1000 | Loss: 0.00002200
Iteration 267/1000 | Loss: 0.00002200
Iteration 268/1000 | Loss: 0.00002200
Iteration 269/1000 | Loss: 0.00002200
Iteration 270/1000 | Loss: 0.00002200
Iteration 271/1000 | Loss: 0.00002200
Iteration 272/1000 | Loss: 0.00002200
Iteration 273/1000 | Loss: 0.00002200
Iteration 274/1000 | Loss: 0.00002200
Iteration 275/1000 | Loss: 0.00002200
Iteration 276/1000 | Loss: 0.00002200
Iteration 277/1000 | Loss: 0.00002200
Iteration 278/1000 | Loss: 0.00002200
Iteration 279/1000 | Loss: 0.00002200
Iteration 280/1000 | Loss: 0.00002200
Iteration 281/1000 | Loss: 0.00002200
Iteration 282/1000 | Loss: 0.00002200
Iteration 283/1000 | Loss: 0.00002200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [2.1998990632710047e-05, 2.1998990632710047e-05, 2.1998990632710047e-05, 2.1998990632710047e-05, 2.1998990632710047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1998990632710047e-05

Optimization complete. Final v2v error: 3.921199083328247 mm

Highest mean error: 12.089961051940918 mm for frame 28

Lowest mean error: 3.513017177581787 mm for frame 101

Saving results

Total time: 216.1301667690277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925628
Iteration 2/25 | Loss: 0.00128789
Iteration 3/25 | Loss: 0.00107989
Iteration 4/25 | Loss: 0.00099181
Iteration 5/25 | Loss: 0.00097724
Iteration 6/25 | Loss: 0.00097149
Iteration 7/25 | Loss: 0.00096375
Iteration 8/25 | Loss: 0.00096265
Iteration 9/25 | Loss: 0.00096341
Iteration 10/25 | Loss: 0.00096314
Iteration 11/25 | Loss: 0.00096261
Iteration 12/25 | Loss: 0.00096211
Iteration 13/25 | Loss: 0.00096270
Iteration 14/25 | Loss: 0.00096190
Iteration 15/25 | Loss: 0.00096176
Iteration 16/25 | Loss: 0.00096176
Iteration 17/25 | Loss: 0.00096175
Iteration 18/25 | Loss: 0.00096175
Iteration 19/25 | Loss: 0.00096175
Iteration 20/25 | Loss: 0.00096231
Iteration 21/25 | Loss: 0.00096230
Iteration 22/25 | Loss: 0.00096230
Iteration 23/25 | Loss: 0.00096223
Iteration 24/25 | Loss: 0.00096201
Iteration 25/25 | Loss: 0.00096184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.55300808
Iteration 2/25 | Loss: 0.00087688
Iteration 3/25 | Loss: 0.00087686
Iteration 4/25 | Loss: 0.00087686
Iteration 5/25 | Loss: 0.00087686
Iteration 6/25 | Loss: 0.00087686
Iteration 7/25 | Loss: 0.00087686
Iteration 8/25 | Loss: 0.00087686
Iteration 9/25 | Loss: 0.00087686
Iteration 10/25 | Loss: 0.00087686
Iteration 11/25 | Loss: 0.00087686
Iteration 12/25 | Loss: 0.00087686
Iteration 13/25 | Loss: 0.00087686
Iteration 14/25 | Loss: 0.00087686
Iteration 15/25 | Loss: 0.00087686
Iteration 16/25 | Loss: 0.00087686
Iteration 17/25 | Loss: 0.00087686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008768602274358273, 0.0008768602274358273, 0.0008768602274358273, 0.0008768602274358273, 0.0008768602274358273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008768602274358273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087686
Iteration 2/1000 | Loss: 0.00002758
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002127
Iteration 6/1000 | Loss: 0.00002061
Iteration 7/1000 | Loss: 0.00002042
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00002003
Iteration 11/1000 | Loss: 0.00002002
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001993
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001977
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001976
Iteration 29/1000 | Loss: 0.00001976
Iteration 30/1000 | Loss: 0.00001976
Iteration 31/1000 | Loss: 0.00001975
Iteration 32/1000 | Loss: 0.00001974
Iteration 33/1000 | Loss: 0.00001974
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001974
Iteration 37/1000 | Loss: 0.00001974
Iteration 38/1000 | Loss: 0.00001973
Iteration 39/1000 | Loss: 0.00001973
Iteration 40/1000 | Loss: 0.00001973
Iteration 41/1000 | Loss: 0.00001973
Iteration 42/1000 | Loss: 0.00001973
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001972
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001971
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001971
Iteration 49/1000 | Loss: 0.00001971
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001970
Iteration 52/1000 | Loss: 0.00001970
Iteration 53/1000 | Loss: 0.00001969
Iteration 54/1000 | Loss: 0.00001969
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001969
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001968
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001967
Iteration 70/1000 | Loss: 0.00001967
Iteration 71/1000 | Loss: 0.00001967
Iteration 72/1000 | Loss: 0.00001967
Iteration 73/1000 | Loss: 0.00001967
Iteration 74/1000 | Loss: 0.00001967
Iteration 75/1000 | Loss: 0.00001967
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001966
Iteration 78/1000 | Loss: 0.00001966
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00001966
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001965
Iteration 83/1000 | Loss: 0.00001965
Iteration 84/1000 | Loss: 0.00001965
Iteration 85/1000 | Loss: 0.00001965
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001965
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00001965
Iteration 104/1000 | Loss: 0.00001965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.964799957931973e-05, 1.964799957931973e-05, 1.964799957931973e-05, 1.964799957931973e-05, 1.964799957931973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.964799957931973e-05

Optimization complete. Final v2v error: 3.862262010574341 mm

Highest mean error: 9.378534317016602 mm for frame 91

Lowest mean error: 3.268021583557129 mm for frame 187

Saving results

Total time: 59.25269317626953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070795
Iteration 2/25 | Loss: 0.00242526
Iteration 3/25 | Loss: 0.00150273
Iteration 4/25 | Loss: 0.00134639
Iteration 5/25 | Loss: 0.00134877
Iteration 6/25 | Loss: 0.00128972
Iteration 7/25 | Loss: 0.00114088
Iteration 8/25 | Loss: 0.00110689
Iteration 9/25 | Loss: 0.00108543
Iteration 10/25 | Loss: 0.00108018
Iteration 11/25 | Loss: 0.00106708
Iteration 12/25 | Loss: 0.00105753
Iteration 13/25 | Loss: 0.00105554
Iteration 14/25 | Loss: 0.00105495
Iteration 15/25 | Loss: 0.00105461
Iteration 16/25 | Loss: 0.00105450
Iteration 17/25 | Loss: 0.00105450
Iteration 18/25 | Loss: 0.00105450
Iteration 19/25 | Loss: 0.00105450
Iteration 20/25 | Loss: 0.00105450
Iteration 21/25 | Loss: 0.00105450
Iteration 22/25 | Loss: 0.00105450
Iteration 23/25 | Loss: 0.00105450
Iteration 24/25 | Loss: 0.00105450
Iteration 25/25 | Loss: 0.00105450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43379402
Iteration 2/25 | Loss: 0.00130281
Iteration 3/25 | Loss: 0.00130281
Iteration 4/25 | Loss: 0.00130281
Iteration 5/25 | Loss: 0.00130281
Iteration 6/25 | Loss: 0.00130281
Iteration 7/25 | Loss: 0.00130281
Iteration 8/25 | Loss: 0.00130281
Iteration 9/25 | Loss: 0.00130281
Iteration 10/25 | Loss: 0.00130281
Iteration 11/25 | Loss: 0.00130281
Iteration 12/25 | Loss: 0.00130281
Iteration 13/25 | Loss: 0.00130281
Iteration 14/25 | Loss: 0.00130281
Iteration 15/25 | Loss: 0.00130281
Iteration 16/25 | Loss: 0.00130281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001302808173932135, 0.001302808173932135, 0.001302808173932135, 0.001302808173932135, 0.001302808173932135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302808173932135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130281
Iteration 2/1000 | Loss: 0.00009938
Iteration 3/1000 | Loss: 0.00006453
Iteration 4/1000 | Loss: 0.00005260
Iteration 5/1000 | Loss: 0.00004830
Iteration 6/1000 | Loss: 0.00004550
Iteration 7/1000 | Loss: 0.00004393
Iteration 8/1000 | Loss: 0.00004269
Iteration 9/1000 | Loss: 0.00004178
Iteration 10/1000 | Loss: 0.00004112
Iteration 11/1000 | Loss: 0.00114464
Iteration 12/1000 | Loss: 0.00005561
Iteration 13/1000 | Loss: 0.00056415
Iteration 14/1000 | Loss: 0.00005311
Iteration 15/1000 | Loss: 0.00059335
Iteration 16/1000 | Loss: 0.00004555
Iteration 17/1000 | Loss: 0.00061700
Iteration 18/1000 | Loss: 0.00004844
Iteration 19/1000 | Loss: 0.00003616
Iteration 20/1000 | Loss: 0.00003341
Iteration 21/1000 | Loss: 0.00065908
Iteration 22/1000 | Loss: 0.00005342
Iteration 23/1000 | Loss: 0.00003330
Iteration 24/1000 | Loss: 0.00060021
Iteration 25/1000 | Loss: 0.00004289
Iteration 26/1000 | Loss: 0.00003158
Iteration 27/1000 | Loss: 0.00002896
Iteration 28/1000 | Loss: 0.00002800
Iteration 29/1000 | Loss: 0.00002730
Iteration 30/1000 | Loss: 0.00002684
Iteration 31/1000 | Loss: 0.00002657
Iteration 32/1000 | Loss: 0.00002624
Iteration 33/1000 | Loss: 0.00002600
Iteration 34/1000 | Loss: 0.00002571
Iteration 35/1000 | Loss: 0.00002546
Iteration 36/1000 | Loss: 0.00002523
Iteration 37/1000 | Loss: 0.00002499
Iteration 38/1000 | Loss: 0.00002482
Iteration 39/1000 | Loss: 0.00055867
Iteration 40/1000 | Loss: 0.00019710
Iteration 41/1000 | Loss: 0.00003318
Iteration 42/1000 | Loss: 0.00003042
Iteration 43/1000 | Loss: 0.00002813
Iteration 44/1000 | Loss: 0.00002561
Iteration 45/1000 | Loss: 0.00002501
Iteration 46/1000 | Loss: 0.00002482
Iteration 47/1000 | Loss: 0.00058969
Iteration 48/1000 | Loss: 0.00008643
Iteration 49/1000 | Loss: 0.00002800
Iteration 50/1000 | Loss: 0.00002594
Iteration 51/1000 | Loss: 0.00002507
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00059241
Iteration 54/1000 | Loss: 0.00005285
Iteration 55/1000 | Loss: 0.00006237
Iteration 56/1000 | Loss: 0.00002878
Iteration 57/1000 | Loss: 0.00002517
Iteration 58/1000 | Loss: 0.00066982
Iteration 59/1000 | Loss: 0.00004515
Iteration 60/1000 | Loss: 0.00003346
Iteration 61/1000 | Loss: 0.00002755
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002185
Iteration 65/1000 | Loss: 0.00002137
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002094
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002065
Iteration 79/1000 | Loss: 0.00002065
Iteration 80/1000 | Loss: 0.00002064
Iteration 81/1000 | Loss: 0.00002064
Iteration 82/1000 | Loss: 0.00002064
Iteration 83/1000 | Loss: 0.00002064
Iteration 84/1000 | Loss: 0.00002063
Iteration 85/1000 | Loss: 0.00002063
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002061
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002059
Iteration 94/1000 | Loss: 0.00002059
Iteration 95/1000 | Loss: 0.00002058
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002057
Iteration 99/1000 | Loss: 0.00002057
Iteration 100/1000 | Loss: 0.00002057
Iteration 101/1000 | Loss: 0.00002057
Iteration 102/1000 | Loss: 0.00002056
Iteration 103/1000 | Loss: 0.00002056
Iteration 104/1000 | Loss: 0.00002056
Iteration 105/1000 | Loss: 0.00002056
Iteration 106/1000 | Loss: 0.00002056
Iteration 107/1000 | Loss: 0.00002056
Iteration 108/1000 | Loss: 0.00002056
Iteration 109/1000 | Loss: 0.00002055
Iteration 110/1000 | Loss: 0.00002055
Iteration 111/1000 | Loss: 0.00002054
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002054
Iteration 115/1000 | Loss: 0.00002054
Iteration 116/1000 | Loss: 0.00002053
Iteration 117/1000 | Loss: 0.00002053
Iteration 118/1000 | Loss: 0.00002053
Iteration 119/1000 | Loss: 0.00002053
Iteration 120/1000 | Loss: 0.00002053
Iteration 121/1000 | Loss: 0.00002052
Iteration 122/1000 | Loss: 0.00002052
Iteration 123/1000 | Loss: 0.00002052
Iteration 124/1000 | Loss: 0.00002052
Iteration 125/1000 | Loss: 0.00002052
Iteration 126/1000 | Loss: 0.00002052
Iteration 127/1000 | Loss: 0.00002051
Iteration 128/1000 | Loss: 0.00002051
Iteration 129/1000 | Loss: 0.00002051
Iteration 130/1000 | Loss: 0.00002050
Iteration 131/1000 | Loss: 0.00002050
Iteration 132/1000 | Loss: 0.00002050
Iteration 133/1000 | Loss: 0.00002049
Iteration 134/1000 | Loss: 0.00002049
Iteration 135/1000 | Loss: 0.00002049
Iteration 136/1000 | Loss: 0.00002048
Iteration 137/1000 | Loss: 0.00002048
Iteration 138/1000 | Loss: 0.00002048
Iteration 139/1000 | Loss: 0.00002048
Iteration 140/1000 | Loss: 0.00002048
Iteration 141/1000 | Loss: 0.00002048
Iteration 142/1000 | Loss: 0.00002048
Iteration 143/1000 | Loss: 0.00002048
Iteration 144/1000 | Loss: 0.00002048
Iteration 145/1000 | Loss: 0.00002048
Iteration 146/1000 | Loss: 0.00002048
Iteration 147/1000 | Loss: 0.00002047
Iteration 148/1000 | Loss: 0.00002047
Iteration 149/1000 | Loss: 0.00002047
Iteration 150/1000 | Loss: 0.00002046
Iteration 151/1000 | Loss: 0.00002046
Iteration 152/1000 | Loss: 0.00002046
Iteration 153/1000 | Loss: 0.00002046
Iteration 154/1000 | Loss: 0.00002046
Iteration 155/1000 | Loss: 0.00002046
Iteration 156/1000 | Loss: 0.00002046
Iteration 157/1000 | Loss: 0.00002046
Iteration 158/1000 | Loss: 0.00002046
Iteration 159/1000 | Loss: 0.00002046
Iteration 160/1000 | Loss: 0.00002046
Iteration 161/1000 | Loss: 0.00002046
Iteration 162/1000 | Loss: 0.00002046
Iteration 163/1000 | Loss: 0.00002046
Iteration 164/1000 | Loss: 0.00002046
Iteration 165/1000 | Loss: 0.00002046
Iteration 166/1000 | Loss: 0.00002046
Iteration 167/1000 | Loss: 0.00002046
Iteration 168/1000 | Loss: 0.00002046
Iteration 169/1000 | Loss: 0.00002046
Iteration 170/1000 | Loss: 0.00002046
Iteration 171/1000 | Loss: 0.00002046
Iteration 172/1000 | Loss: 0.00002046
Iteration 173/1000 | Loss: 0.00002046
Iteration 174/1000 | Loss: 0.00002046
Iteration 175/1000 | Loss: 0.00002046
Iteration 176/1000 | Loss: 0.00002046
Iteration 177/1000 | Loss: 0.00002046
Iteration 178/1000 | Loss: 0.00002046
Iteration 179/1000 | Loss: 0.00002046
Iteration 180/1000 | Loss: 0.00002046
Iteration 181/1000 | Loss: 0.00002046
Iteration 182/1000 | Loss: 0.00002046
Iteration 183/1000 | Loss: 0.00002046
Iteration 184/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.045711880782619e-05, 2.045711880782619e-05, 2.045711880782619e-05, 2.045711880782619e-05, 2.045711880782619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.045711880782619e-05

Optimization complete. Final v2v error: 3.9670135974884033 mm

Highest mean error: 5.140579700469971 mm for frame 5

Lowest mean error: 3.624800682067871 mm for frame 17

Saving results

Total time: 154.86856198310852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905774
Iteration 2/25 | Loss: 0.00176866
Iteration 3/25 | Loss: 0.00135523
Iteration 4/25 | Loss: 0.00126558
Iteration 5/25 | Loss: 0.00124019
Iteration 6/25 | Loss: 0.00120735
Iteration 7/25 | Loss: 0.00120141
Iteration 8/25 | Loss: 0.00119295
Iteration 9/25 | Loss: 0.00119348
Iteration 10/25 | Loss: 0.00118893
Iteration 11/25 | Loss: 0.00117805
Iteration 12/25 | Loss: 0.00118496
Iteration 13/25 | Loss: 0.00118364
Iteration 14/25 | Loss: 0.00118217
Iteration 15/25 | Loss: 0.00118192
Iteration 16/25 | Loss: 0.00118031
Iteration 17/25 | Loss: 0.00117680
Iteration 18/25 | Loss: 0.00118196
Iteration 19/25 | Loss: 0.00117847
Iteration 20/25 | Loss: 0.00117931
Iteration 21/25 | Loss: 0.00117226
Iteration 22/25 | Loss: 0.00117592
Iteration 23/25 | Loss: 0.00117635
Iteration 24/25 | Loss: 0.00117158
Iteration 25/25 | Loss: 0.00117020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61981130
Iteration 2/25 | Loss: 0.00152813
Iteration 3/25 | Loss: 0.00149585
Iteration 4/25 | Loss: 0.00149585
Iteration 5/25 | Loss: 0.00149585
Iteration 6/25 | Loss: 0.00149585
Iteration 7/25 | Loss: 0.00149585
Iteration 8/25 | Loss: 0.00149585
Iteration 9/25 | Loss: 0.00149585
Iteration 10/25 | Loss: 0.00149585
Iteration 11/25 | Loss: 0.00149585
Iteration 12/25 | Loss: 0.00149585
Iteration 13/25 | Loss: 0.00149585
Iteration 14/25 | Loss: 0.00149585
Iteration 15/25 | Loss: 0.00149585
Iteration 16/25 | Loss: 0.00149585
Iteration 17/25 | Loss: 0.00149585
Iteration 18/25 | Loss: 0.00149585
Iteration 19/25 | Loss: 0.00149585
Iteration 20/25 | Loss: 0.00149585
Iteration 21/25 | Loss: 0.00149585
Iteration 22/25 | Loss: 0.00149585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014958473620936275, 0.0014958473620936275, 0.0014958473620936275, 0.0014958473620936275, 0.0014958473620936275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014958473620936275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149585
Iteration 2/1000 | Loss: 0.00017448
Iteration 3/1000 | Loss: 0.00009194
Iteration 4/1000 | Loss: 0.00008806
Iteration 5/1000 | Loss: 0.00007079
Iteration 6/1000 | Loss: 0.00007047
Iteration 7/1000 | Loss: 0.00007036
Iteration 8/1000 | Loss: 0.00041223
Iteration 9/1000 | Loss: 0.00012133
Iteration 10/1000 | Loss: 0.00010585
Iteration 11/1000 | Loss: 0.00009323
Iteration 12/1000 | Loss: 0.00007707
Iteration 13/1000 | Loss: 0.00006822
Iteration 14/1000 | Loss: 0.00007926
Iteration 15/1000 | Loss: 0.00006643
Iteration 16/1000 | Loss: 0.00008884
Iteration 17/1000 | Loss: 0.00008821
Iteration 18/1000 | Loss: 0.00008637
Iteration 19/1000 | Loss: 0.00019697
Iteration 20/1000 | Loss: 0.00023403
Iteration 21/1000 | Loss: 0.00022013
Iteration 22/1000 | Loss: 0.00009817
Iteration 23/1000 | Loss: 0.00009464
Iteration 24/1000 | Loss: 0.00007014
Iteration 25/1000 | Loss: 0.00007744
Iteration 26/1000 | Loss: 0.00006669
Iteration 27/1000 | Loss: 0.00012763
Iteration 28/1000 | Loss: 0.00004858
Iteration 29/1000 | Loss: 0.00004252
Iteration 30/1000 | Loss: 0.00004058
Iteration 31/1000 | Loss: 0.00003916
Iteration 32/1000 | Loss: 0.00003843
Iteration 33/1000 | Loss: 0.00072974
Iteration 34/1000 | Loss: 0.00005436
Iteration 35/1000 | Loss: 0.00004388
Iteration 36/1000 | Loss: 0.00078541
Iteration 37/1000 | Loss: 0.00084356
Iteration 38/1000 | Loss: 0.00006249
Iteration 39/1000 | Loss: 0.00004931
Iteration 40/1000 | Loss: 0.00004427
Iteration 41/1000 | Loss: 0.00034103
Iteration 42/1000 | Loss: 0.00053939
Iteration 43/1000 | Loss: 0.00005671
Iteration 44/1000 | Loss: 0.00063500
Iteration 45/1000 | Loss: 0.00055076
Iteration 46/1000 | Loss: 0.00081735
Iteration 47/1000 | Loss: 0.00066089
Iteration 48/1000 | Loss: 0.00017145
Iteration 49/1000 | Loss: 0.00004862
Iteration 50/1000 | Loss: 0.00031936
Iteration 51/1000 | Loss: 0.00018738
Iteration 52/1000 | Loss: 0.00005200
Iteration 53/1000 | Loss: 0.00004442
Iteration 54/1000 | Loss: 0.00004114
Iteration 55/1000 | Loss: 0.00003947
Iteration 56/1000 | Loss: 0.00038220
Iteration 57/1000 | Loss: 0.00035387
Iteration 58/1000 | Loss: 0.00017721
Iteration 59/1000 | Loss: 0.00012503
Iteration 60/1000 | Loss: 0.00007795
Iteration 61/1000 | Loss: 0.00036605
Iteration 62/1000 | Loss: 0.00056487
Iteration 63/1000 | Loss: 0.00069897
Iteration 64/1000 | Loss: 0.00061861
Iteration 65/1000 | Loss: 0.00063203
Iteration 66/1000 | Loss: 0.00004011
Iteration 67/1000 | Loss: 0.00003630
Iteration 68/1000 | Loss: 0.00003446
Iteration 69/1000 | Loss: 0.00003229
Iteration 70/1000 | Loss: 0.00003179
Iteration 71/1000 | Loss: 0.00003153
Iteration 72/1000 | Loss: 0.00003132
Iteration 73/1000 | Loss: 0.00108355
Iteration 74/1000 | Loss: 0.00028292
Iteration 75/1000 | Loss: 0.00003249
Iteration 76/1000 | Loss: 0.00003114
Iteration 77/1000 | Loss: 0.00003088
Iteration 78/1000 | Loss: 0.00003084
Iteration 79/1000 | Loss: 0.00003083
Iteration 80/1000 | Loss: 0.00003083
Iteration 81/1000 | Loss: 0.00003082
Iteration 82/1000 | Loss: 0.00003082
Iteration 83/1000 | Loss: 0.00003082
Iteration 84/1000 | Loss: 0.00003081
Iteration 85/1000 | Loss: 0.00003080
Iteration 86/1000 | Loss: 0.00003080
Iteration 87/1000 | Loss: 0.00003079
Iteration 88/1000 | Loss: 0.00003079
Iteration 89/1000 | Loss: 0.00003079
Iteration 90/1000 | Loss: 0.00003079
Iteration 91/1000 | Loss: 0.00003078
Iteration 92/1000 | Loss: 0.00003078
Iteration 93/1000 | Loss: 0.00003077
Iteration 94/1000 | Loss: 0.00003077
Iteration 95/1000 | Loss: 0.00003075
Iteration 96/1000 | Loss: 0.00003074
Iteration 97/1000 | Loss: 0.00003073
Iteration 98/1000 | Loss: 0.00003073
Iteration 99/1000 | Loss: 0.00003073
Iteration 100/1000 | Loss: 0.00003071
Iteration 101/1000 | Loss: 0.00003071
Iteration 102/1000 | Loss: 0.00003070
Iteration 103/1000 | Loss: 0.00003070
Iteration 104/1000 | Loss: 0.00003070
Iteration 105/1000 | Loss: 0.00003069
Iteration 106/1000 | Loss: 0.00003069
Iteration 107/1000 | Loss: 0.00003066
Iteration 108/1000 | Loss: 0.00003066
Iteration 109/1000 | Loss: 0.00003065
Iteration 110/1000 | Loss: 0.00003064
Iteration 111/1000 | Loss: 0.00003064
Iteration 112/1000 | Loss: 0.00003064
Iteration 113/1000 | Loss: 0.00003063
Iteration 114/1000 | Loss: 0.00003063
Iteration 115/1000 | Loss: 0.00003063
Iteration 116/1000 | Loss: 0.00003062
Iteration 117/1000 | Loss: 0.00003060
Iteration 118/1000 | Loss: 0.00003060
Iteration 119/1000 | Loss: 0.00003060
Iteration 120/1000 | Loss: 0.00003059
Iteration 121/1000 | Loss: 0.00003059
Iteration 122/1000 | Loss: 0.00003058
Iteration 123/1000 | Loss: 0.00003057
Iteration 124/1000 | Loss: 0.00003057
Iteration 125/1000 | Loss: 0.00003057
Iteration 126/1000 | Loss: 0.00003056
Iteration 127/1000 | Loss: 0.00003056
Iteration 128/1000 | Loss: 0.00003056
Iteration 129/1000 | Loss: 0.00003056
Iteration 130/1000 | Loss: 0.00003056
Iteration 131/1000 | Loss: 0.00003056
Iteration 132/1000 | Loss: 0.00003056
Iteration 133/1000 | Loss: 0.00003056
Iteration 134/1000 | Loss: 0.00003056
Iteration 135/1000 | Loss: 0.00003056
Iteration 136/1000 | Loss: 0.00003056
Iteration 137/1000 | Loss: 0.00003055
Iteration 138/1000 | Loss: 0.00003055
Iteration 139/1000 | Loss: 0.00003054
Iteration 140/1000 | Loss: 0.00003054
Iteration 141/1000 | Loss: 0.00003053
Iteration 142/1000 | Loss: 0.00003053
Iteration 143/1000 | Loss: 0.00003052
Iteration 144/1000 | Loss: 0.00003052
Iteration 145/1000 | Loss: 0.00105690
Iteration 146/1000 | Loss: 0.00006715
Iteration 147/1000 | Loss: 0.00003551
Iteration 148/1000 | Loss: 0.00003147
Iteration 149/1000 | Loss: 0.00002959
Iteration 150/1000 | Loss: 0.00002874
Iteration 151/1000 | Loss: 0.00002854
Iteration 152/1000 | Loss: 0.00002841
Iteration 153/1000 | Loss: 0.00002841
Iteration 154/1000 | Loss: 0.00002839
Iteration 155/1000 | Loss: 0.00002837
Iteration 156/1000 | Loss: 0.00002834
Iteration 157/1000 | Loss: 0.00002834
Iteration 158/1000 | Loss: 0.00002833
Iteration 159/1000 | Loss: 0.00002833
Iteration 160/1000 | Loss: 0.00002833
Iteration 161/1000 | Loss: 0.00002832
Iteration 162/1000 | Loss: 0.00002832
Iteration 163/1000 | Loss: 0.00002832
Iteration 164/1000 | Loss: 0.00002831
Iteration 165/1000 | Loss: 0.00002831
Iteration 166/1000 | Loss: 0.00002831
Iteration 167/1000 | Loss: 0.00002831
Iteration 168/1000 | Loss: 0.00002831
Iteration 169/1000 | Loss: 0.00002831
Iteration 170/1000 | Loss: 0.00002830
Iteration 171/1000 | Loss: 0.00002830
Iteration 172/1000 | Loss: 0.00002830
Iteration 173/1000 | Loss: 0.00002829
Iteration 174/1000 | Loss: 0.00002829
Iteration 175/1000 | Loss: 0.00002829
Iteration 176/1000 | Loss: 0.00002829
Iteration 177/1000 | Loss: 0.00002829
Iteration 178/1000 | Loss: 0.00002829
Iteration 179/1000 | Loss: 0.00002829
Iteration 180/1000 | Loss: 0.00002828
Iteration 181/1000 | Loss: 0.00002828
Iteration 182/1000 | Loss: 0.00002828
Iteration 183/1000 | Loss: 0.00002828
Iteration 184/1000 | Loss: 0.00002828
Iteration 185/1000 | Loss: 0.00002827
Iteration 186/1000 | Loss: 0.00002827
Iteration 187/1000 | Loss: 0.00002827
Iteration 188/1000 | Loss: 0.00002827
Iteration 189/1000 | Loss: 0.00002827
Iteration 190/1000 | Loss: 0.00002827
Iteration 191/1000 | Loss: 0.00002827
Iteration 192/1000 | Loss: 0.00002827
Iteration 193/1000 | Loss: 0.00002827
Iteration 194/1000 | Loss: 0.00002827
Iteration 195/1000 | Loss: 0.00002827
Iteration 196/1000 | Loss: 0.00002827
Iteration 197/1000 | Loss: 0.00002826
Iteration 198/1000 | Loss: 0.00002826
Iteration 199/1000 | Loss: 0.00002826
Iteration 200/1000 | Loss: 0.00002826
Iteration 201/1000 | Loss: 0.00002826
Iteration 202/1000 | Loss: 0.00002826
Iteration 203/1000 | Loss: 0.00002825
Iteration 204/1000 | Loss: 0.00002825
Iteration 205/1000 | Loss: 0.00002825
Iteration 206/1000 | Loss: 0.00002825
Iteration 207/1000 | Loss: 0.00002825
Iteration 208/1000 | Loss: 0.00002825
Iteration 209/1000 | Loss: 0.00002824
Iteration 210/1000 | Loss: 0.00002824
Iteration 211/1000 | Loss: 0.00002824
Iteration 212/1000 | Loss: 0.00002824
Iteration 213/1000 | Loss: 0.00002824
Iteration 214/1000 | Loss: 0.00002824
Iteration 215/1000 | Loss: 0.00002824
Iteration 216/1000 | Loss: 0.00002824
Iteration 217/1000 | Loss: 0.00002824
Iteration 218/1000 | Loss: 0.00002824
Iteration 219/1000 | Loss: 0.00002824
Iteration 220/1000 | Loss: 0.00002824
Iteration 221/1000 | Loss: 0.00002824
Iteration 222/1000 | Loss: 0.00002824
Iteration 223/1000 | Loss: 0.00002824
Iteration 224/1000 | Loss: 0.00002824
Iteration 225/1000 | Loss: 0.00002824
Iteration 226/1000 | Loss: 0.00002824
Iteration 227/1000 | Loss: 0.00002824
Iteration 228/1000 | Loss: 0.00002824
Iteration 229/1000 | Loss: 0.00002824
Iteration 230/1000 | Loss: 0.00002824
Iteration 231/1000 | Loss: 0.00002824
Iteration 232/1000 | Loss: 0.00002824
Iteration 233/1000 | Loss: 0.00002824
Iteration 234/1000 | Loss: 0.00002824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.8235393983777612e-05, 2.8235393983777612e-05, 2.8235393983777612e-05, 2.8235393983777612e-05, 2.8235393983777612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8235393983777612e-05

Optimization complete. Final v2v error: 4.5996599197387695 mm

Highest mean error: 6.692239284515381 mm for frame 185

Lowest mean error: 3.645939826965332 mm for frame 216

Saving results

Total time: 204.75185179710388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471624
Iteration 2/25 | Loss: 0.00126862
Iteration 3/25 | Loss: 0.00104333
Iteration 4/25 | Loss: 0.00102256
Iteration 5/25 | Loss: 0.00101739
Iteration 6/25 | Loss: 0.00101562
Iteration 7/25 | Loss: 0.00101537
Iteration 8/25 | Loss: 0.00101537
Iteration 9/25 | Loss: 0.00101537
Iteration 10/25 | Loss: 0.00101537
Iteration 11/25 | Loss: 0.00101537
Iteration 12/25 | Loss: 0.00101537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010153690818697214, 0.0010153690818697214, 0.0010153690818697214, 0.0010153690818697214, 0.0010153690818697214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010153690818697214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46553111
Iteration 2/25 | Loss: 0.00097375
Iteration 3/25 | Loss: 0.00097375
Iteration 4/25 | Loss: 0.00097375
Iteration 5/25 | Loss: 0.00097375
Iteration 6/25 | Loss: 0.00097375
Iteration 7/25 | Loss: 0.00097375
Iteration 8/25 | Loss: 0.00097375
Iteration 9/25 | Loss: 0.00097375
Iteration 10/25 | Loss: 0.00097375
Iteration 11/25 | Loss: 0.00097375
Iteration 12/25 | Loss: 0.00097375
Iteration 13/25 | Loss: 0.00097375
Iteration 14/25 | Loss: 0.00097375
Iteration 15/25 | Loss: 0.00097375
Iteration 16/25 | Loss: 0.00097375
Iteration 17/25 | Loss: 0.00097375
Iteration 18/25 | Loss: 0.00097375
Iteration 19/25 | Loss: 0.00097375
Iteration 20/25 | Loss: 0.00097375
Iteration 21/25 | Loss: 0.00097375
Iteration 22/25 | Loss: 0.00097375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009737465297803283, 0.0009737465297803283, 0.0009737465297803283, 0.0009737465297803283, 0.0009737465297803283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009737465297803283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097375
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002897
Iteration 4/1000 | Loss: 0.00002647
Iteration 5/1000 | Loss: 0.00002496
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002359
Iteration 8/1000 | Loss: 0.00002310
Iteration 9/1000 | Loss: 0.00002293
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00002263
Iteration 12/1000 | Loss: 0.00002260
Iteration 13/1000 | Loss: 0.00002257
Iteration 14/1000 | Loss: 0.00002256
Iteration 15/1000 | Loss: 0.00002256
Iteration 16/1000 | Loss: 0.00002255
Iteration 17/1000 | Loss: 0.00002255
Iteration 18/1000 | Loss: 0.00002255
Iteration 19/1000 | Loss: 0.00002252
Iteration 20/1000 | Loss: 0.00002252
Iteration 21/1000 | Loss: 0.00002252
Iteration 22/1000 | Loss: 0.00002252
Iteration 23/1000 | Loss: 0.00002251
Iteration 24/1000 | Loss: 0.00002251
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002251
Iteration 27/1000 | Loss: 0.00002250
Iteration 28/1000 | Loss: 0.00002249
Iteration 29/1000 | Loss: 0.00002249
Iteration 30/1000 | Loss: 0.00002249
Iteration 31/1000 | Loss: 0.00002249
Iteration 32/1000 | Loss: 0.00002248
Iteration 33/1000 | Loss: 0.00002248
Iteration 34/1000 | Loss: 0.00002248
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00002246
Iteration 37/1000 | Loss: 0.00002246
Iteration 38/1000 | Loss: 0.00002245
Iteration 39/1000 | Loss: 0.00002245
Iteration 40/1000 | Loss: 0.00002245
Iteration 41/1000 | Loss: 0.00002244
Iteration 42/1000 | Loss: 0.00002244
Iteration 43/1000 | Loss: 0.00002243
Iteration 44/1000 | Loss: 0.00002243
Iteration 45/1000 | Loss: 0.00002243
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00002243
Iteration 48/1000 | Loss: 0.00002242
Iteration 49/1000 | Loss: 0.00002241
Iteration 50/1000 | Loss: 0.00002241
Iteration 51/1000 | Loss: 0.00002241
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00002238
Iteration 57/1000 | Loss: 0.00002238
Iteration 58/1000 | Loss: 0.00002238
Iteration 59/1000 | Loss: 0.00002238
Iteration 60/1000 | Loss: 0.00002238
Iteration 61/1000 | Loss: 0.00002238
Iteration 62/1000 | Loss: 0.00002237
Iteration 63/1000 | Loss: 0.00002237
Iteration 64/1000 | Loss: 0.00002237
Iteration 65/1000 | Loss: 0.00002237
Iteration 66/1000 | Loss: 0.00002237
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002236
Iteration 71/1000 | Loss: 0.00002236
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002235
Iteration 75/1000 | Loss: 0.00002235
Iteration 76/1000 | Loss: 0.00002235
Iteration 77/1000 | Loss: 0.00002235
Iteration 78/1000 | Loss: 0.00002235
Iteration 79/1000 | Loss: 0.00002235
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002234
Iteration 82/1000 | Loss: 0.00002234
Iteration 83/1000 | Loss: 0.00002234
Iteration 84/1000 | Loss: 0.00002234
Iteration 85/1000 | Loss: 0.00002234
Iteration 86/1000 | Loss: 0.00002234
Iteration 87/1000 | Loss: 0.00002234
Iteration 88/1000 | Loss: 0.00002234
Iteration 89/1000 | Loss: 0.00002234
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002233
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002233
Iteration 102/1000 | Loss: 0.00002233
Iteration 103/1000 | Loss: 0.00002233
Iteration 104/1000 | Loss: 0.00002233
Iteration 105/1000 | Loss: 0.00002233
Iteration 106/1000 | Loss: 0.00002233
Iteration 107/1000 | Loss: 0.00002233
Iteration 108/1000 | Loss: 0.00002233
Iteration 109/1000 | Loss: 0.00002233
Iteration 110/1000 | Loss: 0.00002233
Iteration 111/1000 | Loss: 0.00002233
Iteration 112/1000 | Loss: 0.00002233
Iteration 113/1000 | Loss: 0.00002233
Iteration 114/1000 | Loss: 0.00002233
Iteration 115/1000 | Loss: 0.00002233
Iteration 116/1000 | Loss: 0.00002233
Iteration 117/1000 | Loss: 0.00002233
Iteration 118/1000 | Loss: 0.00002233
Iteration 119/1000 | Loss: 0.00002233
Iteration 120/1000 | Loss: 0.00002233
Iteration 121/1000 | Loss: 0.00002233
Iteration 122/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.2326405087369494e-05, 2.2326405087369494e-05, 2.2326405087369494e-05, 2.2326405087369494e-05, 2.2326405087369494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2326405087369494e-05

Optimization complete. Final v2v error: 4.071397304534912 mm

Highest mean error: 4.574015140533447 mm for frame 34

Lowest mean error: 3.509206533432007 mm for frame 149

Saving results

Total time: 37.496121883392334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363196
Iteration 2/25 | Loss: 0.00131694
Iteration 3/25 | Loss: 0.00111501
Iteration 4/25 | Loss: 0.00103130
Iteration 5/25 | Loss: 0.00101811
Iteration 6/25 | Loss: 0.00101559
Iteration 7/25 | Loss: 0.00101509
Iteration 8/25 | Loss: 0.00101491
Iteration 9/25 | Loss: 0.00101487
Iteration 10/25 | Loss: 0.00101487
Iteration 11/25 | Loss: 0.00101487
Iteration 12/25 | Loss: 0.00101487
Iteration 13/25 | Loss: 0.00101487
Iteration 14/25 | Loss: 0.00101487
Iteration 15/25 | Loss: 0.00101487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010148662840947509, 0.0010148662840947509, 0.0010148662840947509, 0.0010148662840947509, 0.0010148662840947509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010148662840947509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47842979
Iteration 2/25 | Loss: 0.00105817
Iteration 3/25 | Loss: 0.00105817
Iteration 4/25 | Loss: 0.00105817
Iteration 5/25 | Loss: 0.00105817
Iteration 6/25 | Loss: 0.00105817
Iteration 7/25 | Loss: 0.00105817
Iteration 8/25 | Loss: 0.00105817
Iteration 9/25 | Loss: 0.00105817
Iteration 10/25 | Loss: 0.00105817
Iteration 11/25 | Loss: 0.00105817
Iteration 12/25 | Loss: 0.00105817
Iteration 13/25 | Loss: 0.00105817
Iteration 14/25 | Loss: 0.00105817
Iteration 15/25 | Loss: 0.00105817
Iteration 16/25 | Loss: 0.00105817
Iteration 17/25 | Loss: 0.00105817
Iteration 18/25 | Loss: 0.00105817
Iteration 19/25 | Loss: 0.00105817
Iteration 20/25 | Loss: 0.00105817
Iteration 21/25 | Loss: 0.00105817
Iteration 22/25 | Loss: 0.00105817
Iteration 23/25 | Loss: 0.00105817
Iteration 24/25 | Loss: 0.00105817
Iteration 25/25 | Loss: 0.00105817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105817
Iteration 2/1000 | Loss: 0.00004378
Iteration 3/1000 | Loss: 0.00002872
Iteration 4/1000 | Loss: 0.00002388
Iteration 5/1000 | Loss: 0.00002171
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00001977
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001868
Iteration 12/1000 | Loss: 0.00001854
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001849
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001843
Iteration 18/1000 | Loss: 0.00001841
Iteration 19/1000 | Loss: 0.00001841
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001837
Iteration 22/1000 | Loss: 0.00001836
Iteration 23/1000 | Loss: 0.00001836
Iteration 24/1000 | Loss: 0.00001835
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001830
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001825
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001824
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001821
Iteration 48/1000 | Loss: 0.00001821
Iteration 49/1000 | Loss: 0.00001821
Iteration 50/1000 | Loss: 0.00001821
Iteration 51/1000 | Loss: 0.00001821
Iteration 52/1000 | Loss: 0.00001821
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001820
Iteration 63/1000 | Loss: 0.00001820
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001819
Iteration 68/1000 | Loss: 0.00001819
Iteration 69/1000 | Loss: 0.00001819
Iteration 70/1000 | Loss: 0.00001819
Iteration 71/1000 | Loss: 0.00001819
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001815
Iteration 91/1000 | Loss: 0.00001815
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001814
Iteration 95/1000 | Loss: 0.00001814
Iteration 96/1000 | Loss: 0.00001814
Iteration 97/1000 | Loss: 0.00001813
Iteration 98/1000 | Loss: 0.00001813
Iteration 99/1000 | Loss: 0.00001813
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001812
Iteration 107/1000 | Loss: 0.00001812
Iteration 108/1000 | Loss: 0.00001812
Iteration 109/1000 | Loss: 0.00001812
Iteration 110/1000 | Loss: 0.00001812
Iteration 111/1000 | Loss: 0.00001812
Iteration 112/1000 | Loss: 0.00001812
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00001812
Iteration 117/1000 | Loss: 0.00001812
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Iteration 120/1000 | Loss: 0.00001812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.8117691070074216e-05, 1.8117691070074216e-05, 1.8117691070074216e-05, 1.8117691070074216e-05, 1.8117691070074216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8117691070074216e-05

Optimization complete. Final v2v error: 3.6612370014190674 mm

Highest mean error: 4.167815208435059 mm for frame 32

Lowest mean error: 3.2043261528015137 mm for frame 153

Saving results

Total time: 38.941200971603394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472257
Iteration 2/25 | Loss: 0.00116824
Iteration 3/25 | Loss: 0.00101810
Iteration 4/25 | Loss: 0.00099614
Iteration 5/25 | Loss: 0.00098811
Iteration 6/25 | Loss: 0.00098662
Iteration 7/25 | Loss: 0.00098651
Iteration 8/25 | Loss: 0.00098651
Iteration 9/25 | Loss: 0.00098651
Iteration 10/25 | Loss: 0.00098651
Iteration 11/25 | Loss: 0.00098651
Iteration 12/25 | Loss: 0.00098651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000986513216048479, 0.000986513216048479, 0.000986513216048479, 0.000986513216048479, 0.000986513216048479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000986513216048479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47133803
Iteration 2/25 | Loss: 0.00086171
Iteration 3/25 | Loss: 0.00086170
Iteration 4/25 | Loss: 0.00086170
Iteration 5/25 | Loss: 0.00086169
Iteration 6/25 | Loss: 0.00086169
Iteration 7/25 | Loss: 0.00086169
Iteration 8/25 | Loss: 0.00086169
Iteration 9/25 | Loss: 0.00086169
Iteration 10/25 | Loss: 0.00086169
Iteration 11/25 | Loss: 0.00086169
Iteration 12/25 | Loss: 0.00086169
Iteration 13/25 | Loss: 0.00086169
Iteration 14/25 | Loss: 0.00086169
Iteration 15/25 | Loss: 0.00086169
Iteration 16/25 | Loss: 0.00086169
Iteration 17/25 | Loss: 0.00086169
Iteration 18/25 | Loss: 0.00086169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008616927661933005, 0.0008616927661933005, 0.0008616927661933005, 0.0008616927661933005, 0.0008616927661933005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008616927661933005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086169
Iteration 2/1000 | Loss: 0.00003642
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001797
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001741
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001711
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001699
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001696
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001695
Iteration 50/1000 | Loss: 0.00001695
Iteration 51/1000 | Loss: 0.00001695
Iteration 52/1000 | Loss: 0.00001695
Iteration 53/1000 | Loss: 0.00001695
Iteration 54/1000 | Loss: 0.00001694
Iteration 55/1000 | Loss: 0.00001694
Iteration 56/1000 | Loss: 0.00001694
Iteration 57/1000 | Loss: 0.00001693
Iteration 58/1000 | Loss: 0.00001693
Iteration 59/1000 | Loss: 0.00001693
Iteration 60/1000 | Loss: 0.00001693
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001692
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001691
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001691
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001690
Iteration 77/1000 | Loss: 0.00001690
Iteration 78/1000 | Loss: 0.00001690
Iteration 79/1000 | Loss: 0.00001690
Iteration 80/1000 | Loss: 0.00001690
Iteration 81/1000 | Loss: 0.00001690
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001690
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001689
Iteration 89/1000 | Loss: 0.00001689
Iteration 90/1000 | Loss: 0.00001689
Iteration 91/1000 | Loss: 0.00001689
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001689
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001686
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001683
Iteration 147/1000 | Loss: 0.00001683
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001683
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Iteration 169/1000 | Loss: 0.00001683
Iteration 170/1000 | Loss: 0.00001683
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001683
Iteration 175/1000 | Loss: 0.00001683
Iteration 176/1000 | Loss: 0.00001683
Iteration 177/1000 | Loss: 0.00001683
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.6825042621348985e-05, 1.6825042621348985e-05, 1.6825042621348985e-05, 1.6825042621348985e-05, 1.6825042621348985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6825042621348985e-05

Optimization complete. Final v2v error: 3.556112766265869 mm

Highest mean error: 3.867461919784546 mm for frame 35

Lowest mean error: 3.1916286945343018 mm for frame 126

Saving results

Total time: 34.790576219558716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927846
Iteration 2/25 | Loss: 0.00145765
Iteration 3/25 | Loss: 0.00131784
Iteration 4/25 | Loss: 0.00128599
Iteration 5/25 | Loss: 0.00127861
Iteration 6/25 | Loss: 0.00127626
Iteration 7/25 | Loss: 0.00127557
Iteration 8/25 | Loss: 0.00127557
Iteration 9/25 | Loss: 0.00127557
Iteration 10/25 | Loss: 0.00127557
Iteration 11/25 | Loss: 0.00127557
Iteration 12/25 | Loss: 0.00127557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012755661737173796, 0.0012755661737173796, 0.0012755661737173796, 0.0012755661737173796, 0.0012755661737173796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012755661737173796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41732764
Iteration 2/25 | Loss: 0.00091776
Iteration 3/25 | Loss: 0.00091768
Iteration 4/25 | Loss: 0.00091767
Iteration 5/25 | Loss: 0.00091767
Iteration 6/25 | Loss: 0.00091767
Iteration 7/25 | Loss: 0.00091767
Iteration 8/25 | Loss: 0.00091767
Iteration 9/25 | Loss: 0.00091767
Iteration 10/25 | Loss: 0.00091767
Iteration 11/25 | Loss: 0.00091767
Iteration 12/25 | Loss: 0.00091767
Iteration 13/25 | Loss: 0.00091767
Iteration 14/25 | Loss: 0.00091767
Iteration 15/25 | Loss: 0.00091767
Iteration 16/25 | Loss: 0.00091767
Iteration 17/25 | Loss: 0.00091767
Iteration 18/25 | Loss: 0.00091767
Iteration 19/25 | Loss: 0.00091767
Iteration 20/25 | Loss: 0.00091767
Iteration 21/25 | Loss: 0.00091767
Iteration 22/25 | Loss: 0.00091767
Iteration 23/25 | Loss: 0.00091767
Iteration 24/25 | Loss: 0.00091767
Iteration 25/25 | Loss: 0.00091767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091767
Iteration 2/1000 | Loss: 0.00007753
Iteration 3/1000 | Loss: 0.00006243
Iteration 4/1000 | Loss: 0.00005686
Iteration 5/1000 | Loss: 0.00005467
Iteration 6/1000 | Loss: 0.00005340
Iteration 7/1000 | Loss: 0.00005163
Iteration 8/1000 | Loss: 0.00005041
Iteration 9/1000 | Loss: 0.00004971
Iteration 10/1000 | Loss: 0.00004905
Iteration 11/1000 | Loss: 0.00004861
Iteration 12/1000 | Loss: 0.00004836
Iteration 13/1000 | Loss: 0.00004835
Iteration 14/1000 | Loss: 0.00004835
Iteration 15/1000 | Loss: 0.00004834
Iteration 16/1000 | Loss: 0.00004825
Iteration 17/1000 | Loss: 0.00004825
Iteration 18/1000 | Loss: 0.00004820
Iteration 19/1000 | Loss: 0.00004814
Iteration 20/1000 | Loss: 0.00004814
Iteration 21/1000 | Loss: 0.00004810
Iteration 22/1000 | Loss: 0.00004809
Iteration 23/1000 | Loss: 0.00004809
Iteration 24/1000 | Loss: 0.00004808
Iteration 25/1000 | Loss: 0.00004808
Iteration 26/1000 | Loss: 0.00004808
Iteration 27/1000 | Loss: 0.00004808
Iteration 28/1000 | Loss: 0.00004808
Iteration 29/1000 | Loss: 0.00004807
Iteration 30/1000 | Loss: 0.00004807
Iteration 31/1000 | Loss: 0.00004807
Iteration 32/1000 | Loss: 0.00004806
Iteration 33/1000 | Loss: 0.00004805
Iteration 34/1000 | Loss: 0.00004805
Iteration 35/1000 | Loss: 0.00004804
Iteration 36/1000 | Loss: 0.00004804
Iteration 37/1000 | Loss: 0.00004804
Iteration 38/1000 | Loss: 0.00004803
Iteration 39/1000 | Loss: 0.00004803
Iteration 40/1000 | Loss: 0.00004802
Iteration 41/1000 | Loss: 0.00004802
Iteration 42/1000 | Loss: 0.00004801
Iteration 43/1000 | Loss: 0.00004801
Iteration 44/1000 | Loss: 0.00004801
Iteration 45/1000 | Loss: 0.00004800
Iteration 46/1000 | Loss: 0.00004800
Iteration 47/1000 | Loss: 0.00004800
Iteration 48/1000 | Loss: 0.00004798
Iteration 49/1000 | Loss: 0.00004798
Iteration 50/1000 | Loss: 0.00004798
Iteration 51/1000 | Loss: 0.00004797
Iteration 52/1000 | Loss: 0.00004797
Iteration 53/1000 | Loss: 0.00004796
Iteration 54/1000 | Loss: 0.00004796
Iteration 55/1000 | Loss: 0.00004796
Iteration 56/1000 | Loss: 0.00004793
Iteration 57/1000 | Loss: 0.00004793
Iteration 58/1000 | Loss: 0.00004792
Iteration 59/1000 | Loss: 0.00004791
Iteration 60/1000 | Loss: 0.00004787
Iteration 61/1000 | Loss: 0.00004787
Iteration 62/1000 | Loss: 0.00004786
Iteration 63/1000 | Loss: 0.00004786
Iteration 64/1000 | Loss: 0.00004785
Iteration 65/1000 | Loss: 0.00004783
Iteration 66/1000 | Loss: 0.00004783
Iteration 67/1000 | Loss: 0.00004782
Iteration 68/1000 | Loss: 0.00004782
Iteration 69/1000 | Loss: 0.00004782
Iteration 70/1000 | Loss: 0.00004781
Iteration 71/1000 | Loss: 0.00004781
Iteration 72/1000 | Loss: 0.00004780
Iteration 73/1000 | Loss: 0.00004779
Iteration 74/1000 | Loss: 0.00004779
Iteration 75/1000 | Loss: 0.00004779
Iteration 76/1000 | Loss: 0.00004778
Iteration 77/1000 | Loss: 0.00004778
Iteration 78/1000 | Loss: 0.00004778
Iteration 79/1000 | Loss: 0.00004778
Iteration 80/1000 | Loss: 0.00004777
Iteration 81/1000 | Loss: 0.00004777
Iteration 82/1000 | Loss: 0.00004776
Iteration 83/1000 | Loss: 0.00004776
Iteration 84/1000 | Loss: 0.00004775
Iteration 85/1000 | Loss: 0.00004775
Iteration 86/1000 | Loss: 0.00004775
Iteration 87/1000 | Loss: 0.00004775
Iteration 88/1000 | Loss: 0.00004775
Iteration 89/1000 | Loss: 0.00004775
Iteration 90/1000 | Loss: 0.00004775
Iteration 91/1000 | Loss: 0.00004775
Iteration 92/1000 | Loss: 0.00004775
Iteration 93/1000 | Loss: 0.00004775
Iteration 94/1000 | Loss: 0.00004775
Iteration 95/1000 | Loss: 0.00004775
Iteration 96/1000 | Loss: 0.00004775
Iteration 97/1000 | Loss: 0.00004774
Iteration 98/1000 | Loss: 0.00004774
Iteration 99/1000 | Loss: 0.00004774
Iteration 100/1000 | Loss: 0.00004774
Iteration 101/1000 | Loss: 0.00004773
Iteration 102/1000 | Loss: 0.00004773
Iteration 103/1000 | Loss: 0.00004773
Iteration 104/1000 | Loss: 0.00004773
Iteration 105/1000 | Loss: 0.00004772
Iteration 106/1000 | Loss: 0.00004772
Iteration 107/1000 | Loss: 0.00004772
Iteration 108/1000 | Loss: 0.00004772
Iteration 109/1000 | Loss: 0.00004771
Iteration 110/1000 | Loss: 0.00004771
Iteration 111/1000 | Loss: 0.00004771
Iteration 112/1000 | Loss: 0.00004771
Iteration 113/1000 | Loss: 0.00004771
Iteration 114/1000 | Loss: 0.00004771
Iteration 115/1000 | Loss: 0.00004771
Iteration 116/1000 | Loss: 0.00004771
Iteration 117/1000 | Loss: 0.00004771
Iteration 118/1000 | Loss: 0.00004771
Iteration 119/1000 | Loss: 0.00004771
Iteration 120/1000 | Loss: 0.00004770
Iteration 121/1000 | Loss: 0.00004770
Iteration 122/1000 | Loss: 0.00004770
Iteration 123/1000 | Loss: 0.00004770
Iteration 124/1000 | Loss: 0.00004770
Iteration 125/1000 | Loss: 0.00004770
Iteration 126/1000 | Loss: 0.00004770
Iteration 127/1000 | Loss: 0.00004770
Iteration 128/1000 | Loss: 0.00004770
Iteration 129/1000 | Loss: 0.00004770
Iteration 130/1000 | Loss: 0.00004770
Iteration 131/1000 | Loss: 0.00004770
Iteration 132/1000 | Loss: 0.00004770
Iteration 133/1000 | Loss: 0.00004770
Iteration 134/1000 | Loss: 0.00004770
Iteration 135/1000 | Loss: 0.00004770
Iteration 136/1000 | Loss: 0.00004770
Iteration 137/1000 | Loss: 0.00004770
Iteration 138/1000 | Loss: 0.00004769
Iteration 139/1000 | Loss: 0.00004769
Iteration 140/1000 | Loss: 0.00004769
Iteration 141/1000 | Loss: 0.00004769
Iteration 142/1000 | Loss: 0.00004769
Iteration 143/1000 | Loss: 0.00004769
Iteration 144/1000 | Loss: 0.00004769
Iteration 145/1000 | Loss: 0.00004769
Iteration 146/1000 | Loss: 0.00004769
Iteration 147/1000 | Loss: 0.00004769
Iteration 148/1000 | Loss: 0.00004769
Iteration 149/1000 | Loss: 0.00004769
Iteration 150/1000 | Loss: 0.00004769
Iteration 151/1000 | Loss: 0.00004768
Iteration 152/1000 | Loss: 0.00004768
Iteration 153/1000 | Loss: 0.00004768
Iteration 154/1000 | Loss: 0.00004768
Iteration 155/1000 | Loss: 0.00004768
Iteration 156/1000 | Loss: 0.00004768
Iteration 157/1000 | Loss: 0.00004768
Iteration 158/1000 | Loss: 0.00004768
Iteration 159/1000 | Loss: 0.00004768
Iteration 160/1000 | Loss: 0.00004768
Iteration 161/1000 | Loss: 0.00004768
Iteration 162/1000 | Loss: 0.00004768
Iteration 163/1000 | Loss: 0.00004768
Iteration 164/1000 | Loss: 0.00004768
Iteration 165/1000 | Loss: 0.00004768
Iteration 166/1000 | Loss: 0.00004768
Iteration 167/1000 | Loss: 0.00004767
Iteration 168/1000 | Loss: 0.00004767
Iteration 169/1000 | Loss: 0.00004767
Iteration 170/1000 | Loss: 0.00004767
Iteration 171/1000 | Loss: 0.00004767
Iteration 172/1000 | Loss: 0.00004767
Iteration 173/1000 | Loss: 0.00004767
Iteration 174/1000 | Loss: 0.00004767
Iteration 175/1000 | Loss: 0.00004767
Iteration 176/1000 | Loss: 0.00004767
Iteration 177/1000 | Loss: 0.00004767
Iteration 178/1000 | Loss: 0.00004767
Iteration 179/1000 | Loss: 0.00004767
Iteration 180/1000 | Loss: 0.00004767
Iteration 181/1000 | Loss: 0.00004767
Iteration 182/1000 | Loss: 0.00004767
Iteration 183/1000 | Loss: 0.00004766
Iteration 184/1000 | Loss: 0.00004766
Iteration 185/1000 | Loss: 0.00004766
Iteration 186/1000 | Loss: 0.00004766
Iteration 187/1000 | Loss: 0.00004766
Iteration 188/1000 | Loss: 0.00004766
Iteration 189/1000 | Loss: 0.00004766
Iteration 190/1000 | Loss: 0.00004766
Iteration 191/1000 | Loss: 0.00004766
Iteration 192/1000 | Loss: 0.00004766
Iteration 193/1000 | Loss: 0.00004766
Iteration 194/1000 | Loss: 0.00004766
Iteration 195/1000 | Loss: 0.00004766
Iteration 196/1000 | Loss: 0.00004766
Iteration 197/1000 | Loss: 0.00004766
Iteration 198/1000 | Loss: 0.00004766
Iteration 199/1000 | Loss: 0.00004766
Iteration 200/1000 | Loss: 0.00004766
Iteration 201/1000 | Loss: 0.00004766
Iteration 202/1000 | Loss: 0.00004766
Iteration 203/1000 | Loss: 0.00004766
Iteration 204/1000 | Loss: 0.00004766
Iteration 205/1000 | Loss: 0.00004766
Iteration 206/1000 | Loss: 0.00004766
Iteration 207/1000 | Loss: 0.00004766
Iteration 208/1000 | Loss: 0.00004766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [4.7657016693847254e-05, 4.7657016693847254e-05, 4.7657016693847254e-05, 4.7657016693847254e-05, 4.7657016693847254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7657016693847254e-05

Optimization complete. Final v2v error: 5.826724052429199 mm

Highest mean error: 6.166920185089111 mm for frame 16

Lowest mean error: 5.322048664093018 mm for frame 118

Saving results

Total time: 43.33208227157593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107118
Iteration 2/25 | Loss: 0.01107118
Iteration 3/25 | Loss: 0.00415953
Iteration 4/25 | Loss: 0.00228729
Iteration 5/25 | Loss: 0.00201366
Iteration 6/25 | Loss: 0.00190397
Iteration 7/25 | Loss: 0.00175874
Iteration 8/25 | Loss: 0.00173170
Iteration 9/25 | Loss: 0.00161697
Iteration 10/25 | Loss: 0.00153730
Iteration 11/25 | Loss: 0.00153251
Iteration 12/25 | Loss: 0.00153642
Iteration 13/25 | Loss: 0.00154814
Iteration 14/25 | Loss: 0.00147287
Iteration 15/25 | Loss: 0.00144972
Iteration 16/25 | Loss: 0.00143952
Iteration 17/25 | Loss: 0.00144479
Iteration 18/25 | Loss: 0.00144230
Iteration 19/25 | Loss: 0.00144641
Iteration 20/25 | Loss: 0.00143602
Iteration 21/25 | Loss: 0.00141458
Iteration 22/25 | Loss: 0.00140919
Iteration 23/25 | Loss: 0.00140630
Iteration 24/25 | Loss: 0.00141725
Iteration 25/25 | Loss: 0.00140204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40451336
Iteration 2/25 | Loss: 0.00827638
Iteration 3/25 | Loss: 0.00480098
Iteration 4/25 | Loss: 0.00480097
Iteration 5/25 | Loss: 0.00480097
Iteration 6/25 | Loss: 0.00480097
Iteration 7/25 | Loss: 0.00480097
Iteration 8/25 | Loss: 0.00480097
Iteration 9/25 | Loss: 0.00480097
Iteration 10/25 | Loss: 0.00480097
Iteration 11/25 | Loss: 0.00480097
Iteration 12/25 | Loss: 0.00480097
Iteration 13/25 | Loss: 0.00480097
Iteration 14/25 | Loss: 0.00480097
Iteration 15/25 | Loss: 0.00480097
Iteration 16/25 | Loss: 0.00480097
Iteration 17/25 | Loss: 0.00480097
Iteration 18/25 | Loss: 0.00480097
Iteration 19/25 | Loss: 0.00480097
Iteration 20/25 | Loss: 0.00480097
Iteration 21/25 | Loss: 0.00480097
Iteration 22/25 | Loss: 0.00480097
Iteration 23/25 | Loss: 0.00480097
Iteration 24/25 | Loss: 0.00480097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004800972528755665, 0.004800972528755665, 0.004800972528755665, 0.004800972528755665, 0.004800972528755665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004800972528755665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00480097
Iteration 2/1000 | Loss: 0.00261744
Iteration 3/1000 | Loss: 0.00504295
Iteration 4/1000 | Loss: 0.00334133
Iteration 5/1000 | Loss: 0.00232654
Iteration 6/1000 | Loss: 0.00555180
Iteration 7/1000 | Loss: 0.01228286
Iteration 8/1000 | Loss: 0.00292723
Iteration 9/1000 | Loss: 0.00499849
Iteration 10/1000 | Loss: 0.00185753
Iteration 11/1000 | Loss: 0.00379431
Iteration 12/1000 | Loss: 0.00544552
Iteration 13/1000 | Loss: 0.00246613
Iteration 14/1000 | Loss: 0.00196506
Iteration 15/1000 | Loss: 0.00280906
Iteration 16/1000 | Loss: 0.00579063
Iteration 17/1000 | Loss: 0.00232676
Iteration 18/1000 | Loss: 0.00267609
Iteration 19/1000 | Loss: 0.00329877
Iteration 20/1000 | Loss: 0.00426792
Iteration 21/1000 | Loss: 0.00328232
Iteration 22/1000 | Loss: 0.00278622
Iteration 23/1000 | Loss: 0.00305904
Iteration 24/1000 | Loss: 0.00215864
Iteration 25/1000 | Loss: 0.00344448
Iteration 26/1000 | Loss: 0.00286925
Iteration 27/1000 | Loss: 0.00222115
Iteration 28/1000 | Loss: 0.00158783
Iteration 29/1000 | Loss: 0.00491921
Iteration 30/1000 | Loss: 0.00454514
Iteration 31/1000 | Loss: 0.00952409
Iteration 32/1000 | Loss: 0.00336678
Iteration 33/1000 | Loss: 0.00380953
Iteration 34/1000 | Loss: 0.00123693
Iteration 35/1000 | Loss: 0.00130923
Iteration 36/1000 | Loss: 0.00282579
Iteration 37/1000 | Loss: 0.00102114
Iteration 38/1000 | Loss: 0.00073839
Iteration 39/1000 | Loss: 0.00116702
Iteration 40/1000 | Loss: 0.00233085
Iteration 41/1000 | Loss: 0.00409960
Iteration 42/1000 | Loss: 0.00455511
Iteration 43/1000 | Loss: 0.00448935
Iteration 44/1000 | Loss: 0.00342784
Iteration 45/1000 | Loss: 0.00421311
Iteration 46/1000 | Loss: 0.00289122
Iteration 47/1000 | Loss: 0.00578251
Iteration 48/1000 | Loss: 0.00463200
Iteration 49/1000 | Loss: 0.00272916
Iteration 50/1000 | Loss: 0.00442369
Iteration 51/1000 | Loss: 0.00199507
Iteration 52/1000 | Loss: 0.00239182
Iteration 53/1000 | Loss: 0.00160534
Iteration 54/1000 | Loss: 0.00052034
Iteration 55/1000 | Loss: 0.00203776
Iteration 56/1000 | Loss: 0.00145283
Iteration 57/1000 | Loss: 0.00210529
Iteration 58/1000 | Loss: 0.00177361
Iteration 59/1000 | Loss: 0.00513823
Iteration 60/1000 | Loss: 0.00309867
Iteration 61/1000 | Loss: 0.00589791
Iteration 62/1000 | Loss: 0.00429790
Iteration 63/1000 | Loss: 0.00218276
Iteration 64/1000 | Loss: 0.00105273
Iteration 65/1000 | Loss: 0.00243481
Iteration 66/1000 | Loss: 0.00307691
Iteration 67/1000 | Loss: 0.00216664
Iteration 68/1000 | Loss: 0.00194152
Iteration 69/1000 | Loss: 0.00185696
Iteration 70/1000 | Loss: 0.00202355
Iteration 71/1000 | Loss: 0.00106822
Iteration 72/1000 | Loss: 0.00103962
Iteration 73/1000 | Loss: 0.00041439
Iteration 74/1000 | Loss: 0.00199717
Iteration 75/1000 | Loss: 0.00447265
Iteration 76/1000 | Loss: 0.00127240
Iteration 77/1000 | Loss: 0.00115336
Iteration 78/1000 | Loss: 0.00163927
Iteration 79/1000 | Loss: 0.00147388
Iteration 80/1000 | Loss: 0.00108424
Iteration 81/1000 | Loss: 0.00157873
Iteration 82/1000 | Loss: 0.00085660
Iteration 83/1000 | Loss: 0.00083028
Iteration 84/1000 | Loss: 0.00095074
Iteration 85/1000 | Loss: 0.00034492
Iteration 86/1000 | Loss: 0.00030990
Iteration 87/1000 | Loss: 0.00027040
Iteration 88/1000 | Loss: 0.00188739
Iteration 89/1000 | Loss: 0.00052530
Iteration 90/1000 | Loss: 0.00054659
Iteration 91/1000 | Loss: 0.00115640
Iteration 92/1000 | Loss: 0.00072105
Iteration 93/1000 | Loss: 0.00039217
Iteration 94/1000 | Loss: 0.00022331
Iteration 95/1000 | Loss: 0.00096903
Iteration 96/1000 | Loss: 0.00017110
Iteration 97/1000 | Loss: 0.00124315
Iteration 98/1000 | Loss: 0.00148522
Iteration 99/1000 | Loss: 0.00117848
Iteration 100/1000 | Loss: 0.00125871
Iteration 101/1000 | Loss: 0.00135333
Iteration 102/1000 | Loss: 0.00103773
Iteration 103/1000 | Loss: 0.00044728
Iteration 104/1000 | Loss: 0.00018224
Iteration 105/1000 | Loss: 0.00016742
Iteration 106/1000 | Loss: 0.00050995
Iteration 107/1000 | Loss: 0.00015570
Iteration 108/1000 | Loss: 0.00068245
Iteration 109/1000 | Loss: 0.00015988
Iteration 110/1000 | Loss: 0.00014461
Iteration 111/1000 | Loss: 0.00071173
Iteration 112/1000 | Loss: 0.00108895
Iteration 113/1000 | Loss: 0.00025657
Iteration 114/1000 | Loss: 0.00118832
Iteration 115/1000 | Loss: 0.00031798
Iteration 116/1000 | Loss: 0.00023983
Iteration 117/1000 | Loss: 0.00032537
Iteration 118/1000 | Loss: 0.00085718
Iteration 119/1000 | Loss: 0.00013331
Iteration 120/1000 | Loss: 0.00013107
Iteration 121/1000 | Loss: 0.00021255
Iteration 122/1000 | Loss: 0.00014569
Iteration 123/1000 | Loss: 0.00021967
Iteration 124/1000 | Loss: 0.00014293
Iteration 125/1000 | Loss: 0.00011090
Iteration 126/1000 | Loss: 0.00022119
Iteration 127/1000 | Loss: 0.00015965
Iteration 128/1000 | Loss: 0.00017626
Iteration 129/1000 | Loss: 0.00013245
Iteration 130/1000 | Loss: 0.00014369
Iteration 131/1000 | Loss: 0.00054179
Iteration 132/1000 | Loss: 0.00015109
Iteration 133/1000 | Loss: 0.00014422
Iteration 134/1000 | Loss: 0.00049429
Iteration 135/1000 | Loss: 0.00039051
Iteration 136/1000 | Loss: 0.00017865
Iteration 137/1000 | Loss: 0.00011778
Iteration 138/1000 | Loss: 0.00017482
Iteration 139/1000 | Loss: 0.00012546
Iteration 140/1000 | Loss: 0.00010602
Iteration 141/1000 | Loss: 0.00037536
Iteration 142/1000 | Loss: 0.00022718
Iteration 143/1000 | Loss: 0.00014274
Iteration 144/1000 | Loss: 0.00011301
Iteration 145/1000 | Loss: 0.00014662
Iteration 146/1000 | Loss: 0.00013066
Iteration 147/1000 | Loss: 0.00011485
Iteration 148/1000 | Loss: 0.00010680
Iteration 149/1000 | Loss: 0.00012078
Iteration 150/1000 | Loss: 0.00012267
Iteration 151/1000 | Loss: 0.00010480
Iteration 152/1000 | Loss: 0.00018205
Iteration 153/1000 | Loss: 0.00013321
Iteration 154/1000 | Loss: 0.00154138
Iteration 155/1000 | Loss: 0.00102931
Iteration 156/1000 | Loss: 0.00036617
Iteration 157/1000 | Loss: 0.00061270
Iteration 158/1000 | Loss: 0.00012351
Iteration 159/1000 | Loss: 0.00086938
Iteration 160/1000 | Loss: 0.00048835
Iteration 161/1000 | Loss: 0.00012167
Iteration 162/1000 | Loss: 0.00013633
Iteration 163/1000 | Loss: 0.00014402
Iteration 164/1000 | Loss: 0.00012234
Iteration 165/1000 | Loss: 0.00011352
Iteration 166/1000 | Loss: 0.00010710
Iteration 167/1000 | Loss: 0.00020683
Iteration 168/1000 | Loss: 0.00011493
Iteration 169/1000 | Loss: 0.00012784
Iteration 170/1000 | Loss: 0.00014470
Iteration 171/1000 | Loss: 0.00024584
Iteration 172/1000 | Loss: 0.00018084
Iteration 173/1000 | Loss: 0.00039819
Iteration 174/1000 | Loss: 0.00018282
Iteration 175/1000 | Loss: 0.00015362
Iteration 176/1000 | Loss: 0.00040071
Iteration 177/1000 | Loss: 0.00011975
Iteration 178/1000 | Loss: 0.00011730
Iteration 179/1000 | Loss: 0.00010944
Iteration 180/1000 | Loss: 0.00014664
Iteration 181/1000 | Loss: 0.00010869
Iteration 182/1000 | Loss: 0.00010324
Iteration 183/1000 | Loss: 0.00015253
Iteration 184/1000 | Loss: 0.00012928
Iteration 185/1000 | Loss: 0.00010039
Iteration 186/1000 | Loss: 0.00020905
Iteration 187/1000 | Loss: 0.00030803
Iteration 188/1000 | Loss: 0.00016906
Iteration 189/1000 | Loss: 0.00012540
Iteration 190/1000 | Loss: 0.00010173
Iteration 191/1000 | Loss: 0.00010652
Iteration 192/1000 | Loss: 0.00013215
Iteration 193/1000 | Loss: 0.00011092
Iteration 194/1000 | Loss: 0.00013723
Iteration 195/1000 | Loss: 0.00013047
Iteration 196/1000 | Loss: 0.00010514
Iteration 197/1000 | Loss: 0.00010197
Iteration 198/1000 | Loss: 0.00010353
Iteration 199/1000 | Loss: 0.00013738
Iteration 200/1000 | Loss: 0.00034204
Iteration 201/1000 | Loss: 0.00020488
Iteration 202/1000 | Loss: 0.00015291
Iteration 203/1000 | Loss: 0.00011866
Iteration 204/1000 | Loss: 0.00011114
Iteration 205/1000 | Loss: 0.00011370
Iteration 206/1000 | Loss: 0.00013369
Iteration 207/1000 | Loss: 0.00012166
Iteration 208/1000 | Loss: 0.00011529
Iteration 209/1000 | Loss: 0.00010769
Iteration 210/1000 | Loss: 0.00011477
Iteration 211/1000 | Loss: 0.00029553
Iteration 212/1000 | Loss: 0.00021260
Iteration 213/1000 | Loss: 0.00011609
Iteration 214/1000 | Loss: 0.00010602
Iteration 215/1000 | Loss: 0.00011263
Iteration 216/1000 | Loss: 0.00011838
Iteration 217/1000 | Loss: 0.00012532
Iteration 218/1000 | Loss: 0.00019305
Iteration 219/1000 | Loss: 0.00012936
Iteration 220/1000 | Loss: 0.00012418
Iteration 221/1000 | Loss: 0.00023280
Iteration 222/1000 | Loss: 0.00010523
Iteration 223/1000 | Loss: 0.00009943
Iteration 224/1000 | Loss: 0.00009687
Iteration 225/1000 | Loss: 0.00009698
Iteration 226/1000 | Loss: 0.00012031
Iteration 227/1000 | Loss: 0.00009677
Iteration 228/1000 | Loss: 0.00016064
Iteration 229/1000 | Loss: 0.00011320
Iteration 230/1000 | Loss: 0.00013424
Iteration 231/1000 | Loss: 0.00014017
Iteration 232/1000 | Loss: 0.00010403
Iteration 233/1000 | Loss: 0.00013697
Iteration 234/1000 | Loss: 0.00009792
Iteration 235/1000 | Loss: 0.00010100
Iteration 236/1000 | Loss: 0.00009605
Iteration 237/1000 | Loss: 0.00009668
Iteration 238/1000 | Loss: 0.00009996
Iteration 239/1000 | Loss: 0.00010879
Iteration 240/1000 | Loss: 0.00009637
Iteration 241/1000 | Loss: 0.00011168
Iteration 242/1000 | Loss: 0.00012931
Iteration 243/1000 | Loss: 0.00011015
Iteration 244/1000 | Loss: 0.00009805
Iteration 245/1000 | Loss: 0.00012599
Iteration 246/1000 | Loss: 0.00010132
Iteration 247/1000 | Loss: 0.00009863
Iteration 248/1000 | Loss: 0.00009615
Iteration 249/1000 | Loss: 0.00013770
Iteration 250/1000 | Loss: 0.00010979
Iteration 251/1000 | Loss: 0.00009845
Iteration 252/1000 | Loss: 0.00014042
Iteration 253/1000 | Loss: 0.00010268
Iteration 254/1000 | Loss: 0.00009807
Iteration 255/1000 | Loss: 0.00012647
Iteration 256/1000 | Loss: 0.00010799
Iteration 257/1000 | Loss: 0.00009903
Iteration 258/1000 | Loss: 0.00009581
Iteration 259/1000 | Loss: 0.00013919
Iteration 260/1000 | Loss: 0.00010428
Iteration 261/1000 | Loss: 0.00010514
Iteration 262/1000 | Loss: 0.00009809
Iteration 263/1000 | Loss: 0.00012043
Iteration 264/1000 | Loss: 0.00010434
Iteration 265/1000 | Loss: 0.00017830
Iteration 266/1000 | Loss: 0.00011766
Iteration 267/1000 | Loss: 0.00013119
Iteration 268/1000 | Loss: 0.00010246
Iteration 269/1000 | Loss: 0.00022624
Iteration 270/1000 | Loss: 0.00012495
Iteration 271/1000 | Loss: 0.00010072
Iteration 272/1000 | Loss: 0.00009766
Iteration 273/1000 | Loss: 0.00009831
Iteration 274/1000 | Loss: 0.00009813
Iteration 275/1000 | Loss: 0.00010358
Iteration 276/1000 | Loss: 0.00009808
Iteration 277/1000 | Loss: 0.00010290
Iteration 278/1000 | Loss: 0.00014691
Iteration 279/1000 | Loss: 0.00010209
Iteration 280/1000 | Loss: 0.00009614
Iteration 281/1000 | Loss: 0.00011201
Iteration 282/1000 | Loss: 0.00010194
Iteration 283/1000 | Loss: 0.00013263
Iteration 284/1000 | Loss: 0.00012422
Iteration 285/1000 | Loss: 0.00010655
Iteration 286/1000 | Loss: 0.00011234
Iteration 287/1000 | Loss: 0.00010149
Iteration 288/1000 | Loss: 0.00009605
Iteration 289/1000 | Loss: 0.00012252
Iteration 290/1000 | Loss: 0.00009843
Iteration 291/1000 | Loss: 0.00010835
Iteration 292/1000 | Loss: 0.00009701
Iteration 293/1000 | Loss: 0.00013170
Iteration 294/1000 | Loss: 0.00010485
Iteration 295/1000 | Loss: 0.00014127
Iteration 296/1000 | Loss: 0.00010394
Iteration 297/1000 | Loss: 0.00010644
Iteration 298/1000 | Loss: 0.00009657
Iteration 299/1000 | Loss: 0.00013536
Iteration 300/1000 | Loss: 0.00009945
Iteration 301/1000 | Loss: 0.00010223
Iteration 302/1000 | Loss: 0.00010003
Iteration 303/1000 | Loss: 0.00011254
Iteration 304/1000 | Loss: 0.00010744
Iteration 305/1000 | Loss: 0.00010046
Iteration 306/1000 | Loss: 0.00009823
Iteration 307/1000 | Loss: 0.00010409
Iteration 308/1000 | Loss: 0.00010668
Iteration 309/1000 | Loss: 0.00011038
Iteration 310/1000 | Loss: 0.00009737
Iteration 311/1000 | Loss: 0.00010634
Iteration 312/1000 | Loss: 0.00010529
Iteration 313/1000 | Loss: 0.00009804
Iteration 314/1000 | Loss: 0.00010758
Iteration 315/1000 | Loss: 0.00010230
Iteration 316/1000 | Loss: 0.00010100
Iteration 317/1000 | Loss: 0.00010534
Iteration 318/1000 | Loss: 0.00010297
Iteration 319/1000 | Loss: 0.00010998
Iteration 320/1000 | Loss: 0.00012008
Iteration 321/1000 | Loss: 0.00009665
Iteration 322/1000 | Loss: 0.00010276
Iteration 323/1000 | Loss: 0.00010367
Iteration 324/1000 | Loss: 0.00009966
Iteration 325/1000 | Loss: 0.00010570
Iteration 326/1000 | Loss: 0.00009631
Iteration 327/1000 | Loss: 0.00015253
Iteration 328/1000 | Loss: 0.00012861
Iteration 329/1000 | Loss: 0.00009924
Iteration 330/1000 | Loss: 0.00011291
Iteration 331/1000 | Loss: 0.00010103
Iteration 332/1000 | Loss: 0.00010263
Iteration 333/1000 | Loss: 0.00010034
Iteration 334/1000 | Loss: 0.00013961
Iteration 335/1000 | Loss: 0.00011948
Iteration 336/1000 | Loss: 0.00010286
Iteration 337/1000 | Loss: 0.00009680
Iteration 338/1000 | Loss: 0.00009712
Iteration 339/1000 | Loss: 0.00009644
Iteration 340/1000 | Loss: 0.00012811
Iteration 341/1000 | Loss: 0.00010029
Iteration 342/1000 | Loss: 0.00012656
Iteration 343/1000 | Loss: 0.00010249
Iteration 344/1000 | Loss: 0.00009961
Iteration 345/1000 | Loss: 0.00009782
Iteration 346/1000 | Loss: 0.00009595
Iteration 347/1000 | Loss: 0.00014063
Iteration 348/1000 | Loss: 0.00010005
Iteration 349/1000 | Loss: 0.00010363
Iteration 350/1000 | Loss: 0.00011077
Iteration 351/1000 | Loss: 0.00010055
Iteration 352/1000 | Loss: 0.00012560
Iteration 353/1000 | Loss: 0.00010605
Iteration 354/1000 | Loss: 0.00011777
Iteration 355/1000 | Loss: 0.00012298
Iteration 356/1000 | Loss: 0.00009919
Iteration 357/1000 | Loss: 0.00009718
Iteration 358/1000 | Loss: 0.00010738
Iteration 359/1000 | Loss: 0.00011897
Iteration 360/1000 | Loss: 0.00010161
Iteration 361/1000 | Loss: 0.00010764
Iteration 362/1000 | Loss: 0.00009670
Iteration 363/1000 | Loss: 0.00011954
Iteration 364/1000 | Loss: 0.00009759
Iteration 365/1000 | Loss: 0.00011708
Iteration 366/1000 | Loss: 0.00009648
Iteration 367/1000 | Loss: 0.00009601
Iteration 368/1000 | Loss: 0.00010144
Iteration 369/1000 | Loss: 0.00010542
Iteration 370/1000 | Loss: 0.00009643
Iteration 371/1000 | Loss: 0.00009796
Iteration 372/1000 | Loss: 0.00009688
Iteration 373/1000 | Loss: 0.00010640
Iteration 374/1000 | Loss: 0.00011495
Iteration 375/1000 | Loss: 0.00009782
Iteration 376/1000 | Loss: 0.00014636
Iteration 377/1000 | Loss: 0.00010638
Iteration 378/1000 | Loss: 0.00010257
Iteration 379/1000 | Loss: 0.00009606
Iteration 380/1000 | Loss: 0.00011163
Iteration 381/1000 | Loss: 0.00010001
Iteration 382/1000 | Loss: 0.00011196
Iteration 383/1000 | Loss: 0.00009906
Iteration 384/1000 | Loss: 0.00011449
Iteration 385/1000 | Loss: 0.00010228
Iteration 386/1000 | Loss: 0.00012923
Iteration 387/1000 | Loss: 0.00010116
Iteration 388/1000 | Loss: 0.00009760
Iteration 389/1000 | Loss: 0.00009617
Iteration 390/1000 | Loss: 0.00013658
Iteration 391/1000 | Loss: 0.00010911
Iteration 392/1000 | Loss: 0.00009599
Iteration 393/1000 | Loss: 0.00010945
Iteration 394/1000 | Loss: 0.00010245
Iteration 395/1000 | Loss: 0.00010112
Iteration 396/1000 | Loss: 0.00009839
Iteration 397/1000 | Loss: 0.00011733
Iteration 398/1000 | Loss: 0.00028048
Iteration 399/1000 | Loss: 0.00010461
Iteration 400/1000 | Loss: 0.00009631
Iteration 401/1000 | Loss: 0.00010504
Iteration 402/1000 | Loss: 0.00009658
Iteration 403/1000 | Loss: 0.00010291
Iteration 404/1000 | Loss: 0.00009596
Iteration 405/1000 | Loss: 0.00012054
Iteration 406/1000 | Loss: 0.00009896
Iteration 407/1000 | Loss: 0.00009574
Iteration 408/1000 | Loss: 0.00011525
Iteration 409/1000 | Loss: 0.00009720
Iteration 410/1000 | Loss: 0.00013513
Iteration 411/1000 | Loss: 0.00010034
Iteration 412/1000 | Loss: 0.00014988
Iteration 413/1000 | Loss: 0.00009976
Iteration 414/1000 | Loss: 0.00011032
Iteration 415/1000 | Loss: 0.00009824
Iteration 416/1000 | Loss: 0.00009741
Iteration 417/1000 | Loss: 0.00009703
Iteration 418/1000 | Loss: 0.00010124
Iteration 419/1000 | Loss: 0.00009628
Iteration 420/1000 | Loss: 0.00009572
Iteration 421/1000 | Loss: 0.00009571
Iteration 422/1000 | Loss: 0.00009571
Iteration 423/1000 | Loss: 0.00012726
Iteration 424/1000 | Loss: 0.00010976
Iteration 425/1000 | Loss: 0.00009624
Iteration 426/1000 | Loss: 0.00009744
Iteration 427/1000 | Loss: 0.00013021
Iteration 428/1000 | Loss: 0.00019911
Iteration 429/1000 | Loss: 0.00010124
Iteration 430/1000 | Loss: 0.00010561
Iteration 431/1000 | Loss: 0.00009854
Iteration 432/1000 | Loss: 0.00010835
Iteration 433/1000 | Loss: 0.00012486
Iteration 434/1000 | Loss: 0.00010454
Iteration 435/1000 | Loss: 0.00011187
Iteration 436/1000 | Loss: 0.00012720
Iteration 437/1000 | Loss: 0.00009955
Iteration 438/1000 | Loss: 0.00009549
Iteration 439/1000 | Loss: 0.00012901
Iteration 440/1000 | Loss: 0.00011006
Iteration 441/1000 | Loss: 0.00010743
Iteration 442/1000 | Loss: 0.00009879
Iteration 443/1000 | Loss: 0.00012759
Iteration 444/1000 | Loss: 0.00009641
Iteration 445/1000 | Loss: 0.00012270
Iteration 446/1000 | Loss: 0.00009966
Iteration 447/1000 | Loss: 0.00009716
Iteration 448/1000 | Loss: 0.00009639
Iteration 449/1000 | Loss: 0.00014687
Iteration 450/1000 | Loss: 0.00009987
Iteration 451/1000 | Loss: 0.00011336
Iteration 452/1000 | Loss: 0.00009854
Iteration 453/1000 | Loss: 0.00014422
Iteration 454/1000 | Loss: 0.00010509
Iteration 455/1000 | Loss: 0.00011720
Iteration 456/1000 | Loss: 0.00009794
Iteration 457/1000 | Loss: 0.00009979
Iteration 458/1000 | Loss: 0.00009592
Iteration 459/1000 | Loss: 0.00009750
Iteration 460/1000 | Loss: 0.00009572
Iteration 461/1000 | Loss: 0.00013551
Iteration 462/1000 | Loss: 0.00011180
Iteration 463/1000 | Loss: 0.00010591
Iteration 464/1000 | Loss: 0.00009653
Iteration 465/1000 | Loss: 0.00010493
Iteration 466/1000 | Loss: 0.00010139
Iteration 467/1000 | Loss: 0.00010493
Iteration 468/1000 | Loss: 0.00010034
Iteration 469/1000 | Loss: 0.00009878
Iteration 470/1000 | Loss: 0.00010654
Iteration 471/1000 | Loss: 0.00010024
Iteration 472/1000 | Loss: 0.00010244
Iteration 473/1000 | Loss: 0.00010007
Iteration 474/1000 | Loss: 0.00011944
Iteration 475/1000 | Loss: 0.00009894
Iteration 476/1000 | Loss: 0.00009680
Iteration 477/1000 | Loss: 0.00010445
Iteration 478/1000 | Loss: 0.00010940
Iteration 479/1000 | Loss: 0.00010403
Iteration 480/1000 | Loss: 0.00010693
Iteration 481/1000 | Loss: 0.00011260
Iteration 482/1000 | Loss: 0.00011096
Iteration 483/1000 | Loss: 0.00009763
Iteration 484/1000 | Loss: 0.00009958
Iteration 485/1000 | Loss: 0.00013694
Iteration 486/1000 | Loss: 0.00011161
Iteration 487/1000 | Loss: 0.00010319
Iteration 488/1000 | Loss: 0.00009931
Iteration 489/1000 | Loss: 0.00009529
Iteration 490/1000 | Loss: 0.00010161
Iteration 491/1000 | Loss: 0.00010467
Iteration 492/1000 | Loss: 0.00009579
Iteration 493/1000 | Loss: 0.00010368
Iteration 494/1000 | Loss: 0.00011719
Iteration 495/1000 | Loss: 0.00014533
Iteration 496/1000 | Loss: 0.00009905
Iteration 497/1000 | Loss: 0.00009566
Iteration 498/1000 | Loss: 0.00009915
Iteration 499/1000 | Loss: 0.00009694
Iteration 500/1000 | Loss: 0.00009774
Iteration 501/1000 | Loss: 0.00014089
Iteration 502/1000 | Loss: 0.00009683
Iteration 503/1000 | Loss: 0.00012306
Iteration 504/1000 | Loss: 0.00009743
Iteration 505/1000 | Loss: 0.00010258
Iteration 506/1000 | Loss: 0.00009926
Iteration 507/1000 | Loss: 0.00009882
Iteration 508/1000 | Loss: 0.00010340
Iteration 509/1000 | Loss: 0.00010226
Iteration 510/1000 | Loss: 0.00009630
Iteration 511/1000 | Loss: 0.00010141
Iteration 512/1000 | Loss: 0.00012076
Iteration 513/1000 | Loss: 0.00012505
Iteration 514/1000 | Loss: 0.00009682
Iteration 515/1000 | Loss: 0.00009740
Iteration 516/1000 | Loss: 0.00009548
Iteration 517/1000 | Loss: 0.00009547
Iteration 518/1000 | Loss: 0.00009536
Iteration 519/1000 | Loss: 0.00009533
Iteration 520/1000 | Loss: 0.00011314
Iteration 521/1000 | Loss: 0.00009774
Iteration 522/1000 | Loss: 0.00011884
Iteration 523/1000 | Loss: 0.00009939
Iteration 524/1000 | Loss: 0.00009537
Iteration 525/1000 | Loss: 0.00012422
Iteration 526/1000 | Loss: 0.00009680
Iteration 527/1000 | Loss: 0.00011591
Iteration 528/1000 | Loss: 0.00009941
Iteration 529/1000 | Loss: 0.00009544
Iteration 530/1000 | Loss: 0.00012632
Iteration 531/1000 | Loss: 0.00010058
Iteration 532/1000 | Loss: 0.00011383
Iteration 533/1000 | Loss: 0.00011256
Iteration 534/1000 | Loss: 0.00009612
Iteration 535/1000 | Loss: 0.00014792
Iteration 536/1000 | Loss: 0.00012086
Iteration 537/1000 | Loss: 0.00010984
Iteration 538/1000 | Loss: 0.00010290
Iteration 539/1000 | Loss: 0.00010298
Iteration 540/1000 | Loss: 0.00009526
Iteration 541/1000 | Loss: 0.00010786
Iteration 542/1000 | Loss: 0.00013735
Iteration 543/1000 | Loss: 0.00010233
Iteration 544/1000 | Loss: 0.00009572
Iteration 545/1000 | Loss: 0.00016140
Iteration 546/1000 | Loss: 0.00010323
Iteration 547/1000 | Loss: 0.00013849
Iteration 548/1000 | Loss: 0.00009910
Iteration 549/1000 | Loss: 0.00009700
Iteration 550/1000 | Loss: 0.00010707
Iteration 551/1000 | Loss: 0.00009777
Iteration 552/1000 | Loss: 0.00010116
Iteration 553/1000 | Loss: 0.00009677
Iteration 554/1000 | Loss: 0.00010324
Iteration 555/1000 | Loss: 0.00009619
Iteration 556/1000 | Loss: 0.00012425
Iteration 557/1000 | Loss: 0.00010491
Iteration 558/1000 | Loss: 0.00010475
Iteration 559/1000 | Loss: 0.00009746
Iteration 560/1000 | Loss: 0.00022332
Iteration 561/1000 | Loss: 0.00011435
Iteration 562/1000 | Loss: 0.00009553
Iteration 563/1000 | Loss: 0.00013064
Iteration 564/1000 | Loss: 0.00010272
Iteration 565/1000 | Loss: 0.00010037
Iteration 566/1000 | Loss: 0.00013764
Iteration 567/1000 | Loss: 0.00010431
Iteration 568/1000 | Loss: 0.00009706
Iteration 569/1000 | Loss: 0.00010155
Iteration 570/1000 | Loss: 0.00009579
Iteration 571/1000 | Loss: 0.00014286
Iteration 572/1000 | Loss: 0.00011258
Iteration 573/1000 | Loss: 0.00012309
Iteration 574/1000 | Loss: 0.00011295
Iteration 575/1000 | Loss: 0.00009891
Iteration 576/1000 | Loss: 0.00009536
Iteration 577/1000 | Loss: 0.00009532
Iteration 578/1000 | Loss: 0.00009532
Iteration 579/1000 | Loss: 0.00012737
Iteration 580/1000 | Loss: 0.00009694
Iteration 581/1000 | Loss: 0.00012103
Iteration 582/1000 | Loss: 0.00010387
Iteration 583/1000 | Loss: 0.00009912
Iteration 584/1000 | Loss: 0.00009542
Iteration 585/1000 | Loss: 0.00011601
Iteration 586/1000 | Loss: 0.00009621
Iteration 587/1000 | Loss: 0.00012674
Iteration 588/1000 | Loss: 0.00009621
Iteration 589/1000 | Loss: 0.00009611
Iteration 590/1000 | Loss: 0.00012213
Iteration 591/1000 | Loss: 0.00009669
Iteration 592/1000 | Loss: 0.00009680
Iteration 593/1000 | Loss: 0.00011188
Iteration 594/1000 | Loss: 0.00009682
Iteration 595/1000 | Loss: 0.00009617
Iteration 596/1000 | Loss: 0.00010518
Iteration 597/1000 | Loss: 0.00010479
Iteration 598/1000 | Loss: 0.00009642
Iteration 599/1000 | Loss: 0.00009800
Iteration 600/1000 | Loss: 0.00009851
Iteration 601/1000 | Loss: 0.00009842
Iteration 602/1000 | Loss: 0.00012786
Iteration 603/1000 | Loss: 0.00010867
Iteration 604/1000 | Loss: 0.00011361
Iteration 605/1000 | Loss: 0.00009931
Iteration 606/1000 | Loss: 0.00009582
Iteration 607/1000 | Loss: 0.00009851
Iteration 608/1000 | Loss: 0.00009557
Iteration 609/1000 | Loss: 0.00015149
Iteration 610/1000 | Loss: 0.00019919
Iteration 611/1000 | Loss: 0.00010146
Iteration 612/1000 | Loss: 0.00009685
Iteration 613/1000 | Loss: 0.00011831
Iteration 614/1000 | Loss: 0.00009592
Iteration 615/1000 | Loss: 0.00010717
Iteration 616/1000 | Loss: 0.00009600
Iteration 617/1000 | Loss: 0.00010892
Iteration 618/1000 | Loss: 0.00009584
Iteration 619/1000 | Loss: 0.00011019
Iteration 620/1000 | Loss: 0.00009761
Iteration 621/1000 | Loss: 0.00010168
Iteration 622/1000 | Loss: 0.00010974
Iteration 623/1000 | Loss: 0.00009969
Iteration 624/1000 | Loss: 0.00009952
Iteration 625/1000 | Loss: 0.00009648
Iteration 626/1000 | Loss: 0.00010415
Iteration 627/1000 | Loss: 0.00010751
Iteration 628/1000 | Loss: 0.00011535
Iteration 629/1000 | Loss: 0.00011405
Iteration 630/1000 | Loss: 0.00009814
Iteration 631/1000 | Loss: 0.00011556
Iteration 632/1000 | Loss: 0.00010048
Iteration 633/1000 | Loss: 0.00009556
Iteration 634/1000 | Loss: 0.00009614
Iteration 635/1000 | Loss: 0.00009548
Iteration 636/1000 | Loss: 0.00009544
Iteration 637/1000 | Loss: 0.00011017
Iteration 638/1000 | Loss: 0.00011448
Iteration 639/1000 | Loss: 0.00009743
Iteration 640/1000 | Loss: 0.00012221
Iteration 641/1000 | Loss: 0.00023311
Iteration 642/1000 | Loss: 0.00011161
Iteration 643/1000 | Loss: 0.00010344
Iteration 644/1000 | Loss: 0.00012860
Iteration 645/1000 | Loss: 0.00010390
Iteration 646/1000 | Loss: 0.00009542
Iteration 647/1000 | Loss: 0.00010432
Iteration 648/1000 | Loss: 0.00009563
Iteration 649/1000 | Loss: 0.00010846
Iteration 650/1000 | Loss: 0.00010845
Iteration 651/1000 | Loss: 0.00009535
Iteration 652/1000 | Loss: 0.00009533
Iteration 653/1000 | Loss: 0.00017618
Iteration 654/1000 | Loss: 0.00010323
Iteration 655/1000 | Loss: 0.00009609
Iteration 656/1000 | Loss: 0.00015120
Iteration 657/1000 | Loss: 0.00009700
Iteration 658/1000 | Loss: 0.00009809
Iteration 659/1000 | Loss: 0.00016749
Iteration 660/1000 | Loss: 0.00011496
Iteration 661/1000 | Loss: 0.00011444
Iteration 662/1000 | Loss: 0.00009894
Iteration 663/1000 | Loss: 0.00009764
Iteration 664/1000 | Loss: 0.00012674
Iteration 665/1000 | Loss: 0.00011083
Iteration 666/1000 | Loss: 0.00011519
Iteration 667/1000 | Loss: 0.00009942
Iteration 668/1000 | Loss: 0.00013813
Iteration 669/1000 | Loss: 0.00010488
Iteration 670/1000 | Loss: 0.00009573
Iteration 671/1000 | Loss: 0.00009579
Iteration 672/1000 | Loss: 0.00010714
Iteration 673/1000 | Loss: 0.00010393
Iteration 674/1000 | Loss: 0.00012483
Iteration 675/1000 | Loss: 0.00010884
Iteration 676/1000 | Loss: 0.00011563
Iteration 677/1000 | Loss: 0.00010996
Iteration 678/1000 | Loss: 0.00011526
Iteration 679/1000 | Loss: 0.00011167
Iteration 680/1000 | Loss: 0.00014267
Iteration 681/1000 | Loss: 0.00012329
Iteration 682/1000 | Loss: 0.00009551
Iteration 683/1000 | Loss: 0.00010090
Iteration 684/1000 | Loss: 0.00009617
Iteration 685/1000 | Loss: 0.00011943
Iteration 686/1000 | Loss: 0.00010196
Iteration 687/1000 | Loss: 0.00009544
Iteration 688/1000 | Loss: 0.00010186
Iteration 689/1000 | Loss: 0.00009541
Iteration 690/1000 | Loss: 0.00011382
Iteration 691/1000 | Loss: 0.00011382
Iteration 692/1000 | Loss: 0.00022894
Iteration 693/1000 | Loss: 0.00011781
Iteration 694/1000 | Loss: 0.00009984
Iteration 695/1000 | Loss: 0.00009671
Iteration 696/1000 | Loss: 0.00009550
Iteration 697/1000 | Loss: 0.00011702
Iteration 698/1000 | Loss: 0.00010002
Iteration 699/1000 | Loss: 0.00011669
Iteration 700/1000 | Loss: 0.00010085
Iteration 701/1000 | Loss: 0.00014443
Iteration 702/1000 | Loss: 0.00010625
Iteration 703/1000 | Loss: 0.00010152
Iteration 704/1000 | Loss: 0.00010259
Iteration 705/1000 | Loss: 0.00028694
Iteration 706/1000 | Loss: 0.00012402
Iteration 707/1000 | Loss: 0.00009665
Iteration 708/1000 | Loss: 0.00009718
Iteration 709/1000 | Loss: 0.00009753
Iteration 710/1000 | Loss: 0.00010878
Iteration 711/1000 | Loss: 0.00009645
Iteration 712/1000 | Loss: 0.00009553
Iteration 713/1000 | Loss: 0.00010315
Iteration 714/1000 | Loss: 0.00009576
Iteration 715/1000 | Loss: 0.00009534
Iteration 716/1000 | Loss: 0.00014137
Iteration 717/1000 | Loss: 0.00010009
Iteration 718/1000 | Loss: 0.00010510
Iteration 719/1000 | Loss: 0.00009607
Iteration 720/1000 | Loss: 0.00012106
Iteration 721/1000 | Loss: 0.00009929
Iteration 722/1000 | Loss: 0.00009964
Iteration 723/1000 | Loss: 0.00011412
Iteration 724/1000 | Loss: 0.00009899
Iteration 725/1000 | Loss: 0.00010096
Iteration 726/1000 | Loss: 0.00017135
Iteration 727/1000 | Loss: 0.00017641
Iteration 728/1000 | Loss: 0.00010567
Iteration 729/1000 | Loss: 0.00009614
Iteration 730/1000 | Loss: 0.00013358
Iteration 731/1000 | Loss: 0.00010610
Iteration 732/1000 | Loss: 0.00011751
Iteration 733/1000 | Loss: 0.00022380
Iteration 734/1000 | Loss: 0.00013347
Iteration 735/1000 | Loss: 0.00010351
Iteration 736/1000 | Loss: 0.00009613
Iteration 737/1000 | Loss: 0.00012060
Iteration 738/1000 | Loss: 0.00009811
Iteration 739/1000 | Loss: 0.00011409
Iteration 740/1000 | Loss: 0.00009747
Iteration 741/1000 | Loss: 0.00010446
Iteration 742/1000 | Loss: 0.00009601
Iteration 743/1000 | Loss: 0.00009808
Iteration 744/1000 | Loss: 0.00009562
Iteration 745/1000 | Loss: 0.00009540
Iteration 746/1000 | Loss: 0.00011483
Iteration 747/1000 | Loss: 0.00009600
Iteration 748/1000 | Loss: 0.00010099
Iteration 749/1000 | Loss: 0.00009558
Iteration 750/1000 | Loss: 0.00011803
Iteration 751/1000 | Loss: 0.00010032
Iteration 752/1000 | Loss: 0.00009656
Iteration 753/1000 | Loss: 0.00010379
Iteration 754/1000 | Loss: 0.00009542
Iteration 755/1000 | Loss: 0.00010853
Iteration 756/1000 | Loss: 0.00010641
Iteration 757/1000 | Loss: 0.00010608
Iteration 758/1000 | Loss: 0.00009674
Iteration 759/1000 | Loss: 0.00010282
Iteration 760/1000 | Loss: 0.00009548
Iteration 761/1000 | Loss: 0.00009536
Iteration 762/1000 | Loss: 0.00012828
Iteration 763/1000 | Loss: 0.00009658
Iteration 764/1000 | Loss: 0.00009921
Iteration 765/1000 | Loss: 0.00009557
Iteration 766/1000 | Loss: 0.00012111
Iteration 767/1000 | Loss: 0.00009856
Iteration 768/1000 | Loss: 0.00009616
Iteration 769/1000 | Loss: 0.00010330
Iteration 770/1000 | Loss: 0.00009982
Iteration 771/1000 | Loss: 0.00011971
Iteration 772/1000 | Loss: 0.00009563
Iteration 773/1000 | Loss: 0.00010538
Iteration 774/1000 | Loss: 0.00009558
Iteration 775/1000 | Loss: 0.00012402
Iteration 776/1000 | Loss: 0.00017868
Iteration 777/1000 | Loss: 0.00060531
Iteration 778/1000 | Loss: 0.00010692
Iteration 779/1000 | Loss: 0.00009568
Iteration 780/1000 | Loss: 0.00009535
Iteration 781/1000 | Loss: 0.00009530
Iteration 782/1000 | Loss: 0.00009529
Iteration 783/1000 | Loss: 0.00009529
Iteration 784/1000 | Loss: 0.00009521
Iteration 785/1000 | Loss: 0.00010888
Iteration 786/1000 | Loss: 0.00009560
Iteration 787/1000 | Loss: 0.00010269
Iteration 788/1000 | Loss: 0.00009547
Iteration 789/1000 | Loss: 0.00010740
Iteration 790/1000 | Loss: 0.00009553
Iteration 791/1000 | Loss: 0.00009946
Iteration 792/1000 | Loss: 0.00009626
Iteration 793/1000 | Loss: 0.00010619
Iteration 794/1000 | Loss: 0.00009529
Iteration 795/1000 | Loss: 0.00009528
Iteration 796/1000 | Loss: 0.00009527
Iteration 797/1000 | Loss: 0.00009527
Iteration 798/1000 | Loss: 0.00009527
Iteration 799/1000 | Loss: 0.00009526
Iteration 800/1000 | Loss: 0.00009526
Iteration 801/1000 | Loss: 0.00009526
Iteration 802/1000 | Loss: 0.00009526
Iteration 803/1000 | Loss: 0.00009525
Iteration 804/1000 | Loss: 0.00009525
Iteration 805/1000 | Loss: 0.00009525
Iteration 806/1000 | Loss: 0.00009525
Iteration 807/1000 | Loss: 0.00009525
Iteration 808/1000 | Loss: 0.00009525
Iteration 809/1000 | Loss: 0.00009525
Iteration 810/1000 | Loss: 0.00009525
Iteration 811/1000 | Loss: 0.00009524
Iteration 812/1000 | Loss: 0.00009524
Iteration 813/1000 | Loss: 0.00009524
Iteration 814/1000 | Loss: 0.00009524
Iteration 815/1000 | Loss: 0.00009524
Iteration 816/1000 | Loss: 0.00009523
Iteration 817/1000 | Loss: 0.00009523
Iteration 818/1000 | Loss: 0.00009523
Iteration 819/1000 | Loss: 0.00009522
Iteration 820/1000 | Loss: 0.00011809
Iteration 821/1000 | Loss: 0.00026913
Iteration 822/1000 | Loss: 0.00012728
Iteration 823/1000 | Loss: 0.00009544
Iteration 824/1000 | Loss: 0.00009914
Iteration 825/1000 | Loss: 0.00009536
Iteration 826/1000 | Loss: 0.00011616
Iteration 827/1000 | Loss: 0.00011438
Iteration 828/1000 | Loss: 0.00009716
Iteration 829/1000 | Loss: 0.00009524
Iteration 830/1000 | Loss: 0.00011206
Iteration 831/1000 | Loss: 0.00011443
Iteration 832/1000 | Loss: 0.00009527
Iteration 833/1000 | Loss: 0.00010450
Iteration 834/1000 | Loss: 0.00013452
Iteration 835/1000 | Loss: 0.00011816
Iteration 836/1000 | Loss: 0.00009784
Iteration 837/1000 | Loss: 0.00011276
Iteration 838/1000 | Loss: 0.00010023
Iteration 839/1000 | Loss: 0.00009541
Iteration 840/1000 | Loss: 0.00015138
Iteration 841/1000 | Loss: 0.00010077
Iteration 842/1000 | Loss: 0.00011161
Iteration 843/1000 | Loss: 0.00025881
Iteration 844/1000 | Loss: 0.00011482
Iteration 845/1000 | Loss: 0.00009561
Iteration 846/1000 | Loss: 0.00012103
Iteration 847/1000 | Loss: 0.00009836
Iteration 848/1000 | Loss: 0.00009778
Iteration 849/1000 | Loss: 0.00009513
Iteration 850/1000 | Loss: 0.00015046
Iteration 851/1000 | Loss: 0.00016496
Iteration 852/1000 | Loss: 0.00020749
Iteration 853/1000 | Loss: 0.00012523
Iteration 854/1000 | Loss: 0.00010496
Iteration 855/1000 | Loss: 0.00009976
Iteration 856/1000 | Loss: 0.00009547
Iteration 857/1000 | Loss: 0.00011331
Iteration 858/1000 | Loss: 0.00009892
Iteration 859/1000 | Loss: 0.00010219
Iteration 860/1000 | Loss: 0.00009532
Iteration 861/1000 | Loss: 0.00010124
Iteration 862/1000 | Loss: 0.00009554
Iteration 863/1000 | Loss: 0.00013463
Iteration 864/1000 | Loss: 0.00013113
Iteration 865/1000 | Loss: 0.00009615
Iteration 866/1000 | Loss: 0.00009536
Iteration 867/1000 | Loss: 0.00011479
Iteration 868/1000 | Loss: 0.00016965
Iteration 869/1000 | Loss: 0.00009684
Iteration 870/1000 | Loss: 0.00015683
Iteration 871/1000 | Loss: 0.00012922
Iteration 872/1000 | Loss: 0.00009904
Iteration 873/1000 | Loss: 0.00013837
Iteration 874/1000 | Loss: 0.00010204
Iteration 875/1000 | Loss: 0.00009538
Iteration 876/1000 | Loss: 0.00018352
Iteration 877/1000 | Loss: 0.00011064
Iteration 878/1000 | Loss: 0.00009620
Iteration 879/1000 | Loss: 0.00012389
Iteration 880/1000 | Loss: 0.00013130
Iteration 881/1000 | Loss: 0.00010213
Iteration 882/1000 | Loss: 0.00009575
Iteration 883/1000 | Loss: 0.00009529
Iteration 884/1000 | Loss: 0.00011619
Iteration 885/1000 | Loss: 0.00009788
Iteration 886/1000 | Loss: 0.00009826
Iteration 887/1000 | Loss: 0.00009614
Iteration 888/1000 | Loss: 0.00015204
Iteration 889/1000 | Loss: 0.00010380
Iteration 890/1000 | Loss: 0.00009709
Iteration 891/1000 | Loss: 0.00010584
Iteration 892/1000 | Loss: 0.00009559
Iteration 893/1000 | Loss: 0.00010330
Iteration 894/1000 | Loss: 0.00012722
Iteration 895/1000 | Loss: 0.00011119
Iteration 896/1000 | Loss: 0.00009627
Iteration 897/1000 | Loss: 0.00014817
Iteration 898/1000 | Loss: 0.00011695
Iteration 899/1000 | Loss: 0.00017087
Iteration 900/1000 | Loss: 0.00010737
Iteration 901/1000 | Loss: 0.00009777
Iteration 902/1000 | Loss: 0.00009740
Iteration 903/1000 | Loss: 0.00009554
Iteration 904/1000 | Loss: 0.00009523
Iteration 905/1000 | Loss: 0.00011330
Iteration 906/1000 | Loss: 0.00010613
Iteration 907/1000 | Loss: 0.00009641
Iteration 908/1000 | Loss: 0.00009590
Iteration 909/1000 | Loss: 0.00009856
Iteration 910/1000 | Loss: 0.00009542
Iteration 911/1000 | Loss: 0.00013513
Iteration 912/1000 | Loss: 0.00009686
Iteration 913/1000 | Loss: 0.00009599
Iteration 914/1000 | Loss: 0.00012662
Iteration 915/1000 | Loss: 0.00010053
Iteration 916/1000 | Loss: 0.00009547
Iteration 917/1000 | Loss: 0.00009979
Iteration 918/1000 | Loss: 0.00010970
Iteration 919/1000 | Loss: 0.00010650
Iteration 920/1000 | Loss: 0.00011719
Iteration 921/1000 | Loss: 0.00009634
Iteration 922/1000 | Loss: 0.00010304
Iteration 923/1000 | Loss: 0.00012078
Iteration 924/1000 | Loss: 0.00011070
Iteration 925/1000 | Loss: 0.00009603
Iteration 926/1000 | Loss: 0.00009530
Iteration 927/1000 | Loss: 0.00009630
Iteration 928/1000 | Loss: 0.00011109
Iteration 929/1000 | Loss: 0.00014348
Iteration 930/1000 | Loss: 0.00010612
Iteration 931/1000 | Loss: 0.00009550
Iteration 932/1000 | Loss: 0.00010441
Iteration 933/1000 | Loss: 0.00010814
Iteration 934/1000 | Loss: 0.00011534
Iteration 935/1000 | Loss: 0.00017626
Iteration 936/1000 | Loss: 0.00011140
Iteration 937/1000 | Loss: 0.00009630
Iteration 938/1000 | Loss: 0.00010315
Iteration 939/1000 | Loss: 0.00009864
Iteration 940/1000 | Loss: 0.00011099
Iteration 941/1000 | Loss: 0.00010068
Iteration 942/1000 | Loss: 0.00014012
Iteration 943/1000 | Loss: 0.00011440
Iteration 944/1000 | Loss: 0.00012107
Iteration 945/1000 | Loss: 0.00010507
Iteration 946/1000 | Loss: 0.00009939
Iteration 947/1000 | Loss: 0.00012251
Iteration 948/1000 | Loss: 0.00010569
Iteration 949/1000 | Loss: 0.00013014
Iteration 950/1000 | Loss: 0.00022234
Iteration 951/1000 | Loss: 0.00009987
Iteration 952/1000 | Loss: 0.00010808
Iteration 953/1000 | Loss: 0.00009870
Iteration 954/1000 | Loss: 0.00009504
Iteration 955/1000 | Loss: 0.00009500
Iteration 956/1000 | Loss: 0.00009833
Iteration 957/1000 | Loss: 0.00011789
Iteration 958/1000 | Loss: 0.00011788
Iteration 959/1000 | Loss: 0.00011788
Iteration 960/1000 | Loss: 0.00014102
Iteration 961/1000 | Loss: 0.00009651
Iteration 962/1000 | Loss: 0.00014177
Iteration 963/1000 | Loss: 0.00009584
Iteration 964/1000 | Loss: 0.00009522
Iteration 965/1000 | Loss: 0.00009521
Iteration 966/1000 | Loss: 0.00009499
Iteration 967/1000 | Loss: 0.00011694
Iteration 968/1000 | Loss: 0.00009731
Iteration 969/1000 | Loss: 0.00009769
Iteration 970/1000 | Loss: 0.00009503
Iteration 971/1000 | Loss: 0.00009495
Iteration 972/1000 | Loss: 0.00009495
Iteration 973/1000 | Loss: 0.00013268
Iteration 974/1000 | Loss: 0.00010024
Iteration 975/1000 | Loss: 0.00010631
Iteration 976/1000 | Loss: 0.00009726
Iteration 977/1000 | Loss: 0.00011513
Iteration 978/1000 | Loss: 0.00012466
Iteration 979/1000 | Loss: 0.00009789
Iteration 980/1000 | Loss: 0.00009564
Iteration 981/1000 | Loss: 0.00011016
Iteration 982/1000 | Loss: 0.00011278
Iteration 983/1000 | Loss: 0.00010975
Iteration 984/1000 | Loss: 0.00010172
Iteration 985/1000 | Loss: 0.00012668
Iteration 986/1000 | Loss: 0.00009981
Iteration 987/1000 | Loss: 0.00012271
Iteration 988/1000 | Loss: 0.00009609
Iteration 989/1000 | Loss: 0.00013825
Iteration 990/1000 | Loss: 0.00009702
Iteration 991/1000 | Loss: 0.00009513
Iteration 992/1000 | Loss: 0.00012829
Iteration 993/1000 | Loss: 0.00009648
Iteration 994/1000 | Loss: 0.00011191
Iteration 995/1000 | Loss: 0.00009573
Iteration 996/1000 | Loss: 0.00014779
Iteration 997/1000 | Loss: 0.00009928
Iteration 998/1000 | Loss: 0.00011856
Iteration 999/1000 | Loss: 0.00010035
Iteration 1000/1000 | Loss: 0.00014162

Optimization complete. Final v2v error: 6.019730091094971 mm

Highest mean error: 36.898902893066406 mm for frame 219

Lowest mean error: 4.14565372467041 mm for frame 99

Saving results

Total time: 1543.9647572040558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601929
Iteration 2/25 | Loss: 0.00128270
Iteration 3/25 | Loss: 0.00112932
Iteration 4/25 | Loss: 0.00109152
Iteration 5/25 | Loss: 0.00108219
Iteration 6/25 | Loss: 0.00108021
Iteration 7/25 | Loss: 0.00107964
Iteration 8/25 | Loss: 0.00107964
Iteration 9/25 | Loss: 0.00107964
Iteration 10/25 | Loss: 0.00107964
Iteration 11/25 | Loss: 0.00107964
Iteration 12/25 | Loss: 0.00107964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010796368587762117, 0.0010796368587762117, 0.0010796368587762117, 0.0010796368587762117, 0.0010796368587762117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010796368587762117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.89967442
Iteration 2/25 | Loss: 0.00089114
Iteration 3/25 | Loss: 0.00089114
Iteration 4/25 | Loss: 0.00089114
Iteration 5/25 | Loss: 0.00089114
Iteration 6/25 | Loss: 0.00089114
Iteration 7/25 | Loss: 0.00089114
Iteration 8/25 | Loss: 0.00089114
Iteration 9/25 | Loss: 0.00089114
Iteration 10/25 | Loss: 0.00089114
Iteration 11/25 | Loss: 0.00089114
Iteration 12/25 | Loss: 0.00089114
Iteration 13/25 | Loss: 0.00089114
Iteration 14/25 | Loss: 0.00089114
Iteration 15/25 | Loss: 0.00089114
Iteration 16/25 | Loss: 0.00089114
Iteration 17/25 | Loss: 0.00089114
Iteration 18/25 | Loss: 0.00089114
Iteration 19/25 | Loss: 0.00089114
Iteration 20/25 | Loss: 0.00089114
Iteration 21/25 | Loss: 0.00089114
Iteration 22/25 | Loss: 0.00089114
Iteration 23/25 | Loss: 0.00089114
Iteration 24/25 | Loss: 0.00089114
Iteration 25/25 | Loss: 0.00089114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089114
Iteration 2/1000 | Loss: 0.00004384
Iteration 3/1000 | Loss: 0.00003560
Iteration 4/1000 | Loss: 0.00003225
Iteration 5/1000 | Loss: 0.00003059
Iteration 6/1000 | Loss: 0.00002960
Iteration 7/1000 | Loss: 0.00002884
Iteration 8/1000 | Loss: 0.00002825
Iteration 9/1000 | Loss: 0.00002783
Iteration 10/1000 | Loss: 0.00002758
Iteration 11/1000 | Loss: 0.00002743
Iteration 12/1000 | Loss: 0.00002742
Iteration 13/1000 | Loss: 0.00002734
Iteration 14/1000 | Loss: 0.00002731
Iteration 15/1000 | Loss: 0.00002731
Iteration 16/1000 | Loss: 0.00002730
Iteration 17/1000 | Loss: 0.00002730
Iteration 18/1000 | Loss: 0.00002730
Iteration 19/1000 | Loss: 0.00002729
Iteration 20/1000 | Loss: 0.00002727
Iteration 21/1000 | Loss: 0.00002727
Iteration 22/1000 | Loss: 0.00002727
Iteration 23/1000 | Loss: 0.00002727
Iteration 24/1000 | Loss: 0.00002727
Iteration 25/1000 | Loss: 0.00002727
Iteration 26/1000 | Loss: 0.00002727
Iteration 27/1000 | Loss: 0.00002727
Iteration 28/1000 | Loss: 0.00002727
Iteration 29/1000 | Loss: 0.00002727
Iteration 30/1000 | Loss: 0.00002727
Iteration 31/1000 | Loss: 0.00002727
Iteration 32/1000 | Loss: 0.00002726
Iteration 33/1000 | Loss: 0.00002726
Iteration 34/1000 | Loss: 0.00002726
Iteration 35/1000 | Loss: 0.00002726
Iteration 36/1000 | Loss: 0.00002726
Iteration 37/1000 | Loss: 0.00002726
Iteration 38/1000 | Loss: 0.00002725
Iteration 39/1000 | Loss: 0.00002725
Iteration 40/1000 | Loss: 0.00002725
Iteration 41/1000 | Loss: 0.00002725
Iteration 42/1000 | Loss: 0.00002724
Iteration 43/1000 | Loss: 0.00002724
Iteration 44/1000 | Loss: 0.00002724
Iteration 45/1000 | Loss: 0.00002724
Iteration 46/1000 | Loss: 0.00002724
Iteration 47/1000 | Loss: 0.00002723
Iteration 48/1000 | Loss: 0.00002723
Iteration 49/1000 | Loss: 0.00002723
Iteration 50/1000 | Loss: 0.00002723
Iteration 51/1000 | Loss: 0.00002723
Iteration 52/1000 | Loss: 0.00002722
Iteration 53/1000 | Loss: 0.00002722
Iteration 54/1000 | Loss: 0.00002722
Iteration 55/1000 | Loss: 0.00002722
Iteration 56/1000 | Loss: 0.00002722
Iteration 57/1000 | Loss: 0.00002722
Iteration 58/1000 | Loss: 0.00002721
Iteration 59/1000 | Loss: 0.00002721
Iteration 60/1000 | Loss: 0.00002721
Iteration 61/1000 | Loss: 0.00002721
Iteration 62/1000 | Loss: 0.00002721
Iteration 63/1000 | Loss: 0.00002721
Iteration 64/1000 | Loss: 0.00002721
Iteration 65/1000 | Loss: 0.00002721
Iteration 66/1000 | Loss: 0.00002721
Iteration 67/1000 | Loss: 0.00002721
Iteration 68/1000 | Loss: 0.00002721
Iteration 69/1000 | Loss: 0.00002721
Iteration 70/1000 | Loss: 0.00002721
Iteration 71/1000 | Loss: 0.00002720
Iteration 72/1000 | Loss: 0.00002720
Iteration 73/1000 | Loss: 0.00002720
Iteration 74/1000 | Loss: 0.00002720
Iteration 75/1000 | Loss: 0.00002720
Iteration 76/1000 | Loss: 0.00002720
Iteration 77/1000 | Loss: 0.00002720
Iteration 78/1000 | Loss: 0.00002720
Iteration 79/1000 | Loss: 0.00002720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.7202639103052206e-05, 2.7202639103052206e-05, 2.7202639103052206e-05, 2.7202639103052206e-05, 2.7202639103052206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7202639103052206e-05

Optimization complete. Final v2v error: 4.5157246589660645 mm

Highest mean error: 5.693002223968506 mm for frame 97

Lowest mean error: 3.749181032180786 mm for frame 144

Saving results

Total time: 30.537737607955933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810571
Iteration 2/25 | Loss: 0.00169086
Iteration 3/25 | Loss: 0.00144276
Iteration 4/25 | Loss: 0.00153780
Iteration 5/25 | Loss: 0.00149089
Iteration 6/25 | Loss: 0.00136499
Iteration 7/25 | Loss: 0.00124487
Iteration 8/25 | Loss: 0.00127150
Iteration 9/25 | Loss: 0.00128634
Iteration 10/25 | Loss: 0.00129716
Iteration 11/25 | Loss: 0.00120835
Iteration 12/25 | Loss: 0.00123467
Iteration 13/25 | Loss: 0.00127609
Iteration 14/25 | Loss: 0.00124174
Iteration 15/25 | Loss: 0.00118412
Iteration 16/25 | Loss: 0.00117913
Iteration 17/25 | Loss: 0.00118434
Iteration 18/25 | Loss: 0.00117717
Iteration 19/25 | Loss: 0.00117336
Iteration 20/25 | Loss: 0.00117276
Iteration 21/25 | Loss: 0.00117250
Iteration 22/25 | Loss: 0.00120822
Iteration 23/25 | Loss: 0.00116367
Iteration 24/25 | Loss: 0.00116077
Iteration 25/25 | Loss: 0.00116054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54794979
Iteration 2/25 | Loss: 0.00107051
Iteration 3/25 | Loss: 0.00107051
Iteration 4/25 | Loss: 0.00107050
Iteration 5/25 | Loss: 0.00107050
Iteration 6/25 | Loss: 0.00107050
Iteration 7/25 | Loss: 0.00107050
Iteration 8/25 | Loss: 0.00107050
Iteration 9/25 | Loss: 0.00107050
Iteration 10/25 | Loss: 0.00107050
Iteration 11/25 | Loss: 0.00107050
Iteration 12/25 | Loss: 0.00107050
Iteration 13/25 | Loss: 0.00107050
Iteration 14/25 | Loss: 0.00107050
Iteration 15/25 | Loss: 0.00107050
Iteration 16/25 | Loss: 0.00107050
Iteration 17/25 | Loss: 0.00107050
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010705019813030958, 0.0010705019813030958, 0.0010705019813030958, 0.0010705019813030958, 0.0010705019813030958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010705019813030958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107050
Iteration 2/1000 | Loss: 0.00006166
Iteration 3/1000 | Loss: 0.00004873
Iteration 4/1000 | Loss: 0.00004337
Iteration 5/1000 | Loss: 0.00004067
Iteration 6/1000 | Loss: 0.00003961
Iteration 7/1000 | Loss: 0.00003865
Iteration 8/1000 | Loss: 0.00003793
Iteration 9/1000 | Loss: 0.00003719
Iteration 10/1000 | Loss: 0.00003685
Iteration 11/1000 | Loss: 0.00003659
Iteration 12/1000 | Loss: 0.00003640
Iteration 13/1000 | Loss: 0.00003623
Iteration 14/1000 | Loss: 0.00003616
Iteration 15/1000 | Loss: 0.00003611
Iteration 16/1000 | Loss: 0.00003610
Iteration 17/1000 | Loss: 0.00003610
Iteration 18/1000 | Loss: 0.00003606
Iteration 19/1000 | Loss: 0.00003606
Iteration 20/1000 | Loss: 0.00003604
Iteration 21/1000 | Loss: 0.00003604
Iteration 22/1000 | Loss: 0.00003604
Iteration 23/1000 | Loss: 0.00003603
Iteration 24/1000 | Loss: 0.00003603
Iteration 25/1000 | Loss: 0.00003603
Iteration 26/1000 | Loss: 0.00003603
Iteration 27/1000 | Loss: 0.00003603
Iteration 28/1000 | Loss: 0.00003602
Iteration 29/1000 | Loss: 0.00003601
Iteration 30/1000 | Loss: 0.00003601
Iteration 31/1000 | Loss: 0.00003601
Iteration 32/1000 | Loss: 0.00003601
Iteration 33/1000 | Loss: 0.00003601
Iteration 34/1000 | Loss: 0.00003601
Iteration 35/1000 | Loss: 0.00003601
Iteration 36/1000 | Loss: 0.00003601
Iteration 37/1000 | Loss: 0.00003601
Iteration 38/1000 | Loss: 0.00003601
Iteration 39/1000 | Loss: 0.00003601
Iteration 40/1000 | Loss: 0.00003600
Iteration 41/1000 | Loss: 0.00003600
Iteration 42/1000 | Loss: 0.00003600
Iteration 43/1000 | Loss: 0.00003600
Iteration 44/1000 | Loss: 0.00003600
Iteration 45/1000 | Loss: 0.00003599
Iteration 46/1000 | Loss: 0.00003599
Iteration 47/1000 | Loss: 0.00003599
Iteration 48/1000 | Loss: 0.00003599
Iteration 49/1000 | Loss: 0.00003599
Iteration 50/1000 | Loss: 0.00003599
Iteration 51/1000 | Loss: 0.00003599
Iteration 52/1000 | Loss: 0.00003599
Iteration 53/1000 | Loss: 0.00003599
Iteration 54/1000 | Loss: 0.00003599
Iteration 55/1000 | Loss: 0.00003599
Iteration 56/1000 | Loss: 0.00003599
Iteration 57/1000 | Loss: 0.00003599
Iteration 58/1000 | Loss: 0.00003599
Iteration 59/1000 | Loss: 0.00003598
Iteration 60/1000 | Loss: 0.00003598
Iteration 61/1000 | Loss: 0.00003598
Iteration 62/1000 | Loss: 0.00003598
Iteration 63/1000 | Loss: 0.00003598
Iteration 64/1000 | Loss: 0.00003597
Iteration 65/1000 | Loss: 0.00003597
Iteration 66/1000 | Loss: 0.00003597
Iteration 67/1000 | Loss: 0.00003597
Iteration 68/1000 | Loss: 0.00003597
Iteration 69/1000 | Loss: 0.00003597
Iteration 70/1000 | Loss: 0.00003597
Iteration 71/1000 | Loss: 0.00003597
Iteration 72/1000 | Loss: 0.00003597
Iteration 73/1000 | Loss: 0.00003597
Iteration 74/1000 | Loss: 0.00003596
Iteration 75/1000 | Loss: 0.00003596
Iteration 76/1000 | Loss: 0.00003596
Iteration 77/1000 | Loss: 0.00003596
Iteration 78/1000 | Loss: 0.00003596
Iteration 79/1000 | Loss: 0.00003596
Iteration 80/1000 | Loss: 0.00003596
Iteration 81/1000 | Loss: 0.00003596
Iteration 82/1000 | Loss: 0.00003596
Iteration 83/1000 | Loss: 0.00003596
Iteration 84/1000 | Loss: 0.00003596
Iteration 85/1000 | Loss: 0.00003596
Iteration 86/1000 | Loss: 0.00003596
Iteration 87/1000 | Loss: 0.00003596
Iteration 88/1000 | Loss: 0.00003596
Iteration 89/1000 | Loss: 0.00003596
Iteration 90/1000 | Loss: 0.00003596
Iteration 91/1000 | Loss: 0.00003596
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00003596
Iteration 94/1000 | Loss: 0.00003596
Iteration 95/1000 | Loss: 0.00003596
Iteration 96/1000 | Loss: 0.00003596
Iteration 97/1000 | Loss: 0.00003596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [3.596059468691237e-05, 3.596059468691237e-05, 3.596059468691237e-05, 3.596059468691237e-05, 3.596059468691237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.596059468691237e-05

Optimization complete. Final v2v error: 5.122861385345459 mm

Highest mean error: 5.797062873840332 mm for frame 89

Lowest mean error: 4.40604829788208 mm for frame 3

Saving results

Total time: 67.49226474761963
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119308
Iteration 2/25 | Loss: 0.00369104
Iteration 3/25 | Loss: 0.00241253
Iteration 4/25 | Loss: 0.00217445
Iteration 5/25 | Loss: 0.00206203
Iteration 6/25 | Loss: 0.00202752
Iteration 7/25 | Loss: 0.00200136
Iteration 8/25 | Loss: 0.00197746
Iteration 9/25 | Loss: 0.00197763
Iteration 10/25 | Loss: 0.00196826
Iteration 11/25 | Loss: 0.00195760
Iteration 12/25 | Loss: 0.00195610
Iteration 13/25 | Loss: 0.00194676
Iteration 14/25 | Loss: 0.00194650
Iteration 15/25 | Loss: 0.00194348
Iteration 16/25 | Loss: 0.00194219
Iteration 17/25 | Loss: 0.00194394
Iteration 18/25 | Loss: 0.00194198
Iteration 19/25 | Loss: 0.00194196
Iteration 20/25 | Loss: 0.00194196
Iteration 21/25 | Loss: 0.00194196
Iteration 22/25 | Loss: 0.00194196
Iteration 23/25 | Loss: 0.00194196
Iteration 24/25 | Loss: 0.00194196
Iteration 25/25 | Loss: 0.00194196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41429853
Iteration 2/25 | Loss: 0.00974715
Iteration 3/25 | Loss: 0.00924673
Iteration 4/25 | Loss: 0.00924673
Iteration 5/25 | Loss: 0.00924673
Iteration 6/25 | Loss: 0.00924673
Iteration 7/25 | Loss: 0.00924673
Iteration 8/25 | Loss: 0.00924673
Iteration 9/25 | Loss: 0.00924673
Iteration 10/25 | Loss: 0.00924673
Iteration 11/25 | Loss: 0.00924673
Iteration 12/25 | Loss: 0.00924673
Iteration 13/25 | Loss: 0.00924673
Iteration 14/25 | Loss: 0.00924673
Iteration 15/25 | Loss: 0.00924673
Iteration 16/25 | Loss: 0.00924673
Iteration 17/25 | Loss: 0.00924673
Iteration 18/25 | Loss: 0.00924673
Iteration 19/25 | Loss: 0.00924673
Iteration 20/25 | Loss: 0.00924673
Iteration 21/25 | Loss: 0.00924673
Iteration 22/25 | Loss: 0.00924673
Iteration 23/25 | Loss: 0.00924673
Iteration 24/25 | Loss: 0.00924673
Iteration 25/25 | Loss: 0.00924673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00924673
Iteration 2/1000 | Loss: 0.00195473
Iteration 3/1000 | Loss: 0.00151896
Iteration 4/1000 | Loss: 0.00165995
Iteration 5/1000 | Loss: 0.00172028
Iteration 6/1000 | Loss: 0.00147384
Iteration 7/1000 | Loss: 0.00102081
Iteration 8/1000 | Loss: 0.00109261
Iteration 9/1000 | Loss: 0.00087837
Iteration 10/1000 | Loss: 0.00129865
Iteration 11/1000 | Loss: 0.00188477
Iteration 12/1000 | Loss: 0.00075645
Iteration 13/1000 | Loss: 0.00088600
Iteration 14/1000 | Loss: 0.00082638
Iteration 15/1000 | Loss: 0.00086627
Iteration 16/1000 | Loss: 0.00089417
Iteration 17/1000 | Loss: 0.00096677
Iteration 18/1000 | Loss: 0.00122999
Iteration 19/1000 | Loss: 0.00090334
Iteration 20/1000 | Loss: 0.00116422
Iteration 21/1000 | Loss: 0.00106572
Iteration 22/1000 | Loss: 0.00090550
Iteration 23/1000 | Loss: 0.00106614
Iteration 24/1000 | Loss: 0.00125596
Iteration 25/1000 | Loss: 0.00099755
Iteration 26/1000 | Loss: 0.00115085
Iteration 27/1000 | Loss: 0.00120021
Iteration 28/1000 | Loss: 0.00084403
Iteration 29/1000 | Loss: 0.00058406
Iteration 30/1000 | Loss: 0.00064729
Iteration 31/1000 | Loss: 0.00053264
Iteration 32/1000 | Loss: 0.00066222
Iteration 33/1000 | Loss: 0.00075942
Iteration 34/1000 | Loss: 0.00133483
Iteration 35/1000 | Loss: 0.00149469
Iteration 36/1000 | Loss: 0.00094000
Iteration 37/1000 | Loss: 0.00065118
Iteration 38/1000 | Loss: 0.00112903
Iteration 39/1000 | Loss: 0.00104075
Iteration 40/1000 | Loss: 0.00108942
Iteration 41/1000 | Loss: 0.00150998
Iteration 42/1000 | Loss: 0.00061684
Iteration 43/1000 | Loss: 0.00054702
Iteration 44/1000 | Loss: 0.00053562
Iteration 45/1000 | Loss: 0.00067565
Iteration 46/1000 | Loss: 0.00095162
Iteration 47/1000 | Loss: 0.00073224
Iteration 48/1000 | Loss: 0.00071422
Iteration 49/1000 | Loss: 0.00069437
Iteration 50/1000 | Loss: 0.00091255
Iteration 51/1000 | Loss: 0.00068204
Iteration 52/1000 | Loss: 0.00104135
Iteration 53/1000 | Loss: 0.00173653
Iteration 54/1000 | Loss: 0.00090687
Iteration 55/1000 | Loss: 0.00235551
Iteration 56/1000 | Loss: 0.00082209
Iteration 57/1000 | Loss: 0.00056551
Iteration 58/1000 | Loss: 0.00057644
Iteration 59/1000 | Loss: 0.00115940
Iteration 60/1000 | Loss: 0.00164033
Iteration 61/1000 | Loss: 0.00084266
Iteration 62/1000 | Loss: 0.00081087
Iteration 63/1000 | Loss: 0.00059069
Iteration 64/1000 | Loss: 0.00063257
Iteration 65/1000 | Loss: 0.00125585
Iteration 66/1000 | Loss: 0.00050251
Iteration 67/1000 | Loss: 0.00055268
Iteration 68/1000 | Loss: 0.00047300
Iteration 69/1000 | Loss: 0.00050471
Iteration 70/1000 | Loss: 0.00086581
Iteration 71/1000 | Loss: 0.00130615
Iteration 72/1000 | Loss: 0.00065586
Iteration 73/1000 | Loss: 0.00082160
Iteration 74/1000 | Loss: 0.00048832
Iteration 75/1000 | Loss: 0.00050103
Iteration 76/1000 | Loss: 0.00047838
Iteration 77/1000 | Loss: 0.00049948
Iteration 78/1000 | Loss: 0.00045192
Iteration 79/1000 | Loss: 0.00048760
Iteration 80/1000 | Loss: 0.00068540
Iteration 81/1000 | Loss: 0.00078533
Iteration 82/1000 | Loss: 0.00049433
Iteration 83/1000 | Loss: 0.00053947
Iteration 84/1000 | Loss: 0.00046185
Iteration 85/1000 | Loss: 0.00047774
Iteration 86/1000 | Loss: 0.00096303
Iteration 87/1000 | Loss: 0.00057549
Iteration 88/1000 | Loss: 0.00052948
Iteration 89/1000 | Loss: 0.00047066
Iteration 90/1000 | Loss: 0.00044392
Iteration 91/1000 | Loss: 0.00070980
Iteration 92/1000 | Loss: 0.00071842
Iteration 93/1000 | Loss: 0.00073578
Iteration 94/1000 | Loss: 0.00049797
Iteration 95/1000 | Loss: 0.00061400
Iteration 96/1000 | Loss: 0.00055626
Iteration 97/1000 | Loss: 0.00047109
Iteration 98/1000 | Loss: 0.00044610
Iteration 99/1000 | Loss: 0.00067835
Iteration 100/1000 | Loss: 0.00090066
Iteration 101/1000 | Loss: 0.00076059
Iteration 102/1000 | Loss: 0.00141174
Iteration 103/1000 | Loss: 0.00064598
Iteration 104/1000 | Loss: 0.00056410
Iteration 105/1000 | Loss: 0.00059738
Iteration 106/1000 | Loss: 0.00097132
Iteration 107/1000 | Loss: 0.00063487
Iteration 108/1000 | Loss: 0.00130996
Iteration 109/1000 | Loss: 0.00069496
Iteration 110/1000 | Loss: 0.00095784
Iteration 111/1000 | Loss: 0.00049514
Iteration 112/1000 | Loss: 0.00050383
Iteration 113/1000 | Loss: 0.00044920
Iteration 114/1000 | Loss: 0.00050641
Iteration 115/1000 | Loss: 0.00053302
Iteration 116/1000 | Loss: 0.00050928
Iteration 117/1000 | Loss: 0.00051518
Iteration 118/1000 | Loss: 0.00054324
Iteration 119/1000 | Loss: 0.00056698
Iteration 120/1000 | Loss: 0.00057808
Iteration 121/1000 | Loss: 0.00068097
Iteration 122/1000 | Loss: 0.00052289
Iteration 123/1000 | Loss: 0.00047811
Iteration 124/1000 | Loss: 0.00063175
Iteration 125/1000 | Loss: 0.00071747
Iteration 126/1000 | Loss: 0.00061624
Iteration 127/1000 | Loss: 0.00064198
Iteration 128/1000 | Loss: 0.00060796
Iteration 129/1000 | Loss: 0.00070551
Iteration 130/1000 | Loss: 0.00060582
Iteration 131/1000 | Loss: 0.00045154
Iteration 132/1000 | Loss: 0.00048152
Iteration 133/1000 | Loss: 0.00045534
Iteration 134/1000 | Loss: 0.00044202
Iteration 135/1000 | Loss: 0.00044671
Iteration 136/1000 | Loss: 0.00049768
Iteration 137/1000 | Loss: 0.00043748
Iteration 138/1000 | Loss: 0.00044989
Iteration 139/1000 | Loss: 0.00043428
Iteration 140/1000 | Loss: 0.00043980
Iteration 141/1000 | Loss: 0.00043751
Iteration 142/1000 | Loss: 0.00043395
Iteration 143/1000 | Loss: 0.00046149
Iteration 144/1000 | Loss: 0.00048248
Iteration 145/1000 | Loss: 0.00043788
Iteration 146/1000 | Loss: 0.00045341
Iteration 147/1000 | Loss: 0.00045297
Iteration 148/1000 | Loss: 0.00045418
Iteration 149/1000 | Loss: 0.00044690
Iteration 150/1000 | Loss: 0.00045758
Iteration 151/1000 | Loss: 0.00044497
Iteration 152/1000 | Loss: 0.00045804
Iteration 153/1000 | Loss: 0.00043502
Iteration 154/1000 | Loss: 0.00047123
Iteration 155/1000 | Loss: 0.00075336
Iteration 156/1000 | Loss: 0.00044633
Iteration 157/1000 | Loss: 0.00046325
Iteration 158/1000 | Loss: 0.00049330
Iteration 159/1000 | Loss: 0.00046024
Iteration 160/1000 | Loss: 0.00044683
Iteration 161/1000 | Loss: 0.00050521
Iteration 162/1000 | Loss: 0.00045718
Iteration 163/1000 | Loss: 0.00043829
Iteration 164/1000 | Loss: 0.00043276
Iteration 165/1000 | Loss: 0.00043503
Iteration 166/1000 | Loss: 0.00043659
Iteration 167/1000 | Loss: 0.00044796
Iteration 168/1000 | Loss: 0.00043520
Iteration 169/1000 | Loss: 0.00043291
Iteration 170/1000 | Loss: 0.00047452
Iteration 171/1000 | Loss: 0.00047451
Iteration 172/1000 | Loss: 0.00048599
Iteration 173/1000 | Loss: 0.00050872
Iteration 174/1000 | Loss: 0.00043554
Iteration 175/1000 | Loss: 0.00044267
Iteration 176/1000 | Loss: 0.00043640
Iteration 177/1000 | Loss: 0.00044980
Iteration 178/1000 | Loss: 0.00043841
Iteration 179/1000 | Loss: 0.00045338
Iteration 180/1000 | Loss: 0.00064748
Iteration 181/1000 | Loss: 0.00053941
Iteration 182/1000 | Loss: 0.00045956
Iteration 183/1000 | Loss: 0.00043317
Iteration 184/1000 | Loss: 0.00043543
Iteration 185/1000 | Loss: 0.00044274
Iteration 186/1000 | Loss: 0.00043226
Iteration 187/1000 | Loss: 0.00045587
Iteration 188/1000 | Loss: 0.00043360
Iteration 189/1000 | Loss: 0.00043436
Iteration 190/1000 | Loss: 0.00043529
Iteration 191/1000 | Loss: 0.00043205
Iteration 192/1000 | Loss: 0.00043167
Iteration 193/1000 | Loss: 0.00043167
Iteration 194/1000 | Loss: 0.00043167
Iteration 195/1000 | Loss: 0.00043167
Iteration 196/1000 | Loss: 0.00043167
Iteration 197/1000 | Loss: 0.00043167
Iteration 198/1000 | Loss: 0.00043167
Iteration 199/1000 | Loss: 0.00043167
Iteration 200/1000 | Loss: 0.00043167
Iteration 201/1000 | Loss: 0.00043167
Iteration 202/1000 | Loss: 0.00043167
Iteration 203/1000 | Loss: 0.00043167
Iteration 204/1000 | Loss: 0.00043167
Iteration 205/1000 | Loss: 0.00043167
Iteration 206/1000 | Loss: 0.00043167
Iteration 207/1000 | Loss: 0.00043167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [0.0004316671402193606, 0.0004316671402193606, 0.0004316671402193606, 0.0004316671402193606, 0.0004316671402193606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004316671402193606

Optimization complete. Final v2v error: 11.681400299072266 mm

Highest mean error: 12.747443199157715 mm for frame 151

Lowest mean error: 6.709572792053223 mm for frame 238

Saving results

Total time: 342.93897461891174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904236
Iteration 2/25 | Loss: 0.00131304
Iteration 3/25 | Loss: 0.00114377
Iteration 4/25 | Loss: 0.00110558
Iteration 5/25 | Loss: 0.00108795
Iteration 6/25 | Loss: 0.00108407
Iteration 7/25 | Loss: 0.00108245
Iteration 8/25 | Loss: 0.00108227
Iteration 9/25 | Loss: 0.00108227
Iteration 10/25 | Loss: 0.00108227
Iteration 11/25 | Loss: 0.00108227
Iteration 12/25 | Loss: 0.00108227
Iteration 13/25 | Loss: 0.00108227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010822733165696263, 0.0010822733165696263, 0.0010822733165696263, 0.0010822733165696263, 0.0010822733165696263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010822733165696263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63364005
Iteration 2/25 | Loss: 0.00128741
Iteration 3/25 | Loss: 0.00128741
Iteration 4/25 | Loss: 0.00128741
Iteration 5/25 | Loss: 0.00128741
Iteration 6/25 | Loss: 0.00128741
Iteration 7/25 | Loss: 0.00128741
Iteration 8/25 | Loss: 0.00128741
Iteration 9/25 | Loss: 0.00128741
Iteration 10/25 | Loss: 0.00128741
Iteration 11/25 | Loss: 0.00128741
Iteration 12/25 | Loss: 0.00128741
Iteration 13/25 | Loss: 0.00128741
Iteration 14/25 | Loss: 0.00128741
Iteration 15/25 | Loss: 0.00128741
Iteration 16/25 | Loss: 0.00128741
Iteration 17/25 | Loss: 0.00128741
Iteration 18/25 | Loss: 0.00128741
Iteration 19/25 | Loss: 0.00128741
Iteration 20/25 | Loss: 0.00128741
Iteration 21/25 | Loss: 0.00128741
Iteration 22/25 | Loss: 0.00128741
Iteration 23/25 | Loss: 0.00128741
Iteration 24/25 | Loss: 0.00128741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012874093372374773, 0.0012874093372374773, 0.0012874093372374773, 0.0012874093372374773, 0.0012874093372374773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012874093372374773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128741
Iteration 2/1000 | Loss: 0.00007886
Iteration 3/1000 | Loss: 0.00004873
Iteration 4/1000 | Loss: 0.00003990
Iteration 5/1000 | Loss: 0.00003522
Iteration 6/1000 | Loss: 0.00003336
Iteration 7/1000 | Loss: 0.00003185
Iteration 8/1000 | Loss: 0.00003100
Iteration 9/1000 | Loss: 0.00003014
Iteration 10/1000 | Loss: 0.00002947
Iteration 11/1000 | Loss: 0.00002903
Iteration 12/1000 | Loss: 0.00002875
Iteration 13/1000 | Loss: 0.00002848
Iteration 14/1000 | Loss: 0.00002829
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002817
Iteration 17/1000 | Loss: 0.00002810
Iteration 18/1000 | Loss: 0.00002806
Iteration 19/1000 | Loss: 0.00002806
Iteration 20/1000 | Loss: 0.00002804
Iteration 21/1000 | Loss: 0.00002799
Iteration 22/1000 | Loss: 0.00002796
Iteration 23/1000 | Loss: 0.00002796
Iteration 24/1000 | Loss: 0.00002795
Iteration 25/1000 | Loss: 0.00002789
Iteration 26/1000 | Loss: 0.00002785
Iteration 27/1000 | Loss: 0.00002784
Iteration 28/1000 | Loss: 0.00002784
Iteration 29/1000 | Loss: 0.00002784
Iteration 30/1000 | Loss: 0.00002784
Iteration 31/1000 | Loss: 0.00002784
Iteration 32/1000 | Loss: 0.00002784
Iteration 33/1000 | Loss: 0.00002784
Iteration 34/1000 | Loss: 0.00002784
Iteration 35/1000 | Loss: 0.00002783
Iteration 36/1000 | Loss: 0.00002783
Iteration 37/1000 | Loss: 0.00002783
Iteration 38/1000 | Loss: 0.00002783
Iteration 39/1000 | Loss: 0.00002783
Iteration 40/1000 | Loss: 0.00002782
Iteration 41/1000 | Loss: 0.00002782
Iteration 42/1000 | Loss: 0.00002781
Iteration 43/1000 | Loss: 0.00002781
Iteration 44/1000 | Loss: 0.00002781
Iteration 45/1000 | Loss: 0.00002780
Iteration 46/1000 | Loss: 0.00002780
Iteration 47/1000 | Loss: 0.00002780
Iteration 48/1000 | Loss: 0.00002779
Iteration 49/1000 | Loss: 0.00002779
Iteration 50/1000 | Loss: 0.00002779
Iteration 51/1000 | Loss: 0.00002779
Iteration 52/1000 | Loss: 0.00002779
Iteration 53/1000 | Loss: 0.00002778
Iteration 54/1000 | Loss: 0.00002778
Iteration 55/1000 | Loss: 0.00002778
Iteration 56/1000 | Loss: 0.00002778
Iteration 57/1000 | Loss: 0.00002777
Iteration 58/1000 | Loss: 0.00002777
Iteration 59/1000 | Loss: 0.00002777
Iteration 60/1000 | Loss: 0.00002776
Iteration 61/1000 | Loss: 0.00002775
Iteration 62/1000 | Loss: 0.00002775
Iteration 63/1000 | Loss: 0.00002775
Iteration 64/1000 | Loss: 0.00002775
Iteration 65/1000 | Loss: 0.00002775
Iteration 66/1000 | Loss: 0.00002775
Iteration 67/1000 | Loss: 0.00002774
Iteration 68/1000 | Loss: 0.00002774
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002774
Iteration 71/1000 | Loss: 0.00002774
Iteration 72/1000 | Loss: 0.00002774
Iteration 73/1000 | Loss: 0.00002774
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002773
Iteration 77/1000 | Loss: 0.00002773
Iteration 78/1000 | Loss: 0.00002773
Iteration 79/1000 | Loss: 0.00002772
Iteration 80/1000 | Loss: 0.00002772
Iteration 81/1000 | Loss: 0.00002772
Iteration 82/1000 | Loss: 0.00002772
Iteration 83/1000 | Loss: 0.00002771
Iteration 84/1000 | Loss: 0.00002771
Iteration 85/1000 | Loss: 0.00002771
Iteration 86/1000 | Loss: 0.00002770
Iteration 87/1000 | Loss: 0.00002770
Iteration 88/1000 | Loss: 0.00002770
Iteration 89/1000 | Loss: 0.00002770
Iteration 90/1000 | Loss: 0.00002770
Iteration 91/1000 | Loss: 0.00002769
Iteration 92/1000 | Loss: 0.00002769
Iteration 93/1000 | Loss: 0.00002769
Iteration 94/1000 | Loss: 0.00002769
Iteration 95/1000 | Loss: 0.00002769
Iteration 96/1000 | Loss: 0.00002769
Iteration 97/1000 | Loss: 0.00002769
Iteration 98/1000 | Loss: 0.00002769
Iteration 99/1000 | Loss: 0.00002769
Iteration 100/1000 | Loss: 0.00002768
Iteration 101/1000 | Loss: 0.00002768
Iteration 102/1000 | Loss: 0.00002768
Iteration 103/1000 | Loss: 0.00002768
Iteration 104/1000 | Loss: 0.00002768
Iteration 105/1000 | Loss: 0.00002768
Iteration 106/1000 | Loss: 0.00002768
Iteration 107/1000 | Loss: 0.00002768
Iteration 108/1000 | Loss: 0.00002768
Iteration 109/1000 | Loss: 0.00002768
Iteration 110/1000 | Loss: 0.00002768
Iteration 111/1000 | Loss: 0.00002768
Iteration 112/1000 | Loss: 0.00002767
Iteration 113/1000 | Loss: 0.00002767
Iteration 114/1000 | Loss: 0.00002767
Iteration 115/1000 | Loss: 0.00002766
Iteration 116/1000 | Loss: 0.00002766
Iteration 117/1000 | Loss: 0.00002766
Iteration 118/1000 | Loss: 0.00002766
Iteration 119/1000 | Loss: 0.00002766
Iteration 120/1000 | Loss: 0.00002766
Iteration 121/1000 | Loss: 0.00002766
Iteration 122/1000 | Loss: 0.00002766
Iteration 123/1000 | Loss: 0.00002766
Iteration 124/1000 | Loss: 0.00002766
Iteration 125/1000 | Loss: 0.00002766
Iteration 126/1000 | Loss: 0.00002766
Iteration 127/1000 | Loss: 0.00002765
Iteration 128/1000 | Loss: 0.00002765
Iteration 129/1000 | Loss: 0.00002765
Iteration 130/1000 | Loss: 0.00002765
Iteration 131/1000 | Loss: 0.00002765
Iteration 132/1000 | Loss: 0.00002765
Iteration 133/1000 | Loss: 0.00002765
Iteration 134/1000 | Loss: 0.00002765
Iteration 135/1000 | Loss: 0.00002765
Iteration 136/1000 | Loss: 0.00002765
Iteration 137/1000 | Loss: 0.00002765
Iteration 138/1000 | Loss: 0.00002765
Iteration 139/1000 | Loss: 0.00002765
Iteration 140/1000 | Loss: 0.00002764
Iteration 141/1000 | Loss: 0.00002764
Iteration 142/1000 | Loss: 0.00002764
Iteration 143/1000 | Loss: 0.00002764
Iteration 144/1000 | Loss: 0.00002764
Iteration 145/1000 | Loss: 0.00002764
Iteration 146/1000 | Loss: 0.00002764
Iteration 147/1000 | Loss: 0.00002764
Iteration 148/1000 | Loss: 0.00002764
Iteration 149/1000 | Loss: 0.00002764
Iteration 150/1000 | Loss: 0.00002763
Iteration 151/1000 | Loss: 0.00002763
Iteration 152/1000 | Loss: 0.00002763
Iteration 153/1000 | Loss: 0.00002763
Iteration 154/1000 | Loss: 0.00002763
Iteration 155/1000 | Loss: 0.00002762
Iteration 156/1000 | Loss: 0.00002762
Iteration 157/1000 | Loss: 0.00002762
Iteration 158/1000 | Loss: 0.00002762
Iteration 159/1000 | Loss: 0.00002762
Iteration 160/1000 | Loss: 0.00002762
Iteration 161/1000 | Loss: 0.00002762
Iteration 162/1000 | Loss: 0.00002762
Iteration 163/1000 | Loss: 0.00002762
Iteration 164/1000 | Loss: 0.00002762
Iteration 165/1000 | Loss: 0.00002762
Iteration 166/1000 | Loss: 0.00002762
Iteration 167/1000 | Loss: 0.00002762
Iteration 168/1000 | Loss: 0.00002762
Iteration 169/1000 | Loss: 0.00002762
Iteration 170/1000 | Loss: 0.00002762
Iteration 171/1000 | Loss: 0.00002762
Iteration 172/1000 | Loss: 0.00002762
Iteration 173/1000 | Loss: 0.00002762
Iteration 174/1000 | Loss: 0.00002762
Iteration 175/1000 | Loss: 0.00002762
Iteration 176/1000 | Loss: 0.00002762
Iteration 177/1000 | Loss: 0.00002762
Iteration 178/1000 | Loss: 0.00002762
Iteration 179/1000 | Loss: 0.00002762
Iteration 180/1000 | Loss: 0.00002762
Iteration 181/1000 | Loss: 0.00002762
Iteration 182/1000 | Loss: 0.00002762
Iteration 183/1000 | Loss: 0.00002762
Iteration 184/1000 | Loss: 0.00002762
Iteration 185/1000 | Loss: 0.00002762
Iteration 186/1000 | Loss: 0.00002762
Iteration 187/1000 | Loss: 0.00002762
Iteration 188/1000 | Loss: 0.00002762
Iteration 189/1000 | Loss: 0.00002762
Iteration 190/1000 | Loss: 0.00002762
Iteration 191/1000 | Loss: 0.00002762
Iteration 192/1000 | Loss: 0.00002762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.7615758881438524e-05, 2.7615758881438524e-05, 2.7615758881438524e-05, 2.7615758881438524e-05, 2.7615758881438524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7615758881438524e-05

Optimization complete. Final v2v error: 4.496603965759277 mm

Highest mean error: 6.969415187835693 mm for frame 124

Lowest mean error: 3.7216804027557373 mm for frame 155

Saving results

Total time: 46.95883107185364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435303
Iteration 2/25 | Loss: 0.00111070
Iteration 3/25 | Loss: 0.00100372
Iteration 4/25 | Loss: 0.00099212
Iteration 5/25 | Loss: 0.00098735
Iteration 6/25 | Loss: 0.00098623
Iteration 7/25 | Loss: 0.00098615
Iteration 8/25 | Loss: 0.00098615
Iteration 9/25 | Loss: 0.00098615
Iteration 10/25 | Loss: 0.00098615
Iteration 11/25 | Loss: 0.00098615
Iteration 12/25 | Loss: 0.00098615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009861491853371263, 0.0009861491853371263, 0.0009861491853371263, 0.0009861491853371263, 0.0009861491853371263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009861491853371263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47920561
Iteration 2/25 | Loss: 0.00092344
Iteration 3/25 | Loss: 0.00092344
Iteration 4/25 | Loss: 0.00092344
Iteration 5/25 | Loss: 0.00092344
Iteration 6/25 | Loss: 0.00092344
Iteration 7/25 | Loss: 0.00092344
Iteration 8/25 | Loss: 0.00092344
Iteration 9/25 | Loss: 0.00092344
Iteration 10/25 | Loss: 0.00092344
Iteration 11/25 | Loss: 0.00092344
Iteration 12/25 | Loss: 0.00092344
Iteration 13/25 | Loss: 0.00092344
Iteration 14/25 | Loss: 0.00092344
Iteration 15/25 | Loss: 0.00092344
Iteration 16/25 | Loss: 0.00092344
Iteration 17/25 | Loss: 0.00092344
Iteration 18/25 | Loss: 0.00092344
Iteration 19/25 | Loss: 0.00092344
Iteration 20/25 | Loss: 0.00092344
Iteration 21/25 | Loss: 0.00092344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009234357276000082, 0.0009234357276000082, 0.0009234357276000082, 0.0009234357276000082, 0.0009234357276000082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009234357276000082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092344
Iteration 2/1000 | Loss: 0.00003115
Iteration 3/1000 | Loss: 0.00002337
Iteration 4/1000 | Loss: 0.00002115
Iteration 5/1000 | Loss: 0.00002049
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001954
Iteration 8/1000 | Loss: 0.00001941
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001924
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001915
Iteration 15/1000 | Loss: 0.00001914
Iteration 16/1000 | Loss: 0.00001914
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001914
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001913
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001912
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001911
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001907
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001906
Iteration 41/1000 | Loss: 0.00001906
Iteration 42/1000 | Loss: 0.00001906
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001905
Iteration 46/1000 | Loss: 0.00001905
Iteration 47/1000 | Loss: 0.00001905
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001904
Iteration 52/1000 | Loss: 0.00001904
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001903
Iteration 55/1000 | Loss: 0.00001903
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001902
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001901
Iteration 67/1000 | Loss: 0.00001901
Iteration 68/1000 | Loss: 0.00001901
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001901
Iteration 84/1000 | Loss: 0.00001901
Iteration 85/1000 | Loss: 0.00001901
Iteration 86/1000 | Loss: 0.00001901
Iteration 87/1000 | Loss: 0.00001901
Iteration 88/1000 | Loss: 0.00001901
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001901
Iteration 91/1000 | Loss: 0.00001901
Iteration 92/1000 | Loss: 0.00001901
Iteration 93/1000 | Loss: 0.00001901
Iteration 94/1000 | Loss: 0.00001901
Iteration 95/1000 | Loss: 0.00001901
Iteration 96/1000 | Loss: 0.00001901
Iteration 97/1000 | Loss: 0.00001901
Iteration 98/1000 | Loss: 0.00001901
Iteration 99/1000 | Loss: 0.00001901
Iteration 100/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.9009610696230084e-05, 1.9009610696230084e-05, 1.9009610696230084e-05, 1.9009610696230084e-05, 1.9009610696230084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9009610696230084e-05

Optimization complete. Final v2v error: 3.8569648265838623 mm

Highest mean error: 4.102636337280273 mm for frame 2

Lowest mean error: 3.6621551513671875 mm for frame 57

Saving results

Total time: 27.231029987335205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_0445/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_0445/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954714
Iteration 2/25 | Loss: 0.00343478
Iteration 3/25 | Loss: 0.00224070
Iteration 4/25 | Loss: 0.00173343
Iteration 5/25 | Loss: 0.00151877
Iteration 6/25 | Loss: 0.00157224
Iteration 7/25 | Loss: 0.00133397
Iteration 8/25 | Loss: 0.00133289
Iteration 9/25 | Loss: 0.00127768
Iteration 10/25 | Loss: 0.00125148
Iteration 11/25 | Loss: 0.00119293
Iteration 12/25 | Loss: 0.00117940
Iteration 13/25 | Loss: 0.00117482
Iteration 14/25 | Loss: 0.00119627
Iteration 15/25 | Loss: 0.00117621
Iteration 16/25 | Loss: 0.00117068
Iteration 17/25 | Loss: 0.00116539
Iteration 18/25 | Loss: 0.00111665
Iteration 19/25 | Loss: 0.00111976
Iteration 20/25 | Loss: 0.00112128
Iteration 21/25 | Loss: 0.00114135
Iteration 22/25 | Loss: 0.00111718
Iteration 23/25 | Loss: 0.00114612
Iteration 24/25 | Loss: 0.00112474
Iteration 25/25 | Loss: 0.00112674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53339148
Iteration 2/25 | Loss: 0.00392815
Iteration 3/25 | Loss: 0.00319895
Iteration 4/25 | Loss: 0.00319894
Iteration 5/25 | Loss: 0.00319894
Iteration 6/25 | Loss: 0.00319894
Iteration 7/25 | Loss: 0.00319894
Iteration 8/25 | Loss: 0.00319894
Iteration 9/25 | Loss: 0.00319894
Iteration 10/25 | Loss: 0.00319894
Iteration 11/25 | Loss: 0.00319894
Iteration 12/25 | Loss: 0.00319894
Iteration 13/25 | Loss: 0.00319894
Iteration 14/25 | Loss: 0.00319894
Iteration 15/25 | Loss: 0.00319894
Iteration 16/25 | Loss: 0.00319894
Iteration 17/25 | Loss: 0.00319894
Iteration 18/25 | Loss: 0.00319894
Iteration 19/25 | Loss: 0.00319894
Iteration 20/25 | Loss: 0.00319894
Iteration 21/25 | Loss: 0.00319894
Iteration 22/25 | Loss: 0.00319894
Iteration 23/25 | Loss: 0.00319894
Iteration 24/25 | Loss: 0.00319894
Iteration 25/25 | Loss: 0.00319894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319894
Iteration 2/1000 | Loss: 0.00084981
Iteration 3/1000 | Loss: 0.00181298
Iteration 4/1000 | Loss: 0.00499384
Iteration 5/1000 | Loss: 0.00055691
Iteration 6/1000 | Loss: 0.00300647
Iteration 7/1000 | Loss: 0.00261703
Iteration 8/1000 | Loss: 0.00640877
Iteration 9/1000 | Loss: 0.00141240
Iteration 10/1000 | Loss: 0.00228633
Iteration 11/1000 | Loss: 0.00260970
Iteration 12/1000 | Loss: 0.00237373
Iteration 13/1000 | Loss: 0.00494058
Iteration 14/1000 | Loss: 0.00277133
Iteration 15/1000 | Loss: 0.00403157
Iteration 16/1000 | Loss: 0.00224322
Iteration 17/1000 | Loss: 0.00122331
Iteration 18/1000 | Loss: 0.00247634
Iteration 19/1000 | Loss: 0.00231980
Iteration 20/1000 | Loss: 0.00160811
Iteration 21/1000 | Loss: 0.00144811
Iteration 22/1000 | Loss: 0.00089872
Iteration 23/1000 | Loss: 0.00062053
Iteration 24/1000 | Loss: 0.00267367
Iteration 25/1000 | Loss: 0.00155878
Iteration 26/1000 | Loss: 0.00180380
Iteration 27/1000 | Loss: 0.00139234
Iteration 28/1000 | Loss: 0.00130342
Iteration 29/1000 | Loss: 0.00112551
Iteration 30/1000 | Loss: 0.00158542
Iteration 31/1000 | Loss: 0.00076327
Iteration 32/1000 | Loss: 0.00217751
Iteration 33/1000 | Loss: 0.00143242
Iteration 34/1000 | Loss: 0.00110424
Iteration 35/1000 | Loss: 0.00102755
Iteration 36/1000 | Loss: 0.00249436
Iteration 37/1000 | Loss: 0.00091954
Iteration 38/1000 | Loss: 0.00219451
Iteration 39/1000 | Loss: 0.00179647
Iteration 40/1000 | Loss: 0.00189537
Iteration 41/1000 | Loss: 0.00121329
Iteration 42/1000 | Loss: 0.00148074
Iteration 43/1000 | Loss: 0.00103646
Iteration 44/1000 | Loss: 0.00293707
Iteration 45/1000 | Loss: 0.00166806
Iteration 46/1000 | Loss: 0.00204173
Iteration 47/1000 | Loss: 0.00103817
Iteration 48/1000 | Loss: 0.00123212
Iteration 49/1000 | Loss: 0.00159841
Iteration 50/1000 | Loss: 0.00196106
Iteration 51/1000 | Loss: 0.00123304
Iteration 52/1000 | Loss: 0.00072924
Iteration 53/1000 | Loss: 0.00083513
Iteration 54/1000 | Loss: 0.00080531
Iteration 55/1000 | Loss: 0.00112645
Iteration 56/1000 | Loss: 0.00061350
Iteration 57/1000 | Loss: 0.00070984
Iteration 58/1000 | Loss: 0.00168305
Iteration 59/1000 | Loss: 0.00211611
Iteration 60/1000 | Loss: 0.00175099
Iteration 61/1000 | Loss: 0.00134823
Iteration 62/1000 | Loss: 0.00108280
Iteration 63/1000 | Loss: 0.00129494
Iteration 64/1000 | Loss: 0.00133578
Iteration 65/1000 | Loss: 0.00108315
Iteration 66/1000 | Loss: 0.00121928
Iteration 67/1000 | Loss: 0.00111912
Iteration 68/1000 | Loss: 0.00049947
Iteration 69/1000 | Loss: 0.00082914
Iteration 70/1000 | Loss: 0.00165491
Iteration 71/1000 | Loss: 0.00260413
Iteration 72/1000 | Loss: 0.00127779
Iteration 73/1000 | Loss: 0.00086448
Iteration 74/1000 | Loss: 0.00026898
Iteration 75/1000 | Loss: 0.00046911
Iteration 76/1000 | Loss: 0.00120522
Iteration 77/1000 | Loss: 0.00113630
Iteration 78/1000 | Loss: 0.00171048
Iteration 79/1000 | Loss: 0.00104522
Iteration 80/1000 | Loss: 0.00123323
Iteration 81/1000 | Loss: 0.00071142
Iteration 82/1000 | Loss: 0.00042997
Iteration 83/1000 | Loss: 0.00061818
Iteration 84/1000 | Loss: 0.00157287
Iteration 85/1000 | Loss: 0.00077022
Iteration 86/1000 | Loss: 0.00060544
Iteration 87/1000 | Loss: 0.00094166
Iteration 88/1000 | Loss: 0.00100907
Iteration 89/1000 | Loss: 0.00028554
Iteration 90/1000 | Loss: 0.00007582
Iteration 91/1000 | Loss: 0.00005934
Iteration 92/1000 | Loss: 0.00034114
Iteration 93/1000 | Loss: 0.00004464
Iteration 94/1000 | Loss: 0.00045496
Iteration 95/1000 | Loss: 0.00003718
Iteration 96/1000 | Loss: 0.00003498
Iteration 97/1000 | Loss: 0.00003278
Iteration 98/1000 | Loss: 0.00003114
Iteration 99/1000 | Loss: 0.00003009
Iteration 100/1000 | Loss: 0.00040903
Iteration 101/1000 | Loss: 0.00039679
Iteration 102/1000 | Loss: 0.00005951
Iteration 103/1000 | Loss: 0.00003049
Iteration 104/1000 | Loss: 0.00002738
Iteration 105/1000 | Loss: 0.00009215
Iteration 106/1000 | Loss: 0.00049122
Iteration 107/1000 | Loss: 0.00004152
Iteration 108/1000 | Loss: 0.00003066
Iteration 109/1000 | Loss: 0.00002606
Iteration 110/1000 | Loss: 0.00002513
Iteration 111/1000 | Loss: 0.00002476
Iteration 112/1000 | Loss: 0.00002457
Iteration 113/1000 | Loss: 0.00002446
Iteration 114/1000 | Loss: 0.00002429
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002418
Iteration 117/1000 | Loss: 0.00002412
Iteration 118/1000 | Loss: 0.00002411
Iteration 119/1000 | Loss: 0.00002408
Iteration 120/1000 | Loss: 0.00002408
Iteration 121/1000 | Loss: 0.00002407
Iteration 122/1000 | Loss: 0.00002407
Iteration 123/1000 | Loss: 0.00002407
Iteration 124/1000 | Loss: 0.00002407
Iteration 125/1000 | Loss: 0.00002407
Iteration 126/1000 | Loss: 0.00002405
Iteration 127/1000 | Loss: 0.00002405
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002403
Iteration 130/1000 | Loss: 0.00002403
Iteration 131/1000 | Loss: 0.00002402
Iteration 132/1000 | Loss: 0.00002401
Iteration 133/1000 | Loss: 0.00002400
Iteration 134/1000 | Loss: 0.00002400
Iteration 135/1000 | Loss: 0.00002395
Iteration 136/1000 | Loss: 0.00002395
Iteration 137/1000 | Loss: 0.00002395
Iteration 138/1000 | Loss: 0.00002395
Iteration 139/1000 | Loss: 0.00002394
Iteration 140/1000 | Loss: 0.00002394
Iteration 141/1000 | Loss: 0.00002394
Iteration 142/1000 | Loss: 0.00002393
Iteration 143/1000 | Loss: 0.00002393
Iteration 144/1000 | Loss: 0.00002393
Iteration 145/1000 | Loss: 0.00002393
Iteration 146/1000 | Loss: 0.00002392
Iteration 147/1000 | Loss: 0.00002392
Iteration 148/1000 | Loss: 0.00002392
Iteration 149/1000 | Loss: 0.00002392
Iteration 150/1000 | Loss: 0.00002391
Iteration 151/1000 | Loss: 0.00002391
Iteration 152/1000 | Loss: 0.00002391
Iteration 153/1000 | Loss: 0.00002391
Iteration 154/1000 | Loss: 0.00002390
Iteration 155/1000 | Loss: 0.00002390
Iteration 156/1000 | Loss: 0.00002389
Iteration 157/1000 | Loss: 0.00002389
Iteration 158/1000 | Loss: 0.00002389
Iteration 159/1000 | Loss: 0.00002388
Iteration 160/1000 | Loss: 0.00002388
Iteration 161/1000 | Loss: 0.00002387
Iteration 162/1000 | Loss: 0.00002387
Iteration 163/1000 | Loss: 0.00002387
Iteration 164/1000 | Loss: 0.00002387
Iteration 165/1000 | Loss: 0.00002387
Iteration 166/1000 | Loss: 0.00002387
Iteration 167/1000 | Loss: 0.00002386
Iteration 168/1000 | Loss: 0.00002386
Iteration 169/1000 | Loss: 0.00002386
Iteration 170/1000 | Loss: 0.00002386
Iteration 171/1000 | Loss: 0.00002386
Iteration 172/1000 | Loss: 0.00002386
Iteration 173/1000 | Loss: 0.00002386
Iteration 174/1000 | Loss: 0.00002386
Iteration 175/1000 | Loss: 0.00002386
Iteration 176/1000 | Loss: 0.00002386
Iteration 177/1000 | Loss: 0.00002386
Iteration 178/1000 | Loss: 0.00002385
Iteration 179/1000 | Loss: 0.00002385
Iteration 180/1000 | Loss: 0.00002385
Iteration 181/1000 | Loss: 0.00002385
Iteration 182/1000 | Loss: 0.00002385
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002385
Iteration 185/1000 | Loss: 0.00002385
Iteration 186/1000 | Loss: 0.00002384
Iteration 187/1000 | Loss: 0.00002384
Iteration 188/1000 | Loss: 0.00002384
Iteration 189/1000 | Loss: 0.00002384
Iteration 190/1000 | Loss: 0.00002384
Iteration 191/1000 | Loss: 0.00002384
Iteration 192/1000 | Loss: 0.00002384
Iteration 193/1000 | Loss: 0.00002384
Iteration 194/1000 | Loss: 0.00002384
Iteration 195/1000 | Loss: 0.00002384
Iteration 196/1000 | Loss: 0.00002384
Iteration 197/1000 | Loss: 0.00002383
Iteration 198/1000 | Loss: 0.00002383
Iteration 199/1000 | Loss: 0.00002383
Iteration 200/1000 | Loss: 0.00002383
Iteration 201/1000 | Loss: 0.00002383
Iteration 202/1000 | Loss: 0.00002383
Iteration 203/1000 | Loss: 0.00002383
Iteration 204/1000 | Loss: 0.00002383
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002383
Iteration 208/1000 | Loss: 0.00002383
Iteration 209/1000 | Loss: 0.00002383
Iteration 210/1000 | Loss: 0.00002383
Iteration 211/1000 | Loss: 0.00002383
Iteration 212/1000 | Loss: 0.00002383
Iteration 213/1000 | Loss: 0.00002383
Iteration 214/1000 | Loss: 0.00002383
Iteration 215/1000 | Loss: 0.00002383
Iteration 216/1000 | Loss: 0.00002383
Iteration 217/1000 | Loss: 0.00002383
Iteration 218/1000 | Loss: 0.00002383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.382734055572655e-05, 2.382734055572655e-05, 2.382734055572655e-05, 2.382734055572655e-05, 2.382734055572655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.382734055572655e-05

Optimization complete. Final v2v error: 3.921064615249634 mm

Highest mean error: 8.917128562927246 mm for frame 26

Lowest mean error: 2.839726686477661 mm for frame 157

Saving results

Total time: 218.86417984962463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00572752
Iteration 2/25 | Loss: 0.00133630
Iteration 3/25 | Loss: 0.00126079
Iteration 4/25 | Loss: 0.00125114
Iteration 5/25 | Loss: 0.00124776
Iteration 6/25 | Loss: 0.00124662
Iteration 7/25 | Loss: 0.00124662
Iteration 8/25 | Loss: 0.00124662
Iteration 9/25 | Loss: 0.00124662
Iteration 10/25 | Loss: 0.00124662
Iteration 11/25 | Loss: 0.00124662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012466202024370432, 0.0012466202024370432, 0.0012466202024370432, 0.0012466202024370432, 0.0012466202024370432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012466202024370432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09124184
Iteration 2/25 | Loss: 0.00180003
Iteration 3/25 | Loss: 0.00180003
Iteration 4/25 | Loss: 0.00180003
Iteration 5/25 | Loss: 0.00180003
Iteration 6/25 | Loss: 0.00180003
Iteration 7/25 | Loss: 0.00180003
Iteration 8/25 | Loss: 0.00180003
Iteration 9/25 | Loss: 0.00180003
Iteration 10/25 | Loss: 0.00180003
Iteration 11/25 | Loss: 0.00180003
Iteration 12/25 | Loss: 0.00180003
Iteration 13/25 | Loss: 0.00180003
Iteration 14/25 | Loss: 0.00180003
Iteration 15/25 | Loss: 0.00180003
Iteration 16/25 | Loss: 0.00180003
Iteration 17/25 | Loss: 0.00180003
Iteration 18/25 | Loss: 0.00180003
Iteration 19/25 | Loss: 0.00180003
Iteration 20/25 | Loss: 0.00180003
Iteration 21/25 | Loss: 0.00180003
Iteration 22/25 | Loss: 0.00180003
Iteration 23/25 | Loss: 0.00180003
Iteration 24/25 | Loss: 0.00180003
Iteration 25/25 | Loss: 0.00180003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180003
Iteration 2/1000 | Loss: 0.00004293
Iteration 3/1000 | Loss: 0.00002340
Iteration 4/1000 | Loss: 0.00002064
Iteration 5/1000 | Loss: 0.00001943
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001775
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001728
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001710
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001699
Iteration 23/1000 | Loss: 0.00001699
Iteration 24/1000 | Loss: 0.00001698
Iteration 25/1000 | Loss: 0.00001698
Iteration 26/1000 | Loss: 0.00001698
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001697
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001695
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001694
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001690
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001689
Iteration 45/1000 | Loss: 0.00001689
Iteration 46/1000 | Loss: 0.00001689
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001688
Iteration 50/1000 | Loss: 0.00001688
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001686
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001685
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.6846055586938746e-05, 1.6846055586938746e-05, 1.6846055586938746e-05, 1.6846055586938746e-05, 1.6846055586938746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6846055586938746e-05

Optimization complete. Final v2v error: 3.590289354324341 mm

Highest mean error: 3.9040029048919678 mm for frame 94

Lowest mean error: 3.250816583633423 mm for frame 0

Saving results

Total time: 30.48279118537903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923768
Iteration 2/25 | Loss: 0.00148662
Iteration 3/25 | Loss: 0.00138276
Iteration 4/25 | Loss: 0.00135439
Iteration 5/25 | Loss: 0.00134437
Iteration 6/25 | Loss: 0.00134260
Iteration 7/25 | Loss: 0.00134230
Iteration 8/25 | Loss: 0.00134230
Iteration 9/25 | Loss: 0.00134230
Iteration 10/25 | Loss: 0.00134230
Iteration 11/25 | Loss: 0.00134230
Iteration 12/25 | Loss: 0.00134230
Iteration 13/25 | Loss: 0.00134230
Iteration 14/25 | Loss: 0.00134230
Iteration 15/25 | Loss: 0.00134230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013422976480796933, 0.0013422976480796933, 0.0013422976480796933, 0.0013422976480796933, 0.0013422976480796933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013422976480796933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19529343
Iteration 2/25 | Loss: 0.00210295
Iteration 3/25 | Loss: 0.00210288
Iteration 4/25 | Loss: 0.00210288
Iteration 5/25 | Loss: 0.00210288
Iteration 6/25 | Loss: 0.00210288
Iteration 7/25 | Loss: 0.00210288
Iteration 8/25 | Loss: 0.00210288
Iteration 9/25 | Loss: 0.00210288
Iteration 10/25 | Loss: 0.00210288
Iteration 11/25 | Loss: 0.00210288
Iteration 12/25 | Loss: 0.00210288
Iteration 13/25 | Loss: 0.00210288
Iteration 14/25 | Loss: 0.00210288
Iteration 15/25 | Loss: 0.00210288
Iteration 16/25 | Loss: 0.00210288
Iteration 17/25 | Loss: 0.00210288
Iteration 18/25 | Loss: 0.00210288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021028798073530197, 0.0021028798073530197, 0.0021028798073530197, 0.0021028798073530197, 0.0021028798073530197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021028798073530197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210288
Iteration 2/1000 | Loss: 0.00011108
Iteration 3/1000 | Loss: 0.00005437
Iteration 4/1000 | Loss: 0.00003932
Iteration 5/1000 | Loss: 0.00003542
Iteration 6/1000 | Loss: 0.00003353
Iteration 7/1000 | Loss: 0.00003254
Iteration 8/1000 | Loss: 0.00003133
Iteration 9/1000 | Loss: 0.00003049
Iteration 10/1000 | Loss: 0.00002981
Iteration 11/1000 | Loss: 0.00002934
Iteration 12/1000 | Loss: 0.00002897
Iteration 13/1000 | Loss: 0.00002872
Iteration 14/1000 | Loss: 0.00002860
Iteration 15/1000 | Loss: 0.00002846
Iteration 16/1000 | Loss: 0.00002844
Iteration 17/1000 | Loss: 0.00002835
Iteration 18/1000 | Loss: 0.00002829
Iteration 19/1000 | Loss: 0.00002829
Iteration 20/1000 | Loss: 0.00002824
Iteration 21/1000 | Loss: 0.00002824
Iteration 22/1000 | Loss: 0.00002822
Iteration 23/1000 | Loss: 0.00002822
Iteration 24/1000 | Loss: 0.00002820
Iteration 25/1000 | Loss: 0.00002818
Iteration 26/1000 | Loss: 0.00002818
Iteration 27/1000 | Loss: 0.00002818
Iteration 28/1000 | Loss: 0.00002817
Iteration 29/1000 | Loss: 0.00002817
Iteration 30/1000 | Loss: 0.00002815
Iteration 31/1000 | Loss: 0.00002815
Iteration 32/1000 | Loss: 0.00002815
Iteration 33/1000 | Loss: 0.00002815
Iteration 34/1000 | Loss: 0.00002814
Iteration 35/1000 | Loss: 0.00002814
Iteration 36/1000 | Loss: 0.00002814
Iteration 37/1000 | Loss: 0.00002814
Iteration 38/1000 | Loss: 0.00002814
Iteration 39/1000 | Loss: 0.00002814
Iteration 40/1000 | Loss: 0.00002814
Iteration 41/1000 | Loss: 0.00002814
Iteration 42/1000 | Loss: 0.00002814
Iteration 43/1000 | Loss: 0.00002814
Iteration 44/1000 | Loss: 0.00002814
Iteration 45/1000 | Loss: 0.00002813
Iteration 46/1000 | Loss: 0.00002813
Iteration 47/1000 | Loss: 0.00002812
Iteration 48/1000 | Loss: 0.00002812
Iteration 49/1000 | Loss: 0.00002812
Iteration 50/1000 | Loss: 0.00002811
Iteration 51/1000 | Loss: 0.00002811
Iteration 52/1000 | Loss: 0.00002811
Iteration 53/1000 | Loss: 0.00002810
Iteration 54/1000 | Loss: 0.00002810
Iteration 55/1000 | Loss: 0.00002810
Iteration 56/1000 | Loss: 0.00002809
Iteration 57/1000 | Loss: 0.00002809
Iteration 58/1000 | Loss: 0.00002809
Iteration 59/1000 | Loss: 0.00002809
Iteration 60/1000 | Loss: 0.00002809
Iteration 61/1000 | Loss: 0.00002809
Iteration 62/1000 | Loss: 0.00002809
Iteration 63/1000 | Loss: 0.00002808
Iteration 64/1000 | Loss: 0.00002808
Iteration 65/1000 | Loss: 0.00002808
Iteration 66/1000 | Loss: 0.00002808
Iteration 67/1000 | Loss: 0.00002807
Iteration 68/1000 | Loss: 0.00002807
Iteration 69/1000 | Loss: 0.00002807
Iteration 70/1000 | Loss: 0.00002807
Iteration 71/1000 | Loss: 0.00002807
Iteration 72/1000 | Loss: 0.00002807
Iteration 73/1000 | Loss: 0.00002807
Iteration 74/1000 | Loss: 0.00002806
Iteration 75/1000 | Loss: 0.00002806
Iteration 76/1000 | Loss: 0.00002806
Iteration 77/1000 | Loss: 0.00002806
Iteration 78/1000 | Loss: 0.00002806
Iteration 79/1000 | Loss: 0.00002805
Iteration 80/1000 | Loss: 0.00002805
Iteration 81/1000 | Loss: 0.00002805
Iteration 82/1000 | Loss: 0.00002805
Iteration 83/1000 | Loss: 0.00002805
Iteration 84/1000 | Loss: 0.00002805
Iteration 85/1000 | Loss: 0.00002805
Iteration 86/1000 | Loss: 0.00002805
Iteration 87/1000 | Loss: 0.00002804
Iteration 88/1000 | Loss: 0.00002804
Iteration 89/1000 | Loss: 0.00002804
Iteration 90/1000 | Loss: 0.00002804
Iteration 91/1000 | Loss: 0.00002804
Iteration 92/1000 | Loss: 0.00002804
Iteration 93/1000 | Loss: 0.00002804
Iteration 94/1000 | Loss: 0.00002803
Iteration 95/1000 | Loss: 0.00002803
Iteration 96/1000 | Loss: 0.00002803
Iteration 97/1000 | Loss: 0.00002803
Iteration 98/1000 | Loss: 0.00002802
Iteration 99/1000 | Loss: 0.00002802
Iteration 100/1000 | Loss: 0.00002802
Iteration 101/1000 | Loss: 0.00002802
Iteration 102/1000 | Loss: 0.00002802
Iteration 103/1000 | Loss: 0.00002802
Iteration 104/1000 | Loss: 0.00002802
Iteration 105/1000 | Loss: 0.00002802
Iteration 106/1000 | Loss: 0.00002802
Iteration 107/1000 | Loss: 0.00002802
Iteration 108/1000 | Loss: 0.00002802
Iteration 109/1000 | Loss: 0.00002802
Iteration 110/1000 | Loss: 0.00002801
Iteration 111/1000 | Loss: 0.00002801
Iteration 112/1000 | Loss: 0.00002801
Iteration 113/1000 | Loss: 0.00002801
Iteration 114/1000 | Loss: 0.00002801
Iteration 115/1000 | Loss: 0.00002801
Iteration 116/1000 | Loss: 0.00002800
Iteration 117/1000 | Loss: 0.00002800
Iteration 118/1000 | Loss: 0.00002800
Iteration 119/1000 | Loss: 0.00002800
Iteration 120/1000 | Loss: 0.00002800
Iteration 121/1000 | Loss: 0.00002800
Iteration 122/1000 | Loss: 0.00002799
Iteration 123/1000 | Loss: 0.00002799
Iteration 124/1000 | Loss: 0.00002799
Iteration 125/1000 | Loss: 0.00002799
Iteration 126/1000 | Loss: 0.00002799
Iteration 127/1000 | Loss: 0.00002799
Iteration 128/1000 | Loss: 0.00002799
Iteration 129/1000 | Loss: 0.00002799
Iteration 130/1000 | Loss: 0.00002799
Iteration 131/1000 | Loss: 0.00002799
Iteration 132/1000 | Loss: 0.00002799
Iteration 133/1000 | Loss: 0.00002799
Iteration 134/1000 | Loss: 0.00002799
Iteration 135/1000 | Loss: 0.00002799
Iteration 136/1000 | Loss: 0.00002799
Iteration 137/1000 | Loss: 0.00002799
Iteration 138/1000 | Loss: 0.00002799
Iteration 139/1000 | Loss: 0.00002799
Iteration 140/1000 | Loss: 0.00002799
Iteration 141/1000 | Loss: 0.00002799
Iteration 142/1000 | Loss: 0.00002799
Iteration 143/1000 | Loss: 0.00002799
Iteration 144/1000 | Loss: 0.00002799
Iteration 145/1000 | Loss: 0.00002799
Iteration 146/1000 | Loss: 0.00002799
Iteration 147/1000 | Loss: 0.00002799
Iteration 148/1000 | Loss: 0.00002799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.7991067327093333e-05, 2.7991067327093333e-05, 2.7991067327093333e-05, 2.7991067327093333e-05, 2.7991067327093333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7991067327093333e-05

Optimization complete. Final v2v error: 4.521230697631836 mm

Highest mean error: 4.877194881439209 mm for frame 114

Lowest mean error: 3.9513189792633057 mm for frame 0

Saving results

Total time: 42.80622386932373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575364
Iteration 2/25 | Loss: 0.00146419
Iteration 3/25 | Loss: 0.00129024
Iteration 4/25 | Loss: 0.00120994
Iteration 5/25 | Loss: 0.00120553
Iteration 6/25 | Loss: 0.00120796
Iteration 7/25 | Loss: 0.00120533
Iteration 8/25 | Loss: 0.00120468
Iteration 9/25 | Loss: 0.00120459
Iteration 10/25 | Loss: 0.00120459
Iteration 11/25 | Loss: 0.00120459
Iteration 12/25 | Loss: 0.00120459
Iteration 13/25 | Loss: 0.00120458
Iteration 14/25 | Loss: 0.00120458
Iteration 15/25 | Loss: 0.00120458
Iteration 16/25 | Loss: 0.00120458
Iteration 17/25 | Loss: 0.00120458
Iteration 18/25 | Loss: 0.00120458
Iteration 19/25 | Loss: 0.00120458
Iteration 20/25 | Loss: 0.00120661
Iteration 21/25 | Loss: 0.00120553
Iteration 22/25 | Loss: 0.00120457
Iteration 23/25 | Loss: 0.00120457
Iteration 24/25 | Loss: 0.00120457
Iteration 25/25 | Loss: 0.00120457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.10523891
Iteration 2/25 | Loss: 0.00155745
Iteration 3/25 | Loss: 0.00154866
Iteration 4/25 | Loss: 0.00154866
Iteration 5/25 | Loss: 0.00154865
Iteration 6/25 | Loss: 0.00154865
Iteration 7/25 | Loss: 0.00154865
Iteration 8/25 | Loss: 0.00154865
Iteration 9/25 | Loss: 0.00154865
Iteration 10/25 | Loss: 0.00154865
Iteration 11/25 | Loss: 0.00154865
Iteration 12/25 | Loss: 0.00154865
Iteration 13/25 | Loss: 0.00154865
Iteration 14/25 | Loss: 0.00154865
Iteration 15/25 | Loss: 0.00154865
Iteration 16/25 | Loss: 0.00154865
Iteration 17/25 | Loss: 0.00154865
Iteration 18/25 | Loss: 0.00154865
Iteration 19/25 | Loss: 0.00154865
Iteration 20/25 | Loss: 0.00154865
Iteration 21/25 | Loss: 0.00154865
Iteration 22/25 | Loss: 0.00154865
Iteration 23/25 | Loss: 0.00154865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015486528864130378, 0.0015486528864130378, 0.0015486528864130378, 0.0015486528864130378, 0.0015486528864130378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015486528864130378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154865
Iteration 2/1000 | Loss: 0.00004972
Iteration 3/1000 | Loss: 0.00002928
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001530
Iteration 8/1000 | Loss: 0.00001486
Iteration 9/1000 | Loss: 0.00001441
Iteration 10/1000 | Loss: 0.00003288
Iteration 11/1000 | Loss: 0.00002347
Iteration 12/1000 | Loss: 0.00001371
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00002212
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001340
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001340
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001308
Iteration 25/1000 | Loss: 0.00001308
Iteration 26/1000 | Loss: 0.00001307
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001305
Iteration 32/1000 | Loss: 0.00001303
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001298
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001291
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001287
Iteration 90/1000 | Loss: 0.00001287
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001287
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001287
Iteration 104/1000 | Loss: 0.00001286
Iteration 105/1000 | Loss: 0.00001286
Iteration 106/1000 | Loss: 0.00001286
Iteration 107/1000 | Loss: 0.00001286
Iteration 108/1000 | Loss: 0.00001285
Iteration 109/1000 | Loss: 0.00001285
Iteration 110/1000 | Loss: 0.00001285
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001282
Iteration 137/1000 | Loss: 0.00001282
Iteration 138/1000 | Loss: 0.00001282
Iteration 139/1000 | Loss: 0.00001282
Iteration 140/1000 | Loss: 0.00001282
Iteration 141/1000 | Loss: 0.00001282
Iteration 142/1000 | Loss: 0.00001282
Iteration 143/1000 | Loss: 0.00001282
Iteration 144/1000 | Loss: 0.00001281
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001281
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001281
Iteration 150/1000 | Loss: 0.00001281
Iteration 151/1000 | Loss: 0.00001281
Iteration 152/1000 | Loss: 0.00001281
Iteration 153/1000 | Loss: 0.00001281
Iteration 154/1000 | Loss: 0.00001281
Iteration 155/1000 | Loss: 0.00001281
Iteration 156/1000 | Loss: 0.00001280
Iteration 157/1000 | Loss: 0.00001280
Iteration 158/1000 | Loss: 0.00001280
Iteration 159/1000 | Loss: 0.00001280
Iteration 160/1000 | Loss: 0.00001280
Iteration 161/1000 | Loss: 0.00001280
Iteration 162/1000 | Loss: 0.00001280
Iteration 163/1000 | Loss: 0.00001280
Iteration 164/1000 | Loss: 0.00001280
Iteration 165/1000 | Loss: 0.00001280
Iteration 166/1000 | Loss: 0.00001280
Iteration 167/1000 | Loss: 0.00001280
Iteration 168/1000 | Loss: 0.00001280
Iteration 169/1000 | Loss: 0.00001280
Iteration 170/1000 | Loss: 0.00001280
Iteration 171/1000 | Loss: 0.00001280
Iteration 172/1000 | Loss: 0.00001280
Iteration 173/1000 | Loss: 0.00001280
Iteration 174/1000 | Loss: 0.00001280
Iteration 175/1000 | Loss: 0.00001280
Iteration 176/1000 | Loss: 0.00001280
Iteration 177/1000 | Loss: 0.00001280
Iteration 178/1000 | Loss: 0.00001280
Iteration 179/1000 | Loss: 0.00001280
Iteration 180/1000 | Loss: 0.00001280
Iteration 181/1000 | Loss: 0.00001280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.2799925571016502e-05, 1.2799925571016502e-05, 1.2799925571016502e-05, 1.2799925571016502e-05, 1.2799925571016502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2799925571016502e-05

Optimization complete. Final v2v error: 3.0220470428466797 mm

Highest mean error: 3.498002052307129 mm for frame 165

Lowest mean error: 2.745661973953247 mm for frame 19

Saving results

Total time: 60.78623652458191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401839
Iteration 2/25 | Loss: 0.00138381
Iteration 3/25 | Loss: 0.00130322
Iteration 4/25 | Loss: 0.00128653
Iteration 5/25 | Loss: 0.00128038
Iteration 6/25 | Loss: 0.00127851
Iteration 7/25 | Loss: 0.00127801
Iteration 8/25 | Loss: 0.00127801
Iteration 9/25 | Loss: 0.00127801
Iteration 10/25 | Loss: 0.00127801
Iteration 11/25 | Loss: 0.00127801
Iteration 12/25 | Loss: 0.00127801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012780078686773777, 0.0012780078686773777, 0.0012780078686773777, 0.0012780078686773777, 0.0012780078686773777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012780078686773777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35471272
Iteration 2/25 | Loss: 0.00211960
Iteration 3/25 | Loss: 0.00211960
Iteration 4/25 | Loss: 0.00211960
Iteration 5/25 | Loss: 0.00211960
Iteration 6/25 | Loss: 0.00211960
Iteration 7/25 | Loss: 0.00211960
Iteration 8/25 | Loss: 0.00211960
Iteration 9/25 | Loss: 0.00211960
Iteration 10/25 | Loss: 0.00211960
Iteration 11/25 | Loss: 0.00211960
Iteration 12/25 | Loss: 0.00211960
Iteration 13/25 | Loss: 0.00211960
Iteration 14/25 | Loss: 0.00211960
Iteration 15/25 | Loss: 0.00211960
Iteration 16/25 | Loss: 0.00211960
Iteration 17/25 | Loss: 0.00211960
Iteration 18/25 | Loss: 0.00211960
Iteration 19/25 | Loss: 0.00211960
Iteration 20/25 | Loss: 0.00211960
Iteration 21/25 | Loss: 0.00211960
Iteration 22/25 | Loss: 0.00211960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002119597978889942, 0.002119597978889942, 0.002119597978889942, 0.002119597978889942, 0.002119597978889942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002119597978889942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211960
Iteration 2/1000 | Loss: 0.00008299
Iteration 3/1000 | Loss: 0.00004501
Iteration 4/1000 | Loss: 0.00003178
Iteration 5/1000 | Loss: 0.00002778
Iteration 6/1000 | Loss: 0.00002606
Iteration 7/1000 | Loss: 0.00002518
Iteration 8/1000 | Loss: 0.00002468
Iteration 9/1000 | Loss: 0.00002426
Iteration 10/1000 | Loss: 0.00002380
Iteration 11/1000 | Loss: 0.00002349
Iteration 12/1000 | Loss: 0.00002331
Iteration 13/1000 | Loss: 0.00002318
Iteration 14/1000 | Loss: 0.00002309
Iteration 15/1000 | Loss: 0.00002307
Iteration 16/1000 | Loss: 0.00002306
Iteration 17/1000 | Loss: 0.00002304
Iteration 18/1000 | Loss: 0.00002299
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002296
Iteration 21/1000 | Loss: 0.00002294
Iteration 22/1000 | Loss: 0.00002294
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002291
Iteration 25/1000 | Loss: 0.00002284
Iteration 26/1000 | Loss: 0.00002283
Iteration 27/1000 | Loss: 0.00002282
Iteration 28/1000 | Loss: 0.00002282
Iteration 29/1000 | Loss: 0.00002281
Iteration 30/1000 | Loss: 0.00002281
Iteration 31/1000 | Loss: 0.00002281
Iteration 32/1000 | Loss: 0.00002280
Iteration 33/1000 | Loss: 0.00002280
Iteration 34/1000 | Loss: 0.00002279
Iteration 35/1000 | Loss: 0.00002279
Iteration 36/1000 | Loss: 0.00002279
Iteration 37/1000 | Loss: 0.00002279
Iteration 38/1000 | Loss: 0.00002279
Iteration 39/1000 | Loss: 0.00002279
Iteration 40/1000 | Loss: 0.00002279
Iteration 41/1000 | Loss: 0.00002279
Iteration 42/1000 | Loss: 0.00002279
Iteration 43/1000 | Loss: 0.00002278
Iteration 44/1000 | Loss: 0.00002278
Iteration 45/1000 | Loss: 0.00002278
Iteration 46/1000 | Loss: 0.00002278
Iteration 47/1000 | Loss: 0.00002277
Iteration 48/1000 | Loss: 0.00002277
Iteration 49/1000 | Loss: 0.00002277
Iteration 50/1000 | Loss: 0.00002276
Iteration 51/1000 | Loss: 0.00002276
Iteration 52/1000 | Loss: 0.00002276
Iteration 53/1000 | Loss: 0.00002275
Iteration 54/1000 | Loss: 0.00002275
Iteration 55/1000 | Loss: 0.00002274
Iteration 56/1000 | Loss: 0.00002274
Iteration 57/1000 | Loss: 0.00002274
Iteration 58/1000 | Loss: 0.00002273
Iteration 59/1000 | Loss: 0.00002273
Iteration 60/1000 | Loss: 0.00002273
Iteration 61/1000 | Loss: 0.00002272
Iteration 62/1000 | Loss: 0.00002272
Iteration 63/1000 | Loss: 0.00002271
Iteration 64/1000 | Loss: 0.00002270
Iteration 65/1000 | Loss: 0.00002270
Iteration 66/1000 | Loss: 0.00002270
Iteration 67/1000 | Loss: 0.00002270
Iteration 68/1000 | Loss: 0.00002270
Iteration 69/1000 | Loss: 0.00002270
Iteration 70/1000 | Loss: 0.00002270
Iteration 71/1000 | Loss: 0.00002269
Iteration 72/1000 | Loss: 0.00002269
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002269
Iteration 75/1000 | Loss: 0.00002268
Iteration 76/1000 | Loss: 0.00002268
Iteration 77/1000 | Loss: 0.00002267
Iteration 78/1000 | Loss: 0.00002267
Iteration 79/1000 | Loss: 0.00002267
Iteration 80/1000 | Loss: 0.00002267
Iteration 81/1000 | Loss: 0.00002266
Iteration 82/1000 | Loss: 0.00002266
Iteration 83/1000 | Loss: 0.00002266
Iteration 84/1000 | Loss: 0.00002266
Iteration 85/1000 | Loss: 0.00002265
Iteration 86/1000 | Loss: 0.00002265
Iteration 87/1000 | Loss: 0.00002265
Iteration 88/1000 | Loss: 0.00002264
Iteration 89/1000 | Loss: 0.00002264
Iteration 90/1000 | Loss: 0.00002264
Iteration 91/1000 | Loss: 0.00002263
Iteration 92/1000 | Loss: 0.00002263
Iteration 93/1000 | Loss: 0.00002263
Iteration 94/1000 | Loss: 0.00002263
Iteration 95/1000 | Loss: 0.00002263
Iteration 96/1000 | Loss: 0.00002263
Iteration 97/1000 | Loss: 0.00002263
Iteration 98/1000 | Loss: 0.00002263
Iteration 99/1000 | Loss: 0.00002263
Iteration 100/1000 | Loss: 0.00002263
Iteration 101/1000 | Loss: 0.00002263
Iteration 102/1000 | Loss: 0.00002263
Iteration 103/1000 | Loss: 0.00002263
Iteration 104/1000 | Loss: 0.00002263
Iteration 105/1000 | Loss: 0.00002263
Iteration 106/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.262510133732576e-05, 2.262510133732576e-05, 2.262510133732576e-05, 2.262510133732576e-05, 2.262510133732576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.262510133732576e-05

Optimization complete. Final v2v error: 3.916764736175537 mm

Highest mean error: 5.331338882446289 mm for frame 76

Lowest mean error: 3.1068270206451416 mm for frame 89

Saving results

Total time: 37.65834307670593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564603
Iteration 2/25 | Loss: 0.00136383
Iteration 3/25 | Loss: 0.00126846
Iteration 4/25 | Loss: 0.00126137
Iteration 5/25 | Loss: 0.00125922
Iteration 6/25 | Loss: 0.00125871
Iteration 7/25 | Loss: 0.00125871
Iteration 8/25 | Loss: 0.00125871
Iteration 9/25 | Loss: 0.00125871
Iteration 10/25 | Loss: 0.00125871
Iteration 11/25 | Loss: 0.00125871
Iteration 12/25 | Loss: 0.00125871
Iteration 13/25 | Loss: 0.00125871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001258713542483747, 0.001258713542483747, 0.001258713542483747, 0.001258713542483747, 0.001258713542483747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001258713542483747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94596839
Iteration 2/25 | Loss: 0.00151779
Iteration 3/25 | Loss: 0.00151779
Iteration 4/25 | Loss: 0.00151779
Iteration 5/25 | Loss: 0.00151779
Iteration 6/25 | Loss: 0.00151779
Iteration 7/25 | Loss: 0.00151779
Iteration 8/25 | Loss: 0.00151778
Iteration 9/25 | Loss: 0.00151778
Iteration 10/25 | Loss: 0.00151778
Iteration 11/25 | Loss: 0.00151778
Iteration 12/25 | Loss: 0.00151778
Iteration 13/25 | Loss: 0.00151778
Iteration 14/25 | Loss: 0.00151778
Iteration 15/25 | Loss: 0.00151778
Iteration 16/25 | Loss: 0.00151778
Iteration 17/25 | Loss: 0.00151778
Iteration 18/25 | Loss: 0.00151778
Iteration 19/25 | Loss: 0.00151778
Iteration 20/25 | Loss: 0.00151778
Iteration 21/25 | Loss: 0.00151778
Iteration 22/25 | Loss: 0.00151778
Iteration 23/25 | Loss: 0.00151778
Iteration 24/25 | Loss: 0.00151778
Iteration 25/25 | Loss: 0.00151778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151778
Iteration 2/1000 | Loss: 0.00004555
Iteration 3/1000 | Loss: 0.00002892
Iteration 4/1000 | Loss: 0.00002117
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001675
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001644
Iteration 13/1000 | Loss: 0.00001640
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001623
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001612
Iteration 23/1000 | Loss: 0.00001607
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00001606
Iteration 26/1000 | Loss: 0.00001601
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001596
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001595
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001595
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001595
Iteration 57/1000 | Loss: 0.00001595
Iteration 58/1000 | Loss: 0.00001595
Iteration 59/1000 | Loss: 0.00001595
Iteration 60/1000 | Loss: 0.00001595
Iteration 61/1000 | Loss: 0.00001595
Iteration 62/1000 | Loss: 0.00001595
Iteration 63/1000 | Loss: 0.00001595
Iteration 64/1000 | Loss: 0.00001595
Iteration 65/1000 | Loss: 0.00001595
Iteration 66/1000 | Loss: 0.00001595
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001595
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001595
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001595
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001595
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.5946581697789952e-05, 1.5946581697789952e-05, 1.5946581697789952e-05, 1.5946581697789952e-05, 1.5946581697789952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5946581697789952e-05

Optimization complete. Final v2v error: 3.320485830307007 mm

Highest mean error: 3.639275550842285 mm for frame 62

Lowest mean error: 2.9165425300598145 mm for frame 9

Saving results

Total time: 30.707584619522095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846377
Iteration 2/25 | Loss: 0.00157881
Iteration 3/25 | Loss: 0.00139451
Iteration 4/25 | Loss: 0.00134929
Iteration 5/25 | Loss: 0.00131400
Iteration 6/25 | Loss: 0.00129891
Iteration 7/25 | Loss: 0.00128754
Iteration 8/25 | Loss: 0.00128477
Iteration 9/25 | Loss: 0.00128425
Iteration 10/25 | Loss: 0.00128419
Iteration 11/25 | Loss: 0.00128419
Iteration 12/25 | Loss: 0.00128419
Iteration 13/25 | Loss: 0.00128419
Iteration 14/25 | Loss: 0.00128419
Iteration 15/25 | Loss: 0.00128419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001284185447730124, 0.001284185447730124, 0.001284185447730124, 0.001284185447730124, 0.001284185447730124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001284185447730124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23905766
Iteration 2/25 | Loss: 0.00220566
Iteration 3/25 | Loss: 0.00220566
Iteration 4/25 | Loss: 0.00220566
Iteration 5/25 | Loss: 0.00220566
Iteration 6/25 | Loss: 0.00220566
Iteration 7/25 | Loss: 0.00220566
Iteration 8/25 | Loss: 0.00220566
Iteration 9/25 | Loss: 0.00220566
Iteration 10/25 | Loss: 0.00220566
Iteration 11/25 | Loss: 0.00220566
Iteration 12/25 | Loss: 0.00220566
Iteration 13/25 | Loss: 0.00220566
Iteration 14/25 | Loss: 0.00220566
Iteration 15/25 | Loss: 0.00220566
Iteration 16/25 | Loss: 0.00220566
Iteration 17/25 | Loss: 0.00220566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022056573070585728, 0.0022056573070585728, 0.0022056573070585728, 0.0022056573070585728, 0.0022056573070585728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022056573070585728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220566
Iteration 2/1000 | Loss: 0.00007281
Iteration 3/1000 | Loss: 0.00003541
Iteration 4/1000 | Loss: 0.00002561
Iteration 5/1000 | Loss: 0.00002223
Iteration 6/1000 | Loss: 0.00002107
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001973
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001852
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001808
Iteration 14/1000 | Loss: 0.00001793
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001780
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001774
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001773
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001769
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001755
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001754
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001753
Iteration 95/1000 | Loss: 0.00001753
Iteration 96/1000 | Loss: 0.00001753
Iteration 97/1000 | Loss: 0.00001753
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001753
Iteration 100/1000 | Loss: 0.00001753
Iteration 101/1000 | Loss: 0.00001753
Iteration 102/1000 | Loss: 0.00001753
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.7527014279039577e-05, 1.7527014279039577e-05, 1.7527014279039577e-05, 1.7527014279039577e-05, 1.7527014279039577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7527014279039577e-05

Optimization complete. Final v2v error: 3.602997303009033 mm

Highest mean error: 4.187895774841309 mm for frame 12

Lowest mean error: 2.949000120162964 mm for frame 239

Saving results

Total time: 51.20456886291504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397139
Iteration 2/25 | Loss: 0.00128973
Iteration 3/25 | Loss: 0.00121647
Iteration 4/25 | Loss: 0.00121094
Iteration 5/25 | Loss: 0.00120944
Iteration 6/25 | Loss: 0.00120944
Iteration 7/25 | Loss: 0.00120944
Iteration 8/25 | Loss: 0.00120944
Iteration 9/25 | Loss: 0.00120944
Iteration 10/25 | Loss: 0.00120944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012094369158148766, 0.0012094369158148766, 0.0012094369158148766, 0.0012094369158148766, 0.0012094369158148766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012094369158148766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32070577
Iteration 2/25 | Loss: 0.00149608
Iteration 3/25 | Loss: 0.00149608
Iteration 4/25 | Loss: 0.00149608
Iteration 5/25 | Loss: 0.00149608
Iteration 6/25 | Loss: 0.00149608
Iteration 7/25 | Loss: 0.00149608
Iteration 8/25 | Loss: 0.00149608
Iteration 9/25 | Loss: 0.00149608
Iteration 10/25 | Loss: 0.00149608
Iteration 11/25 | Loss: 0.00149608
Iteration 12/25 | Loss: 0.00149608
Iteration 13/25 | Loss: 0.00149608
Iteration 14/25 | Loss: 0.00149608
Iteration 15/25 | Loss: 0.00149608
Iteration 16/25 | Loss: 0.00149608
Iteration 17/25 | Loss: 0.00149608
Iteration 18/25 | Loss: 0.00149608
Iteration 19/25 | Loss: 0.00149608
Iteration 20/25 | Loss: 0.00149608
Iteration 21/25 | Loss: 0.00149608
Iteration 22/25 | Loss: 0.00149608
Iteration 23/25 | Loss: 0.00149608
Iteration 24/25 | Loss: 0.00149608
Iteration 25/25 | Loss: 0.00149608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149608
Iteration 2/1000 | Loss: 0.00005039
Iteration 3/1000 | Loss: 0.00002517
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001681
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001538
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001442
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001411
Iteration 19/1000 | Loss: 0.00001411
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001410
Iteration 23/1000 | Loss: 0.00001409
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001398
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001397
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001383
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001383
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001383
Iteration 72/1000 | Loss: 0.00001383
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001382
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001381
Iteration 80/1000 | Loss: 0.00001380
Iteration 81/1000 | Loss: 0.00001380
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001376
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001375
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001375
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001374
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00001372
Iteration 144/1000 | Loss: 0.00001372
Iteration 145/1000 | Loss: 0.00001372
Iteration 146/1000 | Loss: 0.00001372
Iteration 147/1000 | Loss: 0.00001372
Iteration 148/1000 | Loss: 0.00001372
Iteration 149/1000 | Loss: 0.00001372
Iteration 150/1000 | Loss: 0.00001372
Iteration 151/1000 | Loss: 0.00001372
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Iteration 154/1000 | Loss: 0.00001372
Iteration 155/1000 | Loss: 0.00001372
Iteration 156/1000 | Loss: 0.00001372
Iteration 157/1000 | Loss: 0.00001372
Iteration 158/1000 | Loss: 0.00001372
Iteration 159/1000 | Loss: 0.00001372
Iteration 160/1000 | Loss: 0.00001372
Iteration 161/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3724878044740763e-05, 1.3724878044740763e-05, 1.3724878044740763e-05, 1.3724878044740763e-05, 1.3724878044740763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3724878044740763e-05

Optimization complete. Final v2v error: 3.1907572746276855 mm

Highest mean error: 3.959718704223633 mm for frame 81

Lowest mean error: 2.7776453495025635 mm for frame 64

Saving results

Total time: 35.88674783706665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888547
Iteration 2/25 | Loss: 0.00128055
Iteration 3/25 | Loss: 0.00119585
Iteration 4/25 | Loss: 0.00118695
Iteration 5/25 | Loss: 0.00118533
Iteration 6/25 | Loss: 0.00118533
Iteration 7/25 | Loss: 0.00118533
Iteration 8/25 | Loss: 0.00118533
Iteration 9/25 | Loss: 0.00118533
Iteration 10/25 | Loss: 0.00118533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011853317264467478, 0.0011853317264467478, 0.0011853317264467478, 0.0011853317264467478, 0.0011853317264467478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011853317264467478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39797950
Iteration 2/25 | Loss: 0.00145234
Iteration 3/25 | Loss: 0.00145234
Iteration 4/25 | Loss: 0.00145234
Iteration 5/25 | Loss: 0.00145234
Iteration 6/25 | Loss: 0.00145234
Iteration 7/25 | Loss: 0.00145234
Iteration 8/25 | Loss: 0.00145234
Iteration 9/25 | Loss: 0.00145234
Iteration 10/25 | Loss: 0.00145234
Iteration 11/25 | Loss: 0.00145233
Iteration 12/25 | Loss: 0.00145233
Iteration 13/25 | Loss: 0.00145233
Iteration 14/25 | Loss: 0.00145233
Iteration 15/25 | Loss: 0.00145233
Iteration 16/25 | Loss: 0.00145233
Iteration 17/25 | Loss: 0.00145233
Iteration 18/25 | Loss: 0.00145233
Iteration 19/25 | Loss: 0.00145233
Iteration 20/25 | Loss: 0.00145233
Iteration 21/25 | Loss: 0.00145233
Iteration 22/25 | Loss: 0.00145233
Iteration 23/25 | Loss: 0.00145233
Iteration 24/25 | Loss: 0.00145233
Iteration 25/25 | Loss: 0.00145233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145233
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00001743
Iteration 4/1000 | Loss: 0.00001506
Iteration 5/1000 | Loss: 0.00001387
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001249
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001214
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001205
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001198
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001197
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001191
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001181
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.1778225598391145e-05, 1.1778225598391145e-05, 1.1778225598391145e-05, 1.1778225598391145e-05, 1.1778225598391145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1778225598391145e-05

Optimization complete. Final v2v error: 2.8688271045684814 mm

Highest mean error: 3.133779764175415 mm for frame 164

Lowest mean error: 2.6249639987945557 mm for frame 0

Saving results

Total time: 34.43304133415222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837063
Iteration 2/25 | Loss: 0.00162615
Iteration 3/25 | Loss: 0.00133046
Iteration 4/25 | Loss: 0.00130949
Iteration 5/25 | Loss: 0.00130596
Iteration 6/25 | Loss: 0.00130596
Iteration 7/25 | Loss: 0.00130596
Iteration 8/25 | Loss: 0.00130596
Iteration 9/25 | Loss: 0.00130596
Iteration 10/25 | Loss: 0.00130596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013059593038633466, 0.0013059593038633466, 0.0013059593038633466, 0.0013059593038633466, 0.0013059593038633466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013059593038633466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10626662
Iteration 2/25 | Loss: 0.00131953
Iteration 3/25 | Loss: 0.00131950
Iteration 4/25 | Loss: 0.00131950
Iteration 5/25 | Loss: 0.00131950
Iteration 6/25 | Loss: 0.00131950
Iteration 7/25 | Loss: 0.00131950
Iteration 8/25 | Loss: 0.00131950
Iteration 9/25 | Loss: 0.00131950
Iteration 10/25 | Loss: 0.00131950
Iteration 11/25 | Loss: 0.00131950
Iteration 12/25 | Loss: 0.00131950
Iteration 13/25 | Loss: 0.00131950
Iteration 14/25 | Loss: 0.00131950
Iteration 15/25 | Loss: 0.00131950
Iteration 16/25 | Loss: 0.00131950
Iteration 17/25 | Loss: 0.00131950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013194994535297155, 0.0013194994535297155, 0.0013194994535297155, 0.0013194994535297155, 0.0013194994535297155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013194994535297155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131950
Iteration 2/1000 | Loss: 0.00006659
Iteration 3/1000 | Loss: 0.00003632
Iteration 4/1000 | Loss: 0.00002702
Iteration 5/1000 | Loss: 0.00002525
Iteration 6/1000 | Loss: 0.00002387
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002205
Iteration 10/1000 | Loss: 0.00002161
Iteration 11/1000 | Loss: 0.00002128
Iteration 12/1000 | Loss: 0.00002100
Iteration 13/1000 | Loss: 0.00002078
Iteration 14/1000 | Loss: 0.00002078
Iteration 15/1000 | Loss: 0.00002078
Iteration 16/1000 | Loss: 0.00002064
Iteration 17/1000 | Loss: 0.00002058
Iteration 18/1000 | Loss: 0.00002052
Iteration 19/1000 | Loss: 0.00002042
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002033
Iteration 22/1000 | Loss: 0.00002031
Iteration 23/1000 | Loss: 0.00002029
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002027
Iteration 26/1000 | Loss: 0.00002026
Iteration 27/1000 | Loss: 0.00002026
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002025
Iteration 30/1000 | Loss: 0.00002025
Iteration 31/1000 | Loss: 0.00002025
Iteration 32/1000 | Loss: 0.00002025
Iteration 33/1000 | Loss: 0.00002024
Iteration 34/1000 | Loss: 0.00002024
Iteration 35/1000 | Loss: 0.00002023
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002022
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002022
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002021
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002020
Iteration 47/1000 | Loss: 0.00002020
Iteration 48/1000 | Loss: 0.00002020
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00002019
Iteration 53/1000 | Loss: 0.00002019
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002018
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002017
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002016
Iteration 62/1000 | Loss: 0.00002016
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002016
Iteration 65/1000 | Loss: 0.00002016
Iteration 66/1000 | Loss: 0.00002016
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002015
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002015
Iteration 74/1000 | Loss: 0.00002015
Iteration 75/1000 | Loss: 0.00002015
Iteration 76/1000 | Loss: 0.00002015
Iteration 77/1000 | Loss: 0.00002015
Iteration 78/1000 | Loss: 0.00002015
Iteration 79/1000 | Loss: 0.00002014
Iteration 80/1000 | Loss: 0.00002014
Iteration 81/1000 | Loss: 0.00002014
Iteration 82/1000 | Loss: 0.00002014
Iteration 83/1000 | Loss: 0.00002014
Iteration 84/1000 | Loss: 0.00002014
Iteration 85/1000 | Loss: 0.00002013
Iteration 86/1000 | Loss: 0.00002013
Iteration 87/1000 | Loss: 0.00002013
Iteration 88/1000 | Loss: 0.00002013
Iteration 89/1000 | Loss: 0.00002013
Iteration 90/1000 | Loss: 0.00002013
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002011
Iteration 96/1000 | Loss: 0.00002011
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002007
Iteration 110/1000 | Loss: 0.00002007
Iteration 111/1000 | Loss: 0.00002007
Iteration 112/1000 | Loss: 0.00002007
Iteration 113/1000 | Loss: 0.00002007
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002006
Iteration 120/1000 | Loss: 0.00002006
Iteration 121/1000 | Loss: 0.00002006
Iteration 122/1000 | Loss: 0.00002006
Iteration 123/1000 | Loss: 0.00002006
Iteration 124/1000 | Loss: 0.00002006
Iteration 125/1000 | Loss: 0.00002006
Iteration 126/1000 | Loss: 0.00002006
Iteration 127/1000 | Loss: 0.00002005
Iteration 128/1000 | Loss: 0.00002005
Iteration 129/1000 | Loss: 0.00002005
Iteration 130/1000 | Loss: 0.00002005
Iteration 131/1000 | Loss: 0.00002005
Iteration 132/1000 | Loss: 0.00002005
Iteration 133/1000 | Loss: 0.00002005
Iteration 134/1000 | Loss: 0.00002005
Iteration 135/1000 | Loss: 0.00002004
Iteration 136/1000 | Loss: 0.00002004
Iteration 137/1000 | Loss: 0.00002004
Iteration 138/1000 | Loss: 0.00002004
Iteration 139/1000 | Loss: 0.00002004
Iteration 140/1000 | Loss: 0.00002004
Iteration 141/1000 | Loss: 0.00002004
Iteration 142/1000 | Loss: 0.00002004
Iteration 143/1000 | Loss: 0.00002004
Iteration 144/1000 | Loss: 0.00002004
Iteration 145/1000 | Loss: 0.00002004
Iteration 146/1000 | Loss: 0.00002003
Iteration 147/1000 | Loss: 0.00002003
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00002003
Iteration 150/1000 | Loss: 0.00002002
Iteration 151/1000 | Loss: 0.00002002
Iteration 152/1000 | Loss: 0.00002002
Iteration 153/1000 | Loss: 0.00002002
Iteration 154/1000 | Loss: 0.00002002
Iteration 155/1000 | Loss: 0.00002002
Iteration 156/1000 | Loss: 0.00002002
Iteration 157/1000 | Loss: 0.00002002
Iteration 158/1000 | Loss: 0.00002002
Iteration 159/1000 | Loss: 0.00002002
Iteration 160/1000 | Loss: 0.00002001
Iteration 161/1000 | Loss: 0.00002001
Iteration 162/1000 | Loss: 0.00002001
Iteration 163/1000 | Loss: 0.00002001
Iteration 164/1000 | Loss: 0.00002001
Iteration 165/1000 | Loss: 0.00002001
Iteration 166/1000 | Loss: 0.00002001
Iteration 167/1000 | Loss: 0.00002001
Iteration 168/1000 | Loss: 0.00002001
Iteration 169/1000 | Loss: 0.00002001
Iteration 170/1000 | Loss: 0.00002001
Iteration 171/1000 | Loss: 0.00002001
Iteration 172/1000 | Loss: 0.00002001
Iteration 173/1000 | Loss: 0.00002001
Iteration 174/1000 | Loss: 0.00002001
Iteration 175/1000 | Loss: 0.00002000
Iteration 176/1000 | Loss: 0.00002000
Iteration 177/1000 | Loss: 0.00002000
Iteration 178/1000 | Loss: 0.00002000
Iteration 179/1000 | Loss: 0.00002000
Iteration 180/1000 | Loss: 0.00002000
Iteration 181/1000 | Loss: 0.00002000
Iteration 182/1000 | Loss: 0.00002000
Iteration 183/1000 | Loss: 0.00002000
Iteration 184/1000 | Loss: 0.00002000
Iteration 185/1000 | Loss: 0.00002000
Iteration 186/1000 | Loss: 0.00002000
Iteration 187/1000 | Loss: 0.00002000
Iteration 188/1000 | Loss: 0.00002000
Iteration 189/1000 | Loss: 0.00002000
Iteration 190/1000 | Loss: 0.00002000
Iteration 191/1000 | Loss: 0.00002000
Iteration 192/1000 | Loss: 0.00001999
Iteration 193/1000 | Loss: 0.00001999
Iteration 194/1000 | Loss: 0.00001999
Iteration 195/1000 | Loss: 0.00001999
Iteration 196/1000 | Loss: 0.00001999
Iteration 197/1000 | Loss: 0.00001999
Iteration 198/1000 | Loss: 0.00001999
Iteration 199/1000 | Loss: 0.00001999
Iteration 200/1000 | Loss: 0.00001999
Iteration 201/1000 | Loss: 0.00001999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.99917903955793e-05, 1.99917903955793e-05, 1.99917903955793e-05, 1.99917903955793e-05, 1.99917903955793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.99917903955793e-05

Optimization complete. Final v2v error: 3.840153217315674 mm

Highest mean error: 4.464683532714844 mm for frame 137

Lowest mean error: 3.4209673404693604 mm for frame 57

Saving results

Total time: 49.61936569213867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099168
Iteration 2/25 | Loss: 0.01099168
Iteration 3/25 | Loss: 0.01099168
Iteration 4/25 | Loss: 0.01099168
Iteration 5/25 | Loss: 0.01099168
Iteration 6/25 | Loss: 0.01099167
Iteration 7/25 | Loss: 0.01099167
Iteration 8/25 | Loss: 0.01099167
Iteration 9/25 | Loss: 0.01099167
Iteration 10/25 | Loss: 0.01099167
Iteration 11/25 | Loss: 0.01099167
Iteration 12/25 | Loss: 0.01099167
Iteration 13/25 | Loss: 0.01099167
Iteration 14/25 | Loss: 0.01099167
Iteration 15/25 | Loss: 0.01099167
Iteration 16/25 | Loss: 0.01099167
Iteration 17/25 | Loss: 0.01099167
Iteration 18/25 | Loss: 0.01099167
Iteration 19/25 | Loss: 0.01099167
Iteration 20/25 | Loss: 0.01099167
Iteration 21/25 | Loss: 0.01099167
Iteration 22/25 | Loss: 0.01099167
Iteration 23/25 | Loss: 0.01099167
Iteration 24/25 | Loss: 0.01099166
Iteration 25/25 | Loss: 0.01099166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36094141
Iteration 2/25 | Loss: 0.12751621
Iteration 3/25 | Loss: 0.12739493
Iteration 4/25 | Loss: 0.12733036
Iteration 5/25 | Loss: 0.12732114
Iteration 6/25 | Loss: 0.12733142
Iteration 7/25 | Loss: 0.12731759
Iteration 8/25 | Loss: 0.12731759
Iteration 9/25 | Loss: 0.12731756
Iteration 10/25 | Loss: 0.12731756
Iteration 11/25 | Loss: 0.12731756
Iteration 12/25 | Loss: 0.12731756
Iteration 13/25 | Loss: 0.12731756
Iteration 14/25 | Loss: 0.12731756
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.12731756269931793, 0.12731756269931793, 0.12731756269931793, 0.12731756269931793, 0.12731756269931793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12731756269931793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12731756
Iteration 2/1000 | Loss: 0.00097940
Iteration 3/1000 | Loss: 0.00053091
Iteration 4/1000 | Loss: 0.00038097
Iteration 5/1000 | Loss: 0.00024215
Iteration 6/1000 | Loss: 0.00009141
Iteration 7/1000 | Loss: 0.00005499
Iteration 8/1000 | Loss: 0.00004746
Iteration 9/1000 | Loss: 0.00008236
Iteration 10/1000 | Loss: 0.00004438
Iteration 11/1000 | Loss: 0.00004769
Iteration 12/1000 | Loss: 0.00005718
Iteration 13/1000 | Loss: 0.00006874
Iteration 14/1000 | Loss: 0.00005827
Iteration 15/1000 | Loss: 0.00004302
Iteration 16/1000 | Loss: 0.00003898
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00003689
Iteration 19/1000 | Loss: 0.00004756
Iteration 20/1000 | Loss: 0.00003918
Iteration 21/1000 | Loss: 0.00004535
Iteration 22/1000 | Loss: 0.00002335
Iteration 23/1000 | Loss: 0.00002172
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00001794
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00004059
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00002543
Iteration 30/1000 | Loss: 0.00002868
Iteration 31/1000 | Loss: 0.00020138
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00003978
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001328
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001309
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00003603
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.2896666703454684e-05, 1.2896666703454684e-05, 1.2896666703454684e-05, 1.2896666703454684e-05, 1.2896666703454684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2896666703454684e-05

Optimization complete. Final v2v error: 2.8339943885803223 mm

Highest mean error: 9.339237213134766 mm for frame 212

Lowest mean error: 2.457615613937378 mm for frame 51

Saving results

Total time: 73.7097430229187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01193542
Iteration 2/25 | Loss: 0.00194838
Iteration 3/25 | Loss: 0.00150886
Iteration 4/25 | Loss: 0.00147163
Iteration 5/25 | Loss: 0.00146618
Iteration 6/25 | Loss: 0.00146097
Iteration 7/25 | Loss: 0.00146356
Iteration 8/25 | Loss: 0.00146243
Iteration 9/25 | Loss: 0.00145977
Iteration 10/25 | Loss: 0.00146140
Iteration 11/25 | Loss: 0.00145366
Iteration 12/25 | Loss: 0.00145624
Iteration 13/25 | Loss: 0.00145590
Iteration 14/25 | Loss: 0.00145050
Iteration 15/25 | Loss: 0.00144981
Iteration 16/25 | Loss: 0.00144623
Iteration 17/25 | Loss: 0.00144632
Iteration 18/25 | Loss: 0.00144665
Iteration 19/25 | Loss: 0.00144525
Iteration 20/25 | Loss: 0.00144677
Iteration 21/25 | Loss: 0.00144732
Iteration 22/25 | Loss: 0.00144487
Iteration 23/25 | Loss: 0.00144618
Iteration 24/25 | Loss: 0.00144486
Iteration 25/25 | Loss: 0.00144964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28694522
Iteration 2/25 | Loss: 0.00178966
Iteration 3/25 | Loss: 0.00178964
Iteration 4/25 | Loss: 0.00178963
Iteration 5/25 | Loss: 0.00178963
Iteration 6/25 | Loss: 0.00178963
Iteration 7/25 | Loss: 0.00178963
Iteration 8/25 | Loss: 0.00178963
Iteration 9/25 | Loss: 0.00178963
Iteration 10/25 | Loss: 0.00178963
Iteration 11/25 | Loss: 0.00178963
Iteration 12/25 | Loss: 0.00178963
Iteration 13/25 | Loss: 0.00178963
Iteration 14/25 | Loss: 0.00178963
Iteration 15/25 | Loss: 0.00178963
Iteration 16/25 | Loss: 0.00178963
Iteration 17/25 | Loss: 0.00178963
Iteration 18/25 | Loss: 0.00178963
Iteration 19/25 | Loss: 0.00178963
Iteration 20/25 | Loss: 0.00178963
Iteration 21/25 | Loss: 0.00178963
Iteration 22/25 | Loss: 0.00178963
Iteration 23/25 | Loss: 0.00178963
Iteration 24/25 | Loss: 0.00178963
Iteration 25/25 | Loss: 0.00178963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178963
Iteration 2/1000 | Loss: 0.00018131
Iteration 3/1000 | Loss: 0.00094954
Iteration 4/1000 | Loss: 0.00090105
Iteration 5/1000 | Loss: 0.00268810
Iteration 6/1000 | Loss: 0.00146026
Iteration 7/1000 | Loss: 0.00042350
Iteration 8/1000 | Loss: 0.00023811
Iteration 9/1000 | Loss: 0.00077114
Iteration 10/1000 | Loss: 0.00046970
Iteration 11/1000 | Loss: 0.00017943
Iteration 12/1000 | Loss: 0.00027262
Iteration 13/1000 | Loss: 0.00025204
Iteration 14/1000 | Loss: 0.00037302
Iteration 15/1000 | Loss: 0.00113769
Iteration 16/1000 | Loss: 0.00043572
Iteration 17/1000 | Loss: 0.00055120
Iteration 18/1000 | Loss: 0.00029855
Iteration 19/1000 | Loss: 0.00037901
Iteration 20/1000 | Loss: 0.00087029
Iteration 21/1000 | Loss: 0.00033722
Iteration 22/1000 | Loss: 0.00026842
Iteration 23/1000 | Loss: 0.00019381
Iteration 24/1000 | Loss: 0.00012976
Iteration 25/1000 | Loss: 0.00023151
Iteration 26/1000 | Loss: 0.00021978
Iteration 27/1000 | Loss: 0.00023327
Iteration 28/1000 | Loss: 0.00021159
Iteration 29/1000 | Loss: 0.00018031
Iteration 30/1000 | Loss: 0.00031359
Iteration 31/1000 | Loss: 0.00027585
Iteration 32/1000 | Loss: 0.00027795
Iteration 33/1000 | Loss: 0.00022130
Iteration 34/1000 | Loss: 0.00061553
Iteration 35/1000 | Loss: 0.00050856
Iteration 36/1000 | Loss: 0.00026937
Iteration 37/1000 | Loss: 0.00071742
Iteration 38/1000 | Loss: 0.00076157
Iteration 39/1000 | Loss: 0.00034654
Iteration 40/1000 | Loss: 0.00051188
Iteration 41/1000 | Loss: 0.00042096
Iteration 42/1000 | Loss: 0.00067876
Iteration 43/1000 | Loss: 0.00087645
Iteration 44/1000 | Loss: 0.00070834
Iteration 45/1000 | Loss: 0.00075110
Iteration 46/1000 | Loss: 0.00046506
Iteration 47/1000 | Loss: 0.00027086
Iteration 48/1000 | Loss: 0.00029336
Iteration 49/1000 | Loss: 0.00054366
Iteration 50/1000 | Loss: 0.00045359
Iteration 51/1000 | Loss: 0.00050249
Iteration 52/1000 | Loss: 0.00061258
Iteration 53/1000 | Loss: 0.00044861
Iteration 54/1000 | Loss: 0.00028792
Iteration 55/1000 | Loss: 0.00013086
Iteration 56/1000 | Loss: 0.00013827
Iteration 57/1000 | Loss: 0.00028539
Iteration 58/1000 | Loss: 0.00023544
Iteration 59/1000 | Loss: 0.00024082
Iteration 60/1000 | Loss: 0.00032735
Iteration 61/1000 | Loss: 0.00026164
Iteration 62/1000 | Loss: 0.00031966
Iteration 63/1000 | Loss: 0.00055085
Iteration 64/1000 | Loss: 0.00037435
Iteration 65/1000 | Loss: 0.00026599
Iteration 66/1000 | Loss: 0.00044883
Iteration 67/1000 | Loss: 0.00054477
Iteration 68/1000 | Loss: 0.00048605
Iteration 69/1000 | Loss: 0.00073857
Iteration 70/1000 | Loss: 0.00062928
Iteration 71/1000 | Loss: 0.00073579
Iteration 72/1000 | Loss: 0.00056609
Iteration 73/1000 | Loss: 0.00058508
Iteration 74/1000 | Loss: 0.00043133
Iteration 75/1000 | Loss: 0.00074328
Iteration 76/1000 | Loss: 0.00042696
Iteration 77/1000 | Loss: 0.00031853
Iteration 78/1000 | Loss: 0.00027919
Iteration 79/1000 | Loss: 0.00020930
Iteration 80/1000 | Loss: 0.00049243
Iteration 81/1000 | Loss: 0.00062332
Iteration 82/1000 | Loss: 0.00014644
Iteration 83/1000 | Loss: 0.00040218
Iteration 84/1000 | Loss: 0.00050041
Iteration 85/1000 | Loss: 0.00049045
Iteration 86/1000 | Loss: 0.00055555
Iteration 87/1000 | Loss: 0.00068159
Iteration 88/1000 | Loss: 0.00061505
Iteration 89/1000 | Loss: 0.00059764
Iteration 90/1000 | Loss: 0.00080049
Iteration 91/1000 | Loss: 0.00086855
Iteration 92/1000 | Loss: 0.00047621
Iteration 93/1000 | Loss: 0.00032344
Iteration 94/1000 | Loss: 0.00034726
Iteration 95/1000 | Loss: 0.00091107
Iteration 96/1000 | Loss: 0.00049581
Iteration 97/1000 | Loss: 0.00029136
Iteration 98/1000 | Loss: 0.00020178
Iteration 99/1000 | Loss: 0.00010098
Iteration 100/1000 | Loss: 0.00014473
Iteration 101/1000 | Loss: 0.00010627
Iteration 102/1000 | Loss: 0.00015637
Iteration 103/1000 | Loss: 0.00019048
Iteration 104/1000 | Loss: 0.00020604
Iteration 105/1000 | Loss: 0.00012436
Iteration 106/1000 | Loss: 0.00012567
Iteration 107/1000 | Loss: 0.00018652
Iteration 108/1000 | Loss: 0.00019863
Iteration 109/1000 | Loss: 0.00031371
Iteration 110/1000 | Loss: 0.00039886
Iteration 111/1000 | Loss: 0.00035411
Iteration 112/1000 | Loss: 0.00029761
Iteration 113/1000 | Loss: 0.00018222
Iteration 114/1000 | Loss: 0.00043043
Iteration 115/1000 | Loss: 0.00037654
Iteration 116/1000 | Loss: 0.00017221
Iteration 117/1000 | Loss: 0.00034115
Iteration 118/1000 | Loss: 0.00031757
Iteration 119/1000 | Loss: 0.00038294
Iteration 120/1000 | Loss: 0.00026488
Iteration 121/1000 | Loss: 0.00032943
Iteration 122/1000 | Loss: 0.00038784
Iteration 123/1000 | Loss: 0.00037957
Iteration 124/1000 | Loss: 0.00040780
Iteration 125/1000 | Loss: 0.00037426
Iteration 126/1000 | Loss: 0.00037672
Iteration 127/1000 | Loss: 0.00037614
Iteration 128/1000 | Loss: 0.00022839
Iteration 129/1000 | Loss: 0.00035712
Iteration 130/1000 | Loss: 0.00015055
Iteration 131/1000 | Loss: 0.00025866
Iteration 132/1000 | Loss: 0.00017342
Iteration 133/1000 | Loss: 0.00019088
Iteration 134/1000 | Loss: 0.00015262
Iteration 135/1000 | Loss: 0.00009055
Iteration 136/1000 | Loss: 0.00020942
Iteration 137/1000 | Loss: 0.00040212
Iteration 138/1000 | Loss: 0.00031930
Iteration 139/1000 | Loss: 0.00050117
Iteration 140/1000 | Loss: 0.00030788
Iteration 141/1000 | Loss: 0.00046344
Iteration 142/1000 | Loss: 0.00020567
Iteration 143/1000 | Loss: 0.00020333
Iteration 144/1000 | Loss: 0.00022606
Iteration 145/1000 | Loss: 0.00034312
Iteration 146/1000 | Loss: 0.00021636
Iteration 147/1000 | Loss: 0.00037655
Iteration 148/1000 | Loss: 0.00035755
Iteration 149/1000 | Loss: 0.00025630
Iteration 150/1000 | Loss: 0.00026376
Iteration 151/1000 | Loss: 0.00039853
Iteration 152/1000 | Loss: 0.00042718
Iteration 153/1000 | Loss: 0.00011715
Iteration 154/1000 | Loss: 0.00021830
Iteration 155/1000 | Loss: 0.00006873
Iteration 156/1000 | Loss: 0.00042306
Iteration 157/1000 | Loss: 0.00025618
Iteration 158/1000 | Loss: 0.00008125
Iteration 159/1000 | Loss: 0.00013214
Iteration 160/1000 | Loss: 0.00009048
Iteration 161/1000 | Loss: 0.00014802
Iteration 162/1000 | Loss: 0.00011026
Iteration 163/1000 | Loss: 0.00006173
Iteration 164/1000 | Loss: 0.00007934
Iteration 165/1000 | Loss: 0.00010066
Iteration 166/1000 | Loss: 0.00031407
Iteration 167/1000 | Loss: 0.00026742
Iteration 168/1000 | Loss: 0.00009551
Iteration 169/1000 | Loss: 0.00008748
Iteration 170/1000 | Loss: 0.00005821
Iteration 171/1000 | Loss: 0.00020634
Iteration 172/1000 | Loss: 0.00049362
Iteration 173/1000 | Loss: 0.00067515
Iteration 174/1000 | Loss: 0.00033262
Iteration 175/1000 | Loss: 0.00029018
Iteration 176/1000 | Loss: 0.00030050
Iteration 177/1000 | Loss: 0.00005681
Iteration 178/1000 | Loss: 0.00005357
Iteration 179/1000 | Loss: 0.00005107
Iteration 180/1000 | Loss: 0.00004953
Iteration 181/1000 | Loss: 0.00004874
Iteration 182/1000 | Loss: 0.00004823
Iteration 183/1000 | Loss: 0.00007971
Iteration 184/1000 | Loss: 0.00018323
Iteration 185/1000 | Loss: 0.00010895
Iteration 186/1000 | Loss: 0.00039227
Iteration 187/1000 | Loss: 0.00045307
Iteration 188/1000 | Loss: 0.00010729
Iteration 189/1000 | Loss: 0.00012644
Iteration 190/1000 | Loss: 0.00032914
Iteration 191/1000 | Loss: 0.00031376
Iteration 192/1000 | Loss: 0.00036947
Iteration 193/1000 | Loss: 0.00048692
Iteration 194/1000 | Loss: 0.00025662
Iteration 195/1000 | Loss: 0.00020612
Iteration 196/1000 | Loss: 0.00044561
Iteration 197/1000 | Loss: 0.00034022
Iteration 198/1000 | Loss: 0.00037442
Iteration 199/1000 | Loss: 0.00047861
Iteration 200/1000 | Loss: 0.00006973
Iteration 201/1000 | Loss: 0.00020808
Iteration 202/1000 | Loss: 0.00031701
Iteration 203/1000 | Loss: 0.00014562
Iteration 204/1000 | Loss: 0.00019787
Iteration 205/1000 | Loss: 0.00005503
Iteration 206/1000 | Loss: 0.00005192
Iteration 207/1000 | Loss: 0.00004924
Iteration 208/1000 | Loss: 0.00032371
Iteration 209/1000 | Loss: 0.00008464
Iteration 210/1000 | Loss: 0.00031442
Iteration 211/1000 | Loss: 0.00011410
Iteration 212/1000 | Loss: 0.00020538
Iteration 213/1000 | Loss: 0.00019252
Iteration 214/1000 | Loss: 0.00005703
Iteration 215/1000 | Loss: 0.00019986
Iteration 216/1000 | Loss: 0.00005803
Iteration 217/1000 | Loss: 0.00056319
Iteration 218/1000 | Loss: 0.00024852
Iteration 219/1000 | Loss: 0.00014435
Iteration 220/1000 | Loss: 0.00006154
Iteration 221/1000 | Loss: 0.00005557
Iteration 222/1000 | Loss: 0.00056762
Iteration 223/1000 | Loss: 0.00010711
Iteration 224/1000 | Loss: 0.00010268
Iteration 225/1000 | Loss: 0.00006688
Iteration 226/1000 | Loss: 0.00005123
Iteration 227/1000 | Loss: 0.00013378
Iteration 228/1000 | Loss: 0.00010347
Iteration 229/1000 | Loss: 0.00010425
Iteration 230/1000 | Loss: 0.00009086
Iteration 231/1000 | Loss: 0.00016849
Iteration 232/1000 | Loss: 0.00013380
Iteration 233/1000 | Loss: 0.00004415
Iteration 234/1000 | Loss: 0.00049888
Iteration 235/1000 | Loss: 0.00018289
Iteration 236/1000 | Loss: 0.00034332
Iteration 237/1000 | Loss: 0.00010749
Iteration 238/1000 | Loss: 0.00004392
Iteration 239/1000 | Loss: 0.00050635
Iteration 240/1000 | Loss: 0.00027351
Iteration 241/1000 | Loss: 0.00004875
Iteration 242/1000 | Loss: 0.00004558
Iteration 243/1000 | Loss: 0.00004399
Iteration 244/1000 | Loss: 0.00004241
Iteration 245/1000 | Loss: 0.00004121
Iteration 246/1000 | Loss: 0.00004067
Iteration 247/1000 | Loss: 0.00004036
Iteration 248/1000 | Loss: 0.00004008
Iteration 249/1000 | Loss: 0.00003990
Iteration 250/1000 | Loss: 0.00003975
Iteration 251/1000 | Loss: 0.00003969
Iteration 252/1000 | Loss: 0.00003969
Iteration 253/1000 | Loss: 0.00003969
Iteration 254/1000 | Loss: 0.00003969
Iteration 255/1000 | Loss: 0.00003968
Iteration 256/1000 | Loss: 0.00003964
Iteration 257/1000 | Loss: 0.00003962
Iteration 258/1000 | Loss: 0.00003962
Iteration 259/1000 | Loss: 0.00003961
Iteration 260/1000 | Loss: 0.00003960
Iteration 261/1000 | Loss: 0.00003958
Iteration 262/1000 | Loss: 0.00003954
Iteration 263/1000 | Loss: 0.00003954
Iteration 264/1000 | Loss: 0.00003954
Iteration 265/1000 | Loss: 0.00003954
Iteration 266/1000 | Loss: 0.00003954
Iteration 267/1000 | Loss: 0.00003954
Iteration 268/1000 | Loss: 0.00003954
Iteration 269/1000 | Loss: 0.00003953
Iteration 270/1000 | Loss: 0.00003953
Iteration 271/1000 | Loss: 0.00003953
Iteration 272/1000 | Loss: 0.00003953
Iteration 273/1000 | Loss: 0.00003953
Iteration 274/1000 | Loss: 0.00003953
Iteration 275/1000 | Loss: 0.00003953
Iteration 276/1000 | Loss: 0.00003953
Iteration 277/1000 | Loss: 0.00003952
Iteration 278/1000 | Loss: 0.00003950
Iteration 279/1000 | Loss: 0.00003950
Iteration 280/1000 | Loss: 0.00003949
Iteration 281/1000 | Loss: 0.00003949
Iteration 282/1000 | Loss: 0.00003949
Iteration 283/1000 | Loss: 0.00003949
Iteration 284/1000 | Loss: 0.00003949
Iteration 285/1000 | Loss: 0.00003948
Iteration 286/1000 | Loss: 0.00003948
Iteration 287/1000 | Loss: 0.00003948
Iteration 288/1000 | Loss: 0.00003947
Iteration 289/1000 | Loss: 0.00003947
Iteration 290/1000 | Loss: 0.00003947
Iteration 291/1000 | Loss: 0.00003947
Iteration 292/1000 | Loss: 0.00003947
Iteration 293/1000 | Loss: 0.00003947
Iteration 294/1000 | Loss: 0.00003947
Iteration 295/1000 | Loss: 0.00003946
Iteration 296/1000 | Loss: 0.00003946
Iteration 297/1000 | Loss: 0.00003946
Iteration 298/1000 | Loss: 0.00003946
Iteration 299/1000 | Loss: 0.00003946
Iteration 300/1000 | Loss: 0.00003946
Iteration 301/1000 | Loss: 0.00003945
Iteration 302/1000 | Loss: 0.00003945
Iteration 303/1000 | Loss: 0.00003944
Iteration 304/1000 | Loss: 0.00003944
Iteration 305/1000 | Loss: 0.00003944
Iteration 306/1000 | Loss: 0.00003944
Iteration 307/1000 | Loss: 0.00003944
Iteration 308/1000 | Loss: 0.00003944
Iteration 309/1000 | Loss: 0.00003943
Iteration 310/1000 | Loss: 0.00003943
Iteration 311/1000 | Loss: 0.00003943
Iteration 312/1000 | Loss: 0.00003943
Iteration 313/1000 | Loss: 0.00003943
Iteration 314/1000 | Loss: 0.00003943
Iteration 315/1000 | Loss: 0.00003943
Iteration 316/1000 | Loss: 0.00003943
Iteration 317/1000 | Loss: 0.00003942
Iteration 318/1000 | Loss: 0.00003942
Iteration 319/1000 | Loss: 0.00003942
Iteration 320/1000 | Loss: 0.00003942
Iteration 321/1000 | Loss: 0.00003942
Iteration 322/1000 | Loss: 0.00003942
Iteration 323/1000 | Loss: 0.00003942
Iteration 324/1000 | Loss: 0.00003942
Iteration 325/1000 | Loss: 0.00003942
Iteration 326/1000 | Loss: 0.00003942
Iteration 327/1000 | Loss: 0.00003942
Iteration 328/1000 | Loss: 0.00003942
Iteration 329/1000 | Loss: 0.00003942
Iteration 330/1000 | Loss: 0.00003942
Iteration 331/1000 | Loss: 0.00003942
Iteration 332/1000 | Loss: 0.00003942
Iteration 333/1000 | Loss: 0.00003942
Iteration 334/1000 | Loss: 0.00003941
Iteration 335/1000 | Loss: 0.00003941
Iteration 336/1000 | Loss: 0.00003941
Iteration 337/1000 | Loss: 0.00003941
Iteration 338/1000 | Loss: 0.00003941
Iteration 339/1000 | Loss: 0.00003941
Iteration 340/1000 | Loss: 0.00003941
Iteration 341/1000 | Loss: 0.00003941
Iteration 342/1000 | Loss: 0.00003941
Iteration 343/1000 | Loss: 0.00003941
Iteration 344/1000 | Loss: 0.00003941
Iteration 345/1000 | Loss: 0.00003940
Iteration 346/1000 | Loss: 0.00003940
Iteration 347/1000 | Loss: 0.00003940
Iteration 348/1000 | Loss: 0.00003940
Iteration 349/1000 | Loss: 0.00003940
Iteration 350/1000 | Loss: 0.00003940
Iteration 351/1000 | Loss: 0.00003940
Iteration 352/1000 | Loss: 0.00003939
Iteration 353/1000 | Loss: 0.00003939
Iteration 354/1000 | Loss: 0.00003939
Iteration 355/1000 | Loss: 0.00003939
Iteration 356/1000 | Loss: 0.00003938
Iteration 357/1000 | Loss: 0.00003938
Iteration 358/1000 | Loss: 0.00003938
Iteration 359/1000 | Loss: 0.00003938
Iteration 360/1000 | Loss: 0.00003938
Iteration 361/1000 | Loss: 0.00003938
Iteration 362/1000 | Loss: 0.00003938
Iteration 363/1000 | Loss: 0.00003938
Iteration 364/1000 | Loss: 0.00003938
Iteration 365/1000 | Loss: 0.00003938
Iteration 366/1000 | Loss: 0.00003937
Iteration 367/1000 | Loss: 0.00003937
Iteration 368/1000 | Loss: 0.00003937
Iteration 369/1000 | Loss: 0.00003937
Iteration 370/1000 | Loss: 0.00003937
Iteration 371/1000 | Loss: 0.00003937
Iteration 372/1000 | Loss: 0.00003937
Iteration 373/1000 | Loss: 0.00003937
Iteration 374/1000 | Loss: 0.00003936
Iteration 375/1000 | Loss: 0.00003936
Iteration 376/1000 | Loss: 0.00003936
Iteration 377/1000 | Loss: 0.00003936
Iteration 378/1000 | Loss: 0.00003936
Iteration 379/1000 | Loss: 0.00003936
Iteration 380/1000 | Loss: 0.00003936
Iteration 381/1000 | Loss: 0.00003936
Iteration 382/1000 | Loss: 0.00003935
Iteration 383/1000 | Loss: 0.00003935
Iteration 384/1000 | Loss: 0.00003935
Iteration 385/1000 | Loss: 0.00003935
Iteration 386/1000 | Loss: 0.00003935
Iteration 387/1000 | Loss: 0.00003934
Iteration 388/1000 | Loss: 0.00003934
Iteration 389/1000 | Loss: 0.00003934
Iteration 390/1000 | Loss: 0.00003934
Iteration 391/1000 | Loss: 0.00003934
Iteration 392/1000 | Loss: 0.00003934
Iteration 393/1000 | Loss: 0.00003934
Iteration 394/1000 | Loss: 0.00003934
Iteration 395/1000 | Loss: 0.00003934
Iteration 396/1000 | Loss: 0.00003934
Iteration 397/1000 | Loss: 0.00003933
Iteration 398/1000 | Loss: 0.00003933
Iteration 399/1000 | Loss: 0.00003933
Iteration 400/1000 | Loss: 0.00003933
Iteration 401/1000 | Loss: 0.00003933
Iteration 402/1000 | Loss: 0.00003933
Iteration 403/1000 | Loss: 0.00003932
Iteration 404/1000 | Loss: 0.00003932
Iteration 405/1000 | Loss: 0.00003932
Iteration 406/1000 | Loss: 0.00003932
Iteration 407/1000 | Loss: 0.00003932
Iteration 408/1000 | Loss: 0.00003932
Iteration 409/1000 | Loss: 0.00003932
Iteration 410/1000 | Loss: 0.00003932
Iteration 411/1000 | Loss: 0.00003932
Iteration 412/1000 | Loss: 0.00003932
Iteration 413/1000 | Loss: 0.00003931
Iteration 414/1000 | Loss: 0.00003931
Iteration 415/1000 | Loss: 0.00003931
Iteration 416/1000 | Loss: 0.00003931
Iteration 417/1000 | Loss: 0.00003931
Iteration 418/1000 | Loss: 0.00003931
Iteration 419/1000 | Loss: 0.00003931
Iteration 420/1000 | Loss: 0.00003931
Iteration 421/1000 | Loss: 0.00003931
Iteration 422/1000 | Loss: 0.00003930
Iteration 423/1000 | Loss: 0.00003930
Iteration 424/1000 | Loss: 0.00003930
Iteration 425/1000 | Loss: 0.00003930
Iteration 426/1000 | Loss: 0.00003929
Iteration 427/1000 | Loss: 0.00003929
Iteration 428/1000 | Loss: 0.00003929
Iteration 429/1000 | Loss: 0.00003929
Iteration 430/1000 | Loss: 0.00003929
Iteration 431/1000 | Loss: 0.00003929
Iteration 432/1000 | Loss: 0.00003929
Iteration 433/1000 | Loss: 0.00003929
Iteration 434/1000 | Loss: 0.00003929
Iteration 435/1000 | Loss: 0.00003929
Iteration 436/1000 | Loss: 0.00003929
Iteration 437/1000 | Loss: 0.00003929
Iteration 438/1000 | Loss: 0.00003928
Iteration 439/1000 | Loss: 0.00003928
Iteration 440/1000 | Loss: 0.00003928
Iteration 441/1000 | Loss: 0.00003928
Iteration 442/1000 | Loss: 0.00003928
Iteration 443/1000 | Loss: 0.00003928
Iteration 444/1000 | Loss: 0.00003928
Iteration 445/1000 | Loss: 0.00003928
Iteration 446/1000 | Loss: 0.00003928
Iteration 447/1000 | Loss: 0.00003928
Iteration 448/1000 | Loss: 0.00003928
Iteration 449/1000 | Loss: 0.00003927
Iteration 450/1000 | Loss: 0.00003927
Iteration 451/1000 | Loss: 0.00003927
Iteration 452/1000 | Loss: 0.00003927
Iteration 453/1000 | Loss: 0.00003927
Iteration 454/1000 | Loss: 0.00003927
Iteration 455/1000 | Loss: 0.00003927
Iteration 456/1000 | Loss: 0.00003926
Iteration 457/1000 | Loss: 0.00003926
Iteration 458/1000 | Loss: 0.00003926
Iteration 459/1000 | Loss: 0.00003926
Iteration 460/1000 | Loss: 0.00003926
Iteration 461/1000 | Loss: 0.00003926
Iteration 462/1000 | Loss: 0.00003926
Iteration 463/1000 | Loss: 0.00003926
Iteration 464/1000 | Loss: 0.00003926
Iteration 465/1000 | Loss: 0.00003925
Iteration 466/1000 | Loss: 0.00003925
Iteration 467/1000 | Loss: 0.00003925
Iteration 468/1000 | Loss: 0.00003925
Iteration 469/1000 | Loss: 0.00003925
Iteration 470/1000 | Loss: 0.00003924
Iteration 471/1000 | Loss: 0.00003924
Iteration 472/1000 | Loss: 0.00003924
Iteration 473/1000 | Loss: 0.00003923
Iteration 474/1000 | Loss: 0.00003923
Iteration 475/1000 | Loss: 0.00003923
Iteration 476/1000 | Loss: 0.00003922
Iteration 477/1000 | Loss: 0.00003922
Iteration 478/1000 | Loss: 0.00003922
Iteration 479/1000 | Loss: 0.00003922
Iteration 480/1000 | Loss: 0.00003921
Iteration 481/1000 | Loss: 0.00003920
Iteration 482/1000 | Loss: 0.00003920
Iteration 483/1000 | Loss: 0.00003920
Iteration 484/1000 | Loss: 0.00003920
Iteration 485/1000 | Loss: 0.00003920
Iteration 486/1000 | Loss: 0.00003920
Iteration 487/1000 | Loss: 0.00003920
Iteration 488/1000 | Loss: 0.00003920
Iteration 489/1000 | Loss: 0.00003919
Iteration 490/1000 | Loss: 0.00003919
Iteration 491/1000 | Loss: 0.00003918
Iteration 492/1000 | Loss: 0.00003918
Iteration 493/1000 | Loss: 0.00003918
Iteration 494/1000 | Loss: 0.00003918
Iteration 495/1000 | Loss: 0.00003918
Iteration 496/1000 | Loss: 0.00003918
Iteration 497/1000 | Loss: 0.00003918
Iteration 498/1000 | Loss: 0.00003918
Iteration 499/1000 | Loss: 0.00003918
Iteration 500/1000 | Loss: 0.00003917
Iteration 501/1000 | Loss: 0.00003917
Iteration 502/1000 | Loss: 0.00003917
Iteration 503/1000 | Loss: 0.00003917
Iteration 504/1000 | Loss: 0.00003917
Iteration 505/1000 | Loss: 0.00003917
Iteration 506/1000 | Loss: 0.00003917
Iteration 507/1000 | Loss: 0.00003917
Iteration 508/1000 | Loss: 0.00003917
Iteration 509/1000 | Loss: 0.00003917
Iteration 510/1000 | Loss: 0.00003917
Iteration 511/1000 | Loss: 0.00003917
Iteration 512/1000 | Loss: 0.00003917
Iteration 513/1000 | Loss: 0.00003916
Iteration 514/1000 | Loss: 0.00003916
Iteration 515/1000 | Loss: 0.00003916
Iteration 516/1000 | Loss: 0.00003916
Iteration 517/1000 | Loss: 0.00003916
Iteration 518/1000 | Loss: 0.00003916
Iteration 519/1000 | Loss: 0.00003916
Iteration 520/1000 | Loss: 0.00003916
Iteration 521/1000 | Loss: 0.00003916
Iteration 522/1000 | Loss: 0.00003916
Iteration 523/1000 | Loss: 0.00003916
Iteration 524/1000 | Loss: 0.00003916
Iteration 525/1000 | Loss: 0.00003916
Iteration 526/1000 | Loss: 0.00003916
Iteration 527/1000 | Loss: 0.00003916
Iteration 528/1000 | Loss: 0.00003916
Iteration 529/1000 | Loss: 0.00003916
Iteration 530/1000 | Loss: 0.00003916
Iteration 531/1000 | Loss: 0.00003916
Iteration 532/1000 | Loss: 0.00003916
Iteration 533/1000 | Loss: 0.00003916
Iteration 534/1000 | Loss: 0.00003916
Iteration 535/1000 | Loss: 0.00003916
Iteration 536/1000 | Loss: 0.00003916
Iteration 537/1000 | Loss: 0.00003916
Iteration 538/1000 | Loss: 0.00003916
Iteration 539/1000 | Loss: 0.00003916
Iteration 540/1000 | Loss: 0.00003916
Iteration 541/1000 | Loss: 0.00003916
Iteration 542/1000 | Loss: 0.00003916
Iteration 543/1000 | Loss: 0.00003916
Iteration 544/1000 | Loss: 0.00003916
Iteration 545/1000 | Loss: 0.00003916
Iteration 546/1000 | Loss: 0.00003916
Iteration 547/1000 | Loss: 0.00003916
Iteration 548/1000 | Loss: 0.00003916
Iteration 549/1000 | Loss: 0.00003916
Iteration 550/1000 | Loss: 0.00003916
Iteration 551/1000 | Loss: 0.00003916
Iteration 552/1000 | Loss: 0.00003916
Iteration 553/1000 | Loss: 0.00003916
Iteration 554/1000 | Loss: 0.00003916
Iteration 555/1000 | Loss: 0.00003916
Iteration 556/1000 | Loss: 0.00003916
Iteration 557/1000 | Loss: 0.00003916
Iteration 558/1000 | Loss: 0.00003916
Iteration 559/1000 | Loss: 0.00003916
Iteration 560/1000 | Loss: 0.00003916
Iteration 561/1000 | Loss: 0.00003916
Iteration 562/1000 | Loss: 0.00003916
Iteration 563/1000 | Loss: 0.00003916
Iteration 564/1000 | Loss: 0.00003916
Iteration 565/1000 | Loss: 0.00003916
Iteration 566/1000 | Loss: 0.00003916
Iteration 567/1000 | Loss: 0.00003916
Iteration 568/1000 | Loss: 0.00003916
Iteration 569/1000 | Loss: 0.00003916
Iteration 570/1000 | Loss: 0.00003916
Iteration 571/1000 | Loss: 0.00003916
Iteration 572/1000 | Loss: 0.00003916
Iteration 573/1000 | Loss: 0.00003916
Iteration 574/1000 | Loss: 0.00003916
Iteration 575/1000 | Loss: 0.00003916
Iteration 576/1000 | Loss: 0.00003916
Iteration 577/1000 | Loss: 0.00003916
Iteration 578/1000 | Loss: 0.00003916
Iteration 579/1000 | Loss: 0.00003916
Iteration 580/1000 | Loss: 0.00003916
Iteration 581/1000 | Loss: 0.00003916
Iteration 582/1000 | Loss: 0.00003916
Iteration 583/1000 | Loss: 0.00003916
Iteration 584/1000 | Loss: 0.00003916
Iteration 585/1000 | Loss: 0.00003916
Iteration 586/1000 | Loss: 0.00003916
Iteration 587/1000 | Loss: 0.00003916
Iteration 588/1000 | Loss: 0.00003916
Iteration 589/1000 | Loss: 0.00003916
Iteration 590/1000 | Loss: 0.00003916
Iteration 591/1000 | Loss: 0.00003916
Iteration 592/1000 | Loss: 0.00003916
Iteration 593/1000 | Loss: 0.00003916
Iteration 594/1000 | Loss: 0.00003916
Iteration 595/1000 | Loss: 0.00003916
Iteration 596/1000 | Loss: 0.00003916
Iteration 597/1000 | Loss: 0.00003916
Iteration 598/1000 | Loss: 0.00003916
Iteration 599/1000 | Loss: 0.00003916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 599. Stopping optimization.
Last 5 losses: [3.916172499884851e-05, 3.916172499884851e-05, 3.916172499884851e-05, 3.916172499884851e-05, 3.916172499884851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.916172499884851e-05

Optimization complete. Final v2v error: 4.940803527832031 mm

Highest mean error: 6.536774158477783 mm for frame 88

Lowest mean error: 3.8439040184020996 mm for frame 0

Saving results

Total time: 486.4332058429718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086093
Iteration 2/25 | Loss: 0.00209536
Iteration 3/25 | Loss: 0.00162984
Iteration 4/25 | Loss: 0.00156279
Iteration 5/25 | Loss: 0.00153864
Iteration 6/25 | Loss: 0.00149495
Iteration 7/25 | Loss: 0.00145406
Iteration 8/25 | Loss: 0.00139249
Iteration 9/25 | Loss: 0.00140840
Iteration 10/25 | Loss: 0.00134659
Iteration 11/25 | Loss: 0.00132692
Iteration 12/25 | Loss: 0.00132905
Iteration 13/25 | Loss: 0.00133564
Iteration 14/25 | Loss: 0.00133326
Iteration 15/25 | Loss: 0.00132319
Iteration 16/25 | Loss: 0.00132824
Iteration 17/25 | Loss: 0.00132674
Iteration 18/25 | Loss: 0.00133097
Iteration 19/25 | Loss: 0.00132348
Iteration 20/25 | Loss: 0.00132501
Iteration 21/25 | Loss: 0.00132155
Iteration 22/25 | Loss: 0.00132408
Iteration 23/25 | Loss: 0.00132126
Iteration 24/25 | Loss: 0.00131529
Iteration 25/25 | Loss: 0.00130492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37599373
Iteration 2/25 | Loss: 0.00186564
Iteration 3/25 | Loss: 0.00186564
Iteration 4/25 | Loss: 0.00186564
Iteration 5/25 | Loss: 0.00186564
Iteration 6/25 | Loss: 0.00186564
Iteration 7/25 | Loss: 0.00186564
Iteration 8/25 | Loss: 0.00186564
Iteration 9/25 | Loss: 0.00186564
Iteration 10/25 | Loss: 0.00186564
Iteration 11/25 | Loss: 0.00186564
Iteration 12/25 | Loss: 0.00186564
Iteration 13/25 | Loss: 0.00186564
Iteration 14/25 | Loss: 0.00186564
Iteration 15/25 | Loss: 0.00186564
Iteration 16/25 | Loss: 0.00186564
Iteration 17/25 | Loss: 0.00186564
Iteration 18/25 | Loss: 0.00186564
Iteration 19/25 | Loss: 0.00186564
Iteration 20/25 | Loss: 0.00186564
Iteration 21/25 | Loss: 0.00186564
Iteration 22/25 | Loss: 0.00186564
Iteration 23/25 | Loss: 0.00186564
Iteration 24/25 | Loss: 0.00186564
Iteration 25/25 | Loss: 0.00186564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186564
Iteration 2/1000 | Loss: 0.00010955
Iteration 3/1000 | Loss: 0.00016326
Iteration 4/1000 | Loss: 0.00031147
Iteration 5/1000 | Loss: 0.00015602
Iteration 6/1000 | Loss: 0.00029639
Iteration 7/1000 | Loss: 0.00018619
Iteration 8/1000 | Loss: 0.00030054
Iteration 9/1000 | Loss: 0.00017417
Iteration 10/1000 | Loss: 0.00029336
Iteration 11/1000 | Loss: 0.00029850
Iteration 12/1000 | Loss: 0.00035901
Iteration 13/1000 | Loss: 0.00046444
Iteration 14/1000 | Loss: 0.00010546
Iteration 15/1000 | Loss: 0.00036947
Iteration 16/1000 | Loss: 0.00046609
Iteration 17/1000 | Loss: 0.00050210
Iteration 18/1000 | Loss: 0.00056783
Iteration 19/1000 | Loss: 0.00042417
Iteration 20/1000 | Loss: 0.00042494
Iteration 21/1000 | Loss: 0.00019297
Iteration 22/1000 | Loss: 0.00019153
Iteration 23/1000 | Loss: 0.00040329
Iteration 24/1000 | Loss: 0.00025196
Iteration 25/1000 | Loss: 0.00035509
Iteration 26/1000 | Loss: 0.00040818
Iteration 27/1000 | Loss: 0.00038490
Iteration 28/1000 | Loss: 0.00031363
Iteration 29/1000 | Loss: 0.00018077
Iteration 30/1000 | Loss: 0.00026458
Iteration 31/1000 | Loss: 0.00027662
Iteration 32/1000 | Loss: 0.00017901
Iteration 33/1000 | Loss: 0.00016033
Iteration 34/1000 | Loss: 0.00022944
Iteration 35/1000 | Loss: 0.00029461
Iteration 36/1000 | Loss: 0.00025679
Iteration 37/1000 | Loss: 0.00027736
Iteration 38/1000 | Loss: 0.00040730
Iteration 39/1000 | Loss: 0.00019897
Iteration 40/1000 | Loss: 0.00023923
Iteration 41/1000 | Loss: 0.00024632
Iteration 42/1000 | Loss: 0.00022832
Iteration 43/1000 | Loss: 0.00023548
Iteration 44/1000 | Loss: 0.00030902
Iteration 45/1000 | Loss: 0.00021371
Iteration 46/1000 | Loss: 0.00011669
Iteration 47/1000 | Loss: 0.00016410
Iteration 48/1000 | Loss: 0.00018858
Iteration 49/1000 | Loss: 0.00024493
Iteration 50/1000 | Loss: 0.00023698
Iteration 51/1000 | Loss: 0.00025736
Iteration 52/1000 | Loss: 0.00030256
Iteration 53/1000 | Loss: 0.00029644
Iteration 54/1000 | Loss: 0.00030047
Iteration 55/1000 | Loss: 0.00031465
Iteration 56/1000 | Loss: 0.00021544
Iteration 57/1000 | Loss: 0.00017723
Iteration 58/1000 | Loss: 0.00030173
Iteration 59/1000 | Loss: 0.00027384
Iteration 60/1000 | Loss: 0.00046192
Iteration 61/1000 | Loss: 0.00024781
Iteration 62/1000 | Loss: 0.00026256
Iteration 63/1000 | Loss: 0.00011835
Iteration 64/1000 | Loss: 0.00042409
Iteration 65/1000 | Loss: 0.00025362
Iteration 66/1000 | Loss: 0.00039119
Iteration 67/1000 | Loss: 0.00024715
Iteration 68/1000 | Loss: 0.00050172
Iteration 69/1000 | Loss: 0.00012626
Iteration 70/1000 | Loss: 0.00024699
Iteration 71/1000 | Loss: 0.00023263
Iteration 72/1000 | Loss: 0.00024302
Iteration 73/1000 | Loss: 0.00042078
Iteration 74/1000 | Loss: 0.00025814
Iteration 75/1000 | Loss: 0.00044039
Iteration 76/1000 | Loss: 0.00010941
Iteration 77/1000 | Loss: 0.00071330
Iteration 78/1000 | Loss: 0.00122302
Iteration 79/1000 | Loss: 0.00061607
Iteration 80/1000 | Loss: 0.00005540
Iteration 81/1000 | Loss: 0.00019562
Iteration 82/1000 | Loss: 0.00012033
Iteration 83/1000 | Loss: 0.00009270
Iteration 84/1000 | Loss: 0.00031169
Iteration 85/1000 | Loss: 0.00021267
Iteration 86/1000 | Loss: 0.00059310
Iteration 87/1000 | Loss: 0.00052691
Iteration 88/1000 | Loss: 0.00014914
Iteration 89/1000 | Loss: 0.00005267
Iteration 90/1000 | Loss: 0.00005786
Iteration 91/1000 | Loss: 0.00007617
Iteration 92/1000 | Loss: 0.00031925
Iteration 93/1000 | Loss: 0.00019244
Iteration 94/1000 | Loss: 0.00024161
Iteration 95/1000 | Loss: 0.00018889
Iteration 96/1000 | Loss: 0.00018315
Iteration 97/1000 | Loss: 0.00010205
Iteration 98/1000 | Loss: 0.00023720
Iteration 99/1000 | Loss: 0.00016582
Iteration 100/1000 | Loss: 0.00018226
Iteration 101/1000 | Loss: 0.00025012
Iteration 102/1000 | Loss: 0.00018245
Iteration 103/1000 | Loss: 0.00027394
Iteration 104/1000 | Loss: 0.00031523
Iteration 105/1000 | Loss: 0.00028855
Iteration 106/1000 | Loss: 0.00041624
Iteration 107/1000 | Loss: 0.00017173
Iteration 108/1000 | Loss: 0.00028101
Iteration 109/1000 | Loss: 0.00025302
Iteration 110/1000 | Loss: 0.00010321
Iteration 111/1000 | Loss: 0.00020699
Iteration 112/1000 | Loss: 0.00030721
Iteration 113/1000 | Loss: 0.00027055
Iteration 114/1000 | Loss: 0.00021203
Iteration 115/1000 | Loss: 0.00017073
Iteration 116/1000 | Loss: 0.00029533
Iteration 117/1000 | Loss: 0.00023415
Iteration 118/1000 | Loss: 0.00018129
Iteration 119/1000 | Loss: 0.00009297
Iteration 120/1000 | Loss: 0.00026426
Iteration 121/1000 | Loss: 0.00022015
Iteration 122/1000 | Loss: 0.00037232
Iteration 123/1000 | Loss: 0.00012660
Iteration 124/1000 | Loss: 0.00022261
Iteration 125/1000 | Loss: 0.00012926
Iteration 126/1000 | Loss: 0.00016032
Iteration 127/1000 | Loss: 0.00038755
Iteration 128/1000 | Loss: 0.00033695
Iteration 129/1000 | Loss: 0.00035662
Iteration 130/1000 | Loss: 0.00029984
Iteration 131/1000 | Loss: 0.00040386
Iteration 132/1000 | Loss: 0.00023677
Iteration 133/1000 | Loss: 0.00016212
Iteration 134/1000 | Loss: 0.00022612
Iteration 135/1000 | Loss: 0.00023324
Iteration 136/1000 | Loss: 0.00030873
Iteration 137/1000 | Loss: 0.00006014
Iteration 138/1000 | Loss: 0.00025231
Iteration 139/1000 | Loss: 0.00047522
Iteration 140/1000 | Loss: 0.00016895
Iteration 141/1000 | Loss: 0.00007960
Iteration 142/1000 | Loss: 0.00011520
Iteration 143/1000 | Loss: 0.00019669
Iteration 144/1000 | Loss: 0.00041010
Iteration 145/1000 | Loss: 0.00051359
Iteration 146/1000 | Loss: 0.00015130
Iteration 147/1000 | Loss: 0.00009372
Iteration 148/1000 | Loss: 0.00010184
Iteration 149/1000 | Loss: 0.00010961
Iteration 150/1000 | Loss: 0.00021309
Iteration 151/1000 | Loss: 0.00034218
Iteration 152/1000 | Loss: 0.00018272
Iteration 153/1000 | Loss: 0.00004685
Iteration 154/1000 | Loss: 0.00015584
Iteration 155/1000 | Loss: 0.00012528
Iteration 156/1000 | Loss: 0.00005059
Iteration 157/1000 | Loss: 0.00015311
Iteration 158/1000 | Loss: 0.00011505
Iteration 159/1000 | Loss: 0.00006390
Iteration 160/1000 | Loss: 0.00014632
Iteration 161/1000 | Loss: 0.00041796
Iteration 162/1000 | Loss: 0.00027852
Iteration 163/1000 | Loss: 0.00041585
Iteration 164/1000 | Loss: 0.00055092
Iteration 165/1000 | Loss: 0.00018376
Iteration 166/1000 | Loss: 0.00032900
Iteration 167/1000 | Loss: 0.00006723
Iteration 168/1000 | Loss: 0.00006088
Iteration 169/1000 | Loss: 0.00006824
Iteration 170/1000 | Loss: 0.00006095
Iteration 171/1000 | Loss: 0.00006081
Iteration 172/1000 | Loss: 0.00006007
Iteration 173/1000 | Loss: 0.00006002
Iteration 174/1000 | Loss: 0.00006070
Iteration 175/1000 | Loss: 0.00018687
Iteration 176/1000 | Loss: 0.00019259
Iteration 177/1000 | Loss: 0.00017592
Iteration 178/1000 | Loss: 0.00006718
Iteration 179/1000 | Loss: 0.00004293
Iteration 180/1000 | Loss: 0.00004073
Iteration 181/1000 | Loss: 0.00033813
Iteration 182/1000 | Loss: 0.00006859
Iteration 183/1000 | Loss: 0.00025747
Iteration 184/1000 | Loss: 0.00023782
Iteration 185/1000 | Loss: 0.00026502
Iteration 186/1000 | Loss: 0.00024091
Iteration 187/1000 | Loss: 0.00005393
Iteration 188/1000 | Loss: 0.00005148
Iteration 189/1000 | Loss: 0.00003208
Iteration 190/1000 | Loss: 0.00027892
Iteration 191/1000 | Loss: 0.00010596
Iteration 192/1000 | Loss: 0.00003973
Iteration 193/1000 | Loss: 0.00003096
Iteration 194/1000 | Loss: 0.00003714
Iteration 195/1000 | Loss: 0.00003280
Iteration 196/1000 | Loss: 0.00004958
Iteration 197/1000 | Loss: 0.00004233
Iteration 198/1000 | Loss: 0.00004359
Iteration 199/1000 | Loss: 0.00003688
Iteration 200/1000 | Loss: 0.00026991
Iteration 201/1000 | Loss: 0.00031803
Iteration 202/1000 | Loss: 0.00007918
Iteration 203/1000 | Loss: 0.00003967
Iteration 204/1000 | Loss: 0.00025257
Iteration 205/1000 | Loss: 0.00017467
Iteration 206/1000 | Loss: 0.00003597
Iteration 207/1000 | Loss: 0.00004461
Iteration 208/1000 | Loss: 0.00024488
Iteration 209/1000 | Loss: 0.00003582
Iteration 210/1000 | Loss: 0.00003472
Iteration 211/1000 | Loss: 0.00002452
Iteration 212/1000 | Loss: 0.00016687
Iteration 213/1000 | Loss: 0.00002146
Iteration 214/1000 | Loss: 0.00012479
Iteration 215/1000 | Loss: 0.00012800
Iteration 216/1000 | Loss: 0.00003330
Iteration 217/1000 | Loss: 0.00018521
Iteration 218/1000 | Loss: 0.00010306
Iteration 219/1000 | Loss: 0.00002352
Iteration 220/1000 | Loss: 0.00002292
Iteration 221/1000 | Loss: 0.00002291
Iteration 222/1000 | Loss: 0.00002248
Iteration 223/1000 | Loss: 0.00006676
Iteration 224/1000 | Loss: 0.00026662
Iteration 225/1000 | Loss: 0.00024767
Iteration 226/1000 | Loss: 0.00024632
Iteration 227/1000 | Loss: 0.00009892
Iteration 228/1000 | Loss: 0.00002336
Iteration 229/1000 | Loss: 0.00007500
Iteration 230/1000 | Loss: 0.00002152
Iteration 231/1000 | Loss: 0.00002079
Iteration 232/1000 | Loss: 0.00001973
Iteration 233/1000 | Loss: 0.00001903
Iteration 234/1000 | Loss: 0.00001798
Iteration 235/1000 | Loss: 0.00002005
Iteration 236/1000 | Loss: 0.00001698
Iteration 237/1000 | Loss: 0.00001837
Iteration 238/1000 | Loss: 0.00001646
Iteration 239/1000 | Loss: 0.00001643
Iteration 240/1000 | Loss: 0.00001608
Iteration 241/1000 | Loss: 0.00002061
Iteration 242/1000 | Loss: 0.00001559
Iteration 243/1000 | Loss: 0.00001558
Iteration 244/1000 | Loss: 0.00001558
Iteration 245/1000 | Loss: 0.00001558
Iteration 246/1000 | Loss: 0.00001557
Iteration 247/1000 | Loss: 0.00001557
Iteration 248/1000 | Loss: 0.00001557
Iteration 249/1000 | Loss: 0.00001557
Iteration 250/1000 | Loss: 0.00001557
Iteration 251/1000 | Loss: 0.00001557
Iteration 252/1000 | Loss: 0.00001557
Iteration 253/1000 | Loss: 0.00001557
Iteration 254/1000 | Loss: 0.00001557
Iteration 255/1000 | Loss: 0.00001557
Iteration 256/1000 | Loss: 0.00001556
Iteration 257/1000 | Loss: 0.00001556
Iteration 258/1000 | Loss: 0.00001555
Iteration 259/1000 | Loss: 0.00001555
Iteration 260/1000 | Loss: 0.00001555
Iteration 261/1000 | Loss: 0.00001555
Iteration 262/1000 | Loss: 0.00001554
Iteration 263/1000 | Loss: 0.00001554
Iteration 264/1000 | Loss: 0.00001554
Iteration 265/1000 | Loss: 0.00001554
Iteration 266/1000 | Loss: 0.00001554
Iteration 267/1000 | Loss: 0.00001554
Iteration 268/1000 | Loss: 0.00001554
Iteration 269/1000 | Loss: 0.00001554
Iteration 270/1000 | Loss: 0.00001553
Iteration 271/1000 | Loss: 0.00001553
Iteration 272/1000 | Loss: 0.00001553
Iteration 273/1000 | Loss: 0.00001553
Iteration 274/1000 | Loss: 0.00001553
Iteration 275/1000 | Loss: 0.00001552
Iteration 276/1000 | Loss: 0.00001551
Iteration 277/1000 | Loss: 0.00001551
Iteration 278/1000 | Loss: 0.00001551
Iteration 279/1000 | Loss: 0.00001551
Iteration 280/1000 | Loss: 0.00001550
Iteration 281/1000 | Loss: 0.00001550
Iteration 282/1000 | Loss: 0.00001550
Iteration 283/1000 | Loss: 0.00001550
Iteration 284/1000 | Loss: 0.00001550
Iteration 285/1000 | Loss: 0.00001550
Iteration 286/1000 | Loss: 0.00001550
Iteration 287/1000 | Loss: 0.00001550
Iteration 288/1000 | Loss: 0.00001550
Iteration 289/1000 | Loss: 0.00001787
Iteration 290/1000 | Loss: 0.00001602
Iteration 291/1000 | Loss: 0.00001556
Iteration 292/1000 | Loss: 0.00001548
Iteration 293/1000 | Loss: 0.00001548
Iteration 294/1000 | Loss: 0.00001547
Iteration 295/1000 | Loss: 0.00001547
Iteration 296/1000 | Loss: 0.00001547
Iteration 297/1000 | Loss: 0.00001547
Iteration 298/1000 | Loss: 0.00001547
Iteration 299/1000 | Loss: 0.00001547
Iteration 300/1000 | Loss: 0.00001547
Iteration 301/1000 | Loss: 0.00001547
Iteration 302/1000 | Loss: 0.00001547
Iteration 303/1000 | Loss: 0.00001547
Iteration 304/1000 | Loss: 0.00001547
Iteration 305/1000 | Loss: 0.00001547
Iteration 306/1000 | Loss: 0.00001547
Iteration 307/1000 | Loss: 0.00001547
Iteration 308/1000 | Loss: 0.00001547
Iteration 309/1000 | Loss: 0.00001547
Iteration 310/1000 | Loss: 0.00001547
Iteration 311/1000 | Loss: 0.00001547
Iteration 312/1000 | Loss: 0.00001547
Iteration 313/1000 | Loss: 0.00001547
Iteration 314/1000 | Loss: 0.00001547
Iteration 315/1000 | Loss: 0.00001547
Iteration 316/1000 | Loss: 0.00001547
Iteration 317/1000 | Loss: 0.00001547
Iteration 318/1000 | Loss: 0.00001547
Iteration 319/1000 | Loss: 0.00001547
Iteration 320/1000 | Loss: 0.00001547
Iteration 321/1000 | Loss: 0.00001547
Iteration 322/1000 | Loss: 0.00001547
Iteration 323/1000 | Loss: 0.00001547
Iteration 324/1000 | Loss: 0.00001547
Iteration 325/1000 | Loss: 0.00001547
Iteration 326/1000 | Loss: 0.00001547
Iteration 327/1000 | Loss: 0.00001547
Iteration 328/1000 | Loss: 0.00001547
Iteration 329/1000 | Loss: 0.00001547
Iteration 330/1000 | Loss: 0.00001547
Iteration 331/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.5471610822714865e-05, 1.5471610822714865e-05, 1.5471610822714865e-05, 1.5471610822714865e-05, 1.5471610822714865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5471610822714865e-05

Optimization complete. Final v2v error: 3.3992578983306885 mm

Highest mean error: 4.734875679016113 mm for frame 43

Lowest mean error: 2.785968065261841 mm for frame 6

Saving results

Total time: 385.0301399230957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00723087
Iteration 2/25 | Loss: 0.00135961
Iteration 3/25 | Loss: 0.00125748
Iteration 4/25 | Loss: 0.00121444
Iteration 5/25 | Loss: 0.00120531
Iteration 6/25 | Loss: 0.00120284
Iteration 7/25 | Loss: 0.00120281
Iteration 8/25 | Loss: 0.00120241
Iteration 9/25 | Loss: 0.00120199
Iteration 10/25 | Loss: 0.00120183
Iteration 11/25 | Loss: 0.00120162
Iteration 12/25 | Loss: 0.00120141
Iteration 13/25 | Loss: 0.00120220
Iteration 14/25 | Loss: 0.00120155
Iteration 15/25 | Loss: 0.00120209
Iteration 16/25 | Loss: 0.00120147
Iteration 17/25 | Loss: 0.00120201
Iteration 18/25 | Loss: 0.00120150
Iteration 19/25 | Loss: 0.00120202
Iteration 20/25 | Loss: 0.00120164
Iteration 21/25 | Loss: 0.00120204
Iteration 22/25 | Loss: 0.00120165
Iteration 23/25 | Loss: 0.00120204
Iteration 24/25 | Loss: 0.00120164
Iteration 25/25 | Loss: 0.00120205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37591076
Iteration 2/25 | Loss: 0.00160511
Iteration 3/25 | Loss: 0.00160511
Iteration 4/25 | Loss: 0.00160511
Iteration 5/25 | Loss: 0.00160511
Iteration 6/25 | Loss: 0.00160510
Iteration 7/25 | Loss: 0.00160510
Iteration 8/25 | Loss: 0.00160510
Iteration 9/25 | Loss: 0.00160510
Iteration 10/25 | Loss: 0.00160510
Iteration 11/25 | Loss: 0.00160510
Iteration 12/25 | Loss: 0.00160510
Iteration 13/25 | Loss: 0.00160510
Iteration 14/25 | Loss: 0.00160510
Iteration 15/25 | Loss: 0.00160510
Iteration 16/25 | Loss: 0.00160510
Iteration 17/25 | Loss: 0.00160510
Iteration 18/25 | Loss: 0.00160510
Iteration 19/25 | Loss: 0.00160510
Iteration 20/25 | Loss: 0.00160510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016051033744588494, 0.0016051033744588494, 0.0016051033744588494, 0.0016051033744588494, 0.0016051033744588494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016051033744588494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160510
Iteration 2/1000 | Loss: 0.00003294
Iteration 3/1000 | Loss: 0.00002187
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001882
Iteration 6/1000 | Loss: 0.00001794
Iteration 7/1000 | Loss: 0.00001735
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001656
Iteration 15/1000 | Loss: 0.00001660
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001652
Iteration 19/1000 | Loss: 0.00001652
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001651
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001650
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001645
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001643
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001644
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001635
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001631
Iteration 47/1000 | Loss: 0.00001631
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001617
Iteration 69/1000 | Loss: 0.00001617
Iteration 70/1000 | Loss: 0.00001617
Iteration 71/1000 | Loss: 0.00001630
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001617
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001615
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001611
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001609
Iteration 113/1000 | Loss: 0.00001609
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001609
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001607
Iteration 126/1000 | Loss: 0.00001607
Iteration 127/1000 | Loss: 0.00001607
Iteration 128/1000 | Loss: 0.00001607
Iteration 129/1000 | Loss: 0.00001607
Iteration 130/1000 | Loss: 0.00001607
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001606
Iteration 133/1000 | Loss: 0.00001606
Iteration 134/1000 | Loss: 0.00001606
Iteration 135/1000 | Loss: 0.00001606
Iteration 136/1000 | Loss: 0.00001606
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001606
Iteration 139/1000 | Loss: 0.00001606
Iteration 140/1000 | Loss: 0.00001605
Iteration 141/1000 | Loss: 0.00001605
Iteration 142/1000 | Loss: 0.00001605
Iteration 143/1000 | Loss: 0.00001605
Iteration 144/1000 | Loss: 0.00001605
Iteration 145/1000 | Loss: 0.00001605
Iteration 146/1000 | Loss: 0.00001605
Iteration 147/1000 | Loss: 0.00001605
Iteration 148/1000 | Loss: 0.00001605
Iteration 149/1000 | Loss: 0.00001605
Iteration 150/1000 | Loss: 0.00001605
Iteration 151/1000 | Loss: 0.00001604
Iteration 152/1000 | Loss: 0.00001604
Iteration 153/1000 | Loss: 0.00001604
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001603
Iteration 156/1000 | Loss: 0.00001603
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001603
Iteration 159/1000 | Loss: 0.00001603
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001602
Iteration 164/1000 | Loss: 0.00001602
Iteration 165/1000 | Loss: 0.00001602
Iteration 166/1000 | Loss: 0.00001602
Iteration 167/1000 | Loss: 0.00001601
Iteration 168/1000 | Loss: 0.00001601
Iteration 169/1000 | Loss: 0.00001601
Iteration 170/1000 | Loss: 0.00001601
Iteration 171/1000 | Loss: 0.00001601
Iteration 172/1000 | Loss: 0.00001601
Iteration 173/1000 | Loss: 0.00001601
Iteration 174/1000 | Loss: 0.00001601
Iteration 175/1000 | Loss: 0.00001601
Iteration 176/1000 | Loss: 0.00001601
Iteration 177/1000 | Loss: 0.00001601
Iteration 178/1000 | Loss: 0.00001601
Iteration 179/1000 | Loss: 0.00001601
Iteration 180/1000 | Loss: 0.00001601
Iteration 181/1000 | Loss: 0.00001601
Iteration 182/1000 | Loss: 0.00001601
Iteration 183/1000 | Loss: 0.00001601
Iteration 184/1000 | Loss: 0.00001601
Iteration 185/1000 | Loss: 0.00001601
Iteration 186/1000 | Loss: 0.00001601
Iteration 187/1000 | Loss: 0.00001601
Iteration 188/1000 | Loss: 0.00001601
Iteration 189/1000 | Loss: 0.00001601
Iteration 190/1000 | Loss: 0.00001601
Iteration 191/1000 | Loss: 0.00001601
Iteration 192/1000 | Loss: 0.00001601
Iteration 193/1000 | Loss: 0.00001601
Iteration 194/1000 | Loss: 0.00001601
Iteration 195/1000 | Loss: 0.00001601
Iteration 196/1000 | Loss: 0.00001601
Iteration 197/1000 | Loss: 0.00001601
Iteration 198/1000 | Loss: 0.00001601
Iteration 199/1000 | Loss: 0.00001601
Iteration 200/1000 | Loss: 0.00001601
Iteration 201/1000 | Loss: 0.00001601
Iteration 202/1000 | Loss: 0.00001601
Iteration 203/1000 | Loss: 0.00001601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.60116542247124e-05, 1.60116542247124e-05, 1.60116542247124e-05, 1.60116542247124e-05, 1.60116542247124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.60116542247124e-05

Optimization complete. Final v2v error: 3.355734348297119 mm

Highest mean error: 8.841788291931152 mm for frame 135

Lowest mean error: 2.9070427417755127 mm for frame 143

Saving results

Total time: 86.46171116828918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00554936
Iteration 2/25 | Loss: 0.00150215
Iteration 3/25 | Loss: 0.00128786
Iteration 4/25 | Loss: 0.00127265
Iteration 5/25 | Loss: 0.00127020
Iteration 6/25 | Loss: 0.00126986
Iteration 7/25 | Loss: 0.00126986
Iteration 8/25 | Loss: 0.00126986
Iteration 9/25 | Loss: 0.00126986
Iteration 10/25 | Loss: 0.00126986
Iteration 11/25 | Loss: 0.00126986
Iteration 12/25 | Loss: 0.00126986
Iteration 13/25 | Loss: 0.00126986
Iteration 14/25 | Loss: 0.00126986
Iteration 15/25 | Loss: 0.00126986
Iteration 16/25 | Loss: 0.00126986
Iteration 17/25 | Loss: 0.00126986
Iteration 18/25 | Loss: 0.00126986
Iteration 19/25 | Loss: 0.00126986
Iteration 20/25 | Loss: 0.00126986
Iteration 21/25 | Loss: 0.00126986
Iteration 22/25 | Loss: 0.00126986
Iteration 23/25 | Loss: 0.00126986
Iteration 24/25 | Loss: 0.00126986
Iteration 25/25 | Loss: 0.00126986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22376478
Iteration 2/25 | Loss: 0.00125964
Iteration 3/25 | Loss: 0.00125962
Iteration 4/25 | Loss: 0.00125962
Iteration 5/25 | Loss: 0.00125962
Iteration 6/25 | Loss: 0.00125962
Iteration 7/25 | Loss: 0.00125962
Iteration 8/25 | Loss: 0.00125962
Iteration 9/25 | Loss: 0.00125962
Iteration 10/25 | Loss: 0.00125962
Iteration 11/25 | Loss: 0.00125962
Iteration 12/25 | Loss: 0.00125962
Iteration 13/25 | Loss: 0.00125962
Iteration 14/25 | Loss: 0.00125962
Iteration 15/25 | Loss: 0.00125962
Iteration 16/25 | Loss: 0.00125962
Iteration 17/25 | Loss: 0.00125962
Iteration 18/25 | Loss: 0.00125962
Iteration 19/25 | Loss: 0.00125962
Iteration 20/25 | Loss: 0.00125962
Iteration 21/25 | Loss: 0.00125962
Iteration 22/25 | Loss: 0.00125962
Iteration 23/25 | Loss: 0.00125962
Iteration 24/25 | Loss: 0.00125962
Iteration 25/25 | Loss: 0.00125962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125962
Iteration 2/1000 | Loss: 0.00005668
Iteration 3/1000 | Loss: 0.00003539
Iteration 4/1000 | Loss: 0.00002784
Iteration 5/1000 | Loss: 0.00002401
Iteration 6/1000 | Loss: 0.00002215
Iteration 7/1000 | Loss: 0.00002150
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001916
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001889
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001870
Iteration 17/1000 | Loss: 0.00001866
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001863
Iteration 20/1000 | Loss: 0.00001863
Iteration 21/1000 | Loss: 0.00001863
Iteration 22/1000 | Loss: 0.00001862
Iteration 23/1000 | Loss: 0.00001862
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001860
Iteration 26/1000 | Loss: 0.00001860
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001859
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001859
Iteration 32/1000 | Loss: 0.00001859
Iteration 33/1000 | Loss: 0.00001859
Iteration 34/1000 | Loss: 0.00001859
Iteration 35/1000 | Loss: 0.00001859
Iteration 36/1000 | Loss: 0.00001858
Iteration 37/1000 | Loss: 0.00001858
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001857
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001857
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001857
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001851
Iteration 69/1000 | Loss: 0.00001851
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001850
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001848
Iteration 86/1000 | Loss: 0.00001848
Iteration 87/1000 | Loss: 0.00001848
Iteration 88/1000 | Loss: 0.00001848
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001846
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001845
Iteration 95/1000 | Loss: 0.00001845
Iteration 96/1000 | Loss: 0.00001845
Iteration 97/1000 | Loss: 0.00001845
Iteration 98/1000 | Loss: 0.00001845
Iteration 99/1000 | Loss: 0.00001845
Iteration 100/1000 | Loss: 0.00001845
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00001843
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Iteration 109/1000 | Loss: 0.00001842
Iteration 110/1000 | Loss: 0.00001842
Iteration 111/1000 | Loss: 0.00001841
Iteration 112/1000 | Loss: 0.00001841
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001841
Iteration 115/1000 | Loss: 0.00001841
Iteration 116/1000 | Loss: 0.00001840
Iteration 117/1000 | Loss: 0.00001840
Iteration 118/1000 | Loss: 0.00001840
Iteration 119/1000 | Loss: 0.00001839
Iteration 120/1000 | Loss: 0.00001839
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001833
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001833
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001832
Iteration 141/1000 | Loss: 0.00001832
Iteration 142/1000 | Loss: 0.00001832
Iteration 143/1000 | Loss: 0.00001832
Iteration 144/1000 | Loss: 0.00001832
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001831
Iteration 149/1000 | Loss: 0.00001831
Iteration 150/1000 | Loss: 0.00001831
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Iteration 153/1000 | Loss: 0.00001830
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001830
Iteration 156/1000 | Loss: 0.00001830
Iteration 157/1000 | Loss: 0.00001830
Iteration 158/1000 | Loss: 0.00001830
Iteration 159/1000 | Loss: 0.00001830
Iteration 160/1000 | Loss: 0.00001830
Iteration 161/1000 | Loss: 0.00001830
Iteration 162/1000 | Loss: 0.00001830
Iteration 163/1000 | Loss: 0.00001830
Iteration 164/1000 | Loss: 0.00001830
Iteration 165/1000 | Loss: 0.00001830
Iteration 166/1000 | Loss: 0.00001830
Iteration 167/1000 | Loss: 0.00001830
Iteration 168/1000 | Loss: 0.00001830
Iteration 169/1000 | Loss: 0.00001830
Iteration 170/1000 | Loss: 0.00001830
Iteration 171/1000 | Loss: 0.00001830
Iteration 172/1000 | Loss: 0.00001830
Iteration 173/1000 | Loss: 0.00001830
Iteration 174/1000 | Loss: 0.00001830
Iteration 175/1000 | Loss: 0.00001830
Iteration 176/1000 | Loss: 0.00001830
Iteration 177/1000 | Loss: 0.00001830
Iteration 178/1000 | Loss: 0.00001830
Iteration 179/1000 | Loss: 0.00001830
Iteration 180/1000 | Loss: 0.00001830
Iteration 181/1000 | Loss: 0.00001830
Iteration 182/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.8296370399184525e-05, 1.8296370399184525e-05, 1.8296370399184525e-05, 1.8296370399184525e-05, 1.8296370399184525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8296370399184525e-05

Optimization complete. Final v2v error: 3.680276393890381 mm

Highest mean error: 4.037274360656738 mm for frame 49

Lowest mean error: 3.1296157836914062 mm for frame 145

Saving results

Total time: 39.434223651885986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420592
Iteration 2/25 | Loss: 0.00134636
Iteration 3/25 | Loss: 0.00124752
Iteration 4/25 | Loss: 0.00124018
Iteration 5/25 | Loss: 0.00123884
Iteration 6/25 | Loss: 0.00123884
Iteration 7/25 | Loss: 0.00123884
Iteration 8/25 | Loss: 0.00123884
Iteration 9/25 | Loss: 0.00123884
Iteration 10/25 | Loss: 0.00123884
Iteration 11/25 | Loss: 0.00123884
Iteration 12/25 | Loss: 0.00123884
Iteration 13/25 | Loss: 0.00123884
Iteration 14/25 | Loss: 0.00123884
Iteration 15/25 | Loss: 0.00123884
Iteration 16/25 | Loss: 0.00123884
Iteration 17/25 | Loss: 0.00123884
Iteration 18/25 | Loss: 0.00123884
Iteration 19/25 | Loss: 0.00123884
Iteration 20/25 | Loss: 0.00123884
Iteration 21/25 | Loss: 0.00123884
Iteration 22/25 | Loss: 0.00123884
Iteration 23/25 | Loss: 0.00123884
Iteration 24/25 | Loss: 0.00123884
Iteration 25/25 | Loss: 0.00123884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26917171
Iteration 2/25 | Loss: 0.00169129
Iteration 3/25 | Loss: 0.00169129
Iteration 4/25 | Loss: 0.00169129
Iteration 5/25 | Loss: 0.00169129
Iteration 6/25 | Loss: 0.00169129
Iteration 7/25 | Loss: 0.00169129
Iteration 8/25 | Loss: 0.00169129
Iteration 9/25 | Loss: 0.00169129
Iteration 10/25 | Loss: 0.00169129
Iteration 11/25 | Loss: 0.00169129
Iteration 12/25 | Loss: 0.00169129
Iteration 13/25 | Loss: 0.00169129
Iteration 14/25 | Loss: 0.00169129
Iteration 15/25 | Loss: 0.00169129
Iteration 16/25 | Loss: 0.00169129
Iteration 17/25 | Loss: 0.00169129
Iteration 18/25 | Loss: 0.00169129
Iteration 19/25 | Loss: 0.00169129
Iteration 20/25 | Loss: 0.00169129
Iteration 21/25 | Loss: 0.00169129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016912881983444095, 0.0016912881983444095, 0.0016912881983444095, 0.0016912881983444095, 0.0016912881983444095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016912881983444095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169129
Iteration 2/1000 | Loss: 0.00004547
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001842
Iteration 6/1000 | Loss: 0.00001754
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001547
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001541
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001538
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001529
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001528
Iteration 54/1000 | Loss: 0.00001528
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001528
Iteration 57/1000 | Loss: 0.00001527
Iteration 58/1000 | Loss: 0.00001527
Iteration 59/1000 | Loss: 0.00001527
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001526
Iteration 64/1000 | Loss: 0.00001526
Iteration 65/1000 | Loss: 0.00001525
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001525
Iteration 71/1000 | Loss: 0.00001525
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001520
Iteration 111/1000 | Loss: 0.00001520
Iteration 112/1000 | Loss: 0.00001520
Iteration 113/1000 | Loss: 0.00001520
Iteration 114/1000 | Loss: 0.00001520
Iteration 115/1000 | Loss: 0.00001520
Iteration 116/1000 | Loss: 0.00001520
Iteration 117/1000 | Loss: 0.00001520
Iteration 118/1000 | Loss: 0.00001519
Iteration 119/1000 | Loss: 0.00001519
Iteration 120/1000 | Loss: 0.00001519
Iteration 121/1000 | Loss: 0.00001519
Iteration 122/1000 | Loss: 0.00001519
Iteration 123/1000 | Loss: 0.00001519
Iteration 124/1000 | Loss: 0.00001519
Iteration 125/1000 | Loss: 0.00001519
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001518
Iteration 129/1000 | Loss: 0.00001518
Iteration 130/1000 | Loss: 0.00001518
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001517
Iteration 139/1000 | Loss: 0.00001517
Iteration 140/1000 | Loss: 0.00001517
Iteration 141/1000 | Loss: 0.00001517
Iteration 142/1000 | Loss: 0.00001517
Iteration 143/1000 | Loss: 0.00001517
Iteration 144/1000 | Loss: 0.00001517
Iteration 145/1000 | Loss: 0.00001517
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001517
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001517
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001517
Iteration 162/1000 | Loss: 0.00001517
Iteration 163/1000 | Loss: 0.00001517
Iteration 164/1000 | Loss: 0.00001517
Iteration 165/1000 | Loss: 0.00001517
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001517
Iteration 168/1000 | Loss: 0.00001517
Iteration 169/1000 | Loss: 0.00001517
Iteration 170/1000 | Loss: 0.00001517
Iteration 171/1000 | Loss: 0.00001517
Iteration 172/1000 | Loss: 0.00001517
Iteration 173/1000 | Loss: 0.00001517
Iteration 174/1000 | Loss: 0.00001517
Iteration 175/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.5168124264164362e-05, 1.5168124264164362e-05, 1.5168124264164362e-05, 1.5168124264164362e-05, 1.5168124264164362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5168124264164362e-05

Optimization complete. Final v2v error: 3.3168561458587646 mm

Highest mean error: 4.357325077056885 mm for frame 54

Lowest mean error: 2.996450662612915 mm for frame 39

Saving results

Total time: 36.237247705459595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878375
Iteration 2/25 | Loss: 0.00166672
Iteration 3/25 | Loss: 0.00128373
Iteration 4/25 | Loss: 0.00124243
Iteration 5/25 | Loss: 0.00123824
Iteration 6/25 | Loss: 0.00123709
Iteration 7/25 | Loss: 0.00123683
Iteration 8/25 | Loss: 0.00123683
Iteration 9/25 | Loss: 0.00123683
Iteration 10/25 | Loss: 0.00123683
Iteration 11/25 | Loss: 0.00123683
Iteration 12/25 | Loss: 0.00123683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012368306051939726, 0.0012368306051939726, 0.0012368306051939726, 0.0012368306051939726, 0.0012368306051939726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012368306051939726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24488413
Iteration 2/25 | Loss: 0.00149178
Iteration 3/25 | Loss: 0.00149178
Iteration 4/25 | Loss: 0.00149178
Iteration 5/25 | Loss: 0.00149178
Iteration 6/25 | Loss: 0.00149178
Iteration 7/25 | Loss: 0.00149178
Iteration 8/25 | Loss: 0.00149178
Iteration 9/25 | Loss: 0.00149178
Iteration 10/25 | Loss: 0.00149178
Iteration 11/25 | Loss: 0.00149178
Iteration 12/25 | Loss: 0.00149178
Iteration 13/25 | Loss: 0.00149178
Iteration 14/25 | Loss: 0.00149178
Iteration 15/25 | Loss: 0.00149178
Iteration 16/25 | Loss: 0.00149178
Iteration 17/25 | Loss: 0.00149178
Iteration 18/25 | Loss: 0.00149178
Iteration 19/25 | Loss: 0.00149178
Iteration 20/25 | Loss: 0.00149178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014917803928256035, 0.0014917803928256035, 0.0014917803928256035, 0.0014917803928256035, 0.0014917803928256035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014917803928256035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149178
Iteration 2/1000 | Loss: 0.00003812
Iteration 3/1000 | Loss: 0.00002261
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001481
Iteration 10/1000 | Loss: 0.00001456
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001445
Iteration 14/1000 | Loss: 0.00001432
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001396
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001395
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001395
Iteration 36/1000 | Loss: 0.00001395
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001390
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001387
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001381
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001381
Iteration 150/1000 | Loss: 0.00001381
Iteration 151/1000 | Loss: 0.00001381
Iteration 152/1000 | Loss: 0.00001381
Iteration 153/1000 | Loss: 0.00001381
Iteration 154/1000 | Loss: 0.00001381
Iteration 155/1000 | Loss: 0.00001381
Iteration 156/1000 | Loss: 0.00001381
Iteration 157/1000 | Loss: 0.00001381
Iteration 158/1000 | Loss: 0.00001381
Iteration 159/1000 | Loss: 0.00001381
Iteration 160/1000 | Loss: 0.00001381
Iteration 161/1000 | Loss: 0.00001381
Iteration 162/1000 | Loss: 0.00001381
Iteration 163/1000 | Loss: 0.00001381
Iteration 164/1000 | Loss: 0.00001381
Iteration 165/1000 | Loss: 0.00001381
Iteration 166/1000 | Loss: 0.00001381
Iteration 167/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.380582671117736e-05, 1.380582671117736e-05, 1.380582671117736e-05, 1.380582671117736e-05, 1.380582671117736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.380582671117736e-05

Optimization complete. Final v2v error: 3.2102291584014893 mm

Highest mean error: 3.5266780853271484 mm for frame 7

Lowest mean error: 2.9265494346618652 mm for frame 100

Saving results

Total time: 38.196868658065796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00306895
Iteration 2/25 | Loss: 0.00136279
Iteration 3/25 | Loss: 0.00128460
Iteration 4/25 | Loss: 0.00126179
Iteration 5/25 | Loss: 0.00125336
Iteration 6/25 | Loss: 0.00125150
Iteration 7/25 | Loss: 0.00125119
Iteration 8/25 | Loss: 0.00125119
Iteration 9/25 | Loss: 0.00125119
Iteration 10/25 | Loss: 0.00125119
Iteration 11/25 | Loss: 0.00125119
Iteration 12/25 | Loss: 0.00125119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012511914828792214, 0.0012511914828792214, 0.0012511914828792214, 0.0012511914828792214, 0.0012511914828792214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012511914828792214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12960351
Iteration 2/25 | Loss: 0.00228421
Iteration 3/25 | Loss: 0.00228420
Iteration 4/25 | Loss: 0.00228420
Iteration 5/25 | Loss: 0.00228420
Iteration 6/25 | Loss: 0.00228420
Iteration 7/25 | Loss: 0.00228420
Iteration 8/25 | Loss: 0.00228420
Iteration 9/25 | Loss: 0.00228420
Iteration 10/25 | Loss: 0.00228420
Iteration 11/25 | Loss: 0.00228420
Iteration 12/25 | Loss: 0.00228420
Iteration 13/25 | Loss: 0.00228420
Iteration 14/25 | Loss: 0.00228420
Iteration 15/25 | Loss: 0.00228420
Iteration 16/25 | Loss: 0.00228420
Iteration 17/25 | Loss: 0.00228420
Iteration 18/25 | Loss: 0.00228420
Iteration 19/25 | Loss: 0.00228420
Iteration 20/25 | Loss: 0.00228420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022842013277113438, 0.0022842013277113438, 0.0022842013277113438, 0.0022842013277113438, 0.0022842013277113438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022842013277113438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228420
Iteration 2/1000 | Loss: 0.00005693
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00002338
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001974
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001912
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001878
Iteration 14/1000 | Loss: 0.00001878
Iteration 15/1000 | Loss: 0.00001878
Iteration 16/1000 | Loss: 0.00001872
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001872
Iteration 19/1000 | Loss: 0.00001872
Iteration 20/1000 | Loss: 0.00001872
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001871
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001868
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001855
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001852
Iteration 65/1000 | Loss: 0.00001852
Iteration 66/1000 | Loss: 0.00001852
Iteration 67/1000 | Loss: 0.00001852
Iteration 68/1000 | Loss: 0.00001852
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001851
Iteration 74/1000 | Loss: 0.00001851
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001850
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001848
Iteration 84/1000 | Loss: 0.00001848
Iteration 85/1000 | Loss: 0.00001848
Iteration 86/1000 | Loss: 0.00001848
Iteration 87/1000 | Loss: 0.00001848
Iteration 88/1000 | Loss: 0.00001848
Iteration 89/1000 | Loss: 0.00001848
Iteration 90/1000 | Loss: 0.00001848
Iteration 91/1000 | Loss: 0.00001848
Iteration 92/1000 | Loss: 0.00001848
Iteration 93/1000 | Loss: 0.00001848
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001847
Iteration 96/1000 | Loss: 0.00001847
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001846
Iteration 102/1000 | Loss: 0.00001846
Iteration 103/1000 | Loss: 0.00001846
Iteration 104/1000 | Loss: 0.00001846
Iteration 105/1000 | Loss: 0.00001846
Iteration 106/1000 | Loss: 0.00001846
Iteration 107/1000 | Loss: 0.00001846
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001845
Iteration 110/1000 | Loss: 0.00001845
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001844
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001844
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001842
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001841
Iteration 145/1000 | Loss: 0.00001841
Iteration 146/1000 | Loss: 0.00001841
Iteration 147/1000 | Loss: 0.00001841
Iteration 148/1000 | Loss: 0.00001841
Iteration 149/1000 | Loss: 0.00001840
Iteration 150/1000 | Loss: 0.00001840
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001840
Iteration 156/1000 | Loss: 0.00001839
Iteration 157/1000 | Loss: 0.00001839
Iteration 158/1000 | Loss: 0.00001839
Iteration 159/1000 | Loss: 0.00001839
Iteration 160/1000 | Loss: 0.00001839
Iteration 161/1000 | Loss: 0.00001839
Iteration 162/1000 | Loss: 0.00001839
Iteration 163/1000 | Loss: 0.00001838
Iteration 164/1000 | Loss: 0.00001838
Iteration 165/1000 | Loss: 0.00001838
Iteration 166/1000 | Loss: 0.00001838
Iteration 167/1000 | Loss: 0.00001838
Iteration 168/1000 | Loss: 0.00001837
Iteration 169/1000 | Loss: 0.00001837
Iteration 170/1000 | Loss: 0.00001837
Iteration 171/1000 | Loss: 0.00001837
Iteration 172/1000 | Loss: 0.00001837
Iteration 173/1000 | Loss: 0.00001837
Iteration 174/1000 | Loss: 0.00001837
Iteration 175/1000 | Loss: 0.00001836
Iteration 176/1000 | Loss: 0.00001836
Iteration 177/1000 | Loss: 0.00001836
Iteration 178/1000 | Loss: 0.00001836
Iteration 179/1000 | Loss: 0.00001836
Iteration 180/1000 | Loss: 0.00001836
Iteration 181/1000 | Loss: 0.00001836
Iteration 182/1000 | Loss: 0.00001836
Iteration 183/1000 | Loss: 0.00001836
Iteration 184/1000 | Loss: 0.00001836
Iteration 185/1000 | Loss: 0.00001836
Iteration 186/1000 | Loss: 0.00001836
Iteration 187/1000 | Loss: 0.00001836
Iteration 188/1000 | Loss: 0.00001835
Iteration 189/1000 | Loss: 0.00001835
Iteration 190/1000 | Loss: 0.00001835
Iteration 191/1000 | Loss: 0.00001835
Iteration 192/1000 | Loss: 0.00001835
Iteration 193/1000 | Loss: 0.00001835
Iteration 194/1000 | Loss: 0.00001835
Iteration 195/1000 | Loss: 0.00001835
Iteration 196/1000 | Loss: 0.00001835
Iteration 197/1000 | Loss: 0.00001835
Iteration 198/1000 | Loss: 0.00001835
Iteration 199/1000 | Loss: 0.00001835
Iteration 200/1000 | Loss: 0.00001835
Iteration 201/1000 | Loss: 0.00001835
Iteration 202/1000 | Loss: 0.00001835
Iteration 203/1000 | Loss: 0.00001835
Iteration 204/1000 | Loss: 0.00001834
Iteration 205/1000 | Loss: 0.00001834
Iteration 206/1000 | Loss: 0.00001834
Iteration 207/1000 | Loss: 0.00001834
Iteration 208/1000 | Loss: 0.00001834
Iteration 209/1000 | Loss: 0.00001834
Iteration 210/1000 | Loss: 0.00001834
Iteration 211/1000 | Loss: 0.00001834
Iteration 212/1000 | Loss: 0.00001834
Iteration 213/1000 | Loss: 0.00001834
Iteration 214/1000 | Loss: 0.00001834
Iteration 215/1000 | Loss: 0.00001834
Iteration 216/1000 | Loss: 0.00001834
Iteration 217/1000 | Loss: 0.00001834
Iteration 218/1000 | Loss: 0.00001834
Iteration 219/1000 | Loss: 0.00001834
Iteration 220/1000 | Loss: 0.00001834
Iteration 221/1000 | Loss: 0.00001833
Iteration 222/1000 | Loss: 0.00001833
Iteration 223/1000 | Loss: 0.00001833
Iteration 224/1000 | Loss: 0.00001833
Iteration 225/1000 | Loss: 0.00001833
Iteration 226/1000 | Loss: 0.00001833
Iteration 227/1000 | Loss: 0.00001833
Iteration 228/1000 | Loss: 0.00001833
Iteration 229/1000 | Loss: 0.00001833
Iteration 230/1000 | Loss: 0.00001833
Iteration 231/1000 | Loss: 0.00001833
Iteration 232/1000 | Loss: 0.00001833
Iteration 233/1000 | Loss: 0.00001833
Iteration 234/1000 | Loss: 0.00001833
Iteration 235/1000 | Loss: 0.00001833
Iteration 236/1000 | Loss: 0.00001833
Iteration 237/1000 | Loss: 0.00001833
Iteration 238/1000 | Loss: 0.00001833
Iteration 239/1000 | Loss: 0.00001833
Iteration 240/1000 | Loss: 0.00001833
Iteration 241/1000 | Loss: 0.00001833
Iteration 242/1000 | Loss: 0.00001833
Iteration 243/1000 | Loss: 0.00001833
Iteration 244/1000 | Loss: 0.00001833
Iteration 245/1000 | Loss: 0.00001833
Iteration 246/1000 | Loss: 0.00001833
Iteration 247/1000 | Loss: 0.00001833
Iteration 248/1000 | Loss: 0.00001833
Iteration 249/1000 | Loss: 0.00001833
Iteration 250/1000 | Loss: 0.00001833
Iteration 251/1000 | Loss: 0.00001833
Iteration 252/1000 | Loss: 0.00001833
Iteration 253/1000 | Loss: 0.00001833
Iteration 254/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.8325292330700904e-05, 1.8325292330700904e-05, 1.8325292330700904e-05, 1.8325292330700904e-05, 1.8325292330700904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8325292330700904e-05

Optimization complete. Final v2v error: 3.594106912612915 mm

Highest mean error: 4.004104137420654 mm for frame 138

Lowest mean error: 3.337017774581909 mm for frame 163

Saving results

Total time: 42.08610248565674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406240
Iteration 2/25 | Loss: 0.00128876
Iteration 3/25 | Loss: 0.00121989
Iteration 4/25 | Loss: 0.00121092
Iteration 5/25 | Loss: 0.00120823
Iteration 6/25 | Loss: 0.00120823
Iteration 7/25 | Loss: 0.00120823
Iteration 8/25 | Loss: 0.00120823
Iteration 9/25 | Loss: 0.00120823
Iteration 10/25 | Loss: 0.00120823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012082320172339678, 0.0012082320172339678, 0.0012082320172339678, 0.0012082320172339678, 0.0012082320172339678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012082320172339678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64822578
Iteration 2/25 | Loss: 0.00166252
Iteration 3/25 | Loss: 0.00166252
Iteration 4/25 | Loss: 0.00166252
Iteration 5/25 | Loss: 0.00166252
Iteration 6/25 | Loss: 0.00166252
Iteration 7/25 | Loss: 0.00166252
Iteration 8/25 | Loss: 0.00166252
Iteration 9/25 | Loss: 0.00166252
Iteration 10/25 | Loss: 0.00166252
Iteration 11/25 | Loss: 0.00166252
Iteration 12/25 | Loss: 0.00166252
Iteration 13/25 | Loss: 0.00166252
Iteration 14/25 | Loss: 0.00166252
Iteration 15/25 | Loss: 0.00166252
Iteration 16/25 | Loss: 0.00166252
Iteration 17/25 | Loss: 0.00166252
Iteration 18/25 | Loss: 0.00166252
Iteration 19/25 | Loss: 0.00166252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016625167336314917, 0.0016625167336314917, 0.0016625167336314917, 0.0016625167336314917, 0.0016625167336314917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016625167336314917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166252
Iteration 2/1000 | Loss: 0.00004133
Iteration 3/1000 | Loss: 0.00002118
Iteration 4/1000 | Loss: 0.00001763
Iteration 5/1000 | Loss: 0.00001655
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001497
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001471
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001447
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001443
Iteration 21/1000 | Loss: 0.00001443
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001443
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001442
Iteration 26/1000 | Loss: 0.00001442
Iteration 27/1000 | Loss: 0.00001441
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001429
Iteration 41/1000 | Loss: 0.00001429
Iteration 42/1000 | Loss: 0.00001429
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001428
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001422
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001416
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001414
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001408
Iteration 111/1000 | Loss: 0.00001408
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001406
Iteration 127/1000 | Loss: 0.00001406
Iteration 128/1000 | Loss: 0.00001406
Iteration 129/1000 | Loss: 0.00001406
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001406
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001406
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001406
Iteration 140/1000 | Loss: 0.00001406
Iteration 141/1000 | Loss: 0.00001406
Iteration 142/1000 | Loss: 0.00001406
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001406
Iteration 150/1000 | Loss: 0.00001406
Iteration 151/1000 | Loss: 0.00001406
Iteration 152/1000 | Loss: 0.00001406
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Iteration 168/1000 | Loss: 0.00001406
Iteration 169/1000 | Loss: 0.00001406
Iteration 170/1000 | Loss: 0.00001406
Iteration 171/1000 | Loss: 0.00001406
Iteration 172/1000 | Loss: 0.00001406
Iteration 173/1000 | Loss: 0.00001406
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001406
Iteration 177/1000 | Loss: 0.00001406
Iteration 178/1000 | Loss: 0.00001406
Iteration 179/1000 | Loss: 0.00001406
Iteration 180/1000 | Loss: 0.00001406
Iteration 181/1000 | Loss: 0.00001406
Iteration 182/1000 | Loss: 0.00001406
Iteration 183/1000 | Loss: 0.00001406
Iteration 184/1000 | Loss: 0.00001406
Iteration 185/1000 | Loss: 0.00001406
Iteration 186/1000 | Loss: 0.00001406
Iteration 187/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.4059240129427053e-05, 1.4059240129427053e-05, 1.4059240129427053e-05, 1.4059240129427053e-05, 1.4059240129427053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4059240129427053e-05

Optimization complete. Final v2v error: 3.22434139251709 mm

Highest mean error: 3.3814916610717773 mm for frame 198

Lowest mean error: 3.0595366954803467 mm for frame 129

Saving results

Total time: 35.053460359573364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00554702
Iteration 2/25 | Loss: 0.00132598
Iteration 3/25 | Loss: 0.00123440
Iteration 4/25 | Loss: 0.00121989
Iteration 5/25 | Loss: 0.00121569
Iteration 6/25 | Loss: 0.00121448
Iteration 7/25 | Loss: 0.00121448
Iteration 8/25 | Loss: 0.00121448
Iteration 9/25 | Loss: 0.00121448
Iteration 10/25 | Loss: 0.00121448
Iteration 11/25 | Loss: 0.00121448
Iteration 12/25 | Loss: 0.00121448
Iteration 13/25 | Loss: 0.00121448
Iteration 14/25 | Loss: 0.00121448
Iteration 15/25 | Loss: 0.00121448
Iteration 16/25 | Loss: 0.00121448
Iteration 17/25 | Loss: 0.00121448
Iteration 18/25 | Loss: 0.00121448
Iteration 19/25 | Loss: 0.00121448
Iteration 20/25 | Loss: 0.00121448
Iteration 21/25 | Loss: 0.00121448
Iteration 22/25 | Loss: 0.00121448
Iteration 23/25 | Loss: 0.00121448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012144766515120864, 0.0012144766515120864, 0.0012144766515120864, 0.0012144766515120864, 0.0012144766515120864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012144766515120864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58755636
Iteration 2/25 | Loss: 0.00152483
Iteration 3/25 | Loss: 0.00152482
Iteration 4/25 | Loss: 0.00152482
Iteration 5/25 | Loss: 0.00152482
Iteration 6/25 | Loss: 0.00152482
Iteration 7/25 | Loss: 0.00152482
Iteration 8/25 | Loss: 0.00152482
Iteration 9/25 | Loss: 0.00152482
Iteration 10/25 | Loss: 0.00152482
Iteration 11/25 | Loss: 0.00152482
Iteration 12/25 | Loss: 0.00152482
Iteration 13/25 | Loss: 0.00152482
Iteration 14/25 | Loss: 0.00152482
Iteration 15/25 | Loss: 0.00152482
Iteration 16/25 | Loss: 0.00152482
Iteration 17/25 | Loss: 0.00152482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015248205745592713, 0.0015248205745592713, 0.0015248205745592713, 0.0015248205745592713, 0.0015248205745592713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015248205745592713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152482
Iteration 2/1000 | Loss: 0.00003621
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00001915
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001587
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001563
Iteration 14/1000 | Loss: 0.00001555
Iteration 15/1000 | Loss: 0.00001554
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001550
Iteration 19/1000 | Loss: 0.00001545
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001543
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001538
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001533
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001525
Iteration 90/1000 | Loss: 0.00001525
Iteration 91/1000 | Loss: 0.00001525
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001522
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001520
Iteration 113/1000 | Loss: 0.00001520
Iteration 114/1000 | Loss: 0.00001520
Iteration 115/1000 | Loss: 0.00001520
Iteration 116/1000 | Loss: 0.00001519
Iteration 117/1000 | Loss: 0.00001519
Iteration 118/1000 | Loss: 0.00001519
Iteration 119/1000 | Loss: 0.00001519
Iteration 120/1000 | Loss: 0.00001519
Iteration 121/1000 | Loss: 0.00001519
Iteration 122/1000 | Loss: 0.00001519
Iteration 123/1000 | Loss: 0.00001519
Iteration 124/1000 | Loss: 0.00001519
Iteration 125/1000 | Loss: 0.00001519
Iteration 126/1000 | Loss: 0.00001519
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001519
Iteration 129/1000 | Loss: 0.00001519
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001519
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001518
Iteration 134/1000 | Loss: 0.00001518
Iteration 135/1000 | Loss: 0.00001518
Iteration 136/1000 | Loss: 0.00001518
Iteration 137/1000 | Loss: 0.00001518
Iteration 138/1000 | Loss: 0.00001518
Iteration 139/1000 | Loss: 0.00001518
Iteration 140/1000 | Loss: 0.00001518
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001517
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001517
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001517
Iteration 162/1000 | Loss: 0.00001517
Iteration 163/1000 | Loss: 0.00001517
Iteration 164/1000 | Loss: 0.00001517
Iteration 165/1000 | Loss: 0.00001517
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001517
Iteration 168/1000 | Loss: 0.00001517
Iteration 169/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.5173307474469766e-05, 1.5173307474469766e-05, 1.5173307474469766e-05, 1.5173307474469766e-05, 1.5173307474469766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5173307474469766e-05

Optimization complete. Final v2v error: 3.2988288402557373 mm

Highest mean error: 3.883420705795288 mm for frame 71

Lowest mean error: 2.9316585063934326 mm for frame 4

Saving results

Total time: 36.58536624908447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454948
Iteration 2/25 | Loss: 0.00134415
Iteration 3/25 | Loss: 0.00126189
Iteration 4/25 | Loss: 0.00124283
Iteration 5/25 | Loss: 0.00123696
Iteration 6/25 | Loss: 0.00123619
Iteration 7/25 | Loss: 0.00123619
Iteration 8/25 | Loss: 0.00123619
Iteration 9/25 | Loss: 0.00123619
Iteration 10/25 | Loss: 0.00123619
Iteration 11/25 | Loss: 0.00123619
Iteration 12/25 | Loss: 0.00123619
Iteration 13/25 | Loss: 0.00123619
Iteration 14/25 | Loss: 0.00123619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001236187294125557, 0.001236187294125557, 0.001236187294125557, 0.001236187294125557, 0.001236187294125557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001236187294125557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41018260
Iteration 2/25 | Loss: 0.00172485
Iteration 3/25 | Loss: 0.00172484
Iteration 4/25 | Loss: 0.00172484
Iteration 5/25 | Loss: 0.00172484
Iteration 6/25 | Loss: 0.00172484
Iteration 7/25 | Loss: 0.00172484
Iteration 8/25 | Loss: 0.00172484
Iteration 9/25 | Loss: 0.00172484
Iteration 10/25 | Loss: 0.00172484
Iteration 11/25 | Loss: 0.00172484
Iteration 12/25 | Loss: 0.00172484
Iteration 13/25 | Loss: 0.00172484
Iteration 14/25 | Loss: 0.00172484
Iteration 15/25 | Loss: 0.00172484
Iteration 16/25 | Loss: 0.00172484
Iteration 17/25 | Loss: 0.00172484
Iteration 18/25 | Loss: 0.00172484
Iteration 19/25 | Loss: 0.00172484
Iteration 20/25 | Loss: 0.00172484
Iteration 21/25 | Loss: 0.00172484
Iteration 22/25 | Loss: 0.00172484
Iteration 23/25 | Loss: 0.00172484
Iteration 24/25 | Loss: 0.00172484
Iteration 25/25 | Loss: 0.00172484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172484
Iteration 2/1000 | Loss: 0.00004608
Iteration 3/1000 | Loss: 0.00002765
Iteration 4/1000 | Loss: 0.00002240
Iteration 5/1000 | Loss: 0.00002040
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001814
Iteration 9/1000 | Loss: 0.00001781
Iteration 10/1000 | Loss: 0.00001758
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001719
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001711
Iteration 18/1000 | Loss: 0.00001711
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001710
Iteration 22/1000 | Loss: 0.00001709
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001707
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001703
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001701
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001701
Iteration 44/1000 | Loss: 0.00001700
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001698
Iteration 52/1000 | Loss: 0.00001698
Iteration 53/1000 | Loss: 0.00001698
Iteration 54/1000 | Loss: 0.00001698
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001696
Iteration 61/1000 | Loss: 0.00001695
Iteration 62/1000 | Loss: 0.00001695
Iteration 63/1000 | Loss: 0.00001695
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001694
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001692
Iteration 70/1000 | Loss: 0.00001692
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001690
Iteration 77/1000 | Loss: 0.00001690
Iteration 78/1000 | Loss: 0.00001690
Iteration 79/1000 | Loss: 0.00001690
Iteration 80/1000 | Loss: 0.00001689
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001689
Iteration 84/1000 | Loss: 0.00001689
Iteration 85/1000 | Loss: 0.00001689
Iteration 86/1000 | Loss: 0.00001688
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001688
Iteration 94/1000 | Loss: 0.00001688
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001687
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001685
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Iteration 169/1000 | Loss: 0.00001683
Iteration 170/1000 | Loss: 0.00001683
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001683
Iteration 175/1000 | Loss: 0.00001683
Iteration 176/1000 | Loss: 0.00001683
Iteration 177/1000 | Loss: 0.00001683
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001683
Iteration 181/1000 | Loss: 0.00001683
Iteration 182/1000 | Loss: 0.00001683
Iteration 183/1000 | Loss: 0.00001683
Iteration 184/1000 | Loss: 0.00001683
Iteration 185/1000 | Loss: 0.00001683
Iteration 186/1000 | Loss: 0.00001683
Iteration 187/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.6826161299832165e-05, 1.6826161299832165e-05, 1.6826161299832165e-05, 1.6826161299832165e-05, 1.6826161299832165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6826161299832165e-05

Optimization complete. Final v2v error: 3.5588488578796387 mm

Highest mean error: 4.130939960479736 mm for frame 35

Lowest mean error: 3.2559115886688232 mm for frame 156

Saving results

Total time: 38.227208614349365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055116
Iteration 2/25 | Loss: 0.00297045
Iteration 3/25 | Loss: 0.00220774
Iteration 4/25 | Loss: 0.00192436
Iteration 5/25 | Loss: 0.00187751
Iteration 6/25 | Loss: 0.00194896
Iteration 7/25 | Loss: 0.00183552
Iteration 8/25 | Loss: 0.00177305
Iteration 9/25 | Loss: 0.00170672
Iteration 10/25 | Loss: 0.00169648
Iteration 11/25 | Loss: 0.00166944
Iteration 12/25 | Loss: 0.00164373
Iteration 13/25 | Loss: 0.00163828
Iteration 14/25 | Loss: 0.00161564
Iteration 15/25 | Loss: 0.00161773
Iteration 16/25 | Loss: 0.00161456
Iteration 17/25 | Loss: 0.00160866
Iteration 18/25 | Loss: 0.00159707
Iteration 19/25 | Loss: 0.00159564
Iteration 20/25 | Loss: 0.00159492
Iteration 21/25 | Loss: 0.00159258
Iteration 22/25 | Loss: 0.00159151
Iteration 23/25 | Loss: 0.00159203
Iteration 24/25 | Loss: 0.00158916
Iteration 25/25 | Loss: 0.00158725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24744177
Iteration 2/25 | Loss: 0.00596756
Iteration 3/25 | Loss: 0.00439489
Iteration 4/25 | Loss: 0.00439489
Iteration 5/25 | Loss: 0.00439489
Iteration 6/25 | Loss: 0.00439489
Iteration 7/25 | Loss: 0.00439489
Iteration 8/25 | Loss: 0.00439488
Iteration 9/25 | Loss: 0.00439488
Iteration 10/25 | Loss: 0.00439488
Iteration 11/25 | Loss: 0.00439488
Iteration 12/25 | Loss: 0.00439488
Iteration 13/25 | Loss: 0.00439488
Iteration 14/25 | Loss: 0.00439488
Iteration 15/25 | Loss: 0.00439488
Iteration 16/25 | Loss: 0.00439488
Iteration 17/25 | Loss: 0.00439488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00439488235861063, 0.00439488235861063, 0.00439488235861063, 0.00439488235861063, 0.00439488235861063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00439488235861063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00439488
Iteration 2/1000 | Loss: 0.00256606
Iteration 3/1000 | Loss: 0.00144319
Iteration 4/1000 | Loss: 0.00105836
Iteration 5/1000 | Loss: 0.00072834
Iteration 6/1000 | Loss: 0.00165896
Iteration 7/1000 | Loss: 0.00126723
Iteration 8/1000 | Loss: 0.00064543
Iteration 9/1000 | Loss: 0.00196109
Iteration 10/1000 | Loss: 0.00077963
Iteration 11/1000 | Loss: 0.00191778
Iteration 12/1000 | Loss: 0.00033911
Iteration 13/1000 | Loss: 0.00046696
Iteration 14/1000 | Loss: 0.00120273
Iteration 15/1000 | Loss: 0.00029023
Iteration 16/1000 | Loss: 0.00035364
Iteration 17/1000 | Loss: 0.00027830
Iteration 18/1000 | Loss: 0.00062635
Iteration 19/1000 | Loss: 0.00109826
Iteration 20/1000 | Loss: 0.00031507
Iteration 21/1000 | Loss: 0.00105192
Iteration 22/1000 | Loss: 0.00026256
Iteration 23/1000 | Loss: 0.00074859
Iteration 24/1000 | Loss: 0.00254301
Iteration 25/1000 | Loss: 0.00230441
Iteration 26/1000 | Loss: 0.00303233
Iteration 27/1000 | Loss: 0.00444168
Iteration 28/1000 | Loss: 0.00361759
Iteration 29/1000 | Loss: 0.00115018
Iteration 30/1000 | Loss: 0.00120555
Iteration 31/1000 | Loss: 0.00069247
Iteration 32/1000 | Loss: 0.00103519
Iteration 33/1000 | Loss: 0.00033648
Iteration 34/1000 | Loss: 0.00096834
Iteration 35/1000 | Loss: 0.00073601
Iteration 36/1000 | Loss: 0.00029459
Iteration 37/1000 | Loss: 0.00052197
Iteration 38/1000 | Loss: 0.00042891
Iteration 39/1000 | Loss: 0.00024080
Iteration 40/1000 | Loss: 0.00035843
Iteration 41/1000 | Loss: 0.00044479
Iteration 42/1000 | Loss: 0.00037538
Iteration 43/1000 | Loss: 0.00023036
Iteration 44/1000 | Loss: 0.00037757
Iteration 45/1000 | Loss: 0.00056325
Iteration 46/1000 | Loss: 0.00058902
Iteration 47/1000 | Loss: 0.00083371
Iteration 48/1000 | Loss: 0.00059054
Iteration 49/1000 | Loss: 0.00081666
Iteration 50/1000 | Loss: 0.00084048
Iteration 51/1000 | Loss: 0.00029345
Iteration 52/1000 | Loss: 0.00035123
Iteration 53/1000 | Loss: 0.00078757
Iteration 54/1000 | Loss: 0.00204104
Iteration 55/1000 | Loss: 0.00036410
Iteration 56/1000 | Loss: 0.00026888
Iteration 57/1000 | Loss: 0.00036146
Iteration 58/1000 | Loss: 0.00031107
Iteration 59/1000 | Loss: 0.00019371
Iteration 60/1000 | Loss: 0.00045036
Iteration 61/1000 | Loss: 0.00019970
Iteration 62/1000 | Loss: 0.00024319
Iteration 63/1000 | Loss: 0.00036265
Iteration 64/1000 | Loss: 0.00117072
Iteration 65/1000 | Loss: 0.00024579
Iteration 66/1000 | Loss: 0.00021737
Iteration 67/1000 | Loss: 0.00036983
Iteration 68/1000 | Loss: 0.00146339
Iteration 69/1000 | Loss: 0.00042519
Iteration 70/1000 | Loss: 0.00024244
Iteration 71/1000 | Loss: 0.00016956
Iteration 72/1000 | Loss: 0.00024876
Iteration 73/1000 | Loss: 0.00057230
Iteration 74/1000 | Loss: 0.00228701
Iteration 75/1000 | Loss: 0.00030702
Iteration 76/1000 | Loss: 0.00042538
Iteration 77/1000 | Loss: 0.00028467
Iteration 78/1000 | Loss: 0.00040752
Iteration 79/1000 | Loss: 0.00019603
Iteration 80/1000 | Loss: 0.00040094
Iteration 81/1000 | Loss: 0.00038849
Iteration 82/1000 | Loss: 0.00086407
Iteration 83/1000 | Loss: 0.00023630
Iteration 84/1000 | Loss: 0.00051601
Iteration 85/1000 | Loss: 0.00015145
Iteration 86/1000 | Loss: 0.00017292
Iteration 87/1000 | Loss: 0.00062806
Iteration 88/1000 | Loss: 0.00099064
Iteration 89/1000 | Loss: 0.00016934
Iteration 90/1000 | Loss: 0.00022881
Iteration 91/1000 | Loss: 0.00021655
Iteration 92/1000 | Loss: 0.00066737
Iteration 93/1000 | Loss: 0.00016395
Iteration 94/1000 | Loss: 0.00017339
Iteration 95/1000 | Loss: 0.00031138
Iteration 96/1000 | Loss: 0.00013755
Iteration 97/1000 | Loss: 0.00020841
Iteration 98/1000 | Loss: 0.00045876
Iteration 99/1000 | Loss: 0.00020340
Iteration 100/1000 | Loss: 0.00016986
Iteration 101/1000 | Loss: 0.00043885
Iteration 102/1000 | Loss: 0.00011856
Iteration 103/1000 | Loss: 0.00034228
Iteration 104/1000 | Loss: 0.00063320
Iteration 105/1000 | Loss: 0.00021781
Iteration 106/1000 | Loss: 0.00014894
Iteration 107/1000 | Loss: 0.00013436
Iteration 108/1000 | Loss: 0.00029558
Iteration 109/1000 | Loss: 0.00037839
Iteration 110/1000 | Loss: 0.00015220
Iteration 111/1000 | Loss: 0.00021798
Iteration 112/1000 | Loss: 0.00120833
Iteration 113/1000 | Loss: 0.00021782
Iteration 114/1000 | Loss: 0.00023425
Iteration 115/1000 | Loss: 0.00018903
Iteration 116/1000 | Loss: 0.00162268
Iteration 117/1000 | Loss: 0.00018400
Iteration 118/1000 | Loss: 0.00030633
Iteration 119/1000 | Loss: 0.00026463
Iteration 120/1000 | Loss: 0.00054235
Iteration 121/1000 | Loss: 0.00026965
Iteration 122/1000 | Loss: 0.00055131
Iteration 123/1000 | Loss: 0.00091319
Iteration 124/1000 | Loss: 0.00019890
Iteration 125/1000 | Loss: 0.00040203
Iteration 126/1000 | Loss: 0.00032379
Iteration 127/1000 | Loss: 0.00030076
Iteration 128/1000 | Loss: 0.00031155
Iteration 129/1000 | Loss: 0.00030839
Iteration 130/1000 | Loss: 0.00011613
Iteration 131/1000 | Loss: 0.00057054
Iteration 132/1000 | Loss: 0.00015388
Iteration 133/1000 | Loss: 0.00015927
Iteration 134/1000 | Loss: 0.00088729
Iteration 135/1000 | Loss: 0.00019531
Iteration 136/1000 | Loss: 0.00073095
Iteration 137/1000 | Loss: 0.00015797
Iteration 138/1000 | Loss: 0.00011787
Iteration 139/1000 | Loss: 0.00018574
Iteration 140/1000 | Loss: 0.00064483
Iteration 141/1000 | Loss: 0.00015725
Iteration 142/1000 | Loss: 0.00016748
Iteration 143/1000 | Loss: 0.00014179
Iteration 144/1000 | Loss: 0.00014748
Iteration 145/1000 | Loss: 0.00031869
Iteration 146/1000 | Loss: 0.00024553
Iteration 147/1000 | Loss: 0.00007562
Iteration 148/1000 | Loss: 0.00008881
Iteration 149/1000 | Loss: 0.00008894
Iteration 150/1000 | Loss: 0.00012361
Iteration 151/1000 | Loss: 0.00008846
Iteration 152/1000 | Loss: 0.00009197
Iteration 153/1000 | Loss: 0.00031688
Iteration 154/1000 | Loss: 0.00074907
Iteration 155/1000 | Loss: 0.00009706
Iteration 156/1000 | Loss: 0.00011779
Iteration 157/1000 | Loss: 0.00071406
Iteration 158/1000 | Loss: 0.00009456
Iteration 159/1000 | Loss: 0.00012370
Iteration 160/1000 | Loss: 0.00015279
Iteration 161/1000 | Loss: 0.00008255
Iteration 162/1000 | Loss: 0.00072119
Iteration 163/1000 | Loss: 0.00035247
Iteration 164/1000 | Loss: 0.00057600
Iteration 165/1000 | Loss: 0.00058552
Iteration 166/1000 | Loss: 0.00025074
Iteration 167/1000 | Loss: 0.00015172
Iteration 168/1000 | Loss: 0.00006265
Iteration 169/1000 | Loss: 0.00025251
Iteration 170/1000 | Loss: 0.00019231
Iteration 171/1000 | Loss: 0.00123145
Iteration 172/1000 | Loss: 0.00006980
Iteration 173/1000 | Loss: 0.00006611
Iteration 174/1000 | Loss: 0.00005855
Iteration 175/1000 | Loss: 0.00008205
Iteration 176/1000 | Loss: 0.00056307
Iteration 177/1000 | Loss: 0.00101652
Iteration 178/1000 | Loss: 0.00017307
Iteration 179/1000 | Loss: 0.00020498
Iteration 180/1000 | Loss: 0.00032813
Iteration 181/1000 | Loss: 0.00004759
Iteration 182/1000 | Loss: 0.00010670
Iteration 183/1000 | Loss: 0.00004360
Iteration 184/1000 | Loss: 0.00004059
Iteration 185/1000 | Loss: 0.00004942
Iteration 186/1000 | Loss: 0.00049414
Iteration 187/1000 | Loss: 0.00022244
Iteration 188/1000 | Loss: 0.00019045
Iteration 189/1000 | Loss: 0.00014356
Iteration 190/1000 | Loss: 0.00023652
Iteration 191/1000 | Loss: 0.00005949
Iteration 192/1000 | Loss: 0.00005755
Iteration 193/1000 | Loss: 0.00005196
Iteration 194/1000 | Loss: 0.00008872
Iteration 195/1000 | Loss: 0.00017890
Iteration 196/1000 | Loss: 0.00004158
Iteration 197/1000 | Loss: 0.00004801
Iteration 198/1000 | Loss: 0.00004663
Iteration 199/1000 | Loss: 0.00008534
Iteration 200/1000 | Loss: 0.00005650
Iteration 201/1000 | Loss: 0.00016935
Iteration 202/1000 | Loss: 0.00003946
Iteration 203/1000 | Loss: 0.00003664
Iteration 204/1000 | Loss: 0.00004088
Iteration 205/1000 | Loss: 0.00006071
Iteration 206/1000 | Loss: 0.00008793
Iteration 207/1000 | Loss: 0.00011477
Iteration 208/1000 | Loss: 0.00005104
Iteration 209/1000 | Loss: 0.00005064
Iteration 210/1000 | Loss: 0.00010600
Iteration 211/1000 | Loss: 0.00062279
Iteration 212/1000 | Loss: 0.00102394
Iteration 213/1000 | Loss: 0.00179566
Iteration 214/1000 | Loss: 0.00005890
Iteration 215/1000 | Loss: 0.00004513
Iteration 216/1000 | Loss: 0.00006145
Iteration 217/1000 | Loss: 0.00004141
Iteration 218/1000 | Loss: 0.00009836
Iteration 219/1000 | Loss: 0.00012544
Iteration 220/1000 | Loss: 0.00002939
Iteration 221/1000 | Loss: 0.00004008
Iteration 222/1000 | Loss: 0.00017569
Iteration 223/1000 | Loss: 0.00007937
Iteration 224/1000 | Loss: 0.00004355
Iteration 225/1000 | Loss: 0.00005916
Iteration 226/1000 | Loss: 0.00003859
Iteration 227/1000 | Loss: 0.00004552
Iteration 228/1000 | Loss: 0.00011935
Iteration 229/1000 | Loss: 0.00004492
Iteration 230/1000 | Loss: 0.00021674
Iteration 231/1000 | Loss: 0.00012293
Iteration 232/1000 | Loss: 0.00004041
Iteration 233/1000 | Loss: 0.00016706
Iteration 234/1000 | Loss: 0.00005306
Iteration 235/1000 | Loss: 0.00004827
Iteration 236/1000 | Loss: 0.00003960
Iteration 237/1000 | Loss: 0.00005586
Iteration 238/1000 | Loss: 0.00030695
Iteration 239/1000 | Loss: 0.00003910
Iteration 240/1000 | Loss: 0.00014165
Iteration 241/1000 | Loss: 0.00004063
Iteration 242/1000 | Loss: 0.00004485
Iteration 243/1000 | Loss: 0.00004265
Iteration 244/1000 | Loss: 0.00008994
Iteration 245/1000 | Loss: 0.00004314
Iteration 246/1000 | Loss: 0.00005071
Iteration 247/1000 | Loss: 0.00005473
Iteration 248/1000 | Loss: 0.00005135
Iteration 249/1000 | Loss: 0.00005074
Iteration 250/1000 | Loss: 0.00004590
Iteration 251/1000 | Loss: 0.00004937
Iteration 252/1000 | Loss: 0.00004420
Iteration 253/1000 | Loss: 0.00012502
Iteration 254/1000 | Loss: 0.00005842
Iteration 255/1000 | Loss: 0.00004268
Iteration 256/1000 | Loss: 0.00003677
Iteration 257/1000 | Loss: 0.00007583
Iteration 258/1000 | Loss: 0.00004249
Iteration 259/1000 | Loss: 0.00004920
Iteration 260/1000 | Loss: 0.00004961
Iteration 261/1000 | Loss: 0.00005546
Iteration 262/1000 | Loss: 0.00004379
Iteration 263/1000 | Loss: 0.00046988
Iteration 264/1000 | Loss: 0.00005118
Iteration 265/1000 | Loss: 0.00004224
Iteration 266/1000 | Loss: 0.00013323
Iteration 267/1000 | Loss: 0.00005246
Iteration 268/1000 | Loss: 0.00008673
Iteration 269/1000 | Loss: 0.00004613
Iteration 270/1000 | Loss: 0.00005611
Iteration 271/1000 | Loss: 0.00004428
Iteration 272/1000 | Loss: 0.00004507
Iteration 273/1000 | Loss: 0.00014987
Iteration 274/1000 | Loss: 0.00004729
Iteration 275/1000 | Loss: 0.00004253
Iteration 276/1000 | Loss: 0.00004462
Iteration 277/1000 | Loss: 0.00006237
Iteration 278/1000 | Loss: 0.00004649
Iteration 279/1000 | Loss: 0.00004455
Iteration 280/1000 | Loss: 0.00004184
Iteration 281/1000 | Loss: 0.00008123
Iteration 282/1000 | Loss: 0.00010570
Iteration 283/1000 | Loss: 0.00007027
Iteration 284/1000 | Loss: 0.00008356
Iteration 285/1000 | Loss: 0.00003011
Iteration 286/1000 | Loss: 0.00006825
Iteration 287/1000 | Loss: 0.00011148
Iteration 288/1000 | Loss: 0.00002310
Iteration 289/1000 | Loss: 0.00004586
Iteration 290/1000 | Loss: 0.00002138
Iteration 291/1000 | Loss: 0.00003638
Iteration 292/1000 | Loss: 0.00002123
Iteration 293/1000 | Loss: 0.00002094
Iteration 294/1000 | Loss: 0.00003312
Iteration 295/1000 | Loss: 0.00002060
Iteration 296/1000 | Loss: 0.00002047
Iteration 297/1000 | Loss: 0.00085818
Iteration 298/1000 | Loss: 0.00056772
Iteration 299/1000 | Loss: 0.00024268
Iteration 300/1000 | Loss: 0.00007320
Iteration 301/1000 | Loss: 0.00019225
Iteration 302/1000 | Loss: 0.00002108
Iteration 303/1000 | Loss: 0.00006047
Iteration 304/1000 | Loss: 0.00009586
Iteration 305/1000 | Loss: 0.00006151
Iteration 306/1000 | Loss: 0.00001467
Iteration 307/1000 | Loss: 0.00002451
Iteration 308/1000 | Loss: 0.00001376
Iteration 309/1000 | Loss: 0.00021179
Iteration 310/1000 | Loss: 0.00004408
Iteration 311/1000 | Loss: 0.00001592
Iteration 312/1000 | Loss: 0.00014911
Iteration 313/1000 | Loss: 0.00022937
Iteration 314/1000 | Loss: 0.00080472
Iteration 315/1000 | Loss: 0.00070472
Iteration 316/1000 | Loss: 0.00005567
Iteration 317/1000 | Loss: 0.00006882
Iteration 318/1000 | Loss: 0.00025214
Iteration 319/1000 | Loss: 0.00011759
Iteration 320/1000 | Loss: 0.00003268
Iteration 321/1000 | Loss: 0.00011169
Iteration 322/1000 | Loss: 0.00002221
Iteration 323/1000 | Loss: 0.00011634
Iteration 324/1000 | Loss: 0.00022520
Iteration 325/1000 | Loss: 0.00004125
Iteration 326/1000 | Loss: 0.00002194
Iteration 327/1000 | Loss: 0.00001721
Iteration 328/1000 | Loss: 0.00001576
Iteration 329/1000 | Loss: 0.00001499
Iteration 330/1000 | Loss: 0.00002684
Iteration 331/1000 | Loss: 0.00001435
Iteration 332/1000 | Loss: 0.00001399
Iteration 333/1000 | Loss: 0.00008740
Iteration 334/1000 | Loss: 0.00001630
Iteration 335/1000 | Loss: 0.00015862
Iteration 336/1000 | Loss: 0.00032108
Iteration 337/1000 | Loss: 0.00007884
Iteration 338/1000 | Loss: 0.00005270
Iteration 339/1000 | Loss: 0.00005464
Iteration 340/1000 | Loss: 0.00005442
Iteration 341/1000 | Loss: 0.00001764
Iteration 342/1000 | Loss: 0.00014435
Iteration 343/1000 | Loss: 0.00014371
Iteration 344/1000 | Loss: 0.00016398
Iteration 345/1000 | Loss: 0.00002564
Iteration 346/1000 | Loss: 0.00006609
Iteration 347/1000 | Loss: 0.00002157
Iteration 348/1000 | Loss: 0.00006497
Iteration 349/1000 | Loss: 0.00005730
Iteration 350/1000 | Loss: 0.00010525
Iteration 351/1000 | Loss: 0.00001731
Iteration 352/1000 | Loss: 0.00001574
Iteration 353/1000 | Loss: 0.00002887
Iteration 354/1000 | Loss: 0.00001457
Iteration 355/1000 | Loss: 0.00006818
Iteration 356/1000 | Loss: 0.00001444
Iteration 357/1000 | Loss: 0.00001377
Iteration 358/1000 | Loss: 0.00001370
Iteration 359/1000 | Loss: 0.00001369
Iteration 360/1000 | Loss: 0.00001368
Iteration 361/1000 | Loss: 0.00001368
Iteration 362/1000 | Loss: 0.00001360
Iteration 363/1000 | Loss: 0.00001357
Iteration 364/1000 | Loss: 0.00001357
Iteration 365/1000 | Loss: 0.00001344
Iteration 366/1000 | Loss: 0.00001339
Iteration 367/1000 | Loss: 0.00001320
Iteration 368/1000 | Loss: 0.00001312
Iteration 369/1000 | Loss: 0.00001304
Iteration 370/1000 | Loss: 0.00001293
Iteration 371/1000 | Loss: 0.00006806
Iteration 372/1000 | Loss: 0.00007869
Iteration 373/1000 | Loss: 0.00003472
Iteration 374/1000 | Loss: 0.00001619
Iteration 375/1000 | Loss: 0.00004174
Iteration 376/1000 | Loss: 0.00001279
Iteration 377/1000 | Loss: 0.00001262
Iteration 378/1000 | Loss: 0.00001255
Iteration 379/1000 | Loss: 0.00001252
Iteration 380/1000 | Loss: 0.00001252
Iteration 381/1000 | Loss: 0.00001252
Iteration 382/1000 | Loss: 0.00001252
Iteration 383/1000 | Loss: 0.00001252
Iteration 384/1000 | Loss: 0.00001251
Iteration 385/1000 | Loss: 0.00001251
Iteration 386/1000 | Loss: 0.00001251
Iteration 387/1000 | Loss: 0.00001251
Iteration 388/1000 | Loss: 0.00001250
Iteration 389/1000 | Loss: 0.00001249
Iteration 390/1000 | Loss: 0.00001247
Iteration 391/1000 | Loss: 0.00001243
Iteration 392/1000 | Loss: 0.00001241
Iteration 393/1000 | Loss: 0.00001241
Iteration 394/1000 | Loss: 0.00001240
Iteration 395/1000 | Loss: 0.00001240
Iteration 396/1000 | Loss: 0.00001237
Iteration 397/1000 | Loss: 0.00001235
Iteration 398/1000 | Loss: 0.00001235
Iteration 399/1000 | Loss: 0.00001234
Iteration 400/1000 | Loss: 0.00001234
Iteration 401/1000 | Loss: 0.00001230
Iteration 402/1000 | Loss: 0.00001230
Iteration 403/1000 | Loss: 0.00001229
Iteration 404/1000 | Loss: 0.00001229
Iteration 405/1000 | Loss: 0.00001229
Iteration 406/1000 | Loss: 0.00001229
Iteration 407/1000 | Loss: 0.00001228
Iteration 408/1000 | Loss: 0.00001227
Iteration 409/1000 | Loss: 0.00001226
Iteration 410/1000 | Loss: 0.00001226
Iteration 411/1000 | Loss: 0.00001226
Iteration 412/1000 | Loss: 0.00001226
Iteration 413/1000 | Loss: 0.00001226
Iteration 414/1000 | Loss: 0.00001226
Iteration 415/1000 | Loss: 0.00001226
Iteration 416/1000 | Loss: 0.00001226
Iteration 417/1000 | Loss: 0.00001225
Iteration 418/1000 | Loss: 0.00001225
Iteration 419/1000 | Loss: 0.00001225
Iteration 420/1000 | Loss: 0.00001224
Iteration 421/1000 | Loss: 0.00001224
Iteration 422/1000 | Loss: 0.00001223
Iteration 423/1000 | Loss: 0.00001223
Iteration 424/1000 | Loss: 0.00001223
Iteration 425/1000 | Loss: 0.00008498
Iteration 426/1000 | Loss: 0.00001811
Iteration 427/1000 | Loss: 0.00006044
Iteration 428/1000 | Loss: 0.00001473
Iteration 429/1000 | Loss: 0.00001870
Iteration 430/1000 | Loss: 0.00001232
Iteration 431/1000 | Loss: 0.00001824
Iteration 432/1000 | Loss: 0.00001767
Iteration 433/1000 | Loss: 0.00001218
Iteration 434/1000 | Loss: 0.00001218
Iteration 435/1000 | Loss: 0.00001218
Iteration 436/1000 | Loss: 0.00001218
Iteration 437/1000 | Loss: 0.00001218
Iteration 438/1000 | Loss: 0.00001218
Iteration 439/1000 | Loss: 0.00001218
Iteration 440/1000 | Loss: 0.00001218
Iteration 441/1000 | Loss: 0.00001218
Iteration 442/1000 | Loss: 0.00001218
Iteration 443/1000 | Loss: 0.00001218
Iteration 444/1000 | Loss: 0.00001218
Iteration 445/1000 | Loss: 0.00001218
Iteration 446/1000 | Loss: 0.00001218
Iteration 447/1000 | Loss: 0.00001217
Iteration 448/1000 | Loss: 0.00001217
Iteration 449/1000 | Loss: 0.00001217
Iteration 450/1000 | Loss: 0.00001217
Iteration 451/1000 | Loss: 0.00001217
Iteration 452/1000 | Loss: 0.00001217
Iteration 453/1000 | Loss: 0.00001217
Iteration 454/1000 | Loss: 0.00001217
Iteration 455/1000 | Loss: 0.00001217
Iteration 456/1000 | Loss: 0.00001217
Iteration 457/1000 | Loss: 0.00001216
Iteration 458/1000 | Loss: 0.00001216
Iteration 459/1000 | Loss: 0.00001216
Iteration 460/1000 | Loss: 0.00001216
Iteration 461/1000 | Loss: 0.00001216
Iteration 462/1000 | Loss: 0.00001216
Iteration 463/1000 | Loss: 0.00001216
Iteration 464/1000 | Loss: 0.00001216
Iteration 465/1000 | Loss: 0.00001216
Iteration 466/1000 | Loss: 0.00001216
Iteration 467/1000 | Loss: 0.00001216
Iteration 468/1000 | Loss: 0.00001216
Iteration 469/1000 | Loss: 0.00001215
Iteration 470/1000 | Loss: 0.00001215
Iteration 471/1000 | Loss: 0.00001215
Iteration 472/1000 | Loss: 0.00001215
Iteration 473/1000 | Loss: 0.00001215
Iteration 474/1000 | Loss: 0.00001215
Iteration 475/1000 | Loss: 0.00001215
Iteration 476/1000 | Loss: 0.00001215
Iteration 477/1000 | Loss: 0.00001215
Iteration 478/1000 | Loss: 0.00001215
Iteration 479/1000 | Loss: 0.00001215
Iteration 480/1000 | Loss: 0.00001215
Iteration 481/1000 | Loss: 0.00001215
Iteration 482/1000 | Loss: 0.00001214
Iteration 483/1000 | Loss: 0.00001214
Iteration 484/1000 | Loss: 0.00001214
Iteration 485/1000 | Loss: 0.00001214
Iteration 486/1000 | Loss: 0.00001214
Iteration 487/1000 | Loss: 0.00001214
Iteration 488/1000 | Loss: 0.00001214
Iteration 489/1000 | Loss: 0.00001214
Iteration 490/1000 | Loss: 0.00001214
Iteration 491/1000 | Loss: 0.00001214
Iteration 492/1000 | Loss: 0.00001214
Iteration 493/1000 | Loss: 0.00001214
Iteration 494/1000 | Loss: 0.00001214
Iteration 495/1000 | Loss: 0.00001214
Iteration 496/1000 | Loss: 0.00014163
Iteration 497/1000 | Loss: 0.00024663
Iteration 498/1000 | Loss: 0.00010305
Iteration 499/1000 | Loss: 0.00009258
Iteration 500/1000 | Loss: 0.00002089
Iteration 501/1000 | Loss: 0.00001812
Iteration 502/1000 | Loss: 0.00002694
Iteration 503/1000 | Loss: 0.00003482
Iteration 504/1000 | Loss: 0.00001474
Iteration 505/1000 | Loss: 0.00001436
Iteration 506/1000 | Loss: 0.00007715
Iteration 507/1000 | Loss: 0.00006045
Iteration 508/1000 | Loss: 0.00001431
Iteration 509/1000 | Loss: 0.00001379
Iteration 510/1000 | Loss: 0.00001363
Iteration 511/1000 | Loss: 0.00001353
Iteration 512/1000 | Loss: 0.00001335
Iteration 513/1000 | Loss: 0.00001329
Iteration 514/1000 | Loss: 0.00015522
Iteration 515/1000 | Loss: 0.00003963
Iteration 516/1000 | Loss: 0.00012777
Iteration 517/1000 | Loss: 0.00002764
Iteration 518/1000 | Loss: 0.00001427
Iteration 519/1000 | Loss: 0.00014369
Iteration 520/1000 | Loss: 0.00015216
Iteration 521/1000 | Loss: 0.00020555
Iteration 522/1000 | Loss: 0.00011253
Iteration 523/1000 | Loss: 0.00010484
Iteration 524/1000 | Loss: 0.00003331
Iteration 525/1000 | Loss: 0.00001394
Iteration 526/1000 | Loss: 0.00001285
Iteration 527/1000 | Loss: 0.00001256
Iteration 528/1000 | Loss: 0.00001253
Iteration 529/1000 | Loss: 0.00001248
Iteration 530/1000 | Loss: 0.00001241
Iteration 531/1000 | Loss: 0.00001237
Iteration 532/1000 | Loss: 0.00001236
Iteration 533/1000 | Loss: 0.00001234
Iteration 534/1000 | Loss: 0.00001234
Iteration 535/1000 | Loss: 0.00001234
Iteration 536/1000 | Loss: 0.00001234
Iteration 537/1000 | Loss: 0.00001234
Iteration 538/1000 | Loss: 0.00001233
Iteration 539/1000 | Loss: 0.00001233
Iteration 540/1000 | Loss: 0.00001233
Iteration 541/1000 | Loss: 0.00001232
Iteration 542/1000 | Loss: 0.00001232
Iteration 543/1000 | Loss: 0.00001231
Iteration 544/1000 | Loss: 0.00001231
Iteration 545/1000 | Loss: 0.00001231
Iteration 546/1000 | Loss: 0.00001231
Iteration 547/1000 | Loss: 0.00001230
Iteration 548/1000 | Loss: 0.00001230
Iteration 549/1000 | Loss: 0.00001230
Iteration 550/1000 | Loss: 0.00001229
Iteration 551/1000 | Loss: 0.00001229
Iteration 552/1000 | Loss: 0.00001229
Iteration 553/1000 | Loss: 0.00001229
Iteration 554/1000 | Loss: 0.00001228
Iteration 555/1000 | Loss: 0.00001228
Iteration 556/1000 | Loss: 0.00001228
Iteration 557/1000 | Loss: 0.00001227
Iteration 558/1000 | Loss: 0.00001227
Iteration 559/1000 | Loss: 0.00001227
Iteration 560/1000 | Loss: 0.00001227
Iteration 561/1000 | Loss: 0.00001227
Iteration 562/1000 | Loss: 0.00001227
Iteration 563/1000 | Loss: 0.00001227
Iteration 564/1000 | Loss: 0.00001227
Iteration 565/1000 | Loss: 0.00001227
Iteration 566/1000 | Loss: 0.00001227
Iteration 567/1000 | Loss: 0.00001227
Iteration 568/1000 | Loss: 0.00001227
Iteration 569/1000 | Loss: 0.00001227
Iteration 570/1000 | Loss: 0.00001227
Iteration 571/1000 | Loss: 0.00001227
Iteration 572/1000 | Loss: 0.00001227
Iteration 573/1000 | Loss: 0.00001227
Iteration 574/1000 | Loss: 0.00001227
Iteration 575/1000 | Loss: 0.00001227
Iteration 576/1000 | Loss: 0.00001227
Iteration 577/1000 | Loss: 0.00001227
Iteration 578/1000 | Loss: 0.00001227
Iteration 579/1000 | Loss: 0.00001226
Iteration 580/1000 | Loss: 0.00001226
Iteration 581/1000 | Loss: 0.00001226
Iteration 582/1000 | Loss: 0.00001226
Iteration 583/1000 | Loss: 0.00001226
Iteration 584/1000 | Loss: 0.00001226
Iteration 585/1000 | Loss: 0.00001226
Iteration 586/1000 | Loss: 0.00001226
Iteration 587/1000 | Loss: 0.00001226
Iteration 588/1000 | Loss: 0.00001226
Iteration 589/1000 | Loss: 0.00001226
Iteration 590/1000 | Loss: 0.00001225
Iteration 591/1000 | Loss: 0.00001225
Iteration 592/1000 | Loss: 0.00001225
Iteration 593/1000 | Loss: 0.00001225
Iteration 594/1000 | Loss: 0.00001225
Iteration 595/1000 | Loss: 0.00001225
Iteration 596/1000 | Loss: 0.00001225
Iteration 597/1000 | Loss: 0.00001225
Iteration 598/1000 | Loss: 0.00001225
Iteration 599/1000 | Loss: 0.00001225
Iteration 600/1000 | Loss: 0.00001224
Iteration 601/1000 | Loss: 0.00001224
Iteration 602/1000 | Loss: 0.00001224
Iteration 603/1000 | Loss: 0.00001224
Iteration 604/1000 | Loss: 0.00001224
Iteration 605/1000 | Loss: 0.00001224
Iteration 606/1000 | Loss: 0.00001224
Iteration 607/1000 | Loss: 0.00001224
Iteration 608/1000 | Loss: 0.00001223
Iteration 609/1000 | Loss: 0.00001223
Iteration 610/1000 | Loss: 0.00001223
Iteration 611/1000 | Loss: 0.00001223
Iteration 612/1000 | Loss: 0.00001223
Iteration 613/1000 | Loss: 0.00001223
Iteration 614/1000 | Loss: 0.00001223
Iteration 615/1000 | Loss: 0.00001223
Iteration 616/1000 | Loss: 0.00001223
Iteration 617/1000 | Loss: 0.00001223
Iteration 618/1000 | Loss: 0.00001223
Iteration 619/1000 | Loss: 0.00001223
Iteration 620/1000 | Loss: 0.00001223
Iteration 621/1000 | Loss: 0.00001223
Iteration 622/1000 | Loss: 0.00001223
Iteration 623/1000 | Loss: 0.00001222
Iteration 624/1000 | Loss: 0.00001222
Iteration 625/1000 | Loss: 0.00001222
Iteration 626/1000 | Loss: 0.00001222
Iteration 627/1000 | Loss: 0.00001222
Iteration 628/1000 | Loss: 0.00001222
Iteration 629/1000 | Loss: 0.00005960
Iteration 630/1000 | Loss: 0.00001238
Iteration 631/1000 | Loss: 0.00001222
Iteration 632/1000 | Loss: 0.00001222
Iteration 633/1000 | Loss: 0.00001222
Iteration 634/1000 | Loss: 0.00001222
Iteration 635/1000 | Loss: 0.00001221
Iteration 636/1000 | Loss: 0.00001221
Iteration 637/1000 | Loss: 0.00001221
Iteration 638/1000 | Loss: 0.00001221
Iteration 639/1000 | Loss: 0.00001221
Iteration 640/1000 | Loss: 0.00001221
Iteration 641/1000 | Loss: 0.00001221
Iteration 642/1000 | Loss: 0.00001220
Iteration 643/1000 | Loss: 0.00001220
Iteration 644/1000 | Loss: 0.00001220
Iteration 645/1000 | Loss: 0.00001220
Iteration 646/1000 | Loss: 0.00001220
Iteration 647/1000 | Loss: 0.00001220
Iteration 648/1000 | Loss: 0.00001219
Iteration 649/1000 | Loss: 0.00001219
Iteration 650/1000 | Loss: 0.00001219
Iteration 651/1000 | Loss: 0.00001219
Iteration 652/1000 | Loss: 0.00001219
Iteration 653/1000 | Loss: 0.00001219
Iteration 654/1000 | Loss: 0.00001218
Iteration 655/1000 | Loss: 0.00001218
Iteration 656/1000 | Loss: 0.00001218
Iteration 657/1000 | Loss: 0.00001218
Iteration 658/1000 | Loss: 0.00001218
Iteration 659/1000 | Loss: 0.00001218
Iteration 660/1000 | Loss: 0.00001218
Iteration 661/1000 | Loss: 0.00001218
Iteration 662/1000 | Loss: 0.00001218
Iteration 663/1000 | Loss: 0.00001218
Iteration 664/1000 | Loss: 0.00001218
Iteration 665/1000 | Loss: 0.00001218
Iteration 666/1000 | Loss: 0.00001218
Iteration 667/1000 | Loss: 0.00001218
Iteration 668/1000 | Loss: 0.00001217
Iteration 669/1000 | Loss: 0.00001217
Iteration 670/1000 | Loss: 0.00001217
Iteration 671/1000 | Loss: 0.00001217
Iteration 672/1000 | Loss: 0.00001217
Iteration 673/1000 | Loss: 0.00001217
Iteration 674/1000 | Loss: 0.00001217
Iteration 675/1000 | Loss: 0.00001217
Iteration 676/1000 | Loss: 0.00001217
Iteration 677/1000 | Loss: 0.00001217
Iteration 678/1000 | Loss: 0.00001217
Iteration 679/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 679. Stopping optimization.
Last 5 losses: [1.2173582945251837e-05, 1.2173582945251837e-05, 1.2173582945251837e-05, 1.2173582945251837e-05, 1.2173582945251837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2173582945251837e-05

Optimization complete. Final v2v error: 2.8278636932373047 mm

Highest mean error: 9.071614265441895 mm for frame 223

Lowest mean error: 2.446418046951294 mm for frame 197

Saving results

Total time: 727.3915040493011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778246
Iteration 2/25 | Loss: 0.00135101
Iteration 3/25 | Loss: 0.00126180
Iteration 4/25 | Loss: 0.00125384
Iteration 5/25 | Loss: 0.00125026
Iteration 6/25 | Loss: 0.00124901
Iteration 7/25 | Loss: 0.00124898
Iteration 8/25 | Loss: 0.00124898
Iteration 9/25 | Loss: 0.00124898
Iteration 10/25 | Loss: 0.00124898
Iteration 11/25 | Loss: 0.00124898
Iteration 12/25 | Loss: 0.00124898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001248979358933866, 0.001248979358933866, 0.001248979358933866, 0.001248979358933866, 0.001248979358933866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248979358933866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.35405254
Iteration 2/25 | Loss: 0.00159067
Iteration 3/25 | Loss: 0.00159066
Iteration 4/25 | Loss: 0.00159066
Iteration 5/25 | Loss: 0.00159066
Iteration 6/25 | Loss: 0.00159066
Iteration 7/25 | Loss: 0.00159066
Iteration 8/25 | Loss: 0.00159066
Iteration 9/25 | Loss: 0.00159066
Iteration 10/25 | Loss: 0.00159066
Iteration 11/25 | Loss: 0.00159066
Iteration 12/25 | Loss: 0.00159066
Iteration 13/25 | Loss: 0.00159066
Iteration 14/25 | Loss: 0.00159066
Iteration 15/25 | Loss: 0.00159066
Iteration 16/25 | Loss: 0.00159066
Iteration 17/25 | Loss: 0.00159066
Iteration 18/25 | Loss: 0.00159066
Iteration 19/25 | Loss: 0.00159066
Iteration 20/25 | Loss: 0.00159066
Iteration 21/25 | Loss: 0.00159066
Iteration 22/25 | Loss: 0.00159066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015906611224636436, 0.0015906611224636436, 0.0015906611224636436, 0.0015906611224636436, 0.0015906611224636436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015906611224636436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159066
Iteration 2/1000 | Loss: 0.00004048
Iteration 3/1000 | Loss: 0.00002428
Iteration 4/1000 | Loss: 0.00002045
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001774
Iteration 8/1000 | Loss: 0.00001734
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001696
Iteration 12/1000 | Loss: 0.00001684
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001643
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001642
Iteration 31/1000 | Loss: 0.00001642
Iteration 32/1000 | Loss: 0.00001642
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001640
Iteration 43/1000 | Loss: 0.00001640
Iteration 44/1000 | Loss: 0.00001640
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001638
Iteration 48/1000 | Loss: 0.00001638
Iteration 49/1000 | Loss: 0.00001638
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001637
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001636
Iteration 55/1000 | Loss: 0.00001636
Iteration 56/1000 | Loss: 0.00001636
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001633
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001632
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001632
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001632
Iteration 87/1000 | Loss: 0.00001632
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001632
Iteration 91/1000 | Loss: 0.00001632
Iteration 92/1000 | Loss: 0.00001632
Iteration 93/1000 | Loss: 0.00001632
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00001631
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001630
Iteration 106/1000 | Loss: 0.00001630
Iteration 107/1000 | Loss: 0.00001630
Iteration 108/1000 | Loss: 0.00001630
Iteration 109/1000 | Loss: 0.00001630
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001629
Iteration 114/1000 | Loss: 0.00001629
Iteration 115/1000 | Loss: 0.00001629
Iteration 116/1000 | Loss: 0.00001629
Iteration 117/1000 | Loss: 0.00001629
Iteration 118/1000 | Loss: 0.00001629
Iteration 119/1000 | Loss: 0.00001629
Iteration 120/1000 | Loss: 0.00001628
Iteration 121/1000 | Loss: 0.00001628
Iteration 122/1000 | Loss: 0.00001628
Iteration 123/1000 | Loss: 0.00001628
Iteration 124/1000 | Loss: 0.00001628
Iteration 125/1000 | Loss: 0.00001628
Iteration 126/1000 | Loss: 0.00001628
Iteration 127/1000 | Loss: 0.00001628
Iteration 128/1000 | Loss: 0.00001628
Iteration 129/1000 | Loss: 0.00001628
Iteration 130/1000 | Loss: 0.00001628
Iteration 131/1000 | Loss: 0.00001627
Iteration 132/1000 | Loss: 0.00001627
Iteration 133/1000 | Loss: 0.00001627
Iteration 134/1000 | Loss: 0.00001627
Iteration 135/1000 | Loss: 0.00001627
Iteration 136/1000 | Loss: 0.00001627
Iteration 137/1000 | Loss: 0.00001627
Iteration 138/1000 | Loss: 0.00001627
Iteration 139/1000 | Loss: 0.00001627
Iteration 140/1000 | Loss: 0.00001627
Iteration 141/1000 | Loss: 0.00001627
Iteration 142/1000 | Loss: 0.00001627
Iteration 143/1000 | Loss: 0.00001627
Iteration 144/1000 | Loss: 0.00001627
Iteration 145/1000 | Loss: 0.00001627
Iteration 146/1000 | Loss: 0.00001626
Iteration 147/1000 | Loss: 0.00001626
Iteration 148/1000 | Loss: 0.00001626
Iteration 149/1000 | Loss: 0.00001626
Iteration 150/1000 | Loss: 0.00001626
Iteration 151/1000 | Loss: 0.00001626
Iteration 152/1000 | Loss: 0.00001626
Iteration 153/1000 | Loss: 0.00001626
Iteration 154/1000 | Loss: 0.00001626
Iteration 155/1000 | Loss: 0.00001626
Iteration 156/1000 | Loss: 0.00001626
Iteration 157/1000 | Loss: 0.00001626
Iteration 158/1000 | Loss: 0.00001626
Iteration 159/1000 | Loss: 0.00001626
Iteration 160/1000 | Loss: 0.00001626
Iteration 161/1000 | Loss: 0.00001626
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001625
Iteration 167/1000 | Loss: 0.00001625
Iteration 168/1000 | Loss: 0.00001625
Iteration 169/1000 | Loss: 0.00001625
Iteration 170/1000 | Loss: 0.00001625
Iteration 171/1000 | Loss: 0.00001625
Iteration 172/1000 | Loss: 0.00001625
Iteration 173/1000 | Loss: 0.00001625
Iteration 174/1000 | Loss: 0.00001625
Iteration 175/1000 | Loss: 0.00001625
Iteration 176/1000 | Loss: 0.00001625
Iteration 177/1000 | Loss: 0.00001625
Iteration 178/1000 | Loss: 0.00001625
Iteration 179/1000 | Loss: 0.00001624
Iteration 180/1000 | Loss: 0.00001624
Iteration 181/1000 | Loss: 0.00001624
Iteration 182/1000 | Loss: 0.00001624
Iteration 183/1000 | Loss: 0.00001624
Iteration 184/1000 | Loss: 0.00001624
Iteration 185/1000 | Loss: 0.00001624
Iteration 186/1000 | Loss: 0.00001624
Iteration 187/1000 | Loss: 0.00001624
Iteration 188/1000 | Loss: 0.00001624
Iteration 189/1000 | Loss: 0.00001624
Iteration 190/1000 | Loss: 0.00001624
Iteration 191/1000 | Loss: 0.00001624
Iteration 192/1000 | Loss: 0.00001624
Iteration 193/1000 | Loss: 0.00001624
Iteration 194/1000 | Loss: 0.00001624
Iteration 195/1000 | Loss: 0.00001624
Iteration 196/1000 | Loss: 0.00001624
Iteration 197/1000 | Loss: 0.00001624
Iteration 198/1000 | Loss: 0.00001624
Iteration 199/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.623923708393704e-05, 1.623923708393704e-05, 1.623923708393704e-05, 1.623923708393704e-05, 1.623923708393704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.623923708393704e-05

Optimization complete. Final v2v error: 3.3364744186401367 mm

Highest mean error: 3.734700918197632 mm for frame 37

Lowest mean error: 3.0584919452667236 mm for frame 50

Saving results

Total time: 37.734392166137695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582716
Iteration 2/25 | Loss: 0.00143869
Iteration 3/25 | Loss: 0.00130309
Iteration 4/25 | Loss: 0.00128714
Iteration 5/25 | Loss: 0.00128104
Iteration 6/25 | Loss: 0.00128104
Iteration 7/25 | Loss: 0.00128104
Iteration 8/25 | Loss: 0.00128104
Iteration 9/25 | Loss: 0.00128104
Iteration 10/25 | Loss: 0.00128104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012810436310246587, 0.0012810436310246587, 0.0012810436310246587, 0.0012810436310246587, 0.0012810436310246587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012810436310246587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.11972618
Iteration 2/25 | Loss: 0.00126211
Iteration 3/25 | Loss: 0.00126206
Iteration 4/25 | Loss: 0.00126206
Iteration 5/25 | Loss: 0.00126206
Iteration 6/25 | Loss: 0.00126206
Iteration 7/25 | Loss: 0.00126206
Iteration 8/25 | Loss: 0.00126206
Iteration 9/25 | Loss: 0.00126206
Iteration 10/25 | Loss: 0.00126206
Iteration 11/25 | Loss: 0.00126206
Iteration 12/25 | Loss: 0.00126206
Iteration 13/25 | Loss: 0.00126206
Iteration 14/25 | Loss: 0.00126206
Iteration 15/25 | Loss: 0.00126206
Iteration 16/25 | Loss: 0.00126206
Iteration 17/25 | Loss: 0.00126206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012620603665709496, 0.0012620603665709496, 0.0012620603665709496, 0.0012620603665709496, 0.0012620603665709496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012620603665709496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126206
Iteration 2/1000 | Loss: 0.00004720
Iteration 3/1000 | Loss: 0.00003070
Iteration 4/1000 | Loss: 0.00002810
Iteration 5/1000 | Loss: 0.00002700
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002604
Iteration 8/1000 | Loss: 0.00002557
Iteration 9/1000 | Loss: 0.00002518
Iteration 10/1000 | Loss: 0.00002497
Iteration 11/1000 | Loss: 0.00002491
Iteration 12/1000 | Loss: 0.00002489
Iteration 13/1000 | Loss: 0.00002484
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002464
Iteration 16/1000 | Loss: 0.00002463
Iteration 17/1000 | Loss: 0.00002457
Iteration 18/1000 | Loss: 0.00002457
Iteration 19/1000 | Loss: 0.00002451
Iteration 20/1000 | Loss: 0.00002451
Iteration 21/1000 | Loss: 0.00002450
Iteration 22/1000 | Loss: 0.00002448
Iteration 23/1000 | Loss: 0.00002445
Iteration 24/1000 | Loss: 0.00002445
Iteration 25/1000 | Loss: 0.00002445
Iteration 26/1000 | Loss: 0.00002444
Iteration 27/1000 | Loss: 0.00002442
Iteration 28/1000 | Loss: 0.00002442
Iteration 29/1000 | Loss: 0.00002441
Iteration 30/1000 | Loss: 0.00002441
Iteration 31/1000 | Loss: 0.00002440
Iteration 32/1000 | Loss: 0.00002439
Iteration 33/1000 | Loss: 0.00002439
Iteration 34/1000 | Loss: 0.00002439
Iteration 35/1000 | Loss: 0.00002439
Iteration 36/1000 | Loss: 0.00002439
Iteration 37/1000 | Loss: 0.00002438
Iteration 38/1000 | Loss: 0.00002438
Iteration 39/1000 | Loss: 0.00002437
Iteration 40/1000 | Loss: 0.00002437
Iteration 41/1000 | Loss: 0.00002437
Iteration 42/1000 | Loss: 0.00002437
Iteration 43/1000 | Loss: 0.00002437
Iteration 44/1000 | Loss: 0.00002437
Iteration 45/1000 | Loss: 0.00002437
Iteration 46/1000 | Loss: 0.00002437
Iteration 47/1000 | Loss: 0.00002436
Iteration 48/1000 | Loss: 0.00002436
Iteration 49/1000 | Loss: 0.00002436
Iteration 50/1000 | Loss: 0.00002436
Iteration 51/1000 | Loss: 0.00002436
Iteration 52/1000 | Loss: 0.00002436
Iteration 53/1000 | Loss: 0.00002436
Iteration 54/1000 | Loss: 0.00002436
Iteration 55/1000 | Loss: 0.00002436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [2.436347676848527e-05, 2.436347676848527e-05, 2.436347676848527e-05, 2.436347676848527e-05, 2.436347676848527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.436347676848527e-05

Optimization complete. Final v2v error: 4.204005241394043 mm

Highest mean error: 4.5654425621032715 mm for frame 11

Lowest mean error: 3.647082567214966 mm for frame 233

Saving results

Total time: 33.50567030906677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761526
Iteration 2/25 | Loss: 0.00184161
Iteration 3/25 | Loss: 0.00142175
Iteration 4/25 | Loss: 0.00135146
Iteration 5/25 | Loss: 0.00133917
Iteration 6/25 | Loss: 0.00132691
Iteration 7/25 | Loss: 0.00132152
Iteration 8/25 | Loss: 0.00131939
Iteration 9/25 | Loss: 0.00131888
Iteration 10/25 | Loss: 0.00131871
Iteration 11/25 | Loss: 0.00131865
Iteration 12/25 | Loss: 0.00131865
Iteration 13/25 | Loss: 0.00131865
Iteration 14/25 | Loss: 0.00131865
Iteration 15/25 | Loss: 0.00131865
Iteration 16/25 | Loss: 0.00131865
Iteration 17/25 | Loss: 0.00131865
Iteration 18/25 | Loss: 0.00131864
Iteration 19/25 | Loss: 0.00131864
Iteration 20/25 | Loss: 0.00131864
Iteration 21/25 | Loss: 0.00131864
Iteration 22/25 | Loss: 0.00131864
Iteration 23/25 | Loss: 0.00131864
Iteration 24/25 | Loss: 0.00131864
Iteration 25/25 | Loss: 0.00131864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21116734
Iteration 2/25 | Loss: 0.00167519
Iteration 3/25 | Loss: 0.00167518
Iteration 4/25 | Loss: 0.00167518
Iteration 5/25 | Loss: 0.00167518
Iteration 6/25 | Loss: 0.00167518
Iteration 7/25 | Loss: 0.00167518
Iteration 8/25 | Loss: 0.00167518
Iteration 9/25 | Loss: 0.00167518
Iteration 10/25 | Loss: 0.00167518
Iteration 11/25 | Loss: 0.00167518
Iteration 12/25 | Loss: 0.00167518
Iteration 13/25 | Loss: 0.00167518
Iteration 14/25 | Loss: 0.00167518
Iteration 15/25 | Loss: 0.00167518
Iteration 16/25 | Loss: 0.00167518
Iteration 17/25 | Loss: 0.00167518
Iteration 18/25 | Loss: 0.00167518
Iteration 19/25 | Loss: 0.00167518
Iteration 20/25 | Loss: 0.00167518
Iteration 21/25 | Loss: 0.00167518
Iteration 22/25 | Loss: 0.00167518
Iteration 23/25 | Loss: 0.00167518
Iteration 24/25 | Loss: 0.00167518
Iteration 25/25 | Loss: 0.00167518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167518
Iteration 2/1000 | Loss: 0.00012632
Iteration 3/1000 | Loss: 0.00426350
Iteration 4/1000 | Loss: 0.00401272
Iteration 5/1000 | Loss: 0.00019492
Iteration 6/1000 | Loss: 0.00026235
Iteration 7/1000 | Loss: 0.00007056
Iteration 8/1000 | Loss: 0.00357498
Iteration 9/1000 | Loss: 0.00357721
Iteration 10/1000 | Loss: 0.00166823
Iteration 11/1000 | Loss: 0.00084821
Iteration 12/1000 | Loss: 0.00006918
Iteration 13/1000 | Loss: 0.00005919
Iteration 14/1000 | Loss: 0.00072145
Iteration 15/1000 | Loss: 0.00084390
Iteration 16/1000 | Loss: 0.00015257
Iteration 17/1000 | Loss: 0.00005367
Iteration 18/1000 | Loss: 0.00121255
Iteration 19/1000 | Loss: 0.00239228
Iteration 20/1000 | Loss: 0.00217524
Iteration 21/1000 | Loss: 0.00328328
Iteration 22/1000 | Loss: 0.00174768
Iteration 23/1000 | Loss: 0.00157014
Iteration 24/1000 | Loss: 0.00085711
Iteration 25/1000 | Loss: 0.00349272
Iteration 26/1000 | Loss: 0.00263312
Iteration 27/1000 | Loss: 0.00325406
Iteration 28/1000 | Loss: 0.00150143
Iteration 29/1000 | Loss: 0.00222002
Iteration 30/1000 | Loss: 0.00164474
Iteration 31/1000 | Loss: 0.00137249
Iteration 32/1000 | Loss: 0.00069217
Iteration 33/1000 | Loss: 0.00005228
Iteration 34/1000 | Loss: 0.00004600
Iteration 35/1000 | Loss: 0.00004281
Iteration 36/1000 | Loss: 0.00099129
Iteration 37/1000 | Loss: 0.00056111
Iteration 38/1000 | Loss: 0.00006015
Iteration 39/1000 | Loss: 0.00004765
Iteration 40/1000 | Loss: 0.00003988
Iteration 41/1000 | Loss: 0.00003856
Iteration 42/1000 | Loss: 0.00003793
Iteration 43/1000 | Loss: 0.00120702
Iteration 44/1000 | Loss: 0.00253917
Iteration 45/1000 | Loss: 0.00257948
Iteration 46/1000 | Loss: 0.00173093
Iteration 47/1000 | Loss: 0.00229047
Iteration 48/1000 | Loss: 0.00169153
Iteration 49/1000 | Loss: 0.00121464
Iteration 50/1000 | Loss: 0.00095474
Iteration 51/1000 | Loss: 0.00219073
Iteration 52/1000 | Loss: 0.00139830
Iteration 53/1000 | Loss: 0.00235272
Iteration 54/1000 | Loss: 0.00133544
Iteration 55/1000 | Loss: 0.00261915
Iteration 56/1000 | Loss: 0.00172445
Iteration 57/1000 | Loss: 0.00217973
Iteration 58/1000 | Loss: 0.00172585
Iteration 59/1000 | Loss: 0.00076216
Iteration 60/1000 | Loss: 0.00004348
Iteration 61/1000 | Loss: 0.00190072
Iteration 62/1000 | Loss: 0.00067105
Iteration 63/1000 | Loss: 0.00154835
Iteration 64/1000 | Loss: 0.00163619
Iteration 65/1000 | Loss: 0.00211507
Iteration 66/1000 | Loss: 0.00203702
Iteration 67/1000 | Loss: 0.00042021
Iteration 68/1000 | Loss: 0.00006837
Iteration 69/1000 | Loss: 0.00004689
Iteration 70/1000 | Loss: 0.00051432
Iteration 71/1000 | Loss: 0.00183580
Iteration 72/1000 | Loss: 0.00338214
Iteration 73/1000 | Loss: 0.00163279
Iteration 74/1000 | Loss: 0.00011634
Iteration 75/1000 | Loss: 0.00330717
Iteration 76/1000 | Loss: 0.00112414
Iteration 77/1000 | Loss: 0.00234286
Iteration 78/1000 | Loss: 0.00194567
Iteration 79/1000 | Loss: 0.00008384
Iteration 80/1000 | Loss: 0.00251355
Iteration 81/1000 | Loss: 0.00106104
Iteration 82/1000 | Loss: 0.00218099
Iteration 83/1000 | Loss: 0.00164046
Iteration 84/1000 | Loss: 0.00166139
Iteration 85/1000 | Loss: 0.00150717
Iteration 86/1000 | Loss: 0.00033634
Iteration 87/1000 | Loss: 0.00120263
Iteration 88/1000 | Loss: 0.00128837
Iteration 89/1000 | Loss: 0.00156770
Iteration 90/1000 | Loss: 0.00281546
Iteration 91/1000 | Loss: 0.00178800
Iteration 92/1000 | Loss: 0.00315845
Iteration 93/1000 | Loss: 0.00131587
Iteration 94/1000 | Loss: 0.00015257
Iteration 95/1000 | Loss: 0.00141796
Iteration 96/1000 | Loss: 0.00101745
Iteration 97/1000 | Loss: 0.00013051
Iteration 98/1000 | Loss: 0.00005617
Iteration 99/1000 | Loss: 0.00004232
Iteration 100/1000 | Loss: 0.00003707
Iteration 101/1000 | Loss: 0.00074858
Iteration 102/1000 | Loss: 0.00012060
Iteration 103/1000 | Loss: 0.00066373
Iteration 104/1000 | Loss: 0.00006697
Iteration 105/1000 | Loss: 0.00003982
Iteration 106/1000 | Loss: 0.00003635
Iteration 107/1000 | Loss: 0.00003449
Iteration 108/1000 | Loss: 0.00003286
Iteration 109/1000 | Loss: 0.00003150
Iteration 110/1000 | Loss: 0.00003051
Iteration 111/1000 | Loss: 0.00003006
Iteration 112/1000 | Loss: 0.00002969
Iteration 113/1000 | Loss: 0.00002940
Iteration 114/1000 | Loss: 0.00002914
Iteration 115/1000 | Loss: 0.00002891
Iteration 116/1000 | Loss: 0.00002879
Iteration 117/1000 | Loss: 0.00002871
Iteration 118/1000 | Loss: 0.00002867
Iteration 119/1000 | Loss: 0.00002862
Iteration 120/1000 | Loss: 0.00002859
Iteration 121/1000 | Loss: 0.00002855
Iteration 122/1000 | Loss: 0.00002851
Iteration 123/1000 | Loss: 0.00002851
Iteration 124/1000 | Loss: 0.00002849
Iteration 125/1000 | Loss: 0.00002849
Iteration 126/1000 | Loss: 0.00002849
Iteration 127/1000 | Loss: 0.00002849
Iteration 128/1000 | Loss: 0.00002849
Iteration 129/1000 | Loss: 0.00002849
Iteration 130/1000 | Loss: 0.00002849
Iteration 131/1000 | Loss: 0.00002849
Iteration 132/1000 | Loss: 0.00002848
Iteration 133/1000 | Loss: 0.00002848
Iteration 134/1000 | Loss: 0.00002848
Iteration 135/1000 | Loss: 0.00002848
Iteration 136/1000 | Loss: 0.00002848
Iteration 137/1000 | Loss: 0.00002848
Iteration 138/1000 | Loss: 0.00002848
Iteration 139/1000 | Loss: 0.00002847
Iteration 140/1000 | Loss: 0.00002847
Iteration 141/1000 | Loss: 0.00002847
Iteration 142/1000 | Loss: 0.00002847
Iteration 143/1000 | Loss: 0.00002847
Iteration 144/1000 | Loss: 0.00002847
Iteration 145/1000 | Loss: 0.00002847
Iteration 146/1000 | Loss: 0.00002846
Iteration 147/1000 | Loss: 0.00002846
Iteration 148/1000 | Loss: 0.00002846
Iteration 149/1000 | Loss: 0.00002846
Iteration 150/1000 | Loss: 0.00002846
Iteration 151/1000 | Loss: 0.00002846
Iteration 152/1000 | Loss: 0.00002845
Iteration 153/1000 | Loss: 0.00002844
Iteration 154/1000 | Loss: 0.00002843
Iteration 155/1000 | Loss: 0.00002841
Iteration 156/1000 | Loss: 0.00002841
Iteration 157/1000 | Loss: 0.00002840
Iteration 158/1000 | Loss: 0.00002840
Iteration 159/1000 | Loss: 0.00002840
Iteration 160/1000 | Loss: 0.00002839
Iteration 161/1000 | Loss: 0.00002839
Iteration 162/1000 | Loss: 0.00002839
Iteration 163/1000 | Loss: 0.00002839
Iteration 164/1000 | Loss: 0.00002839
Iteration 165/1000 | Loss: 0.00002839
Iteration 166/1000 | Loss: 0.00002839
Iteration 167/1000 | Loss: 0.00002839
Iteration 168/1000 | Loss: 0.00002839
Iteration 169/1000 | Loss: 0.00002839
Iteration 170/1000 | Loss: 0.00002838
Iteration 171/1000 | Loss: 0.00002838
Iteration 172/1000 | Loss: 0.00002838
Iteration 173/1000 | Loss: 0.00002838
Iteration 174/1000 | Loss: 0.00002838
Iteration 175/1000 | Loss: 0.00002838
Iteration 176/1000 | Loss: 0.00002838
Iteration 177/1000 | Loss: 0.00002838
Iteration 178/1000 | Loss: 0.00002838
Iteration 179/1000 | Loss: 0.00002838
Iteration 180/1000 | Loss: 0.00002838
Iteration 181/1000 | Loss: 0.00002838
Iteration 182/1000 | Loss: 0.00002838
Iteration 183/1000 | Loss: 0.00002837
Iteration 184/1000 | Loss: 0.00002837
Iteration 185/1000 | Loss: 0.00002837
Iteration 186/1000 | Loss: 0.00002837
Iteration 187/1000 | Loss: 0.00002837
Iteration 188/1000 | Loss: 0.00002837
Iteration 189/1000 | Loss: 0.00002837
Iteration 190/1000 | Loss: 0.00002837
Iteration 191/1000 | Loss: 0.00002836
Iteration 192/1000 | Loss: 0.00002836
Iteration 193/1000 | Loss: 0.00002836
Iteration 194/1000 | Loss: 0.00002836
Iteration 195/1000 | Loss: 0.00002836
Iteration 196/1000 | Loss: 0.00002836
Iteration 197/1000 | Loss: 0.00002836
Iteration 198/1000 | Loss: 0.00002836
Iteration 199/1000 | Loss: 0.00002836
Iteration 200/1000 | Loss: 0.00002836
Iteration 201/1000 | Loss: 0.00002836
Iteration 202/1000 | Loss: 0.00002836
Iteration 203/1000 | Loss: 0.00002836
Iteration 204/1000 | Loss: 0.00002836
Iteration 205/1000 | Loss: 0.00002836
Iteration 206/1000 | Loss: 0.00002836
Iteration 207/1000 | Loss: 0.00002836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [2.836423482222017e-05, 2.836423482222017e-05, 2.836423482222017e-05, 2.836423482222017e-05, 2.836423482222017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.836423482222017e-05

Optimization complete. Final v2v error: 4.274293422698975 mm

Highest mean error: 9.095829010009766 mm for frame 64

Lowest mean error: 2.9674975872039795 mm for frame 0

Saving results

Total time: 187.03849697113037
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2073/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2073/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769976
Iteration 2/25 | Loss: 0.00236758
Iteration 3/25 | Loss: 0.00182964
Iteration 4/25 | Loss: 0.00176264
Iteration 5/25 | Loss: 0.00173812
Iteration 6/25 | Loss: 0.00167485
Iteration 7/25 | Loss: 0.00163889
Iteration 8/25 | Loss: 0.00171040
Iteration 9/25 | Loss: 0.00175256
Iteration 10/25 | Loss: 0.00155687
Iteration 11/25 | Loss: 0.00144403
Iteration 12/25 | Loss: 0.00144761
Iteration 13/25 | Loss: 0.00144538
Iteration 14/25 | Loss: 0.00145670
Iteration 15/25 | Loss: 0.00135857
Iteration 16/25 | Loss: 0.00133363
Iteration 17/25 | Loss: 0.00132618
Iteration 18/25 | Loss: 0.00132449
Iteration 19/25 | Loss: 0.00132145
Iteration 20/25 | Loss: 0.00131883
Iteration 21/25 | Loss: 0.00131566
Iteration 22/25 | Loss: 0.00131534
Iteration 23/25 | Loss: 0.00131534
Iteration 24/25 | Loss: 0.00131534
Iteration 25/25 | Loss: 0.00131534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36579108
Iteration 2/25 | Loss: 0.00204604
Iteration 3/25 | Loss: 0.00204573
Iteration 4/25 | Loss: 0.00204573
Iteration 5/25 | Loss: 0.00204572
Iteration 6/25 | Loss: 0.00204572
Iteration 7/25 | Loss: 0.00204572
Iteration 8/25 | Loss: 0.00204572
Iteration 9/25 | Loss: 0.00204572
Iteration 10/25 | Loss: 0.00204572
Iteration 11/25 | Loss: 0.00204572
Iteration 12/25 | Loss: 0.00204572
Iteration 13/25 | Loss: 0.00204572
Iteration 14/25 | Loss: 0.00204572
Iteration 15/25 | Loss: 0.00204572
Iteration 16/25 | Loss: 0.00204572
Iteration 17/25 | Loss: 0.00204572
Iteration 18/25 | Loss: 0.00204572
Iteration 19/25 | Loss: 0.00204572
Iteration 20/25 | Loss: 0.00204572
Iteration 21/25 | Loss: 0.00204572
Iteration 22/25 | Loss: 0.00204572
Iteration 23/25 | Loss: 0.00204572
Iteration 24/25 | Loss: 0.00204572
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020457173231989145, 0.0020457173231989145, 0.0020457173231989145, 0.0020457173231989145, 0.0020457173231989145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020457173231989145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204572
Iteration 2/1000 | Loss: 0.00007803
Iteration 3/1000 | Loss: 0.00004546
Iteration 4/1000 | Loss: 0.00003762
Iteration 5/1000 | Loss: 0.00003539
Iteration 6/1000 | Loss: 0.00003388
Iteration 7/1000 | Loss: 0.00007635
Iteration 8/1000 | Loss: 0.00003407
Iteration 9/1000 | Loss: 0.00003151
Iteration 10/1000 | Loss: 0.00003094
Iteration 11/1000 | Loss: 0.00003016
Iteration 12/1000 | Loss: 0.00002969
Iteration 13/1000 | Loss: 0.00002920
Iteration 14/1000 | Loss: 0.00002880
Iteration 15/1000 | Loss: 0.00002849
Iteration 16/1000 | Loss: 0.00002821
Iteration 17/1000 | Loss: 0.00002797
Iteration 18/1000 | Loss: 0.00002778
Iteration 19/1000 | Loss: 0.00002764
Iteration 20/1000 | Loss: 0.00002761
Iteration 21/1000 | Loss: 0.00002760
Iteration 22/1000 | Loss: 0.00002760
Iteration 23/1000 | Loss: 0.00002759
Iteration 24/1000 | Loss: 0.00002759
Iteration 25/1000 | Loss: 0.00002759
Iteration 26/1000 | Loss: 0.00002759
Iteration 27/1000 | Loss: 0.00002759
Iteration 28/1000 | Loss: 0.00002759
Iteration 29/1000 | Loss: 0.00002758
Iteration 30/1000 | Loss: 0.00002756
Iteration 31/1000 | Loss: 0.00002755
Iteration 32/1000 | Loss: 0.00002755
Iteration 33/1000 | Loss: 0.00002755
Iteration 34/1000 | Loss: 0.00002754
Iteration 35/1000 | Loss: 0.00002754
Iteration 36/1000 | Loss: 0.00002754
Iteration 37/1000 | Loss: 0.00002754
Iteration 38/1000 | Loss: 0.00002753
Iteration 39/1000 | Loss: 0.00002751
Iteration 40/1000 | Loss: 0.00002750
Iteration 41/1000 | Loss: 0.00002749
Iteration 42/1000 | Loss: 0.00002749
Iteration 43/1000 | Loss: 0.00002748
Iteration 44/1000 | Loss: 0.00002748
Iteration 45/1000 | Loss: 0.00002748
Iteration 46/1000 | Loss: 0.00002747
Iteration 47/1000 | Loss: 0.00002746
Iteration 48/1000 | Loss: 0.00002746
Iteration 49/1000 | Loss: 0.00002744
Iteration 50/1000 | Loss: 0.00002744
Iteration 51/1000 | Loss: 0.00002744
Iteration 52/1000 | Loss: 0.00002744
Iteration 53/1000 | Loss: 0.00002743
Iteration 54/1000 | Loss: 0.00002743
Iteration 55/1000 | Loss: 0.00002743
Iteration 56/1000 | Loss: 0.00002743
Iteration 57/1000 | Loss: 0.00002743
Iteration 58/1000 | Loss: 0.00002742
Iteration 59/1000 | Loss: 0.00002742
Iteration 60/1000 | Loss: 0.00002741
Iteration 61/1000 | Loss: 0.00002741
Iteration 62/1000 | Loss: 0.00002741
Iteration 63/1000 | Loss: 0.00002741
Iteration 64/1000 | Loss: 0.00002740
Iteration 65/1000 | Loss: 0.00002740
Iteration 66/1000 | Loss: 0.00002740
Iteration 67/1000 | Loss: 0.00002740
Iteration 68/1000 | Loss: 0.00002740
Iteration 69/1000 | Loss: 0.00002740
Iteration 70/1000 | Loss: 0.00002740
Iteration 71/1000 | Loss: 0.00002739
Iteration 72/1000 | Loss: 0.00002739
Iteration 73/1000 | Loss: 0.00002739
Iteration 74/1000 | Loss: 0.00002739
Iteration 75/1000 | Loss: 0.00002738
Iteration 76/1000 | Loss: 0.00002738
Iteration 77/1000 | Loss: 0.00002738
Iteration 78/1000 | Loss: 0.00002738
Iteration 79/1000 | Loss: 0.00002736
Iteration 80/1000 | Loss: 0.00002736
Iteration 81/1000 | Loss: 0.00002736
Iteration 82/1000 | Loss: 0.00002736
Iteration 83/1000 | Loss: 0.00002736
Iteration 84/1000 | Loss: 0.00002735
Iteration 85/1000 | Loss: 0.00002735
Iteration 86/1000 | Loss: 0.00002735
Iteration 87/1000 | Loss: 0.00002735
Iteration 88/1000 | Loss: 0.00002734
Iteration 89/1000 | Loss: 0.00002734
Iteration 90/1000 | Loss: 0.00002734
Iteration 91/1000 | Loss: 0.00002734
Iteration 92/1000 | Loss: 0.00002734
Iteration 93/1000 | Loss: 0.00002733
Iteration 94/1000 | Loss: 0.00002733
Iteration 95/1000 | Loss: 0.00002733
Iteration 96/1000 | Loss: 0.00002733
Iteration 97/1000 | Loss: 0.00002732
Iteration 98/1000 | Loss: 0.00002732
Iteration 99/1000 | Loss: 0.00002732
Iteration 100/1000 | Loss: 0.00002732
Iteration 101/1000 | Loss: 0.00002732
Iteration 102/1000 | Loss: 0.00002732
Iteration 103/1000 | Loss: 0.00002732
Iteration 104/1000 | Loss: 0.00002731
Iteration 105/1000 | Loss: 0.00002731
Iteration 106/1000 | Loss: 0.00002731
Iteration 107/1000 | Loss: 0.00002731
Iteration 108/1000 | Loss: 0.00002731
Iteration 109/1000 | Loss: 0.00002731
Iteration 110/1000 | Loss: 0.00002731
Iteration 111/1000 | Loss: 0.00002731
Iteration 112/1000 | Loss: 0.00002731
Iteration 113/1000 | Loss: 0.00002731
Iteration 114/1000 | Loss: 0.00002731
Iteration 115/1000 | Loss: 0.00002731
Iteration 116/1000 | Loss: 0.00002731
Iteration 117/1000 | Loss: 0.00002731
Iteration 118/1000 | Loss: 0.00002731
Iteration 119/1000 | Loss: 0.00002731
Iteration 120/1000 | Loss: 0.00002731
Iteration 121/1000 | Loss: 0.00002731
Iteration 122/1000 | Loss: 0.00002731
Iteration 123/1000 | Loss: 0.00002731
Iteration 124/1000 | Loss: 0.00002731
Iteration 125/1000 | Loss: 0.00002731
Iteration 126/1000 | Loss: 0.00002731
Iteration 127/1000 | Loss: 0.00002731
Iteration 128/1000 | Loss: 0.00002731
Iteration 129/1000 | Loss: 0.00002731
Iteration 130/1000 | Loss: 0.00002731
Iteration 131/1000 | Loss: 0.00002731
Iteration 132/1000 | Loss: 0.00002731
Iteration 133/1000 | Loss: 0.00002731
Iteration 134/1000 | Loss: 0.00002731
Iteration 135/1000 | Loss: 0.00002731
Iteration 136/1000 | Loss: 0.00002731
Iteration 137/1000 | Loss: 0.00002731
Iteration 138/1000 | Loss: 0.00002731
Iteration 139/1000 | Loss: 0.00002731
Iteration 140/1000 | Loss: 0.00002731
Iteration 141/1000 | Loss: 0.00002731
Iteration 142/1000 | Loss: 0.00002731
Iteration 143/1000 | Loss: 0.00002731
Iteration 144/1000 | Loss: 0.00002731
Iteration 145/1000 | Loss: 0.00002731
Iteration 146/1000 | Loss: 0.00002731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.731494532781653e-05, 2.731494532781653e-05, 2.731494532781653e-05, 2.731494532781653e-05, 2.731494532781653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.731494532781653e-05

Optimization complete. Final v2v error: 4.221852779388428 mm

Highest mean error: 6.1976728439331055 mm for frame 158

Lowest mean error: 3.157202959060669 mm for frame 6

Saving results

Total time: 87.4285089969635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383340
Iteration 2/25 | Loss: 0.00101809
Iteration 3/25 | Loss: 0.00093520
Iteration 4/25 | Loss: 0.00092945
Iteration 5/25 | Loss: 0.00092792
Iteration 6/25 | Loss: 0.00092792
Iteration 7/25 | Loss: 0.00092792
Iteration 8/25 | Loss: 0.00092792
Iteration 9/25 | Loss: 0.00092792
Iteration 10/25 | Loss: 0.00092792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.000927919871173799, 0.000927919871173799, 0.000927919871173799, 0.000927919871173799, 0.000927919871173799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000927919871173799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38787639
Iteration 2/25 | Loss: 0.00061153
Iteration 3/25 | Loss: 0.00061153
Iteration 4/25 | Loss: 0.00061153
Iteration 5/25 | Loss: 0.00061153
Iteration 6/25 | Loss: 0.00061153
Iteration 7/25 | Loss: 0.00061153
Iteration 8/25 | Loss: 0.00061153
Iteration 9/25 | Loss: 0.00061153
Iteration 10/25 | Loss: 0.00061153
Iteration 11/25 | Loss: 0.00061153
Iteration 12/25 | Loss: 0.00061153
Iteration 13/25 | Loss: 0.00061153
Iteration 14/25 | Loss: 0.00061153
Iteration 15/25 | Loss: 0.00061153
Iteration 16/25 | Loss: 0.00061153
Iteration 17/25 | Loss: 0.00061153
Iteration 18/25 | Loss: 0.00061153
Iteration 19/25 | Loss: 0.00061153
Iteration 20/25 | Loss: 0.00061153
Iteration 21/25 | Loss: 0.00061153
Iteration 22/25 | Loss: 0.00061153
Iteration 23/25 | Loss: 0.00061153
Iteration 24/25 | Loss: 0.00061153
Iteration 25/25 | Loss: 0.00061153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061153
Iteration 2/1000 | Loss: 0.00001683
Iteration 3/1000 | Loss: 0.00001304
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001146
Iteration 6/1000 | Loss: 0.00001098
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001040
Iteration 9/1000 | Loss: 0.00001034
Iteration 10/1000 | Loss: 0.00001030
Iteration 11/1000 | Loss: 0.00001029
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001025
Iteration 15/1000 | Loss: 0.00001025
Iteration 16/1000 | Loss: 0.00001024
Iteration 17/1000 | Loss: 0.00001024
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001023
Iteration 20/1000 | Loss: 0.00001022
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001021
Iteration 23/1000 | Loss: 0.00001021
Iteration 24/1000 | Loss: 0.00001021
Iteration 25/1000 | Loss: 0.00001020
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001017
Iteration 28/1000 | Loss: 0.00001017
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001017
Iteration 31/1000 | Loss: 0.00001017
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001017
Iteration 35/1000 | Loss: 0.00001017
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001013
Iteration 41/1000 | Loss: 0.00001012
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001011
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001008
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001007
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001006
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001004
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001004
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001003
Iteration 76/1000 | Loss: 0.00001003
Iteration 77/1000 | Loss: 0.00001003
Iteration 78/1000 | Loss: 0.00001003
Iteration 79/1000 | Loss: 0.00001002
Iteration 80/1000 | Loss: 0.00001002
Iteration 81/1000 | Loss: 0.00001002
Iteration 82/1000 | Loss: 0.00001002
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00001002
Iteration 85/1000 | Loss: 0.00001002
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001001
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001001
Iteration 91/1000 | Loss: 0.00001001
Iteration 92/1000 | Loss: 0.00001001
Iteration 93/1000 | Loss: 0.00001001
Iteration 94/1000 | Loss: 0.00001001
Iteration 95/1000 | Loss: 0.00001001
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001001
Iteration 104/1000 | Loss: 0.00001000
Iteration 105/1000 | Loss: 0.00001000
Iteration 106/1000 | Loss: 0.00001000
Iteration 107/1000 | Loss: 0.00001000
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00000999
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000999
Iteration 119/1000 | Loss: 0.00000999
Iteration 120/1000 | Loss: 0.00000999
Iteration 121/1000 | Loss: 0.00000999
Iteration 122/1000 | Loss: 0.00000999
Iteration 123/1000 | Loss: 0.00000999
Iteration 124/1000 | Loss: 0.00000999
Iteration 125/1000 | Loss: 0.00000999
Iteration 126/1000 | Loss: 0.00000998
Iteration 127/1000 | Loss: 0.00000998
Iteration 128/1000 | Loss: 0.00000998
Iteration 129/1000 | Loss: 0.00000998
Iteration 130/1000 | Loss: 0.00000998
Iteration 131/1000 | Loss: 0.00000998
Iteration 132/1000 | Loss: 0.00000998
Iteration 133/1000 | Loss: 0.00000998
Iteration 134/1000 | Loss: 0.00000997
Iteration 135/1000 | Loss: 0.00000997
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000997
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000996
Iteration 144/1000 | Loss: 0.00000996
Iteration 145/1000 | Loss: 0.00000996
Iteration 146/1000 | Loss: 0.00000996
Iteration 147/1000 | Loss: 0.00000995
Iteration 148/1000 | Loss: 0.00000995
Iteration 149/1000 | Loss: 0.00000995
Iteration 150/1000 | Loss: 0.00000995
Iteration 151/1000 | Loss: 0.00000995
Iteration 152/1000 | Loss: 0.00000995
Iteration 153/1000 | Loss: 0.00000995
Iteration 154/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [9.953419976227451e-06, 9.953419976227451e-06, 9.953419976227451e-06, 9.953419976227451e-06, 9.953419976227451e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.953419976227451e-06

Optimization complete. Final v2v error: 2.6774420738220215 mm

Highest mean error: 3.122891426086426 mm for frame 77

Lowest mean error: 2.4694230556488037 mm for frame 1

Saving results

Total time: 35.425567388534546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984095
Iteration 2/25 | Loss: 0.00500691
Iteration 3/25 | Loss: 0.00372545
Iteration 4/25 | Loss: 0.00332603
Iteration 5/25 | Loss: 0.00249437
Iteration 6/25 | Loss: 0.00238578
Iteration 7/25 | Loss: 0.00209858
Iteration 8/25 | Loss: 0.00189364
Iteration 9/25 | Loss: 0.00176437
Iteration 10/25 | Loss: 0.00179429
Iteration 11/25 | Loss: 0.00168128
Iteration 12/25 | Loss: 0.00162969
Iteration 13/25 | Loss: 0.00159529
Iteration 14/25 | Loss: 0.00154878
Iteration 15/25 | Loss: 0.00154131
Iteration 16/25 | Loss: 0.00152539
Iteration 17/25 | Loss: 0.00152636
Iteration 18/25 | Loss: 0.00149061
Iteration 19/25 | Loss: 0.00147608
Iteration 20/25 | Loss: 0.00146745
Iteration 21/25 | Loss: 0.00147924
Iteration 22/25 | Loss: 0.00144558
Iteration 23/25 | Loss: 0.00143501
Iteration 24/25 | Loss: 0.00141462
Iteration 25/25 | Loss: 0.00139143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37979555
Iteration 2/25 | Loss: 0.00238826
Iteration 3/25 | Loss: 0.00205470
Iteration 4/25 | Loss: 0.00205470
Iteration 5/25 | Loss: 0.00205470
Iteration 6/25 | Loss: 0.00205470
Iteration 7/25 | Loss: 0.00205470
Iteration 8/25 | Loss: 0.00205470
Iteration 9/25 | Loss: 0.00205470
Iteration 10/25 | Loss: 0.00205470
Iteration 11/25 | Loss: 0.00205470
Iteration 12/25 | Loss: 0.00205470
Iteration 13/25 | Loss: 0.00205470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002054697135463357, 0.002054697135463357, 0.002054697135463357, 0.002054697135463357, 0.002054697135463357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002054697135463357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205470
Iteration 2/1000 | Loss: 0.00135766
Iteration 3/1000 | Loss: 0.00093007
Iteration 4/1000 | Loss: 0.00141994
Iteration 5/1000 | Loss: 0.00179538
Iteration 6/1000 | Loss: 0.00065344
Iteration 7/1000 | Loss: 0.00064379
Iteration 8/1000 | Loss: 0.00175841
Iteration 9/1000 | Loss: 0.00149030
Iteration 10/1000 | Loss: 0.00080370
Iteration 11/1000 | Loss: 0.00062736
Iteration 12/1000 | Loss: 0.00078898
Iteration 13/1000 | Loss: 0.00235353
Iteration 14/1000 | Loss: 0.00181934
Iteration 15/1000 | Loss: 0.00774993
Iteration 16/1000 | Loss: 0.00201552
Iteration 17/1000 | Loss: 0.00069515
Iteration 18/1000 | Loss: 0.00100022
Iteration 19/1000 | Loss: 0.00066061
Iteration 20/1000 | Loss: 0.00214747
Iteration 21/1000 | Loss: 0.00083367
Iteration 22/1000 | Loss: 0.00073096
Iteration 23/1000 | Loss: 0.00125925
Iteration 24/1000 | Loss: 0.00485990
Iteration 25/1000 | Loss: 0.00062275
Iteration 26/1000 | Loss: 0.00114622
Iteration 27/1000 | Loss: 0.00076670
Iteration 28/1000 | Loss: 0.00272695
Iteration 29/1000 | Loss: 0.00451980
Iteration 30/1000 | Loss: 0.00300744
Iteration 31/1000 | Loss: 0.00057379
Iteration 32/1000 | Loss: 0.00301862
Iteration 33/1000 | Loss: 0.00045071
Iteration 34/1000 | Loss: 0.00113798
Iteration 35/1000 | Loss: 0.00089807
Iteration 36/1000 | Loss: 0.00048924
Iteration 37/1000 | Loss: 0.00053468
Iteration 38/1000 | Loss: 0.00073330
Iteration 39/1000 | Loss: 0.00062201
Iteration 40/1000 | Loss: 0.00075968
Iteration 41/1000 | Loss: 0.00075636
Iteration 42/1000 | Loss: 0.00076769
Iteration 43/1000 | Loss: 0.00082703
Iteration 44/1000 | Loss: 0.00107818
Iteration 45/1000 | Loss: 0.00064631
Iteration 46/1000 | Loss: 0.00061381
Iteration 47/1000 | Loss: 0.00095893
Iteration 48/1000 | Loss: 0.00108897
Iteration 49/1000 | Loss: 0.00087320
Iteration 50/1000 | Loss: 0.00038275
Iteration 51/1000 | Loss: 0.00056361
Iteration 52/1000 | Loss: 0.00077517
Iteration 53/1000 | Loss: 0.00039861
Iteration 54/1000 | Loss: 0.00048246
Iteration 55/1000 | Loss: 0.00038115
Iteration 56/1000 | Loss: 0.00036977
Iteration 57/1000 | Loss: 0.00164100
Iteration 58/1000 | Loss: 0.00066107
Iteration 59/1000 | Loss: 0.00066091
Iteration 60/1000 | Loss: 0.00041918
Iteration 61/1000 | Loss: 0.00130525
Iteration 62/1000 | Loss: 0.00036923
Iteration 63/1000 | Loss: 0.00223198
Iteration 64/1000 | Loss: 0.00421158
Iteration 65/1000 | Loss: 0.00187440
Iteration 66/1000 | Loss: 0.00244992
Iteration 67/1000 | Loss: 0.00078475
Iteration 68/1000 | Loss: 0.00066155
Iteration 69/1000 | Loss: 0.00219291
Iteration 70/1000 | Loss: 0.00044994
Iteration 71/1000 | Loss: 0.00153437
Iteration 72/1000 | Loss: 0.00050884
Iteration 73/1000 | Loss: 0.00069488
Iteration 74/1000 | Loss: 0.00061321
Iteration 75/1000 | Loss: 0.00053288
Iteration 76/1000 | Loss: 0.00109032
Iteration 77/1000 | Loss: 0.00091708
Iteration 78/1000 | Loss: 0.00088146
Iteration 79/1000 | Loss: 0.00082976
Iteration 80/1000 | Loss: 0.00065006
Iteration 81/1000 | Loss: 0.00066143
Iteration 82/1000 | Loss: 0.00238870
Iteration 83/1000 | Loss: 0.00318981
Iteration 84/1000 | Loss: 0.00261293
Iteration 85/1000 | Loss: 0.00060944
Iteration 86/1000 | Loss: 0.00069382
Iteration 87/1000 | Loss: 0.00042281
Iteration 88/1000 | Loss: 0.00059852
Iteration 89/1000 | Loss: 0.00042656
Iteration 90/1000 | Loss: 0.00043582
Iteration 91/1000 | Loss: 0.00092227
Iteration 92/1000 | Loss: 0.00309999
Iteration 93/1000 | Loss: 0.00140018
Iteration 94/1000 | Loss: 0.00092103
Iteration 95/1000 | Loss: 0.00056783
Iteration 96/1000 | Loss: 0.00125956
Iteration 97/1000 | Loss: 0.00266053
Iteration 98/1000 | Loss: 0.00084882
Iteration 99/1000 | Loss: 0.00092943
Iteration 100/1000 | Loss: 0.00139258
Iteration 101/1000 | Loss: 0.00036683
Iteration 102/1000 | Loss: 0.00032710
Iteration 103/1000 | Loss: 0.00035246
Iteration 104/1000 | Loss: 0.00036554
Iteration 105/1000 | Loss: 0.00036228
Iteration 106/1000 | Loss: 0.00137958
Iteration 107/1000 | Loss: 0.00060766
Iteration 108/1000 | Loss: 0.00059805
Iteration 109/1000 | Loss: 0.00072362
Iteration 110/1000 | Loss: 0.00058695
Iteration 111/1000 | Loss: 0.00053689
Iteration 112/1000 | Loss: 0.00102656
Iteration 113/1000 | Loss: 0.00046631
Iteration 114/1000 | Loss: 0.00121068
Iteration 115/1000 | Loss: 0.00047966
Iteration 116/1000 | Loss: 0.00114236
Iteration 117/1000 | Loss: 0.00062058
Iteration 118/1000 | Loss: 0.00057264
Iteration 119/1000 | Loss: 0.00033770
Iteration 120/1000 | Loss: 0.00052923
Iteration 121/1000 | Loss: 0.00039167
Iteration 122/1000 | Loss: 0.00032957
Iteration 123/1000 | Loss: 0.00034568
Iteration 124/1000 | Loss: 0.00119586
Iteration 125/1000 | Loss: 0.00113464
Iteration 126/1000 | Loss: 0.00187020
Iteration 127/1000 | Loss: 0.00343319
Iteration 128/1000 | Loss: 0.00313833
Iteration 129/1000 | Loss: 0.00213082
Iteration 130/1000 | Loss: 0.00177461
Iteration 131/1000 | Loss: 0.00124951
Iteration 132/1000 | Loss: 0.00179332
Iteration 133/1000 | Loss: 0.00312432
Iteration 134/1000 | Loss: 0.00157422
Iteration 135/1000 | Loss: 0.00092157
Iteration 136/1000 | Loss: 0.00090425
Iteration 137/1000 | Loss: 0.00034260
Iteration 138/1000 | Loss: 0.00034038
Iteration 139/1000 | Loss: 0.00067935
Iteration 140/1000 | Loss: 0.00044696
Iteration 141/1000 | Loss: 0.00060094
Iteration 142/1000 | Loss: 0.00035729
Iteration 143/1000 | Loss: 0.00033411
Iteration 144/1000 | Loss: 0.00083853
Iteration 145/1000 | Loss: 0.00041835
Iteration 146/1000 | Loss: 0.00187363
Iteration 147/1000 | Loss: 0.00128149
Iteration 148/1000 | Loss: 0.00031458
Iteration 149/1000 | Loss: 0.00037298
Iteration 150/1000 | Loss: 0.00072127
Iteration 151/1000 | Loss: 0.00034464
Iteration 152/1000 | Loss: 0.00021831
Iteration 153/1000 | Loss: 0.00037637
Iteration 154/1000 | Loss: 0.00028420
Iteration 155/1000 | Loss: 0.00041739
Iteration 156/1000 | Loss: 0.00028187
Iteration 157/1000 | Loss: 0.00091694
Iteration 158/1000 | Loss: 0.00031055
Iteration 159/1000 | Loss: 0.00075398
Iteration 160/1000 | Loss: 0.00034233
Iteration 161/1000 | Loss: 0.00049650
Iteration 162/1000 | Loss: 0.00029678
Iteration 163/1000 | Loss: 0.00201626
Iteration 164/1000 | Loss: 0.00033383
Iteration 165/1000 | Loss: 0.00063661
Iteration 166/1000 | Loss: 0.00052664
Iteration 167/1000 | Loss: 0.00126640
Iteration 168/1000 | Loss: 0.00063404
Iteration 169/1000 | Loss: 0.00344183
Iteration 170/1000 | Loss: 0.00265010
Iteration 171/1000 | Loss: 0.00325549
Iteration 172/1000 | Loss: 0.00308320
Iteration 173/1000 | Loss: 0.00093814
Iteration 174/1000 | Loss: 0.00027391
Iteration 175/1000 | Loss: 0.00092025
Iteration 176/1000 | Loss: 0.00027273
Iteration 177/1000 | Loss: 0.00026693
Iteration 178/1000 | Loss: 0.00028737
Iteration 179/1000 | Loss: 0.00043200
Iteration 180/1000 | Loss: 0.00274011
Iteration 181/1000 | Loss: 0.00279585
Iteration 182/1000 | Loss: 0.00258220
Iteration 183/1000 | Loss: 0.00205989
Iteration 184/1000 | Loss: 0.00245417
Iteration 185/1000 | Loss: 0.00209261
Iteration 186/1000 | Loss: 0.00257851
Iteration 187/1000 | Loss: 0.00329230
Iteration 188/1000 | Loss: 0.00192614
Iteration 189/1000 | Loss: 0.00310360
Iteration 190/1000 | Loss: 0.00160752
Iteration 191/1000 | Loss: 0.00284622
Iteration 192/1000 | Loss: 0.00275450
Iteration 193/1000 | Loss: 0.00224284
Iteration 194/1000 | Loss: 0.00145253
Iteration 195/1000 | Loss: 0.00263514
Iteration 196/1000 | Loss: 0.00138477
Iteration 197/1000 | Loss: 0.00341210
Iteration 198/1000 | Loss: 0.00080005
Iteration 199/1000 | Loss: 0.00079420
Iteration 200/1000 | Loss: 0.00038460
Iteration 201/1000 | Loss: 0.00026205
Iteration 202/1000 | Loss: 0.00074740
Iteration 203/1000 | Loss: 0.00023100
Iteration 204/1000 | Loss: 0.00020462
Iteration 205/1000 | Loss: 0.00047807
Iteration 206/1000 | Loss: 0.00027118
Iteration 207/1000 | Loss: 0.00050003
Iteration 208/1000 | Loss: 0.00023230
Iteration 209/1000 | Loss: 0.00060683
Iteration 210/1000 | Loss: 0.00020473
Iteration 211/1000 | Loss: 0.00044940
Iteration 212/1000 | Loss: 0.00014549
Iteration 213/1000 | Loss: 0.00037993
Iteration 214/1000 | Loss: 0.00014378
Iteration 215/1000 | Loss: 0.00020442
Iteration 216/1000 | Loss: 0.00031142
Iteration 217/1000 | Loss: 0.00034667
Iteration 218/1000 | Loss: 0.00033642
Iteration 219/1000 | Loss: 0.00006977
Iteration 220/1000 | Loss: 0.00050451
Iteration 221/1000 | Loss: 0.00007930
Iteration 222/1000 | Loss: 0.00006094
Iteration 223/1000 | Loss: 0.00006769
Iteration 224/1000 | Loss: 0.00004135
Iteration 225/1000 | Loss: 0.00026309
Iteration 226/1000 | Loss: 0.00075288
Iteration 227/1000 | Loss: 0.00004776
Iteration 228/1000 | Loss: 0.00006490
Iteration 229/1000 | Loss: 0.00005745
Iteration 230/1000 | Loss: 0.00024758
Iteration 231/1000 | Loss: 0.00008263
Iteration 232/1000 | Loss: 0.00006151
Iteration 233/1000 | Loss: 0.00012736
Iteration 234/1000 | Loss: 0.00020690
Iteration 235/1000 | Loss: 0.00008135
Iteration 236/1000 | Loss: 0.00005716
Iteration 237/1000 | Loss: 0.00041122
Iteration 238/1000 | Loss: 0.00062105
Iteration 239/1000 | Loss: 0.00011902
Iteration 240/1000 | Loss: 0.00019305
Iteration 241/1000 | Loss: 0.00006058
Iteration 242/1000 | Loss: 0.00013985
Iteration 243/1000 | Loss: 0.00006016
Iteration 244/1000 | Loss: 0.00015794
Iteration 245/1000 | Loss: 0.00007259
Iteration 246/1000 | Loss: 0.00006311
Iteration 247/1000 | Loss: 0.00004854
Iteration 248/1000 | Loss: 0.00004181
Iteration 249/1000 | Loss: 0.00006493
Iteration 250/1000 | Loss: 0.00008793
Iteration 251/1000 | Loss: 0.00004713
Iteration 252/1000 | Loss: 0.00008790
Iteration 253/1000 | Loss: 0.00005939
Iteration 254/1000 | Loss: 0.00008563
Iteration 255/1000 | Loss: 0.00004868
Iteration 256/1000 | Loss: 0.00021855
Iteration 257/1000 | Loss: 0.00008242
Iteration 258/1000 | Loss: 0.00006967
Iteration 259/1000 | Loss: 0.00005741
Iteration 260/1000 | Loss: 0.00006734
Iteration 261/1000 | Loss: 0.00015693
Iteration 262/1000 | Loss: 0.00006392
Iteration 263/1000 | Loss: 0.00003335
Iteration 264/1000 | Loss: 0.00002051
Iteration 265/1000 | Loss: 0.00002895
Iteration 266/1000 | Loss: 0.00002636
Iteration 267/1000 | Loss: 0.00004174
Iteration 268/1000 | Loss: 0.00003353
Iteration 269/1000 | Loss: 0.00003412
Iteration 270/1000 | Loss: 0.00002484
Iteration 271/1000 | Loss: 0.00003480
Iteration 272/1000 | Loss: 0.00002114
Iteration 273/1000 | Loss: 0.00003052
Iteration 274/1000 | Loss: 0.00002906
Iteration 275/1000 | Loss: 0.00002385
Iteration 276/1000 | Loss: 0.00003741
Iteration 277/1000 | Loss: 0.00002461
Iteration 278/1000 | Loss: 0.00002786
Iteration 279/1000 | Loss: 0.00002417
Iteration 280/1000 | Loss: 0.00002190
Iteration 281/1000 | Loss: 0.00002018
Iteration 282/1000 | Loss: 0.00002906
Iteration 283/1000 | Loss: 0.00003081
Iteration 284/1000 | Loss: 0.00002943
Iteration 285/1000 | Loss: 0.00004554
Iteration 286/1000 | Loss: 0.00003495
Iteration 287/1000 | Loss: 0.00002896
Iteration 288/1000 | Loss: 0.00004418
Iteration 289/1000 | Loss: 0.00002894
Iteration 290/1000 | Loss: 0.00003844
Iteration 291/1000 | Loss: 0.00002997
Iteration 292/1000 | Loss: 0.00003694
Iteration 293/1000 | Loss: 0.00003046
Iteration 294/1000 | Loss: 0.00003091
Iteration 295/1000 | Loss: 0.00002870
Iteration 296/1000 | Loss: 0.00002056
Iteration 297/1000 | Loss: 0.00002746
Iteration 298/1000 | Loss: 0.00002785
Iteration 299/1000 | Loss: 0.00003046
Iteration 300/1000 | Loss: 0.00003400
Iteration 301/1000 | Loss: 0.00002456
Iteration 302/1000 | Loss: 0.00002695
Iteration 303/1000 | Loss: 0.00002549
Iteration 304/1000 | Loss: 0.00003260
Iteration 305/1000 | Loss: 0.00002635
Iteration 306/1000 | Loss: 0.00002889
Iteration 307/1000 | Loss: 0.00002801
Iteration 308/1000 | Loss: 0.00003382
Iteration 309/1000 | Loss: 0.00003001
Iteration 310/1000 | Loss: 0.00003973
Iteration 311/1000 | Loss: 0.00004413
Iteration 312/1000 | Loss: 0.00004124
Iteration 313/1000 | Loss: 0.00003333
Iteration 314/1000 | Loss: 0.00002724
Iteration 315/1000 | Loss: 0.00002855
Iteration 316/1000 | Loss: 0.00004277
Iteration 317/1000 | Loss: 0.00002914
Iteration 318/1000 | Loss: 0.00003390
Iteration 319/1000 | Loss: 0.00002886
Iteration 320/1000 | Loss: 0.00003358
Iteration 321/1000 | Loss: 0.00002240
Iteration 322/1000 | Loss: 0.00003379
Iteration 323/1000 | Loss: 0.00003293
Iteration 324/1000 | Loss: 0.00003038
Iteration 325/1000 | Loss: 0.00004250
Iteration 326/1000 | Loss: 0.00002989
Iteration 327/1000 | Loss: 0.00003162
Iteration 328/1000 | Loss: 0.00002154
Iteration 329/1000 | Loss: 0.00003399
Iteration 330/1000 | Loss: 0.00003154
Iteration 331/1000 | Loss: 0.00003061
Iteration 332/1000 | Loss: 0.00003835
Iteration 333/1000 | Loss: 0.00003063
Iteration 334/1000 | Loss: 0.00003473
Iteration 335/1000 | Loss: 0.00003043
Iteration 336/1000 | Loss: 0.00003031
Iteration 337/1000 | Loss: 0.00002766
Iteration 338/1000 | Loss: 0.00002604
Iteration 339/1000 | Loss: 0.00003660
Iteration 340/1000 | Loss: 0.00002627
Iteration 341/1000 | Loss: 0.00001759
Iteration 342/1000 | Loss: 0.00003350
Iteration 343/1000 | Loss: 0.00003701
Iteration 344/1000 | Loss: 0.00003233
Iteration 345/1000 | Loss: 0.00004299
Iteration 346/1000 | Loss: 0.00001800
Iteration 347/1000 | Loss: 0.00003001
Iteration 348/1000 | Loss: 0.00003189
Iteration 349/1000 | Loss: 0.00001548
Iteration 350/1000 | Loss: 0.00002192
Iteration 351/1000 | Loss: 0.00002220
Iteration 352/1000 | Loss: 0.00002089
Iteration 353/1000 | Loss: 0.00002326
Iteration 354/1000 | Loss: 0.00002085
Iteration 355/1000 | Loss: 0.00002392
Iteration 356/1000 | Loss: 0.00001907
Iteration 357/1000 | Loss: 0.00001624
Iteration 358/1000 | Loss: 0.00002146
Iteration 359/1000 | Loss: 0.00002416
Iteration 360/1000 | Loss: 0.00001727
Iteration 361/1000 | Loss: 0.00001680
Iteration 362/1000 | Loss: 0.00002029
Iteration 363/1000 | Loss: 0.00001946
Iteration 364/1000 | Loss: 0.00002364
Iteration 365/1000 | Loss: 0.00001512
Iteration 366/1000 | Loss: 0.00001504
Iteration 367/1000 | Loss: 0.00001503
Iteration 368/1000 | Loss: 0.00001490
Iteration 369/1000 | Loss: 0.00001487
Iteration 370/1000 | Loss: 0.00001483
Iteration 371/1000 | Loss: 0.00001481
Iteration 372/1000 | Loss: 0.00001480
Iteration 373/1000 | Loss: 0.00001476
Iteration 374/1000 | Loss: 0.00001458
Iteration 375/1000 | Loss: 0.00001445
Iteration 376/1000 | Loss: 0.00001434
Iteration 377/1000 | Loss: 0.00001434
Iteration 378/1000 | Loss: 0.00001432
Iteration 379/1000 | Loss: 0.00001432
Iteration 380/1000 | Loss: 0.00001432
Iteration 381/1000 | Loss: 0.00001431
Iteration 382/1000 | Loss: 0.00001431
Iteration 383/1000 | Loss: 0.00001430
Iteration 384/1000 | Loss: 0.00001430
Iteration 385/1000 | Loss: 0.00001430
Iteration 386/1000 | Loss: 0.00001430
Iteration 387/1000 | Loss: 0.00001430
Iteration 388/1000 | Loss: 0.00001430
Iteration 389/1000 | Loss: 0.00001429
Iteration 390/1000 | Loss: 0.00001429
Iteration 391/1000 | Loss: 0.00001429
Iteration 392/1000 | Loss: 0.00001429
Iteration 393/1000 | Loss: 0.00001428
Iteration 394/1000 | Loss: 0.00001428
Iteration 395/1000 | Loss: 0.00001428
Iteration 396/1000 | Loss: 0.00001428
Iteration 397/1000 | Loss: 0.00001427
Iteration 398/1000 | Loss: 0.00001427
Iteration 399/1000 | Loss: 0.00001427
Iteration 400/1000 | Loss: 0.00001427
Iteration 401/1000 | Loss: 0.00001426
Iteration 402/1000 | Loss: 0.00001426
Iteration 403/1000 | Loss: 0.00001426
Iteration 404/1000 | Loss: 0.00001426
Iteration 405/1000 | Loss: 0.00001426
Iteration 406/1000 | Loss: 0.00001426
Iteration 407/1000 | Loss: 0.00001426
Iteration 408/1000 | Loss: 0.00001426
Iteration 409/1000 | Loss: 0.00001426
Iteration 410/1000 | Loss: 0.00001426
Iteration 411/1000 | Loss: 0.00001426
Iteration 412/1000 | Loss: 0.00001426
Iteration 413/1000 | Loss: 0.00001426
Iteration 414/1000 | Loss: 0.00001425
Iteration 415/1000 | Loss: 0.00001425
Iteration 416/1000 | Loss: 0.00001425
Iteration 417/1000 | Loss: 0.00001425
Iteration 418/1000 | Loss: 0.00001425
Iteration 419/1000 | Loss: 0.00001425
Iteration 420/1000 | Loss: 0.00001425
Iteration 421/1000 | Loss: 0.00001425
Iteration 422/1000 | Loss: 0.00001424
Iteration 423/1000 | Loss: 0.00001424
Iteration 424/1000 | Loss: 0.00001423
Iteration 425/1000 | Loss: 0.00001423
Iteration 426/1000 | Loss: 0.00001423
Iteration 427/1000 | Loss: 0.00001423
Iteration 428/1000 | Loss: 0.00001423
Iteration 429/1000 | Loss: 0.00001423
Iteration 430/1000 | Loss: 0.00001423
Iteration 431/1000 | Loss: 0.00001422
Iteration 432/1000 | Loss: 0.00001422
Iteration 433/1000 | Loss: 0.00001422
Iteration 434/1000 | Loss: 0.00001422
Iteration 435/1000 | Loss: 0.00001422
Iteration 436/1000 | Loss: 0.00001422
Iteration 437/1000 | Loss: 0.00001422
Iteration 438/1000 | Loss: 0.00001421
Iteration 439/1000 | Loss: 0.00001421
Iteration 440/1000 | Loss: 0.00001421
Iteration 441/1000 | Loss: 0.00001421
Iteration 442/1000 | Loss: 0.00001421
Iteration 443/1000 | Loss: 0.00001421
Iteration 444/1000 | Loss: 0.00001421
Iteration 445/1000 | Loss: 0.00001421
Iteration 446/1000 | Loss: 0.00001421
Iteration 447/1000 | Loss: 0.00001420
Iteration 448/1000 | Loss: 0.00001420
Iteration 449/1000 | Loss: 0.00001420
Iteration 450/1000 | Loss: 0.00001420
Iteration 451/1000 | Loss: 0.00001420
Iteration 452/1000 | Loss: 0.00001420
Iteration 453/1000 | Loss: 0.00001420
Iteration 454/1000 | Loss: 0.00001420
Iteration 455/1000 | Loss: 0.00001420
Iteration 456/1000 | Loss: 0.00001420
Iteration 457/1000 | Loss: 0.00001420
Iteration 458/1000 | Loss: 0.00001420
Iteration 459/1000 | Loss: 0.00001420
Iteration 460/1000 | Loss: 0.00001420
Iteration 461/1000 | Loss: 0.00001420
Iteration 462/1000 | Loss: 0.00001420
Iteration 463/1000 | Loss: 0.00001419
Iteration 464/1000 | Loss: 0.00001419
Iteration 465/1000 | Loss: 0.00001419
Iteration 466/1000 | Loss: 0.00001419
Iteration 467/1000 | Loss: 0.00001419
Iteration 468/1000 | Loss: 0.00001419
Iteration 469/1000 | Loss: 0.00001419
Iteration 470/1000 | Loss: 0.00001419
Iteration 471/1000 | Loss: 0.00001419
Iteration 472/1000 | Loss: 0.00001419
Iteration 473/1000 | Loss: 0.00001419
Iteration 474/1000 | Loss: 0.00001419
Iteration 475/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 475. Stopping optimization.
Last 5 losses: [1.4194200048223138e-05, 1.4194200048223138e-05, 1.4194200048223138e-05, 1.4194200048223138e-05, 1.4194200048223138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4194200048223138e-05

Optimization complete. Final v2v error: 3.2124578952789307 mm

Highest mean error: 4.459061145782471 mm for frame 14

Lowest mean error: 2.848341464996338 mm for frame 7

Saving results

Total time: 605.3266706466675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831895
Iteration 2/25 | Loss: 0.00113653
Iteration 3/25 | Loss: 0.00094948
Iteration 4/25 | Loss: 0.00092574
Iteration 5/25 | Loss: 0.00092293
Iteration 6/25 | Loss: 0.00092284
Iteration 7/25 | Loss: 0.00092284
Iteration 8/25 | Loss: 0.00092284
Iteration 9/25 | Loss: 0.00092284
Iteration 10/25 | Loss: 0.00092284
Iteration 11/25 | Loss: 0.00092284
Iteration 12/25 | Loss: 0.00092284
Iteration 13/25 | Loss: 0.00092284
Iteration 14/25 | Loss: 0.00092284
Iteration 15/25 | Loss: 0.00092284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009228434064425528, 0.0009228434064425528, 0.0009228434064425528, 0.0009228434064425528, 0.0009228434064425528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009228434064425528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39245474
Iteration 2/25 | Loss: 0.00072949
Iteration 3/25 | Loss: 0.00072949
Iteration 4/25 | Loss: 0.00072949
Iteration 5/25 | Loss: 0.00072949
Iteration 6/25 | Loss: 0.00072949
Iteration 7/25 | Loss: 0.00072949
Iteration 8/25 | Loss: 0.00072949
Iteration 9/25 | Loss: 0.00072948
Iteration 10/25 | Loss: 0.00072948
Iteration 11/25 | Loss: 0.00072948
Iteration 12/25 | Loss: 0.00072948
Iteration 13/25 | Loss: 0.00072948
Iteration 14/25 | Loss: 0.00072948
Iteration 15/25 | Loss: 0.00072948
Iteration 16/25 | Loss: 0.00072948
Iteration 17/25 | Loss: 0.00072948
Iteration 18/25 | Loss: 0.00072948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007294844253920019, 0.0007294844253920019, 0.0007294844253920019, 0.0007294844253920019, 0.0007294844253920019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007294844253920019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072948
Iteration 2/1000 | Loss: 0.00002114
Iteration 3/1000 | Loss: 0.00001340
Iteration 4/1000 | Loss: 0.00001184
Iteration 5/1000 | Loss: 0.00001085
Iteration 6/1000 | Loss: 0.00001028
Iteration 7/1000 | Loss: 0.00000992
Iteration 8/1000 | Loss: 0.00000954
Iteration 9/1000 | Loss: 0.00000933
Iteration 10/1000 | Loss: 0.00000917
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000911
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000904
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000899
Iteration 17/1000 | Loss: 0.00000898
Iteration 18/1000 | Loss: 0.00000898
Iteration 19/1000 | Loss: 0.00000897
Iteration 20/1000 | Loss: 0.00000897
Iteration 21/1000 | Loss: 0.00000897
Iteration 22/1000 | Loss: 0.00000897
Iteration 23/1000 | Loss: 0.00000897
Iteration 24/1000 | Loss: 0.00000897
Iteration 25/1000 | Loss: 0.00000897
Iteration 26/1000 | Loss: 0.00000896
Iteration 27/1000 | Loss: 0.00000896
Iteration 28/1000 | Loss: 0.00000896
Iteration 29/1000 | Loss: 0.00000896
Iteration 30/1000 | Loss: 0.00000896
Iteration 31/1000 | Loss: 0.00000895
Iteration 32/1000 | Loss: 0.00000894
Iteration 33/1000 | Loss: 0.00000894
Iteration 34/1000 | Loss: 0.00000894
Iteration 35/1000 | Loss: 0.00000893
Iteration 36/1000 | Loss: 0.00000893
Iteration 37/1000 | Loss: 0.00000892
Iteration 38/1000 | Loss: 0.00000892
Iteration 39/1000 | Loss: 0.00000892
Iteration 40/1000 | Loss: 0.00000892
Iteration 41/1000 | Loss: 0.00000892
Iteration 42/1000 | Loss: 0.00000891
Iteration 43/1000 | Loss: 0.00000891
Iteration 44/1000 | Loss: 0.00000891
Iteration 45/1000 | Loss: 0.00000891
Iteration 46/1000 | Loss: 0.00000891
Iteration 47/1000 | Loss: 0.00000890
Iteration 48/1000 | Loss: 0.00000890
Iteration 49/1000 | Loss: 0.00000890
Iteration 50/1000 | Loss: 0.00000890
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000889
Iteration 53/1000 | Loss: 0.00000889
Iteration 54/1000 | Loss: 0.00000889
Iteration 55/1000 | Loss: 0.00000889
Iteration 56/1000 | Loss: 0.00000888
Iteration 57/1000 | Loss: 0.00000888
Iteration 58/1000 | Loss: 0.00000888
Iteration 59/1000 | Loss: 0.00000888
Iteration 60/1000 | Loss: 0.00000888
Iteration 61/1000 | Loss: 0.00000887
Iteration 62/1000 | Loss: 0.00000887
Iteration 63/1000 | Loss: 0.00000887
Iteration 64/1000 | Loss: 0.00000887
Iteration 65/1000 | Loss: 0.00000887
Iteration 66/1000 | Loss: 0.00000887
Iteration 67/1000 | Loss: 0.00000887
Iteration 68/1000 | Loss: 0.00000886
Iteration 69/1000 | Loss: 0.00000886
Iteration 70/1000 | Loss: 0.00000886
Iteration 71/1000 | Loss: 0.00000886
Iteration 72/1000 | Loss: 0.00000886
Iteration 73/1000 | Loss: 0.00000886
Iteration 74/1000 | Loss: 0.00000886
Iteration 75/1000 | Loss: 0.00000886
Iteration 76/1000 | Loss: 0.00000886
Iteration 77/1000 | Loss: 0.00000885
Iteration 78/1000 | Loss: 0.00000885
Iteration 79/1000 | Loss: 0.00000885
Iteration 80/1000 | Loss: 0.00000885
Iteration 81/1000 | Loss: 0.00000885
Iteration 82/1000 | Loss: 0.00000885
Iteration 83/1000 | Loss: 0.00000884
Iteration 84/1000 | Loss: 0.00000884
Iteration 85/1000 | Loss: 0.00000884
Iteration 86/1000 | Loss: 0.00000884
Iteration 87/1000 | Loss: 0.00000884
Iteration 88/1000 | Loss: 0.00000884
Iteration 89/1000 | Loss: 0.00000884
Iteration 90/1000 | Loss: 0.00000884
Iteration 91/1000 | Loss: 0.00000884
Iteration 92/1000 | Loss: 0.00000884
Iteration 93/1000 | Loss: 0.00000884
Iteration 94/1000 | Loss: 0.00000884
Iteration 95/1000 | Loss: 0.00000884
Iteration 96/1000 | Loss: 0.00000884
Iteration 97/1000 | Loss: 0.00000883
Iteration 98/1000 | Loss: 0.00000883
Iteration 99/1000 | Loss: 0.00000883
Iteration 100/1000 | Loss: 0.00000883
Iteration 101/1000 | Loss: 0.00000883
Iteration 102/1000 | Loss: 0.00000883
Iteration 103/1000 | Loss: 0.00000883
Iteration 104/1000 | Loss: 0.00000883
Iteration 105/1000 | Loss: 0.00000883
Iteration 106/1000 | Loss: 0.00000883
Iteration 107/1000 | Loss: 0.00000883
Iteration 108/1000 | Loss: 0.00000883
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000883
Iteration 113/1000 | Loss: 0.00000883
Iteration 114/1000 | Loss: 0.00000883
Iteration 115/1000 | Loss: 0.00000883
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000883
Iteration 121/1000 | Loss: 0.00000883
Iteration 122/1000 | Loss: 0.00000883
Iteration 123/1000 | Loss: 0.00000883
Iteration 124/1000 | Loss: 0.00000883
Iteration 125/1000 | Loss: 0.00000883
Iteration 126/1000 | Loss: 0.00000883
Iteration 127/1000 | Loss: 0.00000883
Iteration 128/1000 | Loss: 0.00000883
Iteration 129/1000 | Loss: 0.00000883
Iteration 130/1000 | Loss: 0.00000883
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000883
Iteration 134/1000 | Loss: 0.00000883
Iteration 135/1000 | Loss: 0.00000883
Iteration 136/1000 | Loss: 0.00000883
Iteration 137/1000 | Loss: 0.00000883
Iteration 138/1000 | Loss: 0.00000883
Iteration 139/1000 | Loss: 0.00000883
Iteration 140/1000 | Loss: 0.00000883
Iteration 141/1000 | Loss: 0.00000883
Iteration 142/1000 | Loss: 0.00000883
Iteration 143/1000 | Loss: 0.00000883
Iteration 144/1000 | Loss: 0.00000883
Iteration 145/1000 | Loss: 0.00000883
Iteration 146/1000 | Loss: 0.00000883
Iteration 147/1000 | Loss: 0.00000883
Iteration 148/1000 | Loss: 0.00000883
Iteration 149/1000 | Loss: 0.00000883
Iteration 150/1000 | Loss: 0.00000883
Iteration 151/1000 | Loss: 0.00000883
Iteration 152/1000 | Loss: 0.00000883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [8.832090315991081e-06, 8.832090315991081e-06, 8.832090315991081e-06, 8.832090315991081e-06, 8.832090315991081e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.832090315991081e-06

Optimization complete. Final v2v error: 2.5069515705108643 mm

Highest mean error: 2.7817435264587402 mm for frame 24

Lowest mean error: 2.299443483352661 mm for frame 165

Saving results

Total time: 36.09750294685364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408814
Iteration 2/25 | Loss: 0.00117809
Iteration 3/25 | Loss: 0.00105825
Iteration 4/25 | Loss: 0.00104049
Iteration 5/25 | Loss: 0.00103521
Iteration 6/25 | Loss: 0.00103335
Iteration 7/25 | Loss: 0.00103293
Iteration 8/25 | Loss: 0.00103280
Iteration 9/25 | Loss: 0.00103280
Iteration 10/25 | Loss: 0.00103280
Iteration 11/25 | Loss: 0.00103280
Iteration 12/25 | Loss: 0.00103280
Iteration 13/25 | Loss: 0.00103280
Iteration 14/25 | Loss: 0.00103280
Iteration 15/25 | Loss: 0.00103280
Iteration 16/25 | Loss: 0.00103280
Iteration 17/25 | Loss: 0.00103280
Iteration 18/25 | Loss: 0.00103280
Iteration 19/25 | Loss: 0.00103280
Iteration 20/25 | Loss: 0.00103280
Iteration 21/25 | Loss: 0.00103280
Iteration 22/25 | Loss: 0.00103280
Iteration 23/25 | Loss: 0.00103280
Iteration 24/25 | Loss: 0.00103280
Iteration 25/25 | Loss: 0.00103280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43658710
Iteration 2/25 | Loss: 0.00092235
Iteration 3/25 | Loss: 0.00092235
Iteration 4/25 | Loss: 0.00092235
Iteration 5/25 | Loss: 0.00092235
Iteration 6/25 | Loss: 0.00092235
Iteration 7/25 | Loss: 0.00092235
Iteration 8/25 | Loss: 0.00092235
Iteration 9/25 | Loss: 0.00092235
Iteration 10/25 | Loss: 0.00092235
Iteration 11/25 | Loss: 0.00092235
Iteration 12/25 | Loss: 0.00092235
Iteration 13/25 | Loss: 0.00092235
Iteration 14/25 | Loss: 0.00092235
Iteration 15/25 | Loss: 0.00092235
Iteration 16/25 | Loss: 0.00092235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009223460801877081, 0.0009223460801877081, 0.0009223460801877081, 0.0009223460801877081, 0.0009223460801877081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009223460801877081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092235
Iteration 2/1000 | Loss: 0.00003944
Iteration 3/1000 | Loss: 0.00002532
Iteration 4/1000 | Loss: 0.00002105
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001781
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001646
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001589
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001577
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001569
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001567
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001565
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001564
Iteration 64/1000 | Loss: 0.00001564
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.5640809579053894e-05, 1.5640809579053894e-05, 1.5640809579053894e-05, 1.5640809579053894e-05, 1.5640809579053894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5640809579053894e-05

Optimization complete. Final v2v error: 3.3301842212677 mm

Highest mean error: 4.408786296844482 mm for frame 44

Lowest mean error: 2.65478253364563 mm for frame 0

Saving results

Total time: 34.423542737960815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862726
Iteration 2/25 | Loss: 0.00102702
Iteration 3/25 | Loss: 0.00093851
Iteration 4/25 | Loss: 0.00093157
Iteration 5/25 | Loss: 0.00093023
Iteration 6/25 | Loss: 0.00093023
Iteration 7/25 | Loss: 0.00093023
Iteration 8/25 | Loss: 0.00093023
Iteration 9/25 | Loss: 0.00093023
Iteration 10/25 | Loss: 0.00093023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009302259422838688, 0.0009302259422838688, 0.0009302259422838688, 0.0009302259422838688, 0.0009302259422838688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009302259422838688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54846382
Iteration 2/25 | Loss: 0.00072395
Iteration 3/25 | Loss: 0.00072394
Iteration 4/25 | Loss: 0.00072394
Iteration 5/25 | Loss: 0.00072394
Iteration 6/25 | Loss: 0.00072394
Iteration 7/25 | Loss: 0.00072394
Iteration 8/25 | Loss: 0.00072394
Iteration 9/25 | Loss: 0.00072394
Iteration 10/25 | Loss: 0.00072394
Iteration 11/25 | Loss: 0.00072394
Iteration 12/25 | Loss: 0.00072394
Iteration 13/25 | Loss: 0.00072394
Iteration 14/25 | Loss: 0.00072394
Iteration 15/25 | Loss: 0.00072394
Iteration 16/25 | Loss: 0.00072394
Iteration 17/25 | Loss: 0.00072394
Iteration 18/25 | Loss: 0.00072394
Iteration 19/25 | Loss: 0.00072394
Iteration 20/25 | Loss: 0.00072394
Iteration 21/25 | Loss: 0.00072394
Iteration 22/25 | Loss: 0.00072394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007239393889904022, 0.0007239393889904022, 0.0007239393889904022, 0.0007239393889904022, 0.0007239393889904022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007239393889904022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072394
Iteration 2/1000 | Loss: 0.00001769
Iteration 3/1000 | Loss: 0.00001185
Iteration 4/1000 | Loss: 0.00001033
Iteration 5/1000 | Loss: 0.00000992
Iteration 6/1000 | Loss: 0.00000951
Iteration 7/1000 | Loss: 0.00000926
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000911
Iteration 10/1000 | Loss: 0.00000911
Iteration 11/1000 | Loss: 0.00000909
Iteration 12/1000 | Loss: 0.00000908
Iteration 13/1000 | Loss: 0.00000908
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000908
Iteration 16/1000 | Loss: 0.00000907
Iteration 17/1000 | Loss: 0.00000907
Iteration 18/1000 | Loss: 0.00000907
Iteration 19/1000 | Loss: 0.00000907
Iteration 20/1000 | Loss: 0.00000906
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000905
Iteration 24/1000 | Loss: 0.00000905
Iteration 25/1000 | Loss: 0.00000905
Iteration 26/1000 | Loss: 0.00000904
Iteration 27/1000 | Loss: 0.00000904
Iteration 28/1000 | Loss: 0.00000904
Iteration 29/1000 | Loss: 0.00000903
Iteration 30/1000 | Loss: 0.00000903
Iteration 31/1000 | Loss: 0.00000903
Iteration 32/1000 | Loss: 0.00000903
Iteration 33/1000 | Loss: 0.00000903
Iteration 34/1000 | Loss: 0.00000903
Iteration 35/1000 | Loss: 0.00000903
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000903
Iteration 38/1000 | Loss: 0.00000903
Iteration 39/1000 | Loss: 0.00000903
Iteration 40/1000 | Loss: 0.00000902
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000901
Iteration 46/1000 | Loss: 0.00000901
Iteration 47/1000 | Loss: 0.00000901
Iteration 48/1000 | Loss: 0.00000901
Iteration 49/1000 | Loss: 0.00000901
Iteration 50/1000 | Loss: 0.00000900
Iteration 51/1000 | Loss: 0.00000900
Iteration 52/1000 | Loss: 0.00000900
Iteration 53/1000 | Loss: 0.00000900
Iteration 54/1000 | Loss: 0.00000900
Iteration 55/1000 | Loss: 0.00000900
Iteration 56/1000 | Loss: 0.00000900
Iteration 57/1000 | Loss: 0.00000900
Iteration 58/1000 | Loss: 0.00000900
Iteration 59/1000 | Loss: 0.00000900
Iteration 60/1000 | Loss: 0.00000900
Iteration 61/1000 | Loss: 0.00000900
Iteration 62/1000 | Loss: 0.00000900
Iteration 63/1000 | Loss: 0.00000900
Iteration 64/1000 | Loss: 0.00000900
Iteration 65/1000 | Loss: 0.00000899
Iteration 66/1000 | Loss: 0.00000899
Iteration 67/1000 | Loss: 0.00000899
Iteration 68/1000 | Loss: 0.00000899
Iteration 69/1000 | Loss: 0.00000899
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000899
Iteration 73/1000 | Loss: 0.00000899
Iteration 74/1000 | Loss: 0.00000899
Iteration 75/1000 | Loss: 0.00000899
Iteration 76/1000 | Loss: 0.00000899
Iteration 77/1000 | Loss: 0.00000899
Iteration 78/1000 | Loss: 0.00000899
Iteration 79/1000 | Loss: 0.00000899
Iteration 80/1000 | Loss: 0.00000899
Iteration 81/1000 | Loss: 0.00000899
Iteration 82/1000 | Loss: 0.00000899
Iteration 83/1000 | Loss: 0.00000899
Iteration 84/1000 | Loss: 0.00000899
Iteration 85/1000 | Loss: 0.00000899
Iteration 86/1000 | Loss: 0.00000899
Iteration 87/1000 | Loss: 0.00000899
Iteration 88/1000 | Loss: 0.00000899
Iteration 89/1000 | Loss: 0.00000899
Iteration 90/1000 | Loss: 0.00000899
Iteration 91/1000 | Loss: 0.00000899
Iteration 92/1000 | Loss: 0.00000899
Iteration 93/1000 | Loss: 0.00000899
Iteration 94/1000 | Loss: 0.00000899
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000899
Iteration 101/1000 | Loss: 0.00000899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [8.985070962808095e-06, 8.985070962808095e-06, 8.985070962808095e-06, 8.985070962808095e-06, 8.985070962808095e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.985070962808095e-06

Optimization complete. Final v2v error: 2.5379230976104736 mm

Highest mean error: 2.8786568641662598 mm for frame 57

Lowest mean error: 2.155039072036743 mm for frame 0

Saving results

Total time: 26.898271322250366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480453
Iteration 2/25 | Loss: 0.00108996
Iteration 3/25 | Loss: 0.00099619
Iteration 4/25 | Loss: 0.00098124
Iteration 5/25 | Loss: 0.00097679
Iteration 6/25 | Loss: 0.00097532
Iteration 7/25 | Loss: 0.00097524
Iteration 8/25 | Loss: 0.00097524
Iteration 9/25 | Loss: 0.00097524
Iteration 10/25 | Loss: 0.00097524
Iteration 11/25 | Loss: 0.00097524
Iteration 12/25 | Loss: 0.00097524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009752447949722409, 0.0009752447949722409, 0.0009752447949722409, 0.0009752447949722409, 0.0009752447949722409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009752447949722409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74168444
Iteration 2/25 | Loss: 0.00091641
Iteration 3/25 | Loss: 0.00091640
Iteration 4/25 | Loss: 0.00091640
Iteration 5/25 | Loss: 0.00091640
Iteration 6/25 | Loss: 0.00091640
Iteration 7/25 | Loss: 0.00091640
Iteration 8/25 | Loss: 0.00091640
Iteration 9/25 | Loss: 0.00091640
Iteration 10/25 | Loss: 0.00091640
Iteration 11/25 | Loss: 0.00091640
Iteration 12/25 | Loss: 0.00091640
Iteration 13/25 | Loss: 0.00091640
Iteration 14/25 | Loss: 0.00091640
Iteration 15/25 | Loss: 0.00091640
Iteration 16/25 | Loss: 0.00091640
Iteration 17/25 | Loss: 0.00091640
Iteration 18/25 | Loss: 0.00091640
Iteration 19/25 | Loss: 0.00091640
Iteration 20/25 | Loss: 0.00091640
Iteration 21/25 | Loss: 0.00091640
Iteration 22/25 | Loss: 0.00091640
Iteration 23/25 | Loss: 0.00091640
Iteration 24/25 | Loss: 0.00091640
Iteration 25/25 | Loss: 0.00091640

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091640
Iteration 2/1000 | Loss: 0.00003141
Iteration 3/1000 | Loss: 0.00001921
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001368
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001253
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001248
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001246
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001242
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001239
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.2294383850530721e-05, 1.2294383850530721e-05, 1.2294383850530721e-05, 1.2294383850530721e-05, 1.2294383850530721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2294383850530721e-05

Optimization complete. Final v2v error: 2.998502016067505 mm

Highest mean error: 3.5800745487213135 mm for frame 73

Lowest mean error: 2.673302173614502 mm for frame 130

Saving results

Total time: 31.034899950027466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00748147
Iteration 2/25 | Loss: 0.00147172
Iteration 3/25 | Loss: 0.00113848
Iteration 4/25 | Loss: 0.00108730
Iteration 5/25 | Loss: 0.00106966
Iteration 6/25 | Loss: 0.00107354
Iteration 7/25 | Loss: 0.00106866
Iteration 8/25 | Loss: 0.00106219
Iteration 9/25 | Loss: 0.00106177
Iteration 10/25 | Loss: 0.00106463
Iteration 11/25 | Loss: 0.00106322
Iteration 12/25 | Loss: 0.00105735
Iteration 13/25 | Loss: 0.00104601
Iteration 14/25 | Loss: 0.00104288
Iteration 15/25 | Loss: 0.00104201
Iteration 16/25 | Loss: 0.00104197
Iteration 17/25 | Loss: 0.00104197
Iteration 18/25 | Loss: 0.00104196
Iteration 19/25 | Loss: 0.00104196
Iteration 20/25 | Loss: 0.00104196
Iteration 21/25 | Loss: 0.00104196
Iteration 22/25 | Loss: 0.00104196
Iteration 23/25 | Loss: 0.00104196
Iteration 24/25 | Loss: 0.00104196
Iteration 25/25 | Loss: 0.00104196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.33744907
Iteration 2/25 | Loss: 0.00117823
Iteration 3/25 | Loss: 0.00117812
Iteration 4/25 | Loss: 0.00117811
Iteration 5/25 | Loss: 0.00117811
Iteration 6/25 | Loss: 0.00117811
Iteration 7/25 | Loss: 0.00117811
Iteration 8/25 | Loss: 0.00117811
Iteration 9/25 | Loss: 0.00117811
Iteration 10/25 | Loss: 0.00117811
Iteration 11/25 | Loss: 0.00117811
Iteration 12/25 | Loss: 0.00117811
Iteration 13/25 | Loss: 0.00117811
Iteration 14/25 | Loss: 0.00117811
Iteration 15/25 | Loss: 0.00117811
Iteration 16/25 | Loss: 0.00117811
Iteration 17/25 | Loss: 0.00117811
Iteration 18/25 | Loss: 0.00117811
Iteration 19/25 | Loss: 0.00117811
Iteration 20/25 | Loss: 0.00117811
Iteration 21/25 | Loss: 0.00117811
Iteration 22/25 | Loss: 0.00117811
Iteration 23/25 | Loss: 0.00117811
Iteration 24/25 | Loss: 0.00117811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011781123466789722, 0.0011781123466789722, 0.0011781123466789722, 0.0011781123466789722, 0.0011781123466789722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011781123466789722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117811
Iteration 2/1000 | Loss: 0.00008974
Iteration 3/1000 | Loss: 0.00005375
Iteration 4/1000 | Loss: 0.00004500
Iteration 5/1000 | Loss: 0.00003961
Iteration 6/1000 | Loss: 0.00003662
Iteration 7/1000 | Loss: 0.00005417
Iteration 8/1000 | Loss: 0.00080348
Iteration 9/1000 | Loss: 0.00006506
Iteration 10/1000 | Loss: 0.00006157
Iteration 11/1000 | Loss: 0.00003866
Iteration 12/1000 | Loss: 0.00003531
Iteration 13/1000 | Loss: 0.00005508
Iteration 14/1000 | Loss: 0.00003793
Iteration 15/1000 | Loss: 0.00004773
Iteration 16/1000 | Loss: 0.00005090
Iteration 17/1000 | Loss: 0.00004382
Iteration 18/1000 | Loss: 0.00004604
Iteration 19/1000 | Loss: 0.00004375
Iteration 20/1000 | Loss: 0.00004358
Iteration 21/1000 | Loss: 0.00004181
Iteration 22/1000 | Loss: 0.00004613
Iteration 23/1000 | Loss: 0.00004219
Iteration 24/1000 | Loss: 0.00004467
Iteration 25/1000 | Loss: 0.00004170
Iteration 26/1000 | Loss: 0.00004566
Iteration 27/1000 | Loss: 0.00004316
Iteration 28/1000 | Loss: 0.00004251
Iteration 29/1000 | Loss: 0.00005046
Iteration 30/1000 | Loss: 0.00002958
Iteration 31/1000 | Loss: 0.00002777
Iteration 32/1000 | Loss: 0.00002684
Iteration 33/1000 | Loss: 0.00002587
Iteration 34/1000 | Loss: 0.00002514
Iteration 35/1000 | Loss: 0.00002466
Iteration 36/1000 | Loss: 0.00002442
Iteration 37/1000 | Loss: 0.00002441
Iteration 38/1000 | Loss: 0.00002426
Iteration 39/1000 | Loss: 0.00002423
Iteration 40/1000 | Loss: 0.00002412
Iteration 41/1000 | Loss: 0.00002406
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002402
Iteration 44/1000 | Loss: 0.00005504
Iteration 45/1000 | Loss: 0.00004616
Iteration 46/1000 | Loss: 0.00005512
Iteration 47/1000 | Loss: 0.00004647
Iteration 48/1000 | Loss: 0.00005529
Iteration 49/1000 | Loss: 0.00004416
Iteration 50/1000 | Loss: 0.00005622
Iteration 51/1000 | Loss: 0.00002937
Iteration 52/1000 | Loss: 0.00007590
Iteration 53/1000 | Loss: 0.00005896
Iteration 54/1000 | Loss: 0.00003290
Iteration 55/1000 | Loss: 0.00005854
Iteration 56/1000 | Loss: 0.00006661
Iteration 57/1000 | Loss: 0.00003178
Iteration 58/1000 | Loss: 0.00002720
Iteration 59/1000 | Loss: 0.00002578
Iteration 60/1000 | Loss: 0.00002503
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002370
Iteration 64/1000 | Loss: 0.00002336
Iteration 65/1000 | Loss: 0.00002310
Iteration 66/1000 | Loss: 0.00002302
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002300
Iteration 69/1000 | Loss: 0.00002296
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002294
Iteration 72/1000 | Loss: 0.00002294
Iteration 73/1000 | Loss: 0.00002294
Iteration 74/1000 | Loss: 0.00002294
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002292
Iteration 78/1000 | Loss: 0.00002292
Iteration 79/1000 | Loss: 0.00002289
Iteration 80/1000 | Loss: 0.00002289
Iteration 81/1000 | Loss: 0.00002289
Iteration 82/1000 | Loss: 0.00002288
Iteration 83/1000 | Loss: 0.00002288
Iteration 84/1000 | Loss: 0.00002288
Iteration 85/1000 | Loss: 0.00002287
Iteration 86/1000 | Loss: 0.00002287
Iteration 87/1000 | Loss: 0.00002286
Iteration 88/1000 | Loss: 0.00002286
Iteration 89/1000 | Loss: 0.00002286
Iteration 90/1000 | Loss: 0.00002285
Iteration 91/1000 | Loss: 0.00002285
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002283
Iteration 95/1000 | Loss: 0.00002282
Iteration 96/1000 | Loss: 0.00002282
Iteration 97/1000 | Loss: 0.00002282
Iteration 98/1000 | Loss: 0.00002282
Iteration 99/1000 | Loss: 0.00002282
Iteration 100/1000 | Loss: 0.00002282
Iteration 101/1000 | Loss: 0.00002282
Iteration 102/1000 | Loss: 0.00002281
Iteration 103/1000 | Loss: 0.00002281
Iteration 104/1000 | Loss: 0.00002280
Iteration 105/1000 | Loss: 0.00002280
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002274
Iteration 108/1000 | Loss: 0.00002271
Iteration 109/1000 | Loss: 0.00002270
Iteration 110/1000 | Loss: 0.00002270
Iteration 111/1000 | Loss: 0.00002269
Iteration 112/1000 | Loss: 0.00002269
Iteration 113/1000 | Loss: 0.00002268
Iteration 114/1000 | Loss: 0.00002268
Iteration 115/1000 | Loss: 0.00002261
Iteration 116/1000 | Loss: 0.00002261
Iteration 117/1000 | Loss: 0.00002260
Iteration 118/1000 | Loss: 0.00002260
Iteration 119/1000 | Loss: 0.00002259
Iteration 120/1000 | Loss: 0.00002259
Iteration 121/1000 | Loss: 0.00002258
Iteration 122/1000 | Loss: 0.00002257
Iteration 123/1000 | Loss: 0.00002257
Iteration 124/1000 | Loss: 0.00002257
Iteration 125/1000 | Loss: 0.00002256
Iteration 126/1000 | Loss: 0.00002256
Iteration 127/1000 | Loss: 0.00002256
Iteration 128/1000 | Loss: 0.00002256
Iteration 129/1000 | Loss: 0.00002255
Iteration 130/1000 | Loss: 0.00002254
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002250
Iteration 135/1000 | Loss: 0.00002249
Iteration 136/1000 | Loss: 0.00002248
Iteration 137/1000 | Loss: 0.00002248
Iteration 138/1000 | Loss: 0.00002248
Iteration 139/1000 | Loss: 0.00002248
Iteration 140/1000 | Loss: 0.00002248
Iteration 141/1000 | Loss: 0.00002247
Iteration 142/1000 | Loss: 0.00002246
Iteration 143/1000 | Loss: 0.00002246
Iteration 144/1000 | Loss: 0.00002246
Iteration 145/1000 | Loss: 0.00002246
Iteration 146/1000 | Loss: 0.00002246
Iteration 147/1000 | Loss: 0.00002246
Iteration 148/1000 | Loss: 0.00002246
Iteration 149/1000 | Loss: 0.00002245
Iteration 150/1000 | Loss: 0.00002245
Iteration 151/1000 | Loss: 0.00002245
Iteration 152/1000 | Loss: 0.00002245
Iteration 153/1000 | Loss: 0.00002244
Iteration 154/1000 | Loss: 0.00002244
Iteration 155/1000 | Loss: 0.00002244
Iteration 156/1000 | Loss: 0.00002244
Iteration 157/1000 | Loss: 0.00002244
Iteration 158/1000 | Loss: 0.00002243
Iteration 159/1000 | Loss: 0.00002243
Iteration 160/1000 | Loss: 0.00002243
Iteration 161/1000 | Loss: 0.00002243
Iteration 162/1000 | Loss: 0.00002243
Iteration 163/1000 | Loss: 0.00002243
Iteration 164/1000 | Loss: 0.00002243
Iteration 165/1000 | Loss: 0.00002242
Iteration 166/1000 | Loss: 0.00002242
Iteration 167/1000 | Loss: 0.00002242
Iteration 168/1000 | Loss: 0.00002242
Iteration 169/1000 | Loss: 0.00002242
Iteration 170/1000 | Loss: 0.00002242
Iteration 171/1000 | Loss: 0.00002242
Iteration 172/1000 | Loss: 0.00002242
Iteration 173/1000 | Loss: 0.00002242
Iteration 174/1000 | Loss: 0.00002242
Iteration 175/1000 | Loss: 0.00002242
Iteration 176/1000 | Loss: 0.00002241
Iteration 177/1000 | Loss: 0.00002241
Iteration 178/1000 | Loss: 0.00002241
Iteration 179/1000 | Loss: 0.00002241
Iteration 180/1000 | Loss: 0.00002241
Iteration 181/1000 | Loss: 0.00002241
Iteration 182/1000 | Loss: 0.00002241
Iteration 183/1000 | Loss: 0.00002240
Iteration 184/1000 | Loss: 0.00002240
Iteration 185/1000 | Loss: 0.00002240
Iteration 186/1000 | Loss: 0.00002240
Iteration 187/1000 | Loss: 0.00002240
Iteration 188/1000 | Loss: 0.00002240
Iteration 189/1000 | Loss: 0.00002240
Iteration 190/1000 | Loss: 0.00002240
Iteration 191/1000 | Loss: 0.00002240
Iteration 192/1000 | Loss: 0.00002240
Iteration 193/1000 | Loss: 0.00002240
Iteration 194/1000 | Loss: 0.00002240
Iteration 195/1000 | Loss: 0.00002240
Iteration 196/1000 | Loss: 0.00002240
Iteration 197/1000 | Loss: 0.00002240
Iteration 198/1000 | Loss: 0.00002240
Iteration 199/1000 | Loss: 0.00002239
Iteration 200/1000 | Loss: 0.00002239
Iteration 201/1000 | Loss: 0.00002239
Iteration 202/1000 | Loss: 0.00002239
Iteration 203/1000 | Loss: 0.00002239
Iteration 204/1000 | Loss: 0.00002239
Iteration 205/1000 | Loss: 0.00002239
Iteration 206/1000 | Loss: 0.00002239
Iteration 207/1000 | Loss: 0.00002239
Iteration 208/1000 | Loss: 0.00002239
Iteration 209/1000 | Loss: 0.00002239
Iteration 210/1000 | Loss: 0.00002239
Iteration 211/1000 | Loss: 0.00002239
Iteration 212/1000 | Loss: 0.00002239
Iteration 213/1000 | Loss: 0.00002239
Iteration 214/1000 | Loss: 0.00002239
Iteration 215/1000 | Loss: 0.00002239
Iteration 216/1000 | Loss: 0.00002239
Iteration 217/1000 | Loss: 0.00002239
Iteration 218/1000 | Loss: 0.00002239
Iteration 219/1000 | Loss: 0.00002239
Iteration 220/1000 | Loss: 0.00002239
Iteration 221/1000 | Loss: 0.00002239
Iteration 222/1000 | Loss: 0.00002239
Iteration 223/1000 | Loss: 0.00002239
Iteration 224/1000 | Loss: 0.00002239
Iteration 225/1000 | Loss: 0.00002239
Iteration 226/1000 | Loss: 0.00002239
Iteration 227/1000 | Loss: 0.00002239
Iteration 228/1000 | Loss: 0.00002239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.2391739548766054e-05, 2.2391739548766054e-05, 2.2391739548766054e-05, 2.2391739548766054e-05, 2.2391739548766054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2391739548766054e-05

Optimization complete. Final v2v error: 3.786324977874756 mm

Highest mean error: 6.07302188873291 mm for frame 63

Lowest mean error: 2.601229429244995 mm for frame 0

Saving results

Total time: 127.98381757736206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509666
Iteration 2/25 | Loss: 0.00139352
Iteration 3/25 | Loss: 0.00121975
Iteration 4/25 | Loss: 0.00116937
Iteration 5/25 | Loss: 0.00117267
Iteration 6/25 | Loss: 0.00113394
Iteration 7/25 | Loss: 0.00111147
Iteration 8/25 | Loss: 0.00108031
Iteration 9/25 | Loss: 0.00108143
Iteration 10/25 | Loss: 0.00107641
Iteration 11/25 | Loss: 0.00106056
Iteration 12/25 | Loss: 0.00105715
Iteration 13/25 | Loss: 0.00105632
Iteration 14/25 | Loss: 0.00105569
Iteration 15/25 | Loss: 0.00105883
Iteration 16/25 | Loss: 0.00105724
Iteration 17/25 | Loss: 0.00105500
Iteration 18/25 | Loss: 0.00105291
Iteration 19/25 | Loss: 0.00105266
Iteration 20/25 | Loss: 0.00105260
Iteration 21/25 | Loss: 0.00105260
Iteration 22/25 | Loss: 0.00105260
Iteration 23/25 | Loss: 0.00105259
Iteration 24/25 | Loss: 0.00105259
Iteration 25/25 | Loss: 0.00105259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40473175
Iteration 2/25 | Loss: 0.00152039
Iteration 3/25 | Loss: 0.00152039
Iteration 4/25 | Loss: 0.00152039
Iteration 5/25 | Loss: 0.00152039
Iteration 6/25 | Loss: 0.00152039
Iteration 7/25 | Loss: 0.00152039
Iteration 8/25 | Loss: 0.00152039
Iteration 9/25 | Loss: 0.00152039
Iteration 10/25 | Loss: 0.00152039
Iteration 11/25 | Loss: 0.00152039
Iteration 12/25 | Loss: 0.00152039
Iteration 13/25 | Loss: 0.00152039
Iteration 14/25 | Loss: 0.00152039
Iteration 15/25 | Loss: 0.00152039
Iteration 16/25 | Loss: 0.00152039
Iteration 17/25 | Loss: 0.00152039
Iteration 18/25 | Loss: 0.00152039
Iteration 19/25 | Loss: 0.00152039
Iteration 20/25 | Loss: 0.00152039
Iteration 21/25 | Loss: 0.00152039
Iteration 22/25 | Loss: 0.00152039
Iteration 23/25 | Loss: 0.00152039
Iteration 24/25 | Loss: 0.00152039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015203903894871473, 0.0015203903894871473, 0.0015203903894871473, 0.0015203903894871473, 0.0015203903894871473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015203903894871473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152039
Iteration 2/1000 | Loss: 0.00010975
Iteration 3/1000 | Loss: 0.00006473
Iteration 4/1000 | Loss: 0.00005055
Iteration 5/1000 | Loss: 0.00004586
Iteration 6/1000 | Loss: 0.00004202
Iteration 7/1000 | Loss: 0.00003963
Iteration 8/1000 | Loss: 0.00003843
Iteration 9/1000 | Loss: 0.00005266
Iteration 10/1000 | Loss: 0.00003780
Iteration 11/1000 | Loss: 0.00003635
Iteration 12/1000 | Loss: 0.00003580
Iteration 13/1000 | Loss: 0.00003554
Iteration 14/1000 | Loss: 0.00003532
Iteration 15/1000 | Loss: 0.00003506
Iteration 16/1000 | Loss: 0.00003815
Iteration 17/1000 | Loss: 0.00003488
Iteration 18/1000 | Loss: 0.00003441
Iteration 19/1000 | Loss: 0.00003396
Iteration 20/1000 | Loss: 0.00003364
Iteration 21/1000 | Loss: 0.00003325
Iteration 22/1000 | Loss: 0.00003301
Iteration 23/1000 | Loss: 0.00003280
Iteration 24/1000 | Loss: 0.00003255
Iteration 25/1000 | Loss: 0.00003229
Iteration 26/1000 | Loss: 0.00003210
Iteration 27/1000 | Loss: 0.00003194
Iteration 28/1000 | Loss: 0.00003178
Iteration 29/1000 | Loss: 0.00003177
Iteration 30/1000 | Loss: 0.00003177
Iteration 31/1000 | Loss: 0.00003174
Iteration 32/1000 | Loss: 0.00003172
Iteration 33/1000 | Loss: 0.00003164
Iteration 34/1000 | Loss: 0.00003163
Iteration 35/1000 | Loss: 0.00003162
Iteration 36/1000 | Loss: 0.00003161
Iteration 37/1000 | Loss: 0.00003161
Iteration 38/1000 | Loss: 0.00003161
Iteration 39/1000 | Loss: 0.00003160
Iteration 40/1000 | Loss: 0.00003160
Iteration 41/1000 | Loss: 0.00003159
Iteration 42/1000 | Loss: 0.00003158
Iteration 43/1000 | Loss: 0.00003158
Iteration 44/1000 | Loss: 0.00003157
Iteration 45/1000 | Loss: 0.00003157
Iteration 46/1000 | Loss: 0.00003157
Iteration 47/1000 | Loss: 0.00003155
Iteration 48/1000 | Loss: 0.00003155
Iteration 49/1000 | Loss: 0.00003154
Iteration 50/1000 | Loss: 0.00003154
Iteration 51/1000 | Loss: 0.00003154
Iteration 52/1000 | Loss: 0.00003154
Iteration 53/1000 | Loss: 0.00003154
Iteration 54/1000 | Loss: 0.00003154
Iteration 55/1000 | Loss: 0.00003154
Iteration 56/1000 | Loss: 0.00003154
Iteration 57/1000 | Loss: 0.00003153
Iteration 58/1000 | Loss: 0.00003153
Iteration 59/1000 | Loss: 0.00003153
Iteration 60/1000 | Loss: 0.00003153
Iteration 61/1000 | Loss: 0.00003153
Iteration 62/1000 | Loss: 0.00003152
Iteration 63/1000 | Loss: 0.00003152
Iteration 64/1000 | Loss: 0.00003152
Iteration 65/1000 | Loss: 0.00003152
Iteration 66/1000 | Loss: 0.00003152
Iteration 67/1000 | Loss: 0.00003151
Iteration 68/1000 | Loss: 0.00003151
Iteration 69/1000 | Loss: 0.00003151
Iteration 70/1000 | Loss: 0.00003150
Iteration 71/1000 | Loss: 0.00003150
Iteration 72/1000 | Loss: 0.00003150
Iteration 73/1000 | Loss: 0.00003150
Iteration 74/1000 | Loss: 0.00003149
Iteration 75/1000 | Loss: 0.00003149
Iteration 76/1000 | Loss: 0.00003149
Iteration 77/1000 | Loss: 0.00003149
Iteration 78/1000 | Loss: 0.00003149
Iteration 79/1000 | Loss: 0.00003148
Iteration 80/1000 | Loss: 0.00003148
Iteration 81/1000 | Loss: 0.00003148
Iteration 82/1000 | Loss: 0.00003148
Iteration 83/1000 | Loss: 0.00003148
Iteration 84/1000 | Loss: 0.00003148
Iteration 85/1000 | Loss: 0.00003147
Iteration 86/1000 | Loss: 0.00003147
Iteration 87/1000 | Loss: 0.00003147
Iteration 88/1000 | Loss: 0.00003147
Iteration 89/1000 | Loss: 0.00003146
Iteration 90/1000 | Loss: 0.00003146
Iteration 91/1000 | Loss: 0.00003146
Iteration 92/1000 | Loss: 0.00003146
Iteration 93/1000 | Loss: 0.00003146
Iteration 94/1000 | Loss: 0.00003145
Iteration 95/1000 | Loss: 0.00003145
Iteration 96/1000 | Loss: 0.00003145
Iteration 97/1000 | Loss: 0.00003145
Iteration 98/1000 | Loss: 0.00003145
Iteration 99/1000 | Loss: 0.00003145
Iteration 100/1000 | Loss: 0.00003145
Iteration 101/1000 | Loss: 0.00003145
Iteration 102/1000 | Loss: 0.00003145
Iteration 103/1000 | Loss: 0.00003145
Iteration 104/1000 | Loss: 0.00003145
Iteration 105/1000 | Loss: 0.00003145
Iteration 106/1000 | Loss: 0.00003144
Iteration 107/1000 | Loss: 0.00003144
Iteration 108/1000 | Loss: 0.00003143
Iteration 109/1000 | Loss: 0.00003143
Iteration 110/1000 | Loss: 0.00003141
Iteration 111/1000 | Loss: 0.00003141
Iteration 112/1000 | Loss: 0.00003141
Iteration 113/1000 | Loss: 0.00003141
Iteration 114/1000 | Loss: 0.00003140
Iteration 115/1000 | Loss: 0.00003140
Iteration 116/1000 | Loss: 0.00003140
Iteration 117/1000 | Loss: 0.00003139
Iteration 118/1000 | Loss: 0.00003139
Iteration 119/1000 | Loss: 0.00003139
Iteration 120/1000 | Loss: 0.00003138
Iteration 121/1000 | Loss: 0.00003138
Iteration 122/1000 | Loss: 0.00003137
Iteration 123/1000 | Loss: 0.00003137
Iteration 124/1000 | Loss: 0.00003137
Iteration 125/1000 | Loss: 0.00003137
Iteration 126/1000 | Loss: 0.00003136
Iteration 127/1000 | Loss: 0.00003136
Iteration 128/1000 | Loss: 0.00003136
Iteration 129/1000 | Loss: 0.00003136
Iteration 130/1000 | Loss: 0.00003135
Iteration 131/1000 | Loss: 0.00003135
Iteration 132/1000 | Loss: 0.00003135
Iteration 133/1000 | Loss: 0.00003134
Iteration 134/1000 | Loss: 0.00003134
Iteration 135/1000 | Loss: 0.00003134
Iteration 136/1000 | Loss: 0.00003134
Iteration 137/1000 | Loss: 0.00003134
Iteration 138/1000 | Loss: 0.00003133
Iteration 139/1000 | Loss: 0.00003133
Iteration 140/1000 | Loss: 0.00003133
Iteration 141/1000 | Loss: 0.00003133
Iteration 142/1000 | Loss: 0.00003132
Iteration 143/1000 | Loss: 0.00003132
Iteration 144/1000 | Loss: 0.00003132
Iteration 145/1000 | Loss: 0.00003132
Iteration 146/1000 | Loss: 0.00003132
Iteration 147/1000 | Loss: 0.00003132
Iteration 148/1000 | Loss: 0.00003132
Iteration 149/1000 | Loss: 0.00003131
Iteration 150/1000 | Loss: 0.00003131
Iteration 151/1000 | Loss: 0.00003131
Iteration 152/1000 | Loss: 0.00003131
Iteration 153/1000 | Loss: 0.00003131
Iteration 154/1000 | Loss: 0.00003130
Iteration 155/1000 | Loss: 0.00003130
Iteration 156/1000 | Loss: 0.00003130
Iteration 157/1000 | Loss: 0.00003130
Iteration 158/1000 | Loss: 0.00003130
Iteration 159/1000 | Loss: 0.00003130
Iteration 160/1000 | Loss: 0.00003130
Iteration 161/1000 | Loss: 0.00003129
Iteration 162/1000 | Loss: 0.00003129
Iteration 163/1000 | Loss: 0.00003129
Iteration 164/1000 | Loss: 0.00003129
Iteration 165/1000 | Loss: 0.00003129
Iteration 166/1000 | Loss: 0.00003129
Iteration 167/1000 | Loss: 0.00003129
Iteration 168/1000 | Loss: 0.00003129
Iteration 169/1000 | Loss: 0.00003129
Iteration 170/1000 | Loss: 0.00003129
Iteration 171/1000 | Loss: 0.00003129
Iteration 172/1000 | Loss: 0.00003129
Iteration 173/1000 | Loss: 0.00003129
Iteration 174/1000 | Loss: 0.00003129
Iteration 175/1000 | Loss: 0.00003129
Iteration 176/1000 | Loss: 0.00003129
Iteration 177/1000 | Loss: 0.00003129
Iteration 178/1000 | Loss: 0.00003129
Iteration 179/1000 | Loss: 0.00003129
Iteration 180/1000 | Loss: 0.00003128
Iteration 181/1000 | Loss: 0.00003128
Iteration 182/1000 | Loss: 0.00003128
Iteration 183/1000 | Loss: 0.00003128
Iteration 184/1000 | Loss: 0.00003128
Iteration 185/1000 | Loss: 0.00003128
Iteration 186/1000 | Loss: 0.00003128
Iteration 187/1000 | Loss: 0.00003128
Iteration 188/1000 | Loss: 0.00003128
Iteration 189/1000 | Loss: 0.00003128
Iteration 190/1000 | Loss: 0.00003128
Iteration 191/1000 | Loss: 0.00003127
Iteration 192/1000 | Loss: 0.00003127
Iteration 193/1000 | Loss: 0.00003127
Iteration 194/1000 | Loss: 0.00003127
Iteration 195/1000 | Loss: 0.00003127
Iteration 196/1000 | Loss: 0.00003127
Iteration 197/1000 | Loss: 0.00003127
Iteration 198/1000 | Loss: 0.00003127
Iteration 199/1000 | Loss: 0.00003127
Iteration 200/1000 | Loss: 0.00003127
Iteration 201/1000 | Loss: 0.00003127
Iteration 202/1000 | Loss: 0.00003127
Iteration 203/1000 | Loss: 0.00003127
Iteration 204/1000 | Loss: 0.00003126
Iteration 205/1000 | Loss: 0.00003126
Iteration 206/1000 | Loss: 0.00003126
Iteration 207/1000 | Loss: 0.00003126
Iteration 208/1000 | Loss: 0.00003126
Iteration 209/1000 | Loss: 0.00003126
Iteration 210/1000 | Loss: 0.00003126
Iteration 211/1000 | Loss: 0.00003126
Iteration 212/1000 | Loss: 0.00003126
Iteration 213/1000 | Loss: 0.00003126
Iteration 214/1000 | Loss: 0.00003126
Iteration 215/1000 | Loss: 0.00003126
Iteration 216/1000 | Loss: 0.00003126
Iteration 217/1000 | Loss: 0.00003126
Iteration 218/1000 | Loss: 0.00003125
Iteration 219/1000 | Loss: 0.00003125
Iteration 220/1000 | Loss: 0.00003125
Iteration 221/1000 | Loss: 0.00003125
Iteration 222/1000 | Loss: 0.00003125
Iteration 223/1000 | Loss: 0.00003125
Iteration 224/1000 | Loss: 0.00003125
Iteration 225/1000 | Loss: 0.00003125
Iteration 226/1000 | Loss: 0.00003125
Iteration 227/1000 | Loss: 0.00003125
Iteration 228/1000 | Loss: 0.00003125
Iteration 229/1000 | Loss: 0.00003125
Iteration 230/1000 | Loss: 0.00003124
Iteration 231/1000 | Loss: 0.00003124
Iteration 232/1000 | Loss: 0.00003124
Iteration 233/1000 | Loss: 0.00003124
Iteration 234/1000 | Loss: 0.00003124
Iteration 235/1000 | Loss: 0.00003124
Iteration 236/1000 | Loss: 0.00003124
Iteration 237/1000 | Loss: 0.00003124
Iteration 238/1000 | Loss: 0.00003124
Iteration 239/1000 | Loss: 0.00003124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [3.124317663605325e-05, 3.124317663605325e-05, 3.124317663605325e-05, 3.124317663605325e-05, 3.124317663605325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.124317663605325e-05

Optimization complete. Final v2v error: 3.7216131687164307 mm

Highest mean error: 11.372310638427734 mm for frame 107

Lowest mean error: 2.7188968658447266 mm for frame 173

Saving results

Total time: 93.57682371139526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_29_nl_5885/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_29_nl_5885/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518680
Iteration 2/25 | Loss: 0.00114770
Iteration 3/25 | Loss: 0.00100588
Iteration 4/25 | Loss: 0.00098896
Iteration 5/25 | Loss: 0.00098462
Iteration 6/25 | Loss: 0.00098372
Iteration 7/25 | Loss: 0.00098372
Iteration 8/25 | Loss: 0.00098372
Iteration 9/25 | Loss: 0.00098372
Iteration 10/25 | Loss: 0.00098372
Iteration 11/25 | Loss: 0.00098372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009837207617238164, 0.0009837207617238164, 0.0009837207617238164, 0.0009837207617238164, 0.0009837207617238164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009837207617238164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00239444
Iteration 2/25 | Loss: 0.00043341
Iteration 3/25 | Loss: 0.00043339
Iteration 4/25 | Loss: 0.00043339
Iteration 5/25 | Loss: 0.00043339
Iteration 6/25 | Loss: 0.00043339
Iteration 7/25 | Loss: 0.00043339
Iteration 8/25 | Loss: 0.00043339
Iteration 9/25 | Loss: 0.00043339
Iteration 10/25 | Loss: 0.00043339
Iteration 11/25 | Loss: 0.00043339
Iteration 12/25 | Loss: 0.00043339
Iteration 13/25 | Loss: 0.00043339
Iteration 14/25 | Loss: 0.00043339
Iteration 15/25 | Loss: 0.00043339
Iteration 16/25 | Loss: 0.00043339
Iteration 17/25 | Loss: 0.00043339
Iteration 18/25 | Loss: 0.00043339
Iteration 19/25 | Loss: 0.00043339
Iteration 20/25 | Loss: 0.00043339
Iteration 21/25 | Loss: 0.00043339
Iteration 22/25 | Loss: 0.00043339
Iteration 23/25 | Loss: 0.00043339
Iteration 24/25 | Loss: 0.00043339
Iteration 25/25 | Loss: 0.00043339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004333859833423048, 0.0004333859833423048, 0.0004333859833423048, 0.0004333859833423048, 0.0004333859833423048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004333859833423048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043339
Iteration 2/1000 | Loss: 0.00003826
Iteration 3/1000 | Loss: 0.00003171
Iteration 4/1000 | Loss: 0.00002826
Iteration 5/1000 | Loss: 0.00002613
Iteration 6/1000 | Loss: 0.00002547
Iteration 7/1000 | Loss: 0.00002471
Iteration 8/1000 | Loss: 0.00002421
Iteration 9/1000 | Loss: 0.00002397
Iteration 10/1000 | Loss: 0.00002368
Iteration 11/1000 | Loss: 0.00002360
Iteration 12/1000 | Loss: 0.00002359
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002340
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002331
Iteration 17/1000 | Loss: 0.00002331
Iteration 18/1000 | Loss: 0.00002331
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002330
Iteration 21/1000 | Loss: 0.00002330
Iteration 22/1000 | Loss: 0.00002330
Iteration 23/1000 | Loss: 0.00002330
Iteration 24/1000 | Loss: 0.00002330
Iteration 25/1000 | Loss: 0.00002330
Iteration 26/1000 | Loss: 0.00002330
Iteration 27/1000 | Loss: 0.00002330
Iteration 28/1000 | Loss: 0.00002329
Iteration 29/1000 | Loss: 0.00002329
Iteration 30/1000 | Loss: 0.00002329
Iteration 31/1000 | Loss: 0.00002329
Iteration 32/1000 | Loss: 0.00002329
Iteration 33/1000 | Loss: 0.00002329
Iteration 34/1000 | Loss: 0.00002329
Iteration 35/1000 | Loss: 0.00002329
Iteration 36/1000 | Loss: 0.00002329
Iteration 37/1000 | Loss: 0.00002329
Iteration 38/1000 | Loss: 0.00002329
Iteration 39/1000 | Loss: 0.00002329
Iteration 40/1000 | Loss: 0.00002329
Iteration 41/1000 | Loss: 0.00002329
Iteration 42/1000 | Loss: 0.00002329
Iteration 43/1000 | Loss: 0.00002329
Iteration 44/1000 | Loss: 0.00002329
Iteration 45/1000 | Loss: 0.00002329
Iteration 46/1000 | Loss: 0.00002329
Iteration 47/1000 | Loss: 0.00002329
Iteration 48/1000 | Loss: 0.00002329
Iteration 49/1000 | Loss: 0.00002329
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002329
Iteration 52/1000 | Loss: 0.00002329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [2.3286062059924006e-05, 2.3286062059924006e-05, 2.3286062059924006e-05, 2.3286062059924006e-05, 2.3286062059924006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3286062059924006e-05

Optimization complete. Final v2v error: 4.193729877471924 mm

Highest mean error: 4.441995143890381 mm for frame 81

Lowest mean error: 4.010934829711914 mm for frame 144

Saving results

Total time: 31.068469762802124
